Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
1,"Cassio Polpo de Campos, Alessandro Antonucci","Imprecision in Machine Learning and AI",2015,"","","","",1,"2022-07-13 10:06:51","","","","",,,,,1,0.14,1,2,7,"IN this note we consider five different relevant problems in AI and machine learning. We argue that possible solutions to such problems might be achieved by replacing the probability distributions in the systems with sets of them. Such a robust approach is based on the so-called impreciseprobabilistic framework. The proposed solutions provide a persuasive justification of the imprecise framework. The problems we consider are: • proper treatment of missing data, • reliable classification, • sensitivity analysis, • feature selection, • elicitation of qualitative expert knowledge. Before reporting a separate discussion for each problem, let us briefly resume the general ideas characterising imprecise-probabilistic methods.","",""
1,"Dan Kerrigan, J. Hullman, E. Bertini","A Survey of Domain Knowledge Elicitation in Applied Machine Learning",2021,"","","","",2,"2022-07-13 10:06:51","","10.3390/mti5120073","","",,,,,1,1.00,0,3,1,"Eliciting knowledge from domain experts can play an important role throughout the machine learning process, from correctly specifying the task to evaluating model results. However, knowledge elicitation is also fraught with challenges. In this work, we consider why and how machine learning researchers elicit knowledge from experts in the model development process. We develop a taxonomy to characterize elicitation approaches according to the elicitation goal, elicitation target, elicitation process, and use of elicited knowledge. We analyze the elicitation trends observed in 28 papers with this taxonomy and identify opportunities for adding rigor to these elicitation approaches. We suggest future directions for research in elicitation for machine learning by highlighting avenues for further exploration and drawing on what we can learn from elicitation research in other fields.","",""
1,"M. Staron, Helena Odenstedt Herg'es, S. Naredi, L. Block, Ali El-Merhi, Richard Vithal, M. Elam","Robust Machine Learning in Critical Care — Software Engineering and Medical Perspectives",2021,"","","","",3,"2022-07-13 10:06:51","","10.1109/WAIN52551.2021.00016","","",,,,,1,1.00,0,7,1,"Using machine learning in clinical practice poses hard requirements on explainability, reliability, replicability and robustness of these systems. Therefore, developing reliable software for monitoring critically ill patients requires close collaboration between physicians and software engineers. However, these two different disciplines need to find own research perspectives in order to contribute to both the medical and the software engineering domain. In this paper, we address the problem of how to establish a collaboration where software engineering and medicine meets to design robust machine learning systems to be used in patient care. We describe how we designed software systems for monitoring patients under carotid endarterectomy, in particular focusing on the process of knowledge building in the research team. Our results show what to consider when setting up such a collaboration, how it develops over time and what kind of systems can be constructed based on it. We conclude that the main challenge is to find a good research team, where different competences are committed to a common goal.","",""
7,"Jonathan Fürst, Mauricio Fadel Argerich, Bin Cheng, E. Kovacs","Towards Knowledge Infusion for Robust and Transferable Machine Learning in IoT",2020,"","","","",4,"2022-07-13 10:06:51","","","","",,,,,7,3.50,2,4,2,"Machine learning (ML) applications in Internet of Things (IoT) scenarios face the issue that supervision signals, such as labeled data, are scarce and expensive to obtain. For example, it often requires a human to manually label events in a data stream by observing the same events in the real world. In addition, the performance of trained models usually depends on a specific context: (1) location, (2) time and (3) data quality. This context is not static in reality, making it hard to achieve robust and transferable machine learning for IoT systems in practice. In this paper, we address these challenges with an envisioned method that we name Knowledge Infusion. First, we present two past case studies in which we combined external knowledge with traditional data-driven machine learning in IoT scenarios to ease the supervision effort: (1) a weak-supervision approach for the IoT domain to auto-generate labels based on external knowledge (e.g., domain knowledge) encoded in simple labeling functions. Our evaluation for transport mode classification achieves a micro-F1 score of 80.2%, with only seven labeling functions, on par with a fully supervised model that relies on hand-labeled data. (2) We introduce guiding functions to Reinforcement Learning (RL) to guide the agents' decisions and experience. In initial experiments, our guided reinforcement learning achieves more than three times higher reward in the beginning of its training than an agent with no external knowledge. We use the lessons learned from these experiences to develop our vision of knowledge infusion. In knowledge infusion, we aim to automate the inclusion of knowledge from existing knowledge bases and domain experts to combine it with traditional data-driven machine learning techniques during setup/training phase, but also during the execution phase.","",""
7,"Md. Kowsher, A. Tahabilder, S. Murad","Impact-Learning: A Robust Machine Learning Algorithm",2020,"","","","",5,"2022-07-13 10:06:51","","10.1145/3411174.3411185","","",,,,,7,3.50,2,3,2,"The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth.","",""
29,"Nishat Koti, Mahak Pancholi, A. Patra, A. Suresh","SWIFT: Super-fast and Robust Privacy-Preserving Machine Learning",2020,"","","","",6,"2022-07-13 10:06:51","","","","",,,,,29,14.50,7,4,2,"Performing ML computation on private data while maintaining data privacy aka Privacy-preserving Machine Learning (PPML) is an emergent field of research. Recently, PPML has seen a visible shift towards the adoption of Secure Outsourced Computation (SOC) paradigm, due to the heavy computation that it entails. In the SOC paradigm, computation is outsourced to a set of powerful and specially equipped servers that provide service on a pay-per-use basis. In this work, we propose SWIFT, a robust PPML framework for a range of ML algorithms in SOC setting, that guarantees output delivery to the users irrespective of any adversarial behaviour. Robustness, a highly desirable feature, evokes user participation without the fear of denial of service.  At the heart of our framework lies a highly-efficient, maliciously-secure, three-party computation (3PC) over rings that provides guaranteed output delivery (GOD) in the honest-majority setting. To the best of our knowledge, SWIFT is the first robust and efficient PPML framework in the 3PC setting. SWIFT is as fast as the best-known 3PC framework BLAZE (Patra et al. NDSS'20) which only achieves fairness. Fairness ensures either all or none receive the output, whereas GOD ensures guaranteed output delivery no matter what. We extend our 3PC framework for four parties (4PC). In this regime, SWIFT is as fast as the best known fair 4PC framework Trident (Chaudhari et al. NDSS'20) and twice faster than the best-known robust 4PC framework FLASH (Byali et al. PETS'20).  We demonstrate the practical relevance of our framework by benchmarking two important applications-- i) ML algorithms: Logistic Regression and Neural Network, and ii) Biometric matching, both over a 64-bit ring in WAN setting. Our readings reflect our claims as above.","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",7,"2022-07-13 10:06:51","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
3,"Suk Joon Hong, B. Bennett","Tackling Domain-Specific Winograd Schemas with Knowledge-Based Reasoning and Machine Learning",2020,"","","","",8,"2022-07-13 10:06:51","","10.4230/OASIcs.LDK.2021.41","","",,,,,3,1.50,2,2,2,"The Winograd Schema Challenge (WSC) is a common-sense reasoning task that requires background knowledge. In this paper, we contribute to tackling WSC in four ways. Firstly, we suggest a keyword method to define a restricted domain where distinctive high-level semantic patterns can be found. A thanking domain was defined by key-words, and the data set in this domain is used in our experiments. Secondly, we develop a high-level knowledge-based reasoning method using semantic roles which is based on the method of Sharma [2019]. Thirdly, we propose an ensemble method to combine knowledge-based reasoning and machine learning which shows the best performance in our experiments. As a machine learning method, we used Bidirectional Encoder Representations from Transformers (BERT) [Kocijan et al., 2019]. Lastly, in terms of evaluation, we suggest a ""robust"" accuracy measurement by modifying that of Trichelair et al. [2018]. As with their switching method, we evaluate a model by considering its performance on trivial variants of each sentence in the test set.","",""
75,"K. Kashinath, M. Mustafa, A. Albert, J.-L. Wu, C. Jiang, S. Esmaeilzadeh, K. Azizzadenesheli, R. Wang, A. Chattopadhyay, A. Singh, A. Manepalli, D. Chirila, R. Yu, R. Walters, B. White, H. Xiao, H. Tchelepi, P. Marcus, A. Anandkumar, P. Hassanzadeh, Prabhat","Physics-informed machine learning: case studies for weather and climate modelling",2021,"","","","",9,"2022-07-13 10:06:51","","10.1098/rsta.2020.0093","","",,,,,75,75.00,8,21,1,"Machine learning (ML) provides novel and powerful ways of accurately and efficiently recognizing complex patterns, emulating nonlinear dynamics, and predicting the spatio-temporal evolution of weather and climate processes. Off-the-shelf ML models, however, do not necessarily obey the fundamental governing laws of physical systems, nor do they generalize well to scenarios on which they have not been trained. We survey systematic approaches to incorporating physics and domain knowledge into ML models and distill these approaches into broad categories. Through 10 case studies, we show how these approaches have been used successfully for emulating, downscaling, and forecasting weather and climate processes. The accomplishments of these studies include greater physical consistency, reduced training time, improved data efficiency, and better generalization. Finally, we synthesize the lessons learned and identify scientific, diagnostic, computational, and resource challenges for developing truly robust and reliable physics-informed ML models for weather and climate processes. This article is part of the theme issue ‘Machine learning for weather and climate modelling’.","",""
665,"Weihua Hu, Matthias Fey, M. Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, J. Leskovec","Open Graph Benchmark: Datasets for Machine Learning on Graphs",2020,"","","","",10,"2022-07-13 10:06:51","","","","",,,,,665,332.50,83,8,2,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .","",""
2,"Lu Yin, Vlado Menkovski, Mykola Pechenizkiy","Knowledge Elicitation using Deep Metric Learning and Psychometric Testing",2020,"","","","",11,"2022-07-13 10:06:51","","10.1007/978-3-030-67661-2_10","","",,,,,2,1.00,1,3,2,"","",""
1,"Lu Yin","Beyond Labels: Knowledge Elicitation using Deep Metric Learning and Psychometric Testing",2020,"","","","",12,"2022-07-13 10:06:51","","","","",,,,,1,0.50,1,1,2,"Knowledge present in a domain is well expressed as relationships between corresponding concepts. For example, in zoology, animal species form complex hierarchies; in genomics, the different (parts of) molecules are organized in groups and subgroups based on their functions; plants, molecules, and astronomical objects all form complex taxonomies. Nevertheless, when applying supervised machine learning (ML) in such domains, we commonly reduce the complex and rich knowledge to a fixed set of labels. This oversimplifies and limits the potential impact that the ML solution can deliver. The main reason for such a reductionist approach is the difficulty in eliciting the domain knowledge from the experts. Developing a label structure with sufficient fidelity and providing comprehensive multilabel annotation can be exceedingly labor-intensive in many real-world applications. Here, we provide a method for efficient hierarchical knowledge elicitation (HKE) from experts working with highdimensional data such as images or videos. Our method is based on psychometric testing and active deep metric learning. The developed models embed the high-dimensional data in a metric space where distances are semantically meaningful, and the data can be organized in a hierarchical structure.","",""
5,"Carl Wilhjelm, Awad A. Younis","A Threat Analysis Methodology for Security Requirements Elicitation in Machine Learning Based Systems",2020,"","","","",13,"2022-07-13 10:06:51","","10.1109/QRS-C51114.2020.00078","","",,,,,5,2.50,3,2,2,"Machine learning (ML) models are now a key component for many applications. However, machine learning based systems (MLBSs), those systems that incorporate them, have proven vulnerable to various new attacks as a result. Currently, there exists no systematic process for eliciting security requirements for MLBSs that incorporates the identification of adversarial machine learning (AML) threats with those of a traditional non-MLBS. In this research study, we explore the applicability of traditional threat modeling and existing attack libraries in addressing MLBS security in the requirements phase. Using an example MLBS, we examined the applicability of 1) DFD and STRIDE in enumerating AML threats; 2) Microsoft SDL AI/ML Bug Bar in ranking the impact of the identified threats; and 3) the Microsoft AML attack library in eliciting threat mitigations to MLBSs. Such a method has the potential to assist team members, even with only domain specific knowledge, to collaboratively mitigate MLBS threats.","",""
16,"Yonghan Jung, Jin Tian, E. Bareinboim","Estimating Identifiable Causal Effects through Double Machine Learning",2021,"","","","",14,"2022-07-13 10:06:51","","","","",,,,,16,16.00,5,3,1,"Identifying causal effects from observational data is a perva- sive challenge found throughout the empirical sciences. Very general methods have been developed to decide the identiﬁ- ability of a causal quantity from a combination of observational data and causal knowledge about the underlying sys- tem. In practice, however, there are still challenges to estimating identiﬁable causal functionals from ﬁnite samples. Re- cently, a method known as double/debiased machine learning (DML) (Chernozhukov et al. 2018) has been proposed to learn parameters leveraging modern machine learning techniques, which is both robust to model misspeciﬁcation and bias-reducing. Still, DML has only been used for causal estimation in settings when the back-door condition (also known as conditional ignorability) holds. In this paper, we develop a new, general class of estimators for any identiﬁable causal functionals that exhibit DML properties, which we name DML-ID. In particular, we introduce a complete identiﬁca- tion algorithm that returns an inﬂuence function (IF) for any identiﬁable causal functional. We then construct the DML es- timator based on the derived IF. We show that DML-ID estimators hold the key properties of debiasedness and doubly robustness. Simulation results corroborate with the theory.","",""
21,"A. Naimi, Alan Mishler, Edward H. Kennedy","Challenges in Obtaining Valid Causal Effect Estimates with Machine Learning Algorithms.",2017,"","","","",15,"2022-07-13 10:06:51","","10.1093/aje/kwab201","","",,,,,21,4.20,7,3,5,"Unlike parametric regression, machine learning (ML) methods do not generally require precise knowledge of the true data generating mechanisms. As such, numerous authors have advocated for ML methods to estimate causal effects. Unfortunately, ML algorithmscan perform worse than parametric regression. We demonstrate the performance of ML-based single- and double-robust estimators. We use 100 Monte Carlo samples with sample sizes of 200, 1200, and 5000 to investigate bias and confidence interval coverage under several scenarios. In a simple confounding scenario, confounders were related to the treatment and the outcome via parametric models. In a complex confounding scenario, the simple confounders were transformed to induce complicated nonlinear relationships. In the simple scenario, when ML algorithms were used, double-robust estimators were superior to single-robust estimators. In the complex scenario, single-robust estimators with ML algorithms were at least as biased as estimators using misspecified parametric models. Double-robust estimators were less biased, but coverage was well below nominal. The use of sample splitting, inclusion of confounder interactions, reliance on a richly specified ML algorithm, and use of doubly robust estimators was the only explored approach that yielded negligible bias and nominal coverage. Our results suggest that ML based singly robust methods should be avoided.","",""
0,"Ziyi Yang, Zhaofeng Ye, Yijia Xiao, Chang-Yu Hsieh","SPLDExtraTrees: Robust machine learning approach for predicting kinase inhibitor resistance",2021,"","","","",16,"2022-07-13 10:06:51","","10.1093/bib/bbac050","","",,,,,0,0.00,0,4,1,"Drug resistance is a major threat to the global health and a significant concern throughout the clinical treatment of diseases and drug development. The mutation in proteins that is related to drug binding is a common cause for adaptive drug resistance. Therefore, quantitative estimations of how mutations would affect the interaction between a drug and the target protein would be of vital significance for the drug development and the clinical practice. Computational methods that rely on molecular dynamics simulations, Rosetta protocols, as well as machine learning methods have been proven to be capable of predicting ligand affinity changes upon protein mutation. However, the severely limited sample size and heavy noise induced overfitting and generalization issues have impeded wide adoption of machine learning for studying drug resistance. In this paper, we propose a robust machine learning method, termed SPLDExtraTrees, which can accurately predict ligand binding affinity changes upon protein mutation and identify resistance-causing mutations. Especially, the proposed method ranks training data following a specific scheme that starts with easy-to-learn samples and gradually incorporates harder and diverse samples into the training, and then iterates between sample weight recalculations and model updates. In addition, we calculate additional physics-based structural features to provide the machine learning model with the valuable domain knowledge on proteins for these data-limited predictive tasks. The experiments substantiate the capability of the proposed method for predicting kinase inhibitor resistance under three scenarios and achieve predictive accuracy comparable with that of molecular dynamics and Rosetta methods with much less computational costs.","",""
62,"Markus Borg, Cristofer Englund, K. Wnuk, B. Durán, Christoffer Levandowski, Shenjian Gao, Yanwen Tan, Henrik Kaijser, Henrik Lönn, J. Törnqvist","Safely Entering the Deep: A Review of Verification and Validation for Machine Learning and a Challenge Elicitation in the Automotive Industry",2018,"","","","",17,"2022-07-13 10:06:51","","10.2991/JASE.D.190131.001","","",,,,,62,15.50,6,10,4,"Deep Neural Networks (DNN) will emerge as a cornerstone in automotive software engineering. However, developing systems with DNNs introduces novel challenges for safety assessments. This paper reviews the state-of-the-art in verification and validation of safety-critical systems that rely on machine learning. Furthermore, we report from a workshop series on DNNs for perception with automotive experts in Sweden, confirming that ISO 26262 largely contravenes the nature of DNNs. We recommend aerospace-to-automotive knowledge transfer and systems-based safety approaches, e.g., safety cage architectures and simulated system test cases.","",""
0,"B. Boguslawski, Matthieu Boujonnier, Loryne Bissuel-Beauvais, Fahd Saghir","Edge Analytics at the Wellhead: Designing Robust Machine Learning Models for Artificial Lift Failure Detection",2018,"","","","",18,"2022-07-13 10:06:51","","10.2118/192886-MS","","",,,,,0,0.00,0,4,4,"  This paper outlines the challenges and constraints related to deployment of Machine Learning solutions for rod pump abnormal states recognition and diagnosis at the wellhead. Those abnormal states may lead to a failure or to non-optimized production. Particular focus is on two main aspects: 1) Develop a robust Machine Learning model & IIoT architecture to predict rod pump failure directly at the wellhead, 2) Ensure high level of pump failure prediction through Machine Learning to ensure operator confidence.  To the best of our knowledge, this is the first-of-its-kind IIoT Edge Analytics solution which provides operators with the capability of automated Dynagraph Card recognition directly at the wellhead via Machine Learning models. This solution also addresses end-user requirements in terms of confidentiality and communication infrastructure.","",""
6,"P. Paokanta","β-Thalassemia Knowledge Elicitation Using Data Engineering: PCA, Pearson's Chi Square and Machine Learning",2012,"","","","",19,"2022-07-13 10:06:51","","10.7763/IJCTE.2012.V4.561","","",,,,,6,0.60,6,1,10,"Data Engineering is one of the Knowledge Elicitation and Analysis methods, among serveral techniques; Feature Selection methods play an important role for these processes which are the processes in data mining technique esspecially classification tasks. The filtering process is an important pre-treatment for every classification process. Not only decreasing the computational time and cost, but selecting an appropriate variable is increasing the classification accuracy also. In this paper, the Thalassemia knowledge was elicited using Data engineering techniques (PCA, Pearson's Chi square and Machine Learning). This knowledge presented in form of the comparison of classification performance of machine learning techniques between using Principal Components Analysis (PCA) and Pearson's Chi square for screening the genotypes of β-Thalassemia patients. According to using PCA, the classification results show that the Multi-Layer Perceptron (MLP) is the best algorithm, providing that the percentage of accuracy reaches 86.61, K- Nearest Neighbors (KNN), NaiveBayes, Bayesian Networks (BNs) and Multinomial Logistic Regression with the percentage of accuracy 85.83, 85.04, 85.04 and 82.68. On the other hand, these results were compared to the Pearson's Chi Square and presented that…. In the future, we will search for the other feature selection techniques in order to improve the classification performance such as the hybrid method, filtering mathod etc.","",""
0,"H. Anh, Cao Van Kien","Robust extreme learning machine neural approach for uncertain nonlinear hyper‐chaotic system identification",2021,"","","","",20,"2022-07-13 10:06:51","","10.1002/rnc.5756","","",,,,,0,0.00,0,2,1,"This paper proposes a novel nonlinearly parameterized advanced single‐hidden layer neural extreme learning machine (ASHLN‐ELM) model in which the hidden and output weighting values are simultaneously updated using adaptively robust rules that are implemented based on Lyapunov stability principle. The proposed scheme guarantees the fast convergence speed of the state‐estimation residual errors bounded to null regarding to the influence of time‐varied disturbances. Additionally, proposed method needs no any knowledge related to desired weighting values or required approximating error. Typical uncertain hyper‐chaotic benchmark systems are used as to verify the new ASHLN‐ELM approach and to demonstrate the efficiency and the robustness of proposed method.","",""
127,"Lei Zhang, D. Zhang","Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation",2016,"","","","",21,"2022-07-13 10:06:51","","10.1109/TIP.2016.2598679","","",,,,,127,21.17,64,2,6,"We address the problem of visual knowledge adaptation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine (ELM)-based cross-domain network learning framework, that is called ELM-based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the ℓ2,1-norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross-domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as the base classifiers. The network output weights cannot only be analytically determined, but also transferrable. In addition, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semisupervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition demonstrate that our EDA methods outperform the existing cross-domain learning methods.","",""
12,"Atik Mahabub","A robust voting approach for diabetes prediction using traditional machine learning techniques",2019,"","","","",22,"2022-07-13 10:06:51","","10.1007/s42452-019-1759-7","","",,,,,12,4.00,12,1,3,"","",""
10,"Yifan Cui, E. Tchetgen","Selective machine learning of doubly robust functionals.",2019,"","","","",23,"2022-07-13 10:06:51","","","","",,,,,10,3.33,5,2,3,"While model selection is a well-studied topic in parametric and nonparametric regression or density estimation, model selection of possibly high-dimensional nuisance parameters in semiparametric problems is far less developed. In this paper, we propose a selective machine learning framework for making inferences about a finite-dimensional functional defined on a semiparametric model, when the latter admits a doubly robust estimating function. We introduce two model selection criteria for bias reduction of functional of interest, each based on a novel definition of pseudo-risk for the functional that embodies this double robustness property and thus may be used to select the candidate model that is nearest to fulfilling this property even when all models are wrong. We establish an oracle property for a multi-fold cross-validation version of the new model selection criteria which states that our empirical criteria perform nearly as well as an oracle with a priori knowledge of the pseudo-risk for each candidate model. We also describe a smooth approximation to the selection criteria which allows for valid post-selection inference. Finally, we apply the approach to model selection of a semiparametric estimator of average treatment effect given an ensemble of candidate machine learners to account for confounding in an observational study.","",""
1,"Hatim M. Elhassan Ibrahim, Nazir Ahmad, M. Rehman, Iqrar Ahmad, Rizwan Khan","Implementing and Automating Elicitation Technique Selection using Machine Learning",2019,"","","","",24,"2022-07-13 10:06:51","","10.1109/ICCIKE47802.2019.9004398","","",,,,,1,0.33,0,5,3,"Technique selection is one of the frequent issues in the requirement elicitation process; this issue has a major impact on the final requirement report output. The inappropriate selection of the techniques could lead to improper requirements and thus increase the risk of failure for the intended project. This paper addresses the technique selection issue encountered during the requirements elicitation stage, through a proposed a machine learning model to transfer the experts’ knowledge of elicitation technique selection of the less experienced. Based on the system analysts, stakeholders and technique properties as such systems and automate the technique selection process to provide the best optimization technique nomination, for the elicitation case complexity characteristics.","",""
18,"S. Fleming, A. Goodbody","A Machine Learning Metasystem for Robust Probabilistic Nonlinear Regression-Based Forecasting of Seasonal Water Availability in the US West",2019,"","","","",25,"2022-07-13 10:06:51","","10.1109/ACCESS.2019.2936989","","",,,,,18,6.00,9,2,3,"Hydroelectric power generation, water supplies for municipal, agricultural, manufacturing, and service industry uses including technology-sector requirements, dam safety, flood control, recreational uses, and ecological and legal constraints, all place simultaneous, competing demands on the heavily stressed water management infrastructure of the mostly arid American West. Optimally managing these resources depends on predicting water availability. We built a probabilistic nonlinear regression water supply forecast (WSF) technique for the US Department of Agriculture, which runs the largest stand-alone WSF system in the US West. Design criteria included improved accuracy over the existing system; uncertainty estimates that seamlessly handle complex (heteroscedastic, non-Gaussian) prediction errors; integration of physical hydrometeorological process knowledge and domain-specific expert experience; ability to accommodate nonlinearity, model selection uncertainty and equifinality, and predictor multicollinearity and high dimensionality; and relatively easy, low-cost implementation. Some methods satisfied some of these requirements but none met all, leading us to develop a novel, interdisciplinary, and pragmatic prediction metasystem through a carefully considered synthesis of well-established, off-the-shelf components and approaches, spanning supervised and unsupervised machine learning, nonparametric statistical modeling, ensemble learning, and evolutionary optimization, focusing on maintaining but radically updating the principal components regression framework widely used for WSF. Testing this integrated multi-method prediction engine demonstrated its value for river forecasting; USDA adoption is a landmark for transitioning machine learning from research into practice in this field. Its ability to handle all the foregoing design criteria and requirements, which are not unique to WSF, suggests potential for extension to complex probabilistic prediction problems in other fields.","",""
15,"Jungryul Seo, T. Laine, Kyung-ah Sohn","An Exploration of Machine Learning Methods for Robust Boredom Classification Using EEG and GSR Data",2019,"","","","",26,"2022-07-13 10:06:51","","10.3390/s19204561","","",,,,,15,5.00,5,3,3,"In recent years, affective computing has been actively researched to provide a higher level of emotion-awareness. Numerous studies have been conducted to detect the user’s emotions from physiological data. Among a myriad of target emotions, boredom, in particular, has been suggested to cause not only medical issues but also challenges in various facets of daily life. However, to the best of our knowledge, no previous studies have used electroencephalography (EEG) and galvanic skin response (GSR) together for boredom classification, although these data have potential features for emotion classification. To investigate the combined effect of these features on boredom classification, we collected EEG and GSR data from 28 participants using off-the-shelf sensors. During data acquisition, we used a set of stimuli comprising a video clip designed to elicit boredom and two other video clips of entertaining content. The collected samples were labeled based on the participants’ questionnaire-based testimonies on experienced boredom levels. Using the collected data, we initially trained 30 models with 19 machine learning algorithms and selected the top three candidate classifiers. After tuning the hyperparameters, we validated the final models through 1000 iterations of 10-fold cross validation to increase the robustness of the test results. Our results indicated that a Multilayer Perceptron model performed the best with a mean accuracy of 79.98% (AUC: 0.781). It also revealed the correlation between boredom and the combined features of EEG and GSR. These results can be useful for building accurate affective computing systems and understanding the physiological properties of boredom.","",""
5,"Ibrahim Yazici, Ömer Faruk Beyca, O. F. Gurcan, Halil Zaim, D. Delen, S. Zaim","A comparative analysis of machine learning techniques and fuzzy analytic hierarchy process to determine the tacit knowledge criteria",2020,"","","","",27,"2022-07-13 10:06:51","","10.1007/s10479-020-03697-3","","",,,,,5,2.50,1,6,2,"","",""
22,"F. Emmert‐Streib, M. Dehmer","A Machine Learning Perspective on Personalized Medicine: An Automized, Comprehensive Knowledge Base with Ontology for Pattern Recognition",2018,"","","","",28,"2022-07-13 10:06:51","","10.3390/MAKE1010009","","",,,,,22,5.50,11,2,4,"Personalized or precision medicine is a new paradigm that holds great promise for individualized patient diagnosis, treatment, and care. However, personalized medicine has only been described on an informal level rather than through rigorous practical guidelines and statistical protocols that would allow its robust practical realization for implementation in day-to-day clinical practice. In this paper, we discuss three key factors, which we consider dimensions that effect the experimental design for personalized medicine: (I) phenotype categories; (II) population size; and (III) statistical analysis. This formalization allows us to define personalized medicine from a machine learning perspective, as an automized, comprehensive knowledge base with an ontology that performs pattern recognition of patient profiles.","",""
29,"Violeta Mirchevska, M. Luštrek, M. Gams","Combining domain knowledge and machine learning for robust fall detection",2014,"","","","",29,"2022-07-13 10:06:51","","10.1111/exsy.12019","","",,,,,29,3.63,10,3,8,"This paper presents a method for combining domain knowledge and machine learning (CDKML) for classifier generation and online adaptation. The method exploits advantages in domain knowledge and machine learning as complementary information sources. Whereas machine learning may discover patterns in interest domains that are too subtle for humans to detect, domain knowledge may contain information on a domain not present in the available domain dataset. CDKML has three steps. First, prior domain knowledge is enriched with relevant patterns obtained by machine learning to create an initial classifier. Second, genetic algorithms refine the classifier. Third, the classifier is adapted online on the basis of user feedback using the Markov decision process. CDKML was applied in fall detection. Tests showed that the classifiers developed by CDKML have better performance than machine‐learning classifiers generated on a training dataset that does not adequately represent all real‐life cases of the learned concept. The accuracy of the initial classifier was 10 percentage points higher than the best machine‐learning classifier and the refinement added 3 percentage points. The online adaptation improved the accuracy of the refined classifier by an additional 15 percentage points.","",""
6,"F. Sufi, M. Alsulami","Knowledge Discovery of Global Landslides Using Automated Machine Learning Algorithms",2021,"","","","",30,"2022-07-13 10:06:51","","10.1109/ACCESS.2021.3115043","","",,,,,6,6.00,3,2,1,"Understanding the complex dynamics of global landslides is essential for disaster planners to make timely and effective decisions that save lives and reduce the economic impacts on society. Using NASA’s inventory of global landslide data, we developed a new machine learning (ML)–based system for town planners, disaster recovery strategists, and landslide researchers. Our system revealed hidden knowledge about a range of complex scenarios created from five landslide feature attributes. Users of our system can select from a list of $1.295\times {10}^{64}$ possible global landslide scenarios to discover valuable knowledge and predictions about the selected scenario in an interactive manner. Three ML algorithms—anomaly detection, decomposition analysis, and automated regression analysis—are used to elicit detailed knowledge about 25 scenarios selected from 14,532 global landslide records covering 12,220 injuries and 63,573 fatalities across 157 countries. Anomaly detection, logistic regression, and decomposition analysis performed well for all scenarios under study, with the area under the curve averaging 0.951, 0.911, and 0.896, respectively. Moreover, the prediction accuracy of linear regression had a mean absolute percentage error of 0.255. To the best of our knowledge, our scenario-based ML knowledge discovery system is the first of its kind to provide a comprehensive understanding of global landslide data.","",""
75,"M. Prosperi, Yi Guo, M. Sperrin, J. Koopman, Jae Min, Xing He, S. Rich, Mo Wang, I. Buchan, J. Bian","Causal inference and counterfactual prediction in machine learning for actionable healthcare",2020,"","","","",31,"2022-07-13 10:06:51","","10.1038/s42256-020-0197-y","","",,,,,75,37.50,8,10,2,"","",""
19,"A. Aravkin, Damek Davis","A SMART Stochastic Algorithm for Nonconvex Optimization with Applications to Robust Machine Learning",2016,"","","","",32,"2022-07-13 10:06:51","","","","",,,,,19,3.17,10,2,6,"In this paper, we show how to transform any optimization problem that arises from fitting a machine learning model into one that (1) detects and removes contaminated data from the training set while (2) simultaneously fitting the trimmed model on the uncontaminated data that remains. To solve the resulting nonconvex optimization problem, we introduce a fast stochastic proximal-gradient algorithm that incorporates prior knowledge through nonsmooth regularization. For datasets of size $n$, our approach requires $O(n^{2/3}/\varepsilon)$ gradient evaluations to reach $\varepsilon$-accuracy and, when a certain error bound holds, the complexity improves to $O(\kappa n^{2/3}\log(1/\varepsilon))$. These rates are $n^{1/3}$ times better than those achieved by typical, full gradient methods.","",""
4,"Alexandra Renouard, A. Maggi, M. Grunberg, C. Doubre, C. Hibert","Toward False Event Detection and Quarry Blast versus Earthquake Discrimination in an Operational Setting Using Semiautomated Machine Learning",2021,"","","","",33,"2022-07-13 10:06:51","","10.1785/0220200305","","",,,,,4,4.00,1,5,1,"  Small-magnitude earthquakes shed light on the spatial and magnitude distribution of natural seismicity, as well as its rate and occurrence, especially in stable continental regions where natural seismicity remains difficult to explain under slow strain-rate conditions. However, capturing them in catalogs is strongly hindered by signal-to-noise ratio issues, resulting in high rates of false and man-made events also being detected. Accurate and robust discrimination of these events is critical for optimally detecting small earthquakes. This requires uncovering recurrent salient features that can rapidly distinguish first false events from real events, then earthquakes from man-made events (mainly quarry blasts), despite high signal variability and noise content. In this study, we combined the complementary strengths of human and interpretable rule-based machine-learning algorithms for solving this classification problem. We used human expert knowledge to co-create two reliable machine-learning classifiers through human-assisted selection of classification features and review of events with uncertain classifier predictions. The two classifiers are integrated into the SeisComP3 operational monitoring system. The first one discards false events from the set of events obtained with a low short-term average/long-term average threshold; the second one labels the remaining events as either earthquakes or quarry blasts. When run in an operational setting, the first classifier correctly detected more than 99% of false events and just over 93% of earthquakes; the second classifier correctly labeled 95% of quarry blasts and 96% of earthquakes. After a manual review of the second classifier low-confidence outputs, the final catalog contained fewer than 2% of misclassified events. These results confirm that machine learning strengthens the quality of earthquake catalogs and that the performance of machine-learning classifiers can be improved through human expertise. Our study promotes a broader implication of hybrid intelligence monitoring within seismological observatories.","",""
5,"Furkan M. Torun, S. V. Winter, Sophia Doll, Felix M. Riese, A. Vorobyev, Johannes B. Mueller‐Reif, Philipp E. Geyer, Maximilian T. Strauss","Transparent exploration of machine learning for biomarker discovery from proteomics and omics data",2021,"","","","",34,"2022-07-13 10:06:51","","10.1101/2021.03.05.434053","","",,,,,5,5.00,1,8,1,"Biomarkers are of central importance for assessing the health state and to guide medical interventions and their efficacy, but they are lacking for most diseases. Mass spectrometry (MS)-based proteomics is a powerful technology for biomarker discovery, but requires sophisticated bioinformatics to identify robust patterns. Machine learning (ML) has become indispensable for this purpose, however, it is sometimes applied in an opaque manner, generally requires expert knowledge and complex and expensive software. To enable easy access to ML for biomarker discovery without any programming or bioinformatic skills, we developed ‘OmicLearn’ (https://OmicLearn.com), an open-source web-based ML tool using the latest advances in the Python ML ecosystem. We host a web server for the exploration of the researcher’s results that can readily be cloned for internal use. Output tables from proteomics experiments are easily uploaded to the central or a local webserver. OmicLearn enables rapid exploration of the suitability of various ML algorithms for the experimental datasets. It fosters open science via transparent assessment of state-of-the-art algorithms in a standardized format for proteomics and other omics sciences. Graphical Abstract Highlights OmicLearn is an open-source platform allows researchers to apply machine learning (ML) for biomarker discovery The ready-to-use structure of OmicLearn enables accessing state-of-the-art ML algorithms without requiring any prior bioinformatics knowledge OmicLearn’s web-based interface provides an easy-to-follow platform for classification and gaining insights into the dataset Several algorithms and methods for preprocessing, feature selection, classification and cross-validation of omics datasets are integrated All results, settings and method text can be exported in publication-ready formats","",""
37,"Efstathios D. Gennatas, J. Friedman, L. Ungar, R. Pirracchio, Eric Eaton, L. Reichman, Y. Interian, C. Simone, A. Auerbach, E. Delgado, M. J. Laan, T. Solberg, G. Valdes","Expert-augmented machine learning",2019,"","","","",35,"2022-07-13 10:06:51","","10.1073/pnas.1906831117","","",,,,,37,12.33,4,13,3,"Significance Machine learning is increasingly used across fields to derive insights from data, which further our understanding of the world and help us anticipate the future. The performance of predictive modeling is dependent on the amount and quality of available data. In practice, we rely on human experts to perform certain tasks and on machine learning for others. However, the optimal learning strategy may involve combining the complementary strengths of humans and machines. We present expert-augmented machine learning, an automated way to automatically extract problem-specific human expert knowledge and integrate it with machine learning to build robust, dependable, and data-efficient predictive models. Machine learning is proving invaluable across disciplines. However, its success is often limited by the quality and quantity of available data, while its adoption is limited by the level of trust afforded by given models. Human vs. machine performance is commonly compared empirically to decide whether a certain task should be performed by a computer or an expert. In reality, the optimal learning strategy may involve combining the complementary strengths of humans and machines. Here, we present expert-augmented machine learning (EAML), an automated method that guides the extraction of expert knowledge and its integration into machine-learned models. We used a large dataset of intensive-care patient data to derive 126 decision rules that predict hospital mortality. Using an online platform, we asked 15 clinicians to assess the relative risk of the subpopulation defined by each rule compared to the total sample. We compared the clinician-assessed risk to the empirical risk and found that, while clinicians agreed with the data in most cases, there were notable exceptions where they overestimated or underestimated the true risk. Studying the rules with greatest disagreement, we identified problems with the training data, including one miscoded variable and one hidden confounder. Filtering the rules based on the extent of disagreement between clinician-assessed risk and empirical risk, we improved performance on out-of-sample data and were able to train with less data. EAML provides a platform for automated creation of problem-specific priors, which help build robust and dependable machine-learning models in critical applications.","",""
5,"Katharina Beckh, Sebastian Muller, Matthias Jakobs, Vanessa Toborek, Hanxiao Tan, Raphael Fischer, Pascal Welke, Sebastian Houben, Laura von Rueden","Explainable Machine Learning with Prior Knowledge: An Overview",2021,"","","","",36,"2022-07-13 10:06:51","","","","",,,,,5,5.00,1,9,1,"This survey presents an overview of integrating prior knowledge into machine learning systems in order to improve explainability. The complexity of machine learning models has elicited research to make them more explainable. However, most explainability methods cannot provide insight beyond the given data, requiring additional information about the context. We propose to harness prior knowledge to improve upon the explanation capabilities of machine learning models. In this paper, we present a categorization of current research into three main categories which either integrate knowledge into the machine learning pipeline, into the explainability method or derive knowledge from explanations. To classify the papers, we build upon the existing taxonomy of informed machine learning and extend it from the perspective of explainability. We conclude with open challenges and research directions.","",""
3,"Dongsheng Xiao, Brandon Jonathan Forys, M. Vanni, T. Murphy","MesoNet allows automated scaling and segmentation of mouse mesoscale cortical maps using machine learning",2021,"","","","",37,"2022-07-13 10:06:51","","10.1038/s41467-021-26255-2","","",,,,,3,3.00,1,4,1,"","",""
2,"A. Fernández-Fontelo, Pascal J. Kieslich, Felix Henninger, F. Kreuter, S. Greven","Predicting Question Difficulty in Web Surveys: A Machine Learning Approach Based on Mouse Movement Features",2021,"","","","",38,"2022-07-13 10:06:51","","10.1177/08944393211032950","","",,,,,2,2.00,0,5,1,"Survey research aims to collect robust and reliable data from respondents. However, despite researchers’ efforts in designing questionnaires, survey instruments may be imperfect, and question structure not as clear as could be, thus creating a burden for respondents. If it were possible to detect such problems, this knowledge could be used to predict problems in a questionnaire during pretesting, inform real-time interventions through responsive questionnaire design, or to indicate and correct measurement error after the fact. Previous research has used paradata, specifically response times, to detect difficulties and help improve user experience and data quality. Today, richer data sources are available, for example, movements respondents make with their mouse, as an additional detailed indicator for the respondent–survey interaction. This article uses machine learning techniques to explore the predictive value of mouse-tracking data regarding a question’s difficulty. We use data from a survey on respondents’ employment history and demographic information, in which we experimentally manipulate the difficulty of several questions. Using measures derived from mouse movements, we predict whether respondents have answered the easy or difficult version of a question, using and comparing several state-of-the-art supervised learning methods. We have also developed a personalization method that adjusts for respondents’ baseline mouse behavior and evaluate its performance. For all three manipulated survey questions, we find that including the full set of mouse movement measures and accounting for individual differences in these measures improve prediction performance over response-time-only models.","",""
6,"C. Yeomans, R. Shail, S. Grebby, V. Nykänen, M. Middleton, P. Lusty","A machine learning approach to tungsten prospectivity modelling using knowledge-driven feature extraction and model confidence",2019,"","","","",39,"2022-07-13 10:06:51","","10.31223/osf.io/9fet8","","",,,,,6,2.00,1,6,3,"Abstract Novel mineral prospectivity modelling presented here applies knowledge-driven feature extraction to a data-driven machine learning approach for tungsten mineralisation. The method emphasises the importance of appropriate model evaluation and develops a new Confidence Metric to generate spatially refined and robust exploration targets. The data-driven Random Forest™ algorithm is employed to model tungsten mineralisation in SW England using a range of geological, geochemical and geophysical evidence layers which include a depth to granite evidence layer. Two models are presented, one using standardised input variables and a second that implements fuzzy set theory as part of an augmented feature extraction step. The use of fuzzy data transformations mean feature extraction can incorporate some user-knowledge about the mineralisation into the model. The typically subjective approach is guided using the Receiver Operating Characteristics (ROC) curve tool where transformed data are compared to known training samples. The modelling is conducted using 34 known true positive samples with 10 sets of randomly generated true negative samples to test the random effect on the model. The two models have similar accuracy but show different spatial distributions when identifying highly prospective targets. Areal analysis shows that the fuzzy-transformed model is a better discriminator and highlights three areas of high prospectivity that were not previously known. The Confidence Metric, derived from model variance, is employed to further evaluate the models. The new metric is useful for refining exploration targets and highlighting the most robust areas for follow-up investigation. The fuzzy-transformed model is shown to contain larger areas of high model confidence compared to the model using standardised variables. Finally, legacy mining data, from drilling reports and mine descriptions, is used to further validate the fuzzy-transformed model and gauge the depth of potential deposits. Descriptions of mineralisation corroborate that the targets generated in these models could be undercover at depths of less than 300 ​m. In summary, the modelling workflow presented herein provides a novel integration of knowledge-driven feature extraction with data-driven machine learning modelling, while the newly derived Confidence Metric generates reliable mineral exploration targets.","",""
168,"Tao Lin, Lingjing Kong, Sebastian U. Stich, Martin Jaggi","Ensemble Distillation for Robust Model Fusion in Federated Learning",2020,"","","","",40,"2022-07-13 10:06:51","","","","",,,,,168,84.00,42,4,2,"Federated Learning (FL) is a machine learning setting where many devices collaboratively train a machine learning model while keeping the training data decentralized. In most of the current training schemes the central model is refined by averaging the parameters of the server model and the updated parameters from the client side. However, directly averaging model parameters is only possible if all models have the same structure and size, which could be a restrictive constraint in many scenarios.  In this work we investigate more powerful and more flexible aggregation schemes for FL. Specifically, we propose ensemble distillation for model fusion, i.e. training the central classifier through unlabeled data on the outputs of the models from the clients. This knowledge distillation technique mitigates privacy risk and cost to the same extent as the baseline FL algorithms, but allows flexible aggregation over heterogeneous client models that can differ e.g. in size, numerical precision or structure. We show in extensive empirical experiments on various CV/NLP datasets (CIFAR-10/100, ImageNet, AG News, SST2) and settings (heterogeneous models/data) that the server model can be trained much faster, requiring fewer communication rounds than any existing FL technique so far.","",""
20,"Vida Groznik, Matej Guid, A. Sadikov, M. Mozina, D. Georgiev, Veronika Kragelj, S. Ribaric, Z. Pirtošek, I. Bratko","Elicitation of neurological knowledge with argument-based machine learning",2013,"","","","",41,"2022-07-13 10:06:51","","10.1016/j.artmed.2012.08.003","","",,,,,20,2.22,2,9,9,"","",""
242,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",42,"2022-07-13 10:06:51","","10.1145/3308809.3308857","","",,,,,242,48.40,81,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server andm working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of them working machines suffer Byzantine faults - a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.","",""
19,"Yannick Suter, U. Knecht, Mariana Alão, W. Valenzuela, E. Hewer, P. Schucht, R. Wiest, M. Reyes","Radiomics for glioblastoma survival analysis in pre-operative MRI: exploring feature robustness, class boundaries, and machine learning techniques",2020,"","","","",43,"2022-07-13 10:06:51","","10.1186/s40644-020-00329-8","","",,,,,19,9.50,2,8,2,"","",""
87,"G. Peng, M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, Savador Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Multiscale modeling meets machine learning: What can we learn?",2019,"","","","",44,"2022-07-13 10:06:51","","10.1007/s11831-020-09405-5","","",,,,,87,29.00,9,12,3,"","",""
12,"H. Sufriyana, Yu-Wei Wu, E. C. Su","Prediction of Preeclampsia and Intrauterine Growth Restriction: Development of Machine Learning Models on a Prospective Cohort",2020,"","","","",45,"2022-07-13 10:06:51","","10.2196/15411","","",,,,,12,6.00,4,3,2,"Background Preeclampsia and intrauterine growth restriction are placental dysfunction–related disorders (PDDs) that require a referral decision be made within a certain time period. An appropriate prediction model should be developed for these diseases. However, previous models did not demonstrate robust performances and/or they were developed from datasets with highly imbalanced classes. Objective In this study, we developed a predictive model of PDDs by machine learning that uses features at 24-37 weeks’ gestation, including maternal characteristics, uterine artery (UtA) Doppler measures, soluble fms-like tyrosine kinase receptor-1 (sFlt-1), and placental growth factor (PlGF). Methods A public dataset was taken from a prospective cohort study that included pregnant women with PDDs (66/95, 69%) and a control group (29/95, 31%). Preliminary selection of features was based on a statistical analysis using SAS 9.4 (SAS Institute). We used Weka (Waikato Environment for Knowledge Analysis) 3.8.3 (The University of Waikato, Hamilton, NZ) to automatically select the best model using its optimization algorithm. We also manually selected the best of 23 white-box models. Models, including those from recent studies, were also compared by interval estimation of evaluation metrics. We used the Matthew correlation coefficient (MCC) as the main metric. It is not overoptimistic to evaluate the performance of a prediction model developed from a dataset with a class imbalance. Repeated 10-fold cross-validation was applied. Results The classification via regression model was chosen as the best model. Our model had a robust MCC (.93, 95% CI .87-1.00, vs .64, 95% CI .57-.71) and specificity (100%, 95% CI 100-100, vs 90%, 95% CI 90-90) compared to each metric of the best models from recent studies. The sensitivity of this model was not inferior (95%, 95% CI 91-100, vs 100%, 95% CI 92-100). The area under the receiver operating characteristic curve was also competitive (0.970, 95% CI 0.966-0.974, vs 0.987, 95% CI 0.980-0.994). Features in the best model were maternal weight, BMI, pulsatility index of the UtA, sFlt-1, and PlGF. The most important feature was the sFlt-1/PlGF ratio. This model used an M5P algorithm consisting of a decision tree and four linear models with different thresholds. Our study was also better than the best ones among recent studies in terms of the class balance and the size of the case class (66/95, 69%, vs 27/239, 11.3%). Conclusions Our model had a robust predictive performance. It was also developed to deal with the problem of a class imbalance. In the context of clinical management, this model may improve maternal mortality and neonatal morbidity and reduce health care costs.","",""
7,"Jina Suh, S. Ghorashi, Gonzalo A. Ramos, N. Chen, S. Drucker, J. Verwey, P. Simard","AnchorViz: Facilitating Semantic Data Exploration and Concept Discovery for Interactive Machine Learning",2019,"","","","",46,"2022-07-13 10:06:51","","10.1145/3241379","","",,,,,7,2.33,1,7,3,"When building a classifier in interactive machine learning (iML), human knowledge about the target class can be a powerful reference to make the classifier robust to unseen items. The main challenge lies in finding unlabeled items that can either help discover or refine concepts for which the current classifier has no corresponding features (i.e., it has feature blindness). Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare concepts that are hard to recall. This article presents AnchorViz, an interactive visualization that facilitates the discovery of prediction errors and previously unseen concepts through human-driven semantic data exploration. By creating example-based or dictionary-based anchors representing concepts, users create a topology that (a) spreads data based on their similarity to the concepts and (b) surfaces the prediction and label inconsistencies between data points that are semantically related. Once such inconsistencies and errors are discovered, users can encode the new information as labels or features and interact with the retrained classifier to validate their actions in an iterative loop. We evaluated AnchorViz through two user studies. Our results show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods. Furthermore, during the beginning stages of a training task, an iML tool with AnchorViz can help users build classifiers comparable to the ones built with the same tool with uncertainty sampling and keyword search, but with fewer labels and more generalizable features. We discuss exploration strategies observed during the two studies and how AnchorViz supports discovering, labeling, and refining of concepts through a sensemaking loop.","",""
7,"D. Jacob","Cross-Fitting and Averaging for Machine Learning Estimation of Heterogeneous Treatment Effects",2020,"","","","",47,"2022-07-13 10:06:51","","","","",,,,,7,3.50,7,1,2,"We investigate the finite sample performance of sample splitting, cross-fitting and averaging for the estimation of the conditional average treatment effect. Recently proposed methods, so-called meta-learners, make use of machine learning to estimate different nuisance functions and hence allow for fewer restrictions on the underlying structure of the data. To limit a potential overfitting bias that may result when using machine learning methods, cross-fitting estimators have been proposed. This includes the splitting of the data in different folds to reduce bias and averaging over folds to restore efficiency. To the best of our knowledge, it is not yet clear how exactly the data should be split and averaged. We employ a Monte Carlo study with different data generation processes and consider twelve different estimators that vary in sample-splitting, cross-fitting and averaging procedures. We investigate the performance of each estimator independently on four different meta-learners: the doubly-robust-learner, R-learner, T-learner and X-learner. We find that the performance of all meta-learners heavily depends on the procedure of splitting and averaging. The best performance in terms of mean squared error (MSE) among the sample split estimators can be achieved when applying cross-fitting plus taking the median over multiple different sample-splitting iterations. Some meta-learners exhibit a high variance when the lasso is included in the ML methods. Excluding the lasso decreases the variance and leads to robust and at least competitive results.","",""
6,"Syed Javeed Pasha, E. Mohamed","Novel Feature Reduction (NFR) Model With Machine Learning and Data Mining Algorithms for Effective Disease Risk Prediction",2020,"","","","",48,"2022-07-13 10:06:51","","10.1109/ACCESS.2020.3028714","","",,,,,6,3.00,3,2,2,"Presently, the application of machine learning (ML) and data mining (DM) techniques have a vital role in healthcare systems and wisely convert all obtainable data into beneficial knowledge. It is proven from the literature works that a chance of 12% error remains in the diagnosis of the diseases by the medical practitioners. Moreover, for effective disease risk prediction in medical analysis, more emphasis is accorded to the area under the curve (AUC) with accuracy as an evaluation metric. However, the role of the AUC has not been previously characterized notably. In this research article, a novel feature reduction (NFR) model that is aligned with the ML and DM algorithms is proposed to reduce the error rate and further improve the performance. The proposed NFR model comprises of two approaches and uses the AUC in addition to the accuracy to achieve a robust and effective disease risk prediction. The first approach is based on a heuristic process evaluating performance by reducing features with respect to the improvement in the AUC besides the accuracy as evaluation metrics, working to obtain the best subset of highly contributing features in the prediction. The second approach evaluates the accuracy and AUC of all individual features and forms the subsets with the highest accuracies, AUCs, and least difference between them, which are combined in various combinations to achieve the best-reduced set of highly relevant features. For this purpose, the benchmarked public heart datasets of the ML repository of the University of California, Irvine (UCI) are tested; the results are promising. The highest accuracy and AUC achieved with the proposed NFR model are 95.52% and 99.20% with 41.67% feature reduction, respectively. The accuracy is 4.22% higher than recent existing research with a significant improvement of 25% in the performance of the running time of the algorithm.","",""
1,"Anay Raj","Malaria Disease Diagnosis using Machine Learning Techniques",2020,"","","","",49,"2022-07-13 10:06:51","","","","",,,,,1,0.50,1,1,2,"Malaria is a major infectious disease of humans, with roughly 200 million cases worldwide and more than 400,000 deaths per year. Malaria could be prevented, controlled, and cured more effectively if a more accurate and efficient diagnostic method was available. The standard diagnostic method for malaria is the microscopic examination of blood smears for infected erythrocytes by qualified microscopists. However, this method is inefficient and the quality of the diagnosis depends on the experience and knowledge of the microscopists. This study proposes a new and robust machine learning model based on a Convolutional neural network (CNN) to automatically classify single cells in thin blood smears on standard microscope slides as either infected or uninfected. This will help in the faster diagnosis of malaria and save valuable time for beginning the treatment.","",""
25,"H. Yoon, Jae-Hoon Sim, M. Han","Analytic continuation via domain knowledge free machine learning",2018,"","","","",50,"2022-07-13 10:06:51","","10.1103/PhysRevB.98.245101","","",,,,,25,6.25,8,3,4,"We present a machine-learning approach to a long-standing issue in quantum many-body physics, namely, analytic continuation. This notorious ill-conditioned problem of obtaining spectral function from an imaginary time Green's function has been a focus of new method developments for past decades. Here we demonstrate the usefulness of modern machine-learning techniques including convolutional neural networks and the variants of a stochastic gradient descent optimizer. The machine-learning continuation kernel is successfully realized without any ``domain knowledge,'' which means that any physical ``prior'' is not utilized in the kernel construction and the neural networks ``learn'' the knowledge solely from ``training.'' The outstanding performance is achieved for both insulating and metallic band structure. Our machine-learning-based approach not only provides the more accurate spectrum than the conventional methods in terms of peak positions and heights, but is also more robust against the noise which is the required key feature for any continuation technique to be successful. Furthermore, its computation speed is ${10}^{4}\text{--}{10}^{5}$ times faster than the maximum entropy method.","",""
5,"Dan Jiang, Weihua Lin, N. Raghavan","A Novel Framework for Semiconductor Manufacturing Final Test Yield Classification Using Machine Learning Techniques",2020,"","","","",51,"2022-07-13 10:06:51","","10.1109/ACCESS.2020.3034680","","",,,,,5,2.50,2,3,2,"Advanced data analysis tools and techniques are important for semiconductor companies to gain competitive advantage. In particular, yield prediction tools, which fully utilize production data, help to improve operational efficiency and reduce production costs. This paper introduces a novel and scalable framework for semiconductor manufacturing Final Test (FT) yield prediction leveraging machine learning techniques. This framework is able to predict FT yield at wafer fabrication stage, so that FT low yield problems can be caught at an earlier production stage compared to past studies. Our work presents a robust solution to automatically handle both numerical and categorical production related data without prior knowledge of the low yield root cause. Gaussian Mixture Models, One Hot Encoder and Label Encoder techniques are adopted for data pre-processing. To improve model performance for both binary and multi-class classification, model selection and model ensemble using the F1-macro method is demonstrated. The framework has been applied to three mass production products with different wafer technologies and manufacturing flows. All of them achieved high F1-macro test score indicative of the robustness of our framework.","",""
12,"Yingxu Wang, Omar A. Zatarain","A Novel Machine Learning Algorithm for Cognitive Concept Elicitation by Cognitive Robots",2017,"","","","",52,"2022-07-13 10:06:51","","10.4018/IJCINI.2017070103","","",,,,,12,2.40,6,2,5,"Cognitive knowledge learning (CKL) is a fundamental methodology for cognitive robots and machine learning. Traditional technologies for machine learning deal with object identification, cluster classification, pattern recognition, functional regression and behavior acquisition. A new category of CKL is presented in this paper embodied by the Algorithm of Cognitive Concept Elicitation (ACCE). Formal concepts are autonomously generated based on collective intension (attributes) and extension (objects) elicited from informal descriptions in dictionaries. A system of formal concept generation by cognitive robots is implemented based on the ACCE algorithm. Experiments on machine learning for knowledge acquisition reveal that a cognitive robot is able to learn synergized concepts in human knowledge in order to build its own knowledge base. The machine–generated knowledge base demonstrates that the ACCE algorithm can outperform human knowledge expressions in terms of relevance, accuracy, quantification and cohesiveness.","",""
12,"N. Khoa, M. M. Alamdari, T. Rakotoarivelo, Ali Anaissi, Yang Wang","Structural Health Monitoring Using Machine Learning Techniques and Domain Knowledge Based Features",2018,"","","","",53,"2022-07-13 10:06:51","","10.1007/978-3-319-90403-0_20","","",,,,,12,3.00,2,5,4,"","",""
4,"Shuteng Niu, Jian Wang, Yongxin Liu, H. Song","Transfer Learning based Data-Efficient Machine Learning Enabled Classification",2020,"","","","",54,"2022-07-13 10:06:51","","10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00108","","",,,,,4,2.00,1,4,2,"Recently, waste sorting has become more and more important in our daily life. It plays an essential role in the big picture of waste recycling, reducing environmental pollution significantly. Deep learning (DL) methods have been dominating the field of image classification and have been successfully applied to waste sorting tasks to achieve state-of-art performance. However, most traditional DL methods require a massive amount of annotated data for the training phase. Unfortunately, there is only one small data set for waste sorting, TrashNet created by Standford. In addition, manually collecting and labeling a massive data-set can be too costly. To address this issue, we decided to implement transfer learning (TL) techniques to construct a robust model based on a fairly small set of training data by transferring knowledge from existing deep networks, such as AlexNet, Resnet, and DensNet. As an innovation, we propose a novel domain loss function, Dual Dynamic Domain Distance (4D), to produce a more accurate domain distance measurement. There are three contributions to this paper. First, our model has achieved the best performance on the TrashNet data. Secondly, it is the first time that TL has been used for waste sorting. Finally, the proposed novel 4D domain loss has improved the performance of TL for this task. In this paper, we implemented two types of transfer learning methods, DDC, DeepCoral, to TrashNet data-set. Moreover, the DeepCoral-Resnet50 model yields the best performance of 96% test accuracy. More importantly, this work can be easily generalized to other image classification tasks.","",""
14,"M. Levine, A. Stuart","A Framework for Machine Learning of Model Error in Dynamical Systems",2021,"","","","",55,"2022-07-13 10:06:51","","","","",,,,,14,14.00,7,2,1,"The development of data-informed predictive models for dynamical systems is of widespread interest in many disciplines. We present a unifying framework for blending mechanistic and machine-learning approaches to identify dynamical systems from noisily and partially observed data. We compare pure data-driven learning with hybrid models which incorporate imperfect domain knowledge, referring to the discrepancy between an assumed truth model and the imperfect mechanistic model as model error. Our formulation is agnostic to the chosen machine learning model, is presented in both continuousand discrete-time settings, and is compatible both with model errors that exhibit substantial memory and errors that are memoryless. First, we study memoryless linear (w.r.t. parametric-dependence) model error from a learning theory perspective, defining excess risk and generalization error. For ergodic continuous-time systems, we prove that both excess risk and generalization error are bounded above by terms that diminish with the square-root of T , the time-interval over which training data is specified. Secondly, we study scenarios that benefit from modeling with memory, proving universal approximation theorems for two classes of continuous-time recurrent neural networks (RNNs): both can learn memory-dependent model error, assuming that it is governed by a finite-dimensional hidden variable and that, together, the observed and hidden variables form a continuous-time Markovian system. In addition, we connect one class of RNNs to reservoir computing, thereby relating learning of memory-dependent error to recent work on supervised learning between Banach spaces using random features. Numerical results are presented (Lorenz ’63, Lorenz ’96 Multiscale systems) to compare purely data-driven and hybrid approaches, finding hybrid methods less data-hungry and more parametrically efficient. We also find that, while a continuous-time framing allows for robustness to irregular sampling and desirable domain-interpretability, a discrete-time framing can provide similar or better predictive performance, especially when data are undersampled and the vector field defining the true dynamics cannot be identified. Finally, we demonstrate numerically how data assimilation can be leveraged to learn hidden dynamics from noisy, partially-observed data, and illustrate challenges in representing memory by this approach, and in the training of such models. Received by the editors July 13, 2021. 2020 Mathematics Subject Classification. Primary 68T30, 37A30, 37M10; Secondary 37M25,","",""
14,"M. Vollmer, B. Glampson, T. Mellan, Swapnil Mishra, L. Mercuri, Ceire Costello, R. Klaber, G. Cooke, S. Flaxman, S. Bhatt","A unified machine learning approach to time series forecasting applied to demand at emergency departments",2020,"","","","",56,"2022-07-13 10:06:51","","10.1186/s12873-020-00395-y","","",,,,,14,7.00,1,10,2,"","",""
0,"T. Nguyen","Layered Approximation Approach to Knowledge Elicitation in Machine Learning",2010,"","","","",57,"2022-07-13 10:06:51","","10.1007/978-3-642-13529-3_48","","",,,,,0,0.00,0,1,12,"","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",58,"2022-07-13 10:06:51","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
1920,"A. Kurakin, Ian J. Goodfellow, Samy Bengio","Adversarial Machine Learning at Scale",2016,"","","","",59,"2022-07-13 10:06:51","","","","",,,,,1920,320.00,640,3,6,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.","",""
33,"J. Schneider, J. Handali","Personalized Explanation for Machine Learning: a Conceptualization",2019,"","","","",60,"2022-07-13 10:06:51","","","","",,,,,33,11.00,17,2,3,"Explanation in machine learning and related fields such as artificial intelligence aims at making machine learning models and their decisions understandable to humans. Existing work suggests that personalizing explanations might help to improve understandability. In this work, we derive a conceptualization of personalized explanation by defining and structuring the problem based on prior work on machine learning explanation, personalization (in machine learning) and concepts and techniques from other domains such as privacy and knowledge elicitation. We perform a categorization of explainee data used in the process of personalization as well as describing means to collect this data. We also identify three key explanation properties that are amendable to personalization: complexity, decision information and presentation. We also enhance existing work on explanation by introducing additional desiderata and measures to quantify the quality of personalized explanations.","",""
41,"Xiang Lu, M. Hasanipanah, Kathirvel Brindhadevi, H. Bakhshandeh Amnieh, Seyedamirhesam Khalafi","ORELM: A Novel Machine Learning Approach for Prediction of Flyrock in Mine Blasting",2019,"","","","",61,"2022-07-13 10:06:51","","10.1007/s11053-019-09532-2","","",,,,,41,13.67,8,5,3,"","",""
10,"Yifan Cui, E. Tchetgen","Bias-aware model selection for machine learning of doubly robust functionals",2019,"","","","",62,"2022-07-13 10:06:51","","","","",,,,,10,3.33,5,2,3,"While model selection is a well-studied topic in parametric and nonparametric regression or density estimation, model selection of possibly high dimensional nuisance parameters in semiparametric problems is far less developed. In this paper, we propose a new model selection framework for making inferences about a finite dimensional functional defined on a semiparametric model, when the latter admits a doubly robust estimating function. The class of such doubly robust functionals is quite large, including many missing data and causal inference problems. Under double robustness, the estimated functional should incur no bias if either of two nuisance parameters is evaluated at the truth while the other spans a large collection of candidate models. We introduce two model selection criteria for bias reduction of functional of interest, each based on a novel definition of pseudo-risk for the functional that embodies this double robustness property and thus may be used to select the candidate model that is nearest to fulfilling this property even when all models are wrong. Both selection criteria have a bias awareness property that selection of one nuisance parameter can be made to compensate for excessive bias due to poor learning of the other nuisance parameter. We establish an oracle property for a multi-fold cross-validation version of the new model selection criteria which states that our empirical criteria perform nearly as well as an oracle with a priori knowledge of the pseudo-risk for each candidate model. We also describe a smooth approximation to the selection criteria which allows for valid post-selection inference. Finally, we apply the approach to perform model selection of a semiparametric estimator of average treatment effect given an ensemble of candidate machine learning methods to account for confounding in a study of right heart catheterization in the intensive care unit of critically ill patients.","",""
20,"Shichao Pei, Lu Yu, Guoxian Yu, Xiangliang Zhang","REA: Robust Cross-lingual Entity Alignment Between Knowledge Graphs",2020,"","","","",63,"2022-07-13 10:06:51","","10.1145/3394486.3403268","","",,,,,20,10.00,5,4,2,"Cross-lingual entity alignment aims at associating semantically similar entities in knowledge graphs with different languages. It has been an essential research problem for knowledge integration and knowledge graph connection, and been studied with supervised or semi-supervised machine learning methods with the assumption of clean labeled data. However, labels from human annotations often include errors, which can largely affect the alignment results. We thus aim to formulate and explore the robust entity alignment problem, which is non-trivial, due to the deficiency of noisy labels. Our proposed method named REA (Robust Entity Alignment) consists of two components: noise detection and noise-aware entity alignment. The noise detection is designed by following the adversarial training principle. The noise-aware entity alignment is devised by leveraging graph neural network based knowledge graph encoder as the core. In order to mutually boost the performance of the two components, we propose a unified reinforced training strategy to combine them. To evaluate our REA method, we conduct extensive experiments on several real-world datasets. The experimental results demonstrate the effectiveness of our proposed method and also show that our model consistently outperforms the state-of-the-art methods with significant improvement on alignment accuracy in the noise-involved scenario.","",""
52,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",64,"2022-07-13 10:06:51","","10.1145/3219617.3219655","","",,,,,52,10.40,17,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q łe m for an arbitrarily small but fixed constant ε>0. The parameter estimate converges in O(łog N) rounds with an estimation error on the order of max √dq/N, ~√d/N , which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q . The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log 3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
33,"Jana Sperschneider","Machine learning in plant-pathogen interactions: empowering biological predictions from field scale to genome scale.",2020,"","","","",65,"2022-07-13 10:06:51","","10.1111/nph.15771","","",,,,,33,16.50,33,1,2,"Contents Summary I. A primer on machine learning: what is it and what are the common pitfalls? II. Machine learning applications in plant-pathogen interactions III. Conclusion Acknowledgements References SUMMARY: Machine learning (ML) encompasses statistical methods that learn to identify patterns in complex datasets. Here, I review application areas in plant-pathogen interactions that have recently benefited from ML, such as disease monitoring, the discovery of gene regulatory networks, genomic selection for disease resistance and prediction of pathogen effectors. However, achieving robust performance from ML is not trivial and requires knowledge of both the methodology and the biology. I discuss common pitfalls and challenges in using ML approaches. Finally, I highlight future opportunities for ML as a tool for dissecting plant-pathogen interactions using high-throughput data, for example, through integration of diverse data sources and the analysis with higher resolution, such as from individual cells or on elaborate spatial and temporal scales.","",""
9,"Kexin Pei, Jonas Guan, David Williams-King, Junfeng Yang, S. Jana","XDA: Accurate, Robust Disassembly with Transfer Learning",2020,"","","","",66,"2022-07-13 10:06:51","","10.14722/NDSS.2021.23112","","",,,,,9,4.50,2,5,2,"Accurate and robust disassembly of stripped binaries is challenging. The root of the difficulty is that high-level structures, such as instruction and function boundaries, are absent in stripped binaries and must be recovered based on incomplete information. Current disassembly approaches rely on heuristics or simple pattern matching to approximate the recovery, but these methods are often inaccurate and brittle, especially across different compiler optimizations.  We present XDA, a transfer-learning-based disassembly framework that learns different contextual dependencies present in machine code and transfers this knowledge for accurate and robust disassembly. We design a self-supervised learning task motivated by masked Language Modeling to learn interactions among byte sequences in binaries. The outputs from this task are byte embeddings that encode sophisticated contextual dependencies between input binaries' byte tokens, which can then be finetuned for downstream disassembly tasks.  We evaluate XDA's performance on two disassembly tasks, recovering function boundaries and assembly instructions, on a collection of 3,121 binaries taken from SPEC CPU2017, SPEC CPU2006, and the BAP corpus. The binaries are compiled by GCC, ICC, and MSVC on x86/x64 Windows and Linux platforms over 4 optimization levels. XDA achieves 99.0% and 99.7% F1 score at recovering function boundaries and instructions, respectively, surpassing the previous state-of-the-art on both tasks. It also maintains speed on par with the fastest ML-based approach and is up to 38x faster than hand-written disassemblers like IDA Pro.","",""
5,"Haohan Wang, Zeyi Huang, Hanlin Zhang, Eric P. Xing","Toward Learning Human-aligned Cross-domain Robust Models by Countering Misaligned Features",2021,"","","","",67,"2022-07-13 10:06:51","","","","",,,,,5,5.00,1,4,1,"Machine learning has demonstrated remarkable prediction accuracy over i.i.d data, but the accuracy often drops when tested with data from another distribution. In this paper, we aim to offer another view of this problem in a perspective as-suming the reason behind this accuracy drop is the reliance of models on the features that are not aligned well with how a data annotator considers similar across these two datasets. We refer to these features as misaligned features. We extend the conventional generalization error bound to a new one for this setup with the knowledge of how the misaligned features are associated with the label. Our analysis offers a set of techniques for this problem, and these techniques are naturally linked to many previous methods in robust machine learning literature. We also compared the empirical strength of these methods demonstrated the performance when these previous techniques are combined, with implementation available here.","",""
19,"T. Le, M. Penna, D. Winkler, I. Yarovsky","Quantitative design rules for protein-resistant surface coatings using machine learning",2019,"","","","",68,"2022-07-13 10:06:51","","10.1038/s41598-018-36597-5","","",,,,,19,6.33,5,4,3,"","",""
18,"P. Fusar-Poli, Dominic Stringer, Alice M. S. Durieux, G. Rutigliano, I. Bonoldi, A. De Micheli, D. Ståhl","Clinical-learning versus machine-learning for transdiagnostic prediction of psychosis onset in individuals at-risk",2019,"","","","",69,"2022-07-13 10:06:51","","10.1038/s41398-019-0600-9","","",,,,,18,6.00,3,7,3,"","",""
0,"Rafael M. Frongillo","Machine Learning and Microeconomics Elicitation and Crowdsourcing",2015,"","","","",70,"2022-07-13 10:06:51","","","","",,,,,0,0.00,0,1,7,"The focus of my research is on theoretical problems at the interface between machine learning and microeconomics. This interface is broad, spanning domains such as crowdsourcing and prediction markets, to finance and algorithmic game theory. I am particularly interested in applying machine learning theory to economic domains, such as using coordinate descent to understand market behavior, and conversely using economic models to develop more robust and realistic techniques for machine learning.","",""
658,"Wieland Brendel, Jonas Rauber, M. Bethge","Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",2017,"","","","",71,"2022-07-13 10:06:51","","","","",,,,,658,131.60,219,3,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .","",""
13,"Karolis Liulys","Machine Learning Application in Predictive Maintenance",2019,"","","","",72,"2022-07-13 10:06:51","","10.1109/ESTREAM.2019.8732146","","",,,,,13,4.33,13,1,3,"Industrial organizations worldwide cannot ignore Industry 4.0 and its impact to their businesses. The biggest struggle is to find the way how to adopt all the possibilities for each plants unique use cases. In those situations where it is hard to find unified solutions internet is playing major part. Inseparable part of Industry 4.0 is Internet of Things (IoT) paradigm, where it is possible to connect all devices into united system. While robust Distributed Control Systems (DCS) are preferred for their safety, Industrial IoT (IIoT) allows next level prospects: big data performance analyzation, control patterns identification and predictive preventative maintenance by using machine learning algorithms. The study shows how implementing open-source software enables engineers to develop predictive maintenance applications with basic programming knowledge. These type of applications can be widely used in industrial field to inform about possible equipment malfunction helping reduce possible damages.","",""
24,"Saikat Das, Ph.D., Ahmed M. Mahfouz, D. Venugopal, S. Shiva","DDoS Intrusion Detection Through Machine Learning Ensemble",2019,"","","","",73,"2022-07-13 10:06:51","","10.1109/QRS-C.2019.00090","","",,,,,24,8.00,6,4,3,"Distributed Denial of Service (DDoS) attacks have been the prominent attacks over the last decade. A Network Intrusion Detection System (NIDS) should seamlessly configure to fight against these attackers' new approaches and patterns of DDoS attack. In this paper, we propose a NIDS which can detect existing as well as new types of DDoS attacks. The key feature of our NIDS is that it combines different classifiers using ensemble models, with the idea that each classifier can target specific aspects/types of intrusions, and in doing so provides a more robust defense mechanism against new intrusions. Further, we perform a detailed analysis of DDoS attacks, and based on this domain-knowledge verify the reduced feature set [27, 28] to significantly improve accuracy. We experiment with and analyze NSL-KDD dataset with reduced feature set and our proposed NIDS can detect 99.1% of DDoS attacks successfully. We compare our results with other existing approaches. Our NIDS approach has the learning capability to keep up with new and emerging DDoS attack patterns.","",""
8,"Sakshi Udeshi, Sudipta Chattopadhyay","Grammar Based Directed Testing of Machine Learning Systems",2019,"","","","",74,"2022-07-13 10:06:51","","10.1109/tse.2019.2953066","","",,,,,8,2.67,4,2,3,"The massive progress of machine learning has seen its application over a variety of domains in the past decade. But how do we develop a systematic, scalable and modular strategy to validate machine-learning systems? We present, to the best of our knowledge, the first approach, which provides a systematic test framework for machine-learning systems that accepts grammar-based inputs. Our Ogma approach automatically discovers erroneous behaviours in classifiers and leverages these erroneous behaviours to improve the respective models. Ogma leverages inherent robustness properties present in any well trained machine-learning model to direct test generation and thus, implementing a scalable test generation methodology. To evaluate our Ogma approach, we have tested it on three real world natural language processing (NLP) classifiers. We have found thousands of erroneous behaviours in these systems. We also compare Ogma with a random test generation approach and observe that Ogma is more effective than such random test generation by up to 489 percent.","",""
8,"Dongqi Han, Zhiliang Wang, Ying Zhong, Wenqi Chen, Jiahai Yang, Shuqiang Lu, Xingang Shi, Xia Yin","Evaluating and Improving Adversarial Robustness of Machine Learning-Based Network Intrusion Detectors",2020,"","","","",75,"2022-07-13 10:06:51","","10.1109/JSAC.2021.3087242","","",,,,,8,4.00,1,8,2,"Machine learning (ML), especially deep learning (DL) techniques have been increasingly used in anomaly-based network intrusion detection systems (NIDS). However, ML/DL has shown to be extremely vulnerable to adversarial attacks, especially in such security-sensitive systems. Many adversarial attacks have been proposed to evaluate the robustness of ML-based NIDSs. Unfortunately, existing attacks mostly focused on feature-space and/or white-box attacks, which make impractical assumptions in real-world scenarios, leaving the study on practical gray/black-box attacks largely unexplored. To bridge this gap, we conduct the first systematic study of the gray/black-box traffic-space adversarial attacks to evaluate the robustness of ML-based NIDSs. Our work outperforms previous ones in the following aspects: (i) practical —the proposed attack can automatically mutate original traffic with extremely limited knowledge and affordable overhead while preserving its functionality; (ii) generic —the proposed attack is effective for evaluating the robustness of various NIDSs using diverse ML/DL models and non-payload-based features; (iii) explainable —we propose an explanation method for the fragile robustness of ML-based NIDSs. Based on this, we also propose a defense scheme against adversarial attacks to improve system robustness. We extensively evaluate the robustness of various NIDSs using diverse feature sets and ML/DL models. Experimental results show our attack is effective (e.g., >97% evasion rate in half cases for Kitsune, a state-of-the-art NIDS) with affordable execution cost and the proposed defense method can effectively mitigate such attacks (evasion rate is reduced by >50% in most cases).","",""
30,"J. Bullock, A. Momeni","Ml.lib: robust, cross-platform, open-source machine learning for max and pure data",2015,"","","","",76,"2022-07-13 10:06:51","","","","",,,,,30,4.29,15,2,7,"This paper documents the development of ml.lib: a set of open-source tools designed for employing a wide range of machine learning techniques within two popular real-time programming environments, namely Max and Pure Data. ml.lib is a cross-platform, lightweight wrapper around Nick Gillian's Gesture Recognition Toolkit, a C++ library that includes a wide range of data processing and machine learning techniques. ml.lib adapts these techniques for real-time use within popular data-flow IDEs, allowing instrument designers and performers to integrate robust learning, classification and mapping approaches within their existing workflows. ml.lib has been carefully de-signed to allow users to experiment with and incorporate ma-chine learning techniques within an interactive arts context with minimal prior knowledge. A simple, logical and consistent, scalable interface has been provided across over sixteen exter-nals in order to maximize learnability and discoverability. A focus on portability and maintainability has enabled ml.lib to support a range of computing architectures - including ARM - and operating systems such as Mac OS, GNU/Linux and Win-dows, making it the most comprehensive machine learning implementation available for Max and Pure Data.","",""
12,"J. Schneider, J. Handali","Personalized explanation in machine learning",2019,"","","","",77,"2022-07-13 10:06:51","","","","",,,,,12,4.00,6,2,3,"Explanation in machine learning and related fields such as artificial intelligence aims at making machine learning models and their decisions understandable to humans. Existing work suggests that personalizing explanations might help to improve understandability. In this work, we derive a conceptualization of personalized explanation by defining and structuring the problem based on prior work on machine learning explanation, personalization (in machine learning) and concepts and techniques from other domains such as privacy and knowledge elicitation. We perform a categorization of explainee information used in the process of personalization as well as describing means to collect this information. We also identify three key explanation properties that are amendable to personalization: complexity, decision information and presentation. We also enhance existing work on explanation by introducing additional desiderata and measures to quantify the quality of personalized explanations.","",""
38,"M. Yoosefzadeh-Najafabadi, H. Earl, D. Tulpan, J. Sulik, M. Eskandari","Application of Machine Learning Algorithms in Plant Breeding: Predicting Yield From Hyperspectral Reflectance in Soybean",2021,"","","","",78,"2022-07-13 10:06:51","","10.3389/fpls.2020.624273","","",,,,,38,38.00,8,5,1,"Recent substantial advances in high-throughput field phenotyping have provided plant breeders with affordable and efficient tools for evaluating a large number of genotypes for important agronomic traits at early growth stages. Nevertheless, the implementation of large datasets generated by high-throughput phenotyping tools such as hyperspectral reflectance in cultivar development programs is still challenging due to the essential need for intensive knowledge in computational and statistical analyses. In this study, the robustness of three common machine learning (ML) algorithms, multilayer perceptron (MLP), support vector machine (SVM), and random forest (RF), were evaluated for predicting soybean (Glycine max) seed yield using hyperspectral reflectance. For this aim, the hyperspectral reflectance data for the whole spectra ranged from 395 to 1005 nm, which were collected at the R4 and R5 growth stages on 250 soybean genotypes grown in four environments. The recursive feature elimination (RFE) approach was performed to reduce the dimensionality of the hyperspectral reflectance data and select variables with the largest importance values. The results indicated that R5 is more informative stage for measuring hyperspectral reflectance to predict seed yields. The 395 nm reflectance band was also identified as the high ranked band in predicting the soybean seed yield. By considering either full or selected variables as the input variables, the ML algorithms were evaluated individually and combined-version using the ensemble–stacking (E–S) method to predict the soybean yield. The RF algorithm had the highest performance with a value of 84% yield classification accuracy among all the individual tested algorithms. Therefore, by selecting RF as the metaClassifier for E–S method, the prediction accuracy increased to 0.93, using all variables, and 0.87, using selected variables showing the success of using E–S as one of the ensemble techniques. This study demonstrated that soybean breeders could implement E–S algorithm using either the full or selected spectra reflectance to select the high-yielding soybean genotypes, among a large number of genotypes, at early growth stages.","",""
46,"F. Yuan, S. A. Zargar, Qiuyi Chen, Shaohan Wang","Machine learning for structural health monitoring: challenges and opportunities",2020,"","","","",79,"2022-07-13 10:06:51","","10.1117/12.2561610","","",,,,,46,23.00,12,4,2,"A physics-based approach to structural health monitoring (SHM) has practical shortcomings which restrict its suitability to simple structures under well controlled environments. With the advances in information and sensing technology (sensors and sensor networks), it has become feasible to monitor large/diverse number of parameters in complex real-world structures either continuously or intermittently by employing large in-situ (wireless) sensor networks. The availability of this historical data has engendered a lot of interest in a data-driven approach as a natural and more viable option for realizing the goal of SHM in such structures. However, the lack of sensor data corresponding to different damage scenarios continues to remain a challenge. Most of the supervised machine-learning/deep-learning techniques, when trained using this inherently limited data, lack robustness and generalizability. Physics-informed learning, which involves the integration of domain knowledge into the learning process, is presented here as a potential remedy to this challenge. As a step towards the goal of automated damage detection (mathematically an inverse problem), preliminary results are presented from dynamic modelling of beam structures using physics-informed artificial neural networks. Forward and inverse problems involving partial differential equations are solved and comparisons reveal a clear superiority of physics-informed approach over one that is purely datadriven vis-à-vis overfitting/generalization. Other ways of incorporating domain knowledge into the machine learning pipeline are then presented through case-studies on various aspects of NDI/SHM (visual inspection, impact diagnosis). Lastly, as the final attribute of an optimal SHM approach, a sensing paradigm for non-contact full-field measurements for damage diagnosis is presented.","",""
3,"A. Horst, Erand Smakaj, E. Natali, Deniz Tosoni, Lmar Babrak, P. Meier, Enkelejda Miho","Machine Learning Detects Anti-DENV Signatures in Antibody Repertoire Sequences",2021,"","","","",80,"2022-07-13 10:06:51","","10.3389/frai.2021.715462","","",,,,,3,3.00,0,7,1,"Dengue infection is a global threat. As of today, there is no universal dengue fever treatment or vaccines unreservedly recommended by the World Health Organization. The investigation of the specific immune response to dengue virus would support antibody discovery as therapeutics for passive immunization and vaccine design. High-throughput sequencing enables the identification of the multitude of antibodies elicited in response to dengue infection at the sequence level. Artificial intelligence can mine the complex data generated and has the potential to uncover patterns in entire antibody repertoires and detect signatures distinctive of single virus-binding antibodies. However, these machine learning have not been harnessed to determine the immune response to dengue virus. In order to enable the application of machine learning, we have benchmarked existing methods for encoding biological and chemical knowledge as inputs and have investigated novel encoding techniques. We have applied different machine learning methods such as neural networks, random forests, and support vector machines and have investigated the parameter space to determine best performing algorithms for the detection and prediction of antibody patterns at the repertoire and antibody sequence levels in dengue-infected individuals. Our results show that immune response signatures to dengue are detectable both at the antibody repertoire and at the antibody sequence levels. By combining machine learning with phylogenies and network analysis, we generated novel sequences that present dengue-binding specific signatures. These results might aid further antibody discovery and support vaccine design.","",""
3,"H. Vardhan, P. Völgyesi, J. Sztipanovits","Machine learning assisted propeller design",2021,"","","","",81,"2022-07-13 10:06:51","","10.1145/3450267.3452001","","",,,,,3,3.00,1,3,1,"Propellers are one of the most widely used propulsive devices for generating thrust from rotational engine motion both in marine vehicles and subsonic air-crafts. Due to their simplicity, robustness and high efficiency, propellers remained the mainstream design choice over the last hundred years. On the other hand, finding the optimal application-specific geometry is still challenging. This work in progress report describes application of modern and rapidly developing Machine Learning (ML) techniques to gain novel designs. We rely on a rich set of preexisting parametric design patterns and accumulated engineering knowledge supplemented by high-fidelity simulation models to formulate the design process as a supervised learning problem. The aim of our work is to develop and evaluate machine learning models for the parametric design of propellers based on application-specific constraints. While the application of ML techniques in optimal propeller design is at a very nascent level, we believe that our early results are promising with a potentially significant impact on the overall design process. The ML-assisted design flow allows for a more automated design space exploration process with less dependency on human intuition and engineering guidance.","",""
3,"N. Yousefpour, Z. Medina-Cetina, F. G. Hernandez-Martinez, A. Al-Tabbaa","Stiffness and Strength of Stabilized Organic Soils—Part II/II: Parametric Analysis and Modeling with Machine Learning",2021,"","","","",82,"2022-07-13 10:06:51","","10.3390/GEOSCIENCES11050218","","",,,,,3,3.00,1,4,1,"Predicting the range of achievable strength and stiffness from stabilized soil mixtures is critical for engineering design and construction, especially for organic soils, which are often considered “unsuitable” due to their high compressibility and the lack of knowledge about their mechanical behavior after stabilization. This study investigates the mechanical behavior of stabilized organic soils using machine learning (ML) methods. ML algorithms were developed and trained using a database from a comprehensive experimental study (see Part I), including more than one thousand unconfined compression tests on organic clay samples stabilized by wet soil mixing (WSM) technique. Three different ML methods were adopted and compared, including two artificial neural networks (ANN) and a linear regression method. ANN models proved reliable in the prediction of the stiffness and strength of stabilized organic soils, significantly outperforming linear regression models. Binder type, mixing ratio, soil organic and water content, sample size, aging, temperature, relative humidity, and carbonation were the control variables (input parameters) incorporated into the ML models. The impacts of these factors were evaluated through rigorous ANN-based parametric analyses. Additionally, the nonlinear relations of stiffness and strength with these parameters were developed, and their optimum ranges were identified through the ANN models. Overall, the robust ML approach presented in this paper can significantly improve the mixture design for organic soil stabilization and minimize the experimental cost for implementing WSM in engineering projects.","",""
3,"Tingting Sun, Yuting Chen, Yuhao Wen, Zefeng Zhu, Minghui Li","PremPLI: a machine learning model for predicting the effects of missense mutations on protein-ligand interactions",2021,"","","","",83,"2022-07-13 10:06:51","","10.1038/s42003-021-02826-3","","",,,,,3,3.00,1,5,1,"","",""
10,"E. Adabor, G. Acquaah-Mensah","Machine learning approaches to decipher hormone and HER2 receptor status phenotypes in breast cancer",2019,"","","","",84,"2022-07-13 10:06:51","","10.1093/bib/bbx138","","",,,,,10,3.33,5,2,3,"Breast cancer prognosis and administration of therapies are aided by knowledge of hormonal and HER2 receptor status. Breast cancer lacking estrogen receptors, progesterone receptors and HER2 receptors are difficult to treat. Regarding large data repositories such as The Cancer Genome Atlas, available wet-lab methods for establishing the presence of these receptors do not always conclusively cover all available samples. To this end, we introduce median-supplement methods to identify hormonal and HER2 receptor status phenotypes of breast cancer patients using gene expression profiles. In these approaches, supplementary instances based on median patient gene expression are introduced to balance a training set from which we build simple models to identify the receptor expression status of patients. In addition, for the purpose of benchmarking, we examine major machine learning approaches that are also applicable to the problem of finding receptor status in breast cancer. We show that our methods are robust and have high sensitivity with extremely low false-positive rates compared with the well-established methods. A successful application of these methods will permit the simultaneous study of large collections of samples of breast cancer patients as well as save time and cost while standardizing interpretation of outcomes of such studies.","",""
1,"Minh-Thang Nguyen, Sungoh Kwon","Machine Learning–Based Mobility Robustness Optimization Under Dynamic Cellular Networks",2021,"","","","",85,"2022-07-13 10:06:51","","10.1109/ACCESS.2021.3083554","","",,,,,1,1.00,1,2,1,"In this paper, we propose a machine learning−based mobility robustness optimization algorithm to optimize handover parameters for seamless mobility under dynamic small-cell networks. Small cells can be arbitrarily deployed, portable, and turned on and off to fulfill wireless traffic demands or energy efficiency. As a result, the small-cell network topology dynamically varies challenging network optimization, especially handover optimization. Previous studies have only considered dynamics due to user mobility in a specific static network topology. To optimize handovers under dynamic network topologies, together with user mobility, we propose an algorithm consisting of two steps: topology adaptation and mobility adaptation. To adapt to a dynamic topology, the algorithm obtains prior knowledge, which presents a belief distribution of the optimal handover parameters, for the current network topology as coarse optimization. In the second step, the algorithm fine-tunes the handover parameters to adapt to user mobility based on reinforcement learning, which utilizes the knowledge obtained during the first step. Under a dynamic small-cell network, we showed that the proposed algorithm reduced adaptation time to 4.17% of the time needed by a comparative machine–based algorithm. Furthermore, the proposed algorithm improved the user satisfaction rate to 416.7% compared to the previous work.","",""
8,"Mazaher Kianpour, Shao-Fang Wen","Timing Attacks on Machine Learning: State of the Art",2019,"","","","",86,"2022-07-13 10:06:51","","10.1007/978-3-030-29516-5_10","","",,,,,8,2.67,4,2,3,"","",""
12,"Eleni A. Chatzimichali, C. Bessant","Novel application of heuristic optimisation enables the creation and thorough evaluation of robust support vector machine ensembles for machine learning applications",2015,"","","","",87,"2022-07-13 10:06:51","","10.1007/s11306-015-0894-4","","",,,,,12,1.71,6,2,7,"","",""
3,"D. Kurrant, M. Omer, Nasim Abdollahi, P. Mojabi, E. Fear, J. Lovetri","Evaluating Performance of Microwave Image Reconstruction Algorithms: Extracting Tissue Types with Segmentation Using Machine Learning",2021,"","","","",88,"2022-07-13 10:06:51","","10.3390/jimaging7010005","","",,,,,3,3.00,1,6,1,"Evaluating the quality of reconstructed images requires consistent approaches to extracting information and applying metrics. Partitioning medical images into tissue types permits the quantitative assessment of regions that contain a specific tissue. The assessment facilitates the evaluation of an imaging algorithm in terms of its ability to reconstruct the properties of various tissue types and identify anomalies. Microwave tomography is an imaging modality that is model-based and reconstructs an approximation of the actual internal spatial distribution of the dielectric properties of a breast over a reconstruction model consisting of discrete elements. The breast tissue types are characterized by their dielectric properties, so the complex permittivity profile that is reconstructed may be used to distinguish different tissue types. This manuscript presents a robust and flexible medical image segmentation technique to partition microwave breast images into tissue types in order to facilitate the evaluation of image quality. The approach combines an unsupervised machine learning method with statistical techniques. The key advantage for using the algorithm over other approaches, such as a threshold-based segmentation method, is that it supports this quantitative analysis without prior assumptions such as knowledge of the expected dielectric property values that characterize each tissue type. Moreover, it can be used for scenarios where there is a scarcity of data available for supervised learning. Microwave images are formed by solving an inverse scattering problem that is severely ill-posed, which has a significant impact on image quality. A number of strategies have been developed to alleviate the ill-posedness of the inverse scattering problem. The degree of success of each strategy varies, leading to reconstructions that have a wide range of image quality. A requirement for the segmentation technique is the ability to partition tissue types over a range of image qualities, which is demonstrated in the first part of the paper. The segmentation of images into regions of interest corresponding to various tissue types leads to the decomposition of the breast interior into disjoint tissue masks. An array of region and distance-based metrics are applied to compare masks extracted from reconstructed images and ground truth models. The quantitative results reveal the accuracy with which the geometric and dielectric properties are reconstructed. The incorporation of the segmentation that results in a framework that effectively furnishes the quantitative assessment of regions that contain a specific tissue is also demonstrated. The algorithm is applied to reconstructed microwave images derived from breasts with various densities and tissue distributions to demonstrate the flexibility of the algorithm and that it is not data-specific. The potential for using the algorithm to assist in diagnosis is exhibited with a tumor tracking example. This example also establishes the usefulness of the approach in evaluating the performance of the reconstruction algorithm in terms of its sensitivity and specificity to malignant tissue and its ability to accurately reconstruct malignant tissue.","",""
12,"Melanie Weber, M. Zaheer, A. Rawat, A. Menon, Sanjiv Kumar","Robust Large-Margin Learning in Hyperbolic Space",2020,"","","","",89,"2022-07-13 10:06:51","","","","",,,,,12,6.00,2,5,2,"Recently, there has been a surge of interest in representation learning in hyperbolic spaces, driven by their ability to represent hierarchical data with significantly fewer dimensions than standard Euclidean spaces. However, the viability and benefits of hyperbolic spaces for downstream machine learning tasks have received less attention. In this paper, we present, to our knowledge, the first theoretical guarantees for learning a classifier in hyperbolic rather than Euclidean space. Specifically, we consider the problem of learning a large-margin classifier for data possessing a hierarchical structure. Our first contribution is a hyperbolic perceptron algorithm, which provably converges to a separating hyperplane. We then provide an algorithm to efficiently learn a large-margin hyperplane, relying on the careful injection of adversarial examples. Finally, we prove that for hierarchical data that embeds well into hyperbolic space, the low embedding dimension ensures superior guarantees when learning the classifier directly in hyperbolic space.","",""
0,"Johannes Schneider","FOR MACHINE LEARNING : A CONCEPTUALIZATION",2019,"","","","",90,"2022-07-13 10:06:51","","","","",,,,,0,0.00,0,1,3,"Explanation in machine learning and related fields such as artificial intelligence aims at making machine learning models and their decisions understandable to humans. Existing work suggests that personalizing explanations might help to improve understandability. In this work, we derive a conceptualization of personalized explanation by defining and structuring the problem based on prior work on machine learning explanation, personalization (in machine learning) and concepts and techniques from other domains such as privacy and knowledge elicitation. We perform a categorization of explainee data used in the process of personalization as well as describing means to collect this data. We also identify three key explanation properties that are amendable to personalization: complexity, decision information and presentation. We also enhance existing work on explanation by introducing additional desiderata and measures to quantify the quality of personalized explanations.","",""
21,"Christopher Culley, S. Vijayakumar, Guido Zampieri, C. Angione","A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth",2020,"","","","",91,"2022-07-13 10:06:51","","10.1073/pnas.2002959117","","",,,,,21,10.50,5,4,2,"Significance Linking genotype and phenotype is a fundamental problem in biology, key to several biomedical and biotechnological applications. Cell growth is a central phenotypic trait, resulting from interactions between environment, gene regulation, and metabolism, yet its functional bases are still not completely understood. We propose and test a machine-learning approach that integrates large-scale gene expression profiles and mechanistic metabolic models, for characterizing cell growth and understanding its driving mechanisms in Saccharomyces cerevisiae. At its core, a custom-built multimodal learning method merges experimentally generated and model-generated data. We show that our approach can leverage the advantages of both machine learning and metabolic modeling, revealing unknown interactions between biological domains, incorporating mechanistic knowledge, and therefore overcoming black-box limitations of conventional data-driven approaches. Metabolic modeling and machine learning are key components in the emerging next generation of systems and synthetic biology tools, targeting the genotype–phenotype–environment relationship. Rather than being used in isolation, it is becoming clear that their value is maximized when they are combined. However, the potential of integrating these two frameworks for omic data augmentation and integration is largely unexplored. We propose, rigorously assess, and compare machine-learning–based data integration techniques, combining gene expression profiles with computationally generated metabolic flux data to predict yeast cell growth. To this end, we create strain-specific metabolic models for 1,143 Saccharomyces cerevisiae mutants and we test 27 machine-learning methods, incorporating state-of-the-art feature selection and multiview learning approaches. We propose a multiview neural network using fluxomic and transcriptomic data, showing that the former increases the predictive accuracy of the latter and reveals functional patterns that are not directly deducible from gene expression alone. We test the proposed neural network on a further 86 strains generated in a different experiment, therefore verifying its robustness to an additional independent dataset. Finally, we show that introducing mechanistic flux features improves the predictions also for knockout strains whose genes were not modeled in the metabolic reconstruction. Our results thus demonstrate that fusing experimental cues with in silico models, based on known biochemistry, can contribute with disjoint information toward biologically informed and interpretable machine learning. Overall, this study provides tools for understanding and manipulating complex phenotypes, increasing both the prediction accuracy and the extent of discernible mechanistic biological insights.","",""
14,"G. Rehm, Jinyoung Han, B. Kuhn, J. Delplanque, N. Anderson, Jason Y. Adams, C. Chuah","Creation of a Robust and Generalizable Machine Learning Classifier for Patient Ventilator Asynchrony.",2018,"","","","",92,"2022-07-13 10:06:51","","10.3414/ME17-02-0012","","",,,,,14,3.50,2,7,4,"BACKGROUND As healthcare increasingly digitizes, streaming waveform data is being made available from an variety of sources, but there still remains a paucity of performant clinical decision support systems. For example, in the intensive care unit (ICU) existing automated alarm systems typically rely on simple thresholding that result in frequent false positives. Recurrent false positive alerts create distrust of alarm mechanisms that can be directly detrimental to patient health. To improve patient care in the ICU, we need alert systems that are both pervasive, and accurate so as to be informative and trusted by providers.   OBJECTIVE We aimed to develop a machine learning-based classifier to detect abnormal waveform events using the use case of mechanical ventilation waveform analysis, and the detection of harmful forms of ventilation delivery to patients. We specifically focused on detecting injurious subtypes of patient-ventilator asynchrony (PVA).   METHODS Using a dataset of breaths recorded from 35 different patients, we used machine learning to create computational models to automatically detect, and classify two types of injurious PVA, double trigger asynchrony (DTA), breath stacking asynchrony (BSA). We examined the use of synthetic minority over-sampling technique (SMOTE) to overcome class imbalance problems, varied methods for feature selection, and use of ensemble methods to optimize the performance of our model.   RESULTS We created an ensemble classifier that is able to accurately detect DTA at a sensitivity/specificity of 0.960/0.975, BSA at sensitivity/specificity of 0.944/0.987, and non-PVA events at sensitivity/specificity of .967/.980.   CONCLUSIONS Our results suggest that it is possible to create a high-performing machine learning-based model for detecting PVA in mechanical ventilator waveform data in spite of both intra-patient, and inter-patient variability in waveform patterns, and the presence of clinical artifacts like cough and suction procedures. Our work highlights the importance of addressing class imbalance in clinical data sets, and the combined use of statistical methods and expert knowledge in feature selection.","",""
2,"Juan Gao, Chunfang Li, Zhen-Guo Liu, Lian-Zhong Liu","Elicitation of machine learning to human learning from iterative error correcting",2013,"","","","",93,"2022-07-13 10:06:51","","10.1109/ICMLC.2013.6890473","","",,,,,2,0.22,1,4,9,"Numerous high performance machine learning algorithms are designed based on human learning, while human learning can also acquire elicitation from machine learning to investigate highly efficient learning process. This paper presents two iteratively error correcting based probabilistic neural networks (PNN) for connecting human learning and machine learning. C-PNN, G-PNN and G-PNN have been used to delete redundancy samples in our learning software based on question bank. In detail, we propose a recommendation approach of learning samples which selects samples according to density of knowledge points through calculating data field of knowledge points covered by problems. The approach also deletes redundant problems in order to deal with the question-sea tactical and remedy the defects of random selecting usually used in human learning.","",""
13,"Koosha Sadeghi, A. Banerjee, S. Gupta","A System-Driven Taxonomy of Attacks and Defenses in Adversarial Machine Learning",2020,"","","","",94,"2022-07-13 10:06:51","","10.1109/TETCI.2020.2968933","","",,,,,13,6.50,4,3,2,"Machine Learning (ML) algorithms, specifically supervised learning, are widely used in modern real-world applications, which utilize Computational Intelligence (CI) as their core technology, such as autonomous vehicles, assistive robots, and biometric systems. Attacks that cause misclassifications or mispredictions can lead to erroneous decisions resulting in unreliable operations. Designing robust ML with the ability to provide reliable results in the presence of such attacks has become a top priority in the field of adversarial machine learning. An essential characteristic for rapid development of robust ML is an arms race between attack and defense strategists. However, an important prerequisite for the arms race is access to a well-defined system model so that experiments can be repeated by independent researchers. This article proposes a fine-grained system-driven taxonomy to specify ML applications and adversarial system models in an unambiguous manner such that independent researchers can replicate experiments and escalate the arms race to develop more evolved and robust ML applications. The article provides taxonomies for: 1) the dataset, 2) the ML architecture, 3) the adversary's knowledge, capability, and goal, 4) adversary's strategy, and 5) the defense response. In addition, the relationships among these models and taxonomies are analyzed by proposing an adversarial machine learning cycle. The provided models and taxonomies are merged to form a comprehensive system-driven taxonomy, which represents the arms race between the ML applications and adversaries in recent years. The taxonomies encode best practices in the field and help evaluate and compare the contributions of research works and reveals gaps in the field.","",""
10,"M. Campi, S. Garatti","Scenario optimization with relaxation: a new tool for design and application to machine learning problems",2020,"","","","",95,"2022-07-13 10:06:51","","10.1109/CDC42340.2020.9303914","","",,,,,10,5.00,5,2,2,"Scenario optimization is by now a well established technique to perform designs in the presence of uncertainty. It relies on domain knowledge integrated with first-hand information that comes from data and generates solutions that are also accompanied by precise statements of reliability. In this paper, following recent developments in [22], we venture beyond the traditional set-up of scenario optimization by analyzing the concept of constraints relaxation. By a solid theoretical underpinning, this new paradigm furnishes fundamental tools to perform designs that meet a proper compromise between robustness and performance. After suitably expanding the scope of constraints relaxation as proposed in [22], we focus on various classical Support Vector methods in machine learning – including SVM (Support Vector Machine), SVR (Support Vector Regression) and SVDD (Support Vector Data Description) – and derive new results that attest the ability of these methods to generalize.","",""
21,"Aaron M. Smith, J. Walsh, John J Long, Craig B Davis, Peter V. Henstock, M. Hodge, M. Maciejewski, X. Mu, Stephen Ra, Shanrong Zhao, D. Ziemek, Charles K. Fisher","Standard machine learning approaches outperform deep representation learning on phenotype prediction from transcriptomics data",2020,"","","","",96,"2022-07-13 10:06:51","","10.1186/s12859-020-3427-8","","",,,,,21,10.50,2,12,2,"","",""
14,"Zi Zhang, Hong Pan, Xingyu Wang, Zhibin Lin","Machine Learning-Enriched Lamb Wave Approaches for Automated Damage Detection",2020,"","","","",97,"2022-07-13 10:06:51","","10.3390/s20061790","","",,,,,14,7.00,4,4,2,"Lamb wave approaches have been accepted as efficiently non-destructive evaluations in structural health monitoring for identifying damage in different states. Despite significant efforts in signal process of Lamb waves, physics-based prediction is still a big challenge due to complexity nature of the Lamb wave when it propagates, scatters and disperses. Machine learning in recent years has created transformative opportunities for accelerating knowledge discovery and accurately disseminating information where conventional Lamb wave approaches cannot work. Therefore, the learning framework was proposed with a workflow from dataset generation, to sensitive feature extraction, to prediction model for lamb-wave-based damage detection. A total of 17 damage states in terms of different damage type, sizes and orientations were designed to train the feature extraction and sensitive feature selection. A machine learning method, support vector machine (SVM), was employed for the learning model. A grid searching (GS) technique was adopted to optimize the parameters of the SVM model. The results show that the machine learning-enriched Lamb wave-based damage detection method is an efficient and accuracy wave to identify the damage severity and orientation. Results demonstrated that different features generated from different domains had certain levels of sensitivity to damage, while the feature selection method revealed that time-frequency features and wavelet coefficients exhibited the highest damage-sensitivity. These features were also much more robust to noise. With increase of noise, the accuracy of the classification dramatically dropped.","",""
37,"Tianwei Yu, Dean P. Jones","Improving peak detection in high-resolution LC/MS metabolomics data using preexisting knowledge and machine learning approach",2014,"","","","",98,"2022-07-13 10:06:51","","10.1093/bioinformatics/btu430","","",,,,,37,4.63,19,2,8,"MOTIVATION Peak detection is a key step in the preprocessing of untargeted metabolomics data generated from high-resolution liquid chromatography-mass spectrometry (LC/MS). The common practice is to use filters with predetermined parameters to select peaks in the LC/MS profile. This rigid approach can cause suboptimal performance when the choice of peak model and parameters do not suit the data characteristics.   RESULTS Here we present a method that learns directly from various data features of the extracted ion chromatograms (EICs) to differentiate between true peak regions from noise regions in the LC/MS profile. It utilizes the knowledge of known metabolites, as well as robust machine learning approaches. Unlike currently available methods, this new approach does not assume a parametric peak shape model and allows maximum flexibility. We demonstrate the superiority of the new approach using real data. Because matching to known metabolites entails uncertainties and cannot be considered a gold standard, we also developed a probabilistic receiver-operating characteristic (pROC) approach that can incorporate uncertainties.   AVAILABILITY AND IMPLEMENTATION The new peak detection approach is implemented as part of the apLCMS package available at http://web1.sph.emory.edu/apLCMS/ CONTACT: tyu8@emory.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.","",""
8,"C. Rea, K. Montes, A. Pau, R. Granetz, O. Sauter","Progress Toward Interpretable Machine Learning–Based Disruption Predictors Across Tokamaks",2020,"","","","",99,"2022-07-13 10:06:51","","10.1080/15361055.2020.1798589","","",,,,,8,4.00,2,5,2,"Abstract In this paper we lay the groundwork for a robust cross-device comparison of data-driven disruption prediction algorithms on DIII-D and JET tokamaks. In order to consistently carry on a comparative analysis, we define physics-based indicators of disruption precursors based on temperature, density, and radiation profiles that are currently not used in many other machine learning predictors for DIII-D data. These profile-based indicators are shown to well-describe impurity accumulation events in both DIII-D and JET discharges that eventually disrupt. The univariate analysis of the features used as input signals in the data-driven algorithms applied on the data of both tokamaks statistically highlights the differences in the dominant disruption precursors. JET with its ITER-like wall is more prone to impurity accumulation events, while DIII-D is more subject to edge-cooling mechanisms that destabilize dangerous magnetohydrodynamic modes. Even though the analyzed data sets are characterized by such intrinsic differences, we show through a few examples that the inclusion of physics-based disruption markers in data-driven algorithms is a promising path toward the realization of a uniform framework to predict and interpret disruptive scenarios across different tokamaks. As long as the destabilizing precursors are diagnosed in a device-independent way, the knowledge that data-driven algorithms learn on one device can be re-used to explain a disruptive behavior on another device.","",""
11,"M. Elgendi, C. Menon","Machine Learning Ranks ECG as an Optimal Wearable Biosignal for Assessing Driving Stress",2020,"","","","",100,"2022-07-13 10:06:51","","10.1109/ACCESS.2020.2974933","","",,,,,11,5.50,6,2,2,"The demand for wearable devices that can detect anxiety and stress when driving is increasing. Recent studies have attempted to use multiple biosignals to detect driving stress. However, collecting multiple biosignals can be complex and is associated with numerous challenges. Determining the optimal biosignal for assessing driving stress can save lives. To the best of our knowledge, no study has investigated both longitudinal and transitional stress assessment using supervised and unsupervised ML techniques. Thus, this study hypothesizes that the optimal signal for assessing driving stress will consistently detect stress using supervised and unsupervised machine learning (ML) techniques. Two different approaches were used to assess driving stress: longitudinal (a combined repeated measurement of the same biosignals over three driving states) and transitional (switching from state to state such as city to highway driving). The longitudinal analysis did not involve a feature extraction phase while the transitional analysis involved a feature extraction phase. The longitudinal analysis consists of a novel interaction ensemble (INTENSE) that aggregates three unsupervised ML approaches: interaction principal component analysis, connectivity-based clustering, and K-means clustering. INTENSE was developed to uncover new knowledge by revealing the strongest correlation between the biosignal and driving stress marker. These three MLs each have their well-known and distinctive geometrical basis. Thus, the aggregation of their result would provide a more robust examination of the simultaneous non-causal associations between six biosignals: electrocardiogram (ECG), electromyogram, hand galvanic skin resistance, foot galvanic skin resistance, heart rate, respiration, and the driving stress marker. INTENSE indicates that ECG is highly correlated with the driving stress marker. The supervised ML algorithms confirmed that ECG is the most informative biosignal for detecting driving stress, with an overall accuracy of 75.02%.","",""
9,"Phauk Sokkhey, T. Okazaki","Hybrid Machine Learning Algorithms for Predicting Academic Performance",2020,"","","","",101,"2022-07-13 10:06:51","","10.14569/ijacsa.2020.0110104","","",,,,,9,4.50,5,2,2,"The large volume of data and its complexity in educational institutions require the sakes from informative technologies. In order to facilitate this task, many researchers have focused on using machine learning to extract knowledge from the education database to support students and instructors in getting better performance. In prediction models, the challenging task is to choose the effective techniques which could produce satisfying predictive accuracy. Hence, in this work, we introduced a hybrid approach of principal component analysis (PCA) as conjunction with four machines learning (ML) algorithms: random forest (RF), C5.0 of decision tree (DT), and naive Bayes (NB) of Bayes network and support vector machine (SVM), to improve the performances of classification by solving the misclassification problem. Three datasets were used to confirm the robustness of the proposed models. Through the given datasets, we evaluated the classification accuracy and root mean square error (RSME) as evaluation metrics of the proposed models. In this classification problem, 10-fold cross-validation was proposed to evaluate the predictive performance. The proposed hybrid models produced very prediction results which shown itself as the optimal prediction and classification algorithms.","",""
10,"Yang Liu, Jiaheng Wei","Incentives for Federated Learning: a Hypothesis Elicitation Approach",2020,"","","","",102,"2022-07-13 10:06:51","","","","",,,,,10,5.00,5,2,2,"Federated learning provides a promising paradigm for collecting machine learning models from distributed data sources without compromising users' data privacy. The success of a credible federated learning system builds on the assumption that the decentralized and self-interested users will be willing to participate to contribute their local models in a trustworthy way. However, without proper incentives, users might simply opt out the contribution cycle, or will be mis-incentivized to contribute spam/false information. This paper introduces solutions to incentivize truthful reporting of a local, user-side machine learning model for federated learning. Our results build on the literature of information elicitation, but focus on the questions of eliciting hypothesis (rather than eliciting human predictions). We provide a scoring rule based framework that incentivizes truthful reporting of local hypotheses at a Bayesian Nash Equilibrium. We study the market implementation, accuracy as well as robustness properties of our proposed solution too. We verify the effectiveness of our methods using MNIST and CIFAR-10 datasets. Particularly we show that by reporting low-quality hypotheses, users will receive decreasing scores (rewards, or payments).","",""
51,"Hong Chang, Virat Shejwalkar, R. Shokri, A. Houmansadr","Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer",2019,"","","","",103,"2022-07-13 10:06:51","","","","",,,,,51,17.00,13,4,3,"Collaborative (federated) learning enables multiple parties to train a model without sharing their private data, but through repeated sharing of the parameters of their local models. Despite its advantages, this approach has many known privacy and security weaknesses and performance overhead, in addition to being limited only to models with homogeneous architectures. Shared parameters leak a significant amount of information about the local (and supposedly private) datasets. Besides, federated learning is severely vulnerable to poisoning attacks, where some participants can adversarially influence the aggregate parameters. Large models, with high dimensional parameter vectors, are in particular highly susceptible to privacy and security attacks: curse of dimensionality in federated learning. We argue that sharing parameters is the most naive way of information exchange in collaborative learning, as they open all the internal state of the model to inference attacks, and maximize the model's malleability by stealthy poisoning attacks. We propose Cronus, a robust collaborative machine learning framework. The simple yet effective idea behind designing Cronus is to control, unify, and significantly reduce the dimensions of the exchanged information between parties, through robust knowledge transfer between their black-box local models. We evaluate all existing federated learning algorithms against poisoning attacks, and we show that Cronus is the only secure method, due to its tight robustness guarantee. Treating local models as black-box, reduces the information leakage through models, and enables us using existing privacy-preserving algorithms that mitigate the risk of information leakage through the model's output (predictions). Cronus also has a significantly lower sample complexity, compared to federated learning, which does not bind its security to the number of participants.","",""
16,"Pedram Daee, T. Peltola, Aki Vehtari, Samuel Kaski","User Modelling for Avoiding Overfitting in Interactive Knowledge Elicitation for Prediction",2017,"","","","",104,"2022-07-13 10:06:51","","10.1145/3172944.3172989","","",,,,,16,3.20,4,4,5,"In human-in-the-loop machine learning, the user provides information beyond that in the training data. Many algorithms and user interfaces have been designed to optimize and facilitate this human--machine interaction; however, fewer studies have addressed the potential defects the designs can cause. Effective interaction often requires exposing the user to the training data or its statistics. The design of the system is then critical, as this can lead to double use of data and overfitting, if the user reinforces noisy patterns in the data. We propose a user modelling methodology, by assuming simple rational behaviour, to correct the problem. We show, in a user study with 48 participants, that the method improves predictive performance in a sparse linear regression sentiment analysis task, where graded user knowledge on feature relevance is elicited. We believe that the key idea of inferring user knowledge with probabilistic user models has general applicability in guarding against overfitting and improving interactive machine learning.","",""
3,"U. Erdogdu, Mehmet Tan, R. Alhajj, Faruk Polat, J. Rokne, D. Demetrick","Integrating machine learning techniques into robust data enrichment approach and its application to gene expression data",2013,"","","","",105,"2022-07-13 10:06:51","","10.1504/IJDMB.2013.056090","","",,,,,3,0.33,1,6,9,"The availability of enough samples for effective analysis and knowledge discovery has been a challenge in the research community, especially in the area of gene expression data analysis. Thus, the approaches being developed for data analysis have mostly suffered from the lack of enough data to train and test the constructed models. We argue that the process of sample generation could be successfully automated by employing some sophisticated machine learning techniques. An automated sample generation framework could successfully complement the actual sample generation from real cases. This argument is validated in this paper by describing a framework that integrates multiple models (perspectives) for sample generation. We illustrate its applicability for producing new gene expression data samples, a highly demanding area that has not received attention. The three perspectives employed in the process are based on models that are not closely related. The independence eliminates the bias of having the produced approach covering only certain characteristics of the domain and leading to samples skewed towards one direction. The first model is based on the Probabilistic Boolean Network (PBN) representation of the gene regulatory network underlying the given gene expression data. The second model integrates Hierarchical Markov Model (HIMM) and the third model employs a genetic algorithm in the process. Each model learns as much as possible characteristics of the domain being analysed and tries to incorporate the learned characteristics in generating new samples. In other words, the models base their analysis on domain knowledge implicitly present in the data itself. The developed framework has been extensively tested by checking how the new samples complement the original samples. The produced results are very promising in showing the effectiveness, usefulness and applicability of the proposed multi-model framework.","",""
3,"J. Correia, Juliana Alves Pereira, Rafael Maiani de Mello, Alessandro F. Garcia, B. Neto, Márcio Ribeiro, Rohit Gheyi, M. Kalinowski, Renato Cerqueira, Willy Tiengo","Brazilian Data Scientists: Revealing their Challenges and Practices on Machine Learning Model Development",2020,"","","","",106,"2022-07-13 10:06:51","","10.1145/3439961.3439971","","",,,,,3,1.50,0,10,2,"Data scientists often develop machine learning models to solve a variety of problems in the industry and academy. To build these models, these professionals usually perform activities that are also performed in the traditional software development lifecycle, such as eliciting and implementing requirements. One might argue that data scientists could rely on the engineering of traditional software development to build machine learning models. However, machine learning development presents certain characteristics, which may raise challenges that lead to the need for adopting new practices. The literature lacks in characterizing this knowledge from the perspective of the data scientists. In this paper, we characterize challenges and practices addressing the engineering of machine learning models that deserve attention from the research community. To this end, we performed a qualitative study with eight data scientists across five different companies having different levels of experience in developing machine learning models. Our findings suggest that: (i) data processing and feature engineering are the most challenging stages in the development of machine learning models; (ii) it is essential synergy between data scientists and domain experts in most of stages; and (iii) the development of machine learning models lacks the support of a well-engineered process.","",""
5,"Zhuolin Yang, Zhikuan Zhao, Hengzhi Pei, Boxin Wang, Bojan Karlas, Ji Liu, Heng Guo, Bo Li, Ce Zhang","End-to-end Robustness for Sensing-Reasoning Machine Learning Pipelines",2020,"","","","",107,"2022-07-13 10:06:51","","","","",,,,,5,2.50,1,9,2,"As machine learning (ML) being applied to many mission-critical scenarios, certifying ML model robustness becomes increasingly important. Many previous works focuses on the robustness of independent ML and ensemble models, and can only certify a very small magnitude of the adversarial perturbation. In this paper, we take a different viewpoint and improve learning robustness by going beyond independent ML and ensemble models. We aim at promoting the generic Sensing-Reasoning machine learning pipeline which contains both the sensing (e.g. deep neural networks) and reasoning (e.g. Markov logic networks (MLN)) components enriched with domain knowledge. Can domain knowledge help improve learning robustness? Can we formally certify the end-to-end robustness of such an ML pipeline?  We first theoretically analyze the computational complexity of checking the provable robustness in the reasoning component. We then derive the provable robustness bound for several concrete reasoning components. We show that for reasoning components such as MLN and a specific family of Bayesian networks it is possible to certify the robustness of the whole pipeline even with a large magnitude of perturbation which cannot be certified by existing work. Finally, we conduct extensive real-world experiments on large scale datasets to evaluate the certified robustness for Sensing-Reasoning ML pipelines.","",""
85,"Neoklis Polyzotis, Sudip Roy, S. E. Whang, Martin A. Zinkevich","Data Lifecycle Challenges in Production Machine Learning",2018,"","","","",108,"2022-07-13 10:06:51","","10.1145/3299887.3299891","","",,,,,85,21.25,21,4,4,"Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to explore how different constraints are imposed on the solutions depending on where in the lifecycle of a model the problems are encountered and who encounters them.","",""
3,"N. Major, N. Shadbolt","CNN: Integrating Knowledge Elicitation With a Machine Learning Technique",1992,"","","","",109,"2022-07-13 10:06:51","","","","",,,,,3,0.10,2,2,30,"","",""
28,"Lili Su, Jiaming Xu","Securing Distributed Machine Learning in High Dimensions",2018,"","","","",110,"2022-07-13 10:06:51","","","","",,,,,28,7.00,14,2,4,"We consider securing a distributed machine learning system wherein the data is kept confidential by its providers who are recruited as workers to help the learner to train a $d$--dimensional model. In each communication round, up to $q$ out of the $m$ workers suffer Byzantine faults; faulty workers are assumed to have complete knowledge of the system and can collude to behave arbitrarily adversarially against the learner. We assume that each worker keeps a local sample of size $n$. (Thus, the total number of data points is $N=nm$.) Of particular interest is the high-dimensional regime $d \gg n$.  We propose a secured variant of the classical gradient descent method which can tolerate up to a constant fraction of Byzantine workers. We show that the estimation error of the iterates converges to an estimation error $O(\sqrt{q/N} + \sqrt{d/N})$ in $O(\log N)$ rounds. The core of our method is a robust gradient aggregator based on the iterative filtering algorithm proposed by Steinhardt et al. \cite{Steinhardt18} for robust mean estimation. We establish a uniform concentration of the sample covariance matrix of gradients, and show that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function. As a by-product, we develop a new concentration inequality for sample covariance matrices of sub-exponential distributions, which might be of independent interest.","",""
2,"Jiyuan Tu, Weidong Liu, Xiaojun Mao","Byzantine-robust distributed sparse learning for M-estimation",2021,"","","","",111,"2022-07-13 10:06:51","","10.1007/S10994-021-06001-X","","",,,,,2,2.00,1,3,1,"","",""
89,"Huichen Lihuichen","DECISION-BASED ADVERSARIAL ATTACKS: RELIABLE ATTACKS AGAINST BLACK-BOX MACHINE LEARNING MODELS",2017,"","","","",112,"2022-07-13 10:06:51","","","","",,,,,89,17.80,89,1,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradientor score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available at XXXXXX. Gradient-based Model M Untargeted Flip to any label Targeted Flip to target label FGSM, DeepFool L-BFGS-B, Houdini, JSMA, Carlini & Wagner, Iterative Gradient Descent Score-based Detailed Model Prediction Y (e.g. probabilities or logits) ZOO Local Search Decision-based Final Model Prediction Ymax (e.g. max class label) this work (Boundary Attack) Transfer-based Training Data T","",""
23,"Muxin Gu, M. Buckley","Semi-supervised machine learning for automated species identification by collagen peptide mass fingerprinting",2018,"","","","",113,"2022-07-13 10:06:51","","10.1186/s12859-018-2221-3","","",,,,,23,5.75,12,2,4,"","",""
28,"Tristan D. McRae, D. Oleksyn, Jim Miller, Yu-Rong Gao","Robust blind spectral unmixing for fluorescence microscopy using unsupervised learning",2019,"","","","",114,"2022-07-13 10:06:51","","10.1371/journal.pone.0225410","","",,,,,28,9.33,7,4,3,"Due to the overlapping emission spectra of fluorophores, fluorescence microscopy images often have bleed-through problems, leading to a false positive detection. This problem is almost unavoidable when the samples are labeled with three or more fluorophores, and the situation is complicated even further when imaged under a multiphoton microscope. Several methods have been developed and commonly used by biologists for fluorescence microscopy spectral unmixing, such as linear unmixing, non-negative matrix factorization, deconvolution, and principal component analysis. However, they either require pre-knowledge of emission spectra or restrict the number of fluorophores to be the same as detection channels, which highly limits the real-world applications of those spectral unmixing methods. In this paper, we developed a robust and flexible spectral unmixing method: Learning Unsupervised Means of Spectra (LUMoS), which uses an unsupervised machine learning clustering method to learn individual fluorophores’ spectral signatures from mixed images, and blindly separate channels without restrictions on the number of fluorophores that can be imaged. This method highly expands the hardware capability of two-photon microscopy to simultaneously image more fluorophores than is possible with instrumentation alone. Experimental and simulated results demonstrated the robustness of LUMoS in multi-channel separations of two-photon microscopy images. We also extended the application of this method to background/autofluorescence removal and colocalization analysis. Lastly, we integrated this tool into ImageJ to offer an easy to use spectral unmixing tool for fluorescence imaging. LUMoS allows us to gain a higher spectral resolution and obtain a cleaner image without the need to upgrade the imaging hardware capabilities.","",""
7,"N. Ball, R. Brunner, A. Myers","Robust Machine Learning Applied to Terascale Astronomical Datasets",2007,"","","","",115,"2022-07-13 10:06:51","","","","",,,,,7,0.47,2,3,15,"We present recent results from the LCDM (Laboratory for Cosmological Data Mining; this http URL) collaboration between UIUC Astronomy and NCSA to deploy supercomputing cluster resources and machine learning algorithms for the mining of terascale astronomical datasets. This is a novel application in the field of astronomy, because we are using such resources for data mining, and not just performing simulations. Via a modified implementation of the NCSA cyberenvironment Data-to-Knowledge, we are able to provide improved classifications for over 100 million stars and galaxies in the Sloan Digital Sky Survey, improved distance measures, and a full exploitation of the simple but powerful k-nearest neighbor algorithm. A driving principle of this work is that our methods should be extensible from current terascale datasets to upcoming petascale datasets and beyond. We discuss issues encountered to-date, and further issues for the transition to petascale. In particular, disk I/O will become a major limiting factor unless the necessary infrastructure is implemented.","",""
11,"Ghassan Alnwaimi, Talha Zahir, S. Vahid, K. Moessner","Machine Learning Based Knowledge Acquisition on Spectrum Usage for LTE Femtocells",2013,"","","","",116,"2022-07-13 10:06:51","","10.1109/VTCFall.2013.6692276","","",,,,,11,1.22,3,4,9,"The decentralised and ad hoc nature of femtocell deployments calls for distributed learning strategies to mitigate interference. We propose a distributed spectrum awareness scheme for femtocell networks, based on combined payoff and strategy reinforcement learning (RL) models. We present two different learning strategies, based on modifications to the Bush Mosteller (BM) RL and the Roth-Erev RL algorithms. The simulation results show the convergence behaviour of the learning strategies under a dynamic robust game. As compared to the Bush Mosteller (BM) RL, our modified BM (MBM) converges smoothly to a stable satisfactory solution. Moreover, the MBM significantly reduces the interference collision cost during the learning process. Both the MBM and the modified Roth-Erev (MRE) algorithms are stochastic-based learning strategies which require less computation than the gradient follower (GF) learning strategy and have the capability to escape from suboptimal solution.","",""
35,"Feng Ren, Chenglei Wang, Hui Tang","Active control of vortex-induced vibration of a circular cylinder using machine learning",2019,"","","","",117,"2022-07-13 10:06:51","","10.1063/1.5115258","","",,,,,35,11.67,12,3,3,"We demonstrate the use of high-fidelity computational fluid dynamics simulations in machine-learning based active flow control. More specifically, for the first time, we adopt the genetic programming (GP) to select explicit control laws, in a data-driven and unsupervised manner, for the suppression of vortex-induced vibration (VIV) of a circular cylinder in a low-Reynolds-number flow (Re = 100), using blowing/suction at fixed locations. A cost function that balances both VIV suppression and energy consumption for the control is carefully chosen according to the knowledge obtained from pure blowing/suction open-loop controls. By implementing reasonable constraints to VIV amplitude and actuation strength during the GP evolution, the GP-selected best ten control laws all point to suction-type actuation. The best control law suggests that the suction strength should be nonzero when the cylinder is at its equilibrium position and should increase nonlinearly with the cylinder’s transverse displacement. Applying this control law suppresses 94.2% of the VIV amplitude and achieves 21.4% better overall performance than the best open-loop controls. Furthermore, it is found that the GP-selected control law is robust, being effective in flows ranging from Re = 100 to 400. On the contrary, although the P-control can achieve similar performance as the GP-selected control at Re = 100, it deteriorates in higher Reynolds number flows. Although for demonstration purpose the chosen control problem is relatively simple, the training experience and insights obtained from this study can shed some light on future GP-based control of more complicated problems.We demonstrate the use of high-fidelity computational fluid dynamics simulations in machine-learning based active flow control. More specifically, for the first time, we adopt the genetic programming (GP) to select explicit control laws, in a data-driven and unsupervised manner, for the suppression of vortex-induced vibration (VIV) of a circular cylinder in a low-Reynolds-number flow (Re = 100), using blowing/suction at fixed locations. A cost function that balances both VIV suppression and energy consumption for the control is carefully chosen according to the knowledge obtained from pure blowing/suction open-loop controls. By implementing reasonable constraints to VIV amplitude and actuation strength during the GP evolution, the GP-selected best ten control laws all point to suction-type actuation. The best control law suggests that the suction strength should be nonzero when the cylinder is at its equilibrium position and should increase nonlinearly with the cylinder’s transverse displacement. Applying...","",""
16,"E. Swann, B. Sun, D. Cleland, A. Barnard","Representing molecular and materials data for unsupervised machine learning",2018,"","","","",118,"2022-07-13 10:06:51","","10.1080/08927022.2018.1450982","","",,,,,16,4.00,4,4,4,"Abstract Statistical analysis and machine learning can help us understand and predict the collective properties and performance of ensembles of molecules and nanostructures, while accounting for all the complexity and diversity of real world specimens. Combining data-driven techniques with robust and reliable simulation methods can provide insights that cannot be made any other way. However, not all statistical and machine learning methods are right for all occasions; testing, validation and perhaps some trial and error are needed. Domain knowledge alone is not sufficient to choose the right algorithms. Data representation methods that are best suited to machine learning are not necessarily scientifically intuitive. The best descriptors are not always the structural features or physiochemical properties that we are aiming to control, and the way our data is distributed can be as important as what it contains. In this review, we discuss the differences, advantages and disadvantages of some of the common data representation, reduction and classification methods applicable to molecular and materials modelling. Focussing on unsupervised methods, we highlight features of these algorithms that determine their suitability and can inform choices of which learning method to use and how to effectively prepare data. A case study is also provided to demonstrate how testing can be undertaken, and how methods can be combined.","",""
13,"Ali A. Abdallah, S. Saab, Z. Kassas","A machine learning approach for localization in cellular environments",2018,"","","","",119,"2022-07-13 10:06:51","","10.1109/PLANS.2018.8373508","","",,,,,13,3.25,4,3,4,"A machine learning approach is developed for localization based on received signal strength (RSS) from cellular towers. The proposed approach only assumes knowledge of RSS fingerprints of the environment, and does not require knowledge of the cellular base transceiver station (BTS) locations, nor uses any RSS mathematical model. The proposed localization scheme integrates a weighted K-nearest neighbor (WKNN) and a multilayer neural network. The integration takes advantage of the robust clustering ability of WKNN and implements a neural network that could estimate the position within each cluster. Experimental results are presented to demonstrate the proposed approach in two urban environments and one rural environment, achieving a mean distance localization error of 5.9 m and 5.1 m in the urban environments and 8.7 m in the rural environment. This constitutes an improvement of 41%, 45%, and 16%, respectively, over the WKNN-only algorithm.","",""
65,"N. Ball, R. Brunner, A. Myers, D. E. U. O. I. A. Urbana-Champaign, National Center for Supercomputing Applications","Robust machine learning applied to astronomical data sets. I. Star-galaxy classification of the sloan digital sky survey DR3 using decision trees",2006,"","","","",120,"2022-07-13 10:06:51","","10.1086/507440","","",,,,,65,4.06,13,5,16,"We provide classifications for all 143 million nonrepeat photometric objects in the Third Data Release of the SDSS using decision trees trained on 477,068 objects with SDSS spectroscopic data. We demonstrate that these star/galaxy classifications are expected to be reliable for approximately 22 million objects with r 20. The general machine learning environment Data-to-Knowledge and supercomputing resources enabled extensive investigation of the decision tree parameter space. This work presents the first public release of objects classified in this way for an entire SDSS data release. The objects are classified as either galaxy, star, or nsng (neither star nor galaxy), with an associated probability for each class. To demonstrate how to effectively make use of these classifications, we perform several important tests. First, we detail selection criteria within the probability space defined by the three classes to extract samples of stars and galaxies to a given completeness and efficiency. Second, we investigate the efficacy of the classifications and the effect of extrapolating from the spectroscopic regime by performing blind tests on objects in the SDSS, 2dFGRS, and 2QZ surveys. Given the photometric limits of our spectroscopic training data, we effectively begin to extrapolate past our star-galaxy training set at r ~ 18. By comparing the number counts of our training sample with the classified sources, however, we find that our efficiencies appear to remain robust to r ~ 20. As a result, we expect our classifications to be accurate for 900,000 galaxies and 6.7 million stars and remain robust via extrapolation for a total of 8.0 million galaxies and 13.9 million stars.","",""
10,"Runzhe Zhan, Xuebo Liu, Derek F. Wong, Lidia S. Chao","Meta-Curriculum Learning for Domain Adaptation in Neural Machine Translation",2021,"","","","",121,"2022-07-13 10:06:51","","","","",,,,,10,10.00,3,4,1,"Meta-learning has been sufficiently validated to be beneficial for low-resource neural machine translation (NMT). However, we find that meta-trained NMT fails to improve the translation performance of the domain unseen at the metatraining stage. In this paper, we aim to alleviate this issue by proposing a novel meta-curriculum learning for domain adaptation in NMT. During meta-training, the NMT first learns the similar curricula from each domain to avoid falling into a bad local optimum early, and finally learns the curricula of individualities to improve the model robustness for learning domain-specific knowledge. Experimental results on 10 different low-resource domains show that meta-curriculum learning can improve the translation performance of both familiar and unfamiliar domains. All the codes and data are freely available at https://github.com/NLP2CT/ Meta-Curriculum.","",""
22,"Mohamed Maher, S. Sakr","SmartML: A Meta Learning-Based Framework for Automated Selection and Hyperparameter Tuning for Machine Learning Algorithms",2019,"","","","",122,"2022-07-13 10:06:51","","10.5441/002/edbt.2019.54","","",,,,,22,7.33,11,2,3,"Due to the increasing success of machine learning techniques, nowadays, thay have been widely utilized in almost every domain such as financial applications, marketing, recommender systems and user behavior analytics, just to name a few. In practice, the machine learning model creation process is a highly iterative exploratory process. In particular, an effective machine learning modeling process requires solid knowledge and understanding of the different types of machine learning algorithms. In addition, all machine learning algorithms require user-defined inputs to achieve a balance between accuracy and generalizability. This task is referred to as Hyperparameter Tuning . Thus, in practice, data scientists work hard to find the best model or algorithm that meets the specifications of their prob-lem. Such iterative and explorative nature of the modeling process is commonly tedious and time-consuming. We demonstrate SmartML , a meta learning-based framework for automated selection and hyperparameter tuning for machine learning algorithms. Being meta learning-based, the framework is able to simulate the role of the machine learning expert. In particular, the framework is equipped with a continuously updated knowledge base that stores information about the meta-features of all processed datasets along with the associated performance of the different classifiers and their tuned parameters. Thus, for any new dataset, SmartML automatically extracts its meta features and searches its knowledge base for the best performing algorithm to start its optimization process. In addition, SmartML makes use of the new runs to continuously en-rich its knowledge base to improve its performance and robustness for future runs. We will show how our approach outperforms the-state-of-the-art techniques in the domain of automated machine learning frameworks.","",""
20,"Di Wu, Binxing Fang, Junnan Wang, Qixu Liu, Xiang Cui","Evading Machine Learning Botnet Detection Models via Deep Reinforcement Learning",2019,"","","","",123,"2022-07-13 10:06:51","","10.1109/ICC.2019.8761337","","",,,,,20,6.67,4,5,3,"Botnets are one of predominant threats to Internet security. To date, machine learning technology has wide application in botnet detection because that it is able to summarize the features of existing attacks and generalize to never-before-seen botnet families. However, recent works in adversarial machine learning have shown that attackers are able to bypass the detection model by constructing specific samples, which due to many algorithms are vulnerable to almost imperceptible perturbations of their inputs. According to the degree of adversaries' knowledge about the model, adversarial attacks can be classified into several groups, such as gradient- and score-based attacks. In this paper, we propose a more general framework based on deep reinforcement learning (DRL), which effectively generates adversarial traffic flows to deceive the detection model by automatically adding perturbations to samples. Throughout the process, the target detector will be regarded as a black box and more close to realistic attack circumstance. A reinforcement learning agent is equipped for updating the adversarial samples by combining the feedback from the target model (i.e. benign or malicious) and the sequence of actions, which is able to change the temporal and spatial features of the traffic flows while maintaining the original functionality and executability. The experiment results show that the evasion rates of adversarial botnet flows are significantly improved. Furthermore, with the perspective of defense, this research can help the detection model spot its defect and thus enhance the robustness.","",""
16,"Mo Zhou, Yoshimi Fukuoka, Ken Goldberg, E. Vittinghoff, Anil Aswani","Applying machine learning to predict future adherence to physical activity programs",2019,"","","","",124,"2022-07-13 10:06:51","","10.1186/s12911-019-0890-0","","",,,,,16,5.33,3,5,3,"","",""
67,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings",2017,"","","","",125,"2022-07-13 10:06:51","","10.1145/3154503","","",,,,,67,13.40,22,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q ≤ for an arbitrarily small but fixed constant ε > 0. The parameter estimate converges in O(log N) rounds with an estimation error on the order of max{√dq/N, √d/N, which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q. The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
2,"V. Chernozhukov, W. Newey, Victor Quintas-Martinez, Vasilis Syrgkanis","RieszNet and ForestRiesz: Automatic Debiased Machine Learning with Neural Nets and Random Forests",2021,"","","","",126,"2022-07-13 10:06:51","","","","",,,,,2,2.00,1,4,1,"Many causal and policy effects of interest are deﬁned by linear functionals of high-dimensional or non-parametric regression functions. √ n consistent and asymptotically normal estimation of the object of interest requires debiasing to reduce the effects of regularization and/or model selection on the object of interest. Debiasing is typically achieved by adding a correction term to the plug-in estimator of the functional, which leads to properties such as semi-parametric efﬁ-ciency, double robustness, and Neyman orthogo-nality. We implement an automatic debiasing pro-cedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. Our method only relies on black-box evaluation oracle access to the linear functional and does not require knowledge of its analytic form. We propose a multitasking Neural Net debiasing method with stochastic gradient descent minimization of a combined Riesz representer and regression loss, while sharing representation layers for the two functions. We also propose a Random Forest method which learns a locally linear representation of the Riesz function. Even though our method applies to arbitrary functionals, we experimentally ﬁnd that it performs well compared to the state of art neural net based algorithm of Shi et al. (2019) for the case of the average treatment effect functional. We also evaluate our method on the problem of estimating average marginal effects with continuous treat-ments, using semi-synthetic data of gasoline price changes on gasoline demand. Code available at","",""
10,"J. Collins, K. Howe, B. Nachman","CWoLa Hunting: Extending the Bump Hunt with Machine Learning",2018,"","","","",127,"2022-07-13 10:06:51","","","","",,,,,10,2.50,3,3,4,"The oldest and most robust technique to search for new particles is to look for `bumps' in invariant mass spectra over smoothly falling backgrounds. This is a powerful technique, but only uses one-dimensional information. One can restrict the phase space to enhance a potential signal, but such tagging techniques require a signal hypothesis and training a classifier in simulation and applying it on data. We present a new method for using all of the available information (with machine learning) without using any prior knowledge about potential signals. Given the lack of new physics signals at the Large Hadron Collider (LHC), such model independent approaches are critical for ensuring full coverage to fully exploit the rich datasets from the LHC experiments. In addition to illustrating how the new method works in simple test cases, we demonstrate the power of the extended bump hunt on a realistic all-hadronic resonance search in a channel that would not be covered with existing techniques.","",""
3,"M. Mozina","Arguments in Interactive Machine Learning",2018,"","","","",128,"2022-07-13 10:06:51","","","","",,,,,3,0.75,3,1,4,"In most applications of machine learning, domain experts provide domain specic knowledge. From previous experience it is known that domain experts are unable to provide all relevant knowledge in advance, but need to see some results of machine learning rst. Interactive machine learning, where experts and machine learning algorithm improve the model in turns, seems to solve this problem. In this position paper, we propose to use arguments in interaction between machine learning and experts. Since using and understanding arguments is a practical skill that humans learn in everyday life, we believe that arguments will help experts to better understand the models, facilitate easier elicitation of new knowledge from experts, and can be intuitively integrated in machine learning. We describe an argument-based dialogue, which is based on a series of steps such as questions and arguments, that can help obtain from a domain expert exactly that knowledge which is missing in the current model.","",""
32,"Qigang Li, Keyan Zhao, C. Bustamante, Xin Ma, W. Wong","Xrare: a machine learning method jointly modeling phenotypes and genetic evidence for rare disease diagnosis",2019,"","","","",129,"2022-07-13 10:06:51","","10.1038/s41436-019-0439-8","","",,,,,32,10.67,6,5,3,"","",""
5,"Y. Malhotra","AI, Machine Learning & Deep Learning Risk Management & Controls: Beyond Deep Learning and Generative Adversarial Networks: Model Risk Management in AI, Machine Learning & Deep Learning",2018,"","","","",130,"2022-07-13 10:06:51","","10.2139/SSRN.3193693","","",,,,,5,1.25,5,1,4,"The current paper proposes how model risk management in operationalizing machine learning for algorithm deployment can be applied in national C4I and Cyber projects such as Project Maven. It builds upon recent leadership of global Management and Leadership industry executives for AI and Machine Learning Executive Education for MIT Sloan School of Management and the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and invited presentations at Princeton University. After building understanding about why model risk management is most crucial to robust AI, Machine Learning, Deep Learning, and, Neural Networks deployment, it introduces a Knowledge Management Framework for Model Risk Management to advance beyond ‘AI Automation’ to ‘AI Augmentation.’","",""
4,"Henggang Cui, G. Ganger, Phillip B. Gibbons","MLtuner: System Support for Automatic Machine Learning Tuning",2018,"","","","",131,"2022-07-13 10:06:51","","","","",,,,,4,1.00,1,3,4,"MLtuner automatically tunes settings for training tunables (such as the learning rate, the momentum, the mini-batch size, and the data staleness bound) that have a significant impact on large-scale machine learning (ML) performance. Traditionally, these tunables are set manually, which is unsurprisingly error-prone and difficult to do without extensive domain knowledge. MLtuner uses efficient snapshotting, branching, and optimization-guided online trial-and-error to find good initial settings as well as to re-tune settings during execution. Experiments show that MLtuner can robustly find and re-tune tunable settings for a variety of ML applications, including image classification (for 3 models and 2 datasets), video classification, and matrix factorization. Compared to state-of-the-art ML auto-tuning approaches, MLtuner is more robust for large problems and over an order of magnitude faster.","",""
5,"D. Mislis, S. Pyrzas, K. Alsubai","TSARDI: a Machine Learning data rejection algorithm for transiting exoplanet light curves",2018,"","","","",132,"2022-07-13 10:06:51","","10.1093/mnras/sty2361","","",,,,,5,1.25,2,3,4,"We present TSARDI, an efficient rejection algorithm designed to improve the transit detection efficiency in data collected by large scale surveys. TSARDI is based on the Machine Learning clustering algorithm DBSCAN, and its purpose is to serve as a robust and adaptable filter aiming to identify unwanted noise points left over from data detrending processes. TSARDI is an unsupervised method, which can treat each light curve individually; there is no need of previous knowledge of any other field light curves. We conduct a simulated transit search by injecting planets on real data obtained by the QES project and show that TSARDI leads to an overall transit detection efficiency increase of $\sim$11\%, compared to results obtained from the same sample, but using a standard sigma-clip algorithm. For the brighter end of our sample (host star magnitude < 12), TSARDI achieves a detection efficiency of $\sim$80\% of injected planets. While our algorithm has been developed primarily for the field of exoplanets, it is easily adaptable and extendable for use in any time series.","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",133,"2022-07-13 10:06:51","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
2,"L. Yang, Yuncheng Dong, Jiafu Zhuang, Jun Yu Li","A Recognition Algorithm for Workpieces Based on the Machine Learning",2018,"","","","",134,"2022-07-13 10:06:51","","10.1109/ISCID.2018.10185","","",,,,,2,0.50,1,4,4,"In order to learn and grasp the predetermined workpieces for robot actively, a recognition algorithm based on machine learning is proposed. Compared with traditional algorithms, we replenish a MTSM (multi threshold space model) for getting clearer workpiece shapes. To automatically and compactly learn workpieces knowledge, both shape and gradient features are designed to express the specific object by aid of contour mask, meanwhile, the compound descriptors are fed into a SVM classifier and they are trained jointly to minimize a classification loss. Finally, we adopt density estimation to acquire the grasping point of the workpieces from MTSM. Experimental result of workpieces grasping demonstrates the effectiveness and stability in complex environment, and the proposed algorithm is robust to rotation, scaling and deformation of shapes.","",""
524,"S. Raschka","Python Machine Learning",2015,"","","","",135,"2022-07-13 10:06:51","","","","",,,,,524,74.86,524,1,7,"Unlock deeper insights into Machine Leaning with this vital guide to cutting-edge predictive analyticsAbout This BookLeverage Python's most powerful open-source libraries for deep learning, data wrangling, and data visualizationLearn effective strategies and best practices to improve and optimize machine learning systems and algorithmsAsk and answer tough questions of your data with robust statistical models, built for a range of datasetsWho This Book Is ForIf you want to find out how to use Python to start answering critical questions of your data, pick up Python Machine Learning whether you want to get started from scratch or want to extend your data science knowledge, this is an essential and unmissable resource.What You Will LearnExplore how to use different machine learning models to ask different questions of your dataLearn how to build neural networks using Keras and TheanoFind out how to write clean and elegant Python code that will optimize the strength of your algorithmsDiscover how to embed your machine learning model in a web application for increased accessibilityPredict continuous target outcomes using regression analysisUncover hidden patterns and structures in data with clusteringOrganize data using effective pre-processing techniquesGet to grips with sentiment analysis to delve deeper into textual and social media dataIn DetailMachine learning and predictive analytics are transforming the way businesses and other organizations operate. Being able to understand trends and patterns in complex data is critical to success, becoming one of the key strategies for unlocking growth in a challenging contemporary marketplace. Python can help you deliver key insights into your data its unique capabilities as a language let you build sophisticated algorithms and statistical models that can reveal new perspectives and answer key questions that are vital for success.Python Machine Learning gives you access to the world of predictive analytics and demonstrates why Python is one of the world's leading data science languages. If you want to ask better questions of data, or need to improve and extend the capabilities of your machine learning systems, this practical data science book is invaluable. Covering a wide range of powerful Python libraries, including scikit-learn, Theano, and Keras, and featuring guidance and tips on everything from sentiment analysis to neural networks, you'll soon be able to answer some of the most important questions facing you and your organization.Style and approachPython Machine Learning connects the fundamental theoretical principles behind machine learning to their practical application in a way that focuses you on asking and answering the right questions. It walks you through the key elements of Python and its powerful machine learning libraries, while demonstrating how to get to grips with a range of statistical models.","",""
9,"Kevser Ovaz Akpinar, Ibrahim Ozcelik","Analysis of Machine Learning Methods in EtherCAT-Based Anomaly Detection",2019,"","","","",136,"2022-07-13 10:06:51","","10.1109/ACCESS.2019.2960497","","",,,,,9,3.00,5,2,3,"Today, the use of Ethernet-based protocols in industrial control systems (ICS) communications has led to the emergence of attacks based on information technology (IT) on supervisory control and data acquisition systems. In addition, the familiarity of Ethernet and TCP/IP protocols and the diversity and success of attacks on them raises security risks and cyber threats for ICS. This issue is compounded by the absence of encryption, authorization, and authentication mechanisms due to the development of industrial communications protocols only for performance purposes. Recent zero-day attacks, such as Triton, Stuxnet, Havex, Dragonfly, and Blackenergy, as well as the Ukraine cyber-attack, are possible because of the vulnerabilities of the systems; these attacksare carried by the protocols used in communication between PLC and I/O units or HMI and engineering stations. It is evident that there is a need for robust solutions that detect and prevent protocol-based cyber threats. In this paper, machine learning methods are evaluated for anomaly detection, particularly for EtherCAT-based ICS. To the best of the author’s knowledge, there has been no research focusing on machine learning algorithms for anomaly detection of EtherCAT. Before testing anomaly detection, an EtherCAT-based water level control system testbed was developed. Then, a total of 16 events were generated in four categories and applied on the testbed. The dataset created was used for anomaly detection. The results showed that the k-nearest neighbors (k-NN) and support vector machine with genetic algorithm (SVM GA) models perform best among the 18 techniques applied. In addition to detecting anomalies, the methods are able to flag the attack types better than other techniques and are applicable in EtherCAT networks. Also, the dataset and events can be used for further studies since it is difficult to obtain data for ICS due to its critical infrastructure and continuous real-time operation.","",""
11,"J. Halotel, V. Demyanov, A. Gardiner","Value of Geologically Derived Features in Machine Learning Facies Classification",2019,"","","","",137,"2022-07-13 10:06:51","","10.1007/s11004-019-09838-0","","",,,,,11,3.67,4,3,3,"","",""
48,"H. Aghakhani, Fabio Gritti, Francesco Mecca, Martina Lindorfer, Stefano Ortolani, D. Balzarotti, Giovanni Vigna, C. Kruegel","When Malware is Packin' Heat; Limits of Machine Learning Classifiers Based on Static Analysis Features",2020,"","","","",138,"2022-07-13 10:06:51","","10.14722/ndss.2020.24310","","",,,,,48,24.00,6,8,2,"Machine learning techniques are widely used in addition to signatures and heuristics to increase the detection rate of anti-malware software, as they automate the creation of detection models, making it possible to handle an ever-increasing number of new malware samples. In order to foil the analysis of anti-malware systems and evade detection, malware uses packing and other forms of obfuscation. However, few realize that benign applications use packing and obfuscation as well, to protect intellectual property and prevent license abuse. In this paper, we study how machine learning based on static analysis features operates on packed samples. Malware researchers have often assumed that packing would prevent machine learning techniques from building effective classifiers. However, both industry and academia have published results that show that machine-learning-based classifiers can achieve good detection rates, leading many experts to think that classifiers are simply detecting the fact that a sample is packed, as packing is more prevalent in malicious samples. We show that, different from what is commonly assumed, packers do preserve some information when packing programs that is “useful” for malware classification. However, this information does not necessarily capture the sample’s behavior. We demonstrate that the signals extracted from packed executables are not rich enough for machine-learning-based models to (1) generalize their knowledge to operate on unseen packers, and (2) be robust against adversarial examples. We also show that a naı̈ve application of machine learning techniques results in a substantial number of false positives, which, in turn, might have resulted in incorrect labeling of ground-truth data used in past work.","",""
4,"E. Glaab, Armin Rauschenberger, R. Banzi, C. Gerardi, Paula Garcia, J. Demotes","Biomarker discovery studies for patient stratification using machine learning analysis of omics data: a scoping review",2021,"","","","",139,"2022-07-13 10:06:51","","10.1136/bmjopen-2021-053674","","",,,,,4,4.00,1,6,1,"Objective To review biomarker discovery studies using omics data for patient stratification which led to clinically validated FDA-cleared tests or laboratory developed tests, in order to identify common characteristics and derive recommendations for future biomarker projects. Design Scoping review. Methods We searched PubMed, EMBASE and Web of Science to obtain a comprehensive list of articles from the biomedical literature published between January 2000 and July 2021, describing clinically validated biomarker signatures for patient stratification, derived using statistical learning approaches. All documents were screened to retain only peer-reviewed research articles, review articles or opinion articles, covering supervised and unsupervised machine learning applications for omics-based patient stratification. Two reviewers independently confirmed the eligibility. Disagreements were solved by consensus. We focused the final analysis on omics-based biomarkers which achieved the highest level of validation, that is, clinical approval of the developed molecular signature as a laboratory developed test or FDA approved tests. Results Overall, 352 articles fulfilled the eligibility criteria. The analysis of validated biomarker signatures identified multiple common methodological and practical features that may explain the successful test development and guide future biomarker projects. These include study design choices to ensure sufficient statistical power for model building and external testing, suitable combinations of non-targeted and targeted measurement technologies, the integration of prior biological knowledge, strict filtering and inclusion/exclusion criteria, and the adequacy of statistical and machine learning methods for discovery and validation. Conclusions While most clinically validated biomarker models derived from omics data have been developed for personalised oncology, first applications for non-cancer diseases show the potential of multivariate omics biomarker design for other complex disorders. Distinctive characteristics of prior success stories, such as early filtering and robust discovery approaches, continuous improvements in assay design and experimental measurement technology, and rigorous multicohort validation approaches, enable the derivation of specific recommendations for future studies.","",""
1,"Alessio Ragno, A. Baldisserotto, L. Antonini, M. Sabatino, F. Sapienza, E. Baldini, Raissa Buzzi, S. Vertuani, S. Manfredini","Machine Learning Data Augmentation as a Tool to Enhance Quantitative Composition–Activity Relationships of Complex Mixtures. A New Application to Dissect the Role of Main Chemical Components in Bioactive Essential Oils",2021,"","","","",140,"2022-07-13 10:06:51","","10.3390/molecules26206279","","",,,,,1,1.00,0,9,1,"Scientific investigation on essential oils composition and the related biological profile are continuously growing. Nevertheless, only a few studies have been performed on the relationships between chemical composition and biological data. Herein, the investigation of 61 assayed essential oils is reported focusing on their inhibition activity against Microsporum spp. including development of machine learning models with the aim of highlining the possible chemical components mainly related to the inhibitory potency. The application of machine learning and deep learning techniques for predictive and descriptive purposes have been applied successfully to many fields. Quantitative composition–activity relationships machine learning-based models were developed for the 61 essential oils tested as Microsporum spp. growth modulators. The models were built with in-house python scripts implementing data augmentation with the purpose of having a smoother flow between essential oils’ chemical compositions and biological data. High statistical coefficient values (Accuracy, Matthews correlation coefficient and F1 score) were obtained and model inspection permitted to detect possible specific roles related to some components of essential oils’ constituents. Robust machine learning models are far more useful tools to reveal data augmentation in comparison with raw data derived models. To the best of the authors knowledge this is the first report using data augmentation to highlight the role of complex mixture components, in particular a first application of these data will be for the development of ingredients in the dermo-cosmetic field investigating microbial species considering the urge for the use of natural preserving and acting antimicrobial agents.","",""
3,"Yusuke Kawamoto","Towards Logical Specification of Statistical Machine Learning",2019,"","","","",141,"2022-07-13 10:06:51","","10.1007/978-3-030-30446-1_16","","",,,,,3,1.00,3,1,3,"","",""
4,"Niko Murrell, Ryan Bradley, N. Bajaj, Julie Whitney, G. Chiu","A Method for Sensor Reduction in a Supervised Machine Learning Classification System",2019,"","","","",142,"2022-07-13 10:06:51","","10.1109/TMECH.2018.2881889","","",,,,,4,1.33,1,5,3,"Smart devices employing interconnected sensors for feedback and control are being rapidly adopted. Many useful applications for these devices are in markets that demand cost-conscious solutions. Traditional machine-learning-based control systems often rely on multiple measurements from many sensors to achieve performance targets. An alternative method is presented that leverages a time-series output produced by a single sensor. By using domain expert knowledge, the time-series output is discretized into finite intervals that correspond to the physical events occurring in the system. Statistical measures are taken across these intervals to serve as the features to the machine learning system. Additional features that decouple key physical metrics are identified, improving the performance of the system. This novel approach requires a more modest dataset and does not compromise performance. The resulting development effort is significantly more cost-effective than traditional sensor classification systems, not only due to the reduced sensor count, but also due to a significantly simplified and more robust algorithm development and testing step. Results are presented with the case study of a media-type classification system within a printing system, which was deployed to the field as a commercial product.","",""
3,"A. Derville, G. Gey, J. Baderot, S. Martínez, G. Bernard, J. Foucher","Using machine learning technology to accelerate the development of plasma etching processes",2019,"","","","",143,"2022-07-13 10:06:51","","10.1117/12.2514705","","",,,,,3,1.00,1,6,3,"The latest advances in Machine Learning (ML) produce results with unprecedented accuracy, and could signal a new era in the smart manufacturing field. We propose a framework designed to work alongside experts: learning from them and optimizing their knowledge. This framework must be considered as a tool to assist the experts in their daily work. The user creates a measurement recipe which includes an example of the feature as well as the measurements placed by the process engineer. Grouping the measurement recipes of the same object in an entity collection allows the user to train a machine learning recipe which includes a deformation model to handle variations in structure and contrast. The new images are analyzed following the machine learning pipeline which includes the detection of features, repositioning, measurement, quality evaluation and finally the results of measurement are given to the user. We discuss the pipeline and we focus on the metrics to validate the machine learning recipe, providing quantitative results for stability and robustness to variations.","",""
32,"K. Javed","A robust and reliable data-driven prognostics approach based on Extreme Learning Machine and Fuzzy Clustering",2014,"","","","",144,"2022-07-13 10:06:51","","","","",,,,,32,4.00,32,1,8,"Prognostics and Health Management (PHM) aims at extending the life cycle of engineerin gassets, while reducing exploitation and maintenance costs. For this reason,prognostics is considered as a key process with future capabilities. Indeed, accurateestimates of the Remaining Useful Life (RUL) of an equipment enable defining furtherplan of actions to increase safety, minimize downtime, ensure mission completion andefficient production.Recent advances show that data-driven approaches (mainly based on machine learningmethods) are increasingly applied for fault prognostics. They can be seen as black-boxmodels that learn system behavior directly from Condition Monitoring (CM) data, usethat knowledge to infer its current state and predict future progression of failure. However,approximating the behavior of critical machinery is a challenging task that canresult in poor prognostics. As for understanding, some issues of data-driven prognosticsmodeling are highlighted as follows. 1) How to effectively process raw monitoringdata to obtain suitable features that clearly reflect evolution of degradation? 2) Howto discriminate degradation states and define failure criteria (that can vary from caseto case)? 3) How to be sure that learned-models will be robust enough to show steadyperformance over uncertain inputs that deviate from learned experiences, and to bereliable enough to encounter unknown data (i.e., operating conditions, engineering variations,etc.)? 4) How to achieve ease of application under industrial constraints andrequirements? Such issues constitute the problems addressed in this thesis and have ledto develop a novel approach beyond conventional methods of data-driven prognostics.","",""
1,"E. Kuiper, Efthymios Constantinides, S. Vries, Robert F. Marinescu-Muster, F. Metzner","A Framework of Unsupervised Machine Learning Algorithms for User Profiling",2019,"","","","",145,"2022-07-13 10:06:51","","","","",,,,,1,0.33,0,5,3,"Organizations often have difficulties to extract knowledge from data and selecting appropriate Machine Learning algorithms in order to develop accurate Behavioural Profiles or user segments. Moreover, marketing departments often lack a fundamental understanding on data-driven segmentation methodologies. This paper aims to develop a framework outlining Unsupervised Machine Learning algorithms for the purpose of User Profiling with respect to important data properties. A systematic literature review was conducted on the most prominent Unsupervised Machine Learning algorithms and their requirements regarding the characteristics of the dataset. A framework is proposed outlining various Unsupervised Machine Learning algorithms for User Profiling. It provides two-stage clustering strategies for categorical, numerical, and mixed types of data with respect to the data size and data dimensionality. The first stage consists of an hierarchical or model-based clustering algorithm to determine the number of clusters. In the second stage, a non-hierarchical clustering algorithm is applied for cluster refinement. The framework can support researchers and practitioners to determine which Unsupervised Machine Learning algorithms are appropriate for developing robust behavioural profiles or data-driven user segments.","",""
32,"Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, Jingdong Wang","Informative Dropout for Robust Representation Learning: A Shape-bias Perspective",2020,"","","","",146,"2022-07-13 10:06:51","","","","",,,,,32,16.00,5,6,2,"Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at this https URL.","",""
25,"L. Micallef, Iiris Sundin, P. Marttinen, Muhammad Ammad-ud-din, T. Peltola, Marta Soare, G. Jacucci, Samuel Kaski","Interactive Elicitation of Knowledge on Feature Relevance Improves Predictions in Small Data Sets",2016,"","","","",147,"2022-07-13 10:06:51","","10.1145/3025171.3025181","","",,,,,25,4.17,3,8,6,"Providing accurate predictions is challenging for machine learning algorithms when the number of features is larger than the number of samples in the data. Prior knowledge can improve machine learning models by indicating relevant variables and parameter values. Yet, this prior knowledge is often tacit and only available from domain experts. We present a novel approach that uses interactive visualization to elicit the tacit prior knowledge and uses it to improve the accuracy of prediction models. The main component of our approach is a user model that models the domain expert's knowledge of the relevance of different features for a prediction task. In particular, based on the expert's earlier input, the user model guides the selection of the features on which to elicit user's knowledge next. The results of a controlled user study show that the user model significantly improves prior knowledge elicitation and prediction accuracy, when predicting the relative citation counts of scientific documents in a specific domain.","",""
16,"Xiaokai Liu, Cheng-lin Zhao, Pengbiao Wang, Yang Zhang, Tian-le Yang","Blind modulation classification algorithm based on machine learning for spatially correlated MIMO system",2017,"","","","",148,"2022-07-13 10:06:51","","10.1049/iet-com.2015.1222","","",,,,,16,3.20,3,5,5,"Spatial correlation is a decisive factor for pragmatic multiple-input multiple-output (MIMO) system, simultaneously bringing about some problems in the received signal modulation identification respect. In this study, the authors focus on blind digital modulation identification in the spatially correlated MIMO system and deliver a robust signal recognition algorithm based on extreme learning machine (ELM) and higher order statistical features for MIMO signal identification without a priori knowledge of the channel and signal parameters. The superiority of ELM lies in random selections of hidden nodes and ascertains output weights analytically, which result in lower computational complexity. Theoretically, this algorithm has a tendency to supply excellent generalisation performance at staggering learning rate. Further, the simulation results indicate that the ELM could reap a perfectly acceptable recognition performance and thus provides a solid ground structure for tackling MIMO modulation challenges in low signal-to-noise ratio.","",""
87,"Nicholas Wagner, J. Rondinelli","Theory-Guided Machine Learning in Materials Science",2016,"","","","",149,"2022-07-13 10:06:51","","10.3389/fmats.2016.00028","","",,,,,87,14.50,44,2,6,"Materials scientists are increasingly adopting the use of machine learning tools to discover hidden trends in data and make predictions. Applying concepts from data science without foreknowledge of their limitations and the unique qualities of materials data, however, could lead to errant conclusions. The differences that exist between various kinds of experimental and calculated data require careful choices of data processing and machine learning methods. Here, we outline potential pitfalls involved in using machine learning without robust protocols. We address some problems of overfitting to training data using decision trees as an example, rational descriptor selection in the field of perovskites, and preserving physical interpretability in the application of dimensionality reducing techniques such as principal component analysis. We show how proceeding without the guidance of domain knowledge can lead to both quantitatively and qualitatively incorrect predictive models.","",""
20,"D. Grana, L. Azevedo, Mingliang Liu","A comparison of deep machine learning and Monte Carlo methods for facies classification from seismic data",2020,"","","","",150,"2022-07-13 10:06:51","","10.1190/geo2019-0405.1","","",,,,,20,10.00,7,3,2,"Among the large variety of mathematical and computational methods for estimating reservoir properties such as facies and petrophysical variables from geophysical data, deep machine-learning algorithms have gained significant popularity for their ability to obtain accurate solutions for geophysical inverse problems in which the physical models are partially unknown. Solutions of classification and inversion problems are generally not unique, and uncertainty quantification studies are required to quantify the uncertainty in the model predictions and determine the precision of the results. Probabilistic methods, such as Monte Carlo approaches, provide a reliable approach for capturing the variability of the set of possible models that match the measured data. Here, we focused on the classification of facies from seismic data and benchmarked the performance of three different algorithms: recurrent neural network, Monte Carlo acceptance/rejection sampling, and Markov chain Monte Carlo. We tested and validated these approaches at the well locations by comparing classification predictions to the reference facies profile. The accuracy of the classification results is defined as the mismatch between the predictions and the log facies profile. Our study found that when the training data set of the neural network is large enough and the prior information about the transition probabilities of the facies in the Monte Carlo approach is not informative, machine-learning methods lead to more accurate solutions; however, the uncertainty of the solution might be underestimated. When some prior knowledge of the facies model is available, for example, from nearby wells, Monte Carlo methods provide solutions with similar accuracy to the neural network and allow a more robust quantification of the uncertainty, of the solution.","",""
0,"Hsiao-Chi Li, Chang-Yu Cheng, Chia Chou, Chien-Chang Hsu, Meng-Lin Chang, Y. Chiu, J. Chai","Multi-Class Brain Age Discrimination Using Machine Learning Algorithm",2019,"","","","",151,"2022-07-13 10:06:51","","10.1109/ICMLC48188.2019.8949317","","",,,,,0,0.00,0,7,3,"Resting-state functional connectivity analyses have revealed a significant effect on the inter-regional interactions in brain. The brain age prediction based on resting-state functional magnetic resonance imaging has been proved as biomarkers to characterize the typical brain development and neuropsychiatric disorders. The brain age prediction model based on functional connectivity measurements derived from resting-state functional magnetic resonance imaging has received a lots of interest in recent years due to its great success in age prediction. However, some of the recent studies rely on experienced neuroscientist experts to select appropriate connectivity features in order to build a robust model for prediction while the others just selected the features based on trial-and-error test. Besides, the subjects used in this studies omitted some subjects that can be divided into two groups with less similarity which may confused the prediction model. In this study, we proposed a multi-class age categories discrimination method with the connectivity features selected via K-means clustering with no prior knowledge provided. The experimental results show that with K-means selected features the proposed model better discriminate multi-class age categories.","",""
376,"Rui Zhao, Ruqiang Yan, Jinjiang Wang, K. Mao","Learning to Monitor Machine Health with Convolutional Bi-Directional LSTM Networks",2017,"","","","",152,"2022-07-13 10:06:51","","10.3390/s17020273","","",,,,,376,75.20,94,4,5,"In modern manufacturing systems and industries, more and more research efforts have been made in developing effective machine health monitoring systems. Among various machine health monitoring approaches, data-driven methods are gaining in popularity due to the development of advanced sensing and data analytic techniques. However, considering the noise, varying length and irregular sampling behind sensory data, this kind of sequential data cannot be fed into classification and regression models directly. Therefore, previous work focuses on feature extraction/fusion methods requiring expensive human labor and high quality expert knowledge. With the development of deep learning methods in the last few years, which redefine representation learning from raw data, a deep neural network structure named Convolutional Bi-directional Long Short-Term Memory networks (CBLSTM) has been designed here to address raw sensory data. CBLSTM firstly uses CNN to extract local features that are robust and informative from the sequential input. Then, bi-directional LSTM is introduced to encode temporal information. Long Short-Term Memory networks (LSTMs) are able to capture long-term dependencies and model sequential data, and the bi-directional structure enables the capture of past and future contexts. Stacked, fully-connected layers and the linear regression layer are built on top of bi-directional LSTMs to predict the target value. Here, a real-life tool wear test is introduced, and our proposed CBLSTM is able to predict the actual tool wear based on raw sensory data. The experimental results have shown that our model is able to outperform several state-of-the-art baseline methods.","",""
8,"Benjamin Sliwa, Cedrik Schüler, Manuel Patchou, C. Wietfeld","PARRoT: Predictive Ad-hoc Routing Fueled by Reinforcement Learning and Trajectory Knowledge",2020,"","","","",153,"2022-07-13 10:06:51","","10.1109/VTC2021-Spring51267.2021.9448959","","",,,,,8,4.00,2,4,2,"Swarms of collaborating Unmanned Aerial Vehicles (UAVs) that utilize ad-hoc networking technologies for coordinating their actions offer the potential to catalyze emerging research fields such as autonomous exploration of disaster areas, demand-driven network provisioning, and near field packet delivery in Intelligent Transportation Systems (ITSs). As these mobile robotic networks are characterized by high grades of relative mobility, existing routing protocols often fail to adopt their decision making to the implied network topology dynamics. For addressing these challenges, we present Predictive Ad-hoc Routing fueled by Reinforcement learning and Trajectory knowledge (PARRoT) as a novel machine learning-enabled routing protocol which exploits mobility control information for integrating knowledge about the future motion of the mobile agents into the routing process. The performance of the proposed routing approach is evaluated using comprehensive network simulation. In comparison to established routing protocols, PARRoT achieves a massively higher robustness and a significantly lower end-to-end latency.","",""
0,"P. Keogh","Eliciting Knowledge Bases with Defeasible Reasoning: A Comparative Analysis with Machine Learning",2015,"","","","",154,"2022-07-13 10:06:51","","","","",,,,,0,0.00,0,1,7,"Faculty Name DIT School of Computing MSc in Computing (Advanced Software Development) Eliciting Knowledge Bases with Defeasible Reasoning: A Comparative Analysis with Machine Learning by Peter KEOGH This thesis compares the ability of an implementation of Defeasible Reasoning (via Argumentation Theory) to model a construct (mental workload) with Machine Learning. In order to perform this comparison a defeasible reasoning system was designed and implemented in software. This software was used to elicit a knowledge base from an expert in an experiment which was then compared with machine learning. The central findings of this thesis were that the knowledge based approach was better at predicting an objective performance measure, time, than machine learning. However, machine learning was better equiped to identify another object measure task membership. The knowledge base of the expert had a high concurrent validity with objective performance measures and a high convergent validity with existing measures of mental workload.","",""
5,"M. Usama, Muhammad Asim, Junaid Qadir, Ala Al-Fuqaha, M. Imran","Adversarial Machine Learning Attack on Modulation Classification",2019,"","","","",155,"2022-07-13 10:06:51","","10.1109/UCET.2019.8881843","","",,,,,5,1.67,1,5,3,"Modulation classification is an important component of cognitive self-driving networks. Recently many ML-based modulation classification methods have been proposed. We have evaluated the robustness of 9 ML-based modulation classifiers against the powerful Carlini & Wagner (C-W) attack and showed that the current ML-based modulation classifiers do not provide any deterrence against adversarial ML examples. To the best of our knowledge, we are the first to report the results of the application of the C-W attack for creating adversarial examples against various ML models for modulation classification.","",""
35,"Emir Muñoz, V. Nováček, P. Vandenbussche","Facilitating prediction of adverse drug reactions by using knowledge graphs and multi‐label learning models",2019,"","","","",156,"2022-07-13 10:06:51","","10.1093/bib/bbx099","","",,,,,35,11.67,12,3,3,"Abstract Timely identification of adverse drug reactions (ADRs) is highly important in the domains of public health and pharmacology. Early discovery of potential ADRs can limit their effect on patient lives and also make drug development pipelines more robust and efficient. Reliable in silico prediction of ADRs can be helpful in this context, and thus, it has been intensely studied. Recent works achieved promising results using machine learning. The presented work focuses on machine learning methods that use drug profiles for making predictions and use features from multiple data sources. We argue that despite promising results, existing works have limitations, especially regarding flexibility in experimenting with different data sets and/or predictive models. We suggest to address these limitations by generalization of the key principles used by the state of the art. Namely, we explore effects of: (1) using knowledge graphs—machine‐readable interlinked representations of biomedical knowledge—as a convenient uniform representation of heterogeneous data; and (2) casting ADR prediction as a multi‐label ranking problem. We present a specific way of using knowledge graphs to generate different feature sets and demonstrate favourable performance of selected off‐the‐shelf multi‐label learning models in comparison with existing works. Our experiments suggest better suitability of certain multi‐label learning methods for applications where ranking is preferred. The presented approach can be easily extended to other feature sources or machine learning methods, making it flexible for experiments tuned toward specific requirements of end users. Our work also provides a clearly defined and reproducible baseline for any future related experiments.","",""
2,"D. Urda, Rafael Marcos Luque Baena, L. Franco, J. M. Jerez, N. Sánchez-Maroño","Machine learning models to search relevant genetic signatures in clinical context",2017,"","","","",157,"2022-07-13 10:06:51","","10.1109/IJCNN.2017.7966049","","",,,,,2,0.40,0,5,5,"Clinicians are interested in the estimation of robust and relevant genetic signatures from gene sequencing data. Many machine learning approaches have been proposed trying to address well-known issues of this complex task (feature or gene selection, classification or model selection, and prediction assessment). Addressing this problem often requires a deep knowledge of these methods and some of them demand high computational resources that may not be affordable. In this paper, an exhaustive study that includes different types of feature selection methods and classifiers is presented, providing clinicians an useful insight of the most suitable methods for this purpose. Predictions assessment is performed using a bootstrap cross-validation strategy as an honest validation scheme. The results of this study for six benchmark datasets show that filter or embedded methods are preferred, in general, to wrapper methods according to their better statistical significant results, in terms of accuracy, and lower demand for computational resources.","",""
2,"A. Nikitin, S. Kaski","Decision Rule Elicitation for Domain Adaptation",2021,"","","","",158,"2022-07-13 10:06:51","","10.1145/3397481.3450682","","",,,,,2,2.00,1,2,1,"Human-in-the-loop machine learning is widely used in artificial intelligence (AI) to elicit labels for data points from experts or to provide feedback on how close the predicted results are to the target. This simplifies away all the details of the decision-making process of the expert. In this work, we allow the experts to additionally produce decision rules describing their decision-making; the rules are expected to be imperfect but to give additional information. In particular, the rules can extend to new distributions, and hence enable significantly improving performance for cases where the training and testing distributions differ, such as in domain adaptation. We apply the proposed method to lifelong learning and domain adaptation problems and discuss applications in other branches of AI, such as knowledge acquisition problems in expert systems. In simulated and real-user studies, we show that decision rule elicitation improves domain adaptation of the algorithm and helps to propagate expert’s knowledge to the AI model.","",""
2,"Kar Fye Alvin Lee, W. Gan, Georgios Christopoulos","Biomarker-Informed Machine Learning Model of Cognitive Fatigue from a Heart Rate Response Perspective",2021,"","","","",159,"2022-07-13 10:06:51","","10.3390/s21113843","","",,,,,2,2.00,1,3,1,"Cognitive fatigue is a psychological state characterised by feelings of tiredness and impaired cognitive functioning arising from high cognitive demands. This paper examines the recent research progress on the assessment of cognitive fatigue and provides informed recommendations for future research. Traditionally, cognitive fatigue is introspectively assessed through self-report or objectively inferred from a decline in behavioural performance. However, more recently, researchers have attempted to explore the biological underpinnings of cognitive fatigue to understand and measure this phenomenon. In particular, there is evidence indicating that the imbalance between sympathetic and parasympathetic nervous activity appears to be a physiological correlate of cognitive fatigue. This imbalance has been indexed through various heart rate variability indices that have also been proposed as putative biomarkers of cognitive fatigue. Moreover, in contrast to traditional inferential methods, there is also a growing research interest in using data-driven approaches to assessing cognitive fatigue. The ubiquity of wearables with the capability to collect large amounts of physiological data appears to be a major facilitator in the growth of data-driven research in this area. Preliminary findings indicate that such large datasets can be used to accurately predict cognitive fatigue through various machine learning approaches. Overall, the potential of combining domain-specific knowledge gained from biomarker research with machine learning approaches should be further explored to build more robust predictive models of cognitive fatigue.","",""
12,"F. López Seguí, Ricardo Ander Egg Aguilar, Gabriel de Maeztu, A. García‐Altés, Francesc García Cuyàs, Sandra Walsh, Marta Sagarra Castro, J. Vidal-Alaball","Teleconsultations between Patients and Healthcare Professionals in Primary Care in Catalonia: The Evaluation of Text Classification Algorithms Using Supervised Machine Learning",2020,"","","","",160,"2022-07-13 10:06:51","","10.3390/ijerph17031093","","",,,,,12,6.00,2,8,2,"Background: The primary care service in Catalonia has operated an asynchronous teleconsulting service between GPs and patients since 2015 (eConsulta), which has generated some 500,000 messages. New developments in big data analysis tools, particularly those involving natural language, can be used to accurately and systematically evaluate the impact of the service. Objective: The study was intended to assess the predictive potential of eConsulta messages through different combinations of vector representation of text and machine learning algorithms and to evaluate their performance. Methodology: Twenty machine learning algorithms (based on five types of algorithms and four text representation techniques) were trained using a sample of 3559 messages (169,102 words) corresponding to 2268 teleconsultations (1.57 messages per teleconsultation) in order to predict the three variables of interest (avoiding the need for a face-to-face visit, increased demand and type of use of the teleconsultation). The performance of the various combinations was measured in terms of precision, sensitivity, F-value and the ROC curve. Results: The best-trained algorithms are generally effective, proving themselves to be more robust when approximating the two binary variables “avoiding the need of a face-to-face visit” and “increased demand” (precision = 0.98 and 0.97, respectively) rather than the variable “type of query” (precision = 0.48). Conclusion: To the best of our knowledge, this study is the first to investigate a machine learning strategy for text classification using primary care teleconsultation datasets. The study illustrates the possible capacities of text analysis using artificial intelligence. The development of a robust text classification tool could be feasible by validating it with more data, making it potentially more useful for decision support for health professionals.","",""
8,"Marine Carpuat, Cyril Goutte, George F. Foster","Linear Mixture Models for Robust Machine Translation",2014,"","","","",161,"2022-07-13 10:06:51","","10.3115/v1/W14-3363","","",,,,,8,1.00,3,3,8,"As larger and more diverse parallel texts become available, how can we leverage heterogeneous data to train robust machine translation systems that achieve good translation quality on various test domains? This challenge has been addressed so far by repurposing techniques developed for domain adaptation, such as linear mixture models which combine estimates learned on homogeneous subdomains. However, learning from large heterogeneous corpora is quite different from standard adaptation tasks with clear domain distinctions. In this paper, we show that linear mixture models can reliably improve translation quality in very heterogeneous training conditions, even if the mixtures do not use any domain knowledge and attempt to learn generic models rather than adapt them to the target domain. This surprising finding opens new perspectives for using mixture models in machine translation beyond clear cut domain adaptation tasks.","",""
5,"Tom Z. Jiahao, M. Hsieh, E. Forgoston","Knowledge-based learning of nonlinear dynamics and chaos",2020,"","","","",162,"2022-07-13 10:06:51","","10.1063/5.0065617","","",,,,,5,2.50,2,3,2,"Extracting predictive models from nonlinear systems is a central task in scientific machine learning. One key problem is the reconciliation between modern data-driven approaches and first principles. Despite rapid advances in machine learning techniques, embedding domain knowledge into datadriven models remains a challenge. In this work, we present a universal learning framework for extracting predictive models from nonlinear systems based on observations. Our framework can readily incorporate first principle knowledge because it naturally models nonlinear systems as continuous-time systems. This both improves the extracted models’ extrapolation power and reduces the amount of data needed for training. In addition, our framework has the advantages of robustness to observational noise and applicability to irregularly sampled data. We demonstrate the effectiveness of our scheme by learning predictive models for a wide variety of systems including a stiff Van der Pol oscillator, the Lorenz system, and the Kuramoto-Sivashinsky equation. For the Lorenz system, different types of domain knowledge are incorporated to demonstrate the strength of knowledge embedding in data-driven system identification.","",""
161,"A. Bhagoji, Daniel Cullina, Chawin Sitawarin, Prateek Mittal","Enhancing robustness of machine learning systems via data transformations",2017,"","","","",163,"2022-07-13 10:06:51","","10.1109/CISS.2018.8362326","","",,,,,161,32.20,40,4,5,"We propose the use of data transformations as a defense against evasion attacks on ML classifiers. We present and investigate strategies for incorporating a variety of data transformations including dimensionality reduction via Principal Component Analysis to enhance the resilience of machine learning, targeting both the classification and the training phase. We empirically evaluate and demonstrate the feasibility of linear transformations of data as a defense mechanism against evasion attacks using multiple real-world datasets. Our key findings are that the defense is (i) effective against the best known evasion attacks from the literature, resulting in a two-fold increase in the resources required by a white-box adversary with knowledge of the defense for a successful attack, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification and human activity classification.","",""
4,"A. Morera, J. Martínez de Aragón, J. Bonet, Jingjing Liang, S. de-Miguel","Performance of statistical and machine learning-based methods for predicting biogeographical patterns of fungal productivity in forest ecosystems",2020,"","","","",164,"2022-07-13 10:06:51","","10.21203/rs.3.rs-122045/v1","","",,,,,4,2.00,1,5,2,"Background The prediction of biogeographical patterns from a large number of driving factors with complex interactions, correlations and non-linear dependences require advanced analytical methods and modeling tools. This study compares different statistical and machine learning-based models for predicting fungal productivity biogeographical patterns as a case study for the thorough assessment of the performance of alternative modeling approaches to provide accurate and ecologically-consistent predictions. Methods We evaluated and compared the performance of two statistical modeling techniques, namely, generalized linear mixed models and geographically weighted regression, and four techniques based on different machine learning algorithms, namely, random forest, extreme gradient boosting, support vector machine and artificial neural network to predict fungal productivity. Model evaluation was conducted using a systematic methodology combining random, spatial and environmental blocking together with the assessment of the ecological consistency of spatially-explicit model predictions according to scientific knowledge. Results Fungal productivity predictions were sensitive to the modeling approach and the number of predictors used. Moreover, the importance assigned to different predictors varied between machine learning modeling approaches. Decision tree-based models increased prediction accuracy by more than 10% compared to other machine learning approaches, and by more than 20% compared to statistical models, and resulted in higher ecological consistence of the predicted biogeographical patterns of fungal productivity. Conclusions Decision tree-based models were the best approach for prediction both in sampling-like environments as well as in extrapolation beyond the spatial and climatic range of the modeling data. In this study, we show that proper variable selection is crucial to create robust models for extrapolation in biophysically differentiated areas. This allows for reducing the dimensions of the ecosystem space described by the predictors of the models, resulting in higher similarity between the modeling data and the environmental conditions over the whole study area. When dealing with spatial-temporal data in the analysis of biogeographical patterns, environmental blocking is postulated as a highly informative technique to be used in cross-validation to assess the prediction error over larger scales.","",""
4,"Neha Shah, D. Mohan, J. Bashingwa, O. Ummer, A. Chakraborty, A. Lefevre","Using Machine Learning to Optimize the Quality of Survey Data: Protocol for a Use Case in India",2020,"","","","",165,"2022-07-13 10:06:51","","10.2196/17619","","",,,,,4,2.00,1,6,2,"Background Data quality is vital for ensuring the accuracy, reliability, and validity of survey findings. Strategies for ensuring survey data quality have traditionally used quality assurance procedures. Data analytics is an increasingly vital part of survey quality assurance, particularly in light of the increasing use of tablets and other electronic tools, which enable rapid, if not real-time, data access. Routine data analytics are most often concerned with outlier analyses that monitor a series of data quality indicators, including response rates, missing data, and reliability of coefficients for test-retest interviews. Machine learning is emerging as a possible tool for enhancing real-time data monitoring by identifying trends in the data collection, which could compromise quality. Objective This study aimed to describe methods for the quality assessment of a household survey using both traditional methods as well as machine learning analytics. Methods In the Kilkari impact evaluation’s end-line survey amongst postpartum women (n=5095) in Madhya Pradesh, India, we plan to use both traditional and machine learning–based quality assurance procedures to improve the quality of survey data captured on maternal and child health knowledge, care-seeking, and practices. The quality assurance strategy aims to identify biases and other impediments to data quality and includes seven main components: (1) tool development, (2) enumerator recruitment and training, (3) field coordination, (4) field monitoring, (5) data analytics, (6) feedback loops for decision making, and (7) outcomes assessment. Analyses will include basic descriptive and outlier analyses using machine learning algorithms, which will involve creating features from time-stamps, “don’t know” rates, and skip rates. We will also obtain labeled data from self-filled surveys, and build models using k-folds cross-validation on a training data set using both supervised and unsupervised learning algorithms. Based on these models, results will be fed back to the field through various feedback loops. Results Data collection began in late October 2019 and will span through March 2020. We expect to submit quality assurance results by August 2020. Conclusions Machine learning is underutilized as a tool to improve survey data quality in low resource settings. Study findings are anticipated to improve the overall quality of Kilkari survey data and, in turn, enhance the robustness of the impact evaluation. More broadly, the proposed quality assurance approach has implications for data capture applications used for special surveys as well as in the routine collection of health information by health workers. International Registered Report Identifier (IRRID) DERR1-10.2196/17619","",""
9,"Melissa Mozifian, J. A. G. Higuera, D. Meger, Gregory Dudek","Learning Domain Randomization Distributions for Training Robust Locomotion Policies",2019,"","","","",166,"2022-07-13 10:06:51","","10.1109/IROS45743.2020.9341019","","",,,,,9,3.00,2,4,3,"This paper considers the problem of learning behaviors in simulation without knowledge of the precise dynamical properties of the target robot platform(s). In this context, our learning goal is to mutually maximize task efficacy on each environment considered and generalization across the widest possible range of environmental conditions. The physical parameters of the simulator are modified by a component of our technique that learns the Domain Randomization (DR) that is appropriate at each learning epoch to maximally challenge the current behavior policy, without being overly challenging, which can hinder learning progress. This so-called sweet spot distribution is a selection of simulated domains with the following properties: 1) The trained policy should be successful in environments sampled from the domain randomization distribution; and 2) The DR distribution made as wide as possible, to increase variability in the environments. These properties aim to ensure the trajectories encountered in the target system are close to those observed during training, as existing methods in machine learning are better suited for interpolation than extrapolation. We show how adapting the DR distribution while training context-conditioned policies results in improvements on jump-start and asymptotic performance when transferring a learned policy to the target environment1.","",""
40,"G. Choudhury, David F. Lynch, Gaurav Thakur, Simon Tse","Two use cases of machine learning for SDN-enabled ip/optical networks: traffic matrix prediction and optical path performance prediction [Invited]",2018,"","","","",167,"2022-07-13 10:06:51","","10.1364/JOCN.10.000D52","","",,,,,40,10.00,10,4,4,"We describe two applications ofmachine learning in the context of internet protocol (IP)/Optical networks. The first one allows agilemanagement of resources in a core IP/Optical network by using machine learning for shorttermand long-term prediction of traffic flows. It also allows joint global optimization of IP and optical layers using colorless/ directionless (CD) reconfigurable optical add-drop multiplexers (ROADMs). Multilayer coordination allows for significant cost savings, flexible new services to meet dynamic capacity needs, and improved robustness by being able to proactively adapt to new traffic patterns and network conditions. The second application is important as we migrate our networks to Open ROADM networks to allow physical routing without the need for detailed knowledge of optical parameters. We discuss a proof-of-concept study, where detailed performance data for established wavelengths in an existing ROADM network is used for machine learning to predict the optical performance of each wavelength. Both applications can be efficiently implemented by using a software-defined network controller.","",""
5,"Y. Dehbi, André Henn, G. Gröger, Viktor Stroh, L. Plümer","Robust and fast reconstruction of complex roofs with active sampling from 3D point clouds",2020,"","","","",168,"2022-07-13 10:06:51","","10.1111/tgis.12659","","",,,,,5,2.50,1,5,2,"This article proposes a novel method for the 3D reconstruction of LoD2 buildings from LiDAR data. We propose an active sampling strategy which applies a cascade of filters focusing on promising samples at an early stage, thus avoiding the pitfalls of RANSAC‐based approaches. Filters are based on prior knowledge represented by (nonparametric) density distributions. In our approach samples are pairs of surflets—3D points together with normal vectors derived from a plane approximation of their neighborhood. Surflet pairs provide parameters for model candidates such as azimuth, inclination and ridge height, as well as parameters estimating internal precision and consistency. This provides a ranking of roof model candidates and leads to a small number of promising hypotheses. Building footprints are derived in a preprocessing step using machine learning methods, in particular support vector machines.","",""
3,"Agnese Chiatti, E. Motta, E. Daga, G. Bardaro","Fit to Measure: Reasoning about Sizes for Robust Object Recognition",2020,"","","","",169,"2022-07-13 10:06:51","","","","",,,,,3,1.50,1,4,2,"Service robots can help with many of our daily tasks, especially in those cases where it is inconvenient or unsafe for us to intervene: e.g., under extreme weather conditions or when social distance needs to be maintained. However, before we can successfully delegate complex tasks to robots, we need to enhance their ability to make sense of dynamic, real world environments. In this context, the first prerequisite to improving the Visual Intelligence of a robot is building robust and reliable object recognition systems. While object recognition solutions are traditionally based on Machine Learning methods, augmenting them with knowledge based reasoners has been shown to improve their performance. In particular, based on our prior work on identifying the epistemic requirements of Visual Intelligence, we hypothesise that knowledge of the typical size of objects could significantly improve the accuracy of an object recognition system. To verify this hypothesis, in this paper we present an approach to integrating knowledge about object sizes in a ML based architecture. Our experiments in a real world robotic scenario show that this combined approach ensures a significant performance increase over state of the art Machine Learning methods.","",""
38,"S. Raschka, R. S. Olson","Python machine learning : unlock deeper insights into machine learning with this vital guide to cutting-edge predictive analytics",2015,"","","","",170,"2022-07-13 10:06:51","","","","",,,,,38,5.43,19,2,7,"Unlock deeper insights into Machine Leaning with this vital guide to cutting-edge predictive analytics About This Book * Leverage Python's most powerful open-source libraries for deep learning, data wrangling, and data visualization * Learn effective strategies and best practices to improve and optimize machine learning systems and algorithms * Ask and answer tough questions of your data with robust statistical models, built for a range of datasets Who This Book Is For If you want to find out how to use Python to start answering critical questions of your data, pick up Python Machine Learning whether you want to get started from scratch or want to extend your data science knowledge, this is an essential and unmissable resource. What You Will Learn * Explore how to use different machine learning models to ask different questions of your data * Learn how to build neural networks using Keras and Theano * Find out how to write clean and elegant Python code that will optimize the strength of your algorithms * Discover how to embed your machine learning model in a web application for increased accessibility * Predict continuous target outcomes using regression analysis * Uncover hidden patterns and structures in data with clustering * Organize data using effective pre-processing techniques * Get to grips with sentiment analysis to delve deeper into textual and social media data In Detail Machine learning and predictive analytics are transforming the way businesses and other organizations operate. Being able to understand trends and patterns in complex data is critical to success, becoming one of the key strategies for unlocking growth in a challenging contemporary marketplace. Python can help you deliver key insights into your data its unique capabilities as a language let you build sophisticated algorithms and statistical models that can reveal new perspectives and answer key questions that are vital for success. Python Machine Learning gives you access to the world of predictive analytics and demonstrates why Python is one of the world's leading data science languages. If you want to ask better questions of data, or need to improve and extend the capabilities of your machine learning systems, this practical data science book is invaluable. Covering a wide range of powerful Python libraries, including scikit-learn, Theano, and Keras, and featuring guidance and tips on everything from sentiment analysis to neural networks, you'll soon be able to answer some of the most important questions facing you and your organization. Style and approach Python Machine Learning connects the fundamental theoretical principles behind machine learning to their practical application in a way that focuses you on asking and answering the right questions. It walks you through the key elements of Python and its powerful machine learning libraries, while demonstrating how to get to grips with a range of statistical models.","",""
32,"Himabindu Lakkaraju, Nino Arsov, Osbert Bastani","Robust and Stable Black Box Explanations",2020,"","","","",171,"2022-07-13 10:06:51","","","","",,,,,32,16.00,11,3,2,"As machine learning black boxes are increasingly being deployed in real-world applications, there has been a growing interest in developing post hoc explanations that summarize the behaviors of these black boxes. However, existing algorithms for generating such explanations have been shown to lack stability and robustness to distribution shifts. We propose a novel framework for generating robust and stable explanations of black box models based on adversarial training. Our framework optimizes a minimax objective that aims to construct the highest fidelity explanation with respect to the worst-case over a set of adversarial perturbations. We instantiate this algorithm for explanations in the form of linear models and decision sets by devising the required optimization procedures. To the best of our knowledge, this work makes the first attempt at generating post hoc explanations that are robust to a general class of adversarial perturbations that are of practical interest. Experimental evaluation with real-world and synthetic datasets demonstrates that our approach substantially improves robustness of explanations without sacrificing their fidelity on the original data distribution.","",""
79,"Taesik Na, J. Ko, S. Mukhopadhyay","Cascade Adversarial Machine Learning Regularized with a Unified Embedding",2017,"","","","",172,"2022-07-13 10:06:51","","","","",,,,,79,15.80,26,3,5,"Injecting adversarial examples during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we first show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. We also propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.","",""
66,"S. Deeb, S. Tyanova, M. Hummel, M. Schmidt-Supprian, Jüergen Cox, M. Mann","Machine Learning-based Classification of Diffuse Large B-cell Lymphoma Patients by Their Protein Expression Profiles",2015,"","","","",173,"2022-07-13 10:06:51","","10.1074/mcp.M115.050245","","",,,,,66,9.43,11,6,7,"Characterization of tumors at the molecular level has improved our knowledge of cancer causation and progression. Proteomic analysis of their signaling pathways promises to enhance our understanding of cancer aberrations at the functional level, but this requires accurate and robust tools. Here, we develop a state of the art quantitative mass spectrometric pipeline to characterize formalin-fixed paraffin-embedded tissues of patients with closely related subtypes of diffuse large B-cell lymphoma. We combined a super-SILAC approach with label-free quantification (hybrid LFQ) to address situations where the protein is absent in the super-SILAC standard but present in the patient samples. Shotgun proteomic analysis on a quadrupole Orbitrap quantified almost 9,000 tumor proteins in 20 patients. The quantitative accuracy of our approach allowed the segregation of diffuse large B-cell lymphoma patients according to their cell of origin using both their global protein expression patterns and the 55-protein signature obtained previously from patient-derived cell lines (Deeb, S. J., D'Souza, R. C., Cox, J., Schmidt-Supprian, M., and Mann, M. (2012) Mol. Cell. Proteomics 11, 77–89). Expression levels of individual segregation-driving proteins as well as categories such as extracellular matrix proteins behaved consistently with known trends between the subtypes. We used machine learning (support vector machines) to extract candidate proteins with the highest segregating power. A panel of four proteins (PALD1, MME, TNFAIP8, and TBC1D4) is predicted to classify patients with low error rates. Highly ranked proteins from the support vector analysis revealed differential expression of core signaling molecules between the subtypes, elucidating aspects of their pathobiology.","",""
28,"E. Dobriban, Hamed Hassani, D. Hong, Alexander Robey","Provable tradeoffs in adversarially robust classification",2020,"","","","",174,"2022-07-13 10:06:51","","","","",,,,,28,14.00,7,4,2,"It is well known that machine learning methods can be vulnerable to adversarially-chosen perturbations of their inputs. Despite significant progress in the area, foundational open problems remain. Here we address several of these key questions. We derive exact and approximate Bayes-optimal robust classifiers for the important setting of two- and three-class Gaussian classification problems with arbitrary imbalance, for $\ell_2$ and $\ell_\infty$ adversaries. In contrast to classical Bayes-optimal classifiers, decisions here cannot be made pointwise and new theoretical approaches are needed. We develop and leverage new tools, including recent breakthroughs from probability theory on robust isoperimetry (Cianci et al, 2011, Mossel and Neeman 2015), which, to our knowledge, have not yet been used in the area. Our results reveal tradeoffs between standard and robust accuracy that grow when data is imbalanced. We also show further foundational results, including an analysis of the loss landscape, classification calibration for convex losses in certain models, and finite sample rates for the robust risk.","",""
64,"Alexander H S Harris, A. Kuo, Yingjie Weng, A. Trickey, Thomas R Bowe, N. Giori","Can Machine Learning Methods Produce Accurate and Easy-to-use Prediction Models of 30-day Complications and Mortality After Knee or Hip Arthroplasty?",2019,"","","","",175,"2022-07-13 10:06:51","","10.1097/CORR.0000000000000601","","",,,,,64,21.33,11,6,3,"Background Existing universal and procedure-specific surgical risk prediction models of death and major complications after elective total joint arthroplasty (TJA) have limitations including poor transparency, poor to modest accuracy, and insufficient validation to establish performance across diverse settings. Thus, the need remains for accurate and validated prediction models for use in preoperative management, informed consent, shared decision-making, and risk adjustment for reimbursement. Questions/purposes The purpose of this study was to use machine learning methods and large national databases to develop and validate (both internally and externally) parsimonious risk-prediction models for mortality and complications after TJA. Methods Preoperative demographic and clinical variables from all 107,792 nonemergent primary THAs and TKAs in the 2013 to 2014 American College of Surgeons-National Surgical Quality Improvement Program (ACS-NSQIP) were evaluated as predictors of 30-day death and major complications. The NSQIP database was chosen for its high-quality data on important outcomes and rich characterization of preoperative demographic and clinical predictors for demographically and geographically diverse patients. Least absolute shrinkage and selection operator (LASSO) regression, a type of machine learning that optimizes accuracy and parsimony, was used for model development. Tenfold validation was used to produce C-statistics, a measure of how well models discriminate patients who experience an outcome from those who do not. External validation, which evaluates the generalizability of the models to new data sources and patient groups, was accomplished using data from the Veterans Affairs Surgical Quality Improvement Program (VASQIP). Models previously developed from VASQIP data were also externally validated using NSQIP data to examine the generalizability of their performance with a different group of patients outside the VASQIP context. Results The models, developed using LASSO regression with diverse clinical (for example, American Society of Anesthesiologists classification, comorbidities) and demographic (for example, age, gender) inputs, had good accuracy in terms of discriminating the likelihood a patient would experience, within 30 days of arthroplasty, a renal complication (C-statistic, 0.78; 95% confidence interval [CI], 0.76-0.80), death (0.73; 95% CI, 0.70-0.76), or a cardiac complication (0.73; 95% CI, 0.71-0.75) from one who would not. By contrast, the models demonstrated poor accuracy for venous thromboembolism (C-statistic, 0.61; 95% CI, 0.60-0.62) and any complication (C-statistic, 0.64; 95% CI, 0.63-0.65). External validation of the NSQIP- derived models using VASQIP data found them to be robust in terms of predictions about mortality and cardiac complications, but not for predicting renal complications. Models previously developed with VASQIP data had poor accuracy when externally validated with NSQIP data, suggesting they should not be used outside the context of the Veterans Health Administration. Conclusions Moderately accurate predictive models of 30-day mortality and cardiac complications after elective primary TJA were developed as well as internally and externally validated. To our knowledge, these are the most accurate and rigorously validated TJA-specific prediction models currently available (http://med.stanford.edu/s-spire/Resources/clinical-tools-.html). Methods to improve these models, including the addition of nonstandard inputs such as natural language processing of preoperative clinical progress notes or radiographs, should be pursued as should the development and validation of models to predict longer term improvements in pain and function. Level of Evidence Level III, diagnostic study.","",""
133,"A. Perini, A. Susi, P. Avesani","A Machine Learning Approach to Software Requirements Prioritization",2013,"","","","",176,"2022-07-13 10:06:51","","10.1109/TSE.2012.52","","",,,,,133,14.78,44,3,9,"Deciding which, among a set of requirements, are to be considered first and in which order is a strategic process in software development. This task is commonly referred to as requirements prioritization. This paper describes a requirements prioritization method called Case-Based Ranking (CBRank), which combines project's stakeholders preferences with requirements ordering approximations computed through machine learning techniques, bringing promising advantages. First, the human effort to input preference information can be reduced, while preserving the accuracy of the final ranking estimates. Second, domain knowledge encoded as partial order relations defined over the requirement attributes can be exploited, thus supporting an adaptive elicitation process. The techniques CBRank rests on and the associated prioritization process are detailed. Empirical evaluations of properties of CBRank are performed on simulated data and compared with a state-of-the-art prioritization method, providing evidence of the method ability to support the management of the tradeoff between elicitation effort and ranking accuracy and to exploit domain knowledge. A case study on a real software project complements these experimental measurements. Finally, a positioning of CBRank with respect to state-of-the-art requirements prioritization methods is proposed, together with a discussion of benefits and limits of the method.","",""
30,"Aditya Tulsyan, C. Garvin, Cenk Ündey","Advances in industrial biopharmaceutical batch process monitoring: Machine‐learning methods for small data problems",2018,"","","","",177,"2022-07-13 10:06:51","","10.1002/bit.26605","","",,,,,30,7.50,10,3,4,"Biopharmaceutical manufacturing comprises of multiple distinct processing steps that require effective and efficient monitoring of many variables simultaneously in real‐time. The state‐of‐the‐art real‐time multivariate statistical batch process monitoring (BPM) platforms have been in use in recent years to ensure comprehensive monitoring is in place as a complementary tool for continued process verification to detect weak signals. This article addresses a longstanding, industry‐wide problem in BPM, referred to as the “Low‐N” problem, wherein a product has a limited production history. The current best industrial practice to address the Low‐N problem is to switch from a multivariate to a univariate BPM, until sufficient product history is available to build and deploy a multivariate BPM platform. Every batch run without a robust multivariate BPM platform poses risk of not detecting potential weak signals developing in the process that might have an impact on process and product performance. In this article, we propose an approach to solve the Low‐N problem by generating an arbitrarily large number of in silico batches through a combination of hardware exploitation and machine‐learning methods. To the best of authors’ knowledge, this is the first article to provide a solution to the Low‐N problem in biopharmaceutical manufacturing using machine‐learning methods. Several industrial case studies from bulk drug substance manufacturing are presented to demonstrate the efficacy of the proposed approach for BPM under various Low‐N scenarios.","",""
71,"T. Cohen, M. Freytsis, B. Ostdiek","(Machine) learning to do more with less",2017,"","","","",178,"2022-07-13 10:06:51","","10.1007/JHEP02(2018)034","","",,,,,71,14.20,24,3,5,"","",""
13,"D. Hagos, P. Engelstad, A. Yazidi, Ø. Kure","General TCP State Inference Model From Passive Measurements Using Machine Learning Techniques",2018,"","","","",179,"2022-07-13 10:06:51","","10.1109/ACCESS.2018.2833107","","",,,,,13,3.25,3,4,4,"Many applications in the Internet use the reliable end-to-end Transmission Control Protocol (TCP) as a transport protocol due to practical considerations. There are many different TCP variants widely in use, and each variant uses a specific congestion control algorithm to avoid congestion, while also attempting to share the underlying network capacity equally among the competing users. This paper shows how an intermediate node (e.g., a network operator) can identify the transmission state of the TCP client associated with a TCP flow by passively monitoring the TCP traffic. Here, we present a robust, scalable and generic machine learning-based method which may be of interest for network operators that experimentally infers Congestion Window (cwnd) and the underlying variant of loss-based TCP algorithms within a flow from passive traffic measurements collected at an intermediate node. The method can also be extended to predict other TCP transmission states of the client. We believe that our study also has a potential benefit and opportunity for researchers and scientists in the networking community from both academia and industry who want to assess the characteristics of TCP transmission states related to network congestion. We validate the robustness and scalability approach of our prediction model through a large number of controlled experiments. It turns out, surprisingly enough, that the learned prediction model performs reasonably well by leveraging knowledge from the emulated network when it is applied on a real-life scenario setting. Thus, our prediction model is general bearing similarity to the concept of transfer learning in the machine learning community. The accuracy of our experimental results both in an emulated network, realistic and combined scenario settings and across multiple TCP congestion control variants demonstrate that our model is reasonably effective and has considerable potential.","",""
15,"S. Leighton, R. Krishnadas, K. Chung, A. Blair, Susie Brown, S. Clark, K. Sowerbutts, M. Schwannauer, J. Cavanagh, A. Gumley","Predicting one-year outcome in first episode psychosis using machine learning",2018,"","","","",180,"2022-07-13 10:06:51","","10.1101/390096","","",,,,,15,3.75,2,10,4,"Lay Summary Evidence before this study Our knowledge of factors which predict outcome in first episode psychosis (FEP) is incomplete. Poor premorbid adjustment, history of developmental disorder, symptom severity at baseline and duration of untreated psychosis are the most replicated predictors of poor clinical, functional, cognitive, and biological outcomes. Yet, such group level differences are not always replicated in individuals, nor can observational results be clearly equated with causation. Advanced machine learning techniques have potential to revolutionise medicine by looking at causation and the prediction of individual patient outcome. Within psychiatry, Koutsouleris et al employed machine learning to predict 4- and 52-week functional outcome in FEP to a 75% and 73.8% test-fold balanced accuracy on repeated nested internal cross-validation. The authors suggest that before employing a machine learning model “into real-world care, replication is needed in external first episode samples”. Added value of this study We believe our study to be the first externally validated evidence, in a temporally and geographically independent cohort, for predictive modelling in FEP at an individual patient level. Our results demonstrate the ability to predict both symptom remission and functioning (in employment, education or training (EET)) at one-year. The performance of our EET model was particularly robust, with an ability to accurately predict the one-year EET outcome in more than 85% of patients. Regularised regression results in sparse models which are uniquely interpretable and identify meaningful predictors of recovery including specific individual PANSS items, and social support. This builds on existing studies of group-level differences and the elegant work of Koutsouleris et al. Implications of all the available evidence We have demonstrated the externally validated ability to accurately predict one-year symptomatic and functional status in individual patients with FEP. External validation in a plausibly related temporally and geographically distinct population assesses model transportability to an untested situation rather than simply reproducibility alone. We propose that our results represent important and exciting progress in unlocking the potential of predictive modelling in psychiatric illness. The next step prior to implementation into routine clinical practice would be to establish whether, by the accurate identification of individuals who will have poor outcomes, we can meaningful intervene to improve their prognosis. Abstract Background Early illness course correlates with long-term outcome in psychosis. Accurate prediction could allow more focused intervention. Earlier intervention corresponds to significantly better symptomatic and functional outcomes. We use routinely collected baseline demographic and clinical characteristics to predict employment, education or training (EET) status, and symptom remission in patients with first episode psychosis (FEP) at 1 year. Methods 83 FEP patients were recruited from National Health Service (NHS) Glasgow between 2011 and 2014 to a 24-month prospective cohort study with regular assessment of demographic and psychometric measures. An external independent cohort of 79 FEP patients were recruited from NHS Glasgow and Edinburgh during a 12-month study between 2006 and 2009. Elastic net regularised logistic regression models were built to predict binary EET status, period and point remission outcomes at 1 year on 83 Glasgow patients (training dataset). Models were externally validated on an independent dataset of 79 patients from Glasgow and Edinburgh (validation dataset). Only baseline predictors shared across both cohorts were made available for model training and validation. Outcomes After excluding participants with missing outcomes, models were built on the training dataset for EET status, period and point remission outcomes and externally validated on the validation dataset. Models predicted EET status, period and point remission with ROC area under curve (AUC) performances of 0.876 (95%CI: 0.864, 0.887), 0.630 (95%CI: 0.612, 0.647) and 0.652 (95%CI: 0.635, 0.670) respectively. Positive predictors of EET included baseline EET and living with spouse/children. Negative predictors included higher PANSS suspiciousness, hostility and delusions scores. Positive predictors for symptom remission included living with spouse/children, and affective symptoms on the Positive and Negative Syndrome Scale (PANSS). Negative predictors of remission included passive social withdrawal symptoms on PANSS. Interpretation Using advanced statistical machine learning techniques, we provide the first externally validated evidence for the ability to predict 1-year EET status and symptom remission in FEP patients. Funding The authors acknowledge financial support from NHS Research Scotland, the Chief Scientist Office, the Wellcome Trust, and the Scottish Mental Health Research Network.","",""
13,"Nastasiya F. Grinberg, R. King","An evaluation of machine-learning for predicting phenotype: studies in yeast, rice, and wheat",2017,"","","","",181,"2022-07-13 10:06:51","","10.1007/s10994-019-05848-5","","",,,,,13,2.60,7,2,5,"","",""
54,"M. S. Hossain Lipu, M. Hannan, A. Hussain, M. Saad, A. Ayob, M. Uddin","Extreme Learning Machine Model for State-of-Charge Estimation of Lithium-Ion Battery Using Gravitational Search Algorithm",2019,"","","","",182,"2022-07-13 10:06:51","","10.1109/TIA.2019.2902532","","",,,,,54,18.00,9,6,3,"This paper develops a state-of-charge (SOC) estimation model for a lithium-ion battery using an improved extreme learning machine (ELM) algorithm. ELM is suitable for an SOC estimation since the ELM algorithm has fast estimation speed, good generalization performance, and high accuracy. However, the performance of ELM is highly dependent on training accuracy and the number of neurons in a hidden layer. Hence, a gravitational search algorithm (GSA) is applied to improve the ELM computational intelligence by searching for the optimal value hidden layer neurons. The optimal ELM-based GSA model does not require internal battery knowledge and mathematical model for an SOC estimation. The model robustness is validated at different temperatures using different electric vehicle drive cycles. The performance of the ELM-GSA model is verified with two popular neural network methods: back-propagation neural network (BPNN) and radial basis function neural network (RBFNN). The results are evaluated using different error rates and computation costs. The results demonstrate that the ELM-based GSA model offers a higher accuracy and lower SOC error rate than those of BPNN-based GSA and RBFNN-based GSA models. Furthermore, a detailed comparative study between the proposed model and existing SOC strategies is conducted, which also demonstrates the superiority of the proposed model.","",""
9,"S. Nusser","Robust learning in safety related domains: machine learning methods for solving safety related application problems",2009,"","","","",183,"2022-07-13 10:06:51","","","","",,,,,9,0.69,9,1,13,"Today, machine learning methods are successfully deployed in a wide range of applications. A multitude of different learning algorithms has been developed in order to solve classification and regression problems. These common machine learning approaches are regarded with suspicion by domain experts in safety-related application fields because it is often infeasible to sufficiently interpret and validate the learned solutions. Especially for safety-related applications, it is imperative to guarantee that the learned solution is correct and fulfills all given requirements. The basic idea of the approaches proposed within this thesis is to solve high-dimensional application problems by an ensemble of simple submodels, each of which is allowed to only use two or three dimensions of the complete input space. The restriction of the dimensionality of the submodels allows the visualization of the learned models. Thus a visual interpretation and validation according to the existing domain knowledge becomes feasible. Due to the visualization, an unintended and possibly undesired extraand interpolation behavior can be discovered and avoided by changing the model parameters or selecting other submodels. Since the learned submodels are interpretable the correctness of the learned solution can therefore be guaranteed. The ensemble of the submodels compensates for the limited dimensionality of the individual submodels. The proposed ensemble methods are successfully applied on common benchmark problems as well as on real-world application problems with very high requirements on the functional safety of the learned solution.","",""
16,"W. MacInnes, Stephanie Santosa, William Wright","Visual Classification: Expert Knowledge Guides Machine Learning",2010,"","","","",184,"2022-07-13 10:06:51","","10.1109/MCG.2010.18","","",,,,,16,1.33,5,3,12,"Humans use intuition and experience to classify everything they perceive, but only if the distinguishing patterns are visible. Machine-learning algorithms can learn class information from data sets, but the created classes' meaning isn't always clear. A proposed mixed-initiative approach combines intuitive visualizations with machine learning to tap into the strengths of human and machine classification. The use of visualizations in an expert-guided clustering technique allows the display of complex data sets in a way that allows human input into machine clustering. Test participants successfully employed this technique to classify analytic activities using behavioral observations of a creative-analysis task. The results demonstrate how visualization of the machine-learned classification can help users create more robust and intuitive categories.","",""
13,"Suqing Zheng, Wenping Chang, Wen-ping Xu, Yong Xu, Fu Lin","e-Sweet: A Machine-Learning Based Platform for the Prediction of Sweetener and Its Relative Sweetness",2019,"","","","",185,"2022-07-13 10:06:51","","10.3389/fchem.2019.00035","","",,,,,13,4.33,3,5,3,"Artificial sweeteners (AS) can elicit the strong sweet sensation with the low or zero calorie, and are widely used to replace the nutritive sugar in the food and beverage industry. However, the safety issue of current AS is still controversial. Thus, it is imperative to develop more safe and potent AS. Due to the costly and laborious experimental-screening of AS, in-silico sweetener/sweetness prediction could provide a good avenue to identify the potential sweetener candidates before experiment. In this work, we curate the largest dataset of 530 sweeteners and 850 non-sweeteners, and collect the second largest dataset of 352 sweeteners with the relative sweetness (RS) from the literature. In light of these experimental datasets, we adopt five machine-learning methods and conformational-independent molecular fingerprints to derive the classification and regression models for the prediction of sweetener and its RS, respectively via the consensus strategy. Our best classification model achieves the 95% confidence intervals for the accuracy (0.91 ± 0.01), precision (0.90 ± 0.01), specificity (0.94 ± 0.01), sensitivity (0.86 ± 0.01), F1-score (0.88 ± 0.01), and NER (Non-error Rate: 0.90 ± 0.01) on the test set, which outperforms the model (NER = 0.85) of Rojas et al. in terms of NER, and our best regression model gives the 95% confidence intervals for the R2(test set) and ΔR2 [referring to |R2(test set)- R2(cross-validation)|] of 0.77 ± 0.01 and 0.03 ± 0.01, respectively, which is also better than the other works based on the conformation-independent 2D descriptors (e.g., 2D Dragon) according to R2(test set) and ΔR2. Our models are obtained by averaging over nineteen data-splitting schemes, and fully comply with the guidelines of Organization for Economic Cooperation and Development (OECD), which are not completely followed by the previous relevant works that are all on the basis of only one random data-splitting scheme for the cross-validation set and test set. Finally, we develop a user-friendly platform “e-Sweet” for the automatic prediction of sweetener and its corresponding RS. To our best knowledge, it is a first and free platform that can enable the experimental food scientists to exploit the current machine-learning methods to boost the discovery of more AS with the low or zero calorie content.","",""
24,"Youhao Hu, Hai Wang, Z. Cao, Jinchuan Zheng, Zhaowu Ping, Long Chen, Xiaozheng Jin","Extreme-learning-machine-based FNTSM control strategy for electronic throttle",2020,"","","","",186,"2022-07-13 10:06:51","","10.1007/s00521-019-04446-9","","",,,,,24,12.00,3,7,2,"","",""
18,"Rui Gao","Finite-Sample Guarantees for Wasserstein Distributionally Robust Optimization: Breaking the Curse of Dimensionality",2020,"","","","",187,"2022-07-13 10:06:51","","","","",,,,,18,9.00,18,1,2,"Wasserstein distributionally robust optimization (DRO) aims to find robust and generalizable solutions by hedging against data perturbations in Wasserstein distance. Despite its recent empirical success in operations research and machine learning, existing performance guarantees for generic loss functions are either overly conservative due to the curse of dimensionality, or plausible only in large sample asymptotics. In this paper, we develop a non-asymptotic framework for analyzing the out-of-sample performance for Wasserstein robust learning and the generalization bound for its related Lipschitz and gradient regularization problems. To the best of our knowledge, this gives the first finite-sample guarantee for generic Wasserstein DRO problems without suffering from the curse of dimensionality. Our results highlight the bias-variation trade-off intrinsic in the Wasserstein DRO, which automatically balances between the empirical mean of the loss and the variation of the loss, measured by the Lipschitz norm or the gradient norm of the loss. Our analysis is based on two novel methodological developments which are of independent interest: 1) a new concentration inequality characterizing the decay rate of large deviation probabilities by the variation of the loss and, 2) a localized Rademacher complexity theory based on the variation of the loss.","",""
13,"Omar A. Zatarain, Yingxu Wang","Experiments on the supervised learning algorithm for formal concept elicitation by cognitive robots",2016,"","","","",188,"2022-07-13 10:06:51","","10.1109/ICCI-CC.2016.7862015","","",,,,,13,2.17,7,2,6,"Concept elicitation is a fundamental methodology for knowledge extraction and representation in cognitive robot learning. Traditional machine learning technologies deal with object identification, cluster classification, functional regression, and behavior acquisition. This paper presents a supervised machine knowledge learning methodology for concept elicitation from sample dictionaries in natural languages. Formal concepts are autonomously generated based on collective intention of attributes and collective extension of objects elicited from informal definitions in dictionaries. A system of formal concept generation for a cognitive robot is implemented by the Algorithm of Machine Concept Elicitation (AMCE) in MATLAB. Experiments on machine learning for creating a set of twenty formal concepts reveal that the cognitive robot is able to learn synergized concepts in human knowledge in order to build its own cognitive knowledge base. The results of machine-generated concepts demonstrate that the AMCE algorithm can over perform human knowledge expressions in dictionaries in terms of relevance, accuracy, quantitativeness, and cohesiveness.","",""
10,"Tom Seymoens, F. Ongenae, An Jacobs, S. Verstichel, A. Ackaert","A Methodology to Involve Domain Experts and Machine Learning Techniques in the Design of Human-Centered Algorithms",2018,"","","","",189,"2022-07-13 10:06:51","","10.1007/978-3-030-05297-3_14","","",,,,,10,2.50,2,5,4,"","",""
12,"A. Menon, James A. Thompson-Colón, N. Washburn","Hierarchical Machine Learning Model for Mechanical Property Predictions of Polyurethane Elastomers From Small Datasets",2019,"","","","",190,"2022-07-13 10:06:51","","10.3389/fmats.2019.00087","","",,,,,12,4.00,4,3,3,"Polyurethanes are a broad class of material that finds application in coatings, foams, and solid elastomers. The urethane chemistry allows a diversity of monomers to be used, and prediction of mechanical properties, which are determined by complex interplay between monomer chemistry and chain architecture, is an unresolved challenge. Urethanes are based on aromatic or cyclic isocyanates and linear or branched polyols, and polymerization results in linear chains for bifunctional monomers or branched chains for multifunctional monomers. Strong intermolecular interactions between aromatic groups result in the formation of hard-segment domains that generate physical crosslinks between disorganized rubbery domains and anchor the material microstructure, contributing to resistance to deformation. Here, a general hierarchical machine learning (HML) model for predicting the stress-at-break, strain-at-break, and Tan δ for thermoplastic and thermoset polyurethanes is presented. The algorithm was trained on a library of 18 polymers with different diisocyanates, bifunctional or trifunctional polyols, and NCO:OH index. HML reduces data requirements through robust embedding of domain knowledge and surrogate data in a middle layer that bridges input variables (composition) and output responses (mechanical properties). In this work, the middle layer included information on overall polymer composition, predictions of chain architecture derived from Monte Carlo simulations of polymerization, information on interchain interactions from empirically derived molecular potentials and shifts in infrared (IR) spectroscopy absorbances. The HML predictions are shown to be more accurate than those from a Random Forest model directly relating composition and properties, suggesting that embedding domain knowledge provides significant advantages in predicting the properties of complex material systems based on small datasets.","",""
6,"M. Zambetti, R. Sala, D. Russo, G. Pezzotta, R. Pinto","A patent review on machine learning techniques and applications: Depicting main players, relations and technology landscapes",2018,"","","","",191,"2022-07-13 10:06:51","","","","",,,,,6,1.50,1,5,4,"The increasing availability of data, promised by the 4th industrial revolution wave, is challenging companies and organizations in diverse industry sectors to extract useful and actionable information. To this end, a vast array of data management strategies and new analytical methods is becoming available to the large audience of researchers and practitioners. Although traditional statistical approaches are still applicable for different purposes, artificial intelligence techniques, particularly machine learning algorithms, are increasingly being explored and adopted to approach data analysis. Artificial intelligence becomes a necessary ingredient for technology progress. The machine learning domain, in particular, has been extensively investigated by academics, who mainly focused on algorithms and suitable applications, and it is also permeating business reality at an unprecedented rate. Against this background, instead of eliciting knowledge from academics, the proposed research adopts a patent review and analysis approach, with the specific purpose of understanding the ongoing industrial effort on the subject, and new as well as expected trends on machine learning technologies and applications. The paper analyses technological development in various industries by defining patents trend over the years and investigating the different areas of applications according to the Cooperative Patent Classification (CPC), a patent classification system jointly developed by the European and US patent authorities. Patent applicants are also investigated in order to highlight active and competitive players in the domain, as well as collaboration between different companies. Furthermore, the paper includes a patent citation network analysis, which is useful to show critical technologies developed, and to understand applicants’ behaviours, such as influences or infringement trials. Overall, the paper provides an original and “literature-complementary” outlook on the machine learning landscape, giving an understanding on industrial R&D effort in this context, delineating trends related to technology diffusion and innovation from an industrial perspective.","",""
12,"Stephen Bonner, John Brennan, Ibad Kureshi, G. Theodoropoulos, A. McGough, B. Obara","Temporal Graph Offset Reconstruction: Towards Temporally Robust Graph Representation Learning",2018,"","","","",192,"2022-07-13 10:06:51","","10.1109/BigData.2018.8622636","","",,,,,12,3.00,2,6,4,"Graphs are a commonly used construct for representing relationships between elements in complex high dimensional datasets. Many real-world phenomenon are dynamic in nature, meaning that any graph used to represent them is inherently temporal. However, many of the machine learning models designed to capture knowledge about the structure of these graphs ignore this rich temporal information when creating representations of the graph. This results in models which do not perform well when used to make predictions about the future state of the graph – especially when the delta between time stamps is not small. In this work, we explore a novel training procedure and an associated unsupervised model which creates graph representations optimised to predict the future state of the graph. We make use of graph convo-lutional neural networks to encode the graph into a latent representation, which we then use to train our temporal offset reconstruction method, inspired by auto-encoders, to predict a later time point – multiple time steps into the future. Using our method, we demonstrate superior performance for the task of future link prediction compared with none-temporal state-of-the-art baselines. We show our approach to be capable of outperforming non-temporal baselines by 38% on a real world dataset.","",""
381,"Maximilian Nickel, Volker Tresp, H. Kriegel","Factorizing YAGO: scalable machine learning for linked data",2012,"","","","",193,"2022-07-13 10:06:51","","10.1145/2187836.2187874","","",,,,,381,38.10,127,3,10,"Vast amounts of structured information have been published in the Semantic Web's Linked Open Data (LOD) cloud and their size is still growing rapidly. Yet, access to this information via reasoning and querying is sometimes difficult, due to LOD's size, partial data inconsistencies and inherent noisiness. Machine Learning offers an alternative approach to exploiting LOD's data with the advantages that Machine Learning algorithms are typically robust to both noise and data inconsistencies and are able to efficiently utilize non-deterministic dependencies in the data. From a Machine Learning point of view, LOD is challenging due to its relational nature and its scale. Here, we present an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts. Furthermore, we show how ontological knowledge can be incorporated in the factorization to improve learning results and how computation can be distributed across multiple nodes. We demonstrate that our approach is able to factorize the YAGO~2 core ontology and globally predict statements for this large knowledge base using a single dual-core desktop computer. Furthermore, we show experimentally that our approach achieves good results in several relational learning tasks that are relevant to Linked Data. Once a factorization has been computed, our model is able to predict efficiently, and without any additional training, the likelihood of any of the 4.3 ⋅ 1014 possible triples in the YAGO~2 core ontology.","",""
0,"","Robust feature selection using Rough Set based Ant-Lion Optimizer for data classification",2022,"","","","",194,"2022-07-13 10:06:51","","10.4018/ijskd.301263","","",,,,,0,0.00,0,0,1,"The selection of an algorithm to tackle a certain problem is a vital undertaking that necessitates both time and knowledge. Non-functional needs, such as the size, quality, and nature of the data, must frequently be taken into account. To develop a generalized machine learning model for any domain, the most relevant features must be chosen because noisy and irrelevant characteristics degrade data mining performance. However, the selection of the dominating features is still dependent on the search technique. When there are a high number of input features, stochastic optimization can be applied to the search space. In this research, we investigate the Ant Lion Optimization (ALO), a nature-inspired algorithm that mimics the hunting process of ant lions and is further stimulated to identify the smallest reducts. We also investigate Rough Set based ant lion optimizer for feature selection. The actual results reveal that the antlion-based rough set reduct selects a better feature subset and classifies them more accurately.","",""
4,"R. LeMoyne, T. Mastroianni, D. Whiting, N. Tomycz","Preliminary Network Centric Therapy for Machine Learning Classification of Deep Brain Stimulation Status for the Treatment of Parkinson’s Disease with a Conformal Wearable and Wireless Inertial Sensor",2019,"","","","",195,"2022-07-13 10:06:51","","10.4236/apd.2019.84007","","",,,,,4,1.33,1,4,3,"The concept of Network Centric Therapy represents an  amalgamation of wearable and wireless inertial sensor systems and machine  learning with access to a Cloud computing environment. The advent of Network  Centric Therapy is highly relevant to the treatment of Parkinson’s disease  through deep brain stimulation. Originally wearable and wireless systems for  quantifying Parkinson’s disease involved the use a smartphone to quantify hand  tremor. Although originally novel, the smartphone has notable issues as a  wearable application for quantifying movement disorder tremor. The smartphone  has evolved in a pathway that has made the smartphone progressively more  cumbersome to mount about the dorsum of the hand. Furthermore, the smartphone  utilizes an inertial sensor package that is not certified for medical analysis,  and the trial data access a provisional Cloud computing environment through an  email account. These concerns are resolved with the recent development of a  conformal wearable and wireless inertial sensor system. This conformal wearable  and wireless system mounts to the hand with the profile of a bandage by  adhesive and accesses a secure Cloud computing environment through a segmented  wireless connectivity strategy involving a smartphone and tablet. Additionally,  the conformal wearable and wireless system is certified by the FDA of the United  States of America for ascertaining medical grade inertial sensor data. These  characteristics make the conformal wearable and wireless system uniquely suited  for the quantification of Parkinson’s disease treatment through deep brain  stimulation. Preliminary evaluation of the conformal wearable and wireless  system is demonstrated through the differentiation of deep brain stimulation  set to “On” and “Off” status. Based on the robustness of the acceleration  signal, this signal was selected to quantify hand tremor for the prescribed  deep brain stimulation settings. Machine learning classification using the  Waikato Environment for Knowledge Analysis (WEKA) was applied using the  multilayer perceptron neural network. The multilayer perceptron neural network  achieved considerable classification accuracy for distinguishing between the  deep brain stimulation system set to “On” and “Off” status through the  quantified acceleration signal data obtained by this recently developed  conformal wearable and wireless system. The research achievement  establishes a progressive pathway to the future objective of achieving deep  brain stimulation capabilities that promote closed-loop acquisition of  configuration parameters that are uniquely optimized to the individual through  extrinsic means of a highly conformal wearable and wireless inertial sensor  system and machine learning with access to Cloud computing resources.","",""
2,"Marzieh Jalal Abadi, Luca Luceri, M. Hassan, C. Chou, M. Nicoli","A Cooperative Machine Learning Approach for Pedestrian Navigation in Indoor IoT",2019,"","","","",196,"2022-07-13 10:06:51","","10.3390/s19214609","","",,,,,2,0.67,0,5,3,"This paper presents a system based on pedestrian dead reckoning (PDR) for localization of networked mobile users, which relies only on sensors embedded in the devices and device- to-device connectivity. The user trajectory is reconstructed by measuring step by step the user displacements. Though step length can be estimated rather accurately, heading evaluation is extremely problematic in indoor environments. Magnetometer is typically used, however measurements are strongly perturbed. To improve the location accuracy, this paper proposes a novel cooperative system to estimate the direction of motion based on a machine learning approach for perturbation detection and filtering, combined with a consensus algorithm for performance augmentation by cooperative data fusion at multiple devices. A first algorithm filters out perturbed magnetometer measurements based on a-priori information on the Earth’s magnetic field. A second algorithm aggregates groups of users walking in the same direction, while a third one combines the measurements of the aggregated users in a distributed way to extract a more accurate heading estimate. To the best of our knowledge, this is the first approach that combines machine learning with consensus algorithms for cooperative PDR. Compared to other methods in the literature, the method has the advantage of being infrastructure-free, fully distributed and robust to sensor failures thanks to the pre-filtering of perturbed measurements. Extensive indoor experiments show that the heading error is highly reduced by the proposed approach thus leading to noticeable enhancements in localization performance.","",""
6,"Yingxu Wang, Omar A. Zatarain, M. Valipour","Formal description of a supervised learning algorithm for concept elicitation by cognitive robots",2016,"","","","",197,"2022-07-13 10:06:51","","10.1109/ICCI-CC.2016.7862014","","",,,,,6,1.00,2,3,6,"Concept elicitation is centric for machine knowledge extraction and representation in cognitive robot learning. This paper presents a supervised methodology for machine concept elicitation from informal counterparts described in natural languages. The collective opinions of a given concept in ten selected dictionaries are quantitatively analyzed and formally represented according to the attribute-object-relation (OAR) pattern of formal concepts. The concept elicitation methodology for machine learning is aimed to deal with complex problems inherited in informal concepts of natural languages such as diversity, redundancy, ambiguity, inexplicit semantics, inconsistent attributes/objects, mixed synonyms, and fuzzy hyper-/hypo-concept relations. The system of formal concept elicitation is implemented by an algorithms in MATLAB for formal concept extraction and representation. Experiments on supervised machine learning for creating twenty primitive concepts reveal that a cognitive robot is able to learn synergized concepts in human knowledge in order to build its own cognitive knowledge base.","",""
11,"Nami Ashizawa, Naoto Yanai, Jason Paul Cruz, Shingo Okamura","Eth2Vec: Learning Contract-Wide Code Representations for Vulnerability Detection on Ethereum Smart Contracts",2021,"","","","",198,"2022-07-13 10:06:51","","10.1145/3457337.3457841","","",,,,,11,11.00,3,4,1,"Ethereum smart contracts are programs that run on the Ethereum blockchain, and many smart contract vulnerabilities have been discovered in the past decade. Many security analysis tools have been created to detect such vulnerabilities, but their performance decreases drastically when codes to be analyzed are being rewritten. In this paper, we propose Eth2Vec, a machine-learning-based static analysis tool for vulnerability detection in smart contracts. It is also robust against code rewrites, i.e., it can detect vulnerabilities even in rewritten codes. Existing machine-learning-based static analysis tools for vulnerability detection need features, which analysts create manually, as inputs. In contrast, Eth2Vec automatically learns features of vulnerable Ethereum Virtual Machine (EVM) bytecodes with tacit knowledge through a neural network for natural language processing. Therefore, Eth2Vec can detect vulnerabilities in smart contracts by comparing the code similarity between target EVM bytecodes and the EVM bytecodes it already learned. We conducted experiments with existing open databases, such as Etherscan, and our results show that Eth2Vec outperforms a recent model based on support vector machine in terms of well-known metrics, i.e., precision, recall, and F1-score.","",""
11,"S. Tripathi, David Muhr, Manuel Brunner, F. Emmert‐Streib, H. Jodlbauer, M. Dehmer","Ensuring the Robustness and Reliability of Data-Driven Knowledge Discovery Models in Production and Manufacturing",2020,"","","","",199,"2022-07-13 10:06:51","","10.3389/frai.2021.576892","","",,,,,11,5.50,2,6,2,"The Cross-Industry Standard Process for Data Mining (CRISP-DM) is a widely accepted framework in production and manufacturing. This data-driven knowledge discovery framework provides an orderly partition of the often complex data mining processes to ensure a practical implementation of data analytics and machine learning models. However, the practical application of robust industry-specific data-driven knowledge discovery models faces multiple data- and model development-related issues. These issues need to be carefully addressed by allowing a flexible, customized and industry-specific knowledge discovery framework. For this reason, extensions of CRISP-DM are needed. In this paper, we provide a detailed review of CRISP-DM and summarize extensions of this model into a novel framework we call Generalized Cross-Industry Standard Process for Data Science (GCRISP-DS). This framework is designed to allow dynamic interactions between different phases to adequately address data- and model-related issues for achieving robustness. Furthermore, it emphasizes also the need for a detailed business understanding and the interdependencies with the developed models and data quality for fulfilling higher business objectives. Overall, such a customizable GCRISP-DS framework provides an enhancement for model improvements and reusability by minimizing robustness-issues.","",""
10,"Junyu Gao, Tianzhu Zhang, Changsheng Xu","Learning to Model Relationships for Zero-Shot Video Classification",2020,"","","","",200,"2022-07-13 10:06:51","","10.1109/tpami.2020.2985708","","",,,,,10,5.00,3,3,2,"With the explosive growth of video categories, zero-shot learning (ZSL) in video classification has become a promising research direction in pattern analysis and machine learning. Based on some auxiliary information such as word embeddings and attributes, the key to a robust ZSL method is to transfer the learned knowledge from seen classes to unseen classes, which requires relationship modeling between these concepts (e.g., categories and attributes). However, most existing approaches ignore to model the explicit relationships in an end-to-end manner, resulting in low effectiveness of knowledge transfer. To tackle this problem, we reconsider the video ZSL task as a task-driven message passing process to jointly enjoy several merits including alleviated heterogeneity gap, low domain shift, and robust temporal modeling. Specifically, we propose a prototype-sample GNN (PS-GNN) consisting of a prototype branch and a sample branch to directly and adaptively model all the relationships between category-attribute, category-category, and attribute-attribute. The prototype branch aims to learn robust representations of video categories, which takes as input a set of word-embedding vectors corresponding to the concepts. The sample branch is designed to generate features of a video sample by leveraging its object semantics. With the co-adaption and cooperation between both branches, a unified and robust ZSL framework is achieved. Extensive experiments strongly evidence that PS-GNN obtains favorable performance on five popular video benchmarks consistently.","",""
