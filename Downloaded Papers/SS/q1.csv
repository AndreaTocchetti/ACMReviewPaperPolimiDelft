Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
52,"Hamon Ronan, Junklewitz Henrik, S. Ignacio","Robustness and Explainability of Artificial Intelligence",2020,"","","","",1,"2022-07-13 09:19:02","","10.2760/57493","","",,,,,52,26.00,17,3,2,"","",""
0,"Dongfang Li, Dongfang Li, Baotian Hu, Qingcai Chen, Tujie Xu, Jingcong Tao, Yunan Zhang","Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction",2021,"","","","",2,"2022-07-13 09:19:02","","10.1609/aaai.v36i10.21342","","",,,,,0,0.00,0,7,1,"Recent works have shown explainability and robustness are two crucial ingredients of trustworthy and reliable text classification. However, previous works usually address one of two aspects: i) how to extract accurate rationales for explainability while being beneficial to prediction; ii) how to make the predictive model robust to different types of adversarial attacks. Intuitively, a model that produces helpful explanations should be more robust against adversarial attacks, because we cannot trust the model that outputs explanations but changes its prediction under small perturbations. To this end, we propose a joint classification and rationale extraction model named AT-BMC. It includes two key mechanisms: mixed Adversarial Training (AT) is designed to use various perturbations in discrete and embedding space to improve the model’s robustness, and Boundary Match Constraint (BMC) helps to locate rationales more precisely with the guidance of boundary information. Performances on benchmark datasets demonstrate that the proposed AT-BMC outperforms baselines on both classification and rationale extraction by a large margin. Robustness analysis shows that the proposed AT-BMC decreases the attack success rate effectively by up to 69%. The results indicate that there are connections between robust models and better explanations.","",""
0,"Olivia M. Brown, B. Dillman","Proceedings of the Robust Artificial Intelligence System Assurance (RAISA) Workshop 2022",2022,"","","","",3,"2022-07-13 09:19:02","","","","",,,,,0,0.00,0,2,1,"The Robust Artificial Intelligence System Assurance (RAISA) workshop will focus on research, development and application of robust artificial intelligence (AI) and machine learning (ML) systems. Rather than studying robustness with respect to particular ML algorithms, our approach will be to explore robustness assurance at the system architecture level, during both development and deployment, and within the human-machine teaming context. While the research community is converging on robust solutions for individual AI models in specific scenarios, the problem of evaluating and assuring the robustness of an AI system across its entire life cycle is much more complex. Moreover, the operational context in which AI systems are deployed necessitates consideration of robustness and its relation to principles of fairness, privacy, and explainability.","",""
2,"Alexandre Dey, Marc Velay, Jean-Philippe Fauvelle, Sylvain Navers","Adversarial vs behavioural-based defensive AI with joint, continual and active learning: automated evaluation of robustness to deception, poisoning and concept drift",2020,"","","","",4,"2022-07-13 09:19:02","","","","",,,,,2,1.00,1,4,2,"Recent advancements in Artificial Intelligence (AI) have brought new capabilities to behavioural analysis (UEBA) for cyber-security consisting in the detection of hostile action based on the unusual nature of events observed on the Information this http URL our previous work (presented at C\&ESAR 2018 and FIC 2019), we have associated deep neural networks auto-encoders for anomaly detection and graph-based events correlation to address major limitations in UEBA systems. This resulted in reduced false positive and false negative rates, improved alert explainability, while maintaining real-time performances and scalability. However, we did not address the natural evolution of behaviours through time, also known as concept drift. To maintain effective detection capabilities, an anomaly-based detection system must be continually trained, which opens a door to an adversary that can conduct the so-called ""frog-boiling"" attack by progressively distilling unnoticed attack traces inside the behavioural models until the complete attack is considered normal. In this paper, we present a solution to effectively mitigate this attack by improving the detection process and efficiently leveraging human expertise. We also present preliminary work on adversarial AI conducting deception attack, which, in term, will be used to help assess and improve the defense system. These defensive and offensive AI implement joint, continual and active learning, in a step that is necessary in assessing, validating and certifying AI-based defensive solutions.","",""
0,"Hatma Suryotrisongko, Y. Musashi, A. Tsuneda, K. Sugitani","Robust Botnet DGA Detection: Blending XAI and OSINT for Cyber Threat Intelligence Sharing",2022,"","","","",5,"2022-07-13 09:19:02","","10.1109/ACCESS.2022.3162588","","",,,,,0,0.00,0,4,1,"We investigated 12 years DNS query logs of our campus network and identified phenomena of malicious botnet domain generation algorithm (DGA) traffic. DGA-based botnets are difficult to detect using cyber threat intelligence (CTI) systems based on blocklists. Artificial intelligence (AI)/machine learning (ML)-based CTI systems are required. This study (1) proposed a model to detect DGA-based traffic based on statistical features with datasets comprising 55 DGA families, (2) discussed how CTI can be expanded with computable CTI paradigm, and (3) described how to improve the explainability of the model outputs by blending explainable AI (XAI) and open-source intelligence (OSINT) for trust problems, an antidote for skepticism to the shared models and preventing automation bias. We define the XAI-OSINT blending as aggregations of OSINT for AI/ML model outcome validation. Experimental results show the effectiveness of our models (96.3% accuracy). Our random forest model provides better robustness against three state-of-the-art DGA adversarial attacks (CharBot, DeepDGA, MaskDGA) compared with character-based deep learning models (Endgame, CMU, NYU, MIT). We demonstrate the sharing mechanism and confirm that the XAI-OSINT blending improves trust for CTI sharing as evidence to validate our proposed computable CTI paradigm to assist security analysts in security operations centers using an automated, explainable OSINT approach (for second opinion). Therefore, the computable CTI reduces manual intervention in critical cybersecurity decision-making.","",""
6,"Kaveri A. Thakoor, Sharath C. Koorathota, D. Hood, P. Sajda","Robust and Interpretable Convolutional Neural Networks to Detect Glaucoma in Optical Coherence Tomography Images",2020,"","","","",6,"2022-07-13 09:19:02","","10.1109/tbme.2020.3043215","","",,,,,6,3.00,2,4,2,"Recent studies suggest that deep learning systems can now achieve performance on par with medical experts in diagnosis of disease. A prime example is in the field of ophthalmology, where convolutional neural networks (CNNs) have been used to detect retinal and ocular diseases. However, this type of artificial intelligence (AI) has yet to be adopted clinically due to questions regarding robustness of the algorithms to datasets collected at new clinical sites and a lack of explainability of AI-based predictions, especially relative to those of human expert counterparts. In this work, we develop CNN architectures that demonstrate robust detection of glaucoma in optical coherence tomography (OCT) images and test with concept activation vectors (TCAVs) to infer what image concepts CNNs use to generate predictions. Furthermore, we compare TCAV results to eye fixations of clinicians, to identify common decision-making features used by both AI and human experts. We find that employing fine-tuned transfer learning and CNN ensemble learning create end-to-end deep learning models with superior robustness compared to previously reported hybrid deep-learning/machine-learning models, and TCAV/eye-fixation comparison suggests the importance of three OCT report sub-images that are consistent with areas of interest fixated upon by OCT experts to detect glaucoma. The pipeline described here for evaluating CNN robustness and validating interpretable image concepts used by CNNs with eye movements of experts has the potential to help standardize the acceptance of new AI tools for use in the clinic.","",""
4,"Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, Bowen Zhou","Trustworthy AI: From Principles to Practices",2021,"","","","",7,"2022-07-13 09:19:02","","","","",,,,,4,4.00,1,8,1,"The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people’s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.","",""
3,"E. Ates, Burak Aksar, V. Leung, A. Coskun","Counterfactual Explanations for Multivariate Time Series",2021,"","","","",8,"2022-07-13 09:19:02","","10.1109/ICAPAI49758.2021.9462056","","",,,,,3,3.00,1,4,1,"Multivariate time series are used in many science and engineering domains, including health-care, astronomy, and high-performance computing. A recent trend is to use machine learning (ML) to process this complex data and these ML-based frameworks are starting to play a critical role for a variety of applications. However, barriers such as user distrust or difficulty of debugging need to be overcome to enable widespread adoption of such frameworks in production systems. To address this challenge, we propose a novel explainability technique, CoMTE, that provides counterfactual explanations for supervised machine learning frameworks on multivariate time series data. Using various machine learning frameworks and data sets, we compare CoMTE with several state-of-the-art explainability methods and show that we outperform existing methods in comprehensibility and robustness. We also show how CoMTE can be used to debug machine learning frameworks and gain a better understanding of the underlying multivariate time series data.","",""
2,"R. Soklaski, Justin A. Goodwin, Olivia M. Brown, Michael Yee, J. Matterer","Tools and Practices for Responsible AI Engineering",2022,"","","","",9,"2022-07-13 09:19:02","","","","",,,,,2,2.00,0,5,1,"Responsible Artificial Intelligence (AI)—the practice of developing, evaluating, and maintaining accurate AI systems that also exhibit essential properties such as robustness and explainability—represents a multifaceted challenge that often stretches standard machine learning tooling, frameworks, and testing methods beyond their limits. In this paper, we present two new software libraries—hydra-zen and the rAI-toolbox—that address critical needs for responsible AI engineering. hydra-zen dramatically simplifies the process of making complex AI applications configurable, and their behaviors reproducible. The rAI-toolbox is designed to enable methods for evaluating and enhancing the robustness of AI-models in a way that is scalable and that composes naturally with other popular ML frameworks. We describe the design principles and methodologies that make these tools effective, including the use of property-based testing to bolster the reliability of the tools themselves. Finally, we demonstrate the composability and flexibility of the tools by showing how various use cases from adversarial robustness and explainable AI can be concisely implemented with familiar APIs.","",""
1,"G. Vouros","Explainable Deep Reinforcement Learning: State of the Art and Challenges",2022,"","","","",10,"2022-07-13 09:19:02","","10.1145/3527448","","",,,,,1,1.00,1,1,1,"Interpretability, explainability and transparency are key issues to introducing Artificial Intelligence methods in many critical domains: This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability and fairness, and has important consequences towards keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. While the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article we aim to provide a review of state of the art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators - i.e., of those that take the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state of the art methods, categorizing them in classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes identifying open questions and important challenges.","",""
0,"Anas Al-Tirawi, R. Reynolds","How to Design a Trustable Cultural Algorithm Using Common Value Auctions",2021,"","","","",11,"2022-07-13 09:19:02","","10.1109/TransAI51903.2021.00022","","",,,,,0,0.00,0,2,1,"One of the major challenges facing Artificial Intelligence in the future is the design of trustworthy algorithms. In this paper four basic features of trustworthy algorithms are presented. A Cultural Algorithm based upon Common Value Auctions is presented. It is demonstrated that this framework is able to support each of these fundamental principles. The basic principles are: fairness, explainability, responsibility, and sustainability. The first three are features that are part of the Cultural Algorithm configuration used here. The fourth properties was established experimentally. It was shown that the CVA based Cultural Algorithm exhibited improved sustainability in terms of both resilience and robustness over the of a Cultural Algorithm based upon a Wisdom of the Crowds or voting approach..","",""
0,"Andrei-Bogdan Puiu, Anamaria Vizitiu, C. Nita, L. Itu, Puneet S. Sharma, D. Comaniciu","Privacy-Preserving and Explainable AI for Cardiovascular Imaging",2021,"","","","",12,"2022-07-13 09:19:02","","10.24846/v30i2y202102","","",,,,,0,0.00,0,6,1,"Medical imaging provides valuable input for managing cardiovascular disease (CVD), ranging from risk assessment to diagnosis, therapy planning and follow-up. Artificial intelligence (AI) based medical image analysis algorithms provide nowadays state-of-the-art results in CVD management, mainly due to the increase in computational power and data storage capacities. Various challenges remain to be addressed to speed-up the adoption of AI based solutions in routine CVD management. Although medical imaging and in general health data are abundant, the access and transfer of such data is difficult to realize due to ethical considerations. Hence, AI algorithms are often trained on relatively small datasets, thus limiting their robustness, and potentially leading to biased or skewed results for certain patient or pathology sub-groups. Furthermore, explainability and interpretability have become core requirements for AI algorithms, to ensure that the rationale behind output inference can be revealed. The paper focuses on recent developments related to these two challenges, discusses the clinical impact of proposed solutions, and provides conclusions for further research and development. It also presents examples related to the diagnosis of stable coronary artery disease, a whole-body circulation model for the assessment of structural heart disease, and to the diagnosis and treatment planning of aortic coarctation, a congenital heart disease.","",""
0,"T. Exarchos","DEVELOPMENT OF FEDERATED LYMPHOMA CLASSIFICATION MODELS ACROSS MULTIPLE HARMONIZED COHORTS OF PATIENTS WITH PRIMARY SJÖGREN’S SYNDROME",2021,"","","","",13,"2022-07-13 09:19:02","","10.46793/iccbi21.046e","","",,,,,0,0.00,0,1,1,"Primary Sjögren’s Syndrome (pSS) is a chronic autoimmune disease followed by exocrine gland dysfunction, where it has been long stated that 5% of pSS patients are prone to lymphoma development. In this work, we present a federated AI (artificial intelligence) strategy which enables the federated training and validation of AI algorithms for lymphoma classification across 21 European cohorts with pSS patients. Advanced AI algorithms were developed, including federated gradient boosting trees with and without dropouts, federated Multilayer Perceptron and federated Multinomial Naïve Bayes. Two large-scale case studies were conducted to demonstrate the applicability and robustness of the federated AI models, where emphasis is given on class imbalance handling and explainability analysis. The federated gradient boosting trees with dropouts achieved the best classification performance yielding more than 0.8 sensitivity and specificity along with 5 biomarkers as prominent for lymphoma development and progression.","",""
0,"M. Milling, Florian B. Pokorny, K. D. Bartl-Pokorny, Björn W. Schuller","Is Speech the New Blood? Recent Progress in AI-Based Disease Detection From Audio in a Nutshell",2022,"","","","",14,"2022-07-13 09:19:02","","10.3389/fdgth.2022.886615","","",,,,,0,0.00,0,4,1,"In recent years, advancements in the field of artificial intelligence (AI) have impacted several areas of research and application. Besides more prominent examples like self-driving cars or media consumption algorithms, AI-based systems have further started to gain more and more popularity in the health care sector, however whilst being restrained by high requirements for accuracy, robustness, and explainability. Health-oriented AI research as a sub-field of digital health investigates a plethora of human-centered modalities. In this article, we address recent advances in the so far understudied but highly promising audio domain with a particular focus on speech data and present corresponding state-of-the-art technologies. Moreover, we give an excerpt of recent studies on the automatic audio-based detection of diseases ranging from acute and chronic respiratory diseases via psychiatric disorders to developmental disorders and neurodegenerative disorders. Our selection of presented literature shows that the recent success of deep learning methods in other fields of AI also more and more translates to the field of digital health, albeit expert-designed feature extractors and classical ML methodologies are still prominently used. Limiting factors, especially for speech-based disease detection systems, are related to the amount and diversity of available data, e. g., the number of patients and healthy controls as well as the underlying distribution of age, languages, and cultures. Finally, we contextualize and outline application scenarios of speech-based disease detection systems as supportive tools for health-care professionals under ethical consideration of privacy protection and faulty prediction.","",""
5,"P. Santhanam","Quality Management of Machine Learning Systems",2020,"","","","",15,"2022-07-13 09:19:02","","10.1007/978-3-030-62144-5_1","","",,,,,5,2.50,5,1,2,"","",""
0,"J. Filipe, Ashish Ghosh, R. Prates, O. Shehory, E. Farchi, Guy Barash","Engineering Dependable and Secure Machine Learning Systems: Third International Workshop, EDSMLS 2020, New York City, NY, USA, February 7, 2020, Revised Selected Papers",2020,"","","","",16,"2022-07-13 09:19:02","","10.1007/978-3-030-62144-5","","",,,,,0,0.00,0,6,2,"","",""
11,"R. Confalonieri, Tarek R. Besold, Tillman Weyde, Kathleen A. Creel, T. Lombrozo, Shane T. Mueller, Patrick Shafto","What makes a good explanation? Cognitive dimensions of explaining intelligent machines",2019,"","","","",17,"2022-07-13 09:19:02","","","","",,,,,11,3.67,2,7,3,"Explainability is assumed to be a key factor for the adoption of Artificial Intelligence systems in a wide range of contexts (Hoffman, Mueller, & Klein, 2017; Hoffman, Mueller, Klein, & Litman, 2018; Doran, Schulz, & Besold, 2017; Lipton, 2018; Miller, 2017; Lombrozo, 2016). The use of AI components in self-driving cars, medical diagnosis, or insurance and financial services has shown that when decisions are taken or suggested by automated systems it is essential for practical, social, and increasingly legal reasons that an explanation can be provided to users, developers or regulators.1Moreover, the reasons for equipping intelligent systems with explanation capabilities are not limited to user rights and acceptance. Explainability is also needed for designers and developers to enhance system robustness and enable diagnostics to prevent bias, unfairness and discrimination, as well as to increase trust by all users in why and how decisions are made. Against that background, increased efforts are directed towards studying and provisioning explainable intelligent systems, both in industry and academia, sparked by initiatives like the DARPA Explainable Artificial Intelligence Program (DARPA, 2016). In parallel, scientific conferences and workshops dedicated to explainability are now regularly organised, such as the ‘ACM Conference on Fairness, Accountability, and Transparency (ACM FAT)’ (Friedler & Wilson, n.d.) or the ‘Workshop on Explainability in AI’ at the 2017 and 2018 editions of the International Joint Conference on Artificial Intelligence. However, one important question remains hitherto unanswered: What are the criteria for a good explanation?","",""
7,"J. McDermid, Yan Jia, Zoe Porter, I. Habli","Artificial intelligence explainability: the technical and ethical dimensions",2021,"","","","",18,"2022-07-13 09:19:02","","10.1098/rsta.2020.0363","","",,,,,7,7.00,2,4,1,"In recent years, several new technical methods have been developed to make AI-models more transparent and interpretable. These techniques are often referred to collectively as ‘AI explainability’ or ‘XAI’ methods. This paper presents an overview of XAI methods, and links them to stakeholder purposes for seeking an explanation. Because the underlying stakeholder purposes are broadly ethical in nature, we see this analysis as a contribution towards bringing together the technical and ethical dimensions of XAI. We emphasize that use of XAI methods must be linked to explanations of human decisions made during the development life cycle. Situated within that wider accountability framework, our analysis may offer a helpful starting point for designers, safety engineers, service providers and regulators who need to make practical judgements about which XAI methods to employ or to require. This article is part of the theme issue ‘Towards symbiotic autonomous systems’.","",""
60,"A. Markus, J. Kors, P. Rijnbeek","The role of explainability in creating trustworthy artificial intelligence for health care: a comprehensive survey of the terminology, design choices, and evaluation strategies",2020,"","","","",19,"2022-07-13 09:19:02","","10.1016/j.jbi.2020.103655","","",,,,,60,30.00,20,3,2,"","",""
22,"Giulia Vilone, L. Longo","Notions of explainability and evaluation approaches for explainable artificial intelligence",2021,"","","","",20,"2022-07-13 09:19:02","","10.1016/J.INFFUS.2021.05.009","","",,,,,22,22.00,11,2,1,"","",""
131,"J. Amann, A. Blasimme, E. Vayena, D. Frey, V. Madai","Explainability for artificial intelligence in healthcare: a multidisciplinary perspective",2020,"","","","",21,"2022-07-13 09:19:02","","10.1186/s12911-020-01332-6","","",,,,,131,65.50,26,5,2,"","",""
81,"M. Coeckelbergh","Artificial Intelligence, Responsibility Attribution, and a Relational Justification of Explainability",2019,"","","","",22,"2022-07-13 09:19:02","","10.1007/s11948-019-00146-8","","",,,,,81,27.00,81,1,3,"","",""
11,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor","Towards Quantification of Explainability in Explainable Artificial Intelligence Methods",2019,"","","","",23,"2022-07-13 09:19:02","","","","",,,,,11,3.67,4,3,3,"Artificial Intelligence (AI) has become an integral part of domains such as security, finance, healthcare, medicine, and criminal justice. Explaining the decisions of AI systems in human terms is a key challenge--due to the high complexity of the model, as well as the potential implications on human interests, rights, and lives . While Explainable AI is an emerging field of research, there is no consensus on the definition, quantification, and formalization of explainability. In fact, the quantification of explainability is an open challenge. In our previous work, we incorporated domain knowledge for better explainability, however, we were unable to quantify the extent of explainability. In this work, we (1) briefly analyze the definitions of explainability from the perspective of different disciplines (e.g., psychology, social science), properties of explanation, explanation methods, and human-friendly explanations; and (2) propose and formulate an approach to quantify the extent of explainability. Our experimental result suggests a reasonable and model-agnostic way to quantify explainability","",""
423,"Andreas Holzinger, G. Langs, H. Denk, K. Zatloukal, Heimo Müller","Causability and explainability of artificial intelligence in medicine",2019,"","","","",24,"2022-07-13 09:19:02","","10.1002/widm.1312","","",,,,,423,141.00,85,5,3,"Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system","",""
4,"Tingting Wu, Yunwei Dong, Zhiwei Dong, Aziz Singa, Xiong Chen, Yu Zhang","Testing Artificial Intelligence System Towards Safety and Robustness: State of the Art",2020,"","","","",25,"2022-07-13 09:19:02","","","","",,,,,4,2.00,1,6,2,"With the increasing development of machine learning, conventional embedded systems cannot meet the requirement of current academic researches and industrial applications. Artificial Intelligence System (AIS) based on machine learning has been widely used in various safety-critical systems, such as machine vision, autonomous vehicles, collision avoidance system. Different from conventional embedded systems, AIS generates and updates control strategies through learning algorithms which make the control behaviors nondeterministic and bring about the test oracle problem in AIS testing procedure. There have been various testing approaches for AIS to guarantee the safety and robustness. However, few researches explain how to conduct AIS testing with a complete workflow systematically. This paper provides a comprehensive survey of existing testing techniques to detect the erroneous behaviors of AIS, and sums up the involved key steps and testing components in terms of test coverage criterion, test data generation, testing approach and common dataset. This literature review aims at organizing a standardized workflow and leading to a practicable insight and research trend towards AIS testing.","",""
195,"A. London","Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability.",2019,"","","","",26,"2022-07-13 09:19:02","","10.1002/hast.973","","",,,,,195,65.00,195,1,3,"Although decision-making algorithms are not new to medicine, the availability of vast stores of medical data, gains in computing power, and breakthroughs in machine learning are accelerating the pace of their development, expanding the range of questions they can address, and increasing their predictive power. In many cases, however, the most powerful machine learning techniques purchase diagnostic or predictive accuracy at the expense of our ability to access ""the knowledge within the machine."" Without an explanation in terms of reasons or a rationale for particular decisions in individual cases, some commentators regard ceding medical decision-making to black box systems as contravening the profound moral responsibilities of clinicians. I argue, however, that opaque decisions are more common in medicine than critics realize. Moreover, as Aristotle noted over two millennia ago, when our knowledge of causal systems is incomplete and precarious-as it often is in medicine-the ability to explain how results are produced can be less important than the ability to produce such results and empirically verify their accuracy.","",""
47,"L. Faes, B. Geerts, Xiaoxuan Liu, L. Morgan, P. Watkinson, P. McCulloch","DECIDE-AI: new reporting guidelines to bridge the development-to-implementation gap in clinical artificial intelligence.",2021,"","","","",27,"2022-07-13 09:19:02","","10.1038/s41591-021-01229-5","","",,,,,47,47.00,8,6,1,"","",""
39,"R. Confalonieri, Ludovik Çoba, Benedikt Wagner, Tarek R. Besold","A historical perspective of explainable Artificial Intelligence",2020,"","","","",28,"2022-07-13 09:19:02","","10.1002/widm.1391","","",,,,,39,19.50,10,4,2,"Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision‐making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge‐based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural‐symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human‐understandable explainable systems.","",""
29,"P. Angelov, E. Soares, Richard Jiang, Nicholas I. Arnold, Peter M. Atkinson","Explainable artificial intelligence: an analytical review",2021,"","","","",29,"2022-07-13 09:19:02","","10.1002/widm.1424","","",,,,,29,29.00,6,5,1,"This paper provides a brief analytical review of the current state‐of‐the‐art in relation to the explainability of artificial intelligence in the context of recent advances in machine learning and deep learning. The paper starts with a brief historical introduction and a taxonomy, and formulates the main challenges in terms of explainability building on the recently formulated National Institute of Standards four principles of explainability. Recently published methods related to the topic are then critically reviewed and analyzed. Finally, future directions for research are suggested.","",""
16,"J. Korteling, G. V. D. Boer-Visschedijk, R. Blankendaal, R. Boonekamp, A. Eikelboom","Human- versus Artificial Intelligence",2021,"","","","",30,"2022-07-13 09:19:02","","10.3389/frai.2021.622364","","",,,,,16,16.00,3,5,1,"AI is one of the most debated subjects of today and there seems little common understanding concerning the differences and similarities of human intelligence and artificial intelligence. Discussions on many relevant topics, such as trustworthiness, explainability, and ethics are characterized by implicit anthropocentric and anthropomorphistic conceptions and, for instance, the pursuit of human-like intelligence as the golden standard for Artificial Intelligence. In order to provide more agreement and to substantiate possible future research objectives, this paper presents three notions on the similarities and differences between human- and artificial intelligence: 1) the fundamental constraints of human (and artificial) intelligence, 2) human intelligence as one of many possible forms of general intelligence, and 3) the high potential impact of multiple (integrated) forms of narrow-hybrid AI applications. For the time being, AI systems will have fundamentally different cognitive qualities and abilities than biological systems. For this reason, a most prominent issue is how we can use (and “collaborate” with) these systems as effectively as possible? For what tasks and under what conditions, decisions are safe to leave to AI and when is human judgment required? How can we capitalize on the specific strengths of human- and artificial intelligence? How to deploy AI systems effectively to complement and compensate for the inherent constraints of human cognition (and vice versa)? Should we pursue the development of AI “partners” with human (-level) intelligence or should we focus more at supplementing human limitations? In order to answer these questions, humans working with AI systems in the workplace or in policy making have to develop an adequate mental model of the underlying ‘psychological’ mechanisms of AI. So, in order to obtain well-functioning human-AI systems, Intelligence Awareness in humans should be addressed more vigorously. For this purpose a first framework for educational content is proposed.","",""
15,"Alexandros Vassiliades, Nick Bassiliades, T. Patkos","Argumentation and explainable artificial intelligence: a survey",2021,"","","","",31,"2022-07-13 09:19:02","","10.1017/S0269888921000011","","",,,,,15,15.00,5,3,1,"Abstract Argumentation and eXplainable Artificial Intelligence (XAI) are closely related, as in the recent years, Argumentation has been used for providing Explainability to AI. Argumentation can show step by step how an AI System reaches a decision; it can provide reasoning over uncertainty and can find solutions when conflicting information is faced. In this survey, we elaborate over the topics of Argumentation and XAI combined, by reviewing all the important methods and studies, as well as implementations that use Argumentation to provide Explainability in AI. More specifically, we show how Argumentation can enable Explainability for solving various types of problems in decision-making, justification of an opinion, and dialogues. Subsequently, we elaborate on how Argumentation can help in constructing explainable systems in various applications domains, such as in Medical Informatics, Law, the Semantic Web, Security, Robotics, and some general purpose systems. Finally, we present approaches that combine Machine Learning and Argumentation Theory, toward more interpretable predictive models.","",""
19,"Ruhhee Tabbussum, A. Q. Dar","Performance evaluation of artificial intelligence paradigms—artificial neural networks, fuzzy logic, and adaptive neuro-fuzzy inference system for flood prediction",2021,"","","","",32,"2022-07-13 09:19:02","","10.1007/s11356-021-12410-1","","",,,,,19,19.00,10,2,1,"","",""
14,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor, Mohiuddin Ahmed","Explainable Artificial Intelligence Approaches: A Survey",2021,"","","","",33,"2022-07-13 09:19:02","","","","",,,,,14,14.00,4,4,1,"The lack of explainability of a decision from an Artificial Intelligence (AI) based “black box” system/model, despite its superiority in many real-world applications, is a key stumbling block for adopting AI in many high stakes applications of different domain or industry. While many popular Explainable Artificial Intelligence (XAI) methods or approaches are available to facilitate a human-friendly explanation of the decision, each has its own merits and demerits, with a plethora of open challenges. We demonstrate popular XAI methods with a mutual case study/task (i.e., credit default prediction), analyze for competitive advantages from multiple perspectives (e.g., local, global), provide meaningful insight on quantifying explainability, and recommend paths towards responsible or human-centered AI using XAI as a medium. Practitioners can use this work as a catalog to understand, compare, and correlate competitive advantages of popular XAI methods. In addition, this survey elicits future research directions towards responsible or human-centric AI systems, which is crucial to adopt AI in high stakes applications.","",""
11,"Taehyun Ha, Sangwon Lee, Sangyeon Kim","Designing Explainability of an Artificial Intelligence System",2018,"","","","",34,"2022-07-13 09:19:02","","10.1145/3183654.3183683","","",,,,,11,2.75,4,3,4,"Explainability and accuracy of the machine learning algorithms usually laid on a trade-off relationship. Several algorithms such as deep-learning artificial neural networks have high accuracy but low explainability. Since there were only limited ways to access the learning and prediction processes in algorithms, researchers and users were not able to understand how the results were given to them. However, a recent project, explainable artificial intelligence (XAI) by DARPA, showed that AI systems can be highly explainable but also accurate. Several technical reports of XAI suggested ways of extracting explainable features and their positive effects on users; the results showed that explainability of AI was helpful to make users understand and trust the system. However, only a few studies have addressed why the explainability can bring positive effects to users. We suggest theoretical reasons from the attribution theory and anthropomorphism studies. Trough a review, we develop three hypotheses: (1) causal attribution is a human nature and thus a system which provides casual explanation on their process will affect users to attribute the result of system; (2) Based on the attribution results, users will perceive the system as human-like and which will be a motivation of anthropomorphism; (3) The system will be perceived by the users through the anthropomorphism. We provide a research framework for designing causal explainability of an AI system and discuss the expected results of the research.","",""
195,"Jessica Fjeld, Nele Achten, Hannah Hilligoss, Ádám Nagy, Madhulika Srikumar","Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI",2020,"","","","",35,"2022-07-13 09:19:02","","10.2139/ssrn.3518482","","",,,,,195,97.50,39,5,2,"The rapid spread of artificial intelligence (AI) systems has precipitated a rise in ethical and human rights-based frameworks intended to guide the development and use of these technologies. Despite the proliferation of these ""AI principles,"" there has been little scholarly focus on understanding these efforts either individually or as contextualized within an expanding universe of principles with discernible trends.    To that end, this white paper and its associated data visualization compare the contents of thirty-six prominent AI principles documents side-by-side. This effort uncovered a growing consensus around eight key thematic trends: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. Underlying this “normative core,” our analysis examined the forty-seven individual principles that make up the themes, detailing notable similarities and differences in interpretation found across the documents. In sharing these observations, it is our hope that policymakers, advocates, scholars, and others working to maximize the benefits and minimize the harms of AI will be better positioned to build on existing efforts and to push the fractured, global conversation on the future of AI toward consensus.","",""
85,"Giulia Vilone, L. Longo","Explainable Artificial Intelligence: a Systematic Review",2020,"","","","",36,"2022-07-13 09:19:02","","","","",,,,,85,42.50,43,2,2,"Explainable Artificial Intelligence (XAI) has experienced a significant growth over the last few years. This is due to the widespread application of machine learning, particularly deep learning, that has led to the development of highly accurate models but lack explainability and interpretability. A plethora of methods to tackle this problem have been proposed, developed and tested. This systematic review contributes to the body of knowledge by clustering these methods with a hierarchical classification system with four main clusters: review articles, theories and notions, methods and their evaluation. It also summarises the state-of-the-art in XAI and recommends future research directions.","",""
90,"R. Shafin, Lingjia Liu, V. Chandrasekhar, Hao Chen, J. Reed, Jianzhong Zhang","Artificial Intelligence-Enabled Cellular Networks: A Critical Path to Beyond-5G and 6G",2019,"","","","",37,"2022-07-13 09:19:02","","10.1109/MWC.001.1900323","","",,,,,90,30.00,15,6,3,"Mobile network operators (MNOs) are in the process of overlaying their conventional macro cellular networks with shorter range cells such as outdoor pico cells. The resultant increase in network complexity creates substantial overhead in terms of operating expenses, time, and labor for their planning and management. Artificial intelligence (AI) offers the potential for MNOs to operate their networks in a more organic and cost-efficient manner. We argue that deploying AI in fifth generation (5G) and beyond will require surmounting significant technical barriers in terms of robustness, performance, and complexity. We outline future research directions, identify top five challenges, and present a possible roadmap to realize the vision of AI-enabled cellular networks for Beyond- 5G and sixth generation (6G) networks.","",""
42,"Weisi Guo","Explainable Artificial Intelligence for 6G: Improving Trust between Human and Machine",2019,"","","","",38,"2022-07-13 09:19:02","","10.1109/MCOM.001.2000050","","",,,,,42,14.00,42,1,3,"As 5G mobile networks are bringing about global societal benefits, the design phase for 6G has started. Evolved 5G and 6G will need sophisticated AI to automate information delivery simultaneously for mass autonomy, human machine interfacing, and targeted healthcare. Trust will become increasingly critical for 6G as it manages a wide range of mission-critical services. As we migrate from traditional mathematical model-dependent optimization to data-dependent deep learning, the insight and trust we have in our optimization modules decrease. This loss of model explainability means we are vulnerable to malicious data, poor neural network design, and the loss of trust from stakeholders and the general public -- all with a range of legal implications. In this review, we outline the core methods of explainable artificial intelligence (XAI) in a wireless network setting, including public and legal motivations, definitions of explainability, performance vs. explainability trade-offs, and XAI algorithms. Our review is grounded in case studies for both wireless PHY and MAC layer optimization and provide the community with an important research area to embark upon.","",""
34,"T. H. Aldhyani, M. Al-Yaari, Hasan Alkahtani, Mashael S. Maashi","Water Quality Prediction Using Artificial Intelligence Algorithms",2020,"","","","",39,"2022-07-13 09:19:02","","10.1155/2020/6659314","","",,,,,34,17.00,9,4,2,"During the last years, water quality has been threatened by various pollutants. Therefore, modeling and predicting water quality have become very important in controlling water pollution. In this work, advanced artificial intelligence (AI) algorithms are developed to predict water quality index (WQI) and water quality classification (WQC). For the WQI prediction, artificial neural network models, namely nonlinear autoregressive neural network (NARNET) and long short-term memory (LSTM) deep learning algorithm, have been developed. In addition, three machine learning algorithms, namely, support vector machine (SVM), K-nearest neighbor (K-NN), and Naive Bayes, have been used for the WQC forecasting. The used dataset has 7 significant parameters, and the developed models were evaluated based on some statistical parameters. The results revealed that the proposed models can accurately predict WQI and classify the water quality according to superior robustness. Prediction results demonstrated that the NARNET model performed slightly better than the LSTM for the prediction of the WQI values and the SVM algorithm has achieved the highest accuracy (97.01%) for the WQC prediction. Furthermore, the NARNET and LSTM models have achieved similar accuracy for the testing phase with a slight difference in the regression coefficient (RNARNET = 96.17% and RLSTM = 94.21%). This kind of promising research can contribute significantly to water management.","",""
42,"Christian Meske, Enrico Bunde, Johannes Schneider, Martin Gersch","Explainable Artificial Intelligence: Objectives, Stakeholders, and Future Research Opportunities",2020,"","","","",40,"2022-07-13 09:19:02","","10.1080/10580530.2020.1849465","","",,,,,42,21.00,11,4,2,"ABSTRACT Artificial Intelligence (AI) has diffused into many areas of our private and professional life. In this research note, we describe exemplary risks of black-box AI, the consequent need for explainability, and previous research on Explainable AI (XAI) in information systems research. Moreover, we discuss the origin of the term XAI, generalized XAI objectives, and stakeholder groups, as well as quality criteria of personalized explanations. We conclude with an outlook to future research on XAI.","",""
4,"José Daniel López-Cabrera, R. Orozco-Morales, Jorge Armando Portal-Díaz, Orlando Lovelle-Enríquez, M. Pérez-Díaz","Current limitations to identify covid-19 using artificial intelligence with chest x-ray imaging (part ii). The shortcut learning problem",2021,"","","","",41,"2022-07-13 09:19:02","","10.1007/s12553-021-00609-8","","",,,,,4,4.00,1,5,1,"","",""
5,"N. Amoroso, Domenico Pomarico, A. Fanizzi, V. Didonna, F. Giotta, D. La Forgia, A. Latorre, A. Monaco, Ester Pantaleo, N. Petruzzellis, P. Tamborra, A. Zito, V. Lorusso, R. Bellotti, R. Massafra","A Roadmap towards Breast Cancer Therapies Supported by Explainable Artificial Intelligence",2021,"","","","",42,"2022-07-13 09:19:02","","10.3390/APP11114881","","",,,,,5,5.00,1,15,1,"In recent years personalized medicine reached an increasing importance, especially in the design of oncological therapies. In particular, the development of patients’ profiling strategies suggests the possibility of promising rewards. In this work, we present an explainable artificial intelligence (XAI) framework based on an adaptive dimensional reduction which (i) outlines the most important clinical features for oncological patients’ profiling and (ii), based on these features, determines the profile, i.e., the cluster a patient belongs to. For these purposes, we collected a cohort of 267 breast cancer patients. The adopted dimensional reduction method determines the relevant subspace where distances among patients are used by a hierarchical clustering procedure to identify the corresponding optimal categories. Our results demonstrate how the molecular subtype is the most important feature for clustering. Then, we assessed the robustness of current therapies and guidelines; our findings show a striking correspondence between available patients’ profiles determined in an unsupervised way and either molecular subtypes or therapies chosen according to guidelines, which guarantees the interpretability characterizing explainable approaches to machine learning techniques. Accordingly, our work suggests the possibility to design data-driven therapies to emphasize the differences observed among the patients.","",""
1,"L. Machowski, T. Marwala","Nano Version Control and the Repo as the Next Data Structure in Computer Science and Artificial Intelligence",2021,"","","","",43,"2022-07-13 09:19:02","","10.1109/EE-RDS53766.2021.9708575","","",,,,,1,1.00,1,2,1,"A new data structure called the Nano Version Control (NanoVC) Repo emerges from its origins in the fundamental data structures of computer science as well as from Git. It is acknowledged as a first-class data structure with the added benefit that the software industry already knows how to reason with it because of their experience with using it to version control software. The NanoVC Repo shines a light on the value of nano-scale modelling and in-memory representation of history. An initial implementation shows promising results where it out-performs Git implementations by 2–3 orders of magnitude. A parallel for fairness, transparency and explainability is made between Git (as used for versioning software algorithms) and the NanoVC Repo, which can be used for data structures. Applications of the NanoVC Repo are in computer science, storage and databases, data modelling, distributed and cloud computing, data quality, event streaming, artificial intelligence, and agent-based simulation. The hope is that this data structure is the next major step in computer science and artificial intelligence applications.","",""
0,"Mir Riyanul Islam, Mobyen Uddin Ahmed, S. Begum","Local and Global Interpretability Using Mutual Information in Explainable Artificial Intelligence",2021,"","","","",44,"2022-07-13 09:19:02","","10.1109/ISCMI53840.2021.9654898","","",,,,,0,0.00,0,3,1,"Numerous studies have exploited the potential of Artificial Intelligence (AI) and Machine Learning (ML) models to develop intelligent systems in diverse domains for complex tasks, such as analysing data, extracting features, prediction, recommendation etc. However, presently these systems embrace acceptability issues from the end-users. The models deployed at the back of the systems mostly analyse the correlations or dependencies between the input and output to uncover the important characteristics of the input features, but they lack explainability and interpretability that causing the acceptability issues of intelligent systems and raising the research domain of eXplainable Artificial Intelligence (XAI). In this study, to overcome these shortcomings, a hybrid XAI approach is developed to explain an AI/ML model’s inference mechanism as well as the final outcome. The overall approach comprises of 1) a convolutional encoder that extracts deep features from the data and computes their relevancy with features extracted using domain knowledge, 2) a model for classifying data points using the features from autoencoder, and 3) a process of explaining the model’s working procedure and decisions using mutual information to provide global and local interpretability. To demonstrate and validate the proposed approach, experimentation was performed using an electroencephalography dataset from road safety to classify drivers’ in-vehicle mental workload. The outcome of the experiment was found to be promising that produced a Support Vector Machine classifier for mental workload with approximately 89% performance accuracy. Moreover, the proposed approach can also provide an explanation for the classifier model’s behaviour and decisions with the combined illustration of Shapely values and mutual information.","",""
20,"L. McCoy, Sujay Nagaraj, F. Morgado, V. Harish, Sunit Das, L. Celi","What do medical students actually need to know about artificial intelligence?",2020,"","","","",45,"2022-07-13 09:19:02","","10.1038/s41746-020-0294-7","","",,,,,20,10.00,3,6,2,"","",""
0,"Jie Wang, Xiangyuan Zheng, Qingdong He","Artificial Intelligence Applied to Extreme Value Prediction of Non-Gaussian Processes with Bandwidth Effect and Non-monotonicity",2021,"","","","",46,"2022-07-13 09:19:02","","10.1109/ICAICA52286.2021.9498204","","",,,,,0,0.00,0,3,1,"Extreme value prediction of a short-term non-Gaussian random process like ocean waves has been a tough issue for decades. In the 1990’s Winterstein proposed a cubic Hermite transformation using skewness and kurtosis, which has been widely applied in many areas for its accuracy and robustness. However, this approach is valid for monotonic transformation and narrow-banded processes. When the bandwidth of a random process is wide, no reasonable methods are available for acquiring the extreme value. This paper therefore applies the artificial neural network and genetic algorithm to do the extreme value prediction, without seeking rigorous mathematical derivations. Not only skewness and kurtosis are used, the spectral moments up to 4th-order reflecting bandwidth effects are also adopted. The results of many random case studies show that the artificial intelligence method is more accurate than the Hermite method in most of situations, especially for non-monotonic transformations. Besides, the artificial intelligence method has a wider application range.","",""
26,"F. Emmert‐Streib, O. Yli-Harja, M. Dehmer","Explainable artificial intelligence and machine learning: A reality rooted perspective",2020,"","","","",47,"2022-07-13 09:19:02","","10.1002/widm.1368","","",,,,,26,13.00,9,3,2,"As a consequence of technological progress, nowadays, one is used to the availability of big data generated in nearly all fields of science. However, the analysis of such data possesses vast challenges. One of these challenges relates to the explainability of methods from artificial intelligence (AI) or machine learning. Currently, many of such methods are nontransparent with respect to their working mechanism and for this reason are called black box models, most notably deep learning methods. However, it has been realized that this constitutes severe problems for a number of fields including the health sciences and criminal justice and arguments have been brought forward in favor of an explainable AI (XAI). In this paper, we do not assume the usual perspective presenting XAI as it should be, but rather provide a discussion what XAI can be. The difference is that we do not present wishful thinking but reality grounded properties in relation to a scientific theory beyond physics.","",""
0,"R. Ochoa-Montiel, G. Olague, Juan Humberto Sossa Azuela","Towards explainable artificial intelligence for the leukemia subtype recognition",2021,"","","","",48,"2022-07-13 09:19:02","","10.1109/LA-CCI48322.2021.9769826","","",,,,,0,0.00,0,3,1,"In this work, we provide a solution to the leukemia subtype recognition problem. Most approaches used for solving a variety of pattern recognition problems have a drawback: in general, they lack explainability. In this paper, we provide a solution for facing this situation. We describe a model whose stages allow deriving knowledge for solving the leukemia subtype recognition problem, are intelligible for the user. Results show that multiclass recognition is achieved from the solutions obtained by the model through multiple runs.","",""
21,"Chuan Zhang, Yeong-Luh Ueng, Christoph Studer, A. Burg","Artificial Intelligence for 5G and Beyond 5G: Implementations, Algorithms, and Optimizations",2020,"","","","",49,"2022-07-13 09:19:02","","10.1109/JETCAS.2020.3000103","","",,,,,21,10.50,5,4,2,"The communication industry is rapidly advancing towards 5G and beyond 5G (B5G) wireless technologies in order to fulfill the ever-growing needs for higher data rates and improved quality-of-service (QoS). Emerging applications require wireless connectivity with tremendously increased data rates, substantially reduced latency, and growing support for a large number of devices. These requirements pose new challenges that can no longer be efficiently addressed by conventional approaches. Artificial intelligence (AI) is considered as one of the most promising solutions to improve the performance and robustness of 5G and B5G systems, fueled by the massive amount of data generated in 5G and B5G networks and the availability of powerful data processing fabrics. As a consequence, a plethora of research on AI-based communication technologies has emerged recently, promising higher data rates and improved QoS with affordable implementation overhead. In this overview paper, we summarize the state-of-the-art of AI-based 5G and B5G techniques on the algorithm, implementation, and optimization levels. We shed light on the advantages and limitations of AI-based solutions, and we provide a summary of emerging techniques and open research problems.","",""
19,"M. Kuzlu, Umit Cali, Vinayak Sharma, Özgür Güler","Gaining Insight Into Solar Photovoltaic Power Generation Forecasting Utilizing Explainable Artificial Intelligence Tools",2020,"","","","",50,"2022-07-13 09:19:02","","10.1109/ACCESS.2020.3031477","","",,,,,19,9.50,5,4,2,"Over the last two decades, Artificial Intelligence (AI) approaches have been applied to various applications of the smart grid, such as demand response, predictive maintenance, and load forecasting. However, AI is still considered to be a “black-box” due to its lack of explainability and transparency, especially for something like solar photovoltaic (PV) forecasts that involves many parameters. Explainable Artificial Intelligence (XAI) has become an emerging research field in the smart grid domain since it addresses this gap and helps understand why the AI system made a forecast decision. This article presents several use cases of solar PV energy forecasting using XAI tools, such as LIME, SHAP, and ELI5, which can contribute to adopting XAI tools for smart grid applications. Understanding the inner workings of a prediction model based on AI can give insights into the application field. Such insight can provide improvements to the solar PV forecasting models and point out relevant parameters.","",""
17,"Colin G. Walsh, B. Chaudhry, P. Dua, K. Goodman, B. Kaplan, Ramakanth Kavuluru, A. Solomonides, V. Subbian","Stigma, biomarkers, and algorithmic bias: recommendations for precision behavioral health with artificial intelligence",2020,"","","","",51,"2022-07-13 09:19:02","","10.1093/jamiaopen/ooz054","","",,,,,17,8.50,2,8,2,"Abstract Effective implementation of artificial intelligence in behavioral healthcare delivery depends on overcoming challenges that are pronounced in this domain. Self and social stigma contribute to under-reported symptoms, and under-coding worsens ascertainment. Health disparities contribute to algorithmic bias. Lack of reliable biological and clinical markers hinders model development, and model explainability challenges impede trust among users. In this perspective, we describe these challenges and discuss design and implementation recommendations to overcome them in intelligent systems for behavioral and mental health.","",""
10,"D. Thakker, B. Mishra, A. Abdullatif, Suvodeep Mazumdar, Sydney Simpson","Explainable Artificial Intelligence for Developing Smart Cities Solutions",2020,"","","","",52,"2022-07-13 09:19:02","","10.3390/smartcities3040065","","",,,,,10,5.00,2,5,2,"Traditional Artificial Intelligence (AI) technologies used in developing smart cities solutions, Machine Learning (ML) and recently Deep Learning (DL), rely more on utilising best representative training datasets and features engineering and less on the available domain expertise. We argue that such an approach to solution development makes the outcome of solutions less explainable, i.e., it is often not possible to explain the results of the model. There is a growing concern among policymakers in cities with this lack of explainability of AI solutions, and this is considered a major hindrance in the wider acceptability and trust in such AI-based solutions. In this work, we survey the concept of ‘explainable deep learning’ as a subset of the ‘explainable AI’ problem and propose a new solution using Semantic Web technologies, demonstrated with a smart cities flood monitoring application in the context of a European Commission-funded project. Monitoring of gullies and drainage in crucial geographical areas susceptible to flooding issues is an important aspect of any flood monitoring solution. Typical solutions for this problem involve the use of cameras to capture images showing the affected areas in real-time with different objects such as leaves, plastic bottles etc., and building a DL-based classifier to detect such objects and classify blockages based on the presence and coverage of these objects in the images. In this work, we uniquely propose an Explainable AI solution using DL and Semantic Web technologies to build a hybrid classifier. In this hybrid classifier, the DL component detects object presence and coverage level and semantic rules designed with close consultation with experts carry out the classification. By using the expert knowledge in the flooding context, our hybrid classifier provides the flexibility on categorising the image using objects and their coverage relationships. The experimental results demonstrated with a real-world use case showed that this hybrid approach of image classification has on average 11% improvement (F-Measure) in image classification performance compared to DL-only classifier. It also has the distinct advantage of integrating experts’ knowledge on defining the decision-making rules to represent the complex circumstances and using such knowledge to explain the results.","",""
257,"David Gunning, M. Stefik, Jaesik Choi, Timothy Miller, Simone Stumpf, Guang-Zhong Yang","XAI—Explainable artificial intelligence",2019,"","","","",53,"2022-07-13 09:19:02","","10.1126/scirobotics.aay7120","","",,,,,257,85.67,43,6,3,"Explainability is essential for users to effectively understand, trust, and manage powerful artificial intelligence applications. Explainability is essential for users to effectively understand, trust, and manage powerful artificial intelligence applications.","",""
3,"M. Nouri, Parveen Sihag, F. Salmasi, O. Kisi","Energy Loss in Skimming Flow over Cascade Spillways: Comparison of Artificial Intelligence-Based and Regression Methods",2020,"","","","",54,"2022-07-13 09:19:02","","10.3390/APP10196903","","",,,,,3,1.50,1,4,2,"In this study, the energy dissipation of cascade spillways was studied by conducting a series of laboratory experiments. Five spillways slope angles (α) (10°, 20°, 30°, 40°, and 50°), various step numbers (N) ranging from 4 to 75, and a wide range of discharges (Q), were considered. Some data-based models were developed to explain the relationships between hydraulic parameters. Multiple linear and nonlinear regression-based equations were developed based on dimensional analysis theory to compute energy dissipation over cascade spillways. For testing the robustness of developed data-based models, M5P, stochastic M5P, and random forest (RF) were used as new artificial intelligence (AI)-based techniques. To relate the input and output variables of energy dissipation, AI-based and regression approaches were developed. It was found that the formulation based on the stochastic M5P approach in solving energy dissipation problems over cascade spillways is more successful than the other regression and AI-based methods. Sensitivity analysis suggests that spillway slope in degrees (α) is the most influential input variable in predicting the relative energy dissipation (%) of the spillway in comparison to other input variables.","",""
0,"A. Paic","Policies for Artificial Intelligence in Science and Innovation",2020,"","","","",55,"2022-07-13 09:19:02","","10.22323/1.372.0045","","",,,,,0,0.00,0,1,2,"This contribution synthesizes the discussions of the special session on policies for Artificial Intelligence in Science and Innovation, organized by the OECD’s Directorate for Science, Technology and Innovation. The session was opened by Dr Judith Arrieta, Minister of the Foreign Service at the Chief of Staff’s Office of the Secretary of Foreign Affairs of Mexico, and the two panels included speakers from governments, industry and civil society from European countries, USA,Canada China and Australia. Participants discussed the disruptive nature of AI and the formidable challenges it poses. Most of the discussion focused under the umbrella title of ethics, but they span very different issues of human-centered values, fairness, transparency, explainability, and many more. Other challenges include employment, education, SME policy, enabling environment, access to data and computing technology. Responses by governments were also discussed with a particular focus on national strategies, whose main pillars are oriented toward knowledge creation through AI research, knowledge diffusion through linkages to the private sector, development of human capital which will underpin the development of the sector, and a strong values, ethical and regulatory framework to create the conditions for the development of trustworthy AI.  In a world of finite resources, discussants concluded that one cannot apply very stringent requirements to all AI decisions, and there is clearly a need to require more transparency, explainability and robustness from systems which have the greatest impact on human lives. Therefore an approach based on algorithmic impact assessment seems reasonable. Such an approach needs to be further developed and standardized.","",""
1,"T. Sethi, R. Awasthi","Use of artificial intelligence based models for learning better policy for maternal and child health",2020,"","","","",56,"2022-07-13 09:19:02","","10.1093/eurpub/ckaa165.291","","",,,,,1,0.50,1,2,2,"  More than 640,000 babies died of sepsis before they reach the age of one month in India in 2016. Despite a large number of government schemes aimed at reducing this rate, this number still remains high because of the complexity and interplay of factors involved. Finding an optimum policy and solutions to this problem needs learning from data. We integrated diverse sources of data and applied Bayesian Artificial Intelligence methods for learning to mitigate sepsis and adverse pregnancy outcomes in India. In this project, we created models that combine the robustness of ensemble averaged Baeysian Networks with decision learning and impact evaluation by using simulations and counterfactual reasoning respectively. We will demonstrate the process of learning these models and how these led us to infer the pivotal role of Water, Sanitation and Hygiene for reducing Adverse Pregnancy Outcome and neonatal sepsis in the population studied. We will also demonstrate the creation of explainable AI models for complex public health challenges and their deployment with wiseR, our in-house, open source platform for doing end-to-end Bayesian Decision Network learning.","",""
1,"Adam Norton, Amy Saretsky, H. Yanco","Developing Metrics and Evaluation Methods for Assessing Artificial Intelligence Enabled Robots in Manufacturing",2020,"","","","",57,"2022-07-13 09:19:02","","","","",,,,,1,0.50,0,3,2,"Evaluating the capabilities of a robotic system for manufacturing can include metrics related to performance, efficiency, and productivity. Measures for traditional industrial automation typically address operations that rely on strict repetition that does not allow for much variation. The inclusion of artificial intelligence (AI) in robotic systems can allow for greater aptitude in maintaining capability in the presence of variation, such as local changes in environmental characteristics or global changes in task execution parameters. New evaluation methods and metrics are needed to allow these advanced capabilities to be appropriately measured. This paper discusses evaluating the robustness, adaptability, generalizability, and versatility of AI-enabled robotic manufacturing systems. The considerations for conducting evaluations of these capabilities are reviewed, including implications for robots that learn and those that are designed to be explainable. Recommendations are made for advancing the development of metrics and evaluation methods that highlight the capabilities afforded by AI. A prototype framework is presented to guide the design of evaluations and classification of metrics.","",""
353,"Erico Tjoa, Cuntai Guan","A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI",2019,"","","","",58,"2022-07-13 09:19:02","","10.1109/TNNLS.2020.3027314","","",,,,,353,117.67,177,2,3,"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.","",""
11,"Aditya Kuppa, N. Le-Khac","Black Box Attacks on Explainable Artificial Intelligence(XAI) methods in Cyber Security",2020,"","","","",59,"2022-07-13 09:19:02","","10.1109/IJCNN48605.2020.9206780","","",,,,,11,5.50,6,2,2,"Cybersecurity community is slowly leveraging Machine Learning (ML) to combat ever evolving threats. One of the biggest drivers for successful adoption of these models is how well domain experts and users are able to understand and trust their functionality. As these black-box models are being employed to make important predictions, the demand for transparency and explainability is increasing from the stakeholders.Explanations supporting the output of ML models are crucial in cyber security, where experts require far more information from the model than a simple binary output for their analysis. Recent approaches in the literature have focused on three different areas: (a) creating and improving explainability methods which help users better understand the internal workings of ML models and their outputs; (b) attacks on interpreters in white box setting; (c) defining the exact properties and metrics of the explanations generated by models. However, they have not covered, the security properties and threat models relevant to cybersecurity domain, and attacks on explainable models in black box settings.In this paper, we bridge this gap by proposing a taxonomy for Explainable Artificial Intelligence (XAI) methods, covering various security properties and threat models relevant to cyber security domain. We design a novel black box attack for analyzing the consistency, correctness and confidence security properties of gradient based XAI methods. We validate our proposed system on 3 security-relevant data-sets and models, and demonstrate that the method achieves attacker’s goal of misleading both the classifier and explanation report and, only explainability method without affecting the classifier output. Our evaluation of the proposed approach shows promising results and can help in designing secure and robust XAI methods.","",""
1,"W. Silva, João Ribeiro Pinto, Tiago Gonçalves, Ana F. Sequeira, Jaime S. Cardoso","Explainable Artificial Intelligence for Face Presentation Attack Detection",2020,"","","","",60,"2022-07-13 09:19:02","","","","",,,,,1,0.50,0,5,2,"The use of deep learning techniques for face presentation attack detection (PAD) is increasingly common due to their ability to reach strong accuracy performances. Nonetheless, the use of complex models such as the ones produced with deep learning techniques raises safety and trust concerns, as one is not able to understand the motifs behind model decisions. Furthermore, traditional metrics of evaluation fall short in terms of capturing the desirable working properties of models, which is particularly worrisome when working in high-regulated areas, such as biometrics. In this work, we propose the use of interpretability techniques to further assess the robustness of face PAD models. Moreover, we also define desirable properties for a face PAD model to have, which can be evaluated through interpretability. Experiments were performed using the ROSE Youtu video collection and showed the additional value of interpretability in the identification of model robustness.","",""
109,"Shilin Qiu, Qihe Liu, Shijie Zhou, Chunjiang Wu","Review of Artificial Intelligence Adversarial Attack and Defense Technologies",2019,"","","","",61,"2022-07-13 09:19:02","","10.3390/APP9050909","","",,,,,109,36.33,27,4,3,"In recent years, artificial intelligence technologies have been widely used in computer vision, natural language processing, automatic driving, and other fields. However, artificial intelligence systems are vulnerable to adversarial attacks, which limit the applications of artificial intelligence (AI) technologies in key security fields. Therefore, improving the robustness of AI systems against adversarial attacks has played an increasingly important role in the further development of AI. This paper aims to comprehensively summarize the latest research progress on adversarial attack and defense technologies in deep learning. According to the target model’s different stages where the adversarial attack occurred, this paper expounds the adversarial attack methods in the training stage and testing stage respectively. Then, we sort out the applications of adversarial attack technologies in computer vision, natural language processing, cyberspace security, and the physical world. Finally, we describe the existing adversarial defense methods respectively in three main categories, i.e., modifying data, modifying models and using auxiliary tools.","",""
51,"Miriam C. Buiten","Towards Intelligent Regulation of Artificial Intelligence",2019,"","","","",62,"2022-07-13 09:19:02","","10.1017/err.2019.8","","",,,,,51,17.00,51,1,3,"Artificial intelligence (AI) is becoming a part of our daily lives at a fast pace, offering myriad benefits for society. At the same time, there is concern about the unpredictability and uncontrollability of AI. In response, legislators and scholars call for more transparency and explainability of AI. This article considers what it would mean to require transparency of AI. It advocates looking beyond the opaque concept of AI, focusing on the concrete risks and biases of its underlying technology: machine-learning algorithms. The article discusses the biases that algorithms may produce through the input data, the testing of the algorithm and the decision model. Any transparency requirement for algorithms should result in explanations of these biases that are both understandable for the prospective recipients, and technically feasible for producers. Before asking how much transparency the law should require from algorithms, we should therefore consider if the explanation that programmers could offer is useful in specific legal contexts.","",""
1660,"Alejandro Barredo Arrieta, Natalia Díaz Rodríguez, J. Ser, Adrien Bennetot, S. Tabik, A. Barbado, S. García, S. Gil-Lopez, D. Molina, Richard Benjamins, Raja Chatila, Francisco Herrera","Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI",2019,"","","","",63,"2022-07-13 09:19:02","","10.1016/j.inffus.2019.12.012","","",,,,,1660,553.33,166,12,3,"","",""
7,"Mir Riyanul Islam, Mobyen Uddin Ahmed, Shaibal Barua, S. Begum","A Systematic Review of Explainable Artificial Intelligence in Terms of Different Application Domains and Tasks",2022,"","","","",64,"2022-07-13 09:19:02","","10.3390/app12031353","","",,,,,7,7.00,2,4,1,"Artificial intelligence (AI) and machine learning (ML) have recently been radically improved and are now being employed in almost every application domain to develop automated or semi-automated systems. To facilitate greater human acceptability of these systems, explainable artificial intelligence (XAI) has experienced significant growth over the last couple of years with the development of highly accurate models but with a paucity of explainability and interpretability. The literature shows evidence from numerous studies on the philosophy and methodologies of XAI. Nonetheless, there is an evident scarcity of secondary studies in connection with the application domains and tasks, let alone review studies following prescribed guidelines, that can enable researchers’ understanding of the current trends in XAI, which could lead to future research for domain- and application-specific method development. Therefore, this paper presents a systematic literature review (SLR) on the recent developments of XAI methods and evaluation metrics concerning different application domains and tasks. This study considers 137 articles published in recent years and identified through the prominent bibliographic databases. This systematic synthesis of research articles resulted in several analytical findings: XAI methods are mostly developed for safety-critical domains worldwide, deep learning and ensemble models are being exploited more than other types of AI/ML models, visual explanations are more acceptable to end-users and robust evaluation metrics are being developed to assess the quality of explanations. Research studies have been performed on the addition of explanations to widely used AI/ML models for expert users. However, more attention is required to generate explanations for general users from sensitive domains such as finance and the judicial system.","",""
0,"Nina de Lacy, Michael J. Ramshaw, J. Kutz","Integrated Evolutionary Learning: An Artificial Intelligence Approach to Joint Learning of Features and Hyperparameters for Optimized, Explainable Machine Learning",2022,"","","","",65,"2022-07-13 09:19:02","","10.3389/frai.2022.832530","","",,,,,0,0.00,0,3,1,"Artificial intelligence and machine learning techniques have proved fertile methods for attacking difficult problems in medicine and public health. These techniques have garnered strong interest for the analysis of the large, multi-domain open science datasets that are increasingly available in health research. Discovery science in large datasets is challenging given the unconstrained nature of the learning environment where there may be a large number of potential predictors and appropriate ranges for model hyperparameters are unknown. As well, it is likely that explainability is at a premium in order to engage in future hypothesis generation or analysis. Here, we present a novel method that addresses these challenges by exploiting evolutionary algorithms to optimize machine learning discovery science while exploring a large solution space and minimizing bias. We demonstrate that our approach, called integrated evolutionary learning (IEL), provides an automated, adaptive method for jointly learning features and hyperparameters while furnishing explainable models where the original features used to make predictions may be obtained even with artificial neural networks. In IEL the machine learning algorithm of choice is nested inside an evolutionary algorithm which selects features and hyperparameters over generations on the basis of an information function to converge on an optimal solution. We apply IEL to three gold standard machine learning algorithms in challenging, heterogenous biobehavioral data: deep learning with artificial neural networks, decision tree-based techniques and baseline linear models. Using our novel IEL approach, artificial neural networks achieved ≥ 95% accuracy, sensitivity and specificity and 45–73% R2 in classification and substantial gains over default settings. IEL may be applied to a wide range of less- or unconstrained discovery science problems where the practitioner wishes to jointly learn features and hyperparameters in an adaptive, principled manner within the same algorithmic process. This approach offers significant flexibility, enlarges the solution space and mitigates bias that may arise from manual or semi-manual hyperparameter tuning and feature selection and presents the opportunity to select the inner machine learning algorithm based on the results of optimized learning for the problem at hand.","",""
0,"G. Antoniou, Emmanuel Papadakis, George Baryannis","Mental Health Diagnosis: A Case for Explainable Artificial Intelligence",2022,"","","","",66,"2022-07-13 09:19:02","","10.1142/s0218213022410032","","",,,,,0,0.00,0,3,1,"Mental illnesses are becoming increasingly prevalent, in turn leading to an increased interest in exploring artificial intelligence (AI) solutions to facilitate and enhance healthcare processes ranging from diagnosis to monitoring and treatment. In contrast to application areas where black box systems may be acceptable, explainability in healthcare applications is essential, especially in the case of diagnosing complex and sensitive mental health issues. In this paper, we first summarize recent developments in AI research for mental health, followed by an overview of approaches to explainable AI and their potential benefits in healthcare settings. We then present a recent case study of applying explainable AI for ADHD diagnosis which is used as a basis to identify challenges in realizing explainable AI solutions for mental health diagnosis and potential future research directions to address these challenges.","",""
0,"Jurgita Černevičienė, Audrius Kabašinskas","Review of Multi-Criteria Decision-Making Methods in Finance Using Explainable Artificial Intelligence",2022,"","","","",67,"2022-07-13 09:19:02","","10.3389/frai.2022.827584","","",,,,,0,0.00,0,2,1,"The influence of Artificial Intelligence is growing, as is the need to make it as explainable as possible. Explainability is one of the main obstacles that AI faces today on the way to more practical implementation. In practise, companies need to use models that balance interpretability and accuracy to make more effective decisions, especially in the field of finance. The main advantages of the multi-criteria decision-making principle (MCDM) in financial decision-making are the ability to structure complex evaluation tasks that allow for well-founded financial decisions, the application of quantitative and qualitative criteria in the analysis process, the possibility of transparency of evaluation and the introduction of improved, universal and practical academic methods to the financial decision-making process. This article presents a review and classification of multi-criteria decision-making methods that help to achieve the goal of forthcoming research: to create artificial intelligence-based methods that are explainable, transparent, and interpretable for most investment decision-makers.","",""
17,"K. Assi, M. Shafiullah, K. Nahiduzzaman, Umer Mansoor","Travel-To-School Mode Choice Modelling Employing Artificial Intelligence Techniques: A Comparative Study",2019,"","","","",68,"2022-07-13 09:19:02","","10.3390/SU11164484","","",,,,,17,5.67,4,4,3,"Many techniques including logistic regression and artificial intelligence have been employed to explain school-goers mode choice behavior. This paper aims to compare the effectiveness, robustness, and convergence of three different machine learning tools (MLT), namely the extreme learning machine (ELM), support vector machine (SVM), and multi-layer perceptron neural network (MLP-NN) to predict school-goers mode choice behavior in Al-Khobar and Dhahran cities of the Kingdom of Saudi Arabia (KSA). It uses the students’ information, including the school grade, the distance between home and school, travel time, family income and size, number of students in the family and education level of parents as input variables to the MLT. However, their outputs were binary, that is, either to choose the passenger car or walking to the school. The study examined a promising performance of the ELM and MLP-NN suggesting their significance as alternatives for school-goers mode choice modeling. The performances of the SVM was satisfactory but not to the same level of significance in comparison with the other two. Moreover, the SVM technique is computationally more expensive over the ELM and MLP-NN. Further, this research develops a majority voting ensemble method based on the outputs of the employed MLT to enhance the overall prediction performance. The presented results confirm the efficacy and superiority of the ensemble method over the others. The study results are likely to guide the transport engineers, planners, and decision-makers by providing them with a reliable way to model and predict the traffic demand for transport infrastructures on the basis of the prevailing mode choice behavior.","",""
427,"D. Ting, L. Pasquale, L. Peng, J. P. Campbell, Aaron Y. Lee, R. Raman, G. Tan, L. Schmetterer, P. Keane, T. Wong","Artificial intelligence and deep learning in ophthalmology",2018,"","","","",69,"2022-07-13 09:19:02","","10.1136/bjophthalmol-2018-313173","","",,,,,427,106.75,43,10,4,"Artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest in recent years. DL has been widely adopted in image recognition, speech recognition and natural language processing, but is only beginning to impact on healthcare. In ophthalmology, DL has been applied to fundus photographs, optical coherence tomography and visual fields, achieving robust classification performance in the detection of diabetic retinopathy and retinopathy of prematurity, the glaucoma-like disc, macular oedema and age-related macular degeneration. DL in ocular imaging may be used in conjunction with telemedicine as a possible solution to screen, diagnose and monitor major eye diseases for patients in primary care and community settings. Nonetheless, there are also potential challenges with DL application in ophthalmology, including clinical and technical challenges, explainability of the algorithm results, medicolegal issues, and physician and patient acceptance of the AI ‘black-box’ algorithms. DL could potentially revolutionise how ophthalmology is practised in the future. This review provides a summary of the state-of-the-art DL systems described for ophthalmic applications, potential challenges in clinical deployment and the path forward.","",""
0,"Wanqing Wei, Linyu Li","The Impact of Artificial Intelligence on the Mental Health of Manufacturing Workers: The Mediating Role of Overtime Work and the Work Environment",2022,"","","","",70,"2022-07-13 09:19:02","","10.3389/fpubh.2022.862407","","",,,,,0,0.00,0,2,1,"Background Work-related mental health and psychological injuries are important issues related to people's livelihood and wellbeing. Currently, digitalization and intelligent technology have an extremely large impact on the workforce. China is actively promoting the deep integration of artificial intelligence (AI) and manufacturing, which may have important implications for the mental health of manufacturing workers. However, existing researches have paid little attention to the influence of AI on the mental wellbeing of workers in China. There is a lack of relevant empirical research, and the findings in existing studies are inconsistent. Methods Using data from the 2018 China Labor Force Dynamics Survey, this paper studies the impact of AI on the depressive symptoms of manufacturing workers and uses stepwise and bootstrapping methods to test whether overtime work and the work environment exhibit mediating effects. Robustness tests were performed by using alternative measures for the dependent and mediating variables. Finally, the heterogeneity in the impact of AI by skill level and generation was examined. Results AI can reduce the psychological depression scores of manufacturing workers by 1.643 points, which indicates that AI promotes workers' mental health. Working overtime is not a mediator between AI and mental health. However, the work environment is a mediator between AI and the mental health of manufacturing workers: it explains 11.509% of workers' mental health. The impact of AI on the mental health of manufacturing workers varies by skill level and generation. AI improves the mental health of low-skilled manufacturing workers by 2.342 points and that of manufacturing workers born before the 1980's by 2.070 points. Conclusions The application of AI is conducive to improvements in the mental health of manufacturing workers. Improving the work environment is a powerful way to increase the positive effects of AI on workers' mental health. The impact of AI on the mental health of manufacturing workers varies by skill level and generation. The mental health of low-skilled workers and workers born after 1980 is affected more positively by the adoption of AI.","",""
68,"Ilia Stepin, J. M. Alonso, Alejandro Catalá, Martin Pereira-Fariña","A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence",2021,"","","","",71,"2022-07-13 09:19:02","","10.1109/ACCESS.2021.3051315","","",,,,,68,68.00,17,4,1,"A number of algorithms in the field of artificial intelligence offer poorly interpretable decisions. To disclose the reasoning behind such algorithms, their output can be explained by means of so-called evidence-based (or factual) explanations. Alternatively, contrastive and counterfactual explanations justify why the output of the algorithms is not any different and how it could be changed, respectively. It is of crucial importance to bridge the gap between theoretical approaches to contrastive and counterfactual explanation and the corresponding computational frameworks. In this work we conduct a systematic literature review which provides readers with a thorough and reproducible analysis of the interdisciplinary research field under study. We first examine theoretical foundations of contrastive and counterfactual accounts of explanation. Then, we report the state-of-the-art computational frameworks for contrastive and counterfactual explanation generation. In addition, we analyze how grounded such frameworks are on the insights from the inspected theoretical approaches. As a result, we highlight a variety of properties of the approaches under study and reveal a number of shortcomings thereof. Moreover, we define a taxonomy regarding both theoretical and practical approaches to contrastive and counterfactual explanation.","",""
0,"Pan Wang, Yangyang Zhong, Zhenan Yao","Modeling and Estimation of CO2 Emissions in China Based on Artificial Intelligence",2022,"","","","",72,"2022-07-13 09:19:02","","10.1155/2022/6822467","","",,,,,0,0.00,0,3,1,"Since China’s reform and opening up, the social economy has achieved rapid development, followed by a sharp increase in carbon dioxide (CO2) emissions. Therefore, at the 75th United Nations General Assembly, China proposed to achieve carbon peaking by 2030 and carbon neutrality by 2060. The research work on advance forecasting of CO2 emissions is essential to achieve the above-mentioned carbon peaking and carbon neutrality goals in China. In order to achieve accurate prediction of CO2 emissions, this study establishes a hybrid intelligent algorithm model suitable for CO2 emissions prediction based on China’s CO2 emissions and related socioeconomic indicator data from 1971 to 2017. The hyperparameters of Least Squares Support Vector Regression (LSSVR) are optimized by the Adaptive Artificial Bee Colony (AABC) algorithm to build a high-performance hybrid intelligence model. The research results show that the hybrid intelligent algorithm model designed in this paper has stronger robustness and accuracy with relative error almost within ±5% in the advance prediction of CO2 emissions. The modeling scheme proposed in this study can not only provide strong support for the Chinese government and industry departments to formulate policies related to the carbon peaking and carbon neutrality goals, but also can be extended to the research of other socioeconomic-related issues.","",""
54,"G. Collins, P. Dhiman, Constanza L. Andaur Navarro, Jie Ma, L. Hooft, J. Reitsma, P. Logullo, Andrew Beam, Lily Peng, B. van Calster, M. van Smeden, R. Riley, K. Moons","Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence",2021,"","","","",73,"2022-07-13 09:19:02","","10.1136/bmjopen-2020-048008","","",,,,,54,54.00,5,13,1,"Introduction The Transparent Reporting of a multivariable prediction model of Individual Prognosis Or Diagnosis (TRIPOD) statement and the Prediction model Risk Of Bias ASsessment Tool (PROBAST) were both published to improve the reporting and critical appraisal of prediction model studies for diagnosis and prognosis. This paper describes the processes and methods that will be used to develop an extension to the TRIPOD statement (TRIPOD-artificial intelligence, AI) and the PROBAST (PROBAST-AI) tool for prediction model studies that applied machine learning techniques. Methods and analysis TRIPOD-AI and PROBAST-AI will be developed following published guidance from the EQUATOR Network, and will comprise five stages. Stage 1 will comprise two systematic reviews (across all medical fields and specifically in oncology) to examine the quality of reporting in published machine-learning-based prediction model studies. In stage 2, we will consult a diverse group of key stakeholders using a Delphi process to identify items to be considered for inclusion in TRIPOD-AI and PROBAST-AI. Stage 3 will be virtual consensus meetings to consolidate and prioritise key items to be included in TRIPOD-AI and PROBAST-AI. Stage 4 will involve developing the TRIPOD-AI checklist and the PROBAST-AI tool, and writing the accompanying explanation and elaboration papers. In the final stage, stage 5, we will disseminate TRIPOD-AI and PROBAST-AI via journals, conferences, blogs, websites (including TRIPOD, PROBAST and EQUATOR Network) and social media. TRIPOD-AI will provide researchers working on prediction model studies based on machine learning with a reporting guideline that can help them report key details that readers need to evaluate the study quality and interpret its findings, potentially reducing research waste. We anticipate PROBAST-AI will help researchers, clinicians, systematic reviewers and policymakers critically appraise the design, conduct and analysis of machine learning based prediction model studies, with a robust standardised tool for bias evaluation. Ethics and dissemination Ethical approval has been granted by the Central University Research Ethics Committee, University of Oxford on 10-December-2020 (R73034/RE001). Findings from this study will be disseminated through peer-review publications. PROSPERO registration number CRD42019140361 and CRD42019161764.","",""
61,"Markus Langer, Daniel Oster, Timo Speith, H. Hermanns, Lena Kästner, Eva Schmidt, Andreas Sesing, Kevin Baum","What Do We Want From Explainable Artificial Intelligence (XAI)? - A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research",2021,"","","","",74,"2022-07-13 09:19:02","","10.1016/j.artint.2021.103473","","",,,,,61,61.00,8,8,1,"","",""
24,"Maxime Sermesant, H. Delingette, H. Cochet, P. Jaïs, N. Ayache","Applications of artificial intelligence in cardiovascular imaging",2021,"","","","",75,"2022-07-13 09:19:02","","10.1038/s41569-021-00527-2","","",,,,,24,24.00,5,5,1,"","",""
15,"R. Cadario, Chiara Longoni, Carey K. Morewedge","Understanding, explaining, and utilizing medical artificial intelligence.",2021,"","","","",76,"2022-07-13 09:19:02","","10.1038/S41562-021-01146-0","","",,,,,15,15.00,5,3,1,"","",""
19,"Basim Mahbooba, Mohan Timilsina, R. Sahal, M. Serrano","Explainable Artificial Intelligence (XAI) to Enhance Trust Management in Intrusion Detection Systems Using Decision Tree Model",2021,"","","","",77,"2022-07-13 09:19:02","","10.1155/2021/6634811","","",,,,,19,19.00,5,4,1,"Despite the growing popularity of machine learning models in the cyber-security applications (e.g., an intrusion detection system (IDS)), most of these models are perceived as a black-box. The eXplainable Artificial Intelligence (XAI) has become increasingly important to interpret the machine learning models to enhance trust management by allowing human experts to understand the underlying data evidence and causal reasoning. According to IDS, the critical role of trust management is to understand the impact of the malicious data to detect any intrusion in the system. The previous studies focused more on the accuracy of the various classification algorithms for trust in IDS. They do not often provide insights into their behavior and reasoning provided by the sophisticated algorithm. Therefore, in this paper, we have addressed XAI concept to enhance trust management by exploring the decision tree model in the area of IDS. We use simple decision tree algorithms that can be easily read and even resemble a human approach to decision-making by splitting the choice into many small subchoices for IDS. We experimented with this approach by extracting rules in a widely used KDD benchmark dataset. We also compared the accuracy of the decision tree approach with the other state-of-the-art algorithms.","",""
24,"P. Giudici, E. Raffinetti","Shapley-Lorenz eXplainable Artificial Intelligence",2020,"","","","",78,"2022-07-13 09:19:02","","10.1016/j.eswa.2020.114104","","",,,,,24,12.00,12,2,2,"","",""
17,"Yi-Shan Lin, Wen-Chuan Lee, Z. B. Celik","What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors",2020,"","","","",79,"2022-07-13 09:19:02","","10.1145/3447548.3467213","","",,,,,17,8.50,6,3,2,"EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.","",""
15,"Hubert Baniecki, Wojciech Kretowicz, Piotr Piatyszek, J. Wiśniewski, P. Biecek","dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python",2020,"","","","",80,"2022-07-13 09:19:02","","","","",,,,,15,7.50,3,5,2,"The increasing amount of available data, computing power, and the constant pursuit for higher performance results in the growing complexity of predictive models. Their black-box nature leads to opaqueness debt phenomenon inflicting increased risks of discrimination, lack of reproducibility, and deflated performance due to data drift. To manage these risks, good MLOps practices ask for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity of deeper model transparency appears not only from scientific and social domains, but also emerging laws and regulations on artificial intelligence. To facilitate the development of responsible machine learning models, we showcase dalex, a Python package which implements the model-agnostic interface for interactive model exploration. It adopts the design crafted through the development of various tools for responsible machine learning; thus, it aims at the unification of the existing solutions. This library's source code and documentation are available under open license at this https URL","",""
44,"A. Vellido","Societal Issues Concerning the Application of Artificial Intelligence in Medicine",2018,"","","","",81,"2022-07-13 09:19:02","","10.1159/000492428","","",,,,,44,11.00,44,1,4,"Background: Medicine is becoming an increasingly data-centred discipline and, beyond classical statistical approaches, artificial intelligence (AI) and, in particular, machine learning (ML) are attracting much interest for the analysis of medical data. It has been argued that AI is experiencing a fast process of commodification. This characterization correctly reflects the current process of industrialization of AI and its reach into society. Therefore, societal issues related to the use of AI and ML should not be ignored any longer and certainly not in the medical domain. These societal issues may take many forms, but they all entail the design of models from a human-centred perspective, incorporating human-relevant requirements and constraints. In this brief paper, we discuss a number of specific issues affecting the use of AI and ML in medicine, such as fairness, privacy and anonymity, explainability and interpretability, but also some broader societal issues, such as ethics and legislation. We reckon that all of these are relevant aspects to consider in order to achieve the objective of fostering acceptance of AI- and ML-based technologies, as well as to comply with an evolving legislation concerning the impact of digital technologies on ethically and privacy sensitive matters. Our specific goal here is to reflect on how all these topics affect medical applications of AI and ML. This paper includes some of the contents of the “2nd Meeting of Science and Dialysis: Artificial Intelligence,” organized in the Bellvitge University Hospital, Barcelona, Spain. Summary and Key Messages: AI and ML are attracting much interest from the medical community as key approaches to knowledge extraction from data. These approaches are increasingly colonizing ambits of social impact, such as medicine and healthcare. Issues of social relevance with an impact on medicine and healthcare include (although they are not limited to) fairness, explainability, privacy, ethics and legislation.","",""
148,"Jos'e Jim'enez-Luna, F. Grisoni, G. Schneider","Drug discovery with explainable artificial intelligence",2020,"","","","",82,"2022-07-13 09:19:02","","10.1038/s42256-020-00236-4","","",,,,,148,74.00,49,3,2,"","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",83,"2022-07-13 09:19:02","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
132,"Shakir Mohamed, Marie-Thérèse Png, William S. Isaac","Decolonial AI: Decolonial Theory as Sociotechnical Foresight in Artificial Intelligence",2020,"","","","",84,"2022-07-13 09:19:02","","10.1007/s13347-020-00405-8","","",,,,,132,66.00,44,3,2,"","",""
129,"Arun Das, P. Rad","Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey",2020,"","","","",85,"2022-07-13 09:19:02","","","","",,,,,129,64.50,65,2,2,"Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",86,"2022-07-13 09:19:02","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
43,"Zeynep Akata, D. Balliet, M. de Rijke, F. Dignum, V. Dignum, Gusz Eiben, Antske Fokkens, D. Grossi, K. Hindriks, H. Hoos, Hayley Hung, C. Jonker, Christof Monz, M. Neerincx, Frans Oliehoek, H. Prakken, S. Schlobach, Linda Christina van der Gaag, F. van Harmelen, Herke van Hoof, Birna van Riemsdijk, A. van Wynsberghe, R. Verbrugge, B. Verheij, P. Vossen, M. Welling","A Research Agenda for Hybrid Intelligence: Augmenting Human Intellect With Collaborative, Adaptive, Responsible, and Explainable Artificial Intelligence",2020,"","","","",87,"2022-07-13 09:19:02","","10.1109/MC.2020.2996587","","",,,,,43,21.50,4,26,2,"We define hybrid intelligence (HI) as the combination of human and machine intelligence, augmenting human intellect and capabilities instead of replacing them and achieving goals that were unreachable by either humans or machines. HI is an important new research focus for artificial intelligence, and we set a research agenda for HI by formulating four challenges.","",""
9,"Junfeng Peng, Kaiqiang Zou, Mi Zhou, Yi Teng, Xiongyong Zhu, Feifei Zhang, Jun Xu","An Explainable Artificial Intelligence Framework for the Deterioration Risk Prediction of Hepatitis Patients",2021,"","","","",88,"2022-07-13 09:19:02","","10.1007/s10916-021-01736-5","","",,,,,9,9.00,1,7,1,"","",""
103,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dentistry: Chances and Challenges",2020,"","","","",89,"2022-07-13 09:19:02","","10.1177/0022034520915714","","",,,,,103,51.50,34,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
89,"S. Lauritsen, Mads Kristensen, Mathias Vassard Olsen, Morten Skaarup Larsen, K. M. Lauritsen, Marianne Johansson Jørgensen, Jeppe Lange, B. Thiesson","Explainable artificial intelligence model to predict acute critical illness from electronic health records",2019,"","","","",90,"2022-07-13 09:19:02","","10.1038/s41467-020-17431-x","","",,,,,89,29.67,11,8,3,"","",""
7,"A. Rawal, J. Mccoy, D. Rawat, Brian M. Sadler, R. Amant","Recent Advances in Trustworthy Explainable Artificial Intelligence: Status, Challenges and Perspectives",2021,"","","","",91,"2022-07-13 09:19:02","","10.36227/techrxiv.17054396.v1","","",,,,,7,7.00,1,5,1,"This is a survey paper on Explainable Artificial Intelligence (XAI).","",""
81,"Thomas G. Dietterich","Steps Toward Robust Artificial Intelligence",2017,"","","","",92,"2022-07-13 09:19:02","","10.1609/aimag.v38i3.2756","","",,,,,81,16.20,81,1,5,"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world","",""
0,"S. Sadeghi, M. Amiri, Farzaneh Mansoori Mooseloo","Artificial Intelligence and Its Application in Optimization under Uncertainty",2021,"","","","",93,"2022-07-13 09:19:02","","10.5772/intechopen.98628","","",,,,,0,0.00,0,3,1,"Nowadays, the increase in data acquisition and availability and complexity around optimization make it imperative to jointly use artificial intelligence (AI) and optimization for devising data-driven and intelligent decision support systems (DSS). A DSS can be successful if large amounts of interactive data proceed fast and robustly and extract useful information and knowledge to help decision-making. In this context, the data-driven approach has gained prominence due to its provision of insights for decision-making and easy implementation. The data-driven approach can discover various database patterns without relying on prior knowledge while also handling flexible objectives and multiple scenarios. This chapter reviews recent advances in data-driven optimization, highlighting the promise of data-driven optimization that integrates mathematical programming and machine learning (ML) for decision-making under uncertainty and identifies potential research opportunities. This chapter provides guidelines and implications for researchers, managers, and practitioners in operations research who want to advance their decision-making capabilities under uncertainty concerning data-driven optimization. Then, a comprehensive review and classification of the relevant publications on the data-driven stochastic program, data-driven robust optimization, and data-driven chance-constrained are presented. This chapter also identifies fertile avenues for future research that focus on deep-data-driven optimization, deep data-driven models, as well as online learning-based data-driven optimization. Perspectives on reinforcement learning (RL)-based data-driven optimization and deep RL for solving NP-hard problems are discussed. We investigate the application of data-driven optimization in different case studies to demonstrate improvements in operational performance over conventional optimization methodology. Finally, some managerial implications and some future directions are provided.","",""
61,"N. Mirchi, Vincent Bissonnette, R. Yilmaz, N. Ledwos, A. Winkler-Schwartz, R. Del Maestro","The Virtual Operative Assistant: An explainable artificial intelligence tool for simulation-based training in surgery and medicine",2020,"","","","",94,"2022-07-13 09:19:02","","10.1371/journal.pone.0229596","","",,,,,61,30.50,10,6,2,"Simulation-based training is increasingly being used for assessment and training of psychomotor skills involved in medicine. The application of artificial intelligence and machine learning technologies has provided new methodologies to utilize large amounts of data for educational purposes. A significant criticism of the use of artificial intelligence in education has been a lack of transparency in the algorithms’ decision-making processes. This study aims to 1) introduce a new framework using explainable artificial intelligence for simulation-based training in surgery, and 2) validate the framework by creating the Virtual Operative Assistant, an automated educational feedback platform. Twenty-eight skilled participants (14 staff neurosurgeons, 4 fellows, 10 PGY 4–6 residents) and 22 novice participants (10 PGY 1–3 residents, 12 medical students) took part in this study. Participants performed a virtual reality subpial brain tumor resection task on the NeuroVR simulator using a simulated ultrasonic aspirator and bipolar. Metrics of performance were developed, and leave-one-out cross validation was employed to train and validate a support vector machine in Matlab. The classifier was combined with a unique educational system to build the Virtual Operative Assistant which provides users with automated feedback on their metric performance with regards to expert proficiency performance benchmarks. The Virtual Operative Assistant successfully classified skilled and novice participants using 4 metrics with an accuracy, specificity and sensitivity of 92, 82 and 100%, respectively. A 2-step feedback system was developed to provide participants with an immediate visual representation of their standing related to expert proficiency performance benchmarks. The educational system outlined establishes a basis for the potential role of integrating artificial intelligence and virtual reality simulation into surgical educational teaching. The potential of linking expertise classification, objective feedback based on proficiency benchmarks, and instructor input creates a novel educational tool by integrating these three components into a formative educational paradigm.","",""
61,"S. N. Payrovnaziri, Zhaoyi Chen, Pablo A Rengifo-Moreno, Tim Miller, J. Bian, Jonathan H. Chen, Xiuwen Liu, Zhe He","Explainable artificial intelligence models using real-world electronic health record data: a systematic scoping review",2020,"","","","",95,"2022-07-13 09:19:02","","10.1093/jamia/ocaa053","","",,,,,61,30.50,8,8,2,"OBJECTIVE To conduct a systematic scoping review of explainable artificial intelligence (XAI) models that use real-world electronic health record data, categorize these techniques according to different biomedical applications, identify gaps of current studies, and suggest future research directions.   MATERIALS AND METHODS We searched MEDLINE, IEEE Xplore, and the Association for Computing Machinery (ACM) Digital Library to identify relevant papers published between January 1, 2009 and May 1, 2019. We summarized these studies based on the year of publication, prediction tasks, machine learning algorithm, dataset(s) used to build the models, the scope, category, and evaluation of the XAI methods. We further assessed the reproducibility of the studies in terms of the availability of data and code and discussed open issues and challenges.   RESULTS Forty-two articles were included in this review. We reported the research trend and most-studied diseases. We grouped XAI methods into 5 categories: knowledge distillation and rule extraction (N = 13), intrinsically interpretable models (N = 9), data dimensionality reduction (N = 8), attention mechanism (N = 7), and feature interaction and importance (N = 5).   DISCUSSION XAI evaluation is an open issue that requires a deeper focus in the case of medical applications. We also discuss the importance of reproducibility of research work in this field, as well as the challenges and opportunities of XAI from 2 medical professionals' point of view.   CONCLUSION Based on our review, we found that XAI evaluation in medicine has not been adequately and formally practiced. Reproducibility remains a critical concern. Ample opportunities exist to advance XAI research in medicine.","",""
25,"P. Phillips, Carina A. Hahn, Peter C. Fontana, David A. Broniatowski, Mark A. Przybocki","Four Principles of Explainable Artificial Intelligence",2020,"","","","",96,"2022-07-13 09:19:02","","10.6028/nist.ir.8312-draft","","",,,,,25,12.50,5,5,2,"We introduce four principles for explainable artificial intelligence (AI) that comprise fundamental properties for explainable AI systems. We propose that explainable AI systems deliver accompanying evidence or reasons for outcomes and processes; provide explanations that are understandable to individual users; provide explanations that correctly reflect the system’s process for generating the output; and that a system only operates under conditions for which it was designed and when it reaches sufficient confidence in its output. We have termed these four principles as explanation, meaningful, explanation accuracy, and knowledge limits, respectively. Through significant stakeholder engagement, these four principles were developed to encompass the multidisciplinary nature of explainable AI, including the fields of computer science, engineering, and psychology. Because one-sizefits-all explanations do not exist, different users will require different types of explanations. We present five categories of explanation and summarize theories of explainable AI. We give an overview of the algorithms in the field that cover the major classes of explainable algorithms. As a baseline comparison, we assess how well explanations provided by people follow our four principles. This assessment provides insights to the challenges of designing explainable AI systems.","",""
43,"M. González-Rivero, Oscar Beijbom, A. Rodriguez-Ramirez, D. Bryant, A. Ganase, Y. González-Marrero, A. Herrera-Reveles, E. Kennedy, Catherine J. S. Kim, S. Lopez-Marcano, Kathryn Markey, B. Neal, K. Osborne, C. Reyes-Nivia, E. Sampayo, Kristin Stolberg, Abbie Taylor, J. Vercelloni, Mathew Wyatt, O. Hoegh‐Guldberg","Monitoring of Coral Reefs Using Artificial Intelligence: A Feasible and Cost-Effective Approach",2020,"","","","",97,"2022-07-13 09:19:02","","10.3390/rs12030489","","",,,,,43,21.50,4,20,2,"Ecosystem monitoring is central to effective management, where rapid reporting is essential to provide timely advice. While digital imagery has greatly improved the speed of underwater data collection for monitoring benthic communities, image analysis remains a bottleneck in reporting observations. In recent years, a rapid evolution of artificial intelligence in image recognition has been evident in its broad applications in modern society, offering new opportunities for increasing the capabilities of coral reef monitoring. Here, we evaluated the performance of Deep Learning Convolutional Neural Networks for automated image analysis, using a global coral reef monitoring dataset. The study demonstrates the advantages of automated image analysis for coral reef monitoring in terms of error and repeatability of benthic abundance estimations, as well as cost and benefit. We found unbiased and high agreement between expert and automated observations (97%). Repeated surveys and comparisons against existing monitoring programs also show that automated estimation of benthic composition is equally robust in detecting change and ensuring the continuity of existing monitoring data. Using this automated approach, data analysis and reporting can be accelerated by at least 200x and at a fraction of the cost (1%). Combining commonly used underwater imagery in monitoring with automated image annotation can dramatically improve how we measure and monitor coral reefs worldwide, particularly in terms of allocating limited resources, rapid reporting and data integration within and across management areas.","",""
42,"M. Nassar, K. Salah, M. H. Rehman, D. Svetinovic","Blockchain for explainable and trustworthy artificial intelligence",2019,"","","","",98,"2022-07-13 09:19:02","","10.1002/widm.1340","","",,,,,42,14.00,11,4,3,"The increasing computational power and proliferation of big data are now empowering Artificial Intelligence (AI) to achieve massive adoption and applicability in many fields. The lack of explanation when it comes to the decisions made by today's AI algorithms is a major drawback in critical decision‐making systems. For example, deep learning does not offer control or reasoning over its internal processes or outputs. More importantly, current black‐box AI implementations are subject to bias and adversarial attacks that may poison the learning or the inference processes. Explainable AI (XAI) is a new trend of AI algorithms that provide explanations of their AI decisions. In this paper, we propose a framework for achieving a more trustworthy and XAI by leveraging features of blockchain, smart contracts, trusted oracles, and decentralized storage. We specify a framework for complex AI systems in which the decision outcomes are reached based on decentralized consensuses of multiple AI and XAI predictors. The paper discusses how our proposed framework can be utilized in key application areas with practical use cases.","",""
41,"M. Y. Chia, Yuk Feng Huang, C. Koo, K. F. Fung","Recent Advances in Evapotranspiration Estimation Using Artificial Intelligence Approaches with a Focus on Hybridization Techniques—A Review",2020,"","","","",99,"2022-07-13 09:19:02","","10.3390/agronomy10010101","","",,,,,41,20.50,10,4,2,"Difficulties are faced when formulating hydrological processes, including that of evapotranspiration (ET). Conventional empirical methods for formulating these possess some shortcomings. The artificial intelligence approach emerges as the best possible solution to map the relationships between climatic parameters and ET, even with limited knowledge of the interactions between variables. This review presents the state-of-the-art application of artificial intelligence models in ET estimation, along with different types and sources of data. This paper discovers the most significant climatic parameters for different climate patterns. The characteristics of the basic artificial intelligence models are also explored in this review. To overcome the pitfalls of the individual models, hybrid models which use techniques such as data fusion and ensemble modeling, data decomposition as well as remote sensing-based hybridization, are introduced. In particular, the principles and applications of the hybridization techniques, as well as their combinations with basic models, are explained. The review covers most of the related and excellent papers published from 2011 to 2019 to keep its relevancy in terms of time frame and field of study. Guidelines for the future prospects of ET estimation in research are advocated. It is anticipated that such work could contribute to the development of agriculture-based economy.","",""
41,"Alexander Campolo, K. Crawford","Enchanted Determinism: Power without Responsibility in Artificial Intelligence",2020,"","","","",100,"2022-07-13 09:19:02","","10.17351/ests2020.277","","",,,,,41,20.50,21,2,2,"Deep learning techniques are growing in popularity within the field of artificial intelligence (AI). These approaches identify patterns in large scale datasets, and make classifications and predictions, which have been celebrated as more accurate than those of humans. But for a number of reasons, including nonlinear path from inputs to outputs, there is a dearth of theory that can explain why deep learning techniques work so well at pattern detection and prediction. Claims about “superhuman” accuracy and insight, paired with the inability to fully explain how these results are produced, form a discourse about AI that we call enchanted determinism . To analyze enchanted determinism, we situate it within a broader epistemological diagnosis of modernity: Max Weber’s theory of disenchantment. Deep learning occupies an ambiguous position in this framework. On one hand, it represents a complex form of technological calculation and prediction, phenomena Weber associated with disenchantment. On the other hand, both deep learning experts and observers deploy enchanted, magical discourses to describe these systems’ uninterpretable mechanisms and counter-intuitive behavior. The combination of predictive accuracy and mysterious or unexplainable properties results in myth-making about deep learning’s transcendent, superhuman capacities, especially when it is applied in social settings. We analyze how discourses of magical deep learning produce techno-optimism, drawing on case studies from game-playing, adversarial examples, and attempts to infer sexual orientation from facial images. Enchantment shields the creators of these systems from accountability while its deterministic, calculative power intensifies social processes of classification and control.","",""
41,"Alexander Campolo, K. Crawford","Enchanted Determinism: Power without Responsibility in Artificial Intelligence",2020,"","","","",101,"2022-07-13 09:19:02","","10.17351/ests2020.277","","",,,,,41,20.50,21,2,2,"Deep learning techniques are growing in popularity within the field of artificial intelligence (AI). These approaches identify patterns in large scale datasets, and make classifications and predictions, which have been celebrated as more accurate than those of humans. But for a number of reasons, including nonlinear path from inputs to outputs, there is a dearth of theory that can explain why deep learning techniques work so well at pattern detection and prediction. Claims about “superhuman” accuracy and insight, paired with the inability to fully explain how these results are produced, form a discourse about AI that we call enchanted determinism . To analyze enchanted determinism, we situate it within a broader epistemological diagnosis of modernity: Max Weber’s theory of disenchantment. Deep learning occupies an ambiguous position in this framework. On one hand, it represents a complex form of technological calculation and prediction, phenomena Weber associated with disenchantment. On the other hand, both deep learning experts and observers deploy enchanted, magical discourses to describe these systems’ uninterpretable mechanisms and counter-intuitive behavior. The combination of predictive accuracy and mysterious or unexplainable properties results in myth-making about deep learning’s transcendent, superhuman capacities, especially when it is applied in social settings. We analyze how discourses of magical deep learning produce techno-optimism, drawing on case studies from game-playing, adversarial examples, and attempts to infer sexual orientation from facial images. Enchantment shields the creators of these systems from accountability while its deterministic, calculative power intensifies social processes of classification and control.","",""
10,"Régis Pierrard, J. Poli, C. Hudelot","Learning Fuzzy Relations and Properties for Explainable Artificial Intelligence",2018,"","","","",102,"2022-07-13 09:19:02","","10.1109/FUZZ-IEEE.2018.8491538","","",,,,,10,2.50,3,3,4,"The goal of explainable artificial intelligence is to solve problems in a way that humans can understand how it does it. However, few approaches have been proposed so far and some of them lay more emphasis on interpretability than on explainability. In this paper, we propose an approach that is based on learning fuzzy relations and fuzzy properties. We extract frequent relations from a dataset to generate an explained decision. Our approach can deal with different problems, such as classification or annotation. A model was built to perform explained classification on a toy dataset that we generated. It managed to correctly classify examples while providing convincing explanations. A few areas for improvement have been spotted, such as the need to filter relations and properties before or while learning them in order to avoid useless computations.","",""
4,"Karim Lekadira, Richard Osuala, C. Gallin, Noussair Lazrak, Kaisar Kushibar, G. Tsakou, Susanna Auss'o, Leonor Cerd'a Alberich, K. Marias, Manolis Tskinakis, S. Colantonio, Nickolas Papanikolaou, Zohaib Salahuddin, H. Woodruff, P. Lambin, L. Mart'i-Bonmat'i","FUTURE-AI: Guiding Principles and Consensus Recommendations for Trustworthy Artificial Intelligence in Medical Imaging",2021,"","","","",103,"2022-07-13 09:19:02","","","","",,,,,4,4.00,0,16,1,"The recent advancements in artificial intelligence (AI) combined with the extensive amount of data generated by today’s clinical systems, has led to the development of imaging AI solutions across the whole value chain of medical imaging, including image reconstruction, medical image segmentation, image-based diagnosis and treatment planning. Notwithstanding the successes and future potential of AI in medical imaging, many stakeholders are concerned of the potential risks and ethical implications of imaging AI solutions, which are perceived as complex, opaque, and difficult to comprehend, utilise, and trust in critical clinical applications. Despite these concerns and risks, there are currently no concrete guidelines and best practices for guiding future AI developments in medical imaging towards increased trust, safety and adoption. To bridge this gap, this paper introduces a careful selection of guiding principles drawn from the accumulated experiences, consensus, and best practices from five large European projects on AI in Health Imaging. These guiding principles are named FUTURE-AI and its building blocks consist of (i) Fairness, (ii) Universality, (iii) Traceability, (iv) Usability, (v) Robustness and (vi) Explainability. In a step-by-step approach, these guidelines are further translated into a framework of concrete recommendations for specifying, developing, evaluating, and deploying technically, clinically and ethically trustworthy AI solutions into clinical practice.","",""
5,"Ayodeji Oseni, Nour Moustafa, H. Janicke, Peng Liu, Z. Tari, A. Vasilakos","Security and Privacy for Artificial Intelligence: Opportunities and Challenges",2021,"","","","",104,"2022-07-13 09:19:02","","","","",,,,,5,5.00,1,6,1,"The increased adoption of Artificial Intelligence (AI) presents an opportunity to solve many socio-economic and environmental challenges; however, this cannot happen without securing AI-enabled technologies. In recent years, most AI models are vulnerable to advanced and sophisticated hacking techniques. This challenge has motivated concerted research efforts into adversarial AI, with the aim of developing robust machine and deep learning models that are resilient to different types of adversarial scenarios. In this paper, we present a holistic cyber security review that demonstrates adversarial attacks against AI applications, including aspects such as adversarial knowledge and capabilities, as well as existing methods for generating adversarial examples and existing cyber defence models. We explain mathematical AI models, especially new variants of reinforcement and federated learning, to demonstrate how attack vectors would exploit vulnerabilities of AI models. We also propose a systematic framework for demonstrating attack techniques against AI applications, and reviewed several cyber defences that would protect the AI applications against those attacks. We also highlight the importance of understanding the adversarial goals and their capabilities, especially the recent attacks against industry applications, to develop adaptive defences that assess to secure AI applications. Finally, we describe the main challenges and future research directions in the domain of security and privacy of AI technologies.","",""
29,"Konstantin D. Pandl, Scott Thiebes, Manuel Schmidt-Kraepelin, A. Sunyaev","On the Convergence of Artificial Intelligence and Distributed Ledger Technology: A Scoping Review and Future Research Agenda",2020,"","","","",105,"2022-07-13 09:19:02","","10.1109/ACCESS.2020.2981447","","",,,,,29,14.50,7,4,2,"Developments in artificial intelligence (AI) and distributed ledger technology (DLT) currently lead to lively debates in academia and practice. AI processes data to perform tasks that were previously thought possible only for humans. DLT has the potential to create consensus over data among a group of participants in untrustworthy environments. In recent research, both technologies are used in similar and even the same systems. This can lead to a convergence of AI and DLT, which in the past, has paved the way for major innovations of other information technologies. Previous work highlights several potential benefits of a convergence of AI and DLT but only provides a limited theoretical framework to describe upcoming real-world integration cases of both technologies. In this research, we review and synthesize extant research on integrating AI with DLT and vice versa to rigorously develop a future research agenda on the convergence of both technologies. In terms of integrating AI with DLT, we identified research opportunities in the areas of secure DLT, automated referee and governance, and privacy-preserving personalization. With regard to integrating DLT with AI, we identified future research opportunities in the areas of decentralized computing for AI, secure data sharing and marketplaces, explainable AI, and coordinating devices. In doing so, this research provides a four-fold contribution. First, it is not constrained to blockchain but instead investigates the broader phenomenon of DLT. Second, it considers the reciprocal nature of a convergence of AI and DLT. Third, it bridges the gap between theory and practice by helping researchers active in AI or DLT to overcome current limitations in their field, and practitioners to develop systems along with the convergence of both technologies. Fourth, it demonstrates the feasibility of applying the convergence concept to research on AI and DLT.","",""
3,"T. Kaur, Anirudra Diwakar, Kirandeep, Pranav Mirpuri, M. Tripathi, P. Chandra, T. Gandhi","Artificial Intelligence in Epilepsy",2021,"","","","",106,"2022-07-13 09:19:02","","10.4103/0028-3886.317233","","",,,,,3,3.00,0,7,1,"Background: The study of seizure patterns in electroencephalography (EEG) requires several years of intensive training. In addition, inadequate training and human error may lead to misinterpretation and incorrect diagnosis. Artificial intelligence (AI)-based automated seizure detection systems hold an exciting potential to create paradigms for proper diagnosis and interpretation. AI holds the promise to transform healthcare into a system where machines and humans can work together to provide an accurate, timely diagnosis, and treatment to the patients. Objective: This article presents a brief overview of research on the use of AI systems for pattern recognition in EEG for clinical diagnosis. Material and Methods: The article begins with the need for understanding nonstationary signals such as EEG and simplifying their complexity for accurate pattern recognition in medical diagnosis. It also explains the core concepts of AI, machine learning (ML), and deep learning (DL) methods. Results and Conclusions: In this present context of epilepsy diagnosis, AI may work in two ways; first by creating visual representations (e.g., color-coded paradigms), which allow persons with limited training to make a diagnosis. The second is by directly explaining a complete automated analysis, which of course requires more complex paradigms than the previous one. We also clarify that AI is not about replacing doctors and strongly emphasize the need for domain knowledge in building robust AI models that can work in real-time scenarios rendering good detection accuracy in a minimum amount of time.","",""
3,"Amy Papadopoulos, J. Salinas, Cindy Crump","Computational modeling approaches to characterize risk and achieve safe, effective, and trusted designs in the development of artificial intelligence and autonomous closed-loop medical systems",2021,"","","","",107,"2022-07-13 09:19:02","","10.1117/12.2586101","","",,,,,3,3.00,1,3,1,"While software using artificial intelligence and machine learning (AI/ML) is pervasive in many areas of society today, the use of these technologies to diagnose and treat medical conditions is limited due to a number of challenges associated with the trustworthiness of the results. This may include the inability to fully explain how an algorithm works inherent to the black-box nature of the system. Additionally, AI/ML may create a potential for bias and artifacts that cannot be validated due to the same limitations. In a medical application, the lack of transparency in how the system operates may lead to a loss of trust by users. Bayesian approaches that use computational modeling to quantify the level of uncertainty in a given result may provide a path towards improved confidence and use. In this paper, evidence from studies in a range of medical applications is presented and discussed, showing how Bayesian approaches can help to foster trust. A retrospective study using a publicly available dataset explored the feasibility of creating predictive models for early intervention in a Type 1 diabetes population. Creating the perfect model was not the goal of the exercise, rather the study aimed to demonstrate how Bayesian methods could be used to identify areas of uncertainty during model development. Feature selection was based on analytical assessment of various patterns found in the data. Models were trained, validated, and tested, generating uncertainty estimates. A two-feature Gaussian Naïve Bayes (GNB) model, using the previous five minutes and ten minutes of blood glucose values, showed similar results for predictive accuracy as a threefeature model that included average change over the preceding 30 minutes. The two-feature model was selected because it allowed for a more easily understood visualization of uncertainty. The 2-feature GNB achieved an AUC = .94. The model showed good sensitivity for exceeding the < 180 mg/dl limit, obtaining threshold prediction = 89.8% and normal range prediction = 90.8%. The sensitivity was lower for the < 70 mg/dl limit, attaining a sensitivity = 77.5%. Posterior probabilities showed differing levels of uncertainty in the prediction of high and low out-of-range conditions. The model demonstrated the feasibility of providing robust parameter estimates. Bayesian machine learning approaches to model uncertainty may improve the transparency, explainability, and applicability of AI/ML in medical treatment, realizing the promise to improve patient safety and outcomes.","",""
3,"T. Sing, J. Yang, S. Yu","Boosted Tree Ensembles for Artificial Intelligence Based Automated Valuation Models (AI-AVM)",2021,"","","","",108,"2022-07-13 09:19:02","","10.1007/s11146-021-09861-1","","",,,,,3,3.00,1,3,1,"","",""
1,"Pingping Sun, Lingang Gu","Fuzzy knowledge graph system for artificial intelligence-based smart education",2021,"","","","",109,"2022-07-13 09:19:02","","10.3233/JIFS-189332","","",,,,,1,1.00,1,2,1,"Fuzzy knowledge graph system is a semantic network that reveals the relationships between entities, and a tool or methodology that can formally describe things in the real world and their relationships. Smart education is an educational concept or model that uses advanced information technology to build a smart environment, integrates theory and practice to build an educational framework for information age, and provides paths to practice it. Artificial intelligence (AI) is a comprehensive discipline developed by the interpenetration of computer science, cybernetics, information theory, linguistics, neurophysiology and other disciplines, which is a direction for the development of information technology in the future. On the basis of summarizing and analyzing of previous research works, this paper expounded the research status and significance of AI technology, elaborated the development background, current status and future challenges of the construction and application of fuzzy knowledge graph system for smart education, introduced the methods and principles of data acquisition methods and digitalized apprenticeship, realized the process design, information extraction, entity recognition and relationship mining of smart education, constructed a systematic framework for fuzzy knowledge graph, and analyzed the high-quality resources sharing and personalized service of AI-assisted smart education, discussed automatic knowledge acquisition and fusion of fuzzy knowledge graph, performed co-occurrence relationship analysis, and finally conducted application case analysis. The results show that the smart education knowledge graph for AI-assisted smart education can integrate teaching experience and domain knowledge of discipline experts, enhance explainable and robust machine intelligence for AI-assisted smart education, and provide data-driven and knowledge-driven information processing methods; it can also discover the analysis hotspots and main content of research objects through clustering of high-frequency topic words, reveal the corresponding research structure in depth, and then systematically explore its research dimensions, subject background and theoretical basis.","",""
1,"R. Joshi, Neeraj Kumar","Artificial Intelligence for Autonomous Molecular Design: A Perspective",2021,"","","","",110,"2022-07-13 09:19:02","","10.3390/molecules26226761","","",,,,,1,1.00,1,2,1,"Domain-aware artificial intelligence has been increasingly adopted in recent years to expedite molecular design in various applications, including drug design and discovery. Recent advances in areas such as physics-informed machine learning and reasoning, software engineering, high-end hardware development, and computing infrastructures are providing opportunities to build scalable and explainable AI molecular discovery systems. This could improve a design hypothesis through feedback analysis, data integration that can provide a basis for the introduction of end-to-end automation for compound discovery and optimization, and enable more intelligent searches of chemical space. Several state-of-the-art ML architectures are predominantly and independently used for predicting the properties of small molecules, their high throughput synthesis, and screening, iteratively identifying and optimizing lead therapeutic candidates. However, such deep learning and ML approaches also raise considerable conceptual, technical, scalability, and end-to-end error quantification challenges, as well as skepticism about the current AI hype to build automated tools. To this end, synergistically and intelligently using these individual components along with robust quantum physics-based molecular representation and data generation tools in a closed-loop holds enormous promise for accelerated therapeutic design to critically analyze the opportunities and challenges for their more widespread application. This article aims to identify the most recent technology and breakthrough achieved by each of the components and discusses how such autonomous AI and ML workflows can be integrated to radically accelerate the protein target or disease model-based probe design that can be iteratively validated experimentally. Taken together, this could significantly reduce the timeline for end-to-end therapeutic discovery and optimization upon the arrival of any novel zoonotic transmission event. Our article serves as a guide for medicinal, computational chemistry and biology, analytical chemistry, and the ML community to practice autonomous molecular design in precision medicine and drug discovery.","",""
2,"D. Rawat","Secure and trustworthy machine learning/artificial intelligence for multi-domain operations",2021,"","","","",111,"2022-07-13 09:19:02","","10.1117/12.2592860","","",,,,,2,2.00,2,1,1,"Machine Learning (ML) algorithms and Artificial Intelligence (AI) are now regarded as very useful for data-driven applications including resilient multi-domain operations. However, ML algorithms and AI systems can be controlled, dodged, biased, and misled through flawed learning models and input data, they need robust security features and trust. Furthermore, ML algorithms and AI systems add challenges when we have (unlabeled/labeled) sparse/small data or big data for training and evaluation. It is very important to design, evaluate and test ML algorithms and AI systems that produce reliable, robust, trustworthy, explainable, and fair/unbiased outcomes to make them acceptable and reliable in mission critical multi-domain operations. ML algorithms rely on data and work on the principle of ``Garbage In, Garbage Out,"" which means that if the input data to learning model is corrupted or compromised, the outcomes of the ML/AI would not be optimal, reliable and trustworthy.","",""
2,"Zahraa Bassyouni, I. Elhajj","Augmented Reality Meets Artificial Intelligence in Robotics: A Systematic Review",2021,"","","","",112,"2022-07-13 09:19:02","","10.3389/frobt.2021.724798","","",,,,,2,2.00,1,2,1,"Recently, advancements in computational machinery have facilitated the integration of artificial intelligence (AI) to almost every field and industry. This fast-paced development in AI and sensing technologies have stirred an evolution in the realm of robotics. Concurrently, augmented reality (AR) applications are providing solutions to a myriad of robotics applications, such as demystifying robot motion intent and supporting intuitive control and feedback. In this paper, research papers combining the potentials of AI and AR in robotics over the last decade are presented and systematically reviewed. Four sources for data collection were utilized: Google Scholar, Scopus database, the International Conference on Robotics and Automation 2020 proceedings, and the references and citations of all identified papers. A total of 29 papers were analyzed from two perspectives: a theme-based perspective showcasing the relation between AR and AI, and an application-based analysis highlighting how the robotics application was affected. These two sections are further categorized based on the type of robotics platform and the type of robotics application, respectively. We analyze the work done and highlight some of the prevailing limitations hindering the field. Results also explain how AR and AI can be combined to solve the model-mismatch paradigm by creating a closed feedback loop between the user and the robot. This forms a solid base for increasing the efficiency of the robotic application and enhancing the user’s situational awareness, safety, and acceptance of AI robots. Our findings affirm the promising future for robust integration of AR and AI in numerous robotic applications.","",""
1,"Vitor Bento, Manoela Kohler, Pedro Diaz, L. Mendoza, Marco Aurélio Cavalcanti Pacheco","Improving deep learning performance by using Explainable Artificial Intelligence (XAI) approaches",2021,"","","","",113,"2022-07-13 09:19:02","","10.1007/s44163-021-00008-y","","",,,,,1,1.00,0,5,1,"","",""
1,"Ke Zhang, Peidong Xu, Tianlu Gao, Jun Zhang","A Trustworthy Framework of Artificial Intelligence for Power Grid Dispatching Systems",2021,"","","","",114,"2022-07-13 09:19:02","","10.1109/DTPI52967.2021.9540198","","",,,,,1,1.00,0,4,1,"With the widespread application of artificial intelligence (AI) technologies in power systems, the properties of lack of reliability and transparency for AI technologies have revealed gradually. Here, how to build a trustworthy-AI framework based on the power system is the focus. Due to the multidimensional and heterogeneous information of power grid data, the heterogeneous graph attention network (HGAT) model of power grid dispatching is established, and the corresponding explainer (HGAT-Explainer) for the model of power equipment faults is proposed to provide more favorable support for the trustworthy-AI systems.","",""
1,"S. Greenstein","Preserving the rule of law in the era of artificial intelligence (AI)",2021,"","","","",115,"2022-07-13 09:19:02","","10.1007/S10506-021-09294-4","","",,,,,1,1.00,1,1,1,"","",""
2,"Kamelia Moh’d Khier Al Momani, Abdulnasr Ibrahim Nour, N. Jamaludin, W. Z. W. Zanani Wan Abdullah","Fourth Industrial Revolution, Artificial Intelligence, Intellectual Capital, and COVID-19 Pandemic",2021,"","","","",116,"2022-07-13 09:19:02","","10.1007/978-3-030-72080-3_5","","",,,,,2,2.00,1,4,1,"","",""
2,"J. Arslan, K. Benke","Artificial Intelligence and Telehealth may Provide Early Warning of Epidemics",2021,"","","","",117,"2022-07-13 09:19:02","","10.3389/frai.2021.556848","","",,,,,2,2.00,1,2,1,"The COVID-19 pandemic produced a very sudden and serious impact on public health around the world, greatly adding to the burden of overloaded professionals and national medical systems. Recent medical research has demonstrated the value of using online systems to predict emerging spatial distributions of transmittable diseases. Concerned internet users often resort to online sources in an effort to explain their medical symptoms. This raises the prospect that incidence of COVID-19 may be tracked online by search queries and social media posts analyzed by advanced methods in data science, such as Artificial Intelligence. Online queries can provide early warning of an impending epidemic, which is valuable information needed to support planning timely interventions. Identification of the location of clusters geographically helps to support containment measures by providing information for decision-making and modeling.","",""
2,"P. W. Grimm, Maura R. Grossman, G. Cormack","Artificial Intelligence as Evidence",2021,"","","","",118,"2022-07-13 09:19:02","","","","",,,,,2,2.00,1,3,1,"This article explores issues that govern the admissibility of Artificial Intelligence (“AI”) applications in civil and criminal cases, from the perspective of a federal trial judge and two computer scientists, one of whom also is an experienced attorney. It provides a detailed yet intelligible discussion of what AI is and how it works, a history of its development, and a description of the wide variety of functions that it is designed to accomplish, stressing that AI applications are ubiquitous, both in the private and public sectors. Applications today include: health care, education, employment-related decision-making, finance, law enforcement, and the legal profession. The article underscores the importance of determining the validity of an AI application (i.e., how accurately the AI measures, classifies, or predicts what it is designed to), as well as its reliability (i.e., the consistency with which the AI produces accurate results when applied to the same or substantially similar circumstances), in deciding whether it should be admitted into evidence in civil and criminal cases. The article further discusses factors that can affect the validity and reliability of AI evidence, including bias of various types, “function creep,” lack of transparency and explainability, and the sufficiency of the objective testing of AI applications before they are released for public use. The article next provides an in-depth discussion of the evidentiary principles that govern whether AI evidence should be admitted in court cases, a topic which, at present, is not the subject of comprehensive analysis in decisional law. The focus of this discussion is on providing a step-by-step analysis of the most important issues, and the factors that affect decisions on whether to admit AI evidence. Finally, the article concludes with a discussion of practical suggestions intended to assist lawyers and judges as they are called upon to introduce, object to, or decide on whether to admit AI evidence. 1 Hon. Paul W. Grimm is a United States District Judge for the District of Maryland, and an adjunct professor at both the University of Maryland Carey School of Law and the University of Baltimore School of Law. Maura R. Grossman, J.D., Ph.D., is a Research Professor, and Gordon V. Cormack, Ph.D., is a Professor, in the David R. Cheriton School of Computer Science at the University of Waterloo. Professor Grossman is also an affiliate faculty member at the Vector Institute for Artificial Intelligence. Her work is funded, in part, by the National Sciences and Engineering Council of Canada (“NESERC”). The opinions expressed in this article are the authors’ own, and do not necessarily reflect the views of the institutions or organizations with which they are affiliated. NORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY 10 INTRODUCTION .............................................................................................................. 10 I. WHAT IS “ARTIFICIAL INTELLIGENCE”? .................................................................... 14 II. WHY AI HAS COME TO THE FOREFRONT TODAY ...................................................... 17 III. THE AI TECHNOLOGY LANDSCAPE .......................................................................... 24 IV. USES OF AI IN BUSINESS AND LAW TODAY .............................................................. 32 V. ISSUES RAISED BY THE USE OF AI IN BUSINESS AND LAW TODAY ............................ 41 A. Bias ............................................................................................................... 42 B. Lack of Robust Testing for Validity and Reliability ....................................... 48 C. Failure to Monitor for Function Creep ......................................................... 51 D. Failure to Ensure Data Privacy and Data Protection .................................. 53 E. Lack of Transparency and Explainabilty ....................................................... 60 F. Lack of Accountability ................................................................................... 65 G. Lack of Resilience ......................................................................................... 72 VI. ESTABLISHING VALIDITY AND RELIABILITY ........................................................... 79 A. Testimony, Expert Testimony, or Technology? .............................................. 79 B. Benchmarks and Goodhart’s Law ................................................................. 82 VII. EVIDENTIARY PRINCIPLES THAT SHOULD BE CONSIDERED IN EVALUATING THE ADMISSIBILITY OF AI EVIDENCE IN CIVIL AND CRIMINAL TRIALS .................... 84 A. Adequacy of the Federal Rules of Evidence in Addressing the Admissibility of AI Evidence ......................................................................... 84 B. Relevance ...................................................................................................... 86 C. Authentication of AI Evidence ....................................................................... 90 D. Usefulness of the Daubert Factors in Determining Whether to Admit AI Evidence ....................................................................................................... 95 E. Practice Pointers for Lawyers and Judges .................................................... 97 CONCLUSION ............................................................................................................... 105","",""
2,"Iam Palatnik de Sousa, Marley Vellasco, E. C. Silva","Explainable Artificial Intelligence for Bias Detection in COVID CT-Scan Classifiers",2021,"","","","",119,"2022-07-13 09:19:02","","10.3390/s21165657","","",,,,,2,2.00,1,3,1,"Problem: An application of Explainable Artificial Intelligence Methods for COVID CT-Scan classifiers is presented. Motivation: It is possible that classifiers are using spurious artifacts in dataset images to achieve high performances, and such explainable techniques can help identify this issue. Aim: For this purpose, several approaches were used in tandem, in order to create a complete overview of the classificatios. Methodology: The techniques used included GradCAM, LIME, RISE, Squaregrid, and direct Gradient approaches (Vanilla, Smooth, Integrated). Main results: Among the deep neural networks architectures evaluated for this image classification task, VGG16 was shown to be most affected by biases towards spurious artifacts, while DenseNet was notably more robust against them. Further impacts: Results further show that small differences in validation accuracies can cause drastic changes in explanation heatmaps for DenseNet architectures, indicating that small changes in validation accuracy may have large impacts on the biases learned by the networks. Notably, it is important to notice that the strong performance metrics achieved by all these networks (Accuracy, F1 score, AUC all in the 80 to 90% range) could give users the erroneous impression that there is no bias. However, the analysis of the explanation heatmaps highlights the bias.","",""
24,"I. Cohen","Informed Consent and Medical Artificial Intelligence: What to Tell the Patient?",2020,"","","","",120,"2022-07-13 09:19:02","","10.2139/ssrn.3529576","","",,,,,24,12.00,24,1,2,"Imagine you are a patient who has been diagnosed with prostate cancer. The two main approaches to treating it in the United States are active surveillance versus the surgical option of radical prostatectomy. Your physician recommends the surgical option, and spends considerable time explaining the steps in the surgery, the benefits of (among other things) eliminating the tumor and the risks of (among other things) erectile dysfunction and urinary incontinence after the surgery. What your physician does not tell you is that she has arrived at her recommendation of prostatectomy over active surveillance based on the analysis of an Artificial Intelligence (AI)/Machine Learning (ML) system, which recommended this treatment plan based on analysis of your age, tumor size, and other personal characteristics found in your electronic health record. Has the doctor secured informed consent from a legal perspective? From an ethical perspective? If the doctor actually chose to “overrule” the AI system, and the doctor fails to tell you that, has she violated your legal or ethical right to informed consent? If you were to find out that the AI/ML system was used to make recommendations on your care and no one told you, how would you feel? Well, come to think of it, do you know whether an AI/ML system was used the last time you saw a physician?    This Article, part of a Symposium in the Georgetown Law Journal, is the first to examine in depth how medical AI/ML interfaces with our concept of informed consent. Part I provides a brief primer on medical Artificial Intelligence and Machine Learning. Part II sets out the core and penumbra of U.S. informed consent law and then seeks to determine to what extent AI/ML involvement in a patient’s health should be disclosed under the current doctrine. Part III examines whether the current doctrine “has it right,” examining more openly empirical and normative approaches to the question.    To forefront my conclusions: while there is some play in the joints, my best reading of the existing legal doctrine is that in general, liability will not lie for failing to inform patients about the use of medical AI/ML to help formulate treatment recommendations. There are a few situations where the doctrine may be more capacious, which I try to draw out (such as when patients inquire, when the medical AI/ML is more opaque, when it is given an outsized role in the final decision-making, or when the AI/ML is used to reduce costs rather than improve patient health), though extending it even here is not certain. I also offer some thoughts on the question: if there is room in the doctrine (either via common law or legislative action), what would it be desirable for the doctrine to look like when it comes to medical AI/ML? I also briefly touch on the question of how the doctrine of informed consent should interact with concerns about biased training data for AI/ML.","",""
442,"M. Ridley","Explainable Artificial Intelligence (XAI)",2022,"","","","",121,"2022-07-13 09:19:02","","10.6017/ital.v41i2.14683","","",,,,,442,442.00,442,1,1,"The field of explainable artificial intelligence (XAI) advances techniques, processes, and strategies that provide explanations for the predictions, recommendations, and decisions of opaque and complex machine learning systems. Increasingly academic libraries are providing library users with systems, services, and collections created and delivered by machine learning. Academic libraries should adopt XAI as a tool set to verify and validate these resources, and advocate for public policy regarding XAI that serves libraries, the academy, and the public interest.","",""
0,"Victoria Tucci, J. Saary, Thomas E. Doyle","Factors influencing trust in medical artificial intelligence for healthcare professionals: a narrative review",2021,"","","","",122,"2022-07-13 09:19:02","","10.21037/jmai-21-25","","",,,,,0,0.00,0,3,1,"Objective: We performed a comprehensive review of the literature to better understand the trust dynamics between medical artificial intelligence (AI) and healthcare expert end-users. We explored the factors that influence trust in these technologies and how they compare to established concepts of trust in the engineering discipline. By identifying the qualitatively and quantitatively assessed factors that influence trust in medical AI, we gain insight into understanding how autonomous systems can be optimized during the development phase to improve decision-making support and clinician-machine teaming. This facilitates an enhanced understanding of the qualities that healthcare professional users seek in AI to consider it trustworthy. We also highlight key considerations for promoting on-going improvement of trust in autonomous medical systems to support the adoption of medical technologies into practice. Background: decision support systems introduces challenges and barriers to adoption and implementation into clinical practice. Methods: We searched databases including, Ovid MEDLINE, Ovid EMBASE, Clarivate Web of Science, and Google Scholar, as well as gray literature, for publications from 2000 to July 15, 2021, that reported features of AI-based diagnostic and clinical decision support systems that contribute to enhanced end-user trust. Papers discussing implications and applications of medical AI in clinical practice were also recorded. Results were based on the quantity of papers that discussed each trust concept, either quantitatively or qualitatively, using frequency of concept commentary as a proxy for importance of a respective concept. Conclusions: Explainability, transparency, interpretability, usability, and education are among the key identified factors thought to influence a healthcare professionals’ trust in medical AI and enhance clinician-machine teaming in critical decision-making healthcare environments. We also identified the need to better evaluate and incorporate other critical factors to promote trust by consulting medical professionals when developing AI systems for clinical decision-making and diagnostic support.","",""
0,"Abdulraqeb Alhammadi, Ayman A. El-Saleh, Ibraheem Shayea","MOS Prediction for Mobile Broadband Networks Using Bayesian Artificial Intelligence",2021,"","","","",123,"2022-07-13 09:19:02","","10.1109/ICAICST53116.2021.9497834","","",,,,,0,0.00,0,3,1,"Mobile broadband (MBB) networks are growing fast with supporting high-speed internet access. Fifth-generation networks promise an enhanced MBB that offers a high-speed data rate and video streaming with ultra-low latency. Thus, monitoring the level quality of these services supported by network providers becomes essential. Mobile network operators continuously optimize their network performance to provide a better quality of service and quality of experience. Moreover, artificial intelligence has been used considerably in optimizations to efficiently meet the requirements of future mobile networks. In this paper, we propose a Bayesian network model to predict the minimum opinion score (MOS), which contributes to evaluating the network performance of video streaming services. The proposed model depends on several input data, namely, bite rate, stalling load, and round-trip time. The predicted MOS depends on prior probability distributions to generate posterior probabilities. The predicted MOS depends on these input data. Results demonstrate that the proposed model achieves a high prediction accuracy of 86%, with a mean square error of 0.34. The proposed model also has a robust performance design through various testing methods.","",""
0,"S. Stanišić, M. Perišić, G. Jovanović, D. Maletic, D. Vudragovic, Anja Vranic, A. Stojić","What Information on Volatile Organic Compounds Can Be Obtained from the Data of a Single Measurement Site Through the Use of Artificial Intelligence?",2021,"","","","",124,"2022-07-13 09:19:02","","10.1007/978-3-030-72711-6_12","","",,,,,0,0.00,0,7,1,"","",""
0,"Xiaohong W. Gao, B. Braden","Artificial intelligence in endoscopy: The challenges and future directions",2021,"","","","",125,"2022-07-13 09:19:02","","10.37126/aige.v2.i4.117","","",,,,,0,0.00,0,2,1,"Artificial intelligence based approaches, in particular deep learning, have achieved state-of-the-art performance in medical fields with increasing number of software systems being approved by both Europe and United States. This paper reviews their applications to early detection of oesophageal cancers with a focus on their advantages and pitfalls. The paper concludes with future recommendations towards the development of a real-time, clinical implementable, interpretable and robust diagnosis support systems.","",""
0,"I. Mohammed","ARTIFICIAL INTELLIGENCE: THE KEY TO SELF-DRIVING IDENTITY GOVERNANCE",2021,"","","","",126,"2022-07-13 09:19:02","","","","",,,,,0,0.00,0,1,1,"The main goal of this article is to examine how artificial intelligence is playing an increasingly important role in advancing identity governance. The speed of global digital change is accelerating, and businesses are under increasing pressure to guarantee that their technology can keep up with the times. Today's headlines often include stories about high-profile data leakage, which have a significant impact on a company's image and financial health [1]. The moment has come for businesses to invest in AIpowered identity security to remain abreast of security and compliance risks. Information technology, a shifting workforce, and an onslaught of compliance regulations have brought an unprecedented number of users, points of access, apps, and data, to the point that IT departments are struggling to stay up [1]. A human-centric approach to identity security can only scale so far, and with it comes inaccuracy in risk identification. Security requirements are becoming more complicated, decentralized, and integrated with business operations, owing to new methods of working enabled by cloud technology [2]. This implies that robust identity governance and access control are more critical than ever. In this paper, we examine the current status of the information technology environment and explain how artificial intelligence will help companies address their present issues relating to identity governance.","",""
0,"Yalin Dong","Application of artificial intelligence in clothing intelligence manufacturing",2021,"","","","",127,"2022-07-13 09:19:02","","10.1109/cisai54367.2021.00172","","",,,,,0,0.00,0,1,1,"At present, the rapid development of science and technology in China not only provides solid scientific and technological support for the development of various intelligent technology products, but also opens up a new road for intelligent clothing manufacturing. Many aspects of artificial intelligence, such as industrial chain and computer application, can make a contribution to clothing intelligent manufacturing. This paper first introduces the relevant information of artificial intelligence, then explains the specific application direction of artificial intelligence in clothing intelligence manufacturing, and finally looks forward to its application prospects.","",""
0,"Sai Prasanth Kadiyala, Wai Lok Woo","Flood Prediction and Analysis on the Relevance of Features using Explainable Artificial Intelligence",2021,"","","","",128,"2022-07-13 09:19:02","","10.1145/3516529.3516530","","",,,,,0,0.00,0,2,1,"This paper presents flood prediction models for the state of Kerala in India by analyzing the monthly rainfall data and applying machine learning algorithms including Logistic Regression, K-Nearest Neighbors, Decision Trees, Random Forests, and Support Vector Machine. Although these models have shown high accuracy prediction of the occurrence of flood in a particular year, they do not quantitatively and qualitatively explain the prediction decision. This paper shows how the background features are learned that contributed to the prediction decision and further extended to explain the models with the development of explainable artificial intelligence modules such as SHAP and LIME. The obtained results have confirmed the validity of the findings uncovered by the explainer modules basing on the historical flood monthly rainfall data in Kerala","",""
0,"Abdulsadek Hassan","The Usage of Artificial Intelligence in Digital Marketing: A Review",2021,"","","","",129,"2022-07-13 09:19:02","","10.1007/978-3-030-72080-3_20","","",,,,,0,0.00,0,1,1,"","",""
0,"R. Sheh","Explainable Artificial Intelligence Requirements for Safe, Intelligent Robots",2021,"","","","",130,"2022-07-13 09:19:02","","10.1109/ISR50024.2021.9419498","","",,,,,0,0.00,0,1,1,"While requirements for robot performance to perform a task are generally well understood, the requirements around the explanatory capabilities of these systems are often at best an afterthought. This results in a dangerous situation where neither users nor experts can predict situations where the robot will or will not work, nor understand what causes failures and unexpected behaviour. In this paper, we discuss and survey the field of Explainable Artificial Intelligence, as it relates to the generation of requirements for the development of safe, intelligent robots. We then present a categorisation of explanatory capabilities and requirements that aims to help users and developers alike to ensure an appropriate match between the types of explanations that a given application requires, and the capabilities of various underlying AI techniques.","",""
128,"Kacper Sokol, Peter A. Flach","Explainability fact sheets: a framework for systematic assessment of explainable approaches",2019,"","","","",131,"2022-07-13 09:19:02","","10.1145/3351095.3372870","","",,,,,128,42.67,64,2,3,"Explanations in Machine Learning come in many forms, but a consensus regarding their desired properties is yet to emerge. In this paper we introduce a taxonomy and a set of descriptors that can be used to characterise and systematically assess explainable systems along five key dimensions: functional, operational, usability, safety and validation. In order to design a comprehensive and representative taxonomy and associated descriptors we surveyed the eXplainable Artificial Intelligence literature, extracting the criteria and desiderata that other authors have proposed or implicitly used in their research. The survey includes papers introducing new explainability algorithms to see what criteria are used to guide their development and how these algorithms are evaluated, as well as papers proposing such criteria from both computer science and social science perspectives. This novel framework allows to systematically compare and contrast explainability approaches, not just to better understand their capabilities but also to identify discrepancies between their theoretical qualities and properties of their implementations. We developed an operationalisation of the framework in the form of Explainability Fact Sheets, which enable researchers and practitioners alike to quickly grasp capabilities and limitations of a particular explainable method. When used as a Work Sheet, our taxonomy can guide the development of new explainability approaches by aiding in their critical evaluation along the five proposed dimensions.","",""
318,"David Gunning","DARPA's explainable artificial intelligence (XAI) program",2019,"","","","",132,"2022-07-13 09:19:02","","10.1145/3301275.3308446","","",,,,,318,106.00,318,1,3,"The DARPA's Explainable Artificial Intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. This talk will summarize the XAI program and present highlights from these Phase 1 evaluations.","",""
19,"Mitsuru Igami","Artificial intelligence as structural estimation: Deep Blue, Bonanza, and AlphaGo",2017,"","","","",133,"2022-07-13 09:19:02","","10.1093/ectj/utaa005","","",,,,,19,3.80,19,1,5,"  This article clarifies the connections between certain algorithms to develop artificial intelligence (AI) and the econometrics of dynamic structural models, with concrete examples of three 'game AIs'. Chess-playing Deep Blue is a calibrated value function, whereas shogi-playing Bonanza is an estimated value function via Rust’s nested fixed-point (NFXP) method. AlphaGo’s 'supervised-learning policy network' is a deep-neural-network implementation of the conditional-choice-probability (CCP) estimation reminiscent of Hotz and Miller's first step; the construction of its 'reinforcement-learning value network' is analogous to their conditional choice simulation (CCS). I then explain the similarities and differences between AI-related methods and structural estimation more generally, and suggest areas of potential cross-fertilization.","",""
19,"Adarsh Ghosh, D. Kandasamy","Interpretable Artificial Intelligence: Why and When.",2020,"","","","",134,"2022-07-13 09:19:02","","10.2214/ajr.19.22145","","",,,,,19,9.50,10,2,2,"OBJECTIVE. The purpose of this article is to discuss the problem of interpretability of artificial intelligence (AI) and highlight the need for continuing scientific discovery using AI algorithms to deal with medical big data. CONCLUSION. A plethora of AI algorithms are currently being used in medical research, but the opacity of these algorithms makes their clinical implementation a dilemma. Clinical decision making cannot be assigned to something that we do not understand. Therefore, AI research should not be limited to reporting accuracy and sensitivity but, rather, should try to explain the underlying reasons for the predictions, in an attempt to enrich biologic understanding and knowledge.","",""
0,"","Neural Network Training Using Genetic Algorithms Series In Machine Perception And Artificial Intelligence",2021,"","","","",135,"2022-07-13 09:19:02","","","","",,,,,0,0.00,0,0,1,"Knowledge-Based Intelligent Information and Engineering Systems 2Nature-inspired Methods in Chemometrics: Genetic Algorithms and Artificial Neural NetworksParallel Implementations of Backpropagation Neural Networks on TransputersEvolutionary Algorithms and Neural NetworksTraining Neural Networks Using Hybrids with Genetic AlgorithmsNeural Network Training Using Genetic AlgorithmsGene Expression ProgrammingTraining a Neural Network with a Genetic AlgorithmMethods and Applications of Artificial IntelligenceClassification and Learning Using Genetic AlgorithmsPractical Computer Vision Applications Using Deep Learning with CNNsIntelligent Hybrid SystemsNeurogenetic LearningAdvances in Neural Networks ISNN 2007Hybrid Intelligent SystemsEncyclopedia of Computer Science and TechnologyMachine LearningUsing a Genetic Algorithm in Training an Artificial Neural Network to Implement the XOR FunctionGenetic and Evolutionary Computation — GECCO 2004Handbook of Fuzzy ComputationNeural Network Data Analysis Using SimulnetTMArtificial Neural Nets and Genetic AlgorithmsApplied Soft Computing Technologies: The Challenge of ComplexityGenetic Algorithm for Artificial Neural Network Training for the Purpose of Automated Part RecognitionNEURAL NETWORKS, FUZZY LOGIC AND GENETIC ALGORITHMAutomatic Generation of Neural Network Architecture Using Evolutionary ComputationPGANETThe Sixth International Symposium on Neural Networks (ISNN 2009)Evolutionary Machine Learning TechniquesModeling Decisions for Artificial IntelligenceEmpirical Studies on the Utility of Genetic Algorithms for Training and Designing of Neural NetworksTraining Neural Networks Using Genetic AlgorithmsTraining feedforward neural networks using genetic algorithmsMetaheuristic Procedures for Training Neural NetworksArtificial Neural Nets and Genetic AlgorithmsNature-Inspired Computing: Concepts, Methodologies, Tools, and ApplicationsApplications of Evolutionary ComputingArtificial Intelligence and CreativityArtificial Neural Nets and Genetic AlgorithmsDeep Learning Using Genetic Algorithms Creativity is one of the least understood aspects of intelligence and is often seen as `intuitive' and not susceptible to rational enquiry. Recently, however, there has been a resurgence of interest in the area, principally in artificial intelligence and cognitive science, but also in psychology, philosophy, computer science, logic, mathematics, sociology, and architecture and design. This volume brings this work together and provides an overview of this rapidly developing field. It addresses a range of issues. Can computers be creative? Can they help us to understand human creativity? How can artificial intelligence (AI) enhance human creativity? How, in particular, can it contribute to the `sciences of the artificial', such as design? Does the new wave of AI (connectionism, geneticism and artificial life) offer more promise in these areas than classical, symbol-handling AI? What would the implications be for AI and cognitive science if computers could not be creative? These issues are explored in five interrelated parts, each of which is introducted and explained by a leading figure in the field. Prologue (Margaret Boden) Part I: Foundational Issues (Terry Dartnall) Part II: Creativity and Cognition (Graeme S. Halford and Robert Levinson) Part III: Creativity and Connectionism (Chris Thornton) Part IV: Creativity and Design (John Gero) Part V: Human Creativity Enhancement (Ernest Edmonds) Epilogue (Douglas Hofstadter) For researchers in AI, cognitive science, computer science, philosophy, psychology, mathematics, logic, sociology, and architecture and design; and anyone interested in the rapidly growing field of artificial intelligence and creativity.From the contents: Neural networks – theory and applications: NNs (= neural networks) classifier on continuous data domains– quantum associative memory – a new class of neuron-like discrete filters to image processing – modular NNs for improving generalisation properties – presynaptic inhibition modelling for image processing application – NN recognition system for a curvature primal sketch – NN based nonlinear temporalspatial noise rejection system – relaxation rate for improving Hopfield network – Oja's NN and influence of the learning gain on its dynamics Genetic algorithms – theory and applications: transposition: a biological-inspired mechanism to use with GAs (= genetic algorithms) – GA for decision tree induction – optimising decision classifications using GAs – scheduling tasks with intertask communication onto multiprocessors by GAs – design of robust networks with GA – effect of degenerate coding on GAs – multiple traffic signal control using a GA – evolving musical harmonisation – niched-penalty approach for constraint handling in GAs – GA with dynamic population size – GA with dynamic niche clustering for multimodal function optimisation Soft computing and uncertainty: self-adaptation of evolutionary constructed decision trees by information spreading – evolutionary programming of near optimal NNsArtificial neural networks and genetic algorithms both are areas of research","",""
44,"Chiara Longoni, Luca Cian","Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect",2020,"","","","",136,"2022-07-13 09:19:02","","10.1177/0022242920957347","","",,,,,44,22.00,22,2,2,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).","",""
30,"L. Longo, R. Goebel, F. Lécué, Peter Kieseberg, Andreas Holzinger","Explainable Artificial Intelligence: Concepts, Applications, Research Challenges and Visions",2020,"","","","",137,"2022-07-13 09:19:02","","10.1007/978-3-030-57321-8_1","","",,,,,30,15.00,6,5,2,"","",""
1549,"Amina Adadi, M. Berrada","Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)",2018,"","","","",138,"2022-07-13 09:19:02","","10.1109/ACCESS.2018.2870052","","",,,,,1549,387.25,775,2,4,"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","",""
1,"Andrea Torcianti, S. Matzka","Explainable Artificial Intelligence for Predictive Maintenance Applications using a Local Surrogate Model",2021,"","","","",139,"2022-07-13 09:19:02","","10.1109/AI4I51902.2021.00029","","",,,,,1,1.00,1,2,1,"This paper provides an explanatory interface using Local Interpretable Model-agnostic Explanations (LIME) for a predictive maintenance dataset. The explanations are evaluated and the explanatory quality of the model is compared to two previous explainable models for the same dataset.","",""
2,"Yuanyuan Hu, Rafael Ferreira Mello, Dragan Gacseviac","Automatic analysis of cognitive presence in online discussions: An approach using deep learning and explainable artificial intelligence",2021,"","","","",140,"2022-07-13 09:19:02","","10.1016/j.caeai.2021.100037","","",,,,,2,2.00,1,3,1,"","",""
1,"Tomasz Rutkowski","Explainable Artificial Intelligence Based on Neuro-Fuzzy Modeling with Applications in Finance",2021,"","","","",141,"2022-07-13 09:19:02","","10.1007/978-3-030-75521-8","","",,,,,1,1.00,1,1,1,"","",""
1,"Ossama H. Embarak","Explainable Artificial Intelligence for Services Exchange in Smart Cities",2021,"","","","",142,"2022-07-13 09:19:02","","10.1201/9781003172772-2","","",,,,,1,1.00,1,1,1,"","",""
1,"Anna Visvizi","Artificial Intelligence (AI): Explaining, Querying, Demystifying",2021,"","","","",143,"2022-07-13 09:19:02","","10.1007/978-3-030-88972-2_2","","",,,,,1,1.00,1,1,1,"","",""
13,"Meicheng Yang, Chengyu Liu, Xingyao Wang, Yuwen Li, Hongxiang Gao, Xing Liu, Jianqing Li","An Explainable Artificial Intelligence Predictor for Early Detection of Sepsis",2020,"","","","",144,"2022-07-13 09:19:02","","10.1097/CCM.0000000000004550","","",,,,,13,6.50,2,7,2,"Supplemental Digital Content is available in the text. Objectives: Early detection of sepsis is critical in clinical practice since each hour of delayed treatment has been associated with an increase in mortality due to irreversible organ damage. This study aimed to develop an explainable artificial intelligence model for early predicting sepsis by analyzing the electronic health record data from ICU provided by the PhysioNet/Computing in Cardiology Challenge 2019. Design: Retrospective observational study. Setting: We developed our model on the shared ICUs publicly data and verified on the full hidden populations for challenge scoring. Patients: Public database included 40,336 patients’ electronic health records sourced from Beth Israel Deaconess Medical Center (hospital system A) and Emory University Hospital (hospital system B). A total of 24,819 patients from hospital systems A, B, and C (an unidentified hospital system) were sequestered as full hidden test sets. Interventions: None. Measurements and Main Results: A total of 168 features were extracted on hourly basis. Explainable artificial intelligence sepsis predictor model was trained to predict sepsis in real time. Impact of each feature on hourly sepsis prediction was explored in-depth to show the interpretability. The algorithm demonstrated the final clinical utility score of 0.364 in this challenge when tested on the full hidden test sets, and the scores on three separate test sets were 0.430, 0.422, and –0.048, respectively. Conclusions: Explainable artificial intelligence sepsis predictor model achieves superior performance for predicting sepsis risk in a real-time way and provides interpretable information for understanding sepsis risk in ICU.","",""
12,"C. Ho, Joseph Ali, K. Caals","Ensuring trustworthy use of artificial intelligence and big data analytics in health insurance",2020,"","","","",145,"2022-07-13 09:19:02","","10.2471/BLT.19.234732","","",,,,,12,6.00,4,3,2,"Abstract Technological advances in big data (large amounts of highly varied data from many different sources that may be processed rapidly), data sciences and artificial intelligence can improve health-system functions and promote personalized care and public good. However, these technologies will not replace the fundamental components of the health system, such as ethical leadership and governance, or avoid the need for a robust ethical and regulatory environment. In this paper, we discuss what a robust ethical and regulatory environment might look like for big data analytics in health insurance, and describe examples of safeguards and participatory mechanisms that should be established. First, a clear and effective data governance framework is critical. Legal standards need to be enacted and insurers should be encouraged and given incentives to adopt a human-centred approach in the design and use of big data analytics and artificial intelligence. Second, a clear and accountable process is necessary to explain what information can be used and how it can be used. Third, people whose data may be used should be empowered through their active involvement in determining how their personal data may be managed and governed. Fourth, insurers and governance bodies, including regulators and policy-makers, need to work together to ensure that the big data analytics based on artificial intelligence that are developed are transparent and accurate. Unless an enabling ethical environment is in place, the use of such analytics will likely contribute to the proliferation of unconnected data systems, worsen existing inequalities, and erode trustworthiness and trust.","",""
20,"H. Anysz, Ł. Brzozowski, Wojciech Kretowicz, P. Narloch","Feature Importance of Stabilised Rammed Earth Components Affecting the Compressive Strength Calculated with Explainable Artificial Intelligence Tools",2020,"","","","",146,"2022-07-13 09:19:02","","10.3390/ma13102317","","",,,,,20,10.00,5,4,2,"Cement-stabilized rammed earth (CSRE) is a sustainable construction material. The use of it allows for economizing on the cost of a structure. These two properties of CSRE are based on the fact that the soil used for the rammed mixture is usually dug close to the construction site, so it has random characteristics. That is the reason for the lack of widely accepted prescriptions for CSRE mixture, which could ascertain high enough compressive strength. Therefore, assessing which components of CSRE have the highest impact on its compressive strength becomes an important issue. There are three machine learning regression tools, i.e., artificial neural networks, decision tree, and random forest, used for predicting the compressive strength based on the relative content of CSRE composites (clay, silt, sand, gravel, cement, and water content). The database consisted of 434 samples of CSRE, which were prepared and crushed for testing purposes. Relatively low prediction errors of aforementioned models allowed for the use of explainable artificial intelligence tools (drop-out loss, mean squared error reduction, accumulated local effect) to rank the influence of the ingredients on the dependent variable—the compressive strength. Consistent results from all above-mentioned methods are discussed and compared to some statistical analysis of selected features. This innovative approach, helpful in designing the construction material is a solid base for reliable conclusions.","",""
0,"E. Cambouropoulos, Maximos A. Kaliakatsos-Papakostas","Cognitive Musicology and Artificial Intelligence: Harmonic Analysis, Learning, and Generation",2021,"","","","",147,"2022-07-13 09:19:02","","10.1007/978-3-030-72116-9_10","","",,,,,0,0.00,0,2,1,"","",""
0,"V. Ahuja","Explainable Artificial Intelligence: Guardian for Cancer Care",2021,"","","","",148,"2022-07-13 09:19:02","","10.1201/9781003172772-5","","",,,,,0,0.00,0,1,1,"","",""
0,"E. Shchetinin, L. Sevastianov","Improving the Learning Power of Artificial Intelligence Using Multimodal Deep Learning",2021,"","","","",149,"2022-07-13 09:19:02","","10.1051/EPJCONF/202124801017","","",,,,,0,0.00,0,2,1,"Computer paralinguistic analysis is widely used in security systems, biometric research, call centers and banks. Paralinguistic models estimate different physical properties of voice, such as pitch, intensity, formants and harmonics to classify emotions. The main goal is to find such features that would be robust to outliers and will retain variety of human voice properties at the same time. Moreover, the model used must be able to estimate features on a time scale for an effective analysis of voice variability. In this paper a paralinguistic model based on Bidirectional Long Short-Term Memory (BLSTM) neural network is described, which was trained for vocal-based emotion recognition. The main advantage of this network architecture is that each module of the network consists of several interconnected layers, providing the ability to recognize flexible long-term dependencies in data, which is important in context of vocal analysis. We explain the architecture of a bidirectional neural network model, its main advantages over regular neural networks and compare experimental results of BLSTM network with other models.","",""
0,"Lucas Mendes Lima, Victor Calebe Cavalcante, Mariana Guimarães de Sousa, Cláudio Afonso Fleury, D. Oliveira, Eduardo Noronha de Andrade Freitas","Artificial Intelligence in Support of Welfare Monitoring of Dairy Cattle: A Systematic Literature Review",2021,"","","","",150,"2022-07-13 09:19:02","","10.1109/CSCI54926.2021.00324","","",,,,,0,0.00,0,6,1,"Context: Although agribusiness corresponded to more than 20% of Brazil’s Gross Domestic Product (GDP), most livestock is under manual control and manual monitoring. Additionally, alternative technologies are either uncomfortable and stressful, or expensive. Now, despite the great scientific advances in the area, there is still a pressing need for an automated robust, inexpensive and (sub)optimal technology to monitor animal behavior in a cost-effective, contact-less and stress-free fashion. Overall, this niche can leverage the benefits of Deep Learning schemes.Objective: This review aims to provide a systematic overview of most current projects in the area of comfort monitoring dairy cattle, as well as their corresponding image recognition-based techniques and technologies.Methods: First, a systematic review planning was carried out, and objectives, research questions, search strings, among others, were defined. Subsequently,a broad survey was conducted to extract, analyze and compile the data, to generate a easy-to-read visual source of information (tables and graphics).Results: Information was extracted from the reviewed papers. Among this data collected from the papers are techniques utilized, target behaviors, cow bodyparts identified in visual computational, besides their paper source font, the publication date, and localization. For example, the papers present are mostly recent. China has had a larger number of relevant papers in the area. The back was the body region most analyzed by the papers and the behaviors most analyzed were body condition score, lameness, cow’s body position and feeding/drinking behavior. Among the methods used is RCNN Inception V3 with the best accuracy for cow’s back region.Conclusion: The aim of this work is to present some of the papers that are being carried out in the area of dairy cow behavior monitoring, using techniques of Artifical Intelligence. It is expected that the information collected and presented in the present systematic review paper contribute to the future researches and projects of the area and the application of new techniques.","",""
0,"C. Yong","Classification of Kinematic Data Using Explainable Artificial Intelligence (XAI) for Smart Motion",2021,"","","","",151,"2022-07-13 09:19:02","","10.1201/9781003172772-10","","",,,,,0,0.00,0,1,1,"","",""
0,"Ana Carolina Borges Monteiro, R. França, Rangel Arthur, Y. Iano","An Overview of Explainable Artificial Intelligence (XAI) from a Modern Perspective",2021,"","","","",152,"2022-07-13 09:19:02","","10.1201/9781003172772-1","","",,,,,0,0.00,0,4,1,"","",""
0,"Keeley A. Crockett, Edwin Colyer, A. Latham","The Ethical Landscape of Data and Artificial Intelligence: Citizen Perspectives",2021,"","","","",153,"2022-07-13 09:19:02","","10.1109/SSCI50451.2021.9660153","","",,,,,0,0.00,0,3,1,"Globally, there is growing acknowledgement that those involved in the development and deployment of AI products and services should act responsibly and conduct their work within robust ethical frameworks. Many of the ethical guidelines now published highlight a requirement for citizens to have greater voice and involvement in this process and to hold actors to account regarding compliance and the impacts of their AI innovations. For citizens to participate in co-creation activities they need to be representative of the diverse communities of society and have an appropriate level of understanding of basic AI concepts. This paper presents the preliminary results of a longitudinal survey designed to capture citizen perspectives of the ethical landscape of data and AI. Forty participants were asked to participate in a survey and results were analyzed based on gender, age range and educational attainment. Results have shown that participant perception of AI, trust, bias and fairness is different but related to specific AI applications, and the context in which is applied. Citizens also are also very receptive to undertaking free courses/workshops on a wide range of AI concepts, ranging from family workshops to work-based training.","",""
11,"I. Linkov, S. Galaitsi, Benjamin D. Trump, J. Keisler, A. Kott","Cybertrust: From Explainable to Actionable and Interpretable Artificial Intelligence",2020,"","","","",154,"2022-07-13 09:19:02","","10.1109/MC.2020.2993623","","",,,,,11,5.50,2,5,2,"We argue that artificial intelligence (AI) systems should be designed with features that build trust by bringing decision-analytic perspectives into AI. Actionable and interpretable AI will incorporate explicit quantifications and visualizations of user confidence in AI recommendations.","",""
8,"Jun Zhu, Hang Su, Bo Zhang","Toward the third generation of artificial intelligence",2020,"","","","",155,"2022-07-13 09:19:02","","10.1360/ssi-2020-0204","","",,,,,8,4.00,3,3,2,"There have been two competing paradigms of artificial intelligence (AI) development since 1956, i.e., symbolism and connectionism (or subsymbolism). Both started at the same time, but symbolism had dominated AI development until the end of the 1980s. Connectionism began to develop in the 1990s and reached its climax at the beginning of this century, and it is likely to displace symbolism. Today, it seems that the two paradigms only simulate the human mind (or brain) in different ways and have their own advantages. True human intelligence cannot be achieved by relying on only one paradigm. Both are necessary to establish a new, explainable, and robust AI theory and method and develop safe, trustworthy, reliable, and extensible AI technology. To this end, it is imperative to combine the two paradigms, and the present article will illustrate this idea. For the sake of description, symbolism, connectionism, and the newly developed paradigm are termed as first-, second-, and third-generation AIs.","",""
7,"R. Y. Goh, L. Lee, H. Seow, Kathiresan Gopal","Hybrid Harmony Search–Artificial Intelligence Models in Credit Scoring",2020,"","","","",156,"2022-07-13 09:19:02","","10.3390/e22090989","","",,,,,7,3.50,2,4,2,"Credit scoring is an important tool used by financial institutions to correctly identify defaulters and non-defaulters. Support Vector Machines (SVM) and Random Forest (RF) are the Artificial Intelligence techniques that have been attracting interest due to their flexibility to account for various data patterns. Both are black-box models which are sensitive to hyperparameter settings. Feature selection can be performed on SVM to enable explanation with the reduced features, whereas feature importance computed by RF can be used for model explanation. The benefits of accuracy and interpretation allow for significant improvement in the area of credit risk and credit scoring. This paper proposes the use of Harmony Search (HS), to form a hybrid HS-SVM to perform feature selection and hyperparameter tuning simultaneously, and a hybrid HS-RF to tune the hyperparameters. A Modified HS (MHS) is also proposed with the main objective to achieve comparable results as the standard HS with a shorter computational time. MHS consists of four main modifications in the standard HS: (i) Elitism selection during memory consideration instead of random selection, (ii) dynamic exploration and exploitation operators in place of the original static operators, (iii) a self-adjusted bandwidth operator, and (iv) inclusion of additional termination criteria to reach faster convergence. Along with parallel computing, MHS effectively reduces the computational time of the proposed hybrid models. The proposed hybrid models are compared with standard statistical models across three different datasets commonly used in credit scoring studies. The computational results show that MHS-RF is most robust in terms of model performance, model explainability and computational time.","",""
5,"Cathy O'Neil, H. Gunn","Near-Term Artificial Intelligence and the Ethical Matrix",2020,"","","","",157,"2022-07-13 09:19:02","","10.1093/oso/9780190905033.003.0009","","",,,,,5,2.50,3,2,2,"This chapter takes up the issue of near-term artificial intelligence, or the algorithms that are already in place in a variety of public and private sectors, guiding decisions from advertising and to credit ratings to sentencing in the justice system. There is a pressing need to recognize and evaluate the ways that structural racism, sexism, classism, and ableism may be embedded in and amplified by these systems. The chapter proposes a framework for ethical analysis that can be used to facilitate more robust ethical reflection in AI development and implementation. It presents an ethical matrix that incorporates the language of data science as a tool that data scientists can build themselves in order to integrate ethical analysis into the design process, addressing the need for immediate analysis and accountability over the design and deployment of near-term AI.","",""
4,"S. Abe, I. Oda","Real‐time pharyngeal cancer detection utilizing artificial intelligence: Journey from the proof of concept to the clinical use",2020,"","","","",158,"2022-07-13 09:19:02","","10.1111/den.13833","","",,,,,4,2.00,2,2,2,"Early detection of pharyngeal cancer contributes to providing excellent long-term outcomes and preserving pharyngeal function. Thus, an efficient detection system for superficial pharyngeal cancer (SPC) is necessary worldwide, especially in Asia. However, early detection proves challenging because SPC is typically flat with subtle color changes. Although image enhanced endoscopy, such as narrow band imaging (NBI), substantially helps us detect SPC, it is still difficult even for experienced endoscopists. Based on this background, artificial intelligence (AI) technologyhasbeen recently utilized for the earlydetection ofpharyngeal cancer.Mascharak et al.first reported imageprocessing andbasic machine learning techniques to automate the assessment of oropharyngeal cancer using white light endoscopy (WLE) and NBI. However, the sensitivity and specificitywere unsatisfactory, even using NBI, perhaps owing to a sample size of only 30 patients. Later, Tamashiro et al. developed an AI system using 5403 still images of pharyngeal cancer. In this study, as expected, the AI system took only 28 seconds to analyze 1912 validation still images consisting of 928 and 984 with or without pharyngeal cancers, respectively. The sensitivity and the specificity per image analysis were 79.7% and were 57.1%, respectively. This AI system demonstrated sensitive pharyngeal cancer detection. However, validation using still images would be subject to bias because still images are usually taken under good conditions (e.g., adequate angle, adequate distance, and in focus). Although the AI system for still image diagnosis can be utilized for doublechecking after screening endoscopy, it is unsuitable for real-world endoscopic diagnosis. In this issue of Digestive Endoscopy, Kono et al. present the diagnostic performance of their AI system for pharyngeal cancer detection using an independent validation dataset of 25 videos of pharyngeal cancer and 36 videos of nonpharyngeal cancer. The sensitivity, specificity, and accuracy for detecting cancer were 92%, 47%, and 66%, respectively. This study highlighted that the AI system allowed for sensitive and real-time endoscopic pharyngeal cancer detection. The validation with video images is more challenging than that with still images because the images inevitably include poor-quality features (e.g., defocus, light reflection, mucus, or saliva), which were excluded in Tamashiro et al. Nevertheless, the video-based validation provides a more realistic and practical assessment of the diagnostic performance of the AI system. The concept of this study is highly valuable because real-time pharyngeal cancer detection is more supportive and educational than still image assessment, particularly for inexperienced endoscopists. Moreover, the concept could play a role in quality assurance. As shown above, the AI system was a promising tool for pharyngeal cancer detection. However, we want to discuss areas of further improvement of the AI system because it appears to be at an investigational stage that requires adaptation for the real-world endoscopic detection. First, further robust data of its sensitivity should be accumulated. In terms of imaging modality, the sensitivity of the AI system appears to be higher for NBI thanWLE, as it is in human diagnosis. Although the sensitivity was reported to be relatively favorable, the sample size was small for its use in real clinical practice. It is desirable to further train the AI system with more pharyngeal cancer images, primarily SPC. Close collaboration with otolaryngologists is considered necessary to increase the sample size as they still manage most SPC. Also, given the low prevalence of pharyngeal cancer, it is not realistic to establish the training in a single center, even if endoscopists focus on high-risk populations. A specific multicenter database such as the Japan Endoscopy Database (JED) will help collect enough samples of rare diseases and establish a better trained AI system. Secondly, more importantly, specificity should also be improved. In both studies, the specificity was unsatisfactory, as the authors pointed out. Consequently, the positive predictive value was low. The positive predictive value of the AI system will become further lower in real clinical practice given the low prevalence of pharyngeal cancer and the proportion of pharyngeal cancer and non-neoplastic lesions. It means that endoscopists will face excessive caution introduced by the AI system during an endoscopy examination. The main reason to explain the low specificity was insufficient training of noncancerous lesions, due to having included only images of pharyngeal cancer in the training set. However, a higher specificity will not be achieved by simply educating with non-","",""
16,"","Knowledge Graphs for eXplainable Artificial Intelligence: Foundations, Applications and Challenges",2020,"","","","",159,"2022-07-13 09:19:02","","","","",,,,,16,8.00,0,0,2,"","",""
1,"D. Dubois, H. Prade","A Glance at Causality Theories for Artificial Intelligence",2020,"","","","",160,"2022-07-13 09:19:02","","10.1007/978-3-030-06164-7_9","","",,,,,1,0.50,1,2,2,"","",""
0,"R. Porcher","CORR Insights®: Does Artificial Intelligence Outperform Natural Intelligence in Interpretation of Musculoskeletal Radiological Studies? A Systematic Review.",2020,"","","","",161,"2022-07-13 09:19:02","","10.1097/CORR.0000000000001415","","",,,,,0,0.00,0,1,2,"Machine learning, and artificial intelligence more generally, are quickly growing areas of applied medical decisionmaking research. Compared with what are now considered moretraditional analytical approaches, such as statistical prediction models, machine learning is seen as providing unique advantages; in particular, it may improve healthcare delivery because it can learn from millions of digitized patient charts or images, and so provide robust, reproducible, and rapid decision-support tools [8, 14]. Artificial intelligence has already transformed many aspects of daily life outside health care; machine-learning algorithms allow us to translate large pieces of text into any language, recognize speech, drive a car, make a plane take off or land, or detect banking fraud. The advantages of machine learning include the ability to analyze enormous amounts of data, capture complex nonlinear relationships among these data, and consider a wide range of data. It can handle structured data, similar to other statistical prediction methods, but machine learning can also analyze free text and images, as well as high-frequency sampled data streams such as those produced by wearable devices. In this respect, no approach other than artificial intelligence and machine learning has enabled the analysis of such data so far. These approaches have begun to show promise in orthopaedic surgery, specifically. For example, one recent study used machine learning to predict whether patients would achieve clinically important improvements in validated outcome scores 2 years after joint arthroplasty [5, 10], which is important in light of the fact that even experienced surgeons’ abilities in this sort of prediction for patients undergoing TKA are no better than a coin toss [6]. However, despite the hype and hopes about artificial intelligence, more-nuanced opinions have emerged [1, 13]. Identifying associations among data does not prevent confounding, and this may prevent us from translating modifiable factors flagged by algorithms into real targets for interventions. Additionally, despite the underlying idea that the more data we have to train an algorithm, the more accurate they are, the greed for more data does not always translate into more-accurate predictions [1]. Predicting what will occur in 1, 5, or 10 years may be difficult because all past data, not just available data, do not contain sufficient information. This may explain why machine-learning algorithms have often outperformed human experts in imaging or diagnostics, where most information is present in the data analyzed [8]. The advantage over moreclassic statistical models for longer-term risk prediction modeling is likely less evident [2]. This CORR Insights is a commentary on the article “Does Artificial Intelligence Outperform Natural Intelligence in Interpretation of Musculoskeletal Radiological Studies? A Systematic Review” by Groot and colleagues available at: DOI: 10.1097/CORR.0000000000001360. The author certifies that he, or anymembers of his immediate family, has no commercial associations (eg, consultancies, stock ownership, equity interest, patent/licensing arrangements, etc) that might pose a conflict of interest in connection with the submitted article. All ICMJE Conflict of Interest Forms for authors and Clinical Orthopaedics and Related Research editors and board members are on file with the publication and can be viewed on request. The opinions expressed are those of the writer, and do not reflect the opinion or policy of CORR or the Association of Bone and Joint Surgeons. R. Porcher ✉, Centre d’Epidémiologie Clinique, Hôpital Hôtel-Dieu, 1 Parvis NotreDame Place Jean-Paul II, 75004 Paris, France, Email: raphael.porcher@aphp.fr R. Porcher, Université de Paris, CRESS UMR1153, INSERM, INRA, F-75004 Paris, France; Centre d’Epidémiologie Clinique, AP-HP, Hôtel-Dieu, F-75004 Paris, France","",""
0,"Charlie T. Veal, Marshall Lindsay, S. Kovaleski, Derek T. Anderson, Stanton R. Price","Evolutionary Algorithm Driven Explainable Adversarial Artificial Intelligence",2020,"","","","",162,"2022-07-13 09:19:02","","10.1109/SSCI47803.2020.9308361","","",,,,,0,0.00,0,5,2,"It is well-known that machine learning algorithms can be susceptible to undesirable effects when exposed to conditions that are not expressed adequately in the training dataset. This leads to a growing interest throughout many communities; where do algorithms and trained models break? Recently, methods such as generative adversarial neural networks and variational autoencoders were proposed to create adversarial examples that challenge algorithms. This results in artificial intelligence having higher false detections or completely losing recognition. The problem is that existing solutions, are for the most part, black boxes. Current gaps include how do we better control and understand adversarial algorithms. Herein, we propose the concept of an adversarial modifier set as an understandable and controlled way to generate adversarial examples. This is achieved by exploiting the improved evolution-constructed algorithm to identify ideal features that a victim algorithm values in imagery. These features are combined to realize a tuple library that preserves spatial relations. Last, a set of algorithmically controlled modifiers that generate the imagery are found by examining the content of the false imagery. Preliminary results are encouraging and demonstrate that this approach has benefits in both generating explainable adversarial examples, as well as shedding some insight into victim algorithm decision making.","",""
0,"T. Sing, J. Yang, S. Yu","Decision Tree and Boosting Techniques in Artificial Intelligence Based Automated Valuation Models (AI-AVM)",2020,"","","","",163,"2022-07-13 09:19:02","","10.2139/ssrn.3605798","","",,,,,0,0.00,0,3,2,"This paper develops an artificial intelligence-based automated valuation model (AI-AVM) using the decision tree and the boosting techniques to predict residential property prices in Singapore. We use more than 300,000 property transaction data from Singapore’s private residential property market for the period from 1995 to 2017 for the training of the AI-AVM models. The two tree-based AI-AVM models show superior performance over the traditional multiple regression analysis (MRA) model when predicting the property prices. We also extend the application of the AI-AVM to more homogenous public housing prices in Singapore, and the predictive performance remains robust. The boosting AI-AVM models that allow for inter-dependence structure in the decision trees is the best model that explains more than 88% of the variance in both private and public housing prices and keep the prediction errors to less than 6% for HDB and 9% for the private market. When subject the AI-AVM to the out-of-sample forecasting using the 2017-2019 testing property sale samples, the prediction errors remain within a narrow range of between 5% and 9%.","",""
0,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dental Diagnostics: Chances and challenges",2020,"","","","",164,"2022-07-13 09:19:02","","","","",,,,,0,0.00,0,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. is a using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning and conduct, e.g. image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, a, predictive, preventive and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to (1) limited data availability, accessibility, structure and comprehensiveness, (2) lacking methodological rigor and standards in their development, (3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, e.g. by improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Last, trustworthiness into and generalizability of dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
0,"Katanosh Morovat, B. Panda","A Survey of Artificial Intelligence in Cybersecurity",2020,"","","","",165,"2022-07-13 09:19:02","","10.1109/CSCI51800.2020.00026","","",,,,,0,0.00,0,2,2,"During the last decades, not only the number of cyberattacks have increased significantly, they have also become more sophisticated. Hence designing a cyber-resilient approach is of paramount importance. Traditional security methods are not adequate to prevent data breaches in case of cyberattacks. Cybercriminals have learned how to use new techniques and robust tools to hack, attack, and breach data. Fortunately, Artificial Intelligence (AI) technologies have been introduced into cyberspace to construct smart models for defending systems from attacks. Since AI technologies can rapidly evolve to address complex situations, they can be used as fundamental tools in the field of cybersecurity. Al-based techniques can provide efficient and powerful cyber defense tools to recognize malware attacks, network intrusions, phishing and spam emails, and data breaches, to name a few, and to alert security incidents when they occur. In this paper, we review the impact of AI in cybersecurity and summarize existing research in terms of benefits of AI in cybersecurity.","",""
0,"T. D. Raheni, P. Thirumoorthi","Stochastic Artificial Intelligence: Review Article",2020,"","","","",166,"2022-07-13 09:19:02","","10.5772/INTECHOPEN.90003","","",,,,,0,0.00,0,2,2,"Artificial intelligence (AI) is a region of computer techniques that deals with the design of intelligent machines that respond like humans. It has the skill to operate as a machine and simulate various human intelligent algorithms according to the user’s choice. It has the ability to solve problems, act like humans, and perceive information. In the current scenario, intelligent techniques minimize human effort especially in industrial fields. Human beings create machines through these intelligent techniques and perform various processes in different fields. Artificial intelligence deals with real-time insights where decisions are made by connecting the data to various resources. To solve real-time problems, powerful machine learning-based techniques such as artificial intelligence, neural networks, fuzzy logic, genetic algorithms, and particle swarm optimization have been used in recent years. This chapter explains artificial neural network-based adaptive linear neuron networks, back-propagation networks, and radial basis networks.","",""
0,"Wei Yan","IEEE Transactions on Artificial Intelligence",2020,"","","","",167,"2022-07-13 09:19:02","","10.1109/tfuzz.2020.2987029","","",,,,,0,0.00,0,1,2,"The IEEE Transactions on Artificial Intelligence (TAI) is a multidisciplinary journal publishing papers on theories and methodologies of Artificial Intelligence. Applications of Artificial Intelligence are also considered. Topics covered by IEEE TAI include, but not limited to, Agent-based Systems, Augmented Intelligence, Autonomic Computing, Constraint Systems, Explainable AI, Knowledge-Based Systems, Learning Theories, Planning, Reasoning, Search, Natural Language Processing, and Applications. Technical papers addressing contemporary topics in AI such as Ethics and Social Implications are welcomed.","",""
19,"B. Verheij","Artificial intelligence as law",2020,"","","","",168,"2022-07-13 09:19:02","","10.1007/s10506-020-09266-0","","",,,,,19,9.50,19,1,2,"","",""
496,"Christopher J. Kelly, A. Karthikesalingam, Mustafa Suleyman, Greg Corrado, Dominic King","Key challenges for delivering clinical impact with artificial intelligence",2019,"","","","",169,"2022-07-13 09:19:02","","10.1186/s12916-019-1426-2","","",,,,,496,165.33,99,5,3,"","",""
199,"Dong Wook Kim, H. Jang, K. Kim, Youngbin Shin, S. Park","Design Characteristics of Studies Reporting the Performance of Artificial Intelligence Algorithms for Diagnostic Analysis of Medical Images: Results from Recently Published Papers",2019,"","","","",170,"2022-07-13 09:19:02","","10.3348/kjr.2019.0025","","",,,,,199,66.33,40,5,3,"Objective To evaluate the design characteristics of studies that evaluated the performance of artificial intelligence (AI) algorithms for the diagnostic analysis of medical images. Materials and Methods PubMed MEDLINE and Embase databases were searched to identify original research articles published between January 1, 2018 and August 17, 2018 that investigated the performance of AI algorithms that analyze medical images to provide diagnostic decisions. Eligible articles were evaluated to determine 1) whether the study used external validation rather than internal validation, and in case of external validation, whether the data for validation were collected, 2) with diagnostic cohort design instead of diagnostic case-control design, 3) from multiple institutions, and 4) in a prospective manner. These are fundamental methodologic features recommended for clinical validation of AI performance in real-world practice. The studies that fulfilled the above criteria were identified. We classified the publishing journals into medical vs. non-medical journal groups. Then, the results were compared between medical and non-medical journals. Results Of 516 eligible published studies, only 6% (31 studies) performed external validation. None of the 31 studies adopted all three design features: diagnostic cohort design, the inclusion of multiple institutions, and prospective data collection for external validation. No significant difference was found between medical and non-medical journals. Conclusion Nearly all of the studies published in the study period that evaluated the performance of AI algorithms for diagnostic analysis of medical images were designed as proof-of-concept technical feasibility studies and did not have the design features that are recommended for robust validation of the real-world clinical performance of AI algorithms.","",""
224,"Stuart Russell","Human Compatible: Artificial Intelligence and the Problem of Control",2019,"","","","",171,"2022-07-13 09:19:02","","","","",,,,,224,74.67,224,1,3,"""The most important book I have read in quite some time"" (Daniel Kahneman); ""A must-read"" (Max Tegmark); ""The book we've all been waiting for"" (Sam Harris) LONGLISTED FOR THE 2019 FINANCIAL TIMES AND MCKINSEY BUSINESS BOOK OF THE YEAR; A FINANCIAL TIMES BEST BOOK OF THE YEAR 2019 Humans dream of super-intelligent machines. But what happens if we actually succeed? Creating superior intelligence would be the biggest event in human history. Unfortunately, according to the world's pre-eminent AI expert, it could also be the last. In this groundbreaking book on the biggest question facing humanity, Stuart Russell explains why he has come to consider his own discipline an existential threat to our species, and lays out how we can change course before it's too late. There is no one better placed to assess the promise and perils of the dominant technology of the future than Russell, who has spent decades at the forefront of AI research. Through brilliant analogies and crisp, lucid prose, he explains how AI actually works, how it has an enormous capacity to improve our lives - but why we must ensure that we never lose control of machines more powerful than we are. Here Russell shows how we can avert the worst threats by reshaping the foundations of AI to guarantee that machines pursue our objectives, not theirs. Profound, urgent and visionary, Human Compatible is the one book everyone needs to read to understand a future that is coming sooner than we think.","",""
66,"Keping Yu, Zhiwei Guo, Yulian Shen, Wei Wang, Jerry Chun‐wei Lin, Takuro Sato","Secure Artificial Intelligence of Things for Implicit Group Recommendations",2021,"","","","",172,"2022-07-13 09:19:02","","10.1109/JIOT.2021.3079574","","",,,,,66,66.00,11,6,1,"The emergence of Artificial Intelligence of Things (AIoT) has provided novel insights for many social computing applications, such as group recommender systems. As the distances between people have been greatly shortened, there has been more general demand for the provision of personalized services aimed at groups instead of individuals. The existing methods for capturing group-level preference features from individuals have mostly been established via aggregation and face two challenges: 1) secure data management workflows are absent and 2) implicit preference feedback is ignored. To tackle these current difficulties, this article proposes secure AIoT for implicit group recommendations (SAIoT-GRs). For the hardware module, a secure Internet of Things structure is developed as the bottom support platform. For the software module, a collaborative Bayesian network model and noncooperative game are introduced as algorithms. This secure AIoT architecture is able to maximize the advantages of the two modules. In addition, a large number of experiments are carried out to evaluate the performance of SAIoT-GR in terms of efficiency and robustness.","",""
183,"David Gunning, D. Aha","DARPA’s Explainable Artificial Intelligence (XAI) Program",2019,"","","","",173,"2022-07-13 09:19:02","","10.1609/AIMAG.V40I2.2850","","",,,,,183,61.00,92,2,3,"Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.","",""
167,"C. Langlotz, Bibb Allen, B. Erickson, Jayashree Kalpathy-Cramer, K. Bigelow, T. Cook, A. Flanders, M. Lungren, D. Mendelson, J. Rudie, Ge Wang, K. Kandarpa","A Roadmap for Foundational Research on Artificial Intelligence in Medical Imaging: From the 2018 NIH/RSNA/ACR/The Academy Workshop.",2019,"","","","",174,"2022-07-13 09:19:02","","10.1148/radiol.2019190613","","",,,,,167,55.67,17,12,3,"Imaging research laboratories are rapidly creating machine learning systems that achieve expert human performance using open-source methods and tools. These artificial intelligence systems are being developed to improve medical image reconstruction, noise reduction, quality assurance, triage, segmentation, computer-aided detection, computer-aided classification, and radiogenomics. In August 2018, a meeting was held in Bethesda, Maryland, at the National Institutes of Health to discuss the current state of the art and knowledge gaps and to develop a roadmap for future research initiatives. Key research priorities include: 1, new image reconstruction methods that efficiently produce images suitable for human interpretation from source data; 2, automated image labeling and annotation methods, including information extraction from the imaging report, electronic phenotyping, and prospective structured image reporting; 3, new machine learning methods for clinical imaging data, such as tailored, pretrained model architectures, and federated machine learning methods; 4, machine learning methods that can explain the advice they provide to human users (so-called explainable artificial intelligence); and 5, validated methods for image de-identification and data sharing to facilitate wide availability of clinical imaging data sets. This research roadmap is intended to identify and prioritize these needs for academic research laboratories, funding agencies, professional societies, and industry.","",""
120,"R. Byrne","Counterfactuals in Explainable Artificial Intelligence (XAI): Evidence from Human Reasoning",2019,"","","","",175,"2022-07-13 09:19:02","","10.24963/IJCAI.2019/876","","",,,,,120,40.00,120,1,3,"Counterfactuals about what could have happened are increasingly used in an array of Artificial Intelligence (AI) applications, and especially in explainable AI (XAI). Counterfactuals can aid the provision of interpretable models to make the decisions of inscrutable systems intelligible to developers and users. However, not all counterfactuals are equally helpful in assisting human comprehension. Discoveries about the nature of the counterfactuals that humans create are a helpful guide to maximize the effectiveness of counterfactual use in AI.","",""
132,"Y. Yang, C. S. Bang","Application of artificial intelligence in gastroenterology",2019,"","","","",176,"2022-07-13 09:19:02","","10.3748/wjg.v25.i14.1666","","",,,,,132,44.00,66,2,3,"Artificial intelligence (AI) using deep-learning (DL) has emerged as a breakthrough computer technology. By the era of big data, the accumulation of an enormous number of digital images and medical records drove the need for the utilization of AI to efficiently deal with these data, which have become fundamental resources for a machine to learn by itself. Among several DL models, the convolutional neural network showed outstanding performance in image analysis. In the field of gastroenterology, physicians handle large amounts of clinical data and various kinds of image devices such as endoscopy and ultrasound. AI has been applied in gastroenterology in terms of diagnosis, prognosis, and image analysis. However, potential inherent selection bias cannot be excluded in the form of retrospective study. Because overfitting and spectrum bias (class imbalance) have the possibility of overestimating the accuracy, external validation using unused datasets for model development, collected in a way that minimizes the spectrum bias, is mandatory. For robust verification, prospective studies with adequate inclusion/exclusion criteria, which represent the target populations, are needed. DL has its own lack of interpretability. Because interpretability is important in that it can provide safety measures, help to detect bias, and create social acceptance, further investigations should be performed.","",""
134,"M. I. Jordan","Artificial Intelligence—The Revolution Hasn’t Happened Yet",2019,"","","","",177,"2022-07-13 09:19:02","","10.1162/99608F92.F06C6E61","","",,,,,134,44.67,134,1,3,"We praise Jordan for bringing much needed clarity about the current status of Artificial Intelligence (AI)—what it currently is and what it is not—as well as explaining the current challenges lying ahead and outlining what is missing and remains to be done. Jordan makes several claims supported by a list of talking points that we hope will reach a wide audience; ideally, that audience will include academic, university, and governmental leaders, at a time where significant resources are being allocated to AI for research and education.","",""
10,"N. Rodríguez, G. Pisoni","Accessible Cultural Heritage through Explainable Artificial Intelligence",2020,"","","","",178,"2022-07-13 09:19:02","","10.1145/3386392.3399276","","",,,,,10,5.00,5,2,2,"Ethics Guidelines for Trustworthy AI advocate for AI technology that is, among other things, more inclusive. Explainable AI (XAI) aims at making state of the art opaque models more transparent, and defends AI-based outcomes endorsed with a rationale explanation, i.e., an explanation that has as target the non-technical users. XAI and Responsible AI principles defend the fact that the audience expertise should be included in the evaluation of explainable AI systems. However, AI has not yet reached all public and audiences, some of which may need it the most. One example of domain where accessibility has not much been influenced by the latest AI advances is cultural heritage. We propose including minorities as special user and evaluator of the latest XAI techniques. In order to define catalytic scenarios for collaboration and improved user experience, we pose some challenges and research questions yet to address by the latest AI models likely to be involved in such synergy.","",""
88,"Jeannette Paschen, Jan H. Kietzmann, T. Kietzmann","Artificial intelligence (AI) and its implications for market knowledge in B2B marketing",2019,"","","","",179,"2022-07-13 09:19:02","","10.1108/JBIM-10-2018-0295","","",,,,,88,29.33,29,3,3," Purpose The purpose of this paper is to explain the technological phenomenon artificial intelligence (AI) and how it can contribute to knowledge-based marketing in B2B. Specifically, this paper describes the foundational building blocks of any artificial intelligence system and their interrelationships. This paper also discusses the implications of the different building blocks with respect to market knowledge in B2B marketing and outlines avenues for future research.   Design/methodology/approach The paper is conceptual and proposes a framework to explicate the phenomenon AI and its building blocks. It further provides a structured discussion of how AI can contribute to different types of market knowledge critical for B2B marketing: customer knowledge, user knowledge and external market knowledge.   Findings The paper explains AI from an input–processes–output lens and explicates the six foundational building blocks of any AI system. It also discussed how the combination of the building blocks transforms data into information and knowledge.   Practical implications Aimed at general marketing executives, rather than AI specialists, this paper explains the phenomenon artificial intelligence, how it works and its relevance for the knowledge-based marketing in B2B firms. The paper highlights illustrative use cases to show how AI can impact B2B marketing functions.   Originality/value The study conceptualizes the technological phenomenon artificial intelligence from a knowledge management perspective and contributes to the literature on knowledge management in the era of big data. It addresses calls for more scholarly research on AI and B2B marketing. ","",""
3,"E. Sala, Stephan Ursprung","Artificial Intelligence in Radiology: The Computer's Helping Hand Needs Guidance.",2020,"","","","",180,"2022-07-13 09:19:02","","10.1148/ryai.2020200207","","",,,,,3,1.50,2,2,2,"A intelligence (AI) is not entirely new to the medical field. We are accustomed to applying tools that have been developed with a varying degree of human and computer input in our clinical practice. Representative examples include handcrafted diagnostic algorithms to triage patients presenting with acute illness (1), statistically derived scores for osteoporotic fracture risk assessment (2), and decision trees for the differentiation of benign and malignant ovarian masses (3). Common to all these tools is that changes to input parameters lead to predictable changes in model output, making them easy to interrogate and understand. More sophisticated, deep learning (DL)–based models employed in decision support systems have been implemented in clinical practice for the automated interpretation of electrocardiograms (4) and detection and classification of lesions on mammography (5). Although DL-based models have exciting potential to solve complex problems, their black box approach faces skepticism despite advances in interpretability and explainability (6). The recent, widespread availability of hardware and software for the development of AI solutions for medicine has inspired an exponential increase in publications. Data-rich medical specialties such as radiology have become a particular focus of rapid development. However, there are ample scope and encouraging initiatives for AI to support the delivery of care from general practice, primary care, the emergency department, and specialist diagnostics to patient self-care (7). This study by Tadavarthi and colleagues (8) has examined the market of AI-enabled image analysis solutions for radiology and provides recommendations for the evaluation of AI tools before purchase. In their market study, the authors illustrate how most solutions are focused on highvolume conditions. Unsurprisingly, many solutions focus on support for lesion detection and quantification rather than decision support for diagnosis and recommendations for management where regulatory stakes and hurdles are higher. Yet only a minority of solutions advertised at the Radiological Society of North America and Society of Imaging Informatics in Medicine annual meetings between November 2016 and June 2019 have received approval for the American or European market. This finding is indicative of a rapidly developing field where, after years of purely scientific development, the first tools start undergoing consolidation, approval, and marketing. The sole focus on solutions advertised at North American conferences risks missing tools by smaller companies with lower marketing budgets and introducing a geographic bias. Indeed, several other solutions have achieved Conformité Européenne marking or U.S. Food and Drug Administration (FDA) approval. Such approval or their equivalent in other territories is a precondition for the implementation of products, but it is by no means sufficient to identify clinically beneficial and financially viable tools. Overall, the adoption of these algorithms into clinical practice is emerging, and further work is needed to transform the scientific enthusiasm for developing advanced AI tools with a broader scope into clinically workable solutions. For tracking the ongoing market, surveys like this study (dating from November 2019) date quickly, leaving a gap for a living review and other market watchers. Tadavarthi et al contribute to a growing number of recommendations for the acquisition and adoption of AI solutions in medicine with a particular focus on radiology (9). They raise important considerations to determine whether an AI tool is a viable solution for an individual service. In addition, one might want to consider the following criteria: Artificial Intelligence in Radiology: The Computer’s Helping Hand Needs Guidance","",""
86,"Alberto Fernández, F. Herrera, O. Cordón, M. J. D. Jesús, F. Marcelloni","Evolutionary Fuzzy Systems for Explainable Artificial Intelligence: Why, When, What for, and Where to?",2019,"","","","",181,"2022-07-13 09:19:02","","10.1109/MCI.2018.2881645","","",,,,,86,28.67,17,5,3,"Evolutionary fuzzy systems are one of the greatest advances within the area of computational intelligence. They consist of evolutionary algorithms applied to the design of fuzzy systems. Thanks to this hybridization, superb abilities are provided to fuzzy modeling in many different data science scenarios. This contribution is intended to comprise a position paper developing a comprehensive analysis of the evolutionary fuzzy systems research field. To this end, the ""4 W"" questions are posed and addressed with the aim of understanding the current context of this topic and its significance. Specifically, it will be pointed out why evolutionary fuzzy systems are important from an explainable point of view, when they began, what they are used for, and where the attention of researchers should be directed to in the near future in this area. They must play an important role for the emerging area of eXplainable Artificial Intelligence (XAI) learning from data.","",""
87,"Carlos Zednik","Solving the Black Box Problem: A Normative Framework for Explainable Artificial Intelligence",2019,"","","","",182,"2022-07-13 09:19:02","","10.1007/s13347-019-00382-7","","",,,,,87,29.00,87,1,3,"","",""
84,"James Shaw, F. Rudzicz, T. Jamieson, Avi Goldfarb","Artificial Intelligence and the Implementation Challenge",2019,"","","","",183,"2022-07-13 09:19:02","","10.2196/13659","","",,,,,84,28.00,21,4,3,"Background Applications of artificial intelligence (AI) in health care have garnered much attention in recent years, but the implementation issues posed by AI have not been substantially addressed. Objective In this paper, we have focused on machine learning (ML) as a form of AI and have provided a framework for thinking about use cases of ML in health care. We have structured our discussion of challenges in the implementation of ML in comparison with other technologies using the framework of Nonadoption, Abandonment, and Challenges to the Scale-Up, Spread, and Sustainability of Health and Care Technologies (NASSS). Methods After providing an overview of AI technology, we describe use cases of ML as falling into the categories of decision support and automation. We suggest these use cases apply to clinical, operational, and epidemiological tasks and that the primary function of ML in health care in the near term will be decision support. We then outline unique implementation issues posed by ML initiatives in the categories addressed by the NASSS framework, specifically including meaningful decision support, explainability, privacy, consent, algorithmic bias, security, scalability, the role of corporations, and the changing nature of health care work. Results Ultimately, we suggest that the future of ML in health care remains positive but uncertain, as support from patients, the public, and a wide range of health care stakeholders is necessary to enable its meaningful implementation. Conclusions If the implementation science community is to facilitate the adoption of ML in ways that stand to generate widespread benefits, the issues raised in this paper will require substantial attention in the coming years.","",""
99,"R. Colling, Helen Pitman, K. Oien, N. Rajpoot, P. Macklin, D. Snead, Tony Sackville, C. Verrill","Artificial intelligence in digital pathology: a roadmap to routine use in clinical practice",2019,"","","","",184,"2022-07-13 09:19:02","","10.1002/path.5310","","",,,,,99,33.00,12,8,3,"The use of artificial intelligence will transform clinical practice over the next decade and the early impact of this will likely be the integration of image analysis and machine learning into routine histopathology. In the UK and around the world, a digital revolution is transforming the reporting practice of diagnostic histopathology and this has sparked a proliferation of image analysis software tools. While this is an exciting development that could discover novel predictive clinical information and potentially address international pathology workforce shortages, there is a clear need for a robust and evidence‐based framework in which to develop these new tools in a collaborative manner that meets regulatory approval. With these issues in mind, the NCRI Cellular Molecular Pathology (CM‐Path) initiative and the British In Vitro Diagnostics Association (BIVDA) have set out a roadmap to help academia, industry, and clinicians develop new software tools to the point of approved clinical use. © 2019 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.","",""
73,"Valentina Bellemo, Gilbert Lim, T. Rim, G. Tan, C. Cheung, S. Sadda, M. He, A. Tufail, M. Lee, W. Hsu, D. Ting","Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application",2019,"","","","",185,"2022-07-13 09:19:02","","10.1007/s11892-019-1189-3","","",,,,,73,24.33,7,11,3,"","",""
62,"Prashan Madumal, Tim Miller, L. Sonenberg, F. Vetere","A Grounded Interaction Protocol for Explainable Artificial Intelligence",2019,"","","","",186,"2022-07-13 09:19:02","","","","",,,,,62,20.67,16,4,3,"Explainable Artificial Intelligence (XAI) systems need to include an explanation model to communicate the internal decisions, behaviours and actions to the interacting humans. Successful explanation involves both cognitive and social processes. In this paper we focus on the challenge of meaningful interaction between an explainer and an explainee and investigate the structural aspects of an interactive explanation to propose an interaction protocol. We follow a bottom-up approach to derive the model by analysing transcripts of different explanation dialogue types with 398 explanation dialogues. We use grounded theory to code and identify key components of an explanation dialogue. We formalize the model using the agent dialogue framework (ADF) as a new dialogue type and then evaluate it in a human-agent interaction study with 101 dialogues from 14 participants. Our results show that the proposed model can closely follow the explanation dialogues of human-agent conversations.","",""
59,"David Gunning, D. Aha","DARPA’s Explainable Artificial Intelligence Program",2019,"","","","",187,"2022-07-13 09:19:02","","","","",,,,,59,19.67,30,2,3,"n Dramatic success in machine learning has led toanewwaveofAIapplications (for example, transportation, security,medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. TheXAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance. Advances inmachine learning (ML) techniques promise to produce AI systems that perceive, learn, decide, and act on their own. However, they will be unable to explain their decisions and actions to human users. This lack is especially important for the Department of Defense, whose challenges require developingmore intelligent, autonomous, and symbiotic systems. Explainable AI will be essential if users are to understand, appropriately trust, and effectively manage these artificially intelligent partners. To address this, DARPA launched its explainable artificial intelligence (XAI) program in May 2017. DARPA defines explainable AI as AI systems that can explain their rationale to a human user, characterize their strengths and weaknesses, and convey an understanding of how theywill behave in the future. Naming this program explainable AI (rather than interpretable, comprehensible, or transparent AI, for example) reflects DARPA’s objective to create more human-understandable AI systems through the use of effective explanations. It also reflects the XAI team’s interest in the human psychology of explanation, which draws on the vast body of research and expertise in the social sciences.","",""
85,"A. Grzybowski, Piotr Brona, Gilbert Lim, P. Ruamviboonsuk, G. Tan, M. Abràmoff, D. Ting","Artificial intelligence for diabetic retinopathy screening: a review",2019,"","","","",188,"2022-07-13 09:19:02","","10.1038/s41433-019-0566-0","","",,,,,85,28.33,12,7,3,"","",""
67,"Yonghui Shang, Hoang Nguyen, X. Bui, Quang-Hieu Tran, H. Moayedi","A Novel Artificial Intelligence Approach to Predict Blast-Induced Ground Vibration in Open-Pit Mines Based on the Firefly Algorithm and Artificial Neural Network",2019,"","","","",189,"2022-07-13 09:19:02","","10.1007/s11053-019-09503-7","","",,,,,67,22.33,13,5,3,"","",""
51,"Lu Minh Le, H. Ly, B. Pham, Vuong Minh Le, T. Pham, Duy-Hung Nguyen, Xuan-Tuan Tran, Tien-Thinh Le","Hybrid Artificial Intelligence Approaches for Predicting Buckling Damage of Steel Columns Under Axial Compression",2019,"","","","",190,"2022-07-13 09:19:02","","10.3390/ma12101670","","",,,,,51,17.00,6,8,3,"This study aims to investigate the prediction of critical buckling load of steel columns using two hybrid Artificial Intelligence (AI) models such as Adaptive Neuro-Fuzzy Inference System optimized by Genetic Algorithm (ANFIS-GA) and Adaptive Neuro-Fuzzy Inference System optimized by Particle Swarm Optimization (ANFIS-PSO). For this purpose, a total number of 57 experimental buckling tests of novel high strength steel Y-section columns were collected from the available literature to generate the dataset for training and validating the two proposed AI models. Quality assessment criteria such as coefficient of determination (R2), Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) were used to validate and evaluate the performance of the prediction models. Results showed that both ANFIS-GA and ANFIS-PSO had a strong ability in predicting the buckling load of steel columns, but ANFIS-PSO (R2 = 0.929, RMSE = 60.522 and MAE = 44.044) was slightly better than ANFIS-GA (R2 = 0.916, RMSE = 65.371 and MAE = 48.588). The two models were also robust even with the presence of input variability, as investigated via Monte Carlo simulations. This study showed that the hybrid AI techniques could help constructing an efficient numerical tool for buckling analysis.","",""
59,"J. Fellous, G. Sapiro, A. Rossi, H. Mayberg, M. Ferrante","Explainable Artificial Intelligence for Neuroscience: Behavioral Neurostimulation",2019,"","","","",191,"2022-07-13 09:19:02","","10.3389/fnins.2019.01346","","",,,,,59,19.67,12,5,3,"The use of Artificial Intelligence and machine learning in basic research and clinical neuroscience is increasing. AI methods enable the interpretation of large multimodal datasets that can provide unbiased insights into the fundamental principles of brain function, potentially paving the way for earlier and more accurate detection of brain disorders and better informed intervention protocols. Despite AI’s ability to create accurate predictions and classifications, in most cases it lacks the ability to provide a mechanistic understanding of how inputs and outputs relate to each other. Explainable Artificial Intelligence (XAI) is a new set of techniques that attempts to provide such an understanding, here we report on some of these practical approaches. We discuss the potential value of XAI to the field of neurostimulation for both basic scientific inquiry and therapeutic purposes, as well as, outstanding questions and obstacles to the success of the XAI approach.","",""
47,"Chengjie Zheng, T. V. Johnson, Aakriti Garg, Michael V. Boland","Artificial intelligence in glaucoma",2019,"","","","",192,"2022-07-13 09:19:02","","10.1097/ICU.0000000000000552","","",,,,,47,15.67,12,4,3,"Purpose of review The use of computers has become increasingly relevant to medical decision-making, and artificial intelligence methods have recently demonstrated significant advances in medicine. We therefore provide an overview of current artificial intelligence methods and their applications, to help the practicing ophthalmologist understand their potential impact on glaucoma care. Recent findings Techniques used in artificial intelligence can successfully analyze and categorize data from visual fields, optic nerve structure [e.g., optical coherence tomography (OCT) and fundus photography], ocular biomechanical properties, and a combination thereof to identify disease severity, determine disease progression, and/or recommend referral for specialized care. Algorithms have become increasingly complex in recent years, utilizing both supervised and unsupervised methods of artificial intelligence. Impressive performance of these algorithms on previously unseen data has been reported, often outperforming standard global indices and expert observers. However, there remains no clearly defined gold standard for determining the presence and severity of glaucoma, which undermines the training of these algorithms. To improve upon existing methodologies, future work must employ more robust definitions of disease, optimize data inputs for artificial intelligence analysis, and improve methods of extracting knowledge from learned results. Summary Artificial intelligence has the potential to revolutionize the screening, diagnosis, and classification of glaucoma, both through the automated processing of large data sets, and by earlier detection of new disease patterns. In addition, artificial intelligence holds promise for fundamentally changing research aimed at understanding the development, progression, and treatment of glaucoma, by identifying novel risk factors and by evaluating the importance of existing ones.","",""
39,"Ashley S. Deeks","The Judicial Demand for Explainable Artificial Intelligence",2019,"","","","",193,"2022-07-13 09:19:02","","","","",,,,,39,13.00,39,1,3,"A recurrent concern about machine learning algorithms is that they operate as “black boxes,” making it difficult to identify how and why the algorithms reach particular decisions, recommendations, or predictions. Yet judges will confront machine learning algorithms with increasing frequency, including in criminal, administrative, and tort cases. This Essay argues that judges should demand explanations for these algorithmic outcomes. One way to address the “black box” problem is to design systems that explain how the algorithms reach their conclusions or predictions. If and as judges demand these explanations, they will play a seminal role in shaping the nature and form of “explainable artificial intelligence” (or “xAI”). Using the tools of the common law, courts can develop what xAI should mean in different legal contexts.    There are advantages to having courts to play this role: Judicial reasoning that builds from the bottom up, using case-by-case consideration of the facts to produce nuanced decisions, is a pragmatic way to develop rules for xAI. Further, courts are likely to stimulate the production of different forms of xAI that are responsive to distinct legal settings and audiences. More generally, we should favor the greater involvement of public actors in shaping xAI, which to date has largely been left in private hands.","",""
41,"C. Macrae","Governing the safety of artificial intelligence in healthcare",2019,"","","","",194,"2022-07-13 09:19:02","","10.1136/bmjqs-2019-009484","","",,,,,41,13.67,41,1,3,"Artificial intelligence (AI) has enormous potential to improve the safety of healthcare, from increasing diagnostic accuracy,1 to optimising treatment planning,2 to forecasting outcomes of care.3 However, integrating AI technologies into the delivery of healthcare is likely to introduce a range of new risks and amplify existing ones. For instance, failures in widely used software have the potential to quickly affect large numbers of patients4; hidden assumptions in underlying data and models can lead to AI systems delivering dangerous recommendations that are insensitive to local care processes,5 6 and opaque AI techniques such as deep learning can make explaining and learning from failure extremely difficult.7 8 To maximise the benefits of AI in healthcare and to build trust among patients and practitioners, it will therefore be essential to robustly govern the risks that AI poses to patient safety.  In a recent review in this journal, Challen and colleagues present an important and timely analysis of some of the key technological risks associated with the application of machine learning in clinical settings.9 Machine learning is a subfield of AI that focuses on the development of algorithms that are automatically derived and optimised through exposure to large quantities of exemplar ‘training’ data.10 The outputs of machine learning algorithms are essentially classifications of patterns that provide some sort of prediction—for instance, predicting whether an image shows a malignant melanoma or a benign mole.11 Some of the basic techniques of machine learning have existed for half a century or more, but progress in the field has accelerated rapidly due to advances in the development of ‘deep’ artificial neural networks12 combined with huge increases in computational power and the availability of enormous quantities of data. These techniques have underpinned recent public demonstrations of AI systems …","",""
44,"L. Gordon, T. Grantcharov, F. Rudzicz","Explainable Artificial Intelligence for Safe Intraoperative Decision Support.",2019,"","","","",195,"2022-07-13 09:19:02","","10.1001/jamasurg.2019.2821","","",,,,,44,14.67,15,3,3,"What Is the Innovation? Intraoperative adverse events are a common and important cause of surgical morbidity.1,2 Strategies to reduce adverse events and mitigate their consequences have traditionally focused on surgical education, structured communication, and adverse event management. However, until now, little could be done to anticipate these events in the operating room. Advances in both data capture in the operating room and explainable artificial intelligence (XAI) techniques to process these data open the way for real-time clinical decision support tools that can help surgical teams anticipate, understand, and prevent intraoperative events. In a systematic review, 64% of studies reported improvements in clinical decisions with automated decision support, especially if suggestions were provided at the same time as the task.3 Machine learning (ML) techniques can provide this real-time decision support, estimating risk automatically from patient and intraoperative data. However, there has been hesitation to adopt ML techniques in health care4 because these systems can have rare catastrophically incorrect predictions, and high accuracies can be achieved in unexpected ways, such as recognizing patterns in the manner of data recording, rather than in the content of the data themselves. Explainable artificial intelligence is a collection of algorithms that improve on traditional ML techniques by providing the evidence behind predictions. For example, while a traditional ML algorithm in radiology may predict that an image contains evidence of cancer, an XAI system will indicate what and where that evidence is (eg, 3 cm, right lower lobe nodule). In 2018, Lundberg et al5 developed an XAI-based warning system called Prescience that predicts hypoxemia during surgical procedures up to 5 minutes before it occurs. This system monitors vital signs and provides the clinician with a risk score that updates in real time. It also continuously updates the clinician with reasons for its predictions, listing risk factors such as vital sign abnormalities and patient comorbidities. This can act like an additional vital sign, regularly updating information to warn the anesthetist in real time about upcoming risk. With XAI, surgeons can receive similar warnings about upcoming intraoperative events to augment their clinical judgement, helping to avoid complications. Our team is currently working in surgical XAI to use laparoscopic videos to warn surgeons about upcoming bleeding events in the operating room and explain this risk in terms of patient and surgical factors. By anticipating and avoiding adverse events, surgical teams may be able to reduce operative times and improve outcomes for patients.","",""
10,"Giuseppina Andresini, Feargus Pendlebury, Fabio Pierazzi, Corrado Loglisci, A. Appice, L. Cavallaro","INSOMNIA: Towards Concept-Drift Robustness in Network Intrusion Detection",2021,"","","","",196,"2022-07-13 09:19:02","","10.1145/3474369.3486864","","",,,,,10,10.00,2,6,1,"Despite decades of research in network traffic analysis and incredible advances in artificial intelligence, network intrusion detection systems based on machine learning (ML) have yet to prove their worth. One core obstacle is the existence of concept drift, an issue for all adversary-facing security systems. Additionally, specific challenges set intrusion detection apart from other ML-based security tasks, such as malware detection. In this work, we offer a new perspective on these challenges. We propose INSOMNIA, a semi-supervised intrusion detector which continuously updates the underlying ML model as network traffic characteristics are affected by concept drift. We use active learning to reduce latency in the model updates, label estimation to reduce labeling overhead, and apply explainable AI to better interpret how the model reacts to the shifting distribution. To evaluate INSOMNIA, we extend TESSERACT - a framework originally proposed for performing sound time-aware evaluations of ML-based malware detectors - to the network intrusion domain. Our evaluation shows that accounting for drifting scenarios is vital for effective intrusion detection systems.","",""
2,"Sunny Raj","Towards Robust Artificial Intelligence Systems",2020,"","","","",197,"2022-07-13 09:19:02","","","","",,,,,2,1.00,2,1,2,"Adoption of deep neural networks (DNNs) into safety-critical and high-assurance systems has been hindered by the inability of DNNs to handle adversarial and out-of-distribution input. State-ofthe-art DNNs misclassify adversarial input and give high confidence output for out-of-distribution input. We attempt to solve this problem by employing two approaches, first, by detecting adversarial input and, second, by developing a confidence metric that can indicate when a DNN system has reached its limits and is not performing to the desired specifications. The effectiveness of our method at detecting adversarial input is demonstrated against the popular DeepFool adversarial image generation method. On a benchmark of 50,000 randomly chosen ImageNet adversarial images generated for CaffeNet and GoogLeNet DNNs, our method can recover the correct label with 95.76% and 97.43% accuracy, respectively. The proposed attribution-based confidence (ABC) metric utilizes attributions used to explain DNN output to characterize whether an output corresponding to an input to the DNN can be trusted. The attribution based approach removes the need to store training or test data or to train an ensemble of models to obtain confidence scores. Hence, the ABC metric can be used when only the trained DNN is available during inference. We test the effectiveness of the ABC metric against both adversarial and out-of-distribution input. We experimental demonstrate that the ABC metric is high for ImageNet input and low for adversarial input generated by FGSM, PGD, DeepFool, CW, and adversarial patch methods. For a DNN trained on MNIST images, ABC metric is high for in-distribution MNIST input and low for out-of-distribution Fashion-MNIST and notMNIST input.","",""
5,"S. Matzka","Explainable Artificial Intelligence for Predictive Maintenance Applications",2020,"","","","",198,"2022-07-13 09:19:02","","10.1109/AI4I49448.2020.00023","","",,,,,5,2.50,5,1,2,"This paper presents and provides a realistic, yet synthetic, predictive maintenance dataset for use in this paper and by the community. An explainable model and an explanatory interface are described, trained using the dataset, and their explanatory performance evaluated and compared.","",""
196,"W. Samek, K. Müller","Towards Explainable Artificial Intelligence",2019,"","","","",199,"2022-07-13 09:19:02","","10.1007/978-3-030-28954-6_1","","",,,,,196,65.33,98,2,3,"","",""
32,"Matt Taddy","The Technological Elements of Artificial Intelligence",2018,"","","","",200,"2022-07-13 09:19:02","","10.3386/W24301","","",,,,,32,8.00,32,1,4,"We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together – each targeting a straightforward prediction task – to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.","",""
