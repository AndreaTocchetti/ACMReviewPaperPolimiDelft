Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
0,"Ethan Li, Emily Liew, Anushya Shankar, Eric Xie, Brian Zheng, Yingying Chen, Yilin Yang","Robustness Testing of Artificial Intelligence-Driven Hand Biometric-Based User Authentication in Mobile Devices",2021,"","","","",1,"2022-07-13 09:21:09","","10.1109/URTC54388.2021.9701620","","",,,,,0,0.00,0,7,1,"User authentication has become increasingly important to secure data digitally. The proposed solution emits an acoustic signal from a device to scan individual hand biometrics for access. This involved sound propagation through a user's hand and analyzing the effects that background noise, hand moisture, hand motion, and hand position have on propagation. MATLAB was used to train and test artificial intelligence models to test robustness. This study found that recordings affected by any of the variables had average validation accuracies of about 90 %. The average validation and test accuracies between different users' baselines were 88 % and 52 %, respectively.","",""
8,"Rui Chen, Meiling Wang, Yi Lai","Analysis of the role and robustness of artificial intelligence in commodity image recognition under deep learning neural network",2020,"","","","",2,"2022-07-13 09:21:09","","10.1371/journal.pone.0235783","","",,,,,8,4.00,3,3,2,"In order to explore the application of the image recognition model based on multi-stage convolutional neural network (MS-CNN) in the deep learning neural network in the intelligent recognition of commodity images and the recognition performance of the method, in the study, the features of color, shape, and texture of commodity images are first analyzed, and the basic structure of deep convolutional neural network (CNN) model is analyzed. Then, 50,000 pictures containing different commodities are constructed to verify the recognition effect of the model. Finally, the MS-CNN model is taken as the research object for improvement to explore the influence of label errors (p = 0.03, 0.05, 0.07, 0.09, 0.12) with different parameter settings and different probabilities (size of convolutional kernel, Dropout rate) on the recognition accuracy of MS-CNN model, at the same time, a CIR system platform based on MS-CNN model is built, and the recognition performance of salt and pepper noise images with different SNR (0, 0.03, 0.05, 0.07, 0.1) was compared, then the performance of the algorithm in the actual image recognition test was compared. The results show that the recognition accuracy is the highest (97.8%) when the convolution kernel size in the MS-CNN model is 2*2 and 3*3, and the average recognition accuracy is the highest (97.8%) when the dropout rate is 0.1; when the error probability of picture label is 12%, the recognition accuracy of the model constructed in this study is above 96%. Finally, the commodity image database constructed in this study is used to identify and verify the model. The recognition accuracy of the algorithm in this study is significantly higher than that of the Minitch stochastic gradient descent algorithm under different SNR conditions, and the recognition accuracy is the highest when SNR = 0 (99.3%). The test results show that the model proposed in this study has good recognition effect in the identification of commodity images in scenes of local occlusion, different perspectives, different backgrounds, and different light intensity, and the recognition accuracy is 97.1%. To sum up, the CIR platform based on MS-CNN model constructed in this study has high recognition accuracy and robustness, which can lay a foundation for the realization of subsequent intelligent commodity recognition technology.","",""
7,"Ming Gao, Runmin Liu, Jie Mao","Noise Robustness Low-Rank Learning Algorithm for Electroencephalogram Signal Classification",2021,"","","","",3,"2022-07-13 09:21:09","","10.3389/fnins.2021.797378","","",,,,,7,7.00,2,3,1,"Electroencephalogram (EEG) is often used in clinical epilepsy treatment to monitor electrical signal changes in the brain of patients with epilepsy. With the development of signal processing and artificial intelligence technology, artificial intelligence classification method plays an important role in the automatic recognition of epilepsy EEG signals. However, traditional classifiers are easily affected by impurities and noise in epileptic EEG signals. To solve this problem, this paper develops a noise robustness low-rank learning (NRLRL) algorithm for EEG signal classification. NRLRL establishes a low-rank subspace to connect the original data space and label space. Making full use of supervision information, it considers the local information preservation of samples to ensure the low-rank representation of within-class compactness and between-classes dispersion. The asymmetric least squares support vector machine (aLS-SVM) is embedded into the objective function of NRLRL. The aLS-SVM finds the maximum quantile distance between the two classes of samples based on the pinball loss function, which further improves the noise robustness of the model. Several classification experiments with different noise intensity are designed on the Bonn data set, and the experiment results verify the effectiveness of the NRLRL algorithm.","",""
0,"Rajole Meghana Bhausaheb","Speed Control of SRM for Hybrid Electric Vehicle Using Artificial Intelligence",2021,"","","","",4,"2022-07-13 09:21:09","","10.1109/ICCCNT51525.2021.9579857","","",,,,,0,0.00,0,1,1,"AI for switched reluctance motor (SRM) drive with integration of front end circuit is appealing for electric vehicle. As SRM carries the highlights like simple construction, high reliability, high fault tolerance capability and low production cost. However, the high torque ripples, running vibrations and acoustic noise are the major drawbacks in SRM. In proposed theory of an Artificial Intelligence for SRM drive overcome this drawback and flip it into advantages like high torque range, low torque ripple and vibration free response with flexible speed control. This paper represents an operating theory of AI for SRM drive. A hybrid ANN Controller is proposed in order to control the speed of the SRM motor. This paper mainly focuses on the comparison of the hybrid ANN controller with conventional PID and to prove the proposed controller provides the best performance and high robustness compared to a conventional PID controller alone. MATLAB/ Simulink are used to simulate.","",""
0,"W. Ho, Tianhao Huang, Po-Yuan Yang, J. Chou, Hong-Siang Huang, Li-Chung Chi, Fu-I Chou, J. Tsai","Artificial intelligence classification model for macular degeneration images: a robust optimization framework for residual neural networks",2021,"","","","",5,"2022-07-13 09:21:09","","10.1186/s12859-021-04085-9","","",,,,,0,0.00,0,8,1,"","",""
0,"Zhaiyi Wang","New Artificial Intelligence Technology Applied in Automobile Lithium Battery Manufacturing",2021,"","","","",6,"2022-07-13 09:21:09","","10.1088/1742-6596/1982/1/012026","","",,,,,0,0.00,0,1,1,"In order to improve the estimation accuracy of the state of charge (SOC) of electric vehicle power batteries, this paper is based on artificial intelligence technology for lithium-ion battery model and parameter identification algorithm, adaptive unscented Kalman filter algorithm and SOC estimation based on battery model fusion Algorithm for research. The simulation results show that the SOC error estimated by the artificial intelligence adaptive Kalman filter method is less than 2.4%, which effectively reduces the impact of unknown interference noise on the battery management system when the electric vehicle is driving. The SOC estimation accuracy is higher than that of the extended Kalman method, and has good robustness.","",""
1,"Eun-Kyeong Kim, Jinyong Kim, Hansoo Lee, Sungshin Kim","Adaptive Data Augmentation to Achieve Noise Robustness and Overcome Data Deficiency for Deep Learning",2021,"","","","",7,"2022-07-13 09:21:09","","10.3390/app11125586","","",,,,,1,1.00,0,4,1,"Artificial intelligence technologies and robot vision systems are core technologies in smart factories. Currently, there is scholarly interest in automatic data feature extraction in smart factories using deep learning networks. However, sufficient training data are required to train these networks. In addition, barely perceptible noise can affect classification accuracy. Therefore, to increase the amount of training data and achieve robustness against noise attacks, a data augmentation method implemented using the adaptive inverse peak signal-to-noise ratio was developed in this study to consider the influence of the color characteristics of the training images. This method was used to automatically determine the optimal perturbation range of the color perturbation method for generating images using weights based on the characteristics of the training images. The experimental results showed that the proposed method could generate new training images from original images, classify noisy images with greater accuracy, and generally improve the classification accuracy. This demonstrates that the proposed method is effective and robust to noise, even when the training data are deficient.","",""
0,"Yeon-Geun Lim, Hyoungju Ji, Jin-Hyun Park, Younsun Kim","Artificial Intelligence-Based Beam Management for High Speed Applications in mmWave Spectrum",2020,"","","","",8,"2022-07-13 09:21:09","","10.1109/GCWkshps50303.2020.9367456","","",,,,,0,0.00,0,4,2,"Future new radio (NR) systems will support high speed applications in the millimeter wave (mmWave) range. To improve user experience, accurate beam management is proposed based on artificial intelligence (AI) technology. This paper also proposes a training set generation method and its manipulation which can properly train the proposed algorithm by exploiting channel characteristics. To collect measurement data before training, we propose a practical data acquisition method based on the reference signal defined in the current NR specification. Evaluation results show that the accuracy approaches 98 percent for beam selection both with ideal hardware and with hardware impairments including phase noise and amplitude noise, as the number of epoch increases. From the results, we confirm that the proposed AI-based beam management can predict the desired beam accurately as well as has robustness to hardware impairments that are crucial in millimeter wave spectrum.","",""
0,"Chi-Yeh Chen, Min-Hsin Huang, Yung-Nien Sun, Chao-Han Lai","Development of Automatic Endotracheal Tube and Carina Detection on Portable Supine Chest Radiographs using Artificial Intelligence",2022,"","","","",9,"2022-07-13 09:21:09","","10.48550/arXiv.2206.03017","","",,,,,0,0.00,0,4,1,"The image quality of portable supine chest radiographs is inherently poor due to low contrast and high noise. The endotracheal intubation detection requires the locations of the endotracheal tube (ETT) tip and carina. The goal is to ﬁnd the distance between the ETT tip and the carina in chest radiography. To overcome such a problem, we propose a feature extraction method with Mask R-CNN. The Mask R-CNN predicts a tube and a tracheal bifurcation in an image. Then, the feature extraction method is used to ﬁnd the feature point of the ETT tip and that of the carina. Therefore, the ETT-carina distance can be obtained. In our experiments, our results can exceed 96% in terms of recall and precision. Moreover, the object error is less than 4 . 7751 ± 5 . 3420 mm, and the ETT-carina distance errors are less than 5 . 5432 ± 6 . 3100 mm. The external validation shows that the proposed method is a high-robustness system. According to the Pearson correlation coeﬃcient, we have a strong correlation between the board-certiﬁed intensivists and our result in terms of ETT-carina distance.","",""
75,"Qing Sun, Min Zhang, A. Mujumdar","Recent developments of artificial intelligence in drying of fresh food: A review",2019,"","","","",10,"2022-07-13 09:21:09","","10.1080/10408398.2018.1446900","","",,,,,75,25.00,25,3,3,"ABSTRACT Intellectualization is an important direction of drying development and artificial intelligence (AI) technologies have been widely used to solve problems of nonlinear function approximation, pattern detection, data interpretation, optimization, simulation, diagnosis, control, data sorting, clustering, and noise reduction in different food drying technologies due to the advantages of self-learning ability, adaptive ability, strong fault tolerance and high degree robustness to map the nonlinear structures of arbitrarily complex and dynamic phenomena. This article presents a comprehensive review on intelligent drying technologies and their applications. The paper starts with the introduction of basic theoretical knowledge of ANN, fuzzy logic and expert system. Then, we summarize the AI application of modeling, predicting, and optimization of heat and mass transfer, thermodynamic performance parameters, and quality indicators as well as physiochemical properties of dried products in artificial biomimetic technology (electronic nose, computer vision) and different conventional drying technologies. Furthermore, opportunities and limitations of AI technique in drying are also outlined to provide more ideas for researchers in this area.","",""
0,"Akos Kovacs, G. Légrádi, A. Wirth, F. Nagy, A. Forgács, S. Barna, I. Garai, T. Bükki","[The value of artificial and human intelligence - the example of bone scintigraphy].",2020,"","","","",11,"2022-07-13 09:21:09","","","","",,,,,0,0.00,0,8,2,"We present a possible method of Artificial Intelligence (AI) based applications that can effectively filter noise-sensitive bone scintigraphy images. The use of special AI, based on preliminary examinations, allows us to significantly reduce study time or activity administered to the patient, thus reducing the patient, assistant, and physician radiation. We present the features of the AI filtering application, its teaching process, which is important to understand, so that the physician can safely take the processed image of the AI as a ""secondary reliable opinion"" to help them make a more accurate diagnosis. We also examine the robustness of the algorithm, the specificities and challenges of complex clinical control.","",""
4,"Valentin Malykh, Taras Khakhulin","Noise Robustness in Aspect Extraction Task",2018,"","","","",12,"2022-07-13 09:21:09","","10.1109/IC-AIAI.2018.8674450","","",,,,,4,1.00,2,2,4,"Aspect extraction from user reviews is one of the sources to make dialog systems, which are on the rise now. A typical user of a conversation system has no time to check the spelling or grammar in his or her utterances. Due to that user utterances contain typos and spelling errors, so the noise robustness should be considered as a significant feature of an aspect extraction model. We analyze noise-robustness of state-of-the-art Attention-Based Aspect Extraction technique and propose the extensions for this model, which lead to more robust behaviour in presence of typos. Experimental results demonstrate how suitable each of the complements to the model that uses the data containing typos.","",""
0,"R. Balaji, N. M. Kutty","Artificial Intelligence used in Robotics",2018,"","","","",13,"2022-07-13 09:21:09","","","","",,,,,0,0.00,0,2,4,"Novel anthropomorphic robotic systems increasingly employ variable impedance actuation with a view to achieving robustness against uncertainty, superior agility and improved efficiency that are hallmarks of biological systems. Controlling and modulating impedance profiles such that they are optimally tuned to the controlled plant is crucial in realizing these benefits. In this work, we propose a methodology to generate optimal control commands for variable impedance actuators under a prescribed tradeoff of task accuracy and energy cost. We employ a supervised learning paradigm to acquire both the plant dynamics and its stochastic properties. This enables us to prescribe an optimal impedance and command profile (i) tuned to the hard-tomodel plant noise characteristics and (ii) adaptable to systematic changes. To evaluate the scalability of our framework to real hardware, we designed and built a novel antagonistic series elastic actuator (SEA) characterized by a simple mechanical architecture and we ran several evaluations on a variety of reach and hold tasks. These results highlight, for the first time on real hardware, how impedance modulation profiles tuned to the plant dynamics emerge from the first principles of stochastic optimization, achieving clear performance gains over classical methods that ignore or are incapable of incorporating stochastic information.","",""
4,"Donghui Li, Xingcong Zhao, Guangjie Yuan, Y. Liu, Guangyuan Liu","Robustness comparison between the capsule network and the convolutional network for facial expression recognition",2020,"","","","",14,"2022-07-13 09:21:09","","10.1007/s10489-020-01895-x","","",,,,,4,2.00,1,5,2,"","",""
1,"M. Tamal","A Phantom Study to Investigate Robustness and Reproducibility of Grey Level Co-Occurrence Matrix (GLCM)-Based Radiomics Features for PET",2021,"","","","",15,"2022-07-13 09:21:09","","10.3390/APP11020535","","",,,,,1,1.00,1,1,1,"Quantification and classification of heterogeneous radiotracer uptake in Positron Emission Tomography (PET) using textural features (termed as radiomics) and artificial intelligence (AI) has the potential to be used as a biomarker of diagnosis and prognosis. However, textural features have been predicted to be strongly correlated with volume, segmentation and quantization, while the impact of image contrast and noise has not been assessed systematically. Further continuous investigations are required to update the existing standardization initiatives. This study aimed to investigate the relationships between textural features and these factors with 18F filled torso NEMA phantom to yield different contrasts and reconstructed with different durations to represent varying levels of noise. The phantom was also scanned with heterogeneous spherical inserts fabricated with 3D printing technology. All spheres were delineated using: (1) the exact boundaries based on their known diameters; (2) 40% fixed; and (3) adaptive threshold. Six textural features were derived from the gray level co-occurrence matrix (GLCM) using different quantization levels. The results indicate that homogeneity and dissimilarity are the most suitable for measuring PET tumor heterogeneity with quantization 64 provided that the segmentation method is robust to noise and contrast variations. To use these textural features as prognostic biomarkers, changes in textural features between baseline and treatment scans should always be reported along with the changes in volumes.","",""
0,"Jiangkuan Li, Meng Lin","Robustness Analysis and Improvement of Fault Diagnosis Model for Nuclear Power Plants Based on Random Forest",2021,"","","","",16,"2022-07-13 09:21:09","","10.1115/icone28-64109","","",,,,,0,0.00,0,2,1,"  With the development of artificial intelligence technology, data-driven methods have become the core of fault diagnosis models in nuclear power plants. Despite the advantages of high flexibility and practicability, data-driven methods may be sensitive to the noise in measurement data, which is inevitable in the process of data measurement in nuclear power plants, especially under fault conditions. In this paper, a fault diagnosis model based on Random Forest (RF) is established. Firstly, its diagnostic performance on noiseless data and noisy data set containing 13 operating conditions (one steady state condition and 12 fault conditions) is analyzed, which shows that the model based on RF has poor robustness under noisy data. In order to improve the robustness of the model under noisy data, a method named ‘Train with Noisy Data’ (TWND) is proposed, the results show that TWND method can effectively improve the robustness of the model based on RF under noisy data, and the degree of improvement is related to the noise levels of added noisy data. This paper can provide reference for robustness analysis and robustness improvement of nuclear power plants fault diagnosis models based on other data-driven methods.","",""
0,"C. R. Rao, V. Ravi, M. Prasad, E. Gopal","Watermarking Using Artificial Intelligence Techniques",2014,"","","","",17,"2022-07-13 09:21:09","","10.4018/978-1-4666-5202-6.CH237","","",,,,,0,0.00,0,4,8,"All Berne union member countries in Berne, Switzerland accepted an international agreement for governing copy right called, The Berne Convention, in 1886 and is modified at Paris in 1971 (Fitzgerald, Brian, Shi, Xiaoxiang, Foong, Cheryl, Pappalardo, & Kylie, 2011). The countries have realized the importance of Intellectual Property Rights (IPR) after the establishment of the World Trade Organization (WTO) in 1995. The term Digital Rights Management (DRM) is applied to refer the protection of copyrights of digital media files (Hannibal Travis, 2008). Digital watermarking represents the authentication through inserting a watermark into the digital content. The quality measures used for digital image watermarking robustness are peak signal to noise ratio (PSNR), normalized correlation (NC), bit correct ratio (BCR) and mean absolute error (MAE) etc. (Shih & Wu, 2005). A number of digital watermarking applications were developed based on Fuzzy logic and Neural Networks applications. The present review is conducted in two broad categories of artificial intelligent techniques. The intelligent techniques considered in the review are (i) Fuzzy logic techniques (ii) different neural network (NN) architectures including Multi-Layer Perception (MLP), Radial basis function neural network (RBFNN) and recurrent neural networks. The review is conducted based on the papers published in journals/international conferences/edited volumes in the areas of digital image watermarking and information hiding. The rest of the paper is organized as follows: Section 2 presents Overview of Fuzzy logic and Neural Networks areas and section 3 presents reviews of papers dealing with the application of fuzzy logic and NN. Section 4 gives insights and section 5 concludes the review with future directions.","",""
0,"N. Homma, K. Fuchigami, M. Sakai, Takakuni Goto, K. Abe","Natural intelligence: noise-resistance of neural spike communication",2008,"","","","",18,"2022-07-13 09:21:09","","10.1007/s10015-007-0485-1","","",,,,,0,0.00,0,5,14,"","",""
14,"R. Khatibi, L. Naghipour, M. Ghorbani, M. Aalami","Predictability of relative humidity by two artificial intelligence techniques using noisy data from two Californian gauging stations",2013,"","","","",19,"2022-07-13 09:21:09","","10.1007/s00521-012-1175-z","","",,,,,14,1.56,4,4,9,"","",""
0,"Minah Lee, Xueyuan She, Biswadeep Chakraborty, Saurabh Dash, B. Mudassar, S. Mukhopadhyay","Reliable Edge Intelligence in Unreliable Environment",2021,"","","","",20,"2022-07-13 09:21:09","","10.23919/DATE51398.2021.9474097","","",,,,,0,0.00,0,6,1,"A key challenge for deployment of artificial intelligence (AI) in real-time safety-critical systems at the edge is to ensure reliable performance even in unreliable environments. This paper will present a broad perspective on how to design AI platforms to achieve this unique goal. First, we will present examples of AI architecture and algorithm that can assist in improving robustness against input perturbations. Next, we will discuss examples of how to make AI platforms robust against hardware induced noise and variation. Finally, we will discuss the concept of using lightweight networks as reliability estimators to generate early warning of potential task failures.","",""
3,"Javier Viaña, Kelly Cohen","Extension to Multidimensional Problems of a Fuzzy- based Explainable & Noise-Resilient Algorithm",2021,"","","","",21,"2022-07-13 09:21:09","","","","",,,,,3,3.00,2,2,1,"While Deep Neural Networks (DNNs) have shown incredible performance in a variety of data, they are brittle and opaque: easily fooled by the presence of noise, and difficult to understand the underlying reasoning for their predictions or choices. This focus on accuracy at the expense of interpretability and robustness caused little concern since, until recently, DNNs were employed primarily for scientific and limited commercial work. An increasing, widespread use of artificial intelligence and growing emphasis on user data protections, however, motivates the need for robust solutions with explainable methods and results. In this work, we extend a novel fuzzy based algorithm for regression to multidimensional problems. Previous research demonstrated that this approach outperforms neural network benchmarks while using only 5% of the number of the parameters.","",""
0,"Bilel Tarchoun, Anouar Ben Khalifa, M. Mahjoub","Investigating the robustness of multi-view detection to current adversarial patch threats",2022,"","","","",22,"2022-07-13 09:21:09","","10.1109/ATSIP55956.2022.9805870","","",,,,,0,0.00,0,3,1,"As deep neural networks are increasingly integrated in our daily lives, the safety and reliability of their results has become of paramount importance. However, the vulnerability of these networks to adversarial attacks are an obstacle to wider adoption, especially in safety-critical applications: A malicious actor can manipulate the results of a deep neural network by adding a nearly imperceptible noise to the input. And adversarial patch attacks make real-life implementations of these threats easier. Therefore, studying these attacks has become a rapidly growing field of artificial intelligence research. One aspect of this research is studying the behavior of patch attacks in various scenarios to understand their inner workings and find novel method to secure deep neural networks. In this paper, we examine the effectiveness of existing adversarial patch attacks against a multi-view detector. To this aim, we propose an evaluation framework where an adversarial patch is trained against a single view of a multi-view dataset and transfer the patch to the other views of the dataset with the use of perspective geometric transforms. Our results confirm that current single-view adversarial patches struggle against multi-view detectors, especially when only few views are attacked. These observations suggest that multi-view detection methods may be a step forward towards reliable and safe AI.","",""
0,"Xuan Chen, Yuena Ma, Shiwei Lu","Use Procedural Noise to Achieve Backdoor Attack",2021,"","","","",23,"2022-07-13 09:21:09","","10.1109/ACCESS.2021.3110239","","",,,,,0,0.00,0,3,1,"In recent years, more researchers pay their attention to the security of artificial intelligence. The backdoor attack is one of the threats and has a powerful, stealthy attack ability. There exist a growing trend towards the triggers is that become dynamic and global. In this paper, we propose a novel global backdoor trigger that is generated by procedural noise. Compared with most triggers, ours are much stealthy and straightforward to implement. In fact, there exist three types of procedural noise, and we evaluate the attack ability of triggers generated by them on the different classification datasets, including CIFAR-10, GTSRB, CelebA, and ImageNet12. The experiment results show that our attack approach can bypass most defense approaches, even for the inspections of humans. We only need poison 5%–10% training data, but the attack success rate(ASR) can reach over 99%. To test the robustness of the backdoor model against the corruption methods that in practice, we introduce 17 corruption methods and compute the accuracy, ASR of the backdoor model with them. The facts show that our backdoor model has strong robustness for most corruption methods, which means it can be applied in reality. Our code is available at https://github.com/928082786/pnoiseattack.","",""
0,"Lingli Long, Yongjin Zhu, Jun Shao, Zheng Kong, Jian Li, Yanzheng Xiang, Xu Zhang","NL2SQL Generation with Noise Labels based on Multi-task Learning",2022,"","","","",24,"2022-07-13 09:21:09","","10.1088/1742-6596/2294/1/012016","","",,,,,0,0.00,0,7,1,"With the rapid development of artificial intelligence technology, semantic recognition technology is becoming more and more mature, providing the preconditions for the development of natural language to SQL (NL2SQL) technology. In the latest research on NL2SQL, the use of pre-trained models as feature extractors for natural language and table schema has led to a very significant improvement in the effectiveness of the models. However, the current models do not take into account the degradation of the noisy labels on the overall SQL statement generation. It is crucial to reduce the impact of noisy labels on the overall SQL generation task and to maximize the return of accurate answers. To address this issue, we propose a restrictive constraint-based approach to mitigate the impact of noise-labeled labels on other tasks. In addition, parameter sharing approach is used in noiseless-labeled labels to capture each part’s correlations and improve the robustness of the model. In addition, we propose to use Kullback-Leibler divergence to constrain the discrepancy between hard and soft constrained coding of noisy labels. Our model is compared with some recent state-of-the-art methods, and experimental results show a significant improvement over the approach in this paper.","",""
0,"C. Jin, Shu-Wei Jin","Intelligence Digital Image Watermark Algorithm Based on Artificial Neural Networks Classifier",2014,"","","","",25,"2022-07-13 09:21:09","","10.1007/978-3-319-06740-7_1","","",,,,,0,0.00,0,2,8,"","",""
6,"Xueyuan She, Yun Long, S. Mukhopadhyay","Improving Robustness of ReRAM-based Spiking Neural Network Accelerator with Stochastic Spike-timing-dependent-plasticity",2019,"","","","",26,"2022-07-13 09:21:09","","10.1109/IJCNN.2019.8851825","","",,,,,6,2.00,2,3,3,"Spike-timing-dependent-plasticity (STDP) is an unsupervised learning algorithm for spiking neural network (SNN), which promises to achieve deeper understanding of human brain and more powerful artificial intelligence. While conventional computing system fails to simulate SNN efficiently, process-inmemory (PIM) based on devices such as ReRAM can be used in designing fast and efficient STDP based SNN accelerators, as it operates in high resemblance with biological neural network. However, the real-life implementation of such design still suffers from impact of input noise and device variation. In this work, we present a novel stochastic STDP algorithm that uses spiking frequency information to dynamically adjust synaptic behavior. The algorithm is tested in pattern recognition task with noisy input and shows accuracy improvement over deterministic STDP. In addition, we show that the new algorithm can be used for designing a robust ReRAM based SNN accelerator that has strong resilience to device variation.","",""
1,"Z. Guan","A robust intelligence algorithm based on optimized K-Means for GPP contour extraction",2019,"","","","",27,"2022-07-13 09:21:09","","10.1109/ITAIC.2019.8785454","","",,,,,1,0.33,1,1,3,"for electronic components that are not packaged, traditional contour extraction algorithms do not work well because of the complex chip surface. This paper proposes a robust intelligent algorithm for GPP defect contour extraction based on optimized k-means. Firstly, the image is binarized by the optimized k-means algorithm based on the GPP gray value division of grain surface. Then, morphological operation and median filtering are applied to filter out the noise. Finally, canny operator's convolution template is used to calculate the amplitude gradient of the image to determine the boundary of the contour. Compared with Canny algorithm and Soble algorithm, the proposed algorithm has good robustness for GPP defect contour extraction and high boundary strength, which can be applied in industrial production.","",""
23,"M. Zamani, Hamed Taherdoost, A. Manaf, R. Ahmad, A. Zeki","An artificial-intelligence-based approach for audio steganography",2009,"","","","",28,"2022-07-13 09:21:09","","","","",,,,,23,1.77,5,5,13,"This paper presents a novel, principled approach to resolve the remained problems of substitution technique of audio steganography. Using the proposed genetic algorithm, message bits are embedded into multiple, vague and higher LSB layers, resulting in increased robustness. The robustness specially would be increased against those intentional attacks which try to reveal the hidden message and also some unintentional attacks like noise addition as well.","",""
2,"Derek M. Shockey","A comparative study of the robustness of voting systems under various models of noise",2008,"","","","",29,"2022-07-13 09:21:09","","","","",,,,,2,0.14,2,1,14,"While the study of election theory is not a new field in and of itself, recent research has applied various concepts in computer science to the study of social choice theory, which includes election theory. From a security perspective, it is pertinent to investigate how stable election systems are in the face of noise, disruption, and manipulation. Recently, work related to computational election systems has also been of interest to artificial intelligence researchers, where it is incorporated into the decision-making processes of distributed systems. The quantitative analysis of a voting rule’s resistance to noise is the robustness, the probability of how likely the outcome of the election is to change given a certain amount of noise. Prior research has studied the robustness of voting rules under very small amounts of noise, e.g. swapping the ranking of two adjacent candidates in one vote. Our research expands upon this previous work by considering a more disruptive form of noise: an arbitrary reordering of an entire vote. Given k noise disruptions, we determine how likely the election is to remain unchanged (the k-robustness) by relating the k-robustness to the 1-robustness. We can thereby provide upper and/or lower bounds on the robustness of voting rules; specifically, we examine five well-established rules: scoring rules (a general class of rules, containing Borda, plurality, and veto, among others), Copeland, Maximin (also known as Minimax or Simpson–Kramer), Bucklin, and plurality with runoff.","",""
10,"Michael R. Smith, T. Martinez","The robustness of majority voting compared to filtering misclassified instances in supervised classification tasks",2016,"","","","",30,"2022-07-13 09:21:09","","10.1007/s10462-016-9518-2","","",,,,,10,1.67,5,2,6,"","",""
20,"Sheeba Lal, S. Rehman, J. H. Shah, Talha Meraj, Hafiz Tayyab Rauf, Robertas Damaševičius, M. Mohammed, Karrar Hameed Abdulkareem","Adversarial Attack and Defence through Adversarial Training and Feature Fusion for Diabetic Retinopathy Recognition",2021,"","","","",31,"2022-07-13 09:21:09","","10.3390/s21113922","","",,,,,20,20.00,3,8,1,"Due to the rapid growth in artificial intelligence (AI) and deep learning (DL) approaches, the security and robustness of the deployed algorithms need to be guaranteed. The security susceptibility of the DL algorithms to adversarial examples has been widely acknowledged. The artificially created examples will lead to different instances negatively identified by the DL models that are humanly considered benign. Practical application in actual physical scenarios with adversarial threats shows their features. Thus, adversarial attacks and defense, including machine learning and its reliability, have drawn growing interest and, in recent years, has been a hot topic of research. We introduce a framework that provides a defensive model against the adversarial speckle-noise attack, the adversarial training, and a feature fusion strategy, which preserves the classification with correct labelling. We evaluate and analyze the adversarial attacks and defenses on the retinal fundus images for the Diabetic Retinopathy recognition problem, which is considered a state-of-the-art endeavor. Results obtained on the retinal fundus images, which are prone to adversarial attacks, are 99% accurate and prove that the proposed defensive model is robust.","",""
19,"Henry Gerdes, P. Casado, A. Dokal, Maruan Hijazi, N. Akhtar, Ruth Osuntola, V. Rajeeve, J. Fitzgibbon, Jon Travers, D. Britton, S. Khorsandi, P. Cutillas","Drug ranking using machine learning systematically predicts the efficacy of anti-cancer drugs",2021,"","","","",32,"2022-07-13 09:21:09","","10.1038/s41467-021-22170-8","","",,,,,19,19.00,2,12,1,"","",""
3,"Yewei Zhang, Kejie Huang, R. Xiao, Bo Wang, Yanfeng Xu, Jicong Fan, Haibin Shen","An 8-Bit in Resistive Memory Computing Core With Regulated Passive Neuron and Bitline Weight Mapping",2020,"","","","",33,"2022-07-13 09:21:09","","10.1109/tvlsi.2022.3140395","","",,,,,3,1.50,0,7,2,"The rapid development of artificial intelligence (AI) and Internet of Things (IoT) increase the requirement for edge computing with low power and relatively high processing speed devices. The computing-in-memory (CIM) schemes based on emerging resistive nonvolatile memory (NVM) show great potential in reducing the power consumption for AI computing. However, the inconsistency of the NVM may significantly degenerate the performance of the neural network. In this article, we propose a low power resistive RAM (RRAM)-based CIM core to not only achieve high computing efficiency but also greatly enhance the robustness by bit line (BL) regulator and BL weight mapping algorithm. The simulation results show that the power consumption of our proposed 8-bit CIM core is only 12.6 mW ( $256\times 256$ at 8b). The spurious-free dynamic range (SFDR) and signal to noise and distortion ratio (SNDR) of the CIM core achieve 62.64 and 45.92 dB, respectively. The proposed BL weight mapping scheme improves the top-1 accuracy by 2.46% and 3.47% for AlexNet and VGG16 on ImageNet Large Scale Visual Recognition Competition 2012 (ILSVRC 2012) in 8-bit mode, respectively.","",""
2,"R. Mohamad, Harlisya Harun, M. Mokhtar, W. Adnan, K. Dimyati","On the robustness of measurement of reliability stopping criterion in turbo iterative decoding",2015,"","","","",34,"2022-07-13 09:21:09","","10.1109/SNPD.2015.7176188","","",,,,,2,0.29,0,5,7,"Measurement of reliability (MOR) stopping criterion is able to terminate early in the low and high signal-to-noise ratio (SNR) while maintaining the bit error rate (BER) performance. However, the performance of MOR is only based on one code structure and hence, the robustness of MOR is still unknown in turbo iterative decoding. Thus, this paper will test the robustness of MOR based on the following parameters: frame size, code structure, channel reliability and code rate. Then, we analyse and compare the average iteration number (AIN) and the BER performance of MOR with the benchmark stopping criterion known as Genie to determine the robustness of MOR. From the analysis, MOR has a BER degradation for low code rate. MOR also fails to perform well if the corret channel reliability is not available at the receiver and this results a large degradation in BER performance. However, MOR has close performance to Genie in terms of BER for various frame sizes, code structures and high code rate with the assistance of correct channel reliability. MOR is also able to save AIN at low SNR as compared to Genie and this can reduce delay and complexity of turbo codes.","",""
4,"S. Back, Seongju Lee, Sungho Shin, Yeonguk Yu, Taekyeong Yuk, Saepomi Jong, Seungjun Ryu, Kyoobin Lee","Robust Skin Disease Classification by Distilling Deep Neural Network Ensemble for the Mobile Diagnosis of Herpes Zoster",2021,"","","","",35,"2022-07-13 09:21:09","","10.1109/ACCESS.2021.3054403","","",,,,,4,4.00,1,8,1,"Herpes zoster (HZ) is a common cutaneous disease affecting one out of five people; hence, early diagnosis of HZ is crucial as it can progress to chronic pain syndrome if antiviral treatment is not provided within 72 hr. Mobile diagnosis of HZ with the assistance of artificial intelligence can prevent neuropathic pain while reducing clinicians’ fatigue and diagnosis cost. However, the clinical images captured from daily mobile devices likely contain visual corruptions, such as motion blur and noise, which can easily mislead the automated system. Hence, this paper aims to train a robust and mobile deep neural network (DNN) that can distinguish HZ from other skin diseases using user-submitted images. To enhance robustness while retaining low computational cost, we propose a knowledge distillation from ensemble via curriculum training (KDE-CT) wherein a student network learns from a stronger teacher network progressively. We established skin diseases dataset for HZ diagnosis and evaluated the robustness against 75 types of corruption. A total of 13 different DNNs was evaluated on both clean and corrupted images. The experiment result shows that the proposed KDE-CT significantly improves corruption robustness when compared with other methods. Our trained MobileNetV3-Small achieved more robust performance (93.5% overall accuracy, 67.6 mean corruption error) than the DNN ensemble with smaller computation (549x fewer multiply-and-accumulate operations), which makes it suitable for mobile skin lesion analysis.","",""
1,"Javier Maroto, Gérôme Bovet, P. Frossard","On the benefits of robust models in modulation recognition",2021,"","","","",36,"2022-07-13 09:21:09","","10.1117/12.2587156","","",,,,,1,1.00,0,3,1,"Given the rapid changes in telecommunication systems and their higher dependence on artificial intelligence, it is increasingly important to have models that can perform well under different, possibly adverse, conditions. Deep Neural Networks (DNNs) using convolutional layers are state-of-the-art in many tasks in communications. However, in other domains, like image classification, DNNs have been shown to be vulnerable to adversarial perturbations, which consist of imperceptible crafted noise that when added to the data fools the model into misclassification. This puts into question the security of DNNs in communication tasks, and in particular in modulation recognition. We propose a novel framework to test the robustness of current state-of-the-art models where the adversarial perturbation strength is dependent on the signal strength and measured with the “signal to perturbation ratio” (SPR). We show that current state-of-the-art models are susceptible to these perturbations. In contrast to current research on the topic of image classification, modulation recognition allows us to have easily accessible insights on the usefulness of the features learned by DNNs by looking at the constellation space. When analyzing these vulnerable models we found that adversarial perturbations do not shift the symbols towards the nearest classes in constellation space. This shows that DNNs do not base their decisions on signal statistics that are important for the Bayes-optimal modulation recognition model, but spurious correlations in the training data. Our feature analysis and proposed framework can help in the task of finding better models for communication systems.","",""
1,"Somya Sharma, Snigdhansu Chatterjee","Winsorization for Robust Bayesian Neural Networks",2021,"","","","",37,"2022-07-13 09:21:09","","10.3390/e23111546","","",,,,,1,1.00,1,2,1,"With the advent of big data and the popularity of black-box deep learning methods, it is imperative to address the robustness of neural networks to noise and outliers. We propose the use of Winsorization to recover model performances when the data may have outliers and other aberrant observations. We provide a comparative analysis of several probabilistic artificial intelligence and machine learning techniques for supervised learning case studies. Broadly, Winsorization is a versatile technique for accounting for outliers in data. However, different probabilistic machine learning techniques have different levels of efficiency when used on outlier-prone data, with or without Winsorization. We notice that Gaussian processes are extremely vulnerable to outliers, while deep learning techniques in general are more robust.","",""
0,"A. Papandreou, A. Kloukiniotis, A. Lalos, K. Moustakas","Deep multi-modal data analysis and fusion for robust scene understanding in CAVs",2021,"","","","",38,"2022-07-13 09:21:09","","10.1109/MMSP53017.2021.9733604","","",,,,,0,0.00,0,4,1,"Deep learning (DL) tends to be the integral part of Autonomous Vehicles (AVs). Therefore the development of scene analysis modules that are robust to various vulnerabilities such as adversarial inputs or cyber-attacks is becoming an imperative need for the future AV perception systems. In this paper, we deal with this issue by exploring the recent progress in Artificial Intelligence (AI) and Machine Learning (ML) to provide holistic situational awareness and eliminate the effect of the previous attacks on the scene analysis modules. We propose novel multi-modal approaches against which achieve robustness to adversarial attacks, by appropriately modifying the analysis Neural networks and by utilizing late fusion methods. More specifically, we propose a holistic approach by adding new layers to a 2D segmentation DL model enhancing its robustness to adversarial noise. Then, a novel late fusion technique has been applied, by extracting direct features from the 3D space and project them into the 2D segmented space for identifying inconsistencies. Extensive evaluation studies using the KITTI odometry dataset provide promising performance results under various types of noise.","",""
0,"Enda Du, Yuetian Liu, Ziyan Cheng, Liang Xue, Jing Ma, Xuan He","Production Forecasting with the Interwell Interference by Integrating Graph Convolutional and Long Short-Term Memory Neural Network",2021,"","","","",39,"2022-07-13 09:21:09","","10.2118/208596-pa","","",,,,,0,0.00,0,6,1,"  Accurate production forecasting is an essential task and accompanies the entire process of reservoir development. With the limitation of prediction principles and processes, the traditional approaches are difficult to make rapid predictions. With the development of artificial intelligence, the data-driven model provides an alternative approach for production forecasting. To fully take the impact of interwell interference on production into account, this paper proposes a deep learning-based hybrid model (GCN-LSTM), where graph convolutional network (GCN) is used to capture complicated spatial patterns between each well, and long short-term memory (LSTM) neural network is adopted to extract intricate temporal correlations from historical production data. To implement the proposed model more efficiently, two data preprocessing procedures are performed: Outliers in the data set are removed by using a box plot visualization, and measurement noise is reduced by a wavelet transform. The robustness and applicability of the proposed model are evaluated in two scenarios of different data types with the root mean square error (RMSE), the mean absolute error (MAE), and the mean absolute percentage error (MAPE). The results show that the proposed model can effectively capture spatial and temporal correlations to make a rapid and accurate oil production forecast.","",""
0,"Shuo Liu, Liwen Xu, Jin-Rong Wang, Yan Sun, Zeran Qin","LCR-GAN: Learning Crucial Representation for Anomaly Detection",2021,"","","","",40,"2022-07-13 09:21:09","","10.1145/3507548.3508229","","",,,,,0,0.00,0,5,1,"Anomaly detection is pivotal and challenging in artificial intelligence, which aims to determine whether a query sample comes from the same class, given a set of normal samples from a particular class. There are a plethora of anomaly detection methods based on generative models; however, these methods aim to make the reconstruction error of the training samples smaller or extract more information from the training samples. We believe that it is more important for anomaly detection to extract crucial representation from normal samples rather than more information, so we propose a semi-supervised method named LCR-GAN. We conducted extensive experiments on four image datasets and 15 tabular datasets to demonstrate the effectiveness of the proposed method. Meanwhile, we also carried out an anti-noise study to demonstrate the robustness of the proposed method.","",""
0,"Qiaochu Gao, Zhiwei Cao, Dou Li","Defensive Distillation Based End-to-end Auto-encoder Communication System",2021,"","","","",41,"2022-07-13 09:21:09","","10.1109/iccc54389.2021.9674255","","",,,,,0,0.00,0,3,1,"The new generation of wireless communication systems proposes the vision that artificial intelligence should play a more significant role in the development of techniques, thus the application of deep learning (DL) in communications has attracted much attention. DL based auto-encoders have been considered to model an end-to-end communication system. In the situation of DL based communication systems, security problems also need to be concerned. Attackers may utilize the broadcast nature of wireless channels to add perturbations which lead to error transmission. In order to defend against such attacks, a defensive distillation based end-to-end system is proposed. Distillation is introduced to modify the structure of the neural network so that it assists to enhance the robustness of the system model against perturbation and decrease error transmission. Additionally, the robustness against channel noise is also demonstrated in the research.","",""
0,"Yinsheng Luo, Jianhao Hu","Symbol detection based on temporal convolutional network in optical communications",2022,"","","","",42,"2022-07-13 09:21:09","","10.23919/JCC.2022.01.021","","",,,,,0,0.00,0,2,1,"Deep learning (DL) is one of the fastest developing areas in artificial intelligence, it has been recently gained studies and application in computer vision, automatic driving, automatic speech recognition, and communication. This paper uses the DL method to design a symbol detection algorithm in receiver for optical communication systems. The proposed DL based method is implemented by a non-causal temporal convolutional network (ncTCN), which is a convolutional neural network and appropriate for sequence processing. Meanwhile, we adopt three methods to realize the training process for multiple signal-to-noise ratios of the AWGN channel. Furthermore, we apply two nonlinear activation functions for the noise robustness to the proposed ncTCN. Without losing generality, we apply the ncTCN-based receiver to the 16-ary quadrature amplitude modulation optical communication system in the simulation experiment. According to the experiment results, the proposed method can obtain some bit error rate performance gain compared to some conventional receivers.","",""
0,"Yuan Zhang, Sen Liu, Zhihui He, Yuwei Zhang, Changming Wang","A CNN Model for Cardiac Arrhythmias Classification Based on Individual ECG Signals",2022,"","","","",43,"2022-07-13 09:21:09","","10.1007/s13239-021-00599-8","","",,,,,0,0.00,0,5,1,"","",""
0,"F. Marulli, S. Marrone, Laura Verde","Sensitivity of Machine Learning Approaches to Fake and Untrusted Data in Healthcare Domain",2022,"","","","",44,"2022-07-13 09:21:09","","10.3390/jsan11020021","","",,,,,0,0.00,0,3,1,"Machine Learning models are susceptible to attacks, such as noise, privacy invasion, replay, false data injection, and evasion attacks, which affect their reliability and trustworthiness. Evasion attacks, performed to probe and identify potential ML-trained models’ vulnerabilities, and poisoning attacks, performed to obtain skewed models whose behavior could be driven when specific inputs are submitted, represent a severe and open issue to face in order to assure security and reliability to critical domains and systems that rely on ML-based or other AI solutions, such as healthcare and justice, for example. In this study, we aimed to perform a comprehensive analysis of the sensitivity of Artificial Intelligence approaches to corrupted data in order to evaluate their reliability and resilience. These systems need to be able to understand what is wrong, figure out how to overcome the resulting problems, and then leverage what they have learned to overcome those challenges and improve their robustness. The main research goal pursued was the evaluation of the sensitivity and responsiveness of Artificial Intelligence algorithms to poisoned signals by comparing several models solicited with both trusted and corrupted data. A case study from the healthcare domain was provided to support the pursued analyses. The results achieved with the experimental campaign were evaluated in terms of accuracy, specificity, sensitivity, F1-score, and ROC area.","",""
0,"Yiheng Chen, Huarong Xu","Research and Implementation of Recognition Algorithm of Long-distance Runners Based on Deep Learning",2022,"","","","",45,"2022-07-13 09:21:09","","10.1145/3523286.3524546","","",,,,,0,0.00,0,2,1,"The recognition of long-distance runners is mainly used to retrieve specific long-distance runner targets across video equipment in long-distance running events, which helps to improve the efficiency of the management of long-distance running events. At present, with the rapid development of artificial intelligence, lots of scholars utilize deep learning-based Person Re-Identification(Re-ID) technology to achieve long-distance runner recognition tasks. However, in practical applications, problems such as occlusion, noise, brightness changes, and color shifts usually affect the collected images of long-distance runners, thereby reducing the recognition accuracy of the existing Re-ID technology. For this reason, this paper proposes a recognition network for long-distance runners named Ldrr-net based on deep learning.Ldrr-net introduces the IGBN structure into the backbone network called Resnet50, which can reduce the adverse effects caused by the captured images, and has stronger robustness. In addition, we modify the loss, and propose Ldrr-loss to train network parameters, so that the network can better achieve intra-class aggregation and inter-class separation in the case of occlusion and similar features, and further improve the accuracy of long-distance runners' recognition. Experiments show that Ldrr-net has certain advantages in the recognition task of long-distance runners.","",""
4,"Sunday Iliya, E. Goodyer, J. Shell, M. Gongora, J. Gow","Optimized Neural Network using differential evolutionary and swarm intelligence optimization algorithms for RF power prediction in cognitive radio network: A comparative study",2014,"","","","",46,"2022-07-13 09:21:09","","10.1109/ICASTECH.2014.7068129","","",,,,,4,0.50,1,5,8,"Cognitive radio (CR) technology has emerged as a promising solution to many wireless communication problems including spectrum scarcity and underutilization. The a priory knowledge of Radio Frequency (RF) power (primary signals and/ or interfering signals plus noise) in the channels to be exploited by CR is of paramount importance. This will enable the selection of channel with less noise among idle (free) channels. Computational Intelligence (CI) techniques can be applied to these scenarios to predict the required RF power in the available channels to achieve optimum Quality of Service (QoS). In this paper, we developed a time domain based optimized Artificial Neural Network (ANN) model for the prediction of real world RF power within the GSM 900, Very High Frequency (VHF) and Ultra High Frequency (UHF) TV bands. The application of the models produced was found to increase the robustness of CR applications, specifically where the CR had no prior knowledge of the RF power related parameters such as signal to noise ratio, bandwidth and bit error rate. The models used, implemented a novel and innovative initial weight optimization of the ANN's through the use of differential evolutionary and swarm intelligence algorithms. This was found to enhance the accuracy and generalization of the ANN model. For this problem, DE/best/1/bin was found to yield a better performance as compared with the other algorithms implemented.","",""
6,"Lin Li, Huijun Hou, W. Meng","Convolutional-Neural-Network-Based Detection Algorithm for Uplink Multiuser Massive MIMO Systems",2020,"","","","",47,"2022-07-13 09:21:09","","10.1109/ACCESS.2020.2985083","","",,,,,6,3.00,2,3,2,"In the coming 6th generation (6G) and beyond in wireless communication, an increasing number of ultrascale intelligent factors, including mobile robot users and smart cars, will result in interference exploitation. The management of this exploitation will be a great challenge for detection algorithms in uplink massive multiple-input and multiple-output (MIMO) systems, especially for high-order quadrature amplitude modulation (QAM) signals. Artificial intelligence technology employing machine learning is one of the key approaches among the 6G technical solutions. In this paper, a convolutional-neural-network-based likelihood ascent search (CNNLAS) detection algorithm is proposed on the basis of a graphical detection model for uplink multiuser massive MIMO systems. Compared with other algorithms, the proposed CNNLAS detection algorithm has a stronger robustness against the channel estimation errors, and requires lower average received signal-to-noise ratios to obtain better bit error rate performance and to achieve the theoretical spectral efficiency with a lower polynomial average per symbol computational complexity, both for the graphical low-order and high-order QAM signals in uplink multiuser massive MIMO systems.","",""
7,"Vasisht Duddu, N. Pillai, D. V. Rao, V. Balas","Fault Tolerance of Neural Networks in Adversarial Settings",2019,"","","","",48,"2022-07-13 09:21:09","","10.3233/JIFS-179677","","",,,,,7,2.33,2,4,3,"Artificial Intelligence systems require a through assessment of different pillars of trust, namely, fairness, interpretability, data and model privacy, reliability (safety) and robustness against against adversarial attacks. While these research problems have been extensively studied in isolation, an understanding of the trade-off between different pillars of trust is lacking. To this extent, the trade-off between fault tolerance, privacy and adversarial robustness is evaluated for the specific case of Deep Neural Networks, by considering two adversarial settings under a security and a privacy threat model. Specifically, this work studies the impact of the fault tolerance of the Neural Network on training the model by adding noise to the input (Adversarial Robustness) and noise to the gradients (Differential Privacy). While training models with noise to inputs, gradients or weights enhances fault tolerance, it is observed that adversarial robustness and fault tolerance are at odds with each other. On the other hand, ($\epsilon,\delta$)-Differentially Private models enhance the fault tolerance, measured using generalisation error, theoretically has an upper bound of $e^{\epsilon} - 1 + \delta$. This novel study of the trade-off between different elements of trust is pivotal for training a model which satisfies the requirements for different pillars of trust simultaneously.","",""
4,"K. Nakadai, H. Okuno","Robot Audition and Computational Auditory Scene Analysis",2020,"","","","",49,"2022-07-13 09:21:09","","10.1002/aisy.202000050","","",,,,,4,2.00,2,2,2,"Robot audition aims at developing robot's ears that work in the real world, that is, machine listening of multiple sound sources. Its critical problem is noise. Speech interfaces have become more familiar and more indispensable as smartphones and artificial intelligence (AI) speakers spread. Their critical problems are noise and multiple simultaneous speakers. Recently two technological advances have contributed to significantly improve the performance of speech interfaces and robot audition. Emerging deep learning technology has improved noise robustness of automatic speech recognition, whereas microphone array processing has improved the performance of preprocessing such as noise reduction. Herein, an overview and history of robot audition are provided together with introduction of an open‐source software for robot audition and its wide applications in the real world. Also, it is discussed how robot audition contributes to the development of computational auditory scene analysis, that is, understanding of real‐world auditory environments.","",""
24,"Ching-Chun Chang, Chang-Tsun Li, Kaimeng Chen","Privacy-Preserving Reversible Information Hiding Based on Arithmetic of Quadratic Residues",2019,"","","","",50,"2022-07-13 09:21:09","","10.1109/ACCESS.2019.2908924","","",,,,,24,8.00,8,3,3,"The phenomenal advances of cloud computing technology have given rise to the research area of privacy-preserving signal processing, which aims to preserve information privacy even when the signals are processed in an insecure environment. Privacy-preserving information hiding is a multidisciplinary study that has opened up a great deal of intriguing real-life applications, such as data exfiltration prevention, data origin authentication, and electronic data management. Information hiding is a practice of embedding intended messages into carrier signals through imperceptible alterations. In view of some content-sensitive scenarios, however, the ability to preserve perfect copies of signals is of crucial importance, for instance, considering the inadequate robustness of recent artificial intelligence-aided automated systems against noise perturbations. Reversibility of information hiding systems is a valuable property that permits recovery of original carrier signals if desired. In this paper, we propose a novel privacy-preserving reversible information hiding scheme inspired by the mathematical concept of quadratic residues. A quadratic residue has four (not necessarily distinct) square roots, which enables payloads to be encoded in a dynamic fashion. Furthermore, a predictive model based upon the projection theorem is devised to assist carrier signal recovery. The experimental results showed significant improvements over the state-of-the-art methods with regard to capacity, fidelity, and reversibility.","",""
3,"A. Lalos, Evangelos Vlachos, K. Berberidis, A. Fournaris, C. Koulamas","Privacy Preservation in Industrial IoT via Fast Adaptive Correlation Matrix Completion",2020,"","","","",51,"2022-07-13 09:21:09","","10.1109/TII.2019.2960275","","",,,,,3,1.50,1,5,2,"The Industrial Internet of Things (IIoT) is a key element of industry 4.0, bringing together modern sensor technology, fog and cloud computing platforms, and artificial intelligence to create smart, self-optimizing industrial equipment and facilities. Though, the scale and sensitivity degree of information continuously increases, giving rise to serious privacy concerns. The scope of this article is to provide efficient privacy preservation techniques, by tracking the correlation of multivariate streams recorded in a network of IIoT devices. The time-varying data covariance matrix is used to add noise that cannot be easily removed by filtering, generating obfuscated measurements and, thus, preventing unauthorized access to the original data. To improve communication efficiency between connected IoT devices, we exploit inherent properties of the correlation matrices, and track the essential correlations from a small subset of correlation values. Extensive simulation studies using constrained IIoT devices validate the robustness, efficiency, and effectiveness of our approach.","",""
1,"Yi Zhang, Kai Lu, Yinghui Gao, Bo Yang","Analysis of image thresholding segmentation algorithms based on swarm intelligence",2013,"","","","",52,"2022-07-13 09:21:09","","10.1117/12.2010732","","",,,,,1,0.11,0,4,9,"Swarm intelligence-based image thresholding segmentation algorithms are playing an important role in the research field of image segmentation. In this paper, we briefly introduce the theories of four existing image segmentation algorithms based on swarm intelligence including fish swarm algorithm, artificial bee colony, bacteria foraging algorithm and particle swarm optimization. Then some image benchmarks are tested in order to show the differences of the segmentation accuracy, time consumption, convergence and robustness for Salt&Pepper noise and Gaussian noise of these four algorithms. Through these comparisons, this paper gives qualitative analyses for the performance variance of the four algorithms. The conclusions in this paper would give a significant guide for the actual image segmentation.","",""
1,"Pedro Cardoso, V. Branco, P. Borges, J. Carvalho, F. Rigal, R. Gabriel, S. Mammola, J. Cascalho, Luís Correia","Automated Discovery of Relationships, Models, and Principles in Ecology",2020,"","","","",53,"2022-07-13 09:21:09","","10.3389/fevo.2020.530135","","",,,,,1,0.50,0,9,2,"Ecological systems are the quintessential complex systems, involving numerous high-order interactions and non-linear relationships. The most used statistical modeling techniques can hardly accommodate the complexity of ecological patterns and processes. Finding hidden relationships in complex data is now possible using massive computational power, particularly by means of artificial intelligence and machine learning methods. Here we explored the potential of symbolic regression (SR), commonly used in other areas, in the field of ecology. Symbolic regression searches for both the formal structure of equations and the fitting parameters simultaneously, hence providing the required flexibility to characterize complex ecological systems. Although the method here presented is automated, it is part of a collaborative human–machine effort and we demonstrate ways to do it. First, we test the robustness of SR to extreme levels of noise when searching for the species-area relationship. Second, we demonstrate how SR can model species richness and spatial distributions. Third, we illustrate how SR can be used to find general models in ecology, namely new formulas for species richness estimators and the general dynamic model of oceanic island biogeography. We propose that evolving free-form equations purely from data, often without prior human inference or hypotheses, may represent a very powerful tool for ecologists and biogeographers to become aware of hidden relationships and suggest general theoretical models and principles.","",""
1,"José Luis M. Pérez, Roberto S. M. Barros, S. G. T. C. Santos","Statistical Tests Ensemble Drift Detector",2020,"","","","",54,"2022-07-13 09:21:09","","10.1109/SSCI47803.2020.9308267","","",,,,,1,0.50,0,3,2,"Several classifiers use supervised inductive learning and, to improve accuracy, they are often combined with concept drift detectors. The ideal learning algorithm associates robustness to noise with sensitivity to concept drifts. Motivated by these statements, this paper proposes a concept drift detector aiming to validate empirically the idea of implementing a drift detection method based on the combination of statistical tests as a viable option to improve the classification. The Statistical Tests Ensemble Detector (STED) uses the results of Brown-Forsythe, O’Brien, and ANOVA statistical tests combined by two voting strategies. To signal warnings, the majority vote is used with the results of the three tests, and, to detect concept drifts, the “Earlyfind-early-report” rule is adopted with the results of the Brown-Forsythe and O’Brien tests only. The experiments’ results using Hoeffding Tree (HT) as base learner in 24 artificial and seven real-world datasets corroborates the efficiency of this proposal: STED achieved the best accuracies and was balanced in the detections of drifts, based on its second position in the evaluation using the Matthews Correlation Coefficient (MCC).","",""
0,"San Hlaing Myint, Yutaka Katsuyama, Toshio Sato, Xin Qi, Zheng Wen, Keping Yu, K. Tokuda, Takuro Sato","Radiometric Passive Imaging for Robust Concealed Object Identification",2020,"","","","",55,"2022-07-13 09:21:09","","10.1109/RadarConf2043947.2020.9266642","","",,,,,0,0.00,0,8,2,"Artificial Intelligence (AI) based millimeter wave radiometric imaging has become popular in a wide range of public security check systems, such as concealed object detection and identification. However, the low radiometric temperature contrast between small objects and low sensitivity is restricted to some extent. In this paper, an advanced radiometric passive imaging simulation model is proposed to improve the radiometric temperature contrast. This model considers additional noise, such as blur, variation in sensors, noise sources and summation of the number of frames. We establish a comprehensive training dataset that considers the physical characteristics of concealed objects. It can effectively fill the lack of a large database to avoid deteriorating the identification accuracy of AI applications. Moreover, it is also a key solution for improving the robustness of AI based object identification by using a convolutional neural network (CNN). Finally, simulation results are presented and analyzed to validate the proposed comprehensive training dataset and simulation model. Consequently, the proposed simulation model can effectively improve the robustness and accuracy of AI-based concealed object identification.","",""
5,"L. Negri, P. Bertemes-Filho, A. S. Paterno","Computational Intelligence Algorithms for Bioimpedance-Based Classification of Biological Material",2011,"","","","",56,"2022-07-13 09:21:09","","10.1007/978-3-642-23508-5_318","","",,,,,5,0.45,2,3,11,"","",""
2,"Douglas Kirkpatrick, A. Hintze","Augmenting neuro-evolutionary adaptation with representations does not incur a speed accuracy trade-off",2019,"","","","",57,"2022-07-13 09:21:09","","10.1145/3319619.3322047","","",,,,,2,0.67,1,2,3,"Representations, or sensor-independent internal models of the environment, are important for any type of intelligent agent to process and act in an environment. Imbuing an artificially intelligent system with such a model of the world it functions in remains a difficult problem. However, using neuro-evolution as the means to optimize such a system allows the artificial intelligence to evolve proper models of the environment. Previous work has found an information-theoretic measure, R, which measures how much information a neural computational architecture (henceforth loosely referred to as a brain) has about its environment, and can additionally be used speed up the neuro-evolutionary process. However, it is possible that this improved evolutionary adaptation comes at a cost to the brain's ability to generalize or the brain's robustness to noise. In this paper, we show that this is not the case; to the contrary, we find an improved ability of the to evolve in noisy environments when the neuro-correlate R is used to augment evolutionary adaptation.","",""
1,"S. Khawandi, Firas Abdallah, Anis Ismail","A Survey On The Different Implemented Captchas",2019,"","","","",58,"2022-07-13 09:21:09","","10.5121/CSIT.2019.90101","","",,,,,1,0.33,0,3,3,"CAPTCHA is almost a standard security technology, and has found widespread application in commercial websites. There are two types: labeling and image based CAPTCHAs. To date, almost all CAPTCHA designs are labeling based. Labeling based CAPTCHAs refer to those that make judgment based on whether the question “what is it?” has been correctly answered. Essentially in Artificial Intelligence (AI), this means judgment depends on whether the new label provided by the user side matches the label already known to the server. Labeling based CAPTCHA designs have some common weaknesses that can be taken advantage of attackers. First, the label set, i.e., the number of classes, is small and fixed. Due to deformation and noise in CAPTCHAs, the classes have to be further reduced to avoid confusion. Second, clean segmentation in current design, in particular character labeling based CAPTCHAs, is feasible. The state of the art of CAPTCHA design suggests that the robustness of character labeling schemes should rely on the difficulty of finding where the character is (segmentation), rather than which character it is (recognition). However, the shapes of alphabet letters and numbers have very limited geometry characteristics that can be used by humans to tell them yet are also easy to be indistinct. Image recognition CAPTCHAs faces many potential problems which have not been fully studied. It is difficult for a small site to acquire a large dictionary of images which an attacker does not have access to and without a means of automatically acquiring new labeled images, an image based challenge does not usually meet the definition of a CAPTCHA. They are either unusable or prone to attacks. In this paper, we present the different types of CAPTCHAs trying to defeat advanced computer programs or bots, discussing the limitations and drawbacks of each.","",""
1,"Shu Zhang, Ge Yan, Yu Li, Jia Liu","Evaluation of Judicial Imprisonment Term Prediction Model Based on Text Mutation",2019,"","","","",59,"2022-07-13 09:21:09","","10.1109/QRS-C.2019.00025","","",,,,,1,0.33,0,4,3,"In recent years, artificial intelligence has witnessed great advancement, and its application in the legal field has experienced more than 60 years. The use of ""machine learning"" technology to aid the decision-making of legal intelligence systems is no longer far away. However, no comprehensive evaluation methods for predictive models of judicial cases can be found. The performance of the machine learning prediction model not only related to the accuracy but also should be measured in many different aspects. Mutation is a common means of traditional software testing, which can be borrowed in the evaluating of prediction models. This paper introduces the text mutation method, to evaluate the robustness of the judicial case prediction model. The following three evaluation methods are adopted: Classification preference test, Word order variation test and Noise variation test. This paper applies the proposed evaluation method to the judicial imprisonment term prediction model. We use the fastText, TextCNN, and Multi-layer LSTM models. Using the proposed evaluation method to test the above prediction model, and evaluate the robustness of the judicial case prediction model in different aspects.","",""
11,"Loai Danial, N. Wainstein, Shraga Kraus, S. Kvatinsky","DIDACTIC: A Data-Intelligent Digital-to-Analog Converter with a Trainable Integrated Circuit using Memristors",2018,"","","","",60,"2022-07-13 09:21:09","","10.1109/JETCAS.2017.2780251","","",,,,,11,2.75,3,4,4,"In an increasingly data-diverse world, in which data are interactively transferred at high rates, there is an ever-growing demand for high-precision data converters. In this paper, we propose a novel digital-to-analog converter (DAC) configuration that is calibrated using an artificial intelligence neural network technique. The proposed technique is demonstrated on an adaptive and self-calibrated binary-weighted DAC that can be configured on-chip in real time. We design a reconfigurable 4-bit DAC with a memristor-based neural network. This circuit uses an online supervised machine learning algorithm called “binary-weighted time-varying gradient descent.” This algorithm fits multiple full-scale voltage ranges and sampling frequencies by iterative synaptic adjustments, while inherently providing mismatch calibration and noise tolerance. Theoretical analysis, as well as simulation results, show the efficiency and robustness of the training algorithm in reconfiguration, self-calibration, and desensitization, leading to a significant improvement in DAC accuracy: 0.12 LSB in terms of integral non-linearity, 0.11 LSB in terms of differential non-linearity, and 3.63 bits in terms of effective number of bits. The findings constitute a promising milestone toward scalable data-driven converters using deep neural networks.","",""
0,"Jian He, Bo Li","Research on Motor Speed Estimation Method Based on Electric Vehicle",2019,"","","","",61,"2022-07-13 09:21:09","","10.1007/978-981-13-9409-6_101","","",,,,,0,0.00,0,2,3,"","",""
7,"Valentin Malykh, Vladislav Lialin","Named Entity Recognition in Noisy Domains",2018,"","","","",62,"2022-07-13 09:21:09","","10.1109/IC-AIAI.2018.8674438","","",,,,,7,1.75,4,2,4,"Named Entity Recognition (NER) task is an important part for conversational AI. A typical user of a conversation system has no time to check the spelling or grammar in his or her utterances. Due to that user utterances contain typos and spelling errors, so the noise robustness should be considered as a significant aspect of NER task. In this work, we study noise robustness properties for variants of state of the art named entity recognition models on three languages, English on CoNLL'03 corpus, Russian, on Persons-1000 corpus and French, on CAp'2017 corpus, also, we demonstrate state of the art results for CAp'2017.","",""
7,"Viorel Mihai, C. Hanganu, G. Stamatescu, D. Popescu","WSN and Fog Computing Integration for Intelligent Data Processing",2018,"","","","",63,"2022-07-13 09:21:09","","10.1109/ECAI.2018.8679064","","",,,,,7,1.75,2,4,4,"Networked embedded systems endowed with sensing, computing, control and communication capabilities allow the development of various application scenarios and represent the building blocks of the Internet of Things (IoT) paradigm. Traditional data collection methods include multiple field level IoT systems that can relay data stemming from a network of distributed ground sensors directly to a cloud platform for storage, analysis and processing. In such applications however, rapid sensor deployment in unstructured environments represents a challenge to the overall robustness of the system. We discuss the fog and mist computing approaches to hierarchically process data along its path from source to destination. The several stages of intermediate data processing reduce the computational and communication effort in a gradual manner. A three-layer topology for smart data monitoring and processing is thus proposed and illustrated to improve the information to noise ratio in a reference scenario.","",""
23,"Pooya Sagharichi Ha, M. Shakeri","License Plate Automatic Recognition based on edge detection",2016,"","","","",64,"2022-07-13 09:21:09","","10.1109/RIOS.2016.7529509","","",,,,,23,3.83,12,2,6,"In this paper, we present an Automatic License Plate Recognition System (ALPRS) to identify license plates which is an application of image processing. The main process of ALPRS is divided into four steps: The noise in the image is removed by using FMH filter. A simple algorithm is used for background subtraction. Canny edge detection is used to localize the license plate location. Finally, letters and digits are extracted through template matching technique. The proposed algorithms have two advantages: First, the method has strong robustness against noise. Second, it can deal with license plates with different colors. The performance of the algorithm is tested in a real-time video stream. Based on the result, our algorithm shows the missing rate is almost 16% from 70 vehicle images.","",""
28,"V. Dunjko, N. Friis, H. Briegel","Quantum-enhanced deliberation of learning agents using trapped ions",2014,"","","","",65,"2022-07-13 09:21:09","","10.1088/1367-2630/17/2/023006","","",,,,,28,3.50,9,3,8,"A scheme that successfully employs quantum mechanics in the design of autonomous learning agents has recently been reported in the context of the projective simulation (PS) model for artificial intelligence. In that approach, the key feature of a PS agent, a specific type of memory which is explored via random walks, was shown to be amenable to quantization, allowing for a speed-up. In this work we propose an implementation of such classical and quantum agents in systems of trapped ions. We employ a generic construction by which the classical agents are ‘upgraded’ to their quantum counterparts by a nested process of adding coherent control, and we outline how this construction can be realized in ion traps. Our results provide a flexible modular architecture for the design of PS agents. Furthermore, we present numerical simulations of simple PS agents which analyze the robustness of our proposal under certain noise models.","",""
1,"Yingying Li, Siyuan Pi, N. Xiao","Speech Recognition Method Based on Spectrogram",2018,"","","","",66,"2022-07-13 09:21:09","","10.1007/978-3-030-00214-5_110","","",,,,,1,0.25,0,3,4,"","",""
15,"G. Indiveri","Neuromorphic Engineering",2015,"","","","",67,"2022-07-13 09:21:09","","10.1007/978-3-662-43505-2_38","","",,,,,15,2.14,15,1,7,"","",""
1,"Bing Li, Lin Li","Artificial Neural Network Based Software Sensor for Yeast Biomass Concentration during Industrial Production",2006,"","","","",68,"2022-07-13 09:21:09","","10.1109/ICCIAS.2006.295402","","",,,,,1,0.06,1,2,16,"The artificial neural network is a potential 'sensor' in the complex bioprocess. The recurrent neural network (RNN) was employed as the software sensor to measure the biomass concentration during the baker's yeast industrial production, owing to its good ability in dealing with non-linear and time-varying process. Based on the data sets provided by the plant, input variables were selected as air flow rate (G), ethanol concentration (Eth), volume of the contents in the reactor (Vol), temperature (T), pH and their time-delay values as well as the predicted values of yeast biomass concentration at delayed time. The topology of the RNN was optimized to be 11-16-1. The RNN showed good generalization ability for the testing samples. The robustness of the RNN was evaluated by adding deliberately inflicted noises to the G and Eth. The RNN showed higher robustness to the noise from Eth than that from G","",""
21,"Claudia d’Amato, N. Fanizzi, B. Fazzinga, G. Gottlob, Thomas Lukasiewicz","Ontology-based semantic search on the Web and its combination with the power of inductive reasoning",2012,"","","","",69,"2022-07-13 09:21:09","","10.1007/s10472-012-9309-7","","",,,,,21,2.10,4,5,10,"","",""
11,"E. S. Abdolkarimi, M. Mosavi, A. Abedi, S. Mirzakuchaki","Optimization of the low-cost INS/GPS navigation system using ANFIS for high speed vehicle application",2015,"","","","",70,"2022-07-13 09:21:09","","10.1109/SPIS.2015.7422319","","",,,,,11,1.57,3,4,7,"Both Global Positioning System (GPS) and Inertial Navigation System (INS) have complementary characteristics and their integration provides continuous and accurate navigation solution, compared to standalone INS or GPS. Extended Kalman filtering (EKF) is the most common INS/GPS integration technique used for this purpose. Kalman filter methods require prior knowledge of the error model of INS, which increases the complexity of the system. These methods have some disadvantages in terms of stability, robustness, immunity to noise effect, and observability, especially when used with low-cost MEMS-based inertial sensors. Therefore, in this paper, low-cost INS/GPS integration is enhanced based on artificial intelligence (AI) techniques that are aimed at providing high-accuracy vehicle state estimates. First, the INS and GPS measurements are fused via an EKF method. Second, an artificial intelligence-based approach for the integration of INS/GPS measurements is improved based upon an Adaptive Neuro-Fuzzy Inference System (ANFIS). The performance of the two sensor fusion approaches are evaluated using a real field test data. The experiments have been conducted using a high speed vehicle. The results show great improvements in positioning for low-cost MEMS-based inertial sensors in terms of GPS blockage compared to the EKF-based approach.","",""
11,"Xiaoguang Mei, Yong Ma, Fan Fan, Chang Li, Chengyin Liu, Jun Huang, Jiayi Ma","Infrared ultraspectral signature classification based on a restricted Boltzmann machine with sparse and prior constraints",2015,"","","","",71,"2022-07-13 09:21:09","","10.1080/01431161.2015.1079664","","",,,,,11,1.57,2,7,7,"The state-of-the-art ultraspectral technology brings a new hope for the high precision applications due to its high spectral resolution. However, it comes with new challenges brought by the improvement of spectral resolution such as the Hughes phenomenon and over-fitting issue, and our work is aimed at addressing these problems. As new Markov random field (MRF) models, the restricted Boltzmann machines (RBMs) have been used as generative models for many different pattern recognition and artificial intelligence applications showing promising and outstanding performance. In this article, we propose a new method for infrared ultraspectral signature classification based on the RBMs, which adopt the regularization-based techniques to improve the classification accuracy and robustness to noise compared to traditional RBMs. First, we add an arctan-like term to the objective function as a sparse constraint to improve the classification accuracy. Second, we utilize a Gaussian prior to avoid the over-fitting problem. Third, to further improve the classification performance, a multi-layer RBM model, a deep belief network (DBN), is adopted for infrared ultraspectral signature classification. Experiments using different spectral libraries provided by the Advanced Spaceborne Thermal Emission and Reflection Radiometer (ASTER) and the Environmental Protection Agency (EPA) were performed to evaluate the performance of the proposed method by comparing it with other traditional methods, including spectral coding-based classifiers (binary coding (BC), spectral feature-based binary coding (SFBC), and spectral derivative feature coding (SDFC) matching methods), a novel feature extraction method termed crosscut feature extraction matching (CF), and three machine learning methods (artificial deoxyribonucleic acid (DNA)-based spectral matching (ADSM), DBN, and sparse deep belief network (SparseDBN)). Experimental results demonstrate that the proposed method is superior to the other methods with which it was compared and can simultaneously improve the accuracy and robustness of classification.","",""
4,"Yuanming Cao, Yijun Liu","Head pose estimation algorithm based on deep learning",2017,"","","","",72,"2022-07-13 09:21:09","","10.1063/1.4982509","","",,,,,4,0.80,2,2,5,"Head pose estimation has been widely used in the field of artificial intelligence, pattern recognition and intelligent human-computer interaction and so on. Good head pose estimation algorithm should deal with light, noise, identity, shelter and other factors robustly, but so far how to improve the accuracy and robustness of attitude estimation remains a major challenge in the field of computer vision. A method based on deep learning for pose estimation is presented. Deep learning with a strong learning ability, it can extract high-level image features of the input image by through a series of non-linear operation, then classifying the input image using the extracted feature. Such characteristics have greater differences in pose, while they are robust of light, identity, occlusion and other factors. The proposed head pose estimation is evaluated on the CAS-PEAL data set. Experimental results show that this method is effective to improve the accuracy of pose estimation.","",""
11,"I. Mporas, T. Ganchev, O. Kocsis, N. Fakotakis, O. Jahn, K. Riede, K. Schuchmann","Automated Acoustic Classification of Bird Species from Real -Field Recordings",2012,"","","","",73,"2022-07-13 09:21:09","","10.1109/ICTAI.2012.110","","",,,,,11,1.10,2,7,10,"We report on a recent progress with the development of an automated bioacoustic bird recognizer, which is part of a long-term project, aiming at the establishment of an automated biodiversity monitoring system at the Hymettus Mountain near Athens. In particular, employing a classical audio processing strategy, which has been proved quite successful in various audio recognition applications, we evaluate the appropriateness of six classifiers on the bird species recognition task. In the experimental evaluation of the acoustic bird recognizer, we made use of real-field audio recordings for seven bird species, which are common for the Hymettus Mountain. Encouraging recognition accuracy was obtained on the real-field data, and further experiments with additive noise demonstrated significant noise robustness in low SNR conditions.","",""
4,"Hafiz Muhammad Wahaj Aziz, J. Iqbal","Flexible joint robotic manipulator: Modeling and design of robust control law",2016,"","","","",74,"2022-07-13 09:21:09","","10.1109/ICRAI.2016.7791230","","",,,,,4,0.67,2,2,6,"This paper presents modeling and sophisticated control of a single Degree Of Freedom (DOF) flexible robotic arm. The derived model is based on Euler-Lagrange approach while the first and second order (super twisting) Sliding Mode Control (SMC) is proposed as a non-linear control strategy. The control laws are subjected to various test inputs including step and sinusoids to demonstrate their tracking efficiency by observing transient and steady state behaviours. Both orders of SMC are then compared to characterize the control performance in terms of robustness, handling external disturbances and chattering. Results dictate that the super twisting SMC is more accurate and robust against the external noise and chattering phenomena compared to the first order SMC.","",""
44,"M. Mozina, J. Zabkar, Trevor J. M. Bench-Capon, I. Bratko","Argument Based Machine Learning Applied to Law",2005,"","","","",75,"2022-07-13 09:21:09","","10.1007/s10506-006-9002-4","","",,,,,44,2.59,11,4,17,"","",""
14,"R. Motwani, M. Motwani, B. D. Bryant, F. Harris, Akshata S. Agarwal","Watermark Embedder Optimization for 3D Mesh Objects Using Classification Based Approach",2010,"","","","",76,"2022-07-13 09:21:09","","10.1109/ICSAP.2010.83","","",,,,,14,1.17,3,5,12,"This paper presents a novel 3D mesh watermarking scheme that utilizes a support vector machine(SVM) based classifier for watermark insertion. Artificial intelligence(AI)based approaches have been employed by watermarking algorithms for various host mediums such as images, audio, and video. However, AI based techniques are yet to be explored by researchers in the 3D domain for watermark insertion and extraction processes. Contributing towards this end, the proposed approach employs a binary SVM to classify vertices as appropriate or inappropriate candidates for watermark insertion. The SVM is trained with feature vectors derived from the curvature estimates of a 1-ring neighborhood of vertices taken from normalized 3D meshes. A geometry-based non-blind approach is used by the watermarking algorithm. The robustness of proposed technique is evaluated experimentally by simulating attacks such as mesh smoothing, cropping and noise addition.","",""
1,"Armin Ehrampoosh, A. Yousefi-Koma, M. Ayati","Development of myoelectric interface based on pattern recognition and regression based models",2016,"","","","",77,"2022-07-13 09:21:09","","10.1109/RIOS.2016.7529505","","",,,,,1,0.17,0,3,6,"This paper proposes a combinatorial strategy for myoelectric control of robotic arm. Activation of main muscles responsible for 1 DOF of elbow joint is recorded. The goal was to create a mapping between muscles' Surface Electromyogram (sEMG) data and kinematics of the joints. The proposed strategy includes two main phases. In the first phase, Linear Discriminant Analysis (LDA) was utilized to classify several classes in user's arm motions. Due to fast training, simple implementation and robustness against long term effect of non-stationary characteristics of sEMG signals, LDA is a common classifier in myoelectric signal classification researches. In the second phase, two Time Delayed Artificial Neural Networks (TDANN) were trained to estimate proportional and continuous angle and velocity related to joint motion classes. Furthermore, two additional methods were used to enhance the prediction results accuracy. First, noise reduction of sEMG signals plays a key role in accurate joint kinematics prediction. Therefore, a new noise reduction approach is investigated based on classification results. Second, final predicted angles were achieved by data fusion of angles and angle difference rates, estimated by TDANN. Results show that, LDA classifies the motion classes with 95% accuracy and final estimated angular positions are significantly close to actual values. Therefore, proposed method is able to create a mapping between muscles' sEMG data and joint kinematics with acceptable error. Practical results confirm the performance of the proposed method.","",""
0,"Cuncun Wei","Object Recognition Based on Descriptor of Improved Histograms of Second-Order Gradients",2016,"","","","",78,"2022-07-13 09:21:09","","","","",,,,,0,0.00,0,1,6,"Object recognition is a very meaningful work in the research and application of computer vision and artificial intelligence. The main task of object identification is to determine whether there exist objects of interest in images taken, if yes, to give reasonable explanation to objects, namely, to judge what the object is and determine its position. According to the defect of classic local feature descriptors in characterizing the curvature related features, as well as the insufficient of classic second order gradient histogram when calculate first-order gradient and the second-order gradient histogram, in order to improve recognition accuracy and noise robustness, an object recognition method based on an improved second-order gradient histogram descriptor is proposed in this paper. Using the absolute value to retain the first-order gradient of all directions, eliminating the defect of lost some direction when the gradient direction is lost odd, Gaussian function weighted is used to calculate second order gradient histograms, which considered the influence of the neighboring pixels to the center pixel and improved anti-noise performance. Experimental results show that the method in this paper improves the recognition accuracy and noise robustness.","",""
2,"G. Singh, Kamlesh Kumar","Retrieving Records of Genealogy registers online using Genetic Algorithm based Adaptive Heuristic Search Technique",2013,"","","","",79,"2022-07-13 09:21:09","","10.5120/14160-2043","","",,,,,2,0.22,1,2,9,"talk about evolutionary computing, genetic algorithm is a main area to be research. It is a growing area of an Artificial Intelligence. This paper focuses on evolution of Genetic algorithm and its impact on genealogy database. GA is also better than conventional Artificial Intelligence because of its robustness. GA focuses on input/output pattern that is if we give some input as chromosomes, we produce some output as an offspring. GA is beneficial for us because they do not break easily even if the inputs changed slightly, or in presence of reasonable noise. In this paper we intended on developing a computerized relational database on the basis of GA Heuristic Search technique. It will be easy to find out the relation in between the people. This paper will lead to a project for Hindu genealogy registers at Haridwar, Uttarakhand, India. This has been a subject of study for many years. People comes from outside for pilgrimage or for cremation of their dead kin and register their family detail there in a register maintained by Brahmin priests called 'Pandas'. Through this project we want the change this register system. As we are living in a technological world and computer is our basic need. So we want to maintain a computerized relational database which will be secure enough while accessing the data from it.","",""
37,"Michael P. O’Mahony, N. Hurley, G. Silvestre","An Evaluation of Neighbourhood Formation on the Performance of Collaborative Filtering",2004,"","","","",80,"2022-07-13 09:21:09","","10.1023/B:AIRE.0000036256.39422.25","","",,,,,37,2.06,12,3,18,"","",""
0,"D. O'Shaughnessy","Automatic speech recognition",2015,"","","","",81,"2022-07-13 09:21:09","","10.1007/978-1-349-22599-6_7","","",,,,,0,0.00,0,1,7,"","",""
33,"G. Baldassarre","Forward and Bidirectional Planning Based on Reinforcement Learning and Neural Networks in a Simulated Robot",2003,"","","","",82,"2022-07-13 09:21:09","","10.1007/978-3-540-45002-3_11","","",,,,,33,1.74,33,1,19,"","",""
5,"Wang Tong-yao","The motion detection based on background difference method and active contour model",2011,"","","","",83,"2022-07-13 09:21:09","","10.1109/ITAIC.2011.6030378","","",,,,,5,0.45,5,1,11,"For the traditional moving target detection algorithm exists some problems such as determining threshold problems and more sensitive to noise, this paper presents a rapid extraction method based on variation level set of the edge blur moving objects. This method used the background difference method extract initial outline of moving target, and then used active contour model constraint edge detection; the final combined variation level set method with a second evolution to obtain accurate image segmentation. Experiments show that this method can rapidly and accurately segment moving object, for moving objects of different shapes and the rules of small-angle shot has a good adaptability and robustness.","",""
6,"Feng Wang, Yun Chen, Hao Wang, Xiuqin Wang","Fingerprint Classification Based on Improved Singular Points Detection and Central Symmetrical Axis",2009,"","","","",84,"2022-07-13 09:21:09","","10.1109/AICI.2009.118","","",,,,,6,0.46,2,4,13,"Effective fingerprint classification not only can provide an important index mechanism for large fingerprint database, but also can improve the efficiency and performance of AFIS. At present, because of traditional Poincare method detection more false singular points and weaker anti-noise problem, this paper presents a fingerprint classification method based on continuously directional image and symmetrical axis. Compared with traditional algorithms, this algorithm has the following two aspects improved: firstly, continuously directional image exhibits not only good continuity, well gradualness, and excellent robustness to the noise, but very high precision, which makes singular points location very accurate; secondly, combined singular points quantity and symmetrical axis location relationship divided fingerprint into belonged to classification. Experimental results prove the effectiveness of the algorithm and robustness at Nanjing University fingerprint database and FVC database.","",""
0,"Ding Mingyue","THE EVALUATION OF ACQUISITION PROBABILITY IN IMAGE MATCHINGI",2010,"","","","",85,"2022-07-13 09:21:09","","","","",,,,,0,0.00,0,1,12,"Ding Mingyue2 Institute of Pattern Recognition and Artificial Intelligence Huazhong University of Science and Technology Wuhan, Hubei 430074, P,R.China In modern navigation and guidance systems, image matching is often used as an efficient approach to increase the registration accuracy. Acquisition probability of image matching is one of the most important parameters in registration accuracy analysis of image matching. It represents the correctness of the position estimated by the navigation and guidance system with respect to the real position in flight. For example, in missile homing guidance, it is the probability of hitting a target. So, it is the main basis for designing a navigation and guidance system. In image matching, Mean Absolute Difference (MAD) is one of the most often used algorithms. It has a lot of advantages such as high registration accuracy, high noise robustness and can be easily realized by hardware etc. In this paper, first, the acquisition probability for the MAD algorithm is derived based on the image pixel-correlation model. Then, in order to evaluate the value of acquisition probability for the MAD algorithm, an approximation formula is given. Finally, the experiments with different optical aerial photographs and infrared remoted sensing photographs have been conducted on a IBM-PC microcomputer system and a 8575 image processing system. By the experimental comparion to the evaluation of Johnson it is demonstrated that the evaluation of acquisition probability for the MAD algorithm proposed in this paper is more accurate and close to the real acquisition probability.","",""
2,"Haifeng Shen, Gang Liu, Jun Guo","Mixed environment compensation based on maximum a posteriori estimation for robust speech recognition",2009,"","","","",86,"2022-07-13 09:21:09","","10.1007/s10462-009-9130-9","","",,,,,2,0.15,1,3,13,"","",""
11,"J. Gan, Shouli Yan, J. Abraham","Design and modeling of a 16-bit 1.5MSPS successive approximation ADC with non-binary capacitor array",2003,"","","","",87,"2022-07-13 09:21:09","","10.1145/764808.764850","","",,,,,11,0.58,4,3,19,"The design and modeling of a high performance successive approximation analog-to-digital converter (ADC) using non-binary capacitor array are presented in this paper. A non-binary capacitor array with 20 capacitors is used to design a 16-bit, 1.5 mega samples per second (MSPS) successive approximation ADC. A perceptron learning rule, originally developed for Artificial Intelligence applications, is used as the capacitor calibration algorithm. The system architecture and the circuit design for the capacitor array, the sampling network and the high performance comparator are discussed. The capacitor weights are adaptively calibrated to match the physical capacitors with better than 22-bit accuracy. Capacitor matching is not a limiting factor to the accuracy. Various sources of noise, interference and distortion are modeled to evaluate their effects and to ensure the robustness of the calibration algorithm. This architecture is especially suitable for mixed-signal VLSI in the Nanometer Era because it relaxes the matching requirement on analog circuitry.","",""
2,"Y. Boniface, Reghis Abdelmalek","Some Experiments Around a Neural Network for Multimodal Associations",2006,"","","","",88,"2022-07-13 09:21:09","","10.5555/1166890.1166922","","",,,,,2,0.13,1,2,16,"This paper presents a study of the model of triple BAM by [11] which is an improved variation of the original BAM model by [7]. This class of model aims at integrating different sensory inputs in order to memorize a unified and distributed representation. An experimental evaluation of the model is presented that underlines its limitations in terms of noise robustness and learning capacities. A new model is presented in order to overcome those initial limitations by introducing a new online learning algorithm adapted from the PRLAB initial algorithm that improve both noise robustness and learning capacities. Finally, model properties and limitations are considered and discussed within the context of multi-modal integration and brain modeling.","",""
52,"Hamon Ronan, Junklewitz Henrik, S. Ignacio","Robustness and Explainability of Artificial Intelligence",2020,"","","","",89,"2022-07-13 09:21:09","","10.2760/57493","","",,,,,52,26.00,17,3,2,"","",""
14,"A. Zaji, H. Bonakdari","Robustness lake water level prediction using the search heuristic-based artificial intelligence methods",2019,"","","","",90,"2022-07-13 09:21:09","","10.1080/09715010.2018.1424568","","",,,,,14,4.67,7,2,3,"Abstract Lakes have a crucial role in the industrial, agricultural, environment, and drinking water fields. Accurate prediction of lake levels is one of the most important parameters in the reservoir management and lakeshore structure designing. The goal of the present study is to examine the robustness of two different Genetic Algorithm-based regression methods namely the Genetic Algorithm Artificial neural network (GAA) and the Genetic Programming (GP) by considering their performance in predicting the non-observed lakes. To do that, data collected from the four-year daily measurements of the Chahnimeh#1 lake in Eastern Iran were used for developing the GAA and GP models and after that, the performance of the considered models are examined to predict the lake water levels of an adjacent lake namely Chahnimeh#4 as the non-observed information. The results showed that both model has the ability to simulate adjacent lakes using the considered lake water levels for the training procedure. In addition, another goal is to develop simple, practical formulation for predicting the lake water level, So that, using the GP method, as the superior model, three different formulations are proposed in order to predict the one, three, and five days ahead lake water level, respectively.","",""
0,"K. Kojima","End-to-End Deep Learning for Phase Noise-Robust Multi-Dimensional Geometric Shaping /Author=Talreja, Veeru; Koike-Akino, Toshiaki; Wang, Ye; Millar, David S.; Kojima, Keisuke; Parsons, Kieran /CreationDate=December 11, 2020 /Subject=Artificial Intelligence, Communications, Multi-Physical Modeling, O",2020,"","","","",91,"2022-07-13 09:21:09","","","","",,,,,0,0.00,0,1,2,"We propose an end-to-end deep learning model for phase noise-robust optical communications. A convolutional embedding layer is integrated with a deep autoencoder for multi-dimensional constellation design to achieve shaping gain. The proposed model offers a significant gain up to 2 dB. European Conference on Optical Communication (ECOC) c © 2020 MERL. This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Mitsubishi Electric Research Laboratories, Inc. 201 Broadway, Cambridge, Massachusetts 02139 End-to-End Deep Learning for Phase Noise-Robust Multi-Dimensional Geometric Shaping Veeru Talreja, Toshiaki Koike-Akino, Ye Wang, David S. Millar, Keisuke Kojima, Kieran Parsons Mitsubishi Electric Research Labs., 201 Broadway, Cambridge, MA 02139, USA., koike@merl.com Abstract We propose an end-to-end deep learning model for phase noise-robust optical communi-We propose an end-to-end deep learning model for phase noise-robust optical communications. A convolutional embedding layer is integrated with a deep autoencoder for multi-dimensional constellation design to achieve shaping gain. The proposed model offers a significant gain up to 2 dB.","",""
755,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xisheng Fang, Shiqin Zhang, J. Xia, Jun Xia","Artificial Intelligence Distinguishes COVID-19 from Community Acquired Pneumonia on Chest CT",2020,"","","","",92,"2022-07-13 09:21:09","","10.1148/radiol.2020200905","","",,,,,755,377.50,76,18,2,"Background Coronavirus disease has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performances. Materials and Methods In this retrospective and multi-center study, a deep learning model, COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT exams for the detection of COVID-19. Community acquired pneumonia (CAP) and other non-pneumonia CT exams were included to test the robustness of the model. The datasets were collected from 6 hospitals between August 2016 and February 2020. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity and specificity. Results The collected dataset consisted of 4356 chest CT exams from 3,322 patients. The average age is 49±15 years and there were slightly more male patients than female (1838 vs 1484; p-value=0.29). The per-exam sensitivity and specificity for detecting COVID-19 in the independent test set was 114 of 127 (90% [95% CI: 83%, 94%]) and 294 of 307 (96% [95% CI: 93%, 98%]), respectively, with an AUC of 0.96 (p-value<0.001). The per-exam sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175) and 92% (239 of 259), respectively, with an AUC of 0.95 (95% CI: 0.93, 0.97). Conclusions A deep learning model can accurately detect COVID-19 and differentiate it from community acquired pneumonia and other lung diseases.","",""
19,"Ruhhee Tabbussum, A. Q. Dar","Performance evaluation of artificial intelligence paradigms—artificial neural networks, fuzzy logic, and adaptive neuro-fuzzy inference system for flood prediction",2021,"","","","",93,"2022-07-13 09:21:09","","10.1007/s11356-021-12410-1","","",,,,,19,19.00,10,2,1,"","",""
6,"M. Ghaderi, H. Javadikia, L. Naderloo, M. Mostafaei, H. Rabbani","An analysis of noise pollution emitted by moving MF285 Tractor using different mixtures of biodiesel, bioethanol and diesel through artificial intelligence",2019,"","","","",94,"2022-07-13 09:21:09","","10.1177/1461348418823572","","",,,,,6,2.00,1,5,3,"In the present study, the noise pollution from different compositions of biodiesel, bioethanol, and diesel fuels in MF285 Tractor was studied in the second and third gears from two positions: driver and bystander, at 1000 and 1600 r/min, and running on 10 different fuel levels. For data analysis, the ANFIS network, neural network, and response surface methodology were applied. Comparing the means of noise pollution at different levels demonstrated that the B25E6D69 fuel, made up of 25% biodiesel and 6% bioethanol, had the lowest noise pollution. The lowest noise pollution was at 1000 r/min. Although the noise pollution emitted in the third gear was a little more than that emitted in the second gear. All the resultant models, laid by response surface methodology, neural network, and ANFIS had excellent results. Considering the statistical criteria, the best models with high correlation coefficients and low mean square errors were ANFIS, response surface methodology, and artificial neural network models, respectively.","",""
822,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xi Fang, Shiqin Zhang, J. Xia, Jun Xia","Using Artificial Intelligence to Detect COVID-19 and Community-acquired Pneumonia Based on Pulmonary CT: Evaluation of the Diagnostic Accuracy",2020,"","","","",95,"2022-07-13 09:21:09","","10.1148/RADIOL.2020200905","","",,,,,822,411.00,82,18,2,"Background Coronavirus disease 2019 (COVID-19) has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performance. Materials and Methods In this retrospective and multicenter study, a deep learning model, the COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT scans for the detection of COVID-19. CT scans of community-acquired pneumonia (CAP) and other non-pneumonia abnormalities were included to test the robustness of the model. The datasets were collected from six hospitals between August 2016 and February 2020. Diagnostic performance was assessed with the area under the receiver operating characteristic curve, sensitivity, and specificity. Results The collected dataset consisted of 4352 chest CT scans from 3322 patients. The average patient age (±standard deviation) was 49 years ± 15, and there were slightly more men than women (1838 vs 1484, respectively; P = .29). The per-scan sensitivity and specificity for detecting COVID-19 in the independent test set was 90% (95% confidence interval [CI]: 83%, 94%; 114 of 127 scans) and 96% (95% CI: 93%, 98%; 294 of 307 scans), respectively, with an area under the receiver operating characteristic curve of 0.96 (P < .001). The per-scan sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175 scans) and 92% (239 of 259 scans), respectively, with an area under the receiver operating characteristic curve of 0.95 (95% CI: 0.93, 0.97). Conclusion A deep learning model can accurately detect coronavirus 2019 and differentiate it from community-acquired pneumonia and other lung conditions. © RSNA, 2020 Online supplemental material is available for this article.","",""
90,"R. Shafin, Lingjia Liu, V. Chandrasekhar, Hao Chen, J. Reed, Jianzhong Zhang","Artificial Intelligence-Enabled Cellular Networks: A Critical Path to Beyond-5G and 6G",2019,"","","","",96,"2022-07-13 09:21:09","","10.1109/MWC.001.1900323","","",,,,,90,30.00,15,6,3,"Mobile network operators (MNOs) are in the process of overlaying their conventional macro cellular networks with shorter range cells such as outdoor pico cells. The resultant increase in network complexity creates substantial overhead in terms of operating expenses, time, and labor for their planning and management. Artificial intelligence (AI) offers the potential for MNOs to operate their networks in a more organic and cost-efficient manner. We argue that deploying AI in fifth generation (5G) and beyond will require surmounting significant technical barriers in terms of robustness, performance, and complexity. We outline future research directions, identify top five challenges, and present a possible roadmap to realize the vision of AI-enabled cellular networks for Beyond- 5G and sixth generation (6G) networks.","",""
34,"T. H. Aldhyani, M. Al-Yaari, Hasan Alkahtani, Mashael S. Maashi","Water Quality Prediction Using Artificial Intelligence Algorithms",2020,"","","","",97,"2022-07-13 09:21:09","","10.1155/2020/6659314","","",,,,,34,17.00,9,4,2,"During the last years, water quality has been threatened by various pollutants. Therefore, modeling and predicting water quality have become very important in controlling water pollution. In this work, advanced artificial intelligence (AI) algorithms are developed to predict water quality index (WQI) and water quality classification (WQC). For the WQI prediction, artificial neural network models, namely nonlinear autoregressive neural network (NARNET) and long short-term memory (LSTM) deep learning algorithm, have been developed. In addition, three machine learning algorithms, namely, support vector machine (SVM), K-nearest neighbor (K-NN), and Naive Bayes, have been used for the WQC forecasting. The used dataset has 7 significant parameters, and the developed models were evaluated based on some statistical parameters. The results revealed that the proposed models can accurately predict WQI and classify the water quality according to superior robustness. Prediction results demonstrated that the NARNET model performed slightly better than the LSTM for the prediction of the WQI values and the SVM algorithm has achieved the highest accuracy (97.01%) for the WQC prediction. Furthermore, the NARNET and LSTM models have achieved similar accuracy for the testing phase with a slight difference in the regression coefficient (RNARNET = 96.17% and RLSTM = 94.21%). This kind of promising research can contribute significantly to water management.","",""
5,"Xiaochen Zhang, Dayu Yang","Research on Music Assisted Teaching System Based on Artificial Intelligence Technology",2021,"","","","",98,"2022-07-13 09:21:09","","10.1088/1742-6596/1852/2/022032","","",,,,,5,5.00,3,2,1,"With the advent of the information age, computer technology has been greatly developed, especially the development of Artificial Intelligence(AI). And with the passage of time, AI began to involve various fields, music education is no exception. In this paper, after a detailed understanding of some research results of AI on music assisted instruction system, we mainly analyze the students’ video, audio and other related information, and save it in the database. This paper first introduces the evaluation process by using AI technology. In fact, it is necessary to find out the relationship between the influencing factors and evaluation of music assisted teaching system. Neural network(NN) is actually a model proposed by simulating the way people think in the brain. It has no strict requirements for data distribution. In terms of nonlinear data processing method, robustness and dynamics, it is very suitable to be used as a model for evaluating music assisted instruction system. Then each factor is taken as the input parameter of the NN. According to the evaluation index of music teaching, a special modeling system is designed. With the help of technical personnel, we obtained the sample data of music performance and completed the neural training. The experimental results show that the development of AI technology has broken the original situation of traditional teaching, especially the application of music system and intelligent music software based on AI in music teaching.","",""
32,"D. Bates, A. Auerbach, Peter F. Schulam, A. Wright, S. Saria","Reporting and Implementing Interventions Involving Machine Learning and Artificial Intelligence",2020,"","","","",99,"2022-07-13 09:21:09","","10.7326/M19-0872","","",,,,,32,16.00,6,5,2,"Increasingly, interventions aimed at improving care are likely to use such technologies as machine learning and artificial intelligence. However, health care has been relatively late to adopt them. This article provides clinical examples in which machine learning and artificial intelligence are already in use in health care and appear to deliver benefit. Three key bottlenecks toward increasing the pace of diffusion and adoption are methodological issues in evaluation of artificial intelligence-based interventions, reporting standards to enable assessment of model performance, and issues that need to be addressed for an institution to adopt these interventions. Methodological best practices will include external validation, ideally at a different site; use of proactive learning algorithms to correct for site-specific biases and increase robustness as algorithms are deployed across multiple sites; addressing subgroup performance; and communicating to providers the uncertainty of predictions. Regarding reporting, especially important issues are the extent to which implementing standardized approaches for introducing clinical decision support has been followed, describing the data sources, reporting on data assumptions, and addressing biases. Although most health care organizations in the United States have adopted electronic health records, they may be ill prepared to adopt machine learning and artificial intelligence. Several steps can enable this: preparing data, developing tools to get suggestions to clinicians in useful ways, and getting clinicians engaged in the process. Open challenges and the role of regulation in this area are briefly discussed. Although these techniques have enormous potential to improve care and personalize recommendations for individuals, the hype regarding them is tremendous. Organizations will need to approach this domain carefully with knowledgeable partners to obtain the hoped-for benefits and avoid failures.","",""
5,"Thulsiram Gantala, K. Balasubramaniam","Automated Defect Recognition for Welds Using Simulation Assisted TFM Imaging with Artificial Intelligence",2021,"","","","",100,"2022-07-13 09:21:09","","10.1007/s10921-021-00761-1","","",,,,,5,5.00,3,2,1,"","",""
5,"Thulsiram Gantala, K. Balasubramaniam","Automated Defect Recognition for Welds Using Simulation Assisted TFM Imaging with Artificial Intelligence",2021,"","","","",101,"2022-07-13 09:21:09","","10.1007/s10921-021-00761-1","","",,,,,5,5.00,3,2,1,"","",""
0,"Jie Wang, Xiangyuan Zheng, Qingdong He","Artificial Intelligence Applied to Extreme Value Prediction of Non-Gaussian Processes with Bandwidth Effect and Non-monotonicity",2021,"","","","",102,"2022-07-13 09:21:09","","10.1109/ICAICA52286.2021.9498204","","",,,,,0,0.00,0,3,1,"Extreme value prediction of a short-term non-Gaussian random process like ocean waves has been a tough issue for decades. In the 1990’s Winterstein proposed a cubic Hermite transformation using skewness and kurtosis, which has been widely applied in many areas for its accuracy and robustness. However, this approach is valid for monotonic transformation and narrow-banded processes. When the bandwidth of a random process is wide, no reasonable methods are available for acquiring the extreme value. This paper therefore applies the artificial neural network and genetic algorithm to do the extreme value prediction, without seeking rigorous mathematical derivations. Not only skewness and kurtosis are used, the spectral moments up to 4th-order reflecting bandwidth effects are also adopted. The results of many random case studies show that the artificial intelligence method is more accurate than the Hermite method in most of situations, especially for non-monotonic transformations. Besides, the artificial intelligence method has a wider application range.","",""
0,"S. Suzuki, J. Motogi, W. Matsuzawa, T. Takayanagi, T. Umemoto, N. Hirota, H. Nakai, A. Hyodo, K. Satoh, T. Otsuka, T. Arita, N. Yagi, J. Yajima, T. Yamashita","Identifying patients with atrial fibrillation during sinus rhythm on ECG: confirming the utility of artificial intelligence algorithm in a small-scale cohort without structural heart diseases",2021,"","","","",103,"2022-07-13 09:21:09","","10.1093/eurheartj/ehab724.3050","","",,,,,0,0.00,0,14,1,"      Detection of atrial fibrillation (AF) out of electrocardiograph (ECG) on sinus rhythm (SR) using artificial intelligence (AI) algorithm has been widely studied within recent couple of years. Generally, it is believed that a huge number of ECGs are necessary for developing an AI-enabled ECG to be adequate to correspond to a lot of minor variations of ECGs. For example, structural heart diseases have typical ECG characteristics, but they could be a noise for the purpose of detecting the small signs of electrocardiographic signature of AF. We hypothesized that when patients with structural heart diseases are excluded, AI-enabled ECG for identifying patients with AF can be developed with a small number of ECGs.        We developed an AI-enabled ECG using a convolutional neural network to detect the electrocardiographic signature of AF present during normal sinus rhythm (NSR) using a digital, standard 10-second, 12-lead ECGs. We included all patients who newly visited the Cardiovascular Institute with at least one NSR ECG between Feb 1, 2010, and March 31, 2018. We classified patients with at least one ECG with a rhythm of AF as positive for AF (AF label) and others as negative for AF (SR label). We allocated ECGs to the training, internal validation, and testing datasets in a 7:1:2 ratio. We calculated the area under the curve (AUC) of the receiver operating characteristic curve for the internal validation dataset to select a probability threshold, which we applied to the testing dataset. We evaluated model performance on the testing dataset by calculating the AUC and the sensitivity, specificity, F1 score, and accuracy with two-sided 95% confidence intervals (CIs).        We totally included 19170 patients with 12-lead ECG. After excluding patients with structural heart diseases, 12825 patients with NSR ECGs at the initial visit were identified (1262 were clinically diagnosed as AF anytime during the time course and 11563 were never diagnosed as AF). Of 11563 non-AF patients, 1818 patients who were followed over 1095 days were selected for the analysis with the SR label, to secure the robustness for maintaining SR. Of 1262 AF patients, 251 patients were selected for the analysis with the AF label, of whom a NSR ECG within 31 days before or after the index AF ECG (the first AF ECG during the time course) could be obtained. In the patients with AF label, the NSR ECG of which the date was the nearest to the index AF ECG was selected for the analysis. The AI-enabled ECG showed an AUC of 0.88 (0.84–0.92) with sensitivity 81% (72–88), specificity 80% (77–83), F1 score 50% (43–57), and overall accuracy 80% (78–83).        An AI-enabled ECG acquired during NSR allowed identification of patients with AF in a small population without structural heart diseases.        Type of funding sources: None. ","",""
0,"A. Alghamdi, Nawaf Saud Almutairi, A. M. Muslim, Humoud H. Khaldi, A. Abdulraheem","Development of a Gas Flow Rate Model for Multi-Stage Choke System in HPHT Gas Wells Using Artificial Intelligence",2021,"","","","",104,"2022-07-13 09:21:09","","10.2118/205163-ms","","",,,,,0,0.00,0,5,1,"      Accurate well production rate measurement is critical for reservoir management. The production rate measurement is carried out using surface devices, such as orifice flow meter and venturi flow meter. For large offshore fields development with a high number of wells, the installation and maintenance costs of these flowmeters can be significant. Therefore, an alternative solution needs to be developed. This paper described the successful implementation of Artificial Intelligence in predicting the production rate of big-bore gas wells in an offshore field.        Successful application of AI depends on capitalizing on a large set of data. Therefore, flowing parameters data were collected for more than 30 gas wells and totaling over 100,000 data points. These wells are producing gas with slight solid production from a high-pressure high-temperature field. In addition, these wells are equipped with a multistage choke that reduces the noise and vibration levels. An Artificial Neural Network is trained on the data using Gradient Descent method as the optimization algorithm. The network takes as an input the upstream and downstream pressure and temperature, and the choke size. The output is the gas rate measured in MMscf/day.        The data set was divided into 70% for training the neural network and 30% for validation. Artificial Neural Network (ANN) was used and the developed model compared exceptionally well with the gas rates measured from the calibrated venturi meters. The gas rate estimation was within a 5% error. The model was developed for two types of completions: 7"" and 9-5/8"" production tubing. One of the challenges was how to estimate the choke wear which plays a major role in the quality of the choke size data. A linear choke wear deterioration is applied in this case, while work in progress is taking place for acquiring acoustic data that can significantly improve the choke wear modeling.        The novel approach presented in this paper capitalizes on Al analytics for estimating accurate gas flow rate values. This approach has improved the reservoir data management by providing accurate production rate values which has drastically improved the reservoir simulation. Moreover, the robustness of the AI model has forced us to rethink the conventional design of installing a flow meter for every well. As shown in this paper, the AI model served as an alternative to conventional venturi meters. We believe that the application of AI models to other aspects of production surveillance will lead to a shift into how operators design production facilities. ","",""
0,"Kailun Deng, Lichao Yang, Haochen Liu, Wenhan Li, J. Erkoyuncu, Yifan Zhao","A Review of Artificial Intelligence Applications in Thermographic Non-Destructive Testing",2021,"","","","",105,"2022-07-13 09:21:09","","10.2139/ssrn.3945926","","",,,,,0,0.00,0,6,1,"Thermographic Non-destructive Testing (TNDT) has gained increasing popularity in various industry fields. It can provide rapid, non-contact, and robust non-invasive detection of both surface and subsurface damage. Artificial Intelligence (AI) is an emerging subject that shows increasing potential in almost all fields and has recently attracted significant interest in TNDT. Thermal signals from TNDT have relatively low signal-noise-ratio (SNR), and most thermal images have the common weakness of edge blurring. The abovementioned obstacles lead to high requirements of field expertise and subjectivity in TNDT inspections. One of the purposes of developing AI is substituting human work more efficiently and objectively. The abovementioned weaknesses in TNDT may seek a way out of AI technologies. This paper offers a review of state-of-art researches on AI deployment in TNDT, discussing the current challenges and a roadmap for application expansion. Deep Learning is the most commonly used AI technology since it has powerful feature extraction and pattern recognition capabilities for imaging processing and computer vision. Most existing research adopted Convolutional Neural Network (CNN) models utilizing only spatial information in thermal images to detect defects such as U-net, VGG, Yolo, etc. Except for defect detection, automated defect depth estimation is another focus in the deep learning method. Recurrent Neural Networks (RNNs) such as LSTM and GRUs are usually applied for extracting the temporal feature from thermal sequences, which is sensitive to defect depth. Furtherly, different deep model variations and integrated algorithms are also reviewed, which improves the performance of defect detectability. Another exciting aspect of learning models using 3-dimensional thermograms is their ability to consider the spatial and temporal features to reveal more hidden defects. Some other points, such as the training dataset, which plays a crucial role in making a robust deep model, are discussed at the end of this paper.","",""
0,"Arkadiusz Czuba","Artificial Intelligence-Based Cognitive Radar Architecture",2021,"","","","",106,"2022-07-13 09:21:09","","10.1109/CSCI54926.2021.00092","","",,,,,0,0.00,0,1,1,"This paper considers a new cognitive radar architecture based on artificial intelligence cognitive architectures (CAs). The CAs are the computational embodiments of the theory about modeling the human mind used in artificial intelligence. They found applications in, e.g., robotics and autonomous vehicles. However, the research related to the topic of integrating artificial intelligence CAs with radar systems is very limited. In this paper, a novel cognitive radar architecture has been proposed. Cognitive abilities such as learning, perception, attention, and decision-making mechanisms were conformed to radar capabilities. This new concept introduces promising visual attention mechanisms for target prioritization, declarative memories for a broader understanding of the radar environment, and reinforcement learning to improve the signal-to-noise ratio. All of the components mentioned above, combined together, look promising to improve an overall radar performance.","",""
21,"Chuan Zhang, Yeong-Luh Ueng, Christoph Studer, A. Burg","Artificial Intelligence for 5G and Beyond 5G: Implementations, Algorithms, and Optimizations",2020,"","","","",107,"2022-07-13 09:21:09","","10.1109/JETCAS.2020.3000103","","",,,,,21,10.50,5,4,2,"The communication industry is rapidly advancing towards 5G and beyond 5G (B5G) wireless technologies in order to fulfill the ever-growing needs for higher data rates and improved quality-of-service (QoS). Emerging applications require wireless connectivity with tremendously increased data rates, substantially reduced latency, and growing support for a large number of devices. These requirements pose new challenges that can no longer be efficiently addressed by conventional approaches. Artificial intelligence (AI) is considered as one of the most promising solutions to improve the performance and robustness of 5G and B5G systems, fueled by the massive amount of data generated in 5G and B5G networks and the availability of powerful data processing fabrics. As a consequence, a plethora of research on AI-based communication technologies has emerged recently, promising higher data rates and improved QoS with affordable implementation overhead. In this overview paper, we summarize the state-of-the-art of AI-based 5G and B5G techniques on the algorithm, implementation, and optimization levels. We shed light on the advantages and limitations of AI-based solutions, and we provide a summary of emerging techniques and open research problems.","",""
0,"Zhiminxing Wang, Shiguang Deng","Artificial Intelligence and Machine Learning Application in NPP MCR Speech Monitoring System",2021,"","","","",108,"2022-07-13 09:21:09","","10.1109/ICAICA52286.2021.9498220","","",,,,,0,0.00,0,2,1,"To explore the diversified human-machine interaction methods in the main control room (MCR) of nuclear power plants (NPP), this project established a complete set of intelligent speech control system based on Mandarin, and created a speech training system based on MFCC (Mel-Frequency Cepstral Coefficients) features and CTC (Connectionist Temporal Classification) related algorithm. This project has further enhanced the multilingual pronunciation sentences recognition rate, strengthened the logical connection between English letters, Chinese characters and numbers. Using the speech training set after training and noise reduction, when recognizing mixed sentences of Chinese characters, English letters, and numbers, this system can achieve a speech recognition accuracy rate higher than 93% and reduce the time spent on tasks by an average of 70%.","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",109,"2022-07-13 09:21:09","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
9,"M. Gorris, S. Hoogenboom, M. Wallace, J. V. van Hooft","Artificial intelligence for the management of pancreatic diseases",2020,"","","","",110,"2022-07-13 09:21:09","","10.1111/den.13875","","",,,,,9,4.50,2,4,2,"Novel artificial intelligence techniques are emerging in all fields of healthcare, including gastroenterology. The aim of this review is to give an overview of artificial intelligence applications in the management of pancreatic diseases. We performed a systematic literature search in PubMed and Medline up to May 2020 to identify relevant articles. Our results showed that the development of machine‐learning based applications is rapidly evolving in the management of pancreatic diseases, guiding precision medicine in clinical, endoscopic and radiologic settings. Before implementation into clinical practice, further research should focus on the external validation of novel techniques, clarifying the accuracy and robustness of these models.","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",111,"2022-07-13 09:21:09","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
2,"Alankrita, S. Srivastava","Application of Artificial Intelligence in Renewable Energy",2020,"","","","",112,"2022-07-13 09:21:09","","10.1109/ComPE49325.2020.9200065","","",,,,,2,1.00,1,2,2,"Recent shift towards renewable energy resources has increased research for addressing shortcomings of these energy resources. As major issues are related to intermittency and uncertainty of renewable supply, new technologies like artificial intelligence and machine learning offers lot of opportunity to address these issues as they are basically meant for processing of uncertain data. This paper analyses application of machine learning in different areas of renewable energy system like forecasting where machine learning is used to build accurate models, maximum power point tracking where machine learning provides robust and smooth control which is not much susceptible to noise in input, inverter where machine learning can be used to provide high quality power without fluctuation even when input is intermittent. Even though machine learning has many prospects which can be used to address different issues associated with renewable system, whether to employ it as effective solution to problem for given system or not depends on host of factors. This paper analyses all these issues and present a methodical exploration of applications of machine learning, its advantages and challenges in hybrid renewable energy system.","",""
54,"G. Collins, P. Dhiman, Constanza L. Andaur Navarro, Jie Ma, L. Hooft, J. Reitsma, P. Logullo, Andrew Beam, Lily Peng, B. van Calster, M. van Smeden, R. Riley, K. Moons","Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence",2021,"","","","",113,"2022-07-13 09:21:09","","10.1136/bmjopen-2020-048008","","",,,,,54,54.00,5,13,1,"Introduction The Transparent Reporting of a multivariable prediction model of Individual Prognosis Or Diagnosis (TRIPOD) statement and the Prediction model Risk Of Bias ASsessment Tool (PROBAST) were both published to improve the reporting and critical appraisal of prediction model studies for diagnosis and prognosis. This paper describes the processes and methods that will be used to develop an extension to the TRIPOD statement (TRIPOD-artificial intelligence, AI) and the PROBAST (PROBAST-AI) tool for prediction model studies that applied machine learning techniques. Methods and analysis TRIPOD-AI and PROBAST-AI will be developed following published guidance from the EQUATOR Network, and will comprise five stages. Stage 1 will comprise two systematic reviews (across all medical fields and specifically in oncology) to examine the quality of reporting in published machine-learning-based prediction model studies. In stage 2, we will consult a diverse group of key stakeholders using a Delphi process to identify items to be considered for inclusion in TRIPOD-AI and PROBAST-AI. Stage 3 will be virtual consensus meetings to consolidate and prioritise key items to be included in TRIPOD-AI and PROBAST-AI. Stage 4 will involve developing the TRIPOD-AI checklist and the PROBAST-AI tool, and writing the accompanying explanation and elaboration papers. In the final stage, stage 5, we will disseminate TRIPOD-AI and PROBAST-AI via journals, conferences, blogs, websites (including TRIPOD, PROBAST and EQUATOR Network) and social media. TRIPOD-AI will provide researchers working on prediction model studies based on machine learning with a reporting guideline that can help them report key details that readers need to evaluate the study quality and interpret its findings, potentially reducing research waste. We anticipate PROBAST-AI will help researchers, clinicians, systematic reviewers and policymakers critically appraise the design, conduct and analysis of machine learning based prediction model studies, with a robust standardised tool for bias evaluation. Ethics and dissemination Ethical approval has been granted by the Central University Research Ethics Committee, University of Oxford on 10-December-2020 (R73034/RE001). Findings from this study will be disseminated through peer-review publications. PROSPERO registration number CRD42019140361 and CRD42019161764.","",""
1,"Hao Wang, Zhi-yuan Wang, Bendong Wang, Zhuo-qun Yu, Zhong-he Jin, J. Crassidis","An artificial intelligence enhanced star identification algorithm",2020,"","","","",114,"2022-07-13 09:21:09","","10.1631/FITEE.1900590","","",,,,,1,0.50,0,6,2,"An artificial intelligence enhanced star identification algorithm is proposed for star trackers in lost-in-space mode. A convolutional neural network model based on Vgg16 is used in the artificial intelligence algorithm to classify star images. The training dataset is constructed to achieve the networks’ optimal performance. Simulation results show that the proposed algorithm is highly robust to many kinds of noise, including position noise, magnitude noise, false stars, and the tracker’s angular velocity. With a deep convolutional neural network, the identification accuracy is maintained at 96% despite noise and interruptions, which is a significant improvement to traditional pyramid and grid algorithms.","",""
0,"Cheng Li, Chris Cheng","Prediction and Optimization of Rate of Penetration using a Hybrid Artificial Intelligence Method based on an Improved Genetic Algorithm and Artificial Neural Network",2020,"","","","",115,"2022-07-13 09:21:09","","10.2118/203229-ms","","",,,,,0,0.00,0,2,2,"  Oil and gas exploration is facing an ever-increasing demand for cost-efficient drilling operations. Improvement of the rate of penetration (ROP) of the drill bit is key in solving the aforementioned challenge. The objective of this study is to develop a more accurate and effective predictive and optimization model for ROP that utilizes a hybrid artificial intelligence model based on an improved genetic algorithm (IGA) and artificial neural network (ANN) for further optimization of drilling processes.  Real field drilling datasets such as the bit type, bit drilling time, rotation per minute, weight on bit, torque, formation type, rock properties, hydraulics, and drilling mud properties are collected and input to train, validate and test the developed IGA-ANN model for ROP prediction and optimization. We apply a Savitzky-Golay (SG) smoothing filter to reduce the noise from the raw datasets. We apply IGA to find the optimal structures, parameters, and types of input of the ANN. By using supplementary population, multi-type crossover and mutation and adaptive dynamics probability adjustments, the developed model, IGA-ANN, avoid the limited optimization and local convergence problems in the classical Genetic Algorithm (GA). Using the developed prediction model, we obtain the optimal operational parameter within a region considering drilling equipment capability and wear to maximize ROP.  From numerical results, we find that the optimal structures and parameters of the ANN can be obtained efficiently by the developed method. For comparison, we compare IGA-ANN with the classical wrapper algorithm for parameter selection. The results indicate that IGA-ANN is more stable and accurate than the wrapper algorithm. We compare the true ROP and predicted ROP from the developed IGA-ANN model using accuracy indicators such as root mean square error, mean absolute error, and regression coefficient (R2). For comparison, the accuracy of the classical regression model is presented. We find that IGA-ANN yielded more accurate test results (R2 = 0.97). We compare the results of IGA-ANN trained using SG smoothing filter processed data and raw data. The test results show that the noise reduction approach used is very efficient in increasing the accuracy of IGA-ANN. Using the developed model, we optimize the choice of drilling operational parameters within a region considering drilling equipment capability and wear. We find that the optimization can increase average ROP significantly.  We develop an efficient and robust algorithm, IGA-ANN, for ROP prediction and optimization. Compared with the classical wrapper algorithm and multiple regression model, the IGA-ANN can efficiently optimize the structures, parameters and types of inputs of ANN to achieve higher ROP prediction accuracy. By utilizing the developed model, we can efficiently maximize ROP and minimize the drilling operation cost.","",""
21,"Adrien Bécue, Isabel Praça, J. Gama","Artificial intelligence, cyber-threats and Industry 4.0: challenges and opportunities",2021,"","","","",116,"2022-07-13 09:21:09","","10.1007/S10462-020-09942-2","","",,,,,21,21.00,7,3,1,"","",""
16,"A. Amritphale, Ranojoy Chatterjee, Suvo Chatterjee, N. Amritphale, Ali Rahnavard, G. Awan, B. Omar, G. Fonarow","Predictors of 30-Day Unplanned Readmission After Carotid Artery Stenting Using Artificial Intelligence",2021,"","","","",117,"2022-07-13 09:21:09","","10.1007/s12325-021-01709-7","","",,,,,16,16.00,2,8,1,"","",""
2,"Yuan Huang, Z. Cheng, Qianyu Zhou, Yuxing Xiang, Ruixiao Zhao","Data Mining Algorithm for Cloud Network Information Based on Artificial Intelligence Decision Mechanism",2020,"","","","",118,"2022-07-13 09:21:09","","10.1109/ACCESS.2020.2981632","","",,,,,2,1.00,0,5,2,"Due to the rapid development of information technology and network technology, there is a lot of data, but the phenomenon of lack of knowledge is becoming more and more serious. Data mining technology has developed vigorously in this environment, and it has shown more and more vitality. Based on Spark programming model, this paper designs the parallel extension of fuzzy c-means. In order to enhance the performance of fuzzy c-means parallel expansion, the improvement strategy of k-means during the initialization phase is borrowed, and k-means// is extended to fuzzy c-means to obtain better clustering performance. Combined with Spark’s programming model, this paper can obtain extended parallel fuzzy c-means algorithm. Several experiments on the data set of the algorithm proposed in this paper have shown good scalability and parallelism, effectively expanding fuzzy c-means clustering to distributed applications, greatly increasing the scale of the data processed by the algorithm. This improves the robustness of the algorithm and the adaptability of the algorithm to the shape and structure of the data, so that the parallel and scalable clustering algorithm can more effectively perform cluster analysis on big data. Three algorithms were simulated on MATLAB platform. We use simple data sets and complex two-dimensional data sets, and compare with the traditional fuzzy c-means algorithm and fuzzy c-means algorithm based on fuzzy entropy. Experiments show that the scalable parallel fuzzy c-means algorithm not only greatly improves the anti-noise performance, but also improves the convergence speed, and it can automatically determine the optimal number of clusters.","",""
34,"M. Aliabadi, M. Farhadian, E. Darvishi","Prediction of hearing loss among the noise-exposed workers in a steel factory using artificial intelligence approach",2015,"","","","",119,"2022-07-13 09:21:09","","10.1007/s00420-014-1004-z","","",,,,,34,4.86,11,3,7,"","",""
109,"Shilin Qiu, Qihe Liu, Shijie Zhou, Chunjiang Wu","Review of Artificial Intelligence Adversarial Attack and Defense Technologies",2019,"","","","",120,"2022-07-13 09:21:09","","10.3390/APP9050909","","",,,,,109,36.33,27,4,3,"In recent years, artificial intelligence technologies have been widely used in computer vision, natural language processing, automatic driving, and other fields. However, artificial intelligence systems are vulnerable to adversarial attacks, which limit the applications of artificial intelligence (AI) technologies in key security fields. Therefore, improving the robustness of AI systems against adversarial attacks has played an increasingly important role in the further development of AI. This paper aims to comprehensively summarize the latest research progress on adversarial attack and defense technologies in deep learning. According to the target model’s different stages where the adversarial attack occurred, this paper expounds the adversarial attack methods in the training stage and testing stage respectively. Then, we sort out the applications of adversarial attack technologies in computer vision, natural language processing, cyberspace security, and the physical world. Finally, we describe the existing adversarial defense methods respectively in three main categories, i.e., modifying data, modifying models and using auxiliary tools.","",""
167,"C. Langlotz, Bibb Allen, B. Erickson, Jayashree Kalpathy-Cramer, K. Bigelow, T. Cook, A. Flanders, M. Lungren, D. Mendelson, J. Rudie, Ge Wang, K. Kandarpa","A Roadmap for Foundational Research on Artificial Intelligence in Medical Imaging: From the 2018 NIH/RSNA/ACR/The Academy Workshop.",2019,"","","","",121,"2022-07-13 09:21:09","","10.1148/radiol.2019190613","","",,,,,167,55.67,17,12,3,"Imaging research laboratories are rapidly creating machine learning systems that achieve expert human performance using open-source methods and tools. These artificial intelligence systems are being developed to improve medical image reconstruction, noise reduction, quality assurance, triage, segmentation, computer-aided detection, computer-aided classification, and radiogenomics. In August 2018, a meeting was held in Bethesda, Maryland, at the National Institutes of Health to discuss the current state of the art and knowledge gaps and to develop a roadmap for future research initiatives. Key research priorities include: 1, new image reconstruction methods that efficiently produce images suitable for human interpretation from source data; 2, automated image labeling and annotation methods, including information extraction from the imaging report, electronic phenotyping, and prospective structured image reporting; 3, new machine learning methods for clinical imaging data, such as tailored, pretrained model architectures, and federated machine learning methods; 4, machine learning methods that can explain the advice they provide to human users (so-called explainable artificial intelligence); and 5, validated methods for image de-identification and data sharing to facilitate wide availability of clinical imaging data sets. This research roadmap is intended to identify and prioritize these needs for academic research laboratories, funding agencies, professional societies, and industry.","",""
31,"T. Ertekin, Qian Sun","Artificial Intelligence Applications in Reservoir Engineering: A Status Check",2019,"","","","",122,"2022-07-13 09:21:09","","10.3390/EN12152897","","",,,,,31,10.33,16,2,3,"This article provides a comprehensive review of the state-of-art in the area of artificial intelligence applications to solve reservoir engineering problems. Research works including proxy model development, artificial-intelligence-assisted history-matching, project design, and optimization, etc. are presented to demonstrate the robustness of the intelligence systems. The successes of the developments prove the advantages of the AI approaches in terms of high computational efficacy and strong learning capabilities. Thus, the implementation of intelligence models enables reservoir engineers to accomplish many challenging and time-intensive works more effectively. However, it is not yet astute to completely replace the conventional reservoir engineering models with intelligent systems, since the defects of the technology cannot be ignored. The trend of research and industrial practices of reservoir engineering area would be establishing a hand-shaking protocol between the conventional modeling and the intelligent systems. Taking advantages of both methods, more robust solutions could be obtained with significantly less computational overheads.","",""
29,"Melanie Mitchell","Artificial Intelligence Hits the Barrier of Meaning",2019,"","","","",123,"2022-07-13 09:21:09","","10.3390/info10020051","","",,,,,29,9.67,29,1,3,"Today’s AI systems sorely lack the essence of human intelligence: Understanding the situations we experience, being able to grasp their meaning. The lack of humanlike understanding in machines is underscored by recent studies demonstrating lack of robustness of state-of-the-art deep-learning systems. Deeper networks and larger datasets alone are not likely to unlock AI’s “barrier of meaning”; instead the field will need to embrace its original roots as an interdisciplinary science of intelligence.","",""
41,"Muhammad Arsalan, Muhammad Owais, T. Mahmood, S. Cho, K. Park","Aiding the Diagnosis of Diabetic and Hypertensive Retinopathy Using Artificial Intelligence-Based Semantic Segmentation",2019,"","","","",124,"2022-07-13 09:21:09","","10.3390/jcm8091446","","",,,,,41,13.67,8,5,3,"Automatic segmentation of retinal images is an important task in computer-assisted medical image analysis for the diagnosis of diseases such as hypertension, diabetic and hypertensive retinopathy, and arteriosclerosis. Among the diseases, diabetic retinopathy, which is the leading cause of vision detachment, can be diagnosed early through the detection of retinal vessels. The manual detection of these retinal vessels is a time-consuming process that can be automated with the help of artificial intelligence with deep learning. The detection of vessels is difficult due to intensity variation and noise from non-ideal imaging. Although there are deep learning approaches for vessel segmentation, these methods require many trainable parameters, which increase the network complexity. To address these issues, this paper presents a dual-residual-stream-based vessel segmentation network (Vess-Net), which is not as deep as conventional semantic segmentation networks, but provides good segmentation with few trainable parameters and layers. The method takes advantage of artificial intelligence for semantic segmentation to aid the diagnosis of retinopathy. To evaluate the proposed Vess-Net method, experiments were conducted with three publicly available datasets for vessel segmentation: digital retinal images for vessel extraction (DRIVE), the Child Heart Health Study in England (CHASE-DB1), and structured analysis of retina (STARE). Experimental results show that Vess-Net achieved superior performance for all datasets with sensitivity (Se), specificity (Sp), area under the curve (AUC), and accuracy (Acc) of 80.22%, 98.1%, 98.2%, and 96.55% for DRVIE; 82.06%, 98.41%, 98.0%, and 97.26% for CHASE-DB1; and 85.26%, 97.91%, 98.83%, and 96.97% for STARE dataset.","",""
0,"Pan Wang, Yangyang Zhong, Zhenan Yao","Modeling and Estimation of CO2 Emissions in China Based on Artificial Intelligence",2022,"","","","",125,"2022-07-13 09:21:09","","10.1155/2022/6822467","","",,,,,0,0.00,0,3,1,"Since China’s reform and opening up, the social economy has achieved rapid development, followed by a sharp increase in carbon dioxide (CO2) emissions. Therefore, at the 75th United Nations General Assembly, China proposed to achieve carbon peaking by 2030 and carbon neutrality by 2060. The research work on advance forecasting of CO2 emissions is essential to achieve the above-mentioned carbon peaking and carbon neutrality goals in China. In order to achieve accurate prediction of CO2 emissions, this study establishes a hybrid intelligent algorithm model suitable for CO2 emissions prediction based on China’s CO2 emissions and related socioeconomic indicator data from 1971 to 2017. The hyperparameters of Least Squares Support Vector Regression (LSSVR) are optimized by the Adaptive Artificial Bee Colony (AABC) algorithm to build a high-performance hybrid intelligence model. The research results show that the hybrid intelligent algorithm model designed in this paper has stronger robustness and accuracy with relative error almost within ±5% in the advance prediction of CO2 emissions. The modeling scheme proposed in this study can not only provide strong support for the Chinese government and industry departments to formulate policies related to the carbon peaking and carbon neutrality goals, but also can be extended to the research of other socioeconomic-related issues.","",""
23,"A. Renda","Artificial Intelligence Ethics, governance and policy challenges. Report of a CEPS Task Force, February 2019",2019,"","","","",126,"2022-07-13 09:21:09","","","","",,,,,23,7.67,23,1,3,"Like an unannounced guest, artificial intelligence (AI) has suddenly emerged  from nerdy discussions in university labs and begun to infiltrate larger venues  and policy circles around the globe. Everywhere, and particularly in Europe, the  debate has been tainted by much noise and fear, as evidenced in the European  Parliament’s resounding report on civil law rules for robotics, in which Mary  Shelley’s Frankenstein is evoked on the opening page (European Parliament,  2016). At countless seminars, workshops and conferences, self-proclaimed  “experts” voice concerns about robots taking our jobs, disrupting our social  interactions, manipulating public opinion and political elections, and ultimately  taking over the world by dismissing human beings, once and for all, as  redundant and inefficient legacies of the past.","",""
17,"Naveed Abbas, Tanveer Ahmed, Syed Habib Ullah Shah, Muhammad Omar, H. Park","Investigating the applications of artificial intelligence in cyber security",2019,"","","","",127,"2022-07-13 09:21:09","","10.1007/s11192-019-03222-9","","",,,,,17,5.67,3,5,3,"","",""
27,"A. Zolfagharian, A. Noshadi, M. R. Khosravani, M. Zain","Unwanted noise and vibration control using finite element analysis and artificial intelligence",2014,"","","","",128,"2022-07-13 09:21:09","","10.1016/J.APM.2013.10.039","","",,,,,27,3.38,7,4,8,"","",""
0,"Jia Wu, Pei Xiao, Haojie Huang, Fangfang Gou, Zhixun Zhou, Z. Dai","An artificial intelligence multiprocessing scheme for the diagnosis of osteosarcoma MRI images.",2022,"","","","",129,"2022-07-13 09:21:09","","10.1109/JBHI.2022.3184930","","",,,,,0,0.00,0,6,1,"Osteosarcoma is the most common malignant osteosarcoma, and most developing countries face great challenges in the diagnosis due to the lack of medical resources. Magnetic resonance imaging (MRI) has always been an important tool for the detection of osteosarcoma, but it is a time-consuming and labor-intensive task for doctors to manually identify MRI images. It is highly subjective and prone to misdiagnosis. Existing computer-aided diagnosis methods of osteosarcoma MRI images focus only on accuracy, ignoring the lack of computing resources in developing countries. In addition, the large amount of redundant and noisy data generated during imaging should also be considered. To alleviate the inefficiency of osteosarcoma diagnosis faced by developing countries, this paper proposed an artificial intelligence multiprocessing scheme for pre-screening, noise reduction, and segmentation of osteosarcoma MRI images. For pre-screening, we propose the Slide Block Filter to remove useless images. Next, we introduced a fast non-local means algorithm using integral images to denoise noisy images. We then segmented the filtered and denoised MRI images using a U-shaped network (ETUNet) embedded with a transformer layer, which enhances the functionality and robustness of the traditional U-shaped architecture. Finally, we further optimized the segmented tumor boundaries using conditional random fields. This paper conducted experiments on more than 70,000 MRI images of osteosarcoma from three hospitals in China. The experimental results show that our proposed methods have good results and better performance in pre-screening, noise reduction, and segmentation.","",""
10,"T. Penzkofer, A. Padhani, B. Turkbey, M. Haider, H. Huisman, J. Walz, G. Salomon, I. Schoots, J. Richenberg, G. Villeirs, V. Panebianco, O. Rouvière, V. Løgager, J. Barentsz","ESUR/ESUI position paper: developing artificial intelligence for precision diagnosis of prostate cancer using magnetic resonance imaging",2021,"","","","",130,"2022-07-13 09:21:09","","10.1007/s00330-021-08021-6","","",,,,,10,10.00,1,14,1,"","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",131,"2022-07-13 09:21:09","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
12,"M. S. Munir, S. F. Abedin, C. Hong","Artificial Intelligence-based Service Aggregation for Mobile-Agent in Edge Computing",2019,"","","","",132,"2022-07-13 09:21:09","","10.23919/APNOMS.2019.8892984","","",,,,,12,4.00,4,3,3,"The ongoing development of edge computing in fifth-generation (5G) networks promises to provide an artificial intelligence-as-a-service (AIaaS) for meeting the stringent requirements of everything as a service (XaaS) in the edge of the networks. Therefore, the concept of edge-artificial intelligence (edge-AI) is not only evolving but also emergent enabler toward AI service fulfillment. In this paper, we investigate an AI-based service aggregation problem for a mobile agent in AIaaS-enabled edge computing. First, we propose an optimization problem for the mobile agent and the objective is to maximize the AI service fulfillment achieved rate while satisfying the computational, memory, and delay requirements. Thus, we show that this optimization problem is NP-hard. Second, we compel the formulated problem in a community discovery problem and derive a solution by executing a data-driven approach. To do this, we incorporate density-based spatial clustering of applications with noise (DBSCAN) and flow control algorithm, and propose a low computational complexity algorithm for AI service aggregation of the mobile agent. Finally, numerical analysis shows the proposed model can perform better over other baseline methods in terms of deprived AI services, server utilization, and complexity analysis.","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",133,"2022-07-13 09:21:09","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
0,"Bhawna Dhruv, Neetu Mittal, Megha Modi","Artificial intelligence optimized image segmentation techniques for renal cyst detection",2022,"","","","",134,"2022-07-13 09:21:09","","10.1080/03091902.2022.2080882","","",,,,,0,0.00,0,3,1,"Abstract The vast number of image modalities available nowadays has given rise and access to a number of medical images. These images perhaps suffer issues such as low contrast, noise, ill-defined boundaries and poor visualisation. Therefore, a need for effective segmentation arises. Medical image segmentation plays a significant role in identifying a disorder, treatment planning, routine follow ups and computer-guided surgery respectively. The paper presents automatic medical image segmentation to overcome the imaging concerns and demarcate each notch & boundary in an image. The proposed algorithm identifies the existing kidney cyst precisely as they may be related to extreme disorders that may affect kidney function. The algorithm has been further tested on automatic segmentation using Genetic Algorithm, Ant Colony Optimisation and Fuzzy C Means Clustering. In terms of visualisation of valuable pathology, GA stands out and further helps in better assessment of the extent of the disease providing with better representation of the kidney cysts thereby giving a better diagnostic assurance and understanding of the nature of any disorder helping the medical practitioners as well as the patients. Experimental results on segmentation of kidney CT images conclusively demonstrate that the Genetic Algorithm is much more effective and robust.","",""
0,"Waleed Albattah, A. Javed, Marriam Nawaz, M. Masood, Saleh Albahli","Artificial Intelligence-Based Drone System for Multiclass Plant Disease Detection Using an Improved Efficient Convolutional Neural Network",2022,"","","","",135,"2022-07-13 09:21:09","","10.3389/fpls.2022.808380","","",,,,,0,0.00,0,5,1,"The role of agricultural development is very important in the economy of a country. However, the occurrence of several plant diseases is a major hindrance to the growth rate and quality of crops. The exact determination and categorization of crop leaf diseases is a complex and time-required activity due to the occurrence of low contrast information in the input samples. Moreover, the alterations in the size, location, structure of crop diseased portion, and existence of noise and blurriness effect in the input images further complicate the classification task. To solve the problems of existing techniques, a robust drone-based deep learning approach is proposed. More specifically, we have introduced an improved EfficientNetV2-B4 with additional added dense layers at the end of the architecture. The customized EfficientNetV2-B4 calculates the deep key points and classifies them in their related classes by utilizing an end-to-end training architecture. For performance evaluation, a standard dataset, namely, the PlantVillage Kaggle along with the samples captured using a drone is used which is complicated in the aspect of varying image samples with diverse image capturing conditions. We attained the average precision, recall, and accuracy values of 99.63, 99.93, and 99.99%, respectively. The obtained results confirm the robustness of our approach in comparison to other recent techniques and also show less time complexity.","",""
0,"A. Celebi, E. Bulut, Aysun Sezer","Artificial intelligence based detection of age-related macular degeneration using optical coherence tomography with unique image preprocessing.",2022,"","","","",136,"2022-07-13 09:21:09","","10.1177/11206721221096294","","",,,,,0,0.00,0,3,1,"PURPOSE The aim of the study is to improve the accuracy of age related macular degeneration (AMD) disease in its earlier phases with proposed Capsule Network (CapsNet) architecture trained on speckle noise reduced spectral domain optical coherence tomography (SD-OCT) images based on an optimized Bayesian non-local mean (OBNLM) filter augmentation techniques.   METHODS A total of 726 local SD-OCT images were collected and labelled as 159 drusen, 145 dry AMD, 156 wet AMD and 266 normal. Region of interest (ROI) was identified. Speckle noise in SD-OCT images were reduced based on OBNLM filter. The processed images were fed to proposed CapsNet architecture to clasify SD-OCT images. Accuracy rates were calculated in both public and local dataset.   RESULTS Accuracy rate of local SD-OCT image dataset classification was achieved to a value of 96.39% after performing data augmentation and speckle noise reduction with OBNLM. The performance of proposed CapsNet was also evaluated on the public Kaggle dataset under the same processing procedures and the accuracy rate was calculated as 98.07%. The sensitivity and specificity rates were 96.72% and 99.98%, respectively.   CONCLUSIONS The classification success of proposed CapsNet may be improved with robust pre-processing steps like; determination of ROI and denoised SD-OCT images based on OBNLM. These impactful image preprocessing steps yielded higher accuracy rates for determining different types of AMD including its precursor lesion on the both local and public dataset with proposed CapsNet architecture.","",""
0,"Zhiyan Zheng, Ruixuan He, Cuijun Lin, Chunyu Huang","Multimodal Magnetic Resonance Imaging to Diagnose Knee Osteoarthritis under Artificial Intelligence",2022,"","","","",137,"2022-07-13 09:21:09","","10.1155/2022/6488889","","",,,,,0,0.00,0,4,1,"This work aimed to investigate the application value of the multimodal magnetic resonance imaging (MRI) algorithm based on the low-rank decomposition denoising (LRDD) in the diagnosis of knee osteoarthritis (KOA), so as to offer a better examination method in the clinic. Seventy-eight patients with KOA were selected as the research objects, and they all underwent T1-weighted imaging (T1WI), T2-weighted imaging (T2WI), fat suppression T2WI (SE-T2WI), and fat saturation T2WI (FS-T2WI). All obtained images were processed by using the I-LRDD algorithm. According to the degree of articular cartilage lesions under arthroscopy, the patients were divided into a group I, a group II, a group III, and a group IV. The sensitivity, specificity, accuracy, and consistency of KOA diagnosis of T1WI, T2WI, SE-T2WI, and FS-T2WI were analyzed by referring to the results of arthroscopy. The results showed that the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM) of the I-LRDD algorithm used in this work were higher than those of image block priori denoising (IBPD) and LRDD, and the time consumption was lower than that of IBDP and LRDD (p < 0.05). The sensitivity, specificity, accuracy, and consistency (Kappa value) of multimodal MRI in the diagnosis of KOA were 88.61%, 85.3%, 87.37%, and 0.73%, respectively, which were higher than those of T1WI, T2WI, SE-T2WI, and FS-T2WI. The sensitivity, specificity, accuracy, and consistency of multimodal MRI in diagnosing lesions in group IV were 95%, 96.10%, 95.88%, and 0.70%, respectively, which were much higher than those in groups I, II, and III (p < 0.05). In conclusion, the LRDD algorithm shows a good image processing efficacy, and the multimodal MRI showed a good diagnosis effect on KOA, which was worthy of promotion clinically.","",""
9,"B. N. Manjunatha Reddy, S. K. Pramada, T. Roshni","Monthly surface runoff prediction using artificial intelligence: A study from a tropical climate river basin",2021,"","","","",138,"2022-07-13 09:21:09","","10.1007/s12040-020-01508-8","","",,,,,9,9.00,3,3,1,"","",""
103,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dentistry: Chances and Challenges",2020,"","","","",139,"2022-07-13 09:21:09","","10.1177/0022034520915714","","",,,,,103,51.50,34,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
0,"S. Sadeghi, M. Amiri, Farzaneh Mansoori Mooseloo","Artificial Intelligence and Its Application in Optimization under Uncertainty",2021,"","","","",140,"2022-07-13 09:21:09","","10.5772/intechopen.98628","","",,,,,0,0.00,0,3,1,"Nowadays, the increase in data acquisition and availability and complexity around optimization make it imperative to jointly use artificial intelligence (AI) and optimization for devising data-driven and intelligent decision support systems (DSS). A DSS can be successful if large amounts of interactive data proceed fast and robustly and extract useful information and knowledge to help decision-making. In this context, the data-driven approach has gained prominence due to its provision of insights for decision-making and easy implementation. The data-driven approach can discover various database patterns without relying on prior knowledge while also handling flexible objectives and multiple scenarios. This chapter reviews recent advances in data-driven optimization, highlighting the promise of data-driven optimization that integrates mathematical programming and machine learning (ML) for decision-making under uncertainty and identifies potential research opportunities. This chapter provides guidelines and implications for researchers, managers, and practitioners in operations research who want to advance their decision-making capabilities under uncertainty concerning data-driven optimization. Then, a comprehensive review and classification of the relevant publications on the data-driven stochastic program, data-driven robust optimization, and data-driven chance-constrained are presented. This chapter also identifies fertile avenues for future research that focus on deep-data-driven optimization, deep data-driven models, as well as online learning-based data-driven optimization. Perspectives on reinforcement learning (RL)-based data-driven optimization and deep RL for solving NP-hard problems are discussed. We investigate the application of data-driven optimization in different case studies to demonstrate improvements in operational performance over conventional optimization methodology. Finally, some managerial implications and some future directions are provided.","",""
50,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello","Artificial Intelligence, Algorithmic Pricing, and Collusion",2020,"","","","",141,"2022-07-13 09:21:09","","10.1257/AER.20190623","","",,,,,50,25.00,13,4,2,"Increasingly, pricing algorithms are supplanting human decision making in real marketplaces. To inform the competition policy debate on the possible consequences of this development, we experiment with pricing algorithms powered by Artificial Intelligence (AI) in controlled environments (computer simulations), studying the interaction among a number of Q-learning algorithms in a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. In this setting the algorithms consistently learn to charge supra-competitive prices, without communicating with one another. The high prices are sustained by classical collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.","",""
43,"M. González-Rivero, Oscar Beijbom, A. Rodriguez-Ramirez, D. Bryant, A. Ganase, Y. González-Marrero, A. Herrera-Reveles, E. Kennedy, Catherine J. S. Kim, S. Lopez-Marcano, Kathryn Markey, B. Neal, K. Osborne, C. Reyes-Nivia, E. Sampayo, Kristin Stolberg, Abbie Taylor, J. Vercelloni, Mathew Wyatt, O. Hoegh‐Guldberg","Monitoring of Coral Reefs Using Artificial Intelligence: A Feasible and Cost-Effective Approach",2020,"","","","",142,"2022-07-13 09:21:09","","10.3390/rs12030489","","",,,,,43,21.50,4,20,2,"Ecosystem monitoring is central to effective management, where rapid reporting is essential to provide timely advice. While digital imagery has greatly improved the speed of underwater data collection for monitoring benthic communities, image analysis remains a bottleneck in reporting observations. In recent years, a rapid evolution of artificial intelligence in image recognition has been evident in its broad applications in modern society, offering new opportunities for increasing the capabilities of coral reef monitoring. Here, we evaluated the performance of Deep Learning Convolutional Neural Networks for automated image analysis, using a global coral reef monitoring dataset. The study demonstrates the advantages of automated image analysis for coral reef monitoring in terms of error and repeatability of benthic abundance estimations, as well as cost and benefit. We found unbiased and high agreement between expert and automated observations (97%). Repeated surveys and comparisons against existing monitoring programs also show that automated estimation of benthic composition is equally robust in detecting change and ensuring the continuity of existing monitoring data. Using this automated approach, data analysis and reporting can be accelerated by at least 200x and at a fraction of the cost (1%). Combining commonly used underwater imagery in monitoring with automated image annotation can dramatically improve how we measure and monitor coral reefs worldwide, particularly in terms of allocating limited resources, rapid reporting and data integration within and across management areas.","",""
38,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases.",2020,"","","","",143,"2022-07-13 09:21:09","","10.1038/s41746-020-0229-3","","",,,,,38,19.00,6,6,2,"","",""
45,"Avishek Choudhury, Onur Asan","Role of Artificial Intelligence in Patient Safety Outcomes: Systematic Literature Review",2020,"","","","",144,"2022-07-13 09:21:09","","10.2196/18599","","",,,,,45,22.50,23,2,2,"Background Artificial intelligence (AI) provides opportunities to identify the health risks of patients and thus influence patient safety outcomes. Objective The purpose of this systematic literature review was to identify and analyze quantitative studies utilizing or integrating AI to address and report clinical-level patient safety outcomes. Methods We restricted our search to the PubMed, PubMed Central, and Web of Science databases to retrieve research articles published in English between January 2009 and August 2019. We focused on quantitative studies that reported positive, negative, or intermediate changes in patient safety outcomes using AI apps, specifically those based on machine-learning algorithms and natural language processing. Quantitative studies reporting only AI performance but not its influence on patient safety outcomes were excluded from further review. Results We identified 53 eligible studies, which were summarized concerning their patient safety subcategories, the most frequently used AI, and reported performance metrics. Recognized safety subcategories were clinical alarms (n=9; mainly based on decision tree models), clinical reports (n=21; based on support vector machine models), and drug safety (n=23; mainly based on decision tree models). Analysis of these 53 studies also identified two essential findings: (1) the lack of a standardized benchmark and (2) heterogeneity in AI reporting. Conclusions This systematic review indicates that AI-enabled decision support systems, when implemented correctly, can aid in enhancing patient safety by improving error detection, patient stratification, and drug management. Future work is still needed for robust validation of these systems in prospective and real-world clinical environments to understand how well AI can predict safety outcomes in health care settings.","",""
37,"Z. Yaseen, Z. H. Ali, Sinan Q. Salih, N. Al‐Ansari","Prediction of Risk Delay in Construction Projects Using a Hybrid Artificial Intelligence Model",2020,"","","","",145,"2022-07-13 09:21:09","","10.3390/su12041514","","",,,,,37,18.50,9,4,2,"Project delays are the major problems tackled by the construction sector owing to the associated complexity and uncertainty in the construction activities. Artificial Intelligence (AI) models have evidenced their capacity to solve dynamic, uncertain and complex tasks. The aim of this current study is to develop a hybrid artificial intelligence model called integrative Random Forest classifier with Genetic Algorithm optimization (RF-GA) for delay problem prediction. At first, related sources and factors of delay problems are identified. A questionnaire is adopted to quantify the impact of delay sources on project performance. The developed hybrid model is trained using the collected data of the previous construction projects. The proposed RF-GA is validated against the classical version of an RF model using statistical performance measure indices. The achieved results of the developed hybrid RF-GA model revealed a good resultant performance in terms of accuracy, kappa and classification error. Based on the measured accuracy, kappa and classification error, RF-GA attained 91.67%, 87% and 8.33%, respectively. Overall, the proposed methodology indicated a robust and reliable technique for project delay prediction that is contributing to the construction project management monitoring and sustainability.","",""
34,"Shashank Vaid, Aaron McAdie, Ran Kremer, V. Khanduja, M. Bhandari","Risk of a second wave of Covid-19 infections: using artificial intelligence to investigate stringency of physical distancing policies in North America",2020,"","","","",146,"2022-07-13 09:21:09","","10.1007/s00264-020-04653-3","","",,,,,34,17.00,7,5,2,"","",""
3,"Soufiane Abi, Hamid Bouyghf, B. Benhala, A. Raihani","An Optimal Design of a Short-Channel RF Low Noise Amplifier Using a Swarm Intelligence Technique",2020,"","","","",147,"2022-07-13 09:21:09","","10.1007/978-981-15-0947-6_14","","",,,,,3,1.50,1,4,2,"","",""
29,"Grayson W. Armstrong, A. Lorch","A(eye): A Review of Current Applications of Artificial Intelligence and Machine Learning in Ophthalmology",2019,"","","","",148,"2022-07-13 09:21:09","","10.1097/IIO.0000000000000298","","",,,,,29,9.67,15,2,3,"Artificial intelligence (AI) is a branch of computer science that aims to enable computers to perform human-like tasks. Although AI is a broad discipline, machine learning is a specific branch of AI that uses computer algorithms capable of “learning” through the simulation of human intelligence. Machine learning algorithms have been applied to the medical field since the 1970s,1 and since that time have proven useful in computerassisted diagnosis, screening, and prognostication of disease.2–7 Ophthalmology is uniquely capable of capitalizing on the promise of AI. Ophthalmologists, during routine clinical encounters, generate robust data sources capable of supporting machine learning algorithms including multimodal ophthalmic images and quantifiable metrics such as visual acuity (VA), intraocular pressure, and cup to disk ratio. To date, AI techniques have been applied to ophthalmology to screen for and diagnose diseases, such as diabetic retinopathy (DR), age-related macular degeneration (AMD), macular edema (ME), glaucoma, keratoconus, postlaserassisted in situ keratomileusis corneal ectasia, retinopathy of prematurity (ROP), and cataracts, as well as predict the prognosis of various ophthalmic diseases. Advances in ophthalmology-specific AI stand to increase patient access to clinical screening and diagnosis as well as decrease health care costs, especially when applied to high-risk populations, low-resource communities, or when combined with telemedicine initiatives. This review provides an introduction to AI andmachine learning, as well as an overview of current applications in the field of ophthalmology.","",""
20,"L. McCoy, Sujay Nagaraj, F. Morgado, V. Harish, Sunit Das, L. Celi","What do medical students actually need to know about artificial intelligence?",2020,"","","","",149,"2022-07-13 09:21:09","","10.1038/s41746-020-0294-7","","",,,,,20,10.00,3,6,2,"","",""
0,"Xiaohong W. Gao, B. Braden","Artificial intelligence in endoscopy: The challenges and future directions",2021,"","","","",150,"2022-07-13 09:21:09","","10.37126/aige.v2.i4.117","","",,,,,0,0.00,0,2,1,"Artificial intelligence based approaches, in particular deep learning, have achieved state-of-the-art performance in medical fields with increasing number of software systems being approved by both Europe and United States. This paper reviews their applications to early detection of oesophageal cancers with a focus on their advantages and pitfalls. The paper concludes with future recommendations towards the development of a real-time, clinical implementable, interpretable and robust diagnosis support systems.","",""
0,"R. Mohanasundaram","Editorial: Special Section on Human-Centered Artificial Intelligence with Big Data Applications",2021,"","","","",151,"2022-07-13 09:21:09","","10.1520/JTE20219999","","",,,,,0,0.00,0,1,1,"In recent years, human-centered artificial intelligence (AI) has become the most promising research domain in both industrial and academic areas worldwide. AI is the next step on the journey from big data to full automation. Human needs are the motivation behind improvements in computing paradigms. In the aforementioned areas, system-generated information such as smart devices, sensors, agents, and meters—as well as human-generated information such as texts, photos, and videos—lead to a tremendous amount of data while new levels of security, performance, and reliability are required. This Special Section aims to highlight the unique areas of human-centered AI with big data applications and various innovations in multidiscipline areas, while also presenting technical evidence and its countermeasure. This Special Section aims to identify the emerging artificial intelligence with big data in all human-centered (HC) related areas. It consists of up-to-date, state-of-the-art research contributions with novel designs and developments of intelligent application, perception, and security methods in human-centered AI, to enhance the reliability and feasibility of HC in real-world applications. The first three papers by Zhang et al., Huang and Liu, and Qing et al. deal with performance and effect analysis of China's financial venture capital development, multimedia-assisted children’s tennis skills, and agglomeration in the middle reach of the Yangtze River. Shree et al. propose a new fusion-based agricultural synthetic aperture radar (SAR) image despeckling by using anisotropic diffusion and discrete wavelet transform method. SAR images have applications in various fields. Speckle noise, which has the characteristic of multiplicative noise, degrades the image quality of SAR images, which causes information loss. This study proposes a speckle noise reduction algorithm while using the speckle reducing anisotropic diffusion filter, discrete wavelet transform to remove speckle noise. The papers by Ren and Cui, Zhang, and Zhang and Li concentrate on the use of multimedia technology in college English reading teaching, gymnastics teaching, and musical drama teaching. They show that multimedia technology has a positive influence on college education, as it promotes scientific, advanced, and vivid development of college physical education. However, there are still problems in the application of multimedia technology in college physical education; for example, the problem in the links between multimedia teaching and traditional teaching and in the great influence of courseware on teaching effects. So it is necessary to accelerate multimedia technology development, strengthen the application of multimedia technology in college education, achieve proper cooperation between traditional and multimedia teaching, and enrich multimedia courseware and its effect. Yao et al. review the general application of multimedia technology in teaching innovation. Li et al. propose a design and implementation of multimedia technology-assisted English vocabulary teaching courseware for industrial engineering majors. Zhang et al. deal with the development and experimental research of multimedia cai courseware for hurdle running. Jena et al. focus on the thermo-mechanical characterization of rice husk filled carbon-reinforced hybrid polymer composites. Rice husk (RH) is a natural sheath that forms around rice grains during their growth. As a type of natural fiber obtained from agro-industrial waste, RH can be used as filler in composites materials in various polymer matrices. Wu addresses the asymmetric impact of inflation in financial development. This study analyzes the asymmetric effects of financial development on economic growth using a model augmented with inflation and asymmetries to inform model specification. The appropriate policies that favor low inflation and reduced expansion of feasibly reformed financial institutions, capital accumulation, and increased resource mobilization should be instituted if real growth is to positively happen. Ouyang et al., Priyadharshini et al., Krithika and Subramani, Gomathi et al., and Thangavel et al. deal with industrial development, such as the study on damage tests based on structure and operating parameters of wire ropes used by conveyors in orchards; development of intelligent smart metering system through remote monitoring and control under robust conditions; neural network-based drive cycle analysis for parallel hybrid electric vehicle; design fabrication and performance analysis of intelligent mesoscale capacitive accelerometer for vibration measurement; and dynamic modeling and control analysis of industrial electro-mechanical servo positioning system using machine learning technique. Pratheep et al. focuses on the genetic algorithm–based robust controller for an inverted pendulum using model order reduction. This paper considered proportional-integral optimized with a genetic algorithm controller on the inverted pendulum for the control of the angle position. The obtained results show that the GA-based PID controller confirms the enhanced performance indexes by holding minimum settling time and peak overshoot on comparing with the conventional PID controller. Tao et al. propose the existence of k-people stable alliance in n-player cooperative games. This paper considers the existence of a stable k-cooperative alliance with a nonempty core in an n-person cooperative game on the premise that the Nash negotiation solution is the distribution criterion. Also, this article provides sufficient conditions for the benefits of all players in a k-man alliance to lie in its internal sub alliance.","",""
0,"Jiaqi Gu","ICCAD: G: Light in Artificial Intelligence: Efficient Neurocomputing with Optical Neural Networks",2021,"","","","",152,"2022-07-13 09:21:09","","","","",,,,,0,0.00,0,1,1,"1 PROBLEM AND MOTIVATION Deep neural networks have received an explosion of interest for their superior performance in various intelligent tasks and high impacts on our lives. The computing capacity is in an arms race with the rapidly escalating model size and data amount for intelligent information processing. Practical application scenarios, e.g., autonomous vehicles, data centers, and edge devices, have strict energy efficiency, latency, and bandwidth constraints, raising a surging need to developmore efficient computing solutions. However, as Moore’s law is winding down, it becomes increasingly challenging for conventional electrical processors to support such massively parallel and energy-hungry artificial intelligence (AI) workloads. Limited clock frequency, millisecond-level latency, high heat density, and large energy consumption of CPUs, FPGAs, and GPUs motivate us to seek an alternative solution using silicon photonics. Silicon photonics is a promising hardware platform that could represent a paradigm shift in efficient AI acceleration with its CMOS-compatibility, intrinsic parallelism of optics, and near-zero power consumption. With potentially petaFLOPS per mm2 execution speed and attojoule/MAC computational efficiency, fully-optical neural networks (ONNs) demonstrate orders-of-magnitude higher performance than their electrical counterparts [1–6]. However, previous ONN designs have a large footprint and noise robustness issues, which prevent practical applications of photonic accelerators. In this work, we propose to explore efficient neuromorphic computing solutions with optical neural networks. Various photonic integrated circuit designs and software-hardware co-optimization methods are explored and presented here to enable high-performance photonic accelerators with lower area cost, better energy efficiency, higher variation-robustness, and more on-device learnability.","",""
0,"Abdulraqeb Alhammadi, Ayman A. El-Saleh, Ibraheem Shayea","MOS Prediction for Mobile Broadband Networks Using Bayesian Artificial Intelligence",2021,"","","","",153,"2022-07-13 09:21:09","","10.1109/ICAICST53116.2021.9497834","","",,,,,0,0.00,0,3,1,"Mobile broadband (MBB) networks are growing fast with supporting high-speed internet access. Fifth-generation networks promise an enhanced MBB that offers a high-speed data rate and video streaming with ultra-low latency. Thus, monitoring the level quality of these services supported by network providers becomes essential. Mobile network operators continuously optimize their network performance to provide a better quality of service and quality of experience. Moreover, artificial intelligence has been used considerably in optimizations to efficiently meet the requirements of future mobile networks. In this paper, we propose a Bayesian network model to predict the minimum opinion score (MOS), which contributes to evaluating the network performance of video streaming services. The proposed model depends on several input data, namely, bite rate, stalling load, and round-trip time. The predicted MOS depends on prior probability distributions to generate posterior probabilities. The predicted MOS depends on these input data. Results demonstrate that the proposed model achieves a high prediction accuracy of 86%, with a mean square error of 0.34. The proposed model also has a robust performance design through various testing methods.","",""
0,"S. Ledesma-Orozco, M. Ibarra-Manzano, D. Almanza-Ojeda, P. Fallavollita, J. Steffener","Artificial Intelligence to Analyze the Cortical Thickness Through Age",2021,"","","","",154,"2022-07-13 09:21:09","","10.3389/frai.2021.549255","","",,,,,0,0.00,0,5,1,"In this study, Artificial Intelligence was used to analyze a dataset containing the cortical thickness from 1,100 healthy individuals. This dataset had the cortical thickness from 31 regions in the left hemisphere of the brain as well as from 31 regions in the right hemisphere. Then, 62 artificial neural networks were trained and validated to estimate the number of neurons in the hidden layer. These neural networks were used to create a model for the cortical thickness through age for each region in the brain. Using the artificial neural networks and kernels with seven points, numerical differentiation was used to compute the derivative of the cortical thickness with respect to age. The derivative was computed to estimate the cortical thickness speed. Finally, color bands were created for each region in the brain to identify a positive derivative, that is, a part of life with an increase in cortical thickness. Likewise, the color bands were used to identify a negative derivative, that is, a lifetime period with a cortical thickness reduction. Regions of the brain with similar derivatives were organized and displayed in clusters. Computer simulations showed that some regions exhibit abrupt changes in cortical thickness at specific periods of life. The simulations also illustrated that some regions in the left hemisphere do not follow the pattern of the same region in the right hemisphere. Finally, it was concluded that each region in the brain must be dynamically modeled. One advantage of using artificial neural networks is that they can learn and model non-linear and complex relationships. Also, artificial neural networks are immune to noise in the samples and can handle unseen data. That is, the models based on artificial neural networks can predict the behavior of samples that were not used for training. Furthermore, several studies have shown that artificial neural networks are capable of deriving information from imprecise data. Because of these advantages, the results obtained in this study by the artificial neural networks provide valuable information to analyze and model the cortical thickness.","",""
28,"H. Alami, L. Rivard, P. Lehoux, S. Hoffman, Stephanie B. M. Cadeddu, Mathilde Savoldelli, M. A. Samri, M. A. Ag Ahmed, R. Fleet, J. Fortin","Artificial intelligence in health care: laying the Foundation for Responsible, sustainable, and inclusive innovation in low- and middle-income countries",2020,"","","","",155,"2022-07-13 09:21:09","","10.1186/s12992-020-00584-1","","",,,,,28,14.00,3,10,2,"","",""
28,"Sathian Dananjayan, G. M. Raj","Artificial Intelligence during a pandemic: The COVID‐19 example",2020,"","","","",156,"2022-07-13 09:21:09","","10.1002/hpm.2987","","",,,,,28,14.00,14,2,2,"Artificial intelligence (AI) is transforming our lifestyle intending to mimic human intelligence by a computer/machine in solving various issues. Initially, AI was designed to overcome simpler problems like winning a chess game, language recognition, image retrieval, among others. With the technological advancements, AI is getting increasingly sophisticated at doing what humans do, but more efficiently, rapidly, and at a lower cost in solving complex problems. AI in healthcare provides an upper hand undoubtedly over traditional analytics and clinical decision-making techniques. Machine learning (ML) algorithms, a subset of AI, can detect patterns from huge complex datasets to become more precise and accurate as they interact with training data, allowing humans to gain unprecedented insights into early detection of diseases, drug discovery, diagnostics, healthcare processes, treatment variability, and patient outcomes. But how effective are the AI algorithms during a disease outbreak or for that matter a pandemic? After 2000, the pandemics are testing the AI's ability to handle extreme events. The two major factors affecting AI algorithms include the availability of historical and real-time data and high computational power. The different roles played by AI during pandemics are early warning and alerts, prediction and detection of outbreak of diseases, real-time disease monitoring worldwide, analysis and visualisation of spreading trends, prediction of infection rate and infection trend, rapid decision-making to identify the effective treatments, study and analysis of the pathogens, and drug discovery. All these are executed at a greater speed with AI. WHO and CDC (United States) are receiving data of several diseases and situations occurring across the world. With modern computer architecture and internet, all these data can be accessed in real-time by different institutes to develop an autonomous or collaborative AI model to handle various tasks. In addition to the official data, AI can gather information from news outlets, forums, healthcare reports, travel data, social media posts, and others in multiple languages across the world by using natural language processing (NLP) techniques and flag their priority. Several terabytes of data which includes patients' case history, geographical events, and social media posts about a new pneumonia are processed at a rapid rate with high-performance computing to predict the possible outbreak of a pandemic. Most importantly unsupervised ML can identify its own pattern from the noise (historical and real-time data) rather than the training it on a preselected dataset, thus giving a wider possibility and new behaviour. An AI model trained to predict a particular disease can be retrained on the new data of a new or different disease. Some noticeable examples of AI that are used to battle the COVID-19 pandemic and others are as follows:","",""
25,"D. Schiff","Out of the laboratory and into the classroom: the future of artificial intelligence in education",2020,"","","","",157,"2022-07-13 09:21:09","","10.1007/s00146-020-01033-8","","",,,,,25,12.50,25,1,2,"","",""
27,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases",2020,"","","","",158,"2022-07-13 09:21:09","","10.1038/s41746-020-0229-3","","",,,,,27,13.50,5,6,2,"","",""
24,"P. Iftikhar, Marcela Kuijpers, Azadeh Khayyat, Aqsa Iftikhar, Maribel DeGouvia De Sa","Artificial Intelligence: A New Paradigm in Obstetrics and Gynecology Research and Clinical Practice",2020,"","","","",159,"2022-07-13 09:21:09","","10.7759/cureus.7124","","",,,,,24,12.00,5,5,2,"Artificial intelligence (AI) is growing exponentially in various fields, including medicine. This paper reviews the pertinent aspects of AI in obstetrics and gynecology (OB/GYN) and how these can be applied to improve patient outcomes and reduce the healthcare costs and workload for clinicians. Herein, we will address current AI uses in OB/GYN, and the use of AI as a tool to interpret fetal heart rate (FHR) and cardiotocography (CTG) to aid in the detection of preterm labor, pregnancy complications, and review discrepancies in its interpretation between clinicians to reduce maternal and infant morbidity and mortality. AI systems can be used as tools to create algorithms identifying asymptomatic women with short cervical length who are at risk of preterm birth. Additionally, the benefits of using the vast data capacity of AI storage can assist in determining the risk factors for preterm labor using multiomics and extensive genomic data. In the field of gynecological surgery, the use of augmented reality helps surgeons detect vital structures, thus decreasing complications, reducing operative time, and helping surgeons in training to practice in a realistic setting. Using three-dimensional (3D) printers can provide materials that mimic real tissues and also helps trainees to practice on a realistic model. Furthermore, 3D imaging allows better depth perception than its two-dimensional (2D) counterpart, allowing the surgeon to create preoperative plans according to tissue depth and dimensions. Although AI has some limitations, this new technology can improve the prognosis and management of patients, reduce healthcare costs, and help OB/GYN practitioners to reduce their workload and increase their efficiency and accuracy by incorporating AI systems into their daily practice. AI has the potential to guide practitioners in decision-making, reaching a diagnosis, and improving case management. It can reduce healthcare costs by decreasing medical errors and providing more dependable predictions. AI systems can accurately provide information on the large array of patients in clinical settings, although more robust data is required.","",""
0,"Xingming Sun, Zhaoqing Pan, E. Bertino","Artificial Intelligence and Security",2019,"","","","",160,"2022-07-13 09:21:09","","10.1007/978-3-030-24271-8","","",,,,,0,0.00,0,3,3,"","",""
0,"Yeunbae Kim, Jaehyuk Cha","Artificial Intelligence Technology and Social Problem Solving",2018,"","","","",161,"2022-07-13 09:21:09","","10.1007/978-981-13-6936-0_2","","",,,,,0,0.00,0,2,4,"","",""
96,"Eduardo H. B. Maia, L. Assis, Tiago Alves de Oliveira, Alisson Marques da Silva, A. Taranto","Structure-Based Virtual Screening: From Classical to Artificial Intelligence",2020,"","","","",162,"2022-07-13 09:21:09","","10.3389/fchem.2020.00343","","",,,,,96,48.00,19,5,2,"The drug development process is a major challenge in the pharmaceutical industry since it takes a substantial amount of time and money to move through all the phases of developing of a new drug. One extensively used method to minimize the cost and time for the drug development process is computer-aided drug design (CADD). CADD allows better focusing on experiments, which can reduce the time and cost involved in researching new drugs. In this context, structure-based virtual screening (SBVS) is robust and useful and is one of the most promising in silico techniques for drug design. SBVS attempts to predict the best interaction mode between two molecules to form a stable complex, and it uses scoring functions to estimate the force of non-covalent interactions between a ligand and molecular target. Thus, scoring functions are the main reason for the success or failure of SBVS software. Many software programs are used to perform SBVS, and since they use different algorithms, it is possible to obtain different results from different software using the same input. In the last decade, a new technique of SBVS called consensus virtual screening (CVS) has been used in some studies to increase the accuracy of SBVS and to reduce the false positives obtained in these experiments. An indispensable condition to be able to utilize SBVS is the availability of a 3D structure of the target protein. Some virtual databases, such as the Protein Data Bank, have been created to store the 3D structures of molecules. However, sometimes it is not possible to experimentally obtain the 3D structure. In this situation, the homology modeling methodology allows the prediction of the 3D structure of a protein from its amino acid sequence. This review presents an overview of the challenges involved in the use of CADD to perform SBVS, the areas where CADD tools support SBVS, a comparison between the most commonly used tools, and the techniques currently used in an attempt to reduce the time and cost in the drug development process. Finally, the final considerations demonstrate the importance of using SBVS in the drug development process.","",""
0,"","Neural Network Training Using Genetic Algorithms Series In Machine Perception And Artificial Intelligence",2021,"","","","",163,"2022-07-13 09:21:09","","","","",,,,,0,0.00,0,0,1,"Knowledge-Based Intelligent Information and Engineering Systems 2Nature-inspired Methods in Chemometrics: Genetic Algorithms and Artificial Neural NetworksParallel Implementations of Backpropagation Neural Networks on TransputersEvolutionary Algorithms and Neural NetworksTraining Neural Networks Using Hybrids with Genetic AlgorithmsNeural Network Training Using Genetic AlgorithmsGene Expression ProgrammingTraining a Neural Network with a Genetic AlgorithmMethods and Applications of Artificial IntelligenceClassification and Learning Using Genetic AlgorithmsPractical Computer Vision Applications Using Deep Learning with CNNsIntelligent Hybrid SystemsNeurogenetic LearningAdvances in Neural Networks ISNN 2007Hybrid Intelligent SystemsEncyclopedia of Computer Science and TechnologyMachine LearningUsing a Genetic Algorithm in Training an Artificial Neural Network to Implement the XOR FunctionGenetic and Evolutionary Computation — GECCO 2004Handbook of Fuzzy ComputationNeural Network Data Analysis Using SimulnetTMArtificial Neural Nets and Genetic AlgorithmsApplied Soft Computing Technologies: The Challenge of ComplexityGenetic Algorithm for Artificial Neural Network Training for the Purpose of Automated Part RecognitionNEURAL NETWORKS, FUZZY LOGIC AND GENETIC ALGORITHMAutomatic Generation of Neural Network Architecture Using Evolutionary ComputationPGANETThe Sixth International Symposium on Neural Networks (ISNN 2009)Evolutionary Machine Learning TechniquesModeling Decisions for Artificial IntelligenceEmpirical Studies on the Utility of Genetic Algorithms for Training and Designing of Neural NetworksTraining Neural Networks Using Genetic AlgorithmsTraining feedforward neural networks using genetic algorithmsMetaheuristic Procedures for Training Neural NetworksArtificial Neural Nets and Genetic AlgorithmsNature-Inspired Computing: Concepts, Methodologies, Tools, and ApplicationsApplications of Evolutionary ComputingArtificial Intelligence and CreativityArtificial Neural Nets and Genetic AlgorithmsDeep Learning Using Genetic Algorithms Creativity is one of the least understood aspects of intelligence and is often seen as `intuitive' and not susceptible to rational enquiry. Recently, however, there has been a resurgence of interest in the area, principally in artificial intelligence and cognitive science, but also in psychology, philosophy, computer science, logic, mathematics, sociology, and architecture and design. This volume brings this work together and provides an overview of this rapidly developing field. It addresses a range of issues. Can computers be creative? Can they help us to understand human creativity? How can artificial intelligence (AI) enhance human creativity? How, in particular, can it contribute to the `sciences of the artificial', such as design? Does the new wave of AI (connectionism, geneticism and artificial life) offer more promise in these areas than classical, symbol-handling AI? What would the implications be for AI and cognitive science if computers could not be creative? These issues are explored in five interrelated parts, each of which is introducted and explained by a leading figure in the field. Prologue (Margaret Boden) Part I: Foundational Issues (Terry Dartnall) Part II: Creativity and Cognition (Graeme S. Halford and Robert Levinson) Part III: Creativity and Connectionism (Chris Thornton) Part IV: Creativity and Design (John Gero) Part V: Human Creativity Enhancement (Ernest Edmonds) Epilogue (Douglas Hofstadter) For researchers in AI, cognitive science, computer science, philosophy, psychology, mathematics, logic, sociology, and architecture and design; and anyone interested in the rapidly growing field of artificial intelligence and creativity.From the contents: Neural networks – theory and applications: NNs (= neural networks) classifier on continuous data domains– quantum associative memory – a new class of neuron-like discrete filters to image processing – modular NNs for improving generalisation properties – presynaptic inhibition modelling for image processing application – NN recognition system for a curvature primal sketch – NN based nonlinear temporalspatial noise rejection system – relaxation rate for improving Hopfield network – Oja's NN and influence of the learning gain on its dynamics Genetic algorithms – theory and applications: transposition: a biological-inspired mechanism to use with GAs (= genetic algorithms) – GA for decision tree induction – optimising decision classifications using GAs – scheduling tasks with intertask communication onto multiprocessors by GAs – design of robust networks with GA – effect of degenerate coding on GAs – multiple traffic signal control using a GA – evolving musical harmonisation – niched-penalty approach for constraint handling in GAs – GA with dynamic population size – GA with dynamic niche clustering for multimodal function optimisation Soft computing and uncertainty: self-adaptation of evolutionary constructed decision trees by information spreading – evolutionary programming of near optimal NNsArtificial neural networks and genetic algorithms both are areas of research","",""
19,"Sandip K. Patel, Bhawana George, Vineeta Rai","Artificial Intelligence to Decode Cancer Mechanism: Beyond Patient Stratification for Precision Oncology",2020,"","","","",164,"2022-07-13 09:21:09","","10.3389/fphar.2020.01177","","",,,,,19,9.50,6,3,2,"The multitude of multi-omics data generated cost-effectively using advanced high-throughput technologies has imposed challenging domain for research in Artificial Intelligence (AI). Data curation poses a significant challenge as different parameters, instruments, and sample preparations approaches are employed for generating these big data sets. AI could reduce the fuzziness and randomness in data handling and build a platform for the data ecosystem, and thus serve as the primary choice for data mining and big data analysis to make informed decisions. However, AI implication remains intricate for researchers/clinicians lacking specific training in computational tools and informatics. Cancer is a major cause of death worldwide, accounting for an estimated 9.6 million deaths in 2018. Certain cancers, such as pancreatic and gastric cancers, are detected only after they have reached their advanced stages with frequent relapses. Cancer is one of the most complex diseases affecting a range of organs with diverse disease progression mechanisms and the effectors ranging from gene-epigenetics to a wide array of metabolites. Hence a comprehensive study, including genomics, epi-genomics, transcriptomics, proteomics, and metabolomics, along with the medical/mass-spectrometry imaging, patient clinical history, treatments provided, genetics, and disease endemicity, is essential. Cancer Moonshot℠ Research Initiatives by NIH National Cancer Institute aims to collect as much information as possible from different regions of the world and make a cancer data repository. AI could play an immense role in (a) analysis of complex and heterogeneous data sets (multi-omics and/or inter-omics), (b) data integration to provide a holistic disease molecular mechanism, (c) identification of diagnostic and prognostic markers, and (d) monitor patient’s response to drugs/treatments and recovery. AI enables precision disease management well beyond the prevalent disease stratification patterns, such as differential expression and supervised classification. This review highlights critical advances and challenges in omics data analysis, dealing with data variability from lab-to-lab, and data integration. We also describe methods used in data mining and AI methods to obtain robust results for precision medicine from “big” data. In the future, AI could be expanded to achieve ground-breaking progress in disease management.","",""
19,"E. I. Fernandez, André Satoshi Ferreira, M. Cecílio, D. S. Chéles, Rebeca Colauto Milanezi de Souza, M. Nogueira, J. C. Rocha","Artificial intelligence in the IVF laboratory: overview through the application of different types of algorithms for the classification of reproductive data",2020,"","","","",165,"2022-07-13 09:21:09","","10.1007/s10815-020-01881-9","","",,,,,19,9.50,3,7,2,"","",""
44,"Chiara Longoni, Luca Cian","Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect",2020,"","","","",166,"2022-07-13 09:21:09","","10.1177/0022242920957347","","",,,,,44,22.00,22,2,2,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).","",""
4,"P. Ballester, J. Carmona","Artificial intelligence for the next generation of precision oncology",2021,"","","","",167,"2022-07-13 09:21:09","","10.1038/s41698-021-00216-w","","",,,,,4,4.00,2,2,1,"","",""
18,"Ahmed Gowida, Salaheldin Elkatatny, Saad F. K. Al-Afnan, A. Abdulraheem","New Computational Artificial Intelligence Models for Generating Synthetic Formation Bulk Density Logs While Drilling",2020,"","","","",168,"2022-07-13 09:21:09","","10.3390/su12020686","","",,,,,18,9.00,5,4,2,"Synthetic well log generation using artificial intelligence tools is a robust solution for situations in which logging data are not available or are partially lost. Formation bulk density (RHOB) logging data greatly assist in identifying downhole formations. These data are measured in the field while drilling by using a density log tool in the form of either a logging while drilling (LWD) technique or (more often) by wireline logging after the formations are drilled. This is due to operational limitations during the drilling process. Therefore, the objective of this study was to develop a predictive tool for estimating RHOB while drilling using an adaptive network-based fuzzy interference system (ANFIS), functional network (FN), and support vector machine (SVM). The proposed model uses the mechanical drilling constraints as feeding input parameters, and the conventional RHOB log data as an output parameter. These mechanical drilling parameters are usually measured while drilling, and their responses vary with different formations. A dataset of 2400 actual datapoints, obtained from a horizontal well in the Middle East, were used to build the proposed models. The obtained dataset was divided into a 70/30 ratio for model training and testing, respectively. The optimized ANFIS-based model outperformed the FN- and SVM-based models with a correlation coefficient (R) of 0.93, and average absolute percentage error (AAPE) of 0.81% between the predicted and measured RHOB values. These results demonstrate the reliability of the developed ANFIS model for predicting RHOB while drilling, based on the mechanical drilling parameters. Subsequently, the ANFIS-based model was validated using unseen data from another well within the same field. The validation process yielded an AAPE of 0.97% between the predicted and actual RHOB values, which confirmed the robustness of the developed model as an effective predictive tool for RHOB.","",""
1,"L. Bori, M. Valera, D. Gilboa, R. Maor, I. Kottel, J. Remohi, D. Seidman, M. Meseguer","O-084 Computer vision can distinguish between euploid and aneuploid embryos. A novel artificial intelligence (AI) approach to measure cell division activity associated with chromosomal status",2021,"","","","",169,"2022-07-13 09:21:09","","10.1093/humrep/deab125.014","","",,,,,1,1.00,0,8,1,"      Can we distinguish between top-grade euploid and aneuploid embryos by AI measurement of cell edges in time-lapse videos?        Aneuploid embryos can be distinguished from euploid embryos by AI determination of a longer time to blastulation and higher cell activity.        Continuous monitoring of the embryo development has brought out morphokinetic parameters that are used to predict pre-implantation genetic testing (PGT) results. Previous publications showed that euploid embryos reach blastulation earlier than non-euploid embryos. However, time-lapse data are currently under-utilized in making predictions about embryo chromosomal content. AI and computer vision could take advantage of the massive amount of data embedded in the images of embryo development. This is the first attempt to distinguish between euploid and aneuploid embryos by computer vision in an objective and indirect way based on the measurement of cell edges as a proxy for cell activity.        We performed a retrospective analysis of 1,314 time-lapse videos from embryos cultured to the blastocyst stage with PGT results. This single-center study involved two phases; a comparison of the start time of blastulation between euploid (n = 544) and aneuploid embryos (n = 797). In phase two, we designed a novel methodology to examine whether precise measurement of cell edges over time could reflect cell activity differences in blastulation.        We assumed that the delay in blastulation is reflected by higher cell activity that could be determined accurately for the first time using computer vision and machine learning to measure the length of the edges (from t2 to t8). We compared computer vision based measurements of cell edges, reflecting cell number and size, in videos of 231 top-grade euploid (n = 111) and aneuploid (n = 120) embryos.        The mean and standard deviation of blastulation start time was 100.1±6.8 h for euploid embryos and 101.8±8.2 h for aneuploid embryos (p < 0.001). Regarding the measurement of cell activity, a computer vision algorithm identified the edges and provided a certainty score for each edge, higher when the algorithm is more certain that this is a cell edge (as opposed to noise in the images). A threshold was set to distinguish cell edges from noise using this score. The following results for top-grade embryos are shown as the sum of the edge lengths (µm) average of 160 pictures per embryo (frames between t2 and t8). The total length of the cell edges increased from two cells (420±85 µm) to eight cells (861±237 µm), in line with the mitosis events. Both the average total edge measured (450±162 µm for euploid embryos and 489±215 µm for aneuploid embryos, p < 0.01) and the average total of the difference between consecutive frames (135±47 µm for euploid embryos and 153±64 µm for aneuploid embryos, p < 0.01) were higher for aneuploid embryos than for euploid embryos. A regression model to differentiate between the two classes achieved 73% sensitivity and 73% specificity on this dataset.        The main limitation of this study is the difficulty to correlate our findings to other measure of cell activity. A more robust AI function (using not only cell edges lengths) would be required for future analysis to measure the cell activity in cell division up to the blastocyst stage.        Our results show for the first time that an AI based system can precisely measure microscopic cell edges in the dividing embryo. Using this novel method, we could distinguish between euploid and aneuploid embryos. This non-invasive method could further enhance our knowledge of the developing embryo.        Not Applicable ","",""
0,"Keeley A. Crockett, Edwin Colyer, A. Latham","The Ethical Landscape of Data and Artificial Intelligence: Citizen Perspectives",2021,"","","","",170,"2022-07-13 09:21:09","","10.1109/SSCI50451.2021.9660153","","",,,,,0,0.00,0,3,1,"Globally, there is growing acknowledgement that those involved in the development and deployment of AI products and services should act responsibly and conduct their work within robust ethical frameworks. Many of the ethical guidelines now published highlight a requirement for citizens to have greater voice and involvement in this process and to hold actors to account regarding compliance and the impacts of their AI innovations. For citizens to participate in co-creation activities they need to be representative of the diverse communities of society and have an appropriate level of understanding of basic AI concepts. This paper presents the preliminary results of a longitudinal survey designed to capture citizen perspectives of the ethical landscape of data and AI. Forty participants were asked to participate in a survey and results were analyzed based on gender, age range and educational attainment. Results have shown that participant perception of AI, trust, bias and fairness is different but related to specific AI applications, and the context in which is applied. Citizens also are also very receptive to undertaking free courses/workshops on a wide range of AI concepts, ranging from family workshops to work-based training.","",""
0,"E. Cambouropoulos, Maximos A. Kaliakatsos-Papakostas","Cognitive Musicology and Artificial Intelligence: Harmonic Analysis, Learning, and Generation",2021,"","","","",171,"2022-07-13 09:21:09","","10.1007/978-3-030-72116-9_10","","",,,,,0,0.00,0,2,1,"","",""
0,"Lucas Mendes Lima, Victor Calebe Cavalcante, Mariana Guimarães de Sousa, Cláudio Afonso Fleury, D. Oliveira, Eduardo Noronha de Andrade Freitas","Artificial Intelligence in Support of Welfare Monitoring of Dairy Cattle: A Systematic Literature Review",2021,"","","","",172,"2022-07-13 09:21:09","","10.1109/CSCI54926.2021.00324","","",,,,,0,0.00,0,6,1,"Context: Although agribusiness corresponded to more than 20% of Brazil’s Gross Domestic Product (GDP), most livestock is under manual control and manual monitoring. Additionally, alternative technologies are either uncomfortable and stressful, or expensive. Now, despite the great scientific advances in the area, there is still a pressing need for an automated robust, inexpensive and (sub)optimal technology to monitor animal behavior in a cost-effective, contact-less and stress-free fashion. Overall, this niche can leverage the benefits of Deep Learning schemes.Objective: This review aims to provide a systematic overview of most current projects in the area of comfort monitoring dairy cattle, as well as their corresponding image recognition-based techniques and technologies.Methods: First, a systematic review planning was carried out, and objectives, research questions, search strings, among others, were defined. Subsequently,a broad survey was conducted to extract, analyze and compile the data, to generate a easy-to-read visual source of information (tables and graphics).Results: Information was extracted from the reviewed papers. Among this data collected from the papers are techniques utilized, target behaviors, cow bodyparts identified in visual computational, besides their paper source font, the publication date, and localization. For example, the papers present are mostly recent. China has had a larger number of relevant papers in the area. The back was the body region most analyzed by the papers and the behaviors most analyzed were body condition score, lameness, cow’s body position and feeding/drinking behavior. Among the methods used is RCNN Inception V3 with the best accuracy for cow’s back region.Conclusion: The aim of this work is to present some of the papers that are being carried out in the area of dairy cow behavior monitoring, using techniques of Artifical Intelligence. It is expected that the information collected and presented in the present systematic review paper contribute to the future researches and projects of the area and the application of new techniques.","",""
14,"A. Burlacu, Adrian Iftene, Daniel Jugrin, I. Popa, Paula Madalina Lupu, C. Vlad, A. Covic","Using Artificial Intelligence Resources in Dialysis and Kidney Transplant Patients: A Literature Review",2020,"","","","",173,"2022-07-13 09:21:09","","10.1155/2020/9867872","","",,,,,14,7.00,2,7,2,"Background The purpose of this review is to depict current research and impact of artificial intelligence/machine learning (AI/ML) algorithms on dialysis and kidney transplantation. Published studies were presented from two points of view: What medical aspects were covered? What AI/ML algorithms have been used? Methods We searched four electronic databases or studies that used AI/ML in hemodialysis (HD), peritoneal dialysis (PD), and kidney transplantation (KT). Sixty-nine studies were split into three categories: AI/ML and HD, PD, and KT, respectively. We identified 43 trials in the first group, 8 in the second, and 18 in the third. Then, studies were classified according to the type of algorithm. Results AI and HD trials covered: (a) dialysis service management, (b) dialysis procedure, (c) anemia management, (d) hormonal/dietary issues, and (e) arteriovenous fistula assessment. PD studies were divided into (a) peritoneal technique issues, (b) infections, and (c) cardiovascular event prediction. AI in transplantation studies were allocated into (a) management systems (ML used as pretransplant organ-matching tools), (b) predicting graft rejection, (c) tacrolimus therapy modulation, and (d) dietary issues. Conclusions Although guidelines are reluctant to recommend AI implementation in daily practice, there is plenty of evidence that AI/ML algorithms can predict better than nephrologists: volumes, Kt/V, and hypotension or cardiovascular events during dialysis. Altogether, these trials report a robust impact of AI/ML on quality of life and survival in G5D/T patients. In the coming years, one would probably witness the emergence of AI/ML devices that facilitate the management of dialysis patients, thus increasing the quality of life and survival.","",""
81,"Thomas G. Dietterich","Steps Toward Robust Artificial Intelligence",2017,"","","","",174,"2022-07-13 09:21:09","","10.1609/aimag.v38i3.2756","","",,,,,81,16.20,81,1,5,"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world","",""
11,"K. Mudgal, Neelanjan Das","The ethical adoption of artificial intelligence in radiology",2019,"","","","",175,"2022-07-13 09:21:09","","10.1259/bjro.20190020","","",,,,,11,3.67,6,2,3,"Artificial intelligence (AI) is rapidly transforming healthcare—with radiology at the pioneering forefront. To be trustfully adopted, AI needs to be lawful, ethical and robust. This article covers the different aspects of a safe and sustainable deployment of AI in radiology during: training, integration and regulation. For training, data must be appropriately valued, and deals with AI companies must be centralized. Companies must clearly define anonymization and consent, and patients must be well-informed about their data usage. Data fed into algorithms must be made AI-ready by refining, purification, digitization and centralization. Finally, data must represent various demographics. AI needs to be safely integrated with radiologists-in-the-loop: guiding forming concepts of AI solutions and supervising training and feedback. To be well-regulated, AI systems must be approved by a health authority and agreements must be made upon liability for errors, roles of supervised and unsupervised AI and fair workforce distribution (between AI and radiologists), with a renewal of policy at regular intervals. Any errors made must have a root-cause analysis, with outcomes fedback to companies to close the loop—thus enabling a dynamic best prediction system. In the distant future, AI may act autonomously with little human supervision. Ethical training and integration can ensure a ""transparent"" technology that will allow insight: helping us reflect on our current understanding of imaging interpretation and fill knowledge gaps, eventually moulding radiological practice. This article proposes recommendations for ethical practise that can guide a nationalized framework to build a sustainable and transparent system.","",""
8,"I. Wiafe, F. N. Koranteng, Emmanuel Nyarko Obeng, Nana Assyne, Abigail Wiafe, S. Gulliver","Artificial Intelligence for Cybersecurity: A Systematic Mapping of Literature",2020,"","","","",176,"2022-07-13 09:21:09","","10.1109/ACCESS.2020.3013145","","",,,,,8,4.00,1,6,2,"Due to the ever-increasing complexities in cybercrimes, there is the need for cybersecurity methods to be more robust and intelligent. This will make defense mechanisms to be capable of making real-time decisions that can effectively respond to sophisticated attacks. To support this, both researchers and practitioners need to be familiar with current methods of ensuring cybersecurity (CyberSec). In particular, the use of artificial intelligence for combating cybercrimes. However, there is lack of summaries on artificial intelligent methods for combating cybercrimes. To address this knowledge gap, this study sampled 131 articles from two main scholarly databases (ACM digital library and IEEE Xplore). Using a systematic mapping, the articles were analyzed using quantitative and qualitative methods. It was observed that artificial intelligent methods have made remarkable contributions to combating cybercrimes with significant improvement in intrusion detection systems. It was also observed that there is a reduction in computational complexity, model training times and false alarms. However, there is a significant skewness within the domain. Most studies have focused on intrusion detection and prevention systems, and the most dominant technique used was support vector machines. The findings also revealed that majority of the studies were published in two journal outlets. It is therefore suggested that to enhance research in artificial intelligence for CyberSec, researchers need to adopt newer techniques and also publish in other related outlets.","",""
8,"Jun Zhu, Hang Su, Bo Zhang","Toward the third generation of artificial intelligence",2020,"","","","",177,"2022-07-13 09:21:09","","10.1360/ssi-2020-0204","","",,,,,8,4.00,3,3,2,"There have been two competing paradigms of artificial intelligence (AI) development since 1956, i.e., symbolism and connectionism (or subsymbolism). Both started at the same time, but symbolism had dominated AI development until the end of the 1980s. Connectionism began to develop in the 1990s and reached its climax at the beginning of this century, and it is likely to displace symbolism. Today, it seems that the two paradigms only simulate the human mind (or brain) in different ways and have their own advantages. True human intelligence cannot be achieved by relying on only one paradigm. Both are necessary to establish a new, explainable, and robust AI theory and method and develop safe, trustworthy, reliable, and extensible AI technology. To this end, it is imperative to combine the two paradigms, and the present article will illustrate this idea. For the sake of description, symbolism, connectionism, and the newly developed paradigm are termed as first-, second-, and third-generation AIs.","",""
5,"Cathy O'Neil, H. Gunn","Near-Term Artificial Intelligence and the Ethical Matrix",2020,"","","","",178,"2022-07-13 09:21:09","","10.1093/oso/9780190905033.003.0009","","",,,,,5,2.50,3,2,2,"This chapter takes up the issue of near-term artificial intelligence, or the algorithms that are already in place in a variety of public and private sectors, guiding decisions from advertising and to credit ratings to sentencing in the justice system. There is a pressing need to recognize and evaluate the ways that structural racism, sexism, classism, and ableism may be embedded in and amplified by these systems. The chapter proposes a framework for ethical analysis that can be used to facilitate more robust ethical reflection in AI development and implementation. It presents an ethical matrix that incorporates the language of data science as a tool that data scientists can build themselves in order to integrate ethical analysis into the design process, addressing the need for immediate analysis and accountability over the design and deployment of near-term AI.","",""
0,"Katanosh Morovat, B. Panda","A Survey of Artificial Intelligence in Cybersecurity",2020,"","","","",179,"2022-07-13 09:21:09","","10.1109/CSCI51800.2020.00026","","",,,,,0,0.00,0,2,2,"During the last decades, not only the number of cyberattacks have increased significantly, they have also become more sophisticated. Hence designing a cyber-resilient approach is of paramount importance. Traditional security methods are not adequate to prevent data breaches in case of cyberattacks. Cybercriminals have learned how to use new techniques and robust tools to hack, attack, and breach data. Fortunately, Artificial Intelligence (AI) technologies have been introduced into cyberspace to construct smart models for defending systems from attacks. Since AI technologies can rapidly evolve to address complex situations, they can be used as fundamental tools in the field of cybersecurity. Al-based techniques can provide efficient and powerful cyber defense tools to recognize malware attacks, network intrusions, phishing and spam emails, and data breaches, to name a few, and to alert security incidents when they occur. In this paper, we review the impact of AI in cybersecurity and summarize existing research in terms of benefits of AI in cybersecurity.","",""
0,"M. Lim, P. Mousavi, J. Sirovljevic, Hui-Jae You","Onboard Artificial Intelligence for Space Situational Awareness with Low-Power GPUs",2020,"","","","",180,"2022-07-13 09:21:09","","","","",,,,,0,0.00,0,4,2,"Onboard processing provides the reduction in latency that is critical to Space Situational Awareness (SSA) applications. By receiving data directly from the sensor and processing it in real-time on board of spacecraft, this technology enables real-time processing and response. Currently, onboard processing is a nascent technology and the capabilities that exist are limited: they are highly-customized, one-off systems typically built for large spacecrafts. This is poised to change drastically in the coming decade, as off-the-shelf computing hardware and algorithms mature while spacecraft operations turn towards more scalable small-sat missions. Within that time period, onboard processing will transform from a research topic to an essential element of most space missions. One of the key functionalities for onboard processing in SSA domain is object classification. Current state of the art classification algorithms are based on Artificial Intelligence (AI) technologies. Due to prohibitively high computational needs for AI applications, their deployment onboard a spacecraft has not been possible to date. However, the rapid advance in AI-oriented computing hardware, especially Graphics Processing Units (GPU), has opened the door to AI in space. In particular, low size, weight, and power (SWaP) GPU devices have been developed that would be ideal for space-based processing. MDA is currently investigating use of state of the art off-the-shelf low-power GPUs for deployment of AI applications essential for real-time object identification as part of the SSA domain. This work is motivated by recent increased focus within the AI community on operationalizing AI methodology. In this paper, we discuss the motivation behind the research, technical details of the implementation and current results. More specifically, three simple neural networks of different sizes trained on MDA Sapphire dataset for space object classification are presented. The classification performance of these algorithms are benchmarked demonstrating that larger models tend to be more robust to added noise in the input image. Effects of computing optimization techniques applied on these models are also presented, which generally show great improvement by several factors in throughput and power efficiency of these algorithms onboard the hardware platform hosting these technologies.","",""
0,"S. Cuddy","THE BENEFITS AND DANGERS OF USING ARTIFICIAL INTELLIGENCE IN PETROPHYSICS",2020,"","","","",181,"2022-07-13 09:21:09","","10.30632/spwla-5066","","",,,,,0,0.00,0,1,2,"Abstract Artificial Intelligence, or AI, is a method of data analysis that learns from data, identify patterns and makes predictions with the minimal human intervention. AI is bringing many benefits to petrophysical evaluation. Using case studies, this paper describes several successful applications. The future of AI has even more potential. However, if used carelessly there are potentially grave consequences. A complex Middle East Carbonate field needed a bespoke shaly water saturation equation. AI was used to ‘evolve’ an ideal equation, together with field specific saturation and cementation exponents. One UKCS gas field had an ‘oil problem’. Here, AI was used to unlock the hidden fluid information in the NMR T1 and T2 spectra and successfully differentiate oil and gas zones in real time. A North Sea field with 30 wells had shear velocity data (Vs) in only 4 wells. Vs was required for reservoir modelling and well bore stability prediction. AI was used to predict Vs in all 30 wells. Incorporating high vertical resolution data, the Vs predictions were even better than the recorded logs. As it is not economic to take core data on every well, AI is used to discover the relationships between logs, core, litho-facies and permeability in multi-dimensional data space. As a consequence, all wells in a field were populated with these data to build a robust reservoir model. In addition, the AI predicted data upscaled correctly unlike many conventional techniques. AI gives impressive results when automatically log quality controlling (LQC) and repairing electrical logs for bad hole and sections of missing data. AI doesn’t require prior knowledge of the petrophysical response equations and is self-calibrating. There are no parameters to pick or cross-plots to make. There is very little user intervention and AI avoids the problem of ‘garbage in, garbage out’ (GIGO), by ignoring noise and outliers. AI programs work with an unlimited number of electrical logs, core and gas chromatography data; and don’t ‘fall-over’ if some of those inputs are missing. AI programs currently being developed include ones where their machine code evolves using similar rules used by life’s DNA code. These AI programs pose considerable dangers far beyond the oil industry as described in this paper. A ‘risk assessment’ is essential on all AI programs so that all hazards and risk factors, that could cause harm, are identified and mitigated.","",""
496,"Christopher J. Kelly, A. Karthikesalingam, Mustafa Suleyman, Greg Corrado, Dominic King","Key challenges for delivering clinical impact with artificial intelligence",2019,"","","","",182,"2022-07-13 09:21:09","","10.1186/s12916-019-1426-2","","",,,,,496,165.33,99,5,3,"","",""
199,"Dong Wook Kim, H. Jang, K. Kim, Youngbin Shin, S. Park","Design Characteristics of Studies Reporting the Performance of Artificial Intelligence Algorithms for Diagnostic Analysis of Medical Images: Results from Recently Published Papers",2019,"","","","",183,"2022-07-13 09:21:09","","10.3348/kjr.2019.0025","","",,,,,199,66.33,40,5,3,"Objective To evaluate the design characteristics of studies that evaluated the performance of artificial intelligence (AI) algorithms for the diagnostic analysis of medical images. Materials and Methods PubMed MEDLINE and Embase databases were searched to identify original research articles published between January 1, 2018 and August 17, 2018 that investigated the performance of AI algorithms that analyze medical images to provide diagnostic decisions. Eligible articles were evaluated to determine 1) whether the study used external validation rather than internal validation, and in case of external validation, whether the data for validation were collected, 2) with diagnostic cohort design instead of diagnostic case-control design, 3) from multiple institutions, and 4) in a prospective manner. These are fundamental methodologic features recommended for clinical validation of AI performance in real-world practice. The studies that fulfilled the above criteria were identified. We classified the publishing journals into medical vs. non-medical journal groups. Then, the results were compared between medical and non-medical journals. Results Of 516 eligible published studies, only 6% (31 studies) performed external validation. None of the 31 studies adopted all three design features: diagnostic cohort design, the inclusion of multiple institutions, and prospective data collection for external validation. No significant difference was found between medical and non-medical journals. Conclusion Nearly all of the studies published in the study period that evaluated the performance of AI algorithms for diagnostic analysis of medical images were designed as proof-of-concept technical feasibility studies and did not have the design features that are recommended for robust validation of the real-world clinical performance of AI algorithms.","",""
66,"Keping Yu, Zhiwei Guo, Yulian Shen, Wei Wang, Jerry Chun‐wei Lin, Takuro Sato","Secure Artificial Intelligence of Things for Implicit Group Recommendations",2021,"","","","",184,"2022-07-13 09:21:09","","10.1109/JIOT.2021.3079574","","",,,,,66,66.00,11,6,1,"The emergence of Artificial Intelligence of Things (AIoT) has provided novel insights for many social computing applications, such as group recommender systems. As the distances between people have been greatly shortened, there has been more general demand for the provision of personalized services aimed at groups instead of individuals. The existing methods for capturing group-level preference features from individuals have mostly been established via aggregation and face two challenges: 1) secure data management workflows are absent and 2) implicit preference feedback is ignored. To tackle these current difficulties, this article proposes secure AIoT for implicit group recommendations (SAIoT-GRs). For the hardware module, a secure Internet of Things structure is developed as the bottom support platform. For the software module, a collaborative Bayesian network model and noncooperative game are introduced as algorithms. This secure AIoT architecture is able to maximize the advantages of the two modules. In addition, a large number of experiments are carried out to evaluate the performance of SAIoT-GR in terms of efficiency and robustness.","",""
10,"M. Alomar, M. Hameed, N. Al‐Ansari, M. Alsaadi","Data-Driven Model for the Prediction of Total Dissolved Gas: Robust Artificial Intelligence Approach",2020,"","","","",185,"2022-07-13 09:21:09","","10.1155/2020/6618842","","",,,,,10,5.00,3,4,2,"Saturated total dissolved gas (TDG) is recently considered as a serious issue in the environmental engineering field since it stands behind the reasons for increasing the mortality rates of fish and aquatic organisms. The accurate and more reliable prediction of TDG has a very significant role in preserving the diversity of aquatic organisms and reducing the phenomenon of fish deaths. Herein, two machine learning approaches called support vector regression (SVR) and extreme learning machine (ELM) have been applied to predict the saturated TDG% at USGS 14150000 and USGS 14181500 stations which are located in the USA. For the USGS 14150000 station, the recorded samples from 13 October 2016 to 14 March 2019 (75%) were used for training set, and the rest from 15 March 2019 to 13 October 2019 (25%) were used for testing requirements. Similarly, for USGS 14181500 station, the hourly data samples which covered the period from 9 June 2017 till 11 March 2019 were used for calibrating the models and from 12 March 2019 until 9 October 2019 were used for testing the predictive models. Eight input combinations based on different parameters have been established as well as nine statistical performance measures have been used for evaluating the accuracy of adopted models, for instance, not limited, correlation of determination (        R      2        ), mean absolute relative error (MAE), and uncertainty at 95% (        U      95        ). The obtained results of the study for both stations revealed that the ELM managed efficiently to estimate the TDG in comparison to SVR technique. For USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.986 (0.986), MAE of 0.316 (0.441), and         U      95        of 3.592 (3.869). Lastly, for USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.991 (0.991), MAE of 0.338 (0.396), and         U      95        of 0.832 (0.837). In addition, ELM’s training process computational time is stated to be much shorter than that of SVM. The results also showed that the temperature parameter was the most significant variable that influenced TDG relative to the other parameters. Overall, the proposed model (ELM) proved to be an appropriate and efficient computer-assisted technology for saturated TDG modeling that will contribute to the basic knowledge of environmental considerations.","",""
132,"Y. Yang, C. S. Bang","Application of artificial intelligence in gastroenterology",2019,"","","","",186,"2022-07-13 09:21:09","","10.3748/wjg.v25.i14.1666","","",,,,,132,44.00,66,2,3,"Artificial intelligence (AI) using deep-learning (DL) has emerged as a breakthrough computer technology. By the era of big data, the accumulation of an enormous number of digital images and medical records drove the need for the utilization of AI to efficiently deal with these data, which have become fundamental resources for a machine to learn by itself. Among several DL models, the convolutional neural network showed outstanding performance in image analysis. In the field of gastroenterology, physicians handle large amounts of clinical data and various kinds of image devices such as endoscopy and ultrasound. AI has been applied in gastroenterology in terms of diagnosis, prognosis, and image analysis. However, potential inherent selection bias cannot be excluded in the form of retrospective study. Because overfitting and spectrum bias (class imbalance) have the possibility of overestimating the accuracy, external validation using unused datasets for model development, collected in a way that minimizes the spectrum bias, is mandatory. For robust verification, prospective studies with adequate inclusion/exclusion criteria, which represent the target populations, are needed. DL has its own lack of interpretability. Because interpretability is important in that it can provide safety measures, help to detect bias, and create social acceptance, further investigations should be performed.","",""
99,"R. Colling, Helen Pitman, K. Oien, N. Rajpoot, P. Macklin, D. Snead, Tony Sackville, C. Verrill","Artificial intelligence in digital pathology: a roadmap to routine use in clinical practice",2019,"","","","",187,"2022-07-13 09:21:09","","10.1002/path.5310","","",,,,,99,33.00,12,8,3,"The use of artificial intelligence will transform clinical practice over the next decade and the early impact of this will likely be the integration of image analysis and machine learning into routine histopathology. In the UK and around the world, a digital revolution is transforming the reporting practice of diagnostic histopathology and this has sparked a proliferation of image analysis software tools. While this is an exciting development that could discover novel predictive clinical information and potentially address international pathology workforce shortages, there is a clear need for a robust and evidence‐based framework in which to develop these new tools in a collaborative manner that meets regulatory approval. With these issues in mind, the NCRI Cellular Molecular Pathology (CM‐Path) initiative and the British In Vitro Diagnostics Association (BIVDA) have set out a roadmap to help academia, industry, and clinicians develop new software tools to the point of approved clinical use. © 2019 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.","",""
73,"Valentina Bellemo, Gilbert Lim, T. Rim, G. Tan, C. Cheung, S. Sadda, M. He, A. Tufail, M. Lee, W. Hsu, D. Ting","Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application",2019,"","","","",188,"2022-07-13 09:21:09","","10.1007/s11892-019-1189-3","","",,,,,73,24.33,7,11,3,"","",""
85,"A. Grzybowski, Piotr Brona, Gilbert Lim, P. Ruamviboonsuk, G. Tan, M. Abràmoff, D. Ting","Artificial intelligence for diabetic retinopathy screening: a review",2019,"","","","",189,"2022-07-13 09:21:09","","10.1038/s41433-019-0566-0","","",,,,,85,28.33,12,7,3,"","",""
67,"Yonghui Shang, Hoang Nguyen, X. Bui, Quang-Hieu Tran, H. Moayedi","A Novel Artificial Intelligence Approach to Predict Blast-Induced Ground Vibration in Open-Pit Mines Based on the Firefly Algorithm and Artificial Neural Network",2019,"","","","",190,"2022-07-13 09:21:09","","10.1007/s11053-019-09503-7","","",,,,,67,22.33,13,5,3,"","",""
51,"Lu Minh Le, H. Ly, B. Pham, Vuong Minh Le, T. Pham, Duy-Hung Nguyen, Xuan-Tuan Tran, Tien-Thinh Le","Hybrid Artificial Intelligence Approaches for Predicting Buckling Damage of Steel Columns Under Axial Compression",2019,"","","","",191,"2022-07-13 09:21:09","","10.3390/ma12101670","","",,,,,51,17.00,6,8,3,"This study aims to investigate the prediction of critical buckling load of steel columns using two hybrid Artificial Intelligence (AI) models such as Adaptive Neuro-Fuzzy Inference System optimized by Genetic Algorithm (ANFIS-GA) and Adaptive Neuro-Fuzzy Inference System optimized by Particle Swarm Optimization (ANFIS-PSO). For this purpose, a total number of 57 experimental buckling tests of novel high strength steel Y-section columns were collected from the available literature to generate the dataset for training and validating the two proposed AI models. Quality assessment criteria such as coefficient of determination (R2), Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) were used to validate and evaluate the performance of the prediction models. Results showed that both ANFIS-GA and ANFIS-PSO had a strong ability in predicting the buckling load of steel columns, but ANFIS-PSO (R2 = 0.929, RMSE = 60.522 and MAE = 44.044) was slightly better than ANFIS-GA (R2 = 0.916, RMSE = 65.371 and MAE = 48.588). The two models were also robust even with the presence of input variability, as investigated via Monte Carlo simulations. This study showed that the hybrid AI techniques could help constructing an efficient numerical tool for buckling analysis.","",""
51,"Xiaohang Wu, Yelin Huang, Zhenzhen Liu, Weiyi Lai, Erping Long, Kai Zhang, Jiewei Jiang, Duoru Lin, Kexin Chen, Tongyong Yu, Dongxuan Wu, Cong Li, Yanyi Chen, Minjie Zou, Chuan Chen, Yi Zhu, Chong Guo, Xiayin Zhang, Ruixin Wang, Yahan Yang, Yifan Xiang, Lijian Chen, Congxin Liu, J. Xiong, Z. Ge, Ding-ding Wang, Guihua Xu, Shao-lin Du, Chi Xiao, Jianghao Wu, Ke Zhu, Dan-yao Nie, Fan Xu, Jian Lv, Weirong Chen, Yizhi Liu, Haotian Lin","Universal artificial intelligence platform for collaborative management of cataracts",2019,"","","","",192,"2022-07-13 09:21:09","","10.1136/bjophthalmol-2019-314729","","",,,,,51,17.00,5,37,3,"Purpose To establish and validate a universal artificial intelligence (AI) platform for collaborative management of cataracts involving multilevel clinical scenarios and explored an AI-based medical referral pattern to improve collaborative efficiency and resource coverage. Methods The training and validation datasets were derived from the Chinese Medical Alliance for Artificial Intelligence, covering multilevel healthcare facilities and capture modes. The datasets were labelled using a three-step strategy: (1) capture mode recognition; (2) cataract diagnosis as a normal lens, cataract or a postoperative eye and (3) detection of referable cataracts with respect to aetiology and severity. Moreover, we integrated the cataract AI agent with a real-world multilevel referral pattern involving self-monitoring at home, primary healthcare and specialised hospital services. Results The universal AI platform and multilevel collaborative pattern showed robust diagnostic performance in three-step tasks: (1) capture mode recognition (area under the curve (AUC) 99.28%–99.71%), (2) cataract diagnosis (normal lens, cataract or postoperative eye with AUCs of 99.82%, 99.96% and 99.93% for mydriatic-slit lamp mode and AUCs >99% for other capture modes) and (3) detection of referable cataracts (AUCs >91% in all tests). In the real-world tertiary referral pattern, the agent suggested 30.3% of people be ‘referred’, substantially increasing the ophthalmologist-to-population service ratio by 10.2-fold compared with the traditional pattern. Conclusions The universal AI platform and multilevel collaborative pattern showed robust diagnostic performance and effective service for cataracts. The context of our AI-based medical referral pattern will be extended to other common disease conditions and resource-intensive situations.","",""
47,"Chengjie Zheng, T. V. Johnson, Aakriti Garg, Michael V. Boland","Artificial intelligence in glaucoma",2019,"","","","",193,"2022-07-13 09:21:09","","10.1097/ICU.0000000000000552","","",,,,,47,15.67,12,4,3,"Purpose of review The use of computers has become increasingly relevant to medical decision-making, and artificial intelligence methods have recently demonstrated significant advances in medicine. We therefore provide an overview of current artificial intelligence methods and their applications, to help the practicing ophthalmologist understand their potential impact on glaucoma care. Recent findings Techniques used in artificial intelligence can successfully analyze and categorize data from visual fields, optic nerve structure [e.g., optical coherence tomography (OCT) and fundus photography], ocular biomechanical properties, and a combination thereof to identify disease severity, determine disease progression, and/or recommend referral for specialized care. Algorithms have become increasingly complex in recent years, utilizing both supervised and unsupervised methods of artificial intelligence. Impressive performance of these algorithms on previously unseen data has been reported, often outperforming standard global indices and expert observers. However, there remains no clearly defined gold standard for determining the presence and severity of glaucoma, which undermines the training of these algorithms. To improve upon existing methodologies, future work must employ more robust definitions of disease, optimize data inputs for artificial intelligence analysis, and improve methods of extracting knowledge from learned results. Summary Artificial intelligence has the potential to revolutionize the screening, diagnosis, and classification of glaucoma, both through the automated processing of large data sets, and by earlier detection of new disease patterns. In addition, artificial intelligence holds promise for fundamentally changing research aimed at understanding the development, progression, and treatment of glaucoma, by identifying novel risk factors and by evaluating the importance of existing ones.","",""
41,"C. Macrae","Governing the safety of artificial intelligence in healthcare",2019,"","","","",194,"2022-07-13 09:21:09","","10.1136/bmjqs-2019-009484","","",,,,,41,13.67,41,1,3,"Artificial intelligence (AI) has enormous potential to improve the safety of healthcare, from increasing diagnostic accuracy,1 to optimising treatment planning,2 to forecasting outcomes of care.3 However, integrating AI technologies into the delivery of healthcare is likely to introduce a range of new risks and amplify existing ones. For instance, failures in widely used software have the potential to quickly affect large numbers of patients4; hidden assumptions in underlying data and models can lead to AI systems delivering dangerous recommendations that are insensitive to local care processes,5 6 and opaque AI techniques such as deep learning can make explaining and learning from failure extremely difficult.7 8 To maximise the benefits of AI in healthcare and to build trust among patients and practitioners, it will therefore be essential to robustly govern the risks that AI poses to patient safety.  In a recent review in this journal, Challen and colleagues present an important and timely analysis of some of the key technological risks associated with the application of machine learning in clinical settings.9 Machine learning is a subfield of AI that focuses on the development of algorithms that are automatically derived and optimised through exposure to large quantities of exemplar ‘training’ data.10 The outputs of machine learning algorithms are essentially classifications of patterns that provide some sort of prediction—for instance, predicting whether an image shows a malignant melanoma or a benign mole.11 Some of the basic techniques of machine learning have existed for half a century or more, but progress in the field has accelerated rapidly due to advances in the development of ‘deep’ artificial neural networks12 combined with huge increases in computational power and the availability of enormous quantities of data. These techniques have underpinned recent public demonstrations of AI systems …","",""
0,"F. Centracchio, L. Burghignoli, Giorgio Palma, Ilaria Cioffi, U. Iemma","Noise shielding surrogate models using dynamic artificial neural networks",2021,"","","","",195,"2022-07-13 09:21:09","","10.3397/in-2021-3008","","",,,,,0,0.00,0,5,1,"The optimal design methodologies in aeronautics are known to be constrained by the computational burden required by direct simulations. Due to this reason, the development of efficient metamodelling techniques represents nowadays an imperative need for the designers. In fact, surrogate  models has been demonstrated to significantly reduce the number of high-fidelity evaluations, thus alleviating the computing effort. Over the last years, the aeronautical designers community has switched from a design approach predominantly based on direct simulations to an extensive use of  metamodels. Recently, to further improve the efficiency, several dynamic approaches based on parameters self-tuning have been developed to support the metamodel construction. This work deals with the use of surrogate models based on Artificial Neural Network for the noise shielding of unconventional  aircraft configurations. Here, the insertion loss field of the a Blended Wing Body is reproduced by means of advanced machine learning techniques. The relevant framework is the calculation of the noise emitted by innovative aircraft configurations by means of suitable corrections of existing  well-assessed noise prediction tools. The self-tuning algorithm has demonstrated to be accurate and efficient, and the observed performance discloses the possibility to implement numerical strategies for the reliable and robust unconventional aircraft optimal design","",""
32,"Jun-Ho Huh, Yeong-Seok Seo","Understanding Edge Computing: Engineering Evolution With Artificial Intelligence",2019,"","","","",196,"2022-07-13 09:21:09","","10.1109/ACCESS.2019.2945338","","",,,,,32,10.67,16,2,3,"The key to the explosion of the Internet of Things and the ability to collect, analyze, and provide big data in the cloud is edge computing, which is a new computing paradigm in which data is processed from edges. Edge Computing has been attracting attention as one of the top 10 strategic technology trends in the past two years and has innovative potential. It provides shorter response times, lower bandwidth costs, and more robust data safety and privacy protection than cloud computing. In particular, artificial intelligence technologies are rapidly incorporating edge computing. In this paper, we introduce the concepts, backgrounds, and pros and cons of edge computing, explain how it operates and its structure hierarchically with artificial intelligence concepts, list examples of its applications in various fields, and finally suggest some improvements and discuss the challenges of its application in three representative technological fields. We intend to clarify various analyses and opinions regarding edge computing and artificial intelligence.","",""
32,"Matt Taddy","The Technological Elements of Artificial Intelligence",2018,"","","","",197,"2022-07-13 09:21:09","","10.3386/W24301","","",,,,,32,8.00,32,1,4,"We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together – each targeting a straightforward prediction task – to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.","",""
33,"Dingqi Yan, Qi Zhou, Jianzhou Wang, N. Zhang","Bayesian regularisation neural network based on artificial intelligence optimisation",2017,"","","","",198,"2022-07-13 09:21:09","","10.1080/00207543.2016.1237785","","",,,,,33,6.60,8,4,5,"Stock prediction is generally considered to be challenging and known for its high noise and strong nonlinearities in financial time series analysis. However, current forecasting models ignore the importance of model parameter optimisation and the use of recent data. In this article, a novel forecasting approach with a Bayesian-regularised artificial neural networks (BR-ANN) was proposed. The weight of the proposed model (BR-ANN) is determined by the particle swarm optimisation (PSO) algorithm. Daily market prices and financial technical indicators are utilised as inputs to predict the one day future closing price of the Shanghai (in China) composite index. The Bayesian-regularised network uses a probabilistic nature for the network weights and can reduce the potential for over-fitting and over-training. Our empirical study and the results of our K-line theory analysis indicate that PSO is determined to be an effective algorithm to optimise the parameters of the Bayesian neural network compared with other well-known prediction algorithms. In particular, the PSO model is more reliable than the simple Bayesian regularisation neural network near the local maximum value.","",""
21,"D. Ting, M. Ang, J. Mehta, D. Ting","Artificial intelligence-assisted telemedicine platform for cataract screening and management: a potential model of care for global eye health",2019,"","","","",199,"2022-07-13 09:21:09","","10.1136/bjophthalmol-2019-315025","","",,,,,21,7.00,5,4,3,"Artificial intelligence (AI) is the fourth industrial revolution.1 Deep learning is a robust machine learning technique that uses convolutional neural network to perform multilevel data abstraction without the need for manual feature engineering.2 In ophthalmology, many studies showed comparable, if not better, diagnostic performance in using AI to screen, diagnose, predict and monitor various eye conditions on fundus photographs and optical coherence tomography,3 4 including diabetic retinopathy (DR),5 age-related macular degeneration,6 glaucoma,7 retinopathy of prematurity (ROP).8   To date, many countries have reported well-established telemedicine programme to screen for DR and ROP,9–12 but limited for cataracts. Cataract is the leading cause of reversible blindness, affecting approximately 12.6 million (3.4–28.7 million) worldwide.13 14 The prevalence of cataract-related visual impairment also varies between high-income and low-income countries, with the latter having poorer access to tertiary care.13 In this issue, Wu et al 15 reported an AI-integrated telemedicine platform to screen and refer patients with cataract. This article consists of two parts: (1) the first part focusing on the AI system in detection of three tasks (capture mode, cataract diagnosis and referable cataract) and (2) the second part describing how these AI algorithms could be integrated in the telemedicine platform for real-world operational use. In this study, the referable cases were defined as: (1) grade 3 and grade 4 nuclear sclerotic …","",""
22,"Rushikesh S. Joshi, Alexander F. Haddad, Darryl Lau, C. Ames","Artificial Intelligence for Adult Spinal Deformity",2019,"","","","",200,"2022-07-13 09:21:09","","10.14245/ns.1938414.207","","",,,,,22,7.33,6,4,3,"Adult spinal deformity (ASD) is a complex disease that significantly affects the lives of many patients. Surgical correction has proven to be effective in achieving improvement of spinopelvic parameters as well as improving quality of life (QoL) for these patients. However, given the relatively high complication risk associated with ASD correction, it is of paramount importance to develop robust prognostic tools for predicting risk profile and outcomes. Historically, statistical models such as linear and logistic regression models were used to identify preoperative factors associated with postoperative outcomes. While these tools were useful for looking at simple associations, they represent generalizations across large populations, with little applicability to individual patients. More recently, predictive analytics utilizing artificial intelligence (AI) through machine learning for comprehensive processing of large amounts of data have become available for surgeons to implement. The use of these computational techniques has given surgeons the ability to leverage far more accurate and individualized predictive tools to better inform individual patients regarding predicted outcomes after ASD correction surgery. Applications range from predicting QoL measures to predicting the risk of major complications, hospital readmission, and reoperation rates. In addition, AI has been used to create a novel classification system for ASD patients, which will help surgeons identify distinct patient subpopulations with unique risk-benefit profiles. Overall, these tools will help surgeons tailor their clinical practice to address patients’ individual needs and create an opportunity for personalized medicine within spine surgery.","",""
