Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
1,"Tim G. J. Rudner, H. Toner","Key Concepts in AI Safety: Interpretability in Machine Learning",2021,"","","","",1,"2022-07-13 09:24:29","","10.51593/20190042","","",,,,,1,1.00,1,2,1,"This paper is the third installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces interpretability as a means to enable assurance in modern machine learning systems.","",""
1,"Jivitesh Sharma, Rohan Kumar Yadav, Ole-Christoffer Granmo, Lei Jiao","Drop Clause: Enhancing Performance, Interpretability and Robustness of the Tsetlin Machine",2021,"","","","",2,"2022-07-13 09:24:29","","","","",,,,,1,1.00,0,4,1,"In this article, we introduce a novel variant of the Tsetlin machine (TM) that randomly drops clauses, the key learning elements of a TM. In effect, TM with drop clause ignores a random selection of the clauses in each epoch, selected according to a predefined probability. In this way, additional stochasticity is introduced in the learning phase of TM. To explore the effects drop clause has on accuracy, training time, interpretability and robustness, we conduct extensive experiments on nine benchmark datasets in natural language processing (NLP) (IMDb, R8, R52, MR and TREC) and image classification (MNIST, Fashion MNIST, CIFAR-10 and CIFAR100). Our proposed model outperforms baseline machine learning algorithms by a wide margin and achieves competitive performance in comparison with recent deep learning model such as BERT and AlexNET-DFA. In brief, we observe up to +10% increase in accuracy and 2× to 4× faster learning compared with standard TM. We further employ the Convolutional TM to document interpretable results on the CIFAR datasets, visualizing how the heatmaps produced by the TM become more interpretable with drop clause. We also evaluate how drop clause affects learning robustness by introducing corruptions and alterations in the image/language test data. Our results show that drop clause makes TM more robust towards such changes1.","",""
0,"Jianbo Chen","Towards Interpretability and Robustness of Machine Learning Models",2019,"","","","",3,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,1,3,"Author(s): Chen, Jianbo | Advisor(s): Jordan, Michael I; Wainwright, Martin J | Abstract: Modern machine learning models can be difficult to probe and understand after they have been trained. This is a major problem for the field, with consequences for trustworthiness, diagnostics, debugging, robustness, and a range of other engineering and human interaction issues surrounding the deployment of a model. Another problem of modern machine learning models is their vulnerability to small adversarial perturbations to the input, which incurs a security risk when they are applied to critical areas.In this thesis, we develop systematic and efficient tools for interpreting machine learning models and evaluating their adversarial robustness. Part I focuses on model interpretation. We derive an efficient feature scoring method by exploiting the graph structure in data. We also develop a learning-based method under an information-based framework. As an attempt to leverage prior knowledge about what constitutes a satisfying interpretation in a given domain, we propose a systematic approach to exploiting syntactic constituency structure by leveraging a parse tree for interpretation of models in the setting of linguistic data. Part II focuses on the evaluation of adversarial robustness. We first propose a probabilistic framework for generating adversarial examples on discrete data, and develop two algorithms to implement it. We also introduce a novel attack method in the setting where the attacker has access to model decisions alone. We investigate the robustness of various machine learning models and existing defense mechanisms under the proposed attack method. In Part III, we build a connection between the two fields by developing a method for detecting adversarial examples via tools in model interpretation.","",""
0,"R. Shokri","Trusting Machine Learning: Privacy, Robustness, and Transparency Challenges",2019,"","","","",4,"2022-07-13 09:24:29","","10.1145/3335203.3335728","","",,,,,0,0.00,0,1,3,"Machine learning algorithms have shown an unprecedented predictive power for many complex learning tasks. As they are increasingly being deployed in large scale critical applications for processing various types of data, new questions related to their trustworthiness would arise. Can machine learning algorithms be trusted to have access to individuals’ sensitive data? Can they be robust against noisy or adversarially perturbed data? Can we reliably interpret their learning process, and explain their predictions? In this talk, I will go over the challenges of building trustworthy machine learning algorithms in centralized and distributed (federated) settings, and will discuss the inter-relation between privacy, robustness, and interpretability.","",""
0,"George J. Siedel, S. Vock, A. Morozov, Stefan Voss","Utilizing Class Separation Distance for the Evaluation of Corruption Robustness of Machine Learning Classifiers",2022,"","","","",5,"2022-07-13 09:24:29","","10.48550/arXiv.2206.13405","","",,,,,0,0.00,0,4,1,"Robustness is a fundamental pillar of Machine Learning (ML) classifiers, substantially determining their reliability. Methods for assessing classifier robustness are therefore essential. In this work, we address the challenge of evaluating corruption robustness in a way that allows comparability and interpretability on a given dataset. We propose a test data augmentation method that uses a robustness distance 𝜖 derived from the datasets minimal class separation distance. The resulting MSCR (mean statistical corruption robustness) metric allows a dataset-specific comparison of different classifiers with respect to their corruption robustness. The MSCR value is interpretable, as it represents the classifiers avoidable loss of accuracy due to statistical corruptions. On 2D and image data, we show that the metric reflects different levels of classifier robustness. Furthermore, we observe unexpected optima in classifiers robust accuracy through training and testing classifiers with different levels of noise. While researchers have frequently reported on a significant tradeoff on accuracy when training robust models, we strengthen the view that a tradeoff between accuracy and corruption robustness is not inherent. Our results indicate that robustness training through simple data augmentation can already slightly improve accuracy.","",""
4,"Kenji Suzuki, M. Reyes, T. Syeda-Mahmood","Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support: Second International Workshop, iMIMIC 2019, and 9th International Workshop, ML-CDS 2019, Held in Conjunction with MICCAI 2019, Shenzhen, China, October 17, 2019, Proceedings",2019,"","","","",6,"2022-07-13 09:24:29","","10.1007/978-3-030-33850-3","","",,,,,4,1.33,1,3,3,"","",""
1,"Khansa Rasheed, A. Qayyum, M. Ghaly, A. Al-Fuqaha, A. Razi, Junaid Qadir","Explainable, Trustworthy, and Ethical Machine Learning for Healthcare: A Survey",2021,"","","","",7,"2022-07-13 09:24:29","","10.36227/TECHRXIV.14376179.V1","","",,,,,1,1.00,0,6,1,"With the advent of machine learning (ML) applications in daily life, the questions about liability, trust, and interpretability of their outputs are raising, especially for healthcare applications. The black-box nature of ML models is a roadblock for clinical utilization. Therefore, to gain the trust of clinicians and patients, researchers need to provide explanations of how and why the model is making a specific decision. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the field of eXplainable ML (XML). In this paper, we provide a comprehensive review of explainable and interpretable ML techniques implemented for providing the reasons behind their decisions for various healthcare applications. Along with highlighting various security, safety, and robustness challenges that hinder the trustworthiness of ML we also discussed the ethical issues of healthcare ML and describe how explainable and trustworthy ML can resolve these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.","",""
0,"Merel Kuijs, C. Jutzeler, B. Rieck, S. Brüningk","Interpretability Aware Model Training to Improve Robustness against Out-of-Distribution Magnetic Resonance Images in Alzheimer's Disease Classification",2021,"","","","",8,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,4,1,"Owing to its pristine soft-tissue contrast and high resolution, structural magnetic resonance imaging (MRI) is widely applied in neurology, making it a valuable data source for imagebased machine learning (ML) and deep learning applications. The physical nature of MRI acquisition and reconstruction, however, causes variations in image intensity, resolution, and signal-to-noise ratio. Since ML models are sensitive to such variations, performance on out-of-distribution data, which is inherent to the setting of a deployed healthcare ML application, typically drops below acceptable levels. We propose an interpretability aware adversarial training regime to improve robustness against out-of-distribution samples originating from different MRI hardware. The approach is applied to 1.5T and 3T MRIs obtained from the Alzheimer’s Disease Neuroimaging Initiative database. We present preliminary results showing promising performance on out-ofdistribution samples.","",""
0,"Aurélien Olivier, C. Hoffmann, A. Mansour, L. Bressollette, Benoit Clement","Survey on machine learning applied to medical image analysis",2021,"","","","",9,"2022-07-13 09:24:29","","10.1109/CISP-BMEI53629.2021.9624442","","",,,,,0,0.00,0,5,1,"This paper presents a selective survey on recent advances in machine learning applied to medical imaging. It aims to highlight both innovations that increase the performance of the models and methods that ensure certainty, interpretability and robustness of the trained models. The paper focuses particularly on new concepts such as attention modules that allow to gather specific features considering global context. Its second main focus is given to domain adaptation methods to enhance model robustness to distribution shifts. Finally, we discuss uncertainty estimation and interpretability methods to evaluate confidence in a trained model.","",""
0,"Hiroto Mizutani, Masateu Tsunoda, K. Nakasai","How to Enlighten Novice Users on Behavior of Machine Learning Models?",2021,"","","","",10,"2022-07-13 09:24:29","","10.1109/SNPD51163.2021.9704891","","",,,,,0,0.00,0,3,1,"Background: Machine learning models are sometimes embedded in software to implement the required functions. As a result, non-experts in machine learning are becoming familiar with the models. However, the interpretability of the built models is often low in machine learning, such as deep learning, and the recognition process of such models is very different from that of humans. Therefore, it is not easy for novice users, such as end-users and beginners, to anticipate the behavior of models that they will use or build. Aim: We assist novice users to realize an aspect of the behavior of machine learning models relating to robustness intuitively. Method: We formalized and evaluated quiz-based analysis, which is often applied by practitioners to test the robustness of machine learning models arbitrarily. To generate test cases of the models, the analysis converts images towards the boundary of classification for both machine learning and humans. It can be regarded as a type of boundary value analysis of software development. Results: In the experiment, we evaluated whether the analysis quantitatively clarified the aspects of the models. The analysis clarified the robustness of the model for image conversion and misclassification quantitatively. Conclusion: The analysis is expected to enlighten novice users on the behavior of machine learning models. This may promote behavioral changes in the evaluation of models for novice users.","",""
15,"Zihao Wang, Yang Su, Saimeng Jin, W. Shen, Jingzheng Ren, Xiang-ping Zhang, J. Clark","A novel unambiguous strategy of molecular feature extraction in machine learning assisted predictive models for environmental properties",2020,"","","","",11,"2022-07-13 09:24:29","","10.1039/d0gc01122c","","",,,,,15,7.50,2,7,2,"Environmental properties of compounds provide significant information in treating organic pollutants, which drives the chemical process and environmental science toward eco-friendly technology. Traditional group contribution methods play an important role in property estimations, whereas various disadvantages emerge in their applications, such as scattered predicted values for certain groups of compounds. In order to address such issues, an extraction strategy for molecular features is proposed in this research, which is characterized by interpretability and discriminating power with regard to isomers. Based on the Henry's law constant data of organic compounds in water, we developed a hybrid predictive model that integrates the proposed strategy in conjunction with a neural network framework. The structure of the predictive model is optimized using cross-validation and grid search to improve its robustness. Moreover, the predictive model is improved by introducing the plane of best fit descriptor as input and adopting k-means clustering in sampling. In contrast with reported models in the literature, the developed predictive model demonstrates improved generality, higher accuracy, and fewer molecular features used in its development.","",""
4,"Hala Abdelkader","Towards Robust Production Machine Learning Systems: Managing Dataset Shift",2020,"","","","",12,"2022-07-13 09:24:29","","10.1145/3324884.3415281","","",,,,,4,2.00,4,1,2,"The advances in machine learning (ML) have stimulated the integration of their capabilities into software systems. However, there is a tangible gap between software engineering and machine learning practices, that is delaying the progress of intelligent services development. Software organisations are devoting effort to adjust the software engineering processes and practices to facilitate the integration of machine learning models. Machine learning researchers as well are focusing on improving the interpretability of machine learning models to support overall system robustness. Our research focuses on bridging this gap through a methodology that evaluates the robustness of machine learning-enabled software engineering systems. In particular, this methodology will automate the evaluation of the robustness properties of software systems against dataset shift problems in ML. It will also feature a notification mechanism that facilitates the debugging of ML components.","",""
4,"Ninghao Liu, Mengnan Du, Xia Hu","Adversarial Machine Learning: An Interpretation Perspective",2020,"","","","",13,"2022-07-13 09:24:29","","","","",,,,,4,2.00,1,3,2,"Recent years have witnessed the significant advances of machine learning in a wide spectrum of applications. However, machine learning models, especially deep neural networks, have been recently found to be vulnerable to carefully-crafted input called adversarial samples. The difference between normal and adversarial samples is almost imperceptible to human. Many work have been proposed to study adversarial attack and defense in different scenarios. An intriguing and crucial aspect among those work is to understand the essential cause of model vulnerability, which requires in-depth exploration of another concept in machine learning models, i.e., interpretability. Interpretable machine learning tries to extract human-understandable terms for the working mechanism of models, which also receives a lot of attention from both academia and industry. Recently, an increasing number of work start to incorporate interpretation into the exploration of adversarial robustness. Furthermore, we observe that many previous work of adversarial attacking, although did not mention it explicitly, can be regarded as natural extension of interpretation. In this paper, we review recent work on adversarial attack and defense, particularly, from the perspective of machine learning interpretation. We categorize interpretation into two types, according to whether it focuses on raw features or model components. For each type of interpretation, we elaborate on how it could be used in attacks, or defense against adversaries. After that, we briefly illustrate other possible correlations between the two domains. Finally, we discuss the challenges and future directions along tackling adversary issues with interpretation.","",""
0,"Alex Gu","Certified Interpretability Robustness for Class Activation Mapping",2020,"","","","",14,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,1,2,"Interpreting machine learning models is challenging but crucial for ensuring the safety of deep networks in autonomous driving systems. Due to the prevalence of deep learning based perception models in autonomous vehicles, accurately interpreting their predictions is crucial. While a variety of such methods have been proposed, most are shown to lack robustness. Yet, little has been done to provide certificates for interpretability robustness. Taking a step in this direction, we present CORGI, short for Certifiably prOvable Robustness Guarantees for Interpretability mapping. CORGI is an algorithm that takes in an input image and gives a certifiable lower bound for the robustness of the top k pixels of its CAM interpretability map. We show the effectiveness of CORGI via a case study on traffic sign data, certifying lower bounds on the minimum adversarial perturbation not far from (4-5x) state-of-the-art attack methods.","",""
0,"Taru Jain","Adversarial Machine Learning for Self Harm Disclosure Analysis (Workshop Paper)",2020,"","","","",15,"2022-07-13 09:24:29","","10.1109/BigMM50055.2020.00070","","",,,,,0,0.00,0,1,2,"Adversarial Machine Learning has been gaining attention from the NLP community due to low interpretability and low robustness of the current state-of-the-art systems. In this work, we study the effect of various adversarial attacks for detection of suicidal intent in social media setting. Suicide Ideation is a sensitive issue and is a leading cause of death. We show how various models are rendered useless after attacks and perform adversarial training using the most ideal attacks to improve their robustness. We also conduct several experiments with the attacks to study their effect and propose an approach for adversarial training using Generative Adversarial Networks.","",""
51,"Shubham Sharma, Jette Henderson, Joydeep Ghosh","CERTIFAI: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models",2019,"","","","",16,"2022-07-13 09:24:29","","10.1145/3375627.3375812","","",,,,,51,17.00,17,3,3,"As artificial intelligence plays an increasingly important role in our society, there are ethical and moral obligations for both businesses and researchers to ensure that their machine learning models are designed, deployed, and maintained responsibly. These models need to be rigorously audited for fairness, robustness, transparency, and interpretability. A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming. In this paper, we introduce a unified and model-agnostic approach to address these issues: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models (CERTIFAI). Unlike previous methods in this domain, CERTIFAI is a general tool that can be applied to any black-box model and any type of input data. Given a model and an input instance, CERTIFAI uses a custom genetic algorithm to generate counterfactuals: instances close to the input that change the prediction of the model. We demonstrate how these counterfactuals can be used to examine issues of robustness, interpretability, transparency, and fairness. Additionally, we introduce CERScore, the first black-box model robustness score that performs comparably to methods that have access to model internals.","",""
28,"Pouya Pezeshkpour, Yifan Tian, Sameer Singh","Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications",2018,"","","","",17,"2022-07-13 09:24:29","","10.18653/v1/N19-1337","","",,,,,28,7.00,9,3,4,"Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on improving accuracy and overlook other aspects such as robustness and interpretability. In this paper, we propose adversarial modifications for link prediction models: identifying the fact to add into or remove from the knowledge graph that changes the prediction for a target fact after the model is retrained. Using these single modifications of the graph, we identify the most influential fact for a predicted link and evaluate the sensitivity of the model to the addition of fake facts. We introduce an efficient approach to estimate the effect of such modifications by approximating the change in the embeddings when the knowledge graph changes. To avoid the combinatorial search over all possible facts, we train a network to decode embeddings to their corresponding graph components, allowing the use of gradient-based optimization to identify the adversarial modification. We use these techniques to evaluate the robustness of link prediction models (by measuring sensitivity to additional facts), study interpretability through the facts most responsible for predictions (by identifying the most influential neighbors), and detect incorrect facts in the knowledge base.","",""
13,"Thibault Laugel, Marie-Jeanne Lesot, C. Marsala, X. Renard, Marcin Detyniecki","Unjustified Classification Regions and Counterfactual Explanations in Machine Learning",2019,"","","","",18,"2022-07-13 09:24:29","","10.1007/978-3-030-46147-8_3","","",,,,,13,4.33,3,5,3,"","",""
1,"Hamed Zamani, Fernando Diaz, M. Dehghani, Donald Metzler, Michael Bendersky","Retrieval-Enhanced Machine Learning",2022,"","","","",19,"2022-07-13 09:24:29","","10.1145/3477495.3531722","","",,,,,1,1.00,0,5,1,"Although information access systems have long supportedpeople in accomplishing a wide range of tasks, we propose broadening the scope of users of information access systems to include task-driven machines, such as machine learning models. In this way, the core principles of indexing, representation, retrieval, and ranking can be applied and extended to substantially improve model generalization, scalability, robustness, and interpretability. We describe a generic retrieval-enhanced machine learning (REML) framework, which includes a number of existing models as special cases. REML challenges information retrieval conventions, presenting opportunities for novel advances in core areas, including optimization. The REML research agenda lays a foundation for a new style of information access research and paves a path towards advancing machine learning and artificial intelligence.","",""
4,"Henriette Violante","Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications",2019,"","","","",20,"2022-07-13 09:24:29","","","","",,,,,4,1.33,4,1,3,"Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on improving accuracy and overlook other aspects of knowledge base representations, such as robustness and interpretability. In this paper, we propose adversarial modifications for link prediction models: identifying the fact to add into or remove from the knowledge graph that changes the prediction for a target fact after the model is retrained. Using these single modifications of the graph, we are able to identify the most influential fact for a predicted link and evaluate the sensitivity of the model to the addition of fake facts. We introduce an efficient approach to estimate the effect of such modifications by approximating the change in the embeddings when the knowledge graph changes. To avoid the combinatorial search over all possible facts, we train a network to decode embeddings to their corresponding graph components, allowing the use of gradient-based optimization to identify the adversarial modification. We use these techniques to evaluate the robustness of link prediction models (by measuring sensitivity to additional facts), study interpretability through the facts most responsible for predictions (by identifying the most influential neighbors), and detect incorrect facts in the knowledge base.","",""
0,"Beilei Wang, Jie Jing, Xiaochun Huang, Cheng Hua, Qin Qin, Y. Jia, Zhiyong Wang, Lei Jiang, Bai Gao, Les J. Wu, Xianfei Zeng, Fubo Wang, Chuanbin Mao, Shanrong Liu","Establishment of a Knowledge‐and‐Data‐Driven Artificial Intelligence System with Robustness and Interpretability in Laboratory Medicine",2022,"","","","",21,"2022-07-13 09:24:29","","10.1002/aisy.202100204","","",,,,,0,0.00,0,14,1,"Laboratory medicine plays an important role in clinical diagnosis. However, no laboratory‐based artificial intelligence (AI) diagnostic system has been applied in current clinical practice due to the lack of robustness and interpretability. Although many attempts have been made, it is still difficult for doctors to adopt the existing machine learning (ML) patterns in interpreting laboratory (lab) big data. Here, a knowledge‐and‐data‐driven laboratory diagnostic system is developed, termed AI‐based Lab tEst tO diagNosis (AI LEON), by integrating an innovative knowledge graph analysis framework and “mixed XGboost and Genetic Algorithm (MiXG)” technique to simulate the doctor's laboratory‐based diagnosis. To establish AI LEON, we included 89 116 949 laboratory data and 10 423 581 diagnosis data points from 730 113 participants. Among them, 686 626 participants were recruited for training and validating purposes with the remaining for testing purposes. AI LEON automatically identified and analyzed 2071 lab indexes, resulting in multiple disease recommendations that involved 441 common diseases in ten organ systems. AI LEON exhibited outstanding transparency and interpretability in three universal clinical application scenarios and outperformed human physicians in interpreting lab reports. AI LEON is an advanced intelligent system that enables a comprehensive interpretation of lab big data, which substantially improves the clinical diagnosis.","",""
1,"Aida Rahmattalabi, Alice Xiang","Promises and Challenges of Causality for Ethical Machine Learning",2022,"","","","",22,"2022-07-13 09:24:29","","","","",,,,,1,1.00,1,2,1,"In recent years, there has been increasing interest in causal reasoning for designing fair decision-making systems due to its compatibility with legal frameworks, interpretability for human stakeholders, and robustness to spurious correlations inherent in observational data, among other factors. The recent attention to causal fairness, however, has been accompanied with great skepticism due to practical and epistemological challenges with applying current causal fairness approaches in the literature. Motivated by the long-standing empirical work on causality in econometrics, social sciences, and biomedical sciences, in this paper we lay out the conditions for appropriate application of causal fairness under the""potential outcomes framework.""We highlight key aspects of causal inference that are often ignored in the causal fairness literature. In particular, we discuss the importance of specifying the nature and timing of interventions on social categories such as race or gender. Precisely, instead of postulating an intervention on immutable attributes, we propose a shift in focus to their perceptions and discuss the implications for fairness evaluation. We argue that such conceptualization of the intervention is key in evaluating the validity of causal assumptions and conducting sound causal analysis including avoiding post-treatment bias. Subsequently, we illustrate how causality can address the limitations of existing fairness metrics, including those that depend upon statistical correlations. Specifically, we introduce causal variants of common statistical notions of fairness, and we make a novel observation that under the causal framework there is no fundamental disagreement between different notions of fairness. Finally, we conduct extensive experiments where we demonstrate our approach for evaluating and mitigating unfairness, specially when post-treatment variables are present.","",""
0,"Ali M’Rabeth","Model Risk in the age of Artificial Intelligence and Machine Learning What are the impacts on Model Risk?",2019,"","","","",23,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,1,3,"An increasing reliance on Artificial Intelligence for decision making is driving financial institutions, regulators, and supervisors towards a clarification of sources and control of risks. These risks were either already present (but marginal) or even non-existent in the usual model risk management framework. In a context where the use of machine learning is becoming massive and industrialized across banks and insurance companies, problematics such as interpretability and dynamic monitoring, robustness, ethics, bias and fairness require a specific attention.","",""
0,"Mohammadreza Amirian, Lukas Tuggener, R. Chavarriaga, Y. Satyawan, F. Schilling, F. Schwenker, Thilo Stadelmann","Two to trust : AutoML for safe modelling and interpretable deep learning for robustness",2020,"","","","",24,"2022-07-13 09:24:29","","10.21256/ZHAW-20217","","",,,,,0,0.00,0,7,2,"With great power comes great responsibility. The success of machine learning, especially deep learning, in research and practice has attracted a great deal of interest, which in turn necessitates increased trust. Sources of mistrust include matters of model genesis (“Is this really the appropriate model?”) and interpretability (“Why did the model come to this conclusion?”, “Is the model safe from being easily fooled by adversaries?”). In this paper, two partners for the trustworthiness tango are presented: recent advances and ideas, as well as practical applications in industry in (a) Automated machine learning (AutoML), a powerful tool to optimize deep neural network architectures and finetune hyperparameters, which promises to build models in a safer and more comprehensive way; (b) Interpretability of neural network outputs, which addresses the vital question regarding the reasoning behind model predictions and provides insights to improve robustness against adversarial attacks.","",""
14,"Steve Agajanian, O. Odeyemi, N. Bischoff, S. Ratra, Gennady M Verkhivker","Machine Learning Classification and Structure-Functional Analysis of Cancer Mutations Reveal Unique Dynamic and Network Signatures of Driver Sites in Oncogenes and Tumor Suppressor Genes",2018,"","","","",25,"2022-07-13 09:24:29","","10.1021/acs.jcim.8b00414","","",,,,,14,3.50,3,5,4,"In this study, we developed two cancer-specific machine learning classifiers for prediction of driver mutations in cancer-associated genes that were validated on canonical data sets of functionally validated mutations and applied to a large cancer genomics data set. By examining sequence, structure, and ensemble-based integrated features, we have shown that evolutionary conservation scores play a critical role in classification of cancer drivers and provide the strongest signal in the machine learning prediction. Through extensive comparative analysis with structure-functional experiments and multicenter mutational calling data from Pan Cancer Atlas studies, we have demonstrated the robustness of our models and addressed the validity of computational predictions. To address the interpretability of cancer-specific classification models and obtain novel insights about molecular signatures of driver mutations, we have complemented machine learning predictions with structure-functional analysis of cancer driver mutations in several important oncogenes and tumor suppressor genes. By examining structural and dynamic signatures of known mutational hotspots and the predicted driver mutations, we have shown that the greater flexibility of specific functional regions targeted by driver mutations in oncogenes may facilitate activating conformational changes, while loss-of-function driver mutations in tumor suppressor genes can preferentially target structurally rigid positions that mediate allosteric communications in residue interaction networks and modulate protein binding interfaces. By revealing molecular signatures of cancer driver mutations, our results highlighted limitations of the binary driver/passenger classification, suggesting that functionally relevant cancer mutations may span a continuum spectrum of driverlike effects. Based on this analysis, we propose for experimental testing a group of novel potential driver mutations that can act by altering structure, global dynamics, and allosteric interaction networks in important cancer genes.","",""
6,"Tanya Tiwari, Tanuj Tiwari, Sanjay Tiwari","How Artificial Intelligence, Machine Learning and Deep Learning are Radically Different?",2018,"","","","",26,"2022-07-13 09:24:29","","10.23956/IJARCSSE.V8I2.569","","",,,,,6,1.50,2,3,4,"There is a lot of confusion these days about Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL). A computer system able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages. Artificial Intelligence has made it possible. Deep learning is a subset of machine learning, and machine learning is a subset of AI, which is an umbrella term for any computer program that does something smart. In other words, all machine learning is AI, but not all AI is machine learning, and so forth. Machine Learning represents a key evolution in the fields of computer science, data analysis, software engineering, and artificial intelligence. Machine learning (ML)is a vibrant field of research, with a range of exciting areas for further development across different methods and applications. These areas include algorithmic interpretability, robustness, privacy, fairness, inference of causality, human-machine interaction, and security. The goal of ML is never to make “perfect” guesses, because ML deals in domains where there is no such thing. The goal is to make guesses that are good enough to be useful. Deep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones. This paper gives an overview of artificial intelligence, machine learning & deep learning techniques and compare these techniques.","",""
0,"Henriette Violante","Investigating Robustness and Interpretability of Link Prediction via Adversarial Attacks",2018,"","","","",27,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,1,4,"Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on improving ranking metrics and ignore other aspects of knowledge base representations, such as robustness, interpretability, and ability to detect errors. In this paper, we propose adversarial attacks on link prediction models (AALP): identifying the fact to add into or remove from the knowledge graph that changes the prediction of a target fact. Using these attacks, we are able to identify the most influential related fact for a predicted link and investigate the sensitivity of the model to additional made-up facts. We introduce an efficient approach to estimate the effect of making a change by approximating the change in the embeddings upon altering the knowledge graph. In order to avoid the combinatorial search over all possible facts, we introduce an inverter function and gradient-based search to identify the adversary in a continuous space. We demonstrate that our models effectively attack the link prediction models by reducing their accuracy between 6− 45% for different metrics. Further, we study patterns in the most influential neighboring facts, as identified by the adversarial attacks. Finally, we use the proposed approach to detect incorrect facts in the knowledge base, achieving up to 55% accuracy in identifying errors.","",""
8,"Hal S. Greenwald, Carsten K. Oertel","Future Directions in Machine Learning",2017,"","","","",28,"2022-07-13 09:24:29","","10.3389/frobt.2016.00079","","",,,,,8,1.60,4,2,5,"Current machine learning algorithms identify statistical regularities in complex data sets and are regularly used across a range of application domains, but they lack the robustness and generalizability associated with human learning. If machine learning techniques could enable computers to learn from fewer examples, transfer knowledge between tasks, and adapt to changing contexts and environments, the results would have very broad scientific and societal impacts. Increased processing and memory resources have enabled larger, more capable learning models, but there is growing recognition that even greater computing resources would not be sufficient to yield algorithms capable of learning from a few examples and generalizing beyond initial training sets. This paper presents perspectives on feature selection, representation schemes and interpretability, transfer learning, continuous learning, and learning and adaptation in time-varying contexts and environments, five key areas that are essential for advancing machine learning capabilities. Appropriate learning tasks that require these capabilities can demonstrate the strengths of novel machine learning approaches that could address these challenges.","",""
1,"F. Santos, C. Zanchettin, L. Matos, P. Novais","On the Impact of Interpretability Methods in Active Image Augmentation Method",2021,"","","","",29,"2022-07-13 09:24:29","","10.1093/jigpal/jzab006","","",,,,,1,1.00,0,4,1,"Robustness is a significant constraint in machine learning models. The performance of the algorithms must not deteriorate when training and testing with slightly different data. Deep neural network models achieve awe-inspiring results in a wide range of applications of computer vision. Still, in the presence of noise or region occlusion, some models exhibit inaccurate performance even with data handled in training. Besides, some experiments suggest deep learning models sometimes use incorrect parts of the input information to perform inference. Activate Image Augmentation (ADA) is an augmentation method that uses interpretability methods to augment the training data and improve its robustness to face the described problems. Although ADA presented interesting results, its original version only used the Vanilla Backpropagation interpretability to train the U-Net model. In this work, we propose an extensive experimental analysis of the interpretability method’s impact on ADA. We use five interpretability methods: Vanilla Backpropagation, Guided Backpropagation, GradCam, Guided GradCam, and InputXGradient. The results show that all methods achieve similar performance at the ending of training, but when combining ADA with GradCam, the U-Net model presented an impressive fast convergence.","",""
1,"Ahmed M. A. Salih, I. Galazzo, Z. Raisi-Estabragh, S. Petersen, Polyxeni Gkontra, K. Lekadir, G. Menegaz, P. Radeva","A new scheme for the assessment of the robustness of Explainable Methods Applied to Brain Age estimation",2021,"","","","",30,"2022-07-13 09:24:29","","10.1109/CBMS52027.2021.00098","","",,,,,1,1.00,0,8,1,"Deep learning methods show great promise in a range of settings including the biomedical field. Explainability of these models is important in these fields for building end-user trust and to facilitate their confident deployment. Although several Machine Learning Interpretability tools have been proposed so far, there is currently no recognized evaluation standard to transfer the explainability results into a quantitative score. Several measures have been proposed as proxies for quantitative assessment of explainability methods. However, the robustness of the list of significant features provided by the explainability methods has not been addressed. In this work, we propose a new proxy for assessing the robustness of the list of significant features provided by two explainability methods. Our validation is defined at functionality-grounded level based on the ranked correlation statistical index and demonstrates its successful application in the framework of brain aging estimation. We assessed our proxy to estimate brain age using neuroscience data. Our results indicate small variability and high robustness in the considered explainability methods using this new proxy.","",""
0,"A. Fontana","And/or trade-off in artificial neurons: impact on adversarial robustness",2021,"","","","",31,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,1,1,"Since its discovery in 2013, the phenomenon of adversarial examples has attracted a growing amount of attention from the machine learning community. A deeper understanding of the problem could lead to a better comprehension of how information is processed and encoded in neural networks and, more in general, could help to solve the issue of interpretability in machine learning. Our idea to increase adversarial resilience starts with the observation that artificial neurons can be divided in two broad categories: AND-like neurons and OR-like neurons. Intuitively, the former are characterised by a relatively low number of combinations of input values which trigger neuron activation, while for the latter the opposite is true. Our hypothesis is that the presence in a network of a sufficiently high number of OR-like neurons could lead to classification “brittleness” and increase the network’s susceptibility to adversarial attacks. After constructing an operational definition of a neuron AND-like behaviour, we proceed to introduce several measures to increase the proportion of AND-like neurons in the network: L1 norm weight normalisation; application of an input filter; comparison between the neuron output’s distribution obtained when the network is fed with the actual data set and the distribution obtained when the network is fed with a randomised version of the former called “scrambled data set”. Tests performed on the MNIST data set hint that the proposed measures could represent an interesting direction to explore.","",""
6,"S. Hegselmann, T. Volkert, Hendrik Ohlenburg, A. Gottschalk, M. Dugas, C. Ertmer","An Evaluation of the Doctor-Interpretability of Generalized Additive Models with Interactions",2020,"","","","",32,"2022-07-13 09:24:29","","","","",,,,,6,3.00,1,6,2,"Applying machine learning in healthcare can be problematic because predictions might be biased, can lack robustness, and are prone to overly rely on correlations. Interpretable machine learning can mitigate these issues by visualizing gaps in problem formalization and putting the responsibility to meet additional desiderata of machine learning systems on human practitioners. Generalized additive models with interactions are transparent, with modular oneand two-dimensional risk functions that can be reviewed and, if necessary, removed. The key objective of this study is to determine whether these models can be interpreted by doctors to safely deploy them in a clinical setting. To this end, we simulated the review process of eight risk functions trained on a clinical task with twelve clinicians and collected information about objective and subjective factors of interpretability. The ratio of correct answers for dichotomous statements covering important properties of risk functions was 0.83±0.02 (n = 360) and the median of the participants’ certainty to correctly understand them was Certain (n = 96) on a seven-level Likert scale (one = Very Uncertain to seven = Very Certain). These results suggest that doctors can correctly interpret risk functions of generalized additive models with interactions and also feel confident to do so. However, the evaluation also identified several interpretability issues and it showed that interpretability of generalized additive models depends on the complexity of risk functions.","",""
7,"M. Singh, Nupur Kumari, Puneet Mangla, Abhishek Sinha, V. Balasubramanian, Balaji Krishnamurthy","Attributional Robustness Training Using Input-Gradient Spatial Alignment",2019,"","","","",33,"2022-07-13 09:24:29","","10.1007/978-3-030-58583-9_31","","",,,,,7,2.33,1,6,3,"","",""
3,"Jivitesh Sharma, Rohan Kumar Yadav, Ole-Christoffer Granmo, Lei Jiao","Human Interpretable AI: Enhancing Tsetlin Machine Stochasticity with Drop Clause",2021,"","","","",34,"2022-07-13 09:24:29","","","","",,,,,3,3.00,1,4,1,"In this article, we introduce a novel variant of the Tsetlin machine (TM) that randomly drops clauses, the key learning elements of a TM. In effect, TM with drop clause ignores a random selection of the clauses in each epoch, selected according to a predefined probability. In this way, additional stochasticity is introduced in the learning phase of TM. Along with producing more distinct and well-structured patterns that improve the performance, we also show that dropping clauses increases learning robustness. To explore the effects clause dropping has on accuracy, training time, and interpretability, we conduct extensive experiments on various benchmark datasets in natural language processing (NLP) (IMDb and SST2) as well as computer vision (MNIST and CIFAR10). In brief, we observe from +2% to +4% increase in accuracy and 2× to 4× faster learning. We further employ the Convolutional TM to document interpretable results on the CIFAR10 dataset. To the best of our knowledge, this is the first time an interpretable machine learning algorithm has been used to produce pixel-level human-interpretable results on CIFAR10. Also, unlike previous interpretable methods that focus on attention visualisation or gradient interpretability, we show that the TM is a more general interpretable method. That is, by producing rule-based propositional logic expressions that are human-interpretable, the TM can explain how it classifies a particular instance at the pixel level for computer vision and at the word level for NLP.","",""
9,"Chun-Hao Chang, R. Caruana, A. Goldenberg","NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning",2021,"","","","",35,"2022-07-13 09:24:29","","","","",,,,,9,9.00,3,3,1,"Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on model’s accuracy but also on its fairness, robustness and interpretability. Generalized Additive Models (GAMs) have a long history of use in these high-risk domains, but lack desirable features of deep learning such as differentiability and scalability. In this work, we propose a neural GAM (NODE-GAM) and neural GAM (NODE-GAM) that scale well to large datasets, while remaining interpretable and accurate. We show that our proposed models have comparable accuracy to other non-interpretable models, and outperform other GAMs on large datasets. We also show that our models are more accurate in self-supervised learning setting when access to labeled data is limited.","",""
327,"Nicolas Papernot, P. Mcdaniel","Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning",2018,"","","","",36,"2022-07-13 09:24:29","","","","",,,,,327,81.75,164,2,4,"Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.","",""
2,"V. Ardulov, Victor R. Martinez, Krishna Somandepalli, S. Zheng, E. Salzman, C. Lord, S. Bishop, Shrikanth S. Narayanan","Robust diagnostic classification via Q-learning",2021,"","","","",37,"2022-07-13 09:24:29","","10.1038/s41598-021-90000-4","","",,,,,2,2.00,0,8,1,"","",""
8,"M. Singh, Nupur Kumari, Puneet Mangla, Abhishek Sinha, V. Balasubramanian, Balaji Krishnamurthy","On the Benefits of Attributional Robustness",2019,"","","","",38,"2022-07-13 09:24:29","","","","",,,,,8,2.67,1,6,3,"Interpretability is an emerging area of research in trustworthy machine learning. Safe deployment of machine learning system mandates that the prediction and its explanation be reliable and robust. Recently, it was shown that one could craft perturbations that produce perceptually indistinguishable inputs having the same prediction, yet very different interpretations. We tackle the problem of attributional robustness (i.e. models having robust explanations) by maximizing the alignment between the input image and its saliency map using soft-margin triplet loss. We propose a robust attribution training methodology that beats the state-of-the-art attributional robustness measure by a margin of approximately 6-18% on several standard datasets, ie. SVHN, CIFAR-10 and GTSRB. We further show the utility of the proposed robust model in the domain of weakly supervised object localization and segmentation. Our proposed robust model also achieves a new state-of-the-art object localization accuracy on the CUB-200 dataset.","",""
0,"Lei Liao, Zhiqiu Huang, Wengjie Wang","A Statistical Learning Model with Deep Learning Characteristics",2021,"","","","",39,"2022-07-13 09:24:29","","10.1109/DSN-W52860.2021.00032","","",,,,,0,0.00,0,3,1,"Although machine learning has achieved great success in many fields, the lack of interpretability and excessive computational amount and poor robustness severely limits its wide application in real-world tasks, especially security-sensitive tasks. But the current deep learning research is still far from truly solving these problems. In order to overcome these problems encountered by the deep learning model, this article does not intend to modify the deep learning model itself, but design a new machine learning model to avoid various problems of deep learning. We proposes a new statistical learning model that learn from the characteristics of deep learning. Then we evaluate these models on two datasets and found that the new model is significantly better than the deep learning model in terms of computational complexity, robustness and interpretability.","",""
1,"Shakti Kumar, Hussain Zaidi","GDC- Generalized Distribution Calibration for Few-Shot Learning",2022,"","","","",40,"2022-07-13 09:24:29","","10.48550/arXiv.2204.05230","","",,,,,1,1.00,1,2,1,". Few shot learning is an important problem in machine learning as large labelled datasets take considerable time and eﬀort to assem-ble. Most few-shot learning algorithms suﬀer from one of two limitations— they either require the design of sophisticated models and loss functions, thus hampering interpretability; or employ statistical techniques but make assumptions that may not hold across diﬀerent datasets or features. Developing on recent work in extrapolating distributions of small sample classes from the most similar larger classes, we propose a Generalized sampling method that learns to estimate few-shot distributions for classiﬁcation as weighted random variables of all large classes. We use a form of covariance shrinkage to provide robustness against singular covariances due to overparameterized features or small datasets. We show that our sampled points are close to few-shot classes even in cases when there are no similar large classes in the training set. Our method works with arbitrary oﬀ-the-shelf feature extractors and outperforms existing state-of-the-art on miniImagenet, CUB and Stanford Dogs datasets by 3% to 5% on 5way-1shot and 5way-5shot tasks and by 1% in challenging cross domain tasks.","",""
1,"Maximilian Springenberg, A. Frommholz, M. Wenzel, Eva Weicken, Jackie Ma, Nils Strodthoff","From CNNs to Vision Transformers -- A Comprehensive Evaluation of Deep Learning Models for Histopathology",2022,"","","","",41,"2022-07-13 09:24:29","","","","",,,,,1,1.00,0,6,1,"—While machine learning is currently transforming the ﬁeld of histopathology, the domain lacks a comprehensive evaluation of state-of-the-art models based on essential but complementary quality requirements beyond a mere classiﬁcation accuracy. In order to ﬁll this gap, we conducted an extensive evaluation by benchmarking a wide range of classiﬁcation models, including recent vision transformers, convolutional neural networks and hybrid models comprising transformer and convolutional models. We thoroughly tested the models on ﬁve widely used histopathology datasets containing whole slide images of breast, gastric, and colorectal cancer and developed a novel approach using an image-to-image translation model to assess the robustness of a cancer classiﬁcation model against stain variations. Further, we extended existing interpretability methods to previously unstudied models and systematically reveal insights of the models’ classiﬁcation strategies that allow for plausibility checks and systematic comparisons. The study resulted in speciﬁc model recommendations for practitioners as well as putting forward a general methodology to quantify a model’s quality according to complementary requirements that can be transferred to future model architectures.","",""
1,"G. Vouros","Explainable Deep Reinforcement Learning: State of the Art and Challenges",2022,"","","","",42,"2022-07-13 09:24:29","","10.1145/3527448","","",,,,,1,1.00,1,1,1,"Interpretability, explainability and transparency are key issues to introducing Artificial Intelligence methods in many critical domains: This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability and fairness, and has important consequences towards keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. While the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article we aim to provide a review of state of the art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators - i.e., of those that take the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state of the art methods, categorizing them in classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes identifying open questions and important challenges.","",""
0,"G. Bomarito, P. Leser, N.C.M Strauss, K. Garbrecht, J. Hochhalter","Automated Learning of Interpretable Models with Quantified Uncertainty",2022,"","","","",43,"2022-07-13 09:24:29","","10.48550/arXiv.2205.01626","","",,,,,0,0.00,0,5,1,"Interpretability and uncertainty quantiﬁcation in machine learning can provide justiﬁcation for decisions, promote scientiﬁc discovery and lead to a better understanding of model behavior. Symbolic regression provides inherently interpretable machine learning, but relatively little work has focused on the use of symbolic regression on noisy data and the accompanying necessity to quantify uncertainty. A new Bayesian framework for genetic-programming-based symbolic regression (GPSR) is introduced that uses model evidence (i.e., marginal likelihood) to formulate replacement probability during the selection phase of evolution. Model parameter uncertainty is automatically quantiﬁed, enabling probabilistic predictions with each equation produced by the GPSR algorithm. Model evidence is also quantiﬁed in this process, and its use is shown to increase interpretability, improve robustness to noise, and reduce overﬁtting when compared to a conventional GPSR implementation on both numerical and physical experiments.","",""
0,"Maximilian-Peter Radtke, Jürgen Bock","Combining Knowledge and Deep Learning for Prognostics and Health Management",2022,"","","","",44,"2022-07-13 09:24:29","","10.36001/phme.2022.v7i1.3302","","",,,,,0,0.00,0,2,1,"In the recent past deep learning approaches have achieved remarkable results in the area of Prognostics and Health Management (PHM). These algorithms rely on large amounts of data, which is often not available, and produce outputs, which are hard to interpret. Before the broad success of deep learning machine faults were often classified using domain expert knowledge based on experience and physical models. In comparison, these approaches only require small amounts of data and produce highly interpretable results. On the downside, however, they struggle to predict unexpected patterns hidden in data. This research aims to combine knowledge and deep learning to increase accuracy, robustness and interpretability of current models.","",""
0,"Yifan Li, Chao Li, S. Price, C. Schönlieb, Xi Chen","Bayesian optimization assisted unsupervised learning for efficient intra-tumor partitioning in MRI and survival prediction for glioblastoma patients",2020,"","","","",45,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,5,2,"Glioblastoma is profoundly heterogeneous in microstructure and vasculature, which may lead to tumor regional diversity and distinct treatment response. Although successful in tumor sub-region segmentation and survival prediction, radiomics based on machine learning algorithms, is challenged by its robustness, due to the vague intermediate process and track changes. Also, the weak interpretability of the model poses challenges to clinical application. Here we proposed a machine learning framework to semi-automatically fine-tune the clustering algorithms and quantitatively identify stable sub-regions for reliable clinical survival prediction. Hyper-parameters are automatically determined by the global minimum of the trained Gaussian Process (GP) surrogate model through Bayesian optimization(BO) to alleviate the difficulty of tuning parameters for clinical researchers. To enhance the interpretability of the survival prediction model, we incorporated the prior knowledge of intra-tumoral heterogeneity, by segmenting tumor sub-regions and extracting sub-regional features. The results demonstrated that the global minimum of the trained GP surrogate can be used as sub-optimal hyper-parameter solutions for efficient. The sub-regions segmented based on physiological MRI can be applied to predict patient survival, which could enhance the clinical interpretability for the machine learning model.","",""
0,"S. Saralajew","New Prototype Concepts in Classification Learning",2020,"","","","",46,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,1,2,"Machine learning algorithms are becoming more and more important in everyday life. Applications in search engines, driver assistance systems, consumer electronics, and so on use them heavily and would not be as powerful without them. Neural Networks (NNs), for example, are state-of-the-art classification approaches and dominate the field. However, they are difficult to interpret and not fully understood. For instance, the existence of adversarial examples that are imperceptible to humans contradicts the general belief that convolutional NNs classify objects in images mainly by breaking them down into increasingly complex object shapes. In this thesis, we study prototype-based classification algorithms with the goal of improving the classification capabilities of such algorithms while simultaneously preserving robustness and interpretability properties. Moreover, we investigate how properties of prototypebased classification algorithms can be transferred to NNs in order to increase their interpretability. First, we derive the concept of set-prototypes and apply it in a Learning Vector Quantization (LVQ) framework—a well-understood classification algorithm. We examine the mathematical properties and show that the derived method is provably robust against adversarial attacks. Furthermore, the method consistently outperforms other LVQ approaches while still being interpretable. Second, we relax the class-specific prototype concept to that of components and apply it in LVQand NN-based classifiers. This framework provides promising interpretation techniques for NNs. For example, we use them to explain how an adversarial attack is fooling an NN. We evaluate the methods on both toy and real-world datasets, including Indian Pine, MNIST, CIFAR-10, GTSRB, and ImageNet.","",""
0,"Shangxi Wu, J. Sang, Xian Zhao, Lizhang Chen","An Experimental Study of Semantic Continuity for Deep Learning Models",2020,"","","","",47,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,4,2,"Deep learning models suffer from the problem of semantic discontinuity: small perturbations in the input space tend to cause semantic-level interference to the model output. We argue that the semantic discontinuity results from these inappropriate training targets and contributes to notorious issues such as adversarial robustness, interpretability, etc. We first conduct data analysis to provide evidence of semantic discontinuity in existing deep learning models, and then design a simple semantic continuity constraint which theoretically enables models to obtain smooth gradients and learn semantic-oriented features. Qualitative and quantitative experiments prove that semantically continuous models successfully reduce the use of non-semantic information, which further contributes to the improvement in adversarial robustness, interpretability, model transfer, and machine bias.","",""
3,"B. Pes","Evaluating Feature Selection Robustness on High-Dimensional Data",2018,"","","","",48,"2022-07-13 09:24:29","","10.1007/978-3-319-92639-1_20","","",,,,,3,0.75,3,1,4,"","",""
10,"Chaochao Chen, Jun Zhou, L. xilinx Wang, Xibin Wu, Wenjing Fang, Jin Tan, Lei Wang, Xiaoxi Ji, A. Liu, Hao Wang, Cheng Hong","When Homomorphic Encryption Marries Secret Sharing: Secure Large-Scale Sparse Logistic Regression and Applications in Risk Control",2020,"","","","",49,"2022-07-13 09:24:29","","10.1145/3447548.3467210","","",,,,,10,5.00,1,11,2,"Logistic Regression (LR) is the most widely used machine learning model in industry for its efficiency, robustness, and interpretability. Due to the problem of data isolation and the requirement of high model performance, many applications in industry call for building a secure and efficient LR model for multiple parties. Most existing work uses either Homomorphic Encryption (HE) or Secret Sharing (SS) to build secure LR. HE based methods can deal with high-dimensional sparse features, but they incur potential security risks. SS based methods have provable security, but they have efficiency issue under high-dimensional sparse features. In this paper, we first present CAESAR, which combines HE and SS to build secure large-scale sparse logistic regression model and achieves both efficiency and security. We then present the distributed implementation of CAESAR for scalability requirement. We have deployed CAESAR in a risk control task and conducted comprehensive experiments. Our experimental results show that CAESAR improves the state-of-the-art model by around 130 times.","",""
7,"B. Zagidullin, Z. Wang, Y. Guan, E. Pitkänen, J. Tang","Comparative analysis of molecular fingerprints in prediction of drug combination effects",2021,"","","","",50,"2022-07-13 09:24:29","","10.1093/bib/bbab291","","",,,,,7,7.00,1,5,1,"Abstract Application of machine and deep learning methods in drug discovery and cancer research has gained a considerable amount of attention in the past years. As the field grows, it becomes crucial to systematically evaluate the performance of novel computational solutions in relation to established techniques. To this end, we compare rule-based and data-driven molecular representations in prediction of drug combination sensitivity and drug synergy scores using standardized results of 14 high-throughput screening studies, comprising 64 200 unique combinations of 4153 molecules tested in 112 cancer cell lines. We evaluate the clustering performance of molecular representations and quantify their similarity by adapting the Centered Kernel Alignment metric. Our work demonstrates that to identify an optimal molecular representation type, it is necessary to supplement quantitative benchmark results with qualitative considerations, such as model interpretability and robustness, which may vary between and throughout preclinical drug development projects.","",""
33,"S. Beetham, J. Capecelatro","Formulating turbulence closures using sparse regression with embedded form invariance",2020,"","","","",51,"2022-07-13 09:24:29","","10.1103/PHYSREVFLUIDS.5.084611","","",,,,,33,16.50,17,2,2,"A data-driven framework for formulation of closures of the Reynolds-Average Navier--Stokes (RANS) equations is presented. In recent years, the scientific community has turned to machine learning techniques to distill a wealth of highly resolved data into improved RANS closures. While the body of work in this area has primarily leveraged Neural Networks (NNs), we alternately leverage a sparse regression framework. This methodology has two important properties: (1) The resultant model is in a closed, algebraic form, allowing for direct physical inferences to be drawn and naive integration into existing computational fluid dynamics solvers, and (2) Galilean invariance can be guaranteed by thoughtful tailoring of the feature space. Our approach is demonstrated for two classes of flows: homogeneous free shear turbulence and turbulent flow over a wavy wall. This work demonstrates equivalent performance to that of modern NNs but with the added benefits of interpretability, increased ease-of-use and dissemination, and robustness to sparse training datasets.","",""
19,"Mohsen Hajiloo, H. Rabiee, Mahdi Anooshahpour","Fuzzy support vector machine: an efficient rule-based classification technique for microarrays",2013,"","","","",52,"2022-07-13 09:24:29","","10.1186/1471-2105-14-S13-S4","","",,,,,19,2.11,6,3,9,"","",""
3,"M. Noack, J. Sethian","Advanced Stationary and Non-Stationary Kernel Designs for Domain-Aware Gaussian Processes",2021,"","","","",53,"2022-07-13 09:24:29","","","","",,,,,3,3.00,2,2,1,"Gaussian process regression is a widely-applied method for function approximation and uncertainty quantification. The technique has gained popularity recently in the machine learning community due to its robustness and interpretability. The mathematical methods we discuss in this paper are an extension of the Gaussian-process framework. We are proposing advanced kernel designs that only allow for functions with certain desirable characteristics to be elements of the reproducing kernel Hilbert space (RKHS) that underlies all kernel methods and serves as the sample space for Gaussian process regression. These desirable characteristics reflect the underlying physics; two obvious examples are symmetry and periodicity constraints. In addition, non-stationary kernel designs can be defined in the same framework to yield flexible multi-task Gaussian processes. We will show the impact of advanced kernel designs on Gaussian processes using several synthetic and two scientific data sets. The results show that including domain knowledge, communicated through advanced kernel designs, has a significant impact on the accuracy and relevance of the function approximation.","",""
3,"Elisabeth Roesch, C. Rackauckas, M. Stumpf","Collocation based training of neural ordinary differential equations",2021,"","","","",54,"2022-07-13 09:24:29","","10.1515/sagmb-2020-0025","","",,,,,3,3.00,1,3,1,"Abstract The predictive power of machine learning models often exceeds that of mechanistic modeling approaches. However, the interpretability of purely data-driven models, without any mechanistic basis is often complicated, and predictive power by itself can be a poor metric by which we might want to judge different methods. In this work, we focus on the relatively new modeling techniques of neural ordinary differential equations. We discuss how they relate to machine learning and mechanistic models, with the potential to narrow the gulf between these two frameworks: they constitute a class of hybrid model that integrates ideas from data-driven and dynamical systems approaches. Training neural ODEs as representations of dynamical systems data has its own specific demands, and we here propose a collocation scheme as a fast and efficient training strategy. This alleviates the need for costly ODE solvers. We illustrate the advantages that collocation approaches offer, as well as their robustness to qualitative features of a dynamical system, and the quantity and quality of observational data. We focus on systems that exemplify some of the hallmarks of complex dynamical systems encountered in systems biology, and we map out how these methods can be used in the analysis of mathematical models of cellular and physiological processes.","",""
3,"Jon Vadillo, Roberto Santana, J. A. Lozano","When and How to Fool Explainable Models (and Humans) with Adversarial Examples",2021,"","","","",55,"2022-07-13 09:24:29","","","","",,,,,3,3.00,1,3,1,"Reliable deployment of machine learning models such as neural networks continues to be challenging due to several limitations. Some of the main shortcomings are the lack of interpretability and the lack of robustness against adversarial examples or out-of-distribution inputs. In this paper, we explore the possibilities and limits of adversarial attacks for explainable machine learning models. First, we extend the notion of adversarial examples to fit in explainable machine learning scenarios, in which the inputs, the output classifications and the explanations of the model’s decisions are assessed by humans. Next, we propose a comprehensive framework to study whether (and how) adversarial examples can be generated for explainable models under human assessment, introducing novel attack paradigms. In particular, our framework considers a wide range of relevant (yet often ignored) factors such as the type of problem, the user expertise or the objective of the explanations in order to identify the attack strategies that should be adopted in each scenario to successfully deceive the model (and the human). These contributions intend to serve as a basis for a more rigorous and realistic study of adversarial examples in the field of explainable machine learning.","",""
3,"Hengtong Zhang, Jing Gao, Lu Su","Data Poisoning Attacks Against Outcome Interpretations of Predictive Models",2021,"","","","",56,"2022-07-13 09:24:29","","10.1145/3447548.3467405","","",,,,,3,3.00,1,3,1,"The past decades have witnessed significant progress towards improving the accuracy of predictions powered by complex machine learning models. Despite much success, the lack of model interpretability prevents the usage of these techniques in life-critical systems such as medical diagnosis and self-driving systems. Recently, the interpretability issue has received much attention, and one critical task is to explain why a predictive model makes a specific decision. We refer to this task as outcome interpretation. Many outcome interpretation methods have been developed to produce human-understandable interpretations by utilizing intermediate results of the machine learning models, such as gradients and model parameters. Although the effectiveness of outcome interpretation approaches has been shown in a benign environment, their robustness against data poisoning attacks (i.e., attacks at the training phase) has not been studied. As the first work towards this direction, we aim to answer an important question: Can training-phase adversarial samples manipulate the outcome interpretation of target samples? To answer this question, we propose a data poisoning attack framework named IMF (Interpretation Manipulation Framework), which can manipulate the interpretations of target samples produced by representative outcome interpretation methods. Extensive evaluations verify the effectiveness and efficiency of the proposed attack strategies on two real-world datasets.","",""
0,"V. Carmona","Experimental analysis of representation learning systems",2018,"","","","",57,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,1,4,"Studying a subject is central to understanding its behavior and what it has learned. In this thesis, we study specific aspects of five representation learning systems for natural language processing tasks. Representation learning systems are a type of machine learning system dedicated to learn representations of data suitable for other machine learning systems, such as classifiers, to operate upon them. Thus, understanding the behavior of and the abilities learned by representation learning systems is crucial for improving the results on the tasks they are used. The aspects on which we focus are interpretability, robustness, and abilities learned. We are interested in obtaining explanations that allow us to understand how a system makes a decision, what factors from the data and internal to the system affect its robustness, and to what extent it has learned a linguistic ability. To do so, we propose to carry out three types of analyses, namely functional, behavioral, and internal analyses which we link with work on the cognitive science, behavioral science, and neuroscience. We present three case studies. In the first study, we provide a functional explanation of a matrix factorization system that allow us to understand how this system makes a prediction. In our second study, we investigate how robust are three systems when the input data suffers a simple transformation and how certain external and internal factors influence their behavior; these systems are trained for the task of natural language inference. Finally, our third study shows that we are able to extract hypernymy from the word embeddings of a popular ReLe system, while studying the influence that the choice of hypernymy dataset plays in the task. In summary, we advance towards better understanding ReLe systems by providing explanations of their predictive behavior and investigating abilities learned by these systems.","",""
2,"Domen Vrevs, Marko Robnik vSikonja","Better sampling in explanation methods can prevent dieselgate-like deception",2021,"","","","",58,"2022-07-13 09:24:29","","","","",,,,,2,2.00,1,2,1,"Machine learning models are used in many sensitive areas where, besides predictive accuracy, their comprehensibility is also important. Interpretability of prediction models is necessary to determine their biases and causes of errors and is a prerequisite for users’ confidence. For complex state-of-the-art black-box models, post-hoc model-independent explanation techniques are an established solution. Popular and effective techniques, such as IME, LIME, and SHAP, use perturbation of instance features to explain individual predictions. Recently, Slack et al. (2020) put their robustness into question by showing that their outcomes can be manipulated due to poor perturbation sampling employed. This weakness would allow dieselgate type cheating of owners of sensitive models who could deceive inspection and hide potentially unethical or illegal biases existing in their predictive models. This could undermine public trust in machine learning models and give rise to legal restrictions on their use. We show that better sampling in these explanation methods prevents malicious manipulations. The proposed sampling uses data generators that learn the training set distribution and generate new perturbation instances much more similar to the training set. We show that the improved sampling increases the LIME and SHAP’s robustness, while the previously untested method IME is already the most robust of all.","",""
2,"K. Yan, Adam P. Harrison","Interpretable Medical Image Classification with Self-Supervised Anatomical Embedding and Prior Knowledge",2021,"","","","",59,"2022-07-13 09:24:29","","","","",,,,,2,2.00,1,2,1,"In medical image analysis tasks, it is important to make machine learning models focus on correct anatomical locations, so as to improve interpretability and robustness of the model. We adopt a latest algorithm called self-supervised anatomical embedding (SAM) to locate point of interest (POI) on computed tomography (CT) scans. SAM can detect arbitrary POI with only one labeled sample needed. Then, we can extract targeted features from the POIs to train a simple prediction model guided by clinical prior knowledge. This approach mimics the practice of human radiologists, thus is interpretable, controllable, and robust. We illustrate our approach on the application of CT contrast phase classification and it outperforms an existing deep learning based method trained on the whole image.","",""
1,"Liwei Chang, Alberto Perez","AlphaFold encodes the principles to identify high affinity peptide binders",2022,"","","","",60,"2022-07-13 09:24:29","","10.1101/2022.03.18.484931","","",,,,,1,1.00,1,2,1,"Machine learning has revolutionized structural biology by solving the problem of predicting structures from sequence information. The community is pushing the limits of interpretability and application of these algorithms beyond their original objective. Already, AlphaFold’s ability to predict bound conformations for complexes has surpassed the performance of docking methods, especially for protein-peptide binding. A key question is the ability of these methods to differentiate binding affinities between several peptides that bind the same receptor. We show a novel application of AlphaFold for competitive binding of different peptides to the same receptor. For systems in which the individual structures of the peptides are well predicted, predictions in which both peptides are introduced capture the stronger binder in the bound state, and the other peptide in the unbound form. The speed and robustness of the method will be a game changer to screen large libraries of peptide sequences to prioritize for detailed experimental characterization.","",""
1,"I. Nejadgholi, Kathleen C. Fraser, Svetlana Kiritchenko","Improving Generalizability in Implicitly Abusive Language Detection with Concept Activation Vectors",2022,"","","","",61,"2022-07-13 09:24:29","","10.48550/arXiv.2204.02261","","",,,,,1,1.00,0,3,1,"Robustness of machine learning models on ever-changing real-world data is critical, especially for applications affecting human well-being such as content moderation. New kinds of abusive language continually emerge in online discussions in response to current events (e.g., COVID-19), and the deployed abuse detection systems should be updated regularly to remain accurate. In this paper, we show that general abusive language classifiers tend to be fairly reliable in detecting out-of-domain explicitly abusive utterances but fail to detect new types of more subtle, implicit abuse. Next, we propose an interpretability technique, based on the Testing Concept Activation Vector (TCAV) method from computer vision, to quantify the sensitivity of a trained model to the human-defined concepts of explicit and implicit abusive language, and use that to explain the generalizability of the model on new data, in this case, COVID-related anti-Asian hate speech. Extending this technique, we introduce a novel metric, Degree of Explicitness, for a single instance and show that the new metric is beneficial in suggesting out-of-domain unlabeled examples to effectively enrich the training data with informative, implicitly abusive texts.","",""
20,"Tyler M. Tomita, J. Browne, Cencheng Shen, Jaewon Chung, Jesse Patsolic, Benjamin Falk, C. Priebe, Jason Yim, R. Burns, M. Maggioni, J. Vogelstein","Sparse Projection Oblique Randomer Forests",2015,"","","","",62,"2022-07-13 09:24:29","","","","",,,,,20,2.86,2,11,7,"Decision forests, including Random Forests and Gradient Boosting Trees, have recently demonstrated state-of-the-art performance in a variety of machine learning settings. Decision forests are typically ensembles of axis-aligned decision trees; that is, trees that split only along feature dimensions. In contrast, many recent extensions to decision forests are based on axis-oblique splits. Unfortunately, these extensions forfeit one or more of the favorable properties of decision forests based on axis-aligned splits, such as robustness to many noise dimensions, interpretability, or computational efficiency. We introduce yet another decision forest, called ""Sparse Projection Oblique Randomer Forests"" (SPORF). SPORF uses very sparse random projections, i.e., linear combinations of a small subset of features. SPORF significantly improves accuracy over existing state-of-the-art algorithms on a standard benchmark suite for classification with >100 problems of varying dimension, sample size, and number of classes. To illustrate how SPORF addresses the limitations of both axis-aligned and existing oblique decision forest methods, we conduct extensive simulated experiments. SPORF typically yields improved performance over existing decision forests, while mitigating computational efficiency and scalability and maintaining interpretability. SPORF can easily be incorporated into other ensemble methods such as boosting to obtain potentially similar gains.","",""
1,"KC Dharma, Chicheng Zhang","Improving the trustworthiness of image classification models by utilizing bounding-box annotations",2021,"","","","",63,"2022-07-13 09:24:29","","","","",,,,,1,1.00,1,2,1,"We study utilizing auxiliary information in training data to improve the trustworthiness of machine learning models. Specifically, in the context of image classification, we propose to optimize a training objective that incorporates bounding box information, which is available in many image classification datasets. Preliminary experimental results show that the proposed algorithm achieves better performance in accuracy, robustness, and interpretability compared with baselines.","",""
1,"Gang Li, Hong-Dong Ma, Rong-Yue Liu, Meng-Di Shen, Ke Zhang","A Two-Stage Hybrid Default Discriminant Model Based on Deep Forest",2021,"","","","",64,"2022-07-13 09:24:29","","10.3390/e23050582","","",,,,,1,1.00,0,5,1,"Background: the credit scoring model is an effective tool for banks and other financial institutions to distinguish potential default borrowers. The credit scoring model represented by machine learning methods such as deep learning performs well in terms of the accuracy of default discrimination, but the model itself also has many shortcomings such as many hyperparameters and large dependence on big data. There is still a lot of room to improve its interpretability and robustness. Methods: the deep forest or multi-Grained Cascade Forest (gcForest) is a decision tree depth model based on the random forest algorithm. Using multidimensional scanning and cascading processing, gcForest can effectively identify and process high-dimensional feature information. At the same time, gcForest has fewer hyperparameters and has strong robustness. So, this paper constructs a two-stage hybrid default discrimination model based on multiple feature selection methods and gcForest algorithm, and at the same time, it optimizes the parameters for the lowest type II error as the first principle, and the highest AUC and accuracy as the second and third principles. GcForest can not only reflect the advantages of traditional statistical models in terms of interpretability and robustness but also take into account the advantages of deep learning models in terms of accuracy. Results: the validity of the hybrid default discrimination model is verified by three real open credit data sets of Australian, Japanese, and German in the UCI database. Conclusions: the performance of the gcForest is better than the current popular single classifiers such as ANN, and the common ensemble classifiers such as LightGBM, and CNNs in type II error, AUC, and accuracy. Besides, in comparison with other similar research results, the robustness and effectiveness of this model are further verified.","",""
1,"Michael Tsang, James Enouen, Yan Liu","Interpretable Artificial Intelligence through the Lens of Feature Interaction",2021,"","","","",65,"2022-07-13 09:24:29","","","","",,,,,1,1.00,0,3,1,"Deep learning, alongside other modern machine learning techniques, has become the state of the art solution for a diverse range of real-world tasks. These include a variety of sensitive applications such as healthcare, finance, autonomous driving, criminal justice, and others which all pose significant concerns for fairness, robustness, safety, and trustworthiness. Despite these applications to critical tasks, deep networks are infamously referred to as black-box models because of their total lack of transparency in decision-making. If we are able to gain insight into how a model is coming to its conclusions, we are able to more clearly assess the trustworthiness and validity of its decisions. Consequently, an abundance of ongoing research is attempting to address model interpretability as the key problem to resolving these issues. There are many methods which are currently used to provide explanations of complex model predictions. LIME (Ribeiro et al., 2016) fits a local linear model around a data point, showing which features positively and negatively influence the prediction results. Despite the overall model being nonlinear, the local model gets an interpretable picture of how the model looks at small scales around the data point. Extensions of this method use other interpretable models like small decision tress. Shapley Values and SHAP follows a similar idea to assign a score to each feature, using a gametheoretic formulation which treats each feature as a player causing the final prediction (Lundberg and Lee, 2017). Its more rigorous formulation yields guarantees of its explanations summing up to the prediction score, but practically it usually must be estimated because of its high computational cost. Shuffle-based feature importance permutes the data of each feature to ascertain its importance in the final prediction in comparison to its normal prediction (Fisher et al., 2018). IG uses the fundamental theorem of calculus to provide additive explanations of a prediction (Sundararajan et al., 2017). This method is very popular in computer vision where its computational efficiency and saliency are prized, even though other work has exposed some of its shortcomings in providing an interpretation (Adebayo et al., 2018). Other methods are specifically designed for computer vision like TCAV (Kim et al., 2018) which finds a ‘concept direction’ corresponding to a large sample of concept images from the user. Surprisingly, all of these most popular interpretability methods share the same one limitation. None of these methods consider the shared importance of groups of two or more features; they only look at the effects had by each of the features individually. A feature interaction between two variables broadly describes a situation where both of the features/ variables are simultaneously important for a model’s prediction. In text applications for sentiment, ”not good” is a very simple example of two words strongly interacting with one another to create a negative sentiment. In modern-day applications, neural networks are usually hailed as amazing function approximators exactly because of their incredible ability to automatically uncover these kinds of complex relationships between the variables of the dataset. In many ways, however,","",""
1,"Nathan Justin, S. Aghaei, A. Gómez, P. Vayanos","Optimal Robust Classification Trees",2021,"","","","",66,"2022-07-13 09:24:29","","","","",,,,,1,1.00,0,4,1,"In many high-stakes domains, the data used to drive machine learning algorithms is noisy (due to e.g., the sensitive nature of the data being collected, limited resources available to validate the data, etc). This may cause a distribution shift to occur, where the distribution of the training data does not match the distribution of the testing data. In the presence of distribution shifts, any trained model can perform poorly in the testing phase. In this paper, motivated by the need for interpretability and robustness, we propose a mixed-integer optimization formulation and a tailored solution algorithm for learning optimal classification trees that are robust to adversarial perturbations in the data features. We evaluate the performance of our approach on numerous publicly available datasets, and compare the performance to a regularized, nonrobust optimal tree. We show an increase of up to 14.16% in worst-case accuracy and increase of up to 4.72% in averagecase accuracy across several data sets and distribution shifts from using our robust solution in comparison to the nonrobust solution.","",""
1,"B. Zagidullin, Z. Wang, Y. Guan, E. Pitkänen, J. Tang","Comparative analysis of molecular representations in prediction of drug combination effects",2021,"","","","",67,"2022-07-13 09:24:29","","10.1101/2021.04.16.439299","","",,,,,1,1.00,0,5,1,"Application of machine and deep learning methods in drug discovery and cancer research has gained a considerable amount of attention in the past years. As the field grows, it becomes crucial to systematically evaluate the performance of novel computational solutions in relation to established techniques. To this end we compare rule-based and data-driven molecular representations in prediction of drug combination sensitivity and drug synergy scores using standardized results of 14 throughput screening studies, comprising 64 200 unique combinations of 4 153 molecules tested in 112 cancer cell lines. We evaluate the clustering performance of molecular representations and quantify their similarity by adapting the Centered Kernel Alignment metric. Our work demonstrates that to identify an optimal molecular representation type it is necessary to supplement quantitative benchmark results with qualitative considerations, such as model interpretability and robustness, which may vary between and throughout preclinical drug development projects.","",""
18,"Luis Oala, J. Fehr, L. Gilli, Pradeep Balachandran, Saul Calderon-Ramirez, D. Li, Gabriel Nobis, E. Alvarado, G. Jaramillo-Gutierrez, C. Matek, Arun Shroff, Ferath Kherif, B. Sanguinetti, Thomas Wiegand, Emily Alsentzer, Matthew B. A. McDermott, Fabian Falck, Suproteem K. Sarkar, Subhrajit Roy, Stephanie L. Hyland","ML4H Auditing: From Paper to Practice",2020,"","","","",68,"2022-07-13 09:24:29","","","","",,,,,18,9.00,2,20,2,"Healthcare systems are currently adapting to digital technologies, producing large quantities of novel data. Based on these data, machine-learning algorithms have been developed to support practitioners in labor-intensive workflows such as diagnosis, prognosis, triage or treatment of disease. However, their translation into medical practice is often hampered by a lack of careful evaluation in different settings. Efforts have started worldwide to establish guidelines for evaluating machine learning for health (ML4H) tools, highlighting the necessity to evaluate models for bias, interpretability, robustness, and possible failure modes. However, testing and adopting these guidelines in practice remains an open challenge. In this work, we target the paper-to-practice gap by applying an ML4H audit framework proposed by the ITU/WHO Focus Group on Artificial Intelligence for Health (FG-AI4H) to three use cases: diagnostic prediction of diabetic retinopathy, diagnostic prediction of Alzheimer’s disease, and cytomorphologic classification for leukemia diagnostics. The assessment comprises dimensions such as bias, interpretability, and robustness. Our results highlight the importance of fine-grained and caseadapted quality assessment, provide support for incorporating proposed quality assessment considerations of ML4H during the entire development life cycle, and suggest improvements for future ML4H reference evaluation frameworks.","",""
0,"Alexander Hvatov, Mikhail Maslyaev, Iana S. Polonskaya, M. Sarafanov, Mark Merezhnikov, Nikolay O. Nikitin","Model-agnostic multi-objective approach for the evolutionary discovery of mathematical models",2021,"","","","",69,"2022-07-13 09:24:29","","10.1007/978-3-030-91885-9_6","","",,,,,0,0.00,0,6,1,"","",""
0,"Jingjing Lu, Shuangyan Yi, Yongsheng Liang, Wei Liu, Jiaoyan Zhao, Qiangqiang Shen","Robust Unsupervised Feature Selection Based on Sparse Reconstruction of Learned Clean Data",2021,"","","","",70,"2022-07-13 09:24:29","","10.1109/CCISP52774.2021.9639338","","",,,,,0,0.00,0,6,1,"Unsupervised feature selection is an essential task in machine learning. Current methods select representative features under slight noise interference. However, their performance is degraded when the data is grossly corrupted. To fundamentally improve the robustness, we propose a robust unsupervised feature selection method based on the sparse reconstruction of learned clean data(SRCD). In this method, we choose the clean data learned by the low-rank constrain as the reconstruction object. Using the noiseless data, SRCD is then capable of obtaining effective features. To further enhance the interpretability, the projection matrix in the reconstruction term is constrained to column sparse patterns for consistent feature selection. Experiment results based on the benchmark and noisy data confirm the effectiveness of the proposed method.","",""
0,"Evan M. Yu, Alan Q. Wang, Adrian V. Dalca, M. Sabuncu","KeypointMorph: Robust Multi-modal A ne Registration via Unsupervised Keypoint Detection",2021,"","","","",71,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,4,1,"Registration is a fundamental task in medical imaging, and recent machine learning methods have become the state-of-the-art. However, these approaches are often not interpretable, lack robustness to large misalignments, and do not incorporate symmetries of the problem. In this work, we propose KeypointMorph, an unsupervised end-to-end learningbased image registration framework that relies on automatically detecting corresponding keypoints. Our core insight is straightforward: matching keypoints between images can be used to obtain the optimal transformation via a di↵erentiable closed-form expression. We use this observation to drive the unsupervised learning of anatomically-consistent keypoints from images. This not only leads to substantially more robust registration but also yields better interpretability, since the keypoints reveal which parts of the image are driving the final alignment. Moreover, KeypointMorph can be designed to be equivariant under image translations and/or symmetric with respect to the input image ordering. We demonstrate the proposed framework in solving 3D a ne registration of multi-modal brain MRI scans. Remarkably, we show that this strategy leads to consistent keypoints, even across modalities. We demonstrate registration accuracy that surpasses current state-of-the-art methods, especially in the context of large displacements. Our code is available at URL1","",""
81,"Yang Young Lu, Yingying Fan, Jinchi Lv, William Stafford Noble","DeepPINK: reproducible feature selection in deep neural networks",2018,"","","","",72,"2022-07-13 09:24:29","","","","",,,,,81,20.25,20,4,4,"Deep learning has become increasingly popular in both supervised and unsupervised machine learning thanks to its outstanding empirical performance. However, because of their intrinsic complexity, most deep learning methods are largely treated as black box tools with little interpretability. Even though recent attempts have been made to facilitate the interpretability of deep neural networks (DNNs), existing methods are susceptible to noise and lack of robustness. Therefore, scientists are justifiably cautious about the reproducibility of the discoveries, which is often related to the interpretability of the underlying statistical models. In this paper, we describe a method to increase the interpretability and reproducibility of DNNs by incorporating the idea of feature selection with controlled error rate. By designing a new DNN architecture and integrating it with the recently proposed knockoffs framework, we perform feature selection with a controlled error rate, while maintaining high power. This new method, DeepPINK (Deep feature selection using Paired-Input Nonlinear Knockoffs), is applied to both simulated and real data sets to demonstrate its empirical utility.","",""
5,"J. Günther, Elias Reichensdorfer, P. Pilarski, K. Diepold","Interpretable PID parameter tuning for control engineering using general dynamic neural networks: An extensive comparison",2019,"","","","",73,"2022-07-13 09:24:29","","10.1371/journal.pone.0243320","","",,,,,5,1.67,1,4,3,"Modern automation systems largely rely on closed loop control, wherein a controller interacts with a controlled process via actions, based on observations. These systems are increasingly complex, yet most deployed controllers are linear Proportional-Integral-Derivative (PID) controllers. PID controllers perform well on linear and near-linear systems but their simplicity is at odds with the robustness required to reliably control complex processes. Modern machine learning techniques offer a way to extend PID controllers beyond their linear control capabilities by using neural networks. However, such an extension comes at the cost of losing stability guarantees and controller interpretability. In this paper, we examine the utility of extending PID controllers with recurrent neural networks—–namely, General Dynamic Neural Networks (GDNN); we show that GDNN (neural) PID controllers perform well on a range of complex control systems and highlight how they can be a scalable and interpretable option for modern control systems. To do so, we provide an extensive study using four benchmark systems that represent the most common control engineering benchmarks. All control environments are evaluated with and without noise as well as with and without disturbances. The neural PID controller performs better than standard PID control in 15 of 16 tasks and better than model-based control in 13 of 16 tasks. As a second contribution, we address the lack of interpretability that prevents neural networks from being used in real-world control processes. We use bounded-input bounded-output stability analysis to evaluate the parameters suggested by the neural network, making them understandable for engineers. This combination of rigorous evaluation paired with better interpretability is an important step towards the acceptance of neural-network-based control approaches for real-world systems. It is furthermore an important step towards interpretable and safely applied artificial intelligence.","",""
1,"Haonan Sun, Luqi Liang, Chunlin Wang, Yi Wu, Fei Yang, M. Rong","Prediction of the Electrical Strength and Boiling Temperature of the Substitutes for Greenhouse Gas SF₆ Using Neural Network and Random Forest",2020,"","","","",74,"2022-07-13 09:24:29","","10.1109/ACCESS.2020.3004519","","",,,,,1,0.50,0,6,2,"Finding substitutes for sulfur hexafluoride (SF6), a gas with extremely high global warming potential, has been a persistent effort for years in the field of high voltage power equipment, which focuses on the evaluation of the electrical strength and boiling temperature for the practical purpose. Following up the previous proposed linear regression models, this work introduces machine learning algorithms including artificial neural network (ANN) and random forest (RF) as the potential approaches to predict the electrical strength and boiling temperature. Based on a series of descriptors derived from the molecular structure of 74 molecules, the performance of three different methods: multiple linear regression, artificial neural network and random forest are compared and assessed in terms of the sensitivity to the sample size, prediction accuracy and stability, and the interpretability of predictors. Considering the available data are limited, random forest shows superior performance with higher robustness and efficiency. The same approaches were applied to the boiling temperature and random forest produced better results as well. Besides, the variable importance ranked by RF improves understanding of the correlation between the molecular properties and electrical strength. It provides important insights to analyze the properties of the SF6 substitutes during the design and synthesis of the new eco-friendly gases in power equipment.","",""
1,"Federico Amato, Fabian Guignard, P. Jacquet, M. Kanevski","On Feature Selection Using Anisotropic General Regression Neural Network",2020,"","","","",75,"2022-07-13 09:24:29","","","","",,,,,1,0.50,0,4,2,"The presence of irrelevant features in the input dataset tends to reduce the interpretability and predictive quality of machine learning models. Therefore, the development of feature selection methods to recognize irrelevant features is a crucial topic in machine learning. Here we show how the General Regression Neural Network used with an anisotropic Gaussian Kernel can be used to perform feature selection. A number of numerical experiments are conducted using simulated data to study the robustness of the proposed methodology and its sensitivity to sample size. Finally, a comparison with four other feature selection methods is performed on several real world datasets.","",""
28,"Yiqun Xie, E. Eftelioglu, Reem Y. Ali, Xun Tang, Yan Li, Ruhi Doshi, S. Shekhar","Transdisciplinary Foundations of Geospatial Data Science",2017,"","","","",76,"2022-07-13 09:24:29","","10.3390/ijgi6120395","","",,,,,28,5.60,4,7,5,"Recent developments in data mining and machine learning approaches have brought lots of excitement in providing solutions for challenging tasks (e.g., computer vision). However, many approaches have limited interpretability, so their success and failure modes are difficult to understand and their scientific robustness is difficult to evaluate. Thus, there is an urgent need for better understanding of the scientific reasoning behind data mining and machine learning approaches. This requires taking a transdisciplinary view of data science and recognizing its foundations in mathematics, statistics, and computer science. Focusing on the geospatial domain, we apply this crucial transdisciplinary perspective to five common geospatial techniques (hotspot detection, colocation detection, prediction, outlier detection and teleconnection detection). We also describe challenges and opportunities for future advancement.","",""
1,"Xiaoxia Wu, Dongqi Yang, Wenyu Zhang, Shuai Zhang","A Hybrid Ensemble Model for Corporate Bankruptcy Prediction Based on Feature Engineering Method",2019,"","","","",77,"2022-07-13 09:24:29","","10.11648/J.IJICS.20190403.12","","",,,,,1,0.33,0,4,3,"The bankruptcy of manufacturing corporates is an important factor affecting economic stability. Corporate bankruptcy has become a hot research topic mainly through financial data analysis and prediction. With the development of data science and artificial intelligence, machine learning technology helps researchers improve the accuracy and robustness of classification models. Ensemble learning, with its strong predictive power and robustness, plays an important role in machine learning and binary classification prediction. In this study, we proposed a bankruptcy classification model combining feature engineering method and ensemble learning method, Synthetic Minority Oversampling Technique (SMOTE) imbalanced data learning algorithm is applied to generate balanced dataset, multi-interval discretization filter is applied to enhance the interpretability of the features and ensemble learning method is applied to get an accurate and objective prediction. To demonstrate the validity and performance of the proposed model, we conducted comparative experiments with ten other baseline classifiers, proving that SMOTE imbalanced learning algorithm and feature engineering method with multi-interval discretization was effective. The comparative experiment results show that the ensemble learning method has a good effect on improving the performance of the proposed model. The final results show that the proposed model has achieved better performance and robustness than other baseline classifiers in terms of classification accuracy, F-measure and Area under Curve (AUC).","",""
13,"S. Saralajew, Lars Holdijk, Maike Rees, T. Villmann","Prototype-based Neural Network Layers: Incorporating Vector Quantization",2018,"","","","",78,"2022-07-13 09:24:29","","","","",,,,,13,3.25,3,4,4,"Neural networks currently dominate the machine learning community and they do so for good reasons. Their accuracy on complex tasks such as image classification is unrivaled at the moment and with recent improvements they are reasonably easy to train. Nevertheless, neural networks are lacking robustness and interpretability. Prototype-based vector quantization methods on the other hand are known for being robust and interpretable. For this reason, we propose techniques and strategies to merge both approaches. This contribution will particularly highlight the similarities between them and outline how to construct a prototype-based classification layer for multilayer networks. Additionally, we provide an alternative, prototype-based, approach to the classical convolution operation. Numerical results are not part of this report, instead the focus lays on establishing a strong theoretical framework. By publishing our framework and the respective theoretical considerations and justifications before finalizing our numerical experiments we hope to jump-start the incorporation of prototype-based learning in neural networks and vice versa.","",""
3,"A. Preece, Daniel Harborne, R. Raghavendra, Richard J. Tomsett, Dave Braines","Provisioning Robust and Interpretable AI/ML-Based Service Bundles",2018,"","","","",79,"2022-07-13 09:24:29","","10.1109/MILCOM.2018.8599838","","",,,,,3,0.75,1,5,4,"Coalition operations environments are characterised by the need to share intelligence, surveillance and reconnaissance services. Increasingly, such services are based on artificial intelligence (AI)and machine learning (ML)technologies. Two key issues in the exploitation of AI/ML services are robustness and interpretability. Employing a diverse portfolio of services can make a system robust to ‘unknown unknowns’. Interpretability - the need for services to offer explanation facilities to engender user trust - can be addressed by a variety of methods to generate either transparent or post hoc explanations according to users' requirements. This paper shows how a service-provisioning framework for coalition operations can be extended to address specific requirements for robustness and interpretability, allowing automatic selection of service bundles for intelligence, surveillance and reconnaissance tasks. The approach is demonstrated in a case study on traffic monitoring featuring a diverse set of AI/ML services based on deep neural networks and heuristic reasoning approaches.","",""
53,"R. Alcalá, J. Alcalá-Fdez, María José Gacto, F. Herrera","Improving fuzzy logic controllers obtained by experts: a case study in HVAC systems",2009,"","","","",80,"2022-07-13 09:24:29","","10.1007/s10489-007-0107-6","","",,,,,53,4.08,13,4,13,"","",""
41,"P. Geurts, N. Touleimat, M. Dutreix, Florence d'Alché-Buc","Inferring biological networks with output kernel trees",2007,"","","","",81,"2022-07-13 09:24:29","","10.1186/1471-2105-8-S2-S4","","",,,,,41,2.73,10,4,15,"","",""
2,"C. Cheng","Random forest training on reconfigurable hardware",2015,"","","","",82,"2022-07-13 09:24:29","","10.25560/28122","","",,,,,2,0.29,2,1,7,"Random Forest (RF) is one of the most widely used supervised learning methods available. An RF is ensemble of decision tree classifiers with injection of several sources of randomness. It demonstrates a set of improvement over single decision and regression trees and is comparable or superior to major classification tools such as support vector machine (SVM) and adaptive boosting (Adaboost) with respect to accuracy, interpretability, robustness and processing speed. RF can be generally divided into training process and predicting process. Recently with emergence of large-scale data mining applications, the RF training process implemented in software on a single computer can no longer induce a complex RF model within reasonable amount of time. Alternative solutions involving computer clusters and GPUs usually come with disadvantages with respect to Performance/Power ratio and are not feasible for portable/embedded applications. In this work a set of FPGA-based implementations of the RF training process are proposed. FPGA devices allow construction of efficient custom hardware architectures and feature lower power consumption than typical GPPs or GPUs therefore are suitable for portable/embedded applications. The proposed hardware training architectures take advantage of different types of inherent parallelism in the RF training algorithm and distribute the workload to a set of parallel workers. Combining the parallel processing techniques with custom hardware designs featuring low latency, the architectures are able to accelerate the training process without loss in accuracy.","",""
2,"A. Murari, J. Vega, D. Mazon, T. Courregelongue","Preliminary numerical investigations of conformal predictors based on fuzzy logic classifiers",2015,"","","","",83,"2022-07-13 09:24:29","","10.1007/s10472-014-9399-5","","",,,,,2,0.29,1,4,7,"","",""
2,"Hsu-Kun Wu, Yih-Lon Lin, J. Hsieh, J. Jeng","Study on semiparametric Wilcoxon fuzzy neural networks",2012,"","","","",84,"2022-07-13 09:24:29","","10.1007/s00500-011-0730-3","","",,,,,2,0.20,1,4,10,"","",""
6,"M. Mramor, Marko Toplak, Gregor Leban, Tomaž Curk, J. Demšar, B. Zupan","On utility of gene set signatures in gene expression-based cancer class prediction",2009,"","","","",85,"2022-07-13 09:24:29","","","","",,,,,6,0.46,1,6,13,"Machine learning methods that can use additional knowledge in their inference process are central to the development of integrative bioinformatics. Inclusion of background knowledge improves robustness, predictive accuracy and interpretability. Recently, a set of such techniques has been proposed that use information on gene sets for supervised data mining of class-labeled microarray data sets. We here present a new gene set-based supervised learning approach named SetSig and systematically investigate the predictive accuracy of this and other gene set approaches compared to the standard inference model where only gene expression information is used. Our results indicate that SetSig outperforms other gene set approaches, but contrary to earlier reports, transformation of gene expression data to the space of gene set signatures does not result in increased accuracy of predictive models when compared to those trained directly from original (not transformed) data.","",""
12,"E. Marchiori, C. Jimenez, Mikkel West-Nielsen, N. Heegaard","Robust SVM-Based Biomarker Selection with Noisy Mass Spectrometric Proteomic Data",2006,"","","","",86,"2022-07-13 09:24:29","","10.1007/11732242_8","","",,,,,12,0.75,3,4,16,"","",""
1,"W. Waegeman, B. Baets, L. Boullart","Integrating Expert Knowledge into Kernel-based Preference Models",2008,"","","","",87,"2022-07-13 09:24:29","","","","",,,,,1,0.07,0,3,14,"In multi-criteria decision making (MCDM) and fuzzy modeling, preference models are typically constructed by interacting with the human decision maker (DM). When the DM experiences difficulties to specify precise for all parameters of the model, inference and elicitation procedures can assist him/her to find a satisfactory model and to assess unlabelled examples. In a related but more statistical way, machine learning algorithms can also infer preference models with similar setups and purposes, but here less interaction with the DM is integrated. We present a hybrid approach that combines the best of both worlds. It consists of a general kernel-based framework for constructing and inferring preference models, in which expert knowledge can be included. Additive models, for which interpretability is preserved, and utility models can be considered as special cases. Besides generality, important benefits of this approach are its robustness to noise and good scalability. We show in detail how this framework can be utilized to aggregate single-criterion outranking relations, resulting in a flexible class of preference models for which domain knowledge can be specified by a DM.","",""
242,"Pantelis Linardatos, Vasilis Papastefanopoulos, S. Kotsiantis","Explainable AI: A Review of Machine Learning Interpretability Methods",2020,"","","","",88,"2022-07-13 09:24:29","","10.3390/e23010018","","",,,,,242,121.00,81,3,2,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.","",""
404,"D. V. Carvalho, E. M. Pereira, Jaime S. Cardoso","Machine Learning Interpretability: A Survey on Methods and Metrics",2019,"","","","",89,"2022-07-13 09:24:29","","10.3390/ELECTRONICS8080832","","",,,,,404,134.67,135,3,3,"Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.","",""
162,"Harsha Nori, Samuel Jenkins, Paul Koch, R. Caruana","InterpretML: A Unified Framework for Machine Learning Interpretability",2019,"","","","",90,"2022-07-13 09:24:29","","","","",,,,,162,54.00,41,4,3,"InterpretML is an open-source Python package which exposes machine learning interpretability algorithms to practitioners and researchers. InterpretML exposes two types of interpretability - glassbox models, which are machine learning models designed for interpretability (ex: linear models, rule lists, generalized additive models), and blackbox explainability techniques for explaining existing systems (ex: Partial Dependence, LIME). The package enables practitioners to easily compare interpretability algorithms by exposing multiple methods under a unified API, and by having a built-in, extensible visualization platform. InterpretML also includes the first implementation of the Explainable Boosting Machine, a powerful, interpretable, glassbox model that can be as accurate as many blackbox models. The MIT licensed source code can be downloaded from github.com/microsoft/interpret.","",""
2,"Yousef Sheikhi Garjan, Mehdi Ghaneezabadi","Machine Learning Interpretability Application to Optimize Well Completion in Montney",2020,"","","","",91,"2022-07-13 09:24:29","","10.2118/200019-ms","","",,,,,2,1.00,1,2,2,"Recently machine learning has being extensively deployed for oil and gas industry for improving result and expedite process. However, the black box models do not explain their prediction which considered as a barrier to adopt machine learning. This paper is about optimizing hydraulic fracture with machine learning methods and making informative decision with interpreting machine learning model. The solution can show that it could save over million dollars per well and improve well performance significantly. Interestingly, the machine leaning explainability approach was utilized to explain and measure the reason behind of why some wells are performing better than other and vice versa.Hydraulic fracturing modeling and optimization in tight oil and unconventional reservoir requires substantial geological modeling, fracture design, post-fracture production simulation with excessive sensitivity analysis due to complexity and uncertainty in the nature of data. These types of studies are computationally and monetarily expensive. Furthermore, digital oil technology has facilitated the process of data gathering enabled operators to have access to huge amount of data. Common approaches are no longer suitable to handle this pile of data but machine learning methods could be successfully utilized for this purpose.In this paper, a variety types of advanced machine learning methods including linear regression, Random forest, Gradient Boost, XGBoost, Bagging, ExtraTrees and neural network were employed to optimize well completion in Montney formation. The objective was to create a robust predictive model capturing all the effective operational well parameters (features) capable of optimizing the first 12 months cumulative of equivalent well production.Special Individual Conditional Expectation (ICE) plots and Partial Dependency plots(PDP) were used to depict how HF completion features influence the prediction of a machine learning model. Furthermore, a novel approach was employed to explain the model prediction of an existing well by computing the contribution of each feature to the prediction.Over 1838 hydraulically fractured (HF) wells producing from 2008 till 2019 in Montney formation have been considered for this analysis. The outcome of Explanatory Data Analysis (EDA) revealed that well production performance has not been improved despite of continues enhancement of hydraulic fracture parameters such as proppant injected volume, length of stimulated horizontal wells, and number of stages per well in the course of two years. This finding raises the concern of whether operators are properly optimizing completion design. After comparing all machine learning methods, Random Forest method was chosen as the most appropriate and accurate method to proceed for further analysis. ICE and PDP plots helped to understand the impacts of different fracturing features on production for individual well in addition to define optimum operation features on Montney Formation. Furthermore, quantifying of each feature’s impact on individual well production and linking it to an economic model, we were able to demonstrate potential profit and loss for each well. The model suggests that some wells could have achieved over $1 million extra profit during the first 12-months of production.In this study, not only a reliable predictive data-driven model has been built for hydraulically-fractured wells in Montney formation, but also a comprehensive workflow of sensitivity and explainatability analysis has been introduced to obtain an optimized fit-to-purpose well completion design.","",""
37,"Radwa El Shawi, Youssef Mohamed, M. Al-mallah, S. Sakr","Interpretability in HealthCare A Comparative Study of Local Machine Learning Interpretability Techniques",2019,"","","","",92,"2022-07-13 09:24:29","","10.1109/CBMS.2019.00065","","",,,,,37,12.33,9,4,3,"Although complex machine learning models (e.g., Random Forest, Neural Networks) are commonly outperforming the traditional simple interpretable models (e.g., Linear Regression, Decision Tree), in the healthcare domain, clinicians find it hard to understand and trust these complex models due to the lack of intuition and explanation of their predictions. With the new General Data Protection Regulation (GDPR), the importance for plausibility and verifiability of the predictions made by machine learning models has become essential. To tackle this challenge, recently, several machine learning interpretability techniques have been developed and introduced. In general, the main aim of these interpretability techniques is to shed light and provide insights into the predictions process of the machine learning models and explain how the model predictions have resulted. However, in practice, assessing the quality of the explanations provided by the various interpretability techniques is still questionable. In this paper, we present a comprehensive experimental evaluation of three recent and popular local model agnostic interpretability techniques, namely, LIME, SHAP and Anchors on different types of real-world healthcare data. Our experimental evaluation covers different aspects for its comparison including identity, stability, separability, similarity, execution time and bias detection. The results of our experiments show that LIME achieves the lowest performance for the identity metric and the highest performance for the separability metric across all datasets included in this study. On average, SHAP has the smallest average time to output explanation across all datasets included in this study. For detecting the bias, SHAP enables the participants to better detect the bias.","",""
1,"T. Welchowski, K. Maloney, R. Mitchell, M. Schmid","Techniques to Improve Ecological Interpretability of Black-Box Machine Learning Models",2021,"","","","",93,"2022-07-13 09:24:29","","10.1007/s13253-021-00479-7","","",,,,,1,1.00,0,4,1,"","",""
932,"Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael A. Specter, Lalana Kagal","Explaining Explanations: An Overview of Interpretability of Machine Learning",2018,"","","","",94,"2022-07-13 09:24:29","","10.1109/DSAA.2018.00018","","",,,,,932,233.00,155,6,4,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.","",""
62,"Philipp Schmidt, F. Biessmann","Quantifying Interpretability and Trust in Machine Learning Systems",2019,"","","","",95,"2022-07-13 09:24:29","","","","",,,,,62,20.67,31,2,3,"Decisions by Machine Learning (ML) models have become ubiquitous. Trusting these decisions requires understanding how algorithms take them. Hence interpretability methods for ML are an active focus of research. A central problem in this context is that both the quality of interpretability methods as well as trust in ML predictions are difficult to measure. Yet evaluations, comparisons and improvements of trust and interpretability require quantifiable measures. Here we propose a quantitative measure for the quality of interpretability methods. Based on that we derive a quantitative measure of trust in ML decisions. Building on previous work we propose to measure intuitive understanding of algorithmic decisions using the information transfer rate at which humans replicate ML model predictions. We provide empirical evidence from crowdsourcing experiments that the proposed metric robustly differentiates interpretability methods. The proposed metric also demonstrates the value of interpretability for ML assisted human decision making: in our experiments providing explanations more than doubled productivity in annotation tasks. However unbiased human judgement is critical for doctors, judges, policy makers and others. Here we derive a trust metric that identifies when human decisions are overly biased towards ML predictions. Our results complement existing qualitative work on trust and interpretability by quantifiable measures that can serve as objectives for further improving methods in this field of research.","",""
0,"Mimansa Jaiswal","Interpreting Multimodal Machine Learning Models Trained for Emotion Recognition to Address Robustness and Privacy Concerns",2020,"","","","",96,"2022-07-13 09:24:29","","10.1609/aaai.v34i10.7130","","",,,,,0,0.00,0,1,2,"Many mobile applications and virtual conversational agents now aim to recognize and adapt to emotions. These predicted emotions are used in variety of downstream applications: (a) generating more human like dialogues, (b) predicting mental health issues, and (c) hate speech detection and intervention. To enable this, data are transmitted from users' devices and stored on central servers. These data are then processed further, either annotated or used as inputs for training a model for a specific task. Yet, these data contain sensitive information that could be used by mobile applications without user's consent or, maliciously, by an eavesdropping adversary. My work focuses on two major issues that are faced while training emotion recognition algorithms: (a) privacy of the generated representations and, (b) explaining and ensuring that the predictions are robust to various situations. Tackling these issues would lead to emotion based algorithms that are deployable and helpful at a larger scale, thus enabling more human like experience when interacting with AI.","",""
0,"M. Barandas, Duarte Folgado, Ricardo Santos, Raquel Simão, Hugo Gamboa","Uncertainty-Based Rejection in Machine Learning: Implications for Model Development and Interpretability",2022,"","","","",97,"2022-07-13 09:24:29","","10.3390/electronics11030396","","",,,,,0,0.00,0,5,1,"Uncertainty is present in every single prediction of Machine Learning (ML) models. Uncertainty Quantification (UQ) is arguably relevant, in particular for safety-critical applications. Prior research focused on the development of methods to quantify uncertainty; however, less attention has been given to how to leverage the knowledge of uncertainty in the process of model development. This work focused on applying UQ into practice, closing the gap of its utility in the ML pipeline and giving insights into how UQ is used to improve model development and its interpretability. We identified three main research questions: (1) How can UQ contribute to choosing the most suitable model for a given classification task? (2) Can UQ be used to combine different models in a principled manner? (3) Can visualization techniques improve UQ’s interpretability? These questions are answered by applying several methods to quantify uncertainty in both a simulated dataset and a real-world dataset of Human Activity Recognition (HAR). Our results showed that uncertainty quantification can increase model robustness and interpretability.","",""
439,"Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin","Model-Agnostic Interpretability of Machine Learning",2016,"","","","",98,"2022-07-13 09:24:29","","","","",,,,,439,73.17,146,3,6,"Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.","",""
13,"Xinlei Mi, Baiming Zou, F. Zou, J. Hu","Permutation-based identification of important biomarkers for complex diseases via machine learning models",2021,"","","","",99,"2022-07-13 09:24:29","","10.1038/s41467-021-22756-2","","",,,,,13,13.00,3,4,1,"","",""
14,"Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, D. Dou","Interpretable Deep Learning: Interpretations, Interpretability, Trustworthiness, and Beyond",2021,"","","","",100,"2022-07-13 09:24:29","","","","",,,,,14,14.00,2,8,1,"Deep neural networks have been well-known for their superb performance in handling various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal the ways that deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we introduce and clarify two basic concepts— interpretations and interpretability—that people usually get confused. First of all, to address the research efforts in interpretations, we elaborate the design of several recent interpretation algorithms, from different perspectives, through proposing a new taxonomy. Then, to understand the results of interpretation, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the existing work in evaluating models’ interpretability using “trustworthy” interpretation algorithms. Finally, we review and discuss the connections between deep models’ interpretations and other factors, such as adversarial robustness and data augmentations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.","",""
11,"Markus Jaeger, Stephan Krügel, D. Marinelli, J. Papenbrock, P. Schwendner","Interpretable Machine Learning for Diversified Portfolio Construction",2020,"","","","",101,"2022-07-13 09:24:29","","10.2139/ssrn.3730144","","",,,,,11,5.50,2,5,2,"In this article, the authors construct a pipeline to benchmark hierarchical risk parity (HRP) relative to equal risk contribution (ERC) as examples of diversification strategies allocating to liquid multi-asset futures markets with dynamic leverage (volatility target). The authors use interpretable machine learning concepts (explainable AI) to compare the robustness of the strategies and to back out implicit rules for decision-making. The empirical dataset consists of 17 equity index, government bond, and commodity futures markets across 20 years. The two strategies are back tested for the empirical dataset and for about 100,000 bootstrapped datasets. XGBoost is used to regress the Calmar ratio spread between the two strategies against features of the bootstrapped datasets. Compared to ERC, HRP shows higher Calmar ratios and better matches the volatility target. Using Shapley values, the Calmar ratio spread can be attributed especially to univariate drawdown measures of the asset classes. TOPICS: Quantitative methods, statistical methods, big data/machine learning, portfolio construction, performance measurement Key Findings ▪ The authors introduce a procedure to benchmark rule-based investment strategies and to explain the differences in path-dependent risk-adjusted performance measures using interpretable machine learning. ▪ They apply the procedure to the Calmar ratio spread between hierarchical risk parity (HRP) and equal risk contribution (ERC) allocations of a multi-asset futures portfolio and find HRP to have superior risk-adjusted performance. ▪ The authors regress the Calmar ratio spread against statistical features of bootstrapped futures return datasets using XGBoost and apply the SHAP framework by Lundberg and Lee (2017) to discuss the local and global feature importance.","",""
6,"S. Kalinin, K. Roccapriore, S. Cho, D. Milliron, R. Vasudevan, M. Ziatdinov, J. Hachtel","Separating Physically Distinct Mechanisms in Complex Infrared Plasmonic Nanostructures via Machine Learning Enhanced Electron Energy Loss Spectroscopy",2020,"","","","",102,"2022-07-13 09:24:29","","10.1002/adom.202001808","","",,,,,6,3.00,1,7,2,"Electron energy loss spectroscopy (EELS) enables direct exploration of plasmonic phenomena at the nanometer level. To isolate individual plasmon modes, linear unmixing methods can be used to separate different physical mechanisms, but in larger and more complex systems the interpretability of the components becomes uncertain. Here, infrared plasmonic resonances in self‐assembled heterogeneous monolayer films of doped‐semiconductor nanoparticles are examined beyond linear unmixing techniques, and both supervised and unsupervised machine‐learning‐based analyses of hyperspectral EELS datasets are demonstrated. In the supervised approach, a human operator labels a small number of pixels in the hyperspectral dataset corresponding to features of interest which are then propagated across the entire dataset. In the unsupervised approach, non‐linear autoencoders are used to create a highly‐reduced latent‐space representation of the dataset, within which insight into the relevant physics can be gleaned from straightforward distance metrics that do not depend on operator input and bias. The advantage of these approaches is that the labeling separates physical mechanisms without altering the data, enabling robust analyses of the influence of heterogeneities in mesoscale complex systems.","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",103,"2022-07-13 09:24:29","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
48,"Clemens Stachl, F. Pargent, S. Hilbert, Gabriella M. Harari, Ramona Schoedel, Sumer S. Vaid, S. Gosling, M. Bühner","Personality Research and Assessment in the Era of Machine Learning",2019,"","","","",104,"2022-07-13 09:24:29","","10.1002/per.2257","","",,,,,48,16.00,6,8,3,"The increasing availability of high–dimensional, fine–grained data about human behaviour, gathered from mobile sensing studies and in the form of digital footprints, is poised to drastically alter the way personality psychologists perform research and undertake personality assessment. These new kinds and quantities of data raise important questions about how to analyse the data and interpret the results appropriately. Machine learning models are well suited to these kinds of data, allowing researchers to model highly complex relationships and to evaluate the generalizability and robustness of their results using resampling methods. The correct usage of machine learning models requires specialized methodological training that considers issues specific to this type of modelling. Here, we first provide a brief overview of past studies using machine learning in personality psychology. Second, we illustrate the main challenges that researchers face when building, interpreting, and validating machine learning models. Third, we discuss the evaluation of personality scales, derived using machine learning methods. Fourth, we highlight some key issues that arise from the use of latent variables in the modelling process. We conclude with an outlook on the future role of machine learning models in personality research and assessment.","",""
5,"Francesco Regazzoni, D. Chapelle, P. Moireau","Combining data assimilation and machine learning to build data‐driven models for unknown long time dynamics—Applications in cardiovascular modeling",2021,"","","","",105,"2022-07-13 09:24:29","","10.1002/cnm.3471","","",,,,,5,5.00,2,3,1,"We propose a method to discover differential equations describing the long‐term dynamics of phenomena featuring a multiscale behavior in time, starting from measurements taken at the fast‐scale. Our methodology is based on a synergetic combination of data assimilation (DA), used to estimate the parameters associated with the known fast‐scale dynamics, and machine learning (ML), used to infer the laws underlying the slow‐scale dynamics. Specifically, by exploiting the scale separation between the fast and the slow dynamics, we propose a decoupling of time scales that allows to drastically lower the computational burden. Then, we propose a ML algorithm that learns a parametric mathematical model from a collection of time series coming from the phenomenon to be modeled. Moreover, we study the interpretability of the data‐driven models obtained within the black‐box learning framework proposed in this paper. In particular, we show that every model can be rewritten in infinitely many different equivalent ways, thus making intrinsically ill‐posed the problem of learning a parametric differential equation starting from time series. Hence, we propose a strategy that allows to select a unique representative model in each equivalence class, thus enhancing the interpretability of the results. We demonstrate the effectiveness and noise‐robustness of the proposed methods through several test cases, in which we reconstruct several differential models starting from time series generated through the models themselves. Finally, we show the results obtained for a test case in the cardiovascular modeling context, which sheds light on a promising field of application of the proposed methods.","",""
4,"Rahul Singh","A Finite Sample Theorem for Longitudinal Causal Inference with Machine Learning: Long Term, Dynamic, and Mediated Effects",2021,"","","","",106,"2022-07-13 09:24:29","","","","",,,,,4,4.00,4,1,1,"I construct and justify confidence intervals for longitudinal causal parameters estimated with machine learning. Longitudinal parameters include long term, dynamic, and mediated effects. I provide a nonasymptotic theorem for any longitudinal causal parameter estimated with any machine learning algorithm that satisfies a few simple, interpretable conditions. The main result encompasses local parameters defined for specific demographics as well as proximal parameters defined in the presence of unobserved confounding. Formally, I prove consistency, Gaussian approximation, and semiparametric efficiency. The rate of convergence is n for global parameters, and it degrades gracefully for local parameters. I articulate a simple set of conditions to translate mean square rates into statistical inference. A key feature of the main result is a new multiple robustness to ill posedness for proximal causal inference in longitudinal settings.","",""
3,"F. Biessmann, D. Refiano","Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated",2021,"","","","",107,"2022-07-13 09:24:29","","","","",,,,,3,3.00,2,2,1,"The field explainable artificial intelligence (XAI) has brought about an arsenal of methods to render Machine Learning (ML) predictions more interpretable. But how useful explanations provided by transparent ML methods are for humans remains difficult to assess. Here we investigate the quality of interpretable computer vision algorithms using techniques from psychophysics. In crowdsourced annotation tasks we study the impact of different interpretability approaches on annotation accuracy and task time. We compare these quality metrics with classical XAI, automated quality metrics. Our results demonstrate that psychophysical experiments allow for robust quality assessment of transparency in machine learning. Interestingly the quality metrics computed without humans in the loop did not provide a consistent ranking of interpretability methods nor were they representative for how useful an explanation was for humans. These findings highlight the potential of methods from classical psychophysics for modern machine learning applications. We hope that our results provide convincing arguments for evaluating interpretability in its natural habitat, human-ML interaction, if the goal is to obtain an authentic assessment of interpretability.","",""
1,"P. Benner, A. Klawonn, M. Stoll","Topical Issue Scientific Machine Learning (1/2)",2021,"","","","",108,"2022-07-13 09:24:29","","10.1002/gamm.202100005","","",,,,,1,1.00,0,3,1,"Scientific Machine Learning is a rapidly evolving field of research that combines and further develops techniques of scientific computing and machine learning. Special emphasis is given to the scientific (physical, chemical, biological, etc.) interpretability of models learned from data and their usefulness for robust predictions. On the other hand, this young field also investigates the utilization of Machine Learning methods for improving numerical algorithms in Scientific Computing. The name Scientific Machine Learning has been coined at a Basic Research Needs Workshop of the US Department of Energy (DOE) in January, 2018. It resulted in a report [2] published in February, 2019; see also [1] for a short brochure on this topic. The present special issue of the GAMM Mitteilungen, which is the first of a two-part series, contains contributions on the topic of Scientific Machine Learning in the context of complex applications across the sciences and engineering. Research in this new exciting field needs to address challenges such as complex physics, uncertain parameters, and possibly limited data through the development of new methods that combine algorithms from computational science and engineering and from numerical analysis with state of the art techniques from machine learning. At the GAMM Annual Meeting 2019, the activity group Computational and Mathematical Methods in Data Science (CoMinDS) has been established. Meanwhile, it has become a meeting place for researchers interested in all aspects of data science. All three editors of this special issue are founding members of this activity group. Because of the rapid development both in the theoretical foundations and the applicability of Scientific Machine Learning techniques, it is time to highlight developments within the field in the hope that it will become an essential domain within the GAMM and topical issues like this will have a frequent occurrence within this journal. We are happy that eight teams of authors have accepted our invitation to report on recent research highlights in Scientific Machine Learning, and to point out the relevant literature as well as software. The four papers in this first part of the special issue are: • Stoll, Benner: Machine Learning for Material Characterization with an Application for Predicting Mechanical Properties. This work explores the use of machine learning techniques for material property prediction. Given the abundance of data available in industrial applications, machine learning methods can help finding patterns in the data and the authors focus on the case of the small punch test and tensile data for illustration purposes. • Beck, Kurz: A Perspective on Machine Modelling Learning Methods in Turbulence. Turbulence modelling remains a humongous challenge in the simulation and analysis of complex flows. The authors review the use of data-driven techniques to open up new ways for studying turbulence and focus on the challenges and opportunities that machine learning brings to this field. • Heinlein, Klawonn, Lanser, Weber: Combining Machine Learning and Domain Decomposition Methods for the Solution of Partial Differential Equations – A Review. Domain decomposition (DD) has been a workhorse of solving complex simulation tasks. The authors review the combination of machine learning approaches with state-of-the-art DD-schemes. Their focus is on the use of ML techniques to improve the computational effort of adaptive domain decomposition schemes and the use of novel ML methods for the discretization and solution of subdomain problems. • Budd, van Gennip, Latz: Classification and image processing with a semi-discrete scheme for fidelity forced Allen–Cahn on graphs. Learning based on graphs provides exciting possibilities for discovering and using additional structure in data. In this work, the authors illustrate the use of a PDE-based learning technique relying on the graph Allen-Cahn equation for the segmentation of images. The authors illustrate that computational and mathematical advances can lead to efficiency and accuracy gains. Peter Benner1,2 Axel Klawonn3,4 Martin Stoll5","",""
2,"Artur Movsessian, D. Cava, D. Tcherniak","Interpretable machine learning in damage detection using Shapley Additive Explanations",2021,"","","","",109,"2022-07-13 09:24:29","","10.31224/osf.io/96yf5","","",,,,,2,2.00,1,3,1,"In recent years, Machine Learning (ML) techniques have gained popularity in Structural Health Monitoring (SHM). These have been particularly used for damage detection in a wide range of engineering applications such as wind turbine blades. The outcomes of previous research studies in this area have demonstrated the capabilities of ML for robust damage detection. However, the primary challenge facing ML in SHM is the lack of interpretability of the prediction models hindering the broader implementation of these techniques. For this purpose, this study integrates the novel Shapley Additive exPlanations (SHAP) method into a ML-based damage detection process as a tool for introducing interpretability and, thus, build evidence for reliable decision-making in SHM applications. The SHAP method is based on coalitional game theory and adds global and local interpretability to ML-based models by computing the marginal contribution of each feature. The contribution is used to understand the nature of damage indices (DIs). The applicability of the SHAP method is first demonstrated on a simple lumped mass-spring-damper system with simulated temperature variabilities. Later, the SHAP method has been evaluated on data from an in-operation V27 wind turbine with artificially introduced damage in one of its blades. The results show the relationship between the environmental and operational variabilities (EOVs) and their direct influence on the damage indices. This ultimately helps to understand the difference between false positives caused by EOVs and true positives resulting from damage in the structure.","",""
3,"A. Heinlein, A. Klawonn, M. Lanser, J. Weber","Combining Machine Learning and Adaptive Coarse Spaces---A Hybrid Approach for Robust FETI-DP Methods in Three Dimensions",2020,"","","","",110,"2022-07-13 09:24:29","","10.1137/20m1344913","","",,,,,3,1.50,1,4,2,"The hybrid ML-FETI-DP algorithm combines the advantages of adaptive coarse spaces in domain decomposition methods and certain supervised machine learning techniques. Adaptive coarse spaces ensure robustness of highly scalable domain decomposition solvers, even for highly heterogeneous coefficient distributions with arbitrary coefficient jumps. However, their construction requires the setup and solution of local generalized eigenvalue problems, which is typically computationally expensive. The idea of ML-FETI-DP is to interpret the coefficient distribution as image data and predict whether an eigenvalue problem has to be solved or can be neglected while still maintaining robustness of the adaptive FETI-DP method. For this purpose, neural networks are used as image classifiers. In the present work, the ML-FETI-DP algorithm is extended to three dimensions, which requires both a complex data preprocessing procedure to construct consistent input data for the neural network as well as a representative training and validation data set to ensure generalization properties of the machine learning model. Numerical experiments for stationary diffusion and linear elasticity problems with realistic coefficient distributions show that a large number of eigenvalue problems can be saved; in the best case of the numerical results presented here, 97% of the eigenvalue problems can be avoided to be set up and solved.","",""
1,"Yusuke Kawamoto","An Epistemic Approach to the Formal Specification of Statistical Machine Learning",2020,"","","","",111,"2022-07-13 09:24:29","","10.1007/S10270-020-00825-2","","",,,,,1,0.50,1,1,2,"","",""
1,"Keyang Cheng, Ning Wang, Maozhen Li","Interpretability of Deep Learning: A Survey",2020,"","","","",112,"2022-07-13 09:24:29","","10.1007/978-3-030-70665-4_54","","",,,,,1,0.50,0,3,2,"","",""
0,"P. Rasouli, Ingrid Chieh Yu","Explainable Debugger for Black-box Machine Learning Models",2021,"","","","",113,"2022-07-13 09:24:29","","10.1109/IJCNN52387.2021.9533944","","",,,,,0,0.00,0,2,1,"The research around developing methods for debugging and refining Machine Learning (ML) models is still in its infancy. We believe employing tailored tools in the development process can help developers in creating more trustworthy and reliable models. This is particularly essential for creating black-box models such as deep neural networks and random forests, as their opaque decision-making and complex structure prevent detailed investigations. Although many explanation techniques provide interpretability in terms of predictive features for a mispredicted instance, it would be beneficial for a developer to find a partition of the training data that significantly influences the anomaly. Such responsible partitions can be subjected to data visualization and data engineering in the development phase to improve the model's accuracy. In this paper, we propose a systematic debugging framework for the development of ML models that guides the data engineering process using the model's decision boundary. Our approach finds the influential neighborhood of anomalous data points using observation-level feature importance and explains them via a novel quasi-global explanation technique. It is also equipped with a robust global explanation approach to reveal general trends and expose potential biases in the neighborhoods. We demonstrate the efficacy of the devised framework through several experiments on standard data sets and black-box models and propose various guidelines on how the framework's components can be practically useful from a developer's perspective.","",""
0,"James Dean, M. Scheffler, Thomas A. R. Purcell, S. Barabash, Rahul Bhowmik, T. Bazhirov","Interpretable Machine Learning for Materials Design",2021,"","","","",114,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,6,1,"Fueled by the widespread adoption of Machine Learning and the high-throughput screening of materials, the data-centric approach to materials design has asserted itself as a robust and powerful tool for the in-silico prediction of materials properties. When training models to predict material properties, researchers often face a difficult choice between a model’s interpretability or its performance. We study this trade-off by leveraging four different state-of-the-art Machine Learning techniques: XGBoost, SISSO, Roost, and TPOT for the prediction of structural and electronic properties of perovskites and 2D materials. We then assess the future outlook of the continued integration of Machine Learning into materials discovery, and identify key problems that will continue to challenge researchers as the size of the literature’s datasets and complexity of models increases. Finally, we offer several possible solutions to these challenges with a focus on retaining interpretability, and share our thoughts on magnifying the impact of Machine Learning on materials design.","",""
0,"Ying Li, Qinglai Yu, Ming Xie, Zhenduo Zhang, Zhanjun Ma, Kai Cao","Identifying Oil Spill Types Based on Remotely Sensed Reflectance Spectra and Multiple Machine Learning Algorithms",2021,"","","","",115,"2022-07-13 09:24:29","","10.1109/JSTARS.2021.3109951","","",,,,,0,0.00,0,6,1,"An accurate identification of oil spill types is the basis of determining the source of leakage, evaluating the potential damage, and deciding a plan of responses for an oil spill event. Despite sufficient studies that interpreted and analyzed hyperspectral data of oil spills, these studies that identify or classify oil spill types is rather limited. Aiming at identifying different types of oil spills, this article analyses the reflectance spectra obtained from high-resolution hyperspectral sensors using multiple machine learning methods. Four types of machine learning models are applied in this article: random forest; support vector machine (SVM); and deep neural network (DNN); and DNN with differential pooling (DP-DNN). The training and testing data are collected by field experiments under different environmental condition in order to verify the robustness of the machine learning models. The characteristics of reflectance is briefly described, and the results conform with results from previous studies. The performances of the machine learning models are evaluated and compared in terms of both accuracy of prediction and computational complexity. The results indicate that the two DNN models are able to achieve the most accurate prediction among the four machine learning models at the cost of more computation. The SVM model, or the proposed DP-DNN model may be a favorable choice when training time is limited.","",""
0,"Katrin Sophie Bohnsack, M. Kaden, Julia Abel, S. Saralajew, T. Villmann","The Resolved Mutual Information Function as a Structural Fingerprint of Biomolecular Sequences for Interpretable Machine Learning Classifiers",2021,"","","","",116,"2022-07-13 09:24:29","","10.3390/e23101357","","",,,,,0,0.00,0,5,1,"In the present article we propose the application of variants of the mutual information function as characteristic fingerprints of biomolecular sequences for classification analysis. In particular, we consider the resolved mutual information functions based on Shannon-, Rényi-, and Tsallis-entropy. In combination with interpretable machine learning classifier models based on generalized learning vector quantization, a powerful methodology for sequence classification is achieved which allows substantial knowledge extraction in addition to the high classification ability due to the model-inherent robustness. Any potential (slightly) inferior performance of the used classifier is compensated by the additional knowledge provided by interpretable models. This knowledge may assist the user in the analysis and understanding of the used data and considered task. After theoretical justification of the concepts, we demonstrate the approach for various example data sets covering different areas in biomolecular sequence analysis.","",""
2,"J. de Nijs, T. J. Burger, Ronald J. Janssen, S. M. Kia, Daniel P J van Opstal, M. D. de Koning, L. de Haan, Behrooz Z. Agna A. Nico J. Richard Lieuwe Philippe Jurjen  Alizadeh Bartels-Velthuis van Beveren Bruggeman de, B. Alizadeh, A. Bartels-Velthuis, N. V. van Beveren, R. Bruggeman, P. Delespaul, J. Luykx, I. Myin-Germeys, R. Kahn, F. Schirmbeck, C. Simons, T. van Amelsvoort, J. van os, R. van Winkel, W. Cahn, H. Schnack","Individualized prediction of three- and six-year outcomes of psychosis in a longitudinal multicenter study: a machine learning approach",2021,"","","","",117,"2022-07-13 09:24:29","","10.1038/s41537-021-00162-3","","",,,,,2,2.00,0,23,1,"","",""
2,"Weishen Pan, Changshui Zhang","The Definitions of Interpretability and Learning of Interpretable Models",2021,"","","","",118,"2022-07-13 09:24:29","","","","",,,,,2,2.00,1,2,1,"As machine learning algorithms getting adopted in an ever-increasing number of applications, interpretation has emerged as a crucial desideratum. In this paper, we propose a mathematical definition for the humaninterpretable model. In particular, we define interpretability between two information process systems. If a prediction model is interpretable by a human recognition system based on the above interpretability definition, the prediction model is defined as a completely human-interpretable model. We further design a practical framework to train a completely human-interpretable model by user interactions. Experiments on image datasets show the advantages of our proposed model in two aspects: 1) The completely human-interpretable model can provide an entire decisionmaking process that is human-understandable; 2) The completely humaninterpretable model is more robust against adversarial attacks.","",""
1973,"Finale Doshi-Velez, Been Kim","Towards A Rigorous Science of Interpretable Machine Learning",2017,"","","","",119,"2022-07-13 09:24:29","","","","",,,,,1973,394.60,987,2,5,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.","",""
506,"W. James Murdoch, Chandan Singh, Karl Kumbier, R. Abbasi-Asl, Bin Yu","Definitions, methods, and applications in interpretable machine learning",2019,"","","","",120,"2022-07-13 09:24:29","","10.1073/pnas.1900654116","","",,,,,506,168.67,101,5,3,"Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.","",""
3,"Numair Sani, Jaron J. R. Lee, Razieh Nabi, I. Shpitser","A Semiparametric Approach to Interpretable Machine Learning",2020,"","","","",121,"2022-07-13 09:24:29","","","","",,,,,3,1.50,1,4,2,"Black box models in machine learning have demonstrated excellent predictive performance in complex problems and high-dimensional settings. However, their lack of transparency and interpretability restrict the applicability of such models in critical decision-making processes. In order to combat this shortcoming, we propose a novel approach to trading off interpretability and performance in prediction models using ideas from semiparametric statistics, allowing us to combine the interpretability of parametric regression models with performance of nonparametric methods. We achieve this by utilizing a two-piece model: the first piece is interpretable and parametric, to which a second, uninterpretable residual piece is added. The performance of the overall model is optimized using methods from the sufficient dimension reduction literature. Influence function based estimators are derived and shown to be doubly robust. This allows for use of approaches such as double Machine Learning in estimating our model parameters. We illustrate the utility of our approach via simulation studies and a data application based on predicting the length of stay in the intensive care unit among surgery patients.","",""
4,"N. Alaya, S. Yahia, M. Lamolle","Predicting the Empirical Robustness of the Ontology Reasoners based on Machine Learning Techniques",2015,"","","","",122,"2022-07-13 09:24:29","","10.5220/0005599800610073","","",,,,,4,0.57,1,3,7,"Reasoning with ontologies is one of the core tasks of research in Description Logics. A variety of reasoners    with highly optimized algorithms have been developed to allow inference tasks on expressive ontology    languages such as OWL (DL). However, unexpected behaviours of reasoner engines is often observed in practice.    Both reasoner time efficiency and result correctness would vary across input ontologies, which is hardly    predictable even for experienced reasoner designers. Seeking for better understanding of reasoner empirical    behaviours, we propose to use supervised machine learning techniques to automatically predict reasoner robustness    from its previous running. For this purpose, we introduced a set of comprehensive ontology features.    We conducted huge body of experiments for 6 well known reasoners and using over 1000 ontologies from the    OREâ2014 corpus. Our learning results show that we could build highly accuracy reasoner robustness predictive    models. Moreover, by interpreting these models, it would be possible to gain insights about particular    ontology features likely to be reasoner robustness degrading factors.","",""
0,"Gabriel D. Patrón, D. León, Edwin Lopez, G. Hernández","An Interpretable Automated Machine Learning Credit Risk Model",2020,"","","","",123,"2022-07-13 09:24:29","","10.1007/978-3-030-61834-6_2","","",,,,,0,0.00,0,4,2,"","",""
0,"T. Gärtner","Interactive Machine Learning with Structured Data",2020,"","","","",124,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,1,2,"In this talk I’ll give an overview of our contributions to what I call interactive machine learning. Often, interaction in Computer Science is interpreted as the interaction of humans with the computer but I intend a broader meaning of the interaction of machine learning algorithms with the real world, including but not restricted to humans. Interactions with humans span a broad range where they can be intentional and guided by the human or they can be guided by the computer such that the human is oblivious of the fact that he is being guided. Another example of an interaction with the real world is the use of machine learning algorithms in cyclic discovery processes such as drug design. Important properties of interactive machine learning algorithms include efficiency, effectiveness, responsiveness, and robustness. In the talk I will show how these can be achieved in a variety of interactive contexts. Copyright © 2020 by the paper’s authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","",""
0,"S. Shah","Addressing the interpretability problem for deep learning using many valued quantum logic",2020,"","","","",125,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,1,2,"Deep learning models are widely used for various industrial and scientific applications. Even though these models have achieved considerable success in recent years, there exists a lack of understanding of the rationale behind decisions made by such systems in the machine learning community. This problem of interpretability is further aggravated by the increasing complexity of such models. This paper utilizes concepts from machine learning, quantum computation and quantum field theory to demonstrate how a many valued quantum logic system naturally arises in a specific class of generative deep learning models called Convolutional Deep Belief Networks. It provides a robust theoretical framework for constructing deep learning models equipped with the interpretability of many valued quantum logic systems without compromising their computing efficiency.","",""
2,"S. Siltanen, Takanori Ide","Electrical Impedance Tomography, Enclosure Method and Machine Learning",2020,"","","","",126,"2022-07-13 09:24:29","","10.1109/MLSP49062.2020.9231717","","",,,,,2,1.00,1,2,2,"Electrical impedance tomography (EIT) is a non-destructive imaging method, where a physical body is probed with electric measurements at the boundary, and information about the internal conductivity is extracted from the data. The enclosure method of Ikehata [J. Inv. III-Posed Prob. 8(2000)] recovers the convex hull of an inclusion of unknown conductivity embedded in known background conductivity. Practical implementations of the enclosure method are based on least-squares (LS) fitting of lines to noise-robust values of the so-called indicator function. It is shown how a convolutional neural network instead of LS fitting improves the accuracy of the enclosure method significantly while retaining interpretability.","",""
200,"W. James Murdoch, Chandan Singh, Karl Kumbier, R. Abbasi-Asl, Bin Yu","Interpretable machine learning: definitions, methods, and applications",2019,"","","","",127,"2022-07-13 09:24:29","","10.1073/pnas.1900654116","","",,,,,200,66.67,40,5,3,"M learning (ML) has recently received considerable attention for its ability to accurately predict a wide variety of complex phenomena. However, there is a growing realization that, in addition to predictions, ML models are capable of producing knowledge about domain relationships contained in data, often referred to as interpretations. These interpretations have found uses both in their own right, e.g. medicine (1), policy-making (2), and science (3, 4), as well as in auditing the predictions themselves in response to issues such as regulatory pressure (5) and fairness (6). In the absence of a well-formed definition of interpretability, a broad range of methods with a correspondingly broad range of outputs (e.g. visualizations, natural language, mathematical equations) have been labeled as interpretation. This has led to considerable confusion about the notion of interpretability. In particular, it is unclear what it means to interpret something, what common threads exist among disparate methods, and how to select an interpretation method for a particular problem/audience. In this paper, we attempt to address these concerns. To do so, we first define interpretability in the context of machine learning and place it within a generic data science life cycle. This allows us to distinguish between two main classes of interpretation methods: model-based∗ and post hoc. We then introduce the Predictive, Descriptive, Relevant (PDR) framework, consisting of three desiderata for evaluating and constructing interpretations: predictive accuracy, descriptive","",""
1,"Vignesh Srinivasan, Nils Strodthoff, Jackie Ma, Alexander Binder, K. Muller, W. Samek","On the Robustness of Pretraining and Self-Supervision for a Deep Learning-based Analysis of Diabetic Retinopathy",2021,"","","","",128,"2022-07-13 09:24:29","","","","",,,,,1,1.00,0,6,1,"Objective: There is an increasing number of medical use-cases where classification algorithms based on deep neural networks reach performance levels that are competitive with human medical experts. To alleviate the challenges of small dataset sizes, these systems often rely on pretraining. In this work, we aim to assess the broader implications of these approaches. Methods: For diabetic retinopathy grading as exemplary use case, we compare the impact of different training procedures including recently established self-supervised pretraining methods based on contrastive learning. To this end, we investigate different aspects such as quantitative performance, statistics of the learned feature representations, interpretability and robustness to image distortions. Results: Our results indicate that models initialized from ImageNet pretraining report a significant increase in performance, generalization and robustness to image distortions. In particular, selfsupervised models show further benefits to supervised models. Conclusion: Self-supervised models with ini∗VS, NS, JM and WS are with the Department of Artificial Intelligence, Fraunhofer Heinrich Hertz Institute, 10587 Berlin, Germany (email: firstname.lastname@hhi.fraunhofer.de). NS and WS are also with BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany. AB is with the Department of Informatics, Oslo University, 0373 Oslo, Norway. KRM is with the Machine Learning Group, Technische Universität Berlin, 10587 Berlin, Germany, and also with the Department of Artificial Intelligence, Korea University, Seoul 136-713, South Korea, the Max Planck Institute for Informatics, 66123 Saarbrücken, Germany, and BIFOLD – Berlin Institute for the Foundations of Learning and Data, 10587 Berlin, Germany. (e-mail: klaus-robert.mueller@tuberlin.de). tialization from ImageNet pretraining not only report higher performance, they also reduce overfitting to large lesions along with improvements in taking into account minute lesions indicative of the progression of the disease. Significance: Understanding the effects of pretraining in a broader sense that goes beyond simple performance comparisons is of crucial importance for the broader medical imaging community beyond the use-case considered in this work.","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",129,"2022-07-13 09:24:29","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
3,"Nathan G. Drenkow, Numair Sani, I. Shpitser, M. Unberath","Robustness in Deep Learning for Computer Vision: Mind the gap?",2021,"","","","",130,"2022-07-13 09:24:29","","","","",,,,,3,3.00,1,4,1,"Deep neural networks for computer vision tasks are deployed in increasingly safety-critical and socially-impactful applications, motivating the need to close the gap in model performance under varied, naturally occurring imaging conditions. Robustness, ambiguously used in multiple contexts including adversarial machine learning, here then refers to preserving model performance under naturally-induced image corruptions or alterations. We perform a systematic review to identify, analyze, and summarize current definitions and progress towards non-adversarial robustness in deep learning for computer vision. We find that this area of research has received disproportionately little attention relative to adversarial machine learning, yet a significant robustness gap exists that often manifests in performance degradation similar in magnitude to adversarial conditions. To provide a more transparent definition of robustness across contexts, we introduce a structural causal model of the data generating process and interpret non-adversarial robustness as pertaining to a model’s behavior on corrupted images which correspond to low-probability samples from the unaltered data distribution. We then identify key architecture-, data augmentation-, and optimization tactics for improving neural network robustness. This causal view of robustness reveals that common practices in the current literature, both in regards to robustness tactics and evaluations, correspond to causal concepts, such as soft interventions resulting in a counterfactually-altered distribution of imaging conditions. Through our findings and analysis, we offer perspectives on how future research may mind this evident and significant non-adversarial robustness gap.","",""
35,"Han Liu, Tony Zhang, N. M. Anoop Krishnan, M. Smedskjaer, J. Ryan, S. Gin, M. Bauchy","Predicting the dissolution kinetics of silicate glasses by topology-informed machine learning",2019,"","","","",131,"2022-07-13 09:24:29","","10.1038/s41529-019-0094-1","","",,,,,35,11.67,5,7,3,"","",""
1981,"C. Rudin","Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",2018,"","","","",132,"2022-07-13 09:24:29","","10.1038/S42256-019-0048-X","","",,,,,1981,495.25,1981,1,4,"","",""
14,"M. Levine, A. Stuart","A Framework for Machine Learning of Model Error in Dynamical Systems",2021,"","","","",133,"2022-07-13 09:24:29","","","","",,,,,14,14.00,7,2,1,"The development of data-informed predictive models for dynamical systems is of widespread interest in many disciplines. We present a unifying framework for blending mechanistic and machine-learning approaches to identify dynamical systems from noisily and partially observed data. We compare pure data-driven learning with hybrid models which incorporate imperfect domain knowledge, referring to the discrepancy between an assumed truth model and the imperfect mechanistic model as model error. Our formulation is agnostic to the chosen machine learning model, is presented in both continuousand discrete-time settings, and is compatible both with model errors that exhibit substantial memory and errors that are memoryless. First, we study memoryless linear (w.r.t. parametric-dependence) model error from a learning theory perspective, defining excess risk and generalization error. For ergodic continuous-time systems, we prove that both excess risk and generalization error are bounded above by terms that diminish with the square-root of T , the time-interval over which training data is specified. Secondly, we study scenarios that benefit from modeling with memory, proving universal approximation theorems for two classes of continuous-time recurrent neural networks (RNNs): both can learn memory-dependent model error, assuming that it is governed by a finite-dimensional hidden variable and that, together, the observed and hidden variables form a continuous-time Markovian system. In addition, we connect one class of RNNs to reservoir computing, thereby relating learning of memory-dependent error to recent work on supervised learning between Banach spaces using random features. Numerical results are presented (Lorenz ’63, Lorenz ’96 Multiscale systems) to compare purely data-driven and hybrid approaches, finding hybrid methods less data-hungry and more parametrically efficient. We also find that, while a continuous-time framing allows for robustness to irregular sampling and desirable domain-interpretability, a discrete-time framing can provide similar or better predictive performance, especially when data are undersampled and the vector field defining the true dynamics cannot be identified. Finally, we demonstrate numerically how data assimilation can be leveraged to learn hidden dynamics from noisy, partially-observed data, and illustrate challenges in representing memory by this approach, and in the training of such models. Received by the editors July 13, 2021. 2020 Mathematics Subject Classification. Primary 68T30, 37A30, 37M10; Secondary 37M25,","",""
21,"Nidan Qiao","A systematic review on machine learning in sellar region diseases: quality and reporting items",2019,"","","","",134,"2022-07-13 09:24:29","","10.1530/EC-19-0156","","",,,,,21,7.00,21,1,3,"Introduction Machine learning methods in sellar region diseases present a particular challenge because of the complexity and the necessity for reproducibility. This systematic review aims to compile the current literature on sellar region diseases that utilized machine learning methods and to propose a quality assessment tool and reporting checklist for future studies. Methods PubMed and Web of Science were searched to identify relevant studies. The quality assessment included five categories: unmet needs, reproducibility, robustness, generalizability and clinical significance. Results Seventeen studies were included with the diagnosis of general pituitary neoplasms, acromegaly, Cushing’s disease, craniopharyngioma and growth hormone deficiency. 87.5% of the studies arbitrarily chose one or two machine learning models. One study chose ensemble models, and one study compared several models. 43.8% of studies did not provide the platform for model training, and roughly half did not offer parameters or hyperparameters. 62.5% of the studies provided a valid method to avoid over-fitting, but only five reported variations in the validation statistics. Only one study validated the algorithm in a different external database. Four studies reported how to interpret the predictors, and most studies (68.8%) suggested possible clinical applications of the developed algorithm. The workflow of a machine-learning study and the recommended reporting items were also provided based on the results. Conclusions Machine learning methods were used to predict diagnosis and posttreatment outcomes in sellar region diseases. Though most studies had substantial unmet need and proposed possible clinical application, replicability, robustness and generalizability were major limits in current studies.","",""
6,"V. Chernozhukov, W. Newey, Rahul Singh","A Simple and General Debiased Machine Learning Theorem with Finite Sample Guarantees",2021,"","","","",135,"2022-07-13 09:24:29","","10.1093/biomet/asac033","","",,,,,6,6.00,2,3,1,"  Debiased machine learning is a meta algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e., scalar summaries, of machine learning algorithms. For example, an analyst may desire the confidence interval for a treatment effect estimated with a neural network. We provide a non-asymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions. Formally, we prove consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments. The rate of convergence is n−1/2 for global functionals, and it degrades gracefully for local functionals. Our results culminate in a simple set of conditions that an analyst can use to translate modern learning theory rates into traditional statistical inference. The conditions reveal a general double robustness property for ill-posed inverse problems.","",""
18,"C. Rudin, David Edwin Carlson","The Secrets of Machine Learning: Ten Things You Wish You Had Known Earlier to be More Effective at Data Analysis",2019,"","","","",136,"2022-07-13 09:24:29","","10.1287/educ.2019.0200","","",,,,,18,6.00,9,2,3,"Despite the widespread usage of machine learning throughout organizations, there are some key principles that are commonly missed. In particular: 1) There are at least four main families for supervised learning: logical modeling methods, linear combination methods, case-based reasoning methods, and iterative summarization methods. 2) For many application domains, almost all machine learning methods perform similarly (with some caveats). Deep learning methods, which are the leading technique for computer vision problems, do not maintain an edge over other methods for most problems (and there are reasons why). 3) Neural networks are hard to train and weird stuff often happens when you try to train them. 4) If you don't use an interpretable model, you can make bad mistakes. 5) Explanations can be misleading and you can't trust them. 6) You can pretty much always find an accurate-yet-interpretable model, even for deep neural networks. 7) Special properties such as decision making or robustness must be built in, they don't happen on their own. 8) Causal inference is different than prediction (correlation is not causation). 9) There is a method to the madness of deep neural architectures, but not always. 10) It is a myth that artificial intelligence can do anything.","",""
14,"A. A. Alshaikh, A. Magana-Mora, S. Gharbi, A. Al-yami","Machine Learning for Detecting Stuck Pipe Incidents: Data Analytics and Models Evaluation",2019,"","","","",137,"2022-07-13 09:24:29","","10.2523/IPTC-19394-MS","","",,,,,14,4.67,4,4,3,"  The earlier a stuck pipe incident is predicted and mitigated, the higher the chance of success in freeing the pipe or avoiding severe sticking in the first place. Time is crucial in such cases as an improper reaction to a stuck pipe incident can easily make it worse. In this work, practical machine learning, classification models were developed using real-time drilling data to automatically detect stuck pipe incidents during drilling operations and communicate the observations and alerts, sufficiently ahead of time, to the rig crew for avoidance or remediation actions to be taken.  The models use machine learning algorithms that feed on identified key drilling parameters to detect stuck pipe anomalies. The parameters used in building the system were selected based on published literature and historical data and reports of stuck pipe incidents and were analyzed and ranked to identify the ones of key influence on the accuracy of stuck pipe detection via a nonlinear relationship. The model exceptionally uses the robustness of data-based analysis along with the physics-based analysis.  The model has shown effective detection of the signs observed by experts ahead of time and has helped with providing enhanced stuck pipe detection and risk assessment. Validating and testing the model on several cases showed promising results as anomalies on simple and complex parameters were detected before or near the actual time stuck pipe incidents were reported from the rig crew. This facilitated better understanding of the underlying physics principles and provided awareness of stuck pipe occurrence.  The model improved monitoring and interpreting the drilling data streams. Beside such pipe signs, the model helped with detecting signs of other impeding problems in the downhole conditions of the wellbore, the drilling equipment, and the sensors. The model is designed to be implemented in the real-time drilling data portal to provide an alarm system for all oil and gas rigs based on the observed abnormalities. The alarm is to be populated on the real-time environment and communicated to the rig crew in a timely manner to ensure optimal results, giving them sufficient time ahead to prevent or remediate a potential stuck pipe incident.","",""
15,"Bingqiang Liu, Ling Han, Xiangrong Liu, Jichang Wu, Q. Ma","Computational Prediction of Sigma-54 Promoters in Bacterial Genomes by Integrating Motif Finding and Machine Learning Strategies",2019,"","","","",138,"2022-07-13 09:24:29","","10.1109/TCBB.2018.2816032","","",,,,,15,5.00,3,5,3,"Sigma factor, as a unit of RNA polymerase holoenzyme, is a critical factor in the process of gene transcriptional regulation. It recognizes the specific DNA sites and brings the core enzyme of RNA polymerase to the upstream regions of target genes. Therefore, the prediction of the promoters for a particular sigma factor is essential for interpreting functional genomic data and observation. This paper develops a new method to predict sigma-54 promoters in bacterial genomes. The new method organically integrates motif finding and machine learning strategies to capture the intrinsic features of sigma-54 promoters. The experiments on E. coli benchmark test set show that our method has good capability to distinguish sigma-54 promoters from surrounding or randomly selected DNA sequences. The applications of the other three bacterial genomes indicate the potential robustness and applicative power of our method on a large number of bacterial genomes. The source code of our method can be freely downloaded at https://github.com/maqin2001/PromotePredictor.","",""
0,"Daniel C. Anderson","Comment on gmd-2022-44 Anonymous Referee # 1 Referee comment on "" A Machine Learning Methodology for the Generation of a Parameterization of the Hydroxyl Radical : a Tool to Improve Computational-Efficiency in Chemistry Climate Models",2022,"","","","",139,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,1,1,"This paper describes the application of a gradient boosted regression tree machine learning approach to derive a parameterization for tropospheric OH based on CCM simulations. The approach is shown to reproduce simulated OH well under current conditions even for cases it has not been trained on, and it behaves acceptably, albeit with increasing errors, when applied to future conditions outside the standard training set. There is substantial novelty in the approach taken, and the results offer a degree of interpretability that is very interesting. The paper is generally well structured, clearly written, and appropriately illustrated. The authors have been thorough in evaluating their approach, and it is particularly good to see robust testing of input variable choice and hyperparameter value selection. the paper approach","",""
0,"Yilin Ning, Siqi Li, M. Ong, F. Xie, B. Chakraborty, D. Ting, Nan Liu","A novel interpretable machine learning system to generate clinical risk scores: An application for predicting early mortality or unplanned readmission in a retrospective cohort study",2022,"","","","",140,"2022-07-13 09:24:29","","10.1371/journal.pdig.0000062","","",,,,,0,0.00,0,7,1,"Risk scores are widely used for clinical decision making and commonly generated from logistic regression models. Machine-learning-based methods may work well for identifying important predictors to create parsimonious scores, but such ‘black box’ variable selection limits interpretability, and variable importance evaluated from a single model can be biased. We propose a robust and interpretable variable selection approach using the recently developed Shapley variable importance cloud (ShapleyVIC) that accounts for variability in variable importance across models. Our approach evaluates and visualizes overall variable contributions for in-depth inference and transparent variable selection, and filters out non-significant contributors to simplify model building steps. We derive an ensemble variable ranking from variable contributions across models, which is easily integrated with an automated and modularized risk score generator, AutoScore, for convenient implementation. In a study of early death or unplanned readmission after hospital discharge, ShapleyVIC selected 6 variables from 41 candidates to create a well-performing risk score, which had similar performance to a 16-variable model from machine-learning-based ranking. Our work contributes to the recent emphasis on interpretability of prediction models for high-stakes decision making, providing a disciplined solution to detailed assessment of variable importance and transparent development of parsimonious clinical risk scores.","",""
0,"Jing-Jing Liu, Jian-chao Liu","Permeability Predictions for Tight Sandstone Reservoir Using Explainable Machine Learning and Particle Swarm Optimization",2022,"","","","",141,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,2,1,"High-precision permeability prediction is of great significance to tight sandstone reservoirs. However, while considerable progress has recently been made in the machine learning based prediction of reservoir permeability, the generalization of this approach is limited by weak interpretability. Hence, an interpretable XGBoost model is proposed herein based on particle swarm optimization to predict the permeability of tight sandstone reservoirs with higher accuracy and robust interpretability. The porosity and permeability of 202 core plugs and 6 logging curves (namely, the gamma-ray (GR) curve, the acoustic curve (AC), the spontaneous potential (SP) curve, the caliper (CAL) curve, the deep lateral resistivity (RILD) curve, and eight lateral resistivity (RFOC) curve) are extracted along with three derived variables (i.e., the shale content, the AC slope, and the GR slope) as data sets. Based on the data preprocessing, global and local interpretations are performed according to the Shapley additive explanations (SHAP) analysis, and the redundant features in the data set are screened to identify the porosity, AC, CAL, and GR slope as the four most important features. The particle swarm optimization algorithm is then used to optimize the hyperparameters of the XGBoost model. The prediction results of the PSO-XGBoost model indicate a superior performance compared with that of the benchmark XGBoost model. In addition, the reliable application of the interpretable PSO-XGBoost model in the prediction of tight sandstone reservoir permeability is examined by comparing the results with those of two traditional mathematical regression models, five machine learning models, and three deep learning models. Thus, the interpretable PSO-XGBoost model is shown to have more advantages in permeability prediction along with the lowest root mean square error, thereby confirming the effectiveness and practicability of this method.","",""
1,"Peishuo Sun, Ying Wu, Chaoyi Yin, H. Jiang, Ying Xu, Huiyan Sun","Molecular Subtyping of Cancer Based on Distinguishing Co-Expression Modules and Machine Learning",2022,"","","","",142,"2022-07-13 09:24:29","","10.3389/fgene.2022.866005","","",,,,,1,1.00,0,6,1,"Molecular subtyping of cancer is recognized as a critical and challenging step towards individualized therapy. Most existing computational methods solve this problem via multi-classification of gene-expressions of cancer samples. Although these methods, especially deep learning, perform well in data classification, they usually require large amounts of data for model training and have limitations in interpretability. Besides, as cancer is a complex systemic disease, the phenotypic difference between cancer samples can hardly be fully understood by only analyzing single molecules, and differential expression-based molecular subtyping methods are reportedly not conserved. To address the above issues, we present here a new framework for molecular subtyping of cancer through identifying a robust specific co-expression module for each subtype of cancer, generating network features for each sample by perturbing correlation levels of specific edges, and then training a deep neural network for multi-class classification. When applied to breast cancer (BRCA) and stomach adenocarcinoma (STAD) molecular subtyping, it has superior classification performance over existing methods. In addition to improving classification performance, we consider the specific co-expressed modules selected for subtyping to be biologically meaningful, which potentially offers new insight for diagnostic biomarker design, mechanistic studies of cancer, and individualized treatment plan selection.","",""
1,"S. Javed, Dinkar Juyal, Zahil Shanis, S. Chakraborty, Harsha Pokkalla, A. Prakash","Rethinking Machine Learning Model Evaluation in Pathology",2022,"","","","",143,"2022-07-13 09:24:29","","10.48550/arXiv.2204.05205","","",,,,,1,1.00,0,6,1,"Machine Learning has been applied to pathology images in research and clinical practice with promising outcomes. However, standard ML models often lack the rigorous evaluation required for clinical decisions. Machine learning techniques for natural images are ill-equipped to deal with pathology images that are signiﬁcantly large and noisy, require expensive labeling, are hard to interpret, and are susceptible to spurious correlations. We propose a set of practical guidelines for ML evaluation in pathology that address the above concerns. The paper includes measures for setting up the evaluation framework, effectively dealing with variability in labels, and a rec-ommended suite of tests to address issues related to domain shift, robustness, and confounding variables. We hope that the proposed framework will bridge the gap between ML researchers and domain experts, leading to wider adoption of ML techniques in pathology and improving patient outcomes.","",""
0,"R. Castellanos, G. Y. C. Maceda, I. D. L. Fuente, B. R. Noack, A. Ianiro, S. Discetti","Machine-learning flow control with few sensor feedback and measurement noise",2022,"","","","",144,"2022-07-13 09:24:29","","10.1063/5.0087208","","",,,,,0,0.00,0,6,1,"A comparative assessment of machine-learning (ML) methods for active flow control is performed. The chosen benchmark problem is the drag reduction of a two-dimensional Kármán vortex street past a circular cylinder at a low Reynolds number ( Re =  100). The flow is manipulated with two blowing/suction actuators on the upper and lower side of a cylinder. The feedback employs several velocity sensors. Two probe configurations are evaluated: 5 and 11 velocity probes located at different points around the cylinder and in the wake. The control laws are optimized with Deep Reinforcement Learning (DRL) and Linear Genetic Programming Control (LGPC). By interacting with the unsteady wake, both methods successfully stabilize the vortex alley and effectively reduce drag while using small mass flow rates for the actuation. DRL has shown higher robustness with respect to different initial conditions and to noise contamination of the sensor data; on the other hand, LGPC is able to identify compact and interpretable control laws, which only use a subset of sensors, thus allowing for the reduction of the system complexity with reasonably good results. Our study points at directions of future machine-learning control combining desirable features of different approaches.","",""
0,"Takaki Yamamoto, Katie Cockburn, V. Greco, Kyogo Kawaguchi","Probing the rules of cell coordination in live tissues by interpretable machine learning based on graph neural networks",2022,"","","","",145,"2022-07-13 09:24:29","","10.1101/2021.06.23.449559","","",,,,,0,0.00,0,4,1,"Robustness in developing and homeostatic tissues is supported by various types of spatiotemporal cell-to-cell interactions. Although live imaging and cell tracking are powerful in providing direct evidence of cell coordination rules, extracting and comparing these rules across many tissues with potentially different length and timescales of coordination requires a versatile framework of analysis. Here we demonstrate that graph neural network (GNN) models are suited for this purpose, by showing how they can be applied to predict cell fate in tissues and utilized to infer the cell interactions governing the multicellular dynamics. Analyzing the live mammalian epidermis data, where spatiotemporal graphs constructed from cell tracks and cell contacts are given as inputs, GNN discovers distinct neighbor cell fate coordination rules that depend on the region of the body. This approach demonstrates how the GNN framework is powerful in inferring general cell interaction rules from live data without prior knowledge of the signaling involved.","",""
9,"T. Botari, Rafael Izbicki, A. Carvalho","Local Interpretation Methods to Machine Learning Using the Domain of the Feature Space",2019,"","","","",146,"2022-07-13 09:24:29","","10.1007/978-3-030-43823-4_21","","",,,,,9,3.00,3,3,3,"","",""
11,"Tamer Karatekin, S. Sancak, G. Celik, S. Topçuoğlu, G. Karatekin, Pınar Kırcı, A. Okatan","Interpretable Machine Learning in Healthcare through Generalized Additive Model with Pairwise Interactions (GA2M): Predicting Severe Retinopathy of Prematurity",2019,"","","","",147,"2022-07-13 09:24:29","","10.1109/Deep-ML.2019.00020","","",,,,,11,3.67,2,7,3,"We have investigated the risk factors that lead to severe retinopathy of prematurity using statistical analysis and logistic regression as a form of generalized additive model (GAM) with pairwise interaction terms (GA2M). In this process, we discuss the trade-off between accuracy and interpretability of these machine learning techniques on clinical data. We also confirm the intuition of expert neonatologists on a few risk factors, such as gender, that were previously deemed as clinically not significant in RoP prediction.","",""
0,"Sasmitha Dasanayaka, S. Silva, V. Shantha, D. Meedeniya, Thanuja D. Ambegoda","Interpretable Machine Learning for Brain Tumor Analysis Using MRI",2022,"","","","",148,"2022-07-13 09:24:29","","10.1109/ICARC54489.2022.9754131","","",,,,,0,0.00,0,5,1,"A brain tumor is a potentially fatal growth of cells in the central nervous system that can be categorized as benign or malignant. Advancements in deep learning in the recent past and the availability of high computational power have been influencing the automation of diagnosing brain tumors. DenseNet and U-Net are considered state of the art deep learning models for classification and segmentation of MRIs respectively. Despite the progress of deep learning in diagnosing using medical images, generic convolutional neural networks are still not fully adopted in clinical settings as they lack robustness and reliability. Moreover, such black-box models don’t offer a human interpretable justification as to why certain classification decisions are made, which makes them less preferable for medical diagnostics. Brain tumor segmentation and classification using deep learning techniques has been a popular research area in the last few decades but still, there are only a few models that are interpretable. In this paper, we have proposed an interpretable deep learning model which is more human understandable than existing black-box models, designed based on U-Net and DenseNet to segment and classify brain tumors using MRI. In our proposed model, we generate a heat map highlighting the contribution of each region of the input to the classification output and have validated the system using the MICCAI 2020 Brain Tumor dataset.","",""
12,"E. M. Mortani Barbosa, B. Georgescu, S. Chaganti, G. Alemañ, Jordi Broncano Cabrero, G. Chabin, T. Flohr, P. Grenier, Sasa Grbic, Nakul Gupta, F. Mellot, S. Nicolaou, Thomas J. Re, P. Sanelli, A. Sauter, Y. Yoo, Valentin Ziebandt, D. Comaniciu","Machine learning automatically detects COVID-19 using chest CTs in a large multicenter cohort",2020,"","","","",149,"2022-07-13 09:24:29","","10.1007/s00330-021-07937-3","","",,,,,12,6.00,1,18,2,"","",""
11,"Tianfang Xu, F. Liang","Machine learning for hydrologic sciences: An introductory overview",2021,"","","","",150,"2022-07-13 09:24:29","","10.1002/wat2.1533","","",,,,,11,11.00,6,2,1,"The hydrologic community has experienced a surge in interest in machine learning in recent years. This interest is primarily driven by rapidly growing hydrologic data repositories, as well as success of machine learning in various academic and commercial applications, now possible due to increasing accessibility to enabling hardware and software. This overview is intended for readers new to the field of machine learning. It provides a non‐technical introduction, placed within a historical context, to commonly used machine learning algorithms and deep learning architectures. Applications in hydrologic sciences are summarized next, with a focus on recent studies. They include the detection of patterns and events such as land use change, approximation of hydrologic variables and processes such as rainfall‐runoff modeling, and mining relationships among variables for identifying controlling factors. The use of machine learning is also discussed in the context of integrated with process‐based modeling for parameterization, surrogate modeling, and bias correction. Finally, the article highlights challenges of extrapolating robustness, physical interpretability, and small sample size in hydrologic applications.","",""
7,"Haoran Li, Yang Weng, E. Farantatos, Mahendra Patel","A Hybrid Machine Learning Framework for Enhancing PMU-based Event Identification with Limited Labels",2019,"","","","",151,"2022-07-13 09:24:29","","10.1109/SGSMA.2019.8784550","","",,,,,7,2.33,2,4,3,"The energy industry is experiencing rapid and dramatic changes on both the generator side and the load side, necessitating faster, more accurate, and robust event detection methods for situational awareness. Growing installations of PMU devices that provide high resolution synchronized measurements combined with the advancement of artificial intelligence and big data analytics techniques have recently attracted the R&D community interest. Some supervised learning techniques have been proposed using PMU measurements, however, they are facing challenges in 1) limited interpretability, 2) biased learning models/results, and 3) insufficient labeled data for learning. To address these issues, we propose a machine learning-based framework for physically-meaningful interpretability, hybrid-learning method with indexes, and a flexible data-preparation approach. Specifically, a thoroughly designed feature selection method is proposed for discovering event signatures. Then, a hybrid machine learning process is constructed to reduce biases of different machine learners due to their diversified working mechanisms. Finally, we propose to utilize unlabeled data via semi-supervised learning and add strategical event data via active learning, e.g., simulations. The goal is to significantly improve the supervised learning results via computational efficient techniques. Extensive simulations are conducted using a commercial power system dynamics simulator and synthetic realistic transmission grid models. Significant improvements are observed via hybrid supervised learning methods, semi-supervised learning, and active learning.","",""
10,"Alan Rozet, I. Kronish, J. Schwartz, K. Davidson","Using Machine Learning to Derive Just-In-Time and Personalized Predictors of Stress: Observational Study Bridging the Gap Between Nomothetic and Ideographic Approaches",2019,"","","","",152,"2022-07-13 09:24:29","","10.2196/12910","","",,,,,10,3.33,3,4,3,"Background Investigations into person-specific predictors of stress have typically taken either a population-level nomothetic approach or an individualized ideographic approach. Nomothetic approaches can quickly identify predictors but can be hindered by the heterogeneity of these predictors across individuals and time. Ideographic approaches may result in more predictive models at the individual level but require a longer period of data collection to identify robust predictors. Objective Our objectives were to compare predictors of stress identified through nomothetic and ideographic models and to assess whether sequentially combining nomothetic and ideographic models could yield more accurate and actionable predictions of stress than relying on either model. At the same time, we sought to maintain the interpretability necessary to retrieve individual predictors of stress despite using nomothetic models. Methods Data collected in a 1-year observational study of 79 participants performing low levels of exercise were used. Physical activity was continuously and objectively monitored by actigraphy. Perceived stress was recorded by participants via daily ecological momentary assessments on a mobile app. Environmental variables including daylight time, temperature, and precipitation were retrieved from the public archives. Using these environmental, actigraphy, and mobile assessment data, we built machine learning models to predict individual stress ratings using linear, decision tree, and neural network techniques employing nomothetic and ideographic approaches. The accuracy of the approaches for predicting individual stress ratings was compared based on classification errors. Results Across the group of patients, an individual’s recent history of stress ratings was most heavily weighted in predicting a future stress rating in the nomothetic recurrent neural network model, whereas environmental factors such as temperature and daylight, as well as duration and frequency of bouts of exercise, were more heavily weighted in the ideographic models. The nomothetic recurrent neural network model was the highest performing nomothetic model and yielded 72% accuracy for an 80%/20% train/test split. Using the same 80/20 split, the ideographic models yielded 75% accuracy. However, restricting ideographic models to participants with more than 50 valid days in the training set, with the same 80/20 split, yielded 85% accuracy. Conclusions We conclude that for some applications, nomothetic models may be useful for yielding higher initial performance while still surfacing personalized predictors of stress, before switching to ideographic models upon sufficient data collection.","",""
2,"Chris Emmery, Ákos Kádár, Travis J. Wiltshire, Andrew T. Hendrickson","Towards Replication in Computational Cognitive Modeling: a Machine Learning Perspective",2019,"","","","",153,"2022-07-13 09:24:29","","10.1007/S42113-019-00055-W","","",,,,,2,0.67,1,4,3,"","",""
1,"W. Monroe, T. Anthony, M. Tanik, F. Skidmore","Towards a Framework for Validating Machine Learning Results in Medical Imaging: Opening the black box",2019,"","","","",154,"2022-07-13 09:24:29","","10.1145/3332186.3332193","","",,,,,1,0.33,0,4,3,"In the medical imaging domain, non-linear warping has enabled pixel by pixel mapping of one image dataset to a reference dataset. This co-registration of data allows for robust, pixel-wise, statistical maps to be developed in the domain, leading to new insights regarding disease mechanisms [20]. Deep learning technologies have given way to some impressive discoveries. In some applications, deep learning algorithms have surpassed the abilities of human image readers to classify data. As long as endpoints are clearly defined, and the input data volume is large enough, deep learning networks can often converge and reach prediction, classification, and segmentation with success rates as high or higher than human operators [13]. However, machine learning, and deep learning algorithms are complex and interpretability is not always a straightforward byproduct of the classification performed. Visualization techniques have been developed to add a layer of interpretability. The work presented here compares a simplified machine learning workflow for medical imaging to a statistical map from a previous study to validate that the machine learning model used does indeed focus its attention on known important regions.","",""
2,"J. Sarkar, Cory Peterson","Operational Workload Impact on Robust Solid-State Storage Analyzed with Interpretable Machine Learning",2019,"","","","",155,"2022-07-13 09:24:29","","10.1109/IRPS.2019.8720510","","",,,,,2,0.67,1,2,3,"Solid-state storage technology is finding increasing adoption in enterprise and data center environments due to their high reliability and reducing cost. With high performance solid-state storage devices (SSDs) internally designed as distributed resilient systems, their operational behavior under materially different workloads is described in this research. Application of interpretable machine learning on internal parametric data of SSDs enables insights on workloads' interaction with the resilient system design. After prior research demonstrated significantly different accelerated workload stress, the analysis on resilience of the SSDs under random vs. pseudo-sequential workloads emphasize the efficacy and importance of their distributed resilience schemes. As such, these results provide causational insights on the mechanism of differential stress of the workloads impacting the resilience design principles. Moreover, the results elucidate guidelines strongly relevant from design robustness perspective for research on novel SSD architectures such as the proposed Open Channel SSD, towards deployment in hyperscale and virtualization environments.","",""
3,"F. Lécué, B. Abeloos, Jonathan Anctil, Manuel Bergeron, Damien Dalla-Rosa, Simon Corbeil-Letourneau, Florian Martet, Tanguy Pommellet, L. Salvan, Simon Veilleux, M. Ziaeefard","Thales XAI Platform: Adaptable Explanation of Machine Learning Systems - A Knowledge Graphs Perspective",2019,"","","","",156,"2022-07-13 09:24:29","","","","",,,,,3,1.00,0,11,3,"Explanation in Machine Learning systems has been identified to be the main asset to have for large scale deployment of Artificial Intelligence (AI) in critical systems. Explanations could be example-, features-, semantics-based or even counterfactual to potentially action on an AI system; they could be represented in many different ways e.g., textual, graphical, or visual. All representations serve different means, purpose and operators. We built the first-of-its-kind XAI (eXplainable AI) platform for critical systems i.e., Thales XAI Platform which aims at serving explanations through various forms. This paper emphasizes on the semantics-based explanations for Machine Learning systems. 1 Explainable AI in Critical Systems Motivation: The current hype of Artificial Intelligence (AI) mostly refers to the success of Machine Learning (ML) and its sub-domain of deep learning. However industries operating with critical systems are either highly regulated, or require high level of certification and robustness. Therefore, such industry constraints do limit the adoption of non deterministic and ML systems. Answers to the question of explainability will be intrinsically connected to the adoption of AI in industry at scale. Indeed explanation, which could be used for debugging intelligent systems or deciding to follow a recommendation in real-time, will increase acceptance and (business) user trust. Explainable AI (XAI) is now referring to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. Focus: Thales XAI Platform is designed to provide explanation for a ML task (classification, regression, object detection, segmentation). Although Thales XAI Platform does provide different levels of explanation e.g., example-based, featuresbased, counterfactual using textual and visual representations, we emphasis only on the semantics-based explanation through knowledge graphs. ? Copyright c © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Critical Applications: From adapting a plane trajectory, stopping a train, refitting a boat to reconfiguring a satellite, all are examples of critical situations where explanation is a must-to-have to follow an AI system decision. 2 Why Knowledge Graphs for Explainable AI? State-of-the-Art Limitations: Most approaches limits explanation of ML systems to features involved in the data and model, or at best to examples, prototypes or counterfactuals. Explanation should go beyond correlation (features importance) and numerical similarity (local explanation). Opportunity: By expanding and linking initial (training, validation and test) data with entities in knowledge graphs, (i) context is encoded, (ii) connections and relations are exposed, and (iii) inference and causation are natively supported. Knowledge graphs are used for encoding better representation of data, structuring a ML model in a more interpretable way, and adopt a semantic similarity for local (instance-based) and global (model-based) explanation. 3 Thales XAI Platform: A Knowledge Graph Perspective (Semantic) Perspective: The platform is combining ML and reasoning functionalities to expose a human-like rational as explanation when (i) recognizing an object (in a raw image) of any class in a knowledge graph, (ii) predicting a link in a knowledge graph. Thales XAI Platform is using state-of-the-art Semantic Web tools for enriching input, output (class) data with DBpedia (4, 233, 000 resources) and domain-specific knowledge graphs, usually enterprise knowledge graphs. This is a crucial step for contextualizing training, validation, test data. Explainable ML Classifications: Starting from raw images, as unstructured data, but with class labels augmented with a domain knowledge graph, Thales XAI Platform relies on existing neural network architectures to build the most appropriate models. All confidence scores of output classes on any input image are updated based on the semantic description of the output classes. For instance, an input classified as a car will have a higher overall confidence score in case some properties of car in the knowledge graph are retrieved e.g., having wheels, being on a road. In addition the platform is embedding naturally explanation i.e., properties of the objects retrieved in both the raw data and knowledge graph. Explainable Relational Learning: Starting from relational data, structured as graph, and augmented with a domain knowledge graph, Thales XAI Platform relies on existing knowledge graph embeddings frameworks to build the most appropriate models. Explanation of any link prediction is retrieved by identifying representative hotspots in the knowledge graph i.e., connected parts of the graphs that negatively impact prediction accuracy when removed.","",""
1,"Antonia Gogoglou, C. B. Bruss, Keegan E. Hines","On the Interpretability and Evaluation of Graph Representation Learning",2019,"","","","",157,"2022-07-13 09:24:29","","","","",,,,,1,0.33,0,3,3,"With the rising interest in graph representation learning, a variety of approaches have been proposed to effectively capture a graph's properties. While these approaches have improved performance in graph machine learning tasks compared to traditional graph techniques, they are still perceived as techniques with limited insight into the information encoded in these representations. In this work, we explore methods to interpret node embeddings and propose the creation of a robust evaluation framework for comparing graph representation learning algorithms and hyperparameters. We test our methods on graphs with different properties and investigate the relationship between embedding training parameters and the ability of the produced embedding to recover the structure of the original graph in a downstream task.","",""
191,"M. Ahmad, A. Teredesai, C. Eckert","Interpretable Machine Learning in Healthcare",2018,"","","","",158,"2022-07-13 09:24:29","","10.1145/3233547.3233667","","",,,,,191,47.75,64,3,4,"This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.","",""
199,"Christoph Molnar, Giuseppe Casalicchio, B. Bischl","iml: An R package for Interpretable Machine Learning",2018,"","","","",159,"2022-07-13 09:24:29","","10.21105/joss.00786","","",,,,,199,49.75,66,3,4,"Complex, non-parametric models, which are typically used in machine learning, have proven to be successful in many prediction tasks. But these models usually operate as black boxes: While they are good at predicting, they are often not interpretable. Many inherently interpretable models have been suggested, which come at the cost of losing predictive power. Another option is to apply interpretability methods to a black box model after model training. Given the velocity of research on new machine learning models, it is preferable to have model-agnostic tools which can be applied to a random forest as well as to a neural network. Tools for model-agnostic interpretability methods should improve the adoption of machine learning.","",""
8,"Jo-Hsuan Wu, T. Y. A. Liu, W. Hsu, J. Ho, Chien-Chang Lee","Performance and Limitation of Machine Learning Algorithms for Diabetic Retinopathy Screening: Meta-analysis",2021,"","","","",160,"2022-07-13 09:24:29","","10.2196/23863","","",,,,,8,8.00,2,5,1,"Background Diabetic retinopathy (DR), whose standard diagnosis is performed by human experts, has high prevalence and requires a more efficient screening method. Although machine learning (ML)–based automated DR diagnosis has gained attention due to recent approval of IDx-DR, performance of this tool has not been examined systematically, and the best ML technique for use in a real-world setting has not been discussed. Objective The aim of this study was to systematically examine the overall diagnostic accuracy of ML in diagnosing DR of different categories based on color fundus photographs and to determine the state-of-the-art ML approach. Methods Published studies in PubMed and EMBASE were searched from inception to June 2020. Studies were screened for relevant outcomes, publication types, and data sufficiency, and a total of 60 out of 2128 (2.82%) studies were retrieved after study selection. Extraction of data was performed by 2 authors according to PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses), and the quality assessment was performed according to the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2). Meta-analysis of diagnostic accuracy was pooled using a bivariate random effects model. The main outcomes included diagnostic accuracy, sensitivity, and specificity of ML in diagnosing DR based on color fundus photographs, as well as the performances of different major types of ML algorithms. Results The primary meta-analysis included 60 color fundus photograph studies (445,175 interpretations). Overall, ML demonstrated high accuracy in diagnosing DR of various categories, with a pooled area under the receiver operating characteristic (AUROC) ranging from 0.97 (95% CI 0.96-0.99) to 0.99 (95% CI 0.98-1.00). The performance of ML in detecting more-than-mild DR was robust (sensitivity 0.95; AUROC 0.97), and by subgroup analyses, we observed that robust performance of ML was not limited to benchmark data sets (sensitivity 0.92; AUROC 0.96) but could be generalized to images collected in clinical practice (sensitivity 0.97; AUROC 0.97). Neural network was the most widely used method, and the subgroup analysis revealed a pooled AUROC of 0.98 (95% CI 0.96-0.99) for studies that used neural networks to diagnose more-than-mild DR. Conclusions This meta-analysis demonstrated high diagnostic accuracy of ML algorithms in detecting DR on color fundus photographs, suggesting that state-of-the-art, ML-based DR screening algorithms are likely ready for clinical applications. However, a significant portion of the earlier published studies had methodology flaws, such as the lack of external validation and presence of spectrum bias. The results of these studies should be interpreted with caution.","",""
8,"Lakshya Singhal, Yash Garg, Philip Yang, A. Tabaie, A. Wong, Akram Mohammed, Lokesh Chinthala, D. Kadaria, A. Sodhi, A. Holder, A. Esper, J. Blum, R. Davis, G. Clifford, G. Martin, R. Kamaleswaran","eARDS: A multi-center validation of an interpretable machine learning algorithm of early onset Acute Respiratory Distress Syndrome (ARDS) among critically ill adults with COVID-19",2021,"","","","",161,"2022-07-13 09:24:29","","10.1371/journal.pone.0257056","","",,,,,8,8.00,1,16,1,"We present an interpretable machine learning algorithm called ‘eARDS’ for predicting ARDS in an ICU population comprising COVID-19 patients, up to 12-hours before satisfying the Berlin clinical criteria. The analysis was conducted on data collected from the Intensive care units (ICU) at Emory Healthcare, Atlanta, GA and University of Tennessee Health Science Center, Memphis, TN and the Cerner® Health Facts Deidentified Database, a multi-site COVID-19 EMR database. The participants in the analysis consisted of adults over 18 years of age. Clinical data from 35,804 patients who developed ARDS and controls were used to generate predictive models that identify risk for ARDS onset up to 12-hours before satisfying the Berlin criteria. We identified salient features from the electronic medical record that predicted respiratory failure among this population. The machine learning algorithm which provided the best performance exhibited AUROC of 0.89 (95% CI = 0.88–0.90), sensitivity of 0.77 (95% CI = 0.75–0.78), specificity 0.85 (95% CI = 085–0.86). Validation performance across two separate health systems (comprising 899 COVID-19 patients) exhibited AUROC of 0.82 (0.81–0.83) and 0.89 (0.87, 0.90). Important features for prediction of ARDS included minimum oxygen saturation (SpO2), standard deviation of the systolic blood pressure (SBP), O2 flow, and maximum respiratory rate over an observational window of 16-hours. Analyzing the performance of the model across various cohorts indicates that the model performed best among a younger age group (18–40) (AUROC = 0.93 [0.92–0.94]), compared to an older age group (80+) (AUROC = 0.81 [0.81–0.82]). The model performance was comparable on both male and female groups, but performed significantly better on the severe ARDS group compared to the mild and moderate groups. The eARDS system demonstrated robust performance for predicting COVID19 patients who developed ARDS at least 12-hours before the Berlin clinical criteria, across two independent health systems.","",""
7,"E. Rozos, P. Dimitriadis, V. Bellos","Machine Learning in Assessing the Performance of Hydrological Models",2021,"","","","",162,"2022-07-13 09:24:29","","10.3390/hydrology9010005","","",,,,,7,7.00,2,3,1,"Machine learning has been employed successfully as a tool virtually in every scientific and technological field. In hydrology, machine learning models first appeared as simple feed-forward networks that were used for short-term forecasting, and have evolved into complex models that can take into account even the static features of catchments, imitating the hydrological experience. Recent studies have found machine learning models to be robust and efficient, frequently outperforming the standard hydrological models (both conceptual and physically based). However, and despite some recent efforts, the results of the machine learning models require significant effort to interpret and derive inferences. Furthermore, all successful applications of machine learning in hydrology are based on networks of fairly complex topology that require significant computational power and CPU time to train. For these reasons, the value of the standard hydrological models remains indisputable. In this study, we suggest employing machine learning models not as a substitute for hydrological models, but as an independent tool to assess their performance. We argue that this approach can help to unveil the anomalies in catchment data that do not fit in the employed hydrological model structure or configuration, and to deal with them without compromising the understanding of the underlying physical processes.","",""
1111,"T. Baltrušaitis, Chaitanya Ahuja, Louis-Philippe Morency","Multimodal Machine Learning: A Survey and Taxonomy",2017,"","","","",163,"2022-07-13 09:24:29","","10.1109/TPAMI.2018.2798607","","",,,,,1111,222.20,370,3,5,"Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.","",""
4,"A. Mei, I. Milosavljevic, A. L. Simpson, Valerie A. Smetanka, Colin P. Feeney, Shay M. Seguin, S. D. Ha, W. Ha, M. Reed","Optimization of quantum-dot qubit fabrication via machine learning",2020,"","","","",164,"2022-07-13 09:24:29","","10.1063/5.0040967","","",,,,,4,2.00,0,9,2,"Precise nanofabrication represents a critical challenge to developing semiconductor quantum-dot qubits for practical quantum computation. Here, we design and train a convolutional neural network to interpret in-line scanning electron micrographs and quantify qualitative features affecting device functionality. The high-throughput strategy is exemplified by optimizing a model lithographic process within a five-dimensional design space and by demonstrating a new approach to address lithographic proximity effects. The present results emphasize the benefits of machine learning for developing robust processes, shortening development cycles, and enforcing quality control during qubit fabrication.","",""
4,"Kengo Ito, Xiangru Xu, J. Kikuchi","Improved Prediction of Carbonless NMR Spectra by the Machine Learning of Theoretical and Fragment Descriptors for Environmental Mixture Analysis.",2021,"","","","",165,"2022-07-13 09:24:29","","10.1021/acs.analchem.1c00756","","",,,,,4,4.00,1,3,1,"As the first multidimensional NMR approach, 2D J-resolved (2DJ) spectroscopy is distinguished by signal resolution and detection sensitivity with remarkable advantages for the exhaustive evaluation of complex mixtures and environmental samples due to its carbonless feature without the requirement of 13C connectivity. Generally, the 2DJ signal assignment of metabolic mixtures is problematic in spite of references to experimental NMR databases, owing to the existence of metabolic ""dark matter."" In this study, a new method to predict 2DJ spectra was developed with a combination of quantum mechanical (QM) computation and machine learning (ML). The predictive accuracy of J-coupling constants was evaluated using validated data. The root-mean-square deviation (RMSD) for QM computation was 3.52 Hz, while the RMSD for QM + ML was 1.21 Hz, indicating a substantial increase in predictive accuracy. The proposed model was applied to predict the 2DJ spectra of 60 standard substances and 55 components of seawater. Furthermore, two practical environmental samples were used to evaluate the robustness of the constructed predictive model. A J-coupling tree and J-split spectra produced from QM + ML of aliphatic moieties had good consistency with the experimental data, as compared with the theoretical data produced by QM computation. The predicted J-coupling tree for the J-coupling multiplet analysis of freely rotating bonds in the complex mixture, which is traditionally difficult, was interpretable. In addition, in silico identification of the J-split 1H NMR signals, which was independent of experimental databases, aided in the discovery of new components in a mixture.","",""
4,"Alexandra Renouard, A. Maggi, M. Grunberg, C. Doubre, C. Hibert","Toward False Event Detection and Quarry Blast versus Earthquake Discrimination in an Operational Setting Using Semiautomated Machine Learning",2021,"","","","",166,"2022-07-13 09:24:29","","10.1785/0220200305","","",,,,,4,4.00,1,5,1,"  Small-magnitude earthquakes shed light on the spatial and magnitude distribution of natural seismicity, as well as its rate and occurrence, especially in stable continental regions where natural seismicity remains difficult to explain under slow strain-rate conditions. However, capturing them in catalogs is strongly hindered by signal-to-noise ratio issues, resulting in high rates of false and man-made events also being detected. Accurate and robust discrimination of these events is critical for optimally detecting small earthquakes. This requires uncovering recurrent salient features that can rapidly distinguish first false events from real events, then earthquakes from man-made events (mainly quarry blasts), despite high signal variability and noise content. In this study, we combined the complementary strengths of human and interpretable rule-based machine-learning algorithms for solving this classification problem. We used human expert knowledge to co-create two reliable machine-learning classifiers through human-assisted selection of classification features and review of events with uncertain classifier predictions. The two classifiers are integrated into the SeisComP3 operational monitoring system. The first one discards false events from the set of events obtained with a low short-term average/long-term average threshold; the second one labels the remaining events as either earthquakes or quarry blasts. When run in an operational setting, the first classifier correctly detected more than 99% of false events and just over 93% of earthquakes; the second classifier correctly labeled 95% of quarry blasts and 96% of earthquakes. After a manual review of the second classifier low-confidence outputs, the final catalog contained fewer than 2% of misclassified events. These results confirm that machine learning strengthens the quality of earthquake catalogs and that the performance of machine-learning classifiers can be improved through human expertise. Our study promotes a broader implication of hybrid intelligence monitoring within seismological observatories.","",""
4,"A. Serban, Joost Visser","An Empirical Study of Software Architecture for Machine Learning",2021,"","","","",167,"2022-07-13 09:24:29","","","","",,,,,4,4.00,2,2,1,"Specific developmental and operational characteristics of machine learning (ML) components, as well as their inherent uncertainty, demand robust engineering principles are used to ensure their quality. We aim to determine how software systems can be (re-) architected to enable robust integration of ML components. Towards this goal, we conducted a mixed-methods empirical study consisting of (i) a systematic literature review to identify the challenges and their solutions in software architecture for ML, (ii) semi-structured interviews with practitioners to qualitatively complement the initial findings, and (iii) a survey to quantitatively validate the challenges and their solutions. In total, we compiled and validated twenty challenges and solutions for (re-) architecting systems with ML components. Our results indicate, for example, that traditional software architecture challenges (e.g., component coupling) also play an important role when using ML components; along new ML specific challenges (e.g., the need for continuous retraining). Moreover, the results indicate that ML heightened decision drivers, such as privacy, play a marginal role compared to traditional decision drivers, such as scalability or interoperability. Using the survey, we were able to establish a link between architectural solutions and software quality attributes; which enabled us to provide twenty architectural tactics used for satisfying individual quality requirements of systems with ML components. Altogether, the results can be interpreted as an empirical framework that supports the process of (re-) architecting software systems with ML components.","",""
3,"Oliver Aasmets, Kreete Lüll, Jennifer M. Lang, Calvin Pan, J. Kuusisto, K. Fischer, M. Laakso, A. Lusis, E. Org","Machine Learning Reveals Time-Varying Microbial Predictors with Complex Effects on Glucose Regulation",2020,"","","","",168,"2022-07-13 09:24:29","","10.1128/mSystems.01191-20","","",,,,,3,1.50,0,9,2,"Recent studies have shown a clear link between gut microbiota and type 2 diabetes. However, current results are based on cross-sectional studies that aim to determine the microbial dysbiosis when the disease is already prevalent. ABSTRACT The incidence of type 2 diabetes (T2D) has been increasing globally, and a growing body of evidence links type 2 diabetes with altered microbiota composition. Type 2 diabetes is preceded by a long prediabetic state characterized by changes in various metabolic parameters. We tested whether the gut microbiome could have predictive potential for T2D development during the healthy and prediabetic disease stages. We used prospective data of 608 well-phenotyped Finnish men collected from the population-based Metabolic Syndrome in Men (METSIM) study to build machine learning models for predicting continuous glucose and insulin measures in a shorter (1.5 year) and longer (4 year) period. Our results show that the inclusion of the gut microbiome improves prediction accuracy for modeling T2D-associated parameters such as glycosylated hemoglobin and insulin measures. We identified novel microbial biomarkers and described their effects on the predictions using interpretable machine learning techniques, which revealed complex linear and nonlinear associations. Additionally, the modeling strategy carried out allowed us to compare the stability of model performance and biomarker selection, also revealing differences in short-term and long-term predictions. The identified microbiome biomarkers provide a predictive measure for various metabolic traits related to T2D, thus providing an additional parameter for personal risk assessment. Our work also highlights the need for robust modeling strategies and the value of interpretable machine learning. IMPORTANCE Recent studies have shown a clear link between gut microbiota and type 2 diabetes. However, current results are based on cross-sectional studies that aim to determine the microbial dysbiosis when the disease is already prevalent. In order to consider the microbiome as a factor in disease risk assessment, prospective studies are needed. Our study is the first study that assesses the gut microbiome as a predictive measure for several type 2 diabetes-associated parameters in a longitudinal study setting. Our results revealed a number of novel microbial biomarkers that can improve the prediction accuracy for continuous insulin measures and glycosylated hemoglobin levels. These results make the prospect of using the microbiome in personalized medicine promising.","",""
3,"R. Barker, S. Barker, M. Cracknell, Elizabeth D. Stock, G. Holmes","Quantitative Mineral Mapping of Drill Core Surfaces II: Long-Wave Infrared Mineral Characterization Using μXRF and Machine Learning",2021,"","","","",169,"2022-07-13 09:24:29","","10.5382/econgeo.4804","","",,,,,3,3.00,1,5,1,"Long-wave infrared (LWIR) spectra can be interpreted using a Random Forest machine learning approach to predict mineral species and abundances. In this study, hydrothermally altered carbonate rock core samples from the Fourmile Carlin-type Au discovery, Nevada, were analyzed by LWIR and micro-X-ray fluorescence (μXRF). Linear programming-derived mineral abundances from quantified μXRF data were used as training data to construct a series of Random Forest regression models. The LWIR Random Forest models produced mineral proportion estimates with root mean square errors of 1.17 to 6.75% (model predictions) and 1.06 to 6.19% (compared to quantitative X-ray diffraction data) for calcite, dolomite, kaolinite, white mica, phlogopite, K-feldspar, and quartz. These results are comparable to the error of proportion estimates from linear spectral deconvolution (±7–15%), a commonly used spectral unmixing technique. Having a mineralogical and chemical training data set makes it possible to identify and quantify mineralogy and provides a more robust and meaningful LWIR spectral interpretation than current methods of utilizing a spectral library or spectral end-member extraction. Using the method presented here, LWIR spectroscopy can be used to overcome the limitations inherent with the use of short-wave infrared (SWIR) in fine-grained, low reflectance rocks. This new approach can be applied to any deposit type, improving the accuracy and speed of infrared data interpretation.","",""
3,"Tao Zhong, Zian Zhuang, Xiao-fei Dong, Ka-hing Wong, W. Wong, Jian Wang, D. He, Shengyuan Liu","Predicting Antituberculosis Drug–Induced Liver Injury Using an Interpretable Machine Learning Method: Model Development and Validation Study",2021,"","","","",170,"2022-07-13 09:24:29","","10.2196/29226","","",,,,,3,3.00,0,8,1,"Background Tuberculosis (TB) is a pandemic, being one of the top 10 causes of death and the main cause of death from a single source of infection. Drug-induced liver injury (DILI) is the most common and serious side effect during the treatment of TB. Objective We aim to predict the status of liver injury in patients with TB at the clinical treatment stage. Methods We designed an interpretable prediction model based on the XGBoost algorithm and identified the most robust and meaningful predictors of the risk of TB-DILI on the basis of clinical data extracted from the Hospital Information System of Shenzhen Nanshan Center for Chronic Disease Control from 2014 to 2019. Results In total, 757 patients were included, and 287 (38%) had developed TB-DILI. Based on values of relative importance and area under the receiver operating characteristic curve, machine learning tools selected patients’ most recent alanine transaminase levels, average rate of change of patients’ last 2 measures of alanine transaminase levels, cumulative dose of pyrazinamide, and cumulative dose of ethambutol as the best predictors for assessing the risk of TB-DILI. In the validation data set, the model had a precision of 90%, recall of 74%, classification accuracy of 76%, and balanced error rate of 77% in predicting cases of TB-DILI. The area under the receiver operating characteristic curve score upon 10-fold cross-validation was 0.912 (95% CI 0.890-0.935). In addition, the model provided warnings of high risk for patients in advance of DILI onset for a median of 15 (IQR 7.3-27.5) days. Conclusions Our model shows high accuracy and interpretability in predicting cases of TB-DILI, which can provide useful information to clinicians to adjust the medication regimen and avoid more serious liver injury in patients.","",""
3,"A. Ounajim, M. Billot, L. Goudman, P. Louis, Y. Slaoui, M. Roulaud, B. Bouche, P. Page, B. Lorgeoux, Sandrine Baron, Nihel Adjali, K. Nivole, Nicolas Naiditch, Chantal Wood, Raphaël Rigoard, R. David, M. Moens, P. Rigoard","Machine Learning Algorithms Provide Greater Prediction of Response to SCS Than Lead Screening Trial: A Predictive AI-Based Multicenter Study",2021,"","","","",171,"2022-07-13 09:24:29","","10.3390/jcm10204764","","",,,,,3,3.00,0,18,1,"Persistent pain after spinal surgery can be successfully addressed by spinal cord stimulation (SCS). International guidelines strongly recommend that a lead trial be performed before any permanent implantation. Recent clinical data highlight some major limitations of this approach. First, it appears that patient outco mes, with or without lead trial, are similar. In contrast, during trialing, infection rate drops drastically within time and can compromise the therapy. Using composite pain assessment experience and previous research, we hypothesized that machine learning models could be robust screening tools and reliable predictors of long-term SCS efficacy. We developed several algorithms including logistic regression, regularized logistic regression (RLR), naive Bayes classifier, artificial neural networks, random forest and gradient-boosted trees to test this hypothesis and to perform internal and external validations, the objective being to confront model predictions with lead trial results using a 1-year composite outcome from 103 patients. While almost all models have demonstrated superiority on lead trialing, the RLR model appears to represent the best compromise between complexity and interpretability in the prediction of SCS efficacy. These results underscore the need to use AI-based predictive medicine, as a synergistic mathematical approach, aimed at helping implanters to optimize their clinical choices on daily practice.","",""
45,"H. Escalante, S. Escalera, I. Guyon, Xavier Baró, Yağmur Güçlütürk, Umut Güçlü, M. V. Gerven","Explainable and Interpretable Models in Computer Vision and Machine Learning",2018,"","","","",172,"2022-07-13 09:24:29","","10.1007/978-3-319-98131-4","","",,,,,45,11.25,6,7,4,"","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",173,"2022-07-13 09:24:29","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
1,"R. Pettit, Robert Fullem, Chao Cheng, Chris Amos","Artificial intelligence, machine learning, and deep learning for clinical outcome prediction",2021,"","","","",174,"2022-07-13 09:24:29","","10.1042/ETLS20210246","","",,,,,1,1.00,0,4,1,"AI is a broad concept, grouping initiatives that use a computer to perform tasks that would usually require a human to complete. AI methods are well suited to predict clinical outcomes. In practice, AI methods can be thought of as functions that learn the outcomes accompanying standardized input data to produce accurate outcome predictions when trialed with new data. Current methods for cleaning, creating, accessing, extracting, augmenting, and representing data for training AI clinical prediction models are well defined. The use of AI to predict clinical outcomes is a dynamic and rapidly evolving arena, with new methods and applications emerging. Extraction or accession of electronic health care records and combining these with patient genetic data is an area of present attention, with tremendous potential for future growth. Machine learning approaches, including decision tree methods of Random Forest and XGBoost, and deep learning techniques including deep multi-layer and recurrent neural networks, afford unique capabilities to accurately create predictions from high dimensional, multimodal data. Furthermore, AI methods are increasing our ability to accurately predict clinical outcomes that previously were difficult to model, including time-dependent and multi-class outcomes. Barriers to robust AI-based clinical outcome model deployment include changing AI product development interfaces, the specificity of regulation requirements, and limitations in ensuring model interpretability, generalizability, and adaptability over time.","",""
2,"S. Newman, R. Furbank","Explainable machine learning models of major crop traits from satellite-monitored continent-wide field trial data",2021,"","","","",175,"2022-07-13 09:24:29","","10.1101/2021.03.08.434495","","",,,,,2,2.00,1,2,1,"Four species of grass generate half of all human-consumed calories1. However, abundant biological data on species that produce our food remains largely inaccessible, imposing direct barriers to understanding crop yield and fitness traits. Here, we assemble and analyse a continent-wide database of field experiments spanning ten years and hundreds of thousands of machine-phenotyped populations of ten major crop species. Training an ensemble of machine learning models, using thousands of variables capturing weather, ground-sensor, soil, chemical and fertiliser dosage, management, and satellite data, produces robust cross-continent yield models exceeding R2 = 0.8 prediction accuracy. In contrast to ‘black box’ analytics, detailed interrogation of these models reveals fundamental drivers of crop behaviour and complex interactions predicting yield and agronomic traits. These results demonstrate the capacity of machine learning models to build unified, interpretable, and explainable models of crop behaviour, and highlight the powerful role of data in the future of food.","",""
1,"Nicola Loi, C. Borile, Daniele Ucci","Towards an Automated Pipeline for Detecting and Classifying Malware through Machine Learning",2021,"","","","",176,"2022-07-13 09:24:29","","","","",,,,,1,1.00,0,3,1,"The constant growth in the number of malware software or code fragment potentially harmful for computers and information networks and the use of sophisticated evasion and obfuscation techniques have seriously hindered classic signature-based approaches. On the other hand, malware detection systems based on machine learning techniques started offering a promising alternative to standard approaches, drastically reducing analysis time and turning out to be more robust against evasion and obfuscation techniques. In this paper, we propose a malware taxonomic classification pipeline able to classify Windows Portable Executable files (PEs). Given an input PE sample, it is first classified as either malicious or benign. If malicious, the pipeline further analyzes it in order to establish its threat type, family, and behavior(s). We tested the proposed pipeline on the open source dataset EMBER, containing approximately 1 million PE samples, analyzed through static analysis. Obtained malware detection results are comparable to other academic works in the current state of art and, in addition, we provide an in-depth classification of malicious samples. Models used in the pipeline provides interpretable results which can help security analysts in better understanding decisions taken by the automated pipeline.","",""
2,"D. Devakumar, Goutham Sunny, B. Sasidharan, S. Bowen, Ambily Nadaraj, L. Jeyseelan, Manu Mathew, A. Irodi, R. Isiah, S. Pavamani, S. John, H. T. T Thomas","Framework for Machine Learning of CT and PET Radiomics to Predict Local Failure after Radiotherapy in Locally Advanced Head and Neck Cancers",2021,"","","","",177,"2022-07-13 09:24:29","","10.4103/jmp.JMP_6_21","","",,,,,2,2.00,0,12,1,"Context: Cancer Radiomics is an emerging field in medical imaging and refers to the process of converting routine radiological images that are typically qualitatively interpreted to quantifiable descriptions of the tumor phenotypes and when combined with statistical analytics can improve the accuracy of clinical outcome prediction models. However, to understand the radiomic features and their correlation to molecular changes in the tumor, first, there is a need for the development of robust image analysis methods, software tools and statistical prediction models which is often limited in low- and middle-income countries (LMIC). Aims: The aim is to build a framework for machine learning of radiomic features of planning computed tomography (CT) and positron emission tomography (PET) using open source radiomics and data analytics platforms to make it widely accessible to clinical groups. The framework is tested in a small cohort to predict local disease failure following radiation treatment for head-and-neck cancer (HNC). The predictors were also compared with the existing Aerts HNC radiomics signature. Settings and Design: Retrospective analysis of patients with locally advanced HNC between 2017 and 2018 and 31 patients with both pre- and post-radiation CT and evaluation PET were selected. Subjects and Methods: Tumor volumes were delineated on baseline PET using the semi-automatic adaptive-threshold algorithm and propagated to CT; PyRadiomics features (total of 110 under shape/intensity/texture classes) were extracted. Two feature-selection methods were tested for model stability. Models were built based on least absolute shrinkage and selection operator-logistic and Ridge regression of the top pretreatment radiomic features and compared to Aerts' HNC-signature. Average model performance across all internal validation test folds was summarized by the area under the receiver operator curve (ROC). Results: Both feature selection methods selected CT features MCC (GLCM), SumEntropy (GLCM) and Sphericity (Shape) that could predict the binary failure status in the cross-validated group and achieved an AUC >0.7. However, models using Aerts' signature features (Energy, Compactness, GLRLM-GrayLevelNonUniformity and GrayLevelNonUniformity-HLH wavelet) could not achieve a clear separation between outcomes (AUC = 0.51–0.54). Conclusions: Radiomics pipeline included open-source workflows which makes it adoptable in LMIC countries. Additional independent validation of data is crucial for the implementation of radiomic models for clinical risk stratification.","",""
2,"N. Frolov, Muhammad Salman Kabir, V. Maksimenko, A. Hramov","Machine learning evaluates changes in functional connectivity under a prolonged cognitive load.",2021,"","","","",178,"2022-07-13 09:24:29","","10.1063/5.0070493","","",,,,,2,2.00,1,4,1,"One must be aware of the black-box problem by applying machine learning models to analyze high-dimensional neuroimaging data. It is due to a lack of understanding of the internal algorithms or the input features upon which most models make decisions despite outstanding performance in classification, pattern recognition, and prediction. Here, we approach the fundamentally high-dimensional problem of classifying cognitive brain states based on functional connectivity by selecting and interpreting the most relevant input features. Specifically, we consider the alterations in the cortical synchrony under a prolonged cognitive load. Our study highlights the advances of this machine learning method in building a robust classification model and percept-related prestimulus connectivity changes over the conventional trial-averaged statistical analysis.","",""
2,"Nick Fountain-Jones, Megan L. Smith, F. Austerlitz","Machine learning in molecular ecology",2021,"","","","",179,"2022-07-13 09:24:29","","10.1111/1755-0998.13532","","",,,,,2,2.00,1,3,1,"Advances in nextgeneration sequencing (NGS) platforms are allowing researchers to routinely collate large genomewide data sets to address a variety of ecological questions. However, with this big data comes big analytical challenges that are increasingly addressed using machine learning (for a review, see Schrider & Kern, 2018). Machine learning is a subfield of artificial intelligence and represents a conglomeration of methods where predictive accuracy is the primary goal (e.g., Belcaid & Toonen, 2015; Breiman, 2001; Elith et al., 2008; Lucas, 2020). Machine learning assumes that the datagenerating process is unknown and complex and finds the dominant patterns by learning the relationships between inputs and responses (Elith et al., 2008). Broadly, machine learning differs from other statistical approaches in two important ways. The first is that predictive performance drives model formulation rather than model selection or expert opinion, and the second is there is less emphasis on model selection ( Breiman, 2001; Lucas, 2020). For these reasons, machine learning has the reputation for being less interpretable and difficult to apply rigorously (Elith et al., 2008; Lucas, 2020; Molnar, 2018). However, in parallel with the revolution of sequencing techniques, there has also been a revolution in data science in terms of predictive performance and techniques to interpret machine learning models (FountainJones et al., 2019; Lucas, 2020; Molnar, 2018). There are now streamlined R and Python packages that make the robust use of algorithms from support vector machines (SVMs) to neural networks readily achievable (e.g., Abadi et al., 2015; Kuhn & Wickham, 2020, see Text Box 1 for some important machine learning terminology). Moreover, other statistical paradigms such as approximate Bayesian computation (ABC) are being applied sidebyside or within machine learning frameworks to enhance the utility of these approaches (e.g., Carlson, 2020; Raynal et al., 2019). The ability of machine learning algorithms to build powerful predictive models that capture complex nonlinear responses with minimal statistical assumptions has been harnessed by most molecular ecology subdisciplines for decades. For example, machine learning models were developed before the turn of the millennium to classify normal or cancerous tissue based on transcription profiles (Furey et al., 2000). Not long after gradient boosting models (GBMs) were developed (e.g., Hastie et al., 2009), researchers were applying the approach to classify population genetics models based on a suite of summary statistics such as Tajima's θπ (Lin et al., 2011). In addition, extensions of the popular random forest algorithm have been utilized in ecological genetics to untangle the drivers of climate adaptation (Fitzpatrick & Keller, 2015). Generally, however, advances in computer science and machine learning are slow to filter down to ecologists (Belcaid & Toonen, 2015; Elith & Hastie, 2008; FountainJones et al., 2019), partly through unfamiliarity with these types of approaches but also because of the rapid rate of advance in the data science field. This Special Issue aims to help expand the use of machine learning approaches and to help bring advances in data science to the toolkits of molecular ecologists. This issue comprises 17 papers grouped into four sections covering a diverse variety of molecular ecology subdisciplines. The first section covers how machine learning can be applied to make inferences about population demography. We further group these papers algorithmically with four papers utilizing random forest architecture and the remaining four using neural networks. The second section highlights how machine learning can detect signatures of selection across loci. The third section highlights how these methods can be applied to untangle the complex ecological drivers of genomic change (‘ecological genomics’) and species community dynamics. The last section explores how advances in machine learning can provide insights into species limits and contribute to biodiversity monitoring.","",""
1,"Ana Kostovska, Matej Petković, Tomaz Stepisnik, L. Lucas, T. Finn, José Antonio Martinez Heras, P. Panov, S. Džeroski, A. Donati, N. Simidjievski, D. Kocev","GalaxAI: Machine learning toolbox for interpretable analysis of spacecraft telemetry data",2021,"","","","",180,"2022-07-13 09:24:29","","10.1109/smc-it51442.2021.00013","","",,,,,1,1.00,0,11,1,"We present GalaxAI - a versatile machine learning toolbox for efficient and interpretable end-to-end analysis of spacecraft telemetry data. GalaxAI employs various machine learning algorithms for multivariate time series analyses, classification, regression and structured output prediction, capable of handling high-throughput heterogeneous data. These methods allow for the construction of robust and accurate predictive models, that are in turn applied to different tasks of spacecraft monitoring and operations planning. More importantly, besides the accurate building of models, GalaxAI implements a visualisation layer, providing mission specialists and operators with a full, detailed and interpretable view of the data analysis process. We show the utility and versatility of GalaxAI on two use-cases concerning two different spacecraft: i) analysis and planning of Mars Express thermal power consumption and ii) predicting of INTEGRAL’s crossings through Van Allen belts.","",""
0,"M. Ntampaka, Matthew Ho, B. Nord","Building Trustworthy Machine Learning Models for Astronomy",2021,"","","","",181,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,3,1,". Astronomy is entering an era of data-driven discovery, due in part to modern machine learning (ML) techniques enabling powerful new ways to interpret observations. This shift in our scientiﬁc approach requires us to consider whether we can trust the black box. Here, we overview methods for an often-overlooked step in the development of ML models: building community trust in the algorithms. Trust is an essential ingredient not just for creating more robust data analysis techniques, but also for building conﬁdence within the astronomy community to embrace machine learning methods and results.","",""
0,"S. Pande, Bineet Kumar Jha","Character Recognition System for Devanagari Script Using Machine Learning Approach",2021,"","","","",182,"2022-07-13 09:24:29","","10.1109/ICCMC51019.2021.9418028","","",,,,,0,0.00,0,2,1,"It is a very difficult task to manually process the handwritten documents due to varieties of handwritten scripts and lack of associated language dictionary to interpret documents. Most of the large companies as well as small-scale industries want to automate the process of script recognition. The big challenge is to make machines recognize the hand-printed scripts. Humans can recognize handwritten or hand-printed words after gaining knowledge of a specific language. In the same way, machines should be trained to recognize the handwritten scripts. This process of transferring human knowledge to computers should be automated. The proposed research work attempts to automate the character recognition system for Devanagari script using various machine learning classifiers like Decision Tree classifier, Nearest Centroid classifier, K Nearest Neighbors classifier, Extra Trees classifiers and Random Forest classifier. The performance of all the classifiers is evaluated using accuracy parameter as success criteria. The Extra Trees classifiers and Random Forest classifier is proved to better than other classifiers with 78% and 77% of accuracy respectively. The robustness to picture quality, writing style, font size is the novelty of the OCR system which makes it ideal to use.","",""
0,"B. Kaiser, J. Saenz, M. Sonnewald, D. Livescu","Objective discovery of dominant dynamical processes with machine learning",2021,"","","","",183,"2022-07-13 09:24:29","","10.21203/rs.3.rs-745356/v1","","",,,,,0,0.00,0,4,1,"  Significant advances in the understanding and modeling of dynamical systems has been enabled by the identification of processes that locally and approximately dominate system behavior, or dynamical regimes. The conventional regime identification method involves tedious and ad hoc parsing of data to judiciously obtain scales to ascertain which governing equation terms are dominant in each regime. Surprisingly, no objective and universally applicable criterion exists to robustly identify dynamical regimes in an unbiased manner, neither for conventional nor for machine learning-based methods of analysis. Here, we formally define dynamical regime identification as an optimization problem by using a verification criterion, and we show that an unsupervised learning framework can automatically and credibly identify regimes. This eliminates reliance upon ad hoc conventional analyses, with vast potential to accelerate discovery. Our verification criterion also enables unbiased comparison of regimes identified by different methods. In addition to diagnostic applications, the verification criterion and learning framework are immediately useful for data-driven dynamical process modeling, and are relevant to researchers interested in the development of inherently interpretable methods for scientific machine learning. Automation of this kind of approximate mechanistic analysis is necessary for scientists to gain new dynamical insights from increasingly large data streams.","",""
0,"A. Serban, Joost Visser","Adapting Software Architectures to Machine Learning Challenges",2021,"","","","",184,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,2,1,"Unique developmental and operational characteristics of machine learning (ML) components as well as their inherent uncertainty demand robust engineering principles are used to ensure their quality. We aim to determine how software systems can be (re-) architected to enable robust integration of ML components. Towards this goal, we conducted a mixed-methods empirical study consisting of (i) a systematic literature review to identify the challenges and their solutions in software architecture for ML, (ii) semi-structured interviews with practitioners to qualitatively complement the initial findings and (iii) a survey to quantitatively validate the challenges and their solutions. We compiled and validated twenty challenges and solutions for (re-) architecting systems with ML components. Our results indicate, for example, that traditional software architecture challenges (e.g., component coupling) also play an important role when using ML components; along with new ML specific challenges (e.g., the need for continuous retraining). Moreover, the results indicate that ML heightened decision drivers, such as privacy, play a marginal role compared to traditional decision drivers, such as scalability. Using the survey we were able to establish a link between architectural solutions and software quality attributes, which enabled us to provide twenty architectural tactics used to satisfy individual quality requirements of systems with ML components. Altogether, the results of the study can be interpreted as an empirical framework that supports the process of (re-) architecting software systems with ML components.","",""
0,"Tochukwu Idika, Ismail Akturk","Attack-Centric Approach for Evaluating Transferability of Adversarial Samples in Machine Learning Models",2021,"","","","",185,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,2,1,"Transferability of adversarial samples became a serious concern due to their impact on the reliability of machine learning system deployments, as they find their way into many critical applications. Knowing factors that influence transferability of adversarial samples can assist experts to make informed decisions on how to build robust and reliable machine learning systems. The goal of this study is to provide insights on the mechanisms behind the transferability of adversarial samples through an attack-centric approach. This attack-centric perspective interprets how adversarial samples would transfer by assessing the impact of machine learning attacks (that generated them) on a given input dataset. To achieve this goal, we generated adversarial samples using attacker models and transferred these samples to victim models. We analyzed the behavior of adversarial samples on victim models and outlined four factors that can influence the transferability of adversarial samples. Although these factors are not necessarily exhaustive, they provide useful insights to researchers and practitioners of machine learning systems.","",""
0,"H. Tomas Rube, Chaitanya Rastogi, Siqian Feng, J. Kribelbauer, Allyson Li, Basheer Becerra, Lucas A. N. Melo, Bach-Viet Do, Xiaoting Li, Hammaad Adam, Neel H. Shah, R. Mann, H. Bussemaker","Probing molecular specificity with deep sequencing and biophysically interpretable machine learning",2021,"","","","",186,"2022-07-13 09:24:29","","10.1101/2021.06.30.450414","","",,,,,0,0.00,0,13,1,"Quantifying sequence-specific protein-ligand interactions is critical for understanding and exploiting numerous cellular processes, including gene regulation and signal transduction. Next-generation sequencing (NGS) based assays are increasingly being used to profile these interactions with high-throughput. However, these assays do not provide the biophysical parameters that have long been used to uncover the quantitative rules underlying sequence recognition. We developed a highly flexible machine learning framework, called ProBound, to define sequence recognition in terms of biophysical parameters based on NGS data. ProBound quantifies transcription factor (TF) behavior with models that accurately predict binding affinity over a range exceeding that of previous resources, captures the impact of DNA modifications and conformational flexibility of multi-TF complexes, and infers specificity directly from in vivo data such as ChIP-seq without peak calling. When coupled with a new assay called Kd-seq, it determines the absolute affinity of protein-ligand interactions. It can also profile the kinetics of kinase-substrate interactions. By constructing a biophysically robust foundation for profiling sequence recognition, ProBound opens up new avenues for decoding biological networks and rationally engineering protein-ligand interactions.","",""
0,"V. Yasaswini, P. Reeshika, K. Roy, N. Kalpana, Rajesh Yamparala","Drowsiness Detection Using Machine Learning Algorithms",2021,"","","","",187,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,5,1,": The abstract presents a literature review of driver drowsiness detection based on behavioural measures using machine learning techniques. Faces contain information that can be used to interpret levels of drowsiness. There are many facial features that can be extracted from the face to infer the level of drowsiness. However, the development of a drowsiness detection system that yields reliable and accurate results is a challenging task as it requires accurate and robust algorithms. A wide range of techniques has been examined to detect driver drowsiness in the past. As a result, machine learning techniques which include convolution neural networks in the context of drowsiness detection. Here convolution neural networks performed better than any other techniques.","",""
0,"Kaiyu Yang","1 Machine Learning for Reasoning",2021,"","","","",188,"2022-07-13 09:24:29","","","","",,,,,0,0.00,0,1,1,"Reasoning is a core component of human intelligence that machines still struggle with. I do research in the field of artificial intelligence, with the long-term goal of building machines that reason precisely, systematically, in ways that are interpretable and robust to ambiguity in real-world environments. My research advances towards this goal by attempting to combine the complementary strengths of machine learning and symbolic reasoning. My graduate research has focused on developing machine learning models that represent reasoning via symbolic proofs. They show the promise of new learning paradigms that I envision to be more robust, interpretable, and trustworthy for deployment in real-world high-stake applications. Symbolic reasoning is precise and generalizes systematically to unseen scenarios. But it has been restricted to domains amenable to rigid formalization. In contrast, machine learning has the flexibility to handle noisy and ambiguous domains that are hard to formalize. But predominant machine learning models, such as deep neural networks, are notoriously uninterpretable, data-hungry, and incapable of generalizing outside the training data distribution. Integrating the strengths of both approaches is essential for building flexible reasoning machines with precise and systematic generalization. However, due to the discrete nature of symbolic reasoning, such integration may require a radical departure from the predominant paradigm of gradient-based learning. And my research tries to answer what that alternative form of learning might look like.","",""
26,"W. Gou, Chu-wen Ling, Yan He, Zengliang Jiang, Yuanqing Fu, Fengzhe Xu, Z. Miao, Ting-yu Sun, Jie-sheng Lin, Hui-lian Zhu, Hongwei Zhou, Yu-ming Chen, Ju-Sheng Zheng","Interpretable Machine Learning Framework Reveals Robust Gut Microbiome Features Associated With Type 2 Diabetes",2020,"","","","",189,"2022-07-13 09:24:29","","10.2337/dc20-1536","","",,,,,26,13.00,3,13,2,"OBJECTIVE To identify the core gut microbial features associated with type 2 diabetes risk and potential demographic, adiposity, and dietary factors associated with these features. RESEARCH DESIGN AND METHODS We used an interpretable machine learning framework to identify the type 2 diabetes–related gut microbiome features in the cross-sectional analyses of three Chinese cohorts: one discovery cohort (n = 1,832, 270 cases of type 2 diabetes) and two validation cohorts (cohort 1: n = 203, 48 cases; cohort 2: n = 7,009, 608 cases). We constructed a microbiome risk score (MRS) with the identified features. We examined the prospective association of the MRS with glucose increment in 249 participants without type 2 diabetes and assessed the correlation between the MRS and host blood metabolites (n = 1,016). We transferred human fecal samples with different MRS levels to germ-free mice to confirm the MRS–type 2 diabetes relationship. We then examined the prospective association of demographic, adiposity, and dietary factors with the MRS (n = 1,832). RESULTS The MRS (including 14 microbial features) consistently associated with type 2 diabetes, with risk ratio for per 1-unit change in MRS 1.28 (95% CI 1.23–1.33), 1.23 (1.13–1.34), and 1.12 (1.06–1.18) across three cohorts. The MRS was positively associated with future glucose increment (P < 0.05) and was correlated with a variety of gut microbiota–derived blood metabolites. Animal study further confirmed the MRS–type 2 diabetes relationship. Body fat distribution was found to be a key factor modulating the gut microbiome–type 2 diabetes relationship. CONCLUSIONS Our results reveal a core set of gut microbiome features associated with type 2 diabetes risk and future glucose increment.","",""
25,"Nastaran Meftahi, M. Klymenko, A. Christofferson, U. Bach, D. Winkler, S. Russo","Machine learning property prediction for organic photovoltaic devices",2020,"","","","",190,"2022-07-13 09:24:29","","10.1038/s41524-020-00429-w","","",,,,,25,12.50,4,6,2,"","",""
25,"F. Noé","Machine Learning for Molecular Dynamics on Long Timescales",2018,"","","","",191,"2022-07-13 09:24:29","","10.1007/978-3-030-40245-7_16","","",,,,,25,6.25,25,1,4,"","",""
21,"Christopher Culley, S. Vijayakumar, Guido Zampieri, C. Angione","A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth",2020,"","","","",192,"2022-07-13 09:24:29","","10.1073/pnas.2002959117","","",,,,,21,10.50,5,4,2,"Significance Linking genotype and phenotype is a fundamental problem in biology, key to several biomedical and biotechnological applications. Cell growth is a central phenotypic trait, resulting from interactions between environment, gene regulation, and metabolism, yet its functional bases are still not completely understood. We propose and test a machine-learning approach that integrates large-scale gene expression profiles and mechanistic metabolic models, for characterizing cell growth and understanding its driving mechanisms in Saccharomyces cerevisiae. At its core, a custom-built multimodal learning method merges experimentally generated and model-generated data. We show that our approach can leverage the advantages of both machine learning and metabolic modeling, revealing unknown interactions between biological domains, incorporating mechanistic knowledge, and therefore overcoming black-box limitations of conventional data-driven approaches. Metabolic modeling and machine learning are key components in the emerging next generation of systems and synthetic biology tools, targeting the genotype–phenotype–environment relationship. Rather than being used in isolation, it is becoming clear that their value is maximized when they are combined. However, the potential of integrating these two frameworks for omic data augmentation and integration is largely unexplored. We propose, rigorously assess, and compare machine-learning–based data integration techniques, combining gene expression profiles with computationally generated metabolic flux data to predict yeast cell growth. To this end, we create strain-specific metabolic models for 1,143 Saccharomyces cerevisiae mutants and we test 27 machine-learning methods, incorporating state-of-the-art feature selection and multiview learning approaches. We propose a multiview neural network using fluxomic and transcriptomic data, showing that the former increases the predictive accuracy of the latter and reveals functional patterns that are not directly deducible from gene expression alone. We test the proposed neural network on a further 86 strains generated in a different experiment, therefore verifying its robustness to an additional independent dataset. Finally, we show that introducing mechanistic flux features improves the predictions also for knockout strains whose genes were not modeled in the metabolic reconstruction. Our results thus demonstrate that fusing experimental cues with in silico models, based on known biochemistry, can contribute with disjoint information toward biologically informed and interpretable machine learning. Overall, this study provides tools for understanding and manipulating complex phenotypes, increasing both the prediction accuracy and the extent of discernible mechanistic biological insights.","",""
19,"P. Choudhury, Ryan T. Allen, Michael G. Endres","Machine Learning for Pattern Discovery in Management Research",2020,"","","","",193,"2022-07-13 09:24:29","","10.2139/ssrn.3518780","","",,,,,19,9.50,6,3,2,"Supervised machine learning (ML) methods are a powerful toolkit for discovering robust patterns in quantitative data. The patterns identified by ML could be used for exploratory inductive or abductive research, or for post-hoc analysis of regression results to detect patterns that may have gone unnoticed. However, ML models should not be treated as the result of a deductive causal test. To demonstrate the application of ML for pattern discovery, we implement ML algorithms to study employee turnover at a large technology company. We interpret the relationships between variables using partial dependence plots, which uncover surprising nonlinear and interdependent patterns between variables that may have gone unnoticed using traditional methods. To guide readers evaluating ML for pattern discovery, we provide guidance for evaluating model performance, highlight human decisions in the process, and warn of common misinterpretation pitfalls. An online appendix provides code and data to implement the algorithms demonstrated in the paper.","",""
19,"Alejandro Lopez-Rincon, Lucero Mendoza-Maldonado, M. Martínez-Archundia, A. Schönhuth, A. Kraneveld, J. Garssen, A. Tonda","Machine Learning-Based Ensemble Recursive Feature Selection of Circulating miRNAs for Cancer Tumor Classification",2020,"","","","",194,"2022-07-13 09:24:29","","10.3390/cancers12071785","","",,,,,19,9.50,3,7,2,"Circulating microRNAs (miRNA) are small noncoding RNA molecules that can be detected in bodily fluids without the need for major invasive procedures on patients. miRNAs have shown great promise as biomarkers for tumors to both assess their presence and to predict their type and subtype. Recently, thanks to the availability of miRNAs datasets, machine learning techniques have been successfully applied to tumor classification. The results, however, are difficult to assess and interpret by medical experts because the algorithms exploit information from thousands of miRNAs. In this work, we propose a novel technique that aims at reducing the necessary information to the smallest possible set of circulating miRNAs. The dimensionality reduction achieved reflects a very important first step in a potential, clinically actionable, circulating miRNA-based precision medicine pipeline. While it is currently under discussion whether this first step can be taken, we demonstrate here that it is possible to perform classification tasks by exploiting a recursive feature elimination procedure that integrates a heterogeneous ensemble of high-quality, state-of-the-art classifiers on circulating miRNAs. Heterogeneous ensembles can compensate inherent biases of classifiers by using different classification algorithms. Selecting features then further eliminates biases emerging from using data from different studies or batches, yielding more robust and reliable outcomes. The proposed approach is first tested on a tumor classification problem in order to separate 10 different types of cancer, with samples collected over 10 different clinical trials, and later is assessed on a cancer subtype classification task, with the aim to distinguish triple negative breast cancer from other subtypes of breast cancer. Overall, the presented methodology proves to be effective and compares favorably to other state-of-the-art feature selection methods.","",""
4,"M. Valetich, C. Le Losq, R. Arculus, S. Umino, J. Mavrogenes","Compositions and Classification of Fractionated Boninite Series Melts from the Izu–Bonin–Mariana Arc: A Machine Learning Approach",2021,"","","","",195,"2022-07-13 09:24:29","","10.1093/PETROLOGY/EGAB013","","",,,,,4,4.00,1,5,1,"  Much of the boninite magmatism in the Izu–Bonin–Mariana arc is preserved as evolved boninite series compositions wherein extensive fractional crystallization of pyroxene and spinel have obscured the diagnostic geochemical indicators of boninite parentage, such as high Mg and low Ti at intermediate silica contents. As a result, the usual geochemical discriminants used for the classification of the broad range of parental boninites are inapplicable to such highly fractionated melts. These issues are compounded by the mixing of demonstrably different whole-rock and glass analyses in classification schemes and petrological interpretations based thereon. Whole-rock compositions are compromised by entrainment of variable proportions of crystalline phases resulting in inconsistent differences from corresponding in situ glass analyses, which arguably better reflect prior melt compositions. To circumvent such issues, we herein present a robust method for the classification of highly fractionated boninite series glasses. This new classification leverages the analysis of trace elements, which are much more sensitive to evolutionary processes than major elements, and benefits from the use of unsupervised machine learning as a classification tool. The results show that the most fractionated boninite series melts preserve geochemical indicators of their parentage, and highlight the pitfalls of interpreting whole-rock and glass analyses interchangeably.","",""
5,"M. Ankenbrand, Liliia Shainberg, M. Hock, D. Lohr, L. Schreiber","Sensitivity analysis for interpretation of machine learning based segmentation models in cardiac MRI",2020,"","","","",196,"2022-07-13 09:24:29","","10.1186/s12880-021-00551-1","","",,,,,5,2.50,1,5,2,"","",""
17,"Yong'ai Li, Junming Jian, P. Pickhardt, F. Ma, W. Xia, Hai-ming Li, Rui Zhang, S. Zhao, S. Cai, Xingyu Zhao, Jiayi Zhang, Guofu Zhang, Jingxuan Jiang, Yan Zhang, Keying Wang, G. Lin, F. Feng, Jingjing Lu, Lin Deng, Xiaodong Wu, J. Qiang, Xin Gao","MRI-Based Machine Learning for Differentiating Borderline From Malignant Epithelial Ovarian Tumors: A Multicenter Study.",2020,"","","","",197,"2022-07-13 09:24:29","","10.1002/jmri.27084","","",,,,,17,8.50,2,22,2,"BACKGROUND Preoperative differentiation of borderline from malignant epithelial ovarian tumors (BEOT from MEOT) can impact surgical management. MRI has improved this assessment but subjective interpretation by radiologists may lead to inconsistent results.   PURPOSE To develop and validate an objective MRI-based machine-learning (ML) assessment model for differentiating BEOT from MEOT, and compare the performance against radiologists' interpretation.   STUDY TYPE Retrospective study of eight clinical centers.   POPULATION In all, 501 women with histopathologically-confirmed BEOT (n = 165) or MEOT (n = 336) from 2010 to 2018 were enrolled. Three cohorts were constructed: a training cohort (n = 250), an internal validation cohort (n = 92), and an external validation cohort (n = 159).   FIELD STRENGTH/SEQUENCE Preoperative MRI within 2 weeks of surgery. Single- and multiparameter (MP) machine-learning assessment models were built utilizing the following four MRI sequences: T2 -weighted imaging (T2 WI), fat saturation (FS), diffusion-weighted imaging (DWI), apparent diffusion coefficient (ADC), and contrast-enhanced (CE)-T1 WI.   ASSESSMENT Diagnostic performance of the models was assessed for both whole tumor (WT) and solid tumor (ST) components. Assessment of the performance of the model in discriminating BEOT vs. early-stage MEOT was made. Six radiologists of varying experience also interpreted the MR images.   STATISTICAL TESTS Mann-Whitney U-test: significance of the clinical characteristics; chi-square test: difference of label; DeLong test: difference of receiver operating characteristic (ROC).   RESULTS The MP-ST model performed better than the MP-WT model for both the internal validation cohort (area under the curve [AUC] = 0.932 vs. 0.917) and external validation cohort (AUC = 0.902 vs. 0.767). The model showed capability in discriminating BEOT vs. early-stage MEOT, with AUCs of 0.909 and 0.920, respectively. Radiologist performance was considerably poorer than both the internal (mean AUC = 0.792; range, 0.679-0.924) and external (mean AUC = 0.797; range, 0.744-0.867) validation cohorts.   DATA CONCLUSION Performance of the MRI-based ML model was robust and superior to subjective assessment of radiologists. If our approach can be implemented in clinical practice, improved preoperative prediction could potentially lead to preserved ovarian function and fertility for some women.   LEVEL OF EVIDENCE Level 4.   TECHNICAL EFFICACY Stage 2.","",""
17,"Frank Male, J. Jensen, L. Lake","Comparison of permeability predictions on cemented sandstones with physics-based and machine learning approaches",2020,"","","","",198,"2022-07-13 09:24:29","","10.31223/osf.io/3w6jx","","",,,,,17,8.50,6,3,2,"Abstract Permeability prediction has been an important problem since the time of Darcy. Most approaches to solve this problem have used either idealized physical models or empirical relations. In recent years, machine learning (ML) has led to more accurate and robust, but less interpretable empirical models. Using 211 core samples collected from 12 wells in the Garn Sandstone from the North Sea, this study compared idealized physical models based on the Carman-Kozeny equation to interpretable ML models. We found that ML models trained on estimates of physical properties are more accurate than physical models. Also, the results show evidence of a threshold of about 10% volume fraction, above which pore-filling cement strongly affects permeability.","",""
1,"T. Thung, Murray E. White, Wei Dai, J. Wilksch, R. Bamert, A. Rocker, C. Stubenrauch, Daniel Williams, Cheng Huang, Ralf Schittelhelm, J. Barr, E. Jameson, S. McGowan, Yanju Zhang, Jiawei Wang, R. Dunstan, T. Lithgow","The component parts of bacteriophage virions accurately defined by a machine-learning approach built on evolutionary features",2021,"","","","",199,"2022-07-13 09:24:29","","10.1101/2021.02.28.433281","","",,,,,1,1.00,0,17,1,"Antimicrobial resistance (AMR) continues to evolve as a major threat to human health and new strategies are required for the treatment of AMR infections. Bacteriophages (phages) that kill bacterial pathogens are being identified for use in phage therapies, with the intention to apply these bactericidal viruses directly into the infection sites in bespoke phage cocktails. Despite the great unsampled phage diversity for this purpose, an issue hampering the roll out of phage therapy is the poor quality annotation of many of the phage genomes, particularly for those from infrequently sampled environmental sources. We developed a computational tool called STEP3 to use the “evolutionary features” that can be recognized in genome sequences of diverse phages. These features, when integrated into an ensemble framework, achieved a stable and robust prediction performance when benchmarked against other prediction tools using phages from diverse sources. Validation of the prediction accuracy of STEP3 was conducted with high-resolution mass spectrometry analysis of two novel phages, isolated from a watercourse in the Southern Hemisphere. STEP3 provides a robust computational approach to distinguish specific and universal features in phages to improve the quality of phage cocktails, and is available for use at http://step3.erc.monash.edu/. IMPORTANCE In response to the global problem of antimicrobial resistance there are moves to use bacteriophages (phages) as therapeutic agents. Selecting which phages will be effective therapeutics relies on interpreting features contributing to shelf-life and applicability to diagnosed infections. However, the protein components of the phage virions that dictate these properties vary so much in sequence that best estimates suggest failure to recognize up to 90% of them. We have utilised this diversity in evolutionary features as an advantage, to apply machine learning for prediction accuracy for diverse components in phage virions. We benchmark this new tool showing the accurate recognition and evaluation of phage components parts using genome sequence data of phages from under-sampled environments, where the richest diversity of phage still lies.","",""
2,"I. M. Lei, Chen Jiang, Chon Lok Lei, S. D. de Rijk, Y. C. Tam, C. Swords, M. Sutcliffe, G. Malliaras, M. Bance, Yan Yan Shery Huang","3D printed biomimetic cochleae and machine learning co-modelling provides clinical informatics for cochlear implant patients",2021,"","","","",200,"2022-07-13 09:24:29","","10.1038/s41467-021-26491-6","","",,,,,2,2.00,0,10,1,"","",""
