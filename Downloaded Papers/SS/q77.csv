Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
9,"Adam Noack, Isaac Ahern, D. Dou, Boyang Li","An Empirical Study on the Relation Between Network Interpretability and Adversarial Robustness",2020,"","","","",1,"2022-07-13 09:30:38","","10.1007/s42979-020-00390-x","","",,,,,9,4.50,2,4,2,"","",""
0,"mina hanna, M. Tayel","A Hybrid Encoded and Adapted-tuned Neural Network for Asset Medical Image Watermarking Technique",2022,"","","","",2,"2022-07-13 09:30:38","","10.21203/rs.3.rs-831690/v1","","",,,,,0,0.00,0,2,1,"  Digital image watermarking techniques are used to authenticate identity of owners and copyright protection of asset images. Asset medical images (AMI) specially require extreme care when embed a watermark message because additional information should affect the AMI quality and the changes in AMI gray levels may interfere with its interpretation. This paper introduces a hybrid encoded and adapted-tuned neural network (TNN) for AMI watermarking technique to cover almost essential watermarking requirements. To attain robustness, security and invisibility, uses human visual system (HVS) and TNN to tune the AMI and to find the maximum amount of adaptive watermark message before the watermark message becomes visible. To achieve transparency, enhance AMI using histogram equalization. Embedding is performed into the middle frequency coefficients of discrete cosine transform of the AMI, to avoid visual parts in the low frequency coefficient and the noise and attacks in high frequency to improve image robustness and increase capacity comparing to spatial domain.","",""
1,"M. H. Mozaffari, Chanho Kim, Won-sook Lee","Ultrasound Tongue Contour Extraction using Dilated Convolutional Neural Network",2019,"","","","",3,"2022-07-13 09:30:38","","10.1109/BIBM47256.2019.8983002","","",,,,,1,0.33,0,3,3,"One application of medical ultrasound imaging is to visualize and characterize human tongue shape and motion to study healthy or impaired speech production. Due to the low-contrast characteristic and noisy nature of ultrasound images, it requires knowledge about the tongue structure and ultrasound data interpretation for users to recognize tongue gestures. Moreover, quantitative analysis of tongue motion needs the tongue contour to be extracted, tracked and visualized automatically. This paper presents two novel deep neural networks that benefit from the ability of global prediction of encoding-decoding fully convolutional networks and the capability of full-resolution extraction of dilated convolutions. Assessment studies over datasets from different ultrasound machines disclosed the outstanding performances of the proposed models in terms of accuracy and robustness.","",""
13,"Hao Wu, Bo Zhang","A deep convolutional encoder-decoder neural network in assisting seismic horizon tracking",2018,"","","","",4,"2022-07-13 09:30:38","","","","",,,,,13,3.25,7,2,4,"Seismic horizons are geologically significant surfaces that can be used for building geology structure and stratigraphy models. However, horizon tracking in 3D seismic data is a time-consuming and challenging problem. Relief human from the tedious seismic interpretation is one of the hot research topics. We proposed a novel automatically seismic horizon tracking method by using a deep convolutional neural network. We employ a state-of-art end-to-end semantic segmentation method to track the seismic horizons automatically. Experiment result shows that our proposed neural network can automatically track multiple horizons simultaneously. We validate the effectiveness and robustness of our proposed method by comparing automatically tracked horizons with manually picked horizons.","",""
51,"S. Mohaghegh","Neural Network: What It Can Do for Petroleum Engineers",1995,"","","","",5,"2022-07-13 09:30:38","","10.2118/29219-PA","","",,,,,51,1.89,51,1,27,"Neural network, a nonalgorithmic, nondigital, intensely parallel and distributive information processing system, is being used more and more every day. The main interest in neural networks is rooted in the recognition that the human brain processes information in a different manner than conventional digital computers. Computers are extremely fast and precise at executing sequences of instructions that have been formulated for them. Once the network has learned the information in the training set and has converged,'' the test set is applied to the network for verification. It is important to note that although the user has the desired output of the test set, it has not been seen by the network. This ensures the integrity and robustness of the trained network. Neural networks have proved to be valuable pattern-recognition tools. They are capable of finding highly complex patterns within large amounts of data. A relevant example is well log interpretation. It is generally accepted that there is more information embedded in well logs than meets the eye. Determination, prediction, or estimation of formation permeability without actual laboratory measurement of the cores or interruption in production for well test data collection has been a fundamental problem for petroleum engineers. From geophysicalmore » well log data, it was possible to predict and/or estimate permeability of a highly heterogeneous formation in West Virginia.« less","",""
1,"J. Feng, Yaqin Sun, Kefei Zhang, Yindi Zhao, Y. Ren, Yu Chen, Huifu Zhuang, Shuo Chen","Autonomous Detection of Spodoptera frugiperda by Feeding Symptoms Directly from UAV RGB Imagery",2022,"","","","",6,"2022-07-13 09:30:38","","10.3390/app12052592","","",,,,,1,1.00,0,8,1,"The use of digital technologies to detect, position, and quantify pests quickly and accurately is very important in precision agriculture. Imagery acquisition using air-borne drones in combination with the deep learning technique is a new and viable solution to replace human labor such as visual interpretation, which consumes a lot of time and effort. In this study, we developed a method for automatic detecting an important maize pest—Spodoptera frugiperda—by its gnawing holes on maize leaves based on convolution neural network. We validated the split-attention mechanism in the classical network structure ResNet50, which improves the accuracy and robustness, and verified the feasibility of two kinds of gnawing holes as the identification features of Spodoptera frugiperda invasion and the degree. In order to verify the robustness of this detection method against plant morphological changes, images at the jointing stage and heading stage were used for training and testing, respectively. The performance of the models trained with the jointing stage images has been achieved the validation accuracy of ResNeSt50, ResNet50, EfficientNet, and RegNet at 98.77%, 97.59%, 97.89%, and 98.07%, with a heading stage test accuracy of 89.39%, 81.88%, 86.21%, and 84.21%.","",""
0,"Junho Kim, Seong-Tae Kim, Seong Tae Kim, Y. Ro","Robust Perturbation for Visual Explanation: Cross-Checking Mask Optimization to Avoid Class Distortion",2021,"","","","",7,"2022-07-13 09:30:38","","10.1109/TIP.2021.3130526","","",,,,,0,0.00,0,4,1,"Along with the outstanding performance of the deep neural networks (DNNs), considerable research efforts have been devoted to finding ways to understand the decision of DNNs structures. In the computer vision domain, visualizing the attribution map is one of the most intuitive and understandable ways to achieve human-level interpretation. Among them, perturbation-based visualization can explain the “black box” property of the given network by optimizing perturbation masks that alter the network prediction of the target class the most. However, existing perturbation methods could make unexpected changes to network predictions after applying a perturbation mask to the input image, resulting in a loss of robustness and fidelity of the perturbation mechanisms. In this paper, we define class distortion as the unexpected changes of the network prediction during the perturbation process. To handle that, we propose a novel visual interpretation framework, Robust Perturbation, which shows robustness against the unexpected class distortion during the mask optimization. With a new cross-checking mask optimization strategy, our proposed framework perturbs the target prediction of the network while upholding the non-target predictions, providing more reliable and accurate visual explanations. We evaluate our framework on three different public datasets through extensive experiments. Furthermore, we propose a new metric for class distortion evaluation. In both quantitative and qualitative experiments, tackling the class distortion problem turns out to enhance the quality and fidelity of the visual explanation in comparison with the existing perturbation-based methods.","",""
6,"Astrid Zeman, Oliver Obst, K. Brooks","Complex cells decrease errors for the Müller-Lyer illusion in a model of the visual ventral stream",2014,"","","","",8,"2022-07-13 09:30:38","","10.3389/fncom.2014.00112","","",,,,,6,0.75,2,3,8,"To improve robustness in object recognition, many artificial visual systems imitate the way in which the human visual cortex encodes object information as a hierarchical set of features. These systems are usually evaluated in terms of their ability to accurately categorize well-defined, unambiguous objects and scenes. In the real world, however, not all objects and scenes are presented clearly, with well-defined labels and interpretations. Visual illusions demonstrate a disparity between perception and objective reality, allowing psychophysicists to methodically manipulate stimuli and study our interpretation of the environment. One prominent effect, the Müller-Lyer illusion, is demonstrated when the perceived length of a line is contracted (or expanded) by the addition of arrowheads (or arrow-tails) to its ends. HMAX, a benchmark object recognition system, consistently produces a bias when classifying Müller-Lyer images. HMAX is a hierarchical, artificial neural network that imitates the “simple” and “complex” cell layers found in the visual ventral stream. In this study, we perform two experiments to explore the Müller-Lyer illusion in HMAX, asking: (1) How do simple vs. complex cell operations within HMAX affect illusory bias and precision? (2) How does varying the position of the figures in the input image affect classification using HMAX? In our first experiment, we assessed classification after traversing each layer of HMAX and found that in general, kernel operations performed by simple cells increase bias and uncertainty while max-pooling operations executed by complex cells decrease bias and uncertainty. In our second experiment, we increased variation in the positions of figures in the input images that reduced bias and uncertainty in HMAX. Our findings suggest that the Müller-Lyer illusion is exacerbated by the vulnerability of simple cell operations to positional fluctuations, but ameliorated by the robustness of complex cell responses to such variance.","",""
4,"A. Hamer, D. Simms, T. Waine","Replacing human interpretation of agricultural land in Afghanistan with a deep convolutional neural network",2021,"","","","",9,"2022-07-13 09:30:38","","10.1080/01431161.2020.1864059","","",,,,,4,4.00,1,3,1,"ABSTRACT Afghanistan’s annual opium survey relies upon time-consuming human interpretation of satellite images to map the area of potential poppy cultivation for statistical sample design. Deep Convolutional Neural Networks (CNNs) have shown ground-breaking performance for image classification tasks by encoding local contextual information, in some cases outperforming trained analysts. In this study, we investigate the development of a CNN to automate the classification of agriculture from medium-resolution satellite imagery as an alternative to manual interpretation. The residual network (ResNet50) CNN architecture was trained and validated for delineating the agricultural area using labelled multi-seasonal Disaster Monitoring Constellation (DMC) satellite imagery (32 m) of Helmand and Kandahar provinces. The effect of input image chip size, training sampling strategy, elevation data, and multi-seasonal imagery were investigated. The best-performing single-year classification used an input chip size of 33 × 33 pixels, a targeted sampling strategy and transfer learning, resulting in high overall accuracy (94%). The inclusion of elevation data marginally lowered performance (93%). Multi-seasonal classification achieved an overall accuracy of 89% using the previous two years’ data. Only 25% of the target year’s training samples were necessary to update the model to achieve >94% overall accuracy. A data-driven approach to automate agricultural mask production using CNNs is proposed to reduce the burden of human interpretation. The ability to continually update CNN models with new data has the potential to significantly improve automatic classification of vegetation across years.","",""
0,"Ying Lin, Junji Ma, Bingjing Huang, Jinbo Zhang, Yining Zhang, Zhengjia Dai","Predicting Human Intrinsic Functional Connectivity From Structural Connectivity: An Artificial Neural Network Approach",2021,"","","","",10,"2022-07-13 09:30:38","","10.1109/tnse.2021.3102667","","",,,,,0,0.00,0,6,1,"How structural connectivity (SC) constrains and shapes functional connectivity (FC) in the human brain to support rich cognitive functions has long been a core issue in neuroscience. Although evidence accumulate to suggest that FC strength is correlated with multiple aspects of SC, few studies has analyzed the SC-to-FC relationship in a multivariate manner. This paper proposed a novel usage of the feedforward neural network to predict FC strength as a nonlinear combination of 115 features that described the geometric and topological aspects of SC. The resulting model outperformed four state-of-the-art models in both terms of predictive power and generalizability. Model interpretation analyses found that the geometric features were generally more predictive than the topological ones, providing novel evidence for the significant impact of geometric relationships on FC generation. Comparison of feature contributions to predicting FC with different structural properties further revealed the crucial role of indirect structural paths for inducing FC, particularly between disconnected and/or distanced regions. Together, our results suggested that the flexible FC is significantly but unevenly influenced by the combination of geometric and topological characteristics of the structural network. The proposed framework would also be used for link prediction on top of an underlying topology.","",""
58,"Greg Anderson, Shankara Pailoor, Isil Dillig, Swarat Chaudhuri","Optimization and abstraction: a synergistic approach for analyzing neural network robustness",2019,"","","","",11,"2022-07-13 09:30:38","","10.1145/3314221.3314614","","",,,,,58,19.33,15,4,3,"In recent years, the notion of local robustness (or robustness for short) has emerged as a desirable property of deep neural networks. Intuitively, robustness means that small perturbations to an input do not cause the network to perform misclassifications. In this paper, we present a novel algorithm for verifying robustness properties of neural networks. Our method synergistically combines gradient-based optimization methods for counterexample search with abstraction-based proof search to obtain a sound and (δ -)complete decision procedure. Our method also employs a data-driven approach to learn a verification policy that guides abstract interpretation during proof search. We have implemented the proposed approach in a tool called Charon and experimentally evaluated it on hundreds of benchmarks. Our experiments show that the proposed approach significantly outperforms three state-of-the-art tools, namely AI^2, Reluplex, and Reluval.","",""
3,"G. I. Parisi","Human Action Recognition and Assessment via Deep Neural Network Self-Organization",2020,"","","","",12,"2022-07-13 09:30:38","","10.1007/978-3-030-46732-6_10","","",,,,,3,1.50,3,1,2,"","",""
539,"Timon Gehr, M. Mirman, Dana Drachsler-Cohen, Petar Tsankov, Swarat Chaudhuri, Martin T. Vechev","AI2: Safety and Robustness Certification of Neural Networks with Abstract Interpretation",2018,"","","","",13,"2022-07-13 09:30:38","","10.1109/SP.2018.00058","","",,,,,539,134.75,90,6,4,"We present AI2, the first sound and scalable analyzer for deep neural networks. Based on overapproximation, AI2 can automatically prove safety properties (e.g., robustness) of realistic neural networks (e.g., convolutional neural networks). The key insight behind AI2 is to phrase reasoning about safety and robustness of neural networks in terms of classic abstract interpretation, enabling us to leverage decades of advances in that area. Concretely, we introduce abstract transformers that capture the behavior of fully connected and convolutional neural network layers with rectified linear unit activations (ReLU), as well as max pooling layers. This allows us to handle real-world neural networks, which are often built out of those types of layers. We present a complete implementation of AI2 together with an extensive evaluation on 20 neural networks. Our results demonstrate that: (i) AI2 is precise enough to prove useful specifications (e.g., robustness), (ii) AI2 can be used to certify the effectiveness of state-of-the-art defenses for neural networks, (iii) AI2 is significantly faster than existing analyzers based on symbolic analysis, which often take hours to verify simple fully connected networks, and (iv) AI2 can handle deep convolutional networks, which are beyond the reach of existing methods.","",""
16,"Xiao Qi, L. G. Brown, D. Foran, J. Nosher, I. Hacihaliloglu","Chest X-ray image phase features for improved diagnosis of COVID-19 using convolutional neural network",2020,"","","","",14,"2022-07-13 09:30:38","","10.1007/s11548-020-02305-w","","",,,,,16,8.00,3,5,2,"","",""
0,"P. Kebria, D. Nahavandi, A. Khosravi, S. Nahavandi, F. Bello","Adaptive Neural Network-based Perception and Awareness of Teleoperation Systems in Human-Machine Interactions",2020,"","","","",15,"2022-07-13 09:30:38","","10.1109/ICHMS49158.2020.9209437","","",,,,,0,0.00,0,5,2,"This paper addresses the problem of perception and awareness of teleoperation systems in the presence of human collaboratives/objects in the workspace. Although the term teleoperation generally refers to operations being executed remotely, in many applications, like telemedicine, there exist human beings in the remote workspace. Hence, it is critically important that the teleoperator system to operate safely enough in the presence of human kinds in the workspace. In this paper, we propose a perception and awareness scheme for a teleoperation system that prevents the teleoperator from imposing extreme and unwanted forces/movements. To achieve this goal, we train a neural network to estimate and predict the motion and force commands from the human operator. Furthermore, we develop an adaptive algorithm for fine-tuning network parameters for robustness purposes. Theoretically proven, stability and performance of the proposed scheme is comparatively evaluated in comprehensive simulations.","",""
37,"Hang Su, Wen Qi, Chenguang Yang, A. Aliverti, G. Ferrigno, E. De Momi","Deep Neural Network Approach in Human-Like Redundancy Optimization for Anthropomorphic Manipulators",2019,"","","","",16,"2022-07-13 09:30:38","","10.1109/ACCESS.2019.2937380","","",,,,,37,12.33,6,6,3,"Human-like behavior has emerged in the robotics area for improving the quality of Human-Robot Interaction (HRI). For the human-like behavior imitation, the kinematic mapping between a human arm and robot manipulator is one of the popular solutions. To fulfill this requirement, a reconstruction method called swivel motion was adopted to achieve human-like imitation. This approach aims at modeling the regression relationship between robot pose and swivel motion angle. Then it reaches the human-like swivel motion using its redundant degrees of the manipulator. This characteristic holds for most of the redundant anthropomorphic robots. Although artificial neural network (ANN) based approaches show moderate robustness, the predictive performance is limited. In this paper, we propose a novel deep convolutional neural network (DCNN) structure for reconstruction enhancement and reducing online prediction time. Finally, we utilized the trained DCNN model for managing redundancy control a 7 DoFs anthropomorphic robot arm (LWR4+, KUKA, Germany) for validation. A demonstration is presented to show the human-like behavior on the anthropomorphic manipulator. The proposed approach can also be applied to control other anthropomorphic robot manipulators in industry area or biomedical engineering.","",""
26,"Tahmina Zebin, Patricia Jane Scully, Niels Peek, A. Casson, K. Ozanyan","Design and Implementation of a Convolutional Neural Network on an Edge Computing Smartphone for Human Activity Recognition",2019,"","","","",17,"2022-07-13 09:30:38","","10.1109/ACCESS.2019.2941836","","",,,,,26,8.67,5,5,3,"Edge computing aims to integrate computing into everyday settings, enabling the system to be context-aware and private to the user. With the increasing success and popularity of deep learning methods, there is an increased demand to leverage these techniques in mobile and wearable computing scenarios. In this paper, we present an assessment of a deep human activity recognition system’s memory and execution time requirements, when implemented on a mid-range smartphone class hardware and the memory implications for embedded hardware. This paper presents the design of a convolutional neural network (CNN) in the context of human activity recognition scenario. Here, layers of CNN automate the feature learning and the influence of various hyper-parameters such as the number of filters and filter size on the performance of CNN. The proposed CNN showed increased robustness with better capability of detecting activities with temporal dependence compared to models using statistical machine learning techniques. The model obtained an accuracy of 96.4% in a five-class static and dynamic activity recognition scenario. We calculated the proposed model memory consumption and execution time requirements needed for using it on a mid-range smartphone. Per-channel quantization of weights and per-layer quantization of activation to 8-bits of precision post-training produces classification accuracy within 2% of floating-point networks for dense, convolutional neural network architecture. Almost all the size and execution time reduction in the optimized model was achieved due to weight quantization. We achieved more than four times reduction in model size when optimized to 8-bit, which ensured a feasible model capable of fast on-device inference.","",""
6,"Xueyuan She, Yun Long, S. Mukhopadhyay","Improving Robustness of ReRAM-based Spiking Neural Network Accelerator with Stochastic Spike-timing-dependent-plasticity",2019,"","","","",18,"2022-07-13 09:30:38","","10.1109/IJCNN.2019.8851825","","",,,,,6,2.00,2,3,3,"Spike-timing-dependent-plasticity (STDP) is an unsupervised learning algorithm for spiking neural network (SNN), which promises to achieve deeper understanding of human brain and more powerful artificial intelligence. While conventional computing system fails to simulate SNN efficiently, process-inmemory (PIM) based on devices such as ReRAM can be used in designing fast and efficient STDP based SNN accelerators, as it operates in high resemblance with biological neural network. However, the real-life implementation of such design still suffers from impact of input noise and device variation. In this work, we present a novel stochastic STDP algorithm that uses spiking frequency information to dynamically adjust synaptic behavior. The algorithm is tested in pattern recognition task with noisy input and shows accuracy improvement over deterministic STDP. In addition, we show that the new algorithm can be used for designing a robust ReRAM based SNN accelerator that has strong resilience to device variation.","",""
3,"Artur Petrosyan, M. Sinkin, M. Lebedev, A. Ossadtchi","Decoding and interpreting cortical signals with a compact convolutional neural network",2021,"","","","",19,"2022-07-13 09:30:38","","10.1088/1741-2552/abe20e","","",,,,,3,3.00,1,4,1,"Objective. Brain–computer interfaces (BCIs) decode information from neural activity and send it to external devices. The use of Deep Learning approaches for decoding allows for automatic feature engineering within the specific decoding task. Physiologically plausible interpretation of the network parameters ensures the robustness of the learned decision rules and opens the exciting opportunity for automatic knowledge discovery. Approach. We describe a compact convolutional network-based architecture for adaptive decoding of electrocorticographic (ECoG) data into finger kinematics. We also propose a novel theoretically justified approach to interpreting the spatial and temporal weights in the architectures that combine adaptation in both space and time. The obtained spatial and frequency patterns characterizing the neuronal populations pivotal to the specific decoding task can then be interpreted by fitting appropriate spatial and dynamical models. Main results. We first tested our solution using realistic Monte-Carlo simulations. Then, when applied to the ECoG data from Berlin BCI competition IV dataset, our architecture performed comparably to the competition winners without requiring explicit feature engineering. Using the proposed approach to the network weights interpretation we could unravel the spatial and the spectral patterns of the neuronal processes underlying the successful decoding of finger kinematics from an ECoG dataset. Finally we have also applied the entire pipeline to the analysis of a 32-channel EEG motor-imagery dataset and observed physiologically plausible patterns specific to the task. Significance. We described a compact and interpretable CNN architecture derived from the basic principles and encompassing the knowledge in the field of neural electrophysiology. For the first time in the context of such multibranch architectures with factorized spatial and temporal processing we presented theoretically justified weights interpretation rules. We verified our recipes using simulations and real data and demonstrated that the proposed solution offers a good decoder and a tool for investigating motor control neural mechanisms.","",""
4,"Onur Sevli","A deep convolutional neural network-based pigmented skin lesion classification application and experts evaluation",2021,"","","","",20,"2022-07-13 09:30:38","","10.1007/S00521-021-05929-4","","",,,,,4,4.00,4,1,1,"","",""
430,"Amirata Ghorbani, Abubakar Abid, James Y. Zou","Interpretation of Neural Networks is Fragile",2017,"","","","",21,"2022-07-13 09:30:38","","10.1609/aaai.v33i01.33013681","","",,,,,430,86.00,143,3,5,"In order for machine learning to be trusted in many applications, it is critical to be able to reliably explain why the machine learning algorithm makes certain predictions. For this reason, a variety of methods have been developed recently to interpret neural network predictions by providing, for example, feature importance maps. For both scientific robustness and security reasons, it is important to know to what extent can the interpretations be altered by small systematic perturbations to the input data, which might be generated by adversaries or by measurement biases. In this paper, we demonstrate how to generate adversarial perturbations that produce perceptively indistinguishable inputs that are assigned the same predicted label, yet have very different interpretations. We systematically characterize the robustness of interpretations generated by several widely-used feature importance interpretation methods (feature importance maps, integrated gradients, and DeepLIFT) on ImageNet and CIFAR-10. In all cases, our experiments show that systematic perturbations can lead to dramatically different interpretations without changing the label. We extend these results to show that interpretations based on exemplars (e.g. influence functions) are similarly susceptible to adversarial attack. Our analysis of the geometry of the Hessian matrix gives insight on why robustness is a general challenge to current interpretation approaches.","",""
21,"C. Li, Shouqian Sun, Xin Min, Wenqian Lin, Binling Nie, Xianfu Zhang","End-to-end learning of deep convolutional neural network for 3D human action recognition",2017,"","","","",22,"2022-07-13 09:30:38","","10.1109/ICMEW.2017.8026281","","",,,,,21,4.20,4,6,5,"Recently, skeleton-based human action recognition has been receiving significant attention from various research communities due to its robustness, succinctness, and view-invariant representation. Most of the existing skeleton-based methods use either well-designed classifiers with hand-crafted features or current neural network (RNN) to recognize human actions. In this paper, inspired by the deep convolutional neural network's breakthroughs in the image domain, we transform a skeleton sequence into an image and perform end-to-end learning of deep convolutional neural network (CNN). The skeleton sequence based image contains spatial temporal information. Our proposed method is tested on the NTU RGB+D dataset which is so far the largest skeleton-based human action dataset, and achieves the state-of-the-art performance for both the cross-view and cross-subject evaluations.","",""
22,"Akhilan Boopathy, Sijia Liu, Gaoyuan Zhang, Cynthia Liu, Pin-Yu Chen, Shiyu Chang, L. Daniel","Proper Network Interpretability Helps Adversarial Robustness in Classification",2020,"","","","",23,"2022-07-13 09:30:38","","","","",,,,,22,11.00,3,7,2,"Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability (namely, making network interpretation maps visually similar), or interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with a proper measurement of interpretation, it is actually difficult to prevent prediction-evasion adversarial attacks from causing interpretation discrepancy, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop an interpretability-aware defensive scheme built only on promoting robust interpretation (without the need for resorting to adversarial loss minimization). We show that our defense achieves both robust classification and robust interpretation, outperforming state-of-the-art adversarial training methods against attacks of large perturbation in particular.","",""
20,"Egor Lakomkin, M. Zamani, C. Weber, S. Magg, S. Wermter","On the Robustness of Speech Emotion Recognition for Human-Robot Interaction with Deep Neural Networks",2018,"","","","",24,"2022-07-13 09:30:38","","10.1109/IROS.2018.8593571","","",,,,,20,5.00,4,5,4,"Speech emotion recognition (SER) is an important aspect of effective human-robot collaboration and received a lot of attention from the research community. For example, many neural network-based architectures were proposed recently and pushed the performance to a new level. However, the applicability of such neural SER models trained only on in-domain data to noisy conditions is currently under-researched. In this work, we evaluate the robustness of state-of-the-art neural acoustic emotion recognition models in human-robot interaction scenarios. We hypothesize that a robot's ego noise, room conditions, and various acoustic events that can occur in a home environment can significantly affect the performance of a model. We conduct several experiments on the iCub robot platform and propose several novel ways to reduce the gap between the model's performance during training and testing in real-world conditions. Furthermore, we observe large improvements in the model performance on the robot and demonstrate the necessity of introducing several data augmentation techniques like overlaying background noise and loudness variations to improve the robustness of the neural approaches.","",""
22,"Bo Li, Cheng Han, Baoxing Bai","Hybrid approach for human posture recognition using anthropometry and BP neural network based on Kinect V2",2019,"","","","",25,"2022-07-13 09:30:38","","10.1186/S13640-018-0393-4","","",,,,,22,7.33,7,3,3,"","",""
367,"V. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, S. Gordon, C. Hung, Brent Lance","EEGNet: A Compact Convolutional Neural Network for EEG-based Brain-Computer Interfaces",2021,"","","","",26,"2022-07-13 09:30:38","","","","",,,,,367,367.00,61,6,1,"Objective: Brain computer interfaces (BCI) enable direct communication with a computer, using neural activity as the control signal. This neural signal is generally chosen from a variety of well-studied electroencephalogram (EEG) signals. For a given BCI paradigm, feature extractors and classifiers are tailored to the distinct characteristics of its expected EEG control signal, limiting its application to that specific signal. Convolutional Neural Networks (CNNs), which have been used in computer vision and speech recognition to perform automatic feature extraction and classification, have successfully been applied to EEG-based BCIs; however, they have mainly been applied to single BCI paradigms and thus it remains unclear how these architectures generalize to other paradigms. Here, we ask if we can design a single CNN architecture to accurately classify EEG signals from different BCI paradigms, while simultaneously being as compact as possible (defined as the number of parameters in the model). Approach: In this work we introduce EEGNet, a compact convolutional neural network for EEG-based BCIs. We introduce the use of depthwise and separable convolutions to construct an EEG-specific model which encapsulates well-known EEG feature extraction concepts for BCI. We compare EEGNet, both for within-subject and cross-subject classification, to current state-of-the-art approaches across four BCI paradigms: P300 visual-evoked potentials, error-related negativity responses (ERN), movement-related cortical potentials (MRCP), and sensory motor rhythms (SMR). Results: We show that EEGNet generalizes across paradigms better than, and achieves comparably high performance to, the reference algorithms when only limited training data is available. We also show that EEGNet effectively generalizes to both ERP and oscillatory-based BCIs. In addition, we demonstrate three different approaches to visualize the contents of a trained EEGNet model to enable interpretation of the learned features. Significance: Our results suggest that EEGNet is robust enough to learn a wide variety of interpretable features over a range of BCI tasks, suggesting that the observed performances were not due to artifact or noise sources in the data. Our models can be found at: https://github.com/vlawhern/arl-eegmodels. 1 ar X iv :1 61 1. 08 02 4v 4 [ cs .L G ] 1 6 M ay 2 01 8","",""
248,"Lukas Schott, Jonas Rauber, M. Bethge, Wieland Brendel","Towards the first adversarially robust neural network model on MNIST",2018,"","","","",27,"2022-07-13 09:30:38","","","","",,,,,248,62.00,62,4,4,"Despite much effort, deep neural networks remain highly susceptible to tiny input perturbations and even for MNIST, one of the most common toy datasets in computer vision, no neural network model exists for which adversarial perturbations are large and make semantic sense to humans. We show that even the widely recognized and by far most successful defense by Madry et al. (1) overfits on the L-infinity metric (it's highly susceptible to L2 and L0 perturbations), (2) classifies unrecognizable images with high certainty, (3) performs not much better than simple input binarization and (4) features adversarial perturbations that make little sense to humans. These results suggest that MNIST is far from being solved in terms of adversarial robustness. We present a novel robust classification model that performs analysis by synthesis using learned class-conditional data distributions. We derive bounds on the robustness and go to great length to empirically evaluate our model using maximally effective adversarial attacks by (a) applying decision-based, score-based, gradient-based and transfer-based attacks for several different Lp norms, (b) by designing a new attack that exploits the structure of our defended model and (c) by devising a novel decision-based attack that seeks to minimize the number of perturbed pixels (L0). The results suggest that our approach yields state-of-the-art robustness on MNIST against L0, L2 and L-infinity perturbations and we demonstrate that most adversarial examples are strongly perturbed towards the perceptual boundary between the original and the adversarial class.","",""
40,"Patrick Esser, Robin Rombach, B. Ommer","A Disentangling Invertible Interpretation Network for Explaining Latent Representations",2020,"","","","",28,"2022-07-13 09:30:38","","10.1109/cvpr42600.2020.00924","","",,,,,40,20.00,13,3,2,"Neural networks have greatly boosted performance in computer vision by learning powerful representations of input data. The drawback of end-to-end training for maximal overall performance are black-box models whose hidden representations are lacking interpretability: Since distributed coding is optimal for latent layers to improve their robustness, attributing meaning to parts of a hidden feature vector or to individual neurons is hindered. We formulate interpretation as a translation of hidden representations onto semantic concepts that are comprehensible to the user. The mapping between both domains has to be bijective so that semantic modifications in the target domain correctly alter the original representation. The proposed invertible interpretation network can be transparently applied on top of existing architectures with no need to modify or retrain them. Consequently, we translate an original representation to an equivalent yet interpretable one and backwards without affecting the expressiveness and performance of the original. The invertible interpretation network disentangles the hidden representation into separate, semantically meaningful concepts. Moreover, we present an efficient approach to define semantic concepts by only sketching two images and also an unsupervised strategy. Experimental evaluation demonstrates the wide applicability to interpretation of existing classification and image generation networks as well as to semantically guided image manipulation.","",""
1,"Zhoufan Jiang, Yanming Wang, Chenwei Shi, Yueyang Wu, Rongjie Hu, Shishuo Chen, Sheng Hu, Xiaoxiao Wang, B. Qiu","Attention module improves both performance and interpretability of four‐dimensional functional magnetic resonance imaging decoding neural network",2022,"","","","",29,"2022-07-13 09:30:38","","10.1002/hbm.25813","","",,,,,1,1.00,0,9,1,"Decoding brain cognitive states from neuroimaging signals is an important topic in neuroscience. In recent years, deep neural networks (DNNs) have been recruited for multiple brain state decoding and achieved good performance. However, the open question of how to interpret the DNN black box remains unanswered. Capitalizing on advances in machine learning, we integrated attention modules into brain decoders to facilitate an in‐depth interpretation of DNN channels. A four‐dimensional (4D) convolution operation was also included to extract temporo‐spatial interaction within the fMRI signal. The experiments showed that the proposed model obtains a very high accuracy (97.4%) and outperforms previous researches on the seven different task benchmarks from the Human Connectome Project (HCP) dataset. The visualization analysis further illustrated the hierarchical emergence of task‐specific masks with depth. Finally, the model was retrained to regress individual traits within the HCP and to classify viewing images from the BOLD5000 dataset, respectively. Transfer learning also achieves good performance. Further visualization analysis shows that, after transfer learning, low‐level attention masks remained similar to the source domain, whereas high‐level attention masks changed adaptively. In conclusion, the proposed 4D model with attention module performed well and facilitated interpretation of DNNs, which is helpful for subsequent research.","",""
1028,"Awni Y. Hannun, Pranav Rajpurkar, M. Haghpanahi, G. Tison, Codie Bourn, M. Turakhia, A. Ng","Cardiologist-level arrhythmia detection and classification in ambulatory electrocardiograms using a deep neural network",2019,"","","","",30,"2022-07-13 09:30:38","","10.1038/s41591-018-0268-3","","",,,,,1028,342.67,147,7,3,"","",""
3,"Daniel Weber, C. Gühmann, T. Seel","RIANN—A Robust Neural Network Outperforms Attitude Estimation Filters",2021,"","","","",31,"2022-07-13 09:30:38","","10.3390/ai2030028","","",,,,,3,3.00,1,3,1,"Inertial-sensor-based attitude estimation is a crucial technology in various applications, from human motion tracking to autonomous aerial and ground vehicles. Application scenarios differ in characteristics of the performed motion, presence of disturbances, and environmental conditions. Since state-of-the-art attitude estimators do not generalize well over these characteristics, their parameters must be tuned for the individual motion characteristics and circumstances. We propose RIANN, a ready-to-use, neural network-based, parameter-free, real-time-capable inertial attitude estimator, which generalizes well across different motion dynamics, environments, and sampling rates, without the need for application-specific adaptations. We gather six publicly available datasets of which we exploit two datasets for the method development and the training, and we use four datasets for evaluation of the trained estimator in three different test scenarios with varying practical relevance. Results show that RIANN outperforms state-of-the-art attitude estimation filters in the sense that it generalizes much better across a variety of motions and conditions in different applications, with different sensor hardware and different sampling frequencies. This is true even if the filters are tuned on each individual test dataset, whereas RIANN was trained on completely separate data and has never seen any of these test datasets. RIANN can be applied directly without adaptations or training and is therefore expected to enable plug-and-play solutions in numerous applications, especially when accuracy is crucial but no ground-truth data is available for tuning or when motion and disturbance characteristics are uncertain. We made RIANN publicly available.","",""
429,"Fu-Chen Chen, M. Jahanshahi","NB-CNN: Deep Learning-Based Crack Detection Using Convolutional Neural Network and Naïve Bayes Data Fusion",2018,"","","","",32,"2022-07-13 09:30:38","","10.1109/TIE.2017.2764844","","",,,,,429,107.25,215,2,4,"Regular inspection of nuclear power plant components is important to guarantee safe operations. However, current practice is time consuming, tedious, and subjective, which involves human technicians reviewing the inspection videos and identifying cracks on reactors. A few vision-based crack detection approaches have been developed for metallic surfaces, and they typically perform poorly when used for analyzing nuclear inspection videos. Detecting these cracks is a challenging task since they are tiny, and noisy patterns exist on the components’ surfaces. This study proposes a deep learning framework, based on a convolutional neural network (CNN) and a Naïve Bayes data fusion scheme, called NB-CNN, to analyze individual video frames for crack detection while a novel data fusion scheme is proposed to aggregate the information extracted from each video frame to enhance the overall performance and robustness of the system. To this end, a CNN is proposed to detect crack patches in each video frame, while the proposed data fusion scheme maintains the spatiotemporal coherence of cracks in videos, and the Naïve Bayes decision making discards false positives effectively. The proposed framework achieves a 98.3% hit rate against 0.1 false positives per frame that is significantly higher than state-of-the-art approaches as presented in this paper.","",""
0,"Jiahua Xu, Zheng Wu, A. Nürnberger, B. Sabel","Decoding Resting-state EEG to Predict Visual Field Defect with Convolutional Neural Network in Stroke",2021,"","","","",33,"2022-07-13 09:30:38","","10.1109/NER49283.2021.9441458","","",,,,,0,0.00,0,4,1,"Stroke is one of the leading factors of human being's death and disability. One-third of stroke patients may suffer partial visual field loss in both eyes. The relationship between brain oscillation after a unilateral occipital stroke and visual field defect is worth investigating. Decoding resting-state Electroencephalography (EEG) to predict patients' visual field distribution could be an essential reference for a better understanding of the compensation of visual functions after a stroke. The result could be beneficial for clinical diagnostics and treatment. This paper proposed a frequency spectrum-based 2D convolutional neural network(CNN) and brain connectivity-based 3DCNN model to predict the visual field defect. The results show that the frequency spectrum-based 2DCNN achieved a higher accuracy on visual field location than the connectivity-based 3DCNN model. Simultaneously, the percentage seems to be not predictable for both domains, and therefore we also explored the patterns of electrophysiological data for feature visualization and interpretation.","",""
7,"Shengzhong Liu, Shuochao Yao, Yifei Huang, Dongxin Liu, Huajie Shao, Yiran Zhao, Jinyang Li, Tianshi Wang, Ruijie Wang, Chaoqi Yang, T. Abdelzaher","Handling Missing Sensors in Topology-Aware IoT Applications with Gated Graph Neural Network",2020,"","","","",34,"2022-07-13 09:30:38","","10.1145/3411818","","",,,,,7,3.50,1,11,2,"Reliable data collection, transmission, and delivery on Internet of Things (IoT) systems is crucial in order to provide high-quality intelligent services. However, sensor data delivery can be interrupted for various reasons, such as sensor malfunction, network failures, and external attacks. Thus, only data from a partial set of sensors may be available. We call it the missing sensor problem. This problem can lead to severe performance degradation at inference time by neural-network-based recognition models trained on the complete sensor set. This paper enhances the robustness of neural network models to the missing sensor problem by introducing a novel feature reconstruction module, named the graph recovery module, that handles missing sensors directly inside the network. Specifically, we consider topology-aware IoT applications, where sensors are placed on a physically interconnected network. We design a novel neural message passing mechanism that logically mimics physical network topology, based on recent advances in graph neural networks (GNNs). We rely on a spatial locality assumption, where only correlations between physically connected sensors are explicitly explored. When encountering missing sensors, information is passed from available sensors to missing sensors to be used to reconstruct their features. Moreover, at each message passing step, we utilize a gating mechanism inspired by Gated Recurrent Units (GRUs) to automatically control information flow between available sensors and missing sensors. We empirically evaluate the reconstruction performance of the graph recovery module with two representative IoT applications; human activity recognition (HAR) and electroencephalogram (EEG)-based motor-imagery classification, on three public datasets. Two different backbone networks are utilized for the tasks. Our design is shown to effectively maintain model performance, suffering only 7% to 18% accuracy loss when as much as 90% of sensors are removed, compared to a drop of 15% to 47% in the accuracy of competing state-of-the-art algorithms under the same conditions. The accuracy gap is largest when more sensors are missing.","",""
78,"Hsien-I Lin, Ming-Hsiang Hsu, Wei-Kai Chen","Human hand gesture recognition using a convolution neural network",2014,"","","","",35,"2022-07-13 09:30:38","","10.1109/CoASE.2014.6899454","","",,,,,78,9.75,26,3,8,"Automatic human gesture recognition from camera images is an interesting topic for developing intelligent vision systems. In this paper, we propose a convolution neural network (CNN) method to recognize hand gestures of human task activities from a camera image. To achieve the robustness performance, the skin model and the calibration of hand position and orientation are applied to obtain the training and testing data for the CNN. Since the light condition seriously affects the skin color, we adopt a Gaussian Mixture model (GMM) to train the skin model which is used to robustly filter out non-skin colors of an image. The calibration of hand position and orientation aims at translating and rotating the hand image to a neutral pose. Then the calibrated images are used to train the CNN. In our experiment, we provided a validation of the proposed method on recognizing human gestures which shows robust results with various hand positions and orientations and light conditions. Our experimental evaluation of seven subjects performing seven hand gestures with average recognition accuracies around 95.96% shows the feasibility and reliability of the proposed method.","",""
50,"Kenneth P. Smith, A. D. Kang, J. Kirby","Automated Interpretation of Blood Culture Gram Stains by Use of a Deep Convolutional Neural Network",2017,"","","","",36,"2022-07-13 09:30:38","","10.1128/JCM.01521-17","","",,,,,50,10.00,17,3,5,"ABSTRACT Microscopic interpretation of stained smears is one of the most operator-dependent and time-intensive activities in the clinical microbiology laboratory. Here, we investigated application of an automated image acquisition and convolutional neural network (CNN)-based approach for automated Gram stain classification. Using an automated microscopy platform, uncoverslipped slides were scanned with a 40× dry objective, generating images of sufficient resolution for interpretation. We collected 25,488 images from positive blood culture Gram stains prepared during routine clinical workup. These images were used to generate 100,213 crops containing Gram-positive cocci in clusters, Gram-positive cocci in chains/pairs, Gram-negative rods, or background (no cells). These categories were targeted for proof-of-concept development as they are associated with the majority of bloodstream infections. Our CNN model achieved a classification accuracy of 94.9% on a test set of image crops. Receiver operating characteristic (ROC) curve analysis indicated a robust ability to differentiate between categories with an area under the curve of >0.98 for each. After training and validation, we applied the classification algorithm to new images collected from 189 whole slides without human intervention. Sensitivity and specificity were 98.4% and 75.0% for Gram-positive cocci in chains and pairs, 93.2% and 97.2% for Gram-positive cocci in clusters, and 96.3% and 98.1% for Gram-negative rods. Taken together, our data support a proof of concept for a fully automated classification methodology for blood-culture Gram stains. Importantly, the algorithm was highly adept at identifying image crops with organisms and could be used to present prescreened, classified crops to technologists to accelerate smear review. This concept could potentially be extended to all Gram stain interpretive activities in the clinical laboratory.","",""
43,"Yair Dgani, H. Greenspan, J. Goldberger","Training a neural network based on unreliable human annotation of medical images",2018,"","","","",37,"2022-07-13 09:30:38","","10.1109/ISBI.2018.8363518","","",,,,,43,10.75,14,3,4,"Building classification models from clinical data often requires human experts for example labeling. However, it is difficult to obtain a perfect set of labels due to the complexity of the medical data and the large variability between experts. In this study we present a neural-network training strategy that is more robust to unreliable labeling by explicitly modeling the label noise as part of the network architecture. Our method is demonstrated on breast microcalcifications classification into benign and malignant categories, given multi-view mammograms. We show that the proposed training procedure outperforms standard training methods that ignore the existence of label noise.","",""
44,"M. Mahmood, A. Jalal, M. A. Sidduqi","Robust Spatio-Temporal Features for Human Interaction Recognition Via Artificial Neural Network",2018,"","","","",38,"2022-07-13 09:30:38","","10.1109/FIT.2018.00045","","",,,,,44,11.00,15,3,4,"Human Interaction Recognition plays a key role in identification of usual and unusual human behaviors and facilitates public dealings, violence detection, robots perception, and virtual entertainments. This paper presents a novel human interaction recognition (HIR) system to recognize human interactions in continuous image sequences. The proposed technology segments full body silhouettes and identifies key body points to extract robust spatio-temporal features having distinct characteristics for each interaction. Our descriptors focus on local descriptions, capture intensity variations, point-to-point distances and time based relations. The system is trained through artificial neural network to recognize six basic interactions taken from UT-Interaction dataset.","",""
2,"Yi Liu, Min Peng, M. Swash, Tong Chen, R. Qin, H. Meng","Holoscopic 3D Microgesture Recognition by Deep Neural Network Model Based on Viewpoint Images and Decision Fusion",2021,"","","","",39,"2022-07-13 09:30:38","","10.1109/THMS.2020.3047914","","",,,,,2,2.00,0,6,1,"Finger microgestures have been widely used in human computer interaction (HCI), particularly for interactive applications, such as virtual reality (VR) and augmented reality (AR) technologies, to provide immersive experience. However, traditional 2D image-based microgesture recognition suffers from low accuracy due to the limitations of 2D imaging sensors, which have no depth information. In this article, we proposed an innovative 3D microgesture recognition system based on a holoscopic 3D imaging sensor. Due to the lack of holoscopic 3D datasets, a comprehensive holoscopic 3D microgesture (HoMG) database is created and used to develop a robust 3D microgesture recognition method. Then, a fast algorithm is proposed to extract multiviewpoint images from one holoscopic image. Furthermore, we applied a CNN model with an attention-based residual block to each viewpoint image to improve the algorithm performance. Finally, bagging classification tree decision-level fusion is applied to combine the predictions. The experimental results demonstrate that the proposed method outperforms state-of-the-art methods and delivers a better accuracy than existing methods.","",""
15,"M. Gogate, K. Dashtipour, P. Bell, A. Hussain","Deep Neural Network Driven Binaural Audio Visual Speech Separation",2020,"","","","",40,"2022-07-13 09:30:38","","10.1109/IJCNN48605.2020.9207517","","",,,,,15,7.50,4,4,2,"The central auditory pathway exploits the auditory signals and visual information sent by both ears and eyes to segregate speech from multiple competing noise sources and help disambiguate phonological ambiguity. In this study, inspired from this unique human ability, we present a deep neural network (DNN) that ingest the binaural sounds received at the two ears as well as the visual frames to selectively suppress the competing noise sources individually at both ears. The model exploits the noisy binaural cues and noise robust visual cues to improve speech intelligibility. The comparative simulation results in terms of objective metrics such as PESQ, STOI, SI-SDR and DBSTOI demonstrate significant performance improvement of the proposed audio-visual (AV) DNN as compared to the audio-only (A-only) variant of the proposed model. Finally, subjective listening tests with the real noisy AV ASPIRE corpus shows the superiority of the proposed AV DNN as compared to state-of-the-art approaches.","",""
50,"A. Hramov, N. Frolov, V. Maksimenko, V. Makarov, A. Koronovskii, J. Garcia-Prieto, L. Antón-Toro, F. Maestú, A. Pisarchik","Artificial neural network detects human uncertainty.",2018,"","","","",41,"2022-07-13 09:30:38","","10.1063/1.5002892","","",,,,,50,12.50,6,9,4,"Artificial neural networks (ANNs) are known to be a powerful tool for data analysis. They are used in social science, robotics, and neurophysiology for solving tasks of classification, forecasting, pattern recognition, etc. In neuroscience, ANNs allow the recognition of specific forms of brain activity from multichannel EEG or MEG data. This makes the ANN an efficient computational core for brain-machine systems. However, despite significant achievements of artificial intelligence in recognition and classification of well-reproducible patterns of neural activity, the use of ANNs for recognition and classification of patterns in neural networks still requires additional attention, especially in ambiguous situations. According to this, in this research, we demonstrate the efficiency of application of the ANN for classification of human MEG trials corresponding to the perception of bistable visual stimuli with different degrees of ambiguity. We show that along with classification of brain states associated with multistable image interpretations, in the case of significant ambiguity, the ANN can detect an uncertain state when the observer doubts about the image interpretation. With the obtained results, we describe the possible application of ANNs for detection of bistable brain activity associated with difficulties in the decision-making process.","",""
49,"Abeer N. Al-Nafjan, M. Hosny, A. Al-Wabil, Y. Al-Ohali","Classification of Human Emotions from Electroencephalogram (EEG) Signal using Deep Neural Network",2017,"","","","",42,"2022-07-13 09:30:38","","10.14569/IJACSA.2017.080955","","",,,,,49,9.80,12,4,5,"Estimation of human emotions from Electroencephalogram (EEG) signals plays a vital role in developing robust Brain-Computer Interface (BCI) systems. In our research, we used Deep Neural Network (DNN) to address EEG-based emotion recognition. This was motivated by the recent advances in accuracy and efficiency from applying deep learning techniques in pattern recognition and classification applications. We adapted DNN to identify human emotions of a given EEG signal (DEAP dataset) from power spectral density (PSD) and frontal asymmetry features. The proposed approach is compared to state-of-the-art emotion detection systems on the same dataset. Results show how EEG based emotion recognition can greatly benefit from using DNNs, especially when a large amount of training data is available.","",""
40,"Mohanad Babiker, Othman Omran Khalifa, K. K. Htike, Aisha Hassan, Muhamed Zaharadeen","Automated daily human activity recognition for video surveillance using neural network",2017,"","","","",43,"2022-07-13 09:30:38","","10.1109/ICSIMA.2017.8312024","","",,,,,40,8.00,8,5,5,"Surveillance video systems are gaining increasing attention in the field of computer vision due to its demands of users for the seek of security. It is promising to observe the human movement and predict such kind of sense of movements. The need arises to develop a surveillance system that capable to overcome the shortcoming of depending on the human resource to stay monitoring, observing the normal and suspect event all the time without any absent mind and to facilitate the control of huge surveillance system network. In this paper, an intelligent human activity system recognition is developed. Series of digital image processing techniques were used in each stage of the proposed system, such as background subtraction, binarization, and morphological operation. A robust neural network was built based on the human activities features database, which was extracted from the frame sequences. Multi-layer feed forward perceptron network used to classify the activities model in the dataset. The classification results show a high performance in all of the stages of training, testing and validation. Finally, these results lead to achieving a promising performance in the activity recognition rate.","",""
7,"Muhammad Fayyaz, Mussarat Yasmin, Muhammad Sharif, M. Raza","J-LDFR: joint low-level and deep neural network feature representations for pedestrian gender classification",2020,"","","","",44,"2022-07-13 09:30:38","","10.1007/s00521-020-05015-1","","",,,,,7,3.50,2,4,2,"","",""
30,"Yu Zeng, Kebei Jiang, Jie Chen","Automatic Seismic Salt Interpretation with Deep Convolutional Neural Networks",2018,"","","","",45,"2022-07-13 09:30:38","","10.1145/3325917.3325926","","",,,,,30,7.50,10,3,4,"One of the most crucial tasks in seismic reflection imaging is to identify the salt bodies with high precision. Traditionally, this is accomplished by visually picking the salt/sediment boundaries, which requires a great amount of manual work and may introduce systematic bias. With recent progress of deep learning algorithm and growing computational power, a great deal of efforts has been made to replace human effort with machine power in salt body interpretation. Currently, the method of Convolutional neural networks (CNN) is revolutionizing the computer vision field and has been a hot topic in the image analysis. In this paper, the benefits of CNN-based classification are demonstrated by using a state-of-art network structure U-Net, along with the residual learning framework ResNet, to delineate salt body with high precision. Network adjustments, including the Exponential Linear Units (ELU) activation function, the Lovasz-Softmax loss function, and stratified K-fold cross-validation, have been deployed to further improve the prediction accuracy. The preliminary result using SEG-SEAM data shows good agreement between the predicted salt body and manually interpreted salt body, especially in areas with weak reflections. This indicates the great potential of applying CNN for salt-related interpretations.","",""
0,"Rassa Ghavami Modegh, Ahmadali Salimi, H. Rabiee","LAP: An Attention-Based Module for Faithful Interpretation and Knowledge Injection in Convolutional Neural Networks",2022,"","","","",46,"2022-07-13 09:30:38","","","","",,,,,0,0.00,0,3,1,"Despite the state-of-the-art performance of deep convolutional neural networks, they are susceptible to bias and malfunction in unseen situations. The complex computation behind their reasoning is not sufficiently human-understandable to develop trust. External explainer methods have tried to interpret the network decisions in a human-understandable way, but they are accused of fallacies due to their assumptions and simplifications. On the other side, the inherent self-interpretability of models, while being more robust to the mentioned fallacies, cannot be applied to the already trained models. In this work, we propose a new attention-based pooling layer, called Local Attention Pooling (LAP), that accomplishes self-interpretability and the possibility for knowledge injection while improving the model’s performance. Moreover, several weakly-supervised knowledge injection methodologies are provided to enhance the process of training. We verified our claims by evaluating several LAP-extended models on three different datasets, including Imagenet. The proposed framework offers more valid humanunderstandable and more faithful-to-the-model interpretations than the commonly used white-box explainer methods.","",""
180,"Junwen Chen, Zhigang Liu, Hongrui Wang, A. Núñez, Zhiwei Han","Automatic Defect Detection of Fasteners on the Catenary Support Device Using Deep Convolutional Neural Network",2018,"","","","",47,"2022-07-13 09:30:38","","10.1109/TIM.2017.2775345","","",,,,,180,45.00,36,5,4,"The excitation and vibration triggered by the long-term operation of railway vehicles inevitably result in defective states of catenary support devices. With the massive construction of high-speed electrified railways, automatic defect detection of diverse and plentiful fasteners on the catenary support device is of great significance for operation safety and cost reduction. Nowadays, the catenary support devices are periodically captured by the cameras mounted on the inspection vehicles during the night, but the inspection still mostly relies on human visual interpretation. To reduce the human involvement, this paper proposes a novel vision-based method that applies the deep convolutional neural networks (DCNNs) in the defect detection of the fasteners. Our system cascades three DCNN-based detection stages in a coarse-to-fine manner, including two detectors to sequentially localize the cantilever joints and their fasteners and a classifier to diagnose the fasteners’ defects. Extensive experiments and comparisons of the defect detection of catenary support devices along the Wuhan–Guangzhou high-speed railway line indicate that the system can achieve a high detection rate with good adaptation and robustness in complex environments.","",""
1,"D. Dellavale, Osvaldo Matías Velarde, G. Mato, E. Urdapilleta","Complex interplay between spectral harmonicity and different types of cross-frequency couplings in nonlinear oscillators and biologically plausible neural network models.",2020,"","","","",48,"2022-07-13 09:30:38","","10.1103/physreve.102.062401","","",,,,,1,0.50,0,4,2,"Cross-frequency coupling (CFC) refers to the nonlinear interaction between oscillations in different frequency bands, and it is a rather ubiquitous phenomenon that has been observed in a variety of physical and biophysical systems. In particular, the coupling between the phase of slow oscillations and the amplitude of fast oscillations, referred as phase-amplitude coupling (PAC), has been intensively explored in the brain activity recorded from animals and humans. However, the interpretation of these CFC patterns remains challenging since harmonic spectral correlations characterizing nonsinusoidal oscillatory dynamics can act as a confounding factor. Specialized signal processing techniques are proposed to address the complex interplay between spectral harmonicity and different types of CFC, not restricted only to PAC. For this, we provide an in-depth characterization of the time locked index (TLI) as a tool aimed to efficiently quantify the harmonic content of noisy time series. It is shown that the proposed TLI measure is more robust and outperforms traditional phase coherence metrics (e.g., phase locking value, pairwise phase consistency) in several aspects. We found that a nonlinear oscillator under the effect of additive noise can produce spurious CFC with low spectral harmonic content. On the other hand, two coupled oscillatory dynamics with independent fundamental frequencies can produce true CFC with high spectral harmonic content via a rectification mechanism or other post-interaction nonlinear processing mechanisms. These results reveal a complex interplay between CFC and harmonicity emerging in the dynamics of biologically plausible neural network models and more generic nonlinear and parametric oscillators. We show that, contrary to what is usually assumed in the literature, the high harmonic content observed in nonsinusoidal oscillatory dynamics is neither a sufficient nor necessary condition to interpret the associated CFC patterns as epiphenomenal. There is mounting evidence suggesting that the combination of multimodal recordings, specialized signal processing techniques, and theoretical modeling is becoming a required step to completely understand CFC patterns observed in oscillatory rich dynamics of physical and biophysical systems.","",""
32,"Fei Tao, C. Busso","Gating Neural Network for Large Vocabulary Audiovisual Speech Recognition",2018,"","","","",49,"2022-07-13 09:30:38","","10.1109/TASLP.2018.2815268","","",,,,,32,8.00,16,2,4,"Audio-based automatic speech recognition (A-ASR) systems are affected by noisy conditions in real-world applications. Adding visual cues to the ASR system is an appealing alternative to improve the robustness of the system, replicating the audiovisual perception process used during human interactions. A common problem observed when using audiovisual automatic speech recognition (AV-ASR) is the drop in performance when speech is clean. In this case, visual features may not provide complementary information, introducing variability that negatively affects the performance of the system. The experimental evaluation in this study clearly demonstrates this problem when we train an audiovisual state-of-the-art hybrid system with a deep neural network (DNN) and hidden Markov models (HMMs). This study proposes a framework that addresses this problem, improving, or at least, maintaining the performance when visual features are used. The proposed approach is a deep learning solution with a gating layer that diminishes the effect of noisy or uninformative visual features, keeping only useful information. The framework is implemented with a subset of the audiovisual CRSS-4ENGLISH-14 corpus which consists of 61 h of speech from 105 subjects simultaneously collected with multiple cameras and microphones. The proposed framework is compared with conventional HMMs with observation models implemented with either a Gaussian mixture model or DNNs. We also compare the system with a multi-stream HMM system. The experimental evaluation indicates that the proposed framework outperforms alternative methods under all configurations, showing the robustness of the gating-based framework for AV-ASR.","",""
73,"Chuan Qin, Hengshu Zhu, Tong Xu, Chen Zhu, Liang Jiang, Enhong Chen, Hui Xiong","Enhancing Person-Job Fit for Talent Recruitment: An Ability-aware Neural Network Approach",2018,"","","","",50,"2022-07-13 09:30:38","","10.1145/3209978.3210025","","",,,,,73,18.25,10,7,4,"The wide spread use of online recruitment services has led to information explosion in the job market. As a result, the recruiters have to seek the intelligent ways for Person-Job Fit, which is the bridge for adapting the right job seekers to the right positions. Existing studies on Person-Job Fit have a focus on measuring the matching degree between the talent qualification and the job requirements mainly based on the manual inspection of human resource experts despite of the subjective, incomplete, and inefficient nature of the human judgement. To this end, in this paper, we propose a novel end-to-end A bility-aware P erson-J ob F it N eural N etwork (APJFNN) model, which has a goal of reducing the dependence on manual labour and can provide better interpretation about the fitting results. The key idea is to exploit the rich information available at abundant historical job application data. Specifically, we propose a word-level semantic representation for both job requirements and job seekers' experiences based on Recurrent Neural Network (RNN). Along this line, four hierarchical ability-aware attention strategies are designed to measure the different importance of job requirements for semantic representation, as well as measuring the different contribution of each job experience to a specific ability requirement. Finally, extensive experiments on a large-scale real-world data set clearly validate the effectiveness and interpretability of the APJFNN framework compared with several baselines.","",""
0,"D. Dellavale, Osvaldo Matías Velarde, G. Mato, E. Urdapilleta","On the complex interplay between spectral harmonicity and different types of cross frequency couplings in non linear oscillators and biologically plausible neural network models",2020,"","","","",51,"2022-07-13 09:30:38","","10.1101/2020.10.15.341800","","",,,,,0,0.00,0,4,2,"Background Cross-frequency coupling (CFC) refers to the non linear interaction between oscillations in different frequency bands, and it is a rather ubiquitous phenomenon that has been observed in a variety of physical and biophysical systems. In particular, the coupling between the phase of slow oscillations and the amplitude of fast oscillations, referred as phase-amplitude coupling (PAC), has been intensively explored in the brain activity recorded from animals and humans. However, the interpretation of these CFC patterns remains challenging since harmonic spectral correlations characterizing non sinusoidal oscillatory dynamics can act as a confounding factor. Methods Specialized signal processing techniques are proposed to address the complex interplay between spectral harmonicity and different types of CFC, not restricted only to PAC. For this, we provide an in-depth characterization of the Time Locked Index (TLI) as a novel tool aimed to efficiently quantify the harmonic content of noisy time series. It is shown that the proposed TLI measure is more robust and outperform traditional phase coherence metrics (e.g. Phase Locking Value, Pairwise Phase Consistency) in several aspects. Results We found that a non linear oscillator under the effect of additive noise can produce spurious CFC with low spectral harmonic content. On the other hand, two coupled oscillatory dynamics with independent fundamental frequencies can produce true CFC with high spectral harmonic content via a rectification mechanism or other post-interaction nonlinear processing mechanisms. These results reveal a complex interplay between CFC and harmonicity emerging in the dynamics of biologically plausible neural network models and more generic non linear and parametric oscillators. Conclusions We show that, contrary to what is usually assumed in the literature, the high harmonic content observed in non sinusoidal oscillatory dynamics, is neither sufficient nor necessary condition to interpret the associated CFC patterns as epiphenomenal. There is mounting evidence suggesting that the combination of multimodal recordings, specialized signal processing techniques and theoretical modeling is becoming a required step to completely understand CFC patterns observed in oscillatory rich dynamics of physical and biophysical systems. Highlights Time locked index efficiently quantifies the harmonic content of noisy time series. A non linear oscillator under the effect of additive noise can produce spurious cross frequency couplings (CFC) with low spectral harmonic content. Two coupled oscillatory dynamics with independent fundamental frequencies can produce true CFC with high spectral harmonic content via rectification mechanisms or other post-interaction nonlinear processing mechanisms. A non sinusoidal oscillatory dynamics with high harmonic content is neither sufficient nor necessary condition for spurious CFC. A complex interplay between CFC and harmonicity emerges from the dynamics of nonlinear, parametric and biologically plausible oscillators.","",""
117,"Cheng Zhang, Wu Liu, Huadong Ma, Huiyuan Fu","Siamese neural network based gait recognition for human identification",2016,"","","","",52,"2022-07-13 09:30:38","","10.1109/ICASSP.2016.7472194","","",,,,,117,19.50,29,4,6,"As the remarkable characteristics of remote accessed, robust and security, gait recognition has gained significant attention in the biometrics based human identification task. However, the existed methods mainly employ the handcrafted gait features, which cannot well handle the indistinctive inter-class differences and large intra-class variations of human gait in real-world situation. In this paper, we have developed a Siamese neural network based gait recognition framework to automatically extract robust and discriminative gait features for human identification. Different from conventional deep neural network, the Siamese network can employ distance metric learning to drive the similarity metric to be small for pairs of gait from the same person, and large for pairs from different persons. In particular, to further learn effective model with limited training data, we composite the gait energy images instead of raw sequence of gaits. Consequently, the experiments on the world's largest gait database show our framework impressively outperforms state-of-the-arts.","",""
276,"Timothy Niven, Hung-Yu Kao","Probing Neural Network Comprehension of Natural Language Arguments",2019,"","","","",53,"2022-07-13 09:30:38","","10.18653/v1/P19-1459","","",,,,,276,92.00,138,2,3,"We are surprised to find that BERT’s peak performance of 77% on the Argument Reasoning Comprehension Task reaches just three points below the average untrained human baseline. However, we show that this result is entirely accounted for by exploitation of spurious statistical cues in the dataset. We analyze the nature of these cues and demonstrate that a range of models all exploit them. This analysis informs the construction of an adversarial dataset on which all models achieve random accuracy. Our adversarial dataset provides a more robust assessment of argument comprehension and should be adopted as the standard in future work.","",""
83,"Juyeon Heo, Sunghwan Joo, Taesup Moon","Fooling Neural Network Interpretations via Adversarial Model Manipulation",2019,"","","","",54,"2022-07-13 09:30:38","","","","",,,,,83,27.67,28,3,3,"We ask whether the neural network interpretation methods can be fooled via adversarial model manipulation, which is defined as a model fine-tuning step that aims to radically alter the explanations without hurting the accuracy of the original models, e.g., VGG19, ResNet50, and DenseNet121. By incorporating the interpretation results directly in the penalty term of the objective function for fine-tuning, we show that the state-of-the-art saliency map based interpreters, e.g., LRP, Grad-CAM, and SimpleGrad, can be easily fooled with our model manipulation. We propose two types of fooling, Passive and Active, and demonstrate such foolings generalize well to the entire validation set as well as transfer to other interpretation methods. Our results are validated by both visually showing the fooled explanations and reporting quantitative metrics that measure the deviations from the original explanations. We claim that the stability of neural network interpretation method with respect to our adversarial model manipulation is an important criterion to check for developing robust and reliable neural network interpretation method.","",""
0,"Jindong Gu, Rui Zhao, Volker Tresp","Semantics for Global and Local Interpretation of Deep Convolutional Neural Networks",2021,"","","","",55,"2022-07-13 09:30:38","","10.1109/IJCNN52387.2021.9533809","","",,,,,0,0.00,0,3,1,"A large number of saliency methods have been proposed to explain individual decisions of deep convolutional neural networks (DCNNs). They work by identifying the relevance of each input feature to the predicted output class. However, the feature representations of hidden layers are difficult to interpret semantically. In this work, human-interpretable semantic concepts are associated with vectors in feature space. The association process is mathematically formulated as an optimization problem. The semantic vectors obtained from the optimal solution are applied to interpret deep neural networks globally and locally. The global interpretations are useful to understand the knowledge learned by DCNNs. The interpretation of local behaviors can help to gain a better understanding of the individual decisions made by DCNNs. The empirical experiments demonstrate how to use identified semantics to interpret the existing DCNNs.","",""
0,"P. Kebria, A. Khosravi, S. Nahavandi","Neural Network Control of Teleoperation Systems with Delay and Uncertainties based on Multilayer Perceptron Estimations",2020,"","","","",56,"2022-07-13 09:30:38","","10.1109/IJCNN48605.2020.9207035","","",,,,,0,0.00,0,3,2,"This paper investigates a novel synchronisation strategy for controlling Internet-based teleoperation systems. These kinds of systems considerably suffer from network-induced latencies. Random time-varying delays resulted by the Internet deteriorate the stability and performance of teleoperation processes. Moreover, uncertain dynamic elements, including human operators and partially known remote environments introduce further difficulties to the control design of such systems. Utilising the learning capabilities of artificial neural networks, this paper develops an adaptive algorithm to deal with time-delays and uncertainties negatively affecting an Internet-based teleoperation process. The stable convergence of the proposed control algorithm is proved by Lyapunov-Krasovskii stability criteria. Moreover, the robust performance of the controller is also verified via experimental evaluations.","",""
22,"Ravi Mangal, A. Nori, A. Orso","Robustness of Neural Networks: A Probabilistic and Practical Approach",2019,"","","","",57,"2022-07-13 09:30:38","","10.1109/ICSE-NIER.2019.00032","","",,,,,22,7.33,7,3,3,"Neural networks are becoming increasingly prevalent in software, and it is therefore important to be able to verify their behavior. Because verifying the correctness of neural networks is extremely challenging, it is common to focus on the verification of other properties of these systems. One important property, in particular, is robustness. Most existing definitions of robustness, however, focus on the worst-case scenario where the inputs are adversarial. Such notions of robustness are too strong, and unlikely to be satisfied by-and verifiable for-practical neural networks. Observing that real-world inputs to neural networks are drawn from non-adversarial probability distributions, we propose a novel notion of robustness: probabilistic robustness, which requires the neural network to be robust with at least (1 - ε) probability with respect to the input distribution. This probabilistic approach is practical and provides a principled way of estimating the robustness of a neural network. We also present an algorithm, based on abstract interpretation and importance sampling, for checking whether a neural network is probabilistically robust. Our algorithm uses abstract interpretation to approximate the behavior of a neural network and compute an overapproximation of the input regions that violate robustness. It then uses importance sampling to counter the effect of such overapproximation and compute an accurate estimate of the probability that the neural network violates the robustness property.","",""
37,"Zhidong Zhao, Yang Zhang, Zafer Comert, Yanjun Deng","Computer-Aided Diagnosis System of Fetal Hypoxia Incorporating Recurrence Plot With Convolutional Neural Network",2019,"","","","",58,"2022-07-13 09:30:38","","10.3389/fphys.2019.00255","","",,,,,37,12.33,9,4,3,"Background: Electronic fetal monitoring (EFM) is widely applied as a routine diagnostic tool by clinicians using fetal heart rate (FHR) signals to prevent fetal hypoxia. However, visual interpretation of the FHR usually leads to significant inter-observer and intra-observer variability, and false positives become the main cause of unnecessary cesarean sections. Goal: The main aim of this study was to ensure a novel, consistent, robust, and effective model for fetal hypoxia detection. Methods: In this work, we proposed a novel computer-aided diagnosis (CAD) system integrated with an advanced deep learning (DL) algorithm. For a 1-dimensional preprocessed FHR signal, the 2-dimensional image was transformed using recurrence plot (RP), which is considered to greatly capture the non-linear characteristics. The ultimate image dataset was enriched by changing several parameters of the RP and was then used to feed the convolutional neural network (CNN). Compared to conventional machine learning (ML) methods, a CNN can self-learn useful features from the input data and does not perform complex manual feature engineering (i.e., feature extraction and selection). Results: Finally, according to the optimization experiment, the CNN model obtained the average performance using optimal configuration across 10-fold: accuracy = 98.69%, sensitivity = 99.29%, specificity = 98.10%, and area under the curve = 98.70%. Conclusion: To the best of our knowledge, this approached achieved better classification performance in predicting fetal hypoxia using FHR signals compared to the other state-of-the-art works. Significance: In summary, the satisfied result proved the effectiveness of our proposed CAD system for assisting obstetricians making objective and accurate medical decisions based on RP and powerful CNN algorithm.","",""
62,"Brendon Lutnick, B. Ginley, D. Govind, S. McGarry, P. LaViolette, R. Yacoub, Sanjay Jain, J. Tomaszewski, K. Jen, P. Sarder","An integrated iterative annotation technique for easing neural network training in medical image analysis",2019,"","","","",59,"2022-07-13 09:30:38","","10.1038/S42256-019-0018-3","","",,,,,62,20.67,6,10,3,"","",""
0,"Pei Ma, Feng Yu, Changlong Zhou, Minghua Jiang","An Integrated Smoke Detection Method based on Convolutional Neural Network and Image Processing",2020,"","","","",60,"2022-07-13 09:30:38","","10.1109/ICCSNT50940.2020.9304985","","",,,,,0,0.00,0,4,2,"Fire is one of the disasters against the Safety of human life and property. Generally, early smoke features are more obvious than fire in the surveillance environment. However, due to the variability of smoke characteristics (e.g., color, shape) and the interference of smoke-like objects (e.g., clouds, rivulet, and fog), the main challenge of smoke detection is false alarms in real-world. To tackle this problem, an integrated method is proposed which combines HSV color space, background subtraction with Faster R-CNN. This method can enhance smoke feature, meanwhile, it can reduce disturbance from smoke-like objects. Furthermore, a dataset is created which are collected from surveillance cameras that are installed in the wild. Our experiments show that the integrated method is more accurate and robust than previous work.","",""
86,"J. Wenzel, H. Matter, K. Schmidt","Predictive Multitask Deep Neural Network Models for ADME-Tox Properties: Learning from Large Data Sets",2019,"","","","",61,"2022-07-13 09:30:38","","10.1021/acs.jcim.8b00785","","",,,,,86,28.67,29,3,3,"Successful drug discovery projects require control and optimization of compound properties related to pharmacokinetics, pharmacodynamics, and safety. While volume and chemotype coverage of public and corporate ADME-Tox (absorption, distribution, excretion, metabolism, and toxicity) databases are constantly growing, deep neural nets (DNN) emerged as transformative artificial intelligence technology to analyze those challenging data. Relevant features are automatically identified, while appropriate data can also be combined to multitask networks to evaluate hidden trends among multiple ADME-Tox parameters for implicitly correlated data sets. Here we describe a novel, fully industrialized approach to parametrize and optimize the setup, training, application, and visual interpretation of DNNs to model ADME-Tox data. Investigated properties include microsomal lability in different species, passive permeability in Caco-2/TC7 cells, and logD. Statistical models are developed using up to 50 000 compounds from public or corporate databases. Both the choice of DNN hyperparameters and the type and quantity of molecular descriptors were found to be important for successful DNN modeling. Alternate learning of multiple ADME-Tox properties, resulting in a multitask approach, performs statistically superior on most studied data sets in comparison to DNN single-task models and also provides a scalable method to predict ADME-Tox properties from heterogeneous data. For example, predictive quality using external validation sets was improved from R2 of 0.6 to 0.7 comparing single-task and multitask DNN networks from human metabolic lability data. Besides statistical evaluation, a new visualization approach is introduced to interpret DNN models termed ""response map"", which is useful to detect local property gradients based on structure fragmentation and derivatization. This method is successfully applied to visualize fragmental contributions to guide further design in drug discovery programs, as illustrated by CRCX3 antagonists and renin inhibitors, respectively.","",""
46,"Xiaoxiao Li, N. Dvornek, Yuan Zhou, Juntang Zhuang, P. Ventola, J. Duncan","Graph Neural Network for Interpreting Task-fMRI Biomarkers",2019,"","","","",62,"2022-07-13 09:30:38","","10.1007/978-3-030-32254-0_54","","",,,,,46,15.33,8,6,3,"","",""
0,"K. Li, Jiang Wang, Shanshan Li, Bin Deng, Haitao Yu","Latent Characteristics and Neural Manifold of Brain Functional Network Under Acupuncture",2022,"","","","",63,"2022-07-13 09:30:38","","10.1109/TNSRE.2022.3157380","","",,,,,0,0.00,0,5,1,"Acupuncture can regulate the cognition of brain system, and different manipulations are the keys of realizing the curative effect of acupuncture on human body. Therefore, it is crucial to distinguish and monitor the different acupuncture manipulations automatically. In this brief, in order to enhance the robustness of electroencephalogram (EEG) detection against noise and interference, we propose an acupuncture manipulation detecting framework based on supervised ISOMAP and recurrent neural network (RNN). Primarily, the low-dimensional embedding neural manifold of brain dynamical functional network is extracted via the reconstructed geodetic distance. It is found that there exhibits stronger acupuncture-specific reconfiguration of brain network. Besides, we show that the distance travel along this manifold correlates strongly with changes of acupuncture manipulations. The low-dimensional brain topological structure of all subjects shows crescent-like feature when acupuncturing at Zusanli acupoints, and fixed-points are varying under diverse manipulation methods. Moreover, Takagi-Sugeno-Kang (TSK) classifier is adopted to identify acupuncture manipulations according to the nonlinear characteristics of neural manifolds. Compared with different classifier, TSK can further improve the accuracy of manipulation identification at 96.71%. The results demonstrate the effectiveness of our model in detecting the acupuncture manipulations, which may provide neural biomarkers for acupuncture physicians.","",""
0,"Tianyi Lan, ZongBin Shi, Ke-jun Wang, Chaoqun Yin","Gait Recognition Algorithm based on Spatial-temporal Graph Neural Network",2022,"","","","",64,"2022-07-13 09:30:38","","10.1109/BDICN55575.2022.00018","","",,,,,0,0.00,0,4,1,"Gait recognition is an emerging biometric recognition technology. Gait features have the advantages of non-contact, long collection distance and so on. It has received extensive attention from researchers in the field of biometric identification. We propose a novel model-based gait recognition method. Early methods were mainly based on appearance. Appearance-based features usually use gait contour maps as input. The gait contour map is easy to obtain and proved to be effective for recognition tasks. However, its appearance will be affected by the changes of clothing and carrying items. Contrast to the contour-based method is the model-based method. We use the human pose estimation algorithm to obtain 3D key points, use the key points coordinates as graph nodes feature to build a spatial-temporal graph, and use graph neural network to extract features for gait recognition tasks. This method is experimented on the large-scale dataset CSAIA-B dataset. The experimental results show that the proposed method can achieve advanced performance. It is also robust to covariate changes.","",""
16,"P. Feldens, A. Darr, Agata Feldens, F. Tauber","Detection of Boulders in Side Scan Sonar Mosaics by a Neural Network",2019,"","","","",65,"2022-07-13 09:30:38","","10.3390/GEOSCIENCES9040159","","",,,,,16,5.33,4,4,3,"Boulders provide ecologically important hard grounds in shelf seas, and form protected habitats under the European Habitats Directive. Boulders on the seafloor can usually be recognized in backscatter mosaics due to a characteristic pattern of high backscatter intensity followed by an acoustic shadow. The manual identification of boulders on mosaics is tedious and subjective, and thus could benefit from automation. In this study, we train an object detection framework, RetinaNet, based on a neural network backbone, ResNet, to detect boulders in backscatter mosaics derived from a sidescan-sonar operating at 384 kHz. A training dataset comprising 4617 boulders and 2005 negative examples similar to boulders was used to train RetinaNet. The trained model was applied to a test area located in the Kriegers Flak area (Baltic Sea), and the results compared to mosaic interpretation by expert analysis. Some misclassification of water column noise and boundaries of artificial plough marks occurs, but the results of the trained model are comparable to the human interpretation. While the trained model correctly identified a higher number of boulders, the human interpreter had an advantage at recognizing smaller objects comprising a bounding box of less than 7 × 7 pixels. Almost identical performance between the best model and expert analysis was found when classifying boulder density into three classes (0, 1–5, more than 5) over 10,000 m² areas, with the best performing model reaching an agreement with the human interpretation of 90%.","",""
77,"Lixin Fan, Kam Woh Ng, Chee Seng Chan","Rethinking Deep Neural Network Ownership Verification: Embedding Passports to Defeat Ambiguity Attacks",2019,"","","","",66,"2022-07-13 09:30:38","","","","",,,,,77,25.67,26,3,3,"With substantial amount of time, resources and human (team) efforts invested to explore and develop successful deep neural networks (DNN), there emerges an urgent need to protect these inventions from being illegally copied, redistributed, or abused without respecting the intellectual properties of legitimate owners. Following recent progresses along this line, we investigate a number of watermark-based DNN ownership verification methods in the face of ambiguity attacks, which aim to cast doubts on the ownership verification by forging counterfeit watermarks. It is shown that ambiguity attacks pose serious threats to existing DNN watermarking methods. As remedies to the above-mentioned loophole, this paper proposes novel passport-based DNN ownership verification schemes which are both robust to network modifications and resilient to ambiguity attacks. The gist of embedding digital passports is to design and train DNN models in a way such that, the DNN inference performance of an original task will be significantly deteriorated due to forged passports. In other words, genuine passports are not only verified by looking for the predefined signatures, but also reasserted by the unyielding DNN model inference performances. Extensive experimental results justify the effectiveness of the proposed passport-based DNN ownership verification schemes. Code and models are available at https://github.com/kamwoh/DeepIPR","",""
90,"Chandan Singh, W. James Murdoch, Bin Yu","Hierarchical interpretations for neural network predictions",2018,"","","","",67,"2022-07-13 09:30:38","","","","",,,,,90,22.50,30,3,4,"Deep neural networks (DNNs) have achieved impressive predictive performance due to their ability to learn complex, non-linear relationships between variables. However, the inability to effectively visualize these relationships has led to DNNs being characterized as black boxes and consequently limited their applications. To ameliorate this problem, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method, agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. Using examples from Stanford Sentiment Treebank and ImageNet, we show that ACD is effective at diagnosing incorrect predictions and identifying dataset bias. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.","",""
129,"Wenzhi Zhao, S. Du, W. Emery","Object-Based Convolutional Neural Network for High-Resolution Imagery Classification",2017,"","","","",68,"2022-07-13 09:30:38","","10.1109/JSTARS.2017.2680324","","",,,,,129,25.80,43,3,5,"Timely and accurate classification and interpretation of high-resolution images are very important for urban planning and disaster rescue. However, as spatial resolution gets finer, it is increasingly difficult to recognize complex patterns in high-resolution remote sensing images. Deep learning offers an efficient strategy to fill the gap between complex image patterns and their semantic labels. However, due to the hierarchical abstract nature of deep learning methods, it is difficult to capture the precise outline of different objects at the pixel level. To further reduce this problem, we propose an object-based deep learning method to accurately classify the high-resolution imagery without intensive human involvement. In this study, high-resolution images were used to accurately classify three different urban scenes: Beijing (China), Pavia (Italy), and Vaihingen (Germany). The proposed method is built on a combination of a deep feature learning strategy and an object-based classification for the interpretation of high-resolution images. Specifically, high-level feature representations extracted through the convolutional neural networks framework have been systematically investigated over five different layer configurations. Furthermore, to improve the classification accuracy, an object-based classification method also has been integrated with the deep learning strategy for more efficient image classification. Experimental results indicate that with the combination of deep learning and object-based classification, it is possible to discriminate different building types in Beijing Scene, such as commercial buildings and residential buildings with classification accuracies above 90%.","",""
0,"S. Wein, A. Schüller, Ana Maria Tom'e, W. M. Malloni, M. Greenlee, E. Lang","Forecasting Brain Activity Based on Models of Spatio-Temporal Brain Dynamics: A Comparison of Graph Neural Network Architectures",2021,"","","","",69,"2022-07-13 09:30:38","","10.1162/netn_a_00252","","",,,,,0,0.00,0,6,1,"  Comprehending the interplay between spatial and temporal characteristics of neural dynamics can contribute to our understanding of information processing in the human brain. Graph neural networks (GNNs) provide a new possibility to interpret graph structured signals like those observed in complex brain networks. In our study we compare different spatio-temporal GNN architectures and study their ability to model neural activity distributions obtained in functional MRI (fMRI) studies. We evaluate the performance of the GNN models on a variety of scenarios in MRI studies and also compare it to a VAR model, which is currently often used for directed functional connectivity analysis. We show that by learning localized functional interactions on the anatomical substrate, GNN based approaches are able to robustly scale to large network studies, even when available data are scarce. By including anatomical connectivity as the physical substrate for information propagation, such GNNs also provide a multi-modal perspective on directed connectivity analysis, offering a novel possibility to investigate the spatio-temporal dynamics in brain networks.","",""
92,"Guy Katz, C. Barrett, D. Dill, Kyle D. Julian, Mykel J. Kochenderfer","Towards Proving the Adversarial Robustness of Deep Neural Networks",2017,"","","","",70,"2022-07-13 09:30:38","","10.4204/EPTCS.257.3","","",,,,,92,18.40,18,5,5,"Autonomous vehicles are highly complex systems, required to function reliably in a wide variety of situations. Manually crafting software controllers for these vehicles is difficult, but there has been some success in using deep neural networks generated using machine-learning. However, deep neural networks are opaque to human engineers, rendering their correctness very difficult to prove manually; and existing automated techniques, which were not designed to operate on neural networks, fail to scale to large systems. This paper focuses on proving the adversarial robustness of deep neural networks, i.e. proving that small perturbations to a correctly-classified input to the network cannot cause it to be misclassified. We describe some of our recent and ongoing work on verifying the adversarial robustness of networks, and discuss some of the open questions we have encountered and how they might be addressed.","",""
67,"D. Gopinath, Guy Katz, C. Pasareanu, C. Barrett","DeepSafe: A Data-driven Approach for Checking Adversarial Robustness in Neural Networks",2017,"","","","",71,"2022-07-13 09:30:38","","","","",,,,,67,13.40,17,4,5,"Deep neural networks have become widely used, obtaining remarkable results in domains such as computer vision, speech recognition, natural language processing, audio recognition, social network filtering, machine translation, and bio-informatics, where they have produced results comparable to human experts. However, these networks can be easily fooled by adversarial perturbations: minimal changes to correctly-classified inputs, that cause the network to mis-classify them. This phenomenon represents a concern for both safety and security, but it is currently unclear how to measure a network's robustness against such perturbations. Existing techniques are limited to checking robustness around a few individual input points, providing only very limited guarantees. We propose a novel approach for automatically identifying safe regions of the input space, within which the network is robust against adversarial perturbations. The approach is data-guided, relying on clustering to identify well-defined geometric regions as candidate safe regions. We then utilize verification techniques to confirm that these regions are safe or to provide counter-examples showing that they are not safe. We also introduce the notion of targeted robustness which, for a given target label and region, ensures that a NN does not map any input in the region to the target label. We evaluated our technique on the MNIST dataset and on a neural network implementation of a controller for the next-generation Airborne Collision Avoidance System for unmanned aircraft (ACAS Xu). For these networks, our approach identified multiple regions which were completely safe as well as some which were only safe for specific labels. It also discovered several adversarial perturbations of interest.","",""
3,"Shanlin Zhong, Junjie Zhou, Hong Qiao","Bioinspired Gain-Modulated Recurrent Neural Network for Controlling Musculoskeletal Robot.",2021,"","","","",72,"2022-07-13 09:30:38","","10.1109/TNNLS.2021.3071196","","",,,,,3,3.00,1,3,1,"The motor cortex can arouse abundant transient responses to generate complex movements with the regulation of neuromodulators, while its architecture remains unchanged. This characteristic endows humans with flexible and robust abilities in adapting to dynamic environments, which is exactly the bottleneck in the control of complex robots. In this article, inspired by the mechanisms of the motor cortex in encoding information and modulating motor commands, a biologically plausible gain-modulated recurrent neural network is proposed to control a highly redundant, coupled, and nonlinear musculoskeletal robot. As the characteristics observed in the motor cortex, this network is able to learn gain patterns for arousing transient responses to complete the desired movements, while the connections of synapses keep unchanged, and the dynamic stability of the network is maintained. A novel learning rule that mimics the mechanism of neuromodulators in regulating the learning process of the brain is put forward to learn gain patterns effectively. Meanwhile, inspired by error-based movement correction mechanism in the cerebellum, gain patterns learned from demonstration samples are leveraged as prior knowledge to improve calculation efficiency of the network in controlling novel movements. Experiments were conducted on an upper extremity musculoskeletal model with 11 muscles and a general articulated robot to perform goal-directed tasks. The results indicate that the gain-modulated neural network can effectively control a complex robot to complete various movements with high accuracy, and the proposed algorithms make it possible to realize fast generalization and incremental learning ability.","",""
3,"Graham Roberts, Rajat Sainju, Brian Hutchinson, M. Toloczko, David Edwards, Yuanyuan Zhu","DefectNet – A Deep Convolutional Neural Network for Semantic Segmentation of Crystallographic Defects in Advanced Microscopy Images",2019,"","","","",73,"2022-07-13 09:30:38","","10.1017/S1431927619001557","","",,,,,3,1.00,1,6,3,"The current practice of identifying defects in microscopy images and deriving metrics such as dislocation density and precipitates/voids diameter remains largely in the purview of human analysis. The lack of automated defect analysis for statistically meaningful quantification of a variety types of crystallographic defects is causing an increasingly large bottleneck for rational alloy design. The first and most important step of automating defect analysis is perceptual defect identification. In terms of digital image processing, semantic segmentation best emulates human recognition of defect features – it tells what defects are in an image and where they are located. In this work, we developed a novel deep convolutional neural network (CNN) model, called DefectNet, for robust and automated semantic segmentation of three crystallographic defects including line dislocations, precipitates, and voids commonly observed in structural metals and alloys [1]. Defect semantic segmentation in TEM micrographs is a challenging deep learning task due to the nature of the image itself. Unlike everyday photographs, the interpretation of image contrast in TEM micrographs is usually not straightforward; multiple contrast mechanisms often contribute to the observation of defect features. Here, we aim at resolving this image-induced challenge by optimizing the image quality. In previous work, we established an experimental protocol for a diffraction contrast imaging scanning transmission electron microscopy (DCI STEM) technique and tailored it specifically for imaging defects in popular iron-based structural alloys [2]. Thus, the DefectNet was trained on a small set of high-quality DCI STEM defect images obtained from HT-9 martensitic steels. The performance of the resulting model for each defect was assessed quantitatively by standard semantic segmentation evaluation metrics, and the resulting defect density and size measurements compared to that from a group of human experts.","",""
56,"Alexander Mathis, Mert Yüksekgönül, Byron Rogers, M. Bethge, M. Mathis","Pretraining boosts out-of-domain robustness for pose estimation",2019,"","","","",74,"2022-07-13 09:30:38","","10.1109/WACV48630.2021.00190","","",,,,,56,18.67,11,5,3,"Neural networks are highly effective tools for pose estimation. However, as in other computer vision tasks, robustness to out-of-domain data remains a challenge, especially for small training sets that are common for real-world applications. Here, we probe the generalization ability with three architecture classes (MobileNetV2s, ResNets, and EfficientNets) for pose estimation. We developed a dataset of 30 horses that allowed for both ""within-domain"" and ""out-of-domain"" (unseen horse) benchmarking—this is a crucial test for robustness that current human pose estimation benchmarks do not directly address. We show that better ImageNet-performing architectures perform better on both within- and out-of-domain data if they are first pretrained on ImageNet. We additionally show that better ImageNet models generalize better across animal species. Furthermore, we introduce Horse-C, a new benchmark for common corruptions for pose estimation, and confirm that pretraining increases performance in this domain shift context as well. Overall, our results demonstrate that transfer learning is beneficial for out-of-domain robustness.","",""
55,"Cassidy Laidlaw, Sahil Singla, S. Feizi","Perceptual Adversarial Robustness: Defense Against Unseen Threat Models",2020,"","","","",75,"2022-07-13 09:30:38","","","","",,,,,55,27.50,18,3,2,"We present adversarial attacks and defenses for the perceptual adversarial threat model: the set of all perturbations to natural images which can mislead a classifier but are imperceptible to human eyes. The perceptual threat model is broad and encompasses $L_2$, $L_\infty$, spatial, and many other existing adversarial threat models. However, it is difficult to determine if an arbitrary perturbation is imperceptible without humans in the loop. To solve this issue, we propose to use a {\it neural perceptual distance}, an approximation of the true perceptual distance between images using internal activations of neural networks. In particular, we use the Learned Perceptual Image Patch Similarity (LPIPS) distance. We then propose the {\it neural perceptual threat model} that includes adversarial examples with a bounded neural perceptual distance to natural images. Under the neural perceptual threat model, we develop two novel perceptual adversarial attacks to find any imperceptible perturbations to images which can fool a classifier. Through an extensive perceptual study, we show that the LPIPS distance correlates well with human judgements of perceptibility of adversarial examples, validating our threat model. Because the LPIPS threat model is very broad, we find that Perceptual Adversarial Training (PAT) against a perceptual attack gives robustness against many other types of adversarial attacks. We test PAT on CIFAR-10 and ImageNet-100 against 12 types of adversarial attacks and find that, for each attack, PAT achieves close to the accuracy of adversarial training against just that perturbation type. That is, PAT generalizes well to unforeseen perturbation types. This is vital in sensitive applications where a particular threat model cannot be assumed, and to the best of our knowledge, PAT is the first adversarial defense with this property.","",""
29,"C. Corbane, V. Syrris, F. Sabo, P. Politis, M. Melchiorri, M. Pesaresi, P. Soille, T. Kemper","Convolutional Neural Networks for Global Human Settlements Mapping from Sentinel-2 Satellite Imagery",2020,"","","","",76,"2022-07-13 09:30:38","","10.1007/S00521-020-05449-7","","",,,,,29,14.50,4,8,2,"","",""
182,"Laksshman Sundaram, Hong Gao, Samskruthi Reddy Padigepati, J. McRae, Yanjun Li, J. Kosmicki, Nondas Fritzilas, J. Hakenberg, Anindita Dutta, J. Shon, Jinbo Xu, S. Batzoglou, Xiaolin Li, K. Farh","Predicting the clinical impact of human mutation with deep neural networks",2018,"","","","",77,"2022-07-13 09:30:38","","10.1038/s41588-018-0167-z","","",,,,,182,45.50,18,14,4,"","",""
4,"Christian Berghoff, Pavol Bielik, Matthias Neu, Petar Tsankov, Arndt von Twickel","Robustness Testing of AI Systems: A Case Study for Traffic Sign Recognition",2021,"","","","",78,"2022-07-13 09:30:38","","10.1007/978-3-030-79150-6_21","","",,,,,4,4.00,1,5,1,"","",""
193,"Ya Su, Youjian Zhao, Chenhao Niu, Rong Liu, Wei Sun, Dan Pei","Robust Anomaly Detection for Multivariate Time Series through Stochastic Recurrent Neural Network",2019,"","","","",79,"2022-07-13 09:30:38","","10.1145/3292500.3330672","","",,,,,193,64.33,32,6,3,"Industry devices (i.e., entities) such as server machines, spacecrafts, engines, etc., are typically monitored with multivariate time series, whose anomaly detection is critical for an entity's service quality management. However, due to the complex temporal dependence and stochasticity of multivariate time series, their anomaly detection remains a big challenge. This paper proposes OmniAnomaly, a stochastic recurrent neural network for multivariate time series anomaly detection that works well robustly for various devices. Its core idea is to capture the normal patterns of multivariate time series by learning their robust representations with key techniques such as stochastic variable connection and planar normalizing flow, reconstruct input data by the representations, and use the reconstruction probabilities to determine anomalies. Moreover, for a detected entity anomaly, OmniAnomaly can provide interpretations based on the reconstruction probabilities of its constituent univariate time series. The evaluation experiments are conducted on two public datasets from aerospace and a new server machine dataset (collected and released by us) from an Internet company. OmniAnomaly achieves an overall F1-Score of 0.86 in three real-world datasets, signicantly outperforming the best performing baseline method by 0.09. The interpretation accuracy for OmniAnomaly is up to 0.89.","",""
0,"Kenneth P. Smith, A. D. Kang, J. Kirby","Automated Interpretation of Blood Culture Gram Stains using a Deep Convolutional Neural 1 Network 2 3 Running Title : Gram stain interpretation with deep learning",2017,"","","","",80,"2022-07-13 09:30:38","","","","",,,,,0,0.00,0,3,5,"24 Microscopic interpretation of stained smears is one of the most operator-dependent and time 25 intensive activities in the clinical microbiology laboratory. Here, we investigated application of 26 an automated image acquisition and convolutional neural network (CNN)-based approach for 27 automated Gram stain classification. Using an automated microscopy platform, uncoverslipped 28 slides were scanned with a 40x dry objective, generating images of sufficient resolution for 29 interpretation. We collected 25,488 images from positive blood culture Gram stains prepared 30 during routine clinical workup. These images were used to generate 100,213 crops containing 31 Gram-positive cocci in clusters, Gram-positive cocci in chains/pairs, Gram-negative rods, or 32 background (no cells). These categories were targeted for proof-of-concept development as they 33 are associated with the majority of bloodstream infections. Our CNN model achieved 34 classification accuracy of 94.9% on a test set of image crops. Receiver operating characteristic 35 curve (ROC) analysis indicated a robust ability to differentiate between categories with area 36 under the curve >0.98 for each. After training and validation, we applied the classification 37 algorithm to new images collected from 189 whole slides without human intervention. 38 Sensitivity/specificity was 98.4/75.0% for Gram-positive cocci in chains/pairs; 93.2/97.2% for 39 Gram-positive cocci in clusters; and 96.3/98.1% for Gram-negative rods. Taken together, our 40 data support proof-of-concept for a fully automated classification methodology for blood-culture 41 Gram-stains. Importantly, the algorithm was highly adept at identifying image crops with 42 organisms and could be used to present prescreened, classified crops to technologists to 43 accelerate smear review. This concept could potentially be extended to all Gram stain 44 interpretive activities in the clinical laboratory. 45 46 on D ecem er 1, 2017 by gest ht://jcm .sm .rg/ D ow nladed fom Introduction 47 Bloodstream infections (BSI) are rapidly progressive infections with mortality rates up to 48 nearly 40% (1, 2). Each day delay in institution of active antimicrobial therapy is associated with 49 up to a ~10% increase in mortality (3, 4). Due to relatively low bacterial burden (<10 CFU mL 50 1 )(5), patient blood is pre-incubated in broth culture to detect presence of bacteria, typically by 51 semi-continuous measurement of CO2 production or pH with an automated blood culture 52 instrument. If organism growth is detected, an aliquot of broth (now containing >10 6 CFU mL -1 ) 53 is removed for Gram stain smear and subculture. The Gram stain provides the first critical piece 54 of information that allows a clinician to tailor appropriate therapy and optimize outcome (6). 55 Despite recent advances in automation in other stages of the BSI diagnosis process 56 (automated blood culture incubators and Gram staining systems) (7), Gram stain interpretation 57 remains labor and time intensive, and highly operator-dependent. With consolidation of hospital 58 systems, increasing workloads, and potential unavailability of highly trained microbiologists on 59 site (8), automated image collection paired with computational interpretation of Gram stains to 60 augment and complement manual testing would provide benefit. However, there has been a 61 dearth of scientific exploration in this area, and several technical difficulties need to be 62 overcome. 63 Practically, automated Gram stain interpretation requires both automated slide imaging 64 and automated image analysis. Although automated slide scanners and microscopes are being 65 used in anatomic pathology, for example, telepathology (9), their application in clinical 66 microbiology has been limited based on several technical challenges. First, Gram stained slides 67 are typically read using 100X objectives, greatly complicating image acquisition due to the need 68 for addition of oil during scanning. Second, microbiology smear material can adequately be 69 on D ecem er 1, 2017 by gest ht://jcm .sm .rg/ D ow nladed fom imaged only in a very narrow field of focus, a challenge for existing slide scanners. Third, Gram 70 stained slides exhibit ubiquitous and highly variable background staining. This background may 71 cause autofocus algorithms to target areas that are either devoid of bacteria or miss the 72 appropriate focal plane entirely. Image analysis to identify Gram stain characteristics presents 73 separate hurdles. Importantly, background and staining artifacts, both fairly ubiquitous, often 74 mimics the shape and color of bacterial cells. Therefore, algorithms relying on color intensity 75 thresholding and shape detection will provide suboptimal accuracy. 76 Here, we provide proof-of-concept for automated, deep learning-based Gram stain 77 analysis. The major conceptual and technical innovations were twofold. First, we developed an 78 imaging protocol using an automated slide imaging platform equipped with a 40X air objective 79 to collect highly resolved data from Gram-stained blood culture slides. Second, image data were 80 used to train a convolutional neural network (CNN)-based model to recognize morphologies 81 representing the most common causative agents of BSI: Gram-negative rods, Gram-positive 82 cocci in clusters, and Gram-positive cocci in pairs or chains (1). CNNs are modeled based on the 83 organization of neurons within the mammalian visual cortex, and were applied here based on 84 their ability to excel in image recognition tasks without requiring time-intensive selective feature 85 extraction by humans (10). Our trained model was subsequently evaluated for accuracy in 86 comparison to manual classification. 87 88 Results 89 Slide collection and manual classification. Blood culture Gram stain slides prepared 90 manually during the course of normal laboratory operation were used for analysis. Slides were 91 selected based on the presence of any of the three most common morphotypes observed in 92 on D ecem er 1, 2017 by gest ht://jcm .sm .rg/ D ow nladed fom bloodstream infection: Gram-positive cocci in clusters, Gram-positive cocci in pairs and chains, 93 and Gram-negative rods. Less common morphotypes (e.g. Gram-positive rods or yeast) and 94 polymicrobial infections were excluded. To capture real-world variability, slides were not pre95 screened for suitability for automated microscopy or deep learning, and had characteristic slide96 to-slide variability in staining intensity, staining artifacts, and sample distribution. We 97 anticipated that inherent variability would pose a real-world challenge to slide classification 98 models. 99 Automated image collection. CNN-based deep learning models require large datasets 100 for training, typically at least on the order of thousands of images (and ideally at least an order of 101 magnitude more). Therefore, an automated microscopy image acquisition strategy was used. We 102 performed image acquisition on the MetaFer Slide Scanning and Imaging Platform 103 (MetaSystems Group, Inc., Newton, MA) based on a robust Gram stain-compatible autofocus 104 system, ability to sample multiple distributed positions on a slide to account for variations in 105 specimen distribution, and automated slide loading capability to enable high throughput slide 106 scanning. 107 Clinically, Gram stains are read under oil immersion. However, semi-continuous addition 108 of oil during automated microscopy was undesirable. In preliminary experiments with 109 uncoverslipped slides (data not shown), we determined that the 40x dry objective provided 110 sufficient resolution for machine-learning applications based on our prior experience (11). 111 Therefore, we selected use of the 40x air objective for image acquisition, thus avoiding the 112 requirement for oil immersion and allowing us to capture a larger field of view in each image. 113 Deep convolutional neural network training. For CNN training, a total of 25,488 114 images were automatically collected from distributed locations on 180 slides. A representative 115 on D ecem er 1, 2017 by gest ht://jcm .sm .rg/ D ow nladed fom image is shown in Fig. 1. This image demonstrates features typical of blood culture Gram stain 116 smears including: (A) intense background staining; (B) stain crystallization artifact; (C) diffuse 117 background staining; (D) individually resolvable, high-contrast Gram-negative cells; and (E) 118 individually resolvable, low-contrast Gram-negative cells. Of note, ubiquitous background 119 material was often similar in color, intensity, and/or shape to bacterial cells. 120 Highly experienced medical technologists can readily differentiate bacteria from this 121 background. However, it is prohibitively difficult to manually define computational rules for 122 Gram-stain classification that would adequately distinguish signal from noise in highly variable 123 Gram-stain preparations. Therefore, we chose instead to use a deep learning approach, more 124 specifically, a CNN, for image analysis. CNNs do not interpret raw images directly. Rather, they 125 consist of a number of layers, each of which convolutes regions of the image to detect specific 126 features. During each step of the learning process, a subset of images is presented to the network, 127 allowing function parameters to be changed such that the CNN identifies features important for 128 classification based on optimization of output accuracy. The final model is defined by a set of 129 weights and biases that control the flow of information through the network such that the most 130 discriminatory features in the images are used for classification. 131 Each CNN model has a unique architecture that differs in organization, function and 132 number of convolutional layers (10). The model used in our analysis, Inception v3, has 133 previously been shown to perform robustly on complex image classification ","",""
69,"Khan Suhail Ahmad, Anil S. Thosar, J. Nirmal, V. S. Pande","A unique approach in text independent speaker recognition using MFCC feature sets and probabilistic neural network",2015,"","","","",81,"2022-07-13 09:30:38","","10.1109/ICAPR.2015.7050669","","",,,,,69,9.86,17,4,7,"This paper motivates the use of combination of mel frequency cepstral coefficients (MFCC) and its delta derivatives (DMFCC and DDMFCC) calculated using mel spaced Gaussian filter banks for text independent speaker recognition. MFCC modeled on the human auditory system shows robustness against noise and session changes and hence has become synonymous with speaker recognition. Our main aim is to test the accuracy of our proposed feature set for different values of frame overlap and MFCC feature vector sizes to identify the system having highest accuracy. Principal component analysis (PCA) is applied before the training and testing stages for feature dimensionality reduction thereby increasing computing speed and puts low constraint on the memory required for processing. The use of probabilistic neural network (PNN) in the modeling domain provided the advantages of achieving lower operational times during the training stages. The experiments examined the percentage identification accuracy (PIA) of MFCC, combination of MFCC and DMFCC as well as combination of all three feature sets MFCC, DMFCC and DDMFCC. The proposed feature set attains an identification accuracy of 94% for frame overlap of 90% and MFCC feature size of 18 coefficients. It outperforms the identification rates of the other two feature sets. These speaker recognition experiments were tested using the Voxforge database.","",""
23,"A. A. Alani, G. Cosma, Aboozar Taherkhani, T. McGinnity","Hand gesture recognition using an adapted convolutional neural network with data augmentation",2018,"","","","",82,"2022-07-13 09:30:38","","10.1109/INFOMAN.2018.8392660","","",,,,,23,5.75,6,4,4,"Hand gestures provide a natural way for humans to interact with computers to perform a variety of different applications. However, factors such as the complexity of hand gesture structures, differences in hand size, hand posture, and environmental illumination can influence the performance of hand gesture recognition algorithms. Recent advances in Deep Learning have significantly advanced the performance of image recognition systems. In particular, the Deep Convolutional Neural Network has demonstrated superior performance in image representation and classification, compared to conventional machine learning approaches. This paper proposes an Adapted Deep Convolutional Neural Network (ADCNN) suitable for hand gesture recognition tasks. Data augmentation is initially applied which shifts images both horizontally and vertically to an extent of 20% of the original dimensions randomly, in order to numerically increase the size of the dataset and to add the robustness needed for a deep learning approach. These images are input into the proposed ADCNN model which is empowered by the presence of network initialization (ReLU and Softmax) and L2 Regularization to eliminate the problem of data overfitting. With these modifications, the experimental results using the ADCNN model demonstrate that it is an effective method of increasing the performance of CNN for hand gesture recognition. The model was trained and tested using 3750 static hand gesture images, which incorporate variations in features such as scale, rotation, translation, illumination and noise. The proposed ADCNN was compared to a baseline Convolutional Neural Network and the results show that the proposed ADCNN achieved a classification recognition accuracy of 99.73%, and a 4% improvement over the baseline Convolutional Neural Network model (95.73%).","",""
81,"Kaipeng Zhang, Zhanpeng Zhang, Chia-Wen Cheng, Winston H. Hsu, Y. Qiao, W. Liu, T. Zhang","Super-Identity Convolutional Neural Network for Face Hallucination",2018,"","","","",83,"2022-07-13 09:30:38","","10.1007/978-3-030-01252-6_12","","",,,,,81,20.25,12,7,4,"","",""
1,"Kunling Geng, Dae C. Shin, D. Song, R. Hampson, S. Deadwyler, T. Berger, V. Marmarelis","Multi-Input, Multi-Output Neuronal Mode Network Approach to Modeling the Encoding Dynamics and Functional Connectivity of Neural Systems",2019,"","","","",84,"2022-07-13 09:30:38","","10.1162/neco_a_01204","","",,,,,1,0.33,0,7,3,"This letter proposes a novel method, multi-input, multi-output neuronal mode network (MIMO-NMN), for modeling encoding dynamics and functional connectivity in neural ensembles such as the hippocampus. Compared with conventional approaches such as the Volterra-Wiener model, linear-nonlinear-cascade (LNC) model, and generalized linear model (GLM), the NMN has several advantages in terms of estimation accuracy, model interpretation, and functional connectivity analysis. We point out the limitations of current neural spike modeling methods, especially the estimation biases caused by the imbalanced class problem when the number of zeros is significantly larger than ones in the spike data. We use synthetic data to test the performance of NMN with a comparison of the traditional methods, and the results indicate the NMN approach could reduce the imbalanced class problem and achieve better predictions. Subsequently, we apply the MIMO-NMN method to analyze data from the human hippocampus. The results indicate that the MIMO-NMN method is a promising approach to modeling neural dynamics and analyzing functional connectivity of multi-neuronal data.","",""
72,"N. Sharma, C. Gregory, Marcus Johnson, W. Dixon","Closed-Loop Neural Network-Based NMES Control for Human Limb Tracking",2012,"","","","",85,"2022-07-13 09:30:38","","10.1109/TCST.2011.2125792","","",,,,,72,7.20,18,4,10,"Closed-loop control of skeletal muscle is complicated by the nonlinear muscle force to length and velocity relationships and the inherent unstructured and time-varying uncertainties in available models. Some pure feedback methods have been developed with some success, but the most promising and popular control methods for neuromuscular electrical stimulation (NMES) are neural network (NN)-based methods. Efforts in this paper focus on the use of a NN feedforward controller that is augmented with a continuous robust feedback term to yield an asymptotic result (in lieu of typical uniformly ultimately bounded stability). Specifically, an NN-based controller and Lyapunov-based stability analysis are provided to enable semi-global asymptotic tracking of a desired limb time-varying trajectory (i.e., non-isometric contractions). The developed controller is applied as an amplitude modulated voltage to external electrodes attached to the distal-medial and proximal-lateral portion of the quadriceps femoris muscle group in non-impaired volunteers. The added value of incorporating a NN feedforward term is illustrated through experiments that compare the developed controller with and without the NN feedforward component.","",""
160,"Pengfei Zhang, Cuiling Lan, Junliang Xing, Wenjun Zeng, Jianru Xue, Nanning Zheng","View Adaptive Neural Networks for High Performance Skeleton-Based Human Action Recognition",2018,"","","","",86,"2022-07-13 09:30:38","","10.1109/TPAMI.2019.2896631","","",,,,,160,40.00,27,6,4,"Skeleton-based human action recognition has recently attracted increasing attention thanks to the accessibility and the popularity of 3D skeleton data. One of the key challenges in action recognition lies in the large variations of action representations when they are captured from different viewpoints. In order to alleviate the effects of view variations, this paper introduces a novel view adaptation scheme, which automatically determines the virtual observation viewpoints over the course of an action in a learning based data driven manner. Instead of re-positioning the skeletons using a fixed human-defined prior criterion, we design two view adaptive neural networks, i.e., VA-RNN and VA-CNN, which are respectively built based on the recurrent neural network (RNN) with the Long Short-term Memory (LSTM) and the convolutional neural network (CNN). For each network, a novel view adaptation module learns and determines the most suitable observation viewpoints, and transforms the skeletons to those viewpoints for the end-to-end recognition with a main classification network. Ablation studies find that the proposed view adaptive models are capable of transforming the skeletons of various views to much more consistent virtual viewpoints. Therefore, the models largely eliminate the influence of the viewpoints, enabling the networks to focus on the learning of action-specific features and thus resulting in superior performance. In addition, we design a two-stream scheme (referred to as VA-fusion) that fuses the scores of the two networks to provide the final prediction, obtaining enhanced performance. Moreover, random rotation of skeleton sequences is employed to improve the robustness of view adaptation models and alleviate overfitting during training. Extensive experimental evaluations on five challenging benchmarks demonstrate the effectiveness of the proposed view-adaptive networks and superior performance over state-of-the-art approaches.","",""
79,"Sang-Ki Ko, Chang Jo Kim, Hyedong Jung, C. Cho","Neural Sign Language Translation based on Human Keypoint Estimation",2018,"","","","",87,"2022-07-13 09:30:38","","10.3390/APP9132683","","",,,,,79,19.75,20,4,4,"We propose a sign language translation system based on human keypoint estimation. It is well-known that many problems in the field of computer vision require a massive dataset to train deep neural network models. The situation is even worse when it comes to the sign language translation problem as it is far more difficult to collect high-quality training data. In this paper, we introduce the KETI (Korea Electronics Technology Institute) sign language dataset, which consists of 14,672 videos of high resolution and quality. Considering the fact that each country has a different and unique sign language, the KETI sign language dataset can be the starting point for further research on the Korean sign language translation. Using the KETI sign language dataset, we develop a neural network model for translating sign videos into natural language sentences by utilizing the human keypoints extracted from the face, hands, and body parts. The obtained human keypoint vector is normalized by the mean and standard deviation of the keypoints and used as input to our translation model based on the sequence-to-sequence architecture. As a result, we show that our approach is robust even when the size of the training data is not sufficient. Our translation model achieved 93.28% (55.28%, respectively) translation accuracy on the validation set (test set, respectively) for 105 sentences that can be used in emergency situations. We compared several types of our neural sign translation models based on different attention mechanisms in terms of classical metrics for measuring the translation performance.","",""
494,"Robert Geirhos, J. Jacobsen, Claudio Michaelis, R. Zemel, Wieland Brendel, M. Bethge, Felix Wichmann","Shortcut Learning in Deep Neural Networks",2020,"","","","",88,"2022-07-13 09:30:38","","10.1038/s42256-020-00257-z","","",,,,,494,247.00,71,7,2,"","",""
34,"Chaolong Li, Zhen Cui, Wenming Zheng, Chunyan Xu, R. Ji, Jian Yang","Action-Attending Graphic Neural Network",2017,"","","","",89,"2022-07-13 09:30:38","","10.1109/TIP.2018.2815744","","",,,,,34,6.80,6,6,5,"The motion analysis of human skeletons is crucial for human action recognition, which is one of the most active topics in computer vision. In this paper, we propose a fully end-to-end action-attending graphic neural network (A2GNN) for skeleton-based action recognition, in which each irregular skeleton is structured as an undirected attribute graph. To extract high-level semantic representation from skeletons, we perform the local spectral graph filtering on the constructed attribute graphs like the standard image convolution operation. Considering not all joints are informative for action analysis, we design an action-attending layer to detect those salient action units by adaptively weighting skeletal joints. Herein, the filtering responses are parameterized into a weighting function irrelevant to the order of input nodes. To further encode continuous motion variations, the deep features learnt from skeletal graphs are gathered along consecutive temporal slices and then fed into a recurrent gated network. Finally, the spectral graph filtering, action-attending, and recurrent temporal encoding are integrated together to jointly train for the sake of robust action recognition as well as the intelligibility of human actions. To evaluate our A2GNN, we conduct extensive experiments on four benchmark skeleton-based action datasets, including the large-scale challenging NTU RGB+D dataset. The experimental results demonstrate that our network achieves the state-of-the-art performances.","",""
54,"N. Ahmadi, G. Akbarizadeh","Hybrid robust iris recognition approach using iris image pre-processing, two-dimensional gabor features and multi-layer perceptron neural network/PSO",2017,"","","","",90,"2022-07-13 09:30:38","","10.1049/iet-bmt.2017.0041","","",,,,,54,10.80,27,2,5,"Computational intelligence is employed to solve factual and complicated global problems, though neural networks (NNs) and evolutionary computing have also affected these issues. Biometric traits are applicable for detecting crime in security systems because they offer attractive features such as stability and uniqueness. Although various methods have been proposed for this objective, feature shortcomings such as computational complexity, long run times, and high memory consumption remain. The current study proposes a novel human iris recognition approach based on a multi-layer perceptron NN and particle swarm optimisation (PSO) algorithms to train the network in order to increase generalisation performance. A combination of these algorithms was used as a classifier. A pre-processing step was performed on the iris images to improve the results and two-dimensional gabor kernel feature extraction was applied. The data was normalised, trained, and tested using the proposed method. A PSO algorithm was applied to train the NN for data classification. The experimental results show that the proposed method performs better than many other well-known techniques. The benchmark Chinese Academy of Science and Institute of Automation (CASIA)-iris V3 and Center for Machine Learning and Intelligent Systems at the University of California, Irvine (UCI) machine learning repository datasets were used for testing and comparison.","",""
32,"Yuxuan Yang, Zhongke Gao, Xinmin Wang, Yanli Li, J. Han, N. Marwan, J. Kurths","A recurrence quantification analysis-based channel-frequency convolutional neural network for emotion recognition from EEG.",2018,"","","","",91,"2022-07-13 09:30:38","","10.1063/1.5023857","","",,,,,32,8.00,5,7,4,"Constructing a reliable and stable emotion recognition system is a critical but challenging issue for realizing an intelligent human-machine interaction. In this study, we contribute a novel channel-frequency convolutional neural network (CFCNN), combined with recurrence quantification analysis (RQA), for the robust recognition of electroencephalogram (EEG) signals collected from different emotion states. We employ movie clips as the stimuli to induce happiness, sadness, and fear emotions and simultaneously measure the corresponding EEG signals. Then the entropy measures, obtained from the RQA operation on EEG signals of different frequency bands, are fed into the novel CFCNN. The results indicate that our system can provide a high emotion recognition accuracy of 92.24% and a relatively excellent stability as well as a satisfactory Kappa value of 0.884, rendering our system particularly useful for the emotion recognition task. Meanwhile, we compare the performance of the entropy measures, extracted from each frequency band, in distinguishing the three emotion states. We mainly find that emotional features extracted from the gamma band present a considerably higher classification accuracy of 90.51% and a Kappa value of 0.858, proving the high relation between emotional process and gamma frequency band.","",""
46,"Wen Qi, Hang Su, Chenguang Yang, G. Ferrigno, E. Momi, A. Aliverti","A Fast and Robust Deep Convolutional Neural Networks for Complex Human Activity Recognition Using Smartphone",2019,"","","","",92,"2022-07-13 09:30:38","","10.3390/s19173731","","",,,,,46,15.33,8,6,3,"As a significant role in healthcare and sports applications, human activity recognition (HAR) techniques are capable of monitoring humans’ daily behavior. It has spurred the demand for intelligent sensors and has been giving rise to the explosive growth of wearable and mobile devices. They provide the most availability of human activity data (big data). Powerful algorithms are required to analyze these heterogeneous and high-dimension streaming data efficiently. This paper proposes a novel fast and robust deep convolutional neural network structure (FR-DCNN) for human activity recognition (HAR) using a smartphone. It enhances the effectiveness and extends the information of the collected raw data from the inertial measurement unit (IMU) sensors by integrating a series of signal processing algorithms and a signal selection module. It enables a fast computational method for building the DCNN classifier by adding a data compression module. Experimental results on the sampled 12 complex activities dataset show that the proposed FR-DCNN model is the best method for fast computation and high accuracy recognition. The FR-DCNN model only needs 0.0029 s to predict activity in an online way with 95.27% accuracy. Meanwhile, it only takes 88 s (average) to establish the DCNN classifier on the compressed dataset with less precision loss 94.18%.","",""
22,"Matej Ulicny, J. Lundström, S. Byttner","Robustness of Deep Convolutional Neural Networks for Image Recognition",2016,"","","","",93,"2022-07-13 09:30:38","","10.1007/978-3-319-30447-2_2","","",,,,,22,3.67,7,3,6,"","",""
74,"Hong Liu, Juanhui Tu, Mengyuan Liu","Two-Stream 3D Convolutional Neural Network for Skeleton-Based Action Recognition",2017,"","","","",94,"2022-07-13 09:30:38","","","","",,,,,74,14.80,25,3,5,"It remains a challenge to efficiently extract spatialtemporal information from skeleton sequences for 3D human action recognition. Although most recent action recognition methods are based on Recurrent Neural Networks which present outstanding performance, one of the shortcomings of these methods is the tendency to overemphasize the temporal information. Since 3D convolutional neural network(3D CNN) is a powerful tool to simultaneously learn features from both spatial and temporal dimensions through capturing the correlations between three dimensional signals, this paper proposes a novel two-stream model using 3D CNN. To our best knowledge, this is the first application of 3D CNN in skeleton-based action recognition. Our method consists of three stages. First, skeleton joints are mapped into a 3D coordinate space and then encoding the spatial and temporal information, respectively. Second, 3D CNN models are seperately adopted to extract deep features from two streams. Third, to enhance the ability of deep features to capture global relationships, we extend every stream into multitemporal version. Extensive experiments on the SmartHome dataset and the large-scale NTU RGB-D dataset demonstrate that our method outperforms most of RNN-based methods, which verify the complementary property between spatial and temporal information and the robustness to noise.","",""
67,"Keisuke Sakaguchi, Kevin Duh, Matt Post, Benjamin Van Durme","Robsut Wrod Reocginiton via Semi-Character Recurrent Neural Network",2016,"","","","",95,"2022-07-13 09:30:38","","10.1609/aaai.v31i1.10970","","",,,,,67,11.17,17,4,6,"    Language processing mechanism by humans is generally more robust than computers. The Cmabrigde Uinervtisy (Cambridge University) effect from the psycholinguistics literature has demonstrated such a robust word processing mechanism, where jumbled words (e.g. Cmabrigde / Cambridge) are recognized with little cost. On the other hand, computational models for word recognition (e.g. spelling checkers) perform poorly on data with such noise. Inspired by the findings from the Cmabrigde Uinervtisy effect, we propose a word recognition model based on a semi-character level recurrent neural network (scRNN). In our experiments, we demonstrate that scRNN has significantly more robust performance in word spelling correction (i.e. word recognition) compared to existing spelling checkers and character-based convolutional neural network. Furthermore, we demonstrate that the model is cognitively plausible by replicating a psycholinguistics experiment about human reading difficulty using our model.   ","",""
1268,"Robert Geirhos, Patricia Rubisch, Claudio Michaelis, M. Bethge, Felix Wichmann, Wieland Brendel","ImageNet-trained CNNs are biased towards texture; increasing shape bias improves accuracy and robustness",2018,"","","","",96,"2022-07-13 09:30:38","","","","",,,,,1268,317.00,211,6,4,"Convolutional Neural Networks (CNNs) are commonly thought to recognise objects by learning increasingly complex representations of object shapes. Some recent studies suggest a more important role of image textures. We here put these conflicting hypotheses to a quantitative test by evaluating CNNs and human observers on images with a texture-shape cue conflict. We show that ImageNet-trained CNNs are strongly biased towards recognising textures rather than shapes, which is in stark contrast to human behavioural evidence and reveals fundamentally different classification strategies. We then demonstrate that the same standard architecture (ResNet-50) that learns a texture-based representation on ImageNet is able to learn a shape-based representation instead when trained on ""Stylized-ImageNet"", a stylized version of ImageNet. This provides a much better fit for human behavioural performance in our well-controlled psychophysical lab setting (nine experiments totalling 48,560 psychophysical trials across 97 observers) and comes with a number of unexpected emergent benefits such as improved object detection performance and previously unseen robustness towards a wide range of image distortions, highlighting advantages of a shape-based representation.","",""
2,"Simone Dari, Nikolay Kadrileev, E. Hüllermeier","A Neural Network-Based Driver Gaze Classification System with Vehicle Signals",2020,"","","","",97,"2022-07-13 09:30:38","","10.1109/IJCNN48605.2020.9207709","","",,,,,2,1.00,1,3,2,"Driver monitoring can play an essential part in avoiding accidents by warning the driver and shifting the driver’s attention to the traffic scenery in time during critical situations. This may apply for the different levels of automated driving, for take-over requests as well as for driving in manual mode. A great proxy for this purpose has always been the driver’s gazing direction. The aim of this work is to introduce a robust gaze detection system. In this regard, we make several contributions that are novel in the area of gaze detection systems. In particular, we propose a deep learning approach to predict gaze regions, which is based on informative features such as eye landmarks and head pose angles of the driver. Moreover, we introduce different post-processing techniques that improve the accuracy by exploiting temporal information from videos and the availability of other vehicle signals. Last but not least, we confirm our method with a leave-one-driver-out cross-validation. Unlike previous studies, we do not use gazes to predict maneuver changes, but we consider the human-computer-interaction aspect and use vehicle signals to improve the performance of the estimation. The proposed system is able to achieve an accuracy of 92.3% outperforming earlier landmark-based gaze estimators.","",""
16,"Nian Chi Tay, T. Connie, T. Ong, K. Goh, Pin Shen Teh","A Robust Abnormal Behavior Detection Method Using Convolutional Neural Network",2018,"","","","",98,"2022-07-13 09:30:38","","10.1007/978-981-13-2622-6_4","","",,,,,16,4.00,3,5,4,"","",""
61,"Da Sun, F. Naghdy, H. Du","Neural Network-Based Passivity Control of Teleoperation System Under Time-Varying Delays",2017,"","","","",99,"2022-07-13 09:30:38","","10.1109/TCYB.2016.2554630","","",,,,,61,12.20,20,3,5,"In this paper, a novel neural network (NN)-based four-channel wave-based time domain passivity approach (TDPA) is proposed for a teleoperation system with time-varying delays. The designed wave-based TDPA aims to robustly guarantee the channels passivity and provide higher transparency than the previous power-based TDPA. The applied NN is used to estimate and eliminate the system’s dynamic uncertainties. The system stability with linearity assumption on human and environment has been analyzed using Lyapunov method. The proposed algorithm is validated through experimental work based on a 3-DOF bilateral teleoperation platform in the presence of different time delays.","",""
275,"Gagandeep Singh, Timon Gehr, M. Mirman, Markus Püschel, Martin T. Vechev","Fast and Effective Robustness Certification",2018,"","","","",100,"2022-07-13 09:30:38","","","","",,,,,275,68.75,55,5,4,"We present a new method and system, called DeepZ, for certifying neural network robustness based on abstract interpretation. Compared to state-of-the-art automated verifiers for neural networks, DeepZ: (i) handles ReLU, Tanh and Sigmoid activation functions, (ii) supports feedforward and convolutional architectures, (iii) is significantly more scalable and precise, and (iv) and is sound with respect to floating point arithmetic. These benefits are due to carefully designed approximations tailored to the setting of neural networks. As an example, DeepZ achieves a verification accuracy of 97% on a large network with 88,500 hidden units under $L_{\infty}$ attack with $\epsilon = 0.1$ with an average runtime of 133 seconds.","",""
29,"Boning Li, A. Sano","Extraction and Interpretation of Deep Autoencoder-based Temporal Features from Wearables for Forecasting Personalized Mood, Health, and Stress",2020,"","","","",101,"2022-07-13 09:30:38","","10.1145/3397318","","",,,,,29,14.50,15,2,2,"Continuous wearable sensor data in high resolution contain physiological and behavioral information that can be utilized to predict human health and wellbeing, establishing the foundation for developing early warning systems to eventually improve human health and wellbeing. We propose a deep neural network framework, the Locally Connected Long Short-Term Memory Denoising AutoEncoder (LC-LSTM-DAE), to automatically extract features from passively collected raw sensor data and perform personalized prediction of self-reported mood, health, and stress scores with high precision. We enabled personalized learning of features by finetuning the general representation model with participant-specific data. The framework was evaluated using wearable sensor data and wellbeing labels collected from college students (total 6391 days from N=239). Sensor data include skin temperature, skin conductance, and acceleration; wellbeing labels include self-reported mood, health and stress scored 0 - 100. Compared to the prediction performance based on hand-crafted features, the proposed framework achieved higher precision with a smaller number of features. We also provide statistical interpretation and visual explanation to the automatically learned features and the prediction models. Our results show the possibility of predicting self-reported mood, health, and stress accurately using an interpretable deep learning framework, ultimately for developing real-time health and wellbeing monitoring and intervention systems that can benefit various populations.","",""
1,"R. A. Gonzales, Qiang Zhang, B. Papież, K. Werys, E. Lukaschuk, Iulia A. Popescu, M. Burrage, M. Shanmuganathan, V. Ferreira, S. Piechnik","MOCOnet: Robust Motion Correction of Cardiovascular Magnetic Resonance T1 Mapping Using Convolutional Neural Networks",2021,"","","","",102,"2022-07-13 09:30:38","","10.3389/fcvm.2021.768245","","",,,,,1,1.00,0,10,1,"Background: Quantitative cardiovascular magnetic resonance (CMR) T1 mapping has shown promise for advanced tissue characterisation in routine clinical practise. However, T1 mapping is prone to motion artefacts, which affects its robustness and clinical interpretation. Current methods for motion correction on T1 mapping are model-driven with no guarantee on generalisability, limiting its widespread use. In contrast, emerging data-driven deep learning approaches have shown good performance in general image registration tasks. We propose MOCOnet, a convolutional neural network solution, for generalisable motion artefact correction in T1 maps. Methods: The network architecture employs U-Net for producing distance vector fields and utilises warping layers to apply deformation to the feature maps in a coarse-to-fine manner. Using the UK Biobank imaging dataset scanned at 1.5T, MOCOnet was trained on 1,536 mid-ventricular T1 maps (acquired using the ShMOLLI method) with motion artefacts, generated by a customised deformation procedure, and tested on a different set of 200 samples with a diverse range of motion. MOCOnet was compared to a well-validated baseline multi-modal image registration method. Motion reduction was visually assessed by 3 human experts, with motion scores ranging from 0% (strictly no motion) to 100% (very severe motion). Results: MOCOnet achieved fast image registration (<1 second per T1 map) and successfully suppressed a wide range of motion artefacts. MOCOnet significantly reduced motion scores from 37.1±21.5 to 13.3±10.5 (p < 0.001), whereas the baseline method reduced it to 15.8±15.6 (p < 0.001). MOCOnet was significantly better than the baseline method in suppressing motion artefacts and more consistently (p = 0.007). Conclusion: MOCOnet demonstrated significantly better motion correction performance compared to a traditional image registration approach. Salvaging data affected by motion with robustness and in a time-efficient manner may enable better image quality and reliable images for immediate clinical interpretation.","",""
4,"Stephan Günnemann","Graph Neural Networks: Adversarial Robustness",2022,"","","","",103,"2022-07-13 09:30:38","","10.1007/978-981-16-6054-2_8","","",,,,,4,4.00,4,1,1,"","",""
31,"Hongyi Xu, Thiemo Alldieck, C. Sminchisescu","H-NeRF: Neural Radiance Fields for Rendering and Temporal Reconstruction of Humans in Motion",2021,"","","","",104,"2022-07-13 09:30:38","","","","",,,,,31,31.00,10,3,1,"We present neural radiance ﬁelds for rendering and temporal (4D) reconstruction of humans in motion (H-NeRF), as captured by a sparse set of cameras or even from a monocular video. Our approach combines ideas from neural scene representation, novel-view synthesis, and implicit statistical geometric human representations, coupled using novel loss functions. Instead of learning a radiance ﬁeld with a uniform occupancy prior, we constrain it by a structured implicit human body model, represented using signed distance functions. This allows us to robustly fuse information from sparse views and generalize well beyond the poses or views observed in training. Moreover, we apply geometric constraints to co-learn the structure of the observed subject – including both body and clothing – and to regularize the radiance ﬁeld to geometrically plausible solutions. Extensive experiments on multiple datasets demonstrate the robustness and the accuracy of our approach, its generalization capabilities signiﬁcantly outside a small training set of poses and views, and statistical extrapolation beyond the observed shape. ﬁeld constrained by an imGHUM-based SDF, which signiﬁcantly improves robustness under sparse training camera views. In ﬁg. 2, we show reconstruction of a static RenderPeople and GHS3D scan, respectively, using the original NeRF [29], the state-of-the-art multi-view reconstruction approach IDR [53], and H-NeRF, for increasing number of cameras. Both novel view image quality and geometric accuracy improve for all methods as the number of training views increases. However, in contrast to competitors, H-NeRF produces good quality output even under sparse training views (2-8), demonstrating the generalization capability of a co-training approach. a compatible implicit statistical 3D human pose and shape model, represented using signed distance functions. Our implicit geometric formulation captures not just the statistical regularities of the human body, but also hair and clothing represented as an implicit residual network. Our model is trained end-to-end based on several novel losses, and achieves good results for both 3D reconstruction and photorealistic rendering. Training the body model in an end-to-end rendering framework carries the promise to learn complex implicit skinning functions based on images only, a performance previously possible only in the realm of human capture using complex and expensive 3D body scanners, in the laboratory. We illustrate the favorable capabilities of H-NeRF through extensive experimentation using several datasets and against other state of the art techniques.","",""
65,"Joel Dapello, Tiago Marques, Martin Schrimpf, Franziska Geiger, D. Cox, J. DiCarlo","Simulating a Primary Visual Cortex at the Front of CNNs Improves Robustness to Image Perturbations",2020,"","","","",105,"2022-07-13 09:30:38","","10.1101/2020.06.16.154542","","",,,,,65,32.50,11,6,2,"Current state-of-the-art object recognition models are largely based on convolutional neural network (CNN) architectures, which are loosely inspired by the primate visual system. However, these CNNs can be fooled by imperceptibly small, explicitly crafted perturbations, and struggle to recognize objects in corrupted images that are easily recognized by humans. Here, by making comparisons with primate neural data, we first observed that CNN models with a neural hidden layer that better matches primate primary visual cortex (V1) are also more robust to adversarial attacks. Inspired by this observation, we developed VOneNets, a new class of hybrid CNN vision models. Each VOneNet contains a fixed weight neural network front-end that simulates primate V1, called the VOneBlock, followed by a neural network back-end adapted from current CNN vision models. The VOneBlock is based on a classical neuroscientific model of V1: the linear-nonlinear-Poisson model, consisting of a biologically-constrained Gabor filter bank, simple and complex cell nonlinearities, and a V1 neuronal stochasticity generator. After training, VOneNets retain high ImageNet performance, but each is substantially more robust, outperforming the base CNNs and state-of-the-art methods by 18% and 3%, respectively, on a conglomerate benchmark of perturbations comprised of white box adversarial attacks and common image corruptions. Finally, we show that all components of the VOneBlock work in synergy to improve robustness. While current CNN architectures are arguably brain-inspired, the results presented here demonstrate that more precisely mimicking just one stage of the primate visual system leads to new gains in ImageNet-level computer vision applications.","",""
8,"Yangyang Xia, R. Stern","A Priori SNR Estimation Based on a Recurrent Neural Network for Robust Speech Enhancement",2018,"","","","",106,"2022-07-13 09:30:38","","10.21437/Interspeech.2018-2423","","",,,,,8,2.00,4,2,4,"Speech enhancement under highly non-stationary noise conditions remains a challenging problem. Classical methods typically attempt to identify a frequency-domain optimal gain function that suppresses noise in noisy speech. These algorithms typically produce artifacts such as “musical noise” that are detrimental to machine and human understanding, largely due to inaccurate estimation of noise power spectra. The optimal gain function is commonly referred to as the ideal ratio mask (IRM) in neural-network-based systems, and the goal becomes estimation of the IRM from the short-time Fourier transform amplitude of degraded speech. While these data-driven techniques are able to enhance speech quality with reduced artifacts, they are frequently not robust to types of noise that they had not been exposed to in the training process. In this paper, we propose a novel recurrent neural network (RNN) that bridges the gap between classical and neural-network-based methods. By reformulating the classical decision-directed approach, the a priori and a posteriori SNRs become latent variables in the RNN, from which the frequency-dependent estimated likelihood of speech presence is used to update recursively the latent variables. The proposed method provides substantial enhancement of speech quality and objective accuracy in machine interpretation of speech.","",""
0,"D. Ebuna, J. Kluesner, K. Cunningham, J. H. Edwards","Statistical approach to neural network imaging of karst systems in 3D seismic reflection data",2018,"","","","",107,"2022-07-13 09:30:38","","10.1190/INT-2017-0197.1","","",,,,,0,0.00,0,4,4,"The current lack of a robust standardized technique for geophysical mapping of karst systems can be attributed to the complexity of the environment and prior technological limitations. Abrupt lateral variations in physical properties that are inherent to karst systems generate significant geophysical noise, challenging conventional seismic signal processing and interpretation. The application of neural networks (NNs) to multiattribute seismic interpretation can provide a semiautomated method for identifying and leveraging the nonlinear relationships exhibited among seismic attributes. The ambiguity generally associated with designing NNs for seismic object detection can be reduced via statistical analysis of the extracted attribute data. A data-driven approach to selecting the appropriate set of input seismic attributes, as well as the locations and suggested number of training examples, provides a more objective and computationally efficient method for identifying karst systems using reflection seismology. This statistically optimized NN technique is demonstrated using 3D seismic reflection data collected from the southeastern portion of the Florida carbonate platform. Several dimensionality reduction methods are applied, and the resulting karst probability models are evaluated relative to one another based on quantitative and qualitative criteria. Comparing the preferred model, using quadratic discriminant analysis, with previously available seismic object detection workflows demonstrates the karst-specific nature of the tool. Results suggest that the karst multiattribute workflow presented is capable of approximating the structural boundaries of karst systems with more accuracy and efficiency than a human counterpart or previously presented seismic interpretation schemes. This objective technique, using solely 3D seismic reflection data, is proposed as a practical approach to mapping karst systems for subsequent hydrogeologic modeling.","",""
2,"Milla S. A. Ferro, Bruno José Torres Fernandes, C. J. A. B. Filho","Non-negative Structured Pyramidal Neural Network for Pattern Recognition",2018,"","","","",108,"2022-07-13 09:30:38","","10.1109/IJCNN.2018.8489216","","",,,,,2,0.50,1,3,4,"Deep learning is a machine learning paradigm that has been widely exploited in the last years due to its high performance in many different problems, most of them related to computer vision. The Structured Pyramidal Neural Network (SPNN) is an artificial neural network which implements some of the deep learning concepts, such as multiple processing layers and receptive fields, with no need to pre-define the features of the problem as inputs. This network architecture has presented equivalent or even better results than other deep network approaches applied to solve specific tasks, but with a much lower computational cost. However, one of the SPNN limitations is the difficulty in contributing to human interpretations, since it has opaque learning, like most of the neural networks. Thus, we propose a non-negative model of the SPNN, to obtain better interpretability of the network learning. We restrict the values of the weights and biases of the network to be non-negative. The proposed model is evaluated in a gender recognition problem using the Face Recognition Technology (FERET) database. The results show that SPNN including non-negative constraint returns comparable recognition rates, but providing gains in the interpretability and stability of the model.","",""
0,"J. Jopling, Brian C. Pridgen, S. Yeung","Deep Convolutional Neural Networks as a Diagnostic Aid-A Step Toward Minimizing Undetected Scaphoid Fractures on Initial Hand Radiographs.",2021,"","","","",109,"2022-07-13 09:30:38","","10.1001/jamanetworkopen.2021.6393","","",,,,,0,0.00,0,3,1,"Scaphoid fractures are the most common carpal bone fracture. If missed and untreated at initial evaluation, they can lead to a progressive pattern of debilitating wrist arthritis that may ultimately require salvage procedures, including wrist fusion. The concern over missing scaphoid fractures that areradiographicallyoccultoninitialplainradiographshasbeenaninspirationformanypriorstudies. 1 However, overtreating scaphoid fractures because of these concerns can lead to costly advanced imaging or unnecessary immobilization in a cast or splint. It is this challenge that Yoon et al 2 addressed in their study. Advances in machine learning and computer vision have allowed for an increasing number of investigations into the potential of computer vision to augment human interpretation of radiographs. The work by Yoon et al 2 adds to this growing body of knowledge. They trained a deep convolutional neural network model that can successfully identify scaphoid fractures in plain radiographs. To highlight the potential future clinical impact,theychosetofocustheiralgorithmnotonlyonradiographicallyapparentscaphoidfractures but specifically on detecting radiographically occult scaphoid fractures, ie, those missed by human interpretation. For radiographs run through their entire algorithm pipeline, 20 of 22 radiographically occult fractures were detected. These results indicate the potential of computer vision algorithms to eventually become a clinically meaningful tool for assessing possible scaphoid fractures in initial radiographs. In a fully realized form, they could facilitate prompt identification of these fractures, expeditious treatment, and decreased reliance on costly advanced imaging. By looking at the details of the study, we can see where additional algorithm development can make contributions toward furthering this goal. The underlying rate of occult scaphoid fractures was","",""
29,"N. Amoroso, R. Errico, S. Bruno, A. Chincarini, E. Garuccio, F. Sensi, S. Tangaro, A. Tateo, R. Bellotti","Hippocampal unified multi-atlas network (HUMAN): protocol and scale validation of a novel segmentation tool.",2015,"","","","",110,"2022-07-13 09:30:38","","10.1088/0031-9155/60/22/8851","","",,,,,29,4.14,3,9,7,"In this study we present a novel fully automated Hippocampal Unified Multi-Atlas-Networks (HUMAN) algorithm for the segmentation of the hippocampus in structural magnetic resonance imaging. In multi-atlas approaches atlas selection is of crucial importance for the accuracy of the segmentation. Here we present an optimized method based on the definition of a small peri-hippocampal region to target the atlas learning with linear and non-linear embedded manifolds. All atlases were co-registered to a data driven template resulting in a computationally efficient method that requires only one test registration. The optimal atlases identified were used to train dedicated artificial neural networks whose labels were then propagated and fused to obtain the final segmentation. To quantify data heterogeneity and protocol inherent effects, HUMAN was tested on two independent data sets provided by the Alzheimer's Disease Neuroimaging Initiative and the Open Access Series of Imaging Studies. HUMAN is accurate and achieves state-of-the-art performance (Dice[Formula: see text] and Dice[Formula: see text]). It is also a robust method that remains stable when applied to the whole hippocampus or to sub-regions (patches). HUMAN also compares favorably with a basic multi-atlas approach and a benchmark segmentation tool such as FreeSurfer.","",""
40,"Bo Li, K. Sim","Improving robustness of deep neural networks via spectral masking for automatic speech recognition",2013,"","","","",111,"2022-07-13 09:30:38","","10.1109/ASRU.2013.6707743","","",,,,,40,4.44,20,2,9,"The performance of human listeners degrades rather slowly compared to machines in noisy environments. This has been attributed to the ability of performing auditory scene analysis which separates the speech prior to recognition. In this work, we investigate two mask estimation approaches, namely the state dependent and the deep neural network (DNN) based estimations, to separate speech from noises for improving DNN acoustic models' noise robustness. The second approach has been experimentally shown to outperform the first one. Due to the stereo data based training and ill-defined masks for speech with channel distortions, both methods do not generalize well to unseen conditions and fail to beat the performance of the multi-style trained baseline system. However, the model trained on masked features demonstrates strong complementariness to the baseline model. The simple average of the two system's posteriors yields word error rates of 4.4% on Aurora2 and 12.3% on Aurora4.","",""
3,"Linhai Ma, Liang Liang","Enhance CNN Robustness Against Noises for Classification of 12-Lead ECG with Variable Length",2020,"","","","",112,"2022-07-13 09:30:38","","10.1109/ICMLA51294.2020.00137","","",,,,,3,1.50,2,2,2,"Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor the condition of the cardiovascular system. Deep neural networks (DNNs), have been developed in many research labs for automatic interpretation of ECG signals to identify potential abnormalities in patient hearts. Studies have shown that given a sufficiently large amount of data, the classification accuracy of DNNs could reach human-expert cardiologist level. However, despite of the excellent performance in classification accuracy, it has been shown that DNNs are highly vulnerable to adversarial noises which are subtle changes in input of a DNN and lead to a wrong class-label prediction with a high confidence. Thus, it is challenging and essential to improve robustness of DNNs against adversarial noises for ECG signal classification -a life-critical application. In this work, we designed a CNN for classification of 12-lead ECG signals with variable length, and we applied three defense methods to improve robustness of this CNN for this classification task. The ECG data in this study is very challenging because the sample size is limited, and the length of each ECG recording varies in a large range. The evaluation results show that our customized CNN reached satisfying F1 score and average accuracy, comparable to the top-6 entries in the CPSC2018 ECG classification challenge, and the defense methods enhanced robustness of our CNN against adversarial noises and white noises, with a minimal reduction in accuracy on clean data.","",""
3,"Linhai Ma, Liang Liang","Improve robustness of DNN for ECG signal classification: a noise-to-signal ratio perspective",2020,"","","","",113,"2022-07-13 09:30:38","","","","",,,,,3,1.50,2,2,2,"Electrocardiogram (ECG) is the most widely used diagnostic tool to monitor the condition of the cardiovascular system. Deep neural networks (DNNs), have been developed in many research labs for automatic interpretation of ECG signals to identify potential abnormalities in patient hearts. Studies have shown that given a sufficiently large amount of data, the classification accuracy of DNNs could reach human-expert cardiologist level. A DNN-based automated ECG diagnostic system would be an affordable solution for patients in developing countries where human-expert cardiologist are lacking. However, despite of the excellent performance in classification accuracy, it has been shown that DNNs are highly vulnerable to adversarial attacks: subtle changes in input of a DNN can lead to a wrong classification output with high confidence. Thus, it is challenging and essential to improve adversarial robustness of DNNs for ECG signal classification, a life-critical application. In this work, we proposed to improve DNN robustness from the perspective of noise-to-signal ratio (NSR) and developed two methods to minimize NSR during training process. We evaluated the proposed methods on PhysionNets MIT-BIH dataset, and the results show that our proposed methods lead to an enhancement in robustness against PGD adversarial attack and SPSA attack, with a minimal change in accuracy on clean data.","",""
23,"Lei Zhao, Zengcai Wang, Guoxin Zhang, Yazhou Qi, Xiaojing Wang","Eye state recognition based on deep integrated neural network and transfer learning",2018,"","","","",114,"2022-07-13 09:30:38","","10.1007/s11042-017-5380-8","","",,,,,23,5.75,5,5,4,"","",""
31,"Benoît Delachaux, Julien Rebetez, A. Pérez-Uribe, Héctor Fabio Satizábal Mejia","Indoor Activity Recognition by Combining One-vs.-All Neural Network Classifiers Exploiting Wearable and Depth Sensors",2013,"","","","",115,"2022-07-13 09:30:38","","10.1007/978-3-642-38682-4_25","","",,,,,31,3.44,8,4,9,"","",""
98,"Yuzhe Yang, Guo Zhang, D. Katabi, Zhi Xu","ME-Net: Towards Effective Adversarial Robustness with Matrix Estimation",2019,"","","","",116,"2022-07-13 09:30:38","","","","",,,,,98,32.67,25,4,3,"Deep neural networks are vulnerable to adversarial attacks. The literature is rich with algorithms that can easily craft successful adversarial examples. In contrast, the performance of defense techniques still lags behind. This paper proposes ME-Net, a defense method that leverages matrix estimation (ME). In ME-Net, images are preprocessed using two steps: first pixels are randomly dropped from the image; then, the image is reconstructed using ME. We show that this process destroys the adversarial structure of the noise, while re-enforcing the global structure in the original image. Since humans typically rely on such global structures in classifying images, the process makes the network mode compatible with human perception. We conduct comprehensive experiments on prevailing benchmarks such as MNIST, CIFAR-10, SVHN, and Tiny-ImageNet. Comparing ME-Net with state-of-the-art defense mechanisms shows that ME-Net consistently outperforms prior techniques, improving robustness against both black-box and white-box attacks.","",""
393,"A. Ross, Finale Doshi-Velez","Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients",2017,"","","","",117,"2022-07-13 09:30:38","","10.1609/aaai.v32i1.11504","","",,,,,393,78.60,197,2,5,"    Deep neural networks have proven remarkably effective at solving many classification problems, but have been criticized recently for two major weaknesses: the reasons behind their predictions are uninterpretable, and the predictions themselves can often be fooled by small adversarial perturbations. These problems pose major obstacles for the adoption of neural networks in domains that require security or transparency. In this work, we evaluate the effectiveness of defenses that differentiably penalize the degree to which small changes in inputs can alter model predictions. Across multiple attacks, architectures, defenses, and datasets, we find that neural networks trained with this input gradient regularization exhibit robustness to transferred adversarial examples generated to fool all of the other models. We also find that adversarial examples generated to fool gradient-regularized models fool all other models equally well, and actually lead to more ""legitimate,"" interpretable misclassifications as rated by people (which we confirm in a human subject experiment). Finally, we demonstrate that regularizing input gradients makes them more naturally interpretable as rationales for model predictions. We conclude by discussing this relationship between interpretability and robustness in deep neural networks.   ","",""
6,"Zi Wang, Aws Albarghouthi, Gautam Prakriya, S. Jha","Interval universal approximation for neural networks",2020,"","","","",118,"2022-07-13 09:30:38","","10.1145/3498675","","",,,,,6,3.00,2,4,2,"To verify safety and robustness of neural networks, researchers have successfully applied abstract interpretation, primarily using the interval abstract domain. In this paper, we study the theoretical power and limits of the interval domain for neural-network verification. First, we introduce the interval universal approximation (IUA) theorem. IUA shows that neural networks not only can approximate any continuous function f (universal approximation) as we have known for decades, but we can find a neural network, using any well-behaved activation function, whose interval bounds are an arbitrarily close approximation of the set semantics of f (the result of applying f to a set of inputs). We call this notion of approximation interval approximation. Our theorem generalizes the recent result of Baader et al. from ReLUs to a rich class of activation functions that we call squashable functions. Additionally, the IUA theorem implies that we can always construct provably robust neural networks under ℓ∞-norm using almost any practical activation function. Second, we study the computational complexity of constructing neural networks that are amenable to precise interval analysis. This is a crucial question, as our constructive proof of IUA is exponential in the size of the approximation domain. We boil this question down to the problem of approximating the range of a neural network with squashable activation functions. We show that the range approximation problem (RA) is a Δ2-intermediate problem, which is strictly harder than NP-complete problems, assuming coNP⊄NP. As a result, IUA is an inherently hard problem: No matter what abstract domain or computational tools we consider to achieve interval approximation, there is no efficient construction of such a universal approximator. This implies that it is hard to construct a provably robust network, even if we have a robust network to start with.","",""
1,"Leon C. Hardy","An application of neurohydrodynamics to a Hopfield neural network",2015,"","","","",119,"2022-07-13 09:30:38","","10.1109/IJCNN.2015.7280359","","",,,,,1,0.14,1,1,7,"In this paper, we apply our approach of Neurohydrodynamics (NHD) to a Hopfield neural network by introducing a one-dimensional spacial diffusion term. This reaction-diffusion equation includes an auxiliary equation that “guides” the weights of the network using the divergence of neuron's activation amplitude, which we call the neuropotential. This guiding principle is similar to de Broglie's “pilot wave” interpretation for Quantum Mechanics or Turing's oracle for “human intuition” of a Turing machine. Finally, using a numerical derivation of the dynamical equations of one-dimensional Hopfield neural network, we include a simulation of the network so that we can discuss its behavior and future directions of NHD.","",""
145,"Haohan Wang, Xindi Wu, Pengcheng Yin, E. Xing","High-Frequency Component Helps Explain the Generalization of Convolutional Neural Networks",2019,"","","","",120,"2022-07-13 09:30:38","","10.1109/cvpr42600.2020.00871","","",,,,,145,48.33,36,4,3,"We investigate the relationship between the frequency spectrum of image data and the generalization behavior of convolutional neural networks (CNN). We first notice CNN's ability in capturing the high-frequency components of images. These high-frequency components are almost imperceptible to a human. Thus the observation leads to multiple hypotheses that are related to the generalization behaviors of CNN, including a potential explanation for adversarial examples, a discussion of CNN's trade-off between robustness and accuracy, and some evidence in understanding training heuristics.","",""
27,"Zeke Xie, Fengxiang He, Shaopeng Fu, Issei Sato, D. Tao, M. Sugiyama","Artificial Neural Variability for Deep Learning: On Overfitting, Noise Memorization, and Catastrophic Forgetting",2020,"","","","",121,"2022-07-13 09:30:38","","10.1162/neco_a_01403","","",,,,,27,13.50,5,6,2,"Deep learning is often criticized by two serious issues that rarely exist in natural nervous systems: overfitting and catastrophic forgetting. It can even memorize randomly labeled data, which has little knowledge behind the instance-label pairs. When a deep network continually learns over time by accommodating new tasks, it usually quickly overwrites the knowledge learned from previous tasks. Referred to as the neural variability, it is well known in neuroscience that human brain reactions exhibit substantial variability even in response to the same stimulus. This mechanism balances accuracy and plasticity/flexibility in the motor learning of natural nervous systems. Thus, it motivates us to design a similar mechanism, named artificial neural variability (ANV), that helps artificial neural networks learn some advantages from “natural” neural networks. We rigorously prove that ANV plays as an implicit regularizer of the mutual information between the training data and the learned model. This result theoretically guarantees ANV a strictly improved generalizability, robustness to label noise, and robustness to catastrophic forgetting. We then devise a neural variable risk minimization (NVRM) framework and neural variable optimizers to achieve ANV for conventional network architectures in practice. The empirical studies demonstrate that NVRM can effectively relieve overfitting, label noise memorization, and catastrophic forgetting at negligible costs.","",""
1,"J. Constantin, A. Bigand, I. Constantin","Pooling spike neural network for fast rendering in global illumination",2019,"","","","",122,"2022-07-13 09:30:38","","10.1007/s00521-018-3941-z","","",,,,,1,0.33,0,3,3,"","",""
180,"Noah D. Brenowitz, C. Bretherton","Prognostic Validation of a Neural Network Unified Physics Parameterization",2018,"","","","",123,"2022-07-13 09:30:38","","10.1029/2018GL078510","","",,,,,180,45.00,90,2,4,"Weather and climate models approximate diabatic and sub‐grid‐scale processes in terms of grid‐scale variables using parameterizations. Current parameterizations are designed by humans based on physical understanding, observations, and process modeling. As a result, they are numerically efficient and interpretable, but potentially oversimplified. However, the advent of global high‐resolution simulations and observations enables a more robust approach based on machine learning. In this letter, a neural network‐based parameterization is trained using a near‐global aqua‐planet simulation with a 4‐km resolution (NG‐Aqua). The neural network predicts the apparent sources of heat and moisture averaged onto (160 km)2 grid boxes. A numerically stable scheme is obtained by minimizing the prediction error over multiple time steps rather than single one. In prognostic single‐column model tests, this scheme matches both the fluctuations and equilibrium of NG‐Aqua simulation better than the Community Atmosphere Model does.","",""
33,"S. Ashwin, S. A. Kumar, J. Ramesh, K. Gunavathi","Efficient and reliable lung nodule detection using a neural network based computer aided diagnosis system",2012,"","","","",124,"2022-07-13 09:30:38","","10.1109/ICETEEEM.2012.6494454","","",,,,,33,3.30,8,4,10,"The manual examination of histological images like computed tomography (CT) images by physicians is prone to subjectivity and limited intra and inter-surgeon reproducibility, due to its heavy reliance on human interpretation. As result of which, diagnosis of cancer especially in lungs becomes less accurate and unreliable. So, a computer-aided diagnosis (CAD) system, based on artificial intelligence that efficiently detects nodules of any shape and size, is used for diagnosis without human intervention. In this work, we have developed a two stage CAD system in which the first stage involves pre-processing applied for a better quality image to enable higher success rate on detection following which the cancerous nodule region is segmented. The second stage involves artificial neural network (ANN) architecture which is trained using a modified BFGS algorithm. The proposed system was trained, tested, and evaluated specifically on the problem of detecting lung cancer nodules found on CT images to give a positive detection. A significant comparative analysis was done between the proposed method and several existing CAD systems used for lung nodule diagnosis and the proposed method using training-based neural networks prove to provide accuracy of 96.7% and also better specificity; thus, the overall performance of the CAD scheme was improved substantially.","",""
28,"Sanghyuk Chun, Seong Joon Oh, Sangdoo Yun, Dongyoon Han, Junsuk Choe, Y. Yoo","An Empirical Evaluation on Robustness and Uncertainty of Regularization Methods",2020,"","","","",125,"2022-07-13 09:30:38","","","","",,,,,28,14.00,5,6,2,"Despite apparent human-level performances of deep neural networks (DNN), they behave fundamentally differently from humans. They easily change predictions when small corruptions such as blur and noise are applied on the input (lack of robustness), and they often produce confident predictions on out-of-distribution samples (improper uncertainty measure). While a number of researches have aimed to address those issues, proposed solutions are typically expensive and complicated (e.g. Bayesian inference and adversarial training). Meanwhile, many simple and cheap regularization methods have been developed to enhance the generalization of classifiers. Such regularization methods have largely been overlooked as baselines for addressing the robustness and uncertainty issues, as they are not specifically designed for that. In this paper, we provide extensive empirical evaluations on the robustness and uncertainty estimates of image classifiers (CIFAR-100 and ImageNet) trained with state-of-the-art regularization methods. Furthermore, experimental results show that certain regularization methods can serve as strong baseline methods for robustness and uncertainty estimation of DNNs.","",""
0,"N. Fjodorova, M. Novič, S. Zuperl, K. Venko","Counter-Propagation Artificial Neural Network Models for Prediction of Carcinogenicity of Non-congeneric Chemicals for Regulatory Uses",2017,"","","","",126,"2022-07-13 09:30:38","","10.1007/978-3-319-56850-8_14","","",,,,,0,0.00,0,4,5,"","",""
4,"Ninghao Liu, Mengnan Du, Xia Hu","Adversarial Machine Learning: An Interpretation Perspective",2020,"","","","",127,"2022-07-13 09:30:38","","","","",,,,,4,2.00,1,3,2,"Recent years have witnessed the significant advances of machine learning in a wide spectrum of applications. However, machine learning models, especially deep neural networks, have been recently found to be vulnerable to carefully-crafted input called adversarial samples. The difference between normal and adversarial samples is almost imperceptible to human. Many work have been proposed to study adversarial attack and defense in different scenarios. An intriguing and crucial aspect among those work is to understand the essential cause of model vulnerability, which requires in-depth exploration of another concept in machine learning models, i.e., interpretability. Interpretable machine learning tries to extract human-understandable terms for the working mechanism of models, which also receives a lot of attention from both academia and industry. Recently, an increasing number of work start to incorporate interpretation into the exploration of adversarial robustness. Furthermore, we observe that many previous work of adversarial attacking, although did not mention it explicitly, can be regarded as natural extension of interpretation. In this paper, we review recent work on adversarial attack and defense, particularly, from the perspective of machine learning interpretation. We categorize interpretation into two types, according to whether it focuses on raw features or model components. For each type of interpretation, we elaborate on how it could be used in attacks, or defense against adversaries. After that, we briefly illustrate other possible correlations between the two domains. Finally, we discuss the challenges and future directions along tackling adversary issues with interpretation.","",""
44,"Minh Nguyen Nhat To, Q. Vu, B. Turkbey, P. Choyke, J. T. Kwak","Deep dense multi-path neural network for prostate segmentation in magnetic resonance imaging",2018,"","","","",128,"2022-07-13 09:30:38","","10.1007/s11548-018-1841-4","","",,,,,44,11.00,9,5,4,"","",""
0,"J. Lope, M. Graña","A Hybrid Time-Distributed Deep Neural Architecture for Speech Emotion Recognition",2022,"","","","",129,"2022-07-13 09:30:38","","10.1142/S0129065722500241","","",,,,,0,0.00,0,2,1,"In recent years, speech emotion recognition (SER) has emerged as one of the most active human-machine interaction research areas. Innovative electronic devices, services and applications are increasingly aiming to check the user emotional state either to issue alerts under some predefined conditions or to adapt the system responses to the user emotions. Voice expression is a very rich and noninvasive source of information for emotion assessment. This paper presents a novel SER approach based on that is a hybrid of a time-distributed convolutional neural network (TD-CNN) and a long short-term memory (LSTM) network. Mel-frequency log-power spectrograms (MFLPSs) extracted from audio recordings are parsed by a sliding window that selects the input for the TD-CNN. The TD-CNN transforms the input image data into a sequence of high-level features that are feed to the LSTM, which carries out the overall signal interpretation. In order to reduce overfitting, the MFLPS representation allows innovative image data augmentation techniques that have no immediate equivalent on the original audio signal. Validation of the proposed hybrid architecture achieves an average recognition accuracy of 73.98% on the most widely and hardest publicly distributed database for SER benchmarking. A permutation test confirms that this result is significantly different from random classification ([Formula: see text]). The proposed architecture outperforms state-of-the-art deep learning models as well as conventional machine learning techniques evaluated on the same database trying to identify the same number of emotions.","",""
4,"J. Mok, Byunggook Na, Hyeokjun Choe, Sungroh Yoon","AdvRush: Searching for Adversarially Robust Neural Architectures",2021,"","","","",130,"2022-07-13 09:30:38","","10.1109/iccv48922.2021.01210","","",,,,,4,4.00,1,4,1,"Deep neural networks continue to awe the world with their remarkable performance. Their predictions, however, are prone to be corrupted by adversarial examples that are imperceptible to humans. Current efforts to improve the robustness of neural networks against adversarial examples are focused on developing robust training methods, which update the weights of a neural network in a more robust direction. In this work, we take a step beyond training of the weight parameters and consider the problem of designing an adversarially robust neural architecture with high intrinsic robustness. We propose AdvRush, a novel adversarial robustness-aware neural architecture search algorithm, based upon a finding that independent of the training method, the intrinsic robustness of a neural network can be represented with the smoothness of its input loss landscape. Through a regularizer that favors a candidate architecture with a smoother input loss landscape, AdvRush successfully discovers an adversarially robust neural architecture. Along with a comprehensive theoretical motivation for AdvRush, we conduct an extensive amount of experiments to demonstrate the efficacy of AdvRush on various benchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91% robust accuracy under FGSM attack after standard training and 50.04% robust accuracy under AutoAttack after 7-step PGD adversarial training.","",""
31,"Jie Song, Xu Chen, Otmar Hilliges","Human Body Model Fitting by Learned Gradient Descent",2020,"","","","",131,"2022-07-13 09:30:38","","10.1007/978-3-030-58565-5_44","","",,,,,31,15.50,10,3,2,"","",""
356,"M. Mirman, Timon Gehr, Martin T. Vechev","Differentiable Abstract Interpretation for Provably Robust Neural Networks",2018,"","","","",132,"2022-07-13 09:30:38","","","","",,,,,356,89.00,119,3,4,"We introduce a scalable method for training robust neural networks based on abstract interpretation. We present several abstract transformers which balance efficiency with precision and show these can be used to train large neural networks that are certifiably robust to adversarial perturbations.","",""
70,"Weijia Li, Runmin Dong, H. Fu, Le Yu","Large-Scale Oil Palm Tree Detection from High-Resolution Satellite Images Using Two-Stage Convolutional Neural Networks",2018,"","","","",133,"2022-07-13 09:30:38","","10.3390/rs11010011","","",,,,,70,17.50,18,4,4,"Being an important economic crop that contributes 35% of the total consumption of vegetable oil, remote sensing-based quantitative detection of oil palm trees has long been a key research direction for both agriculture and environmental purposes. While existing methods already demonstrate satisfactory effectiveness for small regions, performing the detection for a large region with satisfactory accuracy is still challenging. In this study, we proposed a two-stage convolutional neural network (TS-CNN)-based oil palm detection method using high-resolution satellite images (i.e. Quickbird) in a large-scale study area of Malaysia. The TS-CNN consists of one CNN for land cover classification and one CNN for object classification. The two CNNs were trained and optimized independently based on 20,000 samples collected through human interpretation. For the large-scale oil palm detection for an area of 55 km2, we proposed an effective workflow that consists of an overlapping partitioning method for large-scale image division, a multi-scale sliding window method for oil palm coordinate prediction, and a minimum distance filter method for post-processing. Our proposed approach achieves a much higher average F1-score of 94.99% in our study area compared with existing oil palm detection methods (87.95%, 81.80%, 80.61%, and 78.35% for single-stage CNN, Support Vector Machine (SVM), Random Forest (RF), and Artificial Neural Network (ANN), respectively), and much fewer confusions with other vegetation and buildings in the whole image detection results.","",""
25,"A. Vessoni, R. Herai, Jerome V. Karpiak, Angelica M S Leal, C. Trujillo, A. Quinet, L. F. Agnez Lima, C. Menck, A. Muotri","Cockayne syndrome-derived neurons display reduced synapse density and altered neural network synchrony.",2016,"","","","",134,"2022-07-13 09:30:38","","10.1093/hmg/ddw008","","",,,,,25,4.17,3,9,6,"Cockayne syndrome (CS) is a rare genetic disorder in which 80% of cases are caused by mutations in the Excision Repair Cross-Complementation group 6 gene (ERCC6). The encoded ERCC6 protein is more commonly referred to as Cockayne Syndrome B protein (CSB). Classical symptoms of CS patients include failure to thrive and a severe neuropathology characterized by microcephaly, hypomyelination, calcification and neuronal loss. Modeling the neurological aspect of this disease has proven difficult since murine models fail to mirror classical neurological symptoms. Therefore, a robust human in vitro cellular model would advance our fundamental understanding of the disease and reveal potential therapeutic targets. Herein, we successfully derived functional CS neural networks from human CS induced pluripotent stem cells (iPSCs) providing a new tool to facilitate studying this devastating disease. We identified dysregulation of the Growth Hormone/Insulin-like Growth Factor-1 (GH/IGF-1) pathway as well as pathways related to synapse formation, maintenance and neuronal differentiation in CSB neurons using unbiased RNA-seq gene expression analyses. Moreover, when compared to unaffected controls, CSB-deficient neural networks displayed altered electrophysiological activity, including decreased synchrony, and reduced synapse density. Collectively, our work reveals that CSB is required for normal neuronal function and we have established an alternative to previously available models to further study neural-specific aspects of CS.","",""
0,"Hoseok Choi, Seokbeen Lim, Kyeongran Min, Kyoung-ha Ahn, K. Lee, D. Jang","Non–human primate epidural ECoG analysis using explainable deep learning technology",2021,"","","","",135,"2022-07-13 09:30:38","","10.1088/1741-2552/ac3314","","",,,,,0,0.00,0,6,1,"Objective. With the development in the field of neural networks, explainable AI (XAI), is being studied to ensure that artificial intelligence models can be explained. There are some attempts to apply neural networks to neuroscientific studies to explain neurophysiological information with high machine learning performances. However, most of those studies have simply visualized features extracted from XAI and seem to lack an active neuroscientific interpretation of those features. In this study, we have tried to actively explain the high-dimensional learning features contained in the neurophysiological information extracted from XAI, compared with the previously reported neuroscientific results. Approach. We designed a deep neural network classifier using 3D information (3D DNN) and a 3D class activation map (3D CAM) to visualize high-dimensional classification features. We used those tools to classify monkey electrocorticogram (ECoG) data obtained from the unimanual and bimanual movement experiment. Main results. The 3D DNN showed better classification accuracy than other machine learning techniques, such as 2D DNN. Unexpectedly, the activation weight in the 3D CAM analysis was high in the ipsilateral motor and somatosensory cortex regions, whereas the gamma-band power was activated in the contralateral areas during unimanual movement, which suggests that the brain signal acquired from the motor cortex contains information about both contralateral movement and ipsilateral movement. Moreover, the hand-movement classification system used critical temporal information at movement onset and offset when classifying bimanual movements. Significance. As far as we know, this is the first study to use high-dimensional neurophysiological information (spatial, spectral, and temporal) with the deep learning method, reconstruct those features, and explain how the neural network works. We expect that our methods can be widely applied and used in neuroscience and electrophysiology research from the point of view of the explainability of XAI as well as its performance.","",""
2,"Moonis Ali, B. Whitehead, U. Gupta, H. Ferber","Identification and interpretation of patterns in rocket engine data: Artificial intelligence and neural network approaches",1995,"","","","",136,"2022-07-13 09:30:38","","","","",,,,,2,0.07,1,4,27,"This paper describes an expert system which is designed to perform automatic data analysis, identify anomalous events, and determine the characteristic features of these events. We have employed both artificial intelligence and neural net approaches in the design of this expert system. The artificial intelligence approach is useful because it provides (1) the use of human experts' knowledge of sensor behavior and faulty engine conditions in interpreting data; (2) the use of engine design knowledge and physical sensor locations in establishing relationships among the events of multiple sensors; (3) the use of stored analysis of past data of faulty engine conditions; and (4) the use of knowledge-based reasoning in distinguishing sensor failure from actual faults. The neural network approach appears promising because neural nets (1) can be trained on extremely noisy data and produce classifications which are more robust under noisy conditions than other classification techniques; (2) avoid the necessity of noise removal by digital filtering and therefore avoid the need to make assumptions about frequency bands or other signal characteristics of anomalous behavior; (3) can, in effect, generate their own feature detectors based on the characteristics of the sensor data used in training; and (4) are inherently parallel and therefore are potentially implementable in special-purpose parallel hardware.","",""
0,"Rui Li, Le-Dian Liu, Baa-Liang Lu","Measuring Human Decision Confidence from EEG Signals in an Object Detection Task",2021,"","","","",137,"2022-07-13 09:30:38","","10.1109/NER49283.2021.9441157","","",,,,,0,0.00,0,3,1,"In this paper, we investigate human decision confidence during image interpretation in an object detection task using electroencephalography (EEG) signals. We develop an EEG dataset acquired from 14 subjects. Five popular EEG features, differential entropy (DE), power spectral density (PSD), differential asymmetry (DASM), rational asymmetry (RASM) and asymmetry (ASM), and two classifiers, a support vector machine (SVM) and a deep neural network with shortcut connections (DNNS), are adopted to measure decision confidence in the object detection task. The classification results indicate that the DE feature with the DNNS model achieves the best accuracy of 47.36% and F1-score of 43.5% for five decision confidence levels. For the extreme confidence levels, the recognition accuracy reaches 83.98%, with an average Fl-score of 80.93%. We also found that the delta band performs better than the other four bands and that the prefrontal area and parietal area might be sensitive brain regions that represent decision confidence in object detection tasks.","",""
0,"Meng Yang, Haiping Huang, Lichao Huang, Nan Zhang, Jihong Wu, Huanming Yang, F. Mu","LOGO, a contextualized pre-trained language model of human genome flexibly adapts to various downstream tasks by fine-tuning",2021,"","","","",138,"2022-07-13 09:30:38","","10.21203/rs.3.rs-448927/v1","","",,,,,0,0.00,0,7,1,"  Interpretation of non-coding genome remains an unsolved challenge in human genetics due to impracticality of exhaustively annotate biochemically active elements in all conditions. Deep learning based computational approaches emerge recently to help interpretating non-coding regions. Here we present LOGO (Language of Genome), a self-attention based contextualized pre-trained language model that applies self-supervision techniques to learn bidirectional representations of unlabeled human reference genome and extend to a series of downstream tasks via fine-tuning. We also explore a novel knowledge embedded version of LOGO to incorporate prior human annotations. Experiments show that LOGO achieves 15% absolute improvement for promoter identification and up to 4.5% absolute improvement for enhancer-promoter interaction prediction. LOGO exhibits state-of-the-art predictive power on chromatin features with only 3% parameterization against fully supervised convolutional neural network, DeepSEA. Fine-tuned LOGO also shows outstanding performance in prioritizing non-coding variants associated with human diseases. In addition, we apply LOGO to interpret type 2 diabetes (T2D) GWAS signals and infer underlying regulatory mechanisms. We make a conceptual analogy between natural language and human genome and demonstrate LOGO is an accurate, fast, scalable, and robust framework with powerful adaptability to various tasks without substantial task-specific architecture modifications.","",""
31,"Amirata Ghorbani, David Ouyang, Abubakar Abid, B. He, Jonathan H. Chen, R. Harrington, D. Liang, E. Ashley, James Y. Zou","Deep learning interpretation of echocardiograms",2020,"","","","",139,"2022-07-13 09:30:38","","10.1038/s41746-019-0216-8","","",,,,,31,15.50,3,9,2,"","",""
25,"H. Baumgartl, Ricardo Buettner","Development of a Highly Precise Place Recognition Module for Effective Human-robot Interactions in Changing Lighting and Viewpoint Conditions",2020,"","","","",140,"2022-07-13 09:30:38","","10.24251/hicss.2020.069","","",,,,,25,12.50,13,2,2,"We present a highly precise and robust module for indoor place recognition, extending the work by Lemaignan et al. and Robert Jr. by giving the robot the ability to recognize its environment context. We developed a full end-to-end convolutional neural network architecture, using a pre-trained deep convolutional neural network and the explicit inductive bias transfer learning strategy. Experimental results based on the York University and Rzeszów University dataset show excellent performance values (over 94.75 and 97.95 percent accuracy) and a high level of robustness over changes in camera viewpoint and lighting conditions, outperforming current benchmarks. Furthermore, our architecture is 82.46 percent smaller than the current benchmark, making our module suitable for embedding into mobile robots and easily adoptable to other datasets without the need for heavy adjustments.","",""
53,"Hoang-Dung Tran, Stanley Bak, Weiming Xiang, Taylor T. Johnson","Verification of Deep Convolutional Neural Networks Using ImageStars",2020,"","","","",141,"2022-07-13 09:30:38","","10.1007/978-3-030-53288-8_2","","",,,,,53,26.50,13,4,2,"","",""
0,"Tarek Khorshed, Mohamed N. Moustafa, A. Rafea","Learning & Visualizing Genomic Signatures of Cancer Tumors using Deep Neural Networks",2020,"","","","",142,"2022-07-13 09:30:38","","10.1109/IJCNN48605.2020.9207368","","",,,,,0,0.00,0,3,2,"Deep learning for medical diagnosis using genomics is extremely challenging given the high dimensionality of the data and lack of sufficient patient samples. Another challenge is that deep models are conceived as black boxes without much interpretation on how these complex models make predictions. We propose a deep transfer learning framework for cancer diagnosis with the capability of learning the sequence of DNA and RNA in cancer cells and identifying genetic changes that alter cell behavior and cause uncontrollable growth and malignancy. We design a new Convolutional Neural Network architecture with capabilities of learning the genomic signatures of whole-transcriptome gene expressions collected from multiple tumor types covering multiple organ sites. We demonstrate how our trained model can function as a comprehensive multi-tissue cancer classifier by using transfer learning to build classifiers for tumors lacking sufficient human samples to be trained independently. We introduce visualization procedures to provide more biological insight on how our model is learning genomic signatures and accurately making predictions across multiple cancer tissue types.","",""
173,"Lipeng Ke, Ming-Ching Chang, H. Qi, Siwei Lyu","Multi-Scale Structure-Aware Network for Human Pose Estimation",2018,"","","","",143,"2022-07-13 09:30:38","","10.1007/978-3-030-01216-8_44","","",,,,,173,43.25,43,4,4,"","",""
20,"Minjing Dong, Yanxi Li, Yunhe Wang, Chang Xu","Adversarially Robust Neural Architectures",2020,"","","","",144,"2022-07-13 09:30:38","","","","",,,,,20,10.00,5,4,2,"Deep Neural Network (DNN) are vulnerable to adversarial attack. Existing methods are devoted to developing various robust training strategies or regularizations to update the weights of the neural network. But beyond the weights, the overall structure and information flow in the network are explicitly determined by the neural architecture, which remains unexplored. This paper thus aims to improve the adversarial robustness of the network from the architecture perspective with NAS framework. We explore the relationship among adversarial robustness, Lipschitz constant, and architecture parameters and show that an appropriate constraint on architecture parameters could reduce the Lipschitz constant to further improve the robustness. For NAS framework, all the architecture parameters are equally treated when the discrete architecture is sampled from supernet. However, the importance of architecture parameters could vary from operation to operation or connection to connection, which is not explored and might reduce the confidence of robust architecture sampling. Thus, we propose to sample architecture parameters from trainable multivariate log-normal distributions, with which the Lipschitz constant of entire network can be approximated using a univariate log-normal distribution with mean and variance related to architecture parameters. Compared with adversarially trained neural architectures searched by various NAS algorithms as well as efficient human-designed models, our algorithm empirically achieves the best performance among all the models under various attacks on different datasets.","",""
0,"Peter Kok-Yiu Wong, Han Luo, Mingzhu Wang, Jack C. P. Cheng","Enriched and Discriminative Human Features for Person Re-Identification Based on Explainable Behaviors of Convolutional Neural Networks",2020,"","","","",145,"2022-07-13 09:30:38","","10.1007/978-3-030-51295-8_5","","",,,,,0,0.00,0,4,2,"","",""
447,"D. Ciresan, U. Meier, L. Gambardella, J. Schmidhuber","Convolutional Neural Network Committees for Handwritten Character Classification",2011,"","","","",146,"2022-07-13 09:30:38","","10.1109/ICDAR.2011.229","","",,,,,447,40.64,112,4,11,"In 2010, after many years of stagnation, the MNIST handwriting recognition benchmark record dropped from 0.40% error rate to 0.35%. Here we report 0.27% for a committee of seven deep CNNs trained on graphics cards, narrowing the gap to human performance. We also apply the same architecture to NIST SD 19, a more challenging dataset including lower and upper case letters. A committee of seven CNNs obtains the best results published so far for both NIST digits and NIST letters. The robustness of our method is verified by analyzing 78125 different 7-net committees.","",""
4,"H. Kondo, S. Rahman","Human-face recognition using neural network with mosaic pattern",1999,"","","","",147,"2022-07-13 09:30:38","","10.1109/ICSMC.1999.816659","","",,,,,4,0.17,2,2,23,"A flexible robust human-face recognition method using neural networks with mosaic pattern is presented. In human-face recognition differences in facial expressions makes it difficult. However an appropriate mosaic face keeps the fundamental feature of the face. In the paper typical faces with different expressions are used as teacher patterns. As a result of employing such teacher patterns the property of the robustness of the neural network human-face recognition becomes very large and a neural network which is not so sensitive to human expression can be constructed. First it is shown that the presented neural network works well for differences of facial expressions such as smiling, crying, and irritated faces. Furthermore facing, right rotated, and left rotated faces are examined. A high recognition rate is attained for forty persons. The utilized network is a backpropagation neural one with three layers.","",""
32,"Fabio Vesperini, L. Gabrielli, E. Principi, S. Squartini","Polyphonic Sound Event Detection by Using Capsule Neural Networks",2018,"","","","",148,"2022-07-13 09:30:38","","10.1109/JSTSP.2019.2902305","","",,,,,32,8.00,8,4,4,"Artificial sound event detection (SED) aims to mimic the human ability to perceive and understand what is happening in the surroundings. Nowadays, deep learning offers valuable techniques for this goal, such as convolutional neural networks (CNNs). The capsule neural network (CapsNet) architecture has been recently introduced in the image processing field with the intent to overcome some of the known limitations of CNNs, specifically regarding the scarce robustness to affine transformations (i.e., perspective, size, and orientation) and the detection of overlapped images. This motivated the authors to employ CapsNets to deal with the polyphonic SED task, in which multiple sound events occur simultaneously. Specifically, we propose to exploit the capsule units to represent a set of distinctive properties for each individual sound event. Capsule units are connected through a so-called dynamic routing that encourages learning part-whole relationships and improves the detection performance in a polyphonic context. This paper reports extensive evaluations carried out on three publicly available datasets, showing how the CapsNet-based algorithm not only outperforms standard CNNs but also achieves the best results with respect to the state-of-the-art algorithms.","",""
23,"Andrew J. R. Simpson","Over-Sampling in a Deep Neural Network",2015,"","","","",149,"2022-07-13 09:30:38","","","","",,,,,23,3.29,23,1,7,"Deep neural networks (DNN) are the state of the art on many engineering problems such as computer vision and audition. A key factor in the success of the DNN is scalability - bigger networks work better. However, the reason for this scalability is not yet well understood. Here, we interpret the DNN as a discrete system, of linear filters followed by nonlinear activations, that is subject to the laws of sampling theory. In this context, we demonstrate that over-sampled networks are more selective, learn faster and learn more robustly. Our findings may ultimately generalize to the human brain.","",""
5,"Sandareka Wickramanayake, W. Hsu, M. Lee","Comprehensible Convolutional Neural Networks via Guided Concept Learning",2021,"","","","",150,"2022-07-13 09:30:38","","10.1109/IJCNN52387.2021.9534269","","",,,,,5,5.00,2,3,1,"Learning concepts that are consistent with human perception is important for Deep Neural Networks to win end-user trust. Post-hoc interpretation methods lack transparency in the feature representations learned by the models. This work proposes a guided learning approach with an additional concept layer in a CNN-based architecture to learn the associations between visual features and word phrases. We design an objective function that optimizes both prediction accuracy and semantics of the learned feature representations. Experiment results demonstrate that the proposed model can learn concepts that are consistent with human perception and their corresponding contributions to the model decision without compromising accuracy. Further, these learned concepts are transferable to new classes of objects that have similar concepts.","",""
114,"E. Gelenbe, Yutao Feng, K. Krishnan","Neural network methods for volumetric magnetic resonance imaging of the human brain",1996,"","","","",151,"2022-07-13 09:30:38","","10.1109/5.537113","","",,,,,114,4.38,38,3,26,"Brain magnetic resonance (MR) images contain massive information requiring lengthy and complex interpretation (as in the identification of significant portions of the image), quantitative evaluation (as in the determination of the size of certain significant regions), and sophisticated interpretation (as in determining any image portions which indicate signs of lesions or of disease). In this paper we first survey the clinical and research needs for brain imaging. We present the state-of-the-art in relevant image analysis techniques. We then discuss our recent work on the use of novel artificial neural networks which have a recurrent structure to extract precise morphometric information from MRI scans of the human brain. Finally, experimental data using our novel approach is presented and suggestions are made for future research.","",""
30,"Chih-Ta Yen, Yi-Jie Huang","Frequency domain digital watermark recognition using image code sequences with a back-propagation neural network",2016,"","","","",152,"2022-07-13 09:30:38","","10.1007/s11042-015-2718-y","","",,,,,30,5.00,15,2,6,"","",""
286,"Lei Shi, Yifan Zhang, Jian Cheng, Hanqing Lu","Skeleton-Based Action Recognition With Directed Graph Neural Networks",2019,"","","","",153,"2022-07-13 09:30:38","","10.1109/CVPR.2019.00810","","",,,,,286,95.33,72,4,3,"The skeleton data have been widely used for the action recognition tasks since they can robustly accommodate dynamic circumstances and complex backgrounds. In existing methods, both the joint and bone information in skeleton data have been proved to be of great help for action recognition tasks. However, how to incorporate these two types of data to best take advantage of the relationship between joints and bones remains a problem to be solved. In this work, we represent the skeleton data as a directed acyclic graph based on the kinematic dependency between the joints and bones in the natural human body. A novel directed graph neural network is designed specially to extract the information of joints, bones and their relations and make prediction based on the extracted features. In addition, to better fit the action recognition task, the topological structure of the graph is made adaptive based on the training process, which brings notable improvement. Moreover, the motion information of the skeleton sequence is exploited and combined with the spatial information to further enhance the performance in a two-stream framework. Our final model is tested on two large-scale datasets, NTU-RGBD and Skeleton-Kinetics, and exceeds state-of-the-art performance on both of them.","",""
327,"M. Fullana, B. Harrison, C. Soriano-Mas, B. Vervliet, N. Cardoner, A. Àvila-Parcet, J. Raduà","Neural signatures of human fear conditioning: an updated and extended meta-analysis of fMRI studies",2016,"","","","",154,"2022-07-13 09:30:38","","10.1038/mp.2015.88","","",,,,,327,54.50,47,7,6,"","",""
40,"Min Wang, Heba El-Fiqi, Jiankun Hu, H. Abbass","Convolutional Neural Networks Using Dynamic Functional Connectivity for EEG-Based Person Identification in Diverse Human States",2019,"","","","",155,"2022-07-13 09:30:38","","10.1109/TIFS.2019.2916403","","",,,,,40,13.33,10,4,3,"Highly secure access control requires Swiss-cheese-type multi-layer security protocols. The use of electroencephalogram (EEG) to provide cognitive indicators for human workload and fatigue has created environments where the EEG data are well-integrated into systems, making it readily available for more forms of innovative uses including biometrics. However, most of the existing studies on EEG biometrics rely on resting state signals or require specific and repetitive sensory stimulation, limiting their uses in naturalistic settings. Moreover, the limited discriminatory power of uni-variate measures denies an opportunity to use dependences information inherent in brain regions to design more robust biometric identifiers. In this paper, we proposed a novel model for ongoing EEG biometric identification using EEG collected during a diverse set of tasks. The novelty lies in representing EEG signals as a graph based on within-frequency and cross-frequency functional connectivity estimates, and the use of graph convolutional neural network (GCNN) to automatically capture deep intrinsic structural representations from the EEG graphs for person identification. An extensive investigation was carried out to assess the robustness of the method against diverse human states, including resting states under eye-open and eye-closed conditions and active states drawn during the performance of four different tasks. We compared our method with the state-of-the-art EEG features, classifiers, and models of EEG biometrics. Results show that the representation drawn from EEG functional connectivity graphs demonstrates more robust biometric traits than direct use of uni-variate features. Moreover, the GCNN can effectively and efficiently capture discriminative traits, thus generalizing better over diverse human states.","",""
65,"I. Rizvi, B. Mohan","Object-Based Image Analysis of High-Resolution Satellite Images Using Modified Cloud Basis Function Neural Network and Probabilistic Relaxation Labeling Process",2011,"","","","",156,"2022-07-13 09:30:38","","10.1109/TGRS.2011.2171695","","",,,,,65,5.91,33,2,11,"Object-based image analysis is quickly gaining acceptance among remote sensing community, and object-based image classification methods are increasingly being used for classification of land use/cover units from high-resolution satellite images with results closer to human interpretation compared to per-pixel classifiers. The problem of nonlinear separability of classes in a feature space consisting of spectral/spatial/textural features is addressed by kernel-based nonlinear mapping of the feature vectors. This facilitates use of linear discriminant functions for classification as used in artificial neural networks (ANNs). In this paper, performance of a recently introduced kernel called cloud basis function (CBF) is investigated with some modification for classification. The CBF has demonstrated superior performance to the tune of about 4% higher classification accuracy compared to conventional radial basis function used in ANN. The results are further improved by using probabilistic relaxation labeling as a postprocessing step. This paper has potential applications in urban planning and urban studies.","",""
0,"A. Demidovskij","Encoding and Decoding of Recursive Structures in Neural-Symbolic Systems",2021,"","","","",157,"2022-07-13 09:30:38","","10.3103/S1060992X21010033","","",,,,,0,0.00,0,1,1,"","",""
99,"Kun Xia, Jianguang Huang, Hanyu Wang","LSTM-CNN Architecture for Human Activity Recognition",2020,"","","","",158,"2022-07-13 09:30:38","","10.1109/ACCESS.2020.2982225","","",,,,,99,49.50,33,3,2,"In the past years, traditional pattern recognition methods have made great progress. However, these methods rely heavily on manual feature extraction, which may hinder the generalization model performance. With the increasing popularity and success of deep learning methods, using these techniques to recognize human actions in mobile and wearable computing scenarios has attracted widespread attention. In this paper, a deep neural network that combines convolutional layers with long short-term memory (LSTM) was proposed. This model could extract activity features automatically and classify them with a few model parameters. LSTM is a variant of the recurrent neural network (RNN), which is more suitable for processing temporal sequences. In the proposed architecture, the raw data collected by mobile sensors was fed into a two-layer LSTM followed by convolutional layers. In addition, a global average pooling layer (GAP) was applied to replace the fully connected layer after convolution for reducing model parameters. Moreover, a batch normalization layer (BN) was added after the GAP layer to speed up the convergence, and obvious results were achieved. The model performance was evaluated on three public datasets (UCI, WISDM, and OPPORTUNITY). Finally, the overall accuracy of the model in the UCI-HAR dataset is 95.78%, in the WISDM dataset is 95.85%, and in the OPPORTUNITY dataset is 92.63%. The results show that the proposed model has higher robustness and better activity detection capability than some of the reported results. It can not only adaptively extract activity features, but also has fewer parameters and higher accuracy.","",""
54,"Zerong Zheng, Tao Yu, Yebin Liu, Qionghai Dai","PaMIR: Parametric Model-Conditioned Implicit Representation for Image-Based Human Reconstruction",2020,"","","","",159,"2022-07-13 09:30:38","","10.1109/TPAMI.2021.3050505","","",,,,,54,27.00,14,4,2,"Modeling 3D humans accurately and robustly from a single image is very challenging, and the key for such an ill-posed problem is the 3D representation of the human models. To overcome the limitations of regular 3D representations, we propose Parametric Model-Conditioned Implicit Representation (PaMIR), which combines the parametric body model with the free-form deep implicit function. In our PaMIR-based reconstruction framework, a novel deep neural network is proposed to regularize the free-form deep implicit function using the semantic features of the parametric model, which improves the generalization ability under the scenarios of challenging poses and various clothing topologies. Moreover, a novel depth-ambiguity-aware training loss is further integrated to resolve depth ambiguities and enable successful surface detail reconstruction with imperfect body reference. Finally, we propose a body reference optimization method to improve the parametric model estimation accuracy and to enhance the consistency between the parametric model and the implicit function. With the PaMIR representation, our framework can be easily extended to multi-image input scenarios without the need of multi-camera calibration and pose synchronization. Experimental results demonstrate that our method achieves state-of-the-art performance for image-based 3D human reconstruction in the cases of challenging poses and clothing types.","",""
122,"Bowen Xu, Deheng Ye, Zhenchang Xing, Xin Xia, Guibin Chen, Shanping Li","Predicting semantically linkable knowledge in developer online forums via convolutional neural network",2016,"","","","",160,"2022-07-13 09:30:38","","10.1145/2970276.2970357","","",,,,,122,20.33,20,6,6,"Consider a question and its answers in Stack Overflow as a knowledge unit. Knowledge units often contain semantically relevant knowledge, and thus linkable for different purposes, such as duplicate questions, directly linkable for problem solving, indirectly linkable for related information. Recognising different classes of linkable knowledge would support more targeted information needs when users search or explore the knowledge base. Existing methods focus on binary relatedness (i.e., related or not), and are not robust to recognize different classes of semantic relatedness when linkable knowledge units share few words in common (i.e., have lexical gap). In this paper, we formulate the problem of predicting semantically linkable knowledge units as a multiclass classification problem, and solve the problem using deep learning techniques. To overcome the lexical gap issue, we adopt neural language model (word embeddings) and convolutional neural network (CNN) to capture word- and document-level semantics of knowledge units. Instead of using human-engineered classifier features which are hard to design for informal user-generated content, we exploit large amounts of different types of user-created knowledge-unit links to train the CNN to learn the most informative wordlevel and document-level features for the multiclass classification task. Our evaluation shows that our deep-learning based approach significantly and consistently outperforms traditional methods using traditional word representations and human-engineered classifier features.","",""
52,"Aisha Khan, Stephen Gould, M. Salzmann","Deep Convolutional Neural Networks for Human Embryonic Cell Counting",2016,"","","","",161,"2022-07-13 09:30:38","","10.1007/978-3-319-46604-0_25","","",,,,,52,8.67,17,3,6,"","",""
14,"Colin L. Cooke, K. A. Scott","Estimating Sea Ice Concentration From SAR: Training Convolutional Neural Networks With Passive Microwave Data",2019,"","","","",162,"2022-07-13 09:30:38","","10.1109/TGRS.2019.2892723","","",,,,,14,4.67,7,2,3,"Historically, sea ice concentration (SIC) has been measured through the use of passive microwave sensors, as well as human interpretation of synthetic aperture radar (SAR). Although passive microwave data are processed automatically, it suffers from poor spatial resolution and the higher frequency channels are sensitive to weather conditions. Deep learning has demonstrated its ability to perform complex and accurate analysis of images; here, we apply deep learning to estimate ice concentration from SAR scenes. We developed a deep convolutional neural network (CNN) that predicts SIC from SAR, trained upon passive microwave data. The model achieves a 5.24%/7.87% error on its train and test set, respectively. To assess the real-world applicability, we performed an independent validation on 18 SAR scenes (from two distinct geographical regions), not previously seen during training or test. Comparing against human-generated ice analysis charts, we achieved an <inline-formula> <tex-math notation=""LaTeX"">$L1$ </tex-math></inline-formula> error of 0.2059, competitive with passive microwave (<inline-formula> <tex-math notation=""LaTeX"">$E_{L1} = 0.1863$ </tex-math></inline-formula>) for the Canadian Arctic Archipelago. For the Gulf of Saint Lawrence region, we achieved an <inline-formula> <tex-math notation=""LaTeX"">$L1$ </tex-math></inline-formula> error of 0.2653, significantly better than the passive microwave result (<inline-formula> <tex-math notation=""LaTeX"">$E_{L1} = 0.3593$ </tex-math></inline-formula>). By using novel techniques for model training, as well as training entirely upon passive microwave data, we present an accessible and robust method of developing similar systems for processing SAR.<xref ref-type=""fn"" rid=""fn1""><sup>1</sup></xref> Our results suggest that with further postprocessing, CNNs are accurate and robust enough to be used for operational tasks.<fn id=""fn1""><label><sup>1</sup></label><p>Code available: <uri>https://github.com/clvcooke/Estimating-SIC-from-SAR-github.com/clvcooke/Estimating-SIC-from-SAR</uri></p></fn>","",""
367,"Marcella Cornia, L. Baraldi, G. Serra, R. Cucchiara","Predicting Human Eye Fixations via an LSTM-Based Saliency Attentive Model",2016,"","","","",163,"2022-07-13 09:30:38","","10.1109/TIP.2018.2851672","","",,,,,367,61.17,92,4,6,"Data-driven saliency has recently gained a lot of attention thanks to the use of convolutional neural networks for predicting gaze fixations. In this paper, we go beyond standard approaches to saliency prediction, in which gaze maps are computed with a feed-forward network, and present a novel model which can predict accurate saliency maps by incorporating neural attentive mechanisms. The core of our solution is a convolutional long short-term memory that focuses on the most salient regions of the input image to iteratively refine the predicted saliency map. In addition, to tackle the center bias typical of human eye fixations, our model can learn a set of prior maps generated with Gaussian functions. We show, through an extensive evaluation, that the proposed architecture outperforms the current state-of-the-art on public saliency prediction datasets. We further study the contribution of each key component to demonstrate their robustness on different scenarios.","",""
6,"R. Hamad, Masashi Kimura, Longzhi Yang, W. L. Woo, Bo Wei","Dilated causal convolution with multi-head self attention for sensor human activity recognition",2021,"","","","",164,"2022-07-13 09:30:38","","10.1007/S00521-021-06007-5","","",,,,,6,6.00,1,5,1,"","",""
49,"Logan Engstrom, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Brandon Tran, A. Madry","Learning Perceptually-Aligned Representations via Adversarial Robustness",2019,"","","","",165,"2022-07-13 09:30:38","","","","",,,,,49,16.33,8,6,3,"Many applications of machine learning require models that are “human-aligned,” i.e., that make decisions based on human-meaningful information about the input. We identify the pervasive brittleness of deep networks’ learned representations as a fundamental barrier to attaining this goal. We then re-cast robust optimization as a tool for enforcing human priors on the features learned by deep neural networks. The resulting robust feature representations turn out to be significantly more aligned with human perception. We leverage these representations to perform input interpolation, feature manipulation, and sensitivity mapping, without any post-processing or human intervention after model training.1","",""
22,"Adam Kortylewski, Qing Liu, Angtian Wang, Yihong Sun, A. Yuille","Compositional Convolutional Neural Networks: A Robust and Interpretable Model for Object Recognition under Occlusion",2020,"","","","",166,"2022-07-13 09:30:38","","10.1007/s11263-020-01401-3","","",,,,,22,11.00,4,5,2,"","",""
472,"Yonatan Belinkov, Yonatan Bisk","Synthetic and Natural Noise Both Break Neural Machine Translation",2017,"","","","",167,"2022-07-13 09:30:38","","","","",,,,,472,94.40,236,2,5,"Character-based neural machine translation (NMT) models alleviate out-of-vocabulary issues, learn morphology, and move us closer to completely end-to-end translation systems. Unfortunately, they are also very brittle and easily falter when presented with noisy data. In this paper, we confront NMT models with synthetic and natural sources of noise. We find that state-of-the-art models fail to translate even moderately noisy texts that humans have no trouble comprehending. We explore two approaches to increase model robustness: structure-invariant word representations and robust training on noisy texts. We find that a model based on a character convolutional neural network is able to simultaneously learn representations robust to multiple kinds of noise.","",""
12,"O. Husain, E. B. Butler, M. Nayagam, L. Mango, A. Alonzo","An analysis of the variation of human interpretation: Papnet a mini-challenge.",1994,"","","","",168,"2022-07-13 09:30:38","","","","",,,,,12,0.43,2,5,28,"The Papnet was given a mini-challenge of 200 cervical smears loaded to 50% with varying degrees of abnormality as interpreted by the originating laboratory. The range of abnormality extended from 'atypia' to invasive cancer and a few 'glandular' lesions were included as were a few smears which had been reported as 'inadequate'. Three cytologists (two cytopathologists and one cytotechnologist) read and analysed the 128 monitor pictures per slide, selected by the Scanning Algorithm and Neural Network systems. These results were compared with a 'gold standard' report on the glass slide produced by two cytopathologists. The analysis was done for each individual cytologist, for cases in which all three agreed, for a consensus between two of the three and for the 'best of three'. The latter gave an error rate of 4% false negative (Papnet scan negative) and 10% 'false positive' (referred for glass slide examination). Individual cytologists had higher error rates demonstrating that errors could be due to human interaction and not necessarily to the Scanner. This also indicated that wide experience in interpretation of monitor images is needed to achieve high quality results.","",""
12,"Taro Langner, J. Wikström, T. Bjerner, H. Ahlström, J. Kullberg","Identifying Morphological Indicators of Aging With Neural Networks on Large-Scale Whole-Body MRI",2020,"","","","",169,"2022-07-13 09:30:38","","10.1109/TMI.2019.2950092","","",,,,,12,6.00,2,5,2,"A wealth of information is contained in images obtained by whole-body magnetic resonance imaging (MRI). Studying the link between the imaged anatomy and properties known from outside sources has the potential to give new insights into the underlying factors that manifest themselves in individual human morphology. In this work we investigate the expression of age-related changes in the whole-body image. A large dataset of about 32,000 subjects scanned from neck to knee and aged 44–82 years from the UK Biobank study was used for a machine-based analysis. We trained a convolutional neural network based on the VGG16 architecture to predict the age of a given subject based on image data from these scans. In 10-fold cross-validation on 23,000 of these images the network reached a mean absolute error (MAE) of 2.49 years (R2 = 0.83) and showed consistent performance on a separate test set of another 8,000 images. On a second test set of 100 images the network outperformed the averaged estimates given by three experienced radiologists, which reached an MAE of 5.58 years (R2 = 0.08), by more than three years on average. In an attempt to explain these findings, we employ saliency analysis that opens up the image-based criteria used by the automated method to human interpretation. We aggregate the saliency into a single anatomical visualization which clearly highlights structures in the aortic arch and knee as primary indicators of age.","",""
0,"Cihat Kirankaya, Latife Gorkemli Aykut","Training of artificial neural networks with the multi-population based artifical bee colony algorithm.",2022,"","","","",170,"2022-07-13 09:30:38","","10.1080/0954898X.2022.2062472","","",,,,,0,0.00,0,2,1,"Nowadays, artificial intelligence has gained recognition in every aspect of life. Artificial neural networks, one of the most efficient artificial intelligence techniques, is remarkably successful in computers' acquisition of the learning and interpretation capabilities of humans and attainment of meaningful results. Whether artificial intelligence networks can yield meaningful results is directly related to how the network is trained. The traditional algorithms, which are used to train artificial intelligence networks, do not always yield successful results in complicated problems and real-life problems. Metaheuristic algorithms are efficient techniques developed in order to figure out time-consuming and challenging problems fast and as optimally as possible. This study makes use of the artificial bee colony algorithm, which has been widely used recently in solving many kinds of problems so as to train artificial neural networks efficiently. Within this study, different strategies are used on subpopulations, so that the algorithm can search without getting tangled with the local optima. And also same and different parameter settings are considered for each population to reflect different search behaviours. The proposed approach was analysed through applied results of different data sets. The results yielded that the used strategies can be promising alternatives to the current search algorithms.","",""
0,"J. P. Mora, H. Bedle, K. Marfurt","Fault enhancement comparison between coherence enhancement, probabilistic neural networks, and convolutional neural networks in the Taranaki Basin area, New Zealand.",2022,"","","","",171,"2022-07-13 09:30:38","","10.1190/int-2021-0151.1","","",,,,,0,0.00,0,3,1,"Fault identification is a critical component of seismic interpretation. During the past 25 years, coherence, curvature, and other seismic attributes sensitive to faults improved seismic interpretation, but human interaction is still required to generate a complete fault interpretation. Today, image enhancement of fault-sensitive attributes, multiattribute fault analysis using shallow learning, and deep learning algorithms based on extensive training and convolutional neural networks are promising fault interpretation workflows. We compare three workflows to test fault detection capabilities, these include image enhancement, Probabilistic Neural Networks (PNN), and Convolutional Neural Networks (CNN). We compared results to human-interpreted faults as our ground truth for a merged 3D seismic survey acquired in the Taranaki Basin, New Zealand. We extracted fault surfaces from the results of the workflows using them as seed points for an active contour method. Extracted faults are then compared to the human-interpreted surface using the Hausdorff distance. Data conditioning, including spectral balancing and structure-oriented filtering, improved the performance of all three workflows. Although all three approaches produce enhanced fault volumes, we find differences in fault location and different artifacts (mispredicted faults). While all three methods exhibit ""false positive"" predictions, in addition, the enhanced multispectral coherence method produces faults and stratigraphic edges in the final image, including residual stair-step artifacts. In our implementation, PNN produces many salt-and-pepper artifacts through the resulting image, suggesting that we might need to include better training data or reduce the volume size to reduce the number of relevant classes in order to obtain an improved classification. The CNN algorithm trained with synthetic data that provides rapid results, correctly identifying larger faults, but missing smaller faults and, in some cases, misclassifying mass transport deposits and angular unconformities as being faults.","",""
73,"R. Williamson, Benjamin R. Cowley, Ashok Litwin-Kumar, B. Doiron, A. Kohn, M. A. Smith, Byron M. Yu","Scaling Properties of Dimensionality Reduction for Neural Populations and Network Models",2016,"","","","",172,"2022-07-13 09:30:38","","10.1371/journal.pcbi.1005141","","",,,,,73,12.17,10,7,6,"Recent studies have applied dimensionality reduction methods to understand how the multi-dimensional structure of neural population activity gives rise to brain function. It is unclear, however, how the results obtained from dimensionality reduction generalize to recordings with larger numbers of neurons and trials or how these results relate to the underlying network structure. We address these questions by applying factor analysis to recordings in the visual cortex of non-human primates and to spiking network models that self-generate irregular activity through a balance of excitation and inhibition. We compared the scaling trends of two key outputs of dimensionality reduction—shared dimensionality and percent shared variance—with neuron and trial count. We found that the scaling properties of networks with non-clustered and clustered connectivity differed, and that the in vivo recordings were more consistent with the clustered network. Furthermore, recordings from tens of neurons were sufficient to identify the dominant modes of shared variability that generalize to larger portions of the network. These findings can help guide the interpretation of dimensionality reduction outputs in regimes of limited neuron and trial sampling and help relate these outputs to the underlying network structure.","",""
352,"H. Venkatesh, W. Morishita, A. Geraghty, Dana Silverbush, Shawn M. Gillespie, M. Arzt, L. Tam, Cedric Espenel, Anitha Ponnuswami, Lijun Ni, Pamelyn J. Woo, Kathryn R. Taylor, A. Agarwal, A. Regev, D. Brang, H. Vogel, S. Hervey-Jumper, D. Bergles, M. Suvà, R. Malenka, M. Monje","Electrical and synaptic integration of glioma into neural circuits",2019,"","","","",173,"2022-07-13 09:30:38","","10.1038/s41586-019-1563-y","","",,,,,352,117.33,35,21,3,"","",""
55,"Dushyant Mehta, Oleksandr Sotnychenko, Franziska Mueller, Weipeng Xu, Mohamed A. Elgharib, P. Fua, H. Seidel, Helge Rhodin, Gerard Pons-Moll, C. Theobalt","XNect: Real-time Multi-person 3D Human Pose Estimation with a Single RGB Camera",2019,"","","","",174,"2022-07-13 09:30:38","","10.1145/3386569.3392410","","",,,,,55,18.33,6,10,3,"We present a real-time approach for multi-person 3D motion capture at over 30 fps using a single RGB camera. It operates in generic scenes and is robust to difficult occlusions both by other people and objects. Our method operates in subsequent stages. The first stage is a convolutional neural network (CNN) that estimates 2D and 3D pose features along with identity assignments for all visible joints of all individuals. We contribute a new architecture for this CNN, called SelecSLS Net, that uses novel selective long and short range skip connections to improve the information flow allowing for a drastically faster network without compromising accuracy. In the second stage, a fully-connected neural network turns the possibly partial (on account of occlusion) 2D pose and 3D pose features for each subject into a complete 3D pose estimate per individual. The third stage applies space-time skeletal model fitting to the predicted 2D and 3D pose per subject to further reconcile the 2D and 3D pose, and enforce temporal coherence. Our method returns the full skeletal pose in joint angles for each subject. This is a further key distinction from previous work that neither extracted global body positions nor joint angle results of a coherent skeleton in real time for multi-person scenes. The proposed system runs on consumer hardware at a previously unseen speed of more than 30 fps given 512x320 images as input while achieving state-of-the-art accuracy, which we will demonstrate on a range of challenging real-world scenes.","",""
33,"V. Nguyen, J. Starzyk, Wooi-Boon Goh, Daniel Jachyra","Neural Network Structure for Spatio-Temporal Long-Term Memory",2012,"","","","",175,"2022-07-13 09:30:38","","10.1109/TNNLS.2012.2191419","","",,,,,33,3.30,8,4,10,"This paper proposes a neural network structure for spatio-temporal learning and recognition inspired by the long-term memory (LTM) model of the human cortex. Our structure is able to process real-valued and multidimensional sequences. This capability is attained by addressing three critical problems in sequential learning, namely the error tolerance, the significance of sequence elements and memory forgetting. We demonstrate the potential of the framework with a series of synthetic simulations and the Australian sign language (ASL) dataset. Results show that our LTM model is robust to different types of distortions. Second, our LTM model outperforms other sequential processing models in a classification task for the ASL dataset.","",""
1,"C. Bormann, M. Kanakasabapathy, Prudhvi Thirumalaraju, I. Dimitriadis, I. Souter, K. Hammer, H. Shafiee","O-125 Development of an artificial intelligence embryo witnessing system to accurately track and identify patient specific embryos in a human IVF laboratory",2021,"","","","",176,"2022-07-13 09:30:38","","10.1093/humrep/deab126.050","","",,,,,1,1.00,0,7,1,"      Can convolutional neural networks (CNN) be used as a witnessing system to accurately track and identify patient specific embryos at the cleavage stage of development?        We developed the first artificial intelligence driven witnessing system to accurately track cleavage and blastocyst stage embryos in a human ART laboratory.        There are reports of human errors in embryo tracking that have led to the births of children with different genetic makeup than their birth parents. Clinical practices rely on manual identification, barcodes or radio-frequency identification technology to track embryos. These systems are designed to track culture dishes but are unable to monitor developing embryos within the dish to help ensure an error-free patient match. Previously, we developed an AI witnessing system to track blastocysts with 100% accuracy. The goal of this study was to determine whether an AI witnessing system could be developed that accurately tracks cleavage stage embryos.        A pre-developed deep neural network technology was first trained and tested on 4944 embryos images. The algorithm processed embryo images for each patient and produced a unique key that was associated with the patient ID at 60 hpi, which formed our library. When the algorithm evaluated embryos at 64 hpi it generated another key that was matched with the patient’s unique key available in the library.        A total of 3068 embryos from 412 patients were examined by the CNN at both 60 hpi and 64 hpi. These timepoints were chosen as they reflect the time our laboratory evaluates Day 3 embryos (60 hpi) and the time we move them to another dish and prepare them for transfer (64 hpi). The patient cohorts ranged from 3-12 embryos per patient.        The accuracy of the CNN in correctly matching the patient identification with the patient embryo cohort was 100% (CI: 99.1% to 100.0%, n = 412).        Limitations of this study include that all embryos were imaged under identical conditions and within the same EmbryoScope. Additionally, this study only examined fresh Day 3 embryos cultured over a span of 4 hours. Future studies should include images of fresh and frozen/thawed embryos captured using different imaging systems.        This study describes the first artificial intelligence-based approach for cleavage stage embryo tracking and patient specimen identification in the IVF laboratory. This technology offers a robust witnessing step based on unique morphological features that are specific to each individual embryo.        This work was partially supported by the Brigham Precision Medicine Developmental Award (Brigham Precision Medicine Program, Brigham and Women’s Hospital), Partners Innovation Discovery Grant (Partners Healthcare), and R01AI118502, and R01AI138800. ","",""
22,"Hongru Zhu, Peng Tang, A. Yuille","Robustness of Object Recognition under Extreme Occlusion in Humans and Computational Models",2019,"","","","",177,"2022-07-13 09:30:38","","","","",,,,,22,7.33,7,3,3,"Most objects in the visual world are partially occluded, but humans can recognize them without difficulty. However, it remains unknown whether object recognition models like convolutional neural networks (CNNs) can handle real-world occlusion. It is also a question whether efforts to make these models robust to constant mask occlusion are effective for real-world occlusion. We test both humans and the above-mentioned computational models in a challenging task of object recognition under extreme occlusion, where target objects are heavily occluded by irrelevant real objects in real backgrounds. Our results show that human vision is very robust to extreme occlusion while CNNs are not, even with modifications to handle constant mask occlusion. This implies that the ability to handle constant mask occlusion does not entail robustness to real-world occlusion. As a comparison, we propose another computational model that utilizes object parts/subparts in a compositional manner to build robustness to occlusion. This performs significantly better than CNN-based models on our task with error patterns similar to humans. These findings suggest that testing under extreme occlusion can better reveal the robustness of visual recognition, and that the principle of composition can encourage such robustness.","",""
79,"L. Petalidis, A. Oulas, Magnus Backlund, M. Wayland, Lu Liu, K. Plant, L. Happerfield, T. Freeman, P. Poirazi, V. P. Collins","Improved grading and survival prediction of human astrocytic brain tumors by artificial neural network analysis of gene expression microarray data",2008,"","","","",178,"2022-07-13 09:30:38","","10.1158/1535-7163.MCT-07-0177","","",,,,,79,5.64,8,10,14,"Histopathologic grading of astrocytic tumors based on current WHO criteria offers a valuable but simplified representation of oncologic reality and is often insufficient to predict clinical outcome. In this study, we report a new astrocytic tumor microarray gene expression data set (n = 65). We have used a simple artificial neural network algorithm to address grading of human astrocytic tumors, derive specific transcriptional signatures from histopathologic subtypes of astrocytic tumors, and asses whether these molecular signatures define survival prognostic subclasses. Fifty-nine classifier genes were identified and found to fall within three distinct functional classes, that is, angiogenesis, cell differentiation, and lower-grade astrocytic tumor discrimination. These gene classes were found to characterize three molecular tumor subtypes denoted ANGIO, INTER, and LOWER. Grading of samples using these subtypes agreed with prior histopathologic grading for both our data set (96.15%) and an independent data set. Six tumors were particularly challenging to diagnose histopathologically. We present an artificial neural network grading for these samples and offer an evidence-based interpretation of grading results using clinical metadata to substantiate findings. The prognostic value of the three identified tumor subtypes was found to outperform histopathologic grading as well as tumor subtypes reported in other studies, indicating a high survival prognostic potential for the 59 gene classifiers. Finally, 11 gene classifiers that differentiate between primary and secondary glioblastomas were also identified. [Mol Cancer Ther 2008;7(5):1013–24]","",""
66,"Z. Zheng, Pengyu Hong","Robust Detection of Adversarial Attacks by Modeling the Intrinsic Properties of Deep Neural Networks",2018,"","","","",179,"2022-07-13 09:30:38","","","","",,,,,66,16.50,33,2,4,"It has been shown that deep neural network (DNN) based classifiers are vulnerable to human-imperceptive adversarial perturbations which can cause DNN classifiers to output wrong predictions with high confidence. We propose an unsupervised learning approach to detect adversarial inputs without any knowledge of attackers. Our approach tries to capture the intrinsic properties of a DNN classifier and uses them to detect adversarial inputs. The intrinsic properties used in this study are the output distributions of the hidden neurons in a DNN classifier presented with natural images. Our approach can be easily applied to any DNN classifiers or combined with other defense strategy to improve robustness. Experimental results show that our approach demonstrates state-of-the-art robustness in defending black-box and gray-box attacks.","",""
8,"A. Nguyen, Jian Xu, Ming Jiang, D. K. Luu, Tong Wu, Wing-Kin Tam, Wenfeng Zhao, Markus W. Drealan, C. K. Overstreet, Qi Zhao, Jonathan Cheng, E. Keefer, Zhi Yang","A bioelectric neural interface towards intuitive prosthetic control for amputees.",2020,"","","","",180,"2022-07-13 09:30:38","","10.1088/1741-2552/abc3d3","","",,,,,8,4.00,1,13,2,"OBJECTIVE While prosthetic hands with independently actuated digits have become commercially available, state-of-the-art human-machine interfaces (HMI) only permit control over a limited set of grasp patterns, which does not enable amputees to experience sufficient improvement in their daily activities to make an active prosthesis useful.   APPROACH Here we present a technology platform combining fully-integrated bioelectronics, implantable intrafascicular microelectrodes and deep learning-based artificial intelligence (AI) to facilitate this missing bridge by tapping into the intricate motor control signals of peripheral nerves. The bioelectric neural interface includes an ultra-low-noise neural recording system to sense electroneurography (ENG) signals from microelectrode arrays implanted in the residual nerves, and AI models employing the recurrent neural network (RNN) architecture to decode the subject's motor intention.   MAIN RESULTS A pilot human study has been carried out on a transradial amputee. We demonstrate that the information channel established by the proposed neural interface is sufficient to provide high accuracy control of a prosthetic hand up to 15 degrees of freedom (DOF). The interface is intuitive as it directly maps complex prosthesis movements to the patient's true intention.   SIGNIFICANCE Our study layouts the foundation towards not only a robust and dexterous control strategy for modern neuroprostheses at a near-natural level approaching that of the able hand, but also an intuitive conduit for connecting human minds and machines through the peripheral neural pathways. (Clinical trial identifier: NCT02994160).","",""
10,"E. Orekhova, T. Stroganova, J. Schneiderman, S. Lundström, B. Riaz, D. Sarovic, O. Sysoeva, G. Brant, C. Gillberg, N. Hadjikhani","Neural gain control measured through cortical gamma oscillations is associated with sensory sensitivity",2018,"","","","",181,"2022-07-13 09:30:38","","10.1002/hbm.24469","","",,,,,10,2.50,1,10,4,"Gamma oscillations facilitate information processing by shaping the excitatory input/output of neuronal populations. Recent studies in humans and nonhuman primates have shown that strong excitatory drive to the visual cortex leads to suppression of induced gamma oscillations, which may reflect inhibitory‐based gain control of network excitation. The efficiency of the gain control measured through gamma oscillations may in turn affect sensory sensitivity in everyday life. To test this prediction, we assessed the link between self‐reported sensitivity and changes in magneto‐encephalographic gamma oscillations as a function of motion velocity of high‐contrast visual gratings. The induced gamma oscillations increased in frequency and decreased in power with increasing stimulation intensity. As expected, weaker suppression of the gamma response correlated with sensory hypersensitivity. Robustness of this result was confirmed by its replication in the two samples: neurotypical subjects and people with autism, who had generally elevated sensory sensitivity. We conclude that intensity‐related suppression of gamma response is a promising biomarker of homeostatic control of the excitation–inhibition balance in the visual cortex.","",""
18,"I. Turkoğlu, A. Arslan","An intelligent pattern recognition system based on neural network and wavelet decomposition for interpretation of heart sounds",2001,"","","","",182,"2022-07-13 09:30:38","","10.1109/IEMBS.2001.1020555","","",,,,,18,0.86,9,2,21,"We develop a new automated pattern recognition system for interpretation of heart sounds based on wavelet decomposition of signals and classification using neural network. Inputs to the system are the heart sound signals acquired by a stethoscope in a noiseless environment. We generate features for the objective concise representation of heart sound signals by means of wavelet decomposition. Classification of the features is performed using a back propagation neural network with adaptive learning rate. Two hundred record windows obtained from young humans are studied. One hundred of the record windows in the database are selected for use as a training phase for the neural network. The test results of the intelligent pattern recognition system for ten different types of heart sound signals displayed a high success.","",""
51,"Amirata Ghorbani, David Ouyang, Abubakar Abid, B. He, Jonathan H. Chen, R. Harrington, D. Liang, E. Ashley, James Y. Zou","Deep Learning Interpretation of Echocardiograms",2019,"","","","",183,"2022-07-13 09:30:38","","10.1101/681676","","",,,,,51,17.00,6,9,3,"Echocardiography uses ultrasound technology to capture high temporal and spatial resolution images of the heart and surrounding structures and is the most common imaging modality in cardiovascular medicine. Using convolutional neural networks on a large new dataset, we show that deep learning applied to echocardiography can identify local cardiac structures, estimate cardiac function, and predict systemic phenotypes that modify cardiovascular risk but not readily identifiable to human interpretation. Our deep learning model, EchoNet, accurately identified the presence of pacemaker leads (AUC = 0.89), enlarged left atrium (AUC = 0.85), normal left ventricular wall thickness (AUC = 0.75), left ventricular end systolic and diastolic volumes(R2 = 0.73 and R2 = 0.68), and ejection fraction (R2 = 0.48) as well as predicted systemic phenotypes of age (R2 = 0.46), sex (AUC = 0.88), weight (R2 = 0.56), and height (R2 = 0.33). Interpretation analysis validates that EchoNet shows appropriate attention to key cardiac structures when performing human-explainable tasks and highlight hypothesis-generating regions of interest when predicting systemic phenotypes difficult for human interpretation. Machine learning on echocardiography images can streamline repetitive tasks in the clinical workflow, standardize interpretation in areas with insufficient qualified cardiologists, and more consistently produce echocardiographic measurements.","",""
50,"Lei Shi, Zhiyang Teng, Le Wang, Yue Zhang, Alexander Binder","DeepClue: Visual Interpretation of Text-Based Deep Stock Prediction",2019,"","","","",184,"2022-07-13 09:30:38","","10.1109/TKDE.2018.2854193","","",,,,,50,16.67,10,5,3,"The recent advance of deep learning has enabled trading algorithms to predict stock price movements more accurately. Unfortunately, there is a significant gap in the real-world deployment of this breakthrough. For example, professional traders in their long-term careers have accumulated numerous trading rules, the myth of which they can understand quite well. On the other hand, deep learning models have been hardly interpretable. This paper presents DeepClue, a system built to bridge text-based deep learning models and end users through visually interpreting the key factors learned in the stock price prediction model. We make three contributions in DeepClue. First, by designing the deep neural network architecture for interpretation and applying an algorithm to extract relevant predictive factors, we provide a useful case on what can be interpreted out of the prediction model for end users. Second, by exploring hierarchies over the extracted factors and displaying these factors in an interactive, hierarchical visualization interface, we shed light on how to effectively communicate the interpreted model to end users. Specially, the interpretation separates the predictables from the unpredictables for stock prediction through the use of intercept model parameters and a risk visualization design. Third, we evaluate the integrated visualization system through two case studies in predicting the stock price with financial news and company-related tweets from social media. Quantitative experiments comparing the proposed neural network architecture with state-of-the-art models and the human baseline are conducted and reported. Feedbacks from an informal user study with domain experts are summarized and discussed in details. The study results demonstrate the effectiveness of DeepClue in helping to complete stock market investment and analysis tasks.","",""
45,"Pengfei Yang, Jiangchao Liu, Jianlin Li, Liqian Chen, Xiaowei Huang","Analyzing Deep Neural Networks with Symbolic Propagation: Towards Higher Precision and Faster Verification",2019,"","","","",185,"2022-07-13 09:30:38","","10.1007/978-3-030-32304-2_15","","",,,,,45,15.00,9,5,3,"","",""
480,"S. Bosse, D. Maniry, K. Müller, T. Wiegand, W. Samek","Deep Neural Networks for No-Reference and Full-Reference Image Quality Assessment",2016,"","","","",186,"2022-07-13 09:30:38","","10.1109/TIP.2017.2760518","","",,,,,480,80.00,96,5,6,"We present a deep neural network-based approach to image quality assessment (IQA). The network is trained end-to-end and comprises ten convolutional layers and five pooling layers for feature extraction, and two fully connected layers for regression, which makes it significantly deeper than related IQA models. Unique features of the proposed architecture are that: 1) with slight adaptations it can be used in a no-reference (NR) as well as in a full-reference (FR) IQA setting and 2) it allows for joint learning of local quality and local weights, i.e., relative importance of local quality to the global quality estimate, in an unified framework. Our approach is purely data-driven and does not rely on hand-crafted features or other types of prior domain knowledge about the human visual system or image statistics. We evaluate the proposed approach on the LIVE, CISQ, and TID2013 databases as well as the LIVE In the wild image quality challenge database and show superior performance to state-of-the-art NR and FR IQA methods. Finally, cross-database evaluation shows a high ability to generalize between different databases, indicating a high robustness of the learned features.","",""
525,"David R Ha, D. Eck","A Neural Representation of Sketch Drawings",2017,"","","","",187,"2022-07-13 09:30:38","","","","",,,,,525,105.00,263,2,5,"We present sketch-rnn, a recurrent neural network (RNN) able to construct stroke-based drawings of common objects. The model is trained on thousands of crude human-drawn images representing hundreds of classes. We outline a framework for conditional and unconditional sketch generation, and describe new robust training methods for generating coherent sketch drawings in a vector format.","",""
20,"A. Khasnobish, A. Jati, Garima Singh, S. Bhattacharyya, A. Konar, D. Tibarewala, Eunjin Kim, A. Nagar","Object-shape recognition from tactile images using a feed-forward neural network",2012,"","","","",188,"2022-07-13 09:30:38","","10.1109/IJCNN.2012.6252593","","",,,,,20,2.00,3,8,10,"The sense of touch is an extremely important sensory system in the human body which helps to understand object shape, texture, hardness in the world around us. Incorporating artificial haptic sensory systems in rehabilitative aids and in various other human computer interfaces is a thrust area of research presently. This paper presents a novel approach of shape recognition and classification from the tactile pressure images by touching the surface of various real life objects. Here four objects (viz. a planar surface, object with one edge, a cuboid i.e. object with two edges and a cylindrical object) are used for shape recognition. The obtained tactile pressure images of the object surfaces are subjected to segmentation, edge detection and a mapping procedure to finally reconstruct the particular object shapes. The reconstructed images are used as features. The processed tactile pressure images are classified with feed- forward neural network (FFNN) using extracted features. The classifier performance is tested with different signal-to-noise (SNR) ratios. Is is observed that classifier accuracy decreases with decrease in SNR, but at SNR value 6 i.e. when the noise power is one sixth of the signal power, the mean classification accuracy of the classifier is 88%. This shows the robustness of feed-forward neural network in the classification purpose. The performance of FFNN is compared with four classifiers (Linear Discriminant Analysis, Linear Support vector machine, Radial Basis Function SVM, k-Nearest Neighbor). FFNN performed best acquiring first rank with a average classification accuracy of 94.0%.","",""
965,"Pin-Yu Chen, Huan Zhang, Yash Sharma, Jinfeng Yi, Cho-Jui Hsieh","ZOO: Zeroth Order Optimization Based Black-box Attacks to Deep Neural Networks without Training Substitute Models",2017,"","","","",189,"2022-07-13 09:30:38","","10.1145/3128572.3140448","","",,,,,965,193.00,193,5,5,"Deep neural networks (DNNs) are one of the most prominent technologies of our time, as they achieve state-of-the-art performance in many machine learning tasks, including but not limited to image classification, text mining, and speech processing. However, recent research on DNNs has indicated ever-increasing concern on the robustness to adversarial examples, especially for security-critical tasks such as traffic sign identification for autonomous driving. Studies have unveiled the vulnerability of a well-trained DNN by demonstrating the ability of generating barely noticeable (to both human and machines) adversarial images that lead to misclassification. Furthermore, researchers have shown that these adversarial images are highly transferable by simply training and attacking a substitute model built upon the target model, known as a black-box attack to DNNs. Similar to the setting of training substitute models, in this paper we propose an effective black-box attack that also only has access to the input (images) and the output (confidence scores) of a targeted DNN. However, different from leveraging attack transferability from substitute models, we propose zeroth order optimization (ZOO) based attacks to directly estimate the gradients of the targeted DNN for generating adversarial examples. We use zeroth order stochastic coordinate descent along with dimension reduction, hierarchical attack and importance sampling techniques to efficiently attack black-box models. By exploiting zeroth order optimization, improved attacks to the targeted DNN can be accomplished, sparing the need for training substitute models and avoiding the loss in attack transferability. Experimental results on MNIST, CIFAR10 and ImageNet show that the proposed ZOO attack is as effective as the state-of-the-art white-box attack (e.g., Carlini and Wagner's attack) and significantly outperforms existing black-box attacks via substitute models.","",""
307,"Robert Geirhos, Carlos R. Medina Temme, Jonas Rauber, H. Schütt, M. Bethge, Felix Wichmann","Generalisation in humans and deep neural networks",2018,"","","","",190,"2022-07-13 09:30:38","","10.15496/PUBLIKATION-30814","","",,,,,307,76.75,51,6,4,"We compare the robustness of humans and current convolutional deep neural networks (DNNs) on object recognition under twelve different types of image degradations. First, using three well known DNNs (ResNet-152, VGG-19, GoogLeNet) we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and DNNs when the signal gets weaker. Secondly, we show that DNNs trained directly on distorted images consistently surpass human performance on the exact distortion types they were trained on, yet they display extremely poor generalisation abilities when tested on other distortion types. For example, training on salt-and-pepper noise does not imply robustness on uniform white noise and vice versa. Thus, changes in the noise distribution between training and testing constitutes a crucial challenge to deep learning vision systems that can be systematically addressed in a lifelong machine learning approach. Our new dataset consisting of 83K carefully measured human psychophysical trials provide a useful reference for lifelong robustness against image degradations set by the human visual system.","",""
64,"Andras Rozsa, Manuel Günther, T. Boult","Towards Robust Deep Neural Networks with BANG",2016,"","","","",191,"2022-07-13 09:30:38","","10.1109/WACV.2018.00093","","",,,,,64,10.67,21,3,6,"Machine learning models, including state-of-the-art deep neural networks, are vulnerable to small perturbations that cause unexpected classification errors. This unexpected lack of robustness raises fundamental questions about their generalization properties and poses a serious concern for practical deployments. As such perturbations can remain imperceptible – the formed adversarial examples demonstrate an inherent inconsistency between vulnerable machine learning models and human perception – some prior work casts this problem as a security issue. Despite the significance of the discovered instabilities and ensuing research, their cause is not well understood and no effective method has been developed to address the problem. In this paper, we present a novel theory to explain why this unpleasant phenomenon exists in deep neural networks. Based on that theory, we introduce a simple, efficient, and effective training approach, Batch Adjusted Network Gradients (BANG), which significantly improves the robustness of machine learning models. While the BANG technique does not rely on any form of data augmentation or the utilization of adversarial images for training, the resultant classifiers are more resistant to adversarial perturbations while maintaining or even enhancing the overall classification performance.","",""
0,"Nian Chi Tay, T. Ong","Tay, NC and Connie, T and Ong, TS and Goh, KOM and Teh, PS (2019) A robust abnormal behavior detection method using convolutional neural net-",2020,"","","","",192,"2022-07-13 09:30:38","","","","",,,,,0,0.00,0,2,2,"A behavior is considered abnormal when it is seen as unusual under certain contexts. The definition for abnormal behavior varies depending on situations. For example, people running in a field is considered normal but is deemed abnormal if it takes place in a mall. Similarly, loitering in the alleys, fighting or pushing each other in public areas are considered abnormal under specific circumstances. Abnormal behavior detection is crucial due to the increasing crime rate in the society. If an abnormal behavior can be detected earlier, tragedies can be avoided. In recent years, deep learning has been widely applied in the computer vision field and has acquired great success for human detection. In particular, Convolutional Neural Network (CNN) has shown to have achieved state-ofthe-art performance in human detection. In this paper, a CNN-based abnormal behavior detection method is presented. The proposed approach automatically learns the most discriminative characteristics pertaining to human behavior from a large pool of videos containing normal and abnormal behaviors. Since the interpretation for abnormal behavior varies across contexts, extensive experiments have been carried out to assess various conditions and scopes including crowd and single person behavior detection and recognition. The proposed method represents an end-to-end solution to deal with abnormal behavior under different conditions including variations in background, number of subjects (individual, two persons or crowd), and a range of diverse unusual human activities. Experiments on five benchmark datasets validate the performance of the proposed approach.","",""
95,"W. Samek, Alexander Binder, S. Lapuschkin, K. Müller","Understanding and Comparing Deep Neural Networks for Age and Gender Classification",2017,"","","","",193,"2022-07-13 09:30:38","","10.1109/ICCVW.2017.191","","",,,,,95,19.00,24,4,5,"Recently, deep neural networks have demonstrated excellent performances in recognizing the age and gender on human face images. However, these models were applied in a black-box manner with no information provided about which facial features are actually used for prediction and how these features depend on image preprocessing, model initialization and architecture choice. We present a study investigating these different effects. In detail, our work compares four popular neural network architectures, studies the effect of pretraining, evaluates the robustness of the considered alignment preprocessings via cross-method test set swapping and intuitively visualizes the model's prediction strategies in given preprocessing conditions using the recent Layer-wise Relevance Propagation (LRP) algorithm. Our evaluations on the challenging Adience benchmark show that suitable parameter initialization leads to a holistic perception of the input, compensating artefactual data representations. With a combination of simple preprocessing steps, we reach state of the art performance in gender recognition.","",""
293,"Changxing Ding, D. Tao","Trunk-Branch Ensemble Convolutional Neural Networks for Video-Based Face Recognition",2016,"","","","",194,"2022-07-13 09:30:38","","10.1109/TPAMI.2017.2700390","","",,,,,293,48.83,147,2,6,"Human faces in surveillance videos often suffer from severe image blur, dramatic pose variations, and occlusion. In this paper, we propose a comprehensive framework based on Convolutional Neural Networks (CNN) to overcome challenges in video-based face recognition (VFR). First, to learn blur-robust face representations, we artificially blur training data composed of clear still images to account for a shortfall in real-world video training data. Using training data composed of both still images and artificially blurred data, CNN is encouraged to learn blur-insensitive features automatically. Second, to enhance robustness of CNN features to pose variations and occlusion, we propose a Trunk-Branch Ensemble CNN model (TBE-CNN), which extracts complementary information from holistic face images and patches cropped around facial components. TBE-CNN is an end-to-end model that extracts features efficiently by sharing the low- and middle-level convolutional layers between the trunk and branch networks. Third, to further promote the discriminative power of the representations learnt by TBE-CNN, we propose an improved triplet loss function. Systematic experiments justify the effectiveness of the proposed techniques. Most impressively, TBE-CNN achieves state-of-the-art performance on three popular video face databases: PaSC, COX Face, and YouTube Faces. With the proposed techniques, we also obtain the first place in the BTAS 2016 Video Person Recognition Evaluation.","",""
196,"B. Mišić, R. Betzel, M. D. de Reus, M. P. van den Heuvel, M. Berman, A. McIntosh, O. Sporns","Network-Level Structure-Function Relationships in Human Neocortex",2016,"","","","",195,"2022-07-13 09:30:38","","10.1093/cercor/bhw089","","",,,,,196,32.67,28,7,6,"The dynamics of spontaneous fluctuations in neural activity are shaped by underlying patterns of anatomical connectivity. While numerous studies have demonstrated edge-wise correspondence between structural and functional connections, much less is known about how large-scale coherent functional network patterns emerge from the topology of structural networks. In the present study, we deploy a multivariate statistical technique, partial least squares, to investigate the association between spatially extended structural networks and functional networks. We find multiple statistically robust patterns, reflecting reliable combinations of structural and functional subnetworks that are optimally associated with one another. Importantly, these patterns generally do not show a one-to-one correspondence between structural and functional edges, but are instead distributed and heterogeneous, with many functional relationships arising from nonoverlapping sets of anatomical connections. We also find that structural connections between high-degree hubs are disproportionately represented, suggesting that these connections are particularly important in establishing coherent functional networks. Altogether, these results demonstrate that the network organization of the cerebral cortex supports the emergence of diverse functional network configurations that often diverge from the underlying anatomical substrate.","",""
83,"Will Monroe, Robert D. Hawkins, Noah D. Goodman, Christopher Potts","Colors in Context: A Pragmatic Neural Model for Grounded Language Understanding",2017,"","","","",196,"2022-07-13 09:30:38","","10.1162/tacl_a_00064","","",,,,,83,16.60,21,4,5,"We present a model of pragmatic referring expression interpretation in a grounded communication task (identifying colors from descriptions) that draws upon predictions from two recurrent neural network classifiers, a speaker and a listener, unified by a recursive pragmatic reasoning framework. Experiments show that this combined pragmatic model interprets color descriptions more accurately than the classifiers from which it is built, and that much of this improvement results from combining the speaker and listener perspectives. We observe that pragmatic reasoning helps primarily in the hardest cases: when the model must distinguish very similar colors, or when few utterances adequately express the target color. Our findings make use of a newly-collected corpus of human utterances in color reference games, which exhibit a variety of pragmatic behaviors. We also show that the embedded speaker model reproduces many of these pragmatic behaviors.","",""
161,"A. Bhagoji, Daniel Cullina, Chawin Sitawarin, Prateek Mittal","Enhancing robustness of machine learning systems via data transformations",2017,"","","","",197,"2022-07-13 09:30:38","","10.1109/CISS.2018.8362326","","",,,,,161,32.20,40,4,5,"We propose the use of data transformations as a defense against evasion attacks on ML classifiers. We present and investigate strategies for incorporating a variety of data transformations including dimensionality reduction via Principal Component Analysis to enhance the resilience of machine learning, targeting both the classification and the training phase. We empirically evaluate and demonstrate the feasibility of linear transformations of data as a defense mechanism against evasion attacks using multiple real-world datasets. Our key findings are that the defense is (i) effective against the best known evasion attacks from the literature, resulting in a two-fold increase in the resources required by a white-box adversary with knowledge of the defense for a successful attack, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification and human activity classification.","",""
3,"Eman Shalaby, Nada M. Elshennawy, A. Sarhan","Utilizing deep learning models in CSI-based human activity recognition",2022,"","","","",198,"2022-07-13 09:30:38","","10.1007/s00521-021-06787-w","","",,,,,3,3.00,1,3,1,"","",""
267,"Xuanqing Liu, Minhao Cheng, Huan Zhang, Cho-Jui Hsieh","Towards Robust Neural Networks via Random Self-ensemble",2017,"","","","",199,"2022-07-13 09:30:38","","10.1007/978-3-030-01234-2_23","","",,,,,267,53.40,67,4,5,"","",""
1,"Qixian Lv, Xinrong Yuan, Jinzhao Qian, Xinke Li, Haiyan Zhang, Shu Zhan","An Improved U-Net for Human Sperm Head Segmentation",2021,"","","","",200,"2022-07-13 09:30:38","","10.1007/s11063-021-10643-2","","",,,,,1,1.00,0,6,1,"","",""
