Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
34,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, Semeen Rehman, M. Shafique","Robust Machine Learning Systems: Reliability and Security for Deep Neural Networks",2018,"","","","",1,"2022-07-13 09:40:27","","10.1109/IOLTS.2018.8474192","","",,,,,34,8.50,7,5,4,"Machine learning is commonly being used in almost all the areas that involve advanced data analytics and intelligent control. From applications like Natural Language Processing (NLP) to autonomous driving are based upon machine learning algorithms. An increasing trend is observed in the use of Deep Neural Networks (DNNs) for such applications. While the slight inaccuracy in applications like NLP does not have any severe consequences, it is not the same for other safety-critical applications, like autonomous driving and smart healthcare, where a small error can lead to catastrophic effects. Apart from high-accuracy DNN algorithms, there is a significant need for robust machine learning systems and hardware architectures that can generate reliable and trustworthy results in the presence of hardware-level faults while also preserving security and privacy. This paper provides an overview of the challenges being faced in ensuring reliable and secure execution of DNNs. To address the challenges, we present several techniques for analyzing and mitigating the reliability and security threats in machine learning systems.","",""
1,"M. Staron, Helena Odenstedt Herg'es, S. Naredi, L. Block, Ali El-Merhi, Richard Vithal, M. Elam","Robust Machine Learning in Critical Care — Software Engineering and Medical Perspectives",2021,"","","","",2,"2022-07-13 09:40:27","","10.1109/WAIN52551.2021.00016","","",,,,,1,1.00,0,7,1,"Using machine learning in clinical practice poses hard requirements on explainability, reliability, replicability and robustness of these systems. Therefore, developing reliable software for monitoring critically ill patients requires close collaboration between physicians and software engineers. However, these two different disciplines need to find own research perspectives in order to contribute to both the medical and the software engineering domain. In this paper, we address the problem of how to establish a collaboration where software engineering and medicine meets to design robust machine learning systems to be used in patient care. We describe how we designed software systems for monitoring patients under carotid endarterectomy, in particular focusing on the process of knowledge building in the research team. Our results show what to consider when setting up such a collaboration, how it develops over time and what kind of systems can be constructed based on it. We conclude that the main challenge is to find a good research team, where different competences are committed to a common goal.","",""
0,"Eric J. Wyers, Weiyi Qi, P. Franzon","A robust calibration and supervised machine learning reliability framework for digitally-assisted self-healing RFICs",2017,"","","","",3,"2022-07-13 09:40:27","","10.1109/MWSCAS.2017.8053129","","",,,,,0,0.00,0,3,5,"A robust calibration and supervised machine learning reliability framework has been developed to aid the circuit designer in the design and implementation of reliable digitally-reconfigurable self-healing RFICs. For calibration algorithm performance and reliability validation, we advocate the use of surrogate modeling, a supervised machine learning technique, which offers a significant reduction in the required computational complexity relative to relying solely on the execution of expensive circuit simulations. An RF phase rotator test case is used to show the robustness and utility of the developed self-healing reliability framework.","",""
1,"P. Pitathawatchai, Sitthichok Chaichulee, V. Kirtsreesakul","Robust machine learning method for imputing missing values in audiograms collected in children",2021,"","","","",4,"2022-07-13 09:40:27","","10.1080/14992027.2021.1884909","","",,,,,1,1.00,0,3,1,"Abstract Objective To assess the accuracy and reliability of a machine learning (ML) algorithm for predicting the full audiograms of hearing-impaired children relative to the common approach (CA). Design Retrospective study Study sample There were 206 audiograms included from 206 children with sensorineural hearing loss. Nested cross-validation was used for evaluating the performance of the CA and ML. Six audiogram prediction simulations were performed in which either one or two thresholds across 0.5–4 kHz from complete audiograms in the dataset were labelled. Missing thresholds at the remaining frequencies were then predicted using the CA and ML in each simulation. The accuracy of the ML algorithm was determined by comparing the median average absolute threshold differences between the CA and ML using Wilcoxon signed-rank test. The reliability between runs of the ML was also assessed with Cronbach’s alphas. Results The median average absolute threshold differences in ML (5–8 dBHL) were statistically significantly lower than those in CA (6.25–10 dBHL) in all six simulations (p value < 0.05). The ML algorithm was also found to be reliable to predict the audiograms in all six simulations (α > 0.9). Conclusion Using the ML to predict the children’s audiograms was reliable and more accurate than using the CA.","",""
23,"J. Zhang, Kang Liu, Faiq Khalid, Muhammad Abdullah Hanif, Semeen Rehman, T. Theocharides, Alessandro Artussi, M. Shafique, S. Garg","Building Robust Machine Learning Systems: Current Progress, Research Challenges, and Opportunities",2019,"","","","",5,"2022-07-13 09:40:27","","10.1145/3316781.3323472","","",,,,,23,7.67,3,9,3,"Machine learning, in particular deep learning, is being used in almost all the aspects of life to facilitate humans, specifically in mobile and Internet of Things (IoT)-based applications. Due to its state-of-the-art performance, deep learning is also being employed in safety-critical applications, for instance, autonomous vehicles. Reliability and security are two of the key required characteristics for these applications because of the impact they can have on human's life. Towards this, in this paper, we highlight the current progress, challenges and research opportunities in the domain of robust systems for machine learning-based applications.","",""
1,"Natalia F. Espinoza-Sepulveda, J. Sinha","Robust vibration-based faults diagnosis machine learning model for rotating machines to enhance plant reliability",2021,"","","","",6,"2022-07-13 09:40:27","","10.21595/mrcm.2021.22110","","",,,,,1,1.00,1,2,1,"Plant availability and reliability can be improved through a robust condition monitoring and fault diagnosis model to predict the current status (healthy or faulty) of any machines and critical assets. The model can then predict the exact fault for the faulty asset so that remedial maintenance can be carried out in a planned plant outage. Nowadays, the artificial intelligence (AI)-based machine learning (ML) model seems to be current trend to meet these requirements. Hence, the paper is also proposing such vibration-based faults diagnosis ML model through an experimental rotating rig. Here, the 2-Steps approach is used with the ML model to easy the industrial operation and maintenance process. The Step-1 provides the information about the asset health status such as healthy or faulty. The Step-2 then identifies the exact nature of fault to aid the decision making for the fault rectification and maintenance activities to avoid the risk of failure and enhance the reliability.","",""
1,"J. Zhang, Kang Liu, Faiq Khalid, Muhammad Abdullah Hanif, Semeen Rehman, T. Theocharides, Alessandro Artussi, M. Shafique, S. Garg","INVITED: Building Robust Machine Learning Systems: Current Progress, Research Challenges, and Opportunities",2019,"","","","",7,"2022-07-13 09:40:27","","","","",,,,,1,0.33,0,9,3,"Machine learning, in particular deep learning, is being used in almost all the aspects of life to facilitate humans, specifically in mobile and Internet of Things (IoT)-based applications. Due to its state-of-the-art performance, deep learning is also being employed in safety-critical applications, for instance, autonomous vehicles. Reliability and security are two of the key required characteristics for these applications because of the impact they can have on human’s life. Towards this, in this paper, we highlight the current progress, challenges and research opportunities in the domain of robust systems for machine learning-based applications.","",""
3,"Qi Wu, Weiqi Chen, Chen Yu, Haiming Wang, W. Hong","Multilayer Machine Learning-Assisted Optimization-Based Robust Design and Its Applications to Antennas and Array",2020,"","","","",8,"2022-07-13 09:40:27","","10.36227/techrxiv.12681947.v1","","",,,,,3,1.50,1,5,2,"An efficient multilayer machine learning-assisted optimization (ML-MLAO)-based robust design method is proposed for antenna and array applications. Machine learning methods are introduced into multiple layers of the robust design process, including worst-case analysis (WCA), maximum input tolerance hypervolume (MITH) searching, and robust optimization, considerably accelerating the whole robust design process. First, based on a surrogate model mapping between the design parameters and performance, the WCA is performed using a genetic algorithm to ensure reliability. MITH searching is then carried out using an MLAO-based framework to find the MITH of the given design point. Next, based on the training set obtained using MITH searching, correlations between the design parameters and the MITH are learned. The robust design is carried out using surrogate models for both the performance and the MITH, and these models are updated online following the ML-MLAO scheme. Finally, two examples, including an array synthesis problem and an antenna design problem, are used to verify the proposed ML-MLAO method. The numerical results and computation time are discussed to demonstrate the effectiveness of the proposed method.","",""
1,"Tanvi Sharma, Cheng Wang, Amogh Agrawal, K. Roy","Enabling Robust SOT-MTJ Crossbars for Machine Learning using Sparsity-Aware Device-Circuit Co-design",2021,"","","","",9,"2022-07-13 09:40:27","","10.1109/ISLPED52811.2021.9502492","","",,,,,1,1.00,0,4,1,"Embedded non-volatile memory (eNVM) based crossbars have emerged as energy-efficient building blocks for machine learning accelerators. However, the analog computations in crossbars introduce errors due to several non-idealities. Moreover, since communications between crossbars are usually done in the digital domain, the energy and area costs are dominated by the Analog-to-Digital Converters (ADC). Among the eNVM technologies, Resistive Random-Access-Memory (RRAM) and Phase-Change Memory (PCM) devices suffer from poor endurance, Write variability and conductance drift. Whereas magneto-resistive technologies provide superior endurance, write stability and reliability. To that effect, we propose sparsity-aware device/circuit co-design of robust crossbars using Spin-Orbit-Torque Magnetic Tunnel Junctions (SOT-MTJs). Note, standard MTJs have low $\mathrm{R}_{\mathrm{O}\mathrm{F}\mathrm{F}}/\mathrm{R}_{\mathrm{O}\mathrm{N}}$ and low $\mathrm{R}_{\mathrm{O}\mathrm{N}}$, making them unsuitable for crossbars. In this work, we first demonstrate SOT-MTJs as crossbar elements With high $\mathrm{R}_{\mathrm{O}\mathrm{N}}$ and high $\mathrm{R}_{\mathrm{O}\mathrm{F}\mathrm{F}}/\mathrm{R}_{\mathrm{O}\mathrm{N}}$ by allowing the read-path to have thicker tunneling-barrier, leaving the write path undisturbed. Second, through extensive simulations, we quantitatively assess the impact of various device-circuit parameters such as $\mathrm{R}_{\mathrm{O}\mathrm{N}}, \mathrm{R}_{\mathrm{O}\mathrm{F}\mathrm{F}}/\mathrm{R}_{\mathrm{O}\mathrm{N}}$ ratio, crossbar size, along With input and weight sparsity, on both circuit and application level accuracy and energy consumption. We evaluate system accuracy for Resnet-20 inference on CIFAR-10 dataset and show that leveraging sparsity allows reduced ADC precision, Without degrading accuracy. Our results show that an SOT-MTJ $(\mathrm{R}_{\mathrm{O}\mathrm{N}}=200\mathrm{k}\Omega$ and $\mathrm{R}_{\mathrm{O}\mathrm{F}\mathrm{F}}/\mathrm{R}_{\mathrm{O}\mathrm{N}}=7)$ crossbar array of size 32×32 could achieve near-software accuracy. The 64×64 and 128×128 crossbars show an accuracy degradation of 2% and 9.8%, respectively, from the software accuracy and an energy improvement of upto 3.8× and 6.3× compared to a 32×32 array with 4bit-ADC.","",""
1,"Oleksandra Razim, S. Cavuoti, M. Brescia, G. Riccio, M. Salvato, G. Longo","Improving the reliability of photometric redshift with machine learning",2021,"","","","",10,"2022-07-13 09:40:27","","10.1093/mnras/stab2334","","",,,,,1,1.00,0,6,1,"  In order to answer the open questions of modern cosmology and galaxy evolution theory, robust algorithms for calculating photometric redshifts (photo-z) for very large samples of galaxies are needed. Correct estimation of the various photo-z algorithms’ performance requires attention to both the performance metrics and the data used for the estimation. In this work, we use the supervised machine learning algorithm MLPQNA (Multi-Layer Perceptron with Quasi-Newton Algorithm) to calculate photometric redshifts for the galaxies in the COSMOS2015 catalogue and the unsupervised Self-Organizing Maps (SOM) to determine the reliability of the resulting estimates. We find that for zspec < 1.2, MLPQNA photo-z predictions are on the same level of quality as spectral energy distribution fitting photo-z. We show that the SOM successfully detects unreliable zspec that cause biases in the estimation of the photo-z algorithms’ performance. Additionally, we use SOM to select the objects with reliable photo-z predictions. Our cleaning procedures allow us to extract the subset of objects for which the quality of the final photo-z catalogues is improved by a factor of 2, compared to the overall statistics.","",""
1,"Masoomeh Jasemi, S. Hessabi, N. Bagherzadeh","Enhancing Reliability of Emerging Memory Technology for Machine Learning Accelerators",2021,"","","","",11,"2022-07-13 09:40:27","","10.1109/tetc.2020.2984992","","",,,,,1,1.00,0,3,1,"An efficient and reliable Multi-Level Cell (MLC) Spin-Transfer Torque Random Access Memory (STT-RAM) is proposed based on a <italic><underline>D</underline>rop-<underline>A</underline>nd-<underline>R</underline>earrange <underline>A</underline>pproach</italic>, called <italic>DARA</italic>. Since CNN models are rather robust, less important bits are dropped, allowing important bits to be written in safe and reliable Single-Level Cell mode. Also, bits are <italic>rearranged</italic> to make the representation better aligned with memory cell characteristics. Bits with higher impact on the features value are stored in safer bit positions reducing the chance of read/write circuits to malfunction. Experimental results show that our approach provides comparable to error-free scenario reliability level, while doubling the bandwidth and maintaining error rate of less than 0.02 percent.","",""
106,"Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, V. Larivière, A. Beygelzimer, Florence d'Alché-Buc, E. Fox, H. Larochelle","Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",2020,"","","","",12,"2022-07-13 09:40:27","","","","",,,,,106,53.00,13,8,2,"One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.","",""
0,"A. V. Nikolaeva, S. Ulyanov","Intelligent robust control of redundant smart robotic arm Pt I: Soft computing KB optimizer - deep machine learning IT",2020,"","","","",13,"2022-07-13 09:40:27","","10.30564/AIA.V2I1.1339","","",,,,,0,0.00,0,2,2,"Article history Received: 29 October 2019 Accepted: 8 April 2020 Published Online: 15 April 2020 Redundant robotic arm models as a control object discussed. Background of computational intelligence IT on soft computing optimizer of knowledge base in smart robotic manipulators introduced. Soft computing optimizer is the sophisticated computational intelligence toolkit of deep machine learning SW platform with optimal fuzzy neural network structure. The methods for development and design technology of control systems based on soft computing introduced in this Part 1 allow one to implement the principle of design an optimal intelligent control systems with a maximum reliability and controllability level of a complex control object under conditions of uncertainty in the source data, and in the presence of stochastic noises of various physical and statistical characters. The knowledge bases formed with the application of soft computing optimizer produce robust control laws for the schedule of time dependent coefficient gains of conventional PID controllers for a wide range of external perturbations and are maximally insensitive to random variations of the structure of control object. The robustness is achieved by application a vector fitness function for genetic algorithm, whose one component describes the physical principle of minimum production of generalized entropy both in the control object and the control system, and the other components describe conventional control objective functionals such as minimum control error, etc. The application of soft computing technologies (Part I) for the development a robust intelligent control system that solving the problem of precision positioning redundant (3DOF and 7 DOF) manipulators considered. Application of quantum soft computing in robust intelligent control of smart manipulators in Part II described.","",""
0,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, M. Teimoori, F. Kriebel, Jeff Zhang, Kang Liu, Semeen Rehman, T. Theocharides, A. Artusi, S. Garg, M. Shafique","Robust Computing for Machine Learning-Based Systems",2020,"","","","",14,"2022-07-13 09:40:27","","10.1007/978-3-030-52017-5_20","","",,,,,0,0.00,0,12,2,"","",""
1,"Shaomin Wu, Di Wu, R. Peng","Machine learning approaches in reliability and maintenance: classifications of recent literature",2020,"","","","",15,"2022-07-13 09:40:27","","10.1109/APARM49247.2020.9209392","","",,,,,1,0.50,0,3,2,"Reliability and maintenance (R&M) engineering is conventionally notorious for a lack of sufficient failure data to develop robust statistical models. The increasing miniaturization of data collection devices such as wireless sensors has provided a promising infrastructure for gathering information about parameters of the physical systems, which enable practitioners and researchers to apply machine learning (ML) algorithms to improve the efficiency of R&M analysis. The number of published papers on ML in R&M is enormous, this paper will therefore categorizes those papers that were published between 2017 to 16/May/2020, that are written in English, that have received a top 5% number of citations in the year published, and that use support vector methods, random forests, and cluster analysis.","",""
3,"A. Gholami, H. Bonakdari, I. Ebtehaj, S. Khodashenas","Reliability and sensitivity analysis of robust learning machine in prediction of bank profile morphology of threshold sand rivers",2020,"","","","",16,"2022-07-13 09:40:27","","10.1016/j.measurement.2019.107411","","",,,,,3,1.50,1,4,2,"","",""
2,"J. Sarkar, Cory Peterson","Operational Workload Impact on Robust Solid-State Storage Analyzed with Interpretable Machine Learning",2019,"","","","",17,"2022-07-13 09:40:27","","10.1109/IRPS.2019.8720510","","",,,,,2,0.67,1,2,3,"Solid-state storage technology is finding increasing adoption in enterprise and data center environments due to their high reliability and reducing cost. With high performance solid-state storage devices (SSDs) internally designed as distributed resilient systems, their operational behavior under materially different workloads is described in this research. Application of interpretable machine learning on internal parametric data of SSDs enables insights on workloads' interaction with the resilient system design. After prior research demonstrated significantly different accelerated workload stress, the analysis on resilience of the SSDs under random vs. pseudo-sequential workloads emphasize the efficacy and importance of their distributed resilience schemes. As such, these results provide causational insights on the mechanism of differential stress of the workloads impacting the resilience design principles. Moreover, the results elucidate guidelines strongly relevant from design robustness perspective for research on novel SSD architectures such as the proposed Open Channel SSD, towards deployment in hyperscale and virtualization environments.","",""
8,"Anke Wilm, Conrad Stork, C. Bauer, A. Schepky, J. Kühnl, J. Kirchmair","Skin Doctor: Machine Learning Models for Skin Sensitization Prediction that Provide Estimates and Indicators of Prediction Reliability",2019,"","","","",18,"2022-07-13 09:40:27","","10.3390/ijms20194833","","",,,,,8,2.67,1,6,3,"The ability to predict the skin sensitization potential of small organic molecules is of high importance to the development and safe application of cosmetics, drugs and pesticides. One of the most widely accepted methods for predicting this hazard is the local lymph node assay (LLNA). The goal of this work was to develop in silico models for the prediction of the skin sensitization potential of small molecules that go beyond the state of the art, with larger LLNA data sets and, most importantly, a robust and intuitive definition of the applicability domain, paired with additional indicators of the reliability of predictions. We explored a large variety of molecular descriptors and fingerprints in combination with random forest and support vector machine classifiers. The most suitable models were tested on holdout data, on which they yielded competitive performance (Matthews correlation coefficients up to 0.52; accuracies up to 0.76; areas under the receiver operating characteristic curves up to 0.83). The most favorable models are available via a public web service that, in addition to predictions, provides assessments of the applicability domain and indicators of the reliability of the individual predictions.","",""
0,"J. Sarkar, Cory Peterson","Enabling Prognostics of Robust Design with Interpretable Machine Learning",2019,"","","","",19,"2022-07-13 09:40:27","","10.1109/IEDM19573.2019.8993481","","",,,,,0,0.00,0,2,3,"Design of robust systems needs to fully account for reliability physics, operational stresses and interactions thereof - while accommodating range of stresses from qualification to field. This research demonstrates the method of empirically analyzing system-internal parametric data of Solid-State Storage devices (SSD) with Machine Learning (ML). ML is shown to be a necessary, effective and novel means of proactively assessing and interpreting prognostics of the resilient system design. The methodologies and results also bear strong relevance to assessment of current and future designs for evolving usage models and new application areas.","",""
2,"Alfio Di Mauro, Francesco Conti, Pasquale Davide Schiavone, D. Rossi, L. Benini","Pushing On-chip Memories Beyond Reliability Boundaries in Micropower Machine Learning Applications",2019,"","","","",20,"2022-07-13 09:40:27","","10.1109/IEDM19573.2019.8993434","","",,,,,2,0.67,0,5,3,"Memory access dominates inference energy in today’s Deep Neural Network (DNN) accelerators. We analyze voltage over-scaling for on-chip memories and explore the trade-off between energy efficiency and reliability for robust and computationally efficient deep Binary Neural Networks (BNNs). Experimental results on a BNN accelerator fabricated in FDX22 technology with on-chip SRAMs powered down to 0.4V (well below the 0.8V nominal Vdd) demonstrate major energy efficiency improvements (2.3x) at negligible end-to-end classification accuracy degradation.","",""
1,"Tianzhe Bao, Shengquan Xie, Pengfei Yang, P. Zhou, Zhiqiang Zhang","Towards Robust, Adaptive and Reliable Upper-limb Motion Estimation Using Machine Learning and Deep Learning--A Survey in Myoelectric Control.",2022,"","","","",21,"2022-07-13 09:40:27","","10.1109/JBHI.2022.3159792","","",,,,,1,1.00,0,5,1,"To develop multi-functional human-machine interfaces that can help disabled people reconstruct lost functions of upper-limbs, machine learning (ML) and deep learning (DL) techniques have been widely implemented to decode human movement intentions from surface electromyography (sEMG) signals. However, due to the high complexity of upper-limb movements and the inherent non-stable characteristics of sEMG, the usability of ML/DL based control schemes is still greatly limited in practical scenarios. To this end, tremendous efforts have been made to improve model robustness, adaptation, and reliability. In this article, we provide a systematic review on recent achievements, mainly from three categories: multi-modal sensing fusion to gain additional information of the user, transfer learning (TL) methods to eliminate domain shift impacts on estimation models, and post-processing approaches to obtain more reliable outcomes. Special attention is given to fusion strategies, deep TL frameworks, and confidence estimation. \textcolor{red}{Research challenges and emerging opportunities, with respect to hardware development, public resources, and decoding strategies, are also analysed to provide perspectives for future developments.","",""
0,"Rajat Sadhukhan","A Classical and Machine Learning-Based Reliability Analysis on Catalan Object Encryption Scheme",2022,"","","","",22,"2022-07-13 09:40:27","","10.1109/tr.2022.3156478","","",,,,,0,0.00,0,1,1,"Designing lightweight secure cryptographic schemes for Internet of Things (IoT) and radio frequency identification (RFID)-based devices is challenging as a designer needs to address resource-constraints along with physical and classical security notions. Even though plethora of such lightweight encryption schemes have been proposed in the literature, computationally efficient attacks on some has also been published, which stresses the fact that robust and reliable design paradigms to consider during design process. In this article, using classical methods, we launched the full-key attack on a Catalan-object-based encryption scheme proposed by Saracevic et al. (2020) in IEEE Transactions on Reliability. Their proposed encryption scheme is efficient and lightweight specifically meant for IoT applications and is based upon combinatorial structure of Catalan key objects. We proved data privacy violation and recover the full encryption-key with computationally efficient algorithm. We also proposed the machine-learning-based regression model to efficiently predict all bits of ciphertext using just a single plaintext without any key. A high correlation between plaintext and ciphertext and full-key recovery poses the encryption scheme to be unreliable to be used for IoT-based applications. To the best of authors’ knowledge, this is the first work on reliability analysis on Catalan-based encryption schemes.","",""
9,"Yanbin Li, G. Lei, G. Bramerdorfer, S. Peng, Xiaodong Sun, Jianguo Zhu","Machine Learning for Design Optimization of Electromagnetic Devices: Recent Developments and Future Directions",2021,"","","","",23,"2022-07-13 09:40:27","","10.3390/APP11041627","","",,,,,9,9.00,2,6,1,"This paper reviews the recent developments of design optimization methods for electromagnetic devices, with a focus on machine learning methods. First, the recent advances in multi-objective, multidisciplinary, multilevel, topology, fuzzy, and robust design optimization of electromagnetic devices are overviewed. Second, a review is presented to the performance prediction and design optimization of electromagnetic devices based on the machine learning algorithms, including artificial neural network, support vector machine, extreme learning machine, random forest, and deep learning. Last, to meet modern requirements of high manufacturing/production quality and lifetime reliability, several promising topics, including the application of cloud services and digital twin, are discussed as future directions for design optimization of electromagnetic devices.","",""
2,"C. Parmar","Machine learning applications for Radiomics : towards robust non-invasive predictors in clinical oncology",2017,"","","","",24,"2022-07-13 09:40:27","","","","",,,,,2,0.40,2,1,5,"In this big-data era, like every other field, healthcare is also turning towards artificial intelligence (AI) and machine-learning (ML). In this thesis, state-of-the-art machine-learning methods were investigated for radiomic analyses. An unbiased evaluation of these advanced computational methods in terms of their accuracy and reliability is presented. Identification of optimal machine-learning methods for radiomic applications is a crucial step towards stable and clinically relevant radiomic biomarkers, providing a non-invasive way of quantifying and monitoring tumor-phenotypic characteristics in clinical practice. With ever increasing patient specific data, this work could stimulate further research towards brining AI and precision medicine in routine clinical oncology.","",""
4,"Xinyu Li, R. Chiong, A. Page","Group and Period-Based Representations for Improved Machine Learning Prediction of Heterogeneous Alloy Catalysts.",2021,"","","","",25,"2022-07-13 09:40:27","","10.1021/acs.jpclett.1c01319","","",,,,,4,4.00,1,3,1,"Machine learning has recently emerged as an efficient and powerful alternative to density functional theory for studying heterogeneous catalysis. Machine learning methods rely on a geometrical representation of the chemical environment around the catalytic adsorption site based on physical or chemical descriptors. Here, we show that replacing the atomic number in geometrical representations with elemental groups and periods (GP) yields significant improvements in predicted adsorption energies on bimetallic alloy surfaces. Notably, the GP-based Labeled Site Crystal Graph representation reported here achieves mean absolute error (MAE) ∼0.05 eV (near chemical accuracy) in predicting hydrogen adsorption and MAE ∼0.10 eV for other strong binding adsorbates such as carbon, nitrogen, oxygen, and sulfur. We also show GP-based representations to be robust in predicting adsorption on surface facets, elements, and alloys that are not included in the initial training set. This reliability makes GP-based representations an ideal basis for high-throughput approaches and materials discovery based on active learning techniques, which often involve limited training sets.","",""
1,"Shadan Ghaffaripour, A. Miri","Mutually Private Verifiable Machine Learning As-a-service: A Distributed Approach",2021,"","","","",26,"2022-07-13 09:40:27","","10.1109/AIIoT52608.2021.9454203","","",,,,,1,1.00,1,2,1,"Reliability is a crucial component to machine-learning-as-a-service platforms, as more and more critical applications depend on them. Thus, mechanisms employed to assure the integrity of computations performed on such platforms are pivotal to their robust functioning. Moreover, privacy protection, and performance guarantee at scale, are other major challenges surrounding these platforms that are by no means straightforward to overcome at the same time. In this paper, we have proposed a novel distributed approach, which uses specialized composable proof systems at its core, to respond to these challenges. At a high level, we adopt a divide-and-conquer approach to build efficient proof systems for machine-learning-based services in order to ensure the correctness of results. More precisely, the mathematical formulation of the machine learning task is divided into multiple parts, each of which is handled by a different specialized proof system; these proof systems are then combined with the commit-and-prove methodology to guarantee correctness as a whole. With privacy safeguards built into the design, our approach also assures that neither user data nor model parameters, which constitute the intellectual property of service providers are exposed in the process. We have showcased the usability of our approach within a machine learning service provider that offers classification services through a linear support vector machine (SVM) model. Our complexity analysis indicates that our system could be used in practical settings.","",""
1,"Aparna Harichandran, B. Raphael, Abhijit Mukherjee","A hierarchical machine learning framework for the identification of automated construction",2021,"","","","",27,"2022-07-13 09:40:27","","10.36680/j.itcon.2021.031","","",,,,,1,1.00,0,3,1,"A robust monitoring system is essential for ensuring safety and reliability in automated construction. Activity recognition is one of the critical tasks in automated monitoring. Existing studies in this area have not fully exploited the potential for enhancing the performance of machine learning algorithms using domain knowledge, especially in problem formulation. This paper presents a hierarchical machine learning framework for improving the accuracy of identification of Automated Construction System (ACS) operations. The proposed identification framework arranges the operations to be identified in the form of a hierarchy and uses multiple classifiers that are organized hierarchically for separating the operation classes. It is tested on a laboratory prototype of an ACS, which follows a top-down construction method. The ACS consists of a set of lightweight and portable machinery designed to automate the construction of the structural frame of low-rise buildings . Accelerometers were deployed at critical locations on the structure. The acceleration data collected while operating the equipment were used to identify the operations through machine learning techniques. The performance of the proposed framework is compared with that of the conventional approach for equipment operation identification which involves a flat list of classes to be separated. The performance was comparable at the top level. However, the hierarchical framework outperformed the conventional one when fine levels of operations were identified. The versatility and noise tolerance of the hierarchical framework are also reported. Results demonstrate that the framework is robust, and it is feasible to identify the ACS operations precisely. Although the proposed framework is validated on a full-scale prototype of the ACS, the effects of strong ambient disturbances on actual construction sites have not been evaluated. This study will support the development of an automated monitoring system and assist the main operator to ensure safe operations. The high-level operation details collected for this purpose can also be utilised for project performance assessment and progress monitoring. The potential application of the proposed hierarchical framework in the operation recognition of conventional construction equipment is also outlined.","",""
2,"J. C. Wu, M. Rau, Yun Zhang, Yijun Zhou, M. Wright","Towards robust tracking with an unreliable motion sensor using machine learning",2017,"","","","",28,"2022-07-13 09:40:27","","","","",,,,,2,0.40,0,5,5,"This paper presents solutions to improve the reliability and to work around the challenges of using a Leap MotionTM sensor as a gestural control and input device in digital music instrument (DMI) design. We implement supervised learning algorithms, including k-nearest neighbors, support vector machine, binary decision tree, and artificial neural network, to estimate hand motion data, which is not typically captured by the sensor. Two problems are addressed: 1) the sensor is unable to detect overlapping hands 2) The sensor’s limited detection range. Training examples included 7 kinds of overlapping hand gestures as well as hand trajectories where a hand goes out of the sensor’s range. The overlapping gestures were treated as a classification problem and the best performing model was k-nearest neighbors with 62% accuracy. The out-of-range problem was treated first as a clustering problem to group the training examples into a small number of trajectory types, then as a classification problem to predict trajectory type based on the hand’s motion before going out of range. The best performing model was k-nearest neighbors with an accuracy of 30%. The prediction models were implemented in an ongoing multimedia electroacoustic vocal performance and educational project named Embodied Sonic Meditation (ESM).","",""
0,"Vedpal, N. Chauhan","Role of Machine Learning in Software Testing",2021,"","","","",29,"2022-07-13 09:40:27","","10.1109/ISCON52037.2021.9702427","","",,,,,0,0.00,0,2,1,"Software reliability and robustness is the main objective to perform testing of the software. Now the machine learning approaches are used to develop applications in almost every area like health care, business prediction, etc. The testing of such type of software by using the conventional testing techniques does not give promising results. The testing techniques based on machine learning helps to produce reliable and robust software within time and allocated budget. In this paper the role of the machine learning algorithms in the design of testing approaches for testing the software has been presented. The machine learning-based testing techniques are used in the generation and execution order of the test cases. The outcomes of the machine learning based techniques showing the efficacy as compared to the other techniques.","",""
0,"Md. Nazmul Hasan, Insoo Koo","Machine Learning-Based Sensor Drift Fault Classification using Discrete Cosine Transform",2021,"","","","",30,"2022-07-13 09:40:27","","10.1109/ICECIT54077.2021.9641210","","",,,,,0,0.00,0,2,1,"In fourth industrial revolution sensors are playing a crucial role as they provide a real time health and operating conditions of a physical system through continuous real time data. With this real time data in hand, robust and intelligent condition monitoring systems are being implemented through machine learning and deep learning techniques. However, the reliability of the sensor data is critical for good condition monitoring system. Many sensors operate in harsh environment; thus, the sensor signals can be affected by various faults. In this paper we propose a hybrid approach of sensor drift fault classification which involves discrete cosine transform (DCT) for extracting features from time domain sensor signals and in the later stage machine learning algorithms are used to build classification models. Experimental results showed that our proposed DCT-based feature selection method when combined with common machine learning models can exhibit excellent classification accuracy which is above 97% for most common machine learning models.","",""
0,"Tochukwu Idika, Ismail Akturk","Attack-Centric Approach for Evaluating Transferability of Adversarial Samples in Machine Learning Models",2021,"","","","",31,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,2,1,"Transferability of adversarial samples became a serious concern due to their impact on the reliability of machine learning system deployments, as they find their way into many critical applications. Knowing factors that influence transferability of adversarial samples can assist experts to make informed decisions on how to build robust and reliable machine learning systems. The goal of this study is to provide insights on the mechanisms behind the transferability of adversarial samples through an attack-centric approach. This attack-centric perspective interprets how adversarial samples would transfer by assessing the impact of machine learning attacks (that generated them) on a given input dataset. To achieve this goal, we generated adversarial samples using attacker models and transferred these samples to victim models. We analyzed the behavior of adversarial samples on victim models and outlined four factors that can influence the transferability of adversarial samples. Although these factors are not necessarily exhaustive, they provide useful insights to researchers and practitioners of machine learning systems.","",""
0,"Anirudh Itagi, S. Krishvadana, K. Bharath, M. Rajesh Kumar","FPGA Architecture To Enhance Hardware Acceleration for Machine Learning Applications",2021,"","","","",32,"2022-07-13 09:40:27","","10.1109/ICCMC51019.2021.9418015","","",,,,,0,0.00,0,4,1,"Many algorithms have been developed in the field of Machine learning and its sub-fields such as neural networks, Deep learning and so on, for applications such as pattern classification, image and video processing, statistical data mining and so forth. These algorithms perform such tasks with remarkable accuracy. However, when implemented in traditional processor core software, many of these algorithms get choked when the size of the network or computational demand of the algorithm scales up. FPGAs and ASICs offer higher computation capability, throughput and bandwidth. Features such as massive parallel processing, high performance and reliability, are strong arguments for FPGAs for Deep Neural Network applications. FPGAs offer reconfigurable, flexible architectures where even the most popular GPUs fall short. This paper proposes a robust, re-configurable architecture for deploying Machine learning algorithms and presents its advantages by implementing a Neural Network in an FPGA and comparing its results with an implementation in a Raspberry Pi.","",""
0,"Omar Y. López-Rico, R. Ramírez-Chavarría","Smart Seismocardiography: A Machine Learning Approach for Automatic Data Processing",2021,"","","","",33,"2022-07-13 09:40:27","","10.3390/ecsa-8-11325","","",,,,,0,0.00,0,2,1,"Seismocardiography (SCG) is a non-invasive method that measures local vibrations created by the mechanical cardiovascular exercises on the chest wall. Thereby, mechanical movements of the heart are recorded in real-time from vibration sensors positioned on the chest of the subject, to further compute the heart rate and retrieve the SCG waveform. Although such events have been widely studied, robust signal processing methods remain a challenging task. On the other hand, the use of piezoelectric sensors has been favored in recent years due to its features and low cost. However, robust data processing techniques should be developed to increase their performance and reliability. In this work, we propose an attractive method for SCG data processing based on the K-Means clustering algorithm to automatically label waveform events. Interestingly, the SCG signals are recovered from a custom-made device built around an ultra-low-cost piezoelectric sensor. Once the signals are measured, they are pre-processed by spectral filtering. Afterwards, the signal spectrum is used to compute the heart rate (HR). Thereby, the filtered signal is sequentially segmented, and every frame is processed by a light-weight K-Means algorithm. Finally, we show the performance of the smart seismocardiography by analyzing SCG waveforms at different physiological conditions.","",""
15,"P. Krishnakumar, K. Rameshkumar, K. I. Ramachandran","Feature level fusion of vibration and acoustic emission signals in tool condition monitoring using machine learning classifiers",2020,"","","","",34,"2022-07-13 09:40:27","","10.36001/IJPHM.2018.V9I1.2694","","",,,,,15,7.50,5,3,2,"To implement the tool condition monitoring system in a metal cutting process, it is necessary to have sensors which will be able to detect the tool conditions to initiate remedial action. There are different signals for monitoring the cutting process which may require different sensors and signal processing techniques. Each of these signals is capable of providing information about the process at different reliability level. To arrive a good, reliable and robust decision, it is necessary to integrate the features of the different signals captured by the sensors. In this paper, an attempt is made to fuse the features of acoustic emission and vibration signals captured in a precision high speed machining center for monitoring the tool conditions. Tool conditions are classified using machine learning classifiers. The classification efficiency of machine learning algorithms are studied in time-domain, frequencydomain and time-frequency domain by feature level fusion of features extracted from vibration and acoustic emission signature.","",""
9,"Wajid Hassan, T. Chou, Omar Tamer, John Pickard, Patrick Appiah-Kubi, L. Pagliari","Cloud computing survey on services, enhancements and challenges in the era of machine learning and data science",2020,"","","","",35,"2022-07-13 09:40:27","","10.11591/IJICT.V9I2.PP117-139","","",,,,,9,4.50,2,6,2,"Cloud computing has sweeping impact on the human productivity. Today it’s used for Computing, Storage, Predictions and Intelligent Decision Making, among others. Intelligent Decision Making using Machine Learning has pushed for the Cloud Services to be even more fast, robust and accurate. Security remains one of the major concerns which affect the cloud computing growth however there exist various research challenges in cloud computing adoption such as lack of well managed service level agreement (SLA), frequent disconnections, resource scarcity, interoperability, privacy, and reliability. Tremendous amount of work still needs to be done to explore the security challenges arising due to widespread usage of cloud deployment using Containers. We also discuss Impact of Cloud Computing and Cloud Standards. Hence in this research paper, a detailed survey of cloud computing, concepts, architectural principles, key services, and implementation, design and deployment challenges of cloud computing are discussed in detail and important future research directions in the era of Machine Learning and Data Science have been identified.","",""
6,"Max Scheerer, Jonas Klamroth, Ralf H. Reussner, Bernhard Beckert","Towards classes of architectural dependability assurance for machine-learning-based systems",2020,"","","","",36,"2022-07-13 09:40:27","","10.1145/3387939.3388613","","",,,,,6,3.00,2,4,2,"Advances in Machine Learning (ML) have brought previously hard to handle problems within arm's reach. However, this power comes at the cost of unassured reliability and lacking transparency. Overcoming this drawback is very hard due to the probabilistic nature of ML. Current approaches mainly tackle this problem by developing more robust learning procedures. Such algorithmic approaches, however, are limited to certain types of uncertainties and cannot deal with all of them, e.g., hardware failure. This paper discusses how this problem can be addressed at architectural rather than algorithmic level to assess systems dependability properties in early development stages. Moreover, we argue that Self-Adaptive Systems (SAS) are more suited to safeguard ML w.r.t. various uncertainties. As a step towards this we propose classes of dependability in which ML-based systems may be categorized and discuss which and how assurances can be made for each class.","",""
6,"Liwei Song, Vikash Sehwag, A. Bhagoji, Prateek Mittal","A Critical Evaluation of Open-World Machine Learning",2020,"","","","",37,"2022-07-13 09:40:27","","","","",,,,,6,3.00,2,4,2,"Open-world machine learning (ML) combines closed-world models trained on in-distribution data with out-of-distribution (OOD) detectors, which aim to detect and reject OOD inputs. Previous works on open-world ML systems usually fail to test their reliability under diverse, and possibly adversarial conditions. Therefore, in this paper, we seek to understand how resilient are state-of-the-art open-world ML systems to changes in system components? With our evaluation across 6 OOD detectors, we find that the choice of in-distribution data, model architecture and OOD data have a strong impact on OOD detection performance, inducing false positive rates in excess of $70\%$. We further show that OOD inputs with 22 unintentional corruptions or adversarial perturbations render open-world ML systems unusable with false positive rates of up to $100\%$. To increase the resilience of open-world ML, we combine robust classifiers with OOD detection techniques and uncover a new trade-off between OOD detection and robustness.","",""
4,"J. Sarkar, Cory Peterson, A. Sanayei","Machine-learned assessment and prediction of robust solid state storage system reliability physics",2018,"","","","",38,"2022-07-13 09:40:27","","10.1109/IRPS.2018.8353565","","",,,,,4,1.00,1,3,4,"Reliability physics of the complex memory sub-system of modern, robust solid state storage devices (SSDs) under throughput acceleration stress is analyzed leveraging Machine Learning — towards understanding their inherently designed fault-tolerance schemes that mitigate expected memory degradation mechanisms through reliable life as a system. With the strength of multiple designed error-management schemes effectively countering multiple memory degradation mechanisms under stress, the developed empirical data based Machine Learning framework allows inferential and predictive assessments on reliable SSD design at system-level in a quantitative and pro-active manner. Such Machine-Learned quantitative assessments on the system-level health of individual devices can be utilized towards managing qualification reliability assessments, assessing dynamic throughput stress impact on design and/or decision-making on reliability of individual and populations of solid-state storage systems.","",""
5,"K. Varshney","On Mismatched Detection and Safe, Trustworthy Machine Learning",2020,"","","","",39,"2022-07-13 09:40:27","","10.1109/CISS48834.2020.1570627767","","",,,,,5,2.50,5,1,2,"Instilling trust in high-stakes applications of machine learning is becoming essential. Trust may be decomposed into four dimensions: basic accuracy, reliability, human interaction, and aligned purpose. The first two of these also constitute the properties of safe machine learning systems. The second dimension, reliability, is mainly concerned with being robust to epistemic uncertainty and model mismatch. It arises in the machine learning paradigms of distribution shift, data poisoning attacks, and algorithmic fairness. All of these problems can be abstractly modeled using the theory of mismatched hypothesis testing from statistical signal processing. By doing so, we can take advantage of performance characterizations in that literature to better understand the various machine learning issues.","",""
3,"Jessica Dai","Label Bias, Label Shift: Fair Machine Learning with Unreliable Labels",2020,"","","","",40,"2022-07-13 09:40:27","","","","",,,,,3,1.50,3,1,2,"Most supervised learning problems assume that the data available for training is well-representative of the data on which the model will be deployed. Distribution shift is a fairly robust subfield within machine learning more broadly, but there has been little work that integrates findings from the distribution shift literature to fair machine learning, despite an increasing interest in and awareness of long-term dynamics created by fair machine learning systems. In settings where fairness is a concern, there are additional reasons for training data to be unrepresentative of the true data: the available data may have been generated through a process reflecting discrimination, such as the systematic mislabeling of positive examples from a specific group. In this work, we build upon work in both fairness and distribution shift to examine the performance of fair machine learning models when the reliability of labels is uncertain and dynamic, focusing on label bias as the bias model, and label shift as the mechanism of distribution shift. First, we present a framework for approaching and understanding distribution shift problems in the context of fairness. Then, motivated by real-world needs, we consider two scenarios for distribution shift: (i) no access to new labeled data, only new model inputs; and (ii) access to new (potentially-biased) labels. Our experimental results suggest that the combination of distribution shift and label shift may be a plausible failure mode for fair algorithms, indicating the relationship between distribution shift and learned fair models is an important area of continued study.","",""
2,"Martin Gascon, Nikhil Kumar, R. Ghosh","Predicting Power Plant Equipment Life Using Machine Learning",2020,"","","","",41,"2022-07-13 09:40:27","","10.1115/1.4044939","","",,,,,2,1.00,1,3,2,"  There are new challenges for plant operators due to the increased share of renewable energy. Plant operators must maintain high reliability and high profits while plants are being required to be more flexible to compensate for the variable generation addition of these renewables into the grid. Plant operators must deal with the thermal strain and the wear-and-tear of such operations. Various models have been proposed in the literature. However, no work has been reported on the development of a robust prediction model. The aim of this study was to determine which machine learning algorithm gives the best estimation of boiler component remaining useful life using plant operations. The flexible operation for all units was estimated using the Intertek hourly MW analysis and damage modeling software Loads Model™. We used several plant features as predictors (such as equipment manufacturer, operating regime, and ramp rates). We tested five different machine learning techniques and found that gradient boost is the best approach to predict the reduction in life span of the plant with over 90% precision.","",""
1,"Korn Sooksatra, Pablo Rivas","A Review of Machine Learning and Cryptography Applications",2020,"","","","",42,"2022-07-13 09:40:27","","10.1109/CSCI51800.2020.00105","","",,,,,1,0.50,1,2,2,"Adversarially robust neural cryptography deals with the training of a neural-based model using an adversary to leverage the learning process in favor of reliability and trustworthiness. The adversary can be a neural network or a strategy guided by a neural network. These mechanisms are proving successful in finding secure means of data protection. Similarly, machine learning benefits significantly from the cryptography area by protecting models from being accessible to malicious users. This paper is a literature review on the symbiotic relationship between machine learning and cryptography. We explain cryptographic algorithms that have been successfully applied in machine learning problems and, also, deep learning algorithms that have been used in cryptography. We pay special attention to the exciting and relatively new area of adversarial robustness.","",""
1,"Sona D Solanki, Asha D Solanki","Review of Deployment of Machine Learning in Blockchain Methodology",2020,"","","","",43,"2022-07-13 09:40:27","","10.47392/irjash.2020.141","","",,,,,1,0.50,1,2,2,"The evolution of blockchain methodology has been a remarkable, highly transformative and trend-setting platform in current years. BT's accessible platform reinforces data protection and confidentiality. In addition, the consensus framework in it ensures system is protected and accurate. Nevertheless, it introduces additional security challenges such as invasion by the majority and double consumption. Data analysis on encrypted data centered on blockchain is crucial to manage the existing challenges. Insights on these results elevates the value of emerging of Machine Learning technique. It covers the fair quantity of data needed to make specific choices. Consistency of data and its distribution are very critical in ML to increase findings reliability. The fusion of these two techniques will produce extremely accurate outcomes. In this article, we describe a thorough analysis on ML implementation to make smart applications based on BT further robust to threats. There are numerous standard ML approaches such as Support Vector Machines (SVM), Clustering, Bagging, and Deep Learning (DL) algorithms such as Convolutional Neural Network (CNN) and Long-Term Memory (LSTM) that can be employed to evaluate the threats on a block chain network. Finally, we discuss how two different techniques can be implemented in a number of smart applications like Unmanned Aerial Vehicle (UAV), Smart Grid (SG), medical care and Smart cities.","",""
4,"Constantine E. Kontokosta","DataIQ – A Machine Learning Approach to Anomaly Detection for Energy Performance Data Quality and Reliability",2016,"","","","",44,"2022-07-13 09:40:27","","","","",,,,,4,0.67,4,1,6,"The paper develops and tests a strategy for evaluating and improving the data quality and analysis of benchmarking data from privatelyand publicly-owned buildings. We utilize machine learning tools to support the District of Columbia District Department of the Environment (DDOE) in improving the quality of building energy data collected through its mandatory building rating and disclosure policy. Using energy disclosure data from calendar year 2013, this paper presents a new data quality rating algorithm and grading system – known as the Data Integrity and Quality (DataIQ) score – to rank the relative reliability of reported data and provide a tool to improve the identification and prediction of data quality concerns going forward. We apply non-parametric statistical anomaly detection techniques to identify data quality and reliability concerns in self-reported building energy benchmarking data. This approach creates a foundation for more robust and precise analysis of city energy data that can provide policymakers with additional insight to improve the reliability of building benchmarking data analysis and inform the design of data-driven energy efficiency policies. INTRODUCTION The District of Columbia and the City of New York were the first two jurisdictions to adopt mandatory benchmarking and disclosure laws (City of New York 2012, Hsu 2014 Kontokosta 2012, 2013). These laws require large buildings (in both cities, buildings over 50,000 gross square feet) to annually benchmark their energy and water performance using the U.S. Environmental Protection Agency’s Energy Star Portfolio Manager software, and report the results to the cities, which make the data available online. Mandatory benchmarking and Disclosure programs have three fundamental purposes: (1) to provide building owners & managers with better information about the efficiency of their own properties, and how those properties compare to local and national peers; (2) to drive market transformation by allowing market actors to easily compare the performance of properties when leasing, buying, or investing; and (3) to provide policy-makers and program administrators with better information for planning, program design, and targeting. (Hart 2015; Keicher et al. 2012). All of these uses depend fundamentally on the reliability of the reported data (Hsu 2014; Kontokosta 2013; 2015; Palmer and Walls 2015). Building owners, managers, and other market actors must have confidence in the reliability of the benchmarking data, and subsequent analysis, in order to make decisions that save money and energy. And cities, utilities, and contractors must have confidence in the reliability of the data to design policies and programs that address market needs and drive change. If the market comes to question the quality of benchmarking data, the resulting uncertainty will undermine the potential for data-driven market transformation and a precipitate a loss of trust among decision-makers that could be difficult to regain. Thus, both actual or objective and perceived or contextual data reliability are needed to move from the superficial availability of information to proactive decisions based directly on insights derived 12-1 ©2016 ACEEE Summer Study on Energy Efficiency in Buildings from it (Wang and Strong 1996). For this reason, improving the completeness and quality of the benchmarking data is a very high priority of many benchmarking programs, including the District’s. In the implementation of the benchmarking law, the District of Columbia Department of Energy and Environment (DOEE), which implements the benchmarking law in Washington, DC, has found common self-reported data errors. In general, we can classify these errors with respect to the completeness of the data, its consistency, and compliance rate (Pipino, Lee, and Wang 2002). Building on the benchmarking data quality issues in Kontokosta (2013), the types of data errors specific to building energy disclosure data can be divided into five categories: 1. Fatal Errors: User error that leads to a report being submitted without any metrics, such as Gross Floor Area, Energy Use Intensity (EUI), Water Use Intensity (WUI). These errors are usually caused by the respondent not entering in complete meter data for the whole year or setting the “active date” for meter or space use values incorrectly; 2. Energy Data: Inaccurate or incomplete energy consumption information due to either data collection errors (not reporting all the energy meters for the building), data entry error (e.g. incorrect units), or utility company errors (incorrect billing or aggregation); 3. Floor Area: Inaccurate square footage based on either the use of unverified tax data square footage, which can often be wrong, or incorrect understanding of what spaces in the building need to be included (e.g. net vs. gross square footage); 4. Space Use: Inaccurate space use attributes due to data entry errors, confusion about the requirements, or the use of default values; and 5. ENERGY STAR Scoring Issues: Problems in the methodology of ENERGY STAR Portfolio Manager itself (e.g. changes in site-to-source energy ratios, or use of outdated or insufficient reference data sets). Fatal errors can be minimized when cities work directly with building owners to correct these (relatively) simple user errors. In DC, 25% of initial disclosure reports contained this type of error; however, the District has reduced this rate to just 3% through compliance assistance and enforcement. Starting in late 2015, DC is now enforcing compliance standards for observed fatal errors, considering them to be equivalent to non-submittal. The fifth error category problems with Energy Star Portfolio Manager is an important area for academic investigation, but beyond the scope of this paper (for additional information, see Kontokosta 2015 and Hsu 2014). Moreover, the value for city governments of using a federally-supported, industry-standard tool that is common across jurisdictions is an important consideration when evaluating the lack of local control and methodological concerns about the Portfolio Manager software itself. A fundamental concern of cities trying to implement benchmarking laws are errors relating to EUI and space use values (which drive the Energy Star score). Some jurisdictions, such as Chicago, Illinois and Montgomery County, Maryland, have started to require third-party verification of data quality; in other jurisdictions, the burden to ensure data quality falls more heavily on the city. DOEE does operate a Benchmarking Help Center that fields between 1,500 and 2,000 requests for assistance a year. However, the scale of the data makes a purely manual approach to data quality verification insufficient—DC has approximately 1,600 benchmarking reports, while a larger city like NYC receives over 14,000 each year. Therefore, a methodology to triage reports and flag those with data quality concerns is needed. 12-2 ©2016 ACEEE Summer Study on Energy Efficiency in Buildings The paper develops and tests a transparent and robust strategy for evaluating and improving the data quality and analysis of benchmarking data from privatelyand publiclyowned buildings. We utilize machine learning tools to support jurisdictions like the District of Columbia in improving the quality of building energy data collected through its mandatory building rating and disclosure policy. Using energy disclosure data from calendar year 2013, this paper presents a new data quality rating algorithm and grading system – known as the Data Integrity and Quality (DataIQ) score – to rank the relative reliability of reported data for the largest building sectors, and provide a tool to improve the identification and prediction of data quality concerns going forward. We apply non-parametric statistical anomaly detection techniques to identify data quality and reliability concerns in self-reported building energy benchmarking data, focusing here on office buildings. The next section describes the data and data cleaning methodology, followed by a discussion of the DataIQ methodology and results. We conclude with a discussion of the implications of the model and its potential application as part of city energy disclosure policies. DATA AND DESCRIPTIVE STATISTICS The data for this analysis include all properties subject to the requirements of the Clean and Affordable Energy Act of 2008 that reported data to the Washington, DC Department of Energy and Environment for calendar year 2013. The analyzed dataset consists of all private and public buildings subject to the energy disclosure requirement that provided data through September 4, 2015. After the removal of duplicate entries, the 2013 dataset includes a total of 1,774 unique records, divided between private buildings (1,456), public (government) buildings (274), and public housing (44). Due to non-trivial errors and omissions of various reported fields specific to multi-family housing, we discuss only office building data here. Multi-stage data cleaning represents a critical component of building energy analytics and the ability to extract reliable insight from energy data. First, the datasets are merged to create an integrated dataset of private and public buildings for each respective year. As part of this process, variable headers and other entries were cleaned of atypical characters and formatting inconsistencies. The combined dataset was then joined with the provided tax data based on Property ID and Square Suffix Lot (SSL) information. This process successfully matched disclosure and tax lot data for 1,169 properties in the 2013 dataset. Next, entries with duplicate Property IDs were removed, with only the most recent entry retained. Where feasible and appropriate, missing values in the most recent Property ID record were imputed from earlier submissions for the same Property ID. The resulting dataset contained 1,774 for the 2013. Of the reported fields, ","",""
5,"Kayvon Mazooji, Frederic Sala, Guy Van den Broeck, L. Dolecek","Robust channel coding strategies for machine learning data",2016,"","","","",45,"2022-07-13 09:40:27","","10.1109/ALLERTON.2016.7852288","","",,,,,5,0.83,1,4,6,"Two important recent trends are the proliferation of learning algorithms along with the massive increase of data stored on unreliable storage mediums. These trends impact each other; noisy data can have an undesirable effect on the results provided by learning algorithms. Although traditional tools exist to improve the reliability of data storage devices, these tools operate at a different abstraction level and therefore ignore the data application, leading to an inefficient use of resources. In this paper we propose taking the operation of learning algorithms into account when deciding how to best protect data. Specifically, we examine several learning algorithms that operate on data that is stored on noisy mediums and protected by error-correcting codes with a limited budget of redundancy; we develop a principled way to allocate resources so that the harm on the output of the learning algorithm is minimized.","",""
0,"M. Hecht, Jaron Chen, Phanitta Chomsinsap","CLAIM: An Enhanced Machine Learning Technique for Discrepancy Report Analysis",2020,"","","","",46,"2022-07-13 09:40:27","","10.1109/RAMS48030.2020.9153691","","",,,,,0,0.00,0,3,2,"CLAIM is a tool for analyzing and classifying discrepancy reports that allows for the incorporation of domain expert knowledge into a semi-supervised machine learning (ML) process (a semi-supervised learning uses a small number of manually labeled data and a much larger amount of unlabeled for training a machine learning algorithm). By using this domain knowledge, classification accuracy is higher than conventional ML approaches. The advantages are particularly apparent with small, imbalanced data sets that are quite common in discrepancy report data sets (an imbalanced data set has an unequal distribution of documents categories within each category). The CLAIM method is robust against human bias and can tolerate misclassifications of up to 20% of the training set. The increased accuracy of the CLAIM methodology makes ML a viable tool for safety, reliability, and software development process decision making. The modest human labor requirement enables use of the method under circumstances that previously made free text discrepancy report analysis infeasible due to resource, scheduling, and cost constraints.","",""
0,"Dr. Pramod D Patil, Dr. Per Lyngard, Juveria Khan","An Efficient Electric Energy Consumption Prediction System Using Machine Learning Framework An Efficient Electric Energy Consumption Prediction System Using Machine Learning Framework",2020,"","","","",47,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,3,2,"Nowadays, reliability and sustainability of energy are the two critical factors to be considered while checking for any energy sources. Also, increasing utilization of energy has led to development of solutions to save electricity. Prediction of energy is one such scheme that help forecast future energy demand in accordance with the current usage and many others conditions. There are many techniques used for prediction of electricity consumption like statistical, machine learning, deep learning, etc, but prediction with machine learning has gained more popularity. This study proposes a method for forecasting the energy consumption of residential buildings by using machine learning technique. Here the random forest algorithm for classification is used. Random forest classifier uses multitude of decision trees, making it robust and hence increasing the accuracy. The model include preprocessing the data that is removing unwanted and incomplete data, feature engineering phase in which variables of importance are selected, then learning a classification model includes training a prediction model using historical data, and the experimental evaluation on the dataset for individual household electric power consumption. Experimentation was performed on 5 years consumption data and results produced by the proposed system yield a better performance. The model produces electricity consumption prediction for given date and time.","",""
0,"Manoorkar Pragati Bhagwanrao","Driver Distraction Detection and Classification using Machine Learning",2020,"","","","",48,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,1,2,"One of the most critical problem overcome in India is death caused by the road accidents. Almost 80% of accidents are happen due to driver distraction. We attempt to develop robust system for detecting Driver distraction. There are some methods to detect it but that consumes more time. It is important to early detect the distraction, inform to the driver about the distraction. So, computer-based application needs to be developed to detect this distraction as early as possible and minimize the risk of accidents. The aim is to develop a simple and capable method to detect the distraction. There are some proposed methods that contain following stages, preprocessing, feature extraction and classification. To increase the accuracy of the result we use two layer neural network. ELM, softmax are used for feature extraction and SVM-ELM and method is used for classification. The accuracy of proposed method is 97.2% which shows its reliability. Index Terms Driver distraction, SVM , CNN, classification, machine learning.","",""
0,"Qi Wu, Weiqi Chen, Haiming Wang, W. Hong","Machine Learning-Assisted Tolerance Analysis and Its Application to Antennas",2020,"","","","",49,"2022-07-13 09:40:27","","10.1109/IEEECONF35879.2020.9330387","","",,,,,0,0.00,0,4,2,"An efficient machine learning-assisted tolerance analysis (MLATA) method is proposed by applying machine learning (ML) methods into multiple layers of the antenna tolerance analysis. The computational time for operations including worst case performance searching, maximum input tolerance hypervolume searching and robust optimization is greatly reduced while maintaining high reliability due to the introduction of the ML methods. The surrogate models which are built using ML methods have been introduced to predict both antenna performance and tolerance of parameters at given design points. The Pareto front combining antenna performance, robustness and size has been obtained to guide trade-offs for antenna robust design. A planar inverted-L antenna for mobile terminals is simulated to validate the proposed MLATA method.","",""
24,"Saikat Das, Ph.D., Ahmed M. Mahfouz, D. Venugopal, S. Shiva","DDoS Intrusion Detection Through Machine Learning Ensemble",2019,"","","","",50,"2022-07-13 09:40:27","","10.1109/QRS-C.2019.00090","","",,,,,24,8.00,6,4,3,"Distributed Denial of Service (DDoS) attacks have been the prominent attacks over the last decade. A Network Intrusion Detection System (NIDS) should seamlessly configure to fight against these attackers' new approaches and patterns of DDoS attack. In this paper, we propose a NIDS which can detect existing as well as new types of DDoS attacks. The key feature of our NIDS is that it combines different classifiers using ensemble models, with the idea that each classifier can target specific aspects/types of intrusions, and in doing so provides a more robust defense mechanism against new intrusions. Further, we perform a detailed analysis of DDoS attacks, and based on this domain-knowledge verify the reduced feature set [27, 28] to significantly improve accuracy. We experiment with and analyze NSL-KDD dataset with reduced feature set and our proposed NIDS can detect 99.1% of DDoS attacks successfully. We compare our results with other existing approaches. Our NIDS approach has the learning capability to keep up with new and emerging DDoS attack patterns.","",""
34,"Narathip Reamaroon, M. Sjoding, Kaiwen Lin, T. Iwashyna, K. Najarian","Accounting for Label Uncertainty in Machine Learning for Detection of Acute Respiratory Distress Syndrome",2019,"","","","",51,"2022-07-13 09:40:27","","10.1109/JBHI.2018.2810820","","",,,,,34,11.33,7,5,3,"When training a machine learning algorithm for a supervised-learning task in some clinical applications, uncertainty in the correct labels of some patients may adversely affect the performance of the algorithm. For example, even clinical experts may have less confidence when assigning a medical diagnosis to some patients because of ambiguity in the patient's case or imperfect reliability of the diagnostic criteria. As a result, some cases used in algorithm training may be mislabeled, adversely affecting the algorithm's performance. However, experts may also be able to quantify their diagnostic uncertainty in these cases. We present a robust method implemented with support vector machines (SVM) to account for such clinical diagnostic uncertainty when training an algorithm to detect patients who develop the acute respiratory distress syndrome (ARDS). ARDS is a syndrome of the critically ill that is diagnosed using clinical criteria known to be imperfect. We represent uncertainty in the diagnosis of ARDS as a graded weight of confidence associated with each training label. We also performed a novel time-series sampling method to address the problem of intercorrelation among the longitudinal clinical data from each patient used in model training to limit overfitting. Preliminary results show that we can achieve meaningful improvement in the performance of algorithm to detect patients with ARDS on a hold-out sample, when we compare our method that accounts for the uncertainty of training labels with a conventional SVM algorithm.","",""
0,"Keita Mizushina, Shun Suzuki, Hiroki Aihara, K. Takeuchi","3840x Reliability Enhanced Robust NAND flash Optimized to Store Weight Data for Object Detection and Semantic Segmentation of Self-driving Car at High Temperature",2020,"","","","",52,"2022-07-13 09:40:27","","10.1109/SNW50361.2020.9131423","","",,,,,0,0.00,0,4,2,"This paper proposes reliability enhancement techniques in harsh environment to store weight data of machine learning (ML)-based applications, object detection and semantic segmentation for self-driving cars. Proposed techniques consist of robust NAND and Optimized Huffman Coding Compression (OHCC). Proposed robust NAND drastically decreases bit-error rate (BER) in extremely high temperature such as 210degC. Therefore, proposed techniques reduce miss recognition caused by weight data error and contribute to safety self-driving cars. Besides, proposed OHCC modulates weight data of ML-based image recognition applications by utilizing weight data characteristics that concerned around ‘0'. Consequently, proposed techniques extend data-retention (D. R.) time by 3,840 times for object detection and 2,550 times for semantic segmentation, respectively compared with conventional 3D triple-level-cell (TLC) NAND flash. In addition, proposed techniques achieve to decrease read access time and data-overhead by 39% and 94%, respectively.","",""
4,"T. Lange, A. Balakrishnan, M. Glorieux, D. Alexandrescu, L. Sterpone","Machine Learning to Tackle the Challenges of Transient and Soft Errors in Complex Circuits",2019,"","","","",53,"2022-07-13 09:40:27","","10.1109/IOLTS.2019.8854423","","",,,,,4,1.33,1,5,3,"The Functional Failure Rate analysis of today’s complex circuits is a difficult task and requires a significant investment in terms of human efforts, processing resources and tool licenses. Thereby, de-rating or vulnerability factors are a major instrument of failure analysis efforts. Usually computationally intensive fault-injection simulation campaigns are required to obtain a fine-grained reliability metrics for the functional level. Therefore, the use of machine learning algorithms to assist this procedure and thus, optimising and enhancing fault injection efforts, is investigated in this paper. Specifically, machine learning models are used to predict accurate per-instance Functional De-Rating data for the full list of circuit instances, an objective that is difficult to reach using classical methods. The described methodology uses a set of per-instance features, extracted through an analysis approach, combining static elements (cell properties, circuit structure, synthesis attributes) and dynamic elements (signal activity). Reference data is obtained through first-principles fault simulation approaches. One part of this reference dataset is used to train the machine learning model and the remaining is used to validate and benchmark the accuracy of the trained tool. The presented methodology is applied on a practical example and various machine learning models are evaluated and compared.","",""
2,"Constantinos Xanthopoulos, Arnold Neckermann, Paulus List, Klaus-Peter Tschernay, Peter Sarson, Y. Makris","Automated Die Inking through On-line Machine Learning",2019,"","","","",54,"2022-07-13 09:40:27","","10.1109/IOLTS.2019.8854373","","",,,,,2,0.67,0,6,3,"Ensuring high reliability in modern integrated circuits (ICs) requires the employment of several die screening methodologies. One such technique, commonly referred to as die inking, aims to discard devices that are likely to fail, based on their proximity to known failed devices on the wafer. Die inking is traditionally performed manually by visually inspecting each manufactured wafer and thus it is very time-consuming. Recently, machine learning has been used to automate and speed-up the inking process. In this work, we employ on-line machine learning to address the practicability limitations of the current state-of the-art automated inking approach. Effectiveness is demonstrated on an industrial dataset of manually inked wafers.","",""
3,"Yasir Malik, Carlos Campos, Fehmi Jaafar","Detecting Android Security Vulnerabilities Using Machine Learning and System Calls Analysis",2019,"","","","",55,"2022-07-13 09:40:27","","10.1109/QRS-C.2019.00033","","",,,,,3,1.00,1,3,3,"Android operating systems have become a prime target for cyber attackers due to security vulnerabilities in the underlying operating system and application design. Recently, anomaly detection techniques are widely studied for security vulnerabilities detection and classification. However, the ability of the attackers to create new variants of existing malware using various masking techniques makes it harder to deploy these techniques effectively. In this research, we present a robust and effective vulnerabilities detection approach based on anomaly detection in a system calls of benign and malicious Android application. The anomaly in our study is type, frequency, and sequence of system calls that represent a vulnerability. Our system monitors the processes of benign and malicious application and detects security vulnerabilities based on the combination of parameters and metrics, i.e., type, frequency and sequence of system calls to classify the process behavior as benign or malign. The detection algorithm detects the anomaly based on the defined scoring function f and threshold ρ. The system refines the detection process by applying machine learning techniques to find a combination of system call metrics and explore the relationship between security bugs and the pattern of system calls detected. The experiment results show the detection rate of the proposed algorithm based on precision, recall, and f-score for different machine learning algorithms.","",""
0,"Indrajit Mandal","Developing new hybrid machine learning ensembles to improve software reliability for quality diagnosis",2014,"","","","",56,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,1,8,"The work presents precise computational machine learning methods to improve the quality of the state of the art diagnosis of chronic diseases like Parkinson’s disease, Coronary artery disease, Diabetes, Cancer etc. The aim is to prevent delay as well as misdiagnosis of patients using proposed robust inference system. Accurate detection of disease at early stage is a herculean task in the medical research. The research work addresses the drawbacks inherited in existing medical decision support systems. The proposed system consists of multiple classifier system for training purpose and final decision is given by Bayes voting classifier. The crux of the work is to calibrate the parameters involved in the machine learning ensembles for fine tuning them.","",""
19,"Santiago Alonso, J. Bobadilla, F. Ortega, Ricardo Moya","Robust Model-Based Reliability Approach to Tackle Shilling Attacks in Collaborative Filtering Recommender Systems",2019,"","","","",57,"2022-07-13 09:40:27","","10.1109/ACCESS.2019.2905862","","",,,,,19,6.33,5,4,3,"As the use of recommender systems becomes generalized in society, the interest in varying the orientation of their recommendations is increasing. There are shilling attacks’ strategies that introduce malicious profiles in collaborative filtering recommender systems in order to promote the own products or services or to discredit those of the competition. Academic research against shilling attacks has been focused in statistical approaches to detect the unusual patterns in user ratings. Nowadays, there is a growing research area focused on the design of robust machine learning methods to neutralize the malicious profiles inserted into the system. This paper proposes an innovative robust method, based on matrix factorization, to neutralize the shilling attacks. Our method obtains the reliability value associated with each prediction of a user to an item. By monitoring the unusual reliability variations in the items prediction, we can avoid promoting the shilling predictions to the erroneous recommendations. This paper openly provides more than 13 000 individual experiments involving a wide range of attack strategies, both push, and nuke, in order to test the proposed approach. The results show that the proposed method is able to neutralize most of the existing attacks; its performance only decreases in the not relevant situations: when the attack size is not large enough to effectively affect the recommendations provided by the system.","",""
1,"M. Aminpour, R. Alaie, N. Kardani, S. Moridpour, Majidreza Nazem","Slope stability predictions on spatially variable random fields using machine learning surrogate models",2022,"","","","",58,"2022-07-13 09:40:27","","10.48550/arXiv.2204.06097","","",,,,,1,1.00,0,5,1,": Random field Monte Carlo (MC) reliability analysis is a robust stochastic method to determine the probability of failure. This method, however, requires a large number of numerical simulations demanding high computational costs. This paper explores the efficiency of different machine learning (ML) algorithms used as surrogate models trained on a limited number of random field slope stability simulations in predicting the results of large datasets. The MC data in this paper require only the examination of failure or non-failure, circumventing the time-consuming calculation of factors of safety. An extensive dataset is generated, consisting of 120,000 finite difference MC slope stability simulations incorporating different levels of soil heterogeneity and anisotropy. The Bagging Ensemble, Random Forest and Support Vector classifiers are found to be the superior models for this problem amongst 9 different models and ensemble classifiers. Trained only on 0.47% of data (500 samples), the ML model can classify the entire 120,000 samples with an accuracy of %85 and AUC score of %91. The performance of ML methods in classifying the random field slope stability results generally reduces with higher anisotropy and heterogeneity of soil. The ML assisted MC reliability analysis proves a robust stochastic method where errors in the predicted probability of failure using %5 of MC data is only %0.46 in average. The approach reduced the computational time from 306 days to less than 6 hours.","",""
2,"G. Ganapathy, N. Sivakumaran, M. Punniyamoorthy, R. Surendheran, Srijan Thokala","Comparative study of machine learning techniques for breast cancer identification/diagnosis",2019,"","","","",59,"2022-07-13 09:40:27","","10.1504/IJENM.2019.10019586","","",,,,,2,0.67,0,5,3,"The number of new cases of female breast cancer was 124.9 per 100,000 women per year. Similarly, deaths were 21.2 per 100,000 women per year. It calls for an urge to increase the awareness of breast cancer and very accurately analyse the causes which may differ in minute variations. This is why the application of computation techniques are widely increasing to support the diagnostic results. In this paper, we present the application of several machine learning techniques and models like neural network, SVM is used to quantify the classifications. The techniques that are most reliable, accurate and robust are emphasised. It gives a plethora of explorations into the research field for developing predictive models. To achieve higher reliability on the data, we present the comparison of various Machine Learning techniques on a dataset that is available on the website Kaggle.","",""
11,"F. Cabitza, A. Campagner, D. Albano, A. Aliprandi, A. Bruno, V. Chianca, A. Corazza, F. Di Pietto, A. Gambino, S. Gitto, C. Messina, D. Orlandi, L. Pedone, M. Zappia, L. Sconfienza","The Elephant in the Machine: Proposing a New Metric of Data Reliability and its Application to a Medical Case to Assess Classification Reliability",2020,"","","","",60,"2022-07-13 09:40:27","","10.3390/app10114014","","",,,,,11,5.50,1,15,2,"In this paper, we present and discuss a novel reliability metric to quantify the extent a ground truth, generated in multi-rater settings, as a reliable basis for the training and validation of machine learning predictive models. To define this metric, three dimensions are taken into account: agreement (that is, how much a group of raters mutually agree on a single case); confidence (that is, how much a rater is certain of each rating expressed); and competence (that is, how accurate a rater is). Therefore, this metric produces a reliability score weighted for the raters’ confidence and competence, but it only requires the former information to be actually collected, as the latter can be obtained by the ratings themselves, if no further information is available. We found that our proposal was both more conservative and robust to known paradoxes than other existing agreement measures, by virtue of a more articulated notion of the agreement due to chance, which was based on an empirical estimation of the reliability of the single raters involved. We discuss the above metric within a realistic annotation task that involved 13 expert radiologists in labeling the MRNet dataset. We also provide a nomogram by which to assess the actual accuracy of a classification model, given the reliability of its ground truth. In this respect, we also make the point that theoretical estimates of model performance are consistently overestimated if ground truth reliability is not properly taken into account.","",""
0,"Yiyang Chen, Wei Jiang, Themistoklis Charalambous","Machine learning based iterative learning control for non-repetitive time-varying systems",2021,"","","","",61,"2022-07-13 09:40:27","","10.1002/rnc.6272","","",,,,,0,0.00,0,3,1,"The repetitive tracking task for time-varying systems (TVSs) with non-repetitive time-varying parameters, which is also called non-repetitive TVSs, is realized in this paper using iterative learning control (ILC). A machine learning (ML) based nominal model update mechanism, which utilizes the linear regression technique to update the nominal model at each ILC trial only using the current trial information, is proposed for non-repetitive TVSs in order to enhance the ILC performance. Given that the ML mechanism forces the model uncertainties to remain within the ILC robust tolerance, an ILC update law is proposed to deal with non-repetitive TVSs. How to tune parameters inside ML and ILC algorithms to achieve the desired aggregate performance is also provided. The robustness and reliability of the proposed method are verified by simulations. Comparison with current state-of-the-art demonstrates its superior control performance in terms of controlling precision. This paper broadens ILC applications from time-invariant systems to non-repetitive TVSs, adopts ML regression technique to estimate non-repetitive time-varying parameters between two ILC trials and proposes a detailed parameter tuning mechanism to achieve desired performance, which are the main contributions.","",""
0,"Samuel J Bell, Onno P. Kampman, Jesse Dodge, Neil D. Lawrence","Modeling the Machine Learning Multiverse",2022,"","","","",62,"2022-07-13 09:40:27","","10.48550/arXiv.2206.05985","","",,,,,0,0.00,0,4,1,"Amid mounting concern about the reliability and credibility of machine learning research, we present a principled framework for making robust and generalizable claims: the Multiverse Analysis. Our framework builds upon the Multiverse Analysis [1] introduced in response to psychology’s own reproducibility crisis. To efficiently explore high-dimensional and often continuous ML search spaces, we model the multiverse with a Gaussian Process surrogate and apply Bayesian experimental design. Our framework is designed to facilitate drawing robust scientific conclusions about model performance, and thus our approach focuses on exploration rather than conventional optimization. In the first of two case studies, we investigate disputed claims about the relative merit of adaptive optimizers. Second, we synthesize conflicting research on the effect of learning rate on the large batch training generalization gap. For the machine learning community, the Multiverse Analysis is a simple and effective technique for identifying robust claims, for increasing transparency, and a step toward improved reproducibility.","",""
0,"George J. Siedel, S. Vock, A. Morozov, Stefan Voss","Utilizing Class Separation Distance for the Evaluation of Corruption Robustness of Machine Learning Classifiers",2022,"","","","",63,"2022-07-13 09:40:27","","10.48550/arXiv.2206.13405","","",,,,,0,0.00,0,4,1,"Robustness is a fundamental pillar of Machine Learning (ML) classifiers, substantially determining their reliability. Methods for assessing classifier robustness are therefore essential. In this work, we address the challenge of evaluating corruption robustness in a way that allows comparability and interpretability on a given dataset. We propose a test data augmentation method that uses a robustness distance 𝜖 derived from the datasets minimal class separation distance. The resulting MSCR (mean statistical corruption robustness) metric allows a dataset-specific comparison of different classifiers with respect to their corruption robustness. The MSCR value is interpretable, as it represents the classifiers avoidable loss of accuracy due to statistical corruptions. On 2D and image data, we show that the metric reflects different levels of classifier robustness. Furthermore, we observe unexpected optima in classifiers robust accuracy through training and testing classifiers with different levels of noise. While researchers have frequently reported on a significant tradeoff on accuracy when training robust models, we strengthen the view that a tradeoff between accuracy and corruption robustness is not inherent. Our results indicate that robustness training through simple data augmentation can already slightly improve accuracy.","",""
0,"Junzhong Xie, Xu-Yuan Zhou, Dong Luan, Hong Jiang","Machine Learning Force Field Aided Cluster Expansion Approach to Configurationally Disordered Materials: Critical Assessment of Training Set Selection and Size Convergence.",2022,"","","","",64,"2022-07-13 09:40:27","","10.1021/acs.jctc.2c00017","","",,,,,0,0.00,0,4,1,"Cluster expansion (CE) is a powerful theoretical tool to study the configuration-dependent properties of substitutionally disordered systems. Typically, a CE model is built by fitting a few tens or hundreds of target quantities calculated by first-principles approaches. To validate the reliability of the model, a convergence test of the cross-validation (CV) score to the training set size is commonly conducted to verify the sufficiency of the training data. However, such a test only confirms the convergence of the predictive capability of the CE model within the training set, and it is unknown whether the convergence of the CV score would lead to robust thermodynamic simulation results such as order-disorder phase transition temperature Tc. In this work, using carbon defective MoC1-x as a model system and aided by the machine-learning force field technique, a training data pool with about 13000 configurations has been efficiently obtained and used to generate different training sets of the same size randomly. By conducting parallel Monte Carlo simulations with the CE models trained with different randomly selected training sets, the uncertainty in calculated Tc can be evaluated at different training set sizes. It is found that the training set size that is sufficient for the CV score to converge still leads to a significant uncertainty in the predicted Tc and that the latter can be considerably reduced by enlarging the training set to that of a few thousand configurations. This work highlights the importance of using a large training set to build the optimal CE model that can achieve robust statistical modeling results and the facility provided by the machine-learning force field approach to efficiently produce adequate training data.","",""
0,"W. Pannakkong, Kwanluck Thiwa-Anont, Kasidit Singthong, P. Parthanadee, J. Buddhakulsomsiri","Hyperparameter Tuning of Machine Learning Algorithms Using Response Surface Methodology: A Case Study of ANN, SVM, and DBN",2022,"","","","",65,"2022-07-13 09:40:27","","10.1155/2022/8513719","","",,,,,0,0.00,0,5,1,"This study applies response surface methodology (RSM) to the hyperparameter fine-tuning of three machine learning (ML) algorithms: artificial neural network (ANN), support vector machine (SVM), and deep belief network (DBN). The purpose is to demonstrate RSM effectiveness in maintaining ML algorithm performance while reducing the number of runs required to reach effective hyperparameter settings in comparison with the commonly used grid search (GS). The ML algorithms are applied to a case study dataset from a food producer in Thailand. The objective is to predict a raw material quality measured on a numerical scale. K-fold cross-validation is performed to ensure that the ML algorithm performance is robust to the data partitioning process in the training, validation, and testing sets. The mean absolute error (MAE) of the validation set is used as the prediction accuracy measurement. The reliability of the hyperparameter values from GS and RSM is evaluated using confirmation runs. Statistical analysis shows that (1) the prediction accuracy of the three ML algorithms tuned by GS and RSM is similar, (2) hyperparameter settings from GS are 80% reliable for ANN and DBN, and settings from RSM are 90% and 100% reliable for ANN and DBN, respectively, and (3) savings in the number of runs required by RSM over GS are 97.79%, 97.81%, and 80.69% for ANN, SVM, and DBN, respectively.","",""
1,"C. Lakshminarayan, Thiagarajan Ramakrishnan, Awny Al-Omari, Khaled Bouaziz, Faraz Ahmad, S. Raghavan, Prama Agarwal","Enterprise-wide Machine Learning using Teradata Vantage: An Integrated Analytics Platform",2019,"","","","",66,"2022-07-13 09:40:27","","10.1109/BigData47090.2019.9006321","","",,,,,1,0.33,0,7,3,"Big data characterized by variety can be divided into 3 principal categories: numeric structured data, semi-structured data, and unstructured multimedia data involving audio, video, and text. Decision making requires multiple analytical engines suitable for each type of data, programming languages, algorithms, visualization tools, and user interfaces. More often than not, industrial analytics is conducted ad hoc by lashing together analytics components such as distributed data sources, analytics engines, and algorithms. This kind of piecemeal approach ignores scale, security, governance, reliability, model management and fault tolerance that are paramount for industrial strength analytics. A unified, versatile, and robust architecture that combines various components in a single integrated platform is the need of the hour. Teradata Vantage (TD Vantage) is such a platform for delivering production quality enterprise analytics at scale. In this paper, we outline the proposed TD Vantage (available in the market and under continuous development) that unifies data, engines, and algorithms operating in a seamless symphony. We will demonstrate its capabilities through three proofs of concept biz: image data using TensorFlow, text data using Spark, and transaction data using Aster (now renamed Machine Learning Engine or MLE), with Teradata orchestrating interactions among the various components.","",""
4,"T. Fromm, B. Staehle, W. Ertel","Robust multi-algorithm object recognition using Machine Learning methods",2012,"","","","",67,"2022-07-13 09:40:27","","10.1109/MFI.2012.6343014","","",,,,,4,0.40,1,3,10,"Robust object recognition is a crucial requirement for many robotic applications. We propose a method towards increasing reliability and flexibility of object recognition for robotics. This is achieved by the fusion of diverse recognition frameworks and algorithms on score level which use characteristics like shape, texture and color of the objects. Machine Learning allows for the automatic combination of the respective recognition methods' outputs instead of having to adapt their hypothesis metrics to a common basis. We show the applicability of our approach through several real-world experiments in a service robotics environment. Great importance is attached to robustness, especially in varying environments.","",""
0,"B. Roy, D. Bera, Somya Nigam, S. Upadhyay","A study of turbine failure pattern: a model optimization using machine learning",2022,"","","","",68,"2022-07-13 09:40:27","","10.1007/s13198-021-01542-9","","",,,,,0,0.00,0,4,1,"","",""
0,"L. Leunge","Machine Learning Revealing Insights into Soil Stratification: An Application for Dikes and Dams",2019,"","","","",69,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,1,3,"In the Netherlands, robust dike and dam design is a major concern in the context of flood defence. Due to heterogeneity of the subsoil on which these structures are founded, the validity range of in situ tests decreases drastically. Consequently, large uncertainties regarding spatial variation of soil stratification and soil layer parameters are incorporated in the cross-sectional reliability requirements, resulting in conservative designs. This thesis presents a Machine Learning application, which, by learning locally measured information and analysing high spatial resolution surface settlement data, can provide insights into spatial variation of soil stratification. Through the analysis of these insights, the uncertainties regarding spatial variability in cross-sectional reliability requirements can be reduced, which leads to less conservatism in dike and dam construction.","",""
7,"Yulei Wu","Robust Learning-Enabled Intelligence for the Internet of Things: A Survey From the Perspectives of Noisy Data and Adversarial Examples",2021,"","","","",70,"2022-07-13 09:40:27","","10.1109/JIOT.2020.3018691","","",,,,,7,7.00,7,1,1,"The Internet of Things (IoT) has been widely adopted in a range of verticals, e.g., automation, health, energy, and manufacturing. Many of the applications in these sectors, such as self-driving cars and remote surgery, are critical and high stakes applications, calling for advanced machine learning (ML) models for data analytics. Essentially, the training and testing data that are collected by massive IoT devices may contain noise (e.g., abnormal data, incorrect labels, and incomplete information) and adversarial examples. This requires high robustness of ML models to make reliable decisions for IoT applications. The research of robust ML has received tremendous attention from both academia and industry in recent years. This article will investigate the state of the art and representative works of robust ML models that can enable high resilience and reliability of IoT intelligence. Two aspects of robustness will be focused on, i.e., when the training data of ML models contain noises and adversarial examples, which may typically happen in many real-world IoT scenarios. In addition, the reliability of both neural networks and reinforcement learning framework will be investigated. Both of these two ML paradigms have been widely used in handling data in IoT scenarios. The potential research challenges and open issues will be discussed to provide future research directions.","",""
0,"Dewi Tjia, Ritu Gupta, M. Alam","Two-Stage History Matching for Hydrology Models via Machine Learning",2020,"","","","",71,"2022-07-13 09:40:27","","10.1007/978-981-15-3287-0_7","","",,,,,0,0.00,0,3,2,"","",""
46,"A. Hammouri, M. Hammad, Mohammad M. Alnabhan, Fatima Alsarayrah","Software Bug Prediction using Machine Learning Approach",2018,"","","","",72,"2022-07-13 09:40:27","","10.14569/IJACSA.2018.090212","","",,,,,46,11.50,12,4,4,"Software Bug Prediction (SBP) is an important issue in software development and maintenance processes, which concerns with the overall of software successes. This is because predicting the software faults in earlier phase improves the software quality, reliability, efficiency and reduces the software cost. However, developing robust bug prediction model is a challenging task and many techniques have been proposed in the literature. This paper presents a software bug prediction model based on machine learning (ML) algorithms. Three supervised ML algorithms have been used to predict future software faults based on historical data. These classifiers are Naive Bayes (NB), Decision Tree (DT) and Artificial Neural Networks (ANNs). The evaluation process showed that ML algorithms can be used effectively with high accuracy rate. Furthermore, a comparison measure is applied to compare the proposed prediction model with other approaches. The collected results showed that the ML approach has a better performance.","",""
39,"Bin Nie, J. Xue, Saurabh Gupta, Tirthak Patel, C. Engelmann, E. Smirni, Devesh Tiwari","Machine Learning Models for GPU Error Prediction in a Large Scale HPC System",2018,"","","","",73,"2022-07-13 09:40:27","","10.1109/DSN.2018.00022","","",,,,,39,9.75,6,7,4,"GPUs are widely deployed on large-scale HPC systems to provide powerful computational capability for scientific applications from various domains. As those applications are normally long-running, investigating the characteristics of GPU errors becomes imperative for reliability. In this paper, we first study the system conditions that trigger GPU errors using six-month trace data collected from a large-scale, operational HPC system. Then, we use machine learning to predict the occurrence of GPU errors, by taking advantage of temporal and spatial dependencies of the trace data. The resulting machine learning prediction framework is robust and accurate under different workloads.","",""
0,"F. Baghernezhad","Machine Learning-based Strategies for Robust Fault Detection and Identification of Mobile Robots",2012,"","","","",74,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,1,10,"Nowadays, process monitoring and fault diagnosis techniques are becoming a critical component of modern automatic control systems. One of the most crucial issues for the design of automatic control systems is reliability and dependability. Traditional ways to achieve these goals are through designing adaptive and robust controllers to eliminate any influence of faults on an output. Using these approaches, faults are managed but they could ultimately lead to failures; after which no controller could repair such effects. In order to minimize such damages, it is necessary to diagnose and rectify faults as soon as possible. In a fault detection system, residual generation is the first step in detecting faults, but residuals are not the only element of a dependable fault detection system. A fault detection system is reliable when an appropriate residual evaluation method is used along with a suitable residual generation technique.    The problem of fault detection and identification in a nonlinear system with applications to mobile robots is addressed in this thesis. For this purpose, first a new simulator for mobile robots containing kinematic and dynamic equations of a mobile robot and its actuators is designed. To detect faults in the system, a linear velocity of the mobile robot is chosen and modeled with computationally intelligent techniques. Locally linear models (LLM) as a neuro-fuzzy technique and radial basis function (RBF) as a powerful neural network are used to estimate the linear velocity of a mobile robot and generate residuals by comparing these with the system measurements. Subsequently, residuals are evaluated by using fixed and adaptive threshold bands. Adaptive threshold bands are generated using locally model thresholds (LMT) and model error modeling (MEM) technique in order to reduce the fault detection delay and false alarms. Finally, fault identification of a mobile robot by using multiple model technique is presented with the two proposed methods of modeling and threshold generation. The fault identification task consists of determining the occurrence of the fault as well as its location and magnitude. This is accomplished on four different types of faults with different magnitudes that are divided in ten different classes. For each scenario, simulation results are presented to demonstrate and illustrate the advantages and disadvantages of each methodology.    The main contributions of this thesis can be stated as follows: (a) the development and design of a new fault diagnosis method and a residual evaluation scheme by using an adaptive threshold band that is accomplished by using locally linear models of the system, (b) development of a fault detection approach based on computational intelligent algorithms for mobile robots using the MEM algorithm to generate adaptive threshold bands, (c) development of two fault identification approaches based on the concept of multiple models for a mobile robot, and (d) the resulting improvements of the proposed adaptive threshold bands are shown through extensive simulation results.","",""
2,"Christian Fiedler, C. Scherer, S. Trimpe","Learning-enhanced robust controller synthesis with rigorous statistical and control-theoretic guarantees",2021,"","","","",75,"2022-07-13 09:40:27","","10.1109/CDC45484.2021.9682836","","",,,,,2,2.00,1,3,1,"The combination of machine learning with control offers many opportunities, in particular for robust control. However, due to strong safety and reliability requirements in many real-world applications, providing rigorous statistical and control-theoretic guarantees is of utmost importance, yet difficult to achieve for learning-based control schemes. We present a general framework for learning-enhanced robust control that allows for systematic integration of prior engineering knowledge, is fully compatible with modern robust control and still comes with rigorous and practically meaningful guarantees. Building on the established Linear Fractional Representation and Integral Quadratic Constraints framework, we integrate Gaussian Process Regression as a learning component and state-of-the-art robust controller synthesis. In a concrete robust control example, our approach is demonstrated to yield improved performance with more data, while guarantees are maintained throughout.","",""
1,"Farnaz Tahmasebian, Jian Lou, Li Xiong","RobustFed: A Truth Inference Approach for Robust Federated Learning",2021,"","","","",76,"2022-07-13 09:40:27","","","","",,,,,1,1.00,0,3,1,"Federated learning is a prominent framework that enables clients (e.g., mobile devices or organizations) to train a collaboratively global model under a central server’s orchestration while keeping local training datasets’ privacy. However, the aggregation step in federated learning is vulnerable to adversarial attacks as the central server cannot manage clients’ behavior. Therefore, the global model’s performance and convergence of the training process will be affected under such attacks. To mitigate this vulnerability issue, we propose a novel robust aggregation algorithm inspired by the truth inference methods in crowdsourcing via incorporating the worker’s reliability into aggregation. We evaluate our solution on three real-world datasets with a variety of machine learning models. Experimental results show that our solution ensures robust federated learning and is resilient to various types of attacks, including noisy data attacks, Byzantine attacks, and label flipping attacks.","",""
1,"S. Dankwa, Lu Yang","Securing IoT Devices: A Robust and Efficient Deep Learning with a Mixed Batch Adversarial Generation Process for CAPTCHA Security Verification",2021,"","","","",77,"2022-07-13 09:40:27","","10.3390/electronics10151798","","",,,,,1,1.00,1,2,1,"The Internet of Things environment (e.g., smart phones, smart televisions, and smart watches) ensures that the end user experience is easy, by connecting lives on web services via the internet. Integrating Internet of Things devices poses ethical risks related to data security, privacy, reliability and management, data mining, and knowledge exchange. An adversarial machine learning attack is a good practice to adopt, to strengthen the security of text-based CAPTCHA (Completely Automated Public Turing test to tell Computers and Humans Apart), to withstand against malicious attacks from computer hackers, to protect Internet of Things devices and the end user’s privacy. The goal of this current study is to perform security vulnerability verification on adversarial text-based CAPTCHA, based on attacker–defender scenarios. Therefore, this study proposed computation-efficient deep learning with a mixed batch adversarial generation process model, which attempted to break the transferability attack, and mitigate the problem of catastrophic forgetting in the context of adversarial attack defense. After performing K-fold cross-validation, experimental results showed that the proposed defense model achieved mean accuracies in the range of 82–84% among three gradient-based adversarial attack datasets.","",""
1,"Ehsan Hallaji, Roozbeh Razavi-Far, V. Palade, M. Saif","Adversarial Learning on Incomplete and Imbalanced Medical Data for Robust Survival Prediction of Liver Transplant Patients",2021,"","","","",78,"2022-07-13 09:40:27","","10.1109/ACCESS.2021.3081040","","",,,,,1,1.00,0,4,1,"The scarcity of liver transplants necessitates prioritizing patients based on their health condition to minimize deaths on the waiting list. Recently, machine learning methods have gained popularity for automatizing liver transplant allocation systems, which enables prompt and suitable selection of recipients. Nevertheless, raw medical data often contain complexities such as missing values and class imbalance that reduce the reliability of the constructed model. This paper aims at eliminating the respective challenges to ensure the reliability of the decision-making process. To this aim, we first propose a novel deep learning method to simultaneously eliminate these challenges and predict the patients’ survival chance. Secondly, a hybrid framework is designed that contains three main modules for missing data imputation, class imbalance learning, and classification, each of which employing multiple advanced techniques for the given task. Furthermore, these two approaches are compared and evaluated using a real clinical case study. The experimental results indicate the robust and superior performance of the proposed deep learning method in terms of F-measure and area under the receiver operating characteristic curve (AUC).","",""
1,"Adhishree Srivastava, S. K. Parida","A Robust Fault Detection and Location Prediction Module Using Support Vector Machine and Gaussian Process Regression for AC Microgrid",2022,"","","","",79,"2022-07-13 09:40:27","","10.1109/tia.2021.3129982","","",,,,,1,1.00,1,2,1,"Better reliability of power supply is assured with the inculcation of distributed generators (DGs) in a distribution network. Smart sensors and latest grid communication protocols have played a crucial role in the development of intelligent microgrids (MGs). Conventional protection schemes do not provide reliable performance when implemented in MGs. This article proposes an approach which requires root mean square value of one cycle three -phase voltage and current measurements during fault. These data are treated as inputs for developing a fault isolation and locator module. This module is supposed to be available at central protection system, and is designed using machine learning (ML) based techniques viz. Gaussian process regression for fault location prediction and support vector machine for fault identification. Effectiveness of the proposed methodology is evaluated by considering practical grid scenarios with load variation and different DG penetration level. Furthermore, the robustness of the proposed model is assessed by performing sensitivity analysis with consideration of variation in line parameters and load as well as effect of DG correlation. A 7-bus meshed ac MG test system consisting of three DGs and two grid sources is modeled in SIMULINK platform, and is used to demonstrate the proposed module. Data analytics tools of MATLAB 2020a has been explored to develop an ML-based fault isolation and location module for MGs. The proposed scheme has also been validated with real-time MG data obtained from OPAL Real time (OPAL-RT) real-time simulator OP-4510. The accuracy in predicted results proves that the proposed scheme is pertinent for real-time practical applications.","",""
1,"Xinjiang Lu, Yunxu Bai","Robust Least-Squares Support Vector Machine Using Probabilistic Inference",2020,"","","","",80,"2022-07-13 09:40:27","","10.1109/TCYB.2020.3026680","","",,,,,1,0.50,1,2,2,"The least-square support vector machine (LS-SVM) has been deeply studied in the machine-learning field and widely applied on a great deal of occasions. A disadvantage is that it is less efficient in dealing with the non-Gaussian noise. In this article, a novel probabilistic LS-SVM is proposed to enhance the modeling reliability even data contaminated by the non-Gaussian noise. The stochastic effect of noise on the kernel function and the regularization parameter is first analyzed and estimated. On the basis of this, a new objective function is constructed under a probabilistic sense. A probabilistic inference method is then developed to construct the distribution of the model parameter, including distribution estimation of both the kernel function and the regularization parameter from data. Using this distribution information, a solving strategy is then developed for this new objective function. Different from the original LS-SVM that uses a deterministic scenario approach to gain the model, the proposed method builds the distribution relation between the model and noise and makes use of this distribution information in the process of modeling; thus, it is more robust for modeling of noise data. The effectiveness of the proposed probabilistic LS-SVM is demonstrated by using both artificial and real cases.","",""
18,"Jacob Manning, David Langerman, B. Ramesh, Evan Gretok, Christopher M. Wilson, A. George, J. Mackinnon, G. Crum","Machine-Learning Space Applications on SmallSat Platforms with TensorFlow",2018,"","","","",81,"2022-07-13 09:40:27","","","","",,,,,18,4.50,2,8,4,"Due to their attractive benefits, which include affordability, comparatively low development costs, shorter development cycles, and availability of launch opportunities, SmallSats have secured a growing commercial and educational interest for space development. However, despite these advantages, SmallSats, and especially CubeSats, suffer from high failure rates and (with few exceptions to date) have had low impact in providing entirely novel, market-redefining capabilities. To enable these more complex science and defense opportunities in the future, smallspacecraft computing capabilities must be flexible, robust, and intelligent. To provide more intelligent computing, we propose employing machine intelligence on space development platforms, which can contribute to more efficient communications, improve spacecraft reliability, and assist in coordination and management of single or multiple spacecraft autonomously. Using TensorFlow, a popular, open-source, machine-learning framework developed by Google, modern SmallSat computers can run TensorFlow graphs (principal component of TensorFlow applications) with both TensorFlow and TensorFlow Lite. The research showcased in this paper provides a flight-demonstration example, using terrestrial-scene image products collected in flight by our STP-H5/CSP system, currently deployed on the International Space Station, of various Convolutional Neural Networks (CNNs) to identify and characterize newly captured images. This paper compares CNN architectures including MobileNetV1, MobileNetV2, InceptionResNetV2, and NASNet Mobile.","",""
0,"Khouloud Abdelli, J. Cho, S. Pachnicke","Secure and Robust Federated Learning for Predictive Maintenance in Optical Networks",2021,"","","","",82,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,3,1,"Machine learning (ML) has recently emerged as a powerful tool to enhance the proactive optical network maintenance and thereby improves network reliability and reduces unplanned downtime and maintenance costs. However, it is challenging to develop an accurate and reliable ML model for solving predictive maintenance tasks (e.g., anomaly detection, fault diagnosis, remaining useful prediction etc) mainly due to the unavailability of a sufficient amount of training data since the device failure does not occur often in optical networks. Federated learning (FL) is a promising candidate to tackle the aforementioned challenge by enabling the development of a global ML model using datasets owned by many vendors without revealing their business-confidential data. While FL greatly enhances the data privacy, it is vulnerable to various model inversion and poisoning attacks. In this paper, we propose a robust collaborative learning framework for predictive maintenance in a cross-vendor setting, whereby the defensive mechanisms to protect against the aforementioned attacks are implemented. The multi-party computation (MPC)-based secure aggregation is adopted to defend against the model inversion attacks whereas a trained autoencoder based anomaly detection model is used to recognize the model poisoning attacks launched by compromised vendors. The proposed framework is applied to the semiconductor laser degradation prediction use case. We conduct experiments on semiconductor laser reliability data obtained from different laser manufacturers under various attack scenarios to evaluate the attack defense and detection capabilities of the proposed approach. Our experiments confirm that a global ML model can be accurately built with sensitive datasets in federated learning even when a subset of vendors is compromised.","",""
0,"Yiyang Chen, Wei Jiang, Themistoklis Charalambous","Machine learningbased iterative learning control for non-repetitive time-varying systems",2021,"","","","",83,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,3,1,"The repetitive tracking task for time-varying systems (TVSs) with non-repetitive time-varying parameters, which is also called non-repetitive TVSs, is realized in this paper using iterative learning control (ILC). A machine learning (ML) based nominal model update mechanism, which utilizes the linear regression technique to update the nominal model at each ILC trial only using the current trial information, is proposed for non-repetitive TVSs in order to enhance the ILC performance. Given that the ML mechanism forces the model uncertainties to remain within the ILC robust tolerance, an ILC update law is proposed to deal with non-repetitive TVSs. How to tune parameters inside ML and ILC algorithms to achieve the desired aggregate performance is also provided. The robustness and reliability of the proposed method are verified by simulations. Comparison with current state-of-the-art demonstrates its superior control performance in terms of controlling precision. This paper broadens ILC applications from time-invariant systems to non-repetitive TVSs, adopts ML regression technique to estimate non-repetitive time-varying parameters between two ILC trials and proposes a detailed parameter tuning mechanism to achieve desired performance, which are the main contributions.","",""
9,"Wentao Mao, Di Zhang, Siyu Tian, Jiamei Tang","Robust Detection of Bearing Early Fault Based on Deep Transfer Learning",2020,"","","","",84,"2022-07-13 09:40:27","","10.3390/electronics9020323","","",,,,,9,4.50,2,4,2,"In recent years, machine learning techniques have been proven to be a promising tool for early fault detection of rolling bearings. In many actual applications, however, bearing whole-life data are not easy to be historically accumulated, while insufficient data may result in training a detection model that is not good enough. If utilizing the available data under different working conditions to facilitate model training, the data distribution of different bearings are usually quite different, which does not meet the precondition of i n d e p e n d e n t a n d i d e n t i c a l d i s t r i b u t i o n ( i . i . d ) and tends to cause performance reduction. In addition, disturbed by the unstable noise under complex conditions, most of the current detection methods are inclined to raise false alarms, so that the reliability of detection results needs to be improved. To solve these problems, a robust detection method for bearings early fault is proposed based on deep transfer learning. The method includes offline stage and online stage. In the offline stage, by introducing a deep auto-encoder network with domain adaptation, the distribution inconsistency of normal state data among different bearings can be weakened, then the common feature representation of the normal state is obtained. With the extracted common features, a new state assessment method based on the robust deep auto-encoder network is proposed to evaluate the boundary between normal state and early fault state in the low-rank feature space. By training a support vector machine classifier, the detection model is established. In the online stage, along with the data batch arriving sequentially, the features of target bearing are extracted using the common representation learnt in the offline stage, and online detection is conducted by feeding them into the SVM model. Experimental results on IEEE PHM Challenge 2012 bearing dataset and XJTU-SY dataset show that the proposed approach outperforms several state-of-the-art detection methods in terms of detection accuracy and false alarm rate.","",""
2,"Yu-Tang Zheng, Song Huang, Zhan-wei Hui, Yaning Wu","A Method of Optimizing Multi-Locators Based on Machine Learning",2018,"","","","",85,"2022-07-13 09:40:27","","10.1109/QRS-C.2018.00041","","",,,,,2,0.50,1,4,4,"Due to the rapid iteration of Web applications, there are some broken test cases in regression tests. The main reason for the appearance of broken test cases is the failure of element location in the new web page. The element locators in the test cases come from various Web element locating tools, which are used to identify the elements to be convenient for testers to operate them and eventually to test the Web application. Therefore, the Web element locating tools play an essential role in web testing. At present, there are some Web element locating tools, which are supported by a single locating algorithm or multiple locating algorithms. Moreover, the Multi-Locators supported by multiple algorithms are obviously more robust than the one supported by a single algorithm. However, when synthesizing all locating algorithm to generate Multi-Locators, a better method can be selected in assigning weights to each algorithm. Based on this observation, we propose a method to optimize Multi-Locators. In assigning weight to each algorithm, it chooses a weight distribution method based on machine learning, named Learned Weights. Through experimental comparison, it is shown that the locating tool supported by algorithm based on machine learning is more robust than these existing locating tools.","",""
27,"Krzysztof Gajowniczek, T. Zabkowski","Two-Stage Electricity Demand Modeling Using Machine Learning Algorithms",2017,"","","","",86,"2022-07-13 09:40:27","","10.3390/EN10101547","","",,,,,27,5.40,14,2,5,"Forecasting of electricity demand has become one of the most important areas of research in the electric power industry, as it is a critical component of cost-efficient power system management and planning. In this context, accurate and robust load forecasting is supposed to play a key role in reducing generation costs, and deals with the reliability of the power system. However, due to demand peaks in the power system, forecasts are inaccurate and prone to high numbers of errors. In this paper, our contributions comprise a proposed data-mining scheme for demand modeling through peak detection, as well as the use of this information to feed the forecasting system. For this purpose, we have taken a different approach from that of time series forecasting, representing it as a two-stage pattern recognition problem. We have developed a peak classification model followed by a forecasting model to estimate an aggregated demand volume. We have utilized a set of machine learning algorithms to benefit from both accurate detection of the peaks and precise forecasts, as applied to the Polish power system. The key finding is that the algorithms can detect 96.3% of electricity peaks (load value equal to or above the 99th percentile of the load distribution) and deliver accurate forecasts, with mean absolute percentage error (MAPE) of 3.10% and resistant mean absolute percentage error (r-MAPE) of 2.70% for the 24 h forecasting horizon.","",""
24,"A. Weinand, Michael Karrenbauer, R. Sattiraju, H. Schotten","Application of Machine Learning for Channel based Message Authentication in Mission Critical Machine Type Communication",2017,"","","","",87,"2022-07-13 09:40:27","","","","",,,,,24,4.80,6,4,5,"The design of robust wireless communication systems for industrial applications such as closed loop control processes has been considered manifold recently. Additionally, the ongoing advances in the area of connected mobility have similar or even higher requirements regarding system reliability and availability. Beside unfulfilled reliability requirements, the availability of a system can further be reduced, if it is under attack in the sense of violation of information security goals such as data authenticity or integrity. In order to guarantee the safe operation of an application, a system has at least to be able to detect these attacks. Though there are numerous techniques in the sense of conventional cryptography in order to achieve that goal, these are not always suited for the requirements of the applications mentioned due to resource inefficiency. In the present work, we show how the goal of message authenticity based on physical layer security (PHYSEC) can be achieved. The main idea for such techniques is to exploit user specific characteristics of the wireless channel, especially in spatial domain. Additionally, we show the performance of our machine learning based approach and compare it with other existing approaches.","",""
0,"V. Bhan","Identification of Psoriasis Disease In Dermatology Using Machine Learning Technique",2018,"","","","",88,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,1,4,"Accuracy and reliability of classification technique are vital components for any Skin Lesion Detection System. This paper represents a dermatology based Skin Lesion Detection System (SLDS) to do classification automatically of dermatology images into psoriasis disease. The uniqueness of the detection system is an exploration of the various features related to the classification technique in Support Vector Machine (SVM) model. The features consist of texture, color space, redness of psoriatic disease and local binary pattern. The proposed SLDS detection system accompanied with the various technique like pre-preprocessing, data augmentation, feature selection, feature extraction and classification component which make the system robust. This system is trained on skin dataset that consists of psoriatic affected skin disease and healthy skin images. The training procedure mainly deal with machine learning parameters. At first, we have trained our system with 140 images and then tested with test set consist of 60 images, then, at last, the system performance was evaluated based on the various classifiers that we used like CART, SVM, and LR. The proposed SLDS system achieve classification accuracy with 100%, 98.36%, and 98.36% and respectively. We can be shown that the consistency factor and reliability has improved with the use of novel local binary feature through the analysis and research we found that the accuracy is improving with increasing the size of data.","",""
0,"孙哲南, 张慧, 谭铁牛","Method for distinguishing false iris images based on robust texture features and machine learning",2010,"","","","",89,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,3,12,"The invention relates to a method for distinguishing false iris images based on robust texture features and machine learning, which comprises the following steps: preprocessing true iris images or false iris images; extracting the partitioned statistical features of a robust weighted partial binary pattern; and carrying out training and sorting of a support vector machine, and judging whether thetest images are false iris images or not according to the output result of a sorter. The method of the invention combines SIFT descriptors and partial binary pattern features to extract the robust texture features, the description of textures is more stable because of the robustness of the SIFT to brightness, translation, rotation and scale change, and the support vector machine enables the method to have better universality. The invention can be used for effectively distinguishing the false iris images, has the advantages of high precision, high robustness and high reliability, can be used for distinguishing false irises such as paper printing irises, color printing contact lenses, synthetic eyes and the like, and can improve the safety of the system when being applied to the applicationsystem in which iris recognition is used for carrying out identification.","",""
1,"Yinggang Kou, Gaoying Cui, Jie Fan, Xiao Chen, Wei Li","Machine learning based models for fault detection in automatic meter reading systems",2017,"","","","",90,"2022-07-13 09:40:27","","10.1109/SPAC.2017.8304362","","",,,,,1,0.20,0,5,5,"Recently, research has focused on the area of fault detection in Automatic Meter Reading (AMR) systems. The manufacturers and users of AMR systems are now keen to include diagnostic features in the systems to improve salability and reliability. However, traditional manual fault detection methods are time-consuming and inaccurate. automatic fast fault detection methods are urgently needed. In this paper, we propose several machine learning based fault detection models to meet this requirement. Furthermore, we use novel boosting strategy to fuse multiple models to leverage multi-aspect information in AMR systems. The experimental results on simulated data show that the proposed models are accurate and robust, and fusion strategy indeed improve the performance on fault detection.","",""
0,"Matteo Zecchin, Sangwoo Park, O. Simeone, M. Kountouris, D. Gesbert","Robust Bayesian Learning for Reliable Wireless AI: Framework and Applications",2022,"","","","",91,"2022-07-13 09:40:27","","10.48550/arXiv.2207.00300","","",,,,,0,0.00,0,5,1,"—This work takes a critical look at the application of conventional machine learning methods to wireless communication problems through the lens of reliability and robustness. Deep learning techniques adopt a frequentist framework, and are known to provide poorly calibrated decisions that do not reproduce the true uncertainty caused by limitations in the size of the training data. Bayesian learning, while in principle capable of addressing this shortcoming, is in practice impaired by model misspeciﬁcation and by the presence of outliers. Both problems are pervasive in wireless communication settings, in which the capacity of machine learning models is subject to resource constraints and training data is affected by noise and interference. In this context, we explore the application of the framework of robust Bayesian learning. After a tutorial-style introduction to robust Bayesian learning, we showcase the merits of robust Bayesian learning on several important wireless communication problems in terms of accuracy, calibration, and robustness to outliers and misspeciﬁcation.","",""
0,"Yuanyi Zhong, Haoran Tang, Junkun Chen, Jian Peng, Yu-Xiong Wang","Is Self-Supervised Learning More Robust Than Supervised Learning?",2022,"","","","",92,"2022-07-13 09:40:27","","10.48550/arXiv.2206.05259","","",,,,,0,0.00,0,5,1,"Self-supervised contrastive learning is a powerful tool to learn visual representation without labels. Prior work has primarily focused on evaluating the recognition accuracy of various pre-training algorithms, but has overlooked other behavioral aspects. In addition to accuracy, distributional robustness plays a critical role in the reliability of machine learning models. We design and conduct a series of robustness tests to quantify the behavioral differences between contrastive learning and supervised learning to downstream or pretraining data distribution changes. These tests leverage data corruptions at multiple levels, ranging from pixellevel gamma distortion to patch-level shuffling and to dataset-level distribution shift. Our tests unveil intriguing robustness behaviors of contrastive and supervised learning. On the one hand, under downstream corruptions, we generally observe that contrastive learning is surprisingly more robust than supervised learning. On * Equal contribution the other hand, under pre-training corruptions, we find contrastive learning vulnerable to patch shuffling and pixel intensity change, yet less sensitive to dataset-level distribution change. We attempt to explain these results through the role of data augmentation and feature space properties. Our insight has implications in improving the downstream robustness of supervised learning.","",""
0,"Eileen Wang, Soyeon Caren Han, J. Poon","RoViST: Learning Robust Metrics for Visual Storytelling",2022,"","","","",93,"2022-07-13 09:40:27","","10.48550/arXiv.2205.03774","","",,,,,0,0.00,0,3,1,"Visual storytelling (VST) is the task of generating a story paragraph that describes a given image sequence. Most existing storytelling approaches have evaluated their models using traditional natural language generation metrics like BLEU or CIDEr. However, such metrics based on n -gram matching tend to have poor correlation with human evaluation scores and do not explicitly consider other criteria neces-sary for storytelling such as sentence structure or topic coherence. Moreover, a single score is not enough to assess a story as it does not in-form us about what speciﬁc errors were made by the model. In this paper, we propose 3 evaluation metrics sets that analyses which aspects we would look for in a good story: 1) visual grounding, 2) coherence, and 3) non-redundancy. We measure the reliability of our metric sets by analysing its correlation with human judgement scores on a sample of machine stories obtained from 4 state-of-the-arts models trained on the Visual Storytelling Dataset (VIST). Our metric sets outperforms other metrics on human correlation, and could be served as a learning based evaluation metric set that is complementary to existing rule-based metrics. 1","",""
28,"K. Javed, R. Gouriveau, N. Zerhouni, R. Zemouri, Xiang Li","Robust, reliable and applicable tool wear monitoring and prognostic: Approach based on an improved-extreme learning machine",2012,"","","","",94,"2022-07-13 09:40:27","","10.1109/ICPHM.2012.6299516","","",,,,,28,2.80,6,5,10,"Although efforts in this field are significant around the world, real prognostics systems are still scarce in industry. Indeed, it is hard to provide efficient approaches that are able to handle with the inherent uncertainty of prognostics and nobody is able to a priori ensure that an accurate prognostic model can be built. As for an example of remaining problems, consider data-driven prognostics approaches: how to ensure that a model will be able to face with inputs variation with respect to those ones that have been learned, how to ensure that a learned-model will face with unknown data, how to ensure convergence of algorithms, etc. In other words, robustness, reliability and applicability of a prognostic approach are still open areas. Following that, the aim of this paper is to address these challenges by proposing a new neural network (structure and algorithm) that enhances reliability of RUL estimates while improving applicability of the approach. Robustness, reliability and applicability aspects are first discussed and defined according to literature. On this basis, a new connexionist system is proposed for prognostics: the Improved-Extreme Learning machine (Imp-ELM). This neural network, based on complex activation functions, enables to reduce the influence of human choices and initial parameterization, while improving accuracy of estimates and speeding the learning phase. The whole proposition is illustrated by performing tests on a real industrial case of cutting tools from a Computer Numerical Control (CNC) machine. This is achieved by predicting tool condition (wear) in terms of remaining cuts successfully made. Thorough comparisons with adaptive neuro fuzzy inference system (ANFIS) and existing ELM algorithm are also given. Results show improved robustness, reliability and applicability performances.","",""
42,"Weiwei Qian, Shunming Li, Jinrui Wang","A New Transfer Learning Method and its Application on Rotating Machine Fault Diagnosis Under Variant Working Conditions",2018,"","","","",95,"2022-07-13 09:40:27","","10.1109/ACCESS.2018.2880770","","",,,,,42,10.50,14,3,4,"Effective data-driven rotating machine fault diagnosis has recently been a research topic in the diagnosis and health management of machinery systems owing to the benefits, including safety guarantee, labor saving, and reliability improvement. However, in vast real-world applications, the classifier trained on one dataset will be extended to datasets under variant working conditions. Meanwhile, the deviation between datasets can be triggered easily by rotating speed oscillation and load variation, and it will highly degenerate the performance of machine learning-based fault diagnosis methods. Hence, a novel dataset distribution discrepancy measuring algorithm called high-order Kullback–Leibler (HKL) divergence is proposed. Based on HKL divergence and transfer learning, a new fault diagnosis network which is robust to working condition variation is constructed in this paper. In feature extraction, sparse filtering with HKL divergence is proposed to learn sharing and discriminative features of the source and target domains. In feature classification, HKL divergence is introduced into softmax regression to link the domain adaptation with health conditions. Its effectiveness is verified by experiments on a rolling bearing dataset and a gearbox dataset, which include 18 transfer learning cases. Furthermore, the asymmetrical performance phenomenon found in experiments is also analyzed.","",""
8,"Naghmeh Karimi, K. Huang","Prognosis of NBTI aging using a machine learning scheme",2016,"","","","",96,"2022-07-13 09:40:27","","10.1109/DFT.2016.7684060","","",,,,,8,1.33,4,2,6,"Circuit aging is an important failure mechanism in nanoscale designs and is a growing concern for the reliability of future systems. Aging results in circuit performance degradation over time and the ultimate circuit failure. Among aging mechanisms, Negative-Bias Temperature Instability (NBTI) is the main limiting factor of circuits lifetime. Estimating the effect of aging-related degradation, before it actually occurs, is crucial for developing aging prevention/mitigations actions to avoid circuit failures. In this paper, we propose a general-purpose IC aging prognosis approach by considering a comprehensive set of IC operating conditions including workload, usage time and operating temperature. In addition, our model considers process variation by using a calibration technique applied at the time of manufacturing. Experimental results confirms that our model is able to accurately predict the NBTI-related path delay degradation under various operating conditions. The proposed model is robust to process variations.","",""
3,"Eric Yun","Analysis Of Machine Learning Classifier Performance In Adding Custom Gestures To The Leap Motion",2016,"","","","",97,"2022-07-13 09:40:27","","10.15368/THESES.2016.132","","",,,,,3,0.50,3,1,6,"Analysis Of Machine Learning Classifier Performance In Adding Custom Gestures To The Leap Motion Eric Yun The use of supervised machine learning to extend the capabilities and overall viability of motion sensing input devices has been an increasingly popular avenue of research since the release of the Leap Motion in 2013. The device's optical sensors are capable of recognizing and tracking key features of a user's hands and fingers, which can be obtained and manipulated through a robust API. This makes statistical classification ideal for tackling the otherwise laborious and error prone nature of adding new programmer-defined gestures to the set of recognized gestures. Although a handful of studies have explored the effectiveness of machine learning with the Leap Motion, none to our knowledge have run a comparative performance analysis of classification algorithms or made use of more than several of them in their experiments. The aim of this study is to improve the reliability of detecting newly added gestures by identifying the classifiers that produce the best results. To this end, a formal analysis of the most popular classifiers used in the field of machine learning was performed to determine those most appropriate to the requirements of the Leap Motion. A recording and","",""
34,"Petrônio L. Braga, Adriano Oliveira, S. Meira","Software Effort Estimation Using Machine Learning Techniques with Robust Confidence Intervals",2007,"","","","",98,"2022-07-13 09:40:27","","10.1109/HIS.2007.56","","",,,,,34,2.27,11,3,15,"The precision and reliability of the estimation of the effort of software projects is very important for the competitiveness of software companies. Good estimates play a very important role in the management of software projects. Most methods proposed for effort estimation, including methods based on machine learning, provide only an estimate of the effort for a novel project. In this paper we introduce a method based on machine learning which gives the estimation of the effort together with a confidence interval for it. In our method, we propose to employ robust confidence intervals, which do not depend on the form of probability distribution of the errors in the training set. We report on a number of experiments using two datasets aimed to compare machine learning techniques for software effort estimation and to show that robust confidence intervals for the effort estimation can be successfully built.","",""
33,"Petrônio L. Braga, Adriano Oliveira, S. Meira","Software Effort Estimation using Machine Learning Techniques with Robust Confidence Intervals",2007,"","","","",99,"2022-07-13 09:40:27","","10.1109/ICTAI.2007.172","","",,,,,33,2.20,11,3,15,"The precision and reliability of the estimation of the effort of software projects is very important for the competitiveness of software companies. Good estimates play a very important role in the management of software projects. Most methods proposed for effort estimation, including methods based on machine learning, provide only an estimate of the effort for a novel project. In this paper we introduce a method based on machine learning which gives the estimation of the effort together with a confidence interval for it. In our method, we propose to employ robust confidence intervals, which do not depend on the form of probability distribution of the errors in the training set. We report on a number of experiments using two datasets aimed to compare machine learning techniques for software effort estimation and to show that robust confidence intervals can be successfully built.","",""
10,"V. Priya, I. Thaseen, T. Gadekallu, Mohamed K. Aboudaif, E. A. Nasr","Robust Attack Detection Approach for IIoT Using Ensemble Classifier",2021,"","","","",100,"2022-07-13 09:40:27","","10.32604/cmc.2021.013852","","",,,,,10,10.00,2,5,1,"Generally, the risks associated with malicious threats are increasing for the Internet of Things (IoT) and its related applications due to dependency on the Internet and the minimal resource availability of IoT devices. Thus, anomaly-based intrusion detection models for IoT networks are vital. Distinct detection methodologies need to be developed for the Industrial Internet of Things (IIoT) network as threat detection is a significant expectation of stakeholders. Machine learning approaches are considered to be evolving techniques that learn with experience, and such approaches have resulted in superior performance in various applications, such as pattern recognition, outlier analysis, and speech recognition. Traditional techniques and tools are not adequate to secure IIoT networks due to the use of various protocols in industrial systems and restricted possibilities of upgradation. In this paper, the objective is to develop a two-phase anomaly detection model to enhance the reliability of an IIoT network. In the first phase, SVM and Naïve Bayes, are integrated using an ensemble blending technique. K-fold cross-validation is performed while training the data with different training and testing ratios to obtain optimized training and test sets. Ensemble blending uses a random forest technique to predict class labels. An Artificial Neural Network (ANN) classifier that uses the Adam optimizer to achieve better accuracy is also used for prediction. In the second phase, both the ANN and random forest results are fed to the model’s classification unit, and the highest accuracy value is considered the final result. The proposed model is tested on standard IoT attack datasets, such as WUSTL_IIOT-2018, N_BaIoT, and Bot_IoT. The highest accuracy obtained is 99%. A comparative analysis of the proposed model using state-of-the-art ensemble techniques is performed to demonstrate the superiority of the results. The results also demonstrate that the proposed model outperforms traditional techniques and thus improves the reliability of an IIoT network.","",""
21,"Amir Fazlollahi, F. Mériaudeau, V. Villemagne, C. Rowe, P. Yates, Olivier Salvado, P. Bourgeat","Efficient machine learning framework for computer-aided detection of cerebral microbleeds using the Radon transform",2014,"","","","",101,"2022-07-13 09:40:27","","10.1109/ISBI.2014.6867822","","",,,,,21,2.63,3,7,8,"Recent developments of susceptibility weighted MR techniques have improved visualization of venous vasculature and underlying pathologies such as cerebral microbleed (CMB). CMBs are small round hypointense lesions on MRI images that are emerging as a potential biomarker for cerebrovascular disease. CMB manual rating has limited reliability, is time-consuming and is prone to errors as small CMBs can be easily missed or mistaken for venous cross-sections. This paper presents a computer-aided detection technique that utilizes a novel cascade of random forest classifiers which are trained on robust Radon-based features with an unbalanced sample distribution. The training samples and their associated bounding box were acquired from a multi-scale Laplacian of Gaussian technique with respect to their geometric characteristics. Validation results demonstrate that the current approach outperforms state of the art approaches with sensitivity of 92.04% and an average false detection rate of 16.84 per subject.","",""
9,"E. Grooby, Jinyuan He, Julie Kiewsky, D. Fattahi, Lindsay Zhou, A. King, A. Ramanathan, A. Malhotra, G. Dumont, F. Marzbanrad","Neonatal Heart and Lung Sound Quality Assessment for Robust Heart and Breathing Rate Estimation for Telehealth Applications",2020,"","","","",102,"2022-07-13 09:40:27","","10.1109/JBHI.2020.3047602","","",,,,,9,4.50,1,10,2,"With advances in digital stethoscopes, internet of things, signal processing and machine learning, chest sounds can be easily collected and transmitted to the cloud for remote monitoring and diagnosis. However, low quality of recordings complicates remote monitoring and diagnosis, particularly for neonatal care. This paper proposes a new method to objectively and automatically assess the signal quality to improve the accuracy and reliability of heart rate (HR) and breathing rate (BR) estimation from noisy neonatal chest sounds. A total of 88 10-second long chest sounds were taken from 76 preterm and full-term babies. Six annotators independently assessed the signal quality, number of detectable beats, and breathing periods from these recordings. For quality classification, 187 and 182 features were extracted from heart and lung sounds, respectively. After feature selection, class balancing, and hyperparameter optimization, a dynamic binary classification model was trained. Then HR and BR were automatically estimated from the chest sound and several approaches were compared.The results of subject-wise leave-one-out cross-validation, showed that the model distinguished high and low quality recordings in the test set with 96% specificity, 81% sensitivity and 93% accuracy for heart sounds, and 86% specificity, 69% sensitivity and 82% accuracy for lung sounds. The HR and BR estimated from high quality sounds resulted in significantly less median absolute error (4 bpm and 12 bpm difference, respectively) compared to those from low quality sounds. The methods presented in this work, facilitates automated neonatal chest sound auscultation for future telehealth applications.","",""
11,"S. Tripathi, David Muhr, Manuel Brunner, F. Emmert‐Streib, H. Jodlbauer, M. Dehmer","Ensuring the Robustness and Reliability of Data-Driven Knowledge Discovery Models in Production and Manufacturing",2020,"","","","",103,"2022-07-13 09:40:27","","10.3389/frai.2021.576892","","",,,,,11,5.50,2,6,2,"The Cross-Industry Standard Process for Data Mining (CRISP-DM) is a widely accepted framework in production and manufacturing. This data-driven knowledge discovery framework provides an orderly partition of the often complex data mining processes to ensure a practical implementation of data analytics and machine learning models. However, the practical application of robust industry-specific data-driven knowledge discovery models faces multiple data- and model development-related issues. These issues need to be carefully addressed by allowing a flexible, customized and industry-specific knowledge discovery framework. For this reason, extensions of CRISP-DM are needed. In this paper, we provide a detailed review of CRISP-DM and summarize extensions of this model into a novel framework we call Generalized Cross-Industry Standard Process for Data Science (GCRISP-DS). This framework is designed to allow dynamic interactions between different phases to adequately address data- and model-related issues for achieving robustness. Furthermore, it emphasizes also the need for a detailed business understanding and the interdependencies with the developed models and data quality for fulfilling higher business objectives. Overall, such a customizable GCRISP-DS framework provides an enhancement for model improvements and reusability by minimizing robustness-issues.","",""
17,"R. Dutta, Ahsan Morshed","Performance Evaluation of South Esk Hydrological Sensor Web: Unsupervised Machine Learning and Semantic Linked Data Approach",2013,"","","","",104,"2022-07-13 09:40:27","","10.1109/JSEN.2013.2264666","","",,,,,17,1.89,9,2,9,"Technological progress has lead the sensor network domain to an era where environmental and agricultural domain applications are completely dependent on hydrological sensor networks. Data from the sensor networks are being used for knowledge management and critical decision support system. The quality of data can, however, vary widely. Existing automated quality assurance approach based on simple threshold rulebase could potentially miss serious errors requiring robust and complex domain knowledge to identify. This paper proposes a linked data concept, unsupervised pattern recognition, and semantic ontologies based dynamic framework to assess the reliability of hydrological sensor network and evaluate the performance of the sensor network. Newly designed framework is used successfully to evaluate the South Esk hydrological sensor web in Tasmania, indicating that domain ontology based linked data approach could be a very useful methodology for quality assurance of the complex data.","",""
2,"F. Kofler, I. Ezhov, Lucas Fidon, Carolin M. Pirkl, Johannes C. Paetzold, E. Burian, Sarthak Pati, Malek El Husseini, Fernando Navarro, Suprosanna Shit, J. Kirschke, S. Bakas, C. Zimmer, B. Wiestler, B. Menze","Robust, Primitive, and Unsupervised Quality Estimation for Segmentation Ensembles",2021,"","","","",105,"2022-07-13 09:40:27","","10.3389/fnins.2021.752780","","",,,,,2,2.00,0,15,1,"A multitude of image-based machine learning segmentation and classification algorithms has recently been proposed, offering diagnostic decision support for the identification and characterization of glioma, Covid-19 and many other diseases. Even though these algorithms often outperform human experts in segmentation tasks, their limited reliability, and in particular the inability to detect failure cases, has hindered translation into clinical practice. To address this major shortcoming, we propose an unsupervised quality estimation method for segmentation ensembles. Our primitive solution examines discord in binary segmentation maps to automatically flag segmentation results that are particularly error-prone and therefore require special assessment by human readers. We validate our method both on segmentation of brain glioma in multi-modal magnetic resonance - and of lung lesions in computer tomography images. Additionally, our method provides an adaptive prioritization mechanism to maximize efficacy in use of human expert time by enabling radiologists to focus on the most difficult, yet important cases while maintaining full diagnostic autonomy. Our method offers an intuitive and reliable uncertainty estimation from segmentation ensembles and thereby closes an important gap toward successful translation of automatic segmentation into clinical routine.","",""
1,"Farah Othmen, A. Lazzaretti, M. Baklouti, Marwa Jmal, M. Abid","A Sparse Representation Classification for Noise Robust Wrist-based Fall Detection",2021,"","","","",106,"2022-07-13 09:40:27","","10.5220/0010238804090416","","",,,,,1,1.00,0,5,1,"Elderly falls are becoming a more crucial and major health problem relatively with the significant growth of the involved population over the years. Wrist-based fall detection solution gained much interest for its comfortable and indoor-outdoor use, yet, a very moving and unstable location to the Inertial measurement unit. Indeed, acquired data might be exposed to random noises challenging the classifier’s reliability to spot falls among other daily activities. In this paper, we address the limits faced by Machine Learning models regarding noisy and overlapped data by proposing a study of the Supervised Dictionary Learning (SDL) technique for onwrist fall detection. Following the same prior work experimental protocol, the five most popular SDL models were evaluated and compared in performance with two benchmark Machine learning models. The evaluation setup follows two main experiments; processing clean data and casting different additive white Gaussian noise (AWGN). A distinguishable achievement was obtained by the SDL algorithms, of which the Sparse Representation-based Classifier (SRC) algorithm surpass other models especially using noisy data. The latter maintained almost 98% for 0db AWGN versus 96.4% for KNN.","",""
1,"Jaemoo Choi, Changyeon Yoon, Jeongwoo Bae, Myung-joo Kang","Robust Out-of-Distribution Detection on Deep Probabilistic Generative Models",2021,"","","","",107,"2022-07-13 09:40:27","","","","",,,,,1,1.00,0,4,1,"Out-of-distribution (OOD) detection is an important task in machine learning systems for ensuring their reliability and safety. Deep probabilistic generative models facilitate OOD detection by estimating the likelihood of a data sample. However, such models frequently assign a suspiciously high likelihood to a specific outlier. Several recent works have addressed this issue by training a neural network with auxiliary outliers, which are generated by perturbing the input data. In this paper, we discover that these approaches fail for certain OOD datasets. Thus, we suggest a new detection metric that operates without outlier exposure. We observe that our metric is robust to diverse variations of an image compared to the previous outlier-exposing methods. Furthermore, our proposed score requires neither auxiliary models nor additional training. Instead, this paper utilizes the likelihood ratio statistic in a new perspective to extract genuine properties from the given single deep probabilistic generative model. We also apply a novel numerical approximation to enable fast implementation. Finally, we demonstrate comprehensive experiments on various probabilistic generative models and show that our method achieves state-of-the-art performance.","",""
9,"Md Muztoba, U. Gupta, Tanvir Mustofa, Ümit Y. Ogras","Robust communication with IoT devices using wearable brain machine interfaces",2015,"","","","",108,"2022-07-13 09:40:27","","10.1109/ICCAD.2015.7372571","","",,,,,9,1.29,2,4,7,"Proliferation of internet-of-things (IoT) will lead to scenarios where humans will interact with and control a variety of networked devices including sensors and actuators. Wearable brain-machine interfaces (BMI) can be a key enabler of this interaction for people with disabilities and limited motor skills. At the same time, BMI can improve the experience of healthy individuals significantly. However, state-of-the-art BMI systems have limited applicability as they are prone to errors even with sophisticated machine learning algorithms used for classifying the electroencephalogram (EEG) signals. We improve the reliability of BMI communication significantly by proposing two techniques at higher abstraction layers. Our first contribution is a command confirmation protocol that protects the brain-machine communication against false interpretations at run time. The second contribution is an off-line optimal event selection algorithm that identifies the most reliable subset of events supported by the target BMI system. The event selection is guided by novel user specific reliability metrics defined for the first time in this paper. Extensive experiments using a commercial BMI system demonstrate that the proposed techniques increase the communication robustness significantly, and reduce the time to complete a complex navigation task by 63% on average.","",""
0,"Elizabeth Glista, S. Sojoudi","A MILP for Optimal Measurement Choice in Robust Power Grid State Estimation",2021,"","","","",109,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,2,1,"The reliability of the electric power grid is increasingly linked to the reliability of measured data which is used to understand the current state of the system. Determining the current state of the electric grid is the basis for decisionmaking related to the normal operation of the grid as well as operations in the case of an emergency scenario. When some of this data is corrupted in the case of a cyberattack, it is important that we can recover the true state of the system via state estimation (SE). Inspired by the work in [1] and [2], we propose a novel method using a notion in machine learning to optimize the choice of measurements in a given power network, formulating the problem as a mixed-integer linear program (MILP). Using this MILP, we study some test cases and show that it is impossible to certify that the network is fully robust in the case of bad data. However, we propose a method to optimally place the sensors in order to make the network more robust in the case of cyberattacks.","",""
0,"Mingrui Ma, Xuecong Tian, Fangfang Chen, Xiaojian Ma, Wenjia Guo, X. Lv","The application of feature engineering in establishing a rapid and robust model for identifying patients with glioma",2021,"","","","",110,"2022-07-13 09:40:27","","10.1007/s10103-021-03346-6","","",,,,,0,0.00,0,6,1,"","",""
3,"Zehai Gao, Cunbao Ma, Zhiyu She, Xu Dong","An Enhanced Deep Extreme Learning Machine for Integrated Modular Avionics Health State Estimation",2018,"","","","",111,"2022-07-13 09:40:27","","10.1109/ACCESS.2018.2878813","","",,,,,3,0.75,1,4,4,"Integrated modular avionics (IMA) is one of the most advanced systems whose performance deeply impact on the security of civil aircraft. In order to enhance the safety and reliability of aircraft, the health state of the IMA must be estimated accurately. Since IMA is a real-time system, the estimation algorithm should have fast learning speed to satisfy the real-time requirement. In this paper, an enhanced deep extreme learning machine is developed to estimate the health states of IMA. First, the enhanced deep extreme learning machine is built in a novel fashion by using a dropout technique and extreme learning machine autoencoder. Second, multiple-enhanced deep extreme learning machines with different activation functions are employed to estimate the health states, simultaneously. Finally, a synthesis strategy is designed to combine all the results of different enhanced deep extreme learning machines. In such a manner, the robust and accurate estimation results can be obtained. In order to collect the data under different health states, a performance degradation model of IMA is built by the intermittent faults. The proposed method is applied to health state estimation, and the results confirm that the proposed method can present a superior estimation to the conventional methods.","",""
2,"J. Ziegler, T. McJunkin, E. S. Joseph, Sandesh S. Kalantre, B. Harpt, D. Savage, M. Lagally, M. Eriksson, Jacob M. Taylor, Justyna P. Zwolak","Toward Robust Autotuning of Noisy Quantum Dot Devices",2021,"","","","",112,"2022-07-13 09:40:27","","10.1103/PhysRevApplied.17.024069","","",,,,,2,2.00,0,10,1,"The current autotuning approaches for quantum dot (QD) devices, while showing some success, lack an assessment of data reliability. This leads to unexpected failures when noisy or otherwise low-quality data is processed by an autonomous system. In this work, we propose a framework for robust autotuning of QD devices that combines a machine learning (ML) state classiﬁer with a data quality control module. The data quality control module acts as a “gatekeeper” system, ensuring that only reliable data are processed by the state classiﬁer. Lower data quality results in either device recalibration or termination. To train both ML systems, we enhance the QD simulation by incorporating synthetic noise typical of QD experiments. We conﬁrm that the inclusion of synthetic noise in the training of the state classiﬁer signiﬁcantly improves the performance, resulting in an accuracy of 95 . 0(9) % when tested on experimental data. We then validate the functionality of the data quality control module by showing that the state classiﬁer performance deteriorates with decreasing data quality, as expected. Our results establish a robust and ﬂexible ML framework for autonomous tuning of noisy QD devices.","",""
5,"G. Olague, Gerardo Ibarra-Vázquez, Mariana Chan-Ley, Cesar Puente, C. Soubervielle-Montalvo, Axel Martinez","A Deep Genetic Programming based Methodology for Art Media Classification Robust to Adversarial Perturbations",2020,"","","","",113,"2022-07-13 09:40:27","","10.1007/978-3-030-64556-4_6","","",,,,,5,2.50,1,6,2,"","",""
1,"C. He, M. Mahfouf, Luis A. Torres-Salomao","An Adaptive General Type-2 Fuzzy Logic Approach for Psychophysiological State Modeling in Real-Time Human–Machine Interfaces",2021,"","","","",114,"2022-07-13 09:40:27","","10.1109/THMS.2020.3027531","","",,,,,1,1.00,0,3,1,"In this article, a new type-2 fuzzy-based modeling approach is proposed to assess human operators’ psychophysiological states for both safety and reliability of human–machine interface systems. Such a new modeling technique combines type-2 fuzzy sets with state tracking to update the rule base through a Bayesian process. These new configurations successfully lead to an adaptive, robust, and transparent computational framework that can be utilized to identify dynamic (i.e., real time) features without prior training. The proposed framework is validated on mental arithmetic cognitive real-time experiments with ten participants. It is found that the proposed framework outperforms other paradigms (i.e., an adaptive neuro-fuzzy inference system and an adaptive general type-2 fuzzy c-means modeling approach) in terms of disturbance rejection and learning capabilities. The proposed framework achieved the best performance compared to other models that have been presented in the related literature. Therefore, the new framework can be a promising development in human–machine interface systems. It can be further utilized to develop advanced control mechanisms, investigate the origins of human compromised task performance, and identify and remedy psychophysiological breakdown in the early stages.","",""
2,"Tuan-Duy H. Nguyen, Huu-Nghia H. Nguyen","Towards a Robust WiFi-based Fall Detection with Adversarial Data Augmentation",2020,"","","","",115,"2022-07-13 09:40:27","","10.1109/CISS48834.2020.1570617398","","",,,,,2,1.00,1,2,2,"Recent WiFi-based fall detection systems have drawn much attention due to their advantages over other sensory systems. Various implementations have achieved impressive progress in performance, thanks to machine learning and deep learning techniques. However, many of such high accuracy systems have low reliability as they fail to achieve robustness in unseen environments. To address that, this paper investigates a method of generalization through adversarial data augmentation. Our results show a slight improvement in deep learning-systems in unseen domains, though the performance is not significant.","",""
0,"Vinay C. Patil, S. Kundu","Realizing Robust, Lightweight Strong PUFs for Securing Smart Grids",2022,"","","","",116,"2022-07-13 09:40:27","","10.1109/tce.2021.3139356","","",,,,,0,0.00,0,2,1,"Improving the reliability of energy distribution systems is a major concern to multiple parties as they are not only critical infrastructure themselves, but also affect other connected infrastructure. Smart Grids have been proposed to leverage the advantages of Internet of Things (IoT) to allow smarter management and faster recovery of energy distribution systems against disruptions. However, Smart Grid applications require a reliable, lightweight and fast authentication system to realize their potential in a secure manner. In this work, we propose a Strong PUF (physically unclonable function) system that can meet the stringent resource and performance constraints imposed by a Smart Grid operational environment. Our results indicate that the proposed mechanism produces a Strong PUF with close to ideal normalized uniqueness of 50 % and an accuracy of 50 % to modeling attacks using the best known machine learning algorithms. Additionally, our scheme has a low hardware overhead cost of $750~ \mu m^{2}$ in 45 nm technology and a sub-1 ms key generation time, ensuring the entire system is fast and lightweight.","",""
0,"Sanghamitra Dutta, Jason Long, Saumitra Mishra, Cecilia Tilli, D. Magazzeni","Robust Counterfactual Explanations for Tree-Based Ensembles",2022,"","","","",117,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,5,1,"Counterfactual explanations inform ways to achieve a desired outcome from a machine learning model. However, such explanations are not robust to certain real-world changes in the underlying model (e.g., retraining the model, changing hyperparameters, etc.), questioning their reliability in several applications, e.g., credit lending. In this work, we propose a novel strategy – that we call RobX – to generate robust counterfactuals for tree-based ensembles, e.g., XGBoost. Tree-based ensembles pose additional challenges in robust counterfactual generation, e.g., they have a non-smooth and non-diﬀerentiable objective function, and they can change a lot in the parameter space under retraining on very similar data. We ﬁrst introduce a novel metric – that we call Counterfactual Stability – that attempts to quantify how robust a counterfactual is going to be to model changes under retraining, and comes with desirable theoretical properties. Our proposed strategy RobX works with any counterfactual generation method (base method) and searches for robust counterfactuals by iteratively reﬁning the counterfactual generated by the base method using our metric Counterfactual Stability . We compare the performance of RobX with popular counterfactual generation methods (for tree-based ensembles) across benchmark datasets. The results demonstrate that our strategy generates counterfactuals that are signiﬁcantly more robust (nearly 100% validity after actual model changes) and also realistic (in terms of local outlier factor) over existing state-of-the-art methods.","",""
0,"M. Alessandrini, G. Biagetti, P. Crippa, L. Falaschetti, S. Luzzi, C. Turchetti","EEG-Based Alzheimer’s Disease Recognition Using Robust-PCA and LSTM Recurrent Neural Network",2022,"","","","",118,"2022-07-13 09:40:27","","10.3390/s22103696","","",,,,,0,0.00,0,6,1,"The use of electroencephalography (EEG) has recently grown as a means to diagnose neurodegenerative pathologies such as Alzheimer’s disease (AD). AD recognition can benefit from machine learning methods that, compared with traditional manual diagnosis methods, have higher reliability and improved recognition accuracy, being able to manage large amounts of data. Nevertheless, machine learning methods may exhibit lower accuracies when faced with incomplete, corrupted, or otherwise missing data, so it is important do develop robust pre-processing techniques do deal with incomplete data. The aim of this paper is to develop an automatic classification method that can still work well with EEG data affected by artifacts, as can arise during the collection with, e.g., a wireless system that can lose packets. We show that a recurrent neural network (RNN) can operate successfully even in the case of significantly corrupted data, when it is pre-filtered by the robust principal component analysis (RPCA) algorithm. RPCA was selected because of its stated ability to remove outliers from the signal. To demonstrate this idea, we first develop an RNN which operates on EEG data, properly processed through traditional PCA; then, we use corrupted data as input and process them with RPCA to filter outlier components, showing that even with data corruption causing up to 20% erasures, the RPCA was able to increase the detection accuracy by about 5% with respect to the baseline PCA.","",""
0,"P. Vaishnavi, Kevin Eykholt, Amir Rahmati","Transferring Adversarial Robustness Through Robust Representation Matching",2022,"","","","",119,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,3,1,"With the widespread use of machine learning, concerns over its security and reliability have become prevalent. As such, many have developed defenses to harden neural networks against adversarial examples, imperceptibly perturbed inputs that are reliably misclassiﬁed. Adversarial training in which adversarial examples are generated and used during training is one of the few known defenses able to reliably withstand such attacks against neural networks. However, adversarial training imposes a signiﬁcant training overhead and scales poorly with model complexity and input dimension. In this paper, we propose Robust Representation Matching (RRM) , a low-cost method to transfer the robustness of an adversarially trained model to a new model being trained for the same task irrespective of architectural differences. Inspired by student-teacher learning, our method introduces a novel training loss that encourages the student to learn the teacher’s robust representations. Compared to prior works, RRM is superior with respect to both model performance and adversarial training time. On CIFAR-10, RRM trains a robust model ∼ 1 . 8 × faster than the state-of-the-art. Furthermore, RRM remains effective on higher-dimensional datasets. On Restricted-ImageNet, RRM trains a ResNet50 model ∼ 18 × faster than standard adversarial training.","",""
0,"R. Barati, R. Safabakhsh, M. Rahmati","An Analytic Framework for Robust Training of Artificial Neural Networks",2022,"","","","",120,"2022-07-13 09:40:27","","10.48550/arXiv.2205.13502","","",,,,,0,0.00,0,3,1,"The reliability of a learning model is key to the successful deployment of machine learning in various industries. Creating a robust model, particularly one unaffected by adversarial attacks, requires a comprehensive understanding of the adversarial examples phenomenon. However, it is difﬁcult to describe the phenomenon due to the complicated nature of the problems in machine learning. Consequently, many studies investigate the phenomenon by proposing a simpliﬁed model of how adversarial examples occur and validate it by predicting some aspect of the phenomenon. While these studies cover many different characteristics of the adversarial examples, they have not reached a holistic approach to the geometric and analytic modeling of the phenomenon. This paper propose a formal framework to study the phenomenon in learning theory and make use of complex analysis and holomorphicity to offer a robust learning rule for artiﬁcial neural networks. With the help of complex analysis, we can effortlessly move between geometric and analytic perspectives of the phenomenon and offer further insights on the phenomenon by revealing its connection with harmonic functions. Using our model, we can explain some of the most intriguing characteristics of adversarial examples, including transferability of adversarial examples, and pave the way for novel approaches to mitigate the effects of the phenomenon.","",""
0,"Ryota Bingo, H. Yomo","Rate adaptation for robust data transmissions utilizing multi-AP reception and packet-level FEC",2020,"","","","",121,"2022-07-13 09:40:27","","10.1587/comex.2020col0038","","",,,,,0,0.00,0,2,2,"This letter proposes a rate control for robust data transmissions utilizing multiple access points (APs) combined with packet-level forward error correction (FEC). Beacon frames received from multiple APs are used to select an appropriate set of PHY and FEC rates to be employed for uplink transmissions, which is based on a machine-learning technique. With computer simulations, we show that the proposed rate control achieves the required reliability while significantly reducing the occupancy period of the shared channel.","",""
0,"Elie Atallah, N. Rahnavard, Chinwendu Enyioha","Straggler-Robust Distributed Optimization with the Parameter Server Utilizing Coded Gradient",2020,"","","","",122,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,3,2,"Optimization in distributed networks plays a central role in almost all distributed machine learning problems. In principle, the use of distributed task allocation has reduced the computational time, allowing better response rates and higher data reliability. However, for these computational algorithms to run effectively in complex distributed systems, the algorithms ought to compensate for communication asynchrony, and network node failures and delays known as stragglers. These issues can change the effective connection topology of the network, which may vary through time, thus hindering the optimization process. In this paper, we propose a new distributed unconstrained optimization algorithm for minimizing a strongly convex function which is adaptable to a parameter server network. In particular, the network worker nodes solve their local optimization problems, allowing the computation of their local coded gradients, and send them to different server nodes. Then each server node aggregates its communicated local gradients, allowing convergence to the desired optimizer. This algorithm is robust to network worker node failures or disconnection, or delays known as stragglers. One way to overcome the straggler problem is to allow coding over the network. We further extend this coding framework to enhance the convergence of the proposed algorithm under such varying network topologies. Finally, we implement the proposed scheme in MATLAB and provide comparative results demonstrating the effectiveness of the proposed framework.","",""
5,"Manish Kumar, P. Samui","Reliability Analysis of Settlement of Pile Group in Clay Using LSSVM, GMDH, GPR",2020,"","","","",123,"2022-07-13 09:40:27","","10.1007/s10706-020-01464-6","","",,,,,5,2.50,3,2,2,"","",""
4,"Magdalini Tyrtaiou, Andonis Papaleonidas, A. Elenas, L. Iliadis","Accomplished Reliability Level for Seismic Structural Damage Prediction Using Artificial Neural Networks",2020,"","","","",124,"2022-07-13 09:40:27","","10.1007/978-3-030-48791-1_6","","",,,,,4,2.00,1,4,2,"","",""
2,"Huamei Zhang, S. Zhou, Cheng Xu, J. Zhang","A ROBUST APPROACH FOR THREE-DIMENSIONAL REAL-TIME TARGET LOCALIZATION UNDER AMBIGUOUS WALL PARAMETERS",2020,"","","","",125,"2022-07-13 09:40:27","","10.2528/pierm20060701","","",,,,,2,1.00,1,4,2,"To obtain three-dimensional (3-D) high-precision and real-time through-wall location under ambiguous wall parameters, an approach based on the extreme learning machine (ELM) which is a neural network is proposed. The wall’s ambiguity and propagation effects are both included in the hidden layer feedforward network, and then the through-wall location problem is converted to a regression problem. The relationship between the scattered signals and the target properties are determined after the training process. Then the target properties are estimated using the ELM approach. Numerical results demonstrate good performance in terms of effectiveness, generalization, and robustness, especially for the kernel extreme learning machine (KELM) approach. Noiseless and noisy measurements are performed to further demonstrate that the approach can provide good performance in terms of stability and reliability. The location time, including the training time and test time, is also discussed, and the results show that the KELM approach is very suitable for real-time location problems. Compared to the machine learning approach, the KELM approach is better not only in the aspect of accuracy but also in location time.","",""
0,"Roberto Mag'an-Carri'on, D. Urda, Ignacio D'iaz-Cano, B. Dorronsoro","Improving the Reliability of Network Intrusion Detection Systems through Dataset Integration",2021,"","","","",126,"2022-07-13 09:40:27","","10.1109/tetc.2022.3178283","","",,,,,0,0.00,0,4,1,"This work presents Reliable-NIDS (R-NIDS), a novel methodology for Machine Learning (ML) based Network Intrusion Detection Systems (NIDSs) that allows ML models to work on integrated datasets, empowering the learning process with diverse information from different datasets. Therefore, R-NIDS targets the design of more robust models, that generalize better than traditional approaches. We also propose a new dataset, called UNK21. It is built from three of the most well-known network datasets (UGR’16, USNW-NB15 and NLS-KDD), each one gathered from its own network environment, with different features and classes, by using a data aggregation approach present in R-NIDS. Following R-NIDS, in this work we propose to build two well-known ML models (a linear and a non-linear one) based on the information of three of the most common datasets in the literature for NIDS evaluation, those integrated in UNK21. The results that the proposed methodology offers show how these two ML models trained as a NIDS solution could benefit from this approach, being able to generalize better when training on the newly proposed UNK21 dataset. Furthermore, these results are carefully analyzed with statistical tools that provide high confidence on our conclusions.","",""
1,"Elvis Rojas, Esteban Meneses, T. Jones, Don E. Maxwell","Towards a Model to Estimate the Reliability of Large-Scale Hybrid Supercomputers",2020,"","","","",127,"2022-07-13 09:40:27","","10.1007/978-3-030-57675-2_3","","",,,,,1,0.50,0,4,2,"","",""
2,"Yufang Dan, Jianwen Tao, Jianjing Fu, Di Zhou","Possibilistic Clustering-Promoting Semi-Supervised Learning for EEG-Based Emotion Recognition",2021,"","","","",128,"2022-07-13 09:40:27","","10.3389/fnins.2021.690044","","",,,,,2,2.00,1,4,1,"The purpose of the latest brain computer interface is to perform accurate emotion recognition through the customization of their recognizers to each subject. In the field of machine learning, graph-based semi-supervised learning (GSSL) has attracted more and more attention due to its intuitive and good learning performance for emotion recognition. However, the existing GSSL methods are sensitive or not robust enough to noise or outlier electroencephalogram (EEG)-based data since each individual subject may present noise or outlier EEG patterns in the same scenario. To address the problem, in this paper, we invent a Possibilistic Clustering-Promoting semi-supervised learning method for EEG-based Emotion Recognition. Specifically, it constrains each instance to have the same label membership value with its local weighted mean to improve the reliability of the recognition method. In addition, a regularization term about fuzzy entropy is introduced into the objective function, and the generalization ability of membership function is enhanced by increasing the amount of sample discrimination information, which improves the robustness of the method to noise and the outlier. A large number of experimental results on the three real datasets (i.e., DEAP, SEED, and SEED-IV) show that the proposed method improves the reliability and robustness of the EEG-based emotion recognition.","",""
2,"Yan Chen, Jaylin Herskovitz, Walter S. Lasecki, Steve Oney","Bashon: A Hybrid Crowd-Machine Workflow for Shell Command Synthesis",2020,"","","","",129,"2022-07-13 09:40:27","","10.1109/VL/HCC50065.2020.9127248","","",,,,,2,1.00,1,4,2,"Despite advances in machine learning, there has been little progress towards creating automated systems that can reliably solve general purpose tasks, such as programming or scripting. In this paper, we propose techniques for increasing the reliability of automated systems for program synthesis tasks via a hybrid workflow that augments the system with input from crowds of human workers. Unlike previous hybrid workflow systems, which have been focused on less complex tasks that crowd workers can do in their entirety (e.g., image labeling), our proposed workflow handles tasks that untrained crowd workers cannot do alone (i.e., scripting). We evaluate our approach by creating BashOn, a system that increases the performance of an automated program that generates Bash shell commands from natural language descriptions by ~30%. Our approach can not only help people make program synthesis tools more robust, reliable, and trustworthy for end-users to use, but also help lower the cost of downstream data collection for program synthesis when a preliminary model exists.","",""
1,"Nadezhda Chukhno, Olga Chukhno, S. Pizzi, A. Molinaro, A. Iera, G. Araniti","Unsupervised Learning for D2D-Assisted Multicast Scheduling in mmWave Networks",2021,"","","","",130,"2022-07-13 09:40:27","","10.1109/BMSB53066.2021.9547189","","",,,,,1,1.00,0,6,1,"The combination of multicast and directional mmWave communication paves the way for solving spectrum crunch problems, increasing spectrum efficiency, ensuring reliability, and reducing access point load. Furthermore, multi-hop relaying is considered as one of the key interest areas in future 5G+ systems to achieve enhanced system performance. Based on this approach, users located close to the base station may serve as relays towards cell-edge users in their proximity by using more robust device-to-device (D2D) links, which is essential, e.g., to reduce the power consumption for wearable devices. In this paper, we account for the limitations and capabilities of directional mmWave multicast systems by proposing a low-complexity heuristic solution that leverages an unsupervised machine learning algorithm for multicast group formation and by exploiting the D2D technology to deal with the blockage problem.","",""
59,"Durga Prasad Sahoo, Debdeep Mukhopadhyay, R. Chakraborty, Phuong Ha Nguyen","A Multiplexer-Based Arbiter PUF Composition with Enhanced Reliability and Security",2018,"","","","",131,"2022-07-13 09:40:27","","10.1109/TC.2017.2749226","","",,,,,59,14.75,15,4,4,"Arbiter Physically Unclonable Functions (APUFs), while being relatively lightweight, are extremely vulnerable to modeling attacks. Hence, various compositions of APUFs such as XOR APUF and Lightweight Secure PUF have been proposed to be secure alternatives. Previous research has demonstrated that PUF compositions have two major challenges to overcome: vulnerability against modeling and statistical attacks, and lack of reliability. In this paper, we introduce a multiplexer-based composition of APUFs, denoted as MPUF, to simultaneously overcome these challenges. In addition to the basic MPUF design, we propose two MPUF variants namely cMPUF and rMPUF to improve the robustness against cryptanalysis and reliability-based modeling attack, respectively. An rMPUF demonstrates enhanced robustness against the reliability-based modeling attack, while even the well-known XOR APUF, otherwise robust to machine learning based modeling attacks, has been modeled using the same technique with linear data and time complexities. The rMPUF can provide a good trade-off between security and hardware overhead while maintaining a significantly higher reliability level than any practical XOR APUF instance. Moreover, MPUF variants are the first APUF compositions, to the best of our knowledge, that can achieve Strict Avalanche Criterion without using any additional input network (or hardware) for challenge transformation. Finally, we validate our theoretical findings using Matlab-based simulations of MPUFs.","",""
8,"Z. Barger, Charles G. Frye, Danqian Liu, Y. Dan, K. Bouchard","Robust, automated sleep scoring by a compact neural network with distributional shift correction",2019,"","","","",132,"2022-07-13 09:40:27","","10.1101/813345","","",,,,,8,2.67,2,5,3,"Studying the biology of sleep requires the accurate assessment of the state of experimental subjects, and manual analysis of relevant data is a major bottleneck. Recently, deep learning applied to electroencephalogram and electromyogram data has shown great promise as a sleep scoring method, approaching the limits of inter-rater reliability. As with any machine learning algorithm, the inputs to a sleep scoring classifier are typically standardized in order to remove distributional shift caused by variability in the signal collection process. However, in scientific data, experimental manipulations introduce variability that should not be removed. For example, in sleep scoring, the fraction of time spent in each arousal state can vary between control and experimental subjects. We introduce a standardization method, mixture z-scoring, that preserves this crucial form of distributional shift. Using both a simulated experiment and mouse in vivo data, we demonstrate that a common standardization method used by state-of-the-art sleep scoring algorithms introduces systematic bias, but that mixture z-scoring does not. We present a free, open-source user interface that uses a compact neural network and mixture z-scoring to allow for rapid sleep scoring with accuracy that compares well to contemporary methods. This work provides a set of computational tools for the robust automation of sleep scoring.","",""
7,"I. L. Ruiz, M. A. Gómez-Nieto","Building of Robust and Interpretable QSAR Classification Models by Means of the Rivality Index",2019,"","","","",133,"2022-07-13 09:40:27","","10.1021/acs.jcim.9b00264","","",,,,,7,2.33,4,2,3,"An unambiguous algorithm, added to the study of the applicability domain and appropriate measures of the goodness of fit and robustness, represent the key characteristics that should be ideally fulfilled for a QSAR model to be considered for regulatory purposes. In this paper, we propose a new algorithm (RINH) based on the rivality index for the construction of QSAR classification models. This index is capable of predicting the activity of the data set molecules by means of a measurement of the rivality between their nearest neighbors belonging to different classes, contributing with a robust measurement of the reliability of the predictions. In order to demonstrate the goodness of the proposed algorithm we have selected four independent and orthogonally different benchmark data sets (balanced/unbalanced and high/low modelable) and we have compared the results with those obtained using 12 different machine learning algorithms. These results have been validated using 20 data sets of different balancing and sizes, corroborating that the proposed algorithm is able to generate highly accurate classification models and contribute with valuable measurements of the reliability of the predictions and the applicability domain of the built models.","",""
5,"Alvaro H. C. Correia, Cassio P. de Campos","Towards Scalable and Robust Sum-Product Networks",2019,"","","","",134,"2022-07-13 09:40:27","","10.1007/978-3-030-35514-2_31","","",,,,,5,1.67,3,2,3,"","",""
3,"Jiantao Liu, Xiaoxiang Yang, Mingzhu Zhu","Neural Network with Confidence Kernel for Robust Vibration Frequency Prediction",2019,"","","","",135,"2022-07-13 09:40:27","","10.1155/2019/6573513","","",,,,,3,1.00,1,3,3,"Image-based measurement has received increasing attention as it can substantially reduce the cost of labor, measurement equipment, and installation process. Instead of using optical flow, pattern, or marker tracking to extract a displacement signal, in this study, a novel noncontact machine learning-based system was proposed to directly predict vibration frequency with high accuracy and good reliability by using image sequences acquired from a single camera. The performance of the proposed method was demonstrated through experiments conducted in a laboratory and under real-field conditions and compared with those obtained using a contacted sensor. The vibration frequency prediction results of the proposed method are compared with industry-level vibration sensor results in the frequency domain, demonstrating that the proposed method could predict the target-object-vibration frequency as accurately as an industry-level vibration sensor, even under uncontrollable real-field conditions with no additional enhancement or extra signal processing techniques. However, only the principal vibration frequency of a measurement target is predicted, and the measurement range is limited by the trained model. Nonetheless, if these limitations are resolved, this method can potentially be used in real engineering applications in mechanical or civil structural health monitoring thanks to the simple deployment and concise pipeline of this method.","",""
10,"Safa Omri, C. Sinz","Deep Learning for Software Defect Prediction: A Survey",2020,"","","","",136,"2022-07-13 09:40:27","","10.1145/3387940.3391463","","",,,,,10,5.00,5,2,2,"Software fault prediction is an important and beneficial practice for improving software quality and reliability. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Traditional software fault prediction studies mainly focus on manually designing features (e.g. complexity metrics), which are input into machine learning classifiers to identify defective code. However, these features often fail to capture the semantic and structural information of programs. Such information is needed for building accurate fault prediction models. In this survey, we discuss various approaches in fault prediction, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs' semantics and fault prediction features and make accurate predictions.","",""
4,"I. M. Salte, A. Oestvik, E. Smistad, D. Melichova, T. M. Nguyen, H. Brunvand, T. Edvardsen, L. Loevstakken, B. Grenne","545 Deep learning/artificial intelligence for automatic measurement of global longitudinal strain by echocardiography",2020,"","","","",137,"2022-07-13 09:40:27","","10.1093/ehjci/jez319.279","","",,,,,4,2.00,0,9,2,"      The Norwegian Health Association, South-Eastern Norway regional health Authority and the national program for clinical therapy research (KLINBEFORSK).        Global longitudinal strain (GLS) by echocardiography has incremental prognostic value in patients with acute myocardial infarction and heart failure compared to left ventricular (LV) ejection fraction and provides more reproducible measurements of LV function. Recent advances in machine learning for image analysis now open the possibility for robust fully automated tracing of the LV and measurement of global longitudinal strain (GLS), without any operator input. This could make real-time GLS possible and remove inter-reader variability, thus resulting in saved time and improved test-retest reliability. The aim of the present study was to investigate how measurements by this novel automatic method compares to conventional speckle tracking analyses of GLS.        100 transthoracic echocardiographic examinations were included from a clinical database of patients with acute myocardial infarction or de-novo heart failure. Examinations were included consecutively and regardless of image quality. Simpson biplane LV ejection fraction ranged from 7 to 70%. Images of three standard apical planes from each examination were analysed using our novel and fully automated GLS method based on deep learning technology. The automated GLS measurements were compared to conventional speckle tracking GLS measurements of the same acquisitions using vendor specific format and software (EchoPAC, GE Healthcare), performed by a single experienced observer.        GLS was -11.6 ± 4.5% and -12.8 ± 5.0% for the deep learning method and the conventional method, respectively. Bland-Altman analysis showed a bias of -0.7 ± 1,9% and 95% limits of agreement of -4,6 to 3.1. No clear value dependent bias was found by visual inspection (Figure A). Feasibility for measurement of GLS was 93% for the deep learning based method and 99% for the conventional method. The limits of agreement found in our study is comparable to findings in the intervendor comparison study by the EASCVI/ASE/Industry Task force to standardize deformation imaging.        This novel deep learning based method succeeds without any operator input to automatically identify and classify the three apical standard views, trace the myocardium, perform motion estimation and measure global longitudinal strain. This could further facilitate the clinical use of GLS as an important tool for enhancing clinical decision-making.  Abstract 545 Figure. ","",""
0,"Jingyi Dong, Datong Liu","Sensor Data Prediction for Fixed-wing Drone Based on Online Sequential Learning",2022,"","","","",138,"2022-07-13 09:40:27","","10.1109/I2MTC48687.2022.9806574","","",,,,,0,0.00,0,2,1,"The special properties make the fixed-wing drones are widely used in the military area. Thus, the validity and the accuracy of the sensors and actuators are essential for fixed-wing drones. The sensors data prediction with a suitable learning algorithm is usually the basic step for data-driven approaches to increase the reliability of fixed-wing drones. Online learning algorithms could train the model from sensors data in real time, coinciding with the rich rough sensors data during the whole flight time. In addition, online learning algorithms based on Extreme Learning Machine, which is a light weight learning machine, show a great performance for time-series prediction. However, they have two drawbacks: overfitting models and less robust. Thus, we proposed a novel algorithm to promote the weights update strategy by updating the output weights based on statistical learning theory. And, the experiment of sensors data perdition shows the improved performance of the proposed algorithm experiment om compared with other online learning algorithms based on ELM.","",""
8,"Fereshteh Abedini, Mahdi Bahaghighat, Misak S’hoyan","Wind turbine tower detection using feature descriptors and deep learning",2020,"","","","",139,"2022-07-13 09:40:27","","10.2298/FUEE2001133A","","",,,,,8,4.00,3,3,2,"Wind Turbine Towers (WTTs) are the main structures of wind farms. They are costly devices that must be thoroughly inspected according to maintenance plans. Today, existence of machine vision techniques along with unmanned aerial vehicles (UAVs) enable fast, easy, and intelligent visual inspection of the structures. Our work is aimed towards developing a visionbased system to perform Nondestructive tests (NDTs) for wind turbines using UAVs. In order to navigate the ﬂying machine toward the wind turbine tower and reliably land on it, the exact position of the wind turbine and its tower must be detected. We employ several strong computer vision approaches such as Scale-Invariant Feature Transform (SIFT), Speeded Up Robust Features (SURF), Features from Accelerated Segment Test (FAST), Brute-Force, Fast Library for Approximate Nearest Neighbors (FLANN) to detect the WTT. Then, in order to increase the reliability of the system, we apply the ResNet, MobileNet, ShuﬄeNet, EﬀNet, and SqueezeNet pre-trained classiﬁers in order to verify whether a detected object is indeed a turbine tower or not. This intelligent monitoring system has auto navigation ability and can be used for future goals including intelligent fault diagnosis and maintenance purposes. The simulation results show the accuracy of the proposed model are 89.4% in WTT detection and 97.74% in veriﬁcation (classiﬁcation) problems.","",""
56,"F. Mahdisoltani, Ioan A. Stefanovici, Bianca Schroeder","Proactive error prediction to improve storage system reliability",2017,"","","","",140,"2022-07-13 09:40:27","","","","",,,,,56,11.20,19,3,5,"This paper proposes the use of machine learning techniques to make storage systems more reliable in the face of sector errors. Sector errors are partial drive failures, where individual sectors on a drive become unavailable, and occur at a high rate in both hard disk drives and solid state drives. The data in the affected sectors can only be recovered through redundancy in the system (e.g. another drive in the same RAID) and is lost if the error is encountered while the system operates in degraded mode, e.g. during RAID reconstruction. In this paper, we explore a range of different machine learning techniques and show that sector errors can be predicted ahead of time with high accuracy. Prediction is robust, even when only little training data or only training data for a different drive model is available. We also discuss a number of possible use cases for improving storage system reliability through the use of sector error predictors. We evaluate one such use case in detail: We show that the mean time to detecting errors (and hence the window of vulnerability to data loss) can be greatly reduced by adapting the speed of a scrubber based on error predictions.","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",141,"2022-07-13 09:40:27","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
7,"Anudit Nagar","Privacy-Preserving Blockchain Based Federated Learning with Differential Data Sharing",2019,"","","","",142,"2022-07-13 09:40:27","","","","",,,,,7,2.33,7,1,3,"For the modern world where data is becoming one of the most valuable assets, robust data privacy policies rooted in the fundamental infrastructure of networks and applications are becoming an even bigger necessity to secure sensitive user data. In due course with the ever-evolving nature of newer statistical techniques infringing user privacy, machine learning models with algorithms built with respect for user privacy can offer a dynamically adaptive solution to preserve user privacy against the exponentially increasing multidimensional relationships that datasets create. Using these privacy aware ML Models at the core of a Federated Learning Ecosystem can enable the entire network to learn from data in a decentralized manner. By harnessing the ever-increasing computational power of mobile devices, increasing network reliability and IoT devices revolutionizing the smart devices industry, and combining it with a secure and scalable, global learning session backed by a blockchain network with the ability to ensure on-device privacy, we allow any Internet enabled device to participate and contribute data to a global privacy preserving, data sharing network with blockchain technology even allowing the network to reward quality work. This network architecture can also be built on top of existing blockchain networks like Ethereum and Hyperledger, this lets even small startups build enterprise ready decentralized solutions allowing anyone to learn from data across different departments of a company, all the way to thousands of devices participating in a global synchronized learning network.","",""
3,"S. Pasricha","Overcoming Energy and Reliability Challenges for IoT and Mobile Devices with Data Analytics",2018,"","","","",143,"2022-07-13 09:40:27","","10.1109/VLSID.2018.69","","",,,,,3,0.75,3,1,4,"A very large amount of data is produced by mobile and Internet-of-Thing (IoT) devices today. Increasing computational abilities and more sophisticated operating systems (OS) on these devices have allowed us to create applications that are able to leverage this data to deliver better services. But today's mobile and IoT solutions are heavily limited by low battery capacity and limited cooling capabilities. This motivates a search for new ways to optimize for energy-efficiency. Advanced data analytics and machine-learning techniques are becoming increasingly popular to analyze and extract meaning from Big Data. In this paper, we make the case for designing and deploying data analytics and learning mechanisms to improve energy-efficiency in IoT and mobile devices with minimal overheads. We focus on middleware for inserting energy-efficient data analytics-driven solutions and optimizations in a robust manner, without altering the OS or application code. We discuss several case studies of powerful and promising developments in deploying data analytics middleware for energy-efficient and robust execution of a variety of applications on commodity mobile devices.","",""
17,"F. Mahdisoltani, Ioan A. Stefanovici, Bianca Schroeder","Improving Storage System Reliability with Proactive Error Prediction",2017,"","","","",144,"2022-07-13 09:40:27","","","","",,,,,17,3.40,6,3,5,"This paper proposes using techniques from machine learning to make storage systems more reliable in the face of sector errors. Sector errors are partial drive failures, where individual sectors on a drive become unavailable, and occur at a high rate in both hard disk drives and solid state drives. The data in the affected sectors can only be recovered through external forms of redundancy (e.g. another drive in the same RAID), and be lost if the error is encountered while the system operates in degraded mode, e.g. during RAID reconstruction. In this paper, we explore a range of different machine learning techniques and show that sector errors can be predicted ahead of time with high accuracy. Prediction is robust, even when only little training data or only training data for a different drive model is available. We also discuss a number of possible use cases for improving storage system reliability through the use of sector error predictors. We evaluate one such use case in detail: We show that the mean time to detecting errors (and hence the window of vulnerability to data loss) can be greatly reduced by adapting the speed of a scrubber based on error predictions.","",""
3,"Wei Ye, Mohamed Baker Alawieh, Yibo Lin, D. Pan","Tackling signal electromigration with learning-based detection and multistage mitigation",2019,"","","","",145,"2022-07-13 09:40:27","","10.1145/3287624.3287688","","",,,,,3,1.00,1,4,3,"With the continuous scaling of integrated circuit (IC) technologies, electromigration (EM) prevails as one of the major reliability challenges facing the design of robust circuits. With such aggressive scaling in advanced technology nodes, signal nets experience high switching frequency, which further exacerbates the signal EM effect. Traditionally, signal EM fixing approaches analyze EM violations after the routing stage and repair is attempted via iterative incremental routing or cell resizing techniques. However, these ""EM-analysis-then fix"" approaches are ill-equipped when faced with the ever-growing EM violations in advanced technology nodes. In this work, we propose a novel signal EM handling framework that (i) incorporates EM detection and fixing techniques into earlier stages of the physical design process, and (ii) integrates machine learning based detection alongside a multistage mitigation. Experimental results demonstrate that our framework can achieve 15x speedup when compared to the state-of-the-art EDA tool while achieving similar performance in terms of EM mitigation and overhead.","",""
0,"Wei Ye","Tackling Signal Electromigration with Learning-Based Detection and Multistage Mitigation",2019,"","","","",146,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,1,3,"With the continuous scaling of integrated circuit (IC) technologies, electromigration (EM) prevails as one of the major reliability challenges facing the design of robust circuits. With such aggressive scaling in advanced technology nodes, signal nets experience high switching frequency, which further exacerbates the signal EM effect. Traditionally, signal EM fixing approaches analyze EM violations after the routing stage and repair is attempted via iterative incremental routing or cell resizing techniques. However, these “EM-analysis-then fix” approaches are ill-equipped when faced with the ever-growing EM violations in advanced technology nodes. In this work, we propose a novel signal EM handling framework that (i) incorporates EM detection and fixing techniques into earlier stages of the physical design process, and (ii) integrates machine learning based detection alongside a multistage mitigation. Experimental results demonstrate that our framework can achieve 15× speedup when compared to the state-of-the-art EDA tool while achieving similar performance in terms of EM mitigation and overhead.","",""
30,"S. Benatti, E. Farella, E. Gruppioni, L. Benini","Analysis of Robust Implementation of an EMG Pattern Recognition based Control",2014,"","","","",147,"2022-07-13 09:40:27","","10.5220/0004800300450054","","",,,,,30,3.75,8,4,8,"Control of active hand prostheses is an open challenge. In fact, the advances in mechatronics made available prosthetic hands with multiple active degrees of freedom; however the predominant control strategies are still not natural for the user, enabling only few gestures, thus not exploiting the prosthesis potential. Pattern recognition and machine learning techniques can be of great help when applied to surface electromyography signals to offer a natural control based on the contraction of muscles corresponding to the real movements. The implementation of such approach for an active prosthetic system offers many challenges related to the reliability of data collected to train the classification algorithm. This paper focuses on these problems and propose an implementation suitable for an embedded system.","",""
18,"G. Imbalzano, Yongbin Zhuang, V. Kapil, K. Rossi, Edgar A. Engel, F. Grasselli, M. Ceriotti","Uncertainty estimation for molecular dynamics and sampling.",2021,"","","","",148,"2022-07-13 09:40:27","","10.1063/5.0036522","","",,,,,18,18.00,3,7,1,"Machine-learning models have emerged as a very effective strategy to sidestep time-consuming electronic-structure calculations, enabling accurate simulations of greater size, time scale, and complexity. Given the interpolative nature of these models, the reliability of predictions depends on the position in phase space, and it is crucial to obtain an estimate of the error that derives from the finite number of reference structures included during model training. When using a machine-learning potential to sample a finite-temperature ensemble, the uncertainty on individual configurations translates into an error on thermodynamic averages and leads to a loss of accuracy when the simulation enters a previously unexplored region. Here, we discuss how uncertainty quantification can be used, together with a baseline energy model, or a more robust but less accurate interatomic potential, to obtain more resilient simulations and to support active-learning strategies. Furthermore, we introduce an on-the-fly reweighing scheme that makes it possible to estimate the uncertainty in thermodynamic averages extracted from long trajectories. We present examples covering different types of structural and thermodynamic properties and systems as diverse as water and liquid gallium.","",""
26,"R. Malhotra, Arun Negi","Reliability modeling using Particle Swarm Optimization",2013,"","","","",149,"2022-07-13 09:40:27","","10.1007/s13198-012-0139-0","","",,,,,26,2.89,13,2,9,"","",""
4,"Alok Singh, E. Stephan, Todd O. Elsethagen, M. MacDuff, B. Raju, M. Schram, K. K. Dam, D. Kerbyson, I. Altintas","Leveraging large sensor streams for robust cloud control",2016,"","","","",150,"2022-07-13 09:40:27","","10.1109/BigData.2016.7840839","","",,,,,4,0.67,0,9,6,"Today's dynamic computing deployment for commercial and scientific applications is propelling us to an era where minor inefficiencies can snowball into significant performance and operational bottlenecks. Data center operations is increasingly relying on sensors based control systems for key decision insights. The increased sampling frequencies, cheaper storage costs and prolific deployment of sensors is producing massive volumes of operational data. However, there is a lag between rapid development of analytical techniques and its widespread practical deployment. We present empirical evidence of the potential carried by analytical techniques for operations management in computing and data centers. Using Machine Learning modeling techniques on data from a real instrumented cluster, we demonstrate that predictive modeling on operational sensor data can directly reduce systems operations monitoring costs and improve system reliability.","",""
7,"J. Granek, E. Haber","Data mining for real mining: A robust algorithm for prospectivity mapping with uncertainties",2015,"","","","",151,"2022-07-13 09:40:27","","10.1137/1.9781611974010.17","","",,,,,7,1.00,4,2,7,"Mineral prospectivity mapping is an emerging application for machine learning algorithms which presents a series of practical difficulties. The goal is to learn the mapping function which can predict the existence or absence of economic mineralization from a compilation of geoscience datasets (ie: bedrock type, magnetic signature, geochemical response etc). The challenges include sparse, imbalanced labels (mineralization occurrences), varied label reliability, and a wide range in data quality and uncertainty. In order to address these issues an algorithm was developed based on total least squares and support vector machine regression which incorporates both data and label uncertainty into the objective function. This was done without losing sparsity in the residuals, thus maintaining minimal support vectors. Mineral prospectivity mapping is an application for machine learning which presents a series of practical difficulties. The goal is to learn the mapping function which can predict the existence of mineralization from a compilation of geoscience datasets. Challenges include sparse, imbalanced labels, varied label reliability, and a wide range in data uncertainty. To address this, an algorithm was developed based on TLS and SVM which incorporates both data and label uncertainty into the objective function.","",""
10,"Li He, Yuelong Wang, Yongning Yang, Liqiu Huang, Z. Wen","Identifying the Gene Signatures from Gene-Pathway Bipartite Network Guarantees the Robust Model Performance on Predicting the Cancer Prognosis",2014,"","","","",152,"2022-07-13 09:40:27","","10.1155/2014/424509","","",,,,,10,1.25,2,5,8,"For the purpose of improving the prediction of cancer prognosis in the clinical researches, various algorithms have been developed to construct the predictive models with the gene signatures detected by DNA microarrays. Due to the heterogeneity of the clinical samples, the list of differentially expressed genes (DEGs) generated by the statistical methods or the machine learning algorithms often involves a number of false positive genes, which are not associated with the phenotypic differences between the compared clinical conditions, and subsequently impacts the reliability of the predictive models. In this study, we proposed a strategy, which combined the statistical algorithm with the gene-pathway bipartite networks, to generate the reliable lists of cancer-related DEGs and constructed the models by using support vector machine for predicting the prognosis of three types of cancers, namely, breast cancer, acute myeloma leukemia, and glioblastoma. Our results demonstrated that, combined with the gene-pathway bipartite networks, our proposed strategy can efficiently generate the reliable cancer-related DEG lists for constructing the predictive models. In addition, the model performance in the swap analysis was similar to that in the original analysis, indicating the robustness of the models in predicting the cancer outcomes.","",""
15,"P. Michel, V. Heiries","An Adaptive Sigma Point Kalman Filter Hybridized by Support Vector Machine Algorithm for Battery SoC and SoH Estimation",2015,"","","","",153,"2022-07-13 09:40:27","","10.1109/VTCSpring.2015.7145678","","",,,,,15,2.14,8,2,7,"This paper considers the issue of Li-Ion batteries State of Health (SoH) and State of Charge (SoC) accurate and robust estimation for electric vehicle applications. SoC and SoH are two monitoring indicators of primary importance that are used by the Battery Management System (BMS), amongst other benefits, to manage and equalize the battery cells. Improving the estimation precision and reliability of the SoC and the SoH indicators is highly beneficial during operation and maintenance of the vehicle. We propose in this paper a new scheme of SoC and SoH estimation using an hybridization of Kalman filtering, Recursive Least Squares approach and Support Vector Machines learning. The battery SoC and SoH indicators are estimated using an adaptive-Sigma Point Kalman Filter. The battery cell impedance equivalent filter is obtained in real-time by a Recursive Least Square. Furthermore, the cell capacity evolution tracking is achieved by using a Support Vector Machine (SVM) method. Finally, the battery cell capacity and impedance equivalent filter are provided to the SoC estimator in order to update its state and observation models. This architecture yields to a complete SoC and SoH algorithmic solution exhibiting a high level of accuracy and robustness. The SVM method which requires the highest computational load in the architecture is designed to be used only for estimating the variable with the lowest evolution dynamics.","",""
6,"Florian Tschopp, Juan I. Nieto, R. Siegwart, César Cadena","Superquadric Object Representation for Optimization-based Semantic SLAM",2021,"","","","",154,"2022-07-13 09:40:27","","10.3929/ethz-b-000487527","","",,,,,6,6.00,2,4,1,"Introducing semantically meaningful objects to visual Simultaneous Localization And Mapping (SLAM) has the potential to improve both the accuracy and reliability of pose estimates, especially in challenging scenarios with significant viewpoint and appearance changes. However, how semantic objects should be represented for an efficient inclusion in optimizationbased SLAM frameworks is still an open question. Superquadrics (SQs) are an efficient and compact object representation, able to represent most common object types to a high degree, and typically retrieved from 3D point-cloud data. However, accurate 3D point-cloud data might not be available in all applications. Recent advancements in machine learning enabled robust object recognition and semantic mask measurements from camera images under many different appearance conditions. We propose a pipeline to leverage such semantic mask measurements to fit SQ parameters to multi-view camera observations using a multi-stage initialization and optimization procedure. We demonstrate the system’s ability to retrieve randomly generated SQ parameters from multi-view mask observations in preliminary simulation experiments and evaluate different initialization stages and cost functions.","",""
33,"Megha Srivastava, Tatsunori B. Hashimoto, Percy Liang","Robustness to Spurious Correlations via Human Annotations",2020,"","","","",155,"2022-07-13 09:40:27","","","","",,,,,33,16.50,11,3,2,"The reliability of machine learning systems critically assumes that the associations between features and labels remain similar between training and test distributions. However, unmeasured variables, such as confounders, break this assumption---useful correlations between features and labels at training time can become useless or even harmful at test time. For example, high obesity is generally predictive for heart disease, but this relation may not hold for smokers who generally have lower rates of obesity and higher rates of heart disease. We present a framework for making models robust to spurious correlations by leveraging humans' common sense knowledge of causality. Specifically, we use human annotation to augment each training example with a potential unmeasured variable (i.e. an underweight patient with heart disease may be a smoker), reducing the problem to a covariate shift problem. We then introduce a new distributionally robust optimization objective over unmeasured variables (UV-DRO) to control the worst-case loss over possible test-time shifts. Empirically, we show improvements of 5-10% on a digit recognition task confounded by rotation, and 1.5-5% on the task of analyzing NYPD Police Stops confounded by location.","",""
2,"S. Athinarayanan, M. Srinath","ROBUST AND EFFICIENT DIAGNOSIS OF CERVICAL CANCER IN PAP SMEAR IMAGES USING TEXTURES FEATURES WITH RBF AND KERNEL SVM CLASSIFICATION",2016,"","","","",156,"2022-07-13 09:40:27","","","","",,,,,2,0.33,1,2,6,"Classification of medical imagery is a difficult and challenging process due to the intricacy of the images and lack of models of the anatomy that totally captures the probable distortions in each structure. Cervical cancer is one of the major causes of death among other types of the cancers in women worldwide. Proper and timely diagnosis can prevent the life to some level. Consequently we have proposed an automated trustworthy system for the diagnosis of the cervical cancer using texture features and machine learning algorithm in Pap smear images , it is very beneficial to prevent cancer, also increases the reliability of the diagnosis. Proposed system is a multi-stage system for cell nucleus extraction and cancer diagnosis. First, noise removal is performed in the preprocessing step on the Pap smear images. Texture features are extracted from these noise free Pap smear images. Next phase of the proposed system is classification that is based on these extracted features, RBF and kernel based SVM classification is used. More than λ4% accuracy is achieved by the classification phase, proved that the proposed algorithm accuracy is good at detecting the cancer in the Pap smear images.","",""
5,"Mattia Samory, Indira Sen, Julian Kohne, Fabian Flöck, Claudia Wagner","""Call me sexist, but..."" : Revisiting Sexism Detection Using Psychological Scales and Adversarial Samples",2020,"","","","",157,"2022-07-13 09:40:27","","","","",,,,,5,2.50,1,5,2,"Research has focused on automated methods to effectively detect sexism online. Although overt sexism seems easy to spot, its subtle forms and manifold expressions are not. In this paper, we outline the different dimensions of sexism by grounding them in their implementation in psychological scales. From the scales, we derive a codebook for sexism in social media, which we use to annotate existing and novel datasets, surfacing their limitations in breadth and validity with respect to the construct of sexism. Next, we leverage the annotated datasets to generate adversarial examples, and test the reliability of sexism detection methods. Results indicate that current machine learning models pick up on a very narrow set of linguistic markers of sexism and do not generalize well to out-of-domain examples. Yet, including diverse data and adversarial examples at training time results in models that generalize better and that are more robust to artifacts of data collection. By providing a scale-based codebook and insights regarding the shortcomings of the state-of-the-art, we hope to contribute to the development of better and broader models for sexism detection, including reflections on theorydriven approaches to data collection.","",""
4,"Maurice Weber, Nana Liu, Bo Li, Ce Zhang, Zhikuan Zhao","Optimal provable robustness of quantum classification via quantum hypothesis testing",2020,"","","","",158,"2022-07-13 09:40:27","","10.1038/s41534-021-00410-5","","",,,,,4,2.00,1,5,2,"","",""
3,"C. Denamiel, X. Huan, I. Vilibić","Conceptual Design of Extreme Sea-Level Early Warning Systems Based on Uncertainty Quantification and Engineering Optimization Methods",2021,"","","","",159,"2022-07-13 09:40:27","","10.3389/fmars.2021.650279","","",,,,,3,3.00,1,3,1,"Coastal hazards linked to extreme sea-level events are projected to have a direct impact (by flooding) on 630 million of people by year 2100. Numerous operational forecasts already provide coastal hazard assessments around the world. However, they are largely based on either deterministic tools (e.g., numerical ocean and atmospheric models) or ensemble approaches which are both highly demanding in terms of high-performance computing (HPC) resources. Through a robust learning process, we propose conceptual design of an innovative architecture for extreme sea-level early warning systems based on uncertainty quantification/reduction and optimization methods. This approach might be cost-effective in terms of real-time computational needs while maintaining reliability and trustworthiness of the hazard assessments. The proposed architecture relies on three main tools aligning numerical forecasts with observations: (1) surrogate models of extreme sea-levels using polynomial chaos expansion, Gaussian processes or machine learning, (2) fast data assimilation via Bayesian inference, and (3) optimal experimental design of the observational network. A surrogate model developed for meteotsunami events – i.e., atmospherically induced long ocean waves in a tsunami frequency band – has already been proven to greatly improve the reliability of extreme sea-level hazard assessments. Such an approach might be promising for several coastal hazards known to destructively impact the world coasts, like hurricanes or typhoons and seismic tsunamis.","",""
3,"Jiahui Zhang, M. Habibnejad-korayem, Zhiying Liu, Tianyi Lyu, Qian Sun, Yu Zou","A Computer Vision Approach to Evaluate Powder Flowability for Metal Additive Manufacturing",2021,"","","","",160,"2022-07-13 09:40:27","","10.1007/s40192-021-00226-3","","",,,,,3,3.00,1,6,1,"","",""
2,"Z. Magnuska, B. Theek, Milita Darguzyte, M. Palmowski, E. Stickeler, V. Schulz, F. Kiessling","Influence of the Computer-Aided Decision Support System Design on Ultrasound-Based Breast Cancer Classification",2022,"","","","",161,"2022-07-13 09:40:27","","10.3390/cancers14020277","","",,,,,2,2.00,0,7,1,"Simple Summary The implementation of artificial intelligence in the computer-aided decision (CAD) support systems holds great promise for future cancer diagnosis. It is crucial to build these algorithms in a structured manner to ensure reproducibility and reliability. In this context, we used a dataset of breast ultrasound (US) images with 252 breast cancer and 253 benign cases to refine the CAD image analysis workflow. Various dataset preparations (i.e., pre-processing, and spatial augmentation) and machine learning algorithms were tested to establish the framework with the best performance in the detection and classification of breast lesions in US images. The efficacy of the proposed workflows was evaluated regarding accuracy, precision, specificity, and sensitivity. Abstract Automation of medical data analysis is an important topic in modern cancer diagnostics, aiming at robust and reproducible workflows. Therefore, we used a dataset of breast US images (252 malignant and 253 benign cases) to realize and compare different strategies for CAD support in lesion detection and classification. Eight different datasets (including pre-processed and spatially augmented images) were prepared, and machine learning algorithms (i.e., Viola–Jones; YOLOv3) were trained for lesion detection. The radiomics signature (RS) was derived from detection boxes and compared with RS derived from manually obtained segments. Finally, the classification model was established and evaluated concerning accuracy, sensitivity, specificity, and area under the Receiver Operating Characteristic curve. After training on a dataset including logarithmic derivatives of US images, we found that YOLOv3 obtains better results in breast lesion detection (IoU: 0.544 ± 0.081; LE: 0.171 ± 0.009) than the Viola–Jones framework (IoU: 0.399 ± 0.054; LE: 0.096 ± 0.016). Interestingly, our findings show that the classification model trained with RS derived from detection boxes and the model based on the RS derived from a gold standard manual segmentation are comparable (p-value = 0.071). Thus, deriving radiomics signatures from the detection box is a promising technique for building a breast lesion classification model, and may reduce the need for the lesion segmentation step in the future design of CAD systems.","",""
5,"Ali A. Bataleblu, J. Roshanian","Robust trajectory optimization of space launch vehicle using computational intelligence",2015,"","","","",162,"2022-07-13 09:40:27","","10.1109/CEC.2015.7257318","","",,,,,5,0.71,3,2,7,"Metamodeling techniques using computational intelligence have been used in Uncertainty-based Design Optimization (UDO) to reduce the high computational cost of the uncertainty analysis and improve the performance of stochastic optimization problems with computationally expensive simulation models. Optimal trajectory generation is a major part of Space Launch Vehicle (SLV) design and if it is robust relative to uncertainties can improve vehicle reliability, safety and operational cost. This paper presents a combination of Latin Hypercube Sampling (LHS) and Extreme Learning Machine (ELM) in order to create an appropriate trajectory metamodel for reducing computational time of robust trajectory design optimization of a two-stage-to-orbit SLV. The sampled data of LHS is then used as training data for ELM. Complex and costly uncertainty analyses are replaced by an ELM Neural Network (NN) which is used to instantaneously estimate the mean and standard deviation of objective function and constraints. The evolutionary genetic algorithm is used for global optimization of layers' connection weights and biases to minimize the learning error during learning phase of NN. A Hybrid Search Algorithm (HSA), which associates Simulated Annealing (SA) as a global optimizer with Simplex as a local optimizer is employed to find robust optimum point of this metamodel. The optimal and robust trajectories are compared. The results show excellent approximation of highly non-linear design space and drastic reduction in overall UDO time, due to greatly reduced number of exact trajectory analyses.","",""
20,"I. Cortés-Ciriano, A. Bender","Concepts and Applications of Conformal Prediction in Computational Drug Discovery",2019,"","","","",163,"2022-07-13 09:40:27","","10.1039/9781788016841-00063","","",,,,,20,6.67,10,2,3,"Estimating the reliability of individual predictions is key to increase the adoption of computational models and artificial intelligence in preclinical drug discovery, as well as to foster its application to guide decision making in clinical settings. Among the large number of algorithms developed over the last decades to compute prediction errors, Conformal Prediction (CP) has gained increasing attention in the computational drug discovery community. A major reason for its recent popularity is the ease of interpretation of the computed prediction errors in both classification and regression tasks. For instance, at a confidence level of 90% the true value will be within the predicted confidence intervals in at least 90% of the cases. This so called validity of conformal predictors is guaranteed by the robust mathematical foundation underlying CP. The versatility of CP relies on its minimal computational footprint, as it can be easily coupled to any machine learning algorithm at little computational cost. In this review, we summarize underlying concepts and practical applications of CP with a particular focus on virtual screening and activity modelling, and list open source implementations of relevant software. Finally, we describe the current limitations in the field, and provide a perspective on future opportunities for CP in preclinical and clinical drug discovery.","",""
29,"Eva Weigl, W. Heidl, E. Lughofer, Thomas Radauer, C. Eitzinger","On improving performance of surface inspection systems by online active learning and flexible classifier updates",2015,"","","","",164,"2022-07-13 09:40:27","","10.1007/s00138-015-0731-9","","",,,,,29,4.14,6,5,7,"","",""
1,"Utkarsh Sarawgi, Rishab Khincha, Wazeer Zulfikar, Satrajit S. Ghosh, P. Maes","Uncertainty-Aware Boosted Ensembling in Multi-Modal Settings",2021,"","","","",165,"2022-07-13 09:40:27","","10.1109/IJCNN52387.2021.9534161","","",,,,,1,1.00,0,5,1,"Reliability of machine learning (ML) systems is crucial in safety-critical applications such as healthcare, and uncertainty estimation is a widely researched method to highlight the confidence of ML systems in deployment. Sequential and parallel ensemble techniques have shown improved performance of ML systems in multi-modal settings by leveraging the feature sets together. We propose an uncertainty-aware boosting technique for multi-modal ensembling in order to focus on the data points with higher associated uncertainty estimates, rather than the ones with higher loss values. We evaluate this method on healthcare tasks related to Dementia and Parkinson's disease which involve real-world multi-modal speech and text data, wherein our method shows an improved performance. Additional analysis suggests that introducing uncertainty-awareness into the boosted ensembles decreases the overall entropy of the system, making it more robust to heteroscedasticity in the data, as well as better calibrating each of the modalities along with high quality prediction intervals. We open-source our entire codebase at https://github.com/usarawgi911//Uncertainty-aware-boosting.","",""
1,"Yong Yang, Shuaishuai Zheng, Zhilu Ai, Mohammad Jafari","On the Prediction of Biogas Production from Vegetables, Fruits, and Food Wastes by ANFIS- and LSSVM-Based Models",2021,"","","","",166,"2022-07-13 09:40:27","","10.1155/2021/9202127","","",,,,,1,1.00,0,4,1,"This study is aimed at modeling biodigestion systems as a function of the most influencing parameters to generate two robust algorithms on the basis of the machine learning algorithms, including adaptive network-based fuzzy inference system (ANFIS) and least square support vector machine (LSSVM). The models are assessed utilizing multiple statistical analyses for the actual values and model outcomes. Results from the suggested models indicate their great capability of predicting biogas production from vegetable food, fruits, and wastes for a variety of ranges of input parameters. The values that are calculated for the mean relative error (MRE %) and mean squared error (MSE) were 29.318 and 0.0039 for ANFIS, and 2.951 and 0.0001 for LSSVM which shows that the latter model has a better ability to predict the target data. Finally, in order to have additional certainty, two analyses of outlier identification and sensitivity were performed on the input parameter data that proved the proposed model in this paper has higher reliability in assessing output values compared with the previous model.","",""
1,"Niklas Risse, C. Gopfert, Jan Philip Göpfert","How to Compare Adversarial Robustness of Classifiers from a Global Perspective",2020,"","","","",167,"2022-07-13 09:40:27","","10.1007/978-3-030-86362-3_3","","",,,,,1,0.50,0,3,2,"","",""
1,"Pranesh Santikellur, R. Chakraborty","A Computationally Efficient Tensor Regression Network-Based Modeling Attack on XOR Arbiter PUF and Its Variants",2021,"","","","",168,"2022-07-13 09:40:27","","10.1109/TCAD.2020.3032624","","",,,,,1,1.00,1,2,1,"XOR arbiter PUF (XOR APUF), where the outputs of multiple arbiter PUF (APUFs) are XOR-ed, has proven to be more robust to machine learning-based modeling attacks. The reported successful modeling attacks for XOR APUF either employ auxiliary side-channel or reliability information, or require enormous computational effort. This robustness is primarily due to the difficulty in learning the unknown internal delay parameter terms in the mathematical model of a XOR APUF, and the robustness increases as the number of APUFs being XOR-ed increases. In this article, we employ a novel machine learning-based modeling technique called efficient CANDECOMP/PARAFAC-tensor regression network (CP-TRN), a variant of CP-decomposition-based tensor regression network, to reduce the computational resource requirement of model building attacks on XOR APUF. We theoretically prove the reduction in computational complexity, as well as give supporting experimental results. In addition, our proposed technique does not require any auxiliary information, and is robust to noisy training data. The proposed technique allowed us to successfully model 64-bit 8-XOR APUF and 128-bit 7-XOR APUF on a single desktop workstation, with high prediction accuracy. Further, we extend the proposed modeling attack technique to XOR APUF variants, e.g., lightweight secure PUF (LSPUF), which rely on input challenge transformation. The modeling accuracy results obtained by us for the LSPUF are comparable with those obtained by applying other state-of-the-art techniques, while requiring less training data.","",""
1,"Alif Akbar Pranata, Olivier Barais, Johann Bourcier, L. Noirie","ChaT: Evaluation of Reconfigurable Distributed Network Systems Using Metamorphic Testing",2021,"","","","",169,"2022-07-13 09:40:27","","10.1109/GLOBECOM46510.2021.9685879","","",,,,,1,1.00,0,4,1,"Detecting faults in distributed network systems is challenging because of their complexity, but this is required to evaluate and improve their reliability. This paper proposes ChaT, a testing and evaluation methodology under system reconfigurations and perturbations for distributed network systems, to evaluate QoS reliability by discriminating safe and failure-prone behaviors from different testing scenarios. Motivated by meta-morphic testing technique that removes the burden of defining software oracles, we propose some metamorphic relationships that correlate system inputs and outputs to find patterns in executions. Classification techniques based on machine learning (principal component analysis and support vector machine) are used to identify system states and validate the proposed metamorphic relationships. These metamorphic relationships are also used to help anomaly detection. We verify this with several anomaly detection techniques (isolation forest, one-class SVM, local outlier factor, and robust covariance) that categorize experiments belonging to either safe or failure-prone states. We apply ChaT to a video streaming application use case. The simulation results show the effectiveness of ChaT to achieve our goals: identifying execution classes and detecting failure-prone experiments based on metamorphic relationships with high level of statistical scores.","",""
14,"K. Tran, Thanh Duong, Quyen Ho","Credit scoring model: A combination of genetic programming and deep learning",2016,"","","","",170,"2022-07-13 09:40:27","","10.1109/FTC.2016.7821603","","",,,,,14,2.33,5,3,6,"In recent years, the market of customer lending grows rapidly, that is a reason why credit scoring becomes a core task of financial institutes. Many models based on machine learning have been widely using and providing robust performance. Because most machine learning based models are black-box, it is hard to see the relations between input data and scoring results. Therefore, this paper focuses on improving both the accuracy and the reliability of machine learning based model. Thus, we propose a hybrid idea to combine the power of deep learning network and the comprehensive genetic programming which is extracted rules to build a robust credit model. Our empirical experiment on Australian/German customer credit data sets shows that our model provides the best accuracy, highly reduce credit risk, and reliable IF-THEN rules.","",""
0,"M. S. Hossain Lipu, M. Hannan, A. Hussain, Shaheer Ansari, A. Ayob, M. Saad, K. Muttaqi","Differential Search Optimized Random Forest Regression Algorithm for State of Charge Estimation in Electric Vehicle Batteries",2021,"","","","",171,"2022-07-13 09:40:27","","10.1109/ias48185.2021.9677106","","",,,,,0,0.00,0,7,1,"This paper presents an improved machine learning approach for the accurate and robust state of charge (SOC) in electric vehicle (EV) batteries using differential search optimized random forest regression (RFR) algorithm. The precise SOC estimation confirms the safety and reliability of EV. Nevertheless, SOC is influenced by numerous factors which cannot be measured directly. RFR is suitable for SOC estimation due to its robustness to noise, overfitting issues and capacity to work with huge datasets. However, proper selection of RFR architecture and hyper-parameters combination remains a key issue to be explored. Hence, a differential search algorithm (DSA) is employed to search for the optimal values of trees and leaves in RFR algorithm. DSA optimized RFR eliminates the utilization of the filter in data pre-processing steps and does not require a detailed understanding and knowledge about battery chemistry, rather only needs sensors to monitor battery voltage and current. The developed approach is validated at room temperature using two types of lithium-ion batteries under a pulse discharge test. In addition, the proposed model is verified under varying temperature settings under EV drive cycles. The experimental results demonstrate that the DSA optimized RFR algorithm is superior to other optimized machine learning approaches in achieving a lower error rate which illustrates the suitability of the proposed model in the online battery management system.","",""
0,"A. K. Akash, Sean Chung, Shri Shruthi Shridhar, Wissam Kontar","Robustifying Out-of-Distribution Detection: A Self-Supervision and Energy Based Approach",2021,"","","","",172,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,4,1,"Out-of-distribution (OOD) detection is essential to deploying machine learning systems in the real world. However, the reliability of the existing OOD detectors is severely hampered when used in an environment with adversarial/natural perturbations. Being such a critical component, this necessitates the study of techniques to robustify it. In this work, we propose using the representation learning power of self-supervision methods with better OOD scoring mechanism based on energy to improve the robustness of OOD detectors. Speciﬁcally, we propose a blend of ﬂexible loss function formulations that can effectively learn robust features. Our ﬁndings merit the use of a new methodological perspective that focuses on robustifying OOD detection.","",""
0,"Yogesh Kulkarni, Z. SayfHussain, K. Ramamritham, Nivethitha Somu","EnsembleNTLDetect: An Intelligent Framework for Electricity Theft Detection in Smart Grid",2021,"","","","",173,"2022-07-13 09:40:27","","10.1109/ICDMW53433.2021.00070","","",,,,,0,0.00,0,4,1,"Artificial intelligence-based techniques applied to the electricity consumption data generated from the smart grid prove to be an effective solution in reducing Non Technical Loses (NTLs), thereby ensures safety, reliability, and security of the smart energy systems. However, imbalanced data, consecutive missing values, large training times, and complex architectures hinder the real time application of electricity theft detection models. In this paper, we present EnsembleNTLDetect, a robust and scalable electricity theft detection framework that employs a set of efficient data pre-processing techniques and machine learning models to accurately detect electricity theft by analysing consumers’ electricity consumption patterns. This framework utilises an enhanced Dynamic Time Warping Based Imputation (eDTWBI) algorithm to impute missing values in the time series data and leverages the Near-miss undersampling technique to generate balanced data.Further, stacked autoencoder is introduced for dimensionality reduction and to improve training efficiency. A Conditional Generative Adversarial Network (CTGAN) is used to augment the dataset to ensure robust training and a soft voting ensemble classifier is designed to detect the consumers with aberrant consumption patterns. Furthermore, experiments were conducted on the real-time electricity consumption data provided by the State Grid Corporation of China (SGCC) to validate the reliability and efficiency of EnsembleNTLDetect over the state-of-the-art electricity theft detection models in terms of various quality metrics.","",""
0,"Mohamed Abdelhack, Jiaming Zhang, Sandhya Tripathi, B. Fritz, M. Avidan, Yixin Chen, C. King","A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues",2021,"","","","",174,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,7,1,"Data quality is a common problem in machine learning, especially in high-stakes settings such as healthcare. Missing data affects accuracy, calibration, and feature attribution in complex patterns. Developers often train models on carefully curated datasets to minimize missing data bias; however, this reduces the usability of such models in production environments, such as real-time healthcare records. Making machine learning models robust to missing data is therefore crucial for practical application. While some classifiers naturally handle missing data, others, such as deep neural networks, are not designed for unknown values. We propose a novel neural network modification to mitigate the impacts of missing data. The approach is inspired by neuromodulation that is performed by biological neural networks. Our proposal replaces the fixed weights of a fully-connected layer with a function of an additional input (reliability score) at each input, mimicking the ability of cortex to upand down-weight inputs based on the presence of other data. The modulation function is jointly learned with the main task using a multi-layer perceptron. We tested our modulating fully connected layer on multiple classification, regression, and imputation problems, and it either improved performance or generated comparable performance to conventional neural network architectures concatenating reliability to the inputs. Models with modulating layers were more robust against degradation of data quality by introducing additional missingness at evaluation time. These results suggest that explicitly accounting for reduced information quality with a modulating fully connected layer can enable the deployment of artificial intelligence systems in real-time settings. ∗Author has since moved to the Krembil Center for Neuroinformatics, Toronto, ON, Canada Preprint. Under review. ar X iv :2 10 7. 08 57 4v 1 [ cs .L G ] 1 9 Ju l 2 02 1","",""
0,"Lu Yu, Verena Rieser","Adversarial Robustness of Visual Dialog",2022,"","","","",175,"2022-07-13 09:40:27","","","","",,,,,0,0.00,0,2,1,"—Adversarial robustness evaluates the worst-case per- formance scenario of a machine learning model to ensure its safety and reliability. This study is the ﬁrst to investigate the robustness of visually grounded dialog models towards textual attacks. These attacks represent a worst-case scenario where the input question contains a synonym which causes the previously correct model to return a wrong answer. Using this scenario, we ﬁrst aim to understand how multimodal input components contribute to model robustness. Our results show that models which encode dialog history are more robust, and when launching an attack on history, model prediction becomes more uncertain. This is in contrast to prior work which ﬁnds that dialog history is negligible for model performance on this task. We also evaluate how to generate adversarial test examples which successfully fool the model but remain undetected by the user/software designer. We ﬁnd that the textual, as well as the visual context are important to generate plausible worst-case scenarios.","",""
0,"Luyu Qiu, Yi Yang, Caleb Chen Cao, Yueyuan Zheng, H. Ngai, Janet Hsiao, Lei Chen","Generating Perturbation-based Explanations with Robustness to Out-of-Distribution Data",2022,"","","","",176,"2022-07-13 09:40:27","","10.1145/3485447.3512254","","",,,,,0,0.00,0,7,1,"Perturbation-based techniques are promising for explaining black-box machine learning models due to their effectiveness and ease of implementation. However, prior works have faced the problem of Out-of-Distribution (OoD) — an artifact of randomly perturbed data becoming inconsistent with the original dataset, degrading the reliability of generated explanations, which is still under-explored according to our best knowledge. This work addresses the OoD issue by designing a simple yet effective module that can quantify the affinity between the perturbed data and the original dataset distribution. Specifically, we penalize the influences of unreliable OoD data for the perturbed samples by integrating the inlier scores and prediction results of the target models, thereby making the final explanations more robust. Our solution is shown to be compatible with the most popular perturbation-based XAI algorithms: RISE, OCCLUSION, and LIME. Extensive experiments confirmed that our methods exhibit superior performance in most cases with computational and cognitive metrics. In particular, we point out the degradation problem of RISE algorithm for the first time. With our design, the performance of RISE can be boosted significantly. Besides, our solution also resolves a fundamental problem with a faithfulness indicator, a commonly used evaluation metric of XAI algorithms that appears sensitive to the OoD issue.","",""
56,"S. Mousavi, Y. Sheng, Weiqiang Zhu, G. Beroza","STanford EArthquake Dataset (STEAD): A Global Data Set of Seismic Signals for AI",2019,"","","","",177,"2022-07-13 09:40:27","","10.1109/ACCESS.2019.2947848","","",,,,,56,18.67,14,4,3,"Seismology is a data rich and data-driven science. Application of machine learning for gaining new insights from seismic data is a rapidly evolving sub-field of seismology. The availability of a large amount of seismic data and computational resources, together with the development of advanced techniques can foster more robust models and algorithms to process and analyze seismic signals. Known examples or labeled data sets, are the essential requisite for building supervised models. Seismology has labeled data, but the reliability of those labels is highly variable, and the lack of high-quality labeled data sets to serve as ground truth as well as the lack of standard benchmarks are obstacles to more rapid progress. In this paper we present a high-quality, large-scale, and global data set of local earthquake and non-earthquake signals recorded by seismic instruments. The data set in its current state contains two categories: (1) local earthquake waveforms (recorded at “local” distances within 350 km of earthquakes) and (2) seismic noise waveforms that are free of earthquake signals. Together these data comprise ~ 1.2 million time series or more than 19,000 hours of seismic signal recordings. Constructing such a large-scale database with reliable labels is a challenging task. Here, we present the properties of the data set, describe the data collection, quality control procedures, and processing steps we undertook to insure accurate labeling, and discuss potential applications. We hope that the scale and accuracy of STEAD presents new and unparalleled opportunities to researchers in the seismological community and beyond.","",""
212,"P. Prasanna, Kristin J. Dana, N. Gucunski, B. Basily, H. La, R. Lim, H. Parvardeh","Automated Crack Detection on Concrete Bridges",2016,"","","","",178,"2022-07-13 09:40:27","","10.1109/TASE.2014.2354314","","",,,,,212,35.33,30,7,6,"Detection of cracks on bridge decks is a vital task for maintaining the structural health and reliability of concrete bridges. Robotic imaging can be used to obtain bridge surface image sets for automated on-site analysis. We present a novel automated crack detection algorithm, the STRUM (spatially tuned robust multifeature) classifier, and demonstrate results on real bridge data using a state-of-the-art robotic bridge scanning system. By using machine learning classification, we eliminate the need for manually tuning threshold parameters. The algorithm uses robust curve fitting to spatially localize potential crack regions even in the presence of noise. Multiple visual features that are spatially tuned to these regions are computed. Feature computation includes examining the scale-space of the local feature in order to represent the information and the unknown salient scale of the crack. The classification results are obtained with real bridge data from hundreds of crack regions over two bridges. This comprehensive analysis shows a peak STRUM classifier performance of 95% compared with 69% accuracy from a more typical image-based approach. In order to create a composite global view of a large bridge span, an image sequence from the robot is aligned computationally to create a continuous mosaic. A crack density map for the bridge mosaic provides a computational description as well as a global view of the spatial patterns of bridge deck cracking. The bridges surveyed for data collection and testing include Long-Term Bridge Performance program's (LTBP) pilot project bridges at Haymarket, VA, USA, and Sacramento, CA, USA.","",""
37,"S. S. Zalivaka, A. A. Ivaniuk, Chip-Hong Chang","Reliable and Modeling Attack Resistant Authentication of Arbiter PUF in FPGA Implementation With Trinary Quadruple Response",2019,"","","","",179,"2022-07-13 09:40:27","","10.1109/TIFS.2018.2870835","","",,,,,37,12.33,12,3,3,"Field programmable gate array (FPGA) is a potential hotbed for malicious and counterfeit hardware infiltration. Arbiter-based physical unclonable function (A-PUF) has been widely regarded as a suitable lightweight security primitive for FPGA bitstream encryption and device authentication. Unfortunately, the metastability of flip-flop gives rise to poor A-PUF reliability in FPGA implementation. Its linear additive path delays are also vulnerable to modeling attacks. Most reliability enhancement techniques tend to increase the response predictability and ease machine learning attacks. This paper presents a robust device authentication method based on the FPGA implementation of a reliability enhanced A-PUF with trinary digit (trit) quadruple responses. A two flip-flop arbiter is used to produce a trit for metastability detection. By considering the ordered responses to all four combinations of first and last challenge bits, each quadruple response can be compressed into a quadbit that represents one of the five classes of trit quadruple response with greater reproducibility. This challenge-response quadruple classification not only greatly reduces the burden of error correction at the device but also enables a precise A-PUF model to be built at the server without having to store the complete challenge-response pair (CRP) set for authentication. Besides, the real challenge to the A-PUF is generated internally by a lossy, nonlinear, and irreversible maximum length signature generator at both the server and device sides to prevent the naked CRP from being machine learned by the attacker. The A-PUF with short repetition code of length five has been tested to achieve a reliability of 1.0 over the full operating temperature range of the target FPGA board with lower hardware resource utilization than other modeling attack resilient strong PUFs. The proposed authentication protocol has also been experimentally evaluated to be practically secure against various machine learning attacks including evolutionary strategy covariance matrix adaptation.","",""
10,"Erotokritos Skordilis, R. Moghaddass","A Double Hybrid State-Space Model for Real-Time Sensor-Driven Monitoring of Deteriorating Systems",2020,"","","","",180,"2022-07-13 09:40:27","","10.1109/TASE.2019.2921285","","",,,,,10,5.00,5,2,2,"With the rising complexity of deteriorating systems and availability of advanced sensors, the need for more robust and reliable methods for condition monitoring and dynamic maintenance decision-making has significantly increased. To generate more reliable results for reliability analysis of complex systems, we propose a new generative framework for failure prognosis utilizing a hybrid state-space model (SSM) that represents the evolution of the system’s operating condition and its degradation over time. The proposed model can employ a set of real-time sensor measurements to: 1) diagnose the hidden degradation level of the system and 2) predict the likelihood and the uncertainty of failure without imposing unrealistic heavy distributional assumptions. We provide analytical results for the prediction and update steps of the associated particle filter, as well as for the estimation of model parameters. A single-layer feed-forward neural network (Extreme Learning Machine) was used to model the nonparametric relationship between the multi-dimensional observation process and the rest of system’s dynamics. We demonstrate the application of our framework through numerical experiments on a set of simulated data and a turbofan engine degradation data set. Note to Practitioners—The prognosis of the future health status in degrading systems using sensor data generally requires many distributional assumptions, such as a parametric relationship between the hidden degradation level and sensor measurements, and a predefined degradation threshold. This paper proposes a new model that formulates the relationship between degradation level, sensor measurements, and operating conditions in a multi-layer generative manner that helps accommodate interpretability and uncertainty. Results obtained utilizing simulated and real-life data prove that the developed method can yield reasonable prognostic estimates for important measures, such as the remaining useful life of the system.","",""
8,"A. Al-Imam","A Novel Method for Computationally Efficacious Linear and Polynomial Regression Analytics of Big Data in Medicine",2020,"","","","",181,"2022-07-13 09:40:27","","10.5539/mas.v14n5p1","","",,,,,8,4.00,8,1,2,"Background:  Machine learning relies on a hybrid of analytics, including regression analyses. There have been no attempts to deploy a scale-down transformation of data to enhance linear regression models.  Objectives:  We aim to optimize linear regression models by implementing data transformation function to scale down all variables in an attempt to minimize the sum of squared error.  Materials and Methods:  We implemented non-Bayesian statistics using SPSS and MatLab. We used Excel to generate 40 trials of linear regression models, and each has 1,000 observations. We utilized SPSS to conduct regression analyses, Wilcoxon signed-rank test, and Cronbach’s alpha statistics to evaluate the performance of the optimization model.  Results:  The scale-down transformation succeeded by significantly reducing the sum of squared errors [absolute Z-score=5.511, effect size=0.779, p-value<0.001, Wilcoxon signed-rank test]. Inter-item reliability testing confirmed the robust internal consistency of the model [Cronbach’s alpha=0.993].  Conclusions:  The optimization model is valuable for high-impact research based on regression. It can reduce the computational processing demands for powerful real-time and predictive analytics of big data.","",""
5,"Juan Lorenzo Hagad, Ken-ichi Fukui, M. Numao","Deep Visual Models for EEG of Mindfulness Meditation in a Workplace Setting",2019,"","","","",182,"2022-07-13 09:40:27","","10.1007/978-3-030-24409-5_12","","",,,,,5,1.67,2,3,3,"","",""
3,"Taehyeon Kim, Jaeyeon Ahn, Nakyil Kim, Seyoung Yun","Adaptive Local Bayesian Optimization Over Multiple Discrete Variables",2020,"","","","",183,"2022-07-13 09:40:27","","","","",,,,,3,1.50,1,4,2,"In the machine learning algorithms, the choice of the hyperparameter is often an art more than a science, requiring labor-intensive search with expert experience. Therefore, automation on hyperparameter optimization to exclude human intervention is a great appeal, especially for the black-box functions. Recently, there have been increasing demands of solving such concealed tasks for better generalization, though the task-dependent issue is not easy to solve. The Black-Box Optimization challenge (NeurIPS 2020) required competitors to build a robust black-box optimizer across different domains of standard machine learning problems. This paper describes the approach of team KAIST OSI in a step-wise manner, which outperforms the baseline algorithms by up to +20.39%. We first strengthen the local Bayesian search under the concept of region reliability. Then, we design a combinatorial kernel for a Gaussian process kernel. In a similar vein, we combine the methodology of Bayesian and multi-armed bandit,(MAB) approach to select the values with the consideration of the variable types; the real and integer variables are with Bayesian, while the boolean and categorical variables are with MAB. Empirical evaluations demonstrate that our method outperforms the existing methods across different tasks.","",""
16,"Lijie Zhao, Dianhui Wang, T. Chai","Estimation of effluent quality using PLS-based extreme learning machines",2013,"","","","",184,"2022-07-13 09:40:27","","10.1007/s00521-012-0837-1","","",,,,,16,1.78,5,3,9,"","",""
283,"Lan Guo, Yan Ma, B. Cukic, Harshinder Singh","Robust prediction of fault-proneness by random forests",2004,"","","","",185,"2022-07-13 09:40:27","","10.1109/ISSRE.2004.35","","",,,,,283,15.72,71,4,18,"Accurate prediction of fault prone modules (a module is equivalent to a C function or a C+ + method) in software development process enables effective detection and identification of defects. Such prediction models are especially beneficial for large-scale systems, where verification experts need to focus their attention and resources to problem areas in the system under development. This paper presents a novel methodology for predicting fault prone modules, based on random forests. Random forests are an extension of decision tree learning. Instead of generating one decision tree, this methodology generates hundreds or even thousands of trees using subsets of the training data. Classification decision is obtained by voting. We applied random forests in five case studies based on NASA data sets. The prediction accuracy of the proposed methodology is generally higher than that achieved by logistic regression, discriminant analysis and the algorithms in two machine learning software packages, WEKA [I. H. Witten et al. (1999)] and See5. The difference in the performance of the proposed methodology over other methods is statistically significant. Further, the classification accuracy of random forests is more significant over other methods in larger data sets.","",""
2,"S. Bhat, I. B. Sofi, Chong-Yung Chi","Edge Computing and Its Convergence With Blockchain in 5G and Beyond: Security, Challenges, and Opportunities",2020,"","","","",186,"2022-07-13 09:40:27","","10.1109/ACCESS.2020.3037108","","",,,,,2,1.00,1,3,2,"The internet is progressing towards a new technology archetype grounded on smart systems, heavily relying on artificial intelligence (AI), machine learning (ML), blockchain platforms, edge computing, and the internet of things (IoT). The merging of IoT, edge computing, and blockchain will be the most important factor of empowering new automatic service and commercial models with various desirable properties, such as self-verifying, self-executing, immutability, data reliability, and confidentiality provided by the advancement in blockchain smart contracts and containers. Motivated by the potential paradigm shift and the security features brought by blockchain from the traditional centralized model to a more robust and resilient decentralized model, this tutorial article proposes a multi-tier integrated blockchain and edge computing architecture for 5G and beyond for solving some security issues faced by resource-constrained edge devices. We begin with a comprehensive overview of different edge computing paradigms and their research challenges. Next, we present the classification of security threats and current defense mechanisms. Then, we present an overview of blockchain and its potential solutions to the main security issues in edge computing. Furthermore, we present the classification of facilitating developers of different architectures to select an appropriate platform for particular applications and offer insights for potential research directions. Finally, we provide key convergence features of the blockchain and edge computing, followed by some conclusions.","",""
0,"J. Yoon, Ming-Hsuan Yang, Kuk-jin Yoon","IEEE TRANSACTIONS ON PATTERN ANALYSIS AND MACHINE INTELLIGENCE 1 Interacting Multiview Tracker",2015,"","","","",187,"2022-07-13 09:40:27","","10.1109/tpami.2004.1265858","","",,,,,0,0.00,0,3,7,"A robust algorithm is proposed for tracking a target object in dynamic conditions including motion blurs, illumination changes, pose variations, and occlusions. To cope with these challenging factors, multiple trackers based on different feature representations are integrated within a probabilistic framework. Each view of the proposed multiview (multi-channel) feature learning algorithm is concerned with one particular feature representation of a target object from which a tracker is developed with different level of reliability. With the multiple trackers, the proposed algorithm exploits tracker interaction and selection for robust tracking performance. In the tracker interaction, a transition probability matrix is used to estimate dependencies between trackers. Multiple trackers communicate with each other by sharing information of sample distributions. The tracker selection process determines the most reliable one with the highest probability. To account for object appearance changes, the transition probability matrix and tracker probability are updated in a recursive Bayesian framework by reflecting the tracker reliability measured by a robust tracker likelihood function that learns to account for both transient and stable appearance changes. Experimental results on benchmark datasets demonstrate that the proposed interacting multiview algorithm performs robustly and favorably against state-of-the-art methods in terms of several quantitative metrics.","",""
1,"C. Bhushan, Zhaoyuan Yang, Nurali Virani, N. Iyer","Variational Encoder-Based Reliable Classification",2020,"","","","",188,"2022-07-13 09:40:27","","10.1109/ICIP40778.2020.9190836","","",,,,,1,0.50,0,4,2,"Machine learning models provide statistically impressive results which might be individually unreliable. To provide reliability, we propose an Epistemic Classifier (EC) that can provide justification of its belief using support from the training dataset as well as quality of reconstruction. Our approach is based on modified variational auto-encoders that can identify a semantically meaningful low-dimensional space where perceptually similar instances are close in $\ell_{2}-$distance too. Our results demonstrate improved reliability of predictions and robust identification of samples with adversarial attacks as compared to baseline of softmax-based thresholding.","",""
16,"Saleh R. Mousa, Peter R. Bakhit, S. Ishak","An extreme gradient boosting method for identifying the factors contributing to crash/near-crash events: a naturalistic driving study",2019,"","","","",189,"2022-07-13 09:40:27","","10.1139/CJCE-2018-0117","","",,,,,16,5.33,5,3,3,"Despite the research efforts for reducing traffic accidents, the number of global annual vehicle accidents is still on the rise. This continues to motivate researchers to examine the factors contributing to crash and near-crash events (CNC). Recently, many studies attempted to identify the associated crash factors using naturalistic driving study (SHRP2-NDS) data. Despite the many classifiers developed in the literature, the high dimensionality and multicollinearity within the SHRP2-NDS data limit the accuracy and reliability of the developed models. This study develops an extreme gradient boosting (XGB) classifier, robust to multicollinearity, using the SHRP2-NDS dataset for identifying the factors contributing to CNC events. The performance of the XGB classifier is evaluated against three other advanced machine-learning algorithms. Results indicate that the XGB model outperformed the other models with a detection accuracy of 85% and identified the “driver behavior” and “intersection influence” as the most contributing factors to CNC detection.","",""
0,"Mustafa Bilik, Recep Kök","A Comparative Study on Consumption Functions: The Case of the European Union*",2020,"","","","",190,"2022-07-13 09:40:27","","10.21121/eab.793411","","",,,,,0,0.00,0,2,2,"The consumption function describes the relationship between consumption expenditures and income. As is well known, the distribution of macroeconomic data such as income and consumption is unequal. Accordingly, estimators derived from linear models may be inefficient. This study attempted to reach efficient estimators, using gamma distribution, within the limits of this study. The main purpose of this study is to estimate parameters of mainstream consumption functions using the panel-data of EU members and negotiating countries. Using data from the World Bank, the European Union Macroeconomic Database (AMECO), and the Bank of International Settlement (BIS), consumption functions were estimated by the Generalized Linear Model (GLM) approach. The reliability of the estimators was tested with the Generalized Moments Method (GMM). Furthermore, the study uses GLM, based on machine learning, to obtain robust estimators for overfitting. Findings of all three methods are compatible with each other and the “Permanent Income” hypothesis verified.","",""
12,"Ke Zhao, Haidong Shao","Intelligent Fault Diagnosis of Rolling Bearing Using Adaptive Deep Gated Recurrent Unit",2019,"","","","",191,"2022-07-13 09:40:27","","10.1007/s11063-019-10137-2","","",,,,,12,4.00,6,2,3,"","",""
75,"S. Gruss, R. Treister, P. Werner, H. Traue, S. Crawcour, A. Andrade, Steffen Walter","Pain Intensity Recognition Rates via Biopotential Feature Patterns with Support Vector Machines",2015,"","","","",192,"2022-07-13 09:40:27","","10.1371/journal.pone.0140330","","",,,,,75,10.71,11,7,7,"Background The clinically used methods of pain diagnosis do not allow for objective and robust measurement, and physicians must rely on the patient’s report on the pain sensation. Verbal scales, visual analog scales (VAS) or numeric rating scales (NRS) count among the most common tools, which are restricted to patients with normal mental abilities. There also exist instruments for pain assessment in people with verbal and / or cognitive impairments and instruments for pain assessment in people who are sedated and automated ventilated. However, all these diagnostic methods either have limited reliability and validity or are very time-consuming. In contrast, biopotentials can be automatically analyzed with machine learning algorithms to provide a surrogate measure of pain intensity. Methods In this context, we created a database of biopotentials to advance an automated pain recognition system, determine its theoretical testing quality, and optimize its performance. Eighty-five participants were subjected to painful heat stimuli (baseline, pain threshold, two intermediate thresholds, and pain tolerance threshold) under controlled conditions and the signals of electromyography, skin conductance level, and electrocardiography were collected. A total of 159 features were extracted from the mathematical groupings of amplitude, frequency, stationarity, entropy, linearity, variability, and similarity. Results We achieved classification rates of 90.94% for baseline vs. pain tolerance threshold and 79.29% for baseline vs. pain threshold. The most selected pain features stemmed from the amplitude and similarity group and were derived from facial electromyography. Conclusion The machine learning measurement of pain in patients could provide valuable information for a clinical team and thus support the treatment assessment.","",""
23,"Fabricio A. Breve, Liang Zhao, M. Quiles","Semi-supervised learning from imperfect data through particle cooperation and competition",2010,"","","","",193,"2022-07-13 09:40:27","","10.1109/IJCNN.2010.5596659","","",,,,,23,1.92,8,3,12,"In machine learning study, semi-supervised learning has received increasing interests in the last years. It is applied to classification problems where only a small portion of the data points is labeled. In these situations, the reliability of these labels is extremely important because it is common to have mislabeled samples in a data set and these may propagate their wrong labels to a large portion of the data set, resulting in major classification errors. In spite of its importance, wrong label propagation in semi-supervised learning has received little attention from researchers. In this paper we propose a particle walk semi-supervised learning method with both competitive and cooperative mechanisms. Then we study error propagation by applying the proposed model in modular networks. We show that the model is robust against mislabeled samples and it can produce good classification results even in the presence of considerable proportion of mislabeled data. Moreover, our numerical analysis uncover a critical point of mislabeled subset size, below which the network is free of wrong label contamination, but above which the mislabeled samples start to propagate their labels to the rest of the network. These studies have practical importance to design secure and robust machine learning techniques.","",""
91,"J. Verrelst, L. Alonso, J. Caicedo, J. Moreno, G. Camps-Valls","Gaussian Process Retrieval of Chlorophyll Content From Imaging Spectroscopy Data",2013,"","","","",194,"2022-07-13 09:40:27","","10.1109/JSTARS.2012.2222356","","",,,,,91,10.11,18,5,9,"Precise and spatially-explicit knowledge of leaf chlorophyll content (Chl) is crucial to adequately interpret the chlorophyll fluorescence (ChF) signal from space. Accompanying information about the reliability of the Chl estimation becomes more important than ever. Recently, a new statistical method was proposed within the family of nonparametric Bayesian statistics, namely Gaussian Processes regression (GPR). GPR is simpler and more robust than their machine learning family members while maintaining very good numerical performance and stability. Other features include: (i) GPR requires a relatively small training data set and can adopt very flexible kernels, (ii) GPR identifies the relevant bands and observations in establishing relationships with a variable, and finally (iii) along with pixelwise estimations GPR provides accompanying confidence intervals. We used GPR to retrieve Chl from hyperspectral reflectance data and evaluated the portability of the regression model to other images. Based on field Chl measurements from the SPARC dataset and corresponding spaceborne CHRIS spectra (acquired in 2003, Barrax, Spain), GPR developed a regression model that was excellently validated (r2: 0.96, RMSE: 3.82 μg/cm2). The SPARC-trained GPR model was subsequently applied to CHRIS images (Barrax, 2003, 2009) and airborne CASI flightlines (Barrax 2009) to generate Chl maps. The accompanying confidence maps provided insight in the robustness of the retrievals. Similar confidences were achieved by both sensors, which is encouraging for upscaling Chl estimates from field to landscape scale. Because of its robustness and ability to deliver confidence intervals, GPR is evaluated as a promising candidate for implementation into ChF processing chains.","",""
7,"J. Brassey, Christopher Price, Jonny Edwards, Markus Zlabinger, Alexandros Bampoulidis, A. Hanbury","Developing a fully automated evidence synthesis tool for identifying, assessing and collating the evidence",2019,"","","","",195,"2022-07-13 09:40:27","","10.1136/bmjebm-2018-111126","","",,,,,7,2.33,1,6,3,"Evidence synthesis is a key element of evidence-based medicine. However, it is currently hampered by being labour intensive meaning that many trials are not incorporated into robust evidence syntheses and that many are out of date. To overcome this, a variety of techniques are being explored, including using automation technology. Here, we describe a fully automated evidence synthesis system for intervention studies, one that identifies all the relevant evidence, assesses the evidence for reliability and collates it to estimate the relative effectiveness of an intervention. Techniques used include machine learning, natural language processing and rule-based systems. Results are visualised using modern visualisation techniques. We believe this to be the first, publicly available, automated evidence synthesis system: an evidence mapping tool that synthesises evidence on the fly.","",""
20,"A. Gajewicz","How to judge whether QSAR/read-across predictions can be trusted: a novel approach for establishing a model's applicability domain",2018,"","","","",196,"2022-07-13 09:40:27","","10.1039/C7EN00774D","","",,,,,20,5.00,20,1,4,"The EU REACH legislation, the OECD and US EPA official guidance documents, as well as the 3Rs principle (replacement, reduction, refinement of animal testing), all advocate the necessity of developing comprehensive computational methods (e.g. quantitative structure–activity relationship, read-across) that would enable the predictive modeling of both chemical (e.g. nanoparticle) specific functionalities and their hazards. However, since computational (nano)toxicology continues to ‘learn on the fly’ and relies on the use of a vast array of innovative machine-learning algorithms, serious concerns about the reliability of in silico predictions are raised. This study aimed to give an answer to the following question: how to judge whether QSAR/read-across predictions are reliable. Here, an effective approach for graphical assessment of the limits of a model's reliable predictions (so-called applicability domain, AD) was introduced. The probability-oriented distance-based approach (ADProbDist) was proposed as a robust and automatic method for defining the interpolation space where true and reliable predictions can be expected. Its usefulness was confirmed by using four nano-QSAR/read-across models recently reported in the literature. The results of the study showed that the ADProbDist approach is more restrictive in terms of the chemical space that falls in the AD of a model than the range, geometrical, distance and leverage approaches. The advantages of the proposed ADProbDist approach include (but are not limited to) the fact that it works with relatively small datasets and enables the identification of (un)reliable predictions for newly screened chemicals without experimental data. Further, to facilitate the use of the ADProbDist approach, this study provides the developed in-house R-codes.","",""
3,"M. Costanzo, G. Maria, Gaetano Lettera, C. Natale, D. Perrone","A Multimodal Perception System for Detection of Human Operators in Robotic Work Cells",2019,"","","","",197,"2022-07-13 09:40:27","","10.1109/SMC.2019.8914519","","",,,,,3,1.00,1,5,3,"Workspace monitoring is a critical hw/sw component of modern industrial work cells or in service robotics scenarios, where human operators share their workspace with robots. Reliability of human detection is a major requirement not only for safety purposes but also to avoid unnecessary robot stops or slowdowns in case of false positives. The present paper introduces a novel multimodal perception system for human tracking in shared workspaces based on the fusion of depth and thermal images. A machine learning approach is pursued to achieve reliable detection performance in multi-robot collaborative systems. Robust experimental results are finally demonstrated on a real robotic work cell.","",""
3,"Pranesh Santikellur, Lakshya, Shashi Prakash, R. Chakraborty","A Computationally Efficient Tensor Regression Network based Modeling Attack on XOR APUF",2019,"","","","",198,"2022-07-13 09:40:27","","10.1109/AsianHOST47458.2019.9006692","","",,,,,3,1.00,1,4,3,"XOR-arbiter PUF (XOR APUF), where the outputs of multiple APUFs are XOR-ed, has proven to be robust to machine learning based modeling attacks. The reported successful modeling attacks for XOR APUF either employ auxiliary side-channel or reliability information, or require enormous computational effort. This robustness is primarily due to the difficulty in learning the unknown internal delay parameter terms in the mathematical model of a XOR APUF, and the robustness increases as the number of APUFs being XOR-ed increases. In this paper, we employ a novel machine learning based modeling technique called efficient CANDECOMP/PARAFAC-Tensor Regression Network (CP-TRN), a variant of CP-decomposition based tensor regression network, to reduce the computational resource requirement of model building attacks on XOR APUF. In addition, our proposed technique does not require any auxiliary information, and is robust to noisy training data. The proposed technique allowed us to successfully model 64-bit 8-XOR APUF and 128-bit 7-XOR APUF on a single desktop workstation, with high prediction accuracy.","",""
16,"M. Javed, Elyes Ben Hamida, Ala Al-Fuqaha, B. Bhargava","Adaptive Security for Intelligent Transport System Applications",2018,"","","","",199,"2022-07-13 09:40:27","","10.1109/MITS.2018.2806636","","",,,,,16,4.00,4,4,4,"The transportation system is gradually migrating toward autonomous, electric and intelligent vehicles. Wireless-enabled vehicles along with infrastructure units on the road are connected with traffic management centers that use intelligent data analysis tools to efficiently manage city's traffic. However, such wireless connectivity can make the ITS networks vulnerable to security threats; thus, impacting the application's reliability. On the other hand, the use of robust security techniques could hamper applications' quality of service (QoS). To understand the interplay between these two conflicting requirements, this article reviews the security and QoS design challenges in the ITS aspect of smart cities. Using an experimental test-bed, we evaluate the standard compliant security processing delays, develop an on-line tool that presents detailed security benchmark results, and study the impact of security on QoS using simulation results. We also discuss how machine learning based adaptive signature verification techniques can enhance QoS in ITS. We further present future opportunities to optimize the security-QoS balance for ITS applications.","",""
54,"E. Papatheou, N. Dervilis, A. E. Maguire, I. Antoniadou, K. Worden","A Performance Monitoring Approach for the Novel Lillgrund Offshore Wind Farm",2015,"","","","",200,"2022-07-13 09:40:27","","10.1109/TIE.2015.2442212","","",,,,,54,7.71,11,5,7,"The use of offshore wind farms has been growing in recent years. Europe is presenting a geometrically growing interest in exploring and investing in such offshore power plants as the continent's water sites offer impressive wind conditions. Moreover, as human activities tend to complicate the construction of land wind farms, offshore locations, which can be found more easily near densely populated areas, can be seen as an attractive choice. However, the cost of an offshore wind farm is relatively high, and therefore, their reliability is crucial if they ever need to be fully integrated into the energy arena. This paper presents an analysis of supervisory control and data acquisition (SCADA) extracts from the Lillgrund offshore wind farm for the purposes of monitoring. An advanced and robust machine-learning approach is applied, in order to produce individual and population-based power curves and then predict measurements of the power produced from each wind turbine (WT) from the measurements of the other WTs in the farm. Control charts with robust thresholds calculated from extreme value statistics are successfully applied for the monitoring of the turbines.","",""
