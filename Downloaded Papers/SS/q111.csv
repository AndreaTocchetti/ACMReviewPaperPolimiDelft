Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
0,"Maroun Touma, Shalisha Witherspoon, S. Witherspoon, Isabelle Crawford-Eng","A practical approach for applying Machine Learning in the detection and classification of network devices used in building management",2020,"","","","",1,"2022-07-13 09:39:06","","10.22541/au.160689781.19054555/v1","","",,,,,0,0.00,0,4,2,"With the increasing deployment of smart buildings and infrastructure, Supervisory Control and Data Acquisition (SCADA) devices and the underlying IT network have become essential elements for the proper operations of these highly complex systems. Of course, with the increase in automation and the proliferation of SCADA devices, a corresponding increase in surface area of attack on critical infrastructure has increased. Understanding device behaviors in terms of known and understood or potentially qualified activities versus unknown and potentially nefarious activities in near-real time is a key component of any security solution. In this paper, we investigate the challenges with building robust machine learning models to identify unknowns purely from network traffic both inside and outside firewalls, starting with missing or inconsistent labels across sites, feature engineering and learning, temporal dependencies and analysis, and training data quality (including small sample sizes) for both shallow and deep learning methods. To demonstrate these challenges and the capabilities we have developed, we focus on Building Automation and Control networks (BACnet) from a private commercial building system. Our results show that ”Model Zoo” built from binary classifiers based on each device or behavior combined with an ensemble classifier integrating information from all classifiers provides a reliable methodology to identify unknown devices as well as determining specific known devices when the device type is in the training set. The capability of the Model Zoo framework is shown to be directly linked to feature engineering and learning, and the dependency of the feature selection varies depending on both the binary and ensemble classifiers as well.","",""
37,"P. Athamanolap, V. Parekh, S. I. Fraley, Vatsal Agarwal, D. J. Shin, M. Jacobs, Tza-Huei Wang, Samuel Yang","Trainable High Resolution Melt Curve Machine Learning Classifier for Large-Scale Reliable Genotyping of Sequence Variants",2014,"","","","",2,"2022-07-13 09:39:06","","10.1371/journal.pone.0109094","","",,,,,37,4.63,5,8,8,"High resolution melt (HRM) is gaining considerable popularity as a simple and robust method for genotyping sequence variants. However, accurate genotyping of an unknown sample for which a large number of possible variants may exist will require an automated HRM curve identification method capable of comparing unknowns against a large cohort of known sequence variants. Herein, we describe a new method for automated HRM curve classification based on machine learning methods and learned tolerance for reaction condition deviations. We tested this method in silico through multiple cross-validations using curves generated from 9 different simulated experimental conditions to classify 92 known serotypes of Streptococcus pneumoniae and demonstrated over 99% accuracy with 8 training curves per serotype. In vitro verification of the algorithm was tested using sequence variants of a cancer-related gene and demonstrated 100% accuracy with 3 training curves per sequence variant. The machine learning algorithm enabled reliable, scalable, and automated HRM genotyping analysis with broad potential clinical and epidemiological applications.","",""
0,"Hamdah Alotaibi, Fawaz Alsolami, Ehab A. Abozinadah, R. Mehmood","TAWSEEM: A Deep-Learning-Based Tool for Estimating the Number of Unknown Contributors in DNA Profiling",2022,"","","","",3,"2022-07-13 09:39:06","","10.3390/electronics11040548","","",,,,,0,0.00,0,4,1,"DNA profiling involves the analysis of sequences of an individual or mixed DNA profiles to identify the persons that these profiles belong to. A critically important application of DNA profiling is in forensic science to identify criminals by finding a match between their blood samples and the DNA profile found on the crime scene. Other applications include paternity tests, disaster victim identification, missing person investigations, and mapping genetic diseases. A crucial task in DNA profiling is the determination of the number of contributors in a DNA mixture profile, which is challenging due to issues that include allele dropout, stutter, blobs, and noise in DNA profiles; these issues negatively affect the estimation accuracy and the computational complexity. Machine-learning-based methods have been applied for estimating the number of unknowns; however, there is limited work in this area and many more efforts are required to develop robust models and their training on large and diverse datasets. In this paper, we propose and develop a software tool called TAWSEEM that employs a multilayer perceptron (MLP) neural network deep learning model for estimating the number of unknown contributors in DNA mixture profiles using PROVEDIt, the largest publicly available dataset. We investigate the performance of our developed deep learning model using four performance metrics, namely accuracy, F1-score, recall, and precision. The novelty of our tool is evident in the fact that it provides the highest accuracy (97%) compared to any existing work on the most diverse dataset (in terms of the profiles, loci, multiplexes, etc.). We also provide a detailed background on the DNA profiling and literature review, and a detailed account of the deep learning tool development and the performance investigation of the deep learning method.","",""
3,"A. Preece, Daniel Harborne, R. Raghavendra, Richard J. Tomsett, Dave Braines","Provisioning Robust and Interpretable AI/ML-Based Service Bundles",2018,"","","","",4,"2022-07-13 09:39:06","","10.1109/MILCOM.2018.8599838","","",,,,,3,0.75,1,5,4,"Coalition operations environments are characterised by the need to share intelligence, surveillance and reconnaissance services. Increasingly, such services are based on artificial intelligence (AI)and machine learning (ML)technologies. Two key issues in the exploitation of AI/ML services are robustness and interpretability. Employing a diverse portfolio of services can make a system robust to ‘unknown unknowns’. Interpretability - the need for services to offer explanation facilities to engender user trust - can be addressed by a variety of methods to generate either transparent or post hoc explanations according to users' requirements. This paper shows how a service-provisioning framework for coalition operations can be extended to address specific requirements for robustness and interpretability, allowing automatic selection of service bundles for intelligence, surveillance and reconnaissance tasks. The approach is demonstrated in a case study on traffic monitoring featuring a diverse set of AI/ML services based on deep neural networks and heuristic reasoning approaches.","",""
1,"K. Hidenori","A new neural computation scheme of unsupervised learning with applications to robot biped locomotion",2008,"","","","",5,"2022-07-13 09:39:06","","10.1109/CHICC.2008.4604874","","",,,,,1,0.07,1,1,14,"A new neural computational scheme of unsupervised learning is proposed to construct a machine intelligence that is capable of overcoming unpredictable uncertainties and unknowns through proper interactions with environment. Our scheme consists of homogeneous neuron distributions which form layered clusters of computational circuit. Each neuron is very simple and of classical McCulloch-Pitts type equipped with Hebb-type plasticity for their interconnections. The novelty of our neuron lies in its ability to change its threshold according to its firing situation, which makes our scheme stable and configurable. Each cluster of neurons represents the numerical values by the number of firing neurons just like enumerations by fingers. This nonsymbolic nature of computations is shown to be very robust. It is shown that our configuration can act as a type of adaptive control which exhibits brain-like functions in its learning behaviors. Our scheme is shown to be successfully implemented to a biped robot that can walk under unstructured environment.","",""
0,"Shelby Heinecke","Resilient Structures and Robust Machine Learning Algorithms",2020,"","","","",6,"2022-07-13 09:39:06","","10.25417/UIC.13474959.V1","","",,,,,0,0.00,0,1,2,"Networks and learning algorithms are key themes in artificial intelligence (AI). Networks can be used to model the connections of a vast community of internet-of-things (IoT) devices, the internet, and distributed databases, among other important computing contexts for AI. Learning algorithms, which generate trained models, can be strategically designed to learn from a variety of data-rich networks, such as sensor networks, device networks, or even networks of humans working on crowdsourcing tasks. In real-world settings, noise generated from sources such as device inaccuracy or human error are unavoidable. Likewise, attacks in real-world networks, such as the injection of malware, are inevitable due to their ever-evolving sophistication. In this thesis, we study the intrusions of noise and malicious attacks on networks and learning from networks. We devise structural and algorithmic solutions for mitigating the effects of these unwanted intrusions. We first we consider viral propagation under the independent cascade model of infection spread on half-regular bipartite networks and characterize the most resilient structures. Then, we study learning and generalizing from a network of crowd workers, where crowd workers provide erroneous labels to unlabelled data at fixed, unknown error rates. In this setting, we develop a three-step probably approximately correct (PAC) algorithm that incorporates majority voting, pure-exploration bandits, and noisy-PAC learning and demonstrate our algorithm’s improvement over baseline approaches. Finally, we study learning from a network of participants, each with their own distribution on the unlabelled data in the presence of noise. We develop collaborative PAC algorithms robust to classification noise and prove sample complexity bounds. We also study the communication complexity of collaborative PAC learning, with and without classification noise, and develop communication efficient algorithms in both settings.","",""
0,"Alekh Agarwal, Tong Zhang","Minimax Regret Optimization for Robust Machine Learning under Distribution Shift",2022,"","","","",7,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,2,1,"In this paper, we consider learning scenarios where the learned model is evaluated under an unknown test distribution which potentially differs from the training distribution (i.e. distribution shift). The learner has access to a family of weight functions such that the test distribution is a reweighting of the training distribution under one of these functions, a setting typically studied under the name of Distributionally Robust Optimization (DRO). We consider the problem of deriving regret bounds in the classical learning theory setting, and require that the resulting regret bounds hold uniformly for all potential test distributions. We show that the DRO formulation does not guarantee uniformly small regret under distribution shift. We instead propose an alternative method called Minimax Regret Optimization (MRO), and show that under suitable conditions this method achieves uniformly low regret across all test distributions. We also adapt our technique to have stronger guarantees when the test distributions are heterogeneous in their similarity to the training data. Given the widespead optimization of worst case risks in current approaches to robust machine learning, we believe that MRO can be a strong alternative to address distribution shift scenarios.","",""
3,"É. Chouzenoux, Henri G'erard, J. Pesquet","General risk measures for robust machine learning",2019,"","","","",8,"2022-07-13 09:39:06","","10.3934/fods.2019011","","",,,,,3,1.00,1,3,3,"A wide array of machine learning problems are formulated as the minimization of the expectation of a convex loss function on some parameter space. Since the probability distribution of the data of interest is usually unknown, it is is often estimated from training sets, which may lead to poor out-of-sample performance. In this work, we bring new insights in this problem by using the framework which has been developed in quantitative finance for risk measures. We show that the original min-max problem can be recast as a convex minimization problem under suitable assumptions. We discuss several important examples of robust formulations, in particular by defining ambiguity sets based on $\varphi$-divergences and the Wasserstein metric.We also propose an efficient algorithm for solving the corresponding convex optimization problems involving complex convex constraints. Through simulation examples, we demonstrate that this algorithm scales well on real data sets.","",""
5,"Tao Chen, M. Ludkovski","A Machine Learning Approach to Adaptive Robust Utility Maximization and Hedging",2019,"","","","",9,"2022-07-13 09:39:06","","10.1137/20m1336023","","",,,,,5,1.67,3,2,3,"We investigate the adaptive robust control framework for portfolio optimization and loss-based hedging under drift and volatility uncertainty. Adaptive robust problems offer many advantages but require handling a double optimization problem (infimum over market measures, supremum over the control) at each instance. Moreover, the underlying Bellman equations are intrinsically multi-dimensional. We propose a novel machine learning approach that solves for the local saddle-point at a chosen set of inputs and then uses a nonparametric (Gaussian process) regression to obtain a functional representation of the value function. Our algorithm resembles control randomization and regression Monte Carlo techniques but also brings multiple innovations, including adaptive experimental design, separate surrogates for optimal control and the local worst-case measure, and computational speed-ups for the sup-inf optimization. Thanks to the new scheme we are able to consider settings that have been previously computationally intractable and provide several new financial insights about learning and optimal trading under unknown market parameters. In particular, we demonstrate the financial advantages of adaptive robust framework compared to adaptive and static robust alternatives.","",""
0,"M. Rostami, O. Saarela, M. Escobar, D. Lana","Doubly Robust Estimation with Machine Learning Predictions",2021,"","","","",10,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,4,1,"The estimation of Average Treatment Effect (ATE) as a causal parameter is carried out in two steps, where in the first step, the treatment and outcome are modeled to incorporate the potential confounders, and in the second step, the predictions are inserted into the ATE estimators such as the Augmented Inverse Probability Weighting (AIPW) estimator. Due to the concerns regarding the nonlinear or unknown relationships between confounders and the treatment and outcome, there has been an interest in applying non-parametric methods such as Machine Learning (ML) algorithms instead. Farrell et al. (2018) proposed to use two separate Neural Networks (NNs) where there’s no regularization on the network’s parameters except the Stochastic Gradient Descent (SGD) in the NN’s optimization. Our simulations indicate that the AIPW estimator suffers extensively if no regularization is utilized. We propose the normalization of AIPW (referred to as nAIPW) which can be helpful in some scenarios. nAIPW, provably, has the same properties as AIPW, that is double-robustness and orthogonality (Chernozhukov et al., 2018). Further, if the first step algorithms converge fast enough, under regulatory conditions (Chernozhukov et al., 2018), nAIPW will be asymptotically normal. We also compare the performance of AIPW and nAIPW in terms of the bias and variance when small to moderate L1 regularization is imposed on the NNs.","",""
101,"G. Lecu'e, M. Lerasle","Robust machine learning by median-of-means: Theory and practice",2017,"","","","",11,"2022-07-13 09:39:06","","10.1214/19-AOS1828","","",,,,,101,20.20,51,2,5,"We introduce new estimators for robust machine learning based on median-of-means (MOM) estimators of the mean of real valued random variables. These estimators achieve optimal rates of convergence under minimal assumptions on the dataset. The dataset may also have been corrupted by outliers on which no assumption is granted. We also analyze these new estimators with standard tools from robust statistics. In particular, we revisit the concept of breakdown point. We modify the original definition by studying the number of outliers that a dataset can contain without deteriorating the estimation properties of a given estimator. This new notion of breakdown number, that takes into account the statistical performances of the estimators, is non-asymptotic in nature and adapted for machine learning purposes. We proved that the breakdown number of our estimator is of the order of (number of observations)*(rate of convergence). For instance, the breakdown number of our estimators for the problem of estimation of a d-dimensional vector with a noise variance sigma^2 is sigma^2d and it becomes sigma^2 s log(d/s) when this vector has only s non-zero component. Beyond this breakdown point, we proved that the rate of convergence achieved by our estimator is (number of outliers) divided by (number of observation).  Besides these theoretical guarantees, the major improvement brought by these new estimators is that they are easily computable in practice. In fact, basically any algorithm used to approximate the standard Empirical Risk Minimizer (or its regularized versions) has a robust version approximating our estimators. As a proof of concept, we study many algorithms for the classical LASSO estimator. A byproduct of the MOM algorithms is a measure of depth of data that can be used to detect outliers.","",""
34,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, Semeen Rehman, M. Shafique","Robust Machine Learning Systems: Reliability and Security for Deep Neural Networks",2018,"","","","",12,"2022-07-13 09:39:06","","10.1109/IOLTS.2018.8474192","","",,,,,34,8.50,7,5,4,"Machine learning is commonly being used in almost all the areas that involve advanced data analytics and intelligent control. From applications like Natural Language Processing (NLP) to autonomous driving are based upon machine learning algorithms. An increasing trend is observed in the use of Deep Neural Networks (DNNs) for such applications. While the slight inaccuracy in applications like NLP does not have any severe consequences, it is not the same for other safety-critical applications, like autonomous driving and smart healthcare, where a small error can lead to catastrophic effects. Apart from high-accuracy DNN algorithms, there is a significant need for robust machine learning systems and hardware architectures that can generate reliable and trustworthy results in the presence of hardware-level faults while also preserving security and privacy. This paper provides an overview of the challenges being faced in ensuring reliable and secure execution of DNNs. To address the challenges, we present several techniques for analyzing and mitigating the reliability and security threats in machine learning systems.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",13,"2022-07-13 09:39:06","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
9,"Ruben F. Kranenburg, J. Verduin, Y. Weesepoel, M. Alewijn, Marcel Heerschop, G. Koomen, P. Keizers, Frank Bakker, Fionn Wallace, Annette van Esch, Annemieke Hulsbergen, A. V. van Asten","Rapid and robust on‐scene detection of cocaine in street samples using a handheld near‐infrared spectrometer and machine learning algorithms",2020,"","","","",14,"2022-07-13 09:39:06","","10.1002/dta.2895","","",,,,,9,4.50,1,12,2,"Abstract On‐scene drug detection is an increasingly significant challenge due to the fast‐changing drug market as well as the risk of exposure to potent drug substances. Conventional colorimetric cocaine tests involve handling of the unknown material and are prone to false‐positive reactions on common pharmaceuticals used as cutting agents. This study demonstrates the novel application of 740–1070 nm small‐wavelength‐range near‐infrared (NIR) spectroscopy to confidently detect cocaine in case samples. Multistage machine learning algorithms are used to exploit the limited spectral features and predict not only the presence of cocaine but also the concentration and sample composition. A model based on more than 10,000 spectra from case samples yielded 97% true‐positive and 98% true‐negative results. The practical applicability is shown in more than 100 case samples not included in the model design. One of the most exciting aspects of this on‐scene approach is that the model can almost instantly adapt to changes in the illicit‐drug market by updating metadata with results from subsequent confirmatory laboratory analyses. These results demonstrate that advanced machine learning strategies applied on limited‐range NIR spectra from economic handheld sensors can be a valuable procedure for rapid on‐site detection of illicit substances by investigating officers. In addition to forensics, this interesting approach could be beneficial for screening and classification applications in the pharmaceutical, food‐safety, and environmental domains.","",""
3,"Willem Raes, Nicolas Knudde, Jorik De Bruycker, T. Dhaene, N. Stevens","Experimental Evaluation of Machine Learning Methods for Robust Received Signal Strength-Based Visible Light Positioning",2020,"","","","",15,"2022-07-13 09:39:06","","10.3390/s20216109","","",,,,,3,1.50,1,5,2,"In this work, the use of Machine Learning methods for robust Received Signal Strength (RSS)-based Visible Light Positioning (VLP) is experimentally evaluated. The performance of Multilayer Perceptron (MLP) models and Gaussian processes (GP) is investigated when using relative RSS input features. The experimental set-up for the RSS-based VLP technology uses light-emitting diodes (LEDs) transmitting intensity modulated light and a single photodiode (PD) as a receiver. The experiments focus on achieving robustness to cope with unknown received signal strength modifications over time. Therefore, several datasets were collected, where per dataset either the LEDs transmitting power is modified or the PD aperture is partly obfuscated by dust particles. Two relative RSS schemes are investigated. The first scheme uses the maximum received light intensity to normalize the received RSS vector, while the second approach obtains RSS ratios by combining all possible unique pairs of received intensities. The Machine Learning (ML) methods are compared to a relative multilateration implementation. It is demonstrated that the adopted MLP and GP models exhibit superior performance and higher robustness when compared to the multilateration strategies. Furthermore, when comparing the investigated ML models, the GP model is proven to be more robust than the MLP for the considered scenarios.","",""
1,"Iman Al Selaiti, Carlos Mata, L. Saputelli, D. Badmaev, Yara Alatrach, Erismar Rubio, R. Mohan, Daniel Quijada","Robust Data Driven Well Performance Optimization Assisted by Machine Learning Techniques for Natural Flowing and Gas-Lift Wells in Abu Dhabi",2020,"","","","",16,"2022-07-13 09:39:06","","10.2118/201696-ms","","",,,,,1,0.50,0,8,2,"  Asset management success is accomplished when the integrated production system is operating close to its intended potential. Continuous awareness of wells and facility conditions are key factor in the realization of designed capacity. In contrast, unknown status and conditions can severely limit production capacity.  The rise of instrumentation technologies over the last four decades have created new opportunities to understand well and reservoir behavior. However, despite of being proved as a cost-effective surveillance initiative, remote monitoring is still not adopted in over 60% of oil and gas fields around the world. Understanding the value of data through machine learning techniques is the basis for establishing a robust surveillance strategy.  The objective of this paper is to develop a data-driven approach, enabled by Artificial Intelligence (AI) methodologies including machine learning (ML), to find optimal operating envelope for gas-lift wells. The process involves building ML models for generating instantaneous predictions of multiphase flow rates and other quantities of interest, such as GOR, WCT, using real-time sensor data at the surface, historical performance, and sporadic test data.  Additionally, forecasting models were developed for generating short-term (30 days) forecast of cumulative oil, water, gas, and liquid production, multiphase flow rates, WCT, GOR, and reservoir pressure. Using time-series forecasting models, a sensitivity analysis was performed to generate short-term well response for a selected number of combinations of choke settings, and gas injection rates. Sensitivity analysis provides 2D maps of well response highlight an operating envelope, which are proposed to be combined with physical and operational constraints to arrive at optimal operating conditions, which may effortlessly add 2.5% net profit from optimum gas-lift alocation.  The results of this work show encouraging results, and demonstrate value that AI-enabled methodologies can provide in instrumented wells by enabling automated workflows for virtual metering, production allocation, short-term production forecasting, and deriving optimal operating conditions. The developed AI methodology has tremendous potential of integration in an end-to-end workflow of autonomous well control by utilizing available data to produce easy to update ML models, with little to no human intervention.","",""
5,"Yakun Wang, Liangsheng Shi, Lin Lin, M. Holzman, Facundo Carmona, Qiuru Zhang","A robust data‐worth analysis framework for soil moisture flow by hybridizing sequential data assimilation and machine learning",2020,"","","","",17,"2022-07-13 09:39:06","","10.1002/vzj2.20026","","",,,,,5,2.50,1,6,2,"As the collection of soil moisture data is often costly, it is essential to implement data‐worth analysis in advance to obtain a cost‐effective data collection scheme. In previous data‐worth analysis, the model structural error is often neglected. In this paper, we propose a robust data‐worth analysis framework based on a hybrid data assimilation method. By constructing Gaussian process (GP) error model, this study attempts to alleviate biased data‐worth assessments caused by unknown model structural errors, and to excavate complementary values of multisource data without resorting to multiple governing equations. The results demonstrated that this proposed framework effectively identified and compensated for complex model structural errors. By training prior data, more accurate potential observations were obtained and data‐worth estimation accuracy was improved. The scenario diversity played a crucial role in establishing an effective GP training system. The integration of soil temperature into GP training unraveled new information and improved the data‐worth estimation. Instead of traditional evapotranspiration calculations, the direct inclusion of easy‐to‐obtain meteorological data into GP training yielded better data‐worth assessment.","",""
0,"Farzin Piltan, Alexander E. Prosvirin, Jong-Myon Kim","Robot manipulator active fault-tolerant control using a machine learning-based automated robust hybrid observer",2020,"","","","",18,"2022-07-13 09:39:06","","10.3233/JIFS-189109","","",,,,,0,0.00,0,3,2,"Robotic manipulators represent a class of nonlinear and multiple-degrees-of-freedom robots that have pronounced coupling effects and can be used in various applications. The challenge of understanding complexity in a system’s dynamic behavior, coupling effects, and sources of uncertainty presents substantial challenges regarding fault estimation, detection, identification, and tolerant-control (FEDIT) in a robot manipulator. Thus, a proposed active fault-tolerant control algorithm, based on an adaptive modern sliding mode observer, is represented. Due to the effect of the system’s complexities and uncertainties for fault estimation, detection, and identification (FEDI), a sliding mode observer (SMO) is proposed. To address the sliding mode observer drawbacks for FEDI such as high-frequency oscillation (chattering) and fault estimation accuracy, the modern (T-S fuzzy higher order) technique is represented. In addition, the adaptive technique is applied to the modern sliding mode observer (MSMO) to self-tune the coefficients of the fault estimation observer to increase the reliability and robustness of decision-making for diagnosis of the fault. Next, the residual delivered by the adaptive MSMO (AMSMO) is split into windows, and each window is characterized by a numerical parameter. Finally, the machine learning technique known as a decision tree adaptively derives the threshold values that are used for problems of fault detection and fault identification in this work. Due to control of the effective fault, a surface automated new sliding mode controller (SANSMC) is presented in this work. To address the challenge of chattering and unlimited uncertainties (faults), the AMSMO is applied to the sliding mode controller (SMC). In addition, the surface-automated technique is used to fine-tune the surface coefficient to reduce the chattering and faults in the robot manipulator. The results show that the machine learning-based automated robust hybrid observer significantly improves the robustness, reliability, and accuracy of FEDIT in unknown conditions.","",""
53,"Bethany M. Moore, Peipei Wang, P. Fan, Bryan J. Leong, Craig A. Schenck, J. P. Lloyd, Melissa D. Lehti-Shiu, R. Last, E. Pichersky, S. Shiu","Robust predictions of specialized metabolism genes through machine learning",2018,"","","","",19,"2022-07-13 09:39:06","","10.1073/pnas.1817074116","","",,,,,53,13.25,5,10,4,"Significance Specialized metabolites are critical for plant–environment interactions, e.g., attracting pollinators or defending against herbivores, and are important sources of plant-based pharmaceuticals. However, it is unclear what proportion of enzyme-encoding genes play a role in specialized metabolism (SM) as opposed to general metabolism (GM) in any plant species. This is because of the diversity of specialized metabolites and the considerable number of incompletely characterized pathways responsible for their production. In addition, SM gene ancestors frequently played roles in GM. We evaluate features distinguishing SM and GM genes and build a computational model that accurately predicts SM genes. Our predictions provide candidates for experimental studies, and our modeling approach can be applied to other species that produce medicinally or industrially useful compounds. Plant specialized metabolism (SM) enzymes produce lineage-specific metabolites with important ecological, evolutionary, and biotechnological implications. Using Arabidopsis thaliana as a model, we identified distinguishing characteristics of SM and GM (general metabolism, traditionally referred to as primary metabolism) genes through a detailed study of features including duplication pattern, sequence conservation, transcription, protein domain content, and gene network properties. Analysis of multiple sets of benchmark genes revealed that SM genes tend to be tandemly duplicated, coexpressed with their paralogs, narrowly expressed at lower levels, less conserved, and less well connected in gene networks relative to GM genes. Although the values of each of these features significantly differed between SM and GM genes, any single feature was ineffective at predicting SM from GM genes. Using machine learning methods to integrate all features, a prediction model was established with a true positive rate of 87% and a true negative rate of 71%. In addition, 86% of known SM genes not used to create the machine learning model were predicted. We also demonstrated that the model could be further improved when we distinguished between SM, GM, and junction genes responsible for reactions shared by SM and GM pathways, indicating that topological considerations may further improve the SM prediction model. Application of the prediction model led to the identification of 1,220 A. thaliana genes with previously unknown functions, each assigned a confidence measure called an SM score, providing a global estimate of SM gene content in a plant genome.","",""
2,"Jiafei Zhao, Rongkun Jiang, Xuetian Wang, Hongmin Gao","Robust CFAR Detection for Multiple Targets in K-Distributed Sea Clutter Based on Machine Learning",2019,"","","","",20,"2022-07-13 09:39:06","","10.3390/sym11121482","","",,,,,2,0.67,1,4,3,"For K-distributed sea clutter, a constant false alarm rate (CFAR) is crucial as a desired property for automatic target detection in an unknown and non-stationary background. In multiple-target scenarios, the target masking effect reduces the detection performance of CFAR detectors evidently. A machine learning based processor, associating the artificial neural network (ANN) and a clustering algorithm of density-based spatial clustering of applications with noise (DBSCAN), namely, DBSCAN-CFAR, is proposed herein to address this issue. ANN is trained with a symmetrical structure to estimate the shape parameter of background clutter, whereas DBSCAN is devoted to excluding interference targets and sea spikes as outliers in the leading and lagging windows that are symmetrical about the cell under test (CUT). Simulation results verified that the ANN-based method provides the optimal parameter estimation results in the range of 0.1 to 30, which facilitates the control of actual false alarm probability. The effectiveness and robustness of DBSCAN-CFAR are also confirmed by the comparisons of conventional CFAR processors in different clutter conditions, comprised of varying target numbers, shape parameters, and false alarm probabilities. Although the proposed ANN-based DBSCAN-CFAR processor incurs more elapsed time, it achieves superior CFAR performance without a prior knowledge on the number and distribution of interference targets.","",""
1,"Chen Bai, A. Wanigatunga, Santiago Saldana, R. Casanova, T. Manini, Mamoun T. Mardini","Are Machine Learning Models on Wrist Accelerometry Robust against Differences in Physical Performance among Older Adults?",2022,"","","","",21,"2022-07-13 09:39:06","","10.3390/s22083061","","",,,,,1,1.00,0,6,1,"Sufficient physical activity (PA) reduces the risk of a myriad of diseases and preserves physical capabilities in later life. While there have been significant achievements in mapping accelerations to real-life movements using machine learning (ML), errors continue to be common, particularly for wrist-worn devices. It remains unknown whether ML models are robust for estimating age-related loss of physical function. In this study, we evaluated the performance of ML models (XGBoost and LASSO) to estimate the hallmark measures of PA in low physical performance (LPP) and high physical performance (HPP) groups. Our models were built to recognize PA types and intensities, identify each individual activity, and estimate energy expenditure (EE) using wrist-worn accelerometer data (33 activities per participant) from a large sample of participants (n = 247, 57% females, aged 60+ years). Results indicated that the ML models were accurate in recognizing PA by type and intensity while also estimating EE accurately. However, the models built to recognize individual activities were less robust. Across all tasks, XGBoost outperformed LASSO. XGBoost obtained F1-Scores for sedentary (0.932 ± 0.005), locomotion (0.946 ± 0.003), lifestyle (0.927 ± 0.006), and strength flexibility exercise (0.915 ± 0.017) activity type recognition tasks. The F1-Scores for recognizing low, light, and moderate activity intensity were (0.932 ± 0.005), (0.840 ± 0.004), and (0.869 ± 0.005), respectively. The root mean square error for EE estimation was 0.836 ± 0.059 METs. There was no evidence showing that splitting the participants into the LPP and HPP groups improved the models’ performance on estimating the hallmark measures of physical activities. In conclusion, using features derived from wrist-worn accelerometer data, machine learning models can accurately recognize PA types and intensities and estimate EE for older adults with high and low physical function.","",""
0,"Miaomiao Gao, Xiao‐Zheng Jin, L. Ding","Robust Adaptive Fixed-time Trajectory Tracking Control of Manipulator based on Extreme Learning Machine",2020,"","","","",22,"2022-07-13 09:39:06","","10.23919/CCC50068.2020.9188544","","",,,,,0,0.00,0,3,2,"This paper mainly investigates the trajectory tracking control problems for manipulator systems with unknown dynamics and external disturbances. Firstly, an extreme learning machine (ELM) is adopt to compensate unknown dynamics of the manipulator. Then, an updating law is derived to ensure the convergence of ELM output wights. Besides, an indirect method is developed to avoid the potential singularity problem of the fixed-time sliding mode surface. Moreover, a robust adaptive controller is designed based on the outputs of ELM and sliding mode technique. By using the Lyapunov stability theory, the fixed-time convergence and stability of the adaptive control system can be guaranteed. Finally, simulation results is presented to show the efficiency of the proposed control structure with respect to different initial conditions.","",""
11,"B. Celik, J. Vanschoren","Adaptation Strategies for Automated Machine Learning on Evolving Data",2020,"","","","",23,"2022-07-13 09:39:06","","10.1109/TPAMI.2021.3062900","","",,,,,11,5.50,6,2,2,"Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust to changes in the underlying data. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on a variety of AutoML approaches for building machine learning pipelines, including Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.","",""
5,"Jie Zhang, Yanjiao Li, Wendong Xiao, Zhiqiang Zhang","Robust extreme learning machine for modeling with unknown noise",2020,"","","","",24,"2022-07-13 09:39:06","","10.1016/j.jfranklin.2020.06.027","","",,,,,5,2.50,1,4,2,"","",""
112,"Heinrich Jiang, Ofir Nachum","Identifying and Correcting Label Bias in Machine Learning",2019,"","","","",25,"2022-07-13 09:39:06","","","","",,,,,112,37.33,56,2,3,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",26,"2022-07-13 09:39:06","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
2,"Hamdah Alotaibi, Fawaz Alsolami, Rashid Mehmood","DNA Profiling: An Investigation of Six Machine Learning Algorithms for Estimating the Number of Contributors in DNA Mixtures",2021,"","","","",27,"2022-07-13 09:39:06","","10.14569/ijacsa.2021.0121115","","",,,,,2,2.00,1,3,1,"DNA (Deoxyribonucleic acid) profiling involves analysis of sequences of individual or mixed DNA profiles to identify persons these profiles belong to. DNA profiling is used in important applications such as for paternity tests, in forensic science for person identification on a crime scheme, etc. Finding the number of contributors in a DNA mixture is a major task in DNA profiling with challenges caused due to allele dropout, stutter, blobs, and noise. The existing methods for finding the number of unknowns in a DNA mixture suffer from issues including computational complexity and accuracy of estimating the number of unknowns. Machine learning has received attention recently in this area but with limited success. Many more efforts are needed for improving the robustness and accuracy of these methods. Our research aims to advance the state-of-the-art in this area. Specifically, in this paper, we investigate the performance of six machine learning algorithms -Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), Logistic Regression (LR), Stochastic Gradient Descent (SGD), and Gaussian Naïve-Bayes (GNB) -applied to a publicly available dataset called PROVEDIt, containing mixtures with up to five contributors. We evaluate the algorithmic performance using confusion matrices and four performance metrics namely accuracy, F1-Score, Recall, and Precision. The results show that LR provides the highest Accuracy of 95% for mixtures with five contributors. Keywords—Machine learning; DNA profiling; DNA mixtures;","",""
0,"M. Jessell, Jiateng Guo, Yunqiang Li, M. Lindsay, R. Scalzo, J. Giraud, G. Pirot, E. Cripps, V. Ogarko","Into the Noddyverse: A massive data store of 3D geological models for Machine Learning & inversion applications",2021,"","","","",28,"2022-07-13 09:39:06","","10.5194/essd-2021-304","","",,,,,0,0.00,0,9,1,"Abstract. Unlike some other well-known challenges such as facial recognition, where Machine Learning and Inversion algorithms are widely developed, the geosciences suffer from a lack of large, labelled datasets that can be used to validate or train robust Machine Learning and inversion schemes. Publicly available 3D geological models are far too restricted in both number and the range of geological scenarios to serve these purposes. With reference to inverting geophysical data this problem is further exacerbated as in most cases real geophysical observations result from unknown 3D geology, and synthetic test datasets are often not particularly geological, nor geologically diverse. To overcome these limitations, we have used the Noddy modelling platform to generate one million models, which represent the first publicly accessible massive training set for 3D geology and resulting gravity and magnetic datasets. This model suite can be used to train Machine Learning systems, and to provide comprehensive test suites for geophysical inversion. We describe the methodology for producing the model suite, and discuss the opportunities such a model suit affords, as well as its limitations, and how we can grow and access this resource. ","",""
3,"Chao-Yu Guo, Yinghua Yang, Yi-Hau Chen","The Optimal Machine Learning-Based Missing Data Imputation for the Cox Proportional Hazard Model",2021,"","","","",29,"2022-07-13 09:39:06","","10.3389/fpubh.2021.680054","","",,,,,3,3.00,1,3,1,"An adequate imputation of missing data would significantly preserve the statistical power and avoid erroneous conclusions. In the era of big data, machine learning is a great tool to infer the missing values. The root means square error (RMSE) and the proportion of falsely classified entries (PFC) are two standard statistics to evaluate imputation accuracy. However, the Cox proportional hazards model using various types requires deliberate study, and the validity under different missing mechanisms is unknown. In this research, we propose supervised and unsupervised imputations and examine four machine learning-based imputation strategies. We conducted a simulation study under various scenarios with several parameters, such as sample size, missing rate, and different missing mechanisms. The results revealed the type-I errors according to different imputation techniques in the survival data. The simulation results show that the non-parametric “missForest” based on the unsupervised imputation is the only robust method without inflated type-I errors under all missing mechanisms. In contrast, other methods are not valid to test when the missing pattern is informative. Statistical analysis, which is improperly conducted, with missing data may lead to erroneous conclusions. This research provides a clear guideline for a valid survival analysis using the Cox proportional hazard model with machine learning-based imputations.","",""
1,"T. Acharya, Ishan Khatri, A. Annamalai, M. Chouikha","Efficacy of Heterogeneous Ensemble Assisted Machine Learning Model for Binary and Multi-Class Network Intrusion Detection",2021,"","","","",30,"2022-07-13 09:39:06","","10.1109/I2CACIS52118.2021.9495864","","",,,,,1,1.00,0,4,1,"The exponential rise in internet technologies and allied applications encompass a significantly large number of networked devices have alarmed academia-industries to achieve more effective and robust security solutions. Undeniably, digitization has led to revolution globally; however, the security threats, breaches, and subsequent losses indicate the need for a robust cybersecurity solution. Unlike classical intrusion detection systems (IDS), network IDS (NIDS) has been becoming more challenging due to continuous changes in attack-patterns and anomaly behavior. As solution data-driven machine learning methods have exhibited better by learning over network traffic information and detecting anomalies; however, its generalization over a network with both known and unknown patterns remains questionable. Moreover, most of the classical approaches fail to address the key issues of class-imbalance, level-of-significance centric feature selection, normalization and over-fitting problems resulting in different performance by varied machine learning models. In this paper, a novel and robust heterogeneous ensemble machine learning model is developed to detect anomalies in NIDS. The proposed model first applies sub-sampling to alleviate the class-imbalance problem of NIDS datasets. Subsequently, performing normalization using the Min-Max algorithm, it mapped the input data in the range of 0 to 1, thus alleviating overfitting and convergence. The feature reduction is used to reduce the features; it retained the most suitable features without imposing computational overheads, often in meta-heuristic-based approaches. Finally, the proposed NIDS solution designed a Heterogeneous ensemble learning model with J48, k-NN, SVM, Bagging, AdaBoost, and RF algorithms as base-classifier to perform two-class as well as multi-class classification over feature-selected NSL-KDD, KDD99, and UNSW-NB-15 datasets. Performance assessment in terms of true-positive rate, false positive rate and AUC revealed that the proposed NIDS model exhibited better performance than the standalone classifiers and superior to other existing anomaly detection methods.","",""
2,"Aamir S Ahanger, S. M. Khan, F. Masoodi","An Effective Intrusion Detection System using Supervised Machine Learning Techniques",2021,"","","","",31,"2022-07-13 09:39:06","","10.1109/ICCMC51019.2021.9418291","","",,,,,2,2.00,1,3,1,"With the increased use of Internet resources, cyber attackers are using novel ways to attack the services of network. Thus network security is becoming inevitable part of the network system. In order to detect such attacks efficiently and effectively, robust IDS (Intrusion Detection System) is needed. An IDS is a tool that analyzes each and every packet deeply to detects malicious activity by monitoring a network or a system. The main purpose of IDS is to identify unwanted or abnormal action and to inform the network administrator about such actions. Thus, IDS is important tool for the network administrator to prevent the network from both known and unknown attacks that make the network resources more vulnerable. Machine learning methods can be used to employ efficient intrusion detection system (IDS). In this research work four machine learning methods were used namely RF (Random Forest), DT (Decision Tree), MLP (Multilayer perceptron) and SVM (Support Vector Machine) for classification of the data. NSL-KDD dataset was used for training and testing these various machine learning models. Feature selections were used to eliminate the irrelevant and unwanted features from the dataset. Therefore feature selection reduces the dimensionality of the dataset which in turn reduces the computational complexity. The proposed model’s output was evaluated using three feature subsets, randomly selected from the NSL-KDD dataset. The proposed method has a classification accuracy of more than 99 percent.","",""
2,"Joe Kwan, Fan Jiang, Le Hong","Applying machine learning methods to accelerate advanced process node yield ramp",2021,"","","","",32,"2022-07-13 09:39:06","","10.1117/12.2584008","","",,,,,2,2.00,1,3,1,"With the continuous growth in IC manufacturing complexity, developing new process nodes has become an ever increasing challenge. From the initial process node architectural explorations to initial design rule specifications to early RET development and “risk production” early NPI (New Product Introductions), critical decisions with far reaching performance and yield impact must be made. Applying innovative methods to enable early and broad engineered testing informs better architectural decisions and performance tradeoffs. Methods to identify, root-cause, categorize known yield detractors and to flag unknown potentially new risk patterns enable product yield risk mitigation and continuous learning. Accumulated learning from each step, each stage and each new product drives improved testing vehicles, better process optimization, and enhanced PDKs, all leading to more robust designs and ultimately higher performance and improved yield. In this paper, we describe innovative Machine Learning methods in DFM and DTCO Applications to improve test vehicle engineering, inform process development and accelerate process node yield ramp.","",""
1,"L. Ahmed, Y. A. M. Hamad","Machine Learning Techniques for Network-based Intrusion Detection System: A Survey Paper",2021,"","","","",33,"2022-07-13 09:39:06","","10.1109/NCCC49330.2021.9428827","","",,,,,1,1.00,1,2,1,"The rapid growth of Internet technologies and further dependence on online services, increase the demand for keeping these networks and data secure. The protection of online information is becoming even more vital to the national security and economic stability. Recently, network security has become one of the most concerning subjects in the current research and industry fields. Intrusion Detection Systems (IDSs) are considered as the backbone for network and data protection. Throughout time, different IDS approaches have been implemented to attain maximum detection accuracy. Machine learning IDS is one of the promising IDS techniques that have been created to detect known as well as unknown attacks. This paper investigates various machine learning techniques used to deploy Network-based Intrusion Detection System (NIDS). This survey could provide a more robust understanding of the existing techniques and assists intrigued researchers to identify research opportunities and investigate more in this direction.","",""
2,"Liang-Liang Wang, Jun-Jie Ding, Peichang Shi, Li Fu, Li Pan, Jiahao Tian, Dongsheng Cao, Hui Jiang, Xiao-Qin Ding","Ensemble machine learning to evaluate the in vivo acute oral toxicity and in vitro human acetylcholinesterase inhibitory activity of organophosphates.",2021,"","","","",34,"2022-07-13 09:39:06","","10.1007/s00204-021-03056-6","","",,,,,2,2.00,0,9,1,"","",""
2,"E. Amiri Souri, A. Chenoweth, A. Cheung, S. Karagiannis, S. Tsoka","Cancer Grade Model: a multi-gene machine learning-based risk classification for improving prognosis in breast cancer",2021,"","","","",35,"2022-07-13 09:39:06","","10.1038/s41416-021-01455-1","","",,,,,2,2.00,0,5,1,"","",""
8,"Kookjin Lee, Nathaniel Trask, P. Stinis","Machine learning structure preserving brackets for forecasting irreversible processes",2021,"","","","",36,"2022-07-13 09:39:06","","","","",,,,,8,8.00,3,3,1,"Forecasting of time-series data requires imposition of inductive biases to obtain predictive extrapolation, and recent works have imposed Hamiltonian/Lagrangian form to preserve structure for systems with reversible dynamics. In this work we present a novel parameterization of dissipative brackets from metriplectic dynamical systems appropriate for learning irreversible dynamics with unknown a priori model form. The process learns generalized Casimirs for energy and entropy guaranteed to be conserved and nondecreasing, respectively. Furthermore, for the case of added thermal noise, we guarantee exact preservation of a ﬂuctuation-dissipation theorem, ensuring thermodynamic consistency. We provide benchmarks for dissipative systems demonstrating learned dynamics are more robust and generalize better than either ""black-box"" or penalty-based approaches.","",""
0,"Tony Hämmäinen, Julen Kahles","Clustering Unknown IoT Devices in a 5G Mobile Network Security Context via Machine Learning",2021,"","","","",37,"2022-07-13 09:39:06","","10.1109/wimob52687.2021.9606307","","",,,,,0,0.00,0,2,1,"We propose a novel machine-learning pipeline for clustering unknown IoT devices in an industrial 5G mobile-network setting. Organizing IoT devices as few homogeneous device groups improves the applicability of network-intrusion detection systems. More specifically, we develop feature engineering methods that transform IP-flows into device-level data points, define distance metrics between the data points, and apply the DBSCAN algorithm on them. Our experiments on a simulated IoT device network with varying levels of noise show that our proposed methodology outperforms alternative methods and is the only one producing a robust grouping of the IoT devices with noise present in the traffic data.","",""
0,"L. Merte, M. K. Bisbo, I. Sokolović, M. Setvín, Benjamin Hagman, M. Shipilin, M. Schmid, U. Diebold, E. Lundgren, B. Hammer","Structure of an ultrathin oxide film solved by machine learning enhanced global optimization",2021,"","","","",38,"2022-07-13 09:39:06","","10.26434/chemrxiv-2021-mtbq2","","",,,,,0,0.00,0,10,1,"Determination of the atomic structure of solid surfaces is a challenge that has resisted solution despite advancements in experimental methods. Theory-based global optimization has the potential to revolutionize the field by providing reliable structure models as the basis for interpretation of experiments and for prediction of material properties. So far, however, the approach has been limited by the combinatorial complexity and computational expense of sufficiently accurate energy estimation for surfaces. We demonstrate how an evolutionary algorithm, utilizing machine learning for accelerated energy estimation and diverse population generation, can be used to solve an unknown surface structure—the (4 x 4) surface oxide on Pt3Sn(111)--based on limited experimental input. The algorithm is efficient and robust, and should be broadly applicable in surface studies, where it can replace manual, intuition based model generation.","",""
0,"R. Pyke, Dattatreya Mellacheruvu, Charles W. Abbott, Steven Dea, E. Levy, Simo V. Zhang, Nikita Bedi, A. Colevas, Devayani P. Bhave, M. Chinnappa, G. Bartha, J. Lyle, J. West, M. Snyder, J. Sunwoo, Richard Chen, S. Boyle","Abstract 399: Pan-cancer survey of HLA loss of heterozygosity using a robustly validated NGS-based machine learning algorithm",2021,"","","","",39,"2022-07-13 09:39:06","","10.1158/1538-7445.AM2021-399","","",,,,,0,0.00,0,17,1,"HLA loss of heterozygosity (LOH) is increasingly being recognized as an important immune escape mechanism in response to checkpoint inhibitor therapy. HLA LOH reduces the repertoire of neoantigens displayed on the cell surface of cancer cells, limiting the efficacy of the immune system to detect and eliminate them. Though highly accurate HLA LOH detection algorithms are needed to allow clinical utility, the field lacks robust, allele-specific validation approaches. Moreover, algorithms of unknown sensitivity and specificity have led to significant discrepancies in the estimated occurrence of HLA LOH as an immune escape mechanism across tumor types. To address these challenges, we have developed a machine learning algorithm to detect HLA LOH (DASH - Deletion of Allele-Specific HLAs), established the accuracy of the algorithm with an allele-specific PCR validation strategy, investigated the frequencies of HLA LOH across 14 tumor types in a cohort of over 800 patients and observed allele-specific neoantigen expansion in response to immunotherapy. To build DASH, we profiled 279 patients on the ImmunoID NeXT Platform to create a training dataset. Our novel features, which account for allele-specific differences in exome probe capture and capitalize on our whole exome platform by including information about copy number alterations in the regions flanking the HLA genes, were used to train an XGBoost model. Orthogonal, allele-specific validation was required to accurately assess sensitivity and specificity for clinical utility. Thus, we profiled over 30 paired tumor-normal cell lines on the ImmunoID NeXT Platform® and identified cell lines with HLA LOH. Using in silico mixtures, we found 100% sensitivity and specificity for tumors with at least 36% tumor purity. Next, we designed a digital PCR (dPCR) assay using patient-specific, allele-specific primers that target a single HLA allele while avoiding all other HLA alleles and tested the limit of detection of the assay in the same cell lines. Then, we performed dPCR with patient-specific primers on 20 tumor and normal sample pairs and found 94% sensitivity. After establishing the high sensitivity and specificity of DASH, we profiled over 800 patients spanning 14 tumor types on the ImmunoID NeXT Platform. We found that over 25% of patients in the majority of tumor types had at least one HLA LOH event. Further, we observed that novel neoantigens that arose during checkpoint treatment were significantly more likely to bind to deleted HLA alleles as compared to the remaining HLA alleles in a head and neck carcinoma cohort treated with anti-PD-1 therapy, shedding light on the mechanism of immune escape in response to checkpoint inhibitors. In summary, we introduced an HLA LOH detection method, performed allele-specific validation, exposed widespread HLA across tumor types and observed the mechanism of immune escape in response to immunotherapy. Citation Format: Rachel Marty Pyke, Datta Mellacheruvu, Charles Abbott, Steven Dea, Eric Levy, Simo V. Zhang, Nikita Bedi, A. Dimitrios Colevas, Devayani Bhave, Manju Chinnappa, Gabor Bartha, John Lyle, John West, Michael Snyder, John Sunwoo, Richard Chen, Sean Michael Boyle. Pan-cancer survey of HLA loss of heterozygosity using a robustly validated NGS-based machine learning algorithm [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2021; 2021 Apr 10-15 and May 17-21. Philadelphia (PA): AACR; Cancer Res 2021;81(13_Suppl):Abstract nr 399.","",""
0,"R. Mccarthy, Ananya Sen Gupta","Employing and Interpreting a Machine Learning Target-Cognizant Technique for Analysis of Unknown Signals in Multiple Reaction Monitoring",2021,"","","","",40,"2022-07-13 09:39:06","","10.1109/ACCESS.2021.3056955","","",,,,,0,0.00,0,2,1,"The aim of this interdisciplinary work is a robust signal processing and autonomous machine learning framework to associate well-known (target) as well as any potentially unknown (non-target) peaks present within gas chromatography-mass spectrometry (GC/MS/MS) raw instrument signal. Particularly, this work evaluates three machine learning algorithms abilities to autonomously associate raw signal peaks based on accuracy in training and testing. A target is a known congener that is expected to be present within the raw instrument signal and a non-target is an unknown or unexpected compound. Autonomously identifying target peaks within the GC/MS/MS and associating them with non-target peaks can help improve the analysis of collected samples. Association of peaks refers to classifying peaks as known congeners regardless if the peak is a target or non-target. Uncertainty of peaks fitted and discovered through raw instrument signals from GC/MS/MS data is assessed to create topographical illustrations of target annotated peaks among sample raw instrument signals collected across diverse locations in the Chicago area. The term “annotated peak” is used to assign peaks found at specific retention times as a known congener. Adaptive signal processing techniques are utilized to smooth data and correct baseline drifts as well as detect and separate coeluted (overlapped) peaks in the raw instrument signal to provide key feature extraction. 150 air samples are analyzed for individual polychlorinated biphenyls (PCB) with GC/MS/MS across Chicago, IL. 80% of the data is used for training classification of target PCBs and 20% of the data is evaluated to identify and associate consistently occurring non-target peaks with target PCBs. A random forest classifier is used to associate identified peaks to target PCB peaks. Geographical topographical representations of target PCBs in the raw instrument signal demonstrates how PCBs accumulate and degrade in certain locations.","",""
1,"Gi-Ung Kang, J. Ibal, Seungjun Lee, M. Jang, Yeong-Jun Park, Min-Chul Kim, Tae-Hyung Park, Min-Sueng Kim, Ryeong-Hui Kim, Jae-Ho Shin","Alteration of the Soil Microbiota in Ginseng Rusty Roots: Application of Machine Learning Algorithm to Explore Potential Biomarkers for Diagnostic and Predictive Analytics.",2021,"","","","",41,"2022-07-13 09:39:06","","10.1021/acs.jafc.1c01314","","",,,,,1,1.00,0,10,1,"Conceptualization to utilize microbial composition as a prediction tool has been widely applied in human cohorts, yet the potential capacity of soil microbiota as a diagnostic tool to predict plant phenotype remains unknown. Here, we collected 130 soil samples which are 54 healthy controls and 76 ginseng rusty roots (GRRs). Alpha diversities including Shannon, Simpson, Chao1, and phylogenetic diversity were significantly decreased in GRR (P < 0.05). Moreover, we identified 30 potential biomarkers. The optimized markers were obtained through fivefold cross-validation on a support vector machine and yielded a robust area under the curve of 0.856. Notably, evaluation of multi-index classification performance including accuracy, F1-score, and Kappa coefficient also showed robust discriminative capability (90.99%, 0.903, and 0.808). Taken together, our results suggest that the disease affects the microbial community and offers the potential ability of soil microbiota to identifying farms at the risk of GRR.","",""
0,"M. Panda, A. Azar","Hybrid Multi-Objective Grey Wolf Search Optimizer and Machine Learning Approach for Software Bug Prediction",2021,"","","","",42,"2022-07-13 09:39:06","","10.4018/978-1-7998-5788-4.CH013","","",,,,,0,0.00,0,2,1,"Software bugs (or malfunctions) pose a serious threat to software developers with many known and unknown bugs that may be vulnerable to computer systems, demanding new methods, analysis, and techniques for efficient bug detection and repair of new unseen programs at a later stage. This chapter uses evolutionary grey wolf (GW) search optimization as a feature selection technique to improve classifier efficiency. It is also envisaged that software error detection would consider the nature of the error when repairing it for remedial action instead of simply finding it either faulty or non-defective. To address this problem, the authors use bug severity multi-class classification to build an efficient and robust prediction model using multilayer perceptron (MLP), logistic regression (LR), and random forest (RF) for bug severity classification. Both tests are performed on two software error datasets, namely Ant 1.7 and Tomcat.","",""
0,"Chuang Zhou, Yongle Lv, Jie Wu","Probabilistic Weighted Extreme Learning Machine for Robust Modeling",2019,"","","","",43,"2022-07-13 09:39:06","","10.1109/ICEICT.2019.8846409","","",,,,,0,0.00,0,3,3,"Radar performance prediction is becoming more and more important in Product-Life-Cycle-Management of radar. However, modeling the performance is difficult because of the strong nonlinearity from radar’s complex system as well as immeasurable effects like electromagnetic interference, instrument accuracy and so on. This paper proposes a probability weighted extreme learning machine to model the performance of radar under noise. First, a distributed extreme learning machine modeling is developed, upon which the probability distribution function (PDF) of multiple local models is estimated by the Parzen window method. This distribution function is further used as weights to integrate all local models to construct a global robust ELM model. The successful application of this robust probabilistic weighted ELM method to both artificial case and real life case demonstrates its great advantages in the modeling of an unknown system with various kinds of random noise.","",""
15,"Lin Li, X. Guo, N. Ansari","SmartLoc: Smart Wireless Indoor Localization Empowered by Machine Learning",2020,"","","","",44,"2022-07-13 09:39:06","","10.1109/TIE.2019.2931261","","",,,,,15,7.50,5,3,2,"Recently, machine learning (ML) has been widely adopted for fingerprint-based indoor localization because of its potency in delineating relationships between received signal strength (RSS) information and labels accurately. Existing ML-based indoor localization systems are less robust because they only adopt the output with the highest probability. This affects the final location estimate, hence compromising accuracy due to the severity of RSS fluctuations. Since different ML algorithms (MLAs) yield different performances, it is therefore intuitive to fuse predictions from multiple MLAs to improve the positioning performance in the presence of signal fluctuation. In this article, we propose SmartLoc, a smart wireless indoor localization framework to enhance indoor localization. In the offline phase, multiple MLAs are trained by utilizing an offline database. We further apply probability alignment to guarantee the predicted probabilities of each MLA at the same confidence level. In the online phase, given a testing RSS sample of a user at an unknown location, we extract the labels with probabilities greater than a certain threshold from each MLA to construct the space of candidate labels (SCL). The size of SCL can be adaptively determined by using our proposed dynamic size determination algorithm. Based on the SCL, we propose a probabilistic model to intelligently estimate the user's location by evaluating the label credibility simultaneously. A high label credibility indicates that the frequently occurred label is more likely to be true. Experimental results in a real changing environment verify the superiority of SmartLoc, outperforming the best among comparative methods by 10.8% in 75th percentile accuracy.","",""
6,"Marcos Fabietti, M. Mahmud, Ahmad Lotfi","Machine Learning in Analysing Invasively Recorded Neuronal Signals: Available Open Access Data Sources",2020,"","","","",45,"2022-07-13 09:39:06","","10.1007/978-3-030-59277-6_14","","",,,,,6,3.00,2,3,2,"","",""
2,"Shahnaz TayebiHaghighi, Insoo Koo","Fault Diagnosis of Rotating Machine Using an Indirect Observer and Machine Learning",2020,"","","","",46,"2022-07-13 09:39:06","","10.1109/ICTC49870.2020.9289590","","",,,,,2,1.00,1,2,2,"Bearing is one of the important mechanical components to reduce friction in rotating machines. Early fault diagnosis in bearings is an important challenge to the prevention of full failure and avoiding disorder of the machine. In this paper, an indirect observer and machine learning technique are adopted for fault identification in bearing. To develop an indirect observer, in the first step, the autoregressive with uncertainty modeling technique is proposed to modeling the RMS (indirect) normal signal of bearing. After that, the robust (sliding fault detection) proportional multi integral with autoregressive external input modeling (ARPMI) observer was used to solve the unknown signal estimation in bearing. Besides, the support vector machine (SVM) technique for fault classification is proposed. The effectiveness of the proposed scheme is validated using Case Western Reverse University (CWRU) dataset. Experimental results show that, the proposed scheme improves the average performance for various rotational speed fault identification by about 10.5% and 13.5% compared with the proportional multi integral with autoregressive external input modeling (APMI) observer and proportional-integral with autoregressive external input modeling (API) observer, respectively.","",""
1,"N. Thompson, J. Steck, E. Behrman","A non-algorithmic approach to “programming” quantum computers via machine learning",2020,"","","","",47,"2022-07-13 09:39:06","","10.1109/QCE49297.2020.00019","","",,,,,1,0.50,0,3,2,"Major obstacles remain to the implementation of macroscopic quantum computing: hardware problems of noise, decoherence, and scaling; software problems of error correction; and, most important, algorithm construction. Finding truly quantum algorithms is quite difficult, and many of these genuine quantum algorithms, like Shor's prime factoring or phase estimation, require extremely long circuit depth for any practical application, which necessitates error correction. In contrast, we show that machine learning can be used as a systematic method to construct algorithms, that is, to non-algorithmically “program” quantum computers. Quantum machine learning enables us to perform computations without breaking down an algorithm into its gate “building blocks”, eliminating that difficult step and potentially increasing efficiency by simplifying and reducing unnecessary complexity. In addition, our non-algorithmic machine learning approach is robust to both noise and to decoherence, which is ideal for running on inherently noisy NISQ devices which are limited in the number of qubits available for error correction. We demonstrate this using a fundamentally nonclassical calculation: experimentally estimating the entanglement of an unknown quantum state. Results from this have been successfully ported to the IBM hardware and trained using a hybrid reinforcement learning method.","",""
1,"Sicong Ma, Pei-Lin Kang, C. Shang, Zhipan Liu","Chapter 19. Machine Learning for Heterogeneous Catalysis: Global Neural Network Potential from Construction to Applications",2020,"","","","",48,"2022-07-13 09:39:06","","10.1039/9781839160233-00488","","",,,,,1,0.50,0,4,2,"While the potential energy surface (PES) determines the physicochemical properties of matter, chemical system surfaces are often too complex to solve even with modern computing facilities. Heterogeneous catalysis, being widely utilized in industry, calls for new techniques and methods to resolve the active site structure and reaction intermediates at the atomic scale. In this chapter, we provide an overview of recent theoretical progress on large-scale atomistic simulation via the machine learning global neural network (G-NN) potential developed by our research group in recent years, focusing on methodology and representative applications in heterogeneous catalysis. The combination of global optimization and machine learning provides a convenient and automated way to generate the transferable and robust G-NN potential, which can be utilized to reveal new chemistry from unknown regions of the PES at an affordable computational cost. The predictive power of the G-NN potential is demonstrated in several examples, where the method is applied to explore the material crystal phases and the structure of supported catalysts, to follow surface structure evolution under high-pressure hydrogen and to determine the ternary oxide phase diagram. Limitations and future directions of the G-NN potential method are also discussed.","",""
0,"Kriti Srivastava, N. Shekokar","Design of Machine Learning and Rule Based Access Control System with Respect to Adaptability and Genuineness of the Requester",2020,"","","","",49,"2022-07-13 09:39:06","","10.4108/EAI.24-9-2020.166359","","",,,,,0,0.00,0,2,2,"INTRODUCTION: Access control system (ACS) plays a major role in data security. It becomes more challenging for the system to provide accurate ACS, if data is huge and data requesters are not fixed. This is very predominant in the era of big data where new data are adding to the system very frequently. The main issue here is to justify adaptability in ACS.    OBJECTIVE: The objective of this research is to have a comparative analysis of machine learning based access control methods with Rule based access control methods. Propose the most suitable method in detail.    METHODS: Role based access control methods are highly robust and works effectively under known scenarios. We need additional methods to handle unknown scenarios. A decision-making method is used to identify the certainty of the rules and Mamdani fuzzy model is used to evaluate the situation based on current environmental factors. For machine learning based access control method Random Forest is used.    RESULTS: Limitations of machine learning methods are discussed with respect to imbalanced data and bias in the algorithm. The proof of concept for rule-based access control method is tested for all the three modules involved in the framework. Certainty of the rules were accessed with the help of domain experts and accuracy of fuzzy rules were evaluated. Under critical conditions our framework was found to be accurate.    CONCLUSIONS: Machine learning systems are not suitable for access control if they suffer with imbalance data problem. Rule based system are consistent and highly adaptable to unknown situations. Rule based systems have evaluated the genuineness of the requester based on sensitivity of information, time, location, previous history and emergency parameters.","",""
0,"A. Chakrabarty, K. Berntorp, S. D. Cairano","Learning-based Parameter-Adaptive Reference Governors /Author=Chakrabarty, Ankush; Berntorp, Karl; Di Cairano, Stefano /CreationDate=July 3, 2020 /Subject=Control, Machine Learning",2020,"","","","",50,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,3,2,"Reference governors (RGs) provide an effective method for ensuring safety via constraint enforcement in closedloop control systems. When the parameters of the underlying systems are unknown, but constant or slowly-varying, robust formulations of RGs that consider only the worst-case effect may be overly conservative and exhibit poor performance. This paper proposes a parameter-adaptive reference governor (PARG) architecture that is capable of generating safe trajectories in spite of parameter uncertainties without being as conservative as robust RGs. The proposed approach leverages on-line data to inform algorithms for robust parameter estimation. Subsequently, confidence bounds around parameter estimates are fed to supervised machine learners for approximating robust constraint admissible sets leveraged by the PARG. While initially, due to the absence of on-line data, the PARG may be as conservative as a robust RG, as more data is gathered and the confidence bounds become tighter, such conservativeness reduces, as demonstrated in a simulation example. American Control Conference (ACC) This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Copyright c © Mitsubishi Electric Research Laboratories, Inc., 2020 201 Broadway, Cambridge, Massachusetts 02139 Learning-based Parameter-Adaptive Reference Governors Ankush Chakrabarty†, Karl Berntorp, Stefano Di Cairano Abstract—Reference governors (RGs) provide an effective method for ensuring safety via constraint enforcement in closedloop control systems. When the parameters of the underlying systems are unknown, but constant or slowly-varying, robust formulations of RGs that consider only the worst-case effect may be overly conservative and exhibit poor performance. This paper proposes a parameter-adaptive reference governor (PARG) architecture that is capable of generating safe trajectories in spite of parameter uncertainties without being as conservative as robust RGs. The proposed approach leverages on-line data to inform algorithms for robust parameter estimation. Subsequently, confidence bounds around parameter estimates are fed to supervised machine learners for approximating robust constraint admissible sets leveraged by the PARG. While initially, due to the absence of on-line data, the PARG may be as conservative as a robust RG, as more data is gathered and the confidence bounds become tighter, such conservativeness reduces, as demonstrated in a simulation example.","",""
6,"P. Nicholas, Gabriella Rossi, Ella Williams, Michael Bennett, T. Schork","Integrating real-time multi-resolution scanning and machine learning for Conformal Robotic 3D Printing in Architecture",2020,"","","","",51,"2022-07-13 09:39:06","","10.1177/1478077120948203","","",,,,,6,3.00,1,5,2,"Robotic 3D printing applications are rapidly growing in architecture, where they enable the introduction of new materials and bespoke geometries. However, current approaches remain limited to printing on top of a flat build bed. This limits robotic 3D printing’s impact as a sustainable technology: opportunities to customize or enhance existing elements, or to utilize complex material behaviour are missed. This paper addresses the potentials of conformal 3D printing and presents a novel and robust workflow for printing onto unknown and arbitrarily shaped 3D substrates. The workflow combines dual-resolution Robotic Scanning, Neural Network prediction and printing of PETG plastic. This integrated approach offers the advantage of responding directly to unknown geometries through automated performance design customization. This paper firstly contextualizes the work within the current state of the art of conformal printing. We then describe our methodology and the design experiment we have used to test it. We lastly describe the key findings, potentials and limitations of the work, as well as the next steps in this research.","",""
2,"Jacques Janse Van Vuuren, Liqiong Tang, I. Al-Bahadly, K. Arif","A 3-Stage Machine Learning-Based Novel Object Grasping Methodology",2020,"","","","",52,"2022-07-13 09:39:06","","10.1109/ACCESS.2020.2987341","","",,,,,2,1.00,1,4,2,"The automatic grasping of objects previously unseen by a robotic system is a difficult task—of which there is currently no robust solution. The research presented in this article improves upon previous works that employ depth data and learning techniques to generate and select from a pool of hypothesised grasps by focusing on the pruning and selection process. In this work, a vision-based, sampling methodology that generates candidate grasps through a convolutional neural network is proposed. Each candidate grasp is assessed using scores derived from the candidate itself and other related input modalities—such as the centre of gravity of the object. The final selection is determined by a learning algorithm. To overcome human bias, objective measures of grasp performance are established that comprehensively measure the error introduced by the grasp trial itself. The proposed metrics are empirically demonstrated to quantify grasp quality, offer useful criteria for network training and provide better descriptive power than traditional measures of grasp outcome. Experimentation showed that the proposed methodology can generate a meaningful, final grasp within 1.3 seconds. Trials quantitatively demonstrate a small-object-in-isolation performance of 99%. For unknown objects, this equates to a 10% improvement relative to other similar methodologies. Testing also showed that grasp performance was improved by 5% when implementing the proposed metrics—compared to the baseline.","",""
1920,"A. Kurakin, Ian J. Goodfellow, Samy Bengio","Adversarial Machine Learning at Scale",2016,"","","","",53,"2022-07-13 09:39:06","","","","",,,,,1920,320.00,640,3,6,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.","",""
2,"S. Siltanen, Takanori Ide","Electrical Impedance Tomography, Enclosure Method and Machine Learning",2020,"","","","",54,"2022-07-13 09:39:06","","10.1109/MLSP49062.2020.9231717","","",,,,,2,1.00,1,2,2,"Electrical impedance tomography (EIT) is a non-destructive imaging method, where a physical body is probed with electric measurements at the boundary, and information about the internal conductivity is extracted from the data. The enclosure method of Ikehata [J. Inv. III-Posed Prob. 8(2000)] recovers the convex hull of an inclusion of unknown conductivity embedded in known background conductivity. Practical implementations of the enclosure method are based on least-squares (LS) fitting of lines to noise-robust values of the so-called indicator function. It is shown how a convolutional neural network instead of LS fitting improves the accuracy of the enclosure method significantly while retaining interpretability.","",""
0,"C. Calad, Fernando Gutiérrez, Paola Pastor, P. Sarma","Combining Machine Learning with Traditional Reservoir Physics for Predictive Modeling and Optimization of a Large Mature Waterflood Project in the Gulf of San Jorge Basin in Argentina",2020,"","","","",55,"2022-07-13 09:39:06","","10.2118/199048-ms","","",,,,,0,0.00,0,4,2,"  A novel technology that combines the benefits of speed of data sciences with the predictivity capabilities of traditional simulation is being applied to model two blocks of a large waterflood project in the Gulf of San Jorge basin in southern Argentina. The tool is being used to provide a prescription of injection water redistribution that optimizes production and reserves development and reduces injection cost.  The technology used is called DataPhysics* and combines the robustness of reservoir physics with the speed of data sciences techniques. The process solves a limited number of unknowns in a continuous scale making it several orders of magnitude faster than traditional numeric simulation. The reservoir model is created from raw (uninterpreted) data and is updated continuously allowing for close loop reservoir optimization in real time. Long term predictivity is enabled by the fact that the tool honors the reservoir physics.  At the time of writing this paper the recommendations of the predictive model have been implemented in the pilot sector of the field and early positive results have been observed.","",""
26,"Theja Tulabandhula, C. Rudin","Robust Optimization using Machine Learning for Uncertainty Sets",2014,"","","","",56,"2022-07-13 09:39:06","","","","",,,,,26,3.25,13,2,8,"Our goal is to build robust optimization problems for making decisions based on complex data from the past. In robust optimization (RO) generally, the goal is to create a policy for decision-making that is robust to our uncertainty about the future. In particular, we want our policy to best handle the the worst possible situation that could arise, out of an uncertainty set of possible situations. Classically, the uncertainty set is simply chosen by the user, or it might be estimated in overly simplistic ways with strong assumptions; whereas in this work, we learn the uncertainty set from data collected in the past. The past data are drawn randomly from an (unknown) possibly complicated high-dimensional distribution. We propose a new uncertainty set design and show how tools from statistical learning theory can be employed to provide probabilistic guarantees on the robustness of the policy.","",""
1,"M. Jessell, Jiateng Guo, Yunqiang Li, M. Lindsay, R. Scalzo, J. Giraud, G. Pirot, E. Cripps, V. Ogarko","Into the Noddyverse: a massive data store of 3D geological models for machine learning and inversion applications",2022,"","","","",57,"2022-07-13 09:39:06","","10.5194/essd-14-381-2022","","",,,,,1,1.00,0,9,1,"Abstract. Unlike some other well-known challenges such as facial recognition, where machine learning and inversion algorithms are widely developed, the geosciences suffer from a lack of large, labelled data sets that can be used to validate or train robust machine learning and inversion schemes. Publicly available 3D geological models are far too restricted in both number and the range of geological scenarios to serve these purposes. With reference to inverting geophysical data this problem is further exacerbated as in most cases real geophysical observations result from unknown 3D geology, and synthetic test data sets are often not particularly geological or geologically diverse. To overcome these limitations, we have used the Noddy modelling platform to generate 1 million models, which represent the first publicly accessible massive training set for 3D geology and resulting gravity and magnetic data sets (https://doi.org/10.5281/zenodo.4589883, Jessell, 2021). This model suite can be used to train machine learning systems and to provide comprehensive test suites for geophysical inversion. We describe the methodology for producing the model suite and discuss the opportunities such a model suite affords, as well as its limitations, and how we can grow and access this resource. ","",""
6,"Rudrasis Chakraborty, Liu Yang, Søren Hauberg, B. Vemuri","Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear Subspace Learning",2017,"","","","",58,"2022-07-13 09:39:06","","10.1109/tpami.2020.2992392","","",,,,,6,1.20,2,4,5,"Principal component analysis (PCA) and Kernel principal component analysis (KPCA) are fundamental methods in machine learning for dimensionality reduction. The former is a technique for finding this approximation in finite dimensions and the latter is often in an infinite dimensional reproducing Kernel Hilbert-space (RKHS). In this paper, we present a geometric framework for computing the principal linear subspaces in both (finite and infinite) situations as well as for the robust PCA case, that amounts to computing the intrinsic average on the space of all subspaces: the Grassmann manifold. Points on this manifold are defined as the subspaces spanned by <inline-formula><tex-math notation=""LaTeX"">$K$</tex-math><alternatives><mml:math><mml:mi>K</mml:mi></mml:math><inline-graphic xlink:href=""chakraborty-ieq1-2992392.gif""/></alternatives></inline-formula>-tuples of observations. The intrinsic Grassmann average of these subspaces are shown to coincide with the principal components of the observations when they are drawn from a Gaussian distribution. We show similar results in the RKHS case and provide an efficient algorithm for computing the projection onto the this average subspace. The result is a method akin to KPCA which is substantially faster. Further, we present a novel online version of the KPCA using our geometric framework. Competitive performance of all our algorithms are demonstrated on a variety of real and synthetic data sets.","",""
16,"A. Moawad, A. Silge, T. Bocklitz, K. Fischer, P. Rösch, U. Roesler, M. Elschner, J. Popp, H. Neubauer","A Machine Learning-Based Raman Spectroscopic Assay for the Identification of Burkholderia mallei and Related Species",2019,"","","","",59,"2022-07-13 09:39:06","","10.3390/molecules24244516","","",,,,,16,5.33,2,9,3,"Burkholderia (B.) mallei, the causative agent of glanders, and B. pseudomallei, the causative agent of melioidosis in humans and animals, are genetically closely related. The high infectious potential of both organisms, their serological cross-reactivity, and similar clinical symptoms in human and animals make the differentiation from each other and other Burkholderia species challenging. The increased resistance against many antibiotics implies the need for fast and robust identification methods. The use of Raman microspectroscopy in microbial diagnostic has the potential for rapid and reliable identification. Single bacterial cells are directly probed and a broad range of phenotypic information is recorded, which is subsequently analyzed by machine learning methods. Burkholderia were handled under biosafety level 1 (BSL 1) conditions after heat inactivation. The clusters of the spectral phenotypes and the diagnostic relevance of the Burkholderia spp. were considered for an advanced hierarchical machine learning approach. The strain panel for training involved 12 B. mallei, 13 B. pseudomallei and 11 other Burkholderia spp. type strains. The combination of top- and sub-level classifier identified the mallei-complex with high sensitivities (>95%). The reliable identification of unknown B. mallei and B. pseudomallei strains highlighted the robustness of the machine learning-based Raman spectroscopic assay.","",""
2,"Jiyuan Tu, Weidong Liu, Xiaojun Mao","Byzantine-robust distributed sparse learning for M-estimation",2021,"","","","",60,"2022-07-13 09:39:06","","10.1007/S10994-021-06001-X","","",,,,,2,2.00,1,3,1,"","",""
2,"Alex Bauerle, Ángel Alexander Cabrera, Fred Hohman, Megan Maher, David Koski, Xavier Suau, Titus Barik, Dominik Moritz","Symphony: Composing Interactive Interfaces for Machine Learning",2022,"","","","",61,"2022-07-13 09:39:06","","10.1145/3491102.3502102","","",,,,,2,2.00,0,8,1,"Interfaces for machine learning (ML), information and visualizations about models or data, can help practitioners build robust and responsible ML systems. Despite their benefits, recent studies of ML teams and our interviews with practitioners (n=9) showed that ML interfaces have limited adoption in practice. While existing ML interfaces are effective for specific tasks, they are not designed to be reused, explored, and shared by multiple stakeholders in cross-functional teams. To enable analysis and communication between different ML practitioners, we designed and implemented Symphony, a framework for composing interactive ML interfaces with task-specific, data-driven components that can be used across platforms such as computational notebooks and web dashboards. We developed Symphony through participatory design sessions with 10 teams (n=31), and discuss our findings from deploying Symphony to 3 production ML projects at Apple. Symphony helped ML practitioners discover previously unknown issues like data duplicates and blind spots in models while enabling them to share insights with other stakeholders.","",""
1,"L. Merte, M. K. Bisbo, I. Sokolović, M. Setvín, Benjamin Hagman, M. Shipilin, M. Schmid, U. Diebold, E. Lundgren, B. Hammer","Structure of an Ultrathin Oxide on Pt3Sn(111) Solved by Machine Learning Enhanced Global Optimization.",2022,"","","","",62,"2022-07-13 09:39:06","","10.1002/anie.202204244","","",,,,,1,1.00,0,10,1,"Determination of the atomic structure of solid surfaces typically depends on comparison of measured properties with simulations based on hypothesized structural models. For simple structures, the models may be guessed, but for more complex structures there is a need for reliable theory-based search algorithms. So far, such methods have been limited by the combinatorial complexity and computational expense of sufficiently accurate energy estimation for surfaces. However, the introduction of machine learning methods has the potential to change this radically. Here, we demonstrate how an evolutionary algorithm, utilizing machine learning for accelerated energy estimation and diverse population generation, can be used to solve an unknown surface structure-the (4×4) surface oxide on Pt3Sn(111)-based on limited experimental input. The algorithm is efficient and robust, and should be broadly applicable in surface studies, where it can replace manual, intuition based model generation.","",""
0,"J. Steck, N. Thompson, E. Behrman","Programming Quantum Hardware via Levenberg Marquardt Machine Learning",2022,"","","","",63,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,3,1,"Significant challenges remain with the development of macroscopic quantum computing: hardware problems of noise, decoherence, and scaling; software problems of error correction; and, most important, algorithm construction. Finding truly quantum algorithms is quite difficult, and many quantum algorithms, like Shor’s prime factoring or phase estimation, require extremely long circuit depth for any practical application, necessitating error correction. Machine learning can be used as a systematic method to non-algorithmically “program” quantum computers. Quantum machine learning enables us to perform computations without breaking down an algorithm into its gate “building blocks”, eliminating that difficult step and potentially reducing unnecessary complexity. In addition, we have shown that our machine learning approach is robust to both noise and to decoherence, which is ideal for running on inherently noisy NISQ devices which are limited in the number of qubits available for error correction. We demonstrated this using a fundamentally non-classical calculation: experimentally estimating the entanglement of an unknown quantum state. Results from this have been successfully ported to the IBM hardware and trained using a powerful hybrid reinforcement learning technique which is a modified Levenberg-Marquardt (LM) method. The LM method is ideally suited to quantum machine learning as it only requires knowledge of the final measured output of the quantum computation, not intermediate quantum states which are generally not accessible. Since it processes all the learning data simultaneously, it also requires significantly fewer hits on the quantum hardware. Machine learning is demonstrated with results from simulations and runs on the IBM Qiskit hardware","",""
0,"S. Mordensky, J. Lipor, J. DeAngelo, E. Burns, Cary R. Lindsey","Predicting Geothermal Favorability in the Western United States by Using Machine Learning: Addressing Challenges and Developing Solutions",2022,"","","","",64,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,5,1,"Previous moderate- and high-temperature geothermal resource assessments of the western United States utilized weight-of-evidence and logistic regression methods to estimate resource favorability, but these analyses relied upon some expert decisions. While expert decisions can add confidence to aspects of the modeling process by ensuring only reasonable models are employed, expert decisions also introduce human bias into assessments. This bias presents a source of error that may affect the performance of the models and resulting resource estimates. Our study aims to reduce expert input through robust data-driven analyses and better-suited data science techniques, with the goals of saving time, reducing bias, and improving predictive ability. We present six favorability maps for geothermal resources in the western United States created using two strategies applied to three modern machine learning algorithms (logistic regression, support-vector machines, and XGBoost). To provide a direct comparison to previous assessments, we use the same input data as the 2008 U.S. Geological Survey (USGS) conventional moderate- to high-temperature geothermal resource assessment. The six new favorability maps required far less expert decision-making, but broadly agree with the previous assessment. Despite the fact that the 2008 assessment results employed linear methods, the non-linear machine learning algorithms ( i.e., support-vector machines and XGBoost) produced greater agreement with the previous assessment than the linear machine learning algorithm ( i.e., logistic regression). It is not surprising that geothermal systems depend on non-linear combinations of features, and we postulate that the expert decisions during the 2008 assessment accounted for system non-linearities. Substantial challenges to applying machine learning algorithms to predict geothermal resource favorability include severe class imbalance ( i.e., there are very few known geothermal systems compared to the large area considered), and while there are known geothermal systems ( i.e., positive labels), all other sites have an unknown status ( i.e., they are unlabeled), instead of receiving a negative label ( i.e., the known/proven absence of a geothermal resource). We address both challenges through a custom undersampling strategy that can be used with any algorithm and then evaluated using F1 scores. for XGBoost: class weight, learning rate, number of estimators, and maximum depth of estimators. Class weight in XGBoost differs in exact implementation compared with logistic regression and SVMs, but this hyperparameter serves much the same purpose: a greater class weight places greater emphasis on accurately predicting positive labels ( i.e., known geothermal systems) than non-positive labels ( i.e., unknown resource potential). The other parameters are used to maximize prediction performance while also avoiding overfitting (Chen and Guestrin, 2016). We leave the other parameters of XGBoost at the default values found in Python’s XGBoost module as they pertain to the specifics of the optimization routine and have only a modest impact on performance (Chen and Guestrin, 2016).","",""
0,"Junzhong Xie, Xu-Yuan Zhou, Dong Luan, Hong Jiang","Machine Learning Force Field Aided Cluster Expansion Approach to Configurationally Disordered Materials: Critical Assessment of Training Set Selection and Size Convergence.",2022,"","","","",65,"2022-07-13 09:39:06","","10.1021/acs.jctc.2c00017","","",,,,,0,0.00,0,4,1,"Cluster expansion (CE) is a powerful theoretical tool to study the configuration-dependent properties of substitutionally disordered systems. Typically, a CE model is built by fitting a few tens or hundreds of target quantities calculated by first-principles approaches. To validate the reliability of the model, a convergence test of the cross-validation (CV) score to the training set size is commonly conducted to verify the sufficiency of the training data. However, such a test only confirms the convergence of the predictive capability of the CE model within the training set, and it is unknown whether the convergence of the CV score would lead to robust thermodynamic simulation results such as order-disorder phase transition temperature Tc. In this work, using carbon defective MoC1-x as a model system and aided by the machine-learning force field technique, a training data pool with about 13000 configurations has been efficiently obtained and used to generate different training sets of the same size randomly. By conducting parallel Monte Carlo simulations with the CE models trained with different randomly selected training sets, the uncertainty in calculated Tc can be evaluated at different training set sizes. It is found that the training set size that is sufficient for the CV score to converge still leads to a significant uncertainty in the predicted Tc and that the latter can be considerably reduced by enlarging the training set to that of a few thousand configurations. This work highlights the importance of using a large training set to build the optimal CE model that can achieve robust statistical modeling results and the facility provided by the machine-learning force field approach to efficiently produce adequate training data.","",""
0,"Z. Liu, G. Shurin, L. Bian, David L. White, M. Shurin, A. Star","A Carbon Nanotube Sensor Array for the Label-Free Discrimination of Live and Dead Cells with Machine Learning.",2022,"","","","",66,"2022-07-13 09:39:06","","10.1021/acs.analchem.1c04661","","",,,,,0,0.00,0,6,1,"Developing robust cell recognition strategies is important in biochemical research, but the lack of well-defined target molecules creates a bottleneck in some applications. In this paper, a carbon nanotube sensor array was constructed for the label-free discrimination of live and dead mammalian cells. Three types of carbon nanotube field-effect transistors were fabricated, and different features were extracted from the transfer characteristic curves for model training with linear discriminant analysis (LDA) and support-vector machines (SVM). Live and dead cells were accurately classified in more than 90% of samples in each sensor group using LDA as the algorithm. The recursive feature elimination with cross-validation (RFECV) method was applied to handle the overfitting and optimize the model, and cells could be successfully classified with as few as four features and a higher validation accuracy (up to 97.9%) after model optimization. The RFECV method also revealed the crucial features in the classification, indicating the participation of different sensing mechanisms in the classification. Finally, the optimized LDA model was applied for the prediction of unknown samples with an accuracy of 87.5-93.8%, indicating that live and dead cell samples could be well-recognized with the constructed model.","",""
7,"Lin Liu, R. Mukherjee, J. Robins","On Nearly Assumption-Free Tests of Nominal Confidence Interval Coverage for Causal Parameters Estimated by Machine Learning",2019,"","","","",67,"2022-07-13 09:39:06","","10.1214/20-sts786","","",,,,,7,2.33,2,3,3,"For many causal effect parameters of interest, doubly robust machine learning (DRML) estimators ψ^1 are the state-of-the-art, incorporating the good prediction performance of machine learning; the decreased bias of doubly robust estimators; and the analytic tractability and bias reduction of sample splitting with cross-fitting. Nonetheless, even in the absence of confounding by unmeasured factors, the nominal (1−α) Wald confidence interval ψ^1±zα/2s.e.ˆ[ψ^1] may still undercover even in large samples, because the bias of ψ^1 may be of the same or even larger order than its standard error of order n−1/2.  In this paper, we introduce essentially assumption-free tests that (i) can falsify the null hypothesis that the bias of ψ^1  is of smaller order than its standard error, (ii) can provide a upper confidence bound on the true coverage of the Wald interval, and (iii) are valid under the null under no smoothness/sparsity assumptions on the nuisance parameters. The tests, which we refer to as Assumption Free Empirical Coverage Tests (AFECTs), are based on a U-statistic that estimates part of the bias of ψ^1.  Our claims need to be tempered in several important ways. First no test, including ours, of the null hypothesis that the ratio of the bias to its standard error is smaller than some threshold δ  can be consistent [without additional assumptions (e.g., smoothness or sparsity) that may be incorrect]. Second, the above claims only apply to certain parameters in a particular class. For most of the others, our results are unavoidably less sharp. In particular, for these parameters, we cannot directly test whether the nominal Wald interval ψ^1±zα/2s.e.ˆ[ψ^1] undercovers. However, we can often test the validity of the smoothness and/or sparsity assumptions used by an analyst to justify a claim that the reported Wald interval’s actual coverage is no less than nominal. Third, in the main text, with the exception of the simulation study in Section 1, we assume we are in the semisupervised data setting (wherein there is a much larger dataset with information only on the covariates), allowing us to regard the covariance matrix of the covariates as known. In the simulation in Section 1, we consider the setting in which estimation of the covariance matrix is required. In the simulation, we used a data adaptive estimator which performs very well in our simulations, but the estimator’s theoretical sampling behavior remains unknown.","",""
7,"Huanbo Sun, G. Martius","Machine Learning for Haptics: Inferring Multi-Contact Stimulation From Sparse Sensor Configuration",2019,"","","","",68,"2022-07-13 09:39:06","","10.3389/fnbot.2019.00051","","",,,,,7,2.33,4,2,3,"Robust haptic sensation systems are essential for obtaining dexterous robots. Currently, we have solutions for small surface areas, such as fingers, but affordable and robust techniques for covering large areas of an arbitrary 3D surface are still missing. Here, we introduce a general machine learning framework to infer multi-contact haptic forces on a 3D robot's limb surface from internal deformation measured by only a few physical sensors. The general idea of this framework is to predict first the whole surface deformation pattern from the sparsely placed sensors and then to infer number, locations, and force magnitudes of unknown contact points. We show how this can be done even if training data can only be obtained for single-contact points using transfer learning at the example of a modified limb of the Poppy robot. With only 10 strain-gauge sensors we obtain a high accuracy also for multiple-contact points. The method can be applied to arbitrarily shaped surfaces and physical sensor types, as long as training data can be obtained.","",""
0,"Yunzhe Tian, Yike Li, Yingxiao Xiang, Wenjia Niu, Endong Tong, Jiqiang Liu","Demo: Curricular Reinforcement Learning for Robust Policy in Unmanned CarRacing Game",2021,"","","","",69,"2022-07-13 09:39:06","","10.14722/autosec.2021.23011","","",,,,,0,0.00,0,6,1,"[1] Florensa, C., Held, D., Geng, X., and Abbeel, P. Automaticgoal generation for reinforcement learning agents. ICML, 2018. [2] Duan Y, Chen X, Houthooft R, et al. Benchmarking Deep Reinforcement Learning for Continuous Control. International Conference on Machine Learning (ICML). JMLR.org, 2016. [3] G. Brockman, V. Cheung, L. Pettersson, J. Schneider, J. Schulman, J. Tang, and W. Zaremba. Openai gym, 2016. [4] X. B. Peng, M. Andrychowicz, W. Zaremba and P. Abbeel. Sim-to-Real Transfer of Robotic Control with Dynamics Randomization. ICRA, 2018. Abstract—Robust reinforcement learning has been a challenging problem due to always unknown differences between real and training environment. Existing efforts approached the problem through performing random environmental perturbations in learning process. However, one can not guarantee perturbation is positive. Bad ones might bring failures to reinforcement learning. Therefore, in this paper, we propose to utilize GAN to dynamically generate progressive perturbations at each epoch and realize curricular policy learning. Demo we implemented in unmanned CarRacing game validates the effectiveness.","",""
2,"P. Perdikaris, G. Karniadakis","Machine Learning of Space-Fractional Differential Equations | SIAM Journal on Scientific Computing | Vol. 41, No. 4 | Society for Industrial and Applied Mathematics",2019,"","","","",70,"2022-07-13 09:39:06","","","","",,,,,2,0.67,1,2,3,"Data-driven discovery of “hidden physics”—i.e., machine learning of differential equation models underlying observed data—has recently been approached by embedding the discovery problem into a Gaussian process regression of spatial data, treating and discovering unknown equation parameters as hyperparameters of a “physics informed” Gaussian process kernel. This kernel includes the parametrized differential operators applied to a prior covariance kernel. We extend this framework to the data-driven discovery of linear space-fractional differential equations. The methodology is compatible with a wide variety of space-fractional operators in Rd and stationary covariance kernels, including the Matérn class, and allows for optimizing the Matérn parameter during training. Since fractional derivatives are typically not given by closed-form analytic expressions, the main challenges to be addressed are a user-friendly, general way to set up fractionalorder derivatives of covariance kernels, together with feasible and robust numerical methods for such implementations. Making use of the simple Fourier-space representation of space-fractional derivatives in Rd, we provide a unified set of integral formulas for the resulting Gaussian process kernels. The shift property of the Fourier transform results in formulas involving d-dimensional integrals that can be efficiently treated using generalized Gauss–Laguerre quadrature. The implementation of fractional derivatives has several benefits. First, the method allows for discovering models involving fractional-order PDEs for systems characterized by heavy tails or anomalous diffusion, while bypassing the analytical difficulty of fractional calculus. Data sets exhibiting such features are of increasing prevalence in physical and financial domains. Second, a single fractionalorder archetype allows for a derivative term of arbitrary order to be learned, with the order itself being a parameter in the regression. As a result, even when used for discovering integer-order equations, the proposed method has several benefits compared to previous works on data-driven discovery of differential equations; the user is not required to assume a “dictionary” of derivatives of various orders and directly controls the parsimony of the models being discovered. We illustrate our method on several examples, including fractional-order interpolation of advection-diffusion and modeling relative stock performance in the S&P 500 with α-stable motion via a fractional diffusion equation.","",""
0,"Jiashuo Liu, Zheyan Shen, Peng Cui, Linjun Zhou, Kun Kuang, B. Li","Distributionally Robust Learning with Stable Adversarial Training",2021,"","","","",71,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,6,1,"Machine learning algorithms with empirical risk minimization are vulnerable under distributional shifts due to the greedy adoption of all the correlations found in training data. There is an emerging literature on tackling this problem by minimizing the worst-case risk over an uncertainty set. However, existing methods mostly construct ambiguity sets by treating all variables equally regardless of the stability of their correlations with the target, resulting in the overwhelmingly-large uncertainty set and low confidence of the learner. In this paper, we propose a novel Stable Adversarial Learning (SAL) algorithm that leverages heterogeneous data sources to construct a more practical uncertainty set and conduct differentiated robustness optimization, where covariates are differentiated according to the stability of their correlations with the target. We theoretically show that our method is tractable for stochastic gradient-based optimization and provide the performance guarantees for our method. Empirical studies on both simulation and real datasets validate the effectiveness of our method in terms of uniformly good performance across unknown distributional shifts.","",""
9,"Milena Nacchia, F. Fruggiero, A. Lambiase, K. Bruton","A Systematic Mapping of the Advancing Use of Machine Learning Techniques for Predictive Maintenance in the Manufacturing Sector",2021,"","","","",72,"2022-07-13 09:39:06","","10.3390/APP11062546","","",,,,,9,9.00,2,4,1,"The increasing availability of data, gathered by sensors and intelligent machines, is changing the way decisions are made in the manufacturing sector. In particular, based on predictive approach and facilitated by the nowadays growing capabilities of hardware, cloud-based solutions, and new learning approaches, maintenance can be scheduled—over cell engagement and resource monitoring—when required, for minimizing (or managing) unexpected equipment failures, improving uptime through less aggressive maintenance schedules, shortening unplanned downtime, reducing excess (direct and indirect) cost, reducing long-term damage to machines and processes, and improve safety plans. With access to increased levels of data (and over learning mechanisms), companies have the capability to conduct statistical tests using machine learning algorithms, in order to uncover root causes of problems previously unknown. This study analyses the maturity level and contributions of machine learning methods for predictive maintenance. An upward trend in publications for predictive maintenance using machine learning techniques was identified with the USA and China leading. A mapping study—steady set until early 2019 data—was employed as a formal and well-structured method to synthesize material and to report on pervasive areas of research. Type of equipment, sensors, and data are mapped to properly assist new researchers in positioning new research activities in the domain of smart maintenance. Hence, in this paper, we focus on data-driven methods for predictive maintenance (PdM) with a comprehensive survey on applications and methods until, for the sake of commenting on stable proposal, 2019 (early included). An equal repartition between evaluation and validation studies was identified, this being a symptom of an immature but growing research area. In addition, the type of contribution is mainly in the form of models and methodologies. Vibrational signal was marked as the most used data set for diagnosis in manufacturing machinery monitoring; furthermore, supervised learning is reported as the most used predictive approach (ensemble learning is growing fast). Neural networks, followed by random forests and support vector machines, were identified as the most applied methods encompassing 40% of publications, of which 67% related to deep neural network with long short-term memory predominance. Notwithstanding, there is no robust approach (no one reported optimal performance over different case tests) that works best for every problem. We finally conclude the research in this area is moving fast to gather a separate focused analysis over the last two years (whenever stable implementations will appear).","",""
1,"Patrick F. Riley, Samir V. Deshpande","Machine learning based spectral interpretation in chemical detection",2019,"","","","",73,"2022-07-13 09:39:06","","10.1117/12.2518929","","",,,,,1,0.33,1,2,3,"Increasingly the design of chemical detection alarm algorithms to alert Soldiers of danger grows more complex as new threats emerge. These algorithms need to be robust enough to prevent false alarms to interferents and sensitive enough to alarm to the incredibly small doses that could prove lethal. The design of these algorithms have left a plethora of data that can be leveraged and utilized in a variety of machine learning (ML) techniques. ML is a field of computer science that uses a set of programming and statistical techniques to enable computers to “learn” from input data without being explicitly programmed. Presented is an application of ML to change two independent fielded chemical detectors into an orthogonal system to improve detection algorithms. The approach models the data from an ion-mobility spectrometer (IMS) and a photoionization detector containing electrochemical sensors (PIDECS) to train a ML model (MLA). The semi-supervised MLA is trained using a supervised learning data set, composed of partially labeled data from the heterogeneous instruments, and then fine-tuned using an unsupervised learning algorithm. The MLA correctly identifies two chemical species with over-lapping IMS detection windows. ML can be utilized to improve the ability of currently fielded detectors or future devices to accurately label chemical unknowns given the parameters of detection. The techniques discussed here presents a starting point for improving current and future alarm algorithms with ML.","",""
6,"Changyuan Chen, M. Tello Ruiz, E. Lataire, G. Delefortrie, Marc Mansuy, Tianlong Mei, M. Vantorre","Ship Manoeuvring Model Parameter Identification Using Intelligent Machine Learning Method and the Beetle Antennae Search Algorithm",2019,"","","","",74,"2022-07-13 09:39:06","","10.1115/omae2019-95565","","",,,,,6,2.00,1,7,3,"  In order to identify more accurately and efficiently the unknown parameters of a ship motions model, a novel Nonlinear Least Squares Support Vector Machine (NLSSVM) algorithm, whose penalty factor and Radial Basis Function (RBF) kernel parameters are optimised by the Beetle Antennae Search algorithm (BAS), is proposed and investigated. Aiming at validating the accuracy and applicability of the proposed method, the method is employed to identify the linear and nonlinear parameters of the first-order nonlinear Nomoto model with training samples from numerical simulation and experimental data. Subsequently, the identified parameters are applied in predicting the ship motion. The predicted results illustrate that the new NLSSVM-BAS algorithm can be applied in identifying ship motion’s model, and the effectiveness is verified. Compared among traditional identification approaches with the proposed method, the results display that the accuracy is improved. Moreover, the robust and stability of the NLSSVM-BAS are verified by adding noise in the training sample data.","",""
6,"Senwei Liang, Shixiao W. Jiang, J. Harlim, Haizhao Yang","Solving PDEs on Unknown Manifolds with Machine Learning",2021,"","","","",75,"2022-07-13 09:39:06","","","","",,,,,6,6.00,2,4,1,"This paper proposes a mesh-free computational framework and machine learning theory for solving elliptic PDEs on unknown manifolds, identified with point clouds, based on diffusion maps (DM) and deep learning. The PDE solver is formulated as a supervised learning task to solve a least-squares regression problem that imposes an algebraic equation approximating a PDE (and boundary conditions if applicable). This algebraic equation involves a graph-Laplacian type matrix obtained via DM asymptotic expansion, which is a consistent estimator of second-order elliptic differential operators. The resulting numerical method is to solve a highly non-convex empirical risk minimization problem subjected to a solution from a hypothesis space of neural networks. In a well-posed elliptic PDE setting, when the hypothesis space consists of neural networks with either infinite width or depth, we show that the global minimizer of the empirical loss function is a consistent solution in the limit of large training data. When the hypothesis space is a twolayer neural network, we show that for a sufficiently large width, gradient descent can identify a global minimizer of the empirical loss function. Supporting numerical examples demonstrate the convergence of the solutions, ranging from simple manifolds with low and high co-dimensions, to rough surfaces with and without boundaries. We also show that the proposed NN solver can robustly generalize the PDE solution on new data points with generalization errors that are almost identical to the training errors, superseding a Nyström-based interpolation method. K eywords High-Dimensional PDEs ·Diffusion Maps ·Deep Neural Networks ·Convergence Analysis · Least-Squares Minimization · Manifolds · Point Clouds. ar X iv :2 10 6. 06 68 2v 2 [ m at h. N A ] 1 0 Ju n 20 22 A PREPRINT JUNE 13, 2022","",""
6,"H. Sharif, Rizwan Ahmed Khan","A Novel Machine Learning Based Framework for Detection of Autism Spectrum Disorder (ASD)",2019,"","","","",76,"2022-07-13 09:39:06","","10.1080/08839514.2021.2004655","","",,,,,6,2.00,3,2,3,"Computer vision and machine learning are the linchpin of field of automation. The medicine industry has adopted numerous methods to discover the root causes of many diseases in order to automate detection process. But, the biomarkers of Autism Spectrum Disorder (ASD) are still unknown, let alone automating its detection. Studies from the neuroscience domain highlighted the fact that corpus callosum and intracranial brain volume holds significant information for detection of ASD. Such results and studies are not tested and verified by scientists working in the domain of computer vision / machine learning. Thus, in this study we have proposed a machine learning based framework for automatic detection of ASD using features extracted from corpus callosum and intracranial brain volume from ABIDE dataset. Corpus callosum and intracranial brain volume data is obtained from T1-weighted MRI scans. Our proposed framework first calculates weights of features extracted from Corpus callosum and intracranial brain volume data. This step ensures to utilize discriminative capabilities of only those features that will help in robust recognition of ASD. Then, conventional machine learning algorithm (conventional refers to algorithms other than deep learning) is applied on features that are most significant in terms of discriminative capabilities for recognition of ASD. Finally, for benchmarking and to verify potential of deep learning on analyzing neuroimaging data i.e. T1-weighted MRI scans, we have done experiment with state of the art deep learning architecture i.e. VGG16 . We have used transfer learning approach to use already trained VGG16 model for detection of ASD. This is done to help readers understand benefits and bottlenecks of using deep learning approach for analyzing neuroimaging data which is difficult to record in large enough quantity for deep learning.","",""
0,"A. Vose, J. Balma, Damon Farnsworth, Kaylie Anderson, Y. Peterson","PharML.Bind: Pharmacologic Machine Learning for Protein-Ligand Interactions",2019,"","","","",77,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,5,3,"Is it feasible to create an analysis paradigm that can analyze and then accurately and quickly predict known drugs from experimental data? PharML.Bind is a machine learning toolkit which is able to accomplish this feat. Utilizing deep neural networks and big data, PharML.Bind correlates experimentally-derived drug affinities and protein-ligand X-ray structures to create novel predictions. The utility of PharML.Bind is in its application as a rapid, accurate, and robust prediction platform for discovery and personalized medicine. This paper demonstrates that graph neural networks (GNNs) can be trained to screen hundreds of thousands of compounds against thousands of targets in minutes, a vastly shorter time than previous approaches. This manuscript presents results from training and testing using the entirety of BindingDB after cleaning; this includes a test set with 19,708 X-ray structures and 247,633 drugs, leading to 2,708,151 unique protein-ligand pairings. PharML.Bind achieves a prodigious 98.3% accuracy on this test set in under 25 minutes. PharML.Bind is premised on the following key principles: 1) speed and a high enrichment factor per unit compute time, provided by high-quality training data combined with a novel GNN architecture and use of high-performance computing resources, 2) the ability to generalize to proteins and drugs outside of the training set, including those with unknown active sites, through the use of an active-site-agnostic GNN mapping, and 3) the ability to be easily integrated as a component of increasingly-complex prediction and analysis pipelines. PharML.Bind represents a timely and practical approach to leverage the power of machine learning to efficiently analyze and predict drug action on any practical scale and will provide utility in a variety of discovery and medical applications.","",""
0,"E. Foca","Machine Learning Solutions for Process Control in Semiconductor Manufacturing",2019,"","","","",78,"2022-07-13 09:39:06","","10.1109/VLSI-TSA.2019.8804681","","",,,,,0,0.00,0,1,3,"Reliable and robust process control solutions are pivotal in the era of advanced technology nodes where the stochastic effects dominate the pattern characteristics. The key ingredients ofthe modern process control solutions are huge amount of data and smart data analytics algorithms. The state-of-the-art machine learning methods allow screening large amounts of data for statistically relevant, but so far unknown and unexpected correlations, hence providing new options for process optimizations.","",""
0,"Sarah Laroui, E. Debreuve, X. Descombes, F. Villalba, F. Villiers, A. Vernay","Machine-learning assisted phenotyping: From fungal morphology to mode of action hypothesis",2019,"","","","",79,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,6,3,"Beyond growth inhibition, fungicides can also trigger specific morphological modifications visualized under transmitted light microscopy. These morphological changes result from the activity of a given compound via the inhibition of a molecular target, commonly named as its mode of action (MoA). We are hence able to classify different molecules into their respective MoA by observing their phenotypic signature, and even to detect new MoA with unknown phenotypic effect for further deconvolution. The aim of the presented work is to develop a robust method for automated recognition and classification of these phenotypic signatures in order to lead to a Mode of Action hypothesis. We compare two machine-learning methods (Random forest and Convolutional Neural Network) for direct processing of images generated on the grey mold Botrytis cinerea subjected to different antifungal molecules. © Bayer | Abteilung | Verfasser | Datum","",""
0,"L. Ghaoui","Short Course Robust Optimization and Machine Learning Lecture 1: Optimization Models",2012,"","","","",80,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,1,10,"forms. Sometimes, the constraints are described abstractly via a set condition, of the form x ∈ X for some subset X of R. The corresponding notation is min x∈X f0(x). Robust Optimization & Machine Learning 1. Optimization Models What is Optimization? Definition Examples Nomenclature Other standard forms Extensions Convexity Global vs. local optima Convex problems Software Non-convex problems Non-convex problems References Minimization vs. maximization Some problems come in the form of maximization problems. Such problems are readily cast in standard form via the expression max x∈X f0(x) = −min x∈X : g0(x), where g0 := −f0. I Minimization problems correspond to loss, cost or risk minimization. I Maximization problems typically correspond to utility or return (e.g., on investment) maximization. Robust Optimization & Machine Learning 1. Optimization Models What is Optimization? Definition Examples Nomenclature Other standard forms Extensions Convexity Global vs. local optima Convex problems Software Non-convex problems Non-convex problems References Penalization A trade-off between two objecgives is commonly accomplished via a penalized problem: max x f (x) + λg(x), where f and g represent loss and risk functions, and λ > 0 is a risk-aversion parameter. Example: penalized least-squares min w ‖X T w − y‖2 + λ‖w‖2 Here, the risk term ‖w‖2 controls the variance associated with noise in X . Robust Optimization & Machine Learning 1. Optimization Models What is Optimization? Definition Examples Nomenclature Other standard forms Extensions Convexity Global vs. local optima Convex problems Software Non-convex problems Non-convex problems References Robust optimization Definition In many instances the problem data is not known exactly. Assume that the functions fi in the original problem also depend on an “uncertainty” vector u that is unknown, but bounded: u ∈ U , with the set U given. Robust counterpart: min x max u∈U f0(x , u) subject to ∀ u ∈ U , fi(x , u) ≤ 0, i = 1, . . . ,m. I Robust counterparts are sometimes tractable. I If not, systematic procedures exist to generate approximations. Robust Optimization & Machine Learning 1. Optimization Models What is Optimization? Definition Examples Nomenclature Other standard forms Extensions Convexity Global vs. local optima Convex problems Software Non-convex problems Non-convex problems References Robust optimization Geometry Given a ∈ R, b ∈ R, consider the constraint in x ∈ R (a + u) x ≤ b, with u’s components are only known within a given set U . The robust counterpart is: ∀ u ∈ U : (a + u) x ≤ b. Robust counterpart when A is a box (left panel) and a sphere (right panel). Robust Optimization & Machine Learning 1. Optimization Models What is Optimization? Definition Examples Nomenclature Other standard forms Extensions Convexity Global vs. local optima Convex problems Software Non-convex problems Non-convex problems References Stochastic optimization Definition In stochastic programming, the uncertainty is described by a random variable, with known distribution. Two-stage stochastic linear program with recourse: min x∈X a x + f (x) : f (x) = E w [ min y∈Y(x,w) c(w) y ]. I x-variables correspond to decisions taken now. I y -variables correspond to decisions taken when uncertainty w is revealed. I Stochastic problems are usually very hard. I Most known approaches are very expensive to solve. Robust Optimization & Machine Learning 1. Optimization Models What is Optimization? Definition Examples Nomenclature Other standard forms Extensions Convexity Global vs. local optima Convex problems Software Non-convex problems Non-convex problems References Outline What is Optimization? Definition Examples Nomenclature Other standard forms Extensions The Role of Convexity Global vs. local optima Convex problems Software Non-convex problems Non-convex problems","",""
1,"B. Luber, F. Sorribes-Palmer, G. Müller, L. Pietsch, K. Six","On-Board Wheel Profile Classification Based on Vehicle Dynamics - From Physical Effects to Machine Learning",2019,"","","","",81,"2022-07-13 09:39:06","","10.1007/978-3-030-38077-9_13","","",,,,,1,0.33,0,5,3,"","",""
29,"Yuhui Zheng, Le Sun, Shunfeng Wang, Jianwei Zhang, J. Ning","Spatially Regularized Structural Support Vector Machine for Robust Visual Tracking",2019,"","","","",82,"2022-07-13 09:39:06","","10.1109/TNNLS.2018.2855686","","",,,,,29,9.67,6,5,3,"Structural support vector machine (SSVM) is popular in the visual tracking field as it provides a consistent target representation for both learning and detection. However, the spatial distribution of feature is not considered in standard SSVM-based trackers, therefore leading to limited performance. To obtain a robust discriminative classifier, this paper proposes a novel tracking framework that spatially regularizes SSVM, which yields a new spatially regularized SSVM (SRSSVM). We utilize the spatial regularization prior to penalize the learning classifier with the same size as the target region. The location of classifier spatially located far from the center of region is assigned large weight and vice versa. Then, it is introduced into the SSVM model as a regularization factor to learn the robust discriminative model. Furthermore, an optimizing algorithm with dual coordination descent is presented to efficiently solve the SRSSVM tracking model. Our proposed SRSSVM tracking method has low computational cost like the traditional linear SSVM tracker while can significantly improve the robustness of the discriminative classifier. The experimental results on three popular tracking benchmark data sets show that the proposed SRSSVM tracking method performs favorably against the state-of-the-art trackers.","",""
5,"D. Mccoy, W. Mgbara, Nir Horvitzc, W. Getz, A. Hubbard","Ensemble machine learning of factors influencing COVID-19 across US counties",2020,"","","","",83,"2022-07-13 09:39:06","","10.1038/s41598-021-90827-x","","",,,,,5,2.50,1,5,2,"","",""
4,"Zheng Li, Ye Chen, Siyuan Chang, B. Rousseau, Haoxiang Luo","A one-dimensional flow model enhanced by machine learning for simulation of vocal fold vibration.",2021,"","","","",84,"2022-07-13 09:39:06","","10.1121/10.0003561","","",,,,,4,4.00,1,5,1,"A one-dimensional (1D) unsteady and viscous flow model that is derived from the momentum and mass conservation equations is described, and to enhance this physics-based model, a machine learning approach is used to determine the unknown modeling parameters. Specifically, an idealized larynx model is constructed and ten cases of three-dimensional (3D) fluid-structure interaction (FSI) simulations are performed. The flow data are then extracted to train the 1D flow model using a sparse identification approach for nonlinear dynamical systems. As a result of training, we obtain the analytical expressions for the entrance effect and pressure loss in the glottis, which are then incorporated in the flow model to conveniently handle different glottal shapes due to vocal fold vibration. We apply the enhanced 1D flow model in the FSI simulation of both idealized vocal fold geometries and subject-specific anatomical geometries reconstructed from the magnetic resonance imaging images of rabbits' larynges. The 1D flow model is evaluated in both of these setups and shown to have robust performance. Therefore, it provides a fast simulation tool that is superior to the previous 1D models.","",""
5,"Francesco Regazzoni, D. Chapelle, P. Moireau","Combining data assimilation and machine learning to build data‐driven models for unknown long time dynamics—Applications in cardiovascular modeling",2021,"","","","",85,"2022-07-13 09:39:06","","10.1002/cnm.3471","","",,,,,5,5.00,2,3,1,"We propose a method to discover differential equations describing the long‐term dynamics of phenomena featuring a multiscale behavior in time, starting from measurements taken at the fast‐scale. Our methodology is based on a synergetic combination of data assimilation (DA), used to estimate the parameters associated with the known fast‐scale dynamics, and machine learning (ML), used to infer the laws underlying the slow‐scale dynamics. Specifically, by exploiting the scale separation between the fast and the slow dynamics, we propose a decoupling of time scales that allows to drastically lower the computational burden. Then, we propose a ML algorithm that learns a parametric mathematical model from a collection of time series coming from the phenomenon to be modeled. Moreover, we study the interpretability of the data‐driven models obtained within the black‐box learning framework proposed in this paper. In particular, we show that every model can be rewritten in infinitely many different equivalent ways, thus making intrinsically ill‐posed the problem of learning a parametric differential equation starting from time series. Hence, we propose a strategy that allows to select a unique representative model in each equivalence class, thus enhancing the interpretability of the results. We demonstrate the effectiveness and noise‐robustness of the proposed methods through several test cases, in which we reconstruct several differential models starting from time series generated through the models themselves. Finally, we show the results obtained for a test case in the cardiovascular modeling context, which sheds light on a promising field of application of the proposed methods.","",""
3,"M. Ramezani, Pauline Mouches, E. Yoon, Deepthi Rajashekar, J. Ruskey, E. Leveille, Kristina Martens, M. Kibreab, Tracy Hammer, I. Kathol, Nadia Maarouf, J. Sarna, D. Martino, G. Pfeffer, Z. Gan-Or, N. Forkert, O. Monchi","Investigating the relationship between the SNCA gene and cognitive abilities in idiopathic Parkinson’s disease using machine learning",2021,"","","","",86,"2022-07-13 09:39:06","","10.1038/s41598-021-84316-4","","",,,,,3,3.00,0,17,1,"","",""
0,"Serkan Tokgoz, Erik Jonsson","Speaker ID on Apollo 11 corpus: A Study using different Machine Learning Models",2019,"","","","",87,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,2,3,"The main goal here is to match a voice sample from an unknown speaker to one of several labeled speaker models since speech is easily produced. For the feature extraction, Mel Frequency Cepstrum Coefficients will be used since it is one of the most common features used for speaker recognition. Before extracting the features, we will do pre-processing such as Voice Activity detection to ignore unvoiced parts of the speech. For classification and objective comparison, K-Nearest Neighborhood (KNN), Convolutional Neural Network (CNN) and I-vectors/PLDA results will be shared. The dataset used for the project is FEARLESS STEPS that consists of 10 hours of digitized recordings of the Apollo 11 Space Mission. These recordings were digitized by the Centre of Robust Speech Systems (CRSS) of The University of Texas at Dallas. It was typically used for speech activity detection, sentiment analysis and speaker recognition. In the research, there were a few challenges that were met using methods. Our main focus will be detecting speech parts of the speech signals and classifying the respective speakers in the given time frame. Keywords—MFCC, KNN, CNN, i-Vector, PLDA","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",88,"2022-07-13 09:39:06","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
1,"Huda Ali Alatwi, C. Morisset","Adversarial Machine Learning In Network Intrusion Detection Domain: A Systematic Review",2021,"","","","",89,"2022-07-13 09:39:06","","","","",,,,,1,1.00,1,2,1,"Due to their massive success in various domains, deep learning techniques are increasingly used to design network intrusion detection solutions that detect and mitigate unknown and known attacks with high accuracy detection rates and minimal feature engineering. However, it has been found that deep learning models are vulnerable to data instances that can mislead the model to make incorrect classification decisions socalled (adversarial examples). Such vulnerability allows attackers to target NIDSs by adding small crafty perturbations to the malicious traffic to evade detection and disrupt the system’s critical functionalities. The problem of deep adversarial learning has been extensively studied in the computer vision domain; however, it is still an area of open research in network security applications. Therefore, this survey explores the researches that employ different aspects of adversarial machine learning in the area of network intrusion detection in order to provide directions for potential solutions. First, the surveyed studies are categorized based on their contribution to generating adversarial examples, evaluating the robustness of ML-based NIDs towards adversarial examples, and defending these models against such attacks. Second, we highlight the characteristics identified in the surveyed research. Furthermore, we discuss the applicability of the existing generic adversarial attacks for the NIDS domain, the feasibility of launching the proposed attacks in real-world scenarios, and the limitations of the existing mitigation solutions. Keywords—Network Intrusion Detection, Deep Neural Networks, Adversarial Examples, Adversarial Robustness, Adversarial Attacks, Evasion Attacks, Cyber Security, Machine Learning, Adversarial Machine Learning","",""
2,"Nick Fountain-Jones, Megan L. Smith, F. Austerlitz","Machine learning in molecular ecology",2021,"","","","",90,"2022-07-13 09:39:06","","10.1111/1755-0998.13532","","",,,,,2,2.00,1,3,1,"Advances in nextgeneration sequencing (NGS) platforms are allowing researchers to routinely collate large genomewide data sets to address a variety of ecological questions. However, with this big data comes big analytical challenges that are increasingly addressed using machine learning (for a review, see Schrider & Kern, 2018). Machine learning is a subfield of artificial intelligence and represents a conglomeration of methods where predictive accuracy is the primary goal (e.g., Belcaid & Toonen, 2015; Breiman, 2001; Elith et al., 2008; Lucas, 2020). Machine learning assumes that the datagenerating process is unknown and complex and finds the dominant patterns by learning the relationships between inputs and responses (Elith et al., 2008). Broadly, machine learning differs from other statistical approaches in two important ways. The first is that predictive performance drives model formulation rather than model selection or expert opinion, and the second is there is less emphasis on model selection ( Breiman, 2001; Lucas, 2020). For these reasons, machine learning has the reputation for being less interpretable and difficult to apply rigorously (Elith et al., 2008; Lucas, 2020; Molnar, 2018). However, in parallel with the revolution of sequencing techniques, there has also been a revolution in data science in terms of predictive performance and techniques to interpret machine learning models (FountainJones et al., 2019; Lucas, 2020; Molnar, 2018). There are now streamlined R and Python packages that make the robust use of algorithms from support vector machines (SVMs) to neural networks readily achievable (e.g., Abadi et al., 2015; Kuhn & Wickham, 2020, see Text Box 1 for some important machine learning terminology). Moreover, other statistical paradigms such as approximate Bayesian computation (ABC) are being applied sidebyside or within machine learning frameworks to enhance the utility of these approaches (e.g., Carlson, 2020; Raynal et al., 2019). The ability of machine learning algorithms to build powerful predictive models that capture complex nonlinear responses with minimal statistical assumptions has been harnessed by most molecular ecology subdisciplines for decades. For example, machine learning models were developed before the turn of the millennium to classify normal or cancerous tissue based on transcription profiles (Furey et al., 2000). Not long after gradient boosting models (GBMs) were developed (e.g., Hastie et al., 2009), researchers were applying the approach to classify population genetics models based on a suite of summary statistics such as Tajima's θπ (Lin et al., 2011). In addition, extensions of the popular random forest algorithm have been utilized in ecological genetics to untangle the drivers of climate adaptation (Fitzpatrick & Keller, 2015). Generally, however, advances in computer science and machine learning are slow to filter down to ecologists (Belcaid & Toonen, 2015; Elith & Hastie, 2008; FountainJones et al., 2019), partly through unfamiliarity with these types of approaches but also because of the rapid rate of advance in the data science field. This Special Issue aims to help expand the use of machine learning approaches and to help bring advances in data science to the toolkits of molecular ecologists. This issue comprises 17 papers grouped into four sections covering a diverse variety of molecular ecology subdisciplines. The first section covers how machine learning can be applied to make inferences about population demography. We further group these papers algorithmically with four papers utilizing random forest architecture and the remaining four using neural networks. The second section highlights how machine learning can detect signatures of selection across loci. The third section highlights how these methods can be applied to untangle the complex ecological drivers of genomic change (‘ecological genomics’) and species community dynamics. The last section explores how advances in machine learning can provide insights into species limits and contribute to biodiversity monitoring.","",""
1,"Linghua Meng, Huanjun Liu, S. Ustin, Xinle Zhang","Predicting Maize Yield at the Plot Scale of Different Fertilizer Systems by Multi-Source Data and Machine Learning Methods",2021,"","","","",91,"2022-07-13 09:39:06","","10.3390/rs13183760","","",,,,,1,1.00,0,4,1,"Timely and reliable maize yield prediction is essential for the agricultural supply chain and food security. Previous studies using either climate or satellite data or both to build empirical or statistical models have prevailed for decades. However, to what extent climate and satellite data can improve yield prediction is still unknown. In addition, fertilizer information may also improve crop yield prediction, especially in regions with different fertilizer systems, such as cover crop, mineral fertilizer, or compost. Machine learning (ML) has been widely and successfully applied in crop yield prediction. Here, we attempted to predict maize yield from 1994 to 2007 at the plot scale by integrating multi-source data, including monthly climate data, satellite data (i.e., vegetation indices (VIs)), fertilizer data, and soil data to explore the accuracy of different inputs to yield prediction. The results show that incorporating all of the datasets using random forests (RF) and AB (adaptive boosting) can achieve better performances in yield prediction (R2: 0.85~0.98). In addition, the combination of VIs, climate data, and soil data (VCS) can predict maize yield more effectively than other combinations (e.g., combinations of all data and combinations of VIs and soil data). Furthermore, we also found that including different fertilizer systems had different prediction accuracies. This paper aggregates data from multiple sources and distinguishes the effects of different fertilization scenarios on crop yield predictions. In addition, the effects of different data on crop yield were analyzed in this study. Our study provides a paradigm that can be used to improve yield predictions for other crops and is an important effort that combines multi-source remotely sensed and environmental data for maize yield prediction at the plot scale and develops timely and robust methods for maize yield prediction grown under different fertilizing systems.","",""
1,"Soha Mohamed, M. S. Fayed","Modelling of Received Signals in Molecular Communication Systems based machine learning: Comparison of azure machine learning and Python tools",2021,"","","","",92,"2022-07-13 09:39:06","","","","",,,,,1,1.00,1,2,1,"Molecular communication (MC) implemented on Nano networks has extremely attractive characteristics in terms of energy efficiency, dependability, and robustness. Even though, the impact of incredibly slow molecule diffusion and high variability environments remains unknown. Analysis and designs of communication systems usually rely on developing mathematical models that describe the communication channel. However, the underlying channel models are unknown in some systems, such as MC systems, where chemical signals are used to transfer information. In these cases, a new method to analyze and design is needed. In this paper, we concentrate on one critical aspect of the MC system, modelling MC received signal until time t , and demonstrate that using tools from ML makes it promising to train detectors that can be executed well without any information about the channel model. Machine learning (ML) is one of the intelligent methodologies that has shown promising results in the domain. This paper applies Azure Machine Learning (Azure ML) for flexible pavement maintenance regressions problems and solutions. For prediction, four parameters are used as inputs: the receiver radius, transmitter radius, distance between receiver and transmitter, and diffusion coefficient, while the output is mAP (mean average precision) of the received signal. Azure ML enables algorithms that can learn from data and experiences and accomplish tasks without having to be coded. In the established Azure ML, the regression algorithms such as, boost decision tree regression, Bayesian linear regression, neural network, and decision forest regression are selected. The best performance is chosen as an optimality criterion. Finally, a comparison that shows the potential benefits of Azure ML tool over programmed based tool (Python), used by developers on local PCs, is demonstrated.","",""
1,"C. Knaak, M. Kröger, F. Schulze, P. Abels, A. Gillner","Deep Learning and Conventional Machine Learning for Image-Based in-Situ Fault Detection During Laser Welding: A Comparative Study",2021,"","","","",93,"2022-07-13 09:39:06","","10.20944/PREPRINTS202105.0272.V1","","",,,,,1,1.00,0,5,1,"An effective process monitoring strategy is a requirement for meeting the challenges posed by increasingly complex products and manufacturing processes. To address these needs, this study investigates a comprehensive scheme based on classical machine learning methods, deep learning algorithms, and feature extraction and selection techniques. In a first step, a novel deep learning architecture based on convolutional neural networks (CNN) and gated recurrent units (GRU) is introduced to predict the local weld quality based on mid-wave infrared (MWIR) and near-infrared (NIR) image data. The developed technology is used to discover critical welding defects including lack of fusion (false friends), sagging and lack of penetration, and geometric deviations of the weld seam. Additional work is conducted to investigate the significance of various geometrical, statistical, and spatio-temporal features extracted from the keyhole and weld pool regions. Furthermore, the performance of the proposed deep learning architecture is compared to that of classical supervised machine learning algorithms, such as multi-layer perceptron (MLP), logistic regression (LogReg), support vector machines (SVM), decision trees (DT), random forest (RF) and k-Nearest Neighbors (kNN). Optimal hyperparameters for each algorithm are determined by an extensive grid search. Ultimately, the three best classification models are combined into an ensemble classifier that yields the highest detection rates and achieves the most robust estimation of welding defects among all classifiers studied, which is validated on previously unknown welding trials.","",""
0,"C. Knaak, M. Kröger, F. Schulze, P. Abels, A. Gillner","Deep learning and conventional machine learning for image- based in-situ fault detection during laser welding: A compara- tive study",2021,"","","","",94,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,5,1,"An effective process monitoring strategy is a requirement for meeting the challenges posed by increasingly complex products and manufacturing processes. To address these needs, this study investigates a comprehensive scheme based on classical machine learning methods, deep learning algorithms, and feature extraction and selection techniques. In a first step, a novel deep learning architecture based on convolutional neural networks (CNN) and gated recurrent units (GRU) is introduced to predict the local weld quality based on mid-wave infrared (MWIR) and nearinfrared (NIR) image data. The developed technology is used to discover critical welding defects including lack of fusion (false friends), sagging and lack of penetration, and geometric deviations of the weld seam. Additional work is conducted to investigate the significance of various geometrical, statistical, and spatio-temporal features extracted from the keyhole and weld pool regions. Furthermore, the performance of the proposed deep learning architecture is compared to that of classical supervised machine learning algorithms, such as multi-layer perceptron (MLP), logistic regression (LogReg), support vector machines (SVM), decision trees (DT), random forest (RF) and k-Nearest Neighbors (kNN). Optimal hyperparameters for each algorithm are determined by an extensive grid search. Ultimately, the three best classification models are combined into an ensemble classifier that yields the highest detection rates and achieves the most robust estimation of welding defects among all classifiers studied, which is validated on previously unknown welding trials.","",""
3,"Wei Huang, Jing Zhao, Guokuan Yu, P. Wong","Intelligent Vibration Control for Semiactive Suspension Systems Without Prior Knowledge of Dynamical Nonlinear Damper Behaviors Based on Improved Extreme Learning Machine",2020,"","","","",95,"2022-07-13 09:39:06","","10.1109/tmech.2020.3031840","","",,,,,3,1.50,1,4,2,"Aiming at enhancing the vehicle comfort and handling performances, this article concerns with the development of the vibration control method for the semiactive suspension systems installed with electrohydraulic dampers. To reject the external disturbances induced by the irregular road profile, sliding mode was intensively investigated for vibration control owing to its robust feature of insensitivity to the parametric uncertainty and external disturbances. However, the unknown nonlinearity of damper behaviors leads to model mismatch to cause the high-frequency switching of sliding-mode controllers. Consequently, the severe chattering phenomenon produces. Although saturated function is available to alleviate the chattering problem of sliding-mode control, there is always a tradeoff problem between tracking accuracy and chattering suppression. To solve this problem, this study provides a new intelligent robust control method for simultaneous improvements of tracking accuracy and chattering suppression. Given the computational efficiency, an improved extreme learning machine (ELM) is proposed to intelligently approximate and compensate the unmodeled dynamics with unknown nonlinearity to restrain the chattering problem, where a new adaptive learning law is designed in the premise of Lyapunov stability. To validate the effectiveness and efficiency of the proposed ELM-based robust control, a quarter-car test rig was set up for the hardware-in-the-loop test. Experimental results show that the proposed controller outperforms the sliding-mode controller with saturated function in depressing the sprung mass acceleration and tire deflection, showing its significance in both control performance enhancement and chattering elimination.","",""
8,"Mustafa Anil Koçak, David Ramirez, E. Erkip, D. Shasha","SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness",2017,"","","","",96,"2022-07-13 09:39:06","","10.1109/TPAMI.2019.2932415","","",,,,,8,1.60,2,4,5,"<italic>SafePredict</italic> is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, <inline-formula><tex-math notation=""LaTeX"">$1-\epsilon$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""kocak-ieq1-2932415.gif""/></alternatives></inline-formula>, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm so that the error rate on non-refused predictions does not exceed <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq2-2932415.gif""/></alternatives></inline-formula>. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq3-2932415.gif""/></alternatives></inline-formula>, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the-art confidence-based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software is included in the supplementary material, which can be found on the Computer Society Digital Library at <uri>http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2932415</uri>.","",""
20,"D. Grana, L. Azevedo, Mingliang Liu","A comparison of deep machine learning and Monte Carlo methods for facies classification from seismic data",2020,"","","","",97,"2022-07-13 09:39:06","","10.1190/geo2019-0405.1","","",,,,,20,10.00,7,3,2,"Among the large variety of mathematical and computational methods for estimating reservoir properties such as facies and petrophysical variables from geophysical data, deep machine-learning algorithms have gained significant popularity for their ability to obtain accurate solutions for geophysical inverse problems in which the physical models are partially unknown. Solutions of classification and inversion problems are generally not unique, and uncertainty quantification studies are required to quantify the uncertainty in the model predictions and determine the precision of the results. Probabilistic methods, such as Monte Carlo approaches, provide a reliable approach for capturing the variability of the set of possible models that match the measured data. Here, we focused on the classification of facies from seismic data and benchmarked the performance of three different algorithms: recurrent neural network, Monte Carlo acceptance/rejection sampling, and Markov chain Monte Carlo. We tested and validated these approaches at the well locations by comparing classification predictions to the reference facies profile. The accuracy of the classification results is defined as the mismatch between the predictions and the log facies profile. Our study found that when the training data set of the neural network is large enough and the prior information about the transition probabilities of the facies in the Monte Carlo approach is not informative, machine-learning methods lead to more accurate solutions; however, the uncertainty of the solution might be underestimated. When some prior knowledge of the facies model is available, for example, from nearby wells, Monte Carlo methods provide solutions with similar accuracy to the neural network and allow a more robust quantification of the uncertainty, of the solution.","",""
21,"Christopher Culley, S. Vijayakumar, Guido Zampieri, C. Angione","A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth",2020,"","","","",98,"2022-07-13 09:39:06","","10.1073/pnas.2002959117","","",,,,,21,10.50,5,4,2,"Significance Linking genotype and phenotype is a fundamental problem in biology, key to several biomedical and biotechnological applications. Cell growth is a central phenotypic trait, resulting from interactions between environment, gene regulation, and metabolism, yet its functional bases are still not completely understood. We propose and test a machine-learning approach that integrates large-scale gene expression profiles and mechanistic metabolic models, for characterizing cell growth and understanding its driving mechanisms in Saccharomyces cerevisiae. At its core, a custom-built multimodal learning method merges experimentally generated and model-generated data. We show that our approach can leverage the advantages of both machine learning and metabolic modeling, revealing unknown interactions between biological domains, incorporating mechanistic knowledge, and therefore overcoming black-box limitations of conventional data-driven approaches. Metabolic modeling and machine learning are key components in the emerging next generation of systems and synthetic biology tools, targeting the genotype–phenotype–environment relationship. Rather than being used in isolation, it is becoming clear that their value is maximized when they are combined. However, the potential of integrating these two frameworks for omic data augmentation and integration is largely unexplored. We propose, rigorously assess, and compare machine-learning–based data integration techniques, combining gene expression profiles with computationally generated metabolic flux data to predict yeast cell growth. To this end, we create strain-specific metabolic models for 1,143 Saccharomyces cerevisiae mutants and we test 27 machine-learning methods, incorporating state-of-the-art feature selection and multiview learning approaches. We propose a multiview neural network using fluxomic and transcriptomic data, showing that the former increases the predictive accuracy of the latter and reveals functional patterns that are not directly deducible from gene expression alone. We test the proposed neural network on a further 86 strains generated in a different experiment, therefore verifying its robustness to an additional independent dataset. Finally, we show that introducing mechanistic flux features improves the predictions also for knockout strains whose genes were not modeled in the metabolic reconstruction. Our results thus demonstrate that fusing experimental cues with in silico models, based on known biochemistry, can contribute with disjoint information toward biologically informed and interpretable machine learning. Overall, this study provides tools for understanding and manipulating complex phenotypes, increasing both the prediction accuracy and the extent of discernible mechanistic biological insights.","",""
94,"Qiang Zhu, A. Samanta, Bingxi Li, R. Rudd, T. Frolov","Predicting phase behavior of grain boundaries with evolutionary search and machine learning",2017,"","","","",99,"2022-07-13 09:39:06","","10.1038/s41467-018-02937-2","","",,,,,94,18.80,19,5,5,"","",""
3,"Xi Chen, Yining Wang","Robust Dynamic Pricing with Demand Learning in the Presence of Outlier Customers",2020,"","","","",100,"2022-07-13 09:39:06","","10.2139/ssrn.3650656","","",,,,,3,1.50,2,2,2,"Dynamic pricing is a core problem in revenue management. Most existing literature assumes that the demand follows a probabilistic model, with an unknown demand curve as the mean. However, in practice, customers may not always behave according to such a model. In “Robust Dynamic Pricing with Demand Learning in the Presence of Outlier Customers,” Chen and Wang study the dynamic pricing problem under model misspecification. To characterize the behavior of outlier customers, an ε-contamination model—the most fundamental model in robust statistics and machine learning, is adopted. The challenges brought by the presence of outlier customers are mainly due to the fact that arrivals of outliers and their exhibited demand behaviors are completely arbitrary. To address these challenges, the authors propose robust dynamic pricing policies that can handle any outlier arrival and demand patterns. The proposed policies are fully adaptive without requiring prior knowledge of the outlier proportion parameter.","",""
4,"Himaghna Bhattacharjee, Nikolaos Anesiadis, D. Vlachos","Regularized machine learning on molecular graph model explains systematic error in DFT enthalpies",2021,"","","","",101,"2022-07-13 09:39:06","","10.1038/s41598-021-93854-w","","",,,,,4,4.00,1,3,1,"","",""
33,"Ved P. Kafle, Y. Fukushima, P. Martinez-Julia, T. Miyazawa","Consideration On Automation of 5G Network Slicing with Machine Learning",2018,"","","","",102,"2022-07-13 09:39:06","","10.23919/ITU-WT.2018.8597639","","",,,,,33,8.25,8,4,4,"Machine learning has the capability to provide simpler solutions to complex problems by analyzing a huge volume of data in a short time, learning for adapting its functionality to dynamically changing environments, and predicting near future events with reasonably good accuracy. The 5G communication networks are getting complex due to emergence of unprecedentedly huge number of new connected devices and new types of services. Moreover, the requirements of creating virtual network slices suitable to provide optimal services for diverse users and applications are posing challenges to the efficient management of network resources, processing information about a huge volume of traffic, staying robust against all potential security threats, and adaptively adjustment of network functionality for time-varying workload. In this paper, we introduce about the envisioned 5G network slicing and elaborate the necessity of automation of network functions for the design, construction, deployment, operation, control and management of network slices. We then revisit the machine learning techniques that can be applied for the automation of network functions. We also discuss the status of artificial intelligence and machine learning related activities being progressed in standards development organizations and industrial forums.","",""
2,"Siddhant Agarwal, N. Tosi, P. Kessel, S. Padovan, D. Breuer, G. Montavon","Towards constraining Mars' thermal evolution using machine learning",2021,"","","","",103,"2022-07-13 09:39:06","","10.5194/EGUSPHERE-EGU21-4044","","",,,,,2,2.00,0,6,1,"<p>The thermal evolution of terrestrial planets depends strongly on several parameters and initial conditions that are poorly constrained. Often, direct or indirect observables from planetary missions such as elastic lithospheric thickness, crustal thickness and duration of volcanism are inverted to infer the unknown parameter values and initial conditions. The non-uniqueness and non-linearity of this inversion necessitates a probabilistic inversion framework. However, due to the expensive nature of forward dynamic simulations of thermal convection , Markov Chain Monte Carlo methods are rarely used. To address this shortcoming, some studies have recently shown the effectiveness of Mixture Density Networks (MDN) (Bishop 1995) in being able to approximate the posterior probability using only the dataset of simulations run prior to the inversion (Meier et al. 2007, de Wit et al. 2013, K&#228;ufl et al. 2016, Atkins et al. 2016).</p><p>Using MDNs, we systematically isolate the degree to which a parameter can be constrained using different &#8220;present-day&#8221; synthetic observables from 6130 simulations for a Mars-like planet. The dataset &#8211; generated using the mantle convection code GAIA (H&#252;ttig et al. 2013)- is the same as that used by Agarwal et al. (2020) for a surrogate modelling study.</p><p>The loss function used to optimize the MDN (log-likelihood) provides a single robust quantity that can be used to measure how well a parameter can be constrained. We test different numbers and combinations of observables (heat flux at the surface and core-mantle boundary, radial contraction, melt produced, elastic lithospheric thickness, and duration of volcanism) to constrain the following parameters: reference viscosity, activation energy and activation volume of the diffusion creep rheology, an enrichment factor for radiogenic elements in the crust, and initial mantle temperature. If all observables are available, reference viscosity can be constrained to within 32% of its entire range (10<sup>19</sup>&#8722;10<sup>22</sup> Pa s), crustal enrichment factor (1&#8722;50) to within 15%, activation energy (10<sup>5</sup>&#8722;5&#215;10<sup>5</sup> J mol-1 ) to within 80%, and initial mantle temperature (1600&#8722;1800K) to within 39%. The additional availability of the full present-day temperature profile or parts of it as an observable tightens the constraints further. The activation volume (4&#215;10<sup>-6</sup> &#8722;10&#215;10<sup>-6</sup>&#160; m<sup>3</sup> mol<sup>-1</sup>) cannot be constrained and requires research into new observables in space and time, as well as fields other than just temperature. Testing different levels of uncertainty (simulated using Gaussian noise) in the observables, we found that constraints on different parameters loosen at different rates, with initial temperature being the most sensitive. Finally, we present how the marginal MDN proposed by Bishop (1995) can be modified to model the joint probability for all parameters, so that&#160; the inter-parameter correlations and the associated degeneracy can be capture, thereby providing a more comprehensive picture of all the evolution scenarios that fit given observational constraints.</p>","",""
146,"R. Vinayakumar, M. Alazab, K. Soman, P. Poornachandran, S. Venkatraman","Robust Intelligent Malware Detection Using Deep Learning",2019,"","","","",104,"2022-07-13 09:39:06","","10.1109/ACCESS.2019.2906934","","",,,,,146,48.67,29,5,3,"Security breaches due to attacks by malicious software (malware) continue to escalate posing a major security concern in this digital age. With many computer users, corporations, and governments affected due to an exponential growth in malware attacks, malware detection continues to be a hot research topic. Current malware detection solutions that adopt the static and dynamic analysis of malware signatures and behavior patterns are time consuming and have proven to be ineffective in identifying unknown malwares in real-time. Recent malwares use polymorphic, metamorphic, and other evasive techniques to change the malware behaviors quickly and to generate a large number of new malwares. Such new malwares are predominantly variants of existing malwares, and machine learning algorithms (MLAs) are being employed recently to conduct an effective malware analysis. However, such approaches are time consuming as they require extensive feature engineering, feature learning, and feature representation. By using the advanced MLAs such as deep learning, the feature engineering phase can be completely avoided. Recently reported research studies in this direction show the performance of their algorithms with a biased training data, which limits their practical use in real-time situations. There is a compelling need to mitigate bias and evaluate these methods independently in order to arrive at a new enhanced method for effective zero-day malware detection. To fill the gap in the literature, this paper, first, evaluates the classical MLAs and deep learning architectures for malware detection, classification, and categorization using different public and private datasets. Second, we remove all the dataset bias removed in the experimental analysis by having different splits of the public and private datasets to train and test the model in a disjoint way using different timescales. Third, our major contribution is in proposing a novel image processing technique with optimal parameters for MLAs and deep learning architectures to arrive at an effective zero-day malware detection model. A comprehensive comparative study of our model demonstrates that our proposed deep learning architectures outperform classical MLAs. Our novelty in combining visualization and deep learning architectures for static, dynamic, and image processing-based hybrid approach applied in a big data environment is the first of its kind toward achieving robust intelligent zero-day malware detection. Overall, this paper paves way for an effective visual detection of malware using a scalable and hybrid deep learning framework for real-time deployments.","",""
12,"Lixiang Hong, Jinjian Lin, Shuya Li, Fangping Wan, Hui Yang, Tao Jiang, Dan Zhao, Jianyang Zeng","A novel machine learning framework for automated biomedical relation extraction from large-scale literature repositories",2020,"","","","",105,"2022-07-13 09:39:06","","10.1038/s42256-020-0189-y","","",,,,,12,6.00,2,8,2,"","",""
1,"Li Jun","Robust adaptive control for a class of MIMO affine nonlinear systems using extreme learning machine",2015,"","","","",106,"2022-07-13 09:39:06","","","","",,,,,1,0.14,1,1,7,"Abstract: Based on the extreme learning machine(ELM), a robust adaptive neural control method for a class of multipleinput-multiple-output(MIMO) affine nonlinear dynamic systems is presented. ELM for single-hidden layer feedforward networks(SLFNs), which randomly chooses hidden node parameters and analytically determines the output weights of SLFNs, shows good generalized performance at extremely fast learning speed. The proposed control scheme utilizes the ELM to approximate the plant’s unknown nonlinear terms. Meanwhile, output weights of ELM, unknown upper bound values of approximation errors and external disturbances can be online estimated through parameter adaptive laws by using Lyapunov stability analysis, so that semi-global uniform ultimare boundedness of all signals in the closed-loop system can be guaranteed. Finally, simulation results show the effectiveness of the proposed adaptive ELM control.","",""
17,"B. Karg, T. Alamo, S. Lucia","Probabilistic performance validation of deep learning‐based robust NMPC controllers",2019,"","","","",107,"2022-07-13 09:39:06","","10.1002/rnc.5696","","",,,,,17,5.67,6,3,3,"Solving nonlinear model predictive control problems in real time is still an important challenge despite of recent advances in computing hardware, optimization algorithms and tailored implementations. This challenge is even greater when uncertainty is present due to disturbances, unknown parameters or measurement and estimation errors. To enable the application of advanced control schemes to fast systems and on low‐cost embedded hardware, we propose to approximate a robust nonlinear model controller using deep learning and to verify its quality using probabilistic validation techniques. We propose a probabilistic validation technique based on finite families, combined with the idea of generalized maximum and constraint backoff to enable statistically valid conclusions related to general performance indicators. The potential of the proposed approach is demonstrated with simulation results of an uncertain nonlinear system.","",""
111,"T. Cordier, P. Esling, F. Lejzerowicz, J. Visco, Amine Ouadahi, Catarina I M Martins, T. Cedhagen, J. Pawlowski","Predicting the Ecological Quality Status of Marine Environments from eDNA Metabarcoding Data Using Supervised Machine Learning.",2017,"","","","",108,"2022-07-13 09:39:06","","10.1021/acs.est.7b01518","","",,,,,111,22.20,14,8,5,"Monitoring biodiversity is essential to assess the impacts of increasing anthropogenic activities in marine environments. Traditionally, marine biomonitoring involves the sorting and morphological identification of benthic macro-invertebrates, which is time-consuming and taxonomic-expertise demanding. High-throughput amplicon sequencing of environmental DNA (eDNA metabarcoding) represents a promising alternative for benthic monitoring. However, an important fraction of eDNA sequences remains unassigned or belong to taxa of unknown ecology, which prevent their use for assessing the ecological quality status. Here, we show that supervised machine learning (SML) can be used to build robust predictive models for benthic monitoring, regardless of the taxonomic assignment of eDNA sequences. We tested three SML approaches to assess the environmental impact of marine aquaculture using benthic foraminifera eDNA, a group of unicellular eukaryotes known to be good bioindicators, as features to infer macro-invertebrates based biotic indices. We found similar ecological status as obtained from macro-invertebrates inventories. We argue that SML approaches could overcome and even bypass the cost and time-demanding morpho-taxonomic approaches in future biomonitoring.","",""
4,"R. Pattnaik, K. Sharma, K. Alabarta, D. Altamirano, M. Chakraborty, Aniruddha Kembhavi, M. Méndez, J. K. Orwat-Kapola","A Machine Learning Approach For Classifying Low-mass X-ray Binaries Based On Their Compact Object Nature",2020,"","","","",109,"2022-07-13 09:39:06","","10.1093/mnras/staa3899","","",,,,,4,2.00,1,8,2,"  Low Mass X-ray binaries (LMXBs) are binary systems where one of the components is either a black hole or a neutron star and the other is a less massive star. It is challenging to unambiguously determine whether a LMXB hosts a black hole or a neutron star. In the last few decades, multiple observational works have tried, with different levels of success, to address this problem. In this paper, we explore the use of machine learning to tackle this observational challenge. We train a random forest classifier to identify the type of compact object using the energy spectrum in the energy range 5-25 keV obtained from the Rossi X-ray Timing Explorer archive. We report an average accuracy of 87±13% in classifying the spectra of LMXB sources. We further use the trained model for predicting the classes for LMXB systems with unknown or ambiguous classification. With the ever-increasing volume of astronomical data in the X-ray domain from present and upcoming missions (e.g., SWIFT, XMM-Newton, XARM, ATHENA, NICER), such methods can be extremely useful for faster and robust classification of X-ray sources and can also be deployed as part of the data reduction pipeline.","",""
15,"Yeounoh Chung, P. Haas, E. Upfal, Tim Kraska","Unknown Examples & Machine Learning Model Generalization",2018,"","","","",110,"2022-07-13 09:39:06","","","","",,,,,15,3.75,4,4,4,"Over the past decades, researchers and ML practitioners have come up with better and better ways to build, understand and improve the quality of ML models, but mostly under the key assumption that the training data is distributed identically to the testing data. In many real-world applications, however, some potential training examples are unknown to the modeler, due to sample selection bias or, more generally, covariate shift, i.e., a distribution shift between the training and deployment stage. The resulting discrepancy between training and testing distributions leads to poor generalization performance of the ML model and hence biased predictions. We provide novel algorithms that estimate the number and properties of these unknown training examples---unknown unknowns. This information can then be used to correct the training set, prior to seeing any test data. The key idea is to combine species-estimation techniques with data-driven methods for estimating the feature values for the unknown unknowns. Experiments on a variety of ML models and datasets indicate that taking the unknown examples into account can yield a more robust ML model that generalizes better.","",""
32,"K. Javed","A robust and reliable data-driven prognostics approach based on Extreme Learning Machine and Fuzzy Clustering",2014,"","","","",111,"2022-07-13 09:39:06","","","","",,,,,32,4.00,32,1,8,"Prognostics and Health Management (PHM) aims at extending the life cycle of engineerin gassets, while reducing exploitation and maintenance costs. For this reason,prognostics is considered as a key process with future capabilities. Indeed, accurateestimates of the Remaining Useful Life (RUL) of an equipment enable defining furtherplan of actions to increase safety, minimize downtime, ensure mission completion andefficient production.Recent advances show that data-driven approaches (mainly based on machine learningmethods) are increasingly applied for fault prognostics. They can be seen as black-boxmodels that learn system behavior directly from Condition Monitoring (CM) data, usethat knowledge to infer its current state and predict future progression of failure. However,approximating the behavior of critical machinery is a challenging task that canresult in poor prognostics. As for understanding, some issues of data-driven prognosticsmodeling are highlighted as follows. 1) How to effectively process raw monitoringdata to obtain suitable features that clearly reflect evolution of degradation? 2) Howto discriminate degradation states and define failure criteria (that can vary from caseto case)? 3) How to be sure that learned-models will be robust enough to show steadyperformance over uncertain inputs that deviate from learned experiences, and to bereliable enough to encounter unknown data (i.e., operating conditions, engineering variations,etc.)? 4) How to achieve ease of application under industrial constraints andrequirements? Such issues constitute the problems addressed in this thesis and have ledto develop a novel approach beyond conventional methods of data-driven prognostics.","",""
12,"N. Khoa, M. M. Alamdari, T. Rakotoarivelo, Ali Anaissi, Yang Wang","Structural Health Monitoring Using Machine Learning Techniques and Domain Knowledge Based Features",2018,"","","","",112,"2022-07-13 09:39:06","","10.1007/978-3-319-90403-0_20","","",,,,,12,3.00,2,5,4,"","",""
0,"Kelly Hirschbeck Smalenberger","Probabilistic Numerical Integration with Applications in Machine Learning",2020,"","","","",113,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,1,2,"We consider the approximation of unknown or intractable integrals using quadrature when the evaluation of the integrand is considered costly. This is a central problem in machine learning, including model averaging, (hyper-)parameter marginalization, and computing posterior predictive distributions. Recently Batch Bayesian Quadrature (BBQ) has combined the probabilistic integration techniques of Bayesian Quadrature with the parallelization techniques of Batch Bayesian Optimization, resulting in improved performance compared to Monte Carlo techniques, especially when parallelization is increased. While the selection of batches in BBQ mitigates costs of individual point selection, every point within every batch is nevertheless chosen serially, impeding the full potential of batch selection. We resolve this shortcoming. We developed a novel BBQ method which updates points within a batch without the costs of non-serial point selection. To implement this, we devise a dynamic domain decomposition. Combining these efficiently reduces uncertainty, lowers error estimates of the integrand, and results in more numerically robust integral estimates. Furthermore, we close an open question about the cessation criteria, which we establish and support using numerical methods. We present our findings within the context of the history of quadrature, show how our novel methods significantly improve the literature, and provide possibilities for future research. Department of Mathematics, UNC Charlotte, Charlotte, NC 28223","",""
80,"Xiaoqin Zhang, Mingyu Fan, Di Wang, Peng Zhou, D. Tao","Top-k Feature Selection Framework Using Robust 0–1 Integer Programming",2020,"","","","",114,"2022-07-13 09:39:06","","10.1109/TNNLS.2020.3009209","","",,,,,80,40.00,16,5,2,"Feature selection (FS), which identifies the relevant features in a data set to facilitate subsequent data analysis, is a fundamental problem in machine learning and has been widely studied in recent years. Most FS methods rank the features in order of their scores based on a specific criterion and then select the <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> top-ranked features, where <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> is the number of desired features. However, these features are usually not the top-<inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> features and may present a suboptimal choice. To address this issue, we propose a novel FS framework in this article to select the exact top-<inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> features in the unsupervised, semisupervised, and supervised scenarios. The new framework utilizes the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm as the matrix sparsity constraint rather than its relaxations, such as the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{1,2}$ </tex-math></inline-formula>-norm. Since the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm constrained problem is difficult to solve, we transform the discrete <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm-based constraint into an equivalent 0–1 integer constraint and replace the 0–1 integer constraint with two continuous constraints. The obtained top-<inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> FS framework with two continuous constraints is theoretically equivalent to the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm constrained problem and can be optimized by the alternating direction method of multipliers (ADMM). Unsupervised and semisupervised FS methods are developed based on the proposed framework, and extensive experiments on real-world data sets are conducted to demonstrate the effectiveness of the proposed FS framework.","",""
8,"Xiaoxuan Lu, Yushen Long, Han Zou, Chengpu Yu, Lihua Xie","Robust extreme learning machine for regression problems with its application to wifi based indoor positioning system",2014,"","","","",115,"2022-07-13 09:39:06","","10.1109/MLSP.2014.6958903","","",,,,,8,1.00,2,5,8,"We propose two kinds of robust extreme learning machines (RELMs) based on the close-to-mean constraint and the small-residual constraint respectively to solve the problem of noisy measurements in indoor positioning systems (IPSs). We formulate both RELMs as second order cone programming problems. The fact that feature mapping in ELM is known to users is exploited to give the needed information for robust constraints. Real-world indoor localization experimental results show that, the proposed algorithms can not only improve the accuracy and repeatability, but also reduce the deviations and worst case errors of IPSs compared with basic ELM and OPT-ELM based IPSs.","",""
5,"S. Yaqubi, M. Homaeinezhad","Solving predictive control problem of fast‐varying multivariable systems by incorporating unknown active dynamics generated by real‐time adaptive learning machine",2020,"","","","",116,"2022-07-13 09:39:06","","10.1111/exsy.12567","","",,,,,5,2.50,3,2,2,"Not only adaptive predictive control of switched systems is a computationally intensive procedure, it also involves various challenges in addressing the problems of robust stabilization and precise tracking. This study proposes new strategies to deal with the aforementioned issues (namely safe and precise control alongside with reduction of computational burden). The first contribution of this work is reduction of conservatism for described class of systems. Control of switched systems with undetectable switching signals is often conducted in worst case switching configuration to ensure robustness, which potentially results in conservative design. The issue of conservativeness is intensified in multi input‐multi output (MIMO) dynamical systems due to increased dimensions. However, attaining a robust control scheme for all switching configurations while ensuring precise response is inherently paradoxical. To overcome this issue, this study proposes a new dual‐mode algorithm where control modes corresponding to safety and precision are activated at appropriate stages of system response. This is conducted based on incorporation of an adaptive fuzzy‐wavelet neural network identification scheme in predictive control of MIMO switched systems. However, as convergence of the adaptive algorithm to actual system is attained after a finite period of time, a safe‐mode control algorithm is proposed to maintain quality of transient response in convergence period. In other words, the proposed algorithm operates in safe and precise control modes to ensure robust stability in the convergence period and non‐conservative design in steady‐state. Second major contribution of the work is reduction of calculation burden based on incorporation of a suboptimal control algorithm. To this end, we propose a predictive control scheme based on a suboptimal gradient‐descent based controller, calculating feasible stabilizing inputs instead of optimal inputs. Effects of dynamical variations are incorporated in the model predictive control framework for increased compatibility with high‐speed switching dynamics. Then, based on incorporation of dual‐mode algorithm, precise steady‐state performance is attained while preventing notable perturbations in dynamical discontinuities at switching.","",""
0,"H. Karimi, Ning Wang, Z. Man","Learning‐based robust control methodologies under information constraints",2022,"","","","",117,"2022-07-13 09:39:06","","10.1002/rnc.5973","","",,,,,0,0.00,0,3,1,"This editorial is dealing with the collection and report of some recent advances in learning-based robust control methodologies under information constraints. Both theoretical and practical contributions focusing on this theme are partly addressed in this special issue. Particularly, the latest progress of learning-based control in autonomous systems, large-scale systems, interconnected systems, robotics, industrial mechatronics, transportation and variously broad applications are introduced to the literature through this special issue. Within the past decade, various learning-based control technologies have prosperously emerged in both academic and industrial communities, and have expectantly performed remarkable superiority in terms of intelligence, autonomy, conciseness, reliability, resilience, and so forth. At the early stage, neural/fuzzy learning architectures have been widely deployed to online capture complex unknowns including unmodeled dynamics, uncertainties, and disturbances pertaining to the plant which might be a nonlinear system addressing the vehicles, robotics, transportations, mechatronics, informatics, circuits, and so forth. Recently, fruitful machine learning-based approaches, for example, reinforcement learning, deep learning, brain-inspired learning, have been incrementally promoted to innovate traditional learning-based intelligent control methodologies in both theoretical and practical sides. Especially, promising applications have also been developed to autonomous systems and robotics. In addition to booming advances in learning-based control philosophy, within a complex system, unexpected constraints would be inevitably involved, for instance, communication delays, sensor failures/noises, actuator nonlinearities, nonholonomic/underactuated dynamics, and so forth, especially within distributed systems.","",""
12,"Samuel Ackerman, E. Farchi, O. Raz, Marcel Zalmanovici, Parijat Dube","Detection of data drift and outliers affecting machine learning model performance over time",2020,"","","","",118,"2022-07-13 09:39:06","","","","",,,,,12,6.00,2,5,2,"A trained ML model is deployed on another `test' dataset where target feature values (labels) are unknown. Drift is distribution change between the training and deployment data, which is concerning if model performance changes. For a cat/dog image classifier, for instance, drift during deployment could be rabbit images (new class) or cat/dog images with changed characteristics (change in distribution). We wish to detect these changes but can't measure accuracy without deployment data labels. We instead detect drift indirectly by nonparametrically testing the distribution of model prediction confidence for changes. This generalizes our method and sidesteps domain-specific feature representation.  We address important statistical issues, particularly Type-1 error control in sequential testing, using Change Point Models (CPMs; see Adams and Ross 2012). We also use nonparametric outlier methods to show the user suspicious observations for model diagnosis, since the before/after change confidence distributions overlap significantly. In experiments to demonstrate robustness, we train on a subset of MNIST digit classes, then insert drift (e.g., unseen digit class) in deployment data in various settings (gradual/sudden changes in the drift proportion). A novel loss function is introduced to compare the performance (detection delay, Type-1 and 2 errors) of a drift detector under different levels of drift class contamination.","",""
0,"Yi Dou","Robust Graph Learning for Misbehavior Detection",2022,"","","","",119,"2022-07-13 09:39:06","","10.1145/3488560.3502213","","",,,,,0,0.00,0,1,1,"Recent years have witnessed the thriving of online services like social media, e-commerce, and e-finance. Those services facilitate our daily lives while breeding malicious actors like fraudsters and spammers to promote misinformation, gain monetary rewards, or reap end users' privacy. Graph-based machine learning models have been playing a critical and irreplaceable role in modeling and detecting online misbehavior. With the observation that misbehaviors are different from massive regular behaviors, the graph models can leverage the relationship between data entities from a holistic view and reveal suspicious behaviors as anomalous nodes/edges/subgraphs on the graph. In this proposal, we investigate the graph-based misbehavior detection models from an adversarial perspective, considering the adversarial nature of malicious actors and real-world factors that impair graph models' robustness. We first introduce two published works enhancing the robustness of several graph-based misbehavior detectors using reinforcement learning. Then, we propose to explore: 1) the robustness of graph neural networks for misinformation detection on social media; and 2) the general robustness of graph neural networks towards unknown perturbations.","",""
0,"A. A. Neto, L. Mozelli","Robust Longitudinal Control for Vehicular Autonomous Platoons Using Deep Reinforcement Learning",2022,"","","","",120,"2022-07-13 09:39:06","","10.48550/arXiv.2206.01175","","",,,,,0,0.00,0,2,1,"Summary In the last few years, researchers have applied machine learning strategies in the context of vehicular platoons to increase the safety and eﬃciency of cooperative transportation. Reinforcement Learning methods have been employed in the longitudinal spacing control of Cooperative Adaptive Cruise Control systems, but to date, none of those studies have addressed problems of disturbance rejection in such scenarios. Characteristics such as uncertain parameters in the model and external interferences may prevent agents from reaching null-spacing errors when traveling at cruising speed. On the other hand, complex communication topologies lead to speciﬁc training processes that can not be generalized to other contexts, demanding re-training every time the conﬁguration changes. Therefore, in this paper, we propose an approach to generalize the training process of a vehicular platoon, such that the acceleration command of each agent becomes independent of the network topology. Also, we have modeled the acceleration input as a term with integral action, such that the Convolutional Neural Network is capable of learning corrective actions when the states are disturbed by unknown eﬀects. We illustrate the eﬀectiveness of our pro-posal with experiments using diﬀerent network topologies, uncertain parameters, and external forces. Comparative analyses, in terms of the steady-state error and over-shoot response, were conducted against the state-of-the-art literature. The ﬁndings oﬀer new insights concerning generalization and robustness of using Reinforcement Learning in the control of autonomous platoons.","",""
4,"Tharanga K Wijethunga, Jelena Stojaković, Michael A. Bellucci, Xingyu Chen, A. Myerson, B. Trout","General Method for the Identification of Crystal Faces Using Raman Spectroscopy Combined with Machine Learning and Application to the Epitaxial Growth of Acetaminophen.",2018,"","","","",121,"2022-07-13 09:39:06","","10.1021/acs.langmuir.8b01791","","",,,,,4,1.00,1,6,4,"Crystal morphology is one of the key crystallographic characteristics that governs the macroscopic properties of crystalline materials. The identification of crystal faces, or face indexing, is an important technique that is used to get information regarding a crystal's morphology. However, it is mainly limited to single crystal X-ray diffraction (SCXRD) and it is often not applicable to products of routine crystallizations becasue it requires high quality single crystals in a narrow size range. To overcome the limitations of the SCXRD method, we have developed a robust and convenient Raman face indexing method based on work by Moriyama et al. This method exploits small but detectable differences in Raman spectra of crystal faces caused by different orientations of the crystallographic axis relative to the direction and polarization of the excitation laser beam. The method requires the compilation of a Raman spectral library for each compound and must be built and validated by SCXRD face indexing. Once the spectral library is available for a compound, the identity of unknown crystal faces (from any crystal that is larger than laser beam) can be inferred by collecting and comparing the Raman spectra to spectra within the library. We have optimized this approach further by developing a machine-learning algorithm that identifies crystal faces by performing a statistical comparison of the spectra in the Raman library and the Raman spectra of the unknown crystal faces. Here, we report the development of the Raman face indexing method and apply it to three different epitaxial systems: Acetaminophen (APAP) grown as an overlayer crystal on d-mannitol (MAN), d-galactose (GAL), and xylitol (XYL) substrates. For each of these epitaxial systems, the crystals were grown under various experimental conditions and have a wide range of sizes and quality. Using the Raman face indexing method, we were able to perform high-throughput indexing of a large number of crystals from different crystallization conditions, which could not be achieved using SCXRD or other analytical techniques.","",""
50,"A. Romagnoni, S. Jégou, K. Van Steen, G. Wainrib, J. Hugot, L. Peyrin-Biroulet, M. Chamaillard, J. Colombel, M. Cottone, M. D’Amato, R. D'Incà, J. Halfvarson, P. Henderson, A. Karban, N. Kennedy, M. Khan, M. Lémann, A. Levine, D. Massey, M. Milla, S. M. Ng, I. Oikonomou, H. Peeters, D. Proctor, J. Rahier, P. Rutgeerts, F. Seibold, L. Stronati, K. Taylor, L. Törkvist, Kullak Ublick, J. V. van Limbergen, A. van Gossum, M. Vatn, Hu Zhang, Wei Zhang, J. Andrews, P. Bampton, M. Barclay, T. Florin, R. Gearry, K. Krishnaprasad, I. Lawrance, G. Mahy, G. Montgomery, G. Radford-Smith, R. Roberts, L. Simms, K. Hanigan, A. Croft","Comparative performances of machine learning methods for classifying Crohn Disease patients using genome-wide genotyping data",2019,"","","","",122,"2022-07-13 09:39:06","","10.1038/s41598-019-46649-z","","",,,,,50,16.67,5,50,3,"","",""
0,"S. Priya, R. Annie Uthra","An Effective Concept Drift Detection Technique with Kernel Extreme Learning Machine for Email Spam Filtering",2020,"","","","",123,"2022-07-13 09:39:06","","10.1109/ICISS49785.2020.9316055","","",,,,,0,0.00,0,2,2,"The increase in the number of undesirable emails named spam has posed a major requirement to develop a highly dependent and robust antispam filters. This paper presents a novel email spam filtering technique with the capability of adapting with the dynamic environment. Concept drift detector attempts to determine the position of the concept drift in large data stream for replacing the baseline learner next to the modifications in the data distribution and therefore enhances accuracy. The proposed method detects the concept drift depending upon the computation of variation in the email content distribution using Statistical Test of Equal Proportions (STEPD) technique. The STEPD is a simpler commonly available model that identifies the concept drift with respect to a hypothesis test among two proportions. The SPEPD technique is used to determine the criteria of the concept drift for all unknown emails that assist the filtering technique in the recognition of the occurrence of the spam. In addition, the kernel extreme learning machine (KELM) based classification model is applied to classify the instances into two class labels namely spam and non-spam correspondingly. The experimental results of the STEPD-KELM model are tested against Enron dataset and the results are examined interms of distinct aspects. The experimental values indicated that the STEPD-KELM model has resulted to a maximum precision of 93.78%, recall of 96.54%, and accuracy of 95.33%.","",""
7,"H. Singh, Y. Seol, E. Myshakin","Prediction of gas hydrate saturation using machine learning and optimal set of well-logs",2020,"","","","",124,"2022-07-13 09:39:06","","10.1007/s10596-020-10004-3","","",,,,,7,3.50,2,3,2,"","",""
1,"D. Majumdar, Anand Mahato","Comparison of SIFT & SURF Corner Detector as Features and other Machine Learning Techniques for Identification of Commonly used Leaves",2018,"","","","",125,"2022-07-13 09:39:06","","","","",,,,,1,0.25,1,2,4,"1Dean R&D, Prof. & Head, Dept. of M.Tech CSE, Nitte Meenakshi Institute of Technology Yelahanka, Bangalore-560 064, India 2 Student, M.Tech Sem IV, Department of M.Tech CSE, Nitte Meenakshi Institute of Technology Yelahanka, Bangalore-560 064, India ----------------------------------------------------------------------------***---------------------------------------------------------------------------Abstract— Scope of properly identifying the leaves very vast. The properly identifying medicinal leaves for various cures, identifying poisonous plants using leaves, determining the usage of the plant using detected leaves are some of the possible usages of leaf identification. The aim is to build a methodology using various feature extraction techniques to extract features, clustering algorithm to cluster the features and decision trees as a classifier. All these methodologies put together to form an effective method to efficiently recognize the unknown leaf image using trained model. Feature extraction techniques like SIFT and SURF which are robust and provide matching in spite of the change in intensity, size or rotation of the object in the images. Effective corner points are chosen from the image from which magnitude and orientation of surrounding are used to build descriptor that is the vector of feature for each corner points. For Clustering the data, various partitional, hierarchical, density based methods are used to cluster the data which cluster the data with respect to inter-connectivity, similarity, closeness, etc. The clusters data is used to build the decision tree like C4.5 and CART which uses entropy and Gini index as the splitting criteria. The unknown image features are used to traverse the decision tree of the closest cluster to yield the matching image output from the training set.","",""
3,"Ricardo Chavarriaga, Hesam Sagha, H. Bayati, J. Millán, D. Roggen, K. Förster, Alberto Calatroni, G. Tröster, P. Lukowicz, D. Bannach, Marc Kurz, Gerold Hölzl, A. Ferscha","Robust activity recognition for assistive technologies: Benchmarking machine learning techniques",2010,"","","","",126,"2022-07-13 09:39:06","","","","",,,,,3,0.25,0,13,12,"An increasing need for healthcare provision and assistive technologies (AT) calls for the development of machine learning techniques able to cope with the variability inherent to real-world deployments. In the particular case of activity recognition applications sensor networks may be prone to changes at different levels ranging from sensor data variability to network reconfiguration. Robust methods are required to deal with those changes providing graceful degradation upon failure or self-configuration and adaptation capabilities that ensure their proper operation for long periods of time. Currently there is a lack of common tools and datasets that allow for replicable and fair comparison of different recognition approaches. We introduce a large database of human daily activities recorded in a sensor-rich environment. The database provides large amount of instances of the recorded activities using a significant number of sensors. In addition, we reviewed some of the techniques that have been proposed to cope with changes in the system, including missing data, sensor location/orientation change, as well as the possibility to exploit data from unknown discovered sensors. These techniques have been tested in the aforementioned datasets showing its suitability to emulate different sensor network configurations and recognition goals.","",""
36,"King Chung Ho, W. Speier, Haoyue Zhang, F. Scalzo, S. El-Saden, C. Arnold","A Machine Learning Approach for Classifying Ischemic Stroke Onset Time From Imaging",2019,"","","","",127,"2022-07-13 09:39:06","","10.1109/TMI.2019.2901445","","",,,,,36,12.00,6,6,3,"Current clinical practice relies on clinical history to determine the time since stroke (TSS) onset. Imaging-based determination of acute stroke onset time could provide critical information to clinicians in deciding stroke treatment options, such as thrombolysis. The patients with unknown or unwitnessed TSS are usually excluded from thrombolysis, even if their symptoms began within the therapeutic window. In this paper, we demonstrate a machine learning approach for TSS classification using routinely acquired imaging sequences. We develop imaging features from the magnetic resonance (MR) images and train machine learning models to classify the TSS. We also propose a deep-learning model to extract hidden representations for the MR perfusion-weighted images and demonstrate classification improvement by incorporating these additional deep features. The cross-validation results show that our best classifier achieved an area under the curve of 0.765, with a sensitivity of 0.788 and a negative predictive value of 0.609, outperforming existing methods. We show that the features generated by our deep-learning algorithm correlate with the MR imaging features, and validate the robustness of the model on imaging parameter variations (e.g., year of imaging). This paper advances magnetic resonance imaging analysis one-step-closer to an operational decision support tool for stroke treatment guidance.","",""
1,"Steve Göring, Rakesh Rao Ramachandra Rao, A. Raake","Prenc — Predict Number of Video Encoding Passes with Machine Learning",2020,"","","","",128,"2022-07-13 09:39:06","","10.1109/QoMEX48832.2020.9123108","","",,,,,1,0.50,0,3,2,"Video streaming providers spend huge amounts of processing time to get a quality-optimized encoding. While the quality-related impact may be known to the service provider, the impact on video quality is hard to assess, when no reference is available. Here, bitstream-based video quality models may be applicable, delivering estimates that include encoding-specific settings. Such models typically use several input parameters, e.g. bitrate, framerate, resolution, video codec, QP values and more. However, for a given bitstream, to determine which encoding parameters were selected, e.g., the number of encoding passes, is not a trivial task. This leads to our following research question: Given an unknown video bitstream, which encoding settings have been used? To tackle this reverse engineering problem, we introduce a system called prenc. Besides the use in video-quality estimation, such algorithms may also be used in other applications such as video forensics. We prove our concept by applying prenc to distinguish between one- and two-pass encoding. Starting from modeling the problem as a classification task, estimating bitstream-based features, we further describe a machine learning approach with feature selection to automatically predict the number of encoding passes for a given video bitstream. Our large-scale evaluation consists of 16 short movie type 4K videos that were segmented and encoded with different settings (resolutions, codecs, bitrates), so that we in total analyzed 131.976 DASH video segments. We further show that our system is robust, based on a 50% train and 50% validation approach without source video overlapping, where we get a classification performance of 65% F1 score. Moreover, we also describe the used bitstream-based features in detail, the feature pooling strategy and include other machine learning algorithms in our evaluation.","",""
32,"Haipeng Yao, Danyang Fu, Peiying Zhang, Maozhen Li, Yunjie Liu","MSML: A Novel Multilevel Semi-Supervised Machine Learning Framework for Intrusion Detection System",2019,"","","","",129,"2022-07-13 09:39:06","","10.1109/JIOT.2018.2873125","","",,,,,32,10.67,6,5,3,"Intrusion detection technology has received increasing attention in recent years. Many researchers have proposed various intrusion detection systems using machine learning (ML) methods. However, there are two noteworthy factors affecting the robustness of the model. One is the severe imbalance of network traffic in different categories and the other is the nonidentical distribution between training set and test set in feature space. This paper presents a multilevel intrusion detection model framework named multilevel semi-supervised ML (MSML) to address these issues. The MSML framework includes four modules: 1) pure cluster extraction; 2) pattern discovery; 3) fine-grained classification (FC); and 4) model updating. In the pure cluster module, we introduce an concept of “pure cluster” and propose a hierarchical semi-supervised ${k}$ -means algorithm with an aim to find out all the pure clusters. In the pattern discovery module, we define the “unknown pattern” and apply cluster-based method aiming to find those unknown patterns. Then a test sample is sentenced to labeled known pattern or unlabeled unknown pattern. The FC module can achieves FC for those unknown pattern samples. The model updating module provides a mechanism for retraining. KDDCUP99 dataset is applied to evaluate MSML. Experimental results show that MSML is superior to other existing intrusion detection models in terms of overall accuracy, F1-score, and unknown pattern recognition capability.","",""
0,"J. Majumdar, Anand Mahato","Identification of Commonly used Medicinal Leaves using Machine Learning Techniques with SIFT Corner Detector as Features",2018,"","","","",130,"2022-07-13 09:39:06","","10.26438/IJCSE/V6I2.341346","","",,,,,0,0.00,0,2,4,"Medicinal leaves carry a huge value and importance in the medical field which can be directly used or medicines are made for medicinal purposes to cure patients. With the variety of leaves present, the proper identification of the leaves is very difficult without prior knowledge and experience. Computer Vision can bring the accurate identification of such leaves using the various feature extraction techniques using leaf images. The aim is to build a methodology using various feature extraction techniques to extract features, clustering algorithm to cluster the features and decision trees as a classifier. Feature extraction techniques like SIFT key descriptors which are robust and provide matching in spite of the change in intensity, size or rotation of the object in the images. Effective corner points are chosen from the image from which magnitude and orientation of surrounding are used to build descriptor that is the vector of feature for each corner points. For clustering the data, various partitional, hierarchical, density based methods are used to cluster the data which cluster the data with respect to inter-connectivity, similarity, closeness, etc. The clusters data is used to build the decision tree like C4.5 and CART which uses entropy and Gini index as the splitting criteria. All these methodologies put together to form an effective method to efficiently recognize the unknown leaf image using trained model. Keywords— SIFT Corner points, Chameleon Clustering, Decision Tree Classifier.","",""
28,"Yan Zhou, Murat Kantarcioglu, B. Xi","A survey of game theoretic approach for adversarial machine learning",2019,"","","","",131,"2022-07-13 09:39:06","","10.1002/widm.1259","","",,,,,28,9.33,9,3,3,"The field of machine learning is progressing at a faster pace than ever before. Many organizations leverage machine learning tools to extract useful information from a massive amount of data. In particular, machine learning finds its application in cybersecurity that begins to enter the age of automation. However, machine learning applications in cybersecurity face unique challenges other domains rarely do—attacks from active adversaries. Problems in areas such as intrusion detection, banking fraud detection, spam filtering, and malware detection have to face  challenges of adversarial attacks that modify data so that malicious instances would evade detection by the learning systems. The adversarial learning problem naturally resembles a game between the learning system and the adversary. In such a game, both players would attempt to play their best strategies against each other while maximizing their own payoffs. To solve the game, each player would search for an optimal strategy against the opponent based on the prediction of the opponent's strategy choice. The problem becomes even more complicated in settings where the learning system may have to deal with many adversaries of unknown types. Applying game‐theoretic approach, robust learning techniques have been developed to specifically address adversarial attacks and the preliminary results are promising. In this review, we summarize these results.","",""
54,"Kento Hasegawa, Masaru Oya, M. Yanagisawa, N. Togawa","Hardware Trojans classification for gate-level netlists based on machine learning",2016,"","","","",132,"2022-07-13 09:39:06","","10.1109/IOLTS.2016.7604700","","",,,,,54,9.00,14,4,6,"Recently, we face a serious risk that malicious third-party vendors can very easily insert hardware Trojans into their IC products but it is very difficult to analyze huge and complex ICs. In this paper, we propose a hardware-Trojan classification method to identify hardware-Trojan infected nets (or Trojan nets) using a support vector machine (SVM). Firstly, we extract the five hardware-Trojan features in each net in a netlist. Secondly, since we cannot effectively give the simple and fixed threshold values to them to detect hardware Trojans, we represent them to be a five-dimensional vector and learn them by using SVM. Finally, we can successfully classify a set of all the nets in an unknown netlist into Trojan ones and normal ones based on the learned SVM classifier. We have applied our SVM-based hardware-Trojan classification method to Trust-HUB benchmarks and the results demonstrate that our method can much increase the true positive rate compared to the existing state-of-the-art results in most of the cases. In some cases, our method can achieve the true positive rate of 100%, which shows that all the Trojan nets in a netlist are completely detected by our method.","",""
4,"J. Panda, H. Warrior","Evaluation of machine learning algorithms for predictive Reynolds stress transport modeling",2021,"","","","",133,"2022-07-13 09:39:06","","10.1007/s10409-022-09001-w","","",,,,,4,4.00,2,2,1,"","",""
1,"Yuntian Chen, Dongxiao Zhang","Integration of knowledge and data in machine learning",2022,"","","","",134,"2022-07-13 09:39:06","","","","",,,,,1,1.00,1,2,1,"Scientiﬁc research’s mandate is to comprehend and explore the world, as well as to improve it based on experience and knowledge. Knowledge embedding and knowledge discovery are two signif-icant methods of integrating knowledge and data. Through knowledge embedding, the barriers between knowledge and data can be eliminated, and machine learning models with physical common sense can be established. Meanwhile, humans’ understanding of the world is always limited, and knowledge discovery takes advantage of machine learning to extract new knowledge from observations. Knowledge discovery can not only assist researchers to better grasp the nature of physics, but it can also support them in conducting knowledge embedding research. A closed loop of knowledge generation and usage are formed by combining knowledge embedding with knowledge discovery, which can improve the robustness and accuracy of models and uncover previously unknown scientiﬁc principles. This study summarizes and ana-lyzes extant literature, as well as identiﬁes research gaps and future opportunities.","",""
1,"H. Rashidi, J. Pepper, T. Howard, K. Klein, L. May, S. Albahra, B. Phinney, M. Salemi, N. Tran","COMPARATIVE PERFORMANCE OF TWO AUTOMATED MACHINE LEARNING PLATFORMS FOR COVID-19 DETECTION BY MALDI-TOF-MS",2022,"","","","",135,"2022-07-13 09:39:06","","10.1101/2022.02.02.22270298","","",,,,,1,1.00,0,9,1,"The 2019 novel coronavirus infectious disease (COVID-19) pandemic has resulted in an unsustainable need for diagnostic tests. Currently, molecular tests are the accepted standard for the detection of SARS-CoV-2. Mass spectrometry (MS) enhanced by machine learning (ML) has recently been postulated to serve as a rapid, high-throughput, and low-cost alternative to molecular methods. Automated ML is a novel approach that could move mass spectrometry techniques beyond the confines of traditional laboratory settings. However, it remains unknown how different automated ML platforms perform for COVID-19 MS analysis. To this end, the goal of our study is to compare algorithms produced by two commercial automated ML platforms (Platforms A and B). Our study consisted of MS data derived from 361 subjects with molecular confirmation of COVID-19 status including SARS-CoV-2 variants. The top optimized ML model with respect to positive percent agreement (PPA) within Platforms A and B exhibited an accuracy of 94.9%, PPA of 100%, negative percent agreement (NPA) of 93%, and an accuracy of 91.8%, PPA of 100%, and NPA of 89%, respectively. These results illustrate the MS methods robustness against SARS-CoV-2 variants and highlight similarities and differences in automated ML platforms in producing optimal predictive algorithms for a given dataset.","",""
0,"Karthikeyan V, S. S","Hybrid machine learning classification scheme for speaker identification",2022,"","","","",136,"2022-07-13 09:39:06","","10.1111/1556-4029.15006","","",,,,,0,0.00,0,2,1,"Motivated by the requirement to prepare for the next generation of “Automatic Spokesperson Recognition” (ASR) system, this paper applied the fused spectral features with hybrid machine learning (ML) strategy to the speech communication field. This strategy involved the combined spectral features such as mel‐frequency cepstral coefficients (MFCCs), spectral kurtosis, spectral skewness, normalized pitch frequency (NPF), and formants. The characterization of suggested classification method could possibly serve in advanced speaker identification scenarios. Special attention was given to hybrid ML scheme capable of finding unknown speakers equipped with speaker id‐detecting classifier technique, known as “Random Forest‐Support Vector Machine” (RF‐SVM). The extracted speaker precise spectral attributes are applied to the hybrid RF‐SVM classifier to identify/verify the particular speaker. This work aims to construct an ensemble decision tree on a bounded area with minimal misclassification error using a hybrid ensemble RF‐SVM strategy. A series of standard, real‐time speaker databases, and noise conditions are functionally tested to validate its performance with other state‐of‐the‐art mechanisms. The proposed fusion method succeeds in the speaker identification task with a high identification rate (97% avg) and lower equal error rate (EER) (<2%), compared with the individual schemes for the recorded experimental dataset. The robustness of the classifier is validated using the standard ELSDSR, TIMIT, and NIST audio datasets. Experiments on ELSDSR, TIMIT, and NIST datasets show that the hybrid classifier produces 98%, 99%, and 94% accuracy, and EERs were 2%, 1%, and 2% respectively. The findings are then compared with well‐known other speaker recognition schemes and found to be superior.","",""
0,"E. Alladio, Brando Poggiali, Giulia Cosenza, E. Pilli","Multivariate statistical approach and machine learning for the evaluation of biogeographical ancestry inference in the forensic field",2022,"","","","",137,"2022-07-13 09:39:06","","10.1038/s41598-022-12903-0","","",,,,,0,0.00,0,4,1,"","",""
0,"Lane Fitzsimmons, Maya Dewan, J. Dexheimer","Diversity in Machine Learning: A Systematic Review of Text-Based Diagnostic Applications",2022,"","","","",138,"2022-07-13 09:39:06","","10.1055/s-0042-1749119","","",,,,,0,0.00,0,3,1,"OBJECTIVE  As the storage of clinical data has transitioned into electronic formats, medical informatics has become increasingly relevant in providing diagnostic aid. The purpose of this review is to evaluate machine learning models that use text data for diagnosis and to assess the diversity of the included study populations.   METHODS  We conducted a systematic literature review on three public databases. Two authors reviewed every abstract for inclusion. Articles were included if they used or developed machine learning algorithms to aid in diagnosis. Articles focusing on imaging informatics were excluded.   RESULTS  From 2,260 identified papers, we included 78. Of the machine learning models used, neural networks were relied upon most frequently (44.9%). Studies had a median population of 661.5 patients, and diseases and disorders of 10 different body systems were studied. Of the 35.9% (N = 28) of papers that included race data, 57.1% (N = 16) of study populations were majority White, 14.3% were majority Asian, and 7.1% were majority Black. In 75% (N = 21) of papers, White was the largest racial group represented. Of the papers included, 43.6% (N = 34) included the sex ratio of the patient population.   DISCUSSION  With the power to build robust algorithms supported by massive quantities of clinical data, machine learning is shaping the future of diagnostics. Limitations of the underlying data create potential biases, especially if patient demographics are unknown or not included in the training.   CONCLUSION  As the movement toward clinical reliance on machine learning accelerates, both recording demographic information and using diverse training sets should be emphasized. Extrapolating algorithms to demographics beyond the original study population leaves large gaps for potential biases.","",""
0,"G. M. Anand, Heitor C. Megale, Sean H. Murphy, Theresa Weis, Zuwan Lin, Yichun He, Xiao Wang, Jia Liu, S. Ramanathan","Machine learning directed organoid morphogenesis uncovers an excitable system driving human axial elongation",2022,"","","","",139,"2022-07-13 09:39:06","","10.1101/2022.05.10.491358","","",,,,,0,0.00,0,9,1,"The human embryo breaks symmetry to form the anterior-posterior axis of the body. As the embryo elongates along this axis, progenitors in the tailbud give rise to axial tissues that generate the spinal cord, skeleton, and musculature. The mechanisms underlying human axial elongation are unknown. While ethics necessitate in vitro studies, the variability of human organoid systems has hindered mechanistic insights. Here we developed a bioengineering and machine learning framework that optimizes symmetry breaking by tuning the spatial coupling between human pluripotent stem cell-derived organoids. This framework enabled the reproducible generation of hundreds of axially elongating organoids, each possessing a tailbud and an epithelial neural tube with a single lumen. We discovered that an excitable system composed of WNT and FGF signaling drives axial elongation through the induction of a signaling center in the form of neuromesodermal progenitor (NMP)-like cells. The ability of NMP-like cells to function as a signaling center and drive elongation is independent of their potency to generate mesodermal cell types. We further discovered that the instability of the underlying excitable system is suppressed by secreted WNT inhibitors of the secreted frizzled-related protein (SFRP) family. Absence of these inhibitors led to the formation of ectopic tailbuds and branches. Our results identify mechanisms governing stable human axial elongation to achieve robust morphogenesis.","",""
0,"Shizhen Jin, Zhaofeng Guo, Dongli Liu, Yanhua Yang","A Study on the Application of Distributed System Technology-Guided Machine Learning in Malware Detection",2022,"","","","",140,"2022-07-13 09:39:06","","10.1155/2022/4977898","","",,,,,0,0.00,0,4,1,"In recent years, with the development of information technology, the Internet has become an essential tool for human daily life. However, as the popularity and scale of the Internet continue to expand, malware has also emerged as an increasingly widespread trend, and its development has brought many negative impacts to the society. As the number of types of malware is getting enormous, the attacks are constantly updated, and at the same time, the spread is very fast, causing more and more damage to the network, the requirements and standards for malware detection are constantly rising. How to effectively detect malware is a research trend; in order to tackle the new needs and problems arising from the development of malware, this paper proposes to guide machine learning algorithms to implement malware detection in a distributed environment: firstly, each detection node in the distributed network performs anomaly detection on the captured software information and data, then performs feature analysis to discover unknown malware and obtain its samples, updates the new malware features to all feature detection nodes in the whole distributed network, and trains the random forest-based machine learning algorithm for malware classification and detection, thus completing the global response processing capability for malware. By building a distributed system framework, the global capture capability of malware detection is enhanced to robustly respond to the increasing and rapid spread of malware, and machine learning algorithms are integrated into it to achieve effective detection of malware. Extended experiments on the Ember 2017 and Ember 2018 databases show that our proposed approach achieves advanced performance and effectively addresses the problem of malware detection.","",""
142,"John C. Duchi, Hongseok Namkoong","Learning Models with Uniform Performance via Distributionally Robust Optimization",2018,"","","","",141,"2022-07-13 09:39:06","","10.1214/20-aos2004","","",,,,,142,35.50,71,2,4,"A common goal in statistics and machine learning is to learn models that can perform well against distributional shifts, such as latent heterogeneous subpopulations, unknown covariate shifts, or unmodeled temporal effects. We develop and analyze a distributionally robust stochastic optimization (DRO) framework that learns a model providing good performance against perturbations to the data-generating distribution. We give a convex formulation for the problem, providing several convergence guarantees. We prove finite-sample minimax upper and lower bounds, showing that distributional robustness sometimes comes at a cost in convergence rates. We give limit theorems for the learned parameters, where we fully specify the limiting distribution so that confidence intervals can be computed. On real tasks including generalizing to unknown subpopulations, fine-grained recognition, and providing good tail performance, the distributionally robust approach often exhibits improved performance.","",""
21,"Ashkan Rezaei, Anqi Liu, Omid Memarrast, Brian D. Ziebart","Robust Fairness under Covariate Shift",2020,"","","","",142,"2022-07-13 09:39:06","","","","",,,,,21,10.50,5,4,2,"Making predictions that are fair with regard to protected group membership (race, gender, age, etc.) has become an important requirement for classification algorithms. Existing techniques derive a fair model from sampled labeled data relying on the assumption that training and testing data are identically and independently drawn (iid) from the same this http URL practice, distribution shift can and does occur between training and testing datasets as the characteristics of individuals interacting with the machine learning system -- and which individuals interact with the system -- change. We investigate fairness under covariate shift, a relaxation of the iid assumption in which the inputs or covariates change while the conditional label distribution remains the same. We seek fair decisions under these assumptions on target data with unknown labels.We propose an approach that obtains the predictor that is robust to the worst-case in terms of target performance while satisfying target fairness requirements and matching statistical properties of the source data. We demonstrate the benefits of our approach on benchmark prediction tasks.","",""
4,"Wei Xiao, C. Belta, C. Cassandras","Feasibility-Guided Learning for Robust Control in Constrained Optimal Control Problems",2019,"","","","",143,"2022-07-13 09:39:06","","","","",,,,,4,1.33,1,3,3,"Optimal control problems with constraints ensuring safety and convergence to desired states can be mapped onto a sequence of real time optimization problems through the use of Control Barrier Functions (CBFs) and Control Lyapunov Functions (CLFs). One of the main challenges in these approaches is ensuring the feasibility of the resulting quadratic programs (QPs) if the system is affine in controls. The recently proposed penalty method has the potential to improve the existence of feasible solutions to such problems. In this paper, we further improve the feasibility robustness (i.e., feasibility maintenance in the presence of time-varying and unknown unsafe sets) through the definition of a High Order CBF (HOCBF) that works for arbitrary relative degree constraints; this is achieved by a proposed feasibility-guided learning approach. Specifically, we apply machine learning techniques to classify the parameter space of a HOCBF into feasible and infeasible sets, and get a differentiable classifier that is then added to the learning process. The proposed feasibility-guided learning approach is compared with the gradient-descent method on a robot control problem. The simulation results show an improved ability of the feasibility-guided learning approach over the gradient-decent method to determine the optimal parameters in the definition of a HOCBF for the feasibility robustness, as well as show the potential of the CBF method for robot safe navigation in an unknown environment.","",""
2,"Yongxuan Zhang, Jun Yan","Domain-Adversarial Transfer Learning for Robust Intrusion Detection in the Smart Grid",2019,"","","","",144,"2022-07-13 09:39:06","","10.1109/SmartGridComm.2019.8909793","","",,,,,2,0.67,1,2,3,"The smart grid faces growing cyber-physical attack threats aimed at the critical systems and processes communicating over the complex cyber-infrastructure. Thanks to the increasing availability of high-quality data and the success of deep learning algorithms, machine learning (ML)-based detection and classification have been increasingly effective and adopted against sophisticated attacks. However, many of these techniques rely on the assumptions that the training and testing datasets share the same distribution and the same class labels in a stationary environment. As such assumptions may fail to hold when the system dynamics shift and new threat variants emerge in a non-stationary environment, the capability of trained ML models to adapt in complex operating scenarios will be critical to their deployment in real-world smart grid communications. To this aim, this paper proposes a domain-adversarial transfer learning framework for robust intrusion detection against smart grid attacks. The framework introduces domain-adversarial training to create a mapping between the labeled source domain and the unlabeled target domain so that the classifiers can learn in a new feature space against unknown threats. The proposed framework with different baseline classifiers was evaluated using a smart grid cyber-attack dataset collected over a realistic hardware-in-the- loop security testbed. The results have demonstrated effective performance improvements of trained classifiers against unseen threats of different types and locations.","",""
0,"A. Barrett, M. Balme, J., Wright, M. Woods, S. Karachalios, M. Malinowski","CLASSIFYING PLANETARY SURFACES USING MACHINE LEARNING",2022,"","","","",145,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,7,1,"Introduction: Deep learning (e.g. [1]) convolutional neural networks were trained to discriminate meter scale variations in surface texture in satellite images of planetary surfaces [2]. Two versions of the model were trained, one classifying surface textures and aeolian bedforms in HiRISE images of Mars [3]. The second to discriminate between blocky ejecta and textures indicative of impact melt in LROCNAC images of the Moon [4]. An ever increasing volume of remote sensing data is being returned from the Moon and Mars, and while this has the potential to allow large scale mapping efforts at a greater spatial resolution than ever before, it is increasingly challenging for all relevant data to be surveyed in a reasonable amount of time. The networks classify surface textures based on morphological criteria rather than making determinations of perceived geological origin. Automating the production of a geological map is not yet possible and would be counter-productive, since the purpose of a mapping effort is as much to build understanding of the geological history of a site through discussion and exploration as it is to classify the surface. Rather our aim is to use machine learning to augment the human mapping workflow, and speed up the initial surveying needed for such an effort. A network performs “triage” on unmanageably large datasets, indicating to a human operator areas where predefined surface texture assemblages indicate that features of interest could be present. NOAH-H: The Novelty or Anomaly Hunter – HiRiSE (NOAH-H) [5] was developed as part of the ExoMars Rosalind Franklin [6] landing site selection process. It was trained on ~1500 example framelets, selected from across Arabia Terra. It performs semantic segmentation at a pixel scale, identifying surface textures as one of 14 ontological classes (fig 1). Seven surface classes define roughness types, six classes describe aeolian bedforms and the final class describes patches of boulders. The classification scheme is hierarchical. Care was taken to ensure that the class definitions were purely morphological, providing a robust descriptive level. These descriptive classes were then grouped into thematic categories such as bedrock, non-bedrock etc. to form the interpretive layer of the system. This ensures that the classification does not require contextual evidence which would be unknown to the network. While still providing the scientist the information needed for interpretation.","",""
1,"Xinglong Zhang, Jiahang Liu, Xin Xu, Hong Chen","Robust Learning-based Predictive Control for Constrained Nonlinear Systems",2019,"","","","",146,"2022-07-13 09:39:06","","","","",,,,,1,0.33,0,4,3,"The integration of machine learning methods and Model Predictive Control (MPC) has received increasing attention in recent years. In general, learning-based predictive control (LPC) is promising to build data-driven models and solve the online optimization problem with lower computational costs. However, the robustness of LPC is difficult to be guaranteed since there will be uncertainties due to function approximation used in machine learning algorithms. In this paper, a novel robust learning-based predictive control (r-LPC) scheme is proposed for constrained nonlinear systems with unknown dynamics. In r-LPC, the Koopman operator is used to form a global linear representation of the unknown dynamics, and an incremental actor-critic algorithm is presented for receding horizon optimization. To realize the satisfaction of system constraints, soft logarithmic barrier functions are designed within the learning predictive framework. The recursive feasibility and stability of the closed-loop system are discussed under the convergence arguments of the approximation algorithms adopted. Also, the robustness property of r-LPC is analyzed theoretically by taking into consideration the existence of perturbations on the controller due to possible approximation errors. Simulation results with the proposed learning control approach for the data-driven regulation of a Van der Pol oscillator system have been reported, including the comparisons with a classic MPC and an infinite-horizon Dual Heuristic Programming (DHP) algorithm. The results show that the r-LPC significantly outperforms the DHP algorithm in terms of control performance and can be comparative to the MPC in terms of regulating control as well as energy consumption. Moreover, its average computational cost is much smaller than that with the MPC in the adopted environment.","",""
15,"Suyun Liu, L. Vicente","The stochastic multi-gradient algorithm for multi-objective optimization and its application to supervised machine learning",2019,"","","","",147,"2022-07-13 09:39:06","","10.1007/S10479-021-04033-Z","","",,,,,15,5.00,8,2,3,"","",""
1,"Hyun Kwon, Sanghyun Lee","Textual Adversarial Training of Machine Learning Model for Resistance to Adversarial Examples",2022,"","","","",148,"2022-07-13 09:39:06","","10.1155/2022/4511510","","",,,,,1,1.00,1,2,1,"Deep neural networks provide good performance for image recognition, speech recognition, text recognition, and pattern recognition. However, such networks are vulnerable to attack by adversarial examples. Adversarial examples are created by adding a small amount of noise to an original sample in such a way that no problem is perceptible to humans, yet the sample will be incorrectly recognized by a model. Adversarial examples have been studied mainly in the context of images, but research has expanded to include the text domain. In the textual context, an adversarial example is a sample of text in which certain important words have been changed so that the sample will be misclassified by a model even though to humans it is the same as the original text in terms of meaning and grammar. In the text domain, there have been relatively few studies on defenses against adversarial examples compared with the number of studies on adversarial example attacks. In this paper, we propose an adversarial training method to defend against adversarial examples that target the latest text model, bidirectional encoder representations from transformers (BERT). In the proposed method, adversarial examples are generated using various parameters and then are applied in additional training of the target model to instill robustness against unknown adversarial examples. Experiments were conducted using five datasets (AG’s News, a movie review dataset, the IMDB Large Movie Review Dataset (IMDB), the Stanford Natural Language Inference (SNLI) corpus, and the Multi-Genre Natural Language Inference (MultiNLI) corpus), with TensorFlow as the machine learning library. According to the experimental results, the baseline model had an accuracy of 88.1% on the original sentences and an accuracy of 9.2% on the adversarial sentences, whereas the model that underwent the proposed training method maintained an average accuracy of 87.2% on the original sentences and had an average accuracy of 22.5% on the adversarial sentences.","",""
0,"Suleyman Emre Isik, Ali Eren Aytekin, Halil Vurus","A machine learning approach for abstraction and reasoning problems without large amounts of data",2022,"","","","",149,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,3,1,"Journal of Emerging Investigators • www.emerginginvestigators.org level abstraction-reasoning ability which makes it difficult for algorithms to handle volatile and hard-to-predict real-life problems. The problems caused by this task-based nature necessitated flexibility and robustness for certain broader subfields of AI, such as L5 self-driving, domestic robotics, or personal assistants; there is even increasing interest in generality itself (e.g., developmental robotics, artificial general intelligence) (2, 3). The first and most important step to take in order to offer an approach that is closer to human intelligence is to examine the concept of intelligence and to define it in the most useful way. Various definitions have been made for intelligence in the past. Legg and Hutter summarized the definitions made in the context of artificial intelligence research as follows: ""Intelligence measures a person's ability to achieve goals in a wide and varied environment (4)."" Two main characteristics are emphasized here: a task-goal focus and generalizability to a wide range of environments. Accordingly, while human intelligence can perform tasks with its high ability, these abilities can also be generalized for new tasks in new environments (skill acquisition). This feature is a mechanism that human nature has developed in line with evolutionary psychology to solve new unknown tasks and problems (5, 6). In the direction of the development of AI, many approaches have emerged to develop and evaluate AI models. One of them is the human observational approach that examines, judges, and scores the system’s inputs and outputs. This is a highly subjective, difficult, and expensive method to automate. White-box analysis, on the other hand, is inspecting the implementation of the system to determine its input-output response and score it (e.g., an algorithm that plays “Connect Four”) (7). Peer confrontation, for example, is having the system compete against either other AIs or humans. This is the preferred mode of evaluation for player-versus-player games, such as chess. The benchmarking approach, which is based on enabling the system through algorithms to produce outputs for a ""test set"" of inputs (or environments) for which the desired outcome is known (solvable by humans), is another of the most valuable approaches for the evaluation of artificial intelligence. In particular, it is reproducible (test set fixed), scalable (cheap to run the evaluation multiple times), easy to set up, and flexible enough to be applied to a wide variety of possible tasks (8). For this reason, benchmarking has been an important part of progress in artificial intelligence A machine learning approach for abstraction and reasoning problems without large amounts of data","",""
0,"A. Studier-Fischer, S. Seidlitz, J. Sellner, B. Özdemir, M. Wiesenfarth, L. Ayala, J. Odenthal, S. Knödler, K. Kowalewski, C. Haney, I. Camplisson, M. Dietrich, Karsten Schmidt, G. A. Salg, H. Kenngott, T. Adler, N. Schreck, A. Kopp-Schneider, Klaus Maier-Hein, L. Maier-Hein, B. Müller-Stich, F. Nickel","Spectral organ fingerprints for machine learning-based intraoperative tissue classification with hyperspectral imaging in a porcine model",2022,"","","","",150,"2022-07-13 09:39:06","","10.1038/s41598-022-15040-w","","",,,,,0,0.00,0,22,1,"","",""
4,"T. Schmid","Deconstructing the Final Frontier of Artificial Intelligence: Five Theses for a Constructivist Machine Learning",2019,"","","","",151,"2022-07-13 09:39:06","","","","",,,,,4,1.33,4,1,3,"Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subirón 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world – and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (Méhaut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as “hypothetical constructs” (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n","",""
3,"Minsung Hong, R. Akerkar","Analytics and Evolving Landscape of Machine Learning for Emergency Response",2019,"","","","",152,"2022-07-13 09:39:06","","10.1007/978-3-030-15628-2_11","","",,,,,3,1.00,2,2,3,"","",""
4,"Mohammed Ali Khan, A. Haque, V. S. B. Kurukuru","Machine Learning Based Islanding Detection for Grid Connected Photovoltaic System",2019,"","","","",153,"2022-07-13 09:39:06","","10.1109/ICPECA47973.2019.8975614","","",,,,,4,1.33,1,3,3,"This paper focus on developing a new islanding detection method with the help of machine learning and signal processing technique. The islanding detection method make sure that there is a proper remote monitoring of the grid integrated photovoltaic (PV) system. In case of grid fault or maintained of the grid, a proper informed signal is provided to various distributed generation (DG) networks so that they can disconnect with the grid and operate in isolated mode. A simulation of 1kW grid connected PV system is performed. The signal such as voltage, current and frequency are recorded at point of common coupling (PCC). The feature of recorded signals are extracted using wavelet transformation. The extracted features are used to form a islanding scenarios matrix. The matrix is further utilized to train a classifier using machine learning algorithm. From the result it can be observed that the trained classifier depicted 97.9% training accuracy with a training time of 16.9 sec which is better when compared with the literature. Further the trained classifier is subjected to test with an unknown islanding condition to observe the robustness of the classifier.","",""
3,"Kyemyung Park, T. Prüstel, Yong Lu, J. Tsang","Machine learning of stochastic gene network phenotypes",2019,"","","","",154,"2022-07-13 09:39:06","","10.1101/825943","","",,,,,3,1.00,1,4,3,"A recurrent challenge in biology is the development of predictive quantitative models because most molecular and cellular parameters have unknown values and realistic models are analytically intractable. While the dynamics of the system can be analyzed via computer simulations, substantial computational resources are often required given uncertain parameter values resulting in large numbers of parameter combinations, especially when realistic biological features are included. Simulation alone also often does not yield the kinds of intuitive insights from analytical solutions. Here we introduce a general framework combining stochastic/mechanistic simulation of reaction systems and machine learning of the simulation data to generate computationally efficient predictive models and interpretable parameter-phenotype maps. We applied our approach to investigate stochastic gene expression propagation in biological networks, which is a contemporary challenge in the quantitative modeling of single-cell heterogeneity. We found that accurate, predictive machine-learning models of stochastic simulation results can be constructed. Even in the simplest networks existing analytical schemes generated significantly less accurate predictions than our approach, which revealed interesting insights when applied to more complex circuits, including the extensive tunability of information propagation enabled by feedforward circuits and how even single negative feedbacks can utilize stochastic fluctuations to generate robust oscillations. Our approach is applicable beyond biology and opens up a new avenue for exploring complex dynamical systems.","",""
0,"","Runtime Verification for Critical Machine Learning Applications",2019,"","","","",155,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,0,3,"In the last decade, the application of Machine Learning (ML) has encountered an increasing interest in various applicative domains, especially for a wide panel of complex tasks (e.g. image-based pedestrian detection) classically performed by human operators (e.g. the driver). In ML, the objective is to synthesize an intended function (e.g., detect a pedestrian on an image) through a set of examples (images of road). The massive usage of such techniques has demonstrated its effectiveness way beyond other classical methods. Obviously, the designers of critical systems would like to benefit from the effectiveness of ML-based models mainly for complex image processing and model reduction. But, above effectiveness, the designers of critical systems must demonstrate that the obtained models are reasonably safe. Providing elements demonstrating the safety of a system is a classical issue addressed by various techniques tailored to the nature of the system, and covered by many safety standards (DO178C in aeronautics, ISO26262 in automative, IEC61508 in electronics, etc.). Nevertheless, the specificities of ML-based software disclose new safety threats that are not addressed by classical techniques. Despite very good results during training and testing of a ML software, it is not possible to provide sufficient guarantees that the training data set would be sufficient for expected real life situations, that during operational life the system may not face adversary situations (situation slightly different from training ones, but which lead to a complete different result of the ML, also called adversaries attacks), or that the distribution of situations may be different from the ones during training (distribution shift). All these threats are a major brake to ML deployment in safety critical applications. Most of works are focusing on the training data quality in order to increase robustness of the ML algorithm. However, to avoid overfitting, it is accepted that developing the dataset is limited, and a promising approach is to monitor the system at runtime, during operational life, in order to keep the system in a safe state, despite errors of the ML. A first approach inspired from fault tolerance and close to safety monitoring [3], is to adapt the simplex architecture to the monitoring of a neural controller, using a decision block able to detect an error and to switch from the neural controller to a high assurance controller (but less performant) [4]. Some works as [1] may be used to monitor the distance of input/ouput distribution during the exploitation vs the one observed during the learning phase and raise an alert when a “significant gap” is observed. Other works like [2], dedicated to Neural Networks, propose to collect neuron activation patterns during the learning phase and, thanks to an online monitoring, detect the occurrence of an unknown activation pattern that may indicate an erroneous prediction. All these works are ongoing, with very preliminary results, and actually no safety model is integrated in such proposals. We propose in this post-doc to specify, implement and verify a new runtime verification approach for ML, an adversarial runtime monitor. This approach is based on adversaries generated at runtime, and used to assess if the ML maybe fooled in an unsafe state. This might lead the monitor to detect if the ML is in a potential unsafe erroneous state, or in a potential erroneous state but safe. Once such a monitor would be designed, we also plan to use formal methods (verification) to prove the correctness of the monitor. This work will be applied to a case study, a ML software for drone collision avoidance studied and deployed in the context of the Delta project.","",""
1,"Andreas Wunsch, T. Liesch, S. Broda","Uncover Similarities of Groundwater Dynamics with Machine Learning based Hydrograph Clustering (Oral Talk IN43A-07)",2019,"","","","",156,"2022-07-13 09:39:06","","","","",,,,,1,0.33,0,3,3,"Understanding and characterizing groundwater system properties is of great importance to develop sustainable groundwater management strategies. For this purpose, groundwater hydrographs are a valuable source of knowledge, since they contain information about system properties (e.g. aquifer type), artificial (e.g. withdrawal/infiltration) and natural environmental factors (e.g. groundwater-streamflow interaction). Such factors interact and superimpose temporally and spatially, which makes determining the individual contributions a challenging task. However, understanding spatial dynamics patterns is a precious source of information for this purpose. Generally, in many regions, large amounts of groundwater data with high resolution in time and space are available but lack an adequate set of tools for analysis. Data driven models are possibly suited to fill in this gap. We developed a machine learning based ensemble-modelling approach to characterize and cluster groundwater hydrographs on regional scale according to their dynamics. We apply feature-based clustering to reduce data quality requirements and to improve exploitation of heterogeneous datasets. Such features describe hydrograph dynamics and serve as surrogates for clustering based on Self-Organizing-Maps and DS2L-SOM-enrichment for cluster determination. Ensemble modeling assures highly robust cluster results, even for real world observational networks undergoing changes. The test area of the method is the Upper Rhine Graben in Germany/France, using more than 1800 weekly sampled hydrographs in the period of 1986 to 2016. The majority shows lengths of almost 30 years, minimum length is six years. Results show that our approach is capable to identify homogeneous groups of hydrograph dynamics. The resulting clusters showed both known and unknown patterns, of which some correspond to certain environmental factors. However, we also discovered new patterns with unknown origin, which need further examination. Possible application of the found groundwater patterns could be for example regional groundwater forecasting by selecting and predicting representative group members. By adapting the describing features, this data-driven method is easily transferrable to other time-series-clustering frameworks.","",""
1,"Vincent Dangla, Christian Soize, Guilherme Cunha, M. Kassem, A. Mosson, B. V. D. Nieuwenhof","A probabilistic learning on manifolds as a new tool in machine learning and data science with applications in computational mechanics",2019,"","","","",157,"2022-07-13 09:39:06","","","","",,,,,1,0.33,0,6,3,"In Machine Learning (generally devoted to big-data case), the predictive learning (or the supervised learning) approach consists in identifying/learning a random mapping F: w↦ q = F(w), in which the parameters vector w (input) is modelled by a random vector W with known probability distribution Pw(dw) and where the vector of quantities of interest q (outputs) is the non-Gaussian random variable Q = F(W) = f(W,U) whose probability distribution is unknown, given an initial dataset (or training set) DN = {(wj,qj), j=1,…N} of N independent realizations of random vector (W,Q). The measurable mapping f is deterministic and U is a random vector whose probability distribution is known. The approach of probabilistic learning on manifold (recently introduced) will be presented, which allows for constructing a generator of an estimation of the joint probability distribution PW,Q(dw,dq; N) using only DN, which completely characterizes random mapping F. In this framework, novel computational statistical tools will be presented for the small-data challenge for which N is relatively small and consequently, is not sufficient large for constructing converged statistical estimates. In particular, we will present (1) the identification of the optimal independent component partition of the non-Gaussian random vector, (2) the learning from DN for which additional information is available based either on a nonparametric Bayesian approach or on Information Theory. Several applications will be presented such as, the identification of non-Gaussian random fields for random media, the Bayes inference with probabilistic learning, the robust design of an implant in a biological tissue at mesoscale, the nonparametric model-form uncertainties (i) in nonlinear solid dynamics applied a to MEMS and (ii) in nonlinear computational fluid dynamics applied to a Scramjet, nonconvex optimization under uncertainties.    [1] C. Soize, R. Ghanem, Data-driven probability concentration and sampling on manifold, Journal of Computational Physics, 321, 242-258 (2016).  [2] C. Soize, Optimal partition in terms of independent random vectors of any non-Gaussian vector defined by a set of realizations, SIAM/ASA Journal on Uncertainty Quantification, 5(1), 176-211 (2017).  [3] R. Ghanem, C. Soize, Probabilistic nonconvex constrained optimization with fixed number of function evaluations, International Journal for Numerical Methods in Engineering, 113(4), 719-741 (2018).  [4] C. Soize, Design optimization under uncertainties of a mesoscale implant in biological tissues using a probabilistic learning algorithm, Computational Mechanics, 62(3), 477-497 (2018).  [5] C. Soize, C. Farhat, Probabilistic learning for model-form uncertainties in nonlinear computational mechanics, International Journal for Numerical Methods in Engineering, Accepted 24 October 2018.  [6] C. Soize, R. Ghanem, C. Safta, X. Huan, Z.P. Vane, J. Oefelein, G. Lacaze, H.N. Najm, Enhancing model predictability for a scramjet using probabilistic learning on manifold, AIAA Journal, Accepted 13 September 2018.","",""
28,"K. Javed, R. Gouriveau, N. Zerhouni, R. Zemouri, Xiang Li","Robust, reliable and applicable tool wear monitoring and prognostic: Approach based on an improved-extreme learning machine",2012,"","","","",158,"2022-07-13 09:39:06","","10.1109/ICPHM.2012.6299516","","",,,,,28,2.80,6,5,10,"Although efforts in this field are significant around the world, real prognostics systems are still scarce in industry. Indeed, it is hard to provide efficient approaches that are able to handle with the inherent uncertainty of prognostics and nobody is able to a priori ensure that an accurate prognostic model can be built. As for an example of remaining problems, consider data-driven prognostics approaches: how to ensure that a model will be able to face with inputs variation with respect to those ones that have been learned, how to ensure that a learned-model will face with unknown data, how to ensure convergence of algorithms, etc. In other words, robustness, reliability and applicability of a prognostic approach are still open areas. Following that, the aim of this paper is to address these challenges by proposing a new neural network (structure and algorithm) that enhances reliability of RUL estimates while improving applicability of the approach. Robustness, reliability and applicability aspects are first discussed and defined according to literature. On this basis, a new connexionist system is proposed for prognostics: the Improved-Extreme Learning machine (Imp-ELM). This neural network, based on complex activation functions, enables to reduce the influence of human choices and initial parameterization, while improving accuracy of estimates and speeding the learning phase. The whole proposition is illustrated by performing tests on a real industrial case of cutting tools from a Computer Numerical Control (CNC) machine. This is achieved by predicting tool condition (wear) in terms of remaining cuts successfully made. Thorough comparisons with adaptive neuro fuzzy inference system (ANFIS) and existing ELM algorithm are also given. Results show improved robustness, reliability and applicability performances.","",""
9,"A. El Bakri, M. Koumir, I. Boumhidi","Extreme learning machine-based non-linear observer for fault detection and isolation of wind turbine",2019,"","","","",159,"2022-07-13 09:39:06","","10.1080/1448837X.2019.1578044","","",,,,,9,3.00,3,3,3,"ABSTRACT This paper presents a robust fault detection and isolation (FDI) scheme for a variable speed wind turbine. The proposed scheme (extreme learning machine–state-dependent differential Riccati equation (ELM-SDDRE)) is an observer model-based approach, especially, a non-linear observer using SDDRE based on an improved model of the wind turbine by using the ELM. The standard SDDRE can be used for small model uncertainties. However, when the uncertainties are large, the SDDRE cannot detect and isolate the faults. The main objective of the ELM is the prediction of unknown nominal model dynamics to construct a new improved nominal model used by the observer for FDI. This makes the effect of uncertainties weak and consequently allows better faults detection. The faults considered in this paper are sensor faults in the rotating speeds of the rotor and generator outputs. The effectiveness of the proposed approach is illustrated through simulation.","",""
0,"Ross Boczar","Performance Guarantees in Learning and Robust Control",2019,"","","","",160,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,1,3,"Author(s): Boczar, Ross J | Advisor(s): Recht, Benjamin | Abstract: As the systems we control become more complex, first-principle modeling becomes either impossible or intractable, motivating the use of machine learning techniques for the control of systems with continuous action spaces. As impressive as the empirical success of these methods have been, strong theoretical guarantees of performance, safety, or robustness are few and far between. This manuscript takes a step towards such providing such guarantees by establishing finite-data performance guarantees for identifying and controlling fully- or partially-unknown dynamical systems.In this manuscript, we explore three different viewpoints that each provide different quantitative guarantees of performance. First, we present a generalization of the classical theory of integral quadratic constraints. This generalization leads to a tractable computational procedure for finding exponential stability certificates for partially-unknown feedback systems. Second, we present non-asymptotic lower and upper bounds for core problems in the field of system identification. Finally, using the recently developed system-level synthesis framework and tools from high-dimensional statistics, we establish finite-sample performance guarantees for robust output-feedback control of an unknown dynamical system.","",""
0,"Bruce Hajeck","Rademacher Complexity for Adversarially Robust Generalization ∗ Sourya Basu Course Project : ECE 543-Statistical Learning Theory Course instructor : Prof .",2019,"","","","",161,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,1,3,"Machine learning algorithms have shown incredible performance in several inference tasks such as image classi cation, speech recognition, and game playing [2 4]. Although these algorithms work excellently in natural settings (i.e., test data generated in the same way as the train data), their performance drop signi cantly when the test data is corrupted adversarially, at times in a manner that is imperceptible by human beings [5]. Such examples where the input to a trained machine learning model is perturbed so as to increase the probability of wrong inference is called adversarial examples. In image classi cation, adversarial examples might include something as simple as adding small perturbations imperceptible to humans, changing surrounding areas of the main object in an image or even simple rotations or translations [6,7]. The existence of such adversarial examples is alarming in several situations, because of the use machine learning algorithms in several critical examples, such as medical diagnosis or selfdriving cars, e.g. self-driving cars that rely on real-time image recognition might end up with making wrong predictions in the presence of, say, di erent lighting or weather than the one it is trained in, or medical diagnosis might end up predicting some hence making it important for us to analyze the performance of machine learning algorithms in the presence of adversaries. The existence of such examples raise serious questions about the robustness of the existing state-of-the-art machine learning algorithm and risk of using them in critical applications. The existence of such adversarial examples have motivated researchers to come up with algorithms that can defend against particular examples e ectively, however, this also resulted in creation of more and more of such adversarial examples which the existing algorithms does not perform well on, to the extent that there is a virtual race between designing adversarial examples and designing algorithms that can defend them. As reported in the paper [1], in the current scenario, the attackers generating adversarial examples are winning, e.g. it has been shown in [8], that carefully designed gradient-based algorithms may fool most of the existing defense algorithms. Hence, instead of designing defense algorithms against particular adversarial examples, this paper [1] takes a di erent approach and establishes theoretical guarantees on the performance of learning algorithms in the presence of adversaries during the testing phase. The paper [1] uses adversarial training as it appears to be quite e ective against adversarial examples as described in literature [10,11] which optimizes over adversarial loss during the training phase as described next. Let (x, y) ∈ X × Y be the data points drawn according to some unknown distribution D. Let F be a hypothesis class and l(f(x), y) be the loss associated with f ∈ F . This paper [1] considers l∞ adversarial attacks wherein, an -attack can be described as follows: the adversary is allowed to observe the trained model, and given a test data, x, the adversary nds a data point x′ such that ‖x−x‖∞ ≤ and l(f(x′), y) is maximized. Thus, to have better test performance, the learning algorithm should solve the following optimization problem, called as the adversarial risk minimization problem:","",""
0,"T. Ogunfunmi, R. Ramachandran, R. Togneri, B. Y. Smolenski, Visar Berisha","Guest Editorial: Algorithms and Architectures for Machine Learning Based Speech Processing",2019,"","","","",162,"2022-07-13 09:39:06","","10.1007/S00034-019-01161-7","","",,,,,0,0.00,0,5,3,"","",""
0,"Benjamin I Goodlich","Machine learning algorithms for the automatic detection and classification of physical activity in children with cerebral palsy who use mobility aids for ambulation",2019,"","","","",163,"2022-07-13 09:39:06","","10.25904/1912/2123","","",,,,,0,0.00,0,1,3,"Background: Literature related to objective measurement of habitual physical activity (PA) disproportionately over represents children with Cerebral Palsy (CP) who are ambulant. Consequently, it is unknown if methods used to examine PA, such as machine learning models built on accelerometer data, are able to accurately detect PA in children with CP who use mobility aids for ambulation. Objective: To develop and test machine learning models used for the automatic detection and classification of PA type in children with CP who use mobility aids for ambulation. Methods: Eleven children and adolescents with CP, age 11±3yrs (range 6-16yrs); six females; Gross Motor Function Classification System (GMFCS) III: n=5 and IV: n=6 participated. Participants completed six PA trials of increasing intensity while wearing an ActiGraph GT3X+ accelerometer on the wrist, hip and thigh. PA trials included: supine rest, seated colouring, seated ball throwing, overground walking with a mobility aid, wheelchair propulsion and riding on a modified tricycle. Decision Tree (DT), Support Vector Machine (SVM) and Random Forest (RF) classifiers were trained on 40 features in the vector magnitude of raw acceleration signal using 5s non-overlapping windows. Performance was evaluated using leave-one-subject-out cross validation. Comparisons of performance were subsequently made between all single placement models, all combinations of two placement models, and models trained on data from all three placements. Results: The best performing single-placement model was a RF classifier trained on wrist features, yielding an overall prediction accuracy of 79%. The best performing model built on a combination of two placements was a RF classifier trained on wrist and hip features, yielding an overall prediction accuracy of 92%. The combinations of multiple accelerometer placements were significantly more accurate than a single monitor alone. Models based on the combination of two placements were more accurate than those based on a combination of three placements; however, this difference was not significant. Limitations: The PA protocol consisted of structured activity trials performed in a controlled, clinical environment. Thus, the performance of the models under free living conditions require further investigation. The sample size used may limit the generalisability and robustness of the findings given the variability in movement patterns of the population of interest. Conclusions: Machine learning techniques afford robust and accurate classification of PA in children with CP who use mobility aids for ambulation (GMFCS III & IV) within a laboratory setting. This is significant, as it is the first study to develop methods for objectively measuring habitual PA in this population. Future research should investigate performance of the methods utilised in the current project in children engaged in free living conditions.","",""
3,"Noureldin Laban, B. Abdellatif, H. M. Ebeid, H. Shedeed, M. Tolba","Machine Learning for Enhancement Land Cover and Crop Types Classification",2018,"","","","",164,"2022-07-13 09:39:06","","10.1007/978-3-030-02357-7_4","","",,,,,3,0.75,1,5,4,"","",""
6,"Manjari Pradhan, B. Bhattacharya, K. Chakrabarty, B. Bhattacharya","Predicting ${X}$ -Sensitivity of Circuit-Inputs on Test-Coverage: A Machine-Learning Approach",2019,"","","","",165,"2022-07-13 09:39:06","","10.1109/TCAD.2018.2878169","","",,,,,6,2.00,2,4,3,"Digital circuits are often prone to suffer from uncertain timing, inadequate sensor feedback, limited controllability of past states or inability of initializing memory-banks, and erroneous behavior of analog-to-digital converters, which may produce an unknown (<inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>) logic value at various circuit nodes. Additionally, many design bugs that are identified during the post-silicon validation phase manifest themselves as <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-values. The presence of such <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-sources on certain primary or secondary inputs of a logic circuit may cause loss of fault-coverage of a test set, which, in turn, may impact its reliability and robustness. In this paper, we provide a mechanism for predicting the sensitivity of <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-sources in terms of loss of fault-coverage, on the basis of learning only a few structural features of the circuit that are easy to extract from the netlist. We show that the <inline-formula> <tex-math notation=""LaTeX"">${X}$ </tex-math></inline-formula>-sources can be graded satisfactorily according to their sensitivity using support vector regression, thereby obviating the need for costly explicit simulation. Experimental results on several benchmark circuits demonstrate the efficacy, speed, and accuracy of prediction.","",""
7,"Ghislain Takam Tchendjou, Rshdee Alhakim, E. Simeu, F. Lebowsky","Evaluation of machine learning algorithms for image quality assessment",2016,"","","","",166,"2022-07-13 09:39:06","","10.1109/IOLTS.2016.7604697","","",,,,,7,1.17,2,4,6,"In this article, we apply different machine learning (ML) techniques for building objective models, that permit to automatically assess the image quality in agreement with human visual perception. The six ML methods proposed are discriminant analysis, k-nearest neighbors, artificial neural network, non-linear regression, decision tree and fuzzy logic. Both the stability and the robustness of designed models are evaluated by using Monte-Carlo cross-validation approach (MCCV). The simulation results demonstrate that fuzzy logic model provides the best prediction accuracy.","",""
3,"Nicolas Känzig, Roland Meier, L. Gambazzi, Vincent Lenders, L. Vanbever","Machine Learninģ-based Detection of C&C Channels with a Focus on the Locked Shields Cyber Defense Exercise",2019,"","","","",167,"2022-07-13 09:39:06","","10.23919/CYCON.2019.8756814","","",,,,,3,1.00,1,5,3,"The diversity of applications and devices in enterprise networks combined with large traffic volumes make it inherently challenging to quickly identify malicious traffic. When incidents occur, emergency response teams often lose precious time in reverse-engineering the network topology and configuration before they can focus on malicious activities and digital forensics. In this paper, we present a system that quickly and reliably identifies Command and Control (C&C) channels without prior network knowledge. The key idea is to train a classifier using network traffic from attacks that happened in the past and use it to identify C&C connections in the current traffic of other networks. Specifically, we leverage the fact that - while benign traffic differs - malicious traffic bears similarities across networks (e.g., devices participating in a botnet act in a similar manner irrespective of their location). To ensure performance and scalability, we use a random forest classifier based on a set of computationally-efficient features tailored to the detection of C&C traffic. In order to prevent attackers from outwitting our classifier, we tune the model parameters to maximize robustness. We measure high resilience against possible attacks - e.g., attempts to camouflaging C&C flows as benign traffic - and packet loss during the inference. We have implemented our approach and we show its practicality on a real use case: Locked Shields, the world's largest cyber defense exercise. In Locked Shields, defenders have limited resources to protect a large, heterogeneous network against unknown attacks. Using recorded datasets (from 2017 and 2018) from a participating team, we show that our classifier is able to identify C&C channels with 99% precision and over 90% recall in near real time and with realistic resource requirements. If the team had used our system in 2018, it would have discovered 10 out of 12 C&C servers in the first hours of the exercise.","",""
1,"Y. Shiraishi, K. Chiba, A. Okada","SF3B1ness score: screening SF3B1 mutation status from over 60,000 transcriptomes based on a machine learning approach",2019,"","","","",168,"2022-07-13 09:39:06","","10.1101/572834","","",,,,,1,0.33,0,3,3,"In precision oncology, genomic evidence is used to determine the optimal treatment for each patient. However, identification of somatic mutations from genome sequencing data is often technically difficult and functional significance of somatic mutations is inconclusive in many cases. In this paper, to seek for an alternative approach, we tackle the problem of predicting functional mutations from transcriptome sequencing data. Focusing on SF3B1, a key splicing factor gene, we develop SF3B1ness score for classifying functional mutation status using a combination of Naive Bayes classifier and zero-inflated beta-binomial modeling (R package is available at (https://github.com/friend1WS/SF3B1ness). Using 8,992 TCGA exome and RNA sequencing data for evaluation, we show that the classifier based on SF3B1ness score is able to (1) attain very high precision (>93%) and sensitivity (>95%), (2) rescue several somatic mutations not identified by exome sequence analysis especially due to low variant allele frequencies, and (3) successfully measure functional importance for somatic mutation whose significance has been unknown. Furthermore, to demonstrate that the SF3B1ness score is highly robust and can be extensible to the cohorts outside training data, we performed a functional SF3B1 mutation screening on 51,577 additional transcriptome sequencing data. We have detected 135 samples with putative SF3B1 functional mutations including those that are rarely registered in the somatic mutation database (e.g., G664C, L747W, and R775G). Moreover, we could identify two cases with SF3B1 mutations from normal tissues, implying that SF3B1ness score can be used for detecting clonal hematopoiesis.","",""
57,"Alessandro Lumino, E. Polino, A. S. Rab, G. Milani, N. Spagnolo, N. Wiebe, F. Sciarrino","Experimental Phase Estimation Enhanced By Machine Learning",2017,"","","","",169,"2022-07-13 09:39:06","","10.1103/PhysRevApplied.10.044033","","",,,,,57,11.40,8,7,5,"Phase estimation protocols provide a fundamental benchmark for the field of quantum metrology. The latter represents one of the most relevant applications of quantum theory, potentially enabling the capability of measuring unknown physical parameters with improved precision over classical strategies. Within this context, most theoretical and experimental studies have focused on determining the fundamental bounds and how to achieve them in the asymptotic regime where a large number of resources is employed. However, in most applications it is necessary to achieve optimal precisions by performing only a limited number of measurements. To this end, machine learning techniques can be applied as a powerful optimization tool. Here, we implement experimentally single-photon adaptive phase estimation protocols enhanced by machine learning, showing the capability of reaching optimal precision after a small number of trials. In particular, we introduce a new approach for Bayesian estimation that exhibit best performances for very low number of photons N. Furthermore, we study the resilience to noise of the tested methods, showing that the optimized Bayesian approach is very robust in the presence of imperfections. Application of this methodology can be envisaged in the more general multiparameter case, that represents a paradigmatic scenario for several tasks including imaging or Hamiltonian learning.","",""
63,"Bo Du, Zengmao Wang, Lefei Zhang, Liang-pei Zhang, D. Tao","Robust and Discriminative Labeling for Multi-Label Active Learning Based on Maximum Correntropy Criterion",2017,"","","","",170,"2022-07-13 09:39:06","","10.1109/TIP.2017.2651372","","",,,,,63,12.60,13,5,5,"Multi-label learning draws great interests in many real world applications. It is a highly costly task to assign many labels by the oracle for one instance. Meanwhile, it is also hard to build a good model without diagnosing discriminative labels. Can we reduce the label costs and improve the ability to train a good model for multi-label learning simultaneously? Active learning addresses the less training samples problem by querying the most valuable samples to achieve a better performance with little costs. In multi-label active learning, some researches have been done for querying the relevant labels with less training samples or querying all labels without diagnosing the discriminative information. They all cannot effectively handle the outlier labels for the measurement of uncertainty. Since maximum correntropy criterion (MCC) provides a robust analysis for outliers in many machine learning and data mining algorithms, in this paper, we derive a robust multi-label active learning algorithm based on an MCC by merging uncertainty and representativeness, and propose an efficient alternating optimization method to solve it. With MCC, our method can eliminate the influence of outlier labels that are not discriminative to measure the uncertainty. To make further improvement on the ability of information measurement, we merge uncertainty and representativeness with the prediction labels of unknown data. It cannot only enhance the uncertainty but also improve the similarity measurement of multi-label data with labels information. Experiments on benchmark multi-label data sets have shown a superior performance than the state-of-the-art methods.","",""
15,"Adolfo Perrusquía, Wen Yu","Robust control under worst‐case uncertainty for unknown nonlinear systems using modified reinforcement learning",2020,"","","","",171,"2022-07-13 09:39:06","","10.1002/rnc.4911","","",,,,,15,7.50,8,2,2,"Reinforcement learning (RL) is an effective method for the design of robust controllers of unknown nonlinear systems. Normal RLs for robust control, such as actor‐critic (AC) algorithms, depend on the estimation accuracy. Uncertainty in the worst case requires a large state‐action space, this causes overestimation and computational problems. In this article, the RL method is modified with the k‐nearest neighbor and the double Q‐learning algorithm. The modified RL does not need the neural estimator as AC and can stabilize the unknown nonlinear system under the worst‐case uncertainty. The convergence property of the proposed RL method is analyzed. The simulations and the experimental results show that our modified RLs are much more robust compared with the classic controllers, such as the proportional‐integral‐derivative, the sliding mode, and the optimal linear quadratic regulator controllers.","",""
128,"S. Kiranyaz, T. Ince, M. Gabbouj","Multidimensional Particle Swarm Optimization for Machine Learning and Pattern Recognition",2013,"","","","",172,"2022-07-13 09:39:06","","10.1007/978-3-642-37846-1","","",,,,,128,14.22,43,3,9,"","",""
79,"Taesik Na, J. Ko, S. Mukhopadhyay","Cascade Adversarial Machine Learning Regularized with a Unified Embedding",2017,"","","","",173,"2022-07-13 09:39:06","","","","",,,,,79,15.80,26,3,5,"Injecting adversarial examples during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we first show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. We also propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.","",""
7,"Yue Pan, Hongmei Liu, L. Metsch, D. Feaster","Factors Associated with HIV Testing Among Participants from Substance Use Disorder Treatment Programs in the US: A Machine Learning Approach",2017,"","","","",174,"2022-07-13 09:39:06","","10.1007/s10461-016-1628-y","","",,,,,7,1.40,2,4,5,"","",""
2,"H. G. Damavandi","Data analytics, interpretation and machine learning for environmental forensics using peak mapping methods",2016,"","","","",175,"2022-07-13 09:39:06","","10.17077/ETD.IX74OTR7","","",,,,,2,0.33,2,1,6,"In this work our driving motivation is to develop mathematically robust and computationally efficient algorithms that will help chemists towards their goal of pattern matching. Environmental chemistry today broadly faces difficult computational and interpretational challenges for vast and ever-increasing data repositories. A driving factor behind these challenges are little known intricate relationships between constituent analytes that constitute complex mixtures spanning a range of target and non-target compounds. While the end of goal of different environment applications are diverse, computationally speaking, many data interpretation bottlenecks arise from lack of efficient algorithms and robust mathematical frameworks to identify, cluster and interpret compound peaks. There is a compelling need for compound-cognizant quantitative interpretation that accounts for the full informational range of gas chromatographic (and mass spectrometric) datasets. Traditional target-oriented analysis focus only on the dominant compounds of the chemical mixture, and thus are agnostic of the contribution of unknown non-target analytes. On the other extreme, statistical methods prevalent in chemometric interpretation ignore compound identity altogether and consider only the multivariate data statistics, and thus are agnostic of intrinsic relationships between the well-known target and unknown target analytes. Thus, both schools of thought (target-based or statistical) in current-day chemical data analysis and interpretation fall short of quantifying the complex interaction between major and minor compound peaks in molecular mixtures commonly encountered in","",""
3,"Yuanhao Li, Badong Chen, N. Yoshimura, Y. Koike","Restricted Minimum Error Entropy Criterion for Robust Classification.",2019,"","","","",176,"2022-07-13 09:39:06","","10.1109/TNNLS.2021.3082571","","",,,,,3,1.00,1,4,3,"The minimum error entropy (MEE) criterion is a powerful approach for non-Gaussian signal processing and robust machine learning. However, the instantiation of MEE on robust classification is a rather vacancy in the literature. The original MEE purely focuses on minimizing Renyi's quadratic entropy of the prediction errors, which could exhibit inferior capability in noisy classification tasks. To this end, we analyze the optimal error distribution with adverse outliers and introduce a specific codebook for restriction, which optimizes the error distribution toward the optimal case. Half-quadratic-based optimization and convergence analysis of the proposed learning criterion, called restricted MEE (RMEE), are provided. The experimental results considering logistic regression and extreme learning machine on synthetic data and UCI datasets, respectively, are presented to demonstrate the superior robustness of RMEE. Furthermore, we evaluate RMEE on a noisy electroencephalogram dataset, so as to strengthen its practical impact.","",""
3,"Agnese Chiatti, E. Motta, E. Daga, G. Bardaro","Fit to Measure: Reasoning about Sizes for Robust Object Recognition",2020,"","","","",177,"2022-07-13 09:39:06","","","","",,,,,3,1.50,1,4,2,"Service robots can help with many of our daily tasks, especially in those cases where it is inconvenient or unsafe for us to intervene: e.g., under extreme weather conditions or when social distance needs to be maintained. However, before we can successfully delegate complex tasks to robots, we need to enhance their ability to make sense of dynamic, real world environments. In this context, the first prerequisite to improving the Visual Intelligence of a robot is building robust and reliable object recognition systems. While object recognition solutions are traditionally based on Machine Learning methods, augmenting them with knowledge based reasoners has been shown to improve their performance. In particular, based on our prior work on identifying the epistemic requirements of Visual Intelligence, we hypothesise that knowledge of the typical size of objects could significantly improve the accuracy of an object recognition system. To verify this hypothesis, in this paper we present an approach to integrating knowledge about object sizes in a ML based architecture. Our experiments in a real world robotic scenario show that this combined approach ensures a significant performance increase over state of the art Machine Learning methods.","",""
3,"S. Ohmori","A Predictive Prescription Using Minimum Volume k-Nearest Neighbor Enclosing Ellipsoid and Robust Optimization",2021,"","","","",178,"2022-07-13 09:39:06","","10.3390/MATH9020119","","",,,,,3,3.00,3,1,1,"This paper studies the integration of predictive and prescriptive analytics framework for deriving decision from data. Traditionally, in predictive analytics, the purpose is to derive prediction of unknown parameters from data using statistics and machine learning, and in prescriptive analytics, the purpose is to derive a decision from known parameters using optimization technology. These have been studied independently, but the effect of the prediction error in predictive analytics on the decision-making in prescriptive analytics has not been clarified. We propose a modeling framework that integrates machine learning and robust optimization. The proposed algorithm utilizes the k-nearest neighbor model to predict the distribution of uncertain parameters based on the observed auxiliary data. The enclosing minimum volume ellipsoid that contains k-nearest neighbors of is used to form the uncertainty set for the robust optimization formulation. We illustrate the data-driven decision-making framework and our novel robustness notion on a two-stage linear stochastic programming under uncertain parameters. The problem can be reduced to a convex programming, and thus can be solved to optimality very efficiently by the off-the-shelf solvers.","",""
0,"","Context-Robust Object Recognition via Object Manipulations in a Synthetic 3D Environment",2021,"","","","",179,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,0,1,"The remote control is a small object that does not fly in the air and is generally found on a table, not in the sink. Such contextual regularities are ingrained in our perception of the world and previous research suggests that they can even influence human and computational models object recognition ability. However, the exact effects of contextual information on object recognition are still unknown for both humans and machine learning models. Here, we introduce a novel way of studying the effects of different contextual cues in a qualitative and systematic way. We present a diverse synthetic dataset created via a 3D simulation engine that allows for complex object modifications. Our dataset consists of more than 15000 images across 36 object categories and it is designed specifically for studying the effects of gravity, object co-occurrence statistics, and relative size regularities. We conduct a series of psychophysics experiments to assess human performance and establish a benchmark for computational models on the dataset. Additionally, we test state-of-the-art deep learning models on the same dataset and study how contextual information influences their object recognition accuracy. Finally, we propose a context-aware recognition transformer network that integrates contextual and object information via multi-head attention mechanism. Our model captures useful contextual information that allows it to achieve human-level performance and significantly better robustness in out-of-context conditions compared to baseline models across our dataset and another existing out-of-context natural image dataset. Moreover, our model performs in a way that is consistent with human object recognition and shows similar recognition artefacts.","",""
8,"G. Narula, J. A. Herbst, J. Rychen, Richard Hans Robert Hahnloser","Learning auditory discriminations from observation is efficient but less robust than learning from experience",2018,"","","","",180,"2022-07-13 09:39:06","","10.1038/s41467-018-05422-y","","",,,,,8,2.00,2,4,4,"","",""
2,"Seok-Hwan Choi, Jinmyeong Shin, Peng Liu, Yoon-Ho Choi","EEJE: Two-Step Input Transformation for Robust DNN Against Adversarial Examples",2021,"","","","",181,"2022-07-13 09:39:06","","10.1109/tnse.2020.3008394","","",,,,,2,2.00,1,4,1,"Adversarial examples are human-imperceptible perturbations to inputs to machine learning models. While attacking machine learning models, adversarial examples cause the model to make a false positive or a false negative. So far, two representative defense architectures have shown a significant effect: (1) model retraining architecture; and (2) input transformation architecture. However, previous defense methods belonging to these two architectures do not produce good outputs for every input, i.e., adversarial examples and legitimate inputs. Specifically, model retraining methods generate false negatives for unknown adversarial examples, and input transformation methods generate false positives for legitimate inputs. To produce good-enough outputs for every input, we propose and evaluate a new input transformation architecture based on two-step input transformation. To solve the limitations of the previous two defense methods, we intend to answer the following question: How to maintain the performance of Deep Neural Network (DNN) models for legitimate inputs while providing good robustness against various adversarial examples? From the evaluation results under various conditions, we show that the proposed two-step input transformation architecture provides good robustness to DNN models against state-of-the-art adversarial perturbations, while maintaining the high accuracy even for legitimate inputs.","",""
84,"Jiayi Ma, Jia Wu, Ji Zhao, Junjun Jiang, Huabing Zhou, Quan Z. Sheng","Nonrigid Point Set Registration With Robust Transformation Learning Under Manifold Regularization",2019,"","","","",182,"2022-07-13 09:39:06","","10.1109/TNNLS.2018.2872528","","",,,,,84,28.00,14,6,3,"This paper solves the problem of nonrigid point set registration by designing a robust transformation learning scheme. The principle is to iteratively establish point correspondences and learn the nonrigid transformation between two given sets of points. In particular, the local feature descriptors are used to search the correspondences and some unknown outliers will be inevitably introduced. To precisely learn the underlying transformation from noisy correspondences, we cast the point set registration into a semisupervised learning problem, where a set of indicator variables is adopted to help distinguish outliers in a mixture model. To exploit the intrinsic structure of a point set, we constrain the transformation with manifold regularization which plays a role of prior knowledge. Moreover, the transformation is modeled in the reproducing kernel Hilbert space, and a sparsity-induced approximation is utilized to boost efficiency. We apply the proposed method to learning motion flows between image pairs of similar scenes for visual homing, which is a specific type of mobile robot navigation. Extensive experiments on several publicly available data sets reveal the superiority of the proposed method over state-of-the-art competitors, particularly in the context of the degenerated data.","",""
9,"Jinna Li, Jinliang Ding, T. Chai, F. Lewis, S. Jagannathan","Adaptive Interleaved Reinforcement Learning: Robust Stability of Affine Nonlinear Systems With Unknown Uncertainty",2020,"","","","",183,"2022-07-13 09:39:06","","10.1109/tnnls.2020.3027653","","",,,,,9,4.50,2,5,2,"This article investigates adaptive robust controller design for discrete-time (DT) affine nonlinear systems using an adaptive dynamic programming. A novel adaptive interleaved reinforcement learning algorithm is developed for finding a robust controller of DT affine nonlinear systems subject to matched or unmatched uncertainties. To this end, the robust control problem is converted into the optimal control problem for nominal systems by selecting an appropriate utility function. The performance evaluation and control policy update combined with neural networks approximation are alternately implemented at each time step for solving a simplified Hamilton–Jacobi–Bellman (HJB) equation such that the uniformly ultimately bounded (UUB) stability of DT affine nonlinear systems can be guaranteed, allowing for all realization of unknown bounded uncertainties. The rigorously theoretical proofs of convergence of the proposed interleaved RL algorithm and UUB stability of uncertain systems are provided. Simulation results are given to verify the effectiveness of the proposed method.","",""
5,"Stavros Pitoglou, Y. Koumpouros, Athanasios Anastasiou","Using Electronic Health Records and Machine Learning to Make Medical-Related Predictions from Non-Medical Data",2018,"","","","",184,"2022-07-13 09:39:06","","10.1109/ICMLDE.2018.00021","","",,,,,5,1.25,2,3,4,"Objectives: Administrative HIS (Hospital Information System) and EHR (Electronic Health Record) data are characterized by lower privacy sensitivity, thus easier portability and handling, as well as higher information quality. In this paper we test the hypothesis that the application of machine learning techniques on data of this nature can be used to address prediction/forecasting problems in the Health IT domain. The novelty of this approach consists in that medical data (test results, diagnoses, doctors’ notes etc.) are not included in the predictors’ dataset. Moreover, there is limited need for separation of patient cohorts based on specific health conditions. Methods: We experiment with the prediction of the probability of early readmission at the time of a patient’s discharge. We extract real HIS data and perform data processing techniques. We then apply a series of machine learning algorithms (Logistic Regression, Support Vector Machine, Gaussian Naïve Bayes, K-Nearest Neighbors and Deep Multilayer Neural Network) and measure the performance of the emergent models. Results: All applied methods performed well above random guessing, even with minimal hyper-parameter tuning. Conclusions: Given that the experiments provide evidence in favor of the underlying hypothesis, future experimentation on more fine-tuned (thus more robust) models could result in applications suited for productive environments.","",""
0,"U. Topcu","CS 294 – Practical Machine Learning Term Project Robust Classification for Data with Interval Uncertainty and Label Errors",2006,"","","","",185,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,1,16,"A robust linear binary classification problem will be considered. Robustness will be for data with interval uncertainty, i.e., data points are unknown but their mean and bounds on their components are known. Convex optimization formulation for the problem is derived and the method is applied to a genomic micro-array data. An extension for this framework will be developed for data with uncertainties due to label errors. For this problem, the convex optimization formulation is derived. The implementation of this method is postponed due to lack of a useful data set.","",""
0,"Yuqing Shi, Shiqiang Du, Weilan Wang","Robust Low-Rank and Sparse Tensor Decomposition for Low-Rank Tensor Completion",2021,"","","","",186,"2022-07-13 09:39:06","","10.1109/CCDC52312.2021.9601608","","",,,,,0,0.00,0,3,1,"Low-rank tensor completion (LRTC) is a hot research direction in computer vision and machine learning because it can effectively recover the missing entries of tensor. However, most of the existing LRTC methods not only need to repeatedly calculate the time-consuming SVD decomposition, but also only consider a noise distribution in the model. To overcome the above shortcomings, based on the tensor-tensor product (t-product), we propose a new LRTC method-the robust low-rank and sparse tensor decomposition model (RLRST) for tensor completion. Firstly, in order to estimate the unknown entries in tensor data more accurately, two kinds of noise: sparse noise and Gaussian noise are considered simultaneously in RLRST. Secondly, the low-rank recovery tensor is equivalently decomposed into two smaller tensor t-products, which effectively saves the running time of the algorithm. Then, based on the alternate direction method of multipliers (ADMM), an efficient iterative updated algorithm is presented for our RLRST optimization. Finally, numerical experiments on image inpainting tasks demonstrate the effectiveness of our method over other related state-of-the-art tensor completion methods.","",""
0,"Tobias Sutter, A. Krause, D. Kuhn","Robust Generalization despite Distribution Shift via Minimum Discriminating Information",2021,"","","","",187,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,3,1,"Training models that perform well under distribution shifts is a central challenge in machine learning. In this paper, we introduce a modeling framework where, in addition to training data, we have partial structural knowledge of the shifted test distribution. We employ the principle of minimum discriminating information to embed the available prior knowledge, and use distributionally robust optimization to account for uncertainty due to the limited samples. By leveraging large deviation results, we obtain explicit generalization bounds with respect to the unknown shifted distribution. Lastly, we demonstrate the versatility of our framework by demonstrating it on two rather distinct applications: (1) training classiﬁers on systematically biased data and (2) off-policy evaluation in Markov Decision Processes.","",""
0,"Camilla Sterud, Signe Moe, J. Gravdahl","Stable and robust neural network controllers",2021,"","","","",188,"2022-07-13 09:39:06","","10.23919/ecc54610.2021.9655096","","",,,,,0,0.00,0,3,1,"Neural networks are expressive function approimators that can be employed for state estimation in control problems. However, control systems with machine learning in the loop often lack stability proofs and performance guarantees, which are crucial for safety-critical applications. In this work, a feedback controller using a feedforward neural network of arbitrary size to estimate unknown dynamics is suggested. The controller is designed for solving a general trajectory tracking problem for a broad class of two-dimensional nonlinear systems. The controller is proven to stabilize the closed-loop system, such that it is input-to-state and finite-gain ${{\mathcal{L}}_p}$-stable from the neural network estimation error to the tracking error. Furthermore, the controller is proven to make the tracking error globally and exponentially converge to a ball centered at the origin. When the neural network estimate is updated discretely, or the state measurements are affected by bounded noise, the convergence bound is shown to be dependent on the Lipschitz constant of the neural network estimator. In light of this, we demonstrate how regularization techniques can be beneficial when utilizing deep learning in control. Experiments on simulated data confirm the theoretical results.","",""
0,"Soumyabrata Talukder, Ratnesh Kumar","Robust Stability of Neural-Network Controlled Nonlinear Systems with Parametric Variability",2021,"","","","",189,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,2,1,"—Stability certiﬁcation and identiﬁcation of the stabi- lizable operating region of a dynamical system are two important concerns to ensure its operational safety/security and robustness. With the advent of machine-learning tools, these issues are espe-cially important for systems with machine-learned components in the feedback loop. Here, in presence of unknown discrete variation (DV) of its parameters within a bounded range, a system controlled by a static feedback controller in which the closed-loop (CL) equilibria are subject to variation-induced drift is equivalently represented using a class of time-invariant systems, each with the same control policy. To develop a general theory for stability and stabilizability of such a class of neural-network (NN) controlled nonlinear systems, a Lyapunov-based convex stability certiﬁcate is proposed and is further used to devise an estimate of a local Lipschitz upper bound for the NN and a corresponding operating domain in the state space containing an initialization set, starting from where the CL local asymptotic stability of each system in the class is guaranteed, while the trajectory of the original system remains conﬁned to the domain if the DV of the parameters satisﬁes a certain quasi-stationarity condition. To compute such a robustly stabilizing NN controller, a stability-guaranteed training (SGT) algorithm is also proposed. The effectiveness of the proposed framework is demonstrated using illustrative examples.","",""
0,"Ulrich J. Frey, M. Klein","Modelling Complex Investment Decisions for Renewables with Machine Learning",2018,"","","","",190,"2022-07-13 09:39:06","","","","",,,,,0,0.00,0,2,4,"The factors that drive the decision-making process behind private investments in renewables, e.g. solar  on roof tops are still somewhat unknown. We aim to develop a more comprehensive model with  potential factors from various backgrounds including social, economic and geographic drivers. We use  an existing data set of real investments in PV in Germany from 1991 to 2014. These 1.4 million  investment decisions are merged with other data sets with information on social, employment,  rural/urban characteristics, election results and other potential drivers. The variable of interest is the  installed capacity per county.  Since the interactions between these variables may be complex, non-linear and are basically not  known, we decided to use machine learning statistical methods. In order to increase the robustness of  results and to find out which algorithm performs best in terms of model quality, we used Generalized  Linear Models (GLM), random forests, gradient boosting and deep neural networks.  Model predictions are rather accurate: at the county level the adjusted R2 is 0.65 for GLM, 0.66 for  Random Forests, 0.68 for deep neural nets and 0.68 for gradient boosting. Agreement between  methods is only decent with deep neural nets calculating a much more balanced model in contrast to  gradient boosting. Concerning factor importance for investment decisions, the best two models  confirm that the amount of solar insolation received, the absolute number of population per county,  and the density and the distinction between urban and rural areas are most relevant","",""
97,"T. Doster, A. Watnik","Machine learning approach to OAM beam demultiplexing via convolutional neural networks.",2017,"","","","",191,"2022-07-13 09:39:06","","10.1364/AO.56.003386","","",,,,,97,19.40,49,2,5,"Orbital angular momentum (OAM) beams allow for increased channel capacity in free-space optical communication. Conventionally, these OAM beams are multiplexed together at a transmitter and then propagated through the atmosphere to a receiver where, due to their orthogonality properties, they are demultiplexed. We propose a technique to demultiplex these OAM-carrying beams by capturing an image of the unique multiplexing intensity pattern and training a convolutional neural network (CNN) as a classifier. This CNN-based demultiplexing method allows for simplicity of operation as alignment is unnecessary, orthogonality constraints are loosened, and costly optical hardware is not required. We test our CNN-based technique against a traditional demultiplexing method, conjugate mode sorting, with various OAM mode sets and levels of simulated atmospheric turbulence in a laboratory setting. Furthermore, we examine our CNN-based technique with respect to added sensor noise, number of photon detections, number of pixels, unknown levels of turbulence, and training set size. Results show that the CNN-based demultiplexing method is able to demultiplex combinatorially multiplexed OAM modes from a fixed set with >99% accuracy for high levels of turbulence-well exceeding the conjugate mode demultiplexing method. We also show that this new method is robust to added sensor noise, number of photon detections, number of pixels, unknown levels of turbulence, and training set size.","",""
3,"Mattes Ohlenbusch, Aike Ahrens, Christian Rollwage, Jörg Bitzer","Robust Drone Detection for Acoustic Monitoring Applications",2021,"","","","",192,"2022-07-13 09:39:06","","10.23919/Eusipco47968.2020.9287433","","",,,,,3,3.00,1,4,1,"Commercially available light-weight unmanned aerial vehicles (UAVs) present a challenge for public safety, e.g. espionage, transporting dangerous goods or devices. Therefore, countermeasures are necessary. Usually, detection of UAVs is a first step. Along many other modalities, acoustic detection seems promising. Recent publications show interesting results by using machine and deep learning methods. The acoustic detection of UAVs appears to be particularly difficult in adverse situations, such as in heavy wind noise or in the presence of construction noise. In this contribution, the typical feature set is extended to increase separation of background noise and the UAV signature noise. The decision algorithm utilized is support vector machine (SVM) classification. The classification is based on an extended training dataset labeled to support binary classification. The proposed method is evaluated in comparison to previously published algorithms, on the basis of a dataset recorded from different acoustic environments, including unknown UAV types. The results show an improvement over existing methods, especially in terms of false-positive detection rate. For a first step into real-time embedded systems a recursive feature elimination method is applied to reduce the model dimensionality. The results indicate only a slight decreases in detection performance.","",""
28,"Jiuwen Cao, K. Zhang, Hongwei Yong, Xiaoping Lai, Badong Chen, Zhiping Lin","Extreme Learning Machine With Affine Transformation Inputs in an Activation Function",2019,"","","","",193,"2022-07-13 09:39:06","","10.1109/TNNLS.2018.2877468","","",,,,,28,9.33,5,6,3,"The extreme learning machine (ELM) has attracted much attention over the past decade due to its fast learning speed and convincing generalization performance. However, there still remains a practical issue to be approached when applying the ELM: the randomly generated hidden node parameters without tuning can lead to the hidden node outputs being nonuniformly distributed, thus giving rise to poor generalization performance. To address this deficiency, a novel activation function with an affine transformation (AT) on its input is introduced into the ELM, which leads to an improved ELM algorithm that is referred to as an AT-ELM in this paper. The scaling and translation parameters of the AT activation function are computed based on the maximum entropy principle in such a way that the hidden layer outputs approximately obey a uniform distribution. Application of the AT-ELM algorithm in nonlinear function regression shows its robustness to the range scaling of the network inputs. Experiments on nonlinear function regression, real-world data set classification, and benchmark image recognition demonstrate better performance for the AT-ELM compared with the original ELM, the regularized ELM, and the kernel ELM. Recognition results on benchmark image data sets also reveal that the AT-ELM outperforms several other state-of-the-art algorithms in general.","",""
14,"Kevin Li, C. Gibson, D. Ho, Qi Zhou, J. Kim, O. Buhisi, D. Brown, M. Gerber","Assessment of machine learning algorithms in cloud computing frameworks",2013,"","","","",194,"2022-07-13 09:39:06","","10.1109/SIEDS.2013.6549501","","",,,,,14,1.56,2,8,9,"In the past decade, digitization of information has led to a data explosion in both volume and complexity. While traditional computing frameworks have failed to provide adequate computing power for the now common data-intensive computing tasks, cloud computing provides an effective alternative to enhance computing power. Machine learning algorithms are powerful analytical methods that allow machines to recognize patterns and facilitate human learning. However, the performance of individual machine learning algorithms within each cloud computing framework remains largely unknown. Furthermore, the lack of a robust selection methodology matching input data with effective machine learning algorithms limits the ability of practitioners to make effective use of cloud computing. This research compares various machine learning algorithms on the widely adopted Apache Mahout framework and the recently introduced GraphLab framework. Whereas previous work has examined the computational architectures of various cloud computing frameworks, this work focuses on a problem-based approach to architecture selection. The experimental results demonstrate that GraphLab generally outperforms Mahout with respect to runtime, scalability, and usability. However, Mahout outperforms GraphLab when the experiment focus shifts to error measurement.","",""
7,"G. Mastorakis","Human fall detection methodologies: from machine learning using acted data to fall modelling using myoskeletal simulation",2018,"","","","",195,"2022-07-13 09:39:06","","","","",,,,,7,1.75,7,1,4,"Human Fall Detection is a research area with interest from many disciplines and  aims to perform for many assisted-living monitoring applications to promptly identify  life-threatening situations. A fall occurs when a person is unable to maintain  balance due to a variety of issues; physical; mental or environmental. The accurate  detection of the fall is crucial as a missed detection can be fatal. Variability of human  physiological characteristics is currently unstudied as to the impact on a fall  detector's performance as young adults and elderly are expected to fall differently.  Another important issue is the scene occlusions. In the use of visual sensors, an  occluded fall is treated as a missed detection as the whereabouts of the person is  unknown when occluded. Finally, current studies are based on acted fall datasets  on which algorithms are trained. These dataset are unrepresentative of real fall  events and illustrate the events without occlusions or other scene in  uences.  Several fall detection algorithms were developed during the study aiming to achieve  accuracy in detection falls while fall-like actions such as lying down remain undetected.  Human fall datasets were used for training and testing purposes of A  machine learning algorithm using data from depth cameras which captured the  fall events from different views. A new pathway was introduced tackling the issues  of availability issues of data-driven machine learning approaches which was  achieved with the use of simulation data. The use of myoskeletal simulation was  then selected as a closer representation of the human body in terms of structure  and behaviour. With the use of a simulation model, a personalised estimation of  the fall event can be achieved as it is parametrised on a physical characteristic such as the height of the falling person. Alternative technologies such as accelerometers  have been used for fall detection to prove the validity of this approach on other  modalities. A study regarding the impact of occlusions for fall detection which  is one of the issues not properly investigated in current work is proposed and  examined. Synthetic occlusions were added to existing depth data from publicly  available datasets.  The research methodologies were evaluated using the most representative depth  video and accelerometer data from existing datasets, as well as YouTube videos  of real-fall events. The machine learning methodologies achieved good results on  similar body variability datasets. A discussion regarding the proof of concept of the  simulation-based approach for fall modelling is mentioned given the comparative  results against existing methodologies which achieves better than any existing  work evaluated against known datasets. The simulation approach is also evaluated  against occluded fall and non-fall event data, proving the further robustness of  the approach. This platform can be expanded to analyse any type of fall, or body  posture (e.g. elderly), without the use of humans to performs fall events.","",""
5,"W. Flynn, Sandeep Namburi, Carolyn Paisie, H. Reddi, Sheng Li, R. K. Murthy Karuturi, J. George","Pan-cancer machine learning predictors of primary site of origin and molecular subtype",2018,"","","","",196,"2022-07-13 09:39:06","","10.1101/333914","","",,,,,5,1.25,1,7,4,"Background It is estimated by the American Cancer Society that approximately 5% of all metastatic tumors have no defined primary site (tissue) of origin and are classified as cancers of unknown primary (CUPs). The current standard of care for CUP patients depends on immunohistochemistry (IHC) based approaches to identify the primary site. The addition of post-mortem evaluation to IHC based tests helps to reveal the identity of the primary site for only 25% of the CUPs, emphasizing the acute need for better methods of determination of the site of origin. CUP patients are therefore given generic chemotherapeutic agents resulting in poor prognosis. When the tissue of origin is known, patients can be given site specific therapy with significant improvement in clinical outcome. Similarly, identifying the primary site of origin of metastatic cancer is of great importance for designing treatment. Identification of the primary site of origin is an import first step but may not be sufficient information for optimal treatment of the patient. Recent studies, primarily from The Cancer Genome Atlas (TCGA) project, and others, have revealed molecular subtypes in several cancer types with distinct clinical outcome. The molecular subtype captures the fundamental mechanisms driving the cancer and provides information that is essential for the optimal treatment of a cancer. Thus, along with primary site of origin, molecular subtype of a tumor is emerging as a criterion for personalized medicine and patient entry into clinical trials. However, there is no comprehensive toolset available for precise identification of tissue of origin or molecular subtype for precision medicine and translational research. Methods and Findings We posited that metastatic tumors will harbor the gene expression profiles of the primary site of origin of the cancer. Therefore, we decided to learn the molecular characteristics of the primary tumors using the large number of cancer genome profiles available from the TCGA project. Our predictors were trained for 33 cancer types and for the 11 cancers where there are established molecular subtypes. We estimated the accuracy of several machine learning models using cross-validation methods. The extensive testing using independent test sets revealed that the predictors had a median sensitivity and specificity of 97.2% and 99.9% respectively without losing classification of any tumor. Subtype classifiers achieved median sensitivity of 87.7% and specificity of 94.5% via cross validation and presented median sensitivity of 79.6% and specificity of 94.6% in two external datasets of 1,999 total samples. Importantly, these external data shows that our classifiers can robustly predict the primary site of origin from external microarray data, metastatic cancer data, and patient-derived xenograft (PDX) data. Conclusion We have demonstrated the utility of gene expression profiles to solve the important clinical challenge of identifying the primary site of origin and the molecular subtype of cancers based on machine learning algorithms. We show, for the first time to our knowledge, that our pan-cancer classifiers can predict multiple cancers’ primary site of origin from metastatic samples. The predictors will be made available as open source software, freely available for academic non-commercial use.","",""
19,"M. Hausknecht, Wen-Ke Li, M. Mauk, P. Stone","Machine Learning Capabilities of a Simulated Cerebellum",2017,"","","","",197,"2022-07-13 09:39:06","","10.1109/TNNLS.2015.2512838","","",,,,,19,3.80,5,4,5,"This paper describes the learning and control capabilities of a biologically constrained bottom-up model of the mammalian cerebellum. Results are presented from six tasks: 1) eyelid conditioning; 2) pendulum balancing; 3) proportional–integral–derivative control; 4) robot balancing; 5) pattern recognition; and 6) MNIST handwritten digit recognition. These tasks span several paradigms of machine learning, including supervised learning, reinforcement learning, control, and pattern recognition. Results over these six domains indicate that the cerebellar simulation is capable of robustly identifying static input patterns even when randomized across the sensory apparatus. This capability allows the simulated cerebellum to perform several different supervised learning and control tasks. On the other hand, both reinforcement learning and temporal pattern recognition prove problematic due to the delayed nature of error signals and the simulator’s inability to solve the credit assignment problem. These results are consistent with previous findings which hypothesize that in the human brain, the basal ganglia is responsible for reinforcement learning, while the cerebellum handles supervised learning.","",""
0,"Yaping Wang, Zhicheng Peng, Riquan Zhang, Qian Xiao","Robust sequential design for piecewise-stationary multi-armed bandit problem in the presence of outliers",2021,"","","","",198,"2022-07-13 09:39:06","","10.1080/24754269.2021.1902687","","",,,,,0,0.00,0,4,1,"ABSTRACT The multi-armed bandit (MAB) problem studies the sequential decision making in the presence of uncertainty and partial feedback on rewards. Its name comes from imagining a gambler at a row of slot machines who needs to decide the best strategy on the number of times as well as the orders to play each machine. It is a classic reinforcement learning problem which is fundamental to many online learning problems. In many practical applications of the MAB, the reward distributions may change at unknown time steps and the outliers (extreme rewards) often exist. Current sequential design strategies may struggle in such cases, as they tend to infer additional change points to fit the outliers. In this paper, we propose a robust change-detection upper confidence bound (RCD-UCB) algorithm which can distinguish the real change points from the outliers in piecewise-stationary MAB settings. We show that the proposed RCD-UCB algorithm can achieve a nearly optimal regret bound on the order of , where T is the number of time steps, K is the number of arms and S is the number of stationary segments. We demonstrate its superior performance compared to some state-of-the-art algorithms in both simulation experiments and real data analysis. (See https://github.com/woaishufenke/MAB_STRF.git for the codes used in this paper.)","",""
14,"Hongfeng Li, Hong-Qin Zhao, Hong Li","Neural-Response-Based Extreme Learning Machine for Image Classification",2019,"","","","",199,"2022-07-13 09:39:06","","10.1109/TNNLS.2018.2845857","","",,,,,14,4.67,5,3,3,"This paper proposes a novel and simple multilayer feature learning method for image classification by employing the extreme learning machine (ELM). The proposed algorithm is composed of two stages: the multilayer ELM (ML-ELM) feature mapping stage and the ELM learning stage. The ML-ELM feature mapping stage is recursively built by alternating between feature map construction and maximum pooling operation. In particular, the input weights for constructing feature maps are randomly generated and hence need not be trained or tuned, which makes the algorithm highly efficient. Moreover, the maximum pooling operation enables the algorithm to be invariant to certain transformations. During the ELM learning stage, elastic-net regularization is proposed to learn the output weight. Elastic-net regularization helps to learn more compact and meaningful output weight. In addition, we preprocess the input data with the dense scale-invariant feature transform operation to improve both the robustness and invariance of the algorithm. To evaluate the effectiveness of the proposed method, several experiments are conducted on three challenging databases. Compared with the conventional deep learning methods and other related ones, the proposed method achieves the best classification results with high computational efficiency.","",""
133,"Salvatore Corrente, S. Greco, M. Kadziński, R. Słowiński","Robust ordinal regression in preference learning and ranking",2013,"","","","",200,"2022-07-13 09:39:06","","10.1007/s10994-013-5365-4","","",,,,,133,14.78,33,4,9,"","",""
