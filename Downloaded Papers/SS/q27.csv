Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
1,"Anupam Datta, Matt Fredrikson, Klas Leino, Kaiji Lu, S. Sen, Zifan Wang","Machine Learning Explainability and Robustness: Connected at the Hip",2021,"","","","",1,"2022-07-13 09:22:24","","10.1145/3447548.3470806","","",,,,,1,1.00,0,6,1,"This tutorial examines the synergistic relationship between explainability methods for machine learning and a significant problem related to model quality: robustness against adversarial perturbations. We begin with a broad overview of approaches to explainable AI, before narrowing our focus to post-hoc explanation methods for predictive models. We discuss perspectives on what constitutes a ""good'' explanation in various settings, with an emphasis on axiomatic justifications for various explanation methods. In doing so, we will highlight the importance of an explanation method's faithfulness to the target model, as this property allows one to distinguish between explanations that are unintelligible because of the method used to produce them, and cases where a seemingly poor explanation points to model quality issues. Next, we introduce concepts surrounding adversarial robustness, including adversarial attacks as well as a range of corresponding state-of-the-art defenses. Finally, building on the knowledge presented thus far, we present key insights from the recent literature on the connections between explainability and robustness, showing that many commonly-perceived explainability issues may be caused by non-robust model behavior. Accordingly, a careful study of adversarial examples and robustness can lead to models whose explanations better appeal to human intuition and domain knowledge.","",""
4,"Guillaume Vidot, Christophe Gabreau, I. Ober, Iulian Ober","Certification of embedded systems based on Machine Learning: A survey",2021,"","","","",2,"2022-07-13 09:22:24","","","","",,,,,4,4.00,1,4,1,"Advances in machine learning (ML) open the way to innovating functions in the avionic domain, such as navigation/surveillance assistance (e.g. vision-based navigation, obstacle sensing, virtual sensing), speechto-text applications, autonomous flight, predictive maintenance or cockpit assistance. Current certification standards and practices, which were defined and refined decades over decades with classical programming in mind, do not however support this new development paradigm. This article provides an overview of the main challenges raised by the use ML in the demonstration of compliance with regulation requirements, and a survey of literature relevant to these challenges, with particular focus on the issues of robustness and explainability of ML results.","",""
1,"M. Staron, Helena Odenstedt Herg'es, S. Naredi, L. Block, Ali El-Merhi, Richard Vithal, M. Elam","Robust Machine Learning in Critical Care — Software Engineering and Medical Perspectives",2021,"","","","",3,"2022-07-13 09:22:24","","10.1109/WAIN52551.2021.00016","","",,,,,1,1.00,0,7,1,"Using machine learning in clinical practice poses hard requirements on explainability, reliability, replicability and robustness of these systems. Therefore, developing reliable software for monitoring critically ill patients requires close collaboration between physicians and software engineers. However, these two different disciplines need to find own research perspectives in order to contribute to both the medical and the software engineering domain. In this paper, we address the problem of how to establish a collaboration where software engineering and medicine meets to design robust machine learning systems to be used in patient care. We describe how we designed software systems for monitoring patients under carotid endarterectomy, in particular focusing on the process of knowledge building in the research team. Our results show what to consider when setting up such a collaboration, how it develops over time and what kind of systems can be constructed based on it. We conclude that the main challenge is to find a good research team, where different competences are committed to a common goal.","",""
2,"S. Meister, Mahdieu A. M. Wermes, J. Stüve, R. Groves","Explainability of deep learning classifier decisions for optical detection of manufacturing defects in the automated fiber placement process",2021,"","","","",4,"2022-07-13 09:22:24","","10.1117/12.2592584","","",,,,,2,2.00,1,4,1,"Automated fibre layup techniques are commonly used composite manufacturing processes in the aviation sector and require a manual visual inspection. Neural Network classification of defects has the potential to automate this visual inspection, however, the machine decision-making processes are hard to verify. Thus, we present an approach for visualising Convolutional Neural Network (CNN) based classifications of manufacturing defects and quantifying its robustness suitably. Our investigations have shown that especially Smoothed Integrated Gradients and DeepSHAP are particularly well suited for the visualisation of CNN classifications. The Smoothed Integrated Gradients technique also reveals advantages in robustness when evaluating degraded input images.","",""
9,"E. Ates, Burak Aksar, V. Leung, A. Coskun","Counterfactual Explanations for Machine Learning on Multivariate Time Series Data",2020,"","","","",5,"2022-07-13 09:22:24","","10.1109/ICAPAI49758.2021.9462056","","",,,,,9,4.50,2,4,2,"Applying machine learning (ML) on multivariate time series data has growing popularity in many application domains, including in computer system management. For example, recent high performance computing (HPC) research proposes a variety of ML frameworks that use system telemetry data in the form of multivariate time series so as to detect performance variations, perform intelligent scheduling or node allocation, and improve system security. Common barriers for adoption for these ML frameworks include the lack of user trust and the difficulty of debugging. These barriers need to be overcome to enable the widespread adoption of ML frameworks in production systems. To address this challenge, this paper proposes a novel explainability technique for providing counterfactual explanations for supervised ML frameworks that use multivariate time series data. The proposed method outperforms state-of-the-art explainability methods on several different ML frameworks and data sets in metrics such as faithfulness and robustness. The paper also demonstrates how the proposed method can be used to debug ML frameworks and gain a better understanding of HPC system telemetry data.","",""
11,"Alexander Hartl, Maximilian Bachl, J. Fabini, T. Zseby","Explainability and Adversarial Robustness for RNNs",2019,"","","","",6,"2022-07-13 09:22:24","","10.1109/BigDataService49289.2020.00030","","",,,,,11,3.67,3,4,3,"Recurrent Neural Networks (RNNs) yield attractive properties for constructing Intrusion Detection Systems (IDSs) for network data. With the rise of ubiquitous Machine Learning (ML) systems, malicious actors have been catching up quickly to find new ways to exploit ML vulnerabilities for profit. Recently developed adversarial ML techniques focus on computer vision and their applicability to network traffic is not straightforward: Network packets expose fewer features than an image, are sequential and impose several constraints on their features. We show that despite these completely different characteristics, adversarial samples can be generated reliably for RNNs. To understand a classifier's potential for misclassification, we extend existing explainability techniques and propose new ones, suitable particularly for sequential data. Applying them shows that already the first packets of a communication flow are of crucial importance and are likely to be targeted by attackers. Feature importance methods show that even relatively unimportant features can be effectively abused to generate adversarial samples. We thus introduce the concept of feature sensitivity which quantifies how much potential a feature has to cause misclassification. Since traditional evaluation metrics such as accuracy are not sufficient for quantifying the adversarial threat, we propose the Adversarial Robustness Score (ARS) for comparing IDSs and show that an adversarial training procedure can significantly and successfully reduce the attack surface.","",""
5,"P. Santhanam","Quality Management of Machine Learning Systems",2020,"","","","",7,"2022-07-13 09:22:24","","10.1007/978-3-030-62144-5_1","","",,,,,5,2.50,5,1,2,"","",""
0,"E. Ates, Burak Aksar, V. Leung, A. Coskun","Explainable Machine Learning Frameworks for Managing HPC Systems.",2020,"","","","",8,"2022-07-13 09:22:24","","10.2172/1829224","","",,,,,0,0.00,0,4,2,"Recent research on supercomputing proposes a variety of machine learning frameworks that are able to detect performance variations, find optimum application configurations, perform intelligent scheduling or node allocation, and improve system security. Although these goals align well with HPC systems’ needs, barriers such as the lack of user trust or the difficulty of debugging need to be overcome to enable the widespread adoption of such frameworks in production systems. This paper evaluates a new counterfactual time series explainability method and compares it against state-of-the-art explainability methods for supervised machine learning frameworks that use multivariate HPC system telemetry data. The counterfactual time series explainability method outperforms existing methods in terms of comprehensibility and robustness. We also show how explainability techniques can be used to debug machine learning frameworks and gain a better understanding of HPC system telemetry data.","",""
0,"J. Filipe, Ashish Ghosh, R. Prates, O. Shehory, E. Farchi, Guy Barash","Engineering Dependable and Secure Machine Learning Systems: Third International Workshop, EDSMLS 2020, New York City, NY, USA, February 7, 2020, Revised Selected Papers",2020,"","","","",9,"2022-07-13 09:22:24","","10.1007/978-3-030-62144-5","","",,,,,0,0.00,0,6,2,"","",""
5,"Florian Tambon, G. Laberge, Le An, Amin Nikanjam, Paulina Stevia Nouwou Mindom, Yann Pequignot, F. Khomh, G. Antoniol, E. Merlo, Franccois Laviolette","How to Certify Machine Learning Based Safety-critical Systems? A Systematic Literature Review",2021,"","","","",10,"2022-07-13 09:22:24","","10.1007/s10515-022-00337-x","","",,,,,5,5.00,1,10,1,"","",""
0,"Shuai Zhao, Yingzhou Peng, Yi Zhang, Huai Wang","Physics-informed Machine Learning for Parameter Estimation of DC-DC Converter",2022,"","","","",11,"2022-07-13 09:22:24","","10.1109/APEC43599.2022.9773482","","",,,,,0,0.00,0,4,1,"Although various machine learning-based methods have been proposed for condition monitoring in power elec-tronics, they are challenging to be implemented in practice due to the accuracy, data availability, computation burden, explainability, etc. Physics-informed machine learning (PIML) has been emerging as a promising direction where the above challenges can be mitigated by incorporating domain knowledge. In this paper, we propose a PIML- based parameter estimation method for a DC-DC Buck converter, as an exemplary application of PIML in power electronics. By seamlessly integrating a deep neural network and the converter physical model, it can estimate multiple component parameters simultaneously with high accuracy and robustness, while based on a limited dataset. It expects to provide a new perspective to tailor existing ML tools for power electronic applications.","",""
0,"B. Krishnamurthy, S. Shiva","Scalable Hindsight Experience Replay based Q-learning Framework with Explainability for Big Data Applications in Fog Computing",2022,"","","","",12,"2022-07-13 09:22:24","","10.1109/CCWC54503.2022.9720835","","",,,,,0,0.00,0,2,1,"Nowadays Internet of Things (IoT) applications are proliferating with explosive growth and their computational load is very high. Fog computing is an important structure used for processing the IoT devices data at cloud proximity. Big data applications demand scalable computing with stringent performance requirement. The currently available machine learning models do not match the growing scale of big data applications and lack explainability. In this paper an explainable Q-learning framework with hindsight experience replay (Q-HER) is developed to provide holistic scalability solution for big data applications using minimum number of fog nodes. The reward engineering process is streamlined and each episode with original goal and subset of other goals is repeated to yield high quality scalability policies. The mathematical modeling reveals that the generated scalability decisions satisfy the quality assurance parameters like correctness, robustness, model relevance, and ∊ -Differential Data privacy. The performance of the proposed Q-HER is found to be good towards the performance metrics like accuracy, latency, cost, and average resource wastage under two different scenarios of limited and unlimited processor fog nodes.","",""
11,"Y. Nishi, Satoshi Masuda, H. Ogawa, Keiji Uetsuki","A Test Architecture for Machine Learning Product",2018,"","","","",13,"2022-07-13 09:22:24","","10.1109/ICSTW.2018.00060","","",,,,,11,2.75,3,4,4,"As machine learning (ML) technology continues to spread by rapid evolution, the system or service using Machine Learning technology, called ML product, makes big impact on our life, society and economy. Meanwhile, Quality Assurance (QA) for ML product is quite more difficult than hardware, non-ML software and service because performance of ML technology is much better than non-ML technology in exchange for the characteristics of ML product, e.g. low explainability. We must keep rapid evolution and reduce quality risk of ML product simultaneously. In this paper, we show a Quality Assurance Framework for Machine Learning product. Scope of QA in this paper is limited to product evaluation. First, a policy of QA for ML Product is proposed. General principles of product evaluation is introduced and applied to ML product evaluation as a part of the policy. They are composed of A-ARAI: Allowability, Achievability, Robustness, Avoidability and Improvability. A strategy of ML Product Evaluation is constructed as another part of the policy. Quality Integrity Level for ML product is also modelled. Second, we propose a test architecture of ML product testing. It consists of test levels and fundamental test types of ML product testing, including snapshot testing, learning testing and confrontation testing. Finally, we defines QA activity levels for ML product.","",""
34,"Shubham Sharma, Jette Henderson, J. Ghosh","CERTIFAI: A Common Framework to Provide Explanations and Analyse the Fairness and Robustness of Black-box Models",2020,"","","","",14,"2022-07-13 09:22:24","","10.1145/3375627.3375812","","",,,,,34,17.00,11,3,2,"Concerns within the machine learning community and external pressures from regulators over the vulnerabilities of machine learning algorithms have spurred on the fields of explainability, robustness, and fairness. Often, issues in explainability, robustness, and fairness are confined to their specific sub-fields and few tools exist for model developers to use to simultaneously build their modeling pipelines in a transparent, accountable, and fair way. This can lead to a bottleneck on the model developer's side as they must juggle multiple methods to evaluate their algorithms. In this paper, we present a single framework for analyzing the robustness, fairness, and explainability of a classifier. The framework, which is based on the generation of counterfactual explanations through a custom genetic algorithm, is flexible, model-agnostic, and does not require access to model internals. The framework allows the user to calculate robustness and fairness scores for individual models and generate explanations for individual predictions which provide a means for actionable recourse (changes to an input to help get a desired outcome). This is the first time that a unified tool has been developed to address three key issues pertaining towards building a responsible artificial intelligence system.","",""
0,"Bernat Coma-Puig, J. Carmona","A Human-in-the-Loop Approach based on Explainability to Improve NTL Detection",2020,"","","","",15,"2022-07-13 09:22:24","","10.1109/ICDMW53433.2021.00123","","",,,,,0,0.00,0,2,2,"Implementing systems based on Machine Learning to detect fraud and other Non-Technical Losses (NTL) is challenging: the data available is biased, and the algorithms currently used are black-boxes that cannot be either easily trusted or understood by stakeholders. This work explains our human-in-the-loop approach to mitigate these problems in a real system that uses a supervised model to detect Non-Technical Losses (NTL) for an international utility company from Spain. This approach exploits human knowledge (e.g. from the data scientists or the company’s stakeholders) and the information provided by explanatory methods to guide the system during the training process. This simple, efficient method that can be easily implemented in other industrial projects is tested in a real dataset and the results show that the derived prediction model is better in terms of accuracy, interpretability, robustness and flexibility.","",""
1,"Ahmed M. A. Salih, I. Galazzo, Z. Raisi-Estabragh, S. Petersen, Polyxeni Gkontra, K. Lekadir, G. Menegaz, P. Radeva","A new scheme for the assessment of the robustness of Explainable Methods Applied to Brain Age estimation",2021,"","","","",16,"2022-07-13 09:22:24","","10.1109/CBMS52027.2021.00098","","",,,,,1,1.00,0,8,1,"Deep learning methods show great promise in a range of settings including the biomedical field. Explainability of these models is important in these fields for building end-user trust and to facilitate their confident deployment. Although several Machine Learning Interpretability tools have been proposed so far, there is currently no recognized evaluation standard to transfer the explainability results into a quantitative score. Several measures have been proposed as proxies for quantitative assessment of explainability methods. However, the robustness of the list of significant features provided by the explainability methods has not been addressed. In this work, we propose a new proxy for assessing the robustness of the list of significant features provided by two explainability methods. Our validation is defined at functionality-grounded level based on the ranked correlation statistical index and demonstrates its successful application in the framework of brain aging estimation. We assessed our proxy to estimate brain age using neuroscience data. Our results indicate small variability and high robustness in the considered explainability methods using this new proxy.","",""
7,"Sean Saito, Eugene Chua, Nicholas Capel, Rocco Hu","Improving LIME Robustness with Smarter Locality Sampling",2020,"","","","",17,"2022-07-13 09:22:24","","","","",,,,,7,3.50,2,4,2,"Explainability algorithms such as LIME have enabled machine learning systems to adopt transparency and fairness, which are important qualities in commercial use cases. However, recent work has shown that LIME's naive sampling strategy can be exploited by an adversary to conceal biased, harmful behavior. We propose to make LIME more robust by training a generative adversarial network to sample more realistic synthetic data which the explainer uses to generate explanations. Our experiments demonstrate that our proposed method demonstrates an increase in accuracy across three real-world datasets in detecting biased, adversarial behavior compared to vanilla LIME. This is achieved while maintaining comparable explanation quality, with up to 99.94\% in top-1 accuracy in some cases.","",""
0,"Sebastian A. Schober, Y. Bahri, Cecilia Carbonelli, R. Wille","Neural Network Robustness Analysis Using Sensor Simulations for a Graphene-Based Semiconductor Gas Sensor",2022,"","","","",18,"2022-07-13 09:22:24","","10.3390/chemosensors10050152","","",,,,,0,0.00,0,4,1,"Despite their advantages regarding production costs and flexibility, chemiresistive gas sensors often show drawbacks in reproducibility, signal drift and ageing. As pattern recognition algorithms, such as neural networks, are operating on top of raw sensor signals, assessing the impact of these technological drawbacks on the prediction performance is essential for ensuring a suitable measuring accuracy. In this work, we propose a characterization scheme to analyze the robustness of different machine learning models for a chemiresistive gas sensor based on a sensor simulation model. Our investigations are structured into four separate studies: in three studies, the impact of different sensor instabilities on the concentration prediction performance of the algorithms is investigated, including sensor-to-sensor variations, sensor drift and sensor ageing. In a further study, the explainability of the machine learning models is analyzed by applying a state-of-the-art feature ranking method called SHAP. Our results show the feasibility of model-based algorithm testing and substantiate the need for the thorough characterization of chemiresistive sensor algorithms before sensor deployment in order to ensure robust measurement performance.","",""
0,"Mark H. Meng, Guangdong Bai, S. Teo, Zhe Hou, Yan Xiao, Yun Lin, Jin Song Dong","Adversarial Robustness of Deep Neural Networks: A Survey from a Formal Verification Perspective",2022,"","","","",19,"2022-07-13 09:22:24","","10.1109/TDSC.2022.3179131","","",,,,,0,0.00,0,7,1,"—Neural networks have been widely applied in security applications such as spam and phishing detection, intrusion prevention, and malware detection. This black-box method, however, often has uncertainty and poor explainability in applications. Furthermore, neural networks themselves are often vulnerable to adversarial attacks. For those reasons, there is a high demand for trustworthy and rigorous methods to verify the robustness of neural network models. Adversarial robustness, which concerns the reliability of a neural network when dealing with maliciously manipulated inputs, is one of the hottest topics in security and machine learning. In this work, we survey existing literature in adversarial robustness veriﬁcation for neural networks and collect 39 diversiﬁed research works across machine learning, security, and software engineering domains. We systematically analyze their approaches, including how robustness is formulated, what veriﬁcation techniques are used, and the strengths and limitations of each technique. We provide a taxonomy from a formal veriﬁcation perspective for a comprehensive understanding of this topic. We classify the existing techniques based on property speciﬁcation, problem reduction, and reasoning strategies. We also demonstrate representative techniques that have been applied in existing studies with a sample model. Finally, we discuss open questions for future research.","",""
0,"Erick Galinkin","Robustness and Usefulness in AI Explanation Methods",2022,"","","","",20,"2022-07-13 09:22:24","","10.48550/arXiv.2203.03729","","",,,,,0,0.00,0,1,1,"Explainability in machine learning has become incredibly important as machine learning-powered systems become ubiquitous and both regulation and public sentiment begin to demand an understanding of how these systems make decisions. As a result, a number of explanation methods have begun to receive widespread adoption. This work summarizes, compares, and contrasts three popular explanation methods: LIME, SmoothGrad, and SHAP. We evaluate these methods with respect to: robustness, in the sense of sample complexity and stability; understandability, in the sense that provided explanations are consistent with user expectations; and usability, in the sense that the explanations allow for the model to be modified based on the output. This work concludes that current explanation methods are insufficient; that putting faith in and adopting these methods may actually be worse than simply not using them.","",""
0,"L. Oneto, Nicolò Navarin, B. Biggio, Federico Errica, A. Micheli, F. Scarselli, M. Bianchini, A. Sperduti","Complex Data: Learning Trustworthily, Automatically, and with Guarantees",2021,"","","","",21,"2022-07-13 09:22:24","","10.14428/esann/2021.es2021-6","","",,,,,0,0.00,0,8,1,". Machine Learning (ML) achievements enabled automatic extraction of actionable information from data in a wide range of decision-making scenarios. This demands for improving both ML technical aspects (e.g., design and automation) and human-related metrics (e.g., fairness, robustness, privacy, and explainability), with performance guarantees at both levels. The aforementioned scenario posed three main challenges: (i) Learning from Complex Data (i.e., sequence, tree, and graph data), (ii) Learning Trustworthily, and (iii) Learning Automatically with Guarantees. The focus of this special session is on addressing one or more of these challenges with the ﬁnal goal of Learning Trustworthily, Automatically, and with Guarantees from Complex Data.","",""
1,"G. Vouros","Explainable Deep Reinforcement Learning: State of the Art and Challenges",2022,"","","","",22,"2022-07-13 09:22:24","","10.1145/3527448","","",,,,,1,1.00,1,1,1,"Interpretability, explainability and transparency are key issues to introducing Artificial Intelligence methods in many critical domains: This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability and fairness, and has important consequences towards keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. While the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article we aim to provide a review of state of the art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators - i.e., of those that take the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state of the art methods, categorizing them in classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes identifying open questions and important challenges.","",""
5,"Martin Pawelczyk, Shalmali Joshi, Chirag Agarwal, Sohini Upadhyay, Himabindu Lakkaraju","On the Connections between Counterfactual Explanations and Adversarial Examples",2021,"","","","",23,"2022-07-13 09:22:24","","","","",,,,,5,5.00,1,5,1,"Counterfactual explanations and adversarial examples have emerged as critical research areas for addressing the explainability and robustness goals of machine learning (ML). While counterfactual explanations were developed with the goal of providing recourse to individuals adversely impacted by algorithmic decisions, adversarial examples were designed to expose the vulnerabilities of ML models. While prior research has hinted at the commonalities between these frameworks, there has been little to no work on systematically exploring the connections between the literature on counterfactual explanations and adversarial examples. In this work, we make one of the first attempts at formalizing the connections between counterfactual explanations and adversarial examples. More specifically, we theoretically analyze salient counterfactual explanation and adversarial example generation methods, and highlight the conditions under which they behave similarly. Our analysis demonstrates that several popular counterfactual explanation and adversarial example generation methods such as the ones proposed by Wachter et. al. and Carlini and Wagner (with mean squared error loss), and C-CHVAE and natural adversarial examples by Zhao et. al. are equivalent. We also bound the distance between counterfactual explanations and adversarial examples generated by Wachter et. al. and DeepFool methods for linear models. Finally, we empirically validate our theoretical findings using extensive experimentation with synthetic and real world datasets.","",""
3,"E. Ates, Burak Aksar, V. Leung, A. Coskun","Counterfactual Explanations for Multivariate Time Series",2021,"","","","",24,"2022-07-13 09:22:24","","10.1109/ICAPAI49758.2021.9462056","","",,,,,3,3.00,1,4,1,"Multivariate time series are used in many science and engineering domains, including health-care, astronomy, and high-performance computing. A recent trend is to use machine learning (ML) to process this complex data and these ML-based frameworks are starting to play a critical role for a variety of applications. However, barriers such as user distrust or difficulty of debugging need to be overcome to enable widespread adoption of such frameworks in production systems. To address this challenge, we propose a novel explainability technique, CoMTE, that provides counterfactual explanations for supervised machine learning frameworks on multivariate time series data. Using various machine learning frameworks and data sets, we compare CoMTE with several state-of-the-art explainability methods and show that we outperform existing methods in comprehensibility and robustness. We also show how CoMTE can be used to debug machine learning frameworks and gain a better understanding of the underlying multivariate time series data.","",""
2,"R. Soklaski, Justin A. Goodwin, Olivia M. Brown, Michael Yee, J. Matterer","Tools and Practices for Responsible AI Engineering",2022,"","","","",25,"2022-07-13 09:22:24","","","","",,,,,2,2.00,0,5,1,"Responsible Artificial Intelligence (AI)—the practice of developing, evaluating, and maintaining accurate AI systems that also exhibit essential properties such as robustness and explainability—represents a multifaceted challenge that often stretches standard machine learning tooling, frameworks, and testing methods beyond their limits. In this paper, we present two new software libraries—hydra-zen and the rAI-toolbox—that address critical needs for responsible AI engineering. hydra-zen dramatically simplifies the process of making complex AI applications configurable, and their behaviors reproducible. The rAI-toolbox is designed to enable methods for evaluating and enhancing the robustness of AI-models in a way that is scalable and that composes naturally with other popular ML frameworks. We describe the design principles and methodologies that make these tools effective, including the use of property-based testing to bolster the reliability of the tools themselves. Finally, we demonstrate the composability and flexibility of the tools by showing how various use cases from adversarial robustness and explainable AI can be concisely implemented with familiar APIs.","",""
1,"Jiyi Zhang, E. Chang, H. Lee","Confusing and Detecting ML Adversarial Attacks with Injected Attractors",2020,"","","","",26,"2022-07-13 09:22:24","","10.1145/3488932.3497752","","",,,,,1,0.50,0,3,2,"Many machine learning adversarial attacks find adversarial samples of a victim model M by following the gradient of some attack objective functions, either explicitly or implicitly. To confuse and detect such attacks, we take the proactive approach that modifies those functions with the goal of misleading the attacks to some local minima, or to some designated regions that can be easily picked up by an analyzer. To achieve this goal, we propose adding a large number of artifacts, which we called attractors, onto the otherwise smooth function. An attractor is a point in the input space, where samples in its neighborhood have gradient pointing toward it. We observe that decoders of watermarking schemes exhibit properties of attractors and give a generic method that injects attractors from a watermark decoder into the victim model M. This principled approach allows us to leverage on known watermarking schemes for scalability and robustness and provides explainability of the outcomes. Experimental studies show that our method has competitive performance. For instance, for un-targeted attacks on CIFAR-10 dataset, we can reduce the overall attack success rate of DeepFool to 1.9%, whereas known defense LID, FS and MagNet can reduce the rate to 90.8%, 98.5% and 78.5% respectively.","",""
0,"Joao Marques-Silva","Automated Reasoning in Explainable AI",2021,"","","","",27,"2022-07-13 09:22:24","","10.3233/faia210109","","",,,,,0,0.00,0,1,1,"The envisioned applications of machine learning (ML) in high-risk and safetycritical applications hinge on systems that are robust in their operation and that can be trusted. Automated reasoning offers the solution to ensure robustness and to guarantee trust. This talk overviews recent efforts on applying automated reasoning tools in explaining black-box (and so non-interpretable) ML models [6], and relates such efforts with past work on reasoning about inconsistent logic formulas [11]. Moreover, the talk details the computation of rigorous explanations of black-box models, and how these serve for assessing the quality of widely used heuristic explanation approaches. The talk also covers important properties of rigorous explanations, including duality relationships between different kinds of explanations [7,5,4]. Finally, the talk briefly overviews ongoing work on mapping practical efficient [8,3] but also tractable explainability [9,10,2,1].","",""
0,"Romeo Kienzler, I. Nesic","CLAIMED, a visual and scalable component library for Trusted AI",2021,"","","","",28,"2022-07-13 09:22:24","","10.25080/majora-1b6fd038-007","","",,,,,0,0.00,0,2,1,"Deep Learning models are getting more and more popular but constraints on explainability, adversarial robustness and fairness are often major concerns for production deployment. Although the open source ecosystem is abundant on addressing those concerns, fully integrated, end to end systems are lacking in open source. Therefore we provide an entirely open source, reusable component framework, visual editor and execution engine for production grade machine learning on top of Kubernetes, a joint effort between IBM and the University Hospital Basel. It uses Kubeflow Pipelines, the AI Explainability360 toolkit, the AI Fairness360 toolkit and the Adversarial Robustness Toolkit on top of Elyra, Kubeflow, Kubernetes and JupyterLab. Using the Elyra pipeline editor, AI pipelines can be developed visually with a set of jupyter notebooks. We explain how we've created a COVID-19 deep learning classification pipeline based on CT scans. We use the toolkit to highlight parts of the images which have been crucial for the models decisions. We detect bias against age and gender and finally, show how to deploy the model to KFServing to share it across different hospital data centers of the Swiss Personalized Health Network. © 2021 CEUR-WS. All rights reserved.","",""
0,"Hatma Suryotrisongko, Y. Musashi, A. Tsuneda, K. Sugitani","Robust Botnet DGA Detection: Blending XAI and OSINT for Cyber Threat Intelligence Sharing",2022,"","","","",29,"2022-07-13 09:22:24","","10.1109/ACCESS.2022.3162588","","",,,,,0,0.00,0,4,1,"We investigated 12 years DNS query logs of our campus network and identified phenomena of malicious botnet domain generation algorithm (DGA) traffic. DGA-based botnets are difficult to detect using cyber threat intelligence (CTI) systems based on blocklists. Artificial intelligence (AI)/machine learning (ML)-based CTI systems are required. This study (1) proposed a model to detect DGA-based traffic based on statistical features with datasets comprising 55 DGA families, (2) discussed how CTI can be expanded with computable CTI paradigm, and (3) described how to improve the explainability of the model outputs by blending explainable AI (XAI) and open-source intelligence (OSINT) for trust problems, an antidote for skepticism to the shared models and preventing automation bias. We define the XAI-OSINT blending as aggregations of OSINT for AI/ML model outcome validation. Experimental results show the effectiveness of our models (96.3% accuracy). Our random forest model provides better robustness against three state-of-the-art DGA adversarial attacks (CharBot, DeepDGA, MaskDGA) compared with character-based deep learning models (Endgame, CMU, NYU, MIT). We demonstrate the sharing mechanism and confirm that the XAI-OSINT blending improves trust for CTI sharing as evidence to validate our proposed computable CTI paradigm to assist security analysts in security operations centers using an automated, explainable OSINT approach (for second opinion). Therefore, the computable CTI reduces manual intervention in critical cybersecurity decision-making.","",""
0,"Olivia M. Brown, B. Dillman","Proceedings of the Robust Artificial Intelligence System Assurance (RAISA) Workshop 2022",2022,"","","","",30,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,2,1,"The Robust Artificial Intelligence System Assurance (RAISA) workshop will focus on research, development and application of robust artificial intelligence (AI) and machine learning (ML) systems. Rather than studying robustness with respect to particular ML algorithms, our approach will be to explore robustness assurance at the system architecture level, during both development and deployment, and within the human-machine teaming context. While the research community is converging on robust solutions for individual AI models in specific scenarios, the problem of evaluating and assuring the robustness of an AI system across its entire life cycle is much more complex. Moreover, the operational context in which AI systems are deployed necessitates consideration of robustness and its relation to principles of fairness, privacy, and explainability.","",""
0,"Gregory Scafarto, Nicolas Posocco, Antoine Bonnefoy","Calibrate to Interpret",2022,"","","","",31,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,3,1,". Trustworthy machine learning is driving a large number of ML community works in order to improve ML acceptance and adoption. The main aspect of trustworthy machine learning are the followings: fairness, uncertainty, robustness, explainability and formal guaranties. Each of these individual domains gains the ML community interest, visible by the number of related publications. However few works tackle the inter-connection between these ﬁelds. In this paper we show a ﬁrst link between uncertainty and explainability, by studying the relation between calibration and interpretation. As the calibration of a given model changes the way it scores samples, and interpretation approaches often rely on these scores, it seems safe to assume that the conﬁdence-calibration of a model interacts with our ability to interpret such model. In this paper, we show, in the context of networks trained on image classiﬁcation tasks, to what extent interpretations are sensitive to conﬁdence-calibration. It leads us to suggest a simple practice to improve the interpretation outcomes : Calibrate to Interpret .","",""
0,"Qimin Liu, F. Liu","Selective Cascade of Residual ExtraTrees",2020,"","","","",32,"2022-07-13 09:22:24","","10.1007/s42979-020-00358-x","","",,,,,0,0.00,0,2,2,"","",""
1,"Laura Isabel Galindez Olascoaga, Wannes Meert, Nimish Shah, Guy Van den Broeck, M. Verhelst","On Hardware-Aware Probabilistic Frameworks for Resource Constrained Embedded Applications",2019,"","","","",33,"2022-07-13 09:22:24","","10.1109/EMC2-NIPS53020.2019.00023","","",,,,,1,0.33,0,5,3,"Edge reasoning attempts to mitigate latency and privacy shortcomings of cloud computing paradigms. However, it introduces additional challenges linked to the devices' resource constraints and the applications' dynamic conditions. To address these challenges, we have proposed a hardware-aware probabilistic framework that optimizes the target machine learning model under actual hardware constraints. This framework relies on tractable probabilistic models, as they facilitate efficient inference, while exhibiting a number of traits relevant to the application range of interest: robustness to missing data, joint prediction capabilities, explainability, and small data needs. In this work, we expand on this framework by introducing a discriminative-generative approach to model learning, which retains the robustness of a generative model under missing data but can potentially improve its discriminative performance. In addition, we demonstrate how the applicability of this framework goes beyond classification tasks, and can be used for density estimation tasks, relevant to applications such as mobile speaker verification.","",""
1,"Quang-Vinh Dang","Improving the performance of the intrusion detection systems by the machine learning explainability",2021,"","","","",34,"2022-07-13 09:22:24","","10.1108/ijwis-03-2021-0022","","",,,,,1,1.00,1,1,1," Purpose This study aims to explain the state-of-the-art machine learning models that are used in the intrusion detection problem for human-being understandable and study the relationship between the explainability and the performance of the models.   Design/methodology/approach The authors study a recent intrusion data set collected from real-world scenarios and use state-of-the-art machine learning algorithms to detect the intrusion. The authors apply several novel techniques to explain the models, then evaluate manually the explanation. The authors then compare the performance of model post- and prior-explainability-based feature selection.   Findings The authors confirm our hypothesis above and claim that by forcing the explainability, the model becomes more robust, requires less computational power but achieves a better predictive performance.   Originality/value The authors draw our conclusions based on their own research and experimental works. ","",""
0,"A. Agogino, Ritchie Lee, D. Giannakopoulou","Machine Learning Explainability and Transferability for Path Navigation",2020,"","","","",35,"2022-07-13 09:22:24","","10.2514/6.2021-1885","","",,,,,0,0.00,0,3,2,"Deep neural networks are powerful tools for machine perception. Unfortunately their decisions are difficult to explain due to the complexity and size of the networks. Previously we have alleviated this issue by using the representational portion of a deep neural network and combining it with a :-nearest neighbor (KNN) classifier. Through inspection of the decisions made by the KNN, we can directly see the training data responsible for the decisions, allowing us to determine the quality of the overall decision and the quality of the representational layer of the deep NN. While the technique worked well, it requires tens of thousands of latent vectors to be stored for classification. In addition, it lacks the ability to show how parts of an image influence the classification decision. Here we address these issues by 1) Using a radial basis function network (RBFN) in place of the KNN allowing far fewer images to be used in deployment and 2) Using an autoencoder network for explainability. In addition to these techniques, we examine the effects of transfer learning to determine that results are robust. All results are tested on a domain where an unmanned aerial vehicle (UAV) navigates a forest trail through a single camera.","",""
102,"Nadia Burkart, M. Huber","A Survey on the Explainability of Supervised Machine Learning",2020,"","","","",36,"2022-07-13 09:22:24","","10.1613/jair.1.12228","","",,,,,102,51.00,51,2,2,"Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.","",""
8,"Dongqi Han, Zhiliang Wang, Ying Zhong, Wenqi Chen, Jiahai Yang, Shuqiang Lu, Xingang Shi, Xia Yin","Evaluating and Improving Adversarial Robustness of Machine Learning-Based Network Intrusion Detectors",2020,"","","","",37,"2022-07-13 09:22:24","","10.1109/JSAC.2021.3087242","","",,,,,8,4.00,1,8,2,"Machine learning (ML), especially deep learning (DL) techniques have been increasingly used in anomaly-based network intrusion detection systems (NIDS). However, ML/DL has shown to be extremely vulnerable to adversarial attacks, especially in such security-sensitive systems. Many adversarial attacks have been proposed to evaluate the robustness of ML-based NIDSs. Unfortunately, existing attacks mostly focused on feature-space and/or white-box attacks, which make impractical assumptions in real-world scenarios, leaving the study on practical gray/black-box attacks largely unexplored. To bridge this gap, we conduct the first systematic study of the gray/black-box traffic-space adversarial attacks to evaluate the robustness of ML-based NIDSs. Our work outperforms previous ones in the following aspects: (i) practical —the proposed attack can automatically mutate original traffic with extremely limited knowledge and affordable overhead while preserving its functionality; (ii) generic —the proposed attack is effective for evaluating the robustness of various NIDSs using diverse ML/DL models and non-payload-based features; (iii) explainable —we propose an explanation method for the fragile robustness of ML-based NIDSs. Based on this, we also propose a defense scheme against adversarial attacks to improve system robustness. We extensively evaluate the robustness of various NIDSs using diverse feature sets and ML/DL models. Experimental results show our attack is effective (e.g., >97% evasion rate in half cases for Kitsune, a state-of-the-art NIDS) with affordable execution cost and the proposed defense method can effectively mitigate such attacks (evasion rate is reduced by >50% in most cases).","",""
34,"D. Simester, Artem Timoshenko, S. Zoumpoulis","Targeting Prospective Customers: Robustness of Machine-Learning Methods to Typical Data Challenges",2020,"","","","",38,"2022-07-13 09:22:24","","10.1287/mnsc.2019.3308","","",,,,,34,17.00,11,3,2,"We investigate how firms can use the results of field experiments to optimize the targeting of promotions when prospecting for new customers. We evaluate seven widely used machine-learning methods using a series of two large-scale field experiments. The first field experiment generates a common pool of training data for each of the seven methods. We then validate the seven optimized policies provided by each method together with uniform benchmark policies in a second field experiment. The findings not only compare the performance of the targeting methods, but also demonstrate how well the methods address common data challenges. Our results reveal that when the training data are ideal, model-driven methods perform better than distance-driven methods and classification methods. However, the performance advantage vanishes in the presence of challenges that affect the quality of the training data, including the extent to which the training data captures details of the implementation setting. The challenges we study are covariate shift, concept shift, information loss through aggregation, and imbalanced data. Intuitively, the model-driven methods make better use of the information available in the training data, but the performance of these methods is more sensitive to deterioration in the quality of this information. The classification methods we tested performed relatively poorly. We explain the poor performance of the classification methods in our setting and describe how the performance of these methods could be improved. This paper was accepted by Matthew Shum, marketing.","",""
37,"Leif Hancox-Li","Robustness in machine learning explanations: does it matter?",2020,"","","","",39,"2022-07-13 09:22:24","","10.1145/3351095.3372836","","",,,,,37,18.50,37,1,2,"The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.","",""
0,"Wei Chen, Xiangkui Li, Lu Ma, Dong Li","Enhancing Robustness of Machine Learning Integration With Routine Laboratory Blood Tests to Predict Inpatient Mortality After Intracerebral Hemorrhage",2022,"","","","",40,"2022-07-13 09:22:24","","10.3389/fneur.2021.790682","","",,,,,0,0.00,0,4,1,"Objective: The accurate evaluation of outcomes at a personalized level in patients with intracerebral hemorrhage (ICH) is critical clinical implications. This study aims to evaluate how machine learning integrates with routine laboratory tests and electronic health records (EHRs) data to predict inpatient mortality after ICH. Methods: In this machine learning-based prognostic study, we included 1,835 consecutive patients with acute ICH between October 2010 and December 2018. The model building process incorporated five pre-implant ICH score variables (clinical features) and 13 out of 59 available routine laboratory parameters. We assessed model performance according to a range of learning metrics, such as the mean area under the receiver operating characteristic curve [AUROC]. We also used the Shapley additive explanation algorithm to explain the prediction model. Results: Machine learning models using laboratory data achieved AUROCs of 0.71–0.82 in a split-by-year development/testing scheme. The non-linear eXtreme Gradient Boosting model yielded the highest prediction accuracy. In the held-out validation set of development cohort, the predictive model using comprehensive clinical and laboratory parameters outperformed those using clinical alone in predicting in-hospital mortality (AUROC [95% bootstrap confidence interval], 0.899 [0.897–0.901] vs. 0.875 [0.872–0.877]; P <0.001), with over 81% accuracy, sensitivity, and specificity. We observed similar performance in the testing set. Conclusions: Machine learning integrated with routine laboratory tests and EHRs could significantly promote the accuracy of inpatient ICH mortality prediction. This multidimensional composite prediction strategy might become an intelligent assistive prediction for ICH risk reclassification and offer an example for precision medicine.","",""
0,"A. Arslan","RETHINKING ROBUSTNESS IN MACHINE LEARNING: USE OF GENERATIVE ADVERSARIAL NETWORKS FOR ENHANCED ROBUSTNESS",2020,"","","","",41,"2022-07-13 09:22:24","","10.26483/ijarcs.v13i1.6801","","",,,,,0,0.00,0,1,2,"Machine learning (ML) is increasingly being used in real-world applications, so understanding the uncertainty and robustness of a model is necessary to ensure performance in practice. This paper explores approximations for robustness which can meaningfully explain the behavior of any black box model. Starting with a discussion on components of a robust model this paper offers some techniques based on the Generative Adversarial Network (GAN) approach to improve the robustness of a model. The study concludes that a clear understanding of robust models for ML allows improving information for practitioners, and helps to develop tools that assess the robustness of ML. Also, ML tools and libraries could benefit from a clear understanding on how information should be presented and how these tools are used.","",""
0,"M. Hind, Dennis Wei, Yunfeng Zhang","Consumer-Driven Explanations for Machine Learning Decisions: An Empirical Study of Robustness",2020,"","","","",42,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,3,2,"Many proposed methods for explaining machine learning predictions are in fact challenging to understand for nontechnical consumers. This paper builds upon an alternative consumer-driven approach called TED that asks for explanations to be provided in training data, along with target labels. Using semi-synthetic data from credit approval and employee retention applications, experiments are conducted to investigate some practical considerations with TED, including its performance with different classification algorithms, varying numbers of explanations, and variability in explanations. A new algorithm is proposed to handle the case where some training examples do not have explanations. Our results show that TED is robust to increasing numbers of explanations, noisy explanations, and large fractions of missing explanations, thus making advances toward its practical deployment.","",""
0,"Christos Kokkotis, S. Moustakidis, E. Papageorgiou, G. Giakas, D. Tsaopoulos","A Machine Learning workflow for Diagnosis of Knee Osteoarthritis with a focus on post-hoc explainability",2020,"","","","",43,"2022-07-13 09:22:24","","10.1109/IISA50023.2020.9284354","","",,,,,0,0.00,0,5,2,"Knee Osteoarthritis (KOA) is a multifactorial disease-causing joint pain, deformity and dysfunction. The aim of this paper is to provide a data mining approach that could identify important risk factors which contribute to the diagnosis of KOA and their impact on model output, with a focus on posthoc explainability. Data were obtained from the osteoarthritis initiative (OAI) database enrolling people, with nonsymptomatic KOA and symptomatic KOA or being at high risk of developing KOA. The current study considered multidisciplinary data from heterogeneous sources such as questionnaire data, physical activity indexes, self-reported data about joint symptoms, disability and function as well as general health and physical exams’ data from individuals with or without KOA from the baseline visit. For the data mining part, a robust feature selection methodology was employed consisting of filter, wrapper and embedded techniques whereas feature ranking was decided on the basis of a majority vote scheme. The validation of the extracted factors was performed in subgroups employing seven well-known classifiers. A 77.88 % classification accuracy was achieved by Logistic Regression on the group of the first forty selected (40) risk factors. We investigated the behavior of the best model, with respect to classification errors and the impact of used features, to confirm their clinical relevance. The interpretation of the model output was performed by SHAP. The results are the basis for the development of easy-to-use diagnostic tools for clinicians for the early detection of KOA.","",""
0,"Mimansa Jaiswal","Interpreting Multimodal Machine Learning Models Trained for Emotion Recognition to Address Robustness and Privacy Concerns",2020,"","","","",44,"2022-07-13 09:22:24","","10.1609/aaai.v34i10.7130","","",,,,,0,0.00,0,1,2,"Many mobile applications and virtual conversational agents now aim to recognize and adapt to emotions. These predicted emotions are used in variety of downstream applications: (a) generating more human like dialogues, (b) predicting mental health issues, and (c) hate speech detection and intervention. To enable this, data are transmitted from users' devices and stored on central servers. These data are then processed further, either annotated or used as inputs for training a model for a specific task. Yet, these data contain sensitive information that could be used by mobile applications without user's consent or, maliciously, by an eavesdropping adversary. My work focuses on two major issues that are faced while training emotion recognition algorithms: (a) privacy of the generated representations and, (b) explaining and ensuring that the predictions are robust to various situations. Tackling these issues would lead to emotion based algorithms that are deployable and helpful at a larger scale, thus enabling more human like experience when interacting with AI.","",""
312,"Jonas Rauber, Wieland Brendel, M. Bethge","Foolbox: A Python toolbox to benchmark the robustness of machine learning models",2017,"","","","",45,"2022-07-13 09:22:24","","","","",,,,,312,62.40,104,3,5,"Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox. The most up-to-date documentation can be found at http://foolbox.readthedocs.io. In 2013, Szegedy et al. demonstrated that minimal perturbations, often almost imperceptible to humans, can have devastating effects on machine predictions. These so-called adversarial perturbations thus demonstrate a striking difference between human and machine perception. As a result, adversarial perturbations have been subject to many Equal contribution Centre for Integrative Neuroscience, University of Tübingen, Germany Bernstein Center for Computational Neuroscience, Tübingen, Germany International Max Planck Research School for Intelligent Systems, Tübingen, Germany Max Planck Institute for Biological Cybernetics, Tübingen, Germany Institute for Theoretical Physics, University of Tübingen, Germany. Correspondence to: Jonas Rauber <jonas.rauber@bethgelab.org>. Reliable Machine Learning in the Wild Workshop, 34 th International Conference on Machine Learning, Sydney, Australia, 2017. studies concerning the generation of such perturbations and strategies to protect machine learning models such as deep neural networks against them. A practical definition of the robustness R of a model, first used by Szegedy et al. (2013), is the average size of the minimum adversarial perturbation ρ(x) across many samples x, R = 〈ρ(x)〉 x where (1) ρ(x) = min δ d(x,x+ δ) s.t. x+ δ is adversarial (2) and d(·) is some distance measure. Unfortunately, finding the global minimum adversarial perturbation is close to impossible in any practical setting, and we thus employ heuristic attacks to find a suitable approximation. Such heuristics, however, can fail, in which case we could easily be mislead to believe that a model is robust (Brendel & Bethge, 2017). Our best strategy is thus to employ as many attacks as possible, and to use the minimal perturbation found across all attacks as an approximation to the true global minimum. At the moment, however, such a strategy is severely obstructed by two problems: first, the code for most known attack methods is either not available at all, or only available for one particular deep learning framework. Second, implementations of the same attack often differ in many details and are thus not directly comparable. Foolbox improves upon the existing Python package cleverhans by Papernot et al. (2016b) in three important aspects: 1. It interfaces with most popular machine learning frameworks such as PyTorch, Keras, TensorFlow, Theano, Lasagne and MXNet and provides a straight forward way to add support for other frameworks, 2. it provides reference implementations for more than 15 adversarial attacks with a simple and consistent API, and 3. it supports many different criteria for adversarial examples, including custom ones. This technical report is structured as follows: In section 1 we provide an overview over Foolbox and demonstrate Foolbox: A Python toolbox to benchmark the robustness of machine learning models how to benchmark a model and report the result. In section 2 we describe the adversarial attack methods that are implemented in Foolbox and explain the internal hyperparameter tuning.","",""
0,"R. Shokri","Trusting Machine Learning: Privacy, Robustness, and Transparency Challenges",2019,"","","","",46,"2022-07-13 09:22:24","","10.1145/3335203.3335728","","",,,,,0,0.00,0,1,3,"Machine learning algorithms have shown an unprecedented predictive power for many complex learning tasks. As they are increasingly being deployed in large scale critical applications for processing various types of data, new questions related to their trustworthiness would arise. Can machine learning algorithms be trusted to have access to individuals’ sensitive data? Can they be robust against noisy or adversarially perturbed data? Can we reliably interpret their learning process, and explain their predictions? In this talk, I will go over the challenges of building trustworthy machine learning algorithms in centralized and distributed (federated) settings, and will discuss the inter-relation between privacy, robustness, and interpretability.","",""
243,"Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, R. Puri, J. Moura, P. Eckersley","Explainable machine learning in deployment",2019,"","","","",47,"2022-07-13 09:22:24","","10.1145/3351095.3375624","","",,,,,243,81.00,24,10,3,"Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.","",""
224,"R. Roscher, B. Bohn, Marco F. Duarte, J. Garcke","Explainable Machine Learning for Scientific Insights and Discoveries",2019,"","","","",48,"2022-07-13 09:22:24","","10.1109/ACCESS.2020.2976199","","",,,,,224,74.67,56,4,3,"Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.","",""
11,"Markus Jaeger, Stephan Krügel, D. Marinelli, J. Papenbrock, P. Schwendner","Interpretable Machine Learning for Diversified Portfolio Construction",2020,"","","","",49,"2022-07-13 09:22:24","","10.2139/ssrn.3730144","","",,,,,11,5.50,2,5,2,"In this article, the authors construct a pipeline to benchmark hierarchical risk parity (HRP) relative to equal risk contribution (ERC) as examples of diversification strategies allocating to liquid multi-asset futures markets with dynamic leverage (volatility target). The authors use interpretable machine learning concepts (explainable AI) to compare the robustness of the strategies and to back out implicit rules for decision-making. The empirical dataset consists of 17 equity index, government bond, and commodity futures markets across 20 years. The two strategies are back tested for the empirical dataset and for about 100,000 bootstrapped datasets. XGBoost is used to regress the Calmar ratio spread between the two strategies against features of the bootstrapped datasets. Compared to ERC, HRP shows higher Calmar ratios and better matches the volatility target. Using Shapley values, the Calmar ratio spread can be attributed especially to univariate drawdown measures of the asset classes. TOPICS: Quantitative methods, statistical methods, big data/machine learning, portfolio construction, performance measurement Key Findings ▪ The authors introduce a procedure to benchmark rule-based investment strategies and to explain the differences in path-dependent risk-adjusted performance measures using interpretable machine learning. ▪ They apply the procedure to the Calmar ratio spread between hierarchical risk parity (HRP) and equal risk contribution (ERC) allocations of a multi-asset futures portfolio and find HRP to have superior risk-adjusted performance. ▪ The authors regress the Calmar ratio spread against statistical features of bootstrapped futures return datasets using XGBoost and apply the SHAP framework by Lundberg and Lee (2017) to discuss the local and global feature importance.","",""
2,"V. Kulkarni, M. Gawali, A. Kharat","Key Technology Considerations in Developing and Deploying Machine Learning Models in Clinical Radiology Practice",2021,"","","","",50,"2022-07-13 09:22:24","","10.2196/28776","","",,,,,2,2.00,1,3,1,"The use of machine learning to develop intelligent software tools for the interpretation of radiology images has gained widespread attention in recent years. The development, deployment, and eventual adoption of these models in clinical practice, however, remains fraught with challenges. In this paper, we propose a list of key considerations that machine learning researchers must recognize and address to make their models accurate, robust, and usable in practice. We discuss insufficient training data, decentralized data sets, high cost of annotations, ambiguous ground truth, imbalance in class representation, asymmetric misclassification costs, relevant performance metrics, generalization of models to unseen data sets, model decay, adversarial attacks, explainability, fairness and bias, and clinical validation. We describe each consideration and identify the techniques used to address it. Although these techniques have been discussed in prior research, by freshly examining them in the context of medical imaging and compiling them in the form of a laundry list, we hope to make them more accessible to researchers, software developers, radiologists, and other stakeholders.","",""
1,"Khansa Rasheed, A. Qayyum, M. Ghaly, A. Al-Fuqaha, A. Razi, Junaid Qadir","Explainable, Trustworthy, and Ethical Machine Learning for Healthcare: A Survey",2021,"","","","",51,"2022-07-13 09:22:24","","10.36227/TECHRXIV.14376179.V1","","",,,,,1,1.00,0,6,1,"With the advent of machine learning (ML) applications in daily life, the questions about liability, trust, and interpretability of their outputs are raising, especially for healthcare applications. The black-box nature of ML models is a roadblock for clinical utilization. Therefore, to gain the trust of clinicians and patients, researchers need to provide explanations of how and why the model is making a specific decision. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the field of eXplainable ML (XML). In this paper, we provide a comprehensive review of explainable and interpretable ML techniques implemented for providing the reasons behind their decisions for various healthcare applications. Along with highlighting various security, safety, and robustness challenges that hinder the trustworthiness of ML we also discussed the ethical issues of healthcare ML and describe how explainable and trustworthy ML can resolve these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.","",""
1,"Nicolas Jourdan, S. Sen, E. J. Husom, Enrique Garcia-Ceja, Tobias Biegel, J. Metternich","On The Reliability Of Machine Learning Applications In Manufacturing Environments",2021,"","","","",52,"2022-07-13 09:22:24","","","","",,,,,1,1.00,0,6,1,"The increasing deployment of advanced digital technologies such as Internet of Things (IoT) devices and Cyber-Physical Systems (CPS) in industrial environments is enabling the productive use of machine learning (ML) algorithms in the manufacturing domain. As ML applications transcend from research to productive use in real-world industrial environments, the question of reliability arises. Since the majority of ML models are trained and evaluated on static datasets, continuous online monitoring of their performance is required to build reliable systems. Furthermore, concept and sensor drift can lead to degrading accuracy of the algorithm over time, thus compromising safety, acceptance and economics if undetected and not properly addressed. In this work, we exemplarily highlight the severity of the issue on a publicly available industrial dataset which was recorded over the course of 36 months and explain possible sources of drift. We assess the robustness of ML algorithms commonly used in manufacturing and show, that the accuracy strongly declines with increasing drift for all tested algorithms. We further investigate how uncertainty estimation may be leveraged for online performance estimation as well as drift detection as a first step towards continually learning applications. The results indicate, that ensemble algorithms like random forests show the least decay of confidence calibration under drift.","",""
1,"Tonni Das","Performance Analysis of Quantum Machine Learning Classifiers",2021,"","","","",53,"2022-07-13 09:22:24","","","","",,,,,1,1.00,1,1,1,"In recent years, researchers have started looking into data transformations in quantum computation. They want to see how quantum computing affects the robustness and performance of machine learning methods. Quantum mechanics succeed in explaining some phenomena where classical formulas failed in the past. Thus, it expanded in analytical research fields such as Quantum Machine Learning (QML) over the years. The developing QML discipline has proven solutions to issues that are equivalent (or comparable) to those addressed by classical machine learning, including classification and prediction problems using quantum classifiers. As a result of these factors, quantum classifier analysis has become one of the most important topics in QML. This paper studies four quantum classifiers: Support Vector Classification with Quantum Kernel (SVCQK), Quantum Support Vector Classifier (QSVC), Variational Quantum Classifier (VQC), and Circuit Quantum Neural Network Classifier (CQNNC). We also report case study outcomes and results analysis utilizing linearly and non-linearly separable datasets generated. Our research is to explore if quantum information may aid learning or convergence. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.","",""
0,"Zhixin Pan, P. Mishra","Automated Detection of Spectre and Meltdown Attacks Using Explainable Machine Learning",2021,"","","","",54,"2022-07-13 09:22:24","","10.1109/HOST49136.2021.9702278","","",,,,,0,0.00,0,2,1,"Spectre and Meltdown attacks exploit security vulnerabilities of advanced architectural features to access inherently concealed memory data without authorization. Existing defense mechanisms have three major drawbacks: (i) they can be fooled by obfuscation techniques, (ii) the lack of transparency severely limits their applicability, and (iii) it can introduce unacceptable performance degradation. In this paper, we propose a novel detection scheme based on explainable machine learning to address these fundamental challenges. Specifically, this paper makes three important contributions. (1) Our work is the first attempt in applying explainable machine learning for Spectre and Meltdown attack detection. (2) Our proposed method utilizes the temporal differences of hardware events in sequential timestamps instead of overall statistics, which contributes to the robustness of ML models against evasive attacks. (3) Extensive experimental evaluation demonstrates that our approach can significantly improve detection efficiency (38.4% on average) compared to state-of-the-art techniques.","",""
0,"S. E. Whang, Ki Hyun Tae, Yuji Roh, Geon Heo","Responsible AI Challenges in End-to-end Machine Learning",2021,"","","","",55,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,4,1,"Responsible AI is becoming critical as AI is widely used in our everyday lives. Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more. In addition, these objectives are not only relevant to model training, but to all steps of end-to-end machine learning, which include data collection, data cleaning and validation, model training, model evaluation, and model management and serving. Finally, responsible AI is conceptually challenging, and supporting all the objectives must be as easy as possible. We thus propose three key research directions towards this vision – depth, breadth, and usability – to measure progress and introduce our ongoing research. First, responsible AI must be deeply supported where multiple objectives like fairness and robust must be handled together. To this end, we propose FR-Train, a holistic framework for fair and robust model training in the presence of data bias and poisoning. Second, responsible AI must be broadly supported, preferably in all steps of machine learning. Currently we focus on the data pre-processing steps and propose Slice Tuner, a selective data acquisition framework for training fair and accurate models, and MLClean, a data cleaning framework that also improves fairness and robustness. Finally, responsible AI must be usable where the techniques must be easy to deploy and actionable. We propose FairBatch, a batch selection approach for fairness that is effective and simple to use, and Slice Finder, a model evaluation tool that automatically finds problematic slices. We believe we scratched the surface of responsible AI for end-to-end machine learning and suggest research challenges moving forward.","",""
0,"W. Zong, Yang-Wai Chow, W. Susilo","Visual Analysis of Adversarial Examples in Machine Learning",2021,"","","","",56,"2022-07-13 09:22:24","","10.1007/978-981-33-6726-5_4","","",,,,,0,0.00,0,3,1,"","",""
0,"Eike Petersen, Yannik Potdevin, Esfandiar Mohammadi, S. Zidowitz, Sabrina Breyer, Dirk Nowotka, Sandra Henn, Ludwig Pechmann, M. Leucker, P. Rostalski, C. Herzog","Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Technical Challenges and Solutions",2021,"","","","",57,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,11,1,"Machine learning is expected to fuel significant improvements in medical care. To ensure that fundamental principles such as beneficence, respect for human autonomy, prevention of harm, justice, privacy, and transparency are respected, medical machine learning applications must be developed responsibly. A large number of high-level declarations of ethical principles have been put forth for this purpose, but there is a severe lack of technical guidelines explicating the practical consequences for medical machine learning. Similarly, there is currently considerable uncertainty regarding the exact regulatory requirements placed upon medical machine learning systems. In this paper, we survey the technical challenges involved in creating medical machine learning systems responsibly and in conformity with existing regulations, as well as possible solutions to address these challenges. We begin by providing a brief overview of existing regulations affecting medical machine learning, showing that properties such as safety, robustness, reliability, privacy, security, transparency, explainability, and nondiscrimination are all demanded already by existing law and regulations — albeit, in many cases, to an uncertain degree. Next, we discuss the key technical obstacles to achieving these desirable properties, and important techniques to overcome those barriers in the medical context. Since most of the technical challenges are very young and new problems frequently emerge, the scientific discourse is rapidly evolving and has not yet converged on clear best-practice solutions. Nevertheless, we aim to illuminate the underlying technical challenges, possible ways for addressing them, and their respective merits and drawbacks. In particular, we notice that distribution shift, spurious correlations, model underspecification, and data scarcity represent severe challenges in the medical context (and others) that are very difficult to solve with classical black-box deep neural networks. Important measures that may help to address these challenges include the use of large and representative datasets and federated learning as a means to that end, the careful exploitation of domain knowledge wherever feasible, the use of inherently transparent models, comprehensive model testing and verification, as well as stakeholder inclusion. ar X iv :2 10 7. 09 54 6v 1 [ cs .L G ] 2 0 Ju l 2 02 1","",""
0,"Véronique M. Gomes, P. Melo-Pinto","Towards robust Machine Learning models for grape ripeness assessment",2021,"","","","",58,"2022-07-13 09:22:24","","10.1109/JCSSE53117.2021.9493822","","",,,,,0,0.00,0,2,1,"Artificial intelligence methods need to be more transparent for wider acceptance by the industry. In particular deep neural networks (DNN) are not explainable, due to the complex processes the input undergo. The present work addresses model explainability for wine grapes quality assessment through 1D-CNN, using regression activation maps (RAM) to show the contribution score of each wavelength for the prediction of sugar content. This way we identify the relevant regions related to this enological parameter. The results obtained indicate that the proposed approach can successfully highlight important spectral regions related to sugars absorption, improving the current state of the art, and opening way to dimensionality reduction methods and further model interpretation.","",""
2,"A. Smart, Larry James, B. Hutchinson, Simone Wu, Shannon Vallor","Why Reliabilism Is not Enough: Epistemic and Moral Justification in Machine Learning",2020,"","","","",59,"2022-07-13 09:22:24","","10.1145/3375627.3375866","","",,,,,2,1.00,0,5,2,"In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of \em justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed wide adoption of machine learning? We argue that, in general, people implicitly adoptreliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method \citegoldman2012reliabilism. We argue that, in cases where model deployments require \em moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral ""wrapper'' around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification---moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.","",""
2,"Yousef Sheikhi Garjan, Mehdi Ghaneezabadi","Machine Learning Interpretability Application to Optimize Well Completion in Montney",2020,"","","","",60,"2022-07-13 09:22:24","","10.2118/200019-ms","","",,,,,2,1.00,1,2,2,"Recently machine learning has being extensively deployed for oil and gas industry for improving result and expedite process. However, the black box models do not explain their prediction which considered as a barrier to adopt machine learning. This paper is about optimizing hydraulic fracture with machine learning methods and making informative decision with interpreting machine learning model. The solution can show that it could save over million dollars per well and improve well performance significantly. Interestingly, the machine leaning explainability approach was utilized to explain and measure the reason behind of why some wells are performing better than other and vice versa.Hydraulic fracturing modeling and optimization in tight oil and unconventional reservoir requires substantial geological modeling, fracture design, post-fracture production simulation with excessive sensitivity analysis due to complexity and uncertainty in the nature of data. These types of studies are computationally and monetarily expensive. Furthermore, digital oil technology has facilitated the process of data gathering enabled operators to have access to huge amount of data. Common approaches are no longer suitable to handle this pile of data but machine learning methods could be successfully utilized for this purpose.In this paper, a variety types of advanced machine learning methods including linear regression, Random forest, Gradient Boost, XGBoost, Bagging, ExtraTrees and neural network were employed to optimize well completion in Montney formation. The objective was to create a robust predictive model capturing all the effective operational well parameters (features) capable of optimizing the first 12 months cumulative of equivalent well production.Special Individual Conditional Expectation (ICE) plots and Partial Dependency plots(PDP) were used to depict how HF completion features influence the prediction of a machine learning model. Furthermore, a novel approach was employed to explain the model prediction of an existing well by computing the contribution of each feature to the prediction.Over 1838 hydraulically fractured (HF) wells producing from 2008 till 2019 in Montney formation have been considered for this analysis. The outcome of Explanatory Data Analysis (EDA) revealed that well production performance has not been improved despite of continues enhancement of hydraulic fracture parameters such as proppant injected volume, length of stimulated horizontal wells, and number of stages per well in the course of two years. This finding raises the concern of whether operators are properly optimizing completion design. After comparing all machine learning methods, Random Forest method was chosen as the most appropriate and accurate method to proceed for further analysis. ICE and PDP plots helped to understand the impacts of different fracturing features on production for individual well in addition to define optimum operation features on Montney Formation. Furthermore, quantifying of each feature’s impact on individual well production and linking it to an economic model, we were able to demonstrate potential profit and loss for each well. The model suggests that some wells could have achieved over $1 million extra profit during the first 12-months of production.In this study, not only a reliable predictive data-driven model has been built for hydraulically-fractured wells in Montney formation, but also a comprehensive workflow of sensitivity and explainatability analysis has been introduced to obtain an optimized fit-to-purpose well completion design.","",""
6,"Farzin Piltan, Jong-Myon Kim","Bearing Fault Identification Using Machine Learning and Adaptive Cascade Fault Observer",2020,"","","","",61,"2022-07-13 09:22:24","","10.3390/app10175827","","",,,,,6,3.00,3,2,2,"In this work, a hybrid procedure for bearing fault identification using a machine learning and adaptive cascade observer is explained. To design an adaptive cascade observer, the normal signal approximation is the first step. Therefore, the fuzzy orthonormal regressive (FOR) technique was developed to approximate the acoustic emission (AE) and vibration (non-stationary and nonlinear) bearing signals in normal conditions. After approximating the normal signal of bearing using the FOR technique, the adaptive cascade observer is modeled in four steps. First, the linear observation technique using a FOR proportional-integral (PI) observer (FOR-PIO) is developed. In the second step, to increase the power of uncertaintie rejection (robustness) of the FOR-PIO, the structure procedure is used serially. Next, the fuzzy like observer is selected to increase the accuracy of FOR structure PI observer (FOR-SPIO). Moreover, the adaptive technique is used to develop the reliability of the cascade (fuzzy-structure PI) observer. Additionally to fault identification, the machine-learning algorithm using a support vector machine (SVM) is recommended. The effectiveness of the adaptive cascade observer with the SVM fault identifier was validated by a vibration and AE datasets. Based on the results, the average vibration and AE fault diagnosis using the adaptive cascade observer with the SVM fault identifier are 97.8% and 97.65%, respectively.","",""
2,"Markus Jaeger, Stephan Krügel, D. Marinelli, J. Papenbrock, P. Schwendner","Understanding Machine Learning for Diversified Portfolio Construction by Explainable AI",2020,"","","","",62,"2022-07-13 09:22:24","","10.2139/ssrn.3528616","","",,,,,2,1.00,0,5,2,"In this paper, we construct a pipeline to investigate heuristic diversification strategies in asset allocation. We use machine learning concepts (""explainable AI"") to compare the robustness of different strategies and back out implicit rules for decision making.    In a first step, we augment the asset universe (the empirical dataset) with a range of scenarios generated with a block bootstrap from the empirical dataset.    Second, we backtest the candidate strategies over a long period of time, checking their performance variability. Third, we use XGBoost as a regression model to connect the difference between the measured performances between two strategies to a pool of statistical features of the portfolio universe tailored to the investigated strategy.    Finally, we employ the concept of Shapley values to extract the relationships that the model could identify between the portfolio characteristics and the statistical properties of the asset universe.    We test this pipeline for studying risk-parity strategies with a volatility target, and in particular, comparing the machine learning-driven Hierarchical Risk Parity (HRP) to the classical Equal Risk Contribution (ERC) strategy.    In the augmented dataset built from a multi-asset investment universe of commodities, equities and fixed income futures, we find that HRP better matches the volatility target, and shows better risk-adjusted performances. Finally, we train XGBoost to learn the difference between the realized Calmar ratios of HRP and ERC and extract explanations.    The explanations provide fruitful ex-post indications of the connection between the statistical properties of the universe and the strategy performance in the training set. For example, the model confirms that features addressing the hierarchical properties of the universe are connected to the relative performance of HRP respect to ERC.","",""
0,"Tae-Hyun Chun, Yong Yu","Enhancing CHF Prediction of AECL Look-Up Table Along with Machine Learning",2020,"","","","",63,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,2,2,"A critical heat flux (CHF) is a key safety parameter. For the CHF prediction, artificial neural network has been also applied and showed good performances [1, 2]. However, it is hardly accepted in the nuclear community due to a drawback of ‘Explainability’. A machine learning, as a subset of the artificial intelligence, can play a supplementary role for a more robust domain knowledge-based model. AECL Look-up Table (LUT) is widely used for the CHF prediction in reactor thermal-hydraulic design and safety analyses [3]. This domain knowledge model can predict the CHF by two schemes such as DSM (Direct Substitute Method) and HBM (Heat Balance Method). The uncertainty is much large in the DSM relative to the HBM. But the DSM is practically used in the nuclear engineering since HBM requires iterations to reach the heat balance in the CHF prediction. The purpose of this study is to show a feasibility that a machine learning-aided CHF LUT model enhances considerably the accuracy of the CHF prediction.","",""
1,"Chih-Yuan Yang, R. Sahita","Towards a Resilient Machine Learning Classifier - a Case Study of Ransomware Detection",2020,"","","","",64,"2022-07-13 09:22:24","","","","",,,,,1,0.50,1,2,2,"The damage caused by crypto-ransomware, due to encryption, is difficult to revert and cause data losses. In this paper, a machine learning (ML) classifier was built to early detect ransomware (called crypto-ransomware) that uses cryptography by program behavior. If a signature-based detection was missed, a behavior-based detector can be the last line of defense to detect and contain the damages. We find that input/output activities of ransomware and the file-content entropy are unique traits to detect crypto-ransomware. A deep-learning (DL) classifier can detect ransomware with a high accuracy and a low false positive rate. We conduct an adversarial research against the models generated. We use simulated ransomware programs to launch a gray-box analysis to probe the weakness of ML classifiers and to improve model robustness. In addition to accuracy and resiliency, trustworthiness is the other key criteria for a quality detector. Making sure that the correct information was used for inference is important for a security application. The Integrated Gradient method was used to explain the deep learning model and also to reveal why false negatives evade the detection. The approaches to build and to evaluate a real-world detector were demonstrated and discussed.","",""
1,"Korn Sooksatra, Pablo Rivas","A Review of Machine Learning and Cryptography Applications",2020,"","","","",65,"2022-07-13 09:22:24","","10.1109/CSCI51800.2020.00105","","",,,,,1,0.50,1,2,2,"Adversarially robust neural cryptography deals with the training of a neural-based model using an adversary to leverage the learning process in favor of reliability and trustworthiness. The adversary can be a neural network or a strategy guided by a neural network. These mechanisms are proving successful in finding secure means of data protection. Similarly, machine learning benefits significantly from the cryptography area by protecting models from being accessible to malicious users. This paper is a literature review on the symbiotic relationship between machine learning and cryptography. We explain cryptographic algorithms that have been successfully applied in machine learning problems and, also, deep learning algorithms that have been used in cryptography. We pay special attention to the exciting and relatively new area of adversarial robustness.","",""
242,"Pantelis Linardatos, Vasilis Papastefanopoulos, S. Kotsiantis","Explainable AI: A Review of Machine Learning Interpretability Methods",2020,"","","","",66,"2022-07-13 09:22:24","","10.3390/e23010018","","",,,,,242,121.00,81,3,2,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.","",""
932,"Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael A. Specter, Lalana Kagal","Explaining Explanations: An Overview of Interpretability of Machine Learning",2018,"","","","",67,"2022-07-13 09:22:24","","10.1109/DSAA.2018.00018","","",,,,,932,233.00,155,6,4,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.","",""
1,"Dorota Toczydlowska","Machine learning developments in dependency modelling and feature extraction",2020,"","","","",68,"2022-07-13 09:22:24","","","","",,,,,1,0.50,1,1,2,"Three complementary feature extraction approaches are developed in this thesis which addresses the challenge of dimensionality reduction in the presence of multivariate heavy-tailed and asymmetric distributions. First, we demonstrate how to improve the robustness of the standard Probabilistic Principal Component Analysis by adapting the concept of robust mean and covariance estimation within the standard framework. We then introduce feature extraction methods that extend the standard Principal Component Analysis by exploring distribution-based robustification. This is achieved via Probabilistic Principal Component Analysis (PPCA), in which new, statistically robust variants are derived, also treating missing data. We propose a novel generalisation to the t-Student Probabilistic Principal Component methodology which (1) accounts for asymmetric distribution of the observation data, (2) is a framework for grouped and generalised multiple-degree-of-freedom structures, which provides a more flexible framework to model groups of marginal tail dependence in the observation data, and (3) separates the tail effect of the error terms and factors. The new feature extraction methods are derived in an incomplete data setting to efficiently handle the presence of missing values in the observation vector. We discuss statistical properties of their robustness. In the next part of this thesis, we demonstrate the applicability of feature extraction methods to the statistical analysis of multidimensional dynamics. We introduce the class of Hybrid Factor models that combines classical state-space model formulations with incorporation of exogenous factors. We show how to utilize the information obtained from features extracted using introduced robust PPCA in a modelling framework in a meaningful and parsimonious manner. In the first application study, we show the applicability of robust feature extraction methods in the real data environment of financial markets and combine the obtained results with a stochastic multi-factor panel regression-based state-space model in order to model the dynamic of yield curves, whilst incorporating regression factors. We embed the rank-reduced feature extractions into a stochastic representation of state-space models for yield curve dynamics and compare the results to classical multi-factor dynamic Nelson-Siegel state-space models. This leads to important new representations of yield curve models that can have practical importance for addressing questions of financial stress testing and monetary policy interventions which can efficiently incorporate financial big data. We illustrate our results on various financial and macroeconomic data sets from the Euro Zone and international markets. In the second study, we develop a multi-factor extension of the family of Lee-Carter stochastic mortality models. We build upon the time, period and cohort stochastic model structure to include exogenous observable demographic features that can be used as additional factors to improve model fit and forecasting accuracy. We develop a framework in which (a) we employ projection-based techniques of dimensionality reduction that are amenable to different structures of demographic data; (b) we analyse demographic data sets from the patterns of missingness and the impact of such missingness on the feature extraction; (c) we introduce a class of multi-factor stochastic mortality models incorporating time, period, cohort and demographic features, which we develop within a Bayesian state-space estimation framework. Finally (d) we develop an efficient combined Markov chain and filtering framework for sampling the posterior and forecasting. We undertake a detailed case study on the Human Mortality Database demographic data from European countries and we use the extracted features to better explain the term structure of mortality in the UK over time for male and female populations. This is compared to a pure Lee-Carter stochastic mortality model, demonstrating that our feature extraction framework and consequent multi-factor mortality model improves both in-sample fit and, importantly, out-of-sample mortality forecasts by a non-trivial gain in performance.","",""
2,"Chiara Giola, Piero Danti, S. Magnani","Learning Curves: A Novel Approach for Robustness Improvement of Load Forecasting",2021,"","","","",69,"2022-07-13 09:22:24","","10.3390/ENGPROC2021005038","","",,,,,2,2.00,1,3,1,"In the age of AI, companies strive to extract benefits from data. In the first steps of data analysis, an arduous dilemma scientists have to cope with is the definition of the ’right’ quantity of data needed for a certain task. In particular, when dealing with energy management, one of the most thriving application of AI is the consumption’s optimization of energy plant generators. When designing a strategy to improve the generators’ schedule, a piece of essential information is the future energy load requested by the plant. This topic, in the literature it is referred to as load forecasting, has lately gained great popularity; in this paper authors underline the problem of estimating the correct size of data to train prediction algorithms and propose a suitable methodology. The main characters of this methodology are the Learning Curves, a powerful tool to track algorithms performance whilst data training-set size varies. At first, a brief review of the state of the art and a shallow analysis of eligible machine learning techniques are offered. Furthermore, the hypothesis and constraints of the work are explained, presenting the dataset and the goal of the analysis. Finally, the methodology is elucidated and the results are discussed.","",""
81,"Vaishak Belle, I. Papantonis","Principles and Practice of Explainable Machine Learning",2020,"","","","",70,"2022-07-13 09:22:24","","10.3389/fdata.2021.688969","","",,,,,81,40.50,41,2,2,"Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.","",""
0,"S. Al-Zaiti, Alaa A. Alghwiri, Xiao Hu, G. Clermont, A. Peace, P. Macfarlane, R. Bond","A clinician’s guide to understanding and critically appraising machine learning studies: a checklist for Ruling Out Bias Using Standard Tools in Machine Learning (ROBUST-ML)",2022,"","","","",71,"2022-07-13 09:22:24","","10.1093/ehjdh/ztac016","","",,,,,0,0.00,0,7,1,"  Developing functional machine learning (ML)-based models to address unmet clinical needs requires unique considerations for optimal clinical utility. Recent debates about the rigours, transparency, explainability, and reproducibility of ML models, terms which are defined in this article, have raised concerns about their clinical utility and suitability for integration in current evidence-based practice paradigms. This featured article focuses on increasing the literacy of ML among clinicians by providing them with the knowledge and tools needed to understand and critically appraise clinical studies focused on ML. A checklist is provided for evaluating the rigour and reproducibility of the four ML building blocks: data curation, feature engineering, model development, and clinical deployment. Checklists like this are important for quality assurance and to ensure that ML studies are rigourously and confidently reviewed by clinicians and are guided by domain knowledge of the setting in which the findings will be applied. Bridging the gap between clinicians, healthcare scientists, and ML engineers can address many shortcomings and pitfalls of ML-based solutions and their potential deployment at the bedside.","",""
0,"Alireza Rezazadeh, Yasamin Jafarian, A. Kord","Explainable Ensemble Machine Learning for Breast Cancer Diagnosis based on Ultrasound Image Texture Features",2022,"","","","",72,"2022-07-13 09:22:24","","10.3390/forecast4010015","","",,,,,0,0.00,0,3,1,"Image classification is widely used to build predictive models for breast cancer diagnosis. Most existing approaches overwhelmingly rely on deep convolutional networks to build such diagnosis pipelines. These model architectures, although remarkable in performance, are black-box systems that provide minimal insight into the inner logic behind their predictions. This is a major drawback as the explainability of prediction is vital for applications such as cancer diagnosis. In this paper, we address this issue by proposing an explainable machine learning pipeline for breast cancer diagnosis based on ultrasound images. We extract first- and second-order texture features of the ultrasound images and use them to build a probabilistic ensemble of decision tree classifiers. Each decision tree learns to classify the input ultrasound image by learning a set of robust decision thresholds for texture features of the image. The decision path of the model predictions can then be interpreted by decomposing the learned decision trees. Our results show that our proposed framework achieves high predictive performance while being explainable.","",""
0,"David Melching, Tobias Strohmann, Guillermo Requena, E. Breitbarth","Explainable machine learning for precise fatigue crack tip detection",2022,"","","","",73,"2022-07-13 09:22:24","","10.1038/s41598-022-13275-1","","",,,,,0,0.00,0,4,1,"","",""
0,"Tyler Cody, Justin Kauffman, J. Krometis, Daniel Sobien, Laura J. Freeman","Combinatorial coverage framework for machine learning in multi-domain operations",2022,"","","","",74,"2022-07-13 09:22:24","","10.1117/12.2617117","","",,,,,0,0.00,0,5,1,"Multi-domain operations (MDO) are characterized by simultaneous and sequential operations; rapid and continuous integration; and surprise. Machine learning (ML) for MDO is no different. Translated into ML, MDO requires highly assured yet rapid data and model fusion. Assurance demands robustness, reliability, and explain-ability, while speed demands computational efficiency and sample efficiency. Combinatorial interaction testing offers explainable and rigorous techniques to ML for fusing data and models with runtime guarantees. But such methods are underexplored in the literature. Combinatorial coverage has been applied to neuron- and layer-levels of neural networks, but only recently to ML in general. There are also ongoing debates of efficacy in the literature, but these debates are scoped to explainable deep learning. This work presents a framework for using combinatorial coverage for multi-domain operations. We discuss how coverage metrics can incorporate multi-modal meta-data and mission context into fusion processes, how coverage is oriented towards identifying gaps in and between sets of data, and how coverage can identify cases where performance is expected to be difficult. We conclude that combinatorial coverage should be considered a core capability for supporting ML in MDO.","",""
34,"A. Binder, M. Bockmayr, M. Hägele, S. Wienert, D. Heim, Katharina Hellweg, M. Ishii, A. Stenzinger, A. Hocke, C. Denkert, K. Müller, F. Klauschen","Morphological and molecular breast cancer profiling through explainable machine learning",2021,"","","","",75,"2022-07-13 09:22:24","","10.1038/S42256-021-00303-4","","",,,,,34,34.00,3,12,1,"","",""
3,"F. Lécué, B. Abeloos, Jonathan Anctil, Manuel Bergeron, Damien Dalla-Rosa, Simon Corbeil-Letourneau, Florian Martet, Tanguy Pommellet, L. Salvan, Simon Veilleux, M. Ziaeefard","Thales XAI Platform: Adaptable Explanation of Machine Learning Systems - A Knowledge Graphs Perspective",2019,"","","","",76,"2022-07-13 09:22:24","","","","",,,,,3,1.00,0,11,3,"Explanation in Machine Learning systems has been identified to be the main asset to have for large scale deployment of Artificial Intelligence (AI) in critical systems. Explanations could be example-, features-, semantics-based or even counterfactual to potentially action on an AI system; they could be represented in many different ways e.g., textual, graphical, or visual. All representations serve different means, purpose and operators. We built the first-of-its-kind XAI (eXplainable AI) platform for critical systems i.e., Thales XAI Platform which aims at serving explanations through various forms. This paper emphasizes on the semantics-based explanations for Machine Learning systems. 1 Explainable AI in Critical Systems Motivation: The current hype of Artificial Intelligence (AI) mostly refers to the success of Machine Learning (ML) and its sub-domain of deep learning. However industries operating with critical systems are either highly regulated, or require high level of certification and robustness. Therefore, such industry constraints do limit the adoption of non deterministic and ML systems. Answers to the question of explainability will be intrinsically connected to the adoption of AI in industry at scale. Indeed explanation, which could be used for debugging intelligent systems or deciding to follow a recommendation in real-time, will increase acceptance and (business) user trust. Explainable AI (XAI) is now referring to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. Focus: Thales XAI Platform is designed to provide explanation for a ML task (classification, regression, object detection, segmentation). Although Thales XAI Platform does provide different levels of explanation e.g., example-based, featuresbased, counterfactual using textual and visual representations, we emphasis only on the semantics-based explanation through knowledge graphs. ? Copyright c © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Critical Applications: From adapting a plane trajectory, stopping a train, refitting a boat to reconfiguring a satellite, all are examples of critical situations where explanation is a must-to-have to follow an AI system decision. 2 Why Knowledge Graphs for Explainable AI? State-of-the-Art Limitations: Most approaches limits explanation of ML systems to features involved in the data and model, or at best to examples, prototypes or counterfactuals. Explanation should go beyond correlation (features importance) and numerical similarity (local explanation). Opportunity: By expanding and linking initial (training, validation and test) data with entities in knowledge graphs, (i) context is encoded, (ii) connections and relations are exposed, and (iii) inference and causation are natively supported. Knowledge graphs are used for encoding better representation of data, structuring a ML model in a more interpretable way, and adopt a semantic similarity for local (instance-based) and global (model-based) explanation. 3 Thales XAI Platform: A Knowledge Graph Perspective (Semantic) Perspective: The platform is combining ML and reasoning functionalities to expose a human-like rational as explanation when (i) recognizing an object (in a raw image) of any class in a knowledge graph, (ii) predicting a link in a knowledge graph. Thales XAI Platform is using state-of-the-art Semantic Web tools for enriching input, output (class) data with DBpedia (4, 233, 000 resources) and domain-specific knowledge graphs, usually enterprise knowledge graphs. This is a crucial step for contextualizing training, validation, test data. Explainable ML Classifications: Starting from raw images, as unstructured data, but with class labels augmented with a domain knowledge graph, Thales XAI Platform relies on existing neural network architectures to build the most appropriate models. All confidence scores of output classes on any input image are updated based on the semantic description of the output classes. For instance, an input classified as a car will have a higher overall confidence score in case some properties of car in the knowledge graph are retrieved e.g., having wheels, being on a road. In addition the platform is embedding naturally explanation i.e., properties of the objects retrieved in both the raw data and knowledge graph. Explainable Relational Learning: Starting from relational data, structured as graph, and augmented with a domain knowledge graph, Thales XAI Platform relies on existing knowledge graph embeddings frameworks to build the most appropriate models. Explanation of any link prediction is retrieved by identifying representative hotspots in the knowledge graph i.e., connected parts of the graphs that negatively impact prediction accuracy when removed.","",""
2,"Sahar Daraeizadeh, S. Premaratne, Xiaoyu Song, M. Perkowski, A. Matsuura","Machine-learning based three-qubit gate for realization of a Toffoli gate with cQED-based transmon systems",2019,"","","","",77,"2022-07-13 09:22:24","","10.1103/PHYSREVA.102.012601","","",,,,,2,0.67,0,5,3,"We use machine learning techniques to design a 50 ns three-qubit flux-tunable controlled-controlled-phase gate with fidelity of >99.99% for nearest-neighbor coupled transmons in circuit quantum electrodynamics architectures. We explain our gate design procedure where we enforce realistic constraints, and analyze the new gate's robustness under decoherence, distortion, and random noise. Our controlled-controlled-phase gate in combination with two single-qubit gates realizes a Toffoli gate which is widely used in quantum circuits, logic synthesis, quantum error correction, and quantum games.","",""
1,"Quico Spaen","Applications and Advances in Similarity-based Machine Learning",2019,"","","","",78,"2022-07-13 09:22:24","","","","",,,,,1,0.33,1,1,3,"Author(s): Spaen, Quico Pepijn | Advisor(s): Hochbaum, Dorit S | Abstract: Similarity-based machine learning methods differ from traditional machine learning methods in that they also use pairwise similarity relations between objects to infer the labels of unlabeled objects. A recent comparative study for classification problems by Baumann et al. [2019] demonstrated that similarity-based techniques have superior performance and robustness when compared to well-established machine learning techniques. Similarity-based machine learning methods benefit from two advantages that could explain superior their performance: They can make use of the pairwise relations between unlabeled objects, and they are robust due to the transitive property of pairwise similarities. A challenge for similarity-based machine learning methods on large datasets is that the number of pairwise similarity grows quadratically in the size of the dataset. For large datasets, it thus becomes practically impossible to compute all possible pairwise similarities. In 2016, Hochbaum and Baumann proposed the technique of sparse computation to address this growth by computing only those pairwise similarities that are relevant. Their proposed implementation of sparse computation is still difficult to scale to millions objects. This dissertation focuses on advancing the practical implementations of sparse computation to larger datasets and on two applications for which similarity-based machine learning was particularly effective. The applications that are studied here are cell identification in calcium-imaging movies and detecting aberrant linking behavior in directed networks. For sparse computation we present faster, geometric algorithms and a technique, named sparse-reduced computation, that combines sparse computation with compression. The geometric algorithms compute the exact same output as the original implementation of sparse computation, but identify the relevant pairwise similarities faster by using the concept of data shifting for identifying objects in the same or neighboring blocks. Empirical results on datasets with up to 10 million objects show a significant reduction in running time. Sparse-reduced computation combines sparse computation with a technique for compressing highly-similar or identical objects, enabling the use of similarity-based machine learning on massively-large datasets. The computational results demonstrate that sparse-reduced computation provides a significant reduction in running time with a minute loss in accuracy.A major problem facing neuroscientists today is cell identification in calcium-imaging movies. These movies are in-vivo recordings of thousands of neurons at cellular resolution. There is a great need for automated approaches to extract the activity of single neurons from these movies since manual post-processing takes tens of hours per dataset. We present the HNCcorr algorithm for cell identification in calcium-imaging movies. The name HNCcorr is derived from its use of the similarity-based Hochbaum's Normalized Cut (HNC) model with pairwise similarities derived from correlation. In HNCcorr, the task of cell detection is approached as a clustering problem. HNCcorr utilizes HNC to detect cells in these movies as coherent clusters of pixels that are highly distinct from the remaining pixels. HNCcorr guarantees, unlike existing methodologies for cell identification, a globally optimal solution to the underlying optimization problem. Of independent interest is a novel method, named similarity-squared, that we devised for measuring similarity between pixels. We provide an experimental study and demonstrate that HNCcorr is a top performer on the Neurofinder cell identification benchmark and that it improves over algorithms based on matrix factorization.The second application is detecting aberrant agents, such as fake news sources or spam websites, based on their link behavior in networks. Across contexts, a distinguishing characteristic between normal and aberrant agents is that normal agents rarely link to aberrant ones. We refer to this phenomenon as aberrant linking behavior. We present an Markov Random Fields (MRF) formulation, with links as the pairwise similarities, that detects aberrant agents based on aberrant linking behavior and any prior information (if given). This MRF formulation is solved optimally and in polynomial time. We compare the optimal solution for the MRF formulation to well-known algorithms based on random walks. In our empirical experiment with twenty-three different datasets, the MRF method outperforms the other detection algorithms. This work represents the first use of optimization methods for detecting aberrant agents as well as the first time that MRF is applied to directed graphs.","",""
1,"Sicheng Jiang, Sirui Lu, D. Deng","Adversarial Machine Learning Phases of Matter",2019,"","","","",79,"2022-07-13 09:22:24","","","","",,,,,1,0.33,0,3,3,"We study the robustness of machine learning approaches to adversarial perturbations, with a focus on supervised learning scenarios. We find that typical phase classifiers based on deep neural networks are extremely vulnerable to adversarial perturbations: adding a tiny amount of carefully crafted noises into the original legitimate examples will cause the classifiers to make incorrect predictions at a notably high confidence level. Through the lens of activation maps, we find that some important underlying physical principles and symmetries remain to be adequately captured for classifiers with even near-perfect performance. This explains why adversarial perturbations exist for fooling these classifiers. In addition, we find that, after adversarial training the classifiers will become more consistent with physical laws and consequently more robust to certain kinds of adversarial perturbations. Our results provide valuable guidance for both theoretical and experimental future studies on applying machine learning techniques to condensed matter physics.","",""
1981,"C. Rudin","Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",2018,"","","","",80,"2022-07-13 09:22:24","","10.1038/S42256-019-0048-X","","",,,,,1981,495.25,1981,1,4,"","",""
0,"Saeed Mahloujifar","A Complexity Theoretic Approach to Adversarial Machine Learning",2019,"","","","",81,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,1,3,"With the ever increasing applications of machine learning algorithms many new challenges, beyond accuracy, have been raised. Among them, and one of the most important ones, is robustness against adversarial attacks. The persistent impact of these attacks on the security of otherwise successful machine learning algorithms begs a fundamental investigation. My research aims at building a foundation to systematically investigate robustness of machine learning algorithms in the presence of different adversaries. Two special cases of security threats, which have been the focus of many studies in the recent years, are evasion attacks and poisoning attacks. Evasion attacks occur during the inference phase and refer to adversaries who perturb the input to a classifier to get their desired output. Poisoning attacks occur in the training phase where an adversary perturbs the training data, with the goal of leading the learning algorithm to choose an insecure1 hypothesis. Following, I will first explain my work on evasion and then poisoning attacks. I will also discuss the implications of my work to randomness extractors and coin tossing protocols. I will conclude by stating my future research plans. Inference-time Attacks As mentioned above, evasion attacks are one of the important attacks that happen during inference phase. The usual objective of an evasion attack is to degrade the overall performance of the model by perturbing the test instances. in the literature, there exist various definitions of robustness of classifiers in the presence of evasion attacks. Although all these definitions seem to capture the same phenomenon, in [DMM18], we showed that they sometimes lead to significantly different results. However, Adding an stability assumption over the ground truth, all of these definitions converge to a single definition where the goal of adversary is to push instances to the error region of the target classifier. The ground truth is usually stable in the practical applications where evasion attacks are relevant. Thus, we take the error region definition as the default definition of adversarial risk in our studies. Quantifying adversarial risk then leads to identifying the degree of security of a classifier against evasion attacks. Inherent upper bound on the robustness of classification against evasion attacks Persistence of adversarial examples has raised a serious concern regarding possibility of implementing a robust machine learning algorithm. To investigate this important issue we posed a research question. In particular, we asked, is there an upper bound on the robustness of machine learning algorithms against evasion adversaries? Alternatively, is there a lower bound on the power of evasion adversaries? We attended to this questions in a series of publications [DMM18; MDM19; MZME19]. In [DMM18], we showed an inherent upper bound on achievable robustness in the presence of evasion adversaries, with sublinear perturbation, when the instances are drawn from uniform hypercube {0, 1}. In particular, we showed that for any classifier with a constant error rate (e.g. 0.01), there is an adversary who changes only O( √ n) bit of the inputs increasing the error of the classifier to almost 1. Our bounds, which were based on an isoperimetric inequality for hamming cube, were independent of the structure of the classifier in use. In a follow up work [MDM19], we generalized this upper bound to many more metric probability spaces. Explicitly, we drew a connection between the robustness of learning algorithms and a well-studied mathematical phenomena known as concentration of measure. We showed that if the metric probability space of the underlying input distribution is well concentrated and the trained hypothesis has some non-negligible error, for most of the instances, there exist perturbations with sub-linear magnitude 1Insecure could refer to different criteria in different scenarios.","",""
45,"H. Escalante, S. Escalera, I. Guyon, Xavier Baró, Yağmur Güçlütürk, Umut Güçlü, M. V. Gerven","Explainable and Interpretable Models in Computer Vision and Machine Learning",2018,"","","","",82,"2022-07-13 09:22:24","","10.1007/978-3-319-98131-4","","",,,,,45,11.25,6,7,4,"","",""
11,"B. Celik, J. Vanschoren","Adaptation Strategies for Automated Machine Learning on Evolving Data",2020,"","","","",83,"2022-07-13 09:22:24","","10.1109/TPAMI.2021.3062900","","",,,,,11,5.50,6,2,2,"Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust to changes in the underlying data. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on a variety of AutoML approaches for building machine learning pipelines, including Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.","",""
33,"B. Abdollahi, O. Nasraoui","Transparency in Fair Machine Learning: the Case of Explainable Recommender Systems",2018,"","","","",84,"2022-07-13 09:22:24","","10.1007/978-3-319-90403-0_2","","",,,,,33,8.25,17,2,4,"","",""
33,"Zebin Yang, Aijun Zhang, A. Sudjianto","Enhancing Explainability of Neural Networks Through Architecture Constraints",2019,"","","","",85,"2022-07-13 09:22:24","","10.1109/TNNLS.2020.3007259","","",,,,,33,11.00,11,3,3,"Prediction accuracy and model explainability are the two most important objectives when developing machine learning algorithms to solve real-world problems. Neural networks are known to possess good prediction performance but suffer from a lack of model interpretability. In this article, we propose to enhance the explainability of neural networks through the following architecture constraints: 1) sparse additive subnetworks; 2) projection pursuit with orthogonality constraint; and 3) smooth function approximation. It leads to an enhanced explainable neural network (ExNN) with a superior balance between prediction performance and model interpretability. We derive sufficient identifiability conditions for the proposed ExNN model. The multiple parameters are simultaneously estimated by a modified minibatch gradient descent method based on the backpropagation algorithm for calculating the derivatives and the Cayley transform for preserving the projection orthogonality. Through simulation study under six different scenarios, we compare the proposed method to several benchmarks, including least absolute shrinkage and selection operator, support vector machine, random forest, extreme learning machine, and multilayer perceptron. It is shown that the proposed ExNN model keeps the flexibility of pursuing high prediction accuracy while attaining improved interpretability. Finally, a real data example is employed as a showcase application.","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",86,"2022-07-13 09:22:24","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
7,"D. Jacob","CATE meets ML - Conditional Average Treatment Effect and Machine Learning",2021,"","","","",87,"2022-07-13 09:22:24","","10.2139/ssrn.3816558","","",,,,,7,7.00,7,1,1,"For treatment effects - one of the core issues in modern econometric analysis - prediction and estimation are two sides of the same coin. As it turns out, machine learning methods are the tool for generalized prediction models. Combined with econometric theory, they allow us to estimate not only the average but a personalized treatment effect - the conditional average treatment effect (CATE). In this tutorial, we give an overview of novel methods, explain them in detail, and apply them via Quantlets in real data applications. We study the effect that microcredit availability has on the amount of money borrowed and if 401(k) pension plan eligibility has an impact on net financial assets, as two empirical examples. The presented toolbox of methods contains meta-learners, like the Doubly-Robust, R-, T- and X-learner, and methods that are specially designed to estimate the CATE like the causal BART and the generalized random forest. In both, the microcredit and 401(k) example, we find a positive treatment effect for all observations but conflicting evidence of treatment effect heterogeneity. An additional simulation study, where the true treatment effect is known, allows us to compare the different methods and to observe patterns and similarities.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",88,"2022-07-13 09:22:24","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
55,"Samantha Joel, Paul W. Eastwick, C. J. Allison, X. Arriaga, Zachary G. Baker, E. Bar-Kalifa, S. Bergeron, G. Birnbaum, R. Brock, C. Brumbaugh, Cheryl L. Carmichael, Serena Chen, Jennifer A. Clarke, Rebecca J. Cobb, Michael K. Coolsen, Jody L. Davis, David C de Jong, Anik Debrot, E. DeHaas, Jaye L. Derrick, Jami Eller, Marie-Joelle Estrada, Ruddy Faure, E. Finkel, R. C. Fraley, S. Gable, Reuma Gadassi-Polack, Yuthika U. Girme, Amie M. Gordon, Courtney Gosnell, Matthew D. Hammond, P. Hannon, Cheryl Harasymchuk, W. Hofmann, A. Horn, E. Impett, Jeremy P Jamieson, D. Keltner, James J Kim, Jeffrey L. Kirchner, E. Kluwer, M. Kumashiro, Grace M Larson, Gal Lazarus, Jill M. Logan, Laura B. Luchies, G. Macdonald, Laura V. Machia, Michael R Maniaci, J. Maxwell","Machine learning uncovers the most robust self-report predictors of relationship quality across 43 longitudinal couples studies",2020,"","","","",89,"2022-07-13 09:22:24","","10.1073/pnas.1917036117","","",,,,,55,27.50,6,50,2,"Significance What predicts how happy people are with their romantic relationships? Relationship science—an interdisciplinary field spanning psychology, sociology, economics, family studies, and communication—has identified hundreds of variables that purportedly shape romantic relationship quality. The current project used machine learning to directly quantify and compare the predictive power of many such variables among 11,196 romantic couples. People’s own judgments about the relationship itself—such as how satisfied and committed they perceived their partners to be, and how appreciative they felt toward their partners—explained approximately 45% of their current satisfaction. The partner’s judgments did not add information, nor did either person’s personalities or traits. Furthermore, none of these variables could predict whose relationship quality would increase versus decrease over time. Given the powerful implications of relationship quality for health and well-being, a central mission of relationship science is explaining why some romantic relationships thrive more than others. This large-scale project used machine learning (i.e., Random Forests) to 1) quantify the extent to which relationship quality is predictable and 2) identify which constructs reliably predict relationship quality. Across 43 dyadic longitudinal datasets from 29 laboratories, the top relationship-specific predictors of relationship quality were perceived-partner commitment, appreciation, sexual satisfaction, perceived-partner satisfaction, and conflict. The top individual-difference predictors were life satisfaction, negative affect, depression, attachment avoidance, and attachment anxiety. Overall, relationship-specific variables predicted up to 45% of variance at baseline, and up to 18% of variance at the end of each study. Individual differences also performed well (21% and 12%, respectively). Actor-reported variables (i.e., own relationship-specific and individual-difference variables) predicted two to four times more variance than partner-reported variables (i.e., the partner’s ratings on those variables). Importantly, individual differences and partner reports had no predictive effects beyond actor-reported relationship-specific variables alone. These findings imply that the sum of all individual differences and partner experiences exert their influence on relationship quality via a person’s own relationship-specific experiences, and effects due to moderation by individual differences and moderation by partner-reports may be quite small. Finally, relationship-quality change (i.e., increases or decreases in relationship quality over the course of a study) was largely unpredictable from any combination of self-report variables. This collective effort should guide future models of relationships.","",""
44,"Min Zhang, Haoxuan You, Pranav Kadam, Shan Liu, C.-C. Jay Kuo","PointHop: An Explainable Machine Learning Method for Point Cloud Classification",2019,"","","","",90,"2022-07-13 09:22:24","","10.1109/TMM.2019.2963592","","",,,,,44,14.67,9,5,3,"An explainable machine learning method for point cloud classification, called the PointHop method, is proposed in this work. The PointHop method consists of two stages: 1) local-to-global attribute building through iterative one-hop information exchange and 2) classification and ensembles. In the attribute building stage, we address the problem of unordered point cloud data using a space partitioning procedure and developing a robust descriptor that characterizes the relationship between a point and its one-hop neighbor in a PointHop unit. When we put multiple PointHop units in cascade, the attributes of a point will grow by taking its relationship with one-hop neighbor points into account iteratively. Furthermore, to control the rapid dimension growth of the attribute vector associated with a point, we use the Saab transform to reduce the attribute dimension in each PointHop unit. In the classification and ensemble stage, we feed the feature vector obtained from multiple PointHop units to a classifier. We explore ensemble methods to improve the classification performance furthermore. It is shown by experimental results that the PointHop method offers classification performance that is comparable with state-of-the-art methods while demanding much lower training complexity.","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",91,"2022-07-13 09:22:24","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
4,"E. Glaab, Armin Rauschenberger, R. Banzi, C. Gerardi, Paula Garcia, J. Demotes","Biomarker discovery studies for patient stratification using machine learning analysis of omics data: a scoping review",2021,"","","","",92,"2022-07-13 09:22:24","","10.1136/bmjopen-2021-053674","","",,,,,4,4.00,1,6,1,"Objective To review biomarker discovery studies using omics data for patient stratification which led to clinically validated FDA-cleared tests or laboratory developed tests, in order to identify common characteristics and derive recommendations for future biomarker projects. Design Scoping review. Methods We searched PubMed, EMBASE and Web of Science to obtain a comprehensive list of articles from the biomedical literature published between January 2000 and July 2021, describing clinically validated biomarker signatures for patient stratification, derived using statistical learning approaches. All documents were screened to retain only peer-reviewed research articles, review articles or opinion articles, covering supervised and unsupervised machine learning applications for omics-based patient stratification. Two reviewers independently confirmed the eligibility. Disagreements were solved by consensus. We focused the final analysis on omics-based biomarkers which achieved the highest level of validation, that is, clinical approval of the developed molecular signature as a laboratory developed test or FDA approved tests. Results Overall, 352 articles fulfilled the eligibility criteria. The analysis of validated biomarker signatures identified multiple common methodological and practical features that may explain the successful test development and guide future biomarker projects. These include study design choices to ensure sufficient statistical power for model building and external testing, suitable combinations of non-targeted and targeted measurement technologies, the integration of prior biological knowledge, strict filtering and inclusion/exclusion criteria, and the adequacy of statistical and machine learning methods for discovery and validation. Conclusions While most clinically validated biomarker models derived from omics data have been developed for personalised oncology, first applications for non-cancer diseases show the potential of multivariate omics biomarker design for other complex disorders. Distinctive characteristics of prior success stories, such as early filtering and robust discovery approaches, continuous improvements in assay design and experimental measurement technology, and rigorous multicohort validation approaches, enable the derivation of specific recommendations for future studies.","",""
4,"S. Newman, R. Furbank","Explainable machine learning models of major crop traits from satellite-monitored continent-wide field trial data.",2021,"","","","",93,"2022-07-13 09:22:24","","10.1038/s41477-021-01001-0","","",,,,,4,4.00,2,2,1,"","",""
4,"Alexandra Renouard, A. Maggi, M. Grunberg, C. Doubre, C. Hibert","Toward False Event Detection and Quarry Blast versus Earthquake Discrimination in an Operational Setting Using Semiautomated Machine Learning",2021,"","","","",94,"2022-07-13 09:22:24","","10.1785/0220200305","","",,,,,4,4.00,1,5,1,"  Small-magnitude earthquakes shed light on the spatial and magnitude distribution of natural seismicity, as well as its rate and occurrence, especially in stable continental regions where natural seismicity remains difficult to explain under slow strain-rate conditions. However, capturing them in catalogs is strongly hindered by signal-to-noise ratio issues, resulting in high rates of false and man-made events also being detected. Accurate and robust discrimination of these events is critical for optimally detecting small earthquakes. This requires uncovering recurrent salient features that can rapidly distinguish first false events from real events, then earthquakes from man-made events (mainly quarry blasts), despite high signal variability and noise content. In this study, we combined the complementary strengths of human and interpretable rule-based machine-learning algorithms for solving this classification problem. We used human expert knowledge to co-create two reliable machine-learning classifiers through human-assisted selection of classification features and review of events with uncertain classifier predictions. The two classifiers are integrated into the SeisComP3 operational monitoring system. The first one discards false events from the set of events obtained with a low short-term average/long-term average threshold; the second one labels the remaining events as either earthquakes or quarry blasts. When run in an operational setting, the first classifier correctly detected more than 99% of false events and just over 93% of earthquakes; the second classifier correctly labeled 95% of quarry blasts and 96% of earthquakes. After a manual review of the second classifier low-confidence outputs, the final catalog contained fewer than 2% of misclassified events. These results confirm that machine learning strengthens the quality of earthquake catalogs and that the performance of machine-learning classifiers can be improved through human expertise. Our study promotes a broader implication of hybrid intelligence monitoring within seismological observatories.","",""
4,"Abderrahmen Amich, Birhanu Eshete","Explanation-Guided Diagnosis of Machine Learning Evasion Attacks",2021,"","","","",95,"2022-07-13 09:22:24","","10.1007/978-3-030-90019-9_11","","",,,,,4,4.00,2,2,1,"","",""
3,"Kai R. T. Larsen, Daniel S. Becker","Why Use Automated Machine Learning?",2021,"","","","",96,"2022-07-13 09:22:24","","10.1093/oso/9780190941659.003.0001","","",,,,,3,3.00,2,2,1,"Machine learning is involved in search, translation, detecting depression, likelihood of college dropout, finding lost children, and to sell all kinds of products. While barely beyond its inception, the current machine learning revolution will affect people and organizations no less than the Industrial Revolution’s effect on weavers and many other skilled laborers. Machine learning will automate hundreds of millions of jobs that were considered too complex for machines ever to take over even a decade ago, including driving, flying, painting, programming, and customer service, as well as many of the jobs previously reserved for humans in the fields of finance, marketing, operations, accounting, and human resources. This section explains how automated machine learning addresses exploratory data analysis, feature engineering, algorithm selection, hyperparameter tuning, and model diagnostics. The section covers the eight criteria considered essential for AutoML to have significant impact: accuracy, productivity, ease of use, understanding and learning, resource availability, process transparency, generalization, and recommended actions. ","",""
3,"Jean-Jacques Ohana, Steve Ohana, E. Benhamou, D. Saltiel, B. Guez","Explainable AI Models of Stock Crashes: A Machine-Learning Explanation of the Covid March 2020 Equity Meltdown",2021,"","","","",97,"2022-07-13 09:22:24","","10.2139/ssrn.3809308","","",,,,,3,3.00,1,5,1,"We consider a gradient boosting decision trees (GBDT) approach to predict large S&P 500 price drops from a set of 150 technical, fundamental and macroeconomic features. We report an improved accuracy of GBDT over other machine learning (ML) methods on the S&P 500 futures prices. We show that retaining fewer and carefully selected features provides improvements across all ML approaches. Shapley values have recently been introduced from game theory to the field of ML. They allow for a robust identification of the most important variables predicting stock market crises, and of a local explanation of the crisis probability at each date, through a consistent features attribution. We apply this methodology to analyze in detail the March 2020 financial meltdown, for which the model offered a timely out of sample prediction. This analysis unveils in particular the contrarian predictive role of the tech equity sector before and after the crash.","",""
3,"F. Biessmann, D. Refiano","Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated",2021,"","","","",98,"2022-07-13 09:22:24","","","","",,,,,3,3.00,2,2,1,"The field explainable artificial intelligence (XAI) has brought about an arsenal of methods to render Machine Learning (ML) predictions more interpretable. But how useful explanations provided by transparent ML methods are for humans remains difficult to assess. Here we investigate the quality of interpretable computer vision algorithms using techniques from psychophysics. In crowdsourced annotation tasks we study the impact of different interpretability approaches on annotation accuracy and task time. We compare these quality metrics with classical XAI, automated quality metrics. Our results demonstrate that psychophysical experiments allow for robust quality assessment of transparency in machine learning. Interestingly the quality metrics computed without humans in the loop did not provide a consistent ranking of interpretability methods nor were they representative for how useful an explanation was for humans. These findings highlight the potential of methods from classical psychophysics for modern machine learning applications. We hope that our results provide convincing arguments for evaluating interpretability in its natural habitat, human-ML interaction, if the goal is to obtain an authentic assessment of interpretability.","",""
1,"C. Betancourt, S. Stadtler, T. Stomberg, Ann-Kathrin Edrich, Ankit Patnala, R. Roscher, J. Kowalski, M. Schultz","Global fine resolution mapping of ozone metrics through explainable machine learning",2021,"","","","",99,"2022-07-13 09:22:24","","10.5194/EGUSPHERE-EGU21-7596","","",,,,,1,1.00,0,8,1,"<p>Through the availability of multi-year ground based ozone observations on a global scale, substantial geospatial meta data, and high performance computing capacities, it is now possible to use machine learning for a global data-driven ozone assessment. In this presentation, we will show a novel, completely data-driven approach to map tropospheric ozone globally.</p><p>Our goal is to interpolate ozone metrics and aggregated statistics from the database of the Tropospheric Ozone Assessment Report (TOAR) onto a global 0.1&#176; x 0.1&#176; resolution grid. &#160;It is challenging to interpolate ozone, a toxic greenhouse gas because its formation depends on many interconnected environmental factors on small scales. We conduct the interpolation with various machine learning methods trained on aggregated hourly ozone data from five years at more than 5500 locations worldwide. We use several geospatial datasets as training inputs to provide proxy input for environmental factors controlling ozone formation, such as precursor emissions and climate. The resulting maps contain different ozone metrics, i.e. statistical aggregations which are widely used to assess air pollution impacts on health, vegetation, and climate.</p><p>The key aspects of this contribution are twofold: First, we apply explainable machine learning methods to the data-driven ozone assessment. Second, we discuss dominant uncertainties relevant to the ozone mapping and quantify their impact whenever possible. Our methods include a thorough a-priori uncertainty estimation of the various data and methods, assessment of scientific consistency, finding critical model parameters, using ensemble methods, and performing error modeling.</p><p>Our work aims to increase the reliability and integrity of the derived ozone maps through the provision of scientific robustness to a data-centric machine learning task. This study hence represents a blueprint for how to formulate an environmental machine learning task scientifically, gather the necessary data, and develop a data-driven workflow that focuses on optimizing transparency and applicability of its product to maximize its scientific knowledge return.</p>","",""
2,"A. Ayobi, Katarzyna Stawarz, Dmitri S. Katz, P. Marshall, Taku Yamagata, Raúl Santos-Rodríguez, Peter A. Flach, A. O'Kane","Machine Learning Explanations as Boundary Objects: How AI Researchers Explain and Non-Experts Perceive Machine Learning",2021,"","","","",100,"2022-07-13 09:22:24","","","","",,,,,2,2.00,0,8,1,"Understanding artificial intelligence (AI) and machine learning (ML) approaches is becoming increasingly important for people with a wide range of professional backgrounds. However, it is unclear how ML concepts can be effectively explained as part of human-centred and multidisciplinary design processes. We provide a qualitative account of how AI researchers explained and non-experts perceived ML concepts as part of a co-design project that aimed to inform the design of ML applications for diabetes self-care. We identify benefits and challenges of explaining ML concepts with analogical narratives, information visualisations, and publicly available videos. Co-design participants reported not only gaining an improved understanding of ML concepts but also highlighted challenges of understanding ML explanations, including misalignments between scientific models and their lived self-care experiences and individual information needs. We frame our findings through the lens of Stars and Griesemer’s concept of boundary objects to discuss how the presentation of user-centred ML explanations could strike a balance between being plastic and robust enough to support design objectives and people’s individual information needs.","",""
1,"M. A. Al Janabi","Optimization algorithms and investment portfolio analytics with machine learning techniques under time-varying liquidity constraints",2021,"","","","",101,"2022-07-13 09:22:24","","10.1108/JM2-10-2020-0259","","",,,,,1,1.00,1,1,1," Purpose This paper aims to examine from commodity portfolio managers’ perspective the performance of liquidity adjusted risk modeling in assessing the market risk parameters of a large commodity portfolio and in obtaining efficient and coherent portfolios under different market circumstances.   Design/methodology/approach The implemented market risk modeling algorithm and investment portfolio analytics using reinforcement machine learning techniques can simultaneously handle risk-return characteristics of commodity investments under regular and crisis market settings besides considering the particular effects of the time-varying liquidity constraints of the multiple-asset commodity portfolios.   Findings In particular, the paper implements a robust machine learning method to commodity optimal portfolio selection and within a liquidity-adjusted value-at-risk (LVaR) framework. In addition, the paper explains how the adapted LVaR modeling algorithms can be used by a commodity trading unit in a dynamic asset allocation framework for estimating risk exposure, assessing risk reduction alternates and creating efficient and coherent market portfolios.   Originality/value The optimization parameters subject to meaningful operational and financial constraints, investment portfolio analytics and empirical results can have important practical uses and applications for commodity portfolio managers particularly in the wake of the 2007–2009 global financial crisis. In addition, the recommended reinforcement machine learning optimization algorithms can aid in solving some real-world dilemmas under stressed and adverse market conditions (e.g. illiquidity, switching in correlations factors signs, nonlinear and non-normal distribution of assets’ returns) and can have key applications in machine learning, expert systems, smart financial functions, internet of things (IoT) and financial technology (FinTech) in big data ecosystems. ","",""
1,"Alina Oprea","Machine Learning Integrity and Privacy in Adversarial Environments",2021,"","","","",102,"2022-07-13 09:22:24","","10.1145/3450569.3462164","","",,,,,1,1.00,1,1,1,"Machine learning is increasingly being used for automated decisions in applications such as health care, finance, autonomous vehicles, and personalized recommendations. These critical applications require strong guarantees on both the integrity of the machine learning models and the privacy of the user data used to train these models. The area of adversarial machine learning studies the effect of adversarial attacks against machine learning models and aims to design robust defense algorithms. The main challenges in this space are the development of realistic adversarial models that consider the specifics of real-world applications, and the design of machine learning algorithms resilient against a wide range of threats. In this talk, we describe our work on creating a taxonomy of poisoning attacks against machine learning systems at training time. In light of recent software supply chain vulnerabilities revealed by the SolarWinds attack, the supply chain of machine learning development needs to be protected. First, we introduce our optimization approach to create poisoning availability attacks against linear regression and discuss robust defenses based on techniques from robust statistics [1]. Then, we discuss how an attacker with minimal knowledge of a machine learning classifier can inject backdoor poisoning attacks, by leveraging techniques from machine learning explainability [4]. We demonstrate these methods on several malware classifiers and show the challenges of designing robust defenses to protect against these attacks. We also define a new attack model called subpopulation poisoning, that requires a small set of poisoning points to impact the accuracy of the model on a targeted subpopulation [2]. We evaluate our poisoning attacks on multiple data modalities, including image, text, and tabular data. Finally, we highlight a surprising connection between machine learning integrity and privacy attacks, and show how poisoning attacks can be used for auditing the privacy of machine learning algorithms such as differentially private stochastic gradient descent [3].","",""
1,"Hali Lindsay, J. Tröger, A. König","Language Impairment in Alzheimer’s Disease—Robust and Explainable Evidence for AD-Related Deterioration of Spontaneous Speech Through Multilingual Machine Learning",2021,"","","","",103,"2022-07-13 09:22:24","","10.3389/fnagi.2021.642033","","",,,,,1,1.00,0,3,1,"Alzheimer’s disease (AD) is a pervasive neurodegenerative disease that affects millions worldwide and is most prominently associated with broad cognitive decline, including language impairment. Picture description tasks are routinely used to monitor language impairment in AD. Due to the high amount of manual resources needed for an in-depth analysis of thereby-produced spontaneous speech, advanced natural language processing (NLP) combined with machine learning (ML) represents a promising opportunity. In this applied research field though, NLP and ML methodology do not necessarily ensure robust clinically actionable insights into cognitive language impairment in AD and additional precautions must be taken to ensure clinical-validity and generalizability of results. In this study, we add generalizability through multilingual feature statistics to computational approaches for the detection of language impairment in AD. We include 154 participants (78 healthy subjects, 76 patients with AD) from two different languages (106 English speaking and 47 French speaking). Each participant completed a picture description task, in addition to a battery of neuropsychological tests. Each response was recorded and manually transcribed. From this, task-specific, semantic, syntactic and paralinguistic features are extracted using NLP resources. Using inferential statistics, we determined language features, excluding task specific features, that are significant in both languages and therefore represent “generalizable” signs for cognitive language impairment in AD. In a second step, we evaluated all features as well as the generalizable ones for English, French and both languages in a binary discrimination ML scenario (AD vs. healthy) using a variety of classifiers. The generalizable language feature set outperforms the all language feature set in English, French and the multilingual scenarios. Semantic features are the most generalizable while paralinguistic features show no overlap between languages. The multilingual model shows an equal distribution of error in both English and French. By leveraging multilingual statistics combined with a theory-driven approach, we identify AD-related language impairment that generalizes beyond a single corpus or language to model language impairment as a clinically-relevant cognitive symptom. We find a primary impairment in semantics in addition to mild syntactic impairment, possibly confounded by additional impaired cognitive functions.","",""
1,"Ivan Ny Hanitra, F. Criscuolo, S. Carrara, G. De Micheli","Multi-Ion-Sensing Emulator and Multivariate Calibration Optimization by Machine Learning Models",2021,"","","","",104,"2022-07-13 09:22:24","","10.1109/ACCESS.2021.3065754","","",,,,,1,1.00,0,4,1,"One paramount challenge in multi-ion-sensing arises from ion interference that degrades the accuracy of sensor calibration. Machine learning models are here proposed to optimize such multivariate calibration. However, the acquisition of big experimental data is time and resource consuming in practice, necessitating new paradigms and efficient models for these data-limited frameworks. Therefore, a novel approach is presented in this work, where a multi-ion-sensing emulator is designed to explain the response of an ion-sensing array in a mixed-ion environment. A case study is performed emulating the concurrent monitoring of sodium, potassium, lithium, and lead ions, in a medium representative of sweat samples. These analytes are relevant examples of sweat ion-sensing applications for physiology, therapeutic drug monitoring, and heavy metal contamination. It is demonstrated that calibration datasets output by the emulator explain accurately the experimental response of polymeric solid-contact ion-selective electrodes, where root-mean-squared error of 1.37, 1.44, 1.78, $2\,mV$ are obtained, respectively, for Na+, K+, Li+, Pb2+ sensor calibration in artificial sweat. Besides, synthetic datasets of custom size are generated to train, validate, and evaluate different types of multivariate regressors. A Multi-Output Support Vector Regressor (M-SVR) is proposed as a compact, accurate, robust, and efficient multivariate calibration model. It features 13.22% normalized root mean squares, and 20.29% mean root squares improvement compared to a simple linear regression model. It is an unbiased estimator for medium to large datasets, and its average generalization error is of 3.22%. Besides, M-SVR models have a lower computational complexity than single-output SVR or neural network models, making them a suitable solution for memory and energy-constrained edge devices used for continuous and real-time multi-ion monitoring.","",""
2,"Zifan Xu, Xuesu Xiao, Garrett A. Warnell, Anirudh Nair, P. Stone","Machine Learning Methods for Local Motion Planning: A Study of End-to-End vs. Parameter Learning",2021,"","","","",105,"2022-07-13 09:22:24","","10.1109/SSRR53300.2021.9597689","","",,,,,2,2.00,0,5,1,"While decades of research efforts have been devoted to developing classical autonomous navigation systems to move robots from one point to another in a collision-free manner, machine learning approaches to navigation have been recently proposed to learn navigation behaviors from data. Two representative paradigms are end-to-end learning (directly from perception to motion) and parameter learning (from perception to parameters used by a classical underlying planner). These two types of methods are believed to have complementary pros and cons: parameter learning is expected to be robust to different scenarios, have provable guarantees, and exhibit explainable behaviors; end-to-end learning does not require extensive engineering and has the potential to outperform approaches that rely on classical systems. However, these beliefs have not been verified through real-world experiments in a comprehensive way. In this paper, we report on an extensive study to compare end-to-end and parameter learning for local motion planners in a large suite of simulated and physical experiments. In particular, we test the performance of end-to-end motion policies, which directly compute raw motor commands, and parameter policies, which compute parameters to be used by classical planners, with different inputs (e.g., raw sensor data, costmaps), and provide an analysis of the results.","",""
2,"K. Morik, Helena Kotthaus, Lukas Heppe, Danny Heinrich, Raphael Fischer, Andrea Pauly, N. Piatkowski","The Care Label Concept: A Certification Suite for Trustworthy and Resource-Aware Machine Learning",2021,"","","","",106,"2022-07-13 09:22:24","","","","",,,,,2,2.00,0,7,1,"Machine learning applications have become ubiquitous. This has led to an increased effort of making machine learning trustworthy. Explainable and fair AI have already matured. They address knowledgeable users and application engineers. For those who do not want to invest time into understanding the method or the learned model, we offer care labels: easy to understand at a glance, allowing for method or model comparisons, and, at the same time, scientifically well-based. On one hand, this transforms descriptions as given by, e.g., Fact Sheets or Model Cards, into a form that is well-suited for end-users. On the other hand, care labels are the result of a certification suite that tests whether stated guarantees hold. In this paper, we present two experiments with our certification suite. One shows the care labels for configurations of Markov random fields (MRFs). Based on the underlying theory of MRFs, each choice leads to its specific rating of static properties like, e.g., expressivity and reliability. In addition, the implementation is tested and resource consumption is measured yielding dynamic properties. This two-level procedure is followed by another experiment certifying deep neural network (DNN) models. There, we draw the static properties from literature on a particular model and data set. At the second level, experiments are generated that deliver measurements of robustness against certain attacks. We illustrate this by ResNet-18 and MobileNetV3 applied to ImageNet.","",""
1,"C. Betancourt, S. Stadtler, T. Stomberg, Ann-Kathrin Edrich, Ankit Patnala, R. Roscher, J. Kowalski, M. Schultz","Global fine resolution mapping of ozone metrics through explainable machine learning",2021,"","","","",107,"2022-07-13 09:22:24","","10.5194/EGUSPHERE-EGU21-7596","","",,,,,1,1.00,0,8,1,"<p>Through the availability of multi-year ground based ozone observations on a global scale, substantial geospatial meta data, and high performance computing capacities, it is now possible to use machine learning for a global data-driven ozone assessment. In this presentation, we will show a novel, completely data-driven approach to map tropospheric ozone globally.</p><p>Our goal is to interpolate ozone metrics and aggregated statistics from the database of the Tropospheric Ozone Assessment Report (TOAR) onto a global 0.1&#176; x 0.1&#176; resolution grid. &#160;It is challenging to interpolate ozone, a toxic greenhouse gas because its formation depends on many interconnected environmental factors on small scales. We conduct the interpolation with various machine learning methods trained on aggregated hourly ozone data from five years at more than 5500 locations worldwide. We use several geospatial datasets as training inputs to provide proxy input for environmental factors controlling ozone formation, such as precursor emissions and climate. The resulting maps contain different ozone metrics, i.e. statistical aggregations which are widely used to assess air pollution impacts on health, vegetation, and climate.</p><p>The key aspects of this contribution are twofold: First, we apply explainable machine learning methods to the data-driven ozone assessment. Second, we discuss dominant uncertainties relevant to the ozone mapping and quantify their impact whenever possible. Our methods include a thorough a-priori uncertainty estimation of the various data and methods, assessment of scientific consistency, finding critical model parameters, using ensemble methods, and performing error modeling.</p><p>Our work aims to increase the reliability and integrity of the derived ozone maps through the provision of scientific robustness to a data-centric machine learning task. This study hence represents a blueprint for how to formulate an environmental machine learning task scientifically, gather the necessary data, and develop a data-driven workflow that focuses on optimizing transparency and applicability of its product to maximize its scientific knowledge return.</p>","",""
2,"S. Newman, R. Furbank","Explainable machine learning models of major crop traits from satellite-monitored continent-wide field trial data",2021,"","","","",108,"2022-07-13 09:22:24","","10.1101/2021.03.08.434495","","",,,,,2,2.00,1,2,1,"Four species of grass generate half of all human-consumed calories1. However, abundant biological data on species that produce our food remains largely inaccessible, imposing direct barriers to understanding crop yield and fitness traits. Here, we assemble and analyse a continent-wide database of field experiments spanning ten years and hundreds of thousands of machine-phenotyped populations of ten major crop species. Training an ensemble of machine learning models, using thousands of variables capturing weather, ground-sensor, soil, chemical and fertiliser dosage, management, and satellite data, produces robust cross-continent yield models exceeding R2 = 0.8 prediction accuracy. In contrast to ‘black box’ analytics, detailed interrogation of these models reveals fundamental drivers of crop behaviour and complex interactions predicting yield and agronomic traits. These results demonstrate the capacity of machine learning models to build unified, interpretable, and explainable models of crop behaviour, and highlight the powerful role of data in the future of food.","",""
2,"Xin-Yi Song, Ya Gao, Yubo Peng, Sen Huang, Chao Liu, Zhong-ren Peng","A machine learning approach to modelling the spatial variations in the daily fine particulate matter (PM2.5) and nitrogen dioxide (NO2) of Shanghai, China",2021,"","","","",109,"2022-07-13 09:22:24","","10.1177/2399808320975031","","",,,,,2,2.00,0,6,1,"It is challenging to forecast high-resolution spatial-temporal patterns of intra-urban air pollution and identify impacting factors at the regional scale. Studies have attempted to capture features of air pollutants such as fine particulate matter (PM2.5) and nitrogen dioxide (NO2) using land use regression models, but this method overlooks the multi-collinearity of factors, non-linear correlations between factors and air pollutants, and it fails to perform well when processing daily data. However, machine learning is a feasible approach for establishing persuasive intra-urban air pollution daily variation models. In this article, random forest is utilised to establish intra-urban PM2.5 and NO2 spatial-temporal variation models and is compared to the traditional land use regression method. Taking the city of Shanghai, China as the case area, 36 station-measured daily records in two and a half years of PM2.5 and NO2 concentrations were collected. And over 80 different predictors associated with meteorological and geographical conditions, transportation, community population density, land use and points of interest are used to construct the land use regression and random forest models. Results from the two methods are compared and impacting factors identified. Explained variance (R2) is used to quantify and compare model performance. The final land use regression model explains 49.3% and 42.2% of the spatial variation in ambient PM2.5 and NO2, respectively, whereas the random forest model explains 78.1% and 60.5% of the variance. Regression mappings for unsampled sites on a grid pattern of 1 km × 1 km are also implemented. The random forest model is shown to perform much better than the land use regression model. In general, the findings suggest that the random forest approach offers a robust improvement in predicting performance compared to the land use regression model in estimating daily spatial variations in ambient PM2.5 and NO2.","",""
2,"A. Ayobi, Katarzyna Stawarz, Dmitri S. Katz, P. Marshall, Taku Yamagata, Raúl Santos-Rodríguez, Peter A. Flach, A. O'Kane","Machine Learning Explanations as Boundary Objects: How AI Researchers Explain and Non-Experts Perceive Machine Learning",2021,"","","","",110,"2022-07-13 09:22:24","","","","",,,,,2,2.00,0,8,1,"Understanding artificial intelligence (AI) and machine learning (ML) approaches is becoming increasingly important for people with a wide range of professional backgrounds. However, it is unclear how ML concepts can be effectively explained as part of human-centred and multidisciplinary design processes. We provide a qualitative account of how AI researchers explained and non-experts perceived ML concepts as part of a co-design project that aimed to inform the design of ML applications for diabetes self-care. We identify benefits and challenges of explaining ML concepts with analogical narratives, information visualisations, and publicly available videos. Co-design participants reported not only gaining an improved understanding of ML concepts but also highlighted challenges of understanding ML explanations, including misalignments between scientific models and their lived self-care experiences and individual information needs. We frame our findings through the lens of Stars and Griesemer’s concept of boundary objects to discuss how the presentation of user-centred ML explanations could strike a balance between being plastic and robust enough to support design objectives and people’s individual information needs.","",""
21,"Cheng-Yu Hsieh, Chih-Kuan Yeh, Xuanqing Liu, Pradeep Ravikumar, Seungyeon Kim, Sanjiv Kumar, Cho-Jui Hsieh","Evaluations and Methods for Explanation through Robustness Analysis",2019,"","","","",111,"2022-07-13 09:22:24","","","","",,,,,21,7.00,3,7,3,"Among multiple ways of interpreting a machine learning model, measuring the importance of a set of features tied to a prediction is probably one of the most intuitive ways to explain a model. In this paper, we establish the link between a set of features to a prediction with a new evaluation criterion, robustness analysis, which measures the minimum distortion distance of adversarial perturbation. By measuring the tolerance level for an adversarial attack, we can extract a set of features that provides the most robust support for a prediction, and also can extract a set of features that contrasts the current prediction to a target class by setting a targeted adversarial attack. By applying this methodology to various prediction tasks across multiple domains, we observe the derived explanations are indeed capturing the significant feature set qualitatively and quantitatively.","",""
0,"I. Olier, Oghenejokpeme I. Orhobor, T. Dash, Andy M. Davis, L. Soldatova, J. Vanschoren, R. King","Transformational machine learning: Learning how to learn from many related scientific problems",2021,"","","","",112,"2022-07-13 09:22:24","","10.1073/pnas.2108013118","","",,,,,0,0.00,0,7,1,"Significance Machine learning (ML) is the branch of artificial intelligence (AI) that develops computational systems that learn from experience. In supervised ML, the ML system generalizes from labelled examples to learn a model that can predict the labels of unseen examples. Examples are generally represented using features that directly describe the examples. For instance, in drug design, ML uses features that describe molecular shape and so on. In cases where there are multiple related ML problems, it is possible to use a different type of feature: predictions made about the examples by ML models learned on other problems. We call this transformational ML. We show that this results in better predictions and improved understanding when applied to scientific problems. Almost all machine learning (ML) is based on representing examples using intrinsic features. When there are multiple related ML problems (tasks), it is possible to transform these features into extrinsic features by first training ML models on other tasks and letting them each make predictions for each example of the new task, yielding a novel representation. We call this transformational ML (TML). TML is very closely related to, and synergistic with, transfer learning, multitask learning, and stacking. TML is applicable to improving any nonlinear ML method. We tested TML using the most important classes of nonlinear ML: random forests, gradient boosting machines, support vector machines, k-nearest neighbors, and neural networks. To ensure the generality and robustness of the evaluation, we utilized thousands of ML problems from three scientific domains: drug design, predicting gene expression, and ML algorithm selection. We found that TML significantly improved the predictive performance of all the ML methods in all the domains (4 to 50% average improvements) and that TML features generally outperformed intrinsic features. Use of TML also enhances scientific understanding through explainable ML. In drug design, we found that TML provided insight into drug target specificity, the relationships between drugs, and the relationships between target proteins. TML leads to an ecosystem-based approach to ML, where new tasks, examples, predictions, and so on synergistically interact to improve performance. To contribute to this ecosystem, all our data, code, and our ∼50,000 ML models have been fully annotated with metadata, linked, and openly published using Findability, Accessibility, Interoperability, and Reusability principles (∼100 Gbytes).","",""
0,"Jaehun Kim","Increasing trust in complex machine learning systems",2021,"","","","",113,"2022-07-13 09:22:24","","10.1145/3476415.3476435","","",,,,,0,0.00,0,1,1,"Machine learning (ML) has become a core technology for many real-world applications. Modern ML models are applied to unprecedentedly complex and difficult challenges, including very large and subjective problems. For instance, applications towards multimedia understanding have been advanced substantially. Here, it is already prevalent that cultural/artistic objects such as music and videos are analyzed and served to users according to their preference, enabled through ML techniques. One of the most recent breakthroughs in ML is Deep Learning (DL), which has been immensely adopted to tackle such complex problems. DL allows for higher learning capacity, making end-to-end learning possible, which reduces the need for substantial engineering effort, while achieving high effectiveness. At the same time, this also makes DL models more complex than conventional ML models. Reports in several domains indicate that such more complex ML models may have potentially critical hidden problems: various biases embedded in the training data can emerge in the prediction, extremely sensitive models can make unaccountable mistakes. Furthermore, the black-box nature of the DL models hinders the interpretation of the mechanisms behind them. Such unexpected drawbacks result in a significant impact on the trustworthiness of the systems in which the ML models are equipped as the core apparatus. In this thesis, a series of studies investigates aspects of trustworthiness for complex ML applications, namely the reliability and explainability. Specifically, we focus on music as the primary domain of interest, considering its complexity and subjectivity. Due to this nature of music, ML models for music are necessarily complex for achieving meaningful effectiveness. As such, the reliability and explainability of music ML models are crucial in the field. The first main chapter of the thesis investigates the transferability of the neural network in the Music Information Retrieval (MIR) context. Transfer learning, where the pre-trained ML models are used as off-the-shelf modules for the task at hand, has become one of the major ML practices. It is helpful since a substantial amount of the information is already encoded in the pre-trained models, which allows the model to achieve high effectiveness even when the amount of the dataset for the current task is scarce. However, this may not always be true if the ""source"" task which pre-trained the model shares little commonality with the ""target"" task at hand. An experiment including multiple ""source"" tasks and ""target"" tasks was conducted to examine the conditions which have a positive effect on the transferability. The result of the experiment suggests that the number of source tasks is a major factor of transferability. Simultaneously, it is less evident that there is a single source task that is universally effective on multiple target tasks. Overall, we conclude that considering multiple pre-trained models or pre-training a model employing heterogeneous source tasks can increase the chance for successful transfer learning. The second major work investigates the robustness of the DL models in the transfer learning context. The hypothesis is that the DL models can be susceptible to imperceptible noise on the input. This may drastically shift the analysis of similarity among inputs, which is undesirable for tasks such as information retrieval. Several DL models pre-trained in MIR tasks are examined for a set of plausible perturbations in a real-world setup. Based on a proposed sensitivity measure, the experimental results indicate that all the DL models were substantially vulnerable to perturbations, compared to a traditional feature encoder. They also suggest that the experimental framework can be used to test the pre-trained DL models for measuring robustness. In the final main chapter, the explainability of black-box ML models is discussed. In particular, the chapter focuses on the evaluation of the explanation derived from model-agnostic explanation methods. With black-box ML models having become common practice, model-agnostic explanation methods have been developed to explain a prediction. However, the evaluation of such explanations is still an open problem. The work introduces an evaluation framework that measures the quality of the explanations employing fidelity and complexity. Fidelity refers to the explained mechanism's coherence to the black-box model, while complexity is the length of the explanation. Throughout the thesis, we gave special attention to the experimental design, such that robust conclusions can be reached. Furthermore, we focused on delivering machine learning framework and evaluation frameworks. This is crucial, as we intend that the experimental design and results will be reusable in general ML practice. As it implies, we also aim our findings to be applicable beyond the music applications such as computer vision or natural language processing. Trustworthiness in ML is not a domain-specific problem. Thus, it is vital for both researchers and practitioners from diverse problem spaces to increase awareness of complex ML systems' trustworthiness. We believe the research reported in this thesis provides meaningful stepping stones towards the trustworthiness of ML.","",""
0,"Wenxiu Xie, Christine Ji, Tianyong Hao, Chi-Yin Chow","Predicting the Easiness and Complexity of English Health Materials for International Tertiary Students With Linguistically Enhanced Machine Learning Algorithms: Development and Validation Study (Preprint)",2021,"","","","",114,"2022-07-13 09:22:24","","10.2196/preprints.25110","","",,,,,0,0.00,0,4,1,"  BACKGROUND  There is an increasing body of research on the development of machine learning algorithms in the evaluation of online health educational resources for specific readerships. Machine learning algorithms are known for their lack of interpretability compared with statistics. Given their high predictive precision, improving the interpretability of these algorithms can help increase their applicability and replicability in health educational research and applied linguistics, as well as in the development and review of new health education resources for effective and accessible health education.      OBJECTIVE  Our study aimed to develop a linguistically enriched machine learning model to predict binary outcomes of online English health educational resources in terms of their easiness and complexity for international tertiary students.      METHODS  Logistic regression emerged as the best performing algorithm compared with support vector machine (SVM) (linear), SVM (radial basis function), random forest, and extreme gradient boosting on the transformed data set using L2 normalization. We applied recursive feature elimination with SVM to perform automatic feature selection. The automatically selected features (n=67) were then further streamlined through expert review. The finalized feature set of 22 semantic features achieved a similar area under the curve, sensitivity, specificity, and accuracy compared with the initial (n=115) and automatically selected feature sets (n=67). Logistic regression with the linguistically enhanced feature set (n=22) exhibited important stability and robustness on the training data of different sizes (20%, 40%, 60%, and 80%), and showed consistently high performance when compared with the other 4 algorithms (SVM [linear], SVM [radial basis function], random forest, and extreme gradient boosting).      RESULTS  We identified semantic features (with positive regression coefficients) contributing to the prediction of easy-to-understand online health texts and semantic features (with negative regression coefficients) contributing to the prediction of hard-to-understand health materials for readers with nonnative English backgrounds. Language complexity was explained by lexical difficulty (rarity and medical terminology), verbs typical of medical discourse, and syntactic complexity. Language easiness of online health materials was associated with features such as common speech act verbs, personal pronouns, and familiar reasoning verbs. Successive permutation of features illustrated the interaction between these features and their impact on key performance indicators of the machine learning algorithms.      CONCLUSIONS  The new logistic regression model developed exhibited consistency, scalability, and, more importantly, interpretability based on existing health and linguistic research. It was found that low and high linguistic accessibilities of online health materials were explained by 2 sets of distinct semantic features. This revealed the inherent complexity of effective health communication beyond current readability analyses, which were limited to syntactic complexity and lexical difficulty. ","",""
0,"Elizaveta Felsche, R. Ludwig","Applying machine learning for drought prediction using a large ensemble of climate simulations",2021,"","","","",115,"2022-07-13 09:22:24","","10.5194/EGUSPHERE-EGU21-1305","","",,,,,0,0.00,0,2,1,"<p>There is strong scientific and social interest to understand the factors leading to extreme events in order to improve the management of risks associated with hazards like droughts. Recent events like the summer 2018 drought in Germany already had severe und unexpected impacts, e.g. forest fires and crop failures; in order to increase preparedness robust prediction tools are &#160;urgently required. In this study, machine learning methods are applied to predict the occurrence of a drought with lead times of one to three months. The approach takes into account a list of thirty atmospheric and soil variables<strong> </strong>as predictor input parameters from a single regional climate model initial condition large ensemble (CRCM5-LE). The data was produced the context of the ClimEx project by Ouranos with the Canadian Regional Climate Model (CRCM5) driven by 50 members of the Canadian Earth System Model (CanESM2) for the Bavarian and Quebec domains.</p><p>Drought occurrence was defined using the Standardized Precipitation Index. The training and test datasets were chosen from the current climatology (1955-2005) for the Munich and Lisbon subdomain within the CRCM5-LE. The best performing machine learning algorithms managed to obtain a correct classification of drought or no drought for a lead time of one month for around 60 % of the events of each class for the both domains. Explainable AI methods like feature importance and shapley values were applied to gain a better understanding of the trained algorithms. Physical variables like the North Atlantic Oscillation Index and air pressure one month before the event proved to be of high importance for the prediction. The study showed that better accuracies can be obtained for the Lisbon domain, due to the stronger influence of the North Atlantic Oscillation Index on Portugal&#8217;s climate.</p>","",""
0,"A. Nair, F. Yu, P. C. Jost, P. DeMott, E. Levin, J. Jimenez, J. Peischl, I. Pollack, C. Fredrickson, A. Beyersdorf, B. Nault, Minsu Park, S. Yum, B. Palm, Lu Xu, I. Bourgeois, B. Anderson, A. Nenes, L. Ziemba, R. Moore, Taehyoung Lee, T. Park, C. Thompson, F. Flocke, L. Huey, Michelle J. Kim, Q. Peng","Machine learning uncovers aerosol size information from chemistry and meteorology to quantify potential cloud-forming particles",2021,"","","","",116,"2022-07-13 09:22:24","","10.21203/rs.3.rs-244416/v1","","",,,,,0,0.00,0,27,1,"  Cloud condensation nuclei (CCN) are mediators of aerosol–cloud interactions, which contribute to the largest uncertainty in climate change prediction. Here, we present a machine learning/artificial intelligence model that quantifies CCN from variables of aerosol composition, atmospheric trace gases, and meteorology. Comprehensive multi-campaign airborne measurements, covering varied physicochemical regimes in the troposphere, confirm the validity of and help probe the inner workings of this machine learning model: revealing for the first time that different ranges of atmospheric aerosol composition and mass correspond to distinct aerosol number size distributions. Machine learning extracts this information, important for accurate quantification of CCN, additionally from both chemistry and meteorology. This can provide a physicochemically explainable, computationally efficient, robust machine learning pathway in global climate models that only resolve aerosol composition; potentially mitigating the uncertainty of effective radiative forcing due to aerosol–cloud interactions (ERFaci) and improving confidence in assessment of anthropogenic contributions and climate change projections.","",""
0,"G. Truda","Quantified Sleep: Machine learning techniques for observational n-of-1 studies",2021,"","","","",117,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,1,1,"This paper applies statistical learning techniques to an observational Quantified-Self (QS) study to build a descriptive model of sleep quality. A total of 472 days of my sleep data was collected with an Oura ring. This was combined with a variety of lifestyle, environmental, and psychological data, harvested from multiple sensors and manual logs. Such n-of-1 QS projects pose a number of specific challenges: heterogeneous data sources with many missing values; few observations and many features; dynamic feedback loops; and human biases. This paper directly addresses these challenges with an end-to-end QS pipeline for observational studies that combines techniques from statistics and machine learning to produce robust descriptive models. Sleep quality is one of the most difficult modelling targets in QS research, due to high noise and a large number of weakly-contributing factors. Sleep quality was selected so that approaches from this paper would generalise to most other n-of-1 QS projects. Techniques are presented for combining and engineering features for the different classes of data types, sample frequencies, and schema. This includes manually-tracked event logs and automatically-sampled weather and geo-spatial data. Relevant statistical analyses for outliers, normality, (auto)correlation, stationarity, and missing data are detailed, along with a proposed method for hierarchical clustering to identify correlated groups of features. The missing data was overcome using a combination of knowledge-based and statistical techniques, including several multivariate imputation algorithms. “Markov unfolding” is presented for collapsing the time series into a collection of independent observations, whilst incorporating historical information. The final model was interpreted in two key ways: by inspecting the internal β-parameters, and using the SHAP framework, which can explain any “black box” model. These two interpretation techniques were combined to produce a list of the 16 most-predictive features, demonstrating that an observational study can greatly narrow down the number of features that need to be considered when designing interventional QS studies.","",""
0,"Safa Omri, C. Sinz","Machine Learning Techniques for Software Quality Assurance: A Survey",2021,"","","","",118,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,2,1,"Over the last years, machine learning techniques have been applied to more and more application domains, including software engineering and, especially, software quality assurance. Important application domains have been, e.g., software defect prediction or test case selection and prioritization. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Closely related to estimating defect-prone parts of a software system is the question of how to select and prioritize test cases, and indeed test case prioritization has been extensively researched as a means for reducing the time taken to discover regressions in software. In this survey, we discuss various approaches in both fault prediction and test case prioritization, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs’ semantics and fault prediction features. We also review recently proposed machine learning methods for test case prioritization (TCP), and their ability to reduce the cost of regression testing without negatively affecting fault detection capabilities.","",""
0,"Bernhard Kühn, Marc H. Taylor, A. Kempf","Using machine learning to link spatiotemporal information to biological processes in the ocean: a case study for North Sea cod recruitment",2021,"","","","",119,"2022-07-13 09:22:24","","10.3354/MEPS13689","","",,,,,0,0.00,0,3,1,"Marine organisms are subject to environmental variability on various temporal and spatial scales, which affect processes related to growth and mortality of different life stages. Marine scientists are often faced with the challenge of identifying environmental variables that best explain these processes, which, given the complexity of the interactions, can be like searching for a needle in the proverbial haystack. Even after initial hypothesisbased variable selection, a large number of potential candidate variables can remain if different lagged and seasonal influences are considered. To tackle this problem, we propose a machine learning framework that incorporates important steps in model building, ranging from environmental signal extraction to automated variable selection and model validation. Its modular structure allows for the inclusion of both parametric and machine learning models, like random forest. Unsupervised feature extractions via empirical orthogonal functions (EOFs) or self-organising maps (SOMs) are demonstrated as a way to summarize spatiotemporal fields for inclusion in predictive models. The proposed framework offers a robust way to reduce model complexity through a multi-objective genetic algorithm (NSGAII) combined with rigorous cross-validation. We ap plied the framework to recruitment of the North Sea cod stock and investigated the effects of sea surface temperature (SST), salinity and currents on the stock via a modified version of random forest. The best model (5-fold CV r2 = 0.69) incorporated spawning stock biomass and EOF-derived time series of SST and salinity anomalies acting through different seasons, likely relating to differing environmental effects on specific life-history stages during the recruitment year. OPEN ACCESS Linking spatiotemporal information to North Sea cod recruitment via machine learning. Image: B. Kühn, M. Taylor; Gears from https:// commons. wikimedia.org/wiki/File:Gear_7.svg (CC-BY-SA license)","",""
0,"S. Saha, H. Singh, A. Soliman, S. Rajasekaran","A novel computational methodology for GWAS multi-locus analysis based on graph theory and machine learning",2021,"","","","",120,"2022-07-13 09:22:24","","10.1101/2021.10.22.21265388","","",,,,,0,0.00,0,4,1,"Background: Current form of genome-wide association studies (GWAS) is inadequate to accurately explain the genetics of complex traits due to the lack of sufficient statistical power. It explores each variant individually, but current studies show that multiple variants with varying effect sizes actually act in a concerted way to develop a complex disease. To address this issue, we have developed an algorithmic framework that can effectively solve the multi-locus problem in GWAS with a very high level of confidence. Our methodology consists of three novel algorithms based on graph theory and machine learning. It identifies a set of highly discriminating variants that are stable and robust with little (if any) spuriousness. Consequently, likely these variants should be able to interpret missing heritability of a convoluted disease as an entity. Results: To demonstrate the efficacy of our proposed algorithms, we have considered astigmatism case-control GWAS dataset. Astigmatism is a common eye condition that causes blurred vision because of an error in the shape of the cornea. The cause of astigmatism is not entirely known but a sizable inheritability is assumed. Clinical studies show that developmental disorders (such as, autism) and astigmatism co-occur in a statistically significant number of individuals. By performing classical GWAS analysis, we didn't find any genome-wide statistically significant variants. Conversely, we have identified a set of stable, robust, and highly predictive variants that can together explain the genetics of astigmatism. We have performed a set of biological enrichment analyses based on gene ontology (GO) terms, disease ontology (DO) terms, biological pathways, network of pathways, and so forth to manifest the accuracy and novelty of our findings. Conclusions: Rigorous experimental evaluations show that our proposed methodology can solve GWAS multi-locus problem effectively and efficiently. It can identify signals from the GWAS dataset having small number of samples with a high level of accuracy. We believe that the proposed methodology based on graph theory and machine learning is the most comprehensive one compared to any other machine learning based tools in this domain.","",""
0,"Wenxiu Xie, Christine Ji, Tianyong Hao, Chi-Yin Chow","Predicting the Easiness and Complexity of English Health Materials for International Tertiary Students With Linguistically Enhanced Machine Learning Algorithms: Development and Validation Study.",2021,"","","","",121,"2022-07-13 09:22:24","","10.2196/25110","","",,,,,0,0.00,0,4,1,"BACKGROUND There is an increasing body of research on the development of machine learning algorithms in the evaluation of online health educational resources for specific readerships. Machine learning algorithms are known for their lack of interpretability compared with statistics. Given their high predictive precision, improving the interpretability of these algorithms can help increase their applicability and replicability in health educational research and applied linguistics, as well as in the development and review of new health education resources for effective and accessible health education.   OBJECTIVE Our study aimed to develop a linguistically enriched machine learning model to predict binary outcomes of online English health educational resources in terms of their easiness and complexity for international tertiary students.   METHODS Logistic regression emerged as the best performing algorithm compared with support vector machine (SVM) (linear), SVM (radial basis function), random forest, and extreme gradient boosting on the transformed data set using L2 normalization. We applied recursive feature elimination with SVM to perform automatic feature selection. The automatically selected features (n=67) were then further streamlined through expert review. The finalized feature set of 22 semantic features achieved a similar area under the curve, sensitivity, specificity, and accuracy compared with the initial (n=115) and automatically selected feature sets (n=67). Logistic regression with the linguistically enhanced feature set (n=22) exhibited important stability and robustness on the training data of different sizes (20%, 40%, 60%, and 80%), and showed consistently high performance when compared with the other 4 algorithms (SVM [linear], SVM [radial basis function], random forest, and extreme gradient boosting).   RESULTS We identified semantic features (with positive regression coefficients) contributing to the prediction of easy-to-understand online health texts and semantic features (with negative regression coefficients) contributing to the prediction of hard-to-understand health materials for readers with nonnative English backgrounds. Language complexity was explained by lexical difficulty (rarity and medical terminology), verbs typical of medical discourse, and syntactic complexity. Language easiness of online health materials was associated with features such as common speech act verbs, personal pronouns, and familiar reasoning verbs. Successive permutation of features illustrated the interaction between these features and their impact on key performance indicators of the machine learning algorithms.   CONCLUSIONS The new logistic regression model developed exhibited consistency, scalability, and, more importantly, interpretability based on existing health and linguistic research. It was found that low and high linguistic accessibilities of online health materials were explained by 2 sets of distinct semantic features. This revealed the inherent complexity of effective health communication beyond current readability analyses, which were limited to syntactic complexity and lexical difficulty.","",""
0,"Ou Wu, Weiyao Zhu, Yingjun Deng, Haixiang Zhang, Qinghu Hou","A Mathematical Foundation for Robust Machine Learning based on Bias-Variance Trade-off",2021,"","","","",122,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,5,1,"A common assumption in machine learning is that samples are independently and identically distributed (i.i.d). However, the contributions of different samples are not identical in training. Some samples are difficult to learn and some samples are noisy. The unequal contributions of samples has a considerable effect on training performances. Studies focusing on unequal sample contributions (e.g., easy, hard, noisy) in learning usually refer to these contributions as robust machine learning (RML). Weighing and regularization are two common techniques in RML. Numerous learning algorithms have been proposed but the strategies for dealing with easy/hard/noisy samples differ or even contradict with different learning algorithms. For example, some strategies take the hard samples first, whereas some strategies take easy first. Conducting a clear comparison for existing RML algorithms in dealing with different samples is difficult due to lack of a unified theoretical framework for RML. This study attempts to construct a mathematical foundation for RML based on the bias-variance trade-off theory. A series of definitions and properties are presented and proved. Several classical learning algorithms are also explained and compared. Improvements of existing methods are obtained based on the comparison. A unified method that combines two classical learning strategies is proposed.","",""
0,"P. Rasouli, Ingrid Chieh Yu","Explainable Debugger for Black-box Machine Learning Models",2021,"","","","",123,"2022-07-13 09:22:24","","10.1109/IJCNN52387.2021.9533944","","",,,,,0,0.00,0,2,1,"The research around developing methods for debugging and refining Machine Learning (ML) models is still in its infancy. We believe employing tailored tools in the development process can help developers in creating more trustworthy and reliable models. This is particularly essential for creating black-box models such as deep neural networks and random forests, as their opaque decision-making and complex structure prevent detailed investigations. Although many explanation techniques provide interpretability in terms of predictive features for a mispredicted instance, it would be beneficial for a developer to find a partition of the training data that significantly influences the anomaly. Such responsible partitions can be subjected to data visualization and data engineering in the development phase to improve the model's accuracy. In this paper, we propose a systematic debugging framework for the development of ML models that guides the data engineering process using the model's decision boundary. Our approach finds the influential neighborhood of anomalous data points using observation-level feature importance and explains them via a novel quasi-global explanation technique. It is also equipped with a robust global explanation approach to reveal general trends and expose potential biases in the neighborhoods. We demonstrate the efficacy of the devised framework through several experiments on standard data sets and black-box models and propose various guidelines on how the framework's components can be practically useful from a developer's perspective.","",""
0,"İrem ERSÖZ KAYA, Oya Korkmaz","Machine Learning Approach for Predicting Employee Attrition and Factors Leading to Attrition",2021,"","","","",124,"2022-07-13 09:22:24","","10.21605/cukurovaumfd.1040487","","",,,,,0,0.00,0,2,1,"In this study that aims to prevent the attrition of human resource which is so important for enterprises, as well as to prevent the leave of employment which is the natural result of such attrition, employee attrition and factors causing attrition are tried to be determined by predictive analytics approaches. The sample dataset which contains 30 different attributes of 1470 employees was obtained for the analysis from a database provided by IBM Watson Analytics. In the study, seven different machine learning algorithms were used to evaluate the prediction achievements. The gain ratio approach was preferred in determining the factors causing attrition. The key point of the study was to cope with the imbalanced data through resampling with bootstrapping. Thereby, even in the blind test, prospering prediction performances reaching up to 80% accuracy were achieved in robust specificity without sacrificing sensitivity. Therewithal, the effective factors causing attrition were investigated in the study and it was concluded that the first 20 attributes ranked according to their gain ratio were sufficient in explaining attrition.","",""
0,"R. Arya, Jyoti Sharma, R. Shrivastava, Devyani Thapliyal, G. Verros","Modeling of Surfactant-Enhanced Drying of Poly(styrene)-p-xylene Polymeric Coatings Using Machine Learning Technique",2021,"","","","",125,"2022-07-13 09:22:24","","10.3390/coatings11121529","","",,,,,0,0.00,0,5,1,"In this work, a machine learning technique based on a regression tree model was used to model the surfactant enhanced drying of poly(styrene)-p-xylene coatings. The predictions of the developed model based on regression trees are in excellent agreement with the experimental data. A total of 16,258 samples were obtained through experimentation. These samples were separated into two parts: 12,960 samples were used for the training of the regression tree, and the remaining 3298 samples were used to test the tree’s prediction accuracy. MATLAB software was used to grow the regression tree. The mean squared error between the model-predicted values and actual outputs was calculated to be 8.8415 × 10−6. This model has good generalizing ability; predicts weight loss for given values of time, thickness, and triphenyl phosphate; and has a maximum error of 1%. It is robust and for this system, can be used for any composition and thickness for this system, which will drastically reduce the need for further experimentations to explain diffusion and drying.","",""
217,"Ian J. Goodfellow, Nicolas Papernot, P. Mcdaniel","Cleverhans V0.1: an Adversarial Machine Learning Library",2016,"","","","",126,"2022-07-13 09:22:24","","","","",,,,,217,36.17,72,3,6,"cleverhans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models’ performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the cleverhans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system.","",""
17,"E. Casiraghi, D. Malchiodi, G. Trucco, M. Frasca, L. Cappelletti, T. Fontana, A. Esposito, E. Avola, A. Jachetti, J. Reese, A. Rizzi, P. Robinson, G. Valentini","Explainable Machine Learning for Early Assessment of COVID-19 Risk Prediction in Emergency Departments",2020,"","","","",127,"2022-07-13 09:22:24","","10.1109/ACCESS.2020.3034032","","",,,,,17,8.50,2,13,2,"Between January and October of 2020, the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) virus has infected more than 34 million persons in a worldwide pandemic leading to over one million deaths worldwide (data from the Johns Hopkins University). Since the virus begun to spread, emergency departments were busy with COVID-19 patients for whom a quick decision regarding in- or outpatient care was required. The virus can cause characteristic abnormalities in chest radiographs (CXR), but, due to the low sensitivity of CXR, additional variables and criteria are needed to accurately predict risk. Here, we describe a computerized system primarily aimed at extracting the most relevant radiological, clinical, and laboratory variables for improving patient risk prediction, and secondarily at presenting an explainable machine learning system, which may provide simple decision criteria to be used by clinicians as a support for assessing patient risk. To achieve robust and reliable variable selection, Boruta and Random Forest (RF) are combined in a 10-fold cross-validation scheme to produce a variable importance estimate not biased by the presence of surrogates. The most important variables are then selected to train a RF classifier, whose rules may be extracted, simplified, and pruned to finally build an associative tree, particularly appealing for its simplicity. Results show that the radiological score automatically computed through a neural network is highly correlated with the score computed by radiologists, and that laboratory variables, together with the number of comorbidities, aid risk prediction. The prediction performance of our approach was compared to that that of generalized linear models and shown to be effective and robust. The proposed machine learning-based computational system can be easily deployed and used in emergency departments for rapid and accurate risk prediction in COVID-19 patients.","",""
4,"Himaghna Bhattacharjee, Nikolaos Anesiadis, D. Vlachos","Regularized machine learning on molecular graph model explains systematic error in DFT enthalpies",2021,"","","","",128,"2022-07-13 09:22:24","","10.1038/s41598-021-93854-w","","",,,,,4,4.00,1,3,1,"","",""
1,"Guanyu Wang, T. Fearn, Tengyao Wang, K. Choy","Insight Gained from Using Machine Learning Techniques to Predict the Discharge Capacities of Doped Spinel Cathode Materials for Lithium‐Ion Batteries Applications",2021,"","","","",129,"2022-07-13 09:22:24","","10.1002/ENTE.202100053","","",,,,,1,1.00,0,4,1,"Rechargeable lithium-ion batteries (LIBs) are known as the most promising energy storage technology due to their high energy density, high power density, and long charge/discharge life cycle. Presently, an extensive amount of research has been devoted to boosting their performance as to complement the applications in sustainable energy power and this demands for the improvements in storage capacity, steadiness and safety, toxicity, eco-friendliness, and material cost. For this, discovering new cathode materials has become the key as they both contribute to 33% of the total battery cost and compared with the anode, have much lower storage capacity, and therefore greatly limit on the battery discharging capacities. Among all cathode materials, spinel cathode materials (LiMn2O4) are preferred over the widely commercialized lithium cobalt oxide (LiCoO2) material for its nontoxic nature, robust 3D structure (high Li-ion diffusion), and low cost (manganese metal is more abundant than cobalt metal). However, the issues of drastic capacity fading and limited rate performance have restricted their use in large-scale commercial applications. These inferior properties can be explained by two underlying chemical phenomena. The first is the dissolution of the manganese ions Mn3þ from the material surface into various forms of Mn4þ(solid) and Mn2þ(sol), which reduces the Li-ions site energy and eventually lowers the rate of reversible electrochemical reactions. The second reason is the Jahn–Teller distortion (JRD) effects initialized from the high spin electrons interactions from the d-orbital electrons of manganese ion (Mn3þ), which destabilize the overall crystal structure and hence reduce the respective cycle life. Doping the manganese (III) sites with lower valence (lower than 3þ) dopants (Figure 1a) seems to be an effective approach to this problem as it increases the average Mn valence in LiMn2O4 to suppress the JTD effect through reducing the concentration of Mn (III) and eventually decrease the rate of dissolution reaction. Indeed, promising results have been seen in capacity improvement for lower valence dopants such as Al, Cr, Fe, Gd, Ga, Mg, Nd, Ni, Ru, Sc, and Zn; however, the use of higher valence dopants such as Si and Sn are also shown to be effective. Figure 1b shows G. Wang Institute for Materials Discovery University College London Roberts Building, London WC1E 7JE, UK Prof. T. Fearn, Dr. T. Wang Department of Statistical Science University College London 1-19 Torrington Place, London WC1R 7HB, UK E-mail: tengyao.wang@ucl.ac.UK Prof. K.-L. Choy Institute for Materials Discovery Faculty of Maths and Physical Sciences University College London Roberts Building, London WC1E 7JE, UK E-mail: k.choy@ucl.ac.uk","",""
16,"Sushant Agarwal, S. Jabbari, Chirag Agarwal, Sohini Upadhyay, Zhiwei Steven Wu, Himabindu Lakkaraju","Towards the Unification and Robustness of Perturbation and Gradient Based Explanations",2021,"","","","",130,"2022-07-13 09:22:24","","","","",,,,,16,16.00,3,6,1,"As machine learning black boxes are increasingly being deployed in critical domains such as healthcare and criminal justice, there has been a growing emphasis on developing techniques for explaining these black boxes in a post hoc manner. In this work, we analyze two popular post hoc interpretation techniques: SmoothGrad which is a gradient based method, and a variant of LIME which is a perturbation based method. More specifically, we derive explicit closed form expressions for the explanations output by these two methods and show that they both converge to the same explanation in expectation, i.e., when the number of perturbed samples used by these methods is large. We then leverage this connection to establish other desirable properties, such as robustness, for these techniques. We also derive finite sample complexity bounds for the number of perturbations required for these methods to converge to their expected explanation. Finally, we empirically validate our theory using extensive experimentation on both synthetic and real world datasets.1","",""
28,"Jiuwen Cao, K. Zhang, Hongwei Yong, Xiaoping Lai, Badong Chen, Zhiping Lin","Extreme Learning Machine With Affine Transformation Inputs in an Activation Function",2019,"","","","",131,"2022-07-13 09:22:24","","10.1109/TNNLS.2018.2877468","","",,,,,28,9.33,5,6,3,"The extreme learning machine (ELM) has attracted much attention over the past decade due to its fast learning speed and convincing generalization performance. However, there still remains a practical issue to be approached when applying the ELM: the randomly generated hidden node parameters without tuning can lead to the hidden node outputs being nonuniformly distributed, thus giving rise to poor generalization performance. To address this deficiency, a novel activation function with an affine transformation (AT) on its input is introduced into the ELM, which leads to an improved ELM algorithm that is referred to as an AT-ELM in this paper. The scaling and translation parameters of the AT activation function are computed based on the maximum entropy principle in such a way that the hidden layer outputs approximately obey a uniform distribution. Application of the AT-ELM algorithm in nonlinear function regression shows its robustness to the range scaling of the network inputs. Experiments on nonlinear function regression, real-world data set classification, and benchmark image recognition demonstrate better performance for the AT-ELM compared with the original ELM, the regularized ELM, and the kernel ELM. Recognition results on benchmark image data sets also reveal that the AT-ELM outperforms several other state-of-the-art algorithms in general.","",""
10,"Yufang Jin, Bin Chen, B. Lampinen, P. Brown","Advancing Agricultural Production With Machine Learning Analytics: Yield Determinants for California’s Almond Orchards",2020,"","","","",132,"2022-07-13 09:22:24","","10.3389/fpls.2020.00290","","",,,,,10,5.00,3,4,2,"Agricultural productivity is subject to various stressors, including abiotic and biotic threats, many of which are exacerbated by a changing climate, thereby affecting long-term sustainability. The productivity of tree crops such as almond orchards, is particularly complex. To understand and mitigate these threats requires a collection of multi-layer large data sets, and advanced analytics is also critical to integrate these highly heterogeneous datasets to generate insights about the key constraints on the yields at tree and field scales. Here we used a machine learning approach to investigate the determinants of almond yield variation in California’s almond orchards, based on a unique 10-year dataset of field measurements of light interception and almond yield along with meteorological data. We found that overall the maximum almond yield was highly dependent on light interception, e.g., with each one percent increase in light interception resulting in an increase of 57.9 lbs/acre in the potential yield. Light interception was highest for mature sites with higher long term mean spring incoming solar radiation (SRAD), and lowest for younger orchards when March maximum temperature was lower than 19°C. However, at any given level of light interception, actual yield often falls significantly below full yield potential, driven mostly by tree age, temperature profiles in June and winter, summer mean daily maximum vapor pressure deficit (VPDmax), and SRAD. Utilizing a full random forest model, 82% (±1%) of yield variation could be explained when using a sixfold cross validation, with a RMSE of 480 ± 9 lbs/acre. When excluding light interception from the predictors, overall orchard characteristics (such as age, location, and tree density) and inclusive meteorological variables could still explain 78% of yield variation. The model analysis also showed that warmer winter conditions often limited mature orchards from reaching maximum yield potential and summer VPDmax beyond 40 hPa significantly limited the yield. Our findings through the machine learning approach improved our understanding of the complex interaction between climate, canopy light interception, and almond nut production, and demonstrated a relatively robust predictability of almond yield. This will ultimately benefit data-driven climate adaptation and orchard nutrient management approaches.","",""
10,"T. Botari, Frederik Hvilshøj, Rafael Izbicki, A. Carvalho","MeLIME: Meaningful Local Explanation for Machine Learning Models",2020,"","","","",133,"2022-07-13 09:22:24","","","","",,,,,10,5.00,3,4,2,"Most state-of-the-art machine learning algorithms induce black-box models, preventing their application in many sensitive domains. Hence, many methodologies for explaining machine learning models have been proposed to address this problem. In this work, we introduce strategies to improve local explanations taking into account the distribution of the data used to train the black-box models. We show that our approach, MeLIME, produces more meaningful explanations compared to other techniques over different ML models, operating on various types of data. MeLIME generalizes the LIME method, allowing more flexible perturbation sampling and the use of different local interpretable models. Additionally, we introduce modifications to standard training algorithms of local interpretable models fostering more robust explanations, even allowing the production of counterfactual examples. To show the strengths of the proposed approach, we include experiments on tabular data, images, and text; all showing improved explanations. In particular, MeLIME generated more meaningful explanations on the MNIST dataset than methods such as GuidedBackprop, SmoothGrad, and Layer-wise Relevance Propagation. MeLIME is available on this https URL.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",134,"2022-07-13 09:22:24","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
8,"C. Rea, K. Montes, A. Pau, R. Granetz, O. Sauter","Progress Toward Interpretable Machine Learning–Based Disruption Predictors Across Tokamaks",2020,"","","","",135,"2022-07-13 09:22:24","","10.1080/15361055.2020.1798589","","",,,,,8,4.00,2,5,2,"Abstract In this paper we lay the groundwork for a robust cross-device comparison of data-driven disruption prediction algorithms on DIII-D and JET tokamaks. In order to consistently carry on a comparative analysis, we define physics-based indicators of disruption precursors based on temperature, density, and radiation profiles that are currently not used in many other machine learning predictors for DIII-D data. These profile-based indicators are shown to well-describe impurity accumulation events in both DIII-D and JET discharges that eventually disrupt. The univariate analysis of the features used as input signals in the data-driven algorithms applied on the data of both tokamaks statistically highlights the differences in the dominant disruption precursors. JET with its ITER-like wall is more prone to impurity accumulation events, while DIII-D is more subject to edge-cooling mechanisms that destabilize dangerous magnetohydrodynamic modes. Even though the analyzed data sets are characterized by such intrinsic differences, we show through a few examples that the inclusion of physics-based disruption markers in data-driven algorithms is a promising path toward the realization of a uniform framework to predict and interpret disruptive scenarios across different tokamaks. As long as the destabilizing precursors are diagnosed in a device-independent way, the knowledge that data-driven algorithms learn on one device can be re-used to explain a disruptive behavior on another device.","",""
8,"Daniil Bash, Yongqiang Cai, Vijila Chellappan, S. L. Wong, Yang Xu, Pawan Kumar, J. Tan, Anas Abutaha, J. Cheng, Y. Lim, S. Tian, D. Ren, Flore Mekki-Barrada, W. Wong, J. Kumar, Saif A. Khan, Qianxiao Li, T. Buonassisi, K. Hippalgaonkar","Machine Learning and High-Throughput Robust Design of P3HT-CNT Composite Thin Films for High Electrical Conductivity",2020,"","","","",136,"2022-07-13 09:22:24","","10.26434/chemrxiv.13265288.v1","","",,,,,8,4.00,1,19,2,"Combining high-throughput experiments with machine learning allows quick optimization of parameter spaces towards achieving target properties. In this study, we demonstrate that machine learning, combined with multi-labeled datasets, can additionally be used for scientific understanding and hypothesis testing. We introduce an automated flow system with high-throughput drop-casting for thin film preparation, followed by fast characterization of optical and electrical properties, with the capability to complete one cycle of learning of fully labeled ~160 samples in a single day. We combine regio-regular poly-3-hexylthiophene with various carbon nanotubes to achieve electrical conductivities as high as 1200 S/cm. Interestingly, a non-intuitive local optimum emerges when 10% of double-walled carbon nanotubes are added with long single wall carbon nanotubes, where the conductivity is seen to be as high as 700 S/cm, which we subsequently explain with high fidelity optical characterization. Employing dataset resampling strategies and graph-based regressions allows us to account for experimental cost and uncertainty estimation of correlated multi-outputs, and supports the proving of the hypothesis linking charge delocalization to electrical conductivity. We therefore present a robust machine-learning driven high-throughput experimental scheme that can be applied to optimize and understand properties of composites, or hybrid organic-inorganic materials.","",""
6,"Anika Gebauer, Monja Ellinger, Victor M. Brito Gómez, Mareike Ließ","Development of pedotransfer functions for water retention in tropical mountain soil landscapes: spotlight on parameter tuning in machine learning",2020,"","","","",137,"2022-07-13 09:22:24","","10.5194/soil-6-215-2020","","",,,,,6,3.00,2,4,2,"Abstract. Machine-learning algorithms are good at computing non-linear problems and fitting complex composite functions, which makes them an adequate tool for addressing multiple environmental research questions. One important application is the development of pedotransfer functions (PTFs). This study aims to develop water retention PTFs for two remote tropical mountain regions with rather different soil landscapes: (1) those dominated by peat soils and soils under volcanic influence with high organic matter contents and (2) those dominated by tropical mineral soils. Two tuning procedures were compared to fit boosted regression tree models: (1) tuning with grid search, which is the standard approach in pedometrics; and (2) tuning with differential evolution optimization. A nested cross-validation approach was applied to generate robust models. The area-specific PTFs developed outperform other more general PTFs. Furthermore, the first PTF for typical soils of Paramo landscapes (Ecuador), i.e., organic soils under volcanic influence, is presented. Overall, the results confirmed the differential evolution algorithm's high potential for tuning machine-learning models. While models based on tuning with grid search roughly predicted the response variables' mean for both areas, models applying the differential evolution algorithm for parameter tuning explained up to 25 times more of the response variables' variance.","",""
2,"P. Krishnamurthy, Animesh Basak Chowdhury, Benjamin Tan, F. Khorrami, R. Karri","Explaining and Interpreting Machine Learning CAD Decisions: An IC Testing Case Study",2020,"","","","",138,"2022-07-13 09:22:24","","10.1145/3380446.3430643","","",,,,,2,1.00,0,5,2,"We provide a methodology to explain and interpret machine learning decisions in Computer-Aided Design (CAD) flows. We demonstrate the efficacy of the methodology to the VLSI testing case. Such a tool will provide designers with insight into the “black box” machine learning models/classifiers through human readable sentences based on normally understood design rules or new design rules. The methodology builds on an intrinsically explainable, rule-based ML framework, called Sentences in Feature Subsets (SiFS), to mine human readable decision rules from empirical data sets. SiFS derives decision rules as compact Boolean logic sentences involving subsets of features in the input data. The approach is applied to test point insertion problem in circuits and compared to the ground truth and traditional design rules.","",""
5,"J. Papenbrock, P. Schwendner, Markus Jaeger, Stephan Krügel","Matrix Evolutions: Synthetic Correlations and Explainable Machine Learning for Constructing Robust Investment Portfolios",2020,"","","","",139,"2022-07-13 09:22:24","","10.2139/ssrn.3663220","","",,,,,5,2.50,1,4,2,"In this article, the authors present a novel and highly flexible concept to simulate correlation matrixes of financial markets. It produces realistic outcomes regarding stylized facts of empirical correlation matrixes and requires no asset return input data. The matrix generation is based on a multiobjective evolutionary algorithm, so the authors call the approach matrix evolutions. It is suitable for parallel implementation and can be accelerated by graphics processing units and quantum-inspired algorithms. The approach is useful for backtesting, pricing, and hedging correlation-dependent investment strategies and financial products. Its potential is demonstrated in a machine learning case study for robust portfolio construction in a multi-asset universe: An explainable machine learning program links the synthetic matrixes to the portfolio volatility spread of hierarchical risk parity versus equal risk contribution. TOPICS: Statistical methods, big data/machine learning, portfolio construction, performance measurement Key Findings ▪ The authors introduce the matrix evolutions concept based on an evolutionary algorithm to simulate correlation matrixes useful for financial market applications. ▪ They apply the resulting synthetic correlation matrixes to benchmark hierarchical risk parity (HRP) and equal risk contribution allocations of a multi-asset futures portfolio and find HRP to show lower portfolio risk. ▪ The authors evaluate three competing machine learning methods to regress the portfolio risk spread between both allocation methods against statistical features of the synthetic correlation matrixes and then discuss the local and global feature importance using the SHAP framework by Lundberg and Lee (2017).","",""
2,"Yangxiaoyue Liu, Xiaolin Xia, L. Yao, Wenlong Jing, Chenghu Zhou, Wumeng Huang, Yong Li, Ji Yang","Downscaling Satellite Retrieved Soil Moisture Using Regression Tree‐Based Machine Learning Algorithms Over Southwest France",2020,"","","","",140,"2022-07-13 09:22:24","","10.1029/2020EA001267","","",,,,,2,1.00,0,8,2,"Satellite retrieved soil moisture (SM) shows great potential in hydrological, meteorological, ecological, and agricultural applications, while the coarse resolution limits its utilization in regional scale. The regression tree‐based machine learning algorithms reveal promising capability in SM downscaling. However, it lacks systematic study dedicated to intercomparisons of algorithms to explicitly illuminate their characteristics. In this study, comparisons are made to systematically evaluate performances of classification and regression tree (CART), random forest (RF), gradient boost decision tree (GBDT), and extreme gradient boost (XGB) in Soil Moisture Active Passive (SMAP) SM downscaling in southwest France. The results show that the four algorithms downscaled SM are capable of capturing spatial distribution features of the original SMAP SM. The downscaled regions with favorable accuracy are mostly situated in the dominant Mediterranean climate zone with moderate vegetation coverage and mild topography variation. The best results are obtained by GBDT in grassland with R value of 0.77 and ubRMSE value of 0.04 m3/m3. The RF and XGB also achieve good performances. On the whole, the GBDT approach is robust and reliable, which could downscale SM with superior correlation and smaller bias than the others. Besides, it achieves higher accuracy than the original SMAP in grassland and shrubland. The feature importance index of each explainable variable fluctuates regularly among different seasons and models. This study proves the outstanding performance of GBDT in SMAP SM downscaling and is expected to act as a valuable reference for studies focusing on SM scale conversion algorithms.","",""
5,"A. Barnard, A. Parker, B. Motevalli, G. Opletal","The pure and representative types of disordered platinum nanoparticles from machine learning.",2020,"","","","",141,"2022-07-13 09:22:24","","10.1088/1361-6528/abcc23","","",,,,,5,2.50,1,4,2,"The development of interpretable structure/property relationships is a cornerstone of nanoscience, but can be challenging when the structural diversity and complexity exceeds our ability to characterise it. This is often the case for imperfect, disordered and amorphous nanoparticles, where even the nomenclature can be unspecific. Disordered platinum nanoparticles have exhibited superior performance for some reactions, which makes a systematic way of describing them highly desirable. In this study we have used a diverse set of disorder platinum nanoparticles and machine learning to identify the pure and representative structures based on their similarity in 121 dimensions. We identify two prototypes that are representative of separable classes, and seven archetypes that are the pure structures on the convex hull with which all other possibilities can be described. Together these nine nanoparticles can explain all of the variance in the set, and can be described as either single crystal, twinned, spherical or branched; with or without roughened surfaces. This forms a robust sub-set of platinum nanoparticle upon which to base further work, and provides a theoretical basis for discussing structure/property relationships of platinum nanoparticles that are not geometrically ideal.","",""
2,"L. Israel, Felix D. Schönbrodt","Predicting affective appraisals from facial expressions and physiology using machine learning",2020,"","","","",142,"2022-07-13 09:22:24","","10.3758/s13428-020-01435-y","","",,,,,2,1.00,1,2,2,"","",""
3,"N. Lau, Michael Hildebrandt, M. Jeon","Ergonomics in AI: Designing and Interacting With Machine Learning and AI",2020,"","","","",143,"2022-07-13 09:22:24","","10.1177/1064804620915238","","",,,,,3,1.50,1,3,2,"Machine learning and artificial intelligence (AI) enable new types of autonomous systems that are changing our personal and professional lives. While there are plenty of stories about machine learning delivering the promise of a better future, such as autonomous vehicles for improving mobility, safety, and fuel efficiency, many examples have indicated great risks, such as bots on social media for spreading false information and manipulating public opinions. As machine learning approaches ubiquity in industrial systems and consumer products, Human Factors must innovate to support users in coping with emerging autonomous capabilities. The rise of machine learning technologies poses serious questions that have been discussed in the panels at the recent Annual Meetings of the Human Factors Ergonomics Society (Lau et al., 2018; Lau et al., 2019). How can we help users understand the autonomous capabilities developed through supervised or unsupervised learning? What kind of interactions could enhance cooperation between human and machine learning algorithms? We must also take advantage of machine learning techniques in advancing our own research and design science. How can we use machine learning in assessing human states and capabilities? How should we help incorporate human sensing into machine learning algorithms? In this special issue, we embrace the broad spectrum of research and design efforts that investigate machine learning for improving usability and safety of intelligent systems and consumer products. Our goal is to clarify the roles of Human Factors in contributing to a humanist perspective that considers the social, political, ethical and cultural factors of implementing AI into daily human–system interactions. At the same time, this special issue can only accommodate five articles, about one third of the submissions, after a rigorous peer-review process. So, what we have hoped to curate for the readers is an intellectually stimulating, short exhibit of our discipline in developing and applying next-generation AI. The first article in this special issue is a commentary by Hancock who envisions work to be eventually shared between self-evolving machines and humans. This vision of work challenges the Human Factors community to prepare for a future that requires designing interactions and user interfaces for machines whose behaviors we cannot fully anticipate and for work that we do not yet know. The second article by Zhang et al. speaks to the challenge of anticipating the consequences of machine learning in designing technology and making policy decisions. The authors use a speech recognition example to illustrate a violation of inclusivity in design. In their second example, they illustrate how a loan policy aimed at supporting a disadvantaged group ultimately harms the group in the long run. These examples raise questions about how Human Factors professionals can engage in a data-driven design process and how to develop “explainable” AI that presents the true behavior of the machine learning model to the user. Anthropomorphism is a much talkedabout concept to help design AI that humans can understand and interact with intuitively. Muller compares how deep neural networks and humans classify images to illustrate how their differences likely require appropriate interactivity to minimize the mismatch between how human and AI think about the intelligence of each other. The article highlights not only the need for interaction design but also the importance of understanding machine learning algorithms. The final two articles present empirical investigations on how ergonomic research can promote the appropriate use of machine learning tools. Wang et al. describe metrics of stability, robustness and sensitivity for aiding users to interpret prediction results of supervised learning algorithms. The authors illustrate how effective visualization of those metrics can improve decision making. Gilbank et al. describe a qualitative study with ten medical professionals using a machine learning–driven toxicity prediction tool that utilizes 10 years of historical data. The study presents the expectations and perspectives of medical professionals on AI, and the user interface design considerations for promoting trust and, ultimately use of the machine learning system. We hope that these five articles contribute to new thoughts and present exciting challenges. At the same time, we recognize that these articles only represent a speck of all the relevant Human Factors research and design issues related to machine learning and artificial intelligence. As we prepared this special issue, we realized that so much progress is needed to formulate design methods for machine learning. So, we agree with Hancock’s conclusion that our road ahead for ergonomics design of machine learning and artificial intelligence “promises to be a bumpy but exciting ride.”","",""
3,"Barbara Rychalska, Dominika Basaj, P. Biecek","Are you tough enough? Framework for Robustness Validation of Machine Comprehension Systems",2018,"","","","",144,"2022-07-13 09:22:24","","","","",,,,,3,0.75,1,3,4,"Deep Learning NLP domain lacks procedures for the analysis of model robustness. In this paper we propose a framework which validates robustness of any Question Answering model through model explainers. We propose that a robust model should transgress the initial notion of semantic similarity induced by word embeddings to learn a more human-like understanding of meaning. We test this property by manipulating questions in two ways: swapping important question word for 1) its semantically correct synonym and 2) for word vector that is close in embedding space. We estimate importance of words in asked questions with Locally Interpretable Model Agnostic Explanations method (LIME). With these two steps we compare state-of-the-art Q&A models. We show that although accuracy of state-of-the-art models is high, they are very fragile to changes in the input. Moreover, we propose 2 adversarial training scenarios which raise model sensitivity to true synonyms by up to 7% accuracy measure. Our findings help to understand which models are more stable and how they can be improved. In addition, we have created and published a new dataset that may be used for validation of robustness of a Q&A model.","",""
1,"M. Quade, Thomas Isele, Markus Abel","Explainable Machine Learning Control - robust control and stability analysis",2020,"","","","",145,"2022-07-13 09:22:24","","","","",,,,,1,0.50,0,3,2,"Recently, the term explainable AI became known as an approach to produce models from artificial intelligence which allow interpretation. Since a long time, there are models of symbolic regression in use that are perfectly explainable and mathematically tractable: in this contribution we demonstrate how to use symbolic regression methods to infer the optimal control of a dynamical system given one or several optimization criteria, or cost functions. In previous publications, network control was achieved by automatized machine learning control using genetic programming. Here, we focus on the subsequent analysis of the analytical expressions which result from the machine learning. In particular, we use AUTO to analyze the stability properties of the controlled oscillator system which served as our model. As a result, we show that there is a considerable advantage of explainable models over less accessible neural networks.","",""
1,"R. Masoudi, S. Mohaghegh, Daniel Yingling, A. Ansari, Hadi B. Amat, N. Mohamad, Ali Sabzabadi, Dipak Mandel","Subsurface Analytics Case Study; Reservoir Simulation and Modeling of Highly Complex Offshore Field in Malaysia, Using Artificial Intelligent and Machine Learning",2020,"","","","",146,"2022-07-13 09:22:24","","10.2118/201693-ms","","",,,,,1,0.50,0,8,2,"  Using commercial numerical reservoir simulators to build a full field reservoir model and simultaneously history match multiple dynamic variables for a highly complex, offshore mature field in Malaysia, had proven to be challenging, manpower intensive, highly expensive, and not very successful. This field includes almost two hundred wells that have been completed in more than 60 different, non-continuous reservoir layers. The field has been producing oil, gas and water for decades. The objective of this article is to demonstrate how Artificial Intelligence (AI) and Machine Learning is used to build a purely data-driven reservoir simulation model that successfully history match all the dynamic variables for all the wells in this field and subsequently used for production forecast. The model has been validated in space and time.  The AI and Machine Learning technology that was used to build the dynamic reservoir simulation and modeling is called spatio-temporal learning. Spatio-temporal learning is a machine-learning algorithm specifically developed for data-driven modeling of the physics of fluid flow through porous media. Spatio-temporal learning is used in the context of Deconvolutional Neural Networks. In this article Spatio-temporal Learning and Deconvolutional Neural Networks will be explained. This new technology is the result of more than 20 years of research and development in the application of AI and Machine Learning in reservoir modeling. This technology develops a coupled reservoir and wellbore model that for this particular oil & gas field in Malaysia uses choke setting, well-head pressure and well-head temperature as input and simultaneously history matches Oil production, GOR, WC, reservoir pressure, and water saturation for more than a hundred wells through a completely automated process.  Once the data-driven reservoir model is developed and history matched, it is blind validated in space and time in order to establish a reliable and robust reservoir model to be used for decision making purposes and opportunity generation to maximise the field value. The concepts and the methodology of history match of multiple wells, individual offshore platforms, and the entire field will be presented in this article along with the results of blind validation and production forecasting. Results of using this model to perform uncertainty quantification will also be presented.  A case study of a highly complex mature field with large number of wells and years of production has been used to be studied and simulated by this data-driven approach. Time, efforts, and resources required for the development of the dynamic reservoir simulation models using AI and Machine Learning is considerably less than time and resources required using the commercial numerical simulators. It is validated that the TDM developed model can make very reasonable prediction of field behavior and production from the existing wells based on modification of operational constraints and can be a prudent complementary tool to conventional numerical simulators for such complex assets.","",""
1,"M. Amini, Ahmed Imteaj, J. Mohammadi","Distributed Machine Learning for Resilient Operation of Electric Systems",2020,"","","","",147,"2022-07-13 09:22:24","","10.1109/SEST48500.2020.9203368","","",,,,,1,0.50,0,3,2,"Power system resilience is crucial to ensure secure energy delivery to electricity consumers. Power system outages lead to economical and societal burdens for the society and industries. To mitigate the socio-economical impacts of a power outage, we need to develop efficient algorithms to ensure resilient operation of the power system. In this paper, we first explain the notion of data-driven resilience. Then, we present a pathway of leveraging edge intelligence to improve resilience. To this end, we propose a novel distributed machine learning paradigm. Our proposed structure relies on local Resilience Management Systems (RMS) that serve as intelligent decision-making entities in each area, e.g. an autonomous micro-grid or a smart home can act as RMS. The RMS agents, which are available in different areas, can share their local data (i.e., a microgrid's operational data) with their neighboring RMS to coordinate their decisions in a distributed fashion. This will provide two major advantages: 1) distributed intelligence replaces centralized decision-making leading to robust decision-making and enhanced resilience; 2) since local data are locally shared among all entities within an RMS, if one of the RMS agents fails to communicate with the rest of network, we still can maintain a feasible solution (which is not necessarily optimal). Finally, we presents different scenarios in the simulation results section that showcases the system performance for two buildings under various outage scenarios.","",""
1,"Zheren Ma, E. Davani, Xiaodan Ma, Hanna Lee, I. Arslan, Xiang Zhai, H. Darabi, D. Castineira","Finding a Trend Out of Chaos, A Machine Learning Approach for Well Spacing Optimization",2020,"","","","",148,"2022-07-13 09:22:24","","10.2118/201698-ms","","",,,,,1,0.50,0,8,2,"  Data-driven decisions powered by machine-learning methods are increasing in popularity when it comes to optimizing field development in unconventional reservoirs. However, since well performance is impacted by many factors (e.g., geological characteristics, completion design, well design, etc.), the challenge is uncovering trends from all the noise.  By leveraging basin-level knowledge captured by big data sculpting, integrating private and public data with the use of uncertainty quantification, Augmented AI (a combination of expert-based knowledge and advanced AI frameworks) can provide quick and science-based answers for well spacing and fracking optimization and assess the full potential of an asset in unconventional reservoirs.  Augmented AI is artificial intelligence powered by engineering wisdom. The Augmented AI workflow starts with data sculpting, which includes information retrieval, data cleaning and standardization, and finally a smart, deep and systematic data QC. Feature engineering generates all the relevant parameters going into the machine learning model—over 50 features have been generated for this work and categorized. The final step is to perform model tuning and ensemble, evaluating the model robustness, generating model explanation and uncertainty quantification. Augmented AI adopts an iterative machine learning modeling approach. This approach combines new and innovative engineering and G&G workflows with data-driven models so that a deep understanding of the field behavior can be developed. Loops from feature selection to model tuning are used until good model results are achieved. The loop is automated using Bayesians optimization. All machine learning models have different strengths and weaknesses for prediction. Instead of manually determining which machine learning model to use, this approach uses an adaptive ensemble machine learning approach that is a stacking algorithm that combines multiple regression models via a second level machine learning model. It smartly aggregates opinions from different models with reduced variance and better robustness.  Augmented AI has been applied in unconventional reservoirs with great results. A case study in Midland Basin is presented in this paper. Domain-induced feature engineering was performed to obtain important features for predicting well performance, and initial feature selection was conducted using feature correlation analysis. A trusted and explainable ML model was built and enhanced with uncertainty quantification. After running several sensitivity analyses, Augmented AI optimized the attributes of interest, then vetted the outcome, generating a report and visualizing the results.  In addition, further information about the direct impact of well spacing on EUR was deconvoluted from other parameters using an ML explanation technique for Wolfcamp Formation in Permian Basin and subsequently well spacing optimization was presented for the case study in Midland Basin.  An innovative model was created using Augmented AI to optimize well spacing, leveraging big data sculpting, domain and physics-induced feature engineering, and machine learning. The learning was transferred from the basin model to the specific region of interest. Augmented AI provides efficient and systematic private data organization, an explainable machine learning model, robust production forecast with quantified uncertainty and well spacing and frac parameters optimization.  Augmented AI models are already built for major basins such as Midland and Delaware basins. The learning and knowledge of the model can be transferred to any region in a basin and can be refined using more accurate private data. This allows conclusions to be drawn even with a limited number of wells.","",""
14,"Hongfeng Li, Hong-Qin Zhao, Hong Li","Neural-Response-Based Extreme Learning Machine for Image Classification",2019,"","","","",149,"2022-07-13 09:22:24","","10.1109/TNNLS.2018.2845857","","",,,,,14,4.67,5,3,3,"This paper proposes a novel and simple multilayer feature learning method for image classification by employing the extreme learning machine (ELM). The proposed algorithm is composed of two stages: the multilayer ELM (ML-ELM) feature mapping stage and the ELM learning stage. The ML-ELM feature mapping stage is recursively built by alternating between feature map construction and maximum pooling operation. In particular, the input weights for constructing feature maps are randomly generated and hence need not be trained or tuned, which makes the algorithm highly efficient. Moreover, the maximum pooling operation enables the algorithm to be invariant to certain transformations. During the ELM learning stage, elastic-net regularization is proposed to learn the output weight. Elastic-net regularization helps to learn more compact and meaningful output weight. In addition, we preprocess the input data with the dense scale-invariant feature transform operation to improve both the robustness and invariance of the algorithm. To evaluate the effectiveness of the proposed method, several experiments are conducted on three challenging databases. Compared with the conventional deep learning methods and other related ones, the proposed method achieves the best classification results with high computational efficiency.","",""
0,"Gabriel D. Patrón, D. León, Edwin Lopez, G. Hernández","An Interpretable Automated Machine Learning Credit Risk Model",2020,"","","","",150,"2022-07-13 09:22:24","","10.1007/978-3-030-61834-6_2","","",,,,,0,0.00,0,4,2,"","",""
0,"G. Montavon, W. Samek","Statistics meets Machine Learning 5 Abstracts Explaining the decisions of deep neural networks and beyond",2020,"","","","",151,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,2,2,"Explaining the decisions of deep neural networks and beyond Grégoire Montavon and Wojciech Samek (joint work with Klaus-Robert Müller, Sebastian Lapuschkin, Alexander Binder, Jacob Kauffmann) Machine learning models have become increasingly complex and this complexity has allowed them to reach high prediction accuracy on challenging datasets. In some cases, improved predictivity has come at the expense of interpretability, in particular, complex models tend to be perceived as black-boxes. A lack of interpretability is problematic, not only because interpretability is desirable in itself (e.g. to extract useful insights from a model or from the modeled data), but also because common measurements of prediction accuracy can become strongly unreliable when certain assumptions about the training data are not met. Real-world datasets are typically not representative of all possible cases and the truly relevant variables may correlate with other irrelevant variables. In such circumstances, one would need to ensure that the machine learning model does not rely on these irrelevant variables. An assessment based purely on test set accuracy would be oblivious to the exact decision strategy and could overestimate the true prediction performance. This phenomenon has been referred to as the ‘Clever Hans’ effect [9]. Only an extension of the dataset with specific test cases, or an inspection of the model, e.g. via interpretability techniques [3, 16, 12], is capable of highlighting the improper decision structure. In this talk, we look at the question of explaining the predictions of deep neural networks, a successful machine learning approach that has been used increasingly in real-world applications. A challenge for getting these explanations is the complexity of the decision function, which makes it hard to apply simple explanation methods developed in the context of linear models, e.g. based on first-order Taylor expansions. In particular, DNN decision functions are highly nonlinear and multiscale, with a gradient that is highly varying or ‘shattered’ [4]. Also, local searches in the input space easily result in ‘adversarial examples’ [13] where the prediction no longer corresponds to the observed pattern in the input. Layer-wise relevance propagation (LRP) [3] is a technique that was proposed to robustly explain the neural network decision in terms of input features. It was shown to work on numerous models in a wide range of applications [14, 5, 15]. LRP departs from the neural network’s function representation to consider instead its graph structure. Specifically, the LRP algorithm performs an iterative redistribution of the neural network output to the lower layers. Redistribution from each layer to the layer below is achieved by means of propagation rules that satisfy a conservation property analogous to Kirchoff’s conservation laws in electrical circuits. The LRP algorithm terminates once the input layer has been reached. The LRP algorithm can be motivated as decomposing a complex problem 6 Oberwolfach Report 4/2020 (analyzing a highly nonlinear function) into a collection of simpler subproblems (treating each neuron individually). Furthermore, it was shown that the LRP algorithm can be interpreted as a collection of Taylor expansions performed at each layer and neuron of the neural network [11]. Specifically, the ‘relevance’ received by a given neuron is approximately the product of the neuron activation and a locally constant term. In turn, the LRP redistribution step can be interpreted as (1) identifying the linear terms of a Taylor expansion of the relevance expressed as a function of activations in the lower layer, and (2) propagating to the lower layer accordingly. A connection can be made between different proposed LRP propagation rules and the choice of reference point at which the Taylor expansion is performed [11, 10]. This Taylor-based view on the LRP algorithm allows in particular to verify that the corresponding reference points are meaningful, for example, that they satisfy domain membership constraints. This interpretation of LRP as a collection of Taylor expansions is referred to as “deep Taylor decomposition” [11]. The LRP algorithm has been successfully applied to various data types and problems, ranging from computer vision and natural language processing tasks such as classification of concepts in images [3], age prediction [8] or categorization of text documents [2], over reinforcement learning tasks such as playing computer games [9], to various medical data analysis tasks, e.g., decoding of fMRI signals [14] or therapy outcome prediction [15]. In these diverse applications, LRP explanations provide additional insights into the decision strategies used by the model, which not only help to better understand the data, including its biases and artifacts [8, 9], but also help to analyze the learning processes and model’s decision strategies [9]. In the second part of the talk, two recent advances that broaden the usefulness of explanation methods are discussed. First, Spectral Relevance Analysis (SpRAy) [9], a dataset-wide analysis of individual explanations that summarizes the overall decision structure of the model into a finite and easily interpretable set of prototypical decision strategies. This analysis allows to systematically investigate complex models on large datasets. It has unveiled in commonly used datasets, artifacts, that tend to systematically induce flaws into the decision structure of ML models trained on them. For example, a website logo was found in some images of the class ‘truck’ of the ImageNet dataset, which the state-of-the-art VGG-16 neural network would then use for its predictions [1]. Another advance brings successful explanation techniques to non-neural network architectures such as kernel-based models. The approach that we term ‘neuralization’ [6] finds for these non-neural network architectures a functionally equivalent neural network so that state-of-the-art explanation techniques such as LRP can be applied. The approach was successfully applied to various unsupervised models, in particular, kernel one-class SVMs [7] and various k-means clustering models [6], thereby shedding light into what input features make a data point anomalous or member of a given cluster. Statistics meets Machine Learning 7 Although significant progress has been made to improve the transparency of ML models such as deep neural networks, numerous challenges still need to be addressed both on the methods and theory side. In particular, there is a need for standardized and unbiased evaluation benchmarks for assessing the quality and usefulness of an explanation. Furthermore, an important future work will be to adopt a more holistic view on the problem of explanation, that considers how to make best use of the user’s interpretation and feedback capabilities, and that also integrates the end goal of the explanation method, for example, achieving better and more informed decisions, or systematically improving and robustifying a machine learning model.","",""
0,"Yufang Jin, Bin Chen, B. Lampinen, P. Brown","Yield Determinants and Prediction for California’s Almond Orchards Based on Machine Learning Analytics",2020,"","","","",152,"2022-07-13 09:22:24","","10.5194/egusphere-egu2020-10687","","",,,,,0,0.00,0,4,2,"Agricultural productivity is subject to various stressors, including abiotic and biotic threats, many of which are exacerbated by a changing climate. The productivity of tree crops, such as almond orchards, is particularly complex. Moreover, the State of California has implemented legislatively mandated nitrogen (N) management strategies of all growers statewide to minimize nitrogen losses to the environment, and almond growers must now apply N in accordance with the estimated yield in early spring. To understand and mitigate these threats requires a collection of multi-layer large data sets, and advanced analytics is also critical to integrate these highly heterogeneous datasets to generate insights about the key constraints on the yields at tree and field scales. Here we used machine learning approaches to predict orchard-level yield and examine the determinants of almond yield variation in California’s almond orchards, based on a unique 10-year dataset of field measurements of light interception, remote sensing metrics, and almond yield, along with meteorological data. We found that overall the maximum almond yield was highly dependent on light interception, e.g., with each one percent increase in light interception resulting in an increase of 57.9 lbs/acre in the potential yield. Light interception was highest for mature sites with higher long term mean spring incoming solar radiation, and lowest for younger orchards and when March maximum temperature was lower than 19 o C. However, at any given level of light interception, actual yield often falls significantly below full yield potential, driven mostly by tree age, temperature profiles in June and winter, and summer maximum vapor pressure deficit (VPDmax). The full random forest model was found to explain 82% (±1%) of yield variation, with a RMSE of 480±9 lbs/acre. When excluding light interception from the predictors, overall orchard characteristics (such as age, location and tree density) and key meteorological variables could still explain 78% of yield variation. The model analysis also showed that warmer winter conditions often limited mature orchards from reaching maximum yield potential and higher summer VPDmax significantly limited the yield. Our findings through the machine learning approach improved our understanding of the complex interaction between climate, canopy light interception, and almond nut production. The demonstrated relatively robust predictability of almond yield, driven by “big data”, also provides quantitative information and guidance to make informed orchard nutrient management decisions, allocate resources, determine almond price targets, and improve market planning.","",""
0,"Carolina Natel de Moura, J. Seibert, Miriam Rita Moro Mine, Ricardo Carvalho de Almeida","Are Machine Learning methods robust enough for hydrological modeling under changing conditions?",2020,"","","","",153,"2022-07-13 09:22:24","","10.5194/egusphere-egu2020-690","","",,,,,0,0.00,0,4,2,"  <p>The advancement of big data and increased computational power have contributed to an increased use of Machine Learning (ML) approaches in hydrological modelling. These approaches are powerful tools for modeling non-linear systems. However, the applicability of ML in non-stationary conditions needs to be studied further. As climate change will change hydrological patterns, testing ML approaches for non-stationary conditions is essential. Here, we used the Differential Split-Sample Test (DSST) to test the climate transposability of ML approaches (e.g., calibrating in a wet period and validating in a dry one, and vice-versa).&#160; We applied five ML approaches using daily precipitation and temperature as input for the prediction of the daily discharge in six snow-dominated Swiss catchments. Lower and upper benchmarks were used to evaluate performances through a relative performance measure. The lower benchmark is the average of the bucket-type HBV model runs from 1000 random parameter sets. The upper benchmark is the automatically calibrated HBV model. In comparison with the stationary condition, the models performed slightly poorer in the non-stationary condition. The performance of simple ML approaches was poor for non-stationary conditions with an underestimation of peak flows, as well as a poor representation of the snow-melting period. On the other hand, a more complex ML approach (deep learning), the Long Short -Term Memory (LSTM), showed a good performance when compared with the lower and upper benchmarks. This might be explained by the fact that the so-called memory cell allowed to simulate the storage effects.&#160;</p> ","",""
0,"Kanan Mukhtarli","Machine learning for homogeneous grouping of pavements",2020,"","","","",154,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,1,2,"Abstract    Machine learning for homogeneous grouping of pavements.  Kanan Mukhtarli    Rapid pavement deterioration is a major problem in areas with harsh weather conditions or high traffic loading. Despite many studies focused on the pavement management systems, there is not, to the date, a robust method explaining how to process large amounts of pavement data to create homogeneous groups for rehabilitation-related decision making. This thesis employs machine learning to develop an approach capable of partitioning pavement data with a close response to casual factors like traffic and weather conditions and considering its performance through international roughness index and deflections. Two different methods: K-means and Self Organizing Maps (SOM) clustering techniques were tested to understand the correlation between daily factors and pavements deterioration. The goodness of clustering was tested using extrinsic and intrinsic evaluation methods. It was concluded from the results that SOM clustering provided better results as it relies on a soft clustering method where one point can represent two clusters at the same time. Moreover, it became obvious from the methodology that including the previous year’s data has very little to no effect on homogeneous groups. Techniques discussed and developed in this study can help road asset managers with decision making for the maintenance and rehabilitation of pavement. Moreover, future researchers can use the results of this study to further develop the idea of building decision support systems for pavement rehabilitation.","",""
0,"Dhilsath Fathima.M, S. Samuel, R. Haran","AN EFFICIENT MACHINE LEARNING MODEL FOR PREDICTION OF ACUTE MYOCARDIAL INFARCTION",2020,"","","","",155,"2022-07-13 09:22:24","","10.2174/2666255813666200325104317","","",,,,,0,0.00,0,3,2,"   This proposed work is used to develop an improved and robust machine learning model for predicting Myocardial Infarction (MI) could have substantial clinical impact.     This paper explains how to build machine learning based computer-aided analysis system for an early and accurate prediction of Myocardial Infarction (MI) which utilizes framingham heart study dataset for validation and evaluation. This proposed computer-aided analysis model will support medical professionals to predict myocardial infarction proficiently.     The proposed model utilize the mean imputation to remove the missing values from the data set, then applied principal component analysis to extract the optimal features from the data set to enhance the performance of the classifiers. After PCA, the reduced features are partitioned into training dataset and testing dataset where 70% of the training dataset are given as an input to the four well-liked classifiers as support vector machine, k-nearest neighbor, logistic regression and decision tree to train the classifiers and 30% of test dataset is used to evaluate an output of machine learning model using performance metrics as confusion matrix, classifier accuracy, precision, sensitivity, F1-score, AUC-ROC curve.    Output of the classifiers are evaluated using performance measures and we observed that logistic regression provides high accuracy than K-NN, SVM, decision tree classifiers and PCA performs sound as a good feature extraction method to enhance the performance of proposed model. From these analyses, we conclude that logistic regression having good mean accuracy level and standard deviation accuracy compared with the other three algorithms. AUC-ROC curve of the proposed classifiers is analyzed from the output figure.4, figure.5 that logistic regression exhibits good AUC-ROC score, i.e. around 70% compared to k-NN and decision tree algorithm.     From the result analysis, we infer that this proposed machine learning model will act as an optimal decision making system to predict the acute myocardial infarction at an early stage than an existing machine learning based prediction models and it is capable to predict the presence of an acute myocardial Infarction with human using the heart disease risk factors, in order to decide when to start lifestyle modification and medical treatment to prevent the heart disease. ","",""
18,"M. Alhusseini, Firas Abuzaid, A. Rogers, J. Zaman, T. Baykaner, P. Clopton, Peter D. Bailis, M. Zaharia, Paul J. Wang, W. Rappel, S. Narayan","Machine Learning to Classify Intracardiac Electrical Patterns During Atrial Fibrillation",2020,"","","","",156,"2022-07-13 09:22:24","","10.1161/CIRCEP.119.008160","","",,,,,18,9.00,2,11,2,"Supplemental Digital Content is available in the text. Background: Advances in ablation for atrial fibrillation (AF) continue to be hindered by ambiguities in mapping, even between experts. We hypothesized that convolutional neural networks (CNN) may enable objective analysis of intracardiac activation in AF, which could be applied clinically if CNN classifications could also be explained. Methods: We performed panoramic recording of bi-atrial electrical signals in AF. We used the Hilbert-transform to produce 175 000 image grids in 35 patients, labeled for rotational activation by experts who showed consistency but with variability (kappa [κ]=0.79). In each patient, ablation terminated AF. A CNN was developed and trained on 100 000 AF image grids, validated on 25 000 grids, then tested on a separate 50 000 grids. Results: In the separate test cohort (50 000 grids), CNN reproducibly classified AF image grids into those with/without rotational sites with 95.0% accuracy (CI, 94.8%–95.2%). This accuracy exceeded that of support vector machines, traditional linear discriminant, and k-nearest neighbor statistical analyses. To probe the CNN, we applied gradient-weighted class activation mapping which revealed that the decision logic closely mimicked rules used by experts (C statistic 0.96). Conclusions: CNNs improved the classification of intracardiac AF maps compared with other analyses and agreed with expert evaluation. Novel explainability analyses revealed that the CNN operated using a decision logic similar to rules used by experts, even though these rules were not provided in training. We thus describe a scaleable platform for robust comparisons of complex AF data from multiple systems, which may provide immediate clinical utility to guide ablation. Registration: URL: https://www.clinicaltrials.gov; Unique identifier: NCT02997254.","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",157,"2022-07-13 09:22:24","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
166,"M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, S. Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Integrating machine learning and multiscale modeling—perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences",2019,"","","","",158,"2022-07-13 09:22:24","","10.1038/s41746-019-0193-y","","",,,,,166,55.33,17,11,3,"","",""
11,"M. Kovalev, L. Utkin","A robust algorithm for explaining unreliable machine learning survival models using the Kolmogorov-Smirnov bounds",2020,"","","","",159,"2022-07-13 09:22:24","","10.1016/j.neunet.2020.08.007","","",,,,,11,5.50,6,2,2,"","",""
10,"Saeid Tizpaz-Niari, Pavol Cern'y, A. Trivedi","Detecting and understanding real-world differential performance bugs in machine learning libraries",2020,"","","","",160,"2022-07-13 09:22:24","","10.1145/3395363.3404540","","",,,,,10,5.00,3,3,2,"Programming errors that degrade the performance of systems are widespread, yet there is very little tool support for finding and diagnosing these bugs. We present a method and a tool based on differential performance analysis---we find inputs for which the performance varies widely, despite having the same size. To ensure that the differences in the performance are robust (i.e. hold also for large inputs), we compare the performance of not only single inputs, but of classes of inputs, where each class has similar inputs parameterized by their size. Thus, each class is represented by a performance function from the input size to performance. Importantly, we also provide an explanation for why the performance differs in a form that can be readily used to fix a performance bug. The two main phases in our method are discovery with fuzzing and explanation with decision tree classifiers, each of which is supported by clustering. First, we propose an evolutionary fuzzing algorithm to generate inputs that characterize different performance functions. For this fuzzing task, the unique challenge is that we not only need the input class with the worst performance, but rather a set of classes exhibiting differential performance. We use clustering to merge similar input classes which significantly improves the efficiency of our fuzzer. Second, we explain the differential performance in terms of program inputs and internals (e.g., methods and conditions). We adapt discriminant learning approaches with clustering and decision trees to localize suspicious code regions. We applied our techniques on a set of micro-benchmarks and real-world machine learning libraries. On a set of micro-benchmarks, we show that our approach outperforms state-of-the-art fuzzers in finding inputs to characterize differential performance. On a set of case-studies, we discover and explain multiple performance bugs in popular machine learning frameworks, for instance in implementations of logistic regression in scikit-learn. Four of these bugs, reported first in this paper, have since been fixed by the developers.","",""
10,"J. Nicely, B. Duncan, T. Hanisco, G. Wolfe, R. Salawitch, M. Deushi, A. S. Haslerud, P. Jöckel, B. Josse, D. Kinnison, A. Klekociuk, M. Manyin, V. Marécal, O. Morgenstern, L. Murray, G. Myhre, L. Oman, G. Pitari, A. Pozzer, Ilaria Quaglia, L. Revell, E. Rozanov, A. Stenke, K. Stone, S. Strahan, S. Tilmes, H. Tost, D. Westervelt, G. Zeng","A machine learning examination of hydroxyl radical differences among model simulations for CCMI-1",2020,"","","","",161,"2022-07-13 09:22:24","","10.5194/ACP-20-1341-2020","","",,,,,10,5.00,1,29,2,"Abstract. The hydroxyl radical (OH) plays critical roles within the troposphere, such as determining the lifetime of methane ( CH4 ), yet is challenging to model due to its fast cycling and dependence on a multitude of sources and sinks. As a result, the reasons for variations in OH and the resulting methane lifetime ( τ CH 4 ), both between models and in time, are difficult to diagnose. We apply a neural network (NN) approach to address this issue within a group of models that participated in the Chemistry-Climate Model Initiative (CCMI). Analysis of the historical specified dynamics simulations performed for CCMI indicates that the primary drivers of τ CH 4  differences among 10 models are the flux of UV light to the troposphere (indicated by the photolysis frequency JO1D ), the mixing ratio of tropospheric ozone ( O3 ), the abundance of nitrogen oxides ( NO x ≡ NO + NO 2 ), and details of the various chemical mechanisms that drive OH. Water vapour, carbon monoxide (CO), the ratio of NO:NOx , and formaldehyde (HCHO) explain moderate differences in τ CH 4 , while isoprene, methane, the photolysis frequency of NO2 by visible light ( JNO2 ), overhead ozone column, and temperature account for little to no model variation in τ CH 4 . We also apply the NNs to analysis of temporal trends in OH from 1980 to 2015. All models that participated in the specified dynamics historical simulation for CCMI demonstrate a decline in τ CH 4  during the analysed timeframe. The significant contributors to this trend, in order of importance, are tropospheric O3 , JO1D , NOx , and  H2O , with CO also causing substantial interannual variability in OH burden. Finally, the identified trends in τ CH 4 are compared to calculated trends in the tropospheric mean OH concentration from previous work, based on analysis of observations. The comparison reveals a robust result for the effect of rising water vapour on OH and  τ CH 4 , imparting an increasing and decreasing trend of about 0.5 % decade −1 , respectively. The responses due to NOx , ozone column, and temperature are also in reasonably good agreement between the two studies.","",""
1,"Charles A. Ellis, M. Sendi, S. Plis, Robyn L. Miller, V. Calhoun","Algorithm-Agnostic Explainability for Unsupervised Clustering",2021,"","","","",162,"2022-07-13 09:22:24","","","","",,,,,1,1.00,0,5,1,"Supervised machine learning explainability has developed rapidly in recent years. However, clustering explainability has lagged behind. Here, we demonstrate the first adaptation of model-agnostic explainability methods to explain unsupervised clustering. We present two novel ""algorithm-agnostic"" explainability methods – global permutation percent change (G2PC) and local perturbation percent change (L2PC) – that identify feature importance globally to a clustering algorithm and locally to the clustering of individual samples. The methods are (1) easy to implement and (2) broadly applicable across clustering algorithms, which could make them highly impactful. We demonstrate the utility of the methods for explaining five popular clustering methods on low-dimensional synthetic datasets and on high-dimensional functional network connectivity data extracted from a resting-state functional magnetic resonance imaging dataset of 151 individuals with schizophrenia and 160 controls. Our results are consistent with existing literature while also shedding new light on how changes in brain connectivity may lead to schizophrenia symptoms. We further compare the explanations from our methods to an interpretable classifier and find them to be highly similar. Our proposed methods robustly explain multiple clustering algorithms and could facilitate new insights into many applications. We hope this study will greatly accelerate the development of the field of clustering explainability.","",""
66,"R. Cuocolo, Maria Brunella Cipullo, A. Stanzione, L. Ugga, V. Romeo, L. Radice, A. Brunetti, M. Imbriaco","Machine learning applications in prostate cancer magnetic resonance imaging",2019,"","","","",163,"2022-07-13 09:22:24","","10.1186/s41747-019-0109-2","","",,,,,66,22.00,8,8,3,"","",""
5,"P. P. Chaves, G. Zuquim, K. Ruokolainen, J. V. Doninck, R. Kalliola, Elvira Gómez Rivero, H. Tuomisto","Mapping Floristic Patterns of Trees in Peruvian Amazonia Using Remote Sensing and Machine Learning",2020,"","","","",164,"2022-07-13 09:22:24","","10.3390/rs12091523","","",,,,,5,2.50,1,7,2,"Recognition of the spatial variation in tree species composition is a necessary precondition for wise management and conservation of forests. In the Peruvian Amazonia, this goal is not yet achieved mostly because adequate species inventory data has been lacking. The recently started Peruvian national forest inventory (INFFS) is expected to change the situation. Here, we analyzed genus-level variation, summarized through non-metric multidimensional scaling (NMDS), in a set of 157 INFFS inventory plots in lowland to low mountain rain forests (<2000 m above sea level) using Landsat satellite imagery and climatic, edaphic, and elevation data as predictor variables. Genus-level floristic patterns have earlier been found to be indicative of species-level patterns. In correlation tests, the floristic variation of tree genera was most strongly related to Landsat variables and secondly to climatic variables. We used random forest regression, under varying criteria of feature selection and cross-validation, to predict the floristic composition on the basis of Landsat and environmental data. The best model explained >60% of the variation along NMDS axes 1 and 2 and 40% of the variation along NMDS axis 3. We used this model to predict the three NMDS dimensions at a 450-m resolution over all of the Peruvian Amazonia and classified the pixels into 10 floristic classes using k-means classification. An indicator analysis identified statistically significant indicator genera for 8 out of the 10 classes. The results are congruent with earlier studies, suggesting that the approach is robust and can be applied to other tropical regions, which is useful for reducing research gaps and for identifying suitable areas for conservation.","",""
2,"T. Khan, Kushsairy A., S. Nasim, M. Alam, Z. Shahid, M. Mazliham","Proficiency Assessment of Machine Learning Classifiers: An Implementation for the Prognosis of Breast Tumor and Heart Disease Classification",2020,"","","","",165,"2022-07-13 09:22:24","","10.14569/ijacsa.2020.0111170","","",,,,,2,1.00,0,6,2,"Breast cancer and heart disease can be acknowledged as very dangerous and common disease in many countries including Pakistan. In this paper classifiers comparative study has been performed for the tumor and heart disease classification. Around one lac women are diagnosed annually with this life-threatening disease having no family history of the disease. If it is not treated on time it may grow and spread to the other parts of human body. Mammograms are the X-rays of the breast which can be used for the screening of cancer tumor. Prior identification of breast cancer may increase the chance of survival up to 70 percent. Tumors which causes cancer can be categorized into two types: a) Benign and b) Malignant. Benign tumor can be explained as the tumor which are not attached to neighbor tissues or spread in the other parts of the body. In Malignant tumor, other parts may be affected by it as it can grow and spread in the other parts of the body. To classify the tumor as Malignant or Benign is very complex as the similarities of cancer tumor and tumor caused by the skin inflammation are almost same. The early identification of Malignant is mandatory to protect the patient life. Diversified medical methods based on deep learning and machine learning have been developed to treat the patients as cancer is a very serious and crucial issue in this era. In this research paper machine learning algorithms like logistic regression, K-NN and tree have been applied to the breast cancer data set which has been taken from UCI Machine learning repository. Comparative study of classifiers has been performed to determine the better classifier for the robust prediction of breast tumors. Simulated results proved that using Logistic regression, ninety-one percent accuracy was achieved. The research showed that logistic regression can be applied for the accurate and precise early prediction of breast cancer. Cardiovascular disease is very common throughout the world. It has been noticed that health in cardiac patients that there are so many factors which causes heart disease or heart attack. The factors leading to the heart failure includes varying blood pressure, high sugar, cardiac pain, and heart rate, high cholesterol level (LDL), artery blockage and irregular ECG signals. Many researchers proved that stress in patients can also be the reason for the heart disease. Higher numbers of cardiac surgeries like angioplasty and heart by-pass are performed on annual basis. Actually, people don’t care about their lifestyle and diet and fully ignore the symbols. It can be early predicted and cured if proper testing and medication for heart is done. Sometimes there is a false pain which has the same feeling like angina pain depicting cardiovascular disease. To reduce the false alarm and robustly classify the heart disease, several machine learning approaches have been adopted. In proposed research for the accurate classification of heart disease comparison has been performed among support vector machine (SVM), K-nearest neighbors K-NN and linear discriminant analysis. Simulated results demonstrated that Support vector machine was found to be a better classifier having an accuracy of 80.4%. Keywords—Breast cancer; benign; malignant; logistic regression; cardiovascular disease; heart disease diagnosis; support vector machine; classifiers; k-nearest neighbors","",""
10,"Giuseppina Andresini, Feargus Pendlebury, Fabio Pierazzi, Corrado Loglisci, A. Appice, L. Cavallaro","INSOMNIA: Towards Concept-Drift Robustness in Network Intrusion Detection",2021,"","","","",166,"2022-07-13 09:22:24","","10.1145/3474369.3486864","","",,,,,10,10.00,2,6,1,"Despite decades of research in network traffic analysis and incredible advances in artificial intelligence, network intrusion detection systems based on machine learning (ML) have yet to prove their worth. One core obstacle is the existence of concept drift, an issue for all adversary-facing security systems. Additionally, specific challenges set intrusion detection apart from other ML-based security tasks, such as malware detection. In this work, we offer a new perspective on these challenges. We propose INSOMNIA, a semi-supervised intrusion detector which continuously updates the underlying ML model as network traffic characteristics are affected by concept drift. We use active learning to reduce latency in the model updates, label estimation to reduce labeling overhead, and apply explainable AI to better interpret how the model reacts to the shifting distribution. To evaluate INSOMNIA, we extend TESSERACT - a framework originally proposed for performing sound time-aware evaluations of ML-based malware detectors - to the network intrusion domain. Our evaluation shows that accounting for drifting scenarios is vital for effective intrusion detection systems.","",""
1,"N. Howard, Naima Chouikhi, Ahsan Adeel, Katelyn Dial, Adam Howard, A. Hussain","BrainOS: A Novel Artificial Brain-Alike Automatic Machine Learning Framework",2020,"","","","",167,"2022-07-13 09:22:24","","10.3389/fncom.2020.00016","","",,,,,1,0.50,0,6,2,"Human intelligence is constituted by a multitude of cognitive functions activated either directly or indirectly by external stimuli of various kinds. Computational approaches to the cognitive sciences and to neuroscience are partly premised on the idea that computational simulations of such cognitive functions and brain operations suspected to correspond to them can help to further uncover knowledge about those functions and operations, specifically, how they might work together. These approaches are also partly premised on the idea that empirical neuroscience research, whether following on from such a simulation (as indeed simulation and empirical research are complementary) or otherwise, could help us build better artificially intelligent systems. This is based on the assumption that principles by which the brain seemingly operate, to the extent that it can be understood as computational, should at least be tested as principles for the operation of artificial systems. This paper explores some of the principles of the brain that seem to be responsible for its autonomous, problem-adaptive nature. The brain operating system (BrainOS) explicated here is an introduction to ongoing work aiming to create a robust, integrated model, combining the connectionist paradigm underlying neural networks and the symbolic paradigm underlying much else of AI. BrainOS is an automatic approach that selects the most appropriate model based on the (a) input at hand, (b) prior experience (a history of results of prior problem solving attempts), and (c) world knowledge (represented in the symbolic way and used as a means to explain its approach). It is able to accept diverse and mixed input data types, process histories and objectives, extract knowledge and infer a situational context. BrainOS is designed to be efficient through its ability to not only choose the most suitable learning model but to effectively calibrate it based on the task at hand.","",""
34,"A. Sargolzaei, C. Crane, Alireza Abbaspour, S. Noei","A Machine Learning Approach for Fault Detection in Vehicular Cyber-Physical Systems",2016,"","","","",168,"2022-07-13 09:22:24","","10.1109/ICMLA.2016.0112","","",,,,,34,5.67,9,4,6,"A network of vehicular cyber-physical systems (VCPSs) can use wireless communications to interact with each other and the surrounding environment to improve transportation safety, mobility, and sustainability. However, cloud-oriented architectures are vulnerable to cyber attacks, which may endanger passenger and pedestrian safety and privacy, and cause severe property damage. For instance, a hacker can use message falsification attack to affect functionality of a particular application in a platoon of VCPSs. In this paper, a neural network-based fault detection technique is applied to detect and track fault data injection attacks on the cooperative adaptive cruise control layer of a platoon of connected vehicles in real time. A decision support system was developed to reduce the probability and severity of any consequent accident. A case study with its design specifications is demonstrated in detail. The simulation results show that the proposed method can improve system reliability, robustness, and safety.","",""
26,"Leila Etaati","Azure Machine Learning Studio",2019,"","","","",169,"2022-07-13 09:22:24","","10.1007/978-1-4842-3658-1_12","","",,,,,26,8.67,26,1,3,"","",""
21,"F. Grando, L. Granville, L. Lamb","Machine Learning in Network Centrality Measures",2018,"","","","",170,"2022-07-13 09:22:24","","10.1145/3237192","","",,,,,21,5.25,7,3,4,"Complex networks are ubiquitous to several computer science domains. Centrality measures are an important analysis mechanism to uncover vital elements of complex networks. However, these metrics have high computational costs and requirements that hinder their applications in large real-world networks. In this tutorial, we explain how the use of neural network learning algorithms can render the application of the metrics in complex networks of arbitrary size. Moreover, the tutorial describes how to identify the best configuration for neural network training and learning such for tasks, besides presenting an easy way to generate and acquire training data. We do so by means of a general methodology, using complex network models adaptable to any application. We show that a regression model generated by the neural network successfully approximates the metric values and therefore is a robust, effective alternative in real-world applications. The methodology and proposed machine-learning model use only a fraction of time with respect to other approximation algorithms, which is crucial in complex network applications.","",""
19,"Finale Doshi-Velez, R. Perlis","Evaluating Machine Learning Articles.",2019,"","","","",171,"2022-07-13 09:22:24","","10.1001/jama.2019.17304","","",,,,,19,6.33,10,2,3,"In this issue of JAMA, Liu and colleagues1 provide a users’ guide to reading clinical machine learning articles. Beyond a synopsis of selected concepts in modern machine learning, the authors elaborate step-by-step guidance for physicians seeking to evaluate this evidence with a critical eye. In an era when readers are bombarded with artificial intelligence in everyday life, from credit card fraud warnings and smartphones that anticipate their needs to life-like videos of people who do not actually exist, the sanity check provided by this article is most welcome. Reassuringly, many of the key elements in reading a machine learning article draw directly on concerns familiar to JAMA readers of users’ guides, and they have changed little in the 3 decades since Nierenberg described an approach to diagnostic testing.2 Common sense and standard statistical principles still apply when it comes to these more complex models. For example, choices about the inputs and outputs of a model, such as what and how patient features are measured and what is to be predicted, are essential in determining the practical value of an algorithm. Are the inputs measured reliably, and do they draw on readily available technology (facts from electronic health records; routine laboratory studies) or emerging technology (new positron emission tomography tracers, single-cell transcriptomics) that may make implementation and dissemination more challenging? Are the outputs clinically actionable? Generations of medical students recall the adage, ”don’t order a test unless it will change management”; certainly this applies to artificial intelligence as well. Tools to detect retinopathy3 or identify tuberculosis or malaria using smartphone images4 may be particularly beneficial in low-resource settings. Choices about cohort selection and data preparation (most notably, handling of missing data) will have important consequences for subsequent analyses; machine learning does not solve problems of bias introduced by missing data. Were models trained only with canonical or clear-cut examples? In clinical practice, data are noisy and not always complete; failure to consider these circumstances may yield models that perform beautifully on cleaned data sets for the purposes of publication but miserably in practice. Radiologists do not struggle to identify cancer in pristine chest radiologic images accompanied by detailed history but poorer-quality images with superimposed pneumonia and little clinical context pose a more realistic challenge. In addition, proper validation is essential, and replication is a crucial piece of the validation process. As Liu et al1 note, in machine learning studies, it remains critical to know whether the model has been validated across new clinical settings. Many of the most important challenges in machine learning are related to various forms of overfitting in which a model explains a training data set perfectly but fails to generalize. Showing a model performs well in another patient cohort in the same health system is good; showing that it performs well in an entirely different setting is far better. Such replication is the beginning, not the end, of a long process for validation and dissemination— one that draws on decades of lessons from work on developing diagnostics. While much of the guidance in the article by Liu et al1 will be familiar, a few key considerations bear particular emphasis in the context of machine learning applications to medicine. For example, the authors note that more complex machine learning systems are often pretrained on one data set (eg, public images on the internet of places and things) and then refit to another task (eg, retinal images). The kinds of bias introduced by such procedures is not well understood. For example, it seems likely that interpreting ophthalmologic images requires additional features beyond those needed to distinguish major categories, such as with images in general. In this case, the trained model may be systematically failing on those elements specific to opthalmology—that is, requiring features not present in general internet images while performing well overall. Such failures may be particularly concerning if they result in the model performing more poorly for specific types of patients. The preceding example raises a larger point: because machine learning methods are myriad, in a state of rapid development, and less familiar to most clinical readers, authors of articles using machine learning must make their underlying assumptions, model properties, optimization strategies, and limitations explicit in the article. The example of transfer learning reusing a previously trained model is just one way in which properties are implicitly introduced; another is how regularization, a form of smoothing, is performed—smoothing different parameters can have different effects on the final behavior and performance the model. The predictions made by an algorithm may or may not be robust to even tiny changes to the input (eg, how differences in an image that are nondiscernible to the human eye may cause an algorithm to change its predictions).5 Because these failure modes may not be expected, it is essential that the authors of articles reporting on machine learning point out what the failure modes of their algorithmic approaches might be. An acknowledgment of limitations should make readers more rather than less Related article page 1806 Opinion","",""
13,"Cecilia Sullca, Carlos Malo de Molina, Carlos E. Rodríguez, T. Fernández","Diseases Detection in Blueberry Leaves using Computer Vision and Machine Learning Techniques",2019,"","","","",172,"2022-07-13 09:22:24","","10.18178/ijmlc.2019.9.5.854","","",,,,,13,4.33,3,4,3,"This paper explains how image processing techniques and Machine Learning algorithms were used, such as Support Vector Machine (SVM), Artificial Neural Networks (ANN) and Random Forest; and Deep Learning ś technique Convolutional Neural Network (CNN) was also used so we can determine which is the best algorithm for the construction of a recognition model that detects whether a blueberry plant is being affected by a disease or pest, or if it is healthy. The images were processed with different filters such as medianBlur and gaussianblur for the elimination of noise, the add Weighted filter was used for the enhancement of details in the images. The images were compiled by the authors of this work, since there was no accessible database of this specific kind of fruit, for which we visited Valle and Pampa farm so we could take pictures of different blueberry leaves, labeled in three different tags: diseased, plagued and healthy. The extraction of characteristics was done with algorithms such as HOG (Histogram of oriented gradients) and LBP (Local binary patterns), both normalized and not normalized. The results of the model showed an 84% accuracy index using Deep Learning, this model was able to classify whether the blueberry plant was being affected or not. The result of this work provides a solution to a constant problem in the agricultural sector that affects the production of blueberries, because pests as well as diseases are constant problems in this sector.","",""
531,"Scott M. Lundberg, B. Nair, M. Vavilala, M. Horibe, M. Eisses, Trevor L. Adams, D. Liston, Daniel King-Wai Low, Shu-Fang Newman, Jerry H. Kim, Su-In Lee","Explainable machine-learning predictions for the prevention of hypoxaemia during surgery",2018,"","","","",173,"2022-07-13 09:22:24","","10.1038/s41551-018-0304-0","","",,,,,531,132.75,53,11,4,"","",""
2,"Pedro Salas-Rojo, Juan Gabriel Rodríguez","Inheritances and wealth inequality: a machine learning approach",2022,"","","","",174,"2022-07-13 09:22:24","","10.1007/s10888-022-09528-8","","",,,,,2,2.00,1,2,1,"","",""
1,"C. Betancourt, T. Stomberg, Ann-Kathrin Edrich, Ankit Patnala, M. Schultz, R. Roscher, J. Kowalski, S. Stadtler","Global, high-resolution mapping of tropospheric ozone – explainable machine learning and impact of uncertainties",2022,"","","","",175,"2022-07-13 09:22:24","","10.5194/gmd-2022-2","","",,,,,1,1.00,0,8,1,"Abstract. Tropospheric ozone is a toxic greenhouse gas with a highly variable spatial distribution which is challenging to map on a global scale. Here we present a data-driven ozone mapping workflow generating a transparent and reliable product. We map the global distribution of tropospheric ozone from sparse, irregularly placed measurement stations to a high-resolution regular grid using machine learning methods. The produced map contains the average tropospheric ozone concentration of the years 2010–2014 with a resolution of 0.1° × 0.1°. The machine learning model is trained on AQ-Bench, a precompiled benchmark dataset consisting of multi-year ground-based ozone measurements combined with an abundance of high-resolution geospatial data. Going beyond standard mapping methods, this work focuses on two key aspects to increase the integrity of the produced map. Using explainable machine learning methods we ensure that the trained machine learning model is consistent with commonly accepted knowledge about tropospheric ozone. To assess the impact of data and model uncertainties on our ozone map, we show that the machine learning model is robust against typical fluctuations in ozone values and geospatial data. By inspecting the feature space, we ensure that the model is only applied in regions where it is reliable. We provide a rationale for the tools we use to conduct a thorough global analysis. The methods presented here can thus be easily transferred to other mapping applications to ensure the transparency and reliability of the maps produced. ","",""
1,"Daniel Gutierrez-Rojas, I. Christou, Daniel Dantas, A. Narayanan, P. Nardelli, Yongheng Yang","Performance evaluation of machine learning for fault selection in power transmission lines",2022,"","","","",176,"2022-07-13 09:22:24","","10.1007/s10115-022-01657-w","","",,,,,1,1.00,0,6,1,"","",""
0,"J. Escanciano, Joel Robert Terschuur","Debiased Semiparametric U-Statistics: Machine Learning Inference on Inequality of Opportunity",2022,"","","","",177,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,2,1,"We construct locally robust/orthogonal moments in a semiparametric U-statistics setting. These are quadratic moments in the distribution of the data with a zero derivative with respect to first steps at their limit, which reduces model selection bias with machine learning first steps. We use orthogonal moments to propose new debiased estimators and valid inferences in a variety of applications ranging from Inequality of Opportunity (IOp) to distributional treatment effects. U-statistics with machine learning first steps arise naturally in these and many other applications. A leading example in IOp is the Gini coefficient of machine learning fitted values. We introduce a novel U-moment representation of the First Step Influence Function (U-FSIF) to take into account the effect of the first step estimation on an identifying quadratic moment. Adding the U-FISF to the identifying quadratic moment gives rise to an orthogonal quadratic moment. Our leading and motivational application is to measuring IOp, for which we propose a simple debiased estimator, and the first available inferential methods. We give general and simple regularity conditions for asymptotic theory, and demonstrate an improved finite sample performance in simulations for our debiased measures of IOp. In an empirical application, we find that standard measures of IOp are about six times more sensitive to first step machine learners than our debiased measures, and that between 42% and 46% of income inequality in Spain is explained by circumstances out of the control of the individual. JEL Classification: C13; C14; C21; D31; D63 ∗Research founded by Ministerio de Ciencia e Innovación, grant ECO2017-86675-P, MCI/AEI/FEDER/UE, grant PGC 2018-096732-B-100, and Comunidad de Madrid, grants EPUC3M11 (VPRICIT) and H2019/HUM589. 1 ar X iv :2 20 6. 05 23 5v 1 [ ec on .E M ] 1 0 Ju n 20 22","",""
0,"Jing-Jing Liu, Jian-chao Liu","Permeability Predictions for Tight Sandstone Reservoir Using Explainable Machine Learning and Particle Swarm Optimization",2022,"","","","",178,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,2,1,"High-precision permeability prediction is of great significance to tight sandstone reservoirs. However, while considerable progress has recently been made in the machine learning based prediction of reservoir permeability, the generalization of this approach is limited by weak interpretability. Hence, an interpretable XGBoost model is proposed herein based on particle swarm optimization to predict the permeability of tight sandstone reservoirs with higher accuracy and robust interpretability. The porosity and permeability of 202 core plugs and 6 logging curves (namely, the gamma-ray (GR) curve, the acoustic curve (AC), the spontaneous potential (SP) curve, the caliper (CAL) curve, the deep lateral resistivity (RILD) curve, and eight lateral resistivity (RFOC) curve) are extracted along with three derived variables (i.e., the shale content, the AC slope, and the GR slope) as data sets. Based on the data preprocessing, global and local interpretations are performed according to the Shapley additive explanations (SHAP) analysis, and the redundant features in the data set are screened to identify the porosity, AC, CAL, and GR slope as the four most important features. The particle swarm optimization algorithm is then used to optimize the hyperparameters of the XGBoost model. The prediction results of the PSO-XGBoost model indicate a superior performance compared with that of the benchmark XGBoost model. In addition, the reliable application of the interpretable PSO-XGBoost model in the prediction of tight sandstone reservoir permeability is examined by comparing the results with those of two traditional mathematical regression models, five machine learning models, and three deep learning models. Thus, the interpretable PSO-XGBoost model is shown to have more advantages in permeability prediction along with the lowest root mean square error, thereby confirming the effectiveness and practicability of this method.","",""
0,"Jun Su, Pengcheng Zhou","Machine Learning-based Modeling and Prediction of the Intrinsic Relationship between Human Emotion and Music",2022,"","","","",179,"2022-07-13 09:22:24","","10.1145/3534966","","",,,,,0,0.00,0,2,1,"Human emotion is one of the most complex psychophysiological phenomena and has been reported to be affected significantly by music listening. It is supposed that there is an intrinsic relationship between human emotion and music, which can be modeled and predicted quantitatively in a supervised manner. Here, a heuristic clustering analysis is carried out on large-scale free music archive to derive a genre-diverse music library, to which the emotional response of participants is measured using a standard protocol, consequently resulting in a systematic emotion-to-music profile. Eight machine learning methods are employed to statistically correlate the basic sound features of music audio tracks in the library with the measured emotional response of tested people to the music tracks in a training set and to blindly predict the emotional response from sound features in a test set. This study found that nonlinear methods are more robust and predictable but considerably time-consuming than linear approaches. The neural networks have strong internal fittability but are associated with a significant overfitting issue. The support vector machine and Gaussian process exhibit both high internal stability and satisfactory external predictability in all used methods; they are considered as promising tools to model, predict and explain the intrinsic relationship between human emotion and music. The psychological basis and perceptional implication underlying the built machine learning models are also discussed to find out the key music factors that affect human emotion.","",""
0,"Mengdi Huai","Fostering Trustworthiness in Machine Learning via Robust and Automated Model Interpretation",2022,"","","","",180,"2022-07-13 09:22:24","","","","",,,,,0,0.00,0,1,1,"Machine learning models have been widely applied in real world to build intelligent systems (e.g., selfdriving cars, intelligent recommendation systems, and clinical decision support systems). However, traditional machine learning models mainly focus on optimizing accuracy and efficiency, and they fail to consider how to foster trustworthiness in their design. In practice, machine learning models are suffering a crisis of trust when they are applied in real-world applications due to the lack of transparency behind their behaviors. The concern about the “black box” nature of machine learning models makes decision makers reluctant to trust the predicted results, especially when these models are used for making critical decisions (e.g., medical disease diagnosis). In this talk, I will introduce my research efforts towards the goal of making machine learning trustworthy. Specifically, I will discuss how to foster trustworthiness in machine learning via robust and automated model interpretation. I will first describe my recent research on the security vulnerability of model interpretation methods for deep reinforcement learning (DRL) and introduce two malicious attack frameworks that can significantly alter the interpretation results while incurring minor damage to the performance of the original DRL model. Then, I will present an automated and robust model interpretation framework, which can not only automatically generate the concept-based explanations for the predicted results but also provide certified robustness guarantees for the generated explanations. Bio: Mengdi Huai is a Ph.D. candidate in the Department of Computer Science at the University of Virginia. Her research interests lie in the areas of data mining and machine learning, with a current focus on developing novel techniques to build trustworthy learning systems that are explainable, robust, private, and fair. Mengdi is also interested in designing effective data mining and machine learning algorithms to deal with complex data with both strong empirical performance and theoretical guarantees. Her research work has been published in various top-tier venues, such as KDD, AAAI, IJCAI, NeurIPS, and TKDD. Mengdi received multiple prestigious awards from the University of Virginia for her excellence in research, including the Sture G. Olsson Fellowship in Engineering and the John A. Stankovic Research Award. Her recent work on malicious attacks against model interpretation won the Best Paper Runner-up of KDD2020. Mengdi was selected as one of the Rising Stars in EECS at MIT. She was also selected as one of the Rising Stars in Data Science at UChicago.","",""
0,"Mitchell Solomon, Micah Billouin, Anthony O. Smith, Jad Zeineddine, Kevin Chow, A. Rangarajan, A. Peter","Regional infrasonic and seismic event classification with machine learning",2022,"","","","",181,"2022-07-13 09:22:24","","10.1117/12.2623291","","",,,,,0,0.00,0,7,1,"The present work details how convolutional and recurrent deep learning networks can be used to classify infrasonic and seismic events of regional-field rocket demolitions, rocket motor burns, quarry blasts, and earthquakes. To accelerate machine learning adoption within the geophysical sciences, we illustrate the full machine learning pipeline: data acquisition and cleaning, preprocessing, model construction and training, and model understanding using feature-space analysis. Multiple deep learning architectures are evaluated to provide practical lessons learned and insights. The LSTM-RNN suffers from degraded learning on long geophysical signals, while a CNN is more robust to variance in data quality and length. Geophysical time-series signals should be learned with the instrument response deconvolved to avoid gross resampling or decimation of the signal. Frequency-domain feature inputs like spectrograms exhibit improved classification performance, and mapping event-based attributes to the learned feature space of deep networks can provide explainable physical context. All network configurations are validated on geophysical data collected in the Utah region, with experimental results including ablation studies examining different input types, preprocessing strategies, and hyperparameter settings.","",""
0,"Supatcha Lertampaiporn, A. Hongsthong, Warin Wattanapornprom, C. Thammarongtham","Ensemble-AHTPpred: A Robust Ensemble Machine Learning Model Integrated With a New Composite Feature for Identifying Antihypertensive Peptides",2022,"","","","",182,"2022-07-13 09:22:24","","10.3389/fgene.2022.883766","","",,,,,0,0.00,0,4,1,"Hypertension or elevated blood pressure is a serious medical condition that significantly increases the risks of cardiovascular disease, heart disease, diabetes, stroke, kidney disease, and other health problems, that affect people worldwide. Thus, hypertension is one of the major global causes of premature death. Regarding the prevention and treatment of hypertension with no or few side effects, antihypertensive peptides (AHTPs) obtained from natural sources might be useful as nutraceuticals. Therefore, the search for alternative/novel AHTPs in food or natural sources has received much attention, as AHTPs may be functional agents for human health. AHTPs have been observed in diverse organisms, although many of them remain underinvestigated. The identification of peptides with antihypertensive activity in the laboratory is time- and resource-consuming. Alternatively, computational methods based on robust machine learning can identify or screen potential AHTP candidates prior to experimental verification. In this paper, we propose Ensemble-AHTPpred, an ensemble machine learning algorithm composed of a random forest (RF), a support vector machine (SVM), and extreme gradient boosting (XGB), with the aim of integrating diverse heterogeneous algorithms to enhance the robustness of the final predictive model. The selected feature set includes various computed features, such as various physicochemical properties, amino acid compositions (AACs), transitions, n-grams, and secondary structure-related information; these features are able to learn more information in terms of analyzing or explaining the characteristics of the predicted peptide. In addition, the tool is integrated with a newly proposed composite feature (generated based on a logistic regression function) that combines various feature aspects to enable improved AHTP characterization. Our tool, Ensemble-AHTPpred, achieved an overall accuracy above 90% on independent test data. Additionally, the approach was applied to novel experimentally validated AHTPs, obtained from recent studies, which did not overlap with the training and test datasets, and the tool could precisely predict these AHTPs.","",""
0,"Paul W. Eastwick, Samantha Joel, D. Molden, E. Finkel, Kathleen L. Carswell","Predicting Romantic Interest during Early Relationship Development: A Preregistered Investigation using Machine Learning",2022,"","","","",183,"2022-07-13 09:22:24","","10.31219/osf.io/sh7ja","","",,,,,0,0.00,0,5,1,"There are massive literatures on initial romantic attraction and established, “official” relationships. But there is a gap in our knowledge about early relationship development: the interstitial stretch of time in which people experience rising and falling romantic interest for partners who have the potential to—but often do not—become sexual or dating partners. In the current study, 208 single participants reported on 1,065 potential romantic partners across 7,179 data points over seven months. In stage 1 of the analyses, we used machine learning (specifically, Random Forests) to extract estimates of the extent to which different classes of predictors (e.g., individual differences vs. target-specific constructs) accounted for participants’ romantic interest in these potential partners (12% vs. 36%, respectively). Also, the machine learning analyses offered little support for perceiver × target moderation accounts of compatibility: the meta-theoretical perspective that some types of perceivers are likely to experience greater romantic interest for some types of targets. In stage 2, we used traditional multilevel-modeling approaches to depict growth-curve analyses for each predictor retained by the machine learning models; robust (positive) main effects emerged for many variables, including sociosexuality, gender, the potential partner’s positive attributes (e.g., attractive, exciting), attachment features (e.g., proximity seeking, separation distress), and perceived interest. We also directly tested (and found no support for) ideal partner preference-matching effects on romantic interest, which is one popular perceiver × target moderation account of compatibility. We close by discussing the need for new models and perspectives to explain how people assess romantic compatibility.","",""
0,"Eike Petersen, Yannik Potdevin, Esfandiar Mohammadi, Stephan Zidowitz, Sabrina Breyer, Dirk Nowotka, Sandra Henn, Ludwig Pechmann, M. Leucker, P. Rostalski, Christian Herzog","Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Challenges and Solutions",2021,"","","","",184,"2022-07-13 09:22:24","","10.1109/ACCESS.2022.3178382","","",,,,,0,0.00,0,11,1,"Machine learning is expected to fuel significant improvements in medical care. To ensure that fundamental principles such as beneficence, respect for human autonomy, prevention of harm, justice, privacy, and transparency are respected, medical machine learning systems must be developed responsibly. Many high-level declarations of ethical principles have been put forth for this purpose, but there is a severe lack of technical guidelines explicating the practical consequences for medical machine learning. Similarly, there is currently considerable uncertainty regarding the exact regulatory requirements placed upon medical machine learning systems. This survey provides an overview of the technical and procedural challenges involved in creating medical machine learning systems responsibly and in conformity with existing regulations, as well as possible solutions to address these challenges. First, a brief review of existing regulations affecting medical machine learning is provided, showing that properties such as safety, robustness, reliability, privacy, security, transparency, explainability, and nondiscrimination are all demanded already by existing law and regulations—albeit, in many cases, to an uncertain degree. Next, the key technical obstacles to achieving these desirable properties are discussed, as well as important techniques to overcome these obstacles in the medical context. We notice that distribution shift, spurious correlations, model underspecification, uncertainty quantification, and data scarcity represent severe challenges in the medical context. Promising solution approaches include the use of large and representative datasets and federated learning as a means to that end, the careful exploitation of domain knowledge, the use of inherently transparent models, comprehensive out-of-distribution model testing and verification, as well as algorithmic impact assessments.","",""
0,"Emre Gures, Ibraheem Shayea, M. Ergen, M. Azmi, Ayman A. El-Saleh","Machine Learning-Based Load Balancing Algorithms in Future Heterogeneous Networks: A Survey",2022,"","","","",185,"2022-07-13 09:22:24","","10.1109/ACCESS.2022.3161511","","",,,,,0,0.00,0,5,1,"The massive growth of mobile users and the essential need for high communication service quality necessitate the deployment of ultra-dense heterogeneous networks (HetNets) consisting of macro, micro, pico and femto cells. Each cell type provides different cell coverage and distinct system capacity in HetNets. This leads to the pressing need to balance loads between cells, especially with the random distribution of users in numerous mobility directions. This paper provides a survey on the intelligent load balancing models that have been developed in HetNets, including those based on the machine learning (ML) technology. The survey provides a guideline and a roadmap for developing cost-effective, flexible and intelligent load balancing models in future HetNets. An overview of the generic problem of load balancing is also presented. The concept of load balancing is first introduced, and its purpose, functionality and evaluation criteria are then explained. Besides, a basic load balancing model and its operational procedure are described. A comprehensive literature review is then conducted, including techniques and solutions of addressing the load balancing problem. The key performance indicators (KPIs) used in the evaluation of load balancing models in HetNets are presented, along with the concurrent optimisation of coverage (CCO) and mobility robustness optimisation (MRO) relationship of load balancing. A comprehensive literature review of ML-driven load balancing solutions is specifically accomplished to show the historical development of load balancing models. Finally, the current challenges in implementing these models are explained as well as the future operational aspects of load balancing.","",""
0,"L. M. Vowels, M. Vowels, K. Mark","Identifying the strongest self-report predictors of sexual satisfaction using machine learning",2022,"","","","",186,"2022-07-13 09:22:24","","10.1177/02654075211047004","","",,,,,0,0.00,0,3,1,"Sexual satisfaction has been robustly associated with relationship and individual well-being. Previous studies have found several individual (e.g., gender, self-esteem, and attachment) and relational (e.g., relationship satisfaction, relationship length, and sexual desire) factors that predict sexual satisfaction. The aim of the present study was to identify which variables are the strongest, and the least strong, predictors of sexual satisfaction using modern machine learning. Previous research has relied primarily on traditional statistical models which are limited in their ability to estimate a large number of predictors, non-linear associations, and complex interactions. Through a machine learning algorithm, random forest (a potentially more flexible extension of decision trees), we predicted sexual satisfaction across two samples (total N = 1846; includes 754 individuals forming 377 couples). We also used a game theoretic interpretation technique, Shapley values, which allowed us to estimate the size and direction of the effect of each predictor variable on the model outcome. Findings showed that sexual satisfaction is highly predictable (48–62% of variance explained) with relationship variables (relationship satisfaction, importance of sex in relationship, romantic love, and dyadic desire) explaining the most variance in sexual satisfaction. The study highlighted important factors to focus on in future research and interventions.","",""
0,"Desalegn Aweke, Assefa Senbato Genale, B. Sundaram, Amit Pandey, Vijaykumar Janga, P. Karthika","Machine Learning based Network Security in Healthcare System",2022,"","","","",187,"2022-07-13 09:22:24","","10.1109/ICSCDS53736.2022.9760977","","",,,,,0,0.00,0,6,1,"The world is filled with exciting technologies and ideas; scientists build machines to avoid human intervention in completing work. It is highly challenging to complete the task without the Machine Learning (ML) Technology intervention. With the technological development, certain processes or consultations are performed with the aid of doctors available around the world. In this scenario, it could be noticed that health care is one of the world's expected domains that require the most incredible attention in data security while performing data transfer. Nodes in the network are considered based on the weakest link to overcome the cyber attacker's issues. Besides building the software for data storage, a better mechanism has to be incorporated to provide security to the stored data. This process is a delicate task for every network engineer. This paper will explain such concepts related to health prediction and health care by building the most robust network security systems. Finally, the discussion would cross over human-looping systems, which act as one of the common problems that are affected mentally for a person. According to the results, the suggested model achieved the accuracy of 98.89%, that is 4.76% greater than the previous model.","",""
9,"T. Botari, Rafael Izbicki, A. Carvalho","Local Interpretation Methods to Machine Learning Using the Domain of the Feature Space",2019,"","","","",188,"2022-07-13 09:22:24","","10.1007/978-3-030-43823-4_21","","",,,,,9,3.00,3,3,3,"","",""
24,"M. Sheykhmousa, N. Kerle, M. Kuffer, S. Ghaffarian","Post-Disaster Recovery Assessment with Machine Learning-Derived Land Cover and Land Use Information",2019,"","","","",189,"2022-07-13 09:22:24","","10.3390/rs11101174","","",,,,,24,8.00,6,4,3,"Post-disaster recovery (PDR) is a complex, long-lasting, resource intensive, and poorly understood process. PDR goes beyond physical reconstruction (physical recovery) and includes relevant processes such as economic and social (functional recovery) processes. Knowing the size and location of the places that positively or negatively recovered is important to effectively support policymakers to help readjust planning and resource allocation to rebuild better. Disasters and the subsequent recovery are mainly expressed through unique land cover and land use changes (LCLUCs). Although LCLUCs have been widely studied in remote sensing, their value for recovery assessment has not yet been explored, which is the focus of this paper. An RS-based methodology was created for PDR assessment based on multi-temporal, very high-resolution satellite images. Different trajectories of change were analyzed and evaluated, i.e., transition patterns (TPs) that signal positive or negative recovery. Experimental analysis was carried out on three WorldView-2 images acquired over Tacloban city, Philippines, which was heavily affected by Typhoon Haiyan in 2013. Support vector machine, a robust machine learning algorithm, was employed with texture features extracted from the grey level co-occurrence matrix and local binary patterns. Although classification results for the images before and four years after the typhoon show high accuracy, substantial uncertainties mark the results for the immediate post-event image. All land cover (LC) and land use (LU) classified maps were stacked, and only changes related to TPs were extracted. The final products are LC and LU recovery maps that quantify the PDR process at the pixel level. It was found that physical and functional recovery can be mainly explained through the LCLUC information. In addition, LC and LU-based recovery maps support a general and a detailed recovery understanding, respectively. It is therefore suggested to use the LC and LU-based recovery maps to monitor and support the short and the long-term recovery, respectively.","",""
2,"Chirag Agarwal, Bo Dong, D. Schonfeld, A. Hoogs","An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks",2018,"","","","",190,"2022-07-13 09:22:24","","","","",,,,,2,0.50,1,4,4,"Deep Neural Networks(DNN) have excessively advanced the field of computer vision by achieving state of the art performance in various vision tasks. These results are not limited to the field of vision but can also be seen in speech recognition and machine translation tasks. Recently, DNNs are found to poorly fail when tested with samples that are crafted by making imperceptible changes to the original input images. This causes a gap between the validation and adversarial performance of a DNN. An effective and generalizable robustness metric for evaluating the performance of DNN on these adversarial inputs is still missing from the literature. In this paper, we propose Noise Sensitivity Score (NSS), a metric that quantifies the performance of a DNN on a specific input under different forms of fix-directional attacks. An insightful mathematical explanation is provided for deeply understanding the proposed metric. By leveraging the NSS, we also proposed a skewness based dataset robustness metric for evaluating a DNN's adversarial performance on a given dataset. Extensive experiments using widely used state of the art architectures along with popular classification datasets, such as MNIST, CIFAR-10, CIFAR-100, and ImageNet, are used to validate the effectiveness and generalization of our proposed metrics. Instead of simply measuring a DNN's adversarial robustness in the input domain, as previous works, the proposed NSS is built on top of insightful mathematical understanding of the adversarial attack and gives a more explicit explanation of the robustness.","",""
8,"K. Szielasko, B. Wolter, R. Tschuncky, Sargon Youssef","Micromagnetic materials characterization using machine learning",2019,"","","","",191,"2022-07-13 09:22:24","","10.1515/teme-2019-0099","","",,,,,8,2.67,2,4,3,"Abstract Micromagnetic materials characterization is a nondestructive means of predicting mechanical properties and stress of steel and iron products. The method is based on the circumstance that both mechanical and magnetic behaviour relate to microstructure over similar interaction mechanisms, which leads to characteristic correlations between mechanical and magnetic properties of ferromagnetic materials. The prediction of mechanical properties or stress from micromagnetic parameters represents an inverse problem commonly addressed by regression and classification approaches. Challenges for the industrial application of micromagnetic methods lie in the development of robust sensors, definition of significant features, and implementation of powerful machine learning algorithms for a reliable quantitative target value prediction by processing of the micromagnetic features. This contribution briefly explains the background of micromagnetics, describes the typical challenges experienced in practice and provides insight into latest progress in the application of machine learning to micromagnetic data.","",""
7,"Ashkan Ebadi, Yvan Gauthier, Stéphane Tremblay, Patrick Paul","How can Automated Machine Learning Help Business Data Science Teams?",2019,"","","","",192,"2022-07-13 09:22:24","","10.1109/ICMLA.2019.00196","","",,,,,7,2.33,2,4,3,"Artificial intelligence and machine learning have attracted the attention of many commercial and non-profit organizations aiming to leverage advanced analytics, in order to provide a better service to their customers, increase their revenues through creating new or improving their existing internal processes, and better exploit their data by discovering complex hidden patterns. Such advanced solutions require data scientists with rare (and generally expensive) skill sets. Moreover, such solutions are often perceived as complex black boxes to decision-makers. Automated machine learning tools aim to reduce the expertise gap between the technical teams and stakeholders involved in business data science projects, by reducing the amount of time and specialized skills required to generate predictive models. We systematically benchmarked five automated machine learning tools against seven supervised learning problems of a business nature. Our results suggest that such tools, in fully automated mode, must be used cautiously, only where predictive models support low-impact decisions and do not need to be explainable, and only by data scientists capable to ensure that all phases of the data mining process have been performed adequately.","",""
3,"Thomas J. Rademaker, Emmanuel Bengio, P. François","Attack and Defense in Cellular Decision-Making: Lessons from Machine Learning",2018,"","","","",193,"2022-07-13 09:22:24","","10.1103/PhysRevX.9.031012","","",,,,,3,0.75,1,3,4,"Machine learning algorithms can be fooled by small well-designed adversarial perturbations. This is reminiscent of cellular decision-making where ligands (called antagonists) prevent correct signalling, like in early immune recognition. We draw a formal analogy between neural networks used in machine learning and models of cellular decision-making (adaptive proofreading). We apply attacks from machine learning to simple decision-making models, and show explicitly the correspondence to antagonism by weakly bound ligands. Such antagonism is absent in more nonlinear models, which inspired us to implement a biomimetic defence in neural networks filtering out adversarial perturbations. We then apply a gradient-descent approach from machine learning to different cellular decision-making models, and we reveal the existence of two regimes characterized by the presence or absence of a critical point for the gradient. This critical point causes the strongest antagonists to lie close to the decision boundary. This is validated in the loss landscapes of robust neural networks and cellular decision-making models, and observed experimentally for immune cells. For both regimes, we explain how associated defence mechanisms shape the geometry of the loss landscape, and why different adversarial attacks are effective in different regimes. Our work connects evolved cellular decision-making to machine learning, and motivates the design of a general theory of adversarial perturbations, both for in vivo and in silico systems.","",""
2,"J. Prešern, M. S. Smodiš Škerl","Parameters influencing queen body mass and their importance as determined by machine learning in honey bees (Apis mellifera carnica)",2019,"","","","",194,"2022-07-13 09:22:24","","10.1007/s13592-019-00683-y","","",,,,,2,0.67,1,2,3,"","",""
6,"K. Meenakshi, G. Maragatham","A Review on Security Attacks and Protective Strategies of Machine Learning",2019,"","","","",195,"2022-07-13 09:22:24","","10.1007/978-3-030-32150-5_109","","",,,,,6,2.00,3,2,3,"","",""
4,"F. Wick, U. Kerzel, M. Feindt","Cyclic Boosting - An Explainable Supervised Machine Learning Algorithm",2019,"","","","",196,"2022-07-13 09:22:24","","10.1109/ICMLA.2019.00067","","",,,,,4,1.33,1,3,3,"Supervised machine learning algorithms have seen spectacular advances and surpassed human level performance in a wide range of specific applications. However, using complex ensemble or deep learning algorithms typically results in black box models, where the path leading to individual predictions cannot be followed in detail. In order to address this issue, we propose the novel ""Cyclic Boosting"" machine learning algorithm, which allows to efficiently perform accurate regression and classification tasks while at the same time allowing a detailed understanding of how each individual prediction was made.","",""
3,"N. Radziwill","Machine Learning with R, Third Edition (Book Review)",2019,"","","","",197,"2022-07-13 09:22:24","","10.1080/10686967.2019.1648086","","",,,,,3,1.00,3,1,3,"This book is highly recommended for anyone with previous programing experience who seeks a solid, grounded introduction to basic machine learning using the R statistical software. With nearly 100 additional pages added since the first edition in 2013, this update to Brett Lantz’s excellent text is well worth the purchase, even for those who already have an earlier copy on their shelf. Clear writing, robust explanations, and compelling examples appear throughout, and most chapters explain the math underlying the methods in as simple and easy a manner as possible. I liked the first edition so much, I used it as the primary textbook for my applied machine learning class for undergraduate juniors and seniors in science and engineering. Chapter 1 provides an overview of the main concepts associated with developing and using ML models for decision making. It includes discussions of traditional topics like overfitting and emerging issues like bias and artificial intelligence (AI) ethics. The chapter structure follows the same pattern as previous editions, so knn, Naive Bayes, decision trees, four neural networks and SVMs, association rules, k-means, and performance are all covered. Chapter 12 on specialized machine learning topics is significantly updated from previous editions and now covers tidyverse, domain-specific data, and brief examinations of performance optimization techniques like parallelization, MapReduce, Hadoop, and Spark. In most chapters, there are fully reproducible examples clearly broken down into steps. Within those steps, subtasks (for example, transformation, data preparation, model specification) are also clearly specified, making it clear how to structure different types of problems. This book is excellent for beginners and others who want to use R to learn how to skillfully address ML problems using their own data.","",""
5,"V. Staartjes, M. Stienen","Data Mining in Spine Surgery: Leveraging Electronic Health Records for Machine Learning and Clinical Research",2019,"","","","",198,"2022-07-13 09:22:24","","10.14245/ns.1938434.217","","",,,,,5,1.67,3,2,3,"Advances in natural language processing (NLP) and unsupervised learning have recently enabled the long-expected synergy between true “big data“ and analytics based on machine learning (ML). The ability to reliably generate structured data from unstructured electronic health records (EHRs) such as free text reports, document scans, or unlabelled medical imaging has on the one hand allowed development of algorithms based on until recently unseen amounts of data, spanning entire hospital or even national populations, and not just databases compiled by human experts. On the other hand, the capacity to primarily generate structured reports from unstructured raw EHRs has proven valuable for hospital analytics, epidemiological studies, and systematic reviews. In their narrative review, Schwartz et al.1 report on the utilization of EHRs in spine surgery through ML techniques. The authors are to be commended for their detailed description of data types commonly found in EHRs, learning concepts to generate structured data (such as NLP and machine vision), applications of ML for prognosis and prediction, and finally the challenges inherent to using unstructured data from EHRs in medical practice and research. As the authors show, there is no question that ML is already starting to affect surgical practice relevantly in many aspects. Especially the advent of open source algorithms provided by today’s tech giants have largely democratized the development of ML models, and as the authors summarize, this has led to an explosion of publications reporting such algorithms. Still, it is important to preserve the methodological quality of papers utilizing ML techniques, which is often not the case. For example, the authors touch on the issue of ensuring generalizability through robust training structures (i.e., some form of resampling) and external validation, before models are rolled out into clinical practice. We especially value that the authors discuss the problem of uninterpretable “black box” models.2 Currently, many groups are applying complex ML algorithms to relatively small patient samples and for relatively simple tasks. While this might lead to slight benefits in model performance, these complex models (such as deep neural networks for nonimaging applications) are often typical “black box” models with a total loss of the ability to explain what factors lead the algorithm to make a certain decision. Explicability is – unfortunately – often traded in for a small and likely irrelevant increase in model accuracy. Especially in Neurospine 2019;16(4):654-656. https://doi.org/10.14245/ns.1938434.217 Neurospine","",""
3,"K. Stapor, I. Roterman-Konieczna, Piotr Fabian","Machine Learning Methods for the Protein Fold Recognition Problem",2018,"","","","",199,"2022-07-13 09:22:24","","10.1007/978-3-319-94030-4_5","","",,,,,3,0.75,1,3,4,"","",""
4,"T. Schmid","Deconstructing the Final Frontier of Artificial Intelligence: Five Theses for a Constructivist Machine Learning",2019,"","","","",200,"2022-07-13 09:22:24","","","","",,,,,4,1.33,4,1,3,"Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subirón 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world – and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (Méhaut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as “hypothetical constructs” (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n","",""
