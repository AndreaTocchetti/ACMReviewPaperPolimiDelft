Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
1,"Phuong T. Nguyen, Davide Di Ruscio, Juri Di Rocco, Claudio Di Sipio, M. Di Penta","Adversarial Machine Learning: On the Resilience of Third-party Library Recommender Systems",2021,"","","","",1,"2022-07-13 09:40:06","","10.1145/3463274.3463809","","",,,,,1,1.00,0,5,1,"In recent years, we have witnessed a dramatic increase in the application of Machine Learning algorithms in several domains, including the development of recommender systems for software engineering (RSSE). While researchers focused on the underpinning ML techniques to improve recommendation accuracy, little attention has been paid to make such systems robust and resilient to malicious data. By manipulating the algorithms’ training set, i.e., large open-source software (OSS) repositories, it would be possible to make recommender systems vulnerable to adversarial attacks. This paper presents an initial investigation of adversarial machine learning and its possible implications on RSSE. As a proof-of-concept, we show the extent to which the presence of manipulated data can have a negative impact on the outcomes of two state-of-the-art recommender systems which suggest third-party libraries to developers. Our work aims at raising awareness of adversarial techniques and their effects on the Software Engineering community. We also propose equipping recommender systems with the capability to learn to dodge adversarial activities.","",""
2,"T. Ermolieva, Y. Ermoliev, M. Obersteiner, E. Rovenskaya","Chapter 4 Two-Stage Nonsmooth Stochastic Optimization and Iterative Stochastic Quasigradient Procedure for Robust Estimation, Machine Learning and Decision Making",2021,"","","","",2,"2022-07-13 09:40:06","","10.1007/978-3-030-70370-7_4","","",,,,,2,2.00,1,4,1,"","",""
0,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, M. Teimoori, F. Kriebel, Jeff Zhang, Kang Liu, Semeen Rehman, T. Theocharides, A. Artusi, S. Garg, M. Shafique","Robust Computing for Machine Learning-Based Systems",2020,"","","","",3,"2022-07-13 09:40:06","","10.1007/978-3-030-52017-5_20","","",,,,,0,0.00,0,12,2,"","",""
0,"Lauren J. Wong, E. Altland, Joshua Detwiler, Paolo Fermin, Julia Mahon Kuzin, Nathan Moeliono, A. S. Abdalla, William C. Headley, Alan J. Michaels","Resilience Improvements for Space-Based Radio Frequency Machine Learning",2020,"","","","",4,"2022-07-13 09:40:06","","10.1109/ISNCC49221.2020.9297212","","",,,,,0,0.00,0,9,2,"Recent work has quantified the degradations that occur in convolutional neural nets (CNN) deployed in harsh environments like space-based image or radio frequency (RF) processing applications. Such degradations yield a robust correlation and causality between single-event upset (SEU) induced errors in memory weights of on-orbit CNN implementations. However, minimal considerations have been given to how the resilience of CNNs can be improved algorithmically as opposed to via enhanced hardware. This paper focuses on RF-processing CNNs and performs an in-depth analysis of applying software-based error detection and correction mechanisms, which may subsequently be combined with protections of radiation-hardened processor platforms. These techniques are more accessible for low cost smallsat platforms than ruggedized hardware. Additionally, methods for minimizing the memory and computational complexity of the resulting resilience techniques are identified. Combined with periodic scrubbing, the resulting techniques are shown to improve expected lifetimes of CNN-based RF-processing algorithms by several orders of magnitude.","",""
1,"You-Cheng Lai, Chun-Yen Yao, Shao-Hong Yang, Ying-Wei Wu, Tsung-Te Liu","A Robust Area-Efficient Physically Unclonable Function With High Machine Learning Attack Resilience in 28-nm CMOS",2022,"","","","",5,"2022-07-13 09:40:06","","10.1109/tcsi.2021.3098018","","",,,,,1,1.00,0,5,1,"Strong physically unclonable function (PUF) offers a promising solution to low-cost hardware identification and authentication for Internet of Things (IoT) applications. The continuous advancement of machine learning (ML) technology makes the PUF resilience to ML attacks a major design priority. This paper presents a robust and area-efficient strong PUF design with high ML attack resilience. The proposed PUF architecture based on inverter amplifiers operating in the subthreshold region achieves both low energy consumption and high supply and temperature scalability. The proposed nonlinearity topology effectively enhances PUF resilience to various ML attacks with low implementation area and cost. The proposed strong PUF design was designed and implemented using a 28-nm CMOS process. The measurement results show that the proposed PUF design achieves a nearly ideal ML attack resilience of 49.96 % with a small area of 239,857 F<sup>2</sup>, and demonstrates a stable operation across a wide range of supply voltage from 0.5–1.4 V and temperature from −40–100 °C. This represents <inline-formula> <tex-math notation=""LaTeX"">$3\times $ </tex-math></inline-formula> improvement in area efficiency, <inline-formula> <tex-math notation=""LaTeX"">$2.25\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""LaTeX"">$1.08\times $ </tex-math></inline-formula> improvement in operating voltage and temperature range, respectively, compared to the state-of-the-art results.","",""
2,"J. Sarkar, Cory Peterson","Operational Workload Impact on Robust Solid-State Storage Analyzed with Interpretable Machine Learning",2019,"","","","",6,"2022-07-13 09:40:06","","10.1109/IRPS.2019.8720510","","",,,,,2,0.67,1,2,3,"Solid-state storage technology is finding increasing adoption in enterprise and data center environments due to their high reliability and reducing cost. With high performance solid-state storage devices (SSDs) internally designed as distributed resilient systems, their operational behavior under materially different workloads is described in this research. Application of interpretable machine learning on internal parametric data of SSDs enables insights on workloads' interaction with the resilient system design. After prior research demonstrated significantly different accelerated workload stress, the analysis on resilience of the SSDs under random vs. pseudo-sequential workloads emphasize the efficacy and importance of their distributed resilience schemes. As such, these results provide causational insights on the mechanism of differential stress of the workloads impacting the resilience design principles. Moreover, the results elucidate guidelines strongly relevant from design robustness perspective for research on novel SSD architectures such as the proposed Open Channel SSD, towards deployment in hyperscale and virtualization environments.","",""
0,"A. Dvir, Yehonatan Zion, Jonathan Muehlstein, Ofir Pele, Chen Hajaj, Ran Dubin","Robust Machine Learning for Encrypted Traffic Classification",2016,"","","","",7,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,6,6,"Desktops and laptops can be maliciously exploited to violate privacy. In this paper, we consider the daily battle between the passive attacker who is targeting a specific user against a user that may be adversarial opponent. In this scenario, while the attacker tries to choose the best vector attack by surreptitiously monitoring the victims encrypted network traffic in order to identify users parameters such as the Operating System (OS), browser and apps. The user may use tools such as a Virtual Private Network (VPN) or even change protocols parameters to protect his/her privacy. We provide a large dataset of more than 20,000 examples for this task. We run a comprehensive set of experiments, that achieves high (above 85) classification accuracy, robustness and resilience to changes of features as a function of different network conditions at test time. We also show the effect of a small training set on the accuracy.","",""
1,"T. Klemas, Steve Chan","Harnessing Machine Learning, Data Analytics, and Computer-Aided Testing for Cyber Security Applications Achieving Sustained Cyber Resilience for Typical Attack Surface Configurations and Environments",2018,"","","","",8,"2022-07-13 09:40:06","","","","",,,,,1,0.25,1,2,4,"While media reports frequently highlight the exciting aspects of the cyber security field, many cyber security tasks are quite tedious and repetitive. At the same time, however, strong pattern recognition, deductive reasoning, and inference skills are required, as well as a high degree of situational awareness. As a direct consequence, the field of cyber security is replete with potential opportunities to apply data analytics, machine learning, computer aided testing, and other advanced approaches to reduce the frustration of cyber security operators by easing key challenges. In fact, given a typical range of cyber attack surfaces, leveraging these machine-enhanced analysis and decision approaches in conjunction with a robust defense-in-depth posture is a crucial step towards achieving sustained, predictable performance across typical cyber security tasks and promotes cyber resilience. This paper will both outline details for a near-term research effort and explore a variety of key opportunities to exploit these approaches with the objective of raising awareness, providing initial guidance to aid potential adopters, and developing effective strategies to incorporate them into existing cyber security constructs. Keywordsartificial intelligence; expert systems, machine learning; supervised learning, unsupervised learning, pattern recognition, spectral methods, k-means, modularity, Lagrange multiplier, optimization, anomaly detection, data analytics, data science, networks, cyber security operator, cyber defensive tools, cyber resilience.","",""
1,"R. Guerraoui, Arsany Guirguis, Jérémy Plassmann, Anton Ragot, Sébastien Rouault","GARFIELD: System Support for Byzantine Machine Learning (Regular Paper)",2021,"","","","",9,"2022-07-13 09:40:06","","10.1109/DSN48987.2021.00021","","",,,,,1,1.00,0,5,1,"We present GARFIELD, a library to transparently make machine learning (ML) applications, initially built with popular (but fragile) frameworks, e.g., TensorFlow and PyTorch, Byzantine–resilient. GARFIELD relies on a novel object–oriented design, reducing the coding effort, and addressing the vulnerability of the shared–graph architecture followed by classical ML frameworks. GARFIELD encompasses various communication patterns and supports computations on CPUs and GPUs, allowing addressing the general question of the practical cost of Byzantine resilience in ML applications. We report on the usage of GARFIELD on three main ML architectures: (a) a single server with multiple workers, (b) several servers and workers, and (c) peer–to–peer settings. Using GARFIELD, we highlight interesting facts about the cost of Byzantine resilience. In particular, (a) Byzantine resilience, unlike crash resilience, induces an accuracy loss, (b) the throughput overhead comes more from communication than from robust aggregation, and (c) tolerating Byzantine servers costs more than tolerating Byzantine workers.","",""
2,"K. Pattabiraman, Guanpeng Li, Zitao Chen","Error Resilient Machine Learning for Safety-Critical Systems: Position Paper",2020,"","","","",10,"2022-07-13 09:40:06","","10.1109/IOLTS50870.2020.9159749","","",,,,,2,1.00,1,3,2,"Machine learning (ML) has increasingly been adopted in safety-critical systems such as autonomous vehicles (AVs) and industrial robotics. In these domains, reliability and safety are important considerations, and hence it is critical to ensure the resilience of ML systems to faults and errors. On the other hand, soft errors are becoming more frequent in commodity computer systems due to the effects of technology scaling and reduced supply voltages. Further, traditional solutions for masking hardware faults such as Triple-Modular Redundancy (TMR) are prohibitively expensive in terms of their energy and performance overheads. Therefore, there is a compelling need to ensure the resilience of ML applications to soft errors on commodity hardware platforms.We first experimentally assess the resilience of safety-critical ML applications to soft errors. We demonstrate through fault injection experiments that even a single bit flip due to a soft error can lead to misclassification in Deep Neural Network (DNN) applications deployed in AVs, leading to safety violations. However, not all the errors in an DNN will result in serve consequences such as safety violations, and hence it is sufficient to protect the DNN from the ones that do. Unfortunately, finding all possible errors that result in safety violations is a very compute intensive task. We propose BinFI, a fault injection approach that efficiently injects critical faults that are highly likely to result in safety violations, based on the unique properties of DNNs. Finally, we propose Ranger, an approach to protect DNNs from critical faults with minimal performance overheads and no accuracy loss. We will conclude by presenting some of our ongoing work, and the future challenges in this area.","",""
6,"Liwei Song, Vikash Sehwag, A. Bhagoji, Prateek Mittal","A Critical Evaluation of Open-World Machine Learning",2020,"","","","",11,"2022-07-13 09:40:06","","","","",,,,,6,3.00,2,4,2,"Open-world machine learning (ML) combines closed-world models trained on in-distribution data with out-of-distribution (OOD) detectors, which aim to detect and reject OOD inputs. Previous works on open-world ML systems usually fail to test their reliability under diverse, and possibly adversarial conditions. Therefore, in this paper, we seek to understand how resilient are state-of-the-art open-world ML systems to changes in system components? With our evaluation across 6 OOD detectors, we find that the choice of in-distribution data, model architecture and OOD data have a strong impact on OOD detection performance, inducing false positive rates in excess of $70\%$. We further show that OOD inputs with 22 unintentional corruptions or adversarial perturbations render open-world ML systems unusable with false positive rates of up to $100\%$. To increase the resilience of open-world ML, we combine robust classifiers with OOD detection techniques and uncover a new trade-off between OOD detection and robustness.","",""
0,"El-Mahdi El-Mhamdi, R. Guerraoui, Arsany Guirguis, Sébastien Rouault","Garfield: System Support for Byzantine Machine Learning",2020,"","","","",12,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,4,2,"Byzantine Machine Learning (ML) systems are nowadays vulnerable for they require trusted machines and/or a synchronous network. We present Garfield, a system that provably achieves Byzantine resilience in ML applications without assuming any trusted component nor any bound on communication or computation delays. Garfield leverages ML specificities to make progress despite consensus being impossible in such an asynchronous, Byzantine environment. Following the classical server/worker architecture, Garfield replicates the parameter server while relying on the statistical properties of stochastic gradient descent to keep the models on the correct servers close to each other. On the other hand, Garfield uses statistically-robust gradient aggregation rules (GARs) to achieve resilience against Byzantine workers. We integrate Garfield with two widely-used ML frameworks, TensorFlow and PyTorch, while achieving transparency: applications developed with either framework do not need to change their interfaces to be made Byzantine resilient. Our implementation supports full-stack computations on both CPUs and GPUs. We report on our evaluation of Garfield with different (a) baselines, (b) ML models (e.g., ResNet-50 and VGG), and (c) hardware infrastructures (CPUs and GPUs). Our evaluation highlights several interesting facts about the cost of Byzantine resilience. In particular, (a) Byzantine resilience, unlike crash resilience, induces an accuracy loss, and (b) the throughput overhead comes much more from communication (70%) than from aggregation.","",""
9,"M. S. Sidhu, D. Ronanki, S. Williamson","State of Charge Estimation of Lithium-Ion Batteries Using Hybrid Machine Learning Technique",2019,"","","","",13,"2022-07-13 09:40:06","","10.1109/IECON.2019.8927066","","",,,,,9,3.00,3,3,3,"The pivotal features of low self-discharge, high energy density and long calendar life lead the Lithium-ion (Li-ion) batteries as being a mainstream energy storage source in electric vehicles (EVs). A meticulous estimation of the state of charge (SOC) is indispensable for ensuring safe and reliable operations in battery powered EVs. However, SOC estimation of Li-ion battery with high accuracy have become a major challenge in the automotive industry. To fulfill reliable operation in EVs, researchers have proposed numerous SOC estimators through model based or machine learning techniques. This paper presents an improved SOC estimation of Li-ion battery using random forest (RF) regression, which is robust and effective for controlling dynamic systems. To ensure good resilience and accuracy, a Gaussian filter is adopted at the final stage to minimize the variations in the SOC estimation. The proposed SOC estimator is verified on the experimental data of the Li-ion battery under Federal test driving schedules and different operating temperatures. Results show that the proposed SOC estimator displays sufficient accuracy and outperforms the traditional artificial intelligence based approaches.","",""
1,"Yu Zhang","Post-Earthquake Performance Assessment and Decision-Making for Tall Buildings: Integrating Statistical Modeling, Machine Learning, Stochastic Simulation and Optimization",2019,"","","","",14,"2022-07-13 09:40:06","","","","",,,,,1,0.33,1,1,3,"Author(s): Zhang, Yu | Advisor(s): Burton, Henry V | Abstract: With the embrace of the seismic resilience concept as a measure of the ability of constructed facilities and communities to contain the effects of an earthquake and achieve a timely recovery, the critical role of tall buildings in supporting community functionality, has been brought to the forefront. This study presents a series of frameworks for performing post-earthquake assessment and optimal decision-making for tall buildings. A machine learning framework to assess structural safety is first proposed and applied to a low-rise frame building. A similar methodology is then adapted for tall buildings, while incorporating robust techniques to deal with the high-dimension feature space that arises because of the large number of structural components. Seismic risk assessment is then carried out for the tall building by comparing the time-dependent probability of exceeding various response demand limits over a pre-defined period considering both mainshock-only and mainshock-aftershock hazard. The risk-based consistency of the limit state acceptance criteria for engineering demand parameters is also examined. Finally, a methodology to support optimal decision-making following the mainshock is developed with the goal of minimizing expected financial losses and, at the same time, ensuring life safety. The proposed prediction model, risk assessment methodology and optimal decision-making strategy can provide critical insights into the seismic performance of mainshock-damaged tall buildings and inform the post-earthquake actions of occupants, structural engineers, insurance companies and policymakers.","",""
0,"Basel Halak, Mohd Syafiq Mispan","An Overview of Machine Learning Applications in Hardware Security",2022,"","","","",15,"2022-07-13 09:40:06","","10.1109/DTS55284.2022.9809857","","",,,,,0,0.00,0,2,1,"this study explores the uses of machine learning (ML) in the field of hardware security, in particular, two applications areas are considered, namely, hardware Trojan (HT) and IC counterfeits. These examples demonstrate how ML algorithms can be employed as a defense mechanism to detect forged or tampered-with circuits. Our analysis shows that the ML detection accuracy still has not reached 100%. The selection and size of the feature vectors greatly affect the performance of the learning models, however, increasing the number of features or their size can lead to large overheads. Therefore, a thorough analysis is required to only select the appropriate- ate several relevant features that significantly contribute to the accuracy of ML models. The study also highlighted the need for a more robust deployment of ML algorithms to enhance their resilience to adversarial attacks.","",""
7,"Yulei Wu","Robust Learning-Enabled Intelligence for the Internet of Things: A Survey From the Perspectives of Noisy Data and Adversarial Examples",2021,"","","","",16,"2022-07-13 09:40:06","","10.1109/JIOT.2020.3018691","","",,,,,7,7.00,7,1,1,"The Internet of Things (IoT) has been widely adopted in a range of verticals, e.g., automation, health, energy, and manufacturing. Many of the applications in these sectors, such as self-driving cars and remote surgery, are critical and high stakes applications, calling for advanced machine learning (ML) models for data analytics. Essentially, the training and testing data that are collected by massive IoT devices may contain noise (e.g., abnormal data, incorrect labels, and incomplete information) and adversarial examples. This requires high robustness of ML models to make reliable decisions for IoT applications. The research of robust ML has received tremendous attention from both academia and industry in recent years. This article will investigate the state of the art and representative works of robust ML models that can enable high resilience and reliability of IoT intelligence. Two aspects of robustness will be focused on, i.e., when the training data of ML models contain noises and adversarial examples, which may typically happen in many real-world IoT scenarios. In addition, the reliability of both neural networks and reinforcement learning framework will be investigated. Both of these two ML paradigms have been widely used in handling data in IoT scenarios. The potential research challenges and open issues will be discussed to provide future research directions.","",""
41,"P. Blanchard, El Mahdi El Mhamdi, R. Guerraoui, J. Stainer","Byzantine-Tolerant Machine Learning",2017,"","","","",17,"2022-07-13 09:40:06","","","","",,,,,41,8.20,10,4,5,"The growth of data, the need for scalability and the complexity of models used in modern machine learning calls for distributed implementations. Yet, as of today, distributed machine learning frameworks have largely ignored the possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study the robustness to Byzantine failures at the fundamental level of stochastic gradient descent (SGD), the heart of most machine learning algorithms. Assuming a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can SGD be, without limiting the dimension, nor the size of the parameter space.  We first show that no gradient descent update rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the update rule capturing the basic requirements to guarantee convergence despite $f$ Byzantine workers. We finally propose Krum, an update rule that satisfies the resilience property aforementioned. For a $d$-dimensional learning problem, the time complexity of Krum is $O(n^2 \cdot (d + \log n))$.","",""
2,"El-Mahdi El-Mhamdi, R. Guerraoui, Sébastien Rouault","Fast and Robust Distributed Learning in High Dimension",2020,"","","","",18,"2022-07-13 09:40:06","","10.1109/SRDS51746.2020.00015","","",,,,,2,1.00,1,3,2,"Could a gradient aggregation rule (GAR) for distributed machine learning be both robust and fast? This paper answers by the affirmative through MULTI-BULYAN. Given n workers, f of which are arbitrary malicious (Byzantine) and $m=n-f$ are not, we prove that MULTI-BULYAN can ensure a strong form of Byzantine resilience, as well as an $\frac{m}{n}$ slowdown, compared to averaging, the fastest (but non Byzantine resilient) rule for distributed machine learning. When $m\approx n$ (almost all workers are correct), MULTI-BULYAN reaches the speed of averaging. We also prove that MULTI-BULYAN’s cost in local computation is $O(d)$ (like averaging), an important feature for ML where d commonly reaches 109, while robust alternatives have at least quadratic cost in d. Our theoretical findings are complemented with an experimental evaluation which, in addition to supporting the linear O(d) complexity argument, conveys the fact that MULTI-BULYAN’s parallelisability further adds to its efficiency.","",""
0,"M. Hameed, Faidhalrahman Khaleel, Deiaaldeen Khaleel","Employing a robust data-driven model to assess the environmental damages caused by installing grouted columns",2021,"","","","",19,"2022-07-13 09:40:06","","10.1109/IEEECONF53624.2021.9668027","","",,,,,0,0.00,0,3,1,"The jet grouting process involves injecting large quantities of highly pressurized fluids into the soil, which may result in a substantial ground displacement and adverse effects on the environment around the excavation. Consequently, the ground displacement must be estimated accurately in the design phase. In this study, two machine learning models namely, extreme learning machine (ELM) and modified K-nearest neighbor (KNN) are used to estimate the ground displacements. The comparison results show that the ELM is superior to the KNN model in terms of estimation accuracy (coefficient of determination is 0.940). Moreover, the ELM model shows an enhancement by 11.43% higher accuracy in terms of reducing the mean absolute error compared to the KNN model. Overall, the results indicate that ELM has the ability to accurately assess the harmful damages caused by installing grouted columns.","",""
0,"Joe Hays, S. Ramamoorthy, Christian Tetzlaff","Editorial: Robust Artificial Intelligence for Neurorobotics",2021,"","","","",20,"2022-07-13 09:40:06","","10.3389/fnbot.2021.809903","","",,,,,0,0.00,0,3,1,"Neural computing is a powerful paradigm that has revolutionized machine learning. Building from early roots in the study of adaptive behavior and attempts to understand information processing in parallel and distributed neural architectures, modern neural networks have convincingly demonstrated successes in numerous areas—transforming the practice of computer vision, natural language processing, and even computational biology. Applications in robotics bring stringent constraints on size, weight and power constraints (SWaP), which challenge the developers of these technologies in new ways. Indeed, these requirements take us back to the roots of the field of neural computing, forcing us to ask how it could be that the human brain achieves with as little as 12 watts of power what seems to require entire server farms with state of the art computational and numerical methods. Likewise, even lowly insects demonstrate a degree of adaptivity and resilience that still defy easy explanation or computational replication. In this Research Topic, we have compiled the latest research addressing several aspects of these broadly defined challenge questions. As illustrated in Figure 1, the articles are organized into four prevailing themes: Sense, Think, Act, and Tools.","",""
0,"Tanmoy Sen, Haiying Shen, W. Saad, T. Doan","A Resilient and Robust Edge-Cloud Network System Supporting CPS",2021,"","","","",21,"2022-07-13 09:40:06","","10.1109/MASS52906.2021.00039","","",,,,,0,0.00,0,4,1,"Many outages of cloud computing services are caused by the natural disasters or common causes such as software failure, hardware failure, cyber attacks, and power outage. As a result, it is critical to develop resilient and robust cyber-physical system (CPS) to support continuity of service for smart and connected communities (S&CC). In spite of considerate research efforts on virtual machine (VM) migration for enhancing failure-resilience of datacenters, one issue still needs to be effectively addressed: how to determine the best destination for VM migration to avoid the influence from a given failure and enable edge nodes to connect to the VMs continuously. In this paper, we aim to handle these important issues to build a resilient and robust edge-cloud (or fog) network system supporting CPS for S&CC. We propose a machine learning based method for VM migration destination determination for the VMs in the predicted failure domains. We also propose a continuous edge-cloud connection method in wireless network component that enables edge nodes to continuously connect with the VMs through intermediate edge nodes when they cannot directly connect to their original VMs due to VM migration. Our experimental results show our proposed system reduces the number of job failures by 1.8 times and reduces the total job completion time by 48% for completed jobs compared to the case without our system.","",""
0,"Omri Armstrong, Ran Gilad-Bachrach","Robust Model Compression Using Deep Hypotheses",2021,"","","","",22,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,2,1,"Machine Learning models should ideally be compact and ro- bust. Compactness provides efﬁciency and comprehensibil-ity whereas robustness provides resilience. Both topics have been studied in recent years but in isolation. Here we present a robust model compression scheme which is independent of model types: it can compress ensembles, neural networks and other types of models into diverse types of small mod- els. The main building block is the notion of depth derived from robust statistics. Originally, depth was introduced as a measure of the centrality of a point in a sample such that the median is the deepest point. This concept was extended to classiﬁcation functions which makes it possible to deﬁne the depth of a hypothesis and the median hypothesis. Algo- rithms have been suggested to approximate the median but they have been limited to binary classiﬁcation. In this study, we present a new algorithm, the Multiclass Empirical Median Optimization (MEMO) algorithm that ﬁnds a deep hypothesis in multi-class tasks, and prove its correctness. This leads to our Compact Robust Estimated Median Belief Optimization (CREMBO) algorithm for robust model compres- sion. We demonstrate the success of this algorithm empirically by compressing neural networks and random forests into small decision trees, which are interpretable models, and show that they are more accurate and robust than other com- parable methods. In addition, our empirical study shows that our method outperforms Knowledge Distillation on DNN to DNN compression.","",""
0,"Stefano Calzavara, L. Cazzaro, C. Lucchese, Federico Marcuzzi, S. Orlando","Beyond Robustness: Resilience Verification of Tree-Based Classifiers",2021,"","","","",23,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,5,1,"In this paper we criticize the robustness measure traditionally employed to assess the performance of machine learning models deployed in adversarial settings. To mitigate the limitations of robustness, we introduce a new measure called resilience and we focus on its verification. In particular, we discuss how resilience can be verified by combining a traditional robustness verification technique with a data-independent stability analysis, which identifies a subset of the feature space where the model does not change its predictions despite adversarial manipulations. We then introduce a formally sound data-independent stability analysis for decision trees and decision tree ensembles, which we experimentally assess on public datasets and we leverage for resilience verification. Our results show that resilience verification is useful and feasible in practice, yielding a more reliable security assessment of both standard and robust decision tree models.","",""
9,"Yu Yuan, Jie Gao, Yue Zhang","Supervised learning for robust term extraction",2017,"","","","",24,"2022-07-13 09:40:06","","10.1109/IALP.2017.8300603","","",,,,,9,1.80,3,3,5,"We propose a machine learning method to automatically classify the extracted ngrams from a corpus into terms and non-terms. We use 10 common statistics in previous term extraction literature as features for training. The proposed method, applicable to term recognition in multiple domains and languages, can help 1) avoid the laborious work in the post-processing (e.g. subjective threshold setting); 2) handle the skewness and demonstrate noticeable resilience to domain-shift issue of training data. Experiments are carried out on 6 corpora of multiple domains and languages, including GENIA and ACLRD-TEC(1.0) corpus as training set and four TTC subcorpora of wind energy and mobile technology in both Chinese and English as test set. Promising results are found, which indicate that this approach is capable of identifying both single word terms and multiword terms with reasonably good precision and recall.","",""
2,"Şerban Vădineanu, M. Nasri","Robust and Accurate Period Inference using Regression-Based Techniques",2020,"","","","",25,"2022-07-13 09:40:06","","10.1109/RTSS49844.2020.00040","","",,,,,2,1.00,1,2,2,"With the growth in complexity of real-time embedded systems, there is an increasing need for tools and techniques to understand and compare the observed runtime behavior of a system with the expected one. Since many real-time applications require periodic interactions with the environment, one of the fundamental problems in guaranteeing their temporal correctness is to be able to infer the periodicity of certain events in the system. The practicability of a period inference tool, however, depends on both its accuracy and robustness (also its resilience) against noise in the output trace of the system, e.g., when the system trace is impacted by the presence of aperiodic tasks, release jitters, and runtime variations in the execution time of the tasks. This work (i) presents the first period inference framework that uses regression-based machine-learning (RBML) methods, and (ii) thoroughly investigates the accuracy and robustness of different families of RBML methods in the presence of uncertainties in the system parameters. We show, on both synthetically generated traces and traces from actual systems, that our solutions can reduce the error of period estimation by two to three orders of magnitudes w.r.t. state of the art.","",""
0,"Şerban Vădineanu, M. Nasri","Robust and accurate regression-based techniques for period inference in real-time systems",2022,"","","","",26,"2022-07-13 09:40:06","","10.1007/s11241-022-09385-8","","",,,,,0,0.00,0,2,1,"","",""
0,"Zhefu Wu, Agyemang Paul, Junzhuo Cao, Luping Fang","Directional Adversarial Training for Robust Ownership-Based Recommendation System",2022,"","","","",27,"2022-07-13 09:40:06","","10.1109/access.2022.3140352","","",,,,,0,0.00,0,4,1,"Machine learning algorithms are susceptible to cyberattacks, posing security problems in computer vision, speech recognition, and recommendation systems. So far, researchers have made great strides in adopting adversarial training as a defensive strategy. Single-step adversarial training methods have been proposed as viable solutions for improving model generality and resilience. However, there has been little study to address this issue in the context of ownership-based recommendations, which may fail to capture stable results. In this work, we adapt the single-step adversarial training for ownership recommendation systems. Our main technical contributions are as follows: (1) We propose Adversarial Consumption and Production Relationship (ACPR), a model that combines factorization machine and single-step adversarial training for ownership recommendations. It enables us to take advantage of modeling consumption-production interactions with a factorization machine instead of the conventional matrix factorization method for ownership recommendations. (2) We enrich the ACPR technique with directional adversarial training and call our technique Adversarial Consumption and Production Relationship-Aware Directional Adversarial Model (ACPR-ADAM). The idea behind our ACPR-ADAM is that instead of the worst perturbation direction, the perturbation direction in the embedding space is restricted to other examples in the current embedding space, allowing us to incorporate the collaborative signal into the training process. Lastly, through extensive evaluations on Reddit and Pinterest, we demonstrate that our proposed method outperforms state-of-the-art methods. Compared with CPR and ACPR on Reddit and Pinterest datasets, our proposed ACPR-ADAM achieves 93%, 88%, and 72%, 69% enhancement in terms of AUC and HR, respectively.","",""
12,"Jinlong Ji, Xuhui Chen, Qianlong Wang, Lixing Yu, Pan Li","Learning to Learn Gradient Aggregation by Gradient Descent",2019,"","","","",28,"2022-07-13 09:40:06","","10.24963/ijcai.2019/363","","",,,,,12,4.00,2,5,3,"In the big data era, distributed machine learning emerges as an important learning paradigm to mine large volumes of data by taking advantage of distributed computing resources. In this work, motivated by learning to learn, we propose a meta-learning approach to coordinate the learning process in the master-slave type of distributed systems. Specifically, we utilize a recurrent neural network (RNN) in the parameter server (the master) to learn to aggregate the gradients from the workers (the slaves). We design a coordinatewise preprocessing and postprocessing method to make the neural network based aggregator more robust. Besides, to address the fault tolerance, especially the Byzantine attack, in distributed machine learning systems, we propose an RNN aggregator with additional loss information (ARNN) to improve the system resilience. We conduct extensive experiments to demonstrate the effectiveness of the RNN aggregator, and also show that it can be easily generalized and achieve remarkable performance when transferred to other distributed systems. Moreover, under majoritarian Byzantine attacks, the ARNN aggregator outperforms the Krum, the state-of-art fault tolerance aggregation method, by 43.14%. In addition, our RNN aggregator enables the server to aggregate gradients from variant local models, which significantly improve the scalability of distributed learning.","",""
10,"Liran Lerman, Zdenek Martinasek, O. Markowitch","Robust profiled attacks: should the adversary trust the dataset?",2017,"","","","",29,"2022-07-13 09:40:06","","10.1049/iet-ifs.2015.0574","","",,,,,10,2.00,3,3,5,"Side-channel attacks provide tools to analyse the degree of resilience of a cryptographic device against adversaries measuring leakages (e.g. power traces) on the target device executing cryptographic algorithms. In 2002, Chari et al. introduced template attacks (TA) as the strongest parametric profiled attacks in an information theoretic sense. Few years later, Schindler et al. proposed stochastic attacks (representing other parametric profiled attacks) as improved attacks (with respect to TA) when the adversary has information on the data-dependent part of the leakage. Less than ten years later, the machine learning field provided non-parametric profiled attacks especially useful in high dimensionality contexts. In this study, the authors provide new contexts in which profiled attacks based on machine learning outperform conventional parametric profiled attacks: when the set of leakages contains errors or distortions. More precisely, the authors found that (i) profiled attacks based on machine learning remain effective in a wide range of scenarios, and (ii) TA are more sensitive to distortions and errors in the profiling and attacking sets.","",""
141,"Amira Abbas, David Sutter, Christa Zoufal, Aurélien Lucchi, A. Figalli, Stefan Woerner","The power of quantum neural networks",2020,"","","","",30,"2022-07-13 09:40:06","","10.1038/s43588-021-00084-1","","",,,,,141,70.50,24,6,2,"","",""
3,"Jonathan Muehlstein, Yehonatan Zion, Maor Bahumi, Itay Kirshenboim, Ran Dubin, A. Dvir, Ofir Pele","Analyzing HTTPS Traffic for a Robust Identification of Operating System, Browser and Application",2016,"","","","",31,"2022-07-13 09:40:06","","","","",,,,,3,0.50,0,7,6,"Desktops and laptops can be maliciously exploited to violate privacy. There are two main types of attack scenarios: active and passive. In this paper, we consider the passive scenario where the adversary does not interact actively with the device, but he is able to eavesdrop on the network traffic of the device from the network side. Most of the Internet traffic is encrypted and thus passive attacks are challenging. In this paper, we show that an external attacker can robustly identify the operating system, browser and application of HTTP encrypted traffic (HTTPS). We provide a large dataset of more than 20,000 examples for this task. We present a comprehensive evaluation of traffic features including new ones and machine learning algorithms. We run a comprehensive set of experiments, which shows that our classification accuracy is 96.06%. Due to the adaptive nature of the problem, we also investigate the robustness and resilience to changes of features due to different network conditions (e.g., VPN) at test time and the effect of small training set on the accuracy. We show that our proposed solution is robust to these changes.","",""
1,"P. Parrend, Fabio Guigou, J. Navarro, A. Deruyver, Pierre Collet","Artificial Immune Ecosystems: the role of expert-based learning in artificial cognition",2018,"","","","",32,"2022-07-13 09:40:06","","10.2991/jrnal.2018.4.4.10","","",,,,,1,0.25,0,5,4,"The rapid evolution of IT ecosystems significantly challenges the security models our infrastructures rely on. Beyond the old dichotomy between open and closed systems, it is now necessary to handle securely the interaction between heterogeneous devices building dynamic ecosystems. To this regard, bio-inspired approaches provide a rich set of conceptual tools, but they have failed to lay the basis for robust and efficient solutions. Our research effort intends to revisit the contribution of artificial immune system research to bring immune properties: security, resilience, distribution, memory, into IT infrastructures. Artificial immune ecosystems support a comprehensive model for anomaly detection and characterization, but their cognitive capacity are limited by the state of the art in machine learning and the rapid evolution of cybersecurity threats so far. We therefore propose to enrich the cognitive process with expert-based learning for reinforcement, classification and investigation. Application to system supervision using system logs and supervision time series confirms the relevance and performance of this model.","",""
4,"Thomas Cochrane, Peter Foster, Varun Chhabra, M. Lemercier, C. Salvi, Terry Lyons","SK-Tree: a systematic malware detection algorithm on streaming trees via the signature kernel",2021,"","","","",33,"2022-07-13 09:40:06","","10.1109/CSR51186.2021.9527933","","",,,,,4,4.00,1,6,1,"The development of machine learning algorithms in the cyber security domain has been impeded by the complex, hierarchical, sequential and multimodal nature of the data involved. In this paper we introduce the notion of a streaming tree as a generic data structure encompassing a large portion of real-world cyber security data. Starting from host-based event logs we represent computer processes as streaming trees that evolve in continuous time. Leveraging the properties of the signature kernel, a machine learning tool that recently emerged as a leading technology for learning with complex sequences of data, we develop the SK-Tree algorithm. SK-Tree is a supervised learning method for systematic malware detection on streaming trees that is robust to irregular sampling and high dimensionality of the underlying streams. We demonstrate the effectiveness of SK-Tree to detect malicious events on a portion of the publicly available DARPA OpTC dataset, achieving an AUROC score of 98%.","",""
0,"Davide Maiorca","Design and implementation of robust systems for secure malware detection",2016,"","","","",34,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,1,6,"Malicious software (malware) have significantly increased in terms of number and effectiveness  during the past years. Until 2006, such software were mostly used to disrupt  network infrastructures or to show coders’ skills. Nowadays, malware constitute a very  important source of economical profit, and are very difficult to detect. Thousands of  novel variants are released every day, and modern obfuscation techniques are used to  ensure that signature-based anti-malware systems are not able to detect such threats.  This tendency has also appeared on mobile devices, with Android being the most targeted  platform. To counteract this phenomenon, a lot of approaches have been developed  by the scientific community that attempt to increase the resilience of anti-malware systems.  Most of these approaches rely on machine learning, and have become very popular  also in commercial applications. However, attackers are now knowledgeable about these  systems, and have started preparing their countermeasures. This has lead to an arms  race between attackers and developers. Novel systems are progressively built to tackle  the attacks that get more and more sophisticated. For this reason, a necessity grows  for the developers to anticipate the attackers’ moves. This means that defense systems  should be built proactively, i.e., by introducing some security design principles in their  development. The main goal of this work is showing that such proactive approach can  be employed on a number of case studies. To do so, I adopted a global methodology that  can be divided in two steps. First, understanding what are the vulnerabilities of current  state-of-the-art systems (this anticipates the attacker’s moves). Then, developing novel  systems that are robust to these attacks, or suggesting research guidelines with which  current systems can be improved. This work presents two main case studies, concerning  the detection of PDF and Android malware. The idea is showing that a proactive approach  can be applied both on the X86 and mobile world. The contributions provided on  this two case studies are multifolded. With respect to PDF files, I first develop novel attacks  that can empirically and optimally evade current state-of-the-art detectors. Then,  I propose possible solutions with which it is possible to increase the robustness of such  detectors against known and novel attacks. With respect to the Android case study,  I first show how current signature-based tools and academically developed systems are  weak against empirical obfuscation attacks, which can be easily employed without particular  knowledge of the targeted systems. Then, I examine a possible strategy to build a  machine learning detector that is robust against both empirical obfuscation and optimal  attacks. Finally, I will show how proactive approaches can be also employed to develop  systems that are not aimed at detecting malware, such as mobile fingerprinting systems.  In particular, I propose a methodology to build a powerful mobile fingerprinting system,  and examine possible attacks with which users might be able to evade it, thus preserving  their privacy. To provide the aforementioned contributions, I co-developed (with the cooperation  of the researchers at PRALab and Ruhr-Universitat Bochum) various systems:  a library to perform optimal attacks against machine learning systems (AdversariaLib),  a framework for automatically obfuscating Android applications, a system to the robust  detection of Javascript malware inside PDF files (LuxOR), a robust machine learning system  to the detection of Android malware, and a system to fingerprint mobile devices. I  also contributed to develop Android PRAGuard, a dataset containing a lot of empirical  obfuscation attacks against the Android platform. Finally, I entirely developed Slayer  NEO, an evolution of a previous system to the detection of PDF malware. The results  attained by using the aforementioned tools show that it is possible to proactively build  systems that predict possible evasion attacks. This suggests that a proactive approach  is crucial to build systems that provide concrete security against general and evasion  attacks.","",""
0,"Muhammad A Zahid, A. Agouni","Identification of a miRNA signature as a diagnostic and prognostic marker in renal cell carcinoma",2021,"","","","",35,"2022-07-13 09:40:06","","10.29117/quarfe.2021.0109","","",,,,,0,0.00,0,2,1,"Clear cell renal cell carcinoma (ccRCC) is the most common subtype of renal cell carcinoma (RCC). If diagnosed in later stages, ccRCC is associated with high renal cancer related morbidity and poor prognosis. Recently, microRNAs (miRNAs) have attracted interest as potential diagnostic and prognostic biomarkers due to their important role in cancer development and progression. Availability of big omics data in the cancer genome atlas (TCGA) coupled with data mining and machine learning have revolutionized the identification of robust diagnostic and prognostic signatures in different types of cancers. In this study, we have utilized the miRNA sequencing data of 516 ccRCC patients from TCGA to identify a diagnostic and prognostic signature by using a combined approach of differential expression analysis, survival analysis and machine learning. Differential expression analysis identified 30 downregulated and 20 upregulated miRNAs in the primary tumor as compared to solid tissue normal samples. Out of these 50 differentially expressed miRNAs, higher expression of 7 and lower expression of 6 miRNAs were found to be significantly associated with poor survival when analyzed using the Kaplan-Maier survival method. Pathway enrichment analyses related to the differentially expressed miRNAs revealed that fatty acid biosynthesis was the most significantly enriched KEGG pathway while proteoglycans in cancer pathway was enriched by the highest number of survival-associated miRNAs target genes. Differential expression and association with poor survival was used as a prefilter for training a support vector machine model capable of classifying tumor samples from solid tissue normal samples with an accuracy and precision of 99.23% and 98.50%, respectively. We have identified here a nine-miRNA signature in ccRCC patients that is capable of segregating tumor from normal tissue samples with high accuracy and precision. The future validation of this classification model in in a clinical cohort will support translation of these findings into clinical practice for early detection and follow-up of ccRCC.","",""
0,"Hu Ding, Fan Yang, Jiawei Huang","Defending SVMs against poisoning attacks: the hardness and DBSCAN approach",2020,"","","","",36,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,3,2,"Adversarial machine learning has attracted a great amount of attention in recent years. In a poisoning attack, the adversary can inject a small number of specially crafted samples into the training data which make the decision boundary severely deviate and cause unexpected misclassification. Due to the great importance and popular use of support vector machines (SVM), we consider defending SVM against poisoning attacks in this paper. We study two commonly used strategies for defending: designing robust SVM algorithms and data sanitization. Though several robust SVM algorithms have been proposed before, most of them either are in lack of adversarial-resilience, or rely on strong assumptions about the data distribution or the attacker's behavior. Moreover, the research on their complexities is still quite limited. We are the first, to the best of our knowledge, to prove that even the simplest hard-margin one-class SVM with outliers problem is NP-complete, and has no fully PTAS unless P$=$NP (that means it is hard to achieve an even approximate algorithm). For the data sanitization defense, we link it to the intrinsic dimensionality of data; in particular, we provide a sampling theorem in doubling metrics for explaining the effectiveness of DBSCAN (as a density-based outlier removal method) for defending against poisoning attacks. In our empirical experiments, we compare several defenses including the DBSCAN and robust SVM methods, and investigate the influences from the intrinsic dimensionality and data density to their performances.","",""
12,"V. Cimini, M. Barbieri, N. Treps, M. Walschaers, V. Parigi","Neural Networks for Detecting Multimode Wigner Negativity.",2020,"","","","",37,"2022-07-13 09:40:06","","10.1103/PHYSREVLETT.125.160504","","",,,,,12,6.00,2,5,2,"The characterization of quantum features in large Hilbert spaces is a crucial requirement for testing quantum protocols. In the continuous variable encoding, quantum homodyne tomography requires an amount of measurement that increases exponentially with the number of involved modes, which practically makes the protocol intractable even with few modes. Here, we introduce a new technique, based on a machine learning protocol with artificial neural networks, that allows us to directly detect negativity of the Wigner function for multimode quantum states. We test the procedure on a whole class of numerically simulated multimode quantum states for which the Wigner function is known analytically. We demonstrate that the method is fast, accurate, and more robust than conventional methods when limited amounts of data are available. Moreover, the method is applied to an experimental multimode quantum state, for which an additional test of resilience to losses is carried out.","",""
3,"A. Poortinga, Aekkapol Aekakkararungroj, Kritsana Kityuttachai, Q. Nguyen, B. Bhandari, Nyein Soe Thwal, Hannah Priestley, Jiwon Kim, Karis Tenneson, F. Chishtie, P. Towashiraporn, D. Saah","Predictive Analytics for Identifying Land Cover Change Hotspots in the Mekong Region",2020,"","","","",38,"2022-07-13 09:40:06","","10.3390/rs12091472","","",,,,,3,1.50,0,12,2,"Understanding land cover change dynamics and potential pathways of change is of critical importance for sustainable resource management, to promote food security and resilience on a range of spatial scales. Data scarcity is a key concern, however, with the availability of free Earth Observation (EO) data, such challenges can be suitably addressed. In this research we have developed a robust machine learning (random forest) approach utilizing EO and Geographic Information System (GIS) data, which enables an innovative means for our simulations to be driven only by historical drivers of change and hotspot prediction based on probability to change. We used the Mekong region as a case study to generate a training and validation sample from historical land cover patterns of change and used this information to train a random forest machine learning model. Data samples were created from the SERVIR-Mekong land cover data series. Data sets were created for 2 categories both containing 8 classes. The 2 categories included—any generic class to change into a specific one and vice versa. Classes included the following: Aquaculture; Barren; Cropland; Flooded Forest; Mangroves; Forest; Plantations; Wetlands; and Urban. The training points were used to sample a series of satellite-derived surface reflectance products and other data layers such as information on slope, distance to road and census data, which represent the drivers of change. The classifier was trained in binary mode and showed a clear separation between change and no change. An independent validation dataset of historical change pixels show that all median change probabilities are greater than 80% and all lower quantiles, except one, are greater than 70%. The 2018 probability change maps show high probabilities for the Plantations and Forest classes in the ‘Generic to Specific’ and ’Specific to generic’ category, respectively. A time-series analysis of change probability shows that forests have become more likely to convert into other classes during the last two decades, across all countries. We successfully demonstrated that historical change patters combined with big data and machine learning technologies are powerful tools for predictive change analytics on a planetary scale.","",""
1,"Elvis Rojas, Esteban Meneses, T. Jones, Don E. Maxwell","Towards a Model to Estimate the Reliability of Large-Scale Hybrid Supercomputers",2020,"","","","",39,"2022-07-13 09:40:06","","10.1007/978-3-030-57675-2_3","","",,,,,1,0.50,0,4,2,"","",""
15,"E. Vatajelu, G. D. Natale, Mohd Syafiq Mispan, Basel Halak","On the Encryption of the Challenge in Physically Unclonable Functions",2019,"","","","",40,"2022-07-13 09:40:06","","10.1109/IOLTS.2019.8854387","","",,,,,15,5.00,4,4,3,"Physically Unclonable Functions (PUFs) are cryptographic primitives used to implement low-cost device authentication and secure secret key generation. Weak PUFs (i.e., devices able to generate a single signature or to deal with a limited number of challenges) and Strong PUFs (i.e., devices able to deal with large number of challenges) are widely discussed in literature. Strong PUFs are susceptible to machine learning and modeling attacks. In this paper we propose a solution where the challenges of a Strong PUF are encrypted in order to remove the linear challenge-response correlation that can be exploited by those attacks. In this context, a ZeroBit Error Rate Weak PUF generates the encryption key so that all PUF instances have a different, nonlinear correlation between respective challenges and responses. We present two implementations of the proposed solution, and we demonstrate their resilience against machine learning attacks.","",""
0,"Fariborz Salehi, B. Hassibi","Robustifying Binary Classification to Adversarial Perturbation",2020,"","","","",41,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,2,2,"Despite the enormous success of machine learning models in various applications, most of these models lack resilience to (even small) perturbations in their input data. Hence, new methods to robustify machine learning models seem very essential. To this end, in this paper we consider the problem of binary classification with adversarial perturbations. Investigating the solution to a min-max optimization (which considers the worst-case loss in the presence of adversarial perturbations) we introduce a generalization to the max-margin classifier which takes into account the power of the adversary in manipulating the data. We refer to this classifier as the ""Robust Max-margin"" (RM) classifier. Under some mild assumptions on the loss function, we theoretically show that the gradient descent iterates (with sufficiently small step size) converge to the RM classifier in its direction. Therefore, the RM classifier can be studied to compute various performance measures (e.g. generalization error) of binary classification with adversarial perturbations.","",""
0,"Divyansh Agarwal, Yuta Baba, Pratik Sachdeva, Tanya Tandon, Thomas Vetterli, Aziz Alghunaim","Accurate and Scalable Matching of Translators to Displaced Persons for Overcoming Language Barriers",2020,"","","","",42,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,6,2,"Residents of developing countries are disproportionately susceptible to displacement as a result of humanitarian crises. During such crises, language barriers impede aid workers in providing services to those displaced. To build resilience, such services must be flexible and robust to a host of possible languages. \textit{Tarjimly} aims to overcome the barriers by providing a platform capable of matching bilingual volunteers to displaced persons or aid workers in need of translating. However, Tarjimly's large pool of translators comes with the challenge of selecting the right translator per request. In this paper, we describe a machine learning system that matches translator requests to volunteers at scale. We demonstrate that a simple logistic regression, operating on easily computable features, can accurately predict and rank translator response. In deployment, this lightweight system matches 82\% of requests with a median response time of 59 seconds, allowing aid workers to accelerate their services supporting displaced persons.","",""
0,"Rih-Teng Wu","Development and Application of Big Data Analytics and Artificial Intelligence for Structural Health Monitoring and Metamaterial Design",2020,"","","","",43,"2022-07-13 09:40:06","","10.25394/PGS.12858245.V1","","",,,,,0,0.00,0,1,2,"Recent advances in sensor technologies and data acquisition platforms have led to the era of Big Data. The rapid growth of artificial intelligence (AI), computing power and machine learning (ML) algorithms allow Big Data to be processed within affordable time constraints. This opens abundant opportunities to develop novel and efficient approaches to enhance the sustainability and resilience of Smart Cities. This work, by starting with a review of the state-of-the-art data fusion and ML techniques, focuses on the development of advanced solutions to structural health monitoring (SHM) and metamaterial design and discovery strategies. A deep convolutional neural network (CNN) based approach that is more robust against noisy data is proposed to perform structural response estimation and system identification. To efficiently detect surface defects using mobile devices with limited training data, an approach that incorporates network pruning into transfer learning is introduced for crack and corrosion detection. For metamaterial design, a reinforcement learning (RL) and a neural network based approach are proposed to reduce the computation efforts for the design of periodic and non-periodic metamaterials, respectively. Lastly, a physics-constrained deep auto-encoder (DAE) based approach is proposed to design the geometry of wave scatterers that satisfy user-defined downstream acoustic 2D wave fields. The robustness of the proposed approaches as well as their limitations are demonstrated and discussed through experimental data or/and numerical simulations. A roadmap for future works that may benefit the SHM and material design research communities is presented at the end of this dissertation.","",""
0,"Hu Ding, Fan Yang, Jiawei Huang","Defending Support Vector Machines against Poisoning Attacks: the Hardness and Algorithm",2020,"","","","",44,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,3,2,"Adversarial machine learning has attracted a great amount of attention in recent years. In a poisoning attack, the adversary can inject a small number of specially crafted samples into the training data which make the decision boundary severely deviate and cause unexpected misclassification. Due to the great importance and popular use of support vector machines (SVM), we consider defending SVM against poisoning attacks in this paper. We consider two common strategies for defending: designing robust SVM algorithms and data sanitization. Though several robust SVM algorithms have been proposed before, most of them either are in lack of adversarial-resilience or rely on strong assumptions about the data distribution or the attacker's behavior. Moreover, the research on the complexities is still quite limited nowadays. We are the first, to the best of our knowledge, to prove that even the simplest hard-margin one-class SVM with outliers problem is NP-complete, and has no fully PTAS unless P=NP. For the data sanitization defense, we link it to the intrinsic dimensionality of data; in particular, we provide a sampling theorem in doubling metrics for explaining the effectiveness of DBSCAN (as a density-based outlier removal method) for defending against poisoning attacks. In our empirical experiments, we compare several defenses including the DBSCAN and robust SVM methods, and investigate the influences from the intrinsic dimensionality and data density to their performances.","",""
33,"J. Allcock, Chang-Yu Hsieh, Iordanis Kerenidis, Shengyu Zhang","Quantum Algorithms for Feedforward Neural Networks",2018,"","","","",45,"2022-07-13 09:40:06","","10.1145/3411466","","",,,,,33,8.25,8,4,4,"Quantum machine learning has the potential for broad industrial applications, and the development of quantum algorithms for improving the performance of neural networks is of particular interest given the central role they play in machine learning today. We present quantum algorithms for training and evaluating feedforward neural networks based on the canonical classical feedforward and backpropagation algorithms. Our algorithms rely on an efficient quantum subroutine for approximating inner products between vectors in a robust way, and on implicitly storing intermediate values in quantum random access memory for fast retrieval at later stages. The running times of our algorithms can be quadratically faster in the size of the network than their standard classical counterparts since they depend linearly on the number of neurons in the network, and not on the number of connections between neurons. Furthermore, networks trained by our quantum algorithm may have an intrinsic resilience to overfitting, as the algorithm naturally mimics the effects of classical techniques used to regularize networks. Our algorithms can also be used as the basis for new quantum-inspired classical algorithms with the same dependence on the network dimensions as their quantum counterparts but with quadratic overhead in other parameters that makes them relatively impractical.","",""
3,"Thomas Nagunwa, Syed Naqvi, S. Fouad, Hanifa Shah","A Framework of New Hybrid Features for Intelligent Detection of Zero Hour Phishing Websites",2019,"","","","",46,"2022-07-13 09:40:06","","10.1007/978-3-030-20005-3_4","","",,,,,3,1.00,1,4,3,"","",""
2,"Rozhin Eskandarpour, A. Khodaei, Aleksi Paaso, N. M. Abdullah","Artificial Intelligence Assisted Power Grid Hardening in Response to Extreme Weather Events",2018,"","","","",47,"2022-07-13 09:40:06","","","","",,,,,2,0.50,1,4,4,"In this paper, an artificial intelligence based grid hardening model is proposed with the objective of improving power grid resilience in response to extreme weather events. At first, a machine learning model is proposed to predict the component states (either operational or outage) in response to the extreme event. Then, these predictions are fed into a hardening model, which determines strategic locations for placement of distributed generation (DG) units. In contrast to existing literature in hardening and resilience enhancement, this paper co-optimizes grid economic and resilience objectives by considering the intricate dependencies of the two. The numerical simulations on the standard IEEE 118-bus test system illustrate the merits and applicability of the proposed hardening model. The results indicate that the proposed hardening model through decentralized and distributed local energy resources can produce a more robust solution that can protect the system significantly against multiple component outages due to an extreme event.","",""
9,"S. Correa, Renato Cerqueira","Statistical Approaches to Predicting and Diagnosing Performance Problems in Component-Based Distributed Systems: An Experimental Evaluation",2010,"","","","",48,"2022-07-13 09:40:06","","10.1109/SASO.2010.32","","",,,,,9,0.75,5,2,12,"One of the major problems in managing large-scale distributed systems is the prediction of the application performance. The complexity of the systems and the availability of monitored data have motivated the applicability of machine learning and other statistical techniques to induce performance models and forecast performance degradation problems. However, there is a stringent need for additional experimental and comparative studies, since there is no optimal method for all cases. In addition to a deeper comparison of different statistical techniques, studies lack on two important dimensions: resilience to transient failures of the statistical techniques, and diagnostic abilities. In this work, we address these issues, presenting three main contributions: first, we establish the capability of different statistical learning techniques for forecasting the resource needs of component-based distributed systems, second, we investigate an analysis engine that is more robust to false alarms, introducing a novel algorithm that augments the predictive power of statistical learning methods by combining them with a statistical test to identify trends in resources usage, third, we investigate the applicability of statistical tests for identifying the nature and cause of performance problems in component-based distributed systems.","",""
0,"W. Liu, Wei Wang, Guohui Li","A content-aware error resiliency scheme in video transmission over wireless channels",2005,"","","","",49,"2022-07-13 09:40:06","","10.1109/ICMLC.2005.1527880","","",,,,,0,0.00,0,3,17,"Compared with wire-line network, wireless network has the characteristics such as severe bandwidth constraints, high error rates and time varying nature. Video transmission in wireless environment is a challenging task to enable reliable, flexible, adaptive, and robust communication. Traditional error control schemes consider videos as low-level bit streams, ignoring the underlying visual content. The content-aware framework is based on the recognition of strong correlation among video content, required network resources, and resulting video quality. If video content is taken into consideration during error resilience process, the bandwidth can be utilized efficiently, and ""focus of attention"" (FOA) can be protected hierarchically, the reconstructed video can satisfy the user's demand of information maximally. The content-aware error resiliency schemes proposed in this paper divide the video content into three hierarchies according to human's attention sequence, then use content feature to classify the content of each shot into a set of shot classes, and then associate each shot class with representative error resiliency model. The corresponding model is tested to be the best one for the shot class. During video communication, shots are classified and transmitted with the associated error resiliency scheme. As a result, the most valuable information is achieved at the minim cost.","",""
1,"N. Neretti, N. Intrator, L. Cooper","Pulse-train based time-delay estimation improves resiliency to noise",2004,"","","","",50,"2022-07-13 09:40:06","","10.1109/MLSP.2004.1422976","","",,,,,1,0.06,0,3,18,"Time-delay estimation accuracy in echolocating systems decays for increasing levels of noise until a breakpoint is reached, after which accuracy deteriorates by several orders of magnitude. In this paper we present a robust fusion of time-delay estimates from multiple pings that significantly reduces the signal-to-noise ratio corresponding to the accuracy breakpoint. We further show that a simple average of the time-delay estimates does not shift the breakpoint to a lower signal-to-noise ratio. The proposed fusion of multiple pings has the potential of improving the resilience to noise of echolocating systems such as sonar, medical ultrasound and geo-seismic surveys, hence increasing their potential operating range and reducing health/environmental hazards","",""
77,"A. Qayyum, Junaid Qadir, M. Bilal, A. Al-Fuqaha","Secure and Robust Machine Learning for Healthcare: A Survey",2020,"","","","",51,"2022-07-13 09:40:06","","10.1109/RBME.2020.3013489","","",,,,,77,38.50,19,4,2,"Recent years have witnessed widespread adoption of machine learning (ML)/deep learning (DL) techniques due to their superior performance for a variety of healthcare applications ranging from the prediction of cardiac arrest from one-dimensional heart signals to computer-aided diagnosis (CADx) using multi-dimensional medical images. Notwithstanding the impressive performance of ML/DL, there are still lingering doubts regarding the robustness of ML/DL in healthcare settings (which is traditionally considered quite challenging due to the myriad security and privacy issues involved), especially in light of recent results that have shown that ML/DL are vulnerable to adversarial attacks. In this paper, we present an overview of various application areas in healthcare that leverage such techniques from security and privacy point of view and present associated challenges. In addition, we present potential methods to ensure secure and privacy-preserving ML for healthcare applications. Finally, we provide insight into the current research challenges and promising directions for future research.","",""
49,"Eric Wong, J. Z. Kolter","Learning perturbation sets for robust machine learning",2020,"","","","",52,"2022-07-13 09:40:06","","","","",,,,,49,24.50,25,2,2,"Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline spatial transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to train models which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at this https URL.","",""
103,"Yunchao Liu, Srinivasan Arunachalam, K. Temme","A rigorous and robust quantum speed-up in supervised machine learning",2020,"","","","",53,"2022-07-13 09:40:06","","10.1038/s41567-021-01287-z","","",,,,,103,51.50,34,3,2,"","",""
53,"M. Shafique, Mahum Naseer, T. Theocharides, C. Kyrkou, O. Mutlu, Lois Orosa, Jungwook Choi","Robust Machine Learning Systems: Challenges,Current Trends, Perspectives, and the Road Ahead",2020,"","","","",54,"2022-07-13 09:40:06","","10.1109/MDAT.2020.2971217","","",,,,,53,26.50,8,7,2,"Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPS) and Internet-of-Things (IoT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques. —Partha Pratim Pande, Washington State University","",""
0,"Ransalu Senanayake, Daniel J. Fremont, Mykel J. Kochenderfer, A. Lomuscio, D. Margineantu, Cheng Soon Ong","Guest Editorial: Special issue on robust machine learning",2021,"","","","",55,"2022-07-13 09:40:06","","10.1007/s10994-021-06113-4","","",,,,,0,0.00,0,6,1,"","",""
21,"Lie He, Sai Praneeth Karimireddy, Martin Jaggi","Secure Byzantine-Robust Machine Learning",2020,"","","","",56,"2022-07-13 09:40:06","","","","",,,,,21,10.50,7,3,2,"Increasingly machine learning systems are being deployed to edge servers and devices (e.g. mobile phones) and trained in a collaborative manner. Such distributed/federated/decentralized training raises a number of concerns about the robustness, privacy, and security of the procedure. While extensive work has been done in tackling with robustness, privacy, or security individually, their combination has rarely been studied. In this paper, we propose a secure two-server protocol that offers both input privacy and Byzantine-robustness. In addition, this protocol is communication-efficient, fault-tolerant and enjoys local differential privacy.","",""
7,"Md. Kowsher, A. Tahabilder, S. Murad","Impact-Learning: A Robust Machine Learning Algorithm",2020,"","","","",57,"2022-07-13 09:40:06","","10.1145/3411174.3411185","","",,,,,7,3.50,2,3,2,"The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth.","",""
6,"F. Farokhi","Regularization Helps with Mitigating Poisoning Attacks: Distributionally-Robust Machine Learning Using the Wasserstein Distance",2020,"","","","",58,"2022-07-13 09:40:06","","","","",,,,,6,3.00,6,1,2,"We use distributionally-robust optimization for machine learning to mitigate the effect of data poisoning attacks. We provide performance guarantees for the trained model on the original data (not including the poison records) by training the model for the worst-case distribution on a neighbourhood around the empirical distribution (extracted from the training dataset corrupted by a poisoning attack) defined using the Wasserstein distance. We relax the distributionally-robust machine learning problem by finding an upper bound for the worst-case fitness based on the empirical sampled-averaged fitness and the Lipschitz-constant of the fitness function (on the data for given model parameters) as regularizer. For regression models, we prove that this regularizer is equal to the dual norm of the model parameters. We use the Wine Quality dataset, the Boston Housing Market dataset, and the Adult dataset for demonstrating the results of this paper.","",""
15,"Ezekiel O. Soremekun, Sakshi Udeshi, Sudipta Chattopadhyay, A. Zeller","Exposing Backdoors in Robust Machine Learning Models",2020,"","","","",59,"2022-07-13 09:40:06","","","","",,,,,15,7.50,4,4,2,"The introduction of robust optimisation has pushed the state-of-the-art in defending against adversarial attacks. However, the behaviour of such optimisation has not been studied in the light of a fundamentally different class of attacks called backdoors. In this paper, we demonstrate that adversarially robust models are susceptible to backdoor attacks. Subsequently, we observe that backdoors are reflected in the feature representation of such models. Then, this observation is leveraged to detect backdoor-infected models via a detection technique called AEGIS. Specifically, AEGIS uses feature clustering to effectively detect backdoor-infected robust Deep Neural Networks (DNNs).  In our evaluation of major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS effectively detects robust DNNs infected with backdoors. Overall, AEGIS has 97% (70/72) detection accuracy and 0.3% (2/648) false positive rate, for all configurations. Our investigation reveals that salient features of adversarially robust DNNs break the stealthy nature of backdoor attacks.","",""
52,"Hana Dureckova, M. Krykunov, M. Z. Aghaji, T. Woo","Robust Machine Learning Models for Predicting High CO2 Working Capacity and CO2/H2 Selectivity of Gas Adsorption in Metal Organic Frameworks for Precombustion Carbon Capture",2019,"","","","",60,"2022-07-13 09:40:06","","10.1021/ACS.JPCC.8B10644","","",,,,,52,17.33,13,4,3,"This work is devoted to the development of quantitative structure–property relationship (QSPR) models using machine learning to predict CO2 working capacity and CO2/H2 selectivity for precombustion carbon capture using a topologically diverse database of hypothetical metal–organic framework (MOF) structures (358 400 MOFs, 1166 network topologies). Such a diversity of the networks topology is much higher than previously used (<20 network topologies) for rapid and accurate recognition of high-performing MOFs for other gas-separation applications. The gradient boosted trees regression method allowed us to use 80% of the database as a training set, while the rest was used for the validation and test set. The QSPR models are first built using purely geometric descriptors of MOFs such as gravimetric surface area and void fraction. Additional models which account for chemical features of MOFs are constructed using atomic property weighted radial distribution functions (AP-RDFs) with a novel normalization to acco...","",""
50,"Soohyun Nam Liao, Daniel Zingaro, Kevin Thai, Christine Alvarado, W. Griswold, Leo Porter","A Robust Machine Learning Technique to Predict Low-performing Students",2019,"","","","",61,"2022-07-13 09:40:06","","10.1145/3277569","","",,,,,50,16.67,8,6,3,"As enrollments and class sizes in postsecondary institutions have increased, instructors have sought automated and lightweight means to identify students who are at risk of performing poorly in a course. This identification must be performed early enough in the term to allow instructors to assist those students before they fall irreparably behind. This study describes a modeling methodology that predicts student final exam scores in the third week of the term by using the clicker data that is automatically collected for instructors when they employ the Peer Instruction pedagogy. The modeling technique uses a support vector machine binary classifier, trained on one term of a course, to predict outcomes in the subsequent term. We applied this modeling technique to five different courses across the computer science curriculum, taught by three different instructors at two different institutions. Our modeling approach includes a set of strengths not seen wholesale in prior work, while maintaining competitive levels of accuracy with that work. These strengths include using a lightweight source of student data, affording early detection of struggling students, and predicting outcomes across terms in a natural setting (different final exams, minor changes to course content), across multiple courses in a curriculum, and across multiple institutions.","",""
23,"J. Zhang, Kang Liu, Faiq Khalid, Muhammad Abdullah Hanif, Semeen Rehman, T. Theocharides, Alessandro Artussi, M. Shafique, S. Garg","Building Robust Machine Learning Systems: Current Progress, Research Challenges, and Opportunities",2019,"","","","",62,"2022-07-13 09:40:06","","10.1145/3316781.3323472","","",,,,,23,7.67,3,9,3,"Machine learning, in particular deep learning, is being used in almost all the aspects of life to facilitate humans, specifically in mobile and Internet of Things (IoT)-based applications. Due to its state-of-the-art performance, deep learning is also being employed in safety-critical applications, for instance, autonomous vehicles. Reliability and security are two of the key required characteristics for these applications because of the impact they can have on human's life. Towards this, in this paper, we highlight the current progress, challenges and research opportunities in the domain of robust systems for machine learning-based applications.","",""
103,"Muhammad Attique Khan, I. Ashraf, M. Alhaisoni, Robertas Damaševičius, R. Scherer, A. Rehman, S. Bukhari","Multimodal Brain Tumor Classification Using Deep Learning and Robust Feature Selection: A Machine Learning Application for Radiologists",2020,"","","","",63,"2022-07-13 09:40:06","","10.3390/diagnostics10080565","","",,,,,103,51.50,15,7,2,"Manual identification of brain tumors is an error-prone and tedious process for radiologists; therefore, it is crucial to adopt an automated system. The binary classification process, such as malignant or benign is relatively trivial; whereas, the multimodal brain tumors classification (T1, T2, T1CE, and Flair) is a challenging task for radiologists. Here, we present an automated multimodal classification method using deep learning for brain tumor type classification. The proposed method consists of five core steps. In the first step, the linear contrast stretching is employed using edge-based histogram equalization and discrete cosine transform (DCT). In the second step, deep learning feature extraction is performed. By utilizing transfer learning, two pre-trained convolutional neural network (CNN) models, namely VGG16 and VGG19, were used for feature extraction. In the third step, a correntropy-based joint learning approach was implemented along with the extreme learning machine (ELM) for the selection of best features. In the fourth step, the partial least square (PLS)-based robust covariant features were fused in one matrix. The combined matrix was fed to ELM for final classification. The proposed method was validated on the BraTS datasets and an accuracy of 97.8%, 96.9%, 92.5% for BraTs2015, BraTs2017, and BraTs2018, respectively, was achieved.","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",64,"2022-07-13 09:40:06","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
57,"Lal Hussain","Detecting epileptic seizure with different feature extracting strategies using robust machine learning classification techniques by applying advance parameter optimization approach",2018,"","","","",65,"2022-07-13 09:40:06","","10.1007/s11571-018-9477-1","","",,,,,57,14.25,57,1,4,"","",""
101,"G. Lecu'e, M. Lerasle","Robust machine learning by median-of-means: Theory and practice",2017,"","","","",66,"2022-07-13 09:40:06","","10.1214/19-AOS1828","","",,,,,101,20.20,51,2,5,"We introduce new estimators for robust machine learning based on median-of-means (MOM) estimators of the mean of real valued random variables. These estimators achieve optimal rates of convergence under minimal assumptions on the dataset. The dataset may also have been corrupted by outliers on which no assumption is granted. We also analyze these new estimators with standard tools from robust statistics. In particular, we revisit the concept of breakdown point. We modify the original definition by studying the number of outliers that a dataset can contain without deteriorating the estimation properties of a given estimator. This new notion of breakdown number, that takes into account the statistical performances of the estimators, is non-asymptotic in nature and adapted for machine learning purposes. We proved that the breakdown number of our estimator is of the order of (number of observations)*(rate of convergence). For instance, the breakdown number of our estimators for the problem of estimation of a d-dimensional vector with a noise variance sigma^2 is sigma^2d and it becomes sigma^2 s log(d/s) when this vector has only s non-zero component. Beyond this breakdown point, we proved that the rate of convergence achieved by our estimator is (number of outliers) divided by (number of observation).  Besides these theoretical guarantees, the major improvement brought by these new estimators is that they are easily computable in practice. In fact, basically any algorithm used to approximate the standard Empirical Risk Minimizer (or its regularized versions) has a robust version approximating our estimators. As a proof of concept, we study many algorithms for the classical LASSO estimator. A byproduct of the MOM algorithms is a measure of depth of data that can be used to detect outliers.","",""
36,"J. Li","Principled approaches to robust machine learning and beyond",2018,"","","","",67,"2022-07-13 09:40:06","","","","",,,,,36,9.00,36,1,4,"As we apply machine learning to more and more important tasks, it becomes increasingly important that these algorithms are robust to systematic, or worse, malicious, noise. Despite considerable interest, no efficient algorithms were known to be robust to such noise in high dimensional settings for some of the most fundamental statistical tasks for over sixty years of research. In this thesis we devise two novel, but similarly inspired, algorithmic paradigms for estimation in high dimensions in the presence of a small number of adversarially added data points. Both algorithms are the first efficient algorithms which achieve (nearly) optimal error bounds for a number fundamental statistical tasks such as mean estimation and covariance estimation. The goal of this thesis is to present these two frameworks in a clean and unified manner. We show that these insights also have applications for other problems in learning theory. Specifically, we show that these algorithms can be combined with the powerful Sum-of-Squares hierarchy to yield improvements for clustering high dimensional Gaussian mixture models, the first such improvement in over fifteen years of research. Going full circle, we show that Sum-of-Squares also can be used to improve error rates for robust mean estimation. Not only are these algorithms of interest theoretically, but we demonstrate empirically that we can use these insights in practice to uncover patterns in high dimensional data that were previously masked by noise. Based on our algorithms, we give new implementations for robust PCA, new defenses for data poisoning attacks for stochastic optimization, and new defenses for watermarking attacks on deep nets. In all of these tasks, we demonstrate on both synthetic and real data sets that our performance is substantially better than the state-of-the-art, often able to detect most to all corruptions when previous methods could not reliably detect any. Thesis Supervisor: Ankur Moitra Title: Rockwell International CD Associate Professor of Mathematics","",""
34,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, Semeen Rehman, M. Shafique","Robust Machine Learning Systems: Reliability and Security for Deep Neural Networks",2018,"","","","",68,"2022-07-13 09:40:06","","10.1109/IOLTS.2018.8474192","","",,,,,34,8.50,7,5,4,"Machine learning is commonly being used in almost all the areas that involve advanced data analytics and intelligent control. From applications like Natural Language Processing (NLP) to autonomous driving are based upon machine learning algorithms. An increasing trend is observed in the use of Deep Neural Networks (DNNs) for such applications. While the slight inaccuracy in applications like NLP does not have any severe consequences, it is not the same for other safety-critical applications, like autonomous driving and smart healthcare, where a small error can lead to catastrophic effects. Apart from high-accuracy DNN algorithms, there is a significant need for robust machine learning systems and hardware architectures that can generate reliable and trustworthy results in the presence of hardware-level faults while also preserving security and privacy. This paper provides an overview of the challenges being faced in ensuring reliable and secure execution of DNNs. To address the challenges, we present several techniques for analyzing and mitigating the reliability and security threats in machine learning systems.","",""
36,"Daniel S. Berger","Towards Lightweight and Robust Machine Learning for CDN Caching",2018,"","","","",69,"2022-07-13 09:40:06","","10.1145/3286062.3286082","","",,,,,36,9.00,36,1,4,"Recent advances in the field of reinforcement learning promise a general approach to optimize networking systems. This paper argues against the recent trend for generalization by introducing a case study where domain-specific modeling enables the application of lightweight and robust learning techniques. We study CDN caching systems, which make a good case for optimization as their performance directly affects operational costs, while currently relying on many hand-tuned parameters. In caching, reinforcement learning has been shown to perform suboptimally when compared to simple heuristics. A key challenge is that rewards (cache hits) manifest with large delays, which prevents timely feedback to the learning algorithm and introduces significant complexity. This paper shows how to significantly simplify this problem by explicitly modeling optimal caching decisions (OPT). While prior work considered deriving OPT impractical, recent theoretical modeling advances change this assumption. Modeling OPT enables even lightweight decision trees to outperform state-of-the-art CDN caching heuristics.","",""
0,"Susmit Jha, Brian Jalaian, Anirban Roy, Gunjan Verma","Trinity: Trust, Resilience and Interpretability of Machine Learning Models",2021,"","","","",70,"2022-07-13 09:40:06","","10.1002/9781119723950.ch16","","",,,,,0,0.00,0,4,1,"","",""
11,"Sherri Rose","Robust Machine Learning Variable Importance Analyses of Medical Conditions for Health Care Spending",2018,"","","","",71,"2022-07-13 09:40:06","","10.1111/1475-6773.12848","","",,,,,11,2.75,11,1,4,"OBJECTIVE To propose nonparametric double robust machine learning in variable importance analyses of medical conditions for health spending.   DATA SOURCES 2011-2012 Truven MarketScan database.   STUDY DESIGN I evaluate how much more, on average, commercially insured enrollees with each of 26 of the most prevalent medical conditions cost per year after controlling for demographics and other medical conditions. This is accomplished within the nonparametric targeted learning framework, which incorporates ensemble machine learning. Previous literature studying the impact of medical conditions on health care spending has almost exclusively focused on parametric risk adjustment; thus, I compare my approach to parametric regression.   PRINCIPAL FINDINGS My results demonstrate that multiple sclerosis, congestive heart failure, severe cancers, major depression and bipolar disorders, and chronic hepatitis are the most costly medical conditions on average per individual. These findings differed from those obtained using parametric regression.   CONCLUSIONS The literature may be underestimating the spending contributions of several medical conditions, which is a potentially critical oversight. If current methods are not capturing the true incremental effect of medical conditions, undesirable incentives related to care may remain. Further work is needed to directly study these issues in the context of federal formulas.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",72,"2022-07-13 09:40:06","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
3,"J. Xie, Inalvis Alvarez-Fernandez, Wei Sun","A Review of Machine Learning Applications in Power System Resilience",2020,"","","","",73,"2022-07-13 09:40:06","","10.1109/PESGM41954.2020.9282137","","",,,,,3,1.50,1,3,2,"The integration of power electronics enabled devices and the high penetration of renewable energy drastically increase the complexity of power system operation and control. Power systems are still vulnerable to large-scale blackouts caused by extreme natural events or man-made attacks. With the recent development in artificial intelligence technique, machine learning has shown a processing ability in computational, perceptual and cognitive intelligence. It is an urgent challenge to integrate the advanced machine learning technology and large amount of real-time data from wide area measurement systems and intelligent electronic devices, in order to effectively enhance power system resilience and ensure the reliable and secure operation of power systems. Therefore, this paper aims to systematically review the existing application of machine learning methods on power system resilience enhancement, to expand the interest of researchers and scholars in this topic, and to jointly promote the application of artificial intelligence in the field of power systems.","",""
148,"Amedeo Sapio, M. Canini, Chen-Yu Ho, J. Nelson, Panos Kalnis, Changhoon Kim, A. Krishnamurthy, M. Moshref, Dan R. K. Ports, Peter Richtárik","Scaling Distributed Machine Learning with In-Network Aggregation",2019,"","","","",74,"2022-07-13 09:40:06","","","","",,,,,148,49.33,15,10,3,"Training complex machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide a robust, efficient solution that speeds up training by up to 300%, and at least by 20% for a number of real-world benchmark models.","",""
106,"Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, V. Larivière, A. Beygelzimer, Florence d'Alché-Buc, E. Fox, H. Larochelle","Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",2020,"","","","",75,"2022-07-13 09:40:06","","","","",,,,,106,53.00,13,8,2,"One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.","",""
209,"J. Blanchet, Yang Kang, M. KarthyekRajhaaA.","Robust Wasserstein profile inference and applications to machine learning",2016,"","","","",76,"2022-07-13 09:40:06","","10.1017/jpr.2019.49","","",,,,,209,34.83,70,3,6,"We show that several machine learning estimators, including square-root least absolute shrinkage and selection and regularized logistic regression, can be represented as solutions to distributionally robust optimization problems. The associated uncertainty regions are based on suitably defined Wasserstein distances. Hence, our representations allow us to view regularization as a result of introducing an artificial adversary that perturbs the empirical distribution to account for out-of-sample effects in loss estimation. In addition, we introduce RWPI (robust Wasserstein profile inference), a novel inference methodology which extends the use of methods inspired by empirical likelihood to the setting of optimal transport costs (of which Wasserstein distances are a particular case). We use RWPI to show how to optimally select the size of uncertainty regions, and as a consequence we are able to choose regularization parameters for these machine learning estimators without the use of cross validation. Numerical experiments are also given to validate our theoretical findings.","",""
168,"D. Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, Soroosh Shafieezadeh-Abadeh","Wasserstein Distributionally Robust Optimization: Theory and Applications in Machine Learning",2019,"","","","",77,"2022-07-13 09:40:06","","10.1287/EDUC.2019.0198","","",,,,,168,56.00,42,4,3,"Many decision problems in science, engineering and economics are affected by uncertain parameters whose distribution is only indirectly observable through samples. The goal of data-driven decision-making is to learn a decision from finitely many training samples that will perform well on unseen test samples. This learning task is difficult even if all training and test samples are drawn from the same distribution---especially if the dimension of the uncertainty is large relative to the training sample size. Wasserstein distributionally robust optimization seeks data-driven decisions that perform well under the most adverse distribution within a certain Wasserstein distance from a nominal distribution constructed from the training samples. In this tutorial we will argue that this approach has many conceptual and computational benefits. Most prominently, the optimal decisions can often be computed by solving tractable convex optimization problems, and they enjoy rigorous out-of-sample and asymptotic consistency guarantees. We will also show that Wasserstein distributionally robust optimization has interesting ramifications for statistical learning and motivates new approaches for fundamental learning tasks such as classification, regression, maximum likelihood estimation or minimum mean square error estimation, among others.","",""
50,"Megha Byali, Harsh Chaudhari, A. Patra, A. Suresh","FLASH: Fast and Robust Framework for Privacy-preserving Machine Learning",2020,"","","","",78,"2022-07-13 09:40:06","","10.2478/popets-2020-0036","","",,,,,50,25.00,13,4,2,"Abstract Privacy-preserving machine learning (PPML) via Secure Multi-party Computation (MPC) has gained momentum in the recent past. Assuming a minimal network of pair-wise private channels, we propose an efficient four-party PPML framework over rings ℤ2ℓ, FLASH, the first of its kind in the regime of PPML framework, that achieves the strongest security notion of Guaranteed Output Delivery (all parties obtain the output irrespective of adversary’s behaviour). The state of the art ML frameworks such as ABY3 by Mohassel et.al (ACM CCS’18) and SecureNN by Wagh et.al (PETS’19) operate in the setting of 3 parties with one malicious corruption but achieve the weaker security guarantee of abort. We demonstrate PPML with real-time efficiency, using the following custom-made tools that overcome the limitations of the aforementioned state-of-the-art– (a) dot product, which is independent of the vector size unlike the state-of-the-art ABY3, SecureNN and ASTRA by Chaudhari et.al (ACM CCSW’19), all of which have linear dependence on the vector size. (b) Truncation and MSB Extraction, which are constant round and free of circuits like Parallel Prefix Adder (PPA) and Ripple Carry Adder (RCA), unlike ABY3 which uses these circuits and has round complexity of the order of depth of these circuits. We then exhibit the application of our FLASH framework in the secure server-aided prediction of vital algorithms– Linear Regression, Logistic Regression, Deep Neural Networks, and Binarized Neural Networks. We substantiate our theoretical claims through improvement in benchmarks of the aforementioned algorithms when compared with the current best framework ABY3. All the protocols are implemented over a 64-bit ring in LAN and WAN. Our experiments demonstrate that, for MNIST dataset, the improvement (in terms of throughput) ranges from 24 × to 1390 × over LAN and WAN together.","",""
36,"K. Schultebraucks, I. Galatzer-Levy","Machine Learning for Prediction of Posttraumatic Stress and Resilience Following Trauma: An Overview of Basic Concepts and Recent Advances.",2019,"","","","",79,"2022-07-13 09:40:06","","10.1002/jts.22384","","",,,,,36,12.00,18,2,3,"Posttraumatic stress responses are characterized by a heterogeneity in clinical appearance and etiology. This heterogeneity impacts the field's ability to characterize, predict, and remediate maladaptive responses to trauma. Machine learning (ML) approaches are increasingly utilized to overcome this foundational problem in characterization, prediction, and treatment selection across branches of medicine that have struggled with similar clinical realities of heterogeneity in etiology and outcome, such as oncology. In this article, we review and evaluate ML approaches and applications utilized in the areas of posttraumatic stress, stress pathology, and resilience research, and present didactic information and examples to aid researchers interested in the relevance of ML to their own research. The examined studies exemplify the high potential of ML approaches to build accurate predictive and diagnostic models of posttraumatic stress and stress pathology risk based on diverse sources of available information. The use of ML approaches to integrate high-dimensional data demonstrates substantial gains in risk prediction even when the sources of data are the same as those used in traditional predictive models. This area of research will greatly benefit from collaboration and data sharing among researchers of posttraumatic stress disorder, stress pathology, and resilience.","",""
29,"Fahad Shabbir Ahmad, Liaqat Ali, Liaqat Ali, Raza-Ul-Mustafa, Hasan Ali Khattak, Tahir Hameed, Iram Wajahat, Seifedine Kadry, S. Bukhari","A hybrid machine learning framework to predict mortality in paralytic ileus patients using electronic health records (EHRs)",2020,"","","","",80,"2022-07-13 09:40:06","","10.1007/s12652-020-02456-3","","",,,,,29,14.50,3,9,2,"","",""
55,"Georgios Damaskinos, El-Mahdi El-Mhamdi, R. Guerraoui, Arsany Guirguis, Sébastien Rouault","AGGREGATHOR: Byzantine Machine Learning via Robust Gradient Aggregation",2019,"","","","",81,"2022-07-13 09:40:06","","","","",,,,,55,18.33,11,5,3,"We present AGGREGATHOR, a framework that implements state-of-the-art robust (Byzantine-resilient) distributed stochastic gradient descent. Following the standard parameter server model, we assume that a minority of worker machines can be controlled by an adversary and behave arbitrarily. Such a setting has been theoretically studied with several of the existing approaches using a robust aggregation of the workers’ gradient estimations. Yet, the question is whether a Byzantine-resilient aggregation can leverage more workers to speedup learning. We answer this theoretical question, and implement these state-of-the-art theoretical approaches on AGGREGATHOR, to assess their practical costs. We built AGGREGATHOR around TensorFlow and introduce modifications for vanilla TensorFlow towards making it usable in an actual Byzantine setting. AGGREGATHOR also permits the use of unreliable gradient transfer over UDP to provide further speed-up (without losing the accuracy) over the native communication protocols (TCP-based) of TensorFlow in saturated networks. We quantify the overhead of Byzantine resilience of AGGREGATHOR to 19% and 43% (to ensure weak and strong Byzantine resilience respectively) compared to vanilla TensorFlow.","",""
36,"Galal Omer, O. Mutanga, E. Abdel-Rahman, E. Adam","Empirical Prediction of Leaf Area Index (LAI) of Endangered Tree Species in Intact and Fragmented Indigenous Forests Ecosystems Using WorldView-2 Data and Two Robust Machine Learning Algorithms",2016,"","","","",82,"2022-07-13 09:40:06","","10.3390/rs8040324","","",,,,,36,6.00,9,4,6,"Leaf area index (LAI) is an important biophysical trait for forest ecosystem and ecological modeling, as it plays a key role for the forest productivity and structural characteristics. The ground-based methods like the handheld optical instruments for predicting LAI are subjective, pricy and time-consuming. The advent of very high spatial resolutions multispectral data and robust machine learning regression algorithms like support vector machines (SVM) and artificial neural networks (ANN) has provided an opportunity to estimate LAI at tree species level. The objective of the this study was therefore to test the utility of spectral vegetation indices (SVI) calculated from the multispectral WorldView-2 (WV-2) data in predicting LAI at tree species level using the SVM and ANN machine learning regression algorithms. We further tested whether there are significant differences between LAI of intact and fragmented (open) indigenous forest ecosystems at tree species level. The study shows that LAI at tree species level could accurately be estimated using the fragmented stratum data compared with the intact stratum data. Specifically, our study shows that the accurate LAI predictions were achieved for Hymenocardia ulmoides using the fragmented stratum data and SVM regression model based on a validation dataset (R2Val = 0.75, RMSEVal = 0.05 (1.37% of the mean)). Our study further showed that SVM regression approach achieved more accurate models for predicting the LAI of the six endangered tree species compared with ANN regression method. It is concluded that the successful application of the WV-2 data, SVM and ANN methods in predicting LAI of six endangered tree species in the Dukuduku indigenous forest could help in making informed decisions and policies regarding management, protection and conservation of these endangered tree species.","",""
665,"Weihua Hu, Matthias Fey, M. Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, J. Leskovec","Open Graph Benchmark: Datasets for Machine Learning on Graphs",2020,"","","","",83,"2022-07-13 09:40:06","","","","",,,,,665,332.50,83,8,2,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .","",""
19,"A. Aravkin, Damek Davis","A SMART Stochastic Algorithm for Nonconvex Optimization with Applications to Robust Machine Learning",2016,"","","","",84,"2022-07-13 09:40:06","","","","",,,,,19,3.17,10,2,6,"In this paper, we show how to transform any optimization problem that arises from fitting a machine learning model into one that (1) detects and removes contaminated data from the training set while (2) simultaneously fitting the trimmed model on the uncontaminated data that remains. To solve the resulting nonconvex optimization problem, we introduce a fast stochastic proximal-gradient algorithm that incorporates prior knowledge through nonsmooth regularization. For datasets of size $n$, our approach requires $O(n^{2/3}/\varepsilon)$ gradient evaluations to reach $\varepsilon$-accuracy and, when a certain error bound holds, the complexity improves to $O(\kappa n^{2/3}\log(1/\varepsilon))$. These rates are $n^{1/3}$ times better than those achieved by typical, full gradient methods.","",""
0,"H. Anh, Cao Van Kien","Robust extreme learning machine neural approach for uncertain nonlinear hyper‐chaotic system identification",2021,"","","","",85,"2022-07-13 09:40:06","","10.1002/rnc.5756","","",,,,,0,0.00,0,2,1,"This paper proposes a novel nonlinearly parameterized advanced single‐hidden layer neural extreme learning machine (ASHLN‐ELM) model in which the hidden and output weighting values are simultaneously updated using adaptively robust rules that are implemented based on Lyapunov stability principle. The proposed scheme guarantees the fast convergence speed of the state‐estimation residual errors bounded to null regarding to the influence of time‐varied disturbances. Additionally, proposed method needs no any knowledge related to desired weighting values or required approximating error. Typical uncertain hyper‐chaotic benchmark systems are used as to verify the new ASHLN‐ELM approach and to demonstrate the efficiency and the robustness of proposed method.","",""
220,"Ian J. Goodfellow, P. Mcdaniel, Nicolas Papernot","Making machine learning robust against adversarial inputs",2018,"","","","",86,"2022-07-13 09:40:06","","10.1145/3134599","","",,,,,220,55.00,73,3,4,"Such inputs distort how machine-learning-based systems are able to function in the world as it is.","",""
0,"S. Samadi, M. Taslimi","Develop a situation-based prioritization program as a road map to enhance the pre-resilience in flood management using machine learning methods",2022,"","","","",87,"2022-07-13 09:40:06","","10.1108/ijdrbe-12-2021-0161","","",,,,,0,0.00,0,2,1," Purpose This study aims to review the features and challenges of the flood relief chain, identifies administrative measures during and after the flood occurrence and prioritizes them using two machine learning (ML) and analytic hierarchy process (AHP) methods. This paper aims to provide a prioritization program based on flood conditions that optimize flood management and improves society’s resilience against flood occurrence.   Design/methodology/approach The collected database in this paper has been trained by using ML algorithms, including support vector machine (SVM), Naive Bayes (NB) and k-nearest neighbors (kNN), to create a prioritization program. Furthermore, the administrative measures in two phases of during and after the flood are prioritized by using the AHP method and questionnaires completed by experts and relief workers in flood management.   Findings Among the ML algorithms, the SVM method was selected with 91.37% accuracy. The prioritization program provided by the model, which distinguishes it from other existing models, considers five conditions of the flood occurrence to prioritize actions (season, population affected, area affected, damage to houses and human lives lost). Therefore, the model presents a specific plan for each flood with different occurrence conditions.   Research limitations/implications The main limitation is the lack of a comprehensive data set to determine the effect of all flood conditions on the prioritization program and the relief activities that have been done in previous flood disasters.   Originality/value The originality of this paper is the use of ML methods to prioritize administrative measures during and after the flood and presents a prioritization program based on each flood’s conditions. Therefore, through this program, the authority and society can control the adverse impacts of flood more effectively and help to reduce human and financial losses as much as possible. ","",""
92,"Martin Rozycki, T. Satterthwaite, N. Koutsouleris, G. Erus, J. Doshi, D. Wolf, Yong Fan, R. Gur, R. Gur, E. Meisenzahl, C. Zhuo, Hong Yin, Hao Yan, W. Yue, Dai Zhang, C. Davatzikos","Multisite Machine Learning Analysis Provides a Robust Structural Imaging Signature of Schizophrenia Detectable Across Diverse Patient Populations and Within Individuals",2018,"","","","",88,"2022-07-13 09:40:06","","10.1093/schbul/sbx137","","",,,,,92,23.00,9,16,4,"Past work on relatively small, single-site studies using regional volumetry, and more recently machine learning methods, has shown that widespread structural brain abnormalities are prominent in schizophrenia. However, to be clinically useful, structural imaging biomarkers must integrate high-dimensional data and provide reproducible results across clinical populations and on an individual person basis. Using advanced multi-variate analysis tools and pooled data from case-control imaging studies conducted at 5 sites (941 adult participants, including 440 patients with schizophrenia), a neuroanatomical signature of patients with schizophrenia was found, and its robustness and reproducibility across sites, populations, and scanners, was established for single-patient classification. Analyses were conducted at multiple scales, including regional volumes, voxelwise measures, and complex distributed patterns. Single-subject classification was tested for single-site, pooled-site, and leave-site-out generalizability. Regional and voxelwise analyses revealed a pattern of widespread reduced regional gray matter volume, particularly in the medial prefrontal, temporolimbic and peri-Sylvian cortex, along with ventricular and pallidum enlargement. Multivariate classification using pooled data achieved a cross-validated prediction accuracy of 76% (AUC = 0.84). Critically, the leave-site-out validation of the detected schizophrenia signature showed accuracy/AUC range of 72-77%/0.73-0.91, suggesting a robust generalizability across sites and patient cohorts. Finally, individualized patient classifications displayed significant correlations with clinical measures of negative, but not positive, symptoms. Taken together, these results emphasize the potential for structural neuroimaging data to provide a robust and reproducible imaging signature of schizophrenia. A web-accessible portal is offered to allow the community to obtain individualized classifications of magnetic resonance imaging scans using the methods described herein.","",""
0,"Wenjie Huang, M. Ling","Machine Learning-Based Method for Urban Lifeline System Resilience Assessment in GIS*",2019,"","","","",89,"2022-07-13 09:40:06","","10.5772/INTECHOPEN.82748","","",,,,,0,0.00,0,2,3,"System resilience, the capability of a system to sustain and recover from deliberate attacks, accidents, or naturally occurring threats or incidents, is a key property to measure the degree of robustness and coupling effect of complex system. The systems of waste disposal, urban water supply, and electricity transmission are typical systems with complex and high coupling features. In this chapter, a methodology for measuring the system resilience of such systems is proposed. It is a process of integrated decision-making which contains two aspects: (1) a five-dimensional indicator framework of system resilience which includes attributes in infrastructural, economic, and social sectors and (2) a hybrid K-means algorithm, which combines entropy theory, bootstrapping, and analytic network process. Through utilizing real data, the methodology can assist to identify and classify the level of system resilience for different geographical regions which are sustained by lifeline systems. The calculation of algorithm, visualization of processed data, and classification of resilience level can be finally realized in geographic information system. Through utilizing by regional governments and local communities, the final result can serve to provide guideline for resource allocation and the prevention of huge economic loss in disasters.","",""
1146,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Efficient and Robust Automated Machine Learning",2015,"","","","",90,"2022-07-13 09:40:06","","","","",,,,,1146,163.71,191,6,7,"The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.","",""
125,"Ya Zhuo, Aria Mansouri Tehrani, A. Oliynyk, Anna C. Duke, Jakoah Brgoch","Identifying an efficient, thermally robust inorganic phosphor host via machine learning",2018,"","","","",91,"2022-07-13 09:40:06","","10.1038/s41467-018-06625-z","","",,,,,125,31.25,25,5,4,"","",""
242,"I. Evtimov, Kevin Eykholt, Earlence Fernandes, T. Kohno, Bo Li, Atul Prakash, Amir Rahmati, D. Song","Robust Physical-World Attacks on Machine Learning Models",2017,"","","","",92,"2022-07-13 09:40:06","","","","",,,,,242,48.40,30,8,5,"Deep neural network-based classifiers are known to be vulnerable to adversarial examples that can fool them into misclassifying their input through the addition of small-magnitude perturbations. However, recent studies have demonstrated that such adversarial examples are not very effective in the physical world--they either completely fail to cause misclassification or only work in restricted cases where a relatively complex image is perturbed and printed on paper. In this paper we propose a new attack algorithm--Robust Physical Perturbations (RP2)-- that generates perturbations by taking images under different conditions into account. Our algorithm can create spatially-constrained perturbations that mimic vandalism or art to reduce the likelihood of detection by a casual observer. We show that adversarial examples generated by RP2 achieve high success rates under various conditions for real road sign recognition by using an evaluation methodology that captures physical world conditions. We physically realized and evaluated two attacks, one that causes a Stop sign to be misclassified as a Speed Limit sign in 100% of the testing conditions, and one that causes a Right Turn sign to be misclassified as either a Stop or Added Lane sign in 100% of the testing conditions.","",""
18,"I. Galatzer-Levy, K. Ruggles, Zhe Chen","Data Science in the Research Domain Criteria Era: Relevance of Machine Learning to the Study of Stress Pathology, Recovery, and Resilience",2018,"","","","",93,"2022-07-13 09:40:06","","10.1177/2470547017747553","","",,,,,18,4.50,6,3,4,"Diverse environmental and biological systems interact to influence individual differences in response to environmental stress. Understanding the nature of these complex relationships can enhance the development of methods to (1) identify risk, (2) classify individuals as healthy or ill, (3) understand mechanisms of change, and (4) develop effective treatments. The Research Domain Criteria initiative provides a theoretical framework to understand health and illness as the product of multiple interrelated systems but does not provide a framework to characterize or statistically evaluate such complex relationships. Characterizing and statistically evaluating models that integrate multiple levels (e.g. synapses, genes, and environmental factors) as they relate to outcomes that are free from prior diagnostic benchmarks represent a challenge requiring new computational tools that are capable to capture complex relationships and identify clinically relevant populations. In the current review, we will summarize machine learning methods that can achieve these goals.","",""
37,"Ziv Katzir, Y. Elovici","Quantifying the resilience of machine learning classifiers used for cyber security",2018,"","","","",94,"2022-07-13 09:40:06","","10.1016/j.eswa.2017.09.053","","",,,,,37,9.25,19,2,4,"","",""
11,"B. Celik, J. Vanschoren","Adaptation Strategies for Automated Machine Learning on Evolving Data",2020,"","","","",95,"2022-07-13 09:40:06","","10.1109/TPAMI.2021.3062900","","",,,,,11,5.50,6,2,2,"Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust to changes in the underlying data. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on a variety of AutoML approaches for building machine learning pipelines, including Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.","",""
10,"Sam Andersson, Deepti R. Bathula, S. Iliadis, M. Walter, A. Skalkidou","Predicting women with depressive symptoms postpartum with machine learning methods",2021,"","","","",96,"2022-07-13 09:40:06","","10.1038/s41598-021-86368-y","","",,,,,10,10.00,2,5,1,"","",""
9,"Mehran Goli, Jannis Stoppe, R. Drechsler","Resilience Evaluation for Approximating SystemC Designs Using Machine Learning Techniques",2018,"","","","",97,"2022-07-13 09:40:06","","10.1109/RSP.2018.8631997","","",,,,,9,2.25,3,3,4,"As digital circuits have become more complicated than ever, abstract description languages such as SystemC have been introduced, allowing designers to work on more abstract levels during the design process. Design metrics such as performance and energy consumption are a central concern for designers at all levels of abstraction. Approximate computing is a promising way to optimize these criteria, sacrificing accuracy. Defining which parts of a design can be approximated (and to what degree) is a crucial and non-trivial design decision, which is usually connected to a larger programming effort, especially when exploring the design space manually. In this paper, we propose an automated approach based on machine learning techniques in order to detect the resilience of a given SystemC design's modules. This is used to identify components of the design that can be approximated. The effectiveness of the proposed method is evaluated using several SystemC benchmarks from various domains.","",""
112,"Heinrich Jiang, Ofir Nachum","Identifying and Correcting Label Bias in Machine Learning",2019,"","","","",98,"2022-07-13 09:40:06","","","","",,,,,112,37.33,56,2,3,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.","",""
122,"Daniel R. Schrider, A. Kern","S/HIC: Robust Identification of Soft and Hard Sweeps Using Machine Learning",2015,"","","","",99,"2022-07-13 09:40:06","","10.1371/journal.pgen.1005928","","",,,,,122,17.43,61,2,7,"Detecting the targets of adaptive natural selection from whole genome sequencing data is a central problem for population genetics. However, to date most methods have shown sub-optimal performance under realistic demographic scenarios. Moreover, over the past decade there has been a renewed interest in determining the importance of selection from standing variation in adaptation of natural populations, yet very few methods for inferring this model of adaptation at the genome scale have been introduced. Here we introduce a new method, S/HIC, which uses supervised machine learning to precisely infer the location of both hard and soft selective sweeps. We show that S/HIC has unrivaled accuracy for detecting sweeps under demographic histories that are relevant to human populations, and distinguishing sweeps from linked as well as neutrally evolving regions. Moreover we show that S/HIC is uniquely robust among its competitors to model misspecification. Thus even if the true demographic model of a population differs catastrophically from that specified by the user, S/HIC still retains impressive discriminatory power. Finally we apply S/HIC to the case of resequencing data from human chromosome 18 in a European population sample and demonstrate that we can reliably recover selective sweeps that have been identified earlier using less specific and sensitive methods.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",100,"2022-07-13 09:40:06","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
34,"Sebastian Banescu, C. Collberg, A. Pretschner","Predicting the Resilience of Obfuscated Code Against Symbolic Execution Attacks via Machine Learning",2017,"","","","",101,"2022-07-13 09:40:06","","","","",,,,,34,6.80,11,3,5,"Software obfuscation transforms code such that it is more  difficult to reverse engineer. However, it is known that  given enough resources, an attacker will successfully reverse  engineer an obfuscated program. Therefore, an  open challenge for software obfuscation is estimating the  time an obfuscated program is able to withstand a given  reverse engineering attack. This paper proposes a general  framework for choosing the most relevant software  features to estimate the effort of automated attacks. Our  framework uses these software features to build regression  models that can predict the resilience of different  software protection transformations against automated  attacks. To evaluate the effectiveness of our approach,  we instantiate it in a case-study about predicting the time  needed to deobfuscate a set of C programs, using an attack  based on symbolic execution. To train regression  models our system requires a large set of programs as  input. We have therefore implemented a code generator  that can generate large numbers of arbitrarily complex  random C functions. Our results show that features  such as the number of community structures in the graphrepresentation  of symbolic path-constraints, are far more  relevant for predicting deobfuscation time than other features  generally used to measure the potency of controlflow  obfuscation (e.g. cyclomatic complexity). Our best  model is able to predict the number of seconds of symbolic  execution-based deobfuscation attacks with over  90% accuracy for 80% of the programs in our dataset,  which also includes several realistic hash functions.","",""
44,"Pathum Chamikara Mahawaga Arachchige, P. Bertók, I. Khalil, Dongxi Liu, S. Çamtepe, Mohammed Atiquzzaman","A Trustworthy Privacy Preserving Framework for Machine Learning in Industrial IoT Systems",2020,"","","","",102,"2022-07-13 09:40:06","","10.1109/TII.2020.2974555","","",,,,,44,22.00,7,6,2,"Industrial Internet of Things (IIoT) is revolutionizing many leading industries such as energy, agriculture, mining, transportation, and healthcare. IIoT is a major driving force for Industry 4.0, which heavily utilizes machine learning (ML) to capitalize on the massive interconnection and large volumes of IIoT data. However, ML models that are trained on sensitive data tend to leak privacy to adversarial attacks, limiting its full potential in Industry 4.0. This article introduces a framework named PriModChain that enforces privacy and trustworthiness on IIoT data by amalgamating differential privacy, federated ML, Ethereum blockchain, and smart contracts. The feasibility of PriModChain in terms of privacy, security, reliability, safety, and resilience is evaluated using simulations developed in Python with socket programming on a general-purpose computer. We used Ganache_v2.0.1 local test network for the local experiments and Kovan test network for the public blockchain testing. We verify the proposed security protocol using Scyther_v1.1.3 protocol verifier.","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",103,"2022-07-13 09:40:06","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
12,"A. N. Onodera, W. P. Gavião Neto, M. I. Roveri, W. R. Oliveira, I. Sacco","Immediate effects of EVA midsole resilience and upper shoe structure on running biomechanics: a machine learning approach",2017,"","","","",104,"2022-07-13 09:40:06","","10.7717/peerj.3026","","",,,,,12,2.40,2,5,5,"Background Resilience of midsole material and the upper structure of the shoe are conceptual characteristics that can interfere in running biomechanics patterns. Artificial intelligence techniques can capture features from the entire waveform, adding new perspective for biomechanical analysis. This study tested the influence of shoe midsole resilience and upper structure on running kinematics and kinetics of non-professional runners by using feature selection, information gain, and artificial neural network analysis. Methods Twenty-seven experienced male runners (63 ± 44 km/week run) ran in four-shoe design that combined two resilience-cushioning materials (low and high) and two uppers (minimalist and structured). Kinematic data was acquired by six infrared cameras at 300 Hz, and ground reaction forces were acquired by two force plates at 1,200 Hz. We conducted a Machine Learning analysis to identify features from the complete kinematic and kinetic time series and from 42 discrete variables that had better discriminate the four shoes studied. For that analysis, we built an input data matrix of dimensions 1,080 (10 trials × 4 shoes × 27 subjects) × 1,254 (3 joints × 3 planes of movement × 101 data points + 3 vectors forces × 101 data points + 42 discrete calculated kinetic and kinematic features). Results The applied feature selection by information gain and artificial neural networks successfully differentiated the two resilience materials using 200(16%) biomechanical variables with an accuracy of 84.8% by detecting alterations of running biomechanics, and the two upper structures with an accuracy of 93.9%. Discussion The discrimination of midsole resilience resulted in lower accuracy levels than did the discrimination of the shoe uppers. In both cases, the ground reaction forces were among the 25 most relevant features. The resilience of the cushioning material caused significant effects on initial heel impact, while the effects of different uppers were distributed along the stance phase of running. Biomechanical changes due to shoe midsole resilience seemed to be subject-dependent, while those due to upper structure seemed to be subject-independent.","",""
11,"Siddharth Gangadhar, J. Sterbenz","Machine learning aided traffic tolerance to improve resilience for software defined networks",2017,"","","","",105,"2022-07-13 09:40:06","","10.1109/RNDM.2017.8093035","","",,,,,11,2.20,6,2,5,"Software Defined Networks (SDNs) have gained prominence recently due to their flexible management and superior configuration functionality of the underlying network. SDNs, with OpenFlow as their primary implementation, allow for the use of a centralised controller to drive the decision making for all the supported devices in the network and manage traffic through routing table changes for incoming flows. In conventional networks, machine learning has been shown to detect malicious intrusion, and classify attacks such as DoS, user to root, and probe attacks. In this work, we extend the use of machine learning to improve traffic tolerance for SDNs. To achieve this, we extend the functionality of the controller to include a resilience framework, ReSDN, that incorporates machine learning to be able to distinguish DoS attacks, focussing on a neptune attack for our experiments. Our model is trained using the MIT KDD 1999 dataset. The system is developed as a module on top of the POX controller platform and evaluated using the Mininet simulator.","",""
2,"Yifeng Gao, Hosein Mohammadi Makrani, Mehrdad Aliasgari, Amin Rezaei, Jessica Lin, H. Homayoun, H. Sayadi","Adaptive-HMD: Accurate and Cost-Efficient Machine Learning-Driven Malware Detection using Microarchitectural Events",2021,"","","","",106,"2022-07-13 09:40:06","","10.1109/IOLTS52814.2021.9486701","","",,,,,2,2.00,0,7,1,"To address the high complexity and computational overheads of conventional software-based detection techniques, Hardware Malware Detection (HMD) has shown promising results as an alternative anomaly detection solution. HMD methods apply Machine Learning (ML) classifiers on microarchitectural events monitored by built-in Hardware Performance Counter (HPC) registers available in modern microprocessors to recognize the patterns of anomalies (e.g., signatures of malicious applications). Existing hardware malware detection solutions have mainly focused on utilizing standard ML algorithms to detect the existence of malware without considering an adaptive and cost-efficient approach for online malware detection. Our comprehensive analysis across a wide range of malicious software applications and different branches of machine learning algorithms indicates that the type of adopted ML algorithm to detect malicious applications at the hardware level highly correlates with the type of the examined malware, and the ultimate performance evaluation metric (F-measure, robustness, latency, detection rate/cost, etc.) to select the most efficient ML model for distinguishing the target malware from benign program. Therefore, in this work we propose Adaptive-HMD, an accurate and cost-efficient machine learning-driven framework for online malware detection using low-level microarchitectural events collected from HPC registers. Adaptive-HMD is equipped with a lightweight tree-based decision-making algorithm that accurately selects the most efficient ML model to be used for the inference in online malware detection according to the users' preference and optimal performance vs. cost (hardware overhead and latency) criteria. The experimental results demonstrate that Adaptive-HMD achieves up to 94% detection rate (F-measure) while improving the cost-efficiency of ML-based malware detection by more than 5X as compared to existing ensemble-based malware detection methods.","",""
225,"Minghong Fang, Xiaoyu Cao, Jinyuan Jia, N. Gong","Local Model Poisoning Attacks to Byzantine-Robust Federated Learning",2019,"","","","",107,"2022-07-13 09:40:06","","","","",,,,,225,75.00,56,4,3,"In federated learning, multiple client devices jointly learn a machine learning model: each client device maintains a local model for its local training dataset, while a master device maintains a global model via aggregating the local models from the client devices. The machine learning community recently proposed several federated learning methods that were claimed to be robust against Byzantine failures (e.g., system failures, adversarial manipulations) of certain client devices. In this work, we perform the first systematic study on local model poisoning attacks to federated learning. We assume an attacker has compromised some client devices, and the attacker manipulates the local model parameters on the compromised client devices during the learning process such that the global model has a large testing error rate. We formulate our attacks as optimization problems and apply our attacks to four recent Byzantine-robust federated learning methods. Our empirical results on four real-world datasets show that our attacks can substantially increase the error rates of the models learnt by the federated learning methods that were claimed to be robust against Byzantine failures of some client devices. We generalize two defenses for data poisoning attacks to defend against our local model poisoning attacks. Our evaluation results show that one defense can effectively defend against our attacks in some cases, but the defenses are not effective enough in other cases, highlighting the need for new defenses against our local model poisoning attacks to federated learning.","",""
5,"A. Hussein, A. Chehab, A. Kayssi, I. Elhajj","Machine learning for network resilience: The start of a journey",2018,"","","","",108,"2022-07-13 09:40:06","","10.1109/SDS.2018.8370423","","",,,,,5,1.25,1,4,4,"Security is one of the main concerns facing the development of new projects in networking and communications. Another challenge is to verify that a system is working exactly as specified. On the other hand, advances in Artificial Intelligence (AI) technology have opened up new markets and opportunities for progress in critical areas such as network resiliency, health, education, energy, economic inclusion, social welfare, and the environment. AI is expected to play an increasing role in defensive and offensive measures to provide a rapid response to react to the landscape of evolving threats. Software Defined Networking (SDN), being centralized by nature, provides a global view of the network. It is the flexibility and robustness offered by programmable networking that lead us to consider the integration of these two concepts, SDN and AI. Inspired by the fascinating tactics of the human immunity system, we aim to design a general hybrid Artificial Intelligence Resiliency System (ARS) that strikes a good balance between centralized and distributed security systems that may be applicable to different network environments. In addition, we aim to investigate and leverage the latest AI techniques to improve network performance in general and resiliency in particular.","",""
5,"Morgan Ekmefjord, Addi Ait-Mlouk, Sadi Alawadi, Mattias Åkesson, Desislava Stoyanova, O. Spjuth, S. Toor, A. Hellander","Scalable federated machine learning with FEDn",2021,"","","","",109,"2022-07-13 09:40:06","","","","",,,,,5,5.00,1,8,1,"to be production grade for industrial applications and a ﬂexible research tool to explore real-world performance of novel federated algorithms and the framework has been used in number of industrial and academic R&D projects. In this paper we present the architecture and implementation of FEDn. We demonstrate the framework’s scalability and efﬁciency in evaluations based on two case-studies representative for a cross-silo and a cross-device use-case respectively. recover robustly in the events of failures in the FEDn network. Resilience is vital for production grade federated machine learning. The horizontal component replication each","",""
5,"Onur Danaci, Sanjaya Lohani, B. Kirby, R. Glasser","Machine learning pipeline for quantum state estimation with incomplete measurements",2020,"","","","",110,"2022-07-13 09:40:06","","10.1088/2632-2153/abe5f5","","",,,,,5,2.50,1,4,2,"Two-qubit systems typically employ 36 projective measurements for high-fidelity tomographic estimation. The overcomplete nature of the 36 measurements suggests possible robustness of the estimation procedure to missing measurements. In this paper, we explore the resilience of machine-learning-based quantum state estimation techniques to missing measurements by creating a pipeline of stacked machine learning models for imputation, denoising, and state estimation. When applied to simulated noiseless and noisy projective measurement data for both pure and mixed states, we demonstrate quantum state estimation from partial measurement results that outperforms previously developed machine-learning-based methods in reconstruction fidelity and several conventional methods in terms of resource scaling. Notably, our developed model does not require training a separate model for each missing measurement, making it potentially applicable to quantum state estimation of large quantum systems where preprocessing is computationally infeasible due to the exponential scaling of quantum system dimension.","",""
41,"Sirui Lu, L. Duan, D. Deng","Quantum Adversarial Machine Learning",2019,"","","","",111,"2022-07-13 09:40:06","","10.1103/PHYSREVRESEARCH.2.033212","","",,,,,41,13.67,14,3,3,"Adversarial machine learning is an emerging field that focuses on studying vulnerabilities of machine learning approaches in adversarial settings and developing techniques accordingly to make learning robust to adversarial manipulations. It plays a vital role in various machine learning applications and has attracted tremendous attention across different communities recently. In this paper, we explore different adversarial scenarios in the context of quantum machine learning. We find that, similar to traditional classifiers based on classical neural networks, quantum learning systems are likewise vulnerable to crafted adversarial examples, independent of whether the input data is classical or quantum. In particular, we find that a quantum classifier that achieves nearly the state-of-the-art accuracy can be conclusively deceived by adversarial examples obtained via adding imperceptible perturbations to the original legitimate samples. This is explicitly demonstrated with quantum adversarial learning in different scenarios, including classifying real-life images (e.g., handwritten digit images in the dataset MNIST), learning phases of matter (such as, ferromagnetic/paramagnetic orders and symmetry protected topological phases), and classifying quantum data. Furthermore, we show that based on the information of the adversarial examples at hand, practical defense strategies can be designed to fight against a number of different attacks. Our results uncover the notable vulnerability of quantum machine learning systems to adversarial perturbations, which not only reveals a novel perspective in bridging machine learning and quantum physics in theory but also provides valuable guidance for practical applications of quantum classifiers based on both near-term and future quantum technologies.","",""
22,"Lal Hussain, I. Awan, W. Aziz, Sharjil Saeed, Amjad Ali, Farukh Zeeshan, K. Kwak","Detecting Congestive Heart Failure by Extracting Multimodal Features and Employing Machine Learning Techniques",2020,"","","","",112,"2022-07-13 09:40:06","","10.1155/2020/4281243","","",,,,,22,11.00,3,7,2,"The adaptability of heart to external and internal stimuli is reflected by the heart rate variability (HRV). Reduced HRV can be a predictor of negative cardiovascular outcomes. Based on the nonlinear, nonstationary, and highly complex dynamics of the controlling mechanism of the cardiovascular system, linear HRV measures have limited capability to accurately analyze the underlying dynamics. In this study, we propose an automated system to analyze HRV signals by extracting multimodal features to capture temporal, spectral, and complex dynamics. Robust machine learning techniques, such as support vector machine (SVM) with its kernel (linear, Gaussian, radial base function, and polynomial), decision tree (DT), k-nearest neighbor (KNN), and ensemble classifiers, were employed to evaluate the detection performance. Performance was evaluated in terms of specificity, sensitivity, positive predictive value (PPV), negative predictive value (NPV), and area under the receiver operating characteristic curve (AUC). The highest performance was obtained using SVM linear kernel (TA = 93.1%, AUC = 0.97, 95% CI [lower bound = 0.04, upper bound = 0.89]), followed by ensemble subspace discriminant (TA = 91.4%, AUC = 0.96, 95% CI [lower bound 0.07, upper bound = 0.81]) and SVM medium Gaussian kernel (TA = 90.5%, AUC = 0.95, 95% CI [lower bound = 0.07, upper bound = 0.86]). The results reveal that the proposed approach can provide an effective and computationally efficient tool for automatic detection of congestive heart failure patients.","",""
2,"Sarath Shekkizhar, Antonio Ortega","Revisiting Local Neighborhood Methods in Machine Learning",2021,"","","","",113,"2022-07-13 09:40:06","","10.1109/DSLW51110.2021.9523409","","",,,,,2,2.00,1,2,1,"Several machine learning methods leverage the idea of locality by using k-nearest neighbor (KNN) techniques to design better pattern recognition models. However, the choice of KNN parameters such as k is often made experimentally, e.g., via cross-validation, leading to local neighborhoods without a clear geometric interpretation. In this paper, we replace KNN with our recently introduced polytope neighborhood scheme - Non Negative Kernel regression (NNK). NNK formulates neighborhood selection as a sparse signal approximation problem and is adaptive to the local distribution of samples in the neighborhood of the data point of interest. We analyze the benefits of local neighborhood construction based on NNK. In particular, we study the generalization properties of local interpolation using NNK and present data dependent bounds in the non asymptotic setting. The applicability of NNK in transductive few shot learning setting and for measuring distance between two datasets is demonstrated. NNK exhibits robust, superior performance in comparison to standard locally weighted neighborhood methods.","",""
0,"Yifan Zhou, Peng Zhang","Quantum Machine Learning for Power System Stability Assessment",2021,"","","","",114,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,2,1,"Transient stability assessment (TSA), a cornerstone for resilient operations of today’s interconnected power grids, is a grand challenge yet to be addressed since the genesis of electric power systems. This paper is a confluence of quantum computing, data science and machine learning to potentially resolve the aforementioned challenge caused by high dimensionality, nonlinearity and uncertainty. We devise a quantum TSA (qTSA) method, a low-depth, high expressibility quantum neural network, to enable scalable and efficient data-driven transient stability prediction for bulk power systems. qTSA renders the intractable TSA straightforward and effortless in the Hilbert space, and provides rich information that enables unprecedentedly resilient and secure power system operations. Extensive experiments on quantum simulators and real quantum computers verify the accuracy, noise-resilience, scalability and universality of qTSA. qTSA underpins a solid foundation of a quantum-enabled, ultra-resilient power grid which will benefit the people as well as various commercial and industrial sectors. Introduction Texas’ and California’s rolling outages [1, 2] in recent years signaled that our existing power infrastructures can hardly sustain the ever-expanding communities and deep integration of low-inertia renewables [3, 4]. The situations are rapidly deteriorating as our power grids are increasingly integrating massive DERs, such as intermittent rooftop solar photovoltaics (PVs), as well as solar farms and offshore wind systems, and have been subject to more frequent weather events [5, 6]. A key technology to secure today’s bulk power grids is transient stability assessment (TSA) which aims to determine the ability of the system to ride-through large disturbances (contingencies) and to reach the post-contingency steady-state [7]. Transient instability is a fast phenomenon typically taking only a few seconds for the bulk system to collapse after contingencies occur. Due to this very nature, system operators in the control center never have sufficient time to steer the power system away from instability upon the occurrence of contingencies. For this reason, we have to rely on computer-based TSA without any manual interaction from human operators to assess the transient stability of the system [8]. TSA, however, is a grand challenge yet to be addressed since the genesis of power systems in the era of Tesla, Edison and Westinghouse [9]. Interconnected power systems are the largest and most complicated man-made dynamical systems on this planet. Those bulk systems are highly nonlinear, exhibit multi-scale behaviors spatially and temporarily, and are increasingly stochastic and uncertain due to deep integration of renewable energy resources. The majority of the TSA methods being used in power industry rely on the explicit integration or implicit integration of differential equation models of the bulk power systems, which are known to be intractable to handle large power systems [10–12]. Discrete events such as frequent plug-and-plays of renewables, microgrids and loads in today’s power systems [13, 14] and unknown models due to data privacy issues [15] make existing TSA methods even more computationally formidable even if they are executed on the powerful and expensive real-time simulators. Even worse, a large number of power system TSA must be conducted to examine the stability of the system in relation to massive ‘N− k’ contingencies (k components have failed in a power system with N components), which further impedes the application of TSA in real-world power system operations. All the aforementioned challenges have made classical TSA prohibitively difficult for the online operation of large interconnected grids. Today’s power systems are undergoing an Enlightenment, where the confluence of big data, quantum computing and machine learning altogether is to drive a regime shift in the analysis and operation of our critical power infrastructures. Big data is the force behind the revolution: massive new types of intelligent electronic sensors such as synchronized phasor measurement units (PMUs), advanced metering infrastructure (AMI) meters and remote terminal units (RTUs) [16] are continuously generating gigantic volumes of data which allow for the development of data-driven power system analytics. Most recently, the successes in exploiting the potential of quantum supremacy [17, 18] shed lights on a ‘quantum leap’ of computing capabilities. The power of quantum computing is derived from its ability to prepare and maintain complex superpositions of ar X iv :2 10 4. 04 85 5v 2 [ qu an tph ] 2 3 M ay 2 02 1 quantum states across many quantum degrees of freedom. While classically the number of required physical resources N grows exponentially with the system complexity n, N grows linearly with n in a quantum computer, resulting in exponential speedups over classical computing. Furthermore, highly entangled states, very difficult to represent on classical computers, are easily represented on a quantum computer [19, 20]. Therefore, the intractable power system problems aforementioned, if formulated properly through programmable quantum circuits, can be executed efficiently on a quantum computer. Inheriting the exponential speedup of quantum computing in tensor manipulation [21], the swift growth in quantum machine learning (QML) techniques [22–24] ignites new hopes of developing unprecedentedly scalable and efficient data-driven power system analytics. QML is promisingly efficacious for data processing and model training in high-dimensional space that are intractable for classical algorithms [25, 26]. Ideally, unique quantum operators such as superposition and entanglement, which can not be represented by classical operators, enable a superior representation of complicated data relationships [27–29]. Nevertheless, the existing noisy quantum devices are still restrictive, hindering the implementing of QML if deep quantum circuits are needed. This paper is the first attempt to unlock the potential of QML for power system TSA. A low-depth, high expressibility quantum neural network (QNN)-based transient stability assessment (qTSA) method is devised to enable scalable, reliable and efficient data-driven transient stability prediction. In particular, we are focusing on designing an efficient qTSA circuit that is feasible to pursue on near-term devices, considering the noisy-intermediate-scale quantum (NISQ) era [30, 31], but general enough to be directly expandable to the noise-free quantum computer of a distant future (5-10 years). We have designed systematical studies which have demonstrated the robustness, accuracy and fidelity of qTSA on real-scale power systems. Our qTSA has shown consistently high performance on quantum computers at different noise levels. Stability assessment plays a central role in nearly every field of the life sciences, physics and engineering. Our qTSA therefore is promising to positively impact the modeling, analysis, controlling and securing various high-dimensional, nonlinear, hybrid dynamical systems. In particular, qTSA underpins a solid foundation of a quantum-enabled resilient power grid, i.e., tomorrow’s unprecedentedly autonomic power infrastructure towards self-configuration, self-healing, self-optimization and selfprotection against grid changes, renewable power injections, faults, disastrous events and cyberattacks. Such a quantum-enabled grid will benefit the people as well as various commercial and industrial sectors. Results Power system transients are generically modelled as a set of nonlinear differential algebraic equations: { Ẋ = FD(X ,Y ) 0 = FA(X ,Y ) (1a) (1b) where X and Y separately denote the differential variables and algebraic variables; (1a) formulates the nonlinear dynamics of power devices, such as generators (e.g., synchronous machines, distributed energy resources), controllers (e.g., governors, exciters, inverters), power loads, etc; (1b) formulates the instantaneous power flow of the entire power grid. TSA appraises a power system’s capability of resisting large disturbance [7]. Denote Z = (X ,Y ), and φ(t,Z) as the orbit of (1) starting from Z. An asymptotically stable equilibrium point (SEP) Zs of (1) satisfies that: (a) Zs is Lyapunov stable; (b) there exists an open neighborhood O of Zs such that ∀Z ∈O converges to Zs when t approaches infinity [32]. The stability region of Zs encloses all the states that can be attracted by Zs within an infinite time: A(Zs) = {Z ∈ Rn : lim t→∞ φ(t,Z) = Zs} (2) Stability region theory states that system stability after a large disturbance is determined by whether the post-disturbance state is within the stability region of an SEP [32]. Therefore, to formulate the data-driven TSA, the idea is to establish a direct mapping between the post-disturbance power system states and the stability results [33, 34]. The keystone of qTSA, different from classical machine learning techniques, is that the transient stability features in a Euclidean space Z ∈ E are embedded into quantum states in a Hilbert space |ψ〉 ∈H through a variational quantum circuit (VQC), which serves as a QNN to explicitly separate the stable and unstable samples. Our key innovation is a design of low-depth, high expressibility qTSA, as presented in Fig. 1 (see also Methods). Fig. 1(a) first visualizes the low-depth variational qTSA circuit which addresses the dimensionality and nonlinearity obstacles in TSA as well as the nonnegligible noise and source limitations on near-term quantum devices. Kernel ingredients in qTSA circuit include (see Methods): 1) Non-Gaussian feature encoding: |ψE〉=UE(pE ,Z) |0〉 which adopts parameterized, activation-enhanced quantum gates to enable a flexible, nonlinear and dimension-free encoding of power system stability features Z;","",""
0,"Yinyihong Liu","Airbnb Pricing Based on Statistical Machine Learning Models",2021,"","","","",115,"2022-07-13 09:40:06","","10.1109/CONF-SPML54095.2021.00042","","",,,,,0,0.00,0,1,1,"Being one of the largest online accommodation booking platforms, Airbnb has many hosts who are seeking for more proper prices to increase their booking rate. To develop a good pricing prediction model, this paper has employed machine learning models including KNN, MLR, LASSO regression, Ridge regression, Random Forest, Gradient Boosting and XGBoost etc. While past studies on Airbnb pricing have applied quantitative pricing, some face the problems that the models are not robust enough and some face the problem of not training the model plentily. To fill this gap, we give careful consideration in exploratory data analysis to make the dataset more reasonable, apply many robust models ranging from regularized regression to ensemble models and use cross validation and random search to tune each parameter in each model. In this way, we not only select XGBoost as the best model for price prediction with R2 score 0.6321, but also uncover the features which have statistical significance with the target price.","",""
0,"Yan Zhou, Murat Kantarcioglu, B. Xi","A Game Theoretic Perspective on Adversarial Machine Learning and Related Cybersecurity Applications",2021,"","","","",116,"2022-07-13 09:40:06","","10.1002/9781119723950.ch13","","",,,,,0,0.00,0,3,1,"In cybersecurity applications where machine learning algorithms are increasingly used to detect vulnerabilities, a somewhat unique challenge arises as exploits targeting machine learning models are constantly devised by the attackers. Traditional machine learning models are no longer robust and reliable when they are under attack. The action and reaction between machine learning systems and the adversary can be modeled as a game between two or more players. Under well‐defined attack models, game theory can provide robustness guarantee for machine learning models that are otherwise vulnerable to application‐time data corruption. We review two cases of game theory‐based machine learning techniques: in one case, players play a zero sum game by following a minimax strategy, while in the other case, players play a sequential game with one player as the leader and the rest as the followers. Experimental results on e‐mail spam and web spam datasets are presented. In the zero sum game, we demonstrate that an adversarial SVM model built upon the minimax strategy is much more resilient to adversarial attacks than standard SVM and one‐class SVM models. We also show that optimal learning strategies derived to counter overly pessimistic attack models can produce unsatisfactory results when the real attacks are much weaker. In the sequential game, we demonstrate that the mixed strategy, allowing a player to randomize over available strategies, is the best solution in general without knowing what types of adversaries machine learning applications are facing in the wild. We also discuss scenarios where players' behavior may derail rational decision making and models that consider such decision risks.","",""
0,"Uma Gunasilan","Debate as a learning activity for teaching programming: a case in the subject of machine learning",2021,"","","","",117,"2022-07-13 09:40:06","","10.1108/heswbl-01-2021-0006","","",,,,,0,0.00,0,1,1,"PurposeDebates are well known to encompass a variety of skills we would like higher education candidates to embody when they graduate.Design/methodology/approachDebates in a classroom with computer science as the main subject has been popular in high schools particularly with emerging issues around the area, however it does not have as an extensive similar documented outreach in tertiary education, particularly in the area of hard computer sciences and more recent concentrations of computer science, such as machine learning, artificial intelligence and cloud computing.FindingsTo explore further, the debate dataset had more methodologies applied and was split into training and testing sets, whose results were then compared by a standardized measure: Root Mean Square Error (RMSE) which is currently standard in the industry. The rationale of the approach is to quantify that debate activities have an immensely positive impact towards both the teaching and learning in technical subjects and needs to be more often and robustly used within higher education.Originality/valueThe rationale of the approach is that classroom debate activities equip students with verbal and social learning styles and an opportunity to engage with content in a way that is more comfortable than working with traditional lecture-and-laboratory style learning.","",""
34,"Abhay Lokesh Kashyap, Lushan Han, Roberto Yus, Jennifer Sleeman, Taneeya Satyapanich, S. Gandhi, Timothy W. Finin","Robust semantic text similarity using LSA, machine learning, and linguistic resources",2016,"","","","",118,"2022-07-13 09:40:06","","10.1007/s10579-015-9319-2","","",,,,,34,5.67,5,7,6,"","",""
28,"Sicong Zhou, Huawei Huang, Wuhui Chen, Zibin Zheng, Song Guo","PiRATE: A Blockchain-Based Secure Framework of Distributed Machine Learning in 5G Networks",2019,"","","","",119,"2022-07-13 09:40:06","","10.1109/MNET.001.1900658","","",,,,,28,9.33,6,5,3,"in fifth-generation (5G) networks and beyond, communication latency and network bandwidth will be no longer be bottlenecks to mobile users. Thus, almost every mobile device can participate in distributed learning. That is, the availability issue of distributed learning can be eliminated. However, model safety will become a challenge. This is because the distributed learning system is prone to suffering from byzantine attacks during the stages of updating model parameters and aggregating gradients among multiple learning participants. Therefore, to provide the byzantine-resilience for distributed learning in the 5G era, this article proposes a secure computing framework based on the sharding technique of blockchain, namely PiRATE. To prove the feasibility of the proposed PiRATE, we implemented a prototype. A case study shows how the proposed PiRATE contributes to distributed learning. Finally, we also envision some open issues and challenges based on the proposed byzantine- resilient learning framework.","",""
0,"Phillip Williams, Haytham Idriss, M. Bayoumi","Mc-PUF: Memory-based and Machine Learning Resilient Strong PUF for Device Authentication in Internet of Things",2021,"","","","",120,"2022-07-13 09:40:06","","10.1109/CSR51186.2021.9527930","","",,,,,0,0.00,0,3,1,"Physically Unclonable Functions (PUFs) are hardware-based security primitives that utilize manufacturing process variations to realize binary keys (Weak PUFs) or binary functions (Strong PUFs). This primitive is desirable for key generation and authentication in constrained devices, due to its low power and low area overhead. However, in recent years many research papers are focused on the vulnerability of PUFs to modeling attacks. This attack is possible because the PUFs challenge and response exchanges are usually transmitted over communication channel without encryption. Thus, an attacker can collect challenge-response pairs and use it as input into a learning algorithm, to create a model that can predict responses given new challenges. In this paper we introduce a serial and a parallel novel 64-bits memory-based controlled PUF (Mc-PUF) architecture for device authentication that has high uniqueness and randomness, reliable, and resilient against modeling attacks. These architectures generate a response by utilizing bits extracted from the fingerprint of a synchronous random-access memory (SRAM) PUF with a control logic. The synthesis of the serial architecture yielded an area of 1.136K GE, while the parallel architecture was 3.013K GE. The best prediction accuracy obtained from the modeling attack was ~50%, which prevents an attacker from accurately predicting responses to future challenges. We also showcase the scalability of the design through XOR-ing several Mc-PUFs, further improving upon its security and performance. The remainder of the paper presents the proposed architectures, along with their hardware implementations, area and power consumption, and security resilience against modeling attacks. The 3-XOR Mc-PUF had the greatest overhead, but it produced the best randomness, uniqueness, and resilience against modeling attacks.","",""
242,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",121,"2022-07-13 09:40:06","","10.1145/3308809.3308857","","",,,,,242,48.40,81,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server andm working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of them working machines suffer Byzantine faults - a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.","",""
0,"T. Martin, S. Areibi, G. Grewal","Effective Machine-Learning Models for Predicting Routability During FPGA Placement",2021,"","","","",122,"2022-07-13 09:40:06","","10.1109/MLCAD52597.2021.9531243","","",,,,,0,0.00,0,3,1,"The ability to efficiently and accurately predict placement routability, while avoiding the large computational cost of performing routing, is an asset when seeking to reduce total placement and routing runtime. In this paper, we present a series of simple ML models and ensembles to predict the routability of a placement solution. Ensembles based on Bagging, Boosting and Stack of classifiers are introduced to produce more accurate and robust solutions than single/simple models. Our results show an improvement in prediction accuracy and runtime compared to the best published results in the literature.","",""
1,"Sam Yang, B. Vaagensmith, Deepika Patra","Power Grid Contingency Analysis with Machine Learning: A Brief Survey and Prospects",2020,"","","","",123,"2022-07-13 09:40:06","","10.1109/RWS50334.2020.9241293","","",,,,,1,0.50,0,3,2,"We briefly review previous applications of machine learning (ML) in power grid analyses and introduce our ongoing effort toward developing a generative-adversarial (GA) model for fast and reliable grid contingency analyses. According to our review, the persisting limitation of traditional ML techniques in grid analyses is the need for an exhaustive amount of training data for model generalization and accurate predictions. GA models overcome this limitation by first learning true data distribution from a small training set, from which new samples assimilating true data are generated with some variations. Subsequently, GA models can transfer learn or super-generalize with increased accuracy, that is, accurately predict n − (k + 2) contingencies from a small n − k training set and generated n − (k + 1) data. The joint effort between Idaho National Lab and Florida State University strives to develop a zero-shot and deep learning-based contingency analysis tool, named Smart Contingency Analysis Neural Network (SCANN), by leveraging the aforementioned advantages of GA models. The basic architecture of SCANN stems from the Latent Encoding of Atypical Perturbations network combined with an adversarial network, and it is designed to generate imbalanced power flow data from learned true data distributions for prediction purposes. Here we also introduce the abstract concept of resilience-chaos plots, a new resilience characterization tool proposed to complement SCANN by aiding in the assessment of large amounts of high-order contingency predictions.","",""
13,"D. Feldmeyer, C. Meisch, H. Sauter, J. Birkmann","Using OpenStreetMap Data and Machine Learning to Generate Socio-Economic Indicators",2020,"","","","",124,"2022-07-13 09:40:06","","10.3390/ijgi9090498","","",,,,,13,6.50,3,4,2,"Socio-economic indicators are key to understanding societal challenges. They disassemble complex phenomena to gain insights and deepen understanding. Specific subsets of indicators have been developed to describe sustainability, human development, vulnerability, risk, resilience and climate change adaptation. Nonetheless, insufficient quality and availability of data often limit their explanatory power. Spatial and temporal resolution are often not at a scale appropriate for monitoring. Socio-economic indicators are mostly provided by governmental institutions and are therefore limited to administrative boundaries. Furthermore, different methodological computation approaches for the same indicator impair comparability between countries and regions. OpenStreetMap (OSM) provides an unparalleled standardized global database with a high spatiotemporal resolution. Surprisingly, the potential of OSM seems largely unexplored in this context. In this study, we used machine learning to predict four exemplary socio-economic indicators for municipalities based on OSM. By comparing the predictive power of neural networks to statistical regression models, we evaluated the unhinged resources of OSM for indicator development. OSM provides prospects for monitoring across administrative boundaries, interdisciplinary topics, and semi-quantitative factors like social cohesion. Further research is still required to, for example, determine the impact of regional and international differences in user contributions on the outputs. Nonetheless, this database can provide meaningful insight into otherwise unknown spatial differences in social, environmental or economic inequalities.","",""
85,"P. Graff, F. Feroz, M. Hobson, A. Lasenby","SKYNET: an efficient and robust neural network training tool for machine learning in astronomy",2013,"","","","",125,"2022-07-13 09:40:06","","10.1093/mnras/stu642","","",,,,,85,9.44,21,4,9,"We present the first public release of our generic neural network training algorithm, called SKYNET. This efficient and robust machine-learning tool is able to train large and deep feedforward neural networks, including autoencoders, for use in a wide range of supervised and unsupervised learning applications, such as regression, classification, density estimation, clustering and dimensionality reduction. SKYNET uses a powerful ‘pre-training’ method, to obtain a set of network parameters close to the true global maximum of the training objective function, followed by further optimisation using an automatically-regularised variant of Newton’s method; the latter uses second-order derivative information to improve convergence, but without the need to evaluate or store the full Hessian matrix, by using a fast approximate method to calculate Hessian-vector products. This combination of methods allows for the training of complicated networks that are difficult to optimise using standard backpropagation techniques. SKYNET employs convergence criteria that naturally prevent overfitting, and also includes a fast algorithm for estimating the accuracy of network outputs. The utility and flexibility of SKYNET are demonstrated by application to a number of toy problems, and to astronomical problems focusing on the recovery of structure from blurred and noisy images, the identification of gamma-ray bursters, and the compression and denoising of galaxy images. The SKYNET software, which is implemented in standard ANSI C and fully parallelised using MPI, is available at http://www.mrao.cam.ac.uk/software/skynet/.","",""
87,"G. Peng, M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, Savador Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Multiscale modeling meets machine learning: What can we learn?",2019,"","","","",126,"2022-07-13 09:40:06","","10.1007/s11831-020-09405-5","","",,,,,87,29.00,9,12,3,"","",""
8,"Youness Arjoune, Fatima Salahdine, Md. Shoriful Islam, Elias Ghribi, N. Kaabouch","A Novel Jamming Attacks Detection Approach Based on Machine Learning for Wireless Communication",2020,"","","","",127,"2022-07-13 09:40:06","","10.1109/ICOIN48656.2020.9016462","","",,,,,8,4.00,2,5,2,"Jamming attacks target a wireless network creating an unwanted denial of service. 5G is vulnerable to these attacks despite its resilience prompted by the use of millimeter wave bands. Over the last decade, several types of jamming detection techniques have been proposed, including fuzzy logic, game theory, channel surfing, and time series. Most of these techniques are inefficient in detecting smart jammers. Thus, there is a great need for efficient and fast jamming detection techniques with high accuracy. In this paper, we compare the efficiency of several machine learning models in detecting jamming signals. We investigated the types of signal features that identify jamming signals, and generated a large dataset using these parameters. Using this dataset, the machine learning algorithms were trained, evaluated, and tested. These algorithms are random forest, support vector machine, and neural network. The performance of these algorithms was evaluated and compared using the probability of detection, probability of false alarm, probability of miss detection, and accuracy. The simulation results show that jamming detection based random forest algorithm can detect jammers with a high accuracy, high detection probability and low probability of false alarm.","",""
91,"M. Dyrba, M. Ewers, Martin Wegrzyn, I. Kilimann, C. Plant, Annahita Oswald, T. Meindl, M. Pievani, A. Bokde, A. Fellgiebel, M. Filippi, H. Hampel, S. Klöppel, Karlheinz Hauenstein, T. Kirste, S. Teipel","Robust Automated Detection of Microstructural White Matter Degeneration in Alzheimer’s Disease Using Machine Learning Classification of Multicenter DTI Data",2013,"","","","",128,"2022-07-13 09:40:06","","10.1371/journal.pone.0064925","","",,,,,91,10.11,9,16,9,"Diffusion tensor imaging (DTI) based assessment of white matter fiber tract integrity can support the diagnosis of Alzheimer’s disease (AD). The use of DTI as a biomarker, however, depends on its applicability in a multicenter setting accounting for effects of different MRI scanners. We applied multivariate machine learning (ML) to a large multicenter sample from the recently created framework of the European DTI study on Dementia (EDSD). We hypothesized that ML approaches may amend effects of multicenter acquisition. We included a sample of 137 patients with clinically probable AD (MMSE 20.6±5.3) and 143 healthy elderly controls, scanned in nine different scanners. For diagnostic classification we used the DTI indices fractional anisotropy (FA) and mean diffusivity (MD) and, for comparison, gray matter and white matter density maps from anatomical MRI. Data were classified using a Support Vector Machine (SVM) and a Naïve Bayes (NB) classifier. We used two cross-validation approaches, (i) test and training samples randomly drawn from the entire data set (pooled cross-validation) and (ii) data from each scanner as test set, and the data from the remaining scanners as training set (scanner-specific cross-validation). In the pooled cross-validation, SVM achieved an accuracy of 80% for FA and 83% for MD. Accuracies for NB were significantly lower, ranging between 68% and 75%. Removing variance components arising from scanners using principal component analysis did not significantly change the classification results for both classifiers. For the scanner-specific cross-validation, the classification accuracy was reduced for both SVM and NB. After mean correction, classification accuracy reached a level comparable to the results obtained from the pooled cross-validation. Our findings support the notion that machine learning classification allows robust classification of DTI data sets arising from multiple scanners, even if a new data set comes from a scanner that was not part of the training sample.","",""
1,"M. Amini, Ahmed Imteaj, J. Mohammadi","Distributed Machine Learning for Resilient Operation of Electric Systems",2020,"","","","",129,"2022-07-13 09:40:06","","10.1109/SEST48500.2020.9203368","","",,,,,1,0.50,0,3,2,"Power system resilience is crucial to ensure secure energy delivery to electricity consumers. Power system outages lead to economical and societal burdens for the society and industries. To mitigate the socio-economical impacts of a power outage, we need to develop efficient algorithms to ensure resilient operation of the power system. In this paper, we first explain the notion of data-driven resilience. Then, we present a pathway of leveraging edge intelligence to improve resilience. To this end, we propose a novel distributed machine learning paradigm. Our proposed structure relies on local Resilience Management Systems (RMS) that serve as intelligent decision-making entities in each area, e.g. an autonomous micro-grid or a smart home can act as RMS. The RMS agents, which are available in different areas, can share their local data (i.e., a microgrid's operational data) with their neighboring RMS to coordinate their decisions in a distributed fashion. This will provide two major advantages: 1) distributed intelligence replaces centralized decision-making leading to robust decision-making and enhanced resilience; 2) since local data are locally shared among all entities within an RMS, if one of the RMS agents fails to communicate with the rest of network, we still can maintain a feasible solution (which is not necessarily optimal). Finally, we presents different scenarios in the simulation results section that showcases the system performance for two buildings under various outage scenarios.","",""
105,"Zidong Du, K. Palem, L. Avinash, O. Temam, Yunji Chen, Chengyong Wu","Leveraging the error resilience of machine-learning applications for designing highly energy efficient accelerators",2014,"","","","",130,"2022-07-13 09:40:06","","10.1109/ASPDAC.2014.6742890","","",,,,,105,13.13,18,6,8,"In recent years, inexact computing has been increasingly regarded as one of the most promising approaches for reducing energy consumption in many applications that can tolerate a degree of inaccuracy. Driven by the principle of trading tolerable amounts of application accuracy in return for significant resource savings - the energy consumed, the (critical path) delay and the (silicon) area being the resources - this approach has been limited to certain application domains. In this paper, we propose to expand the application scope, error tolerance as well as the energy savings of inexact computing systems through neural network architectures. Such neural networks are fast emerging as popular candidate accelerators for future heterogeneous multi-core platforms, and have flexible error tolerance limits owing to their ability to be trained. Our results based on simulated 65nm technology designs demonstrate that the proposed inexact neural network accelerator could achieve 43.91%-62.49% savings in energy consumption (with corresponding delay and area savings being 18.79% and 31.44% respectively) when compared to existing baseline neural network implementation, at the cost of an accuracy loss (quantified as the Mean Square Error (MSE) which increases from 0.14 to 0.20 on average).","",""
1920,"A. Kurakin, Ian J. Goodfellow, Samy Bengio","Adversarial Machine Learning at Scale",2016,"","","","",131,"2022-07-13 09:40:06","","","","",,,,,1920,320.00,640,3,6,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.","",""
2,"S. Siltanen, Takanori Ide","Electrical Impedance Tomography, Enclosure Method and Machine Learning",2020,"","","","",132,"2022-07-13 09:40:06","","10.1109/MLSP49062.2020.9231717","","",,,,,2,1.00,1,2,2,"Electrical impedance tomography (EIT) is a non-destructive imaging method, where a physical body is probed with electric measurements at the boundary, and information about the internal conductivity is extracted from the data. The enclosure method of Ikehata [J. Inv. III-Posed Prob. 8(2000)] recovers the convex hull of an inclusion of unknown conductivity embedded in known background conductivity. Practical implementations of the enclosure method are based on least-squares (LS) fitting of lines to noise-robust values of the so-called indicator function. It is shown how a convolutional neural network instead of LS fitting improves the accuracy of the enclosure method significantly while retaining interpretability.","",""
0,"D. Efremenko, Himani Jain, Jian Xu","Two Machine Learning Based Schemes for Solving Direct and Inverse Problems of Radiative Transfer Theory",2020,"","","","",133,"2022-07-13 09:40:06","","10.51130/graphicon-2020-2-3-45","","",,,,,0,0.00,0,3,2,"Artificial neural networks (ANNs) are used to substitute computationally expensive radiative transfer models (RTMs) and inverse operators (IO) for retrieving optical parameters of the medium. However, the direct parametrization of RTMs and IOs by means of ANNs has certain drawbacks, such as loss of generality, computations of huge training datasets, robustness issues etc. This paper provides an analysis of different ANN-related methods, based on our results and those published by other authors. In particular, two techniques are proposed. In the first method, the ANN substitutes the eigenvalue solver in the discrete ordinate RTM, thereby reducing the computational time. Unlike classical RTM parametrization schemes based on ANN, in this method the resulting ANN can be used for arbitrary geometry and layer optical thicknesses. In the second method, the IO is trained by using the real measurements (preprocessed Level-2 TROPOMI data) to improve the stability of the inverse operator. This method provides robust results even without applying the Tikhonov regularization method.","",""
166,"M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, S. Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Integrating machine learning and multiscale modeling—perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences",2019,"","","","",134,"2022-07-13 09:40:06","","10.1038/s41746-019-0193-y","","",,,,,166,55.33,17,11,3,"","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",135,"2022-07-13 09:40:06","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
26,"Theja Tulabandhula, C. Rudin","Robust Optimization using Machine Learning for Uncertainty Sets",2014,"","","","",136,"2022-07-13 09:40:06","","","","",,,,,26,3.25,13,2,8,"Our goal is to build robust optimization problems for making decisions based on complex data from the past. In robust optimization (RO) generally, the goal is to create a policy for decision-making that is robust to our uncertainty about the future. In particular, we want our policy to best handle the the worst possible situation that could arise, out of an uncertainty set of possible situations. Classically, the uncertainty set is simply chosen by the user, or it might be estimated in overly simplistic ways with strong assumptions; whereas in this work, we learn the uncertainty set from data collected in the past. The past data are drawn randomly from an (unknown) possibly complicated high-dimensional distribution. We propose a new uncertainty set design and show how tools from statistical learning theory can be employed to provide probabilistic guarantees on the robustness of the policy.","",""
657,"Wieland Brendel, Jonas Rauber, M. Bethge","Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",2017,"","","","",137,"2022-07-13 09:40:06","","","","",,,,,657,131.40,219,3,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .","",""
101,"Andreas K Triantafyllidis, A. Tsanas","Applications of Machine Learning in Real-Life Digital Health Interventions: Review of the Literature",2019,"","","","",138,"2022-07-13 09:40:06","","10.2196/12286","","",,,,,101,33.67,51,2,3,"Background Machine learning has attracted considerable research interest toward developing smart digital health interventions. These interventions have the potential to revolutionize health care and lead to substantial outcomes for patients and medical professionals. Objective Our objective was to review the literature on applications of machine learning in real-life digital health interventions, aiming to improve the understanding of researchers, clinicians, engineers, and policy makers in developing robust and impactful data-driven interventions in the health care domain. Methods We searched the PubMed and Scopus bibliographic databases with terms related to machine learning, to identify real-life studies of digital health interventions incorporating machine learning algorithms. We grouped those interventions according to their target (ie, target condition), study design, number of enrolled participants, follow-up duration, primary outcome and whether this had been statistically significant, machine learning algorithms used in the intervention, and outcome of the algorithms (eg, prediction). Results Our literature search identified 8 interventions incorporating machine learning in a real-life research setting, of which 3 (37%) were evaluated in a randomized controlled trial and 5 (63%) in a pilot or experimental single-group study. The interventions targeted depression prediction and management, speech recognition for people with speech disabilities, self-efficacy for weight loss, detection of changes in biopsychosocial condition of patients with multiple morbidity, stress management, treatment of phantom limb pain, smoking cessation, and personalized nutrition based on glycemic response. The average number of enrolled participants in the studies was 71 (range 8-214), and the average follow-up study duration was 69 days (range 3-180). Of the 8 interventions, 6 (75%) showed statistical significance (at the P=.05 level) in health outcomes. Conclusions This review found that digital health interventions incorporating machine learning algorithms in real-life studies can be useful and effective. Given the low number of studies identified in this review and that they did not follow a rigorous machine learning evaluation methodology, we urge the research community to conduct further studies in intervention settings following evaluation principles and demonstrating the potential of machine learning in clinical practice.","",""
87,"J. Collins, K. Howe, B. Nachman","Extending the search for new resonances with machine learning",2019,"","","","",139,"2022-07-13 09:40:06","","10.1103/physrevd.99.014038","","",,,,,87,29.00,29,3,3,"The oldest and most robust technique to search for new particles is to look for ``bumps'' in invariant mass spectra over smoothly falling backgrounds. We present a new extension of the bump hunt that naturally benefits from modern machine learning algorithms while remaining model agnostic. This approach is based on the classification without labels (CWoLa) method where the invariant mass is used to create two potentially mixed samples, one with little or no signal and one with a potential resonance. Additional features that are uncorrelated with the invariant mass can be used for training the classifier. Given the lack of new physics signals at the Large Hadron Collider (LHC), such model-agnostic approaches are critical for ensuring full coverage to fully exploit the rich datasets from the LHC experiments. In addition to illustrating how the new method works in simple test cases, we demonstrate the power of the extended bump hunt on a realistic all-hadronic resonance search in a channel that would not be covered with existing techniques.","",""
33,"V. D. Florio","Antifragility = Elasticity + Resilience + Machine Learning: Models and Algorithms for Open System Fidelity",2014,"","","","",140,"2022-07-13 09:40:06","","10.1016/j.procs.2014.05.499","","",,,,,33,4.13,33,1,8,"","",""
0,"Zhang Jing, Ren Yong-gong","Robust Multi-feature Extreme Learning Machine",2017,"","","","",141,"2022-07-13 09:40:06","","10.1007/978-3-030-01520-6_13","","",,,,,0,0.00,0,2,5,"","",""
127,"Lei Zhang, D. Zhang","Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation",2016,"","","","",142,"2022-07-13 09:40:06","","10.1109/TIP.2016.2598679","","",,,,,127,21.17,64,2,6,"We address the problem of visual knowledge adaptation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine (ELM)-based cross-domain network learning framework, that is called ELM-based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the ℓ2,1-norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross-domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as the base classifiers. The network output weights cannot only be analytically determined, but also transferrable. In addition, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semisupervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition demonstrate that our EDA methods outperform the existing cross-domain learning methods.","",""
34,"O. Tuncer, E. Ates, Yijia Zhang, Ata Turk, J. Brandt, V. Leung, Manuel Egele, A. Coskun","Online Diagnosis of Performance Variation in HPC Systems Using Machine Learning",2019,"","","","",143,"2022-07-13 09:40:06","","10.1109/TPDS.2018.2870403","","",,,,,34,11.33,4,8,3,"As the size and complexity of high performance computing (HPC) systems grow in line with advancements in hardware and software technology, HPC systems increasingly suffer from performance variations due to shared resource contention as well as software- and hardware-related problems. Such performance variations can lead to failures and inefficiencies, which impact the cost and resilience of HPC systems. To minimize the impact of performance variations, one must quickly and accurately detect and diagnose the anomalies that cause the variations and take mitigating actions. However, it is difficult to identify anomalies based on the voluminous, high-dimensional, and noisy data collected by system monitoring infrastructures. This paper presents a novel machine learning based framework to automatically diagnose performance anomalies at runtime. Our framework leverages historical resource usage data to extract signatures of previously-observed anomalies. We first convert collected time series data into easy-to-compute statistical features. We then identify the features that are required to detect anomalies, and extract the signatures of these anomalies. At runtime, we use these signatures to diagnose anomalies with negligible overhead. We evaluate our framework using experiments on a real-world HPC supercomputer and demonstrate that our approach successfully identifies 98 percent of injected anomalies and consistently outperforms existing anomaly diagnosis techniques.","",""
30,"Zitao Chen, Guanpeng Li, K. Pattabiraman, Nathan Debardeleben","BinFI: an efficient fault injector for safety-critical machine learning systems",2019,"","","","",144,"2022-07-13 09:40:06","","10.1145/3295500.3356177","","",,,,,30,10.00,8,4,3,"As machine learning (ML) becomes pervasive in high performance computing, ML has found its way into safety-critical domains (e.g., autonomous vehicles). Thus the reliability of ML has grown in importance. Specifically, failures of ML systems can have catastrophic consequences, and can occur due to soft errors, which are increasing in frequency due to system scaling. Therefore, we need to evaluate ML systems in the presence of soft errors. In this work, we propose BinFI, an efficient fault injector (FI) for finding the safety-critical bits in ML applications. We find the widely-used ML computations are often monotonic. Thus we can approximate the error propagation behavior of a ML application as a monotonic function. BinFI uses a binary-search like FI technique to pinpoint the safety-critical bits (also measure the overall resilience). BinFI identifies 99.56% of safety-critical bits (with 99.63% precision) in the systems, which significantly outperforms random FI, with much lower costs.","",""
103,"Baibhab Chatterjee, D. Das, Shovan Maity, Shreyas Sen","RF-PUF: Enhancing IoT Security Through Authentication of Wireless Nodes Using In-Situ Machine Learning",2018,"","","","",145,"2022-07-13 09:40:06","","10.1109/JIOT.2018.2849324","","",,,,,103,25.75,26,4,4,"Traditional authentication in radio-frequency (RF) systems enable secure data communication within a network through techniques such as digital signatures and hash-based message authentication codes (HMAC), which suffer from key-recovery attacks. State-of-the-art Internet of Things networks such as Nest also use open authentication (OAuth 2.0) protocols that are vulnerable to cross-site-recovery forgery (CSRF), which shows that these techniques may not prevent an adversary from copying or modeling the secret IDs or encryption keys using invasive, side channel, learning or software attacks. Physical unclonable functions (PUFs), on the other hand, can exploit manufacturing process variations to uniquely identify silicon chips which makes a PUF-based system extremely robust and secure at low cost, as it is practically impossible to replicate the same silicon characteristics across dies. Taking inspiration from human communication, which utilizes inherent variations in the voice signatures to identify a certain speaker, we present RF-PUF: a deep neural network-based framework that allows real-time authentication of wireless nodes, using the effects of inherent process variation on RF properties of the wireless transmitters (Tx), detected through in-situ machine learning at the receiver (Rx) end. The proposed method utilizes the already-existing asymmetric RF communication framework and does not require any additional circuitry for PUF generation or feature extraction. The burden of device identification is completely shifted to the gateway Rx, similar to the operation of a human listener’s brain. Simulation results involving the process variations in a standard 65-nm technology node, and features such as local oscillator offset and  ${I}$ – ${Q}$  imbalance detected with a neural network having 50 neurons in the hidden layer indicate that the framework can distinguish up to 4800 Tx(s) with an accuracy of 99.9% [≈99% for 10000 Tx(s)] under varying channel conditions, and without the need for traditional preambles. The proposed scheme can be used as a stand-alone security feature, or as a part of traditional multifactor authentication.","",""
165,"S. Ardabili, A. Mosavi, Pedram Ghamisi, F. Ferdinand, A. Várkonyi-Kóczy, U. Reuter, T. Rabczuk, P. Atkinson","COVID-19 Outbreak Prediction with Machine Learning",2020,"","","","",146,"2022-07-13 09:40:06","","10.1101/2020.04.17.20070094","","",,,,,165,82.50,21,8,2,"Several outbreak prediction models for COVID-19 are being used by officials around the world to make informed-decisions and enforce relevant control measures. Among the standard models for COVID-19 global pandemic prediction, simple epidemiological and statistical models have received more attention by authorities, and they are popular in the media. Due to a high level of uncertainty and lack of essential data, standard models have shown low accuracy for long-term prediction. Although the literature includes several attempts to address this issue, the essential generalization and robustness abilities of existing models needs to be improved. This paper presents a comparative analysis of machine learning and soft computing models to predict the COVID-19 outbreak. Among a wide range of machine learning models investigated, two models showed promising results (i.e., multi-layered perceptron, MLP, and adaptive network-based fuzzy inference system, ANFIS). Based on the results reported here, and due to the highly complex nature of the COVID-19 outbreak and variation in its behavior from nation-to-nation, this study suggests machine learning as an effective tool to model the outbreak.","",""
52,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",147,"2022-07-13 09:40:06","","10.1145/3219617.3219655","","",,,,,52,10.40,17,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q łe m for an arbitrarily small but fixed constant ε>0. The parameter estimate converges in O(łog N) rounds with an estimation error on the order of max √dq/N, ~√d/N , which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q . The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log 3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
33,"V. Chernozhukov, W. Newey, Rahul Singh","Automatic Debiased Machine Learning of Causal and Structural Effects",2018,"","","","",148,"2022-07-13 09:40:06","","10.3982/ecta18515","","",,,,,33,8.25,11,3,4,"Many causal and structural effects depend on regressions. Examples include policy effects, average derivatives, regression decompositions, average treatment effects, causal mediation, and parameters of economic structural models. The regressions may be high‐dimensional, making machine learning useful. Plugging machine learners into identifying equations can lead to poor inference due to bias from regularization and/or model selection. This paper gives automatic debiasing for linear and nonlinear functions of regressions. The debiasing is automatic in using Lasso and the function of interest without the full form of the bias correction. The debiasing can be applied to any regression learner, including neural nets, random forests, Lasso, boosting, and other high‐dimensional methods. In addition to providing the bias correction, we give standard errors that are robust to misspecification, convergence rates for the bias correction, and primitive conditions for asymptotic inference for estimators of a variety of estimators of structural and causal effects. The automatic debiased machine learning is used to estimate the average treatment effect on the treated for the NSW job training data and to estimate demand elasticities from Nielsen scanner data while allowing preferences to be correlated with prices and income.","",""
1203,"Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. B. McMahan, Sarvar Patel, D. Ramage, Aaron Segal, Karn Seth","Practical Secure Aggregation for Privacy-Preserving Machine Learning",2017,"","","","",149,"2022-07-13 09:40:06","","10.1145/3133956.3133982","","",,,,,1203,240.60,134,9,5,"We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.","",""
69,"Martins Ezuma, F. Erden, C. K. Anjinappa, O. Ozdemir, I. Guvenc","Micro-UAV Detection and Classification from RF Fingerprints Using Machine Learning Techniques",2019,"","","","",150,"2022-07-13 09:40:06","","10.1109/AERO.2019.8741970","","",,,,,69,23.00,14,5,3,"This paper focuses on the detection and classification of micro-unmanned aerial vehicles (UAVs)using radio frequency (RF)fingerprints of the signals transmitted from the controller to the micro-UAV. In the detection phase, raw signals are split into frames and transformed into the wavelet domain to remove the bias in the signals and reduce the size of data to be processed. A naive Bayes approach, which is based on Markov models generated separately for UAV and non-UAV classes, is used to check for the presence of a UAV in each frame. In the classification phase, unlike the traditional approaches that rely solely on time-domain signals and corresponding features, the proposed technique uses the energy transient signal. This approach is more robust to noise and can cope with different modulation techniques. First, the normalized energy trajectory is generated from the energy-time-frequency distribution of the raw control signal. Next, the start and end points of the energy transient are detected by searching for the most abrupt changes in the mean of the energy trajectory. Then, a set of statistical features is extracted from the energy transient. Significant features are selected by performing neighborhood component analysis (NCA)to keep the computational cost of the algorithm low. Finally, selected features are fed to several machine learning algorithms for classification. The algorithms are evaluated experimentally using a database containing 100 RF signals from each of 14 different UAV controllers. The signals are recorded wirelessly using a high-frequency oscilloscope. The data set is randomly partitioned into training and test sets for validation with the ratio 4:1. Ten Monte Carlo simulations are run and results are averaged to assess the performance of the methods. All the micro-UAVs are detected correctly and an average accuracy of 96.3% is achieved using the k-nearest neighbor (kNN)classification. Proposed methods are also tested for different signal-to-noise ratio (SNR)levels and results are reported.","",""
21,"Mohammadreza Mirzahosseini, Pengcheng Jiao, Kaveh Barri, K. Riding, A. Alavi","New machine learning prediction models for compressive strength of concrete modified with glass cullet",2019,"","","","",151,"2022-07-13 09:40:06","","10.1108/EC-08-2018-0348","","",,,,,21,7.00,4,5,3,"PurposeRecycled waste glasses have been widely used in Portland cement and concrete as aggregate or supplementary cementitious material. Compressive strength is one of the most important properties of concrete containing waste glasses, providing information about the loading capacity, pozzolanic reaction and porosity of the mixture. This study aims to propose highly nonlinear models to predict the compressive strength of concrete containing finely ground glass particles.Design/methodology/approachA robust machine leaning method called genetic programming is used the build the compressive strength prediction models. The models are developed using a number of test results on 50-mm mortar cubes containing glass powder according to ASTM C109. Parametric and sensitivity analyses are conducted to evaluate the effect of the predictor variables on the compressive strength. Furthermore, a comparative study is performed to benchmark the proposed models against classical regression models.FindingsThe derived design equations accurately characterize the compressive strength of concrete with ground glass fillers and remarkably outperform the regression models. A key feature of the proposed models as compared to the previous studies is that they include the simultaneous effect of various parameters such as glass compositions, size distributions, curing age and isothermal temperatures. Parametric and sensitivity analyses indicate that compressive strength is very sensitive to the curing age, curing temperature and particle surface area.Originality/valueThis study presents accurate machine learning models for the prediction of one of the most important mechanical properties of cementitious mixtures modified by waste glass, i.e. compressive strength. In addition, it provides an insight into the effect of several parameters influencing the compressive strength. From a computing perspective, a robust machine learning technique that overcomes the shortcomings of existing soft computing methods is introduced.","",""
76,"Yicheng Wang, Mohit Bansal","Robust Machine Comprehension Models via Adversarial Training",2018,"","","","",152,"2022-07-13 09:40:06","","10.18653/v1/N18-2091","","",,,,,76,19.00,38,2,4,"It is shown that many published models for the Stanford Question Answering Dataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50% decrease in F1 score during adversarial evaluation based on the AddSent (Jia and Liang, 2017) algorithm. It has also been shown that retraining models on data generated by AddSent has limited effect on their robustness. We propose a novel alternative adversary-generation algorithm, AddSentDiverse, that significantly increases the variance within the adversarial training data by providing effective examples that punish the model for making certain superficial assumptions. Further, in order to improve robustness to AddSent’s semantic perturbations (e.g., antonyms), we jointly improve the model’s semantic-relationship learning capabilities in addition to our AddSentDiverse-based adversarial training data augmentation. With these additions, we show that we can make a state-of-the-art model significantly more robust, achieving a 36.5% increase in F1 score under many different types of adversarial evaluation while maintaining performance on the regular SQuAD task.","",""
6,"Rudrasis Chakraborty, Liu Yang, Søren Hauberg, B. Vemuri","Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear Subspace Learning",2017,"","","","",153,"2022-07-13 09:40:06","","10.1109/tpami.2020.2992392","","",,,,,6,1.20,2,4,5,"Principal component analysis (PCA) and Kernel principal component analysis (KPCA) are fundamental methods in machine learning for dimensionality reduction. The former is a technique for finding this approximation in finite dimensions and the latter is often in an infinite dimensional reproducing Kernel Hilbert-space (RKHS). In this paper, we present a geometric framework for computing the principal linear subspaces in both (finite and infinite) situations as well as for the robust PCA case, that amounts to computing the intrinsic average on the space of all subspaces: the Grassmann manifold. Points on this manifold are defined as the subspaces spanned by <inline-formula><tex-math notation=""LaTeX"">$K$</tex-math><alternatives><mml:math><mml:mi>K</mml:mi></mml:math><inline-graphic xlink:href=""chakraborty-ieq1-2992392.gif""/></alternatives></inline-formula>-tuples of observations. The intrinsic Grassmann average of these subspaces are shown to coincide with the principal components of the observations when they are drawn from a Gaussian distribution. We show similar results in the RKHS case and provide an efficient algorithm for computing the projection onto the this average subspace. The result is a method akin to KPCA which is substantially faster. Further, we present a novel online version of the KPCA using our geometric framework. Competitive performance of all our algorithms are demonstrated on a variety of real and synthetic data sets.","",""
13,"M. Grassia, M. Domenico, G. Mangioni","Machine learning dismantling and early-warning signals of disintegration in complex systems",2021,"","","","",154,"2022-07-13 09:40:06","","10.1038/s41467-021-25485-8","","",,,,,13,13.00,4,3,1,"","",""
2,"Jiyuan Tu, Weidong Liu, Xiaojun Mao","Byzantine-robust distributed sparse learning for M-estimation",2021,"","","","",155,"2022-07-13 09:40:06","","10.1007/S10994-021-06001-X","","",,,,,2,2.00,1,3,1,"","",""
0,"A. Shamis, P. Pietzuch, Antoine Delignat-Lavaud, Andrew J. Paverd, Manuel Costa","Dropbear: Machine Learning Marketplaces made Trustworthy with Byzantine Model Agreement",2022,"","","","",156,"2022-07-13 09:40:06","","10.48550/arXiv.2205.15757","","",,,,,0,0.00,0,5,1,"Marketplaces for machine learning (ML) models are emerging as a way for organizations to monetize models. They allow model owners to retain control over hosted models by using cloud resources to execute ML inference requests for a fee, preserving model confidentiality. Clients that rely on hosted models require trustworthy inference results, even when models are managed by third parties. While the resilience and robustness of inference results can be improved by combining multiple independent models, such support is unavailable in today’s marketplaces. We describe Dropbear, the first ML model marketplace that provides clients with strong integrity guarantees by combining results from multiple models in a trustworthy fashion. Dropbear replicates inference computation across a model group, which consists of multiple cloud-based GPU nodes belonging to different model owners. Clients receive inference certificates that prove agreement using a Byzantine consensus protocol, even under model heterogeneity and concurrent model updates. To improve performance, Dropbear batches inference and consensus operations separately: it first performs the inference computation across a model group, before ordering requests and model updates. Despite its strong integrity guarantees, Dropbear’s performance matches that of state-ofthe-art ML inference systems: deployed across 3 cloud sites, it handles 800 requests/s with ImageNet models.","",""
0,"Yiyang Chen, Wei Jiang, Themistoklis Charalambous","Machine learning based iterative learning control for non-repetitive time-varying systems",2021,"","","","",157,"2022-07-13 09:40:06","","10.1002/rnc.6272","","",,,,,0,0.00,0,3,1,"The repetitive tracking task for time-varying systems (TVSs) with non-repetitive time-varying parameters, which is also called non-repetitive TVSs, is realized in this paper using iterative learning control (ILC). A machine learning (ML) based nominal model update mechanism, which utilizes the linear regression technique to update the nominal model at each ILC trial only using the current trial information, is proposed for non-repetitive TVSs in order to enhance the ILC performance. Given that the ML mechanism forces the model uncertainties to remain within the ILC robust tolerance, an ILC update law is proposed to deal with non-repetitive TVSs. How to tune parameters inside ML and ILC algorithms to achieve the desired aggregate performance is also provided. The robustness and reliability of the proposed method are verified by simulations. Comparison with current state-of-the-art demonstrates its superior control performance in terms of controlling precision. This paper broadens ILC applications from time-invariant systems to non-repetitive TVSs, adopts ML regression technique to estimate non-repetitive time-varying parameters between two ILC trials and proposes a detailed parameter tuning mechanism to achieve desired performance, which are the main contributions.","",""
0,"F. Marulli, S. Marrone, Laura Verde","Sensitivity of Machine Learning Approaches to Fake and Untrusted Data in Healthcare Domain",2022,"","","","",158,"2022-07-13 09:40:06","","10.3390/jsan11020021","","",,,,,0,0.00,0,3,1,"Machine Learning models are susceptible to attacks, such as noise, privacy invasion, replay, false data injection, and evasion attacks, which affect their reliability and trustworthiness. Evasion attacks, performed to probe and identify potential ML-trained models’ vulnerabilities, and poisoning attacks, performed to obtain skewed models whose behavior could be driven when specific inputs are submitted, represent a severe and open issue to face in order to assure security and reliability to critical domains and systems that rely on ML-based or other AI solutions, such as healthcare and justice, for example. In this study, we aimed to perform a comprehensive analysis of the sensitivity of Artificial Intelligence approaches to corrupted data in order to evaluate their reliability and resilience. These systems need to be able to understand what is wrong, figure out how to overcome the resulting problems, and then leverage what they have learned to overcome those challenges and improve their robustness. The main research goal pursued was the evaluation of the sensitivity and responsiveness of Artificial Intelligence algorithms to poisoned signals by comparing several models solicited with both trusted and corrupted data. A case study from the healthcare domain was provided to support the pursued analyses. The results achieved with the experimental campaign were evaluated in terms of accuracy, specificity, sensitivity, F1-score, and ROC area.","",""
0,"Harsh Chaudhari, Matthew Jagielski, Alina Oprea","SafeNet: Mitigating Data Poisoning Attacks on Private Machine Learning",2022,"","","","",159,"2022-07-13 09:40:06","","10.48550/arXiv.2205.09986","","",,,,,0,0.00,0,3,1,"—Secure multiparty computation (MPC) has been proposed to allow multiple mutually distrustful data owners to jointly train machine learning (ML) models on their combined data. However, the datasets used for training ML models might be under the control of an adversary mounting a data poisoning attack, and MPC prevents inspecting training sets to detect poisoning. We show that multiple MPC frameworks for private ML training are susceptible to backdoor and targeted poisoning attacks. To mitigate this, we propose SafeNet, a framework for building ensemble models in MPC with formal guarantees of robustness to data poisoning attacks. We extend the security deﬁnition of private ML training to account for poisoning and prove that our SafeNet design satisﬁes the deﬁnition. We demonstrate SafeNet’s efﬁciency, accuracy, and resilience to poisoning on several machine learning datasets and models. For instance, SafeNet reduces backdoor attack success from 100% to 0% for a neural network model, while achieving 39 × faster training and 36 × less communication than the four-party MPC framework of Dalskov et al. [26].","",""
0,"Naiara Escudero, P. Costas, Michael W. Hardt, G. Inalhan","Machine Learning Based Visual Navigation System Architecture for Aam Operations with A Discussion on its Certifiability",2022,"","","","",160,"2022-07-13 09:40:06","","10.1109/ICNS54818.2022.9771519","","",,,,,0,0.00,0,4,1,"Advanced Air Mobility (AAM) is expected to revolutionize the future of general transportation expanding the conventional notion of air traffic to include several services carried out by autonomous aerial platforms. However, the significant challenges associated with such complex scenarios require the introduction of sophisticated technologies able to deliver the resilience, robustness, and accuracy needed to achieve safe, autonomous operations [39]. In this context, solutions based on Artificial Intelligence (AI), able to overcome some limitations found in traditional approaches, are becoming a major opportunity for the aviation industry, but, at the same time, a significant challenge with respect to the certification standards.With the focal point on further proposing a certifiable architecture for AI-enhanced vision navigation in AAM operations, this paper first, summarizes the current technologies and fusion methods applied to date to navigation purposes, to later address the certification problem. Regarding certification, it explores three specific points: 1) traditional certification procedures; 2) current status of AI homologation recommendations; and 3) other certification factors to be considered for future discussion.","",""
12,"S. M. Rasel, Hsing-chung Chang, T. Ralph, N. Saintilan, I. J. Diti","Application of feature selection methods and machine learning algorithms for saltmarsh biomass estimation using Worldview-2 imagery",2019,"","","","",161,"2022-07-13 09:40:06","","10.1080/10106049.2019.1624988","","",,,,,12,4.00,2,5,3,"Abstract Assessing large scale plant productivity of coastal marshes is essential to understand the resilience of these systems to climate change. Two machine learning approaches, random forest (RF) and support vector machine (SVM) regression were tested to estimate biomass of a common saltmarshes species, salt couch grass (Sporobolus virginicus). Reflectance and vegetation indices derived from 8 bands of Worldview-2 multispectral data were used for four experiments to develop the biomass model. These four experiments were, Experiment-1: 8 bands of Worldview-2 image, Experiment-2: Possible combination of all bands of Worldview-2 for Normalized Difference Vegetation Index (NDVI) type vegetation indices, Experiment-3: Combination of bands and vegetation indices, Experiment-4: Selected variables derived from experiment-3 using variable selection methods. The main objectives of this study are (i) to recommend an affordable low cost data source to predict biomass of a common saltmarshes species, (ii) to suggest a variable selection method suitable for multispectral data, (iii) to assess the performance of RF and SVM for the biomass prediction model. Cross-validation of parameter optimizations for SVM showed that optimized parameter of ɛ-SVR failed to provide a reliable prediction. Hence, ν-SVR was used for the SVM model. Among the different variable selection methods, recursive feature elimination (RFE) selected a minimum number of variables (only 4) with an RMSE of 0.211 (kg/m2). Experiment-4 (only selected bands) provided the best results for both of the machine learning regression methods, RF (R 2= 0.72, RMSE= 0.166 kg/m2) and SVR (R 2= 0.66, RMSE = 0.200 kg/m2) to predict biomass. When a 10-fold cross validation of the RF model was compared with a 10-fold cross validation of SVR, a significant difference (p = <0.0001) was observed for RMSE. One to one comparisons of actual to predicted biomass showed that RF underestimates the high biomass values, whereas SVR overestimates the values; this suggests a need for further investigation and refinement.","",""
117,"Xiao Chen, Chaoran Li, Derui Wang, S. Wen, Jun Zhang, S. Nepal, Yang Xiang, K. Ren","Android HIV: A Study of Repackaging Malware for Evading Machine-Learning Detection",2018,"","","","",162,"2022-07-13 09:40:06","","10.1109/TIFS.2019.2932228","","",,,,,117,29.25,15,8,4,"Machine learning-based solutions have been successfully employed for the automatic detection of malware on Android. However, machine learning models lack robustness to adversarial examples, which are crafted by adding carefully chosen perturbations to the normal inputs. So far, the adversarial examples can only deceive detectors that rely on syntactic features (e.g., requested permissions, API calls, etc.), and the perturbations can only be implemented by simply modifying application’s manifest. While recent Android malware detectors rely more on semantic features from Dalvik bytecode rather than manifest, existing attacking/defending methods are no longer effective. In this paper, we introduce a new attacking method that generates adversarial examples of Android malware and evades being detected by the current models. To this end, we propose a method of applying optimal perturbations onto Android APK that can successfully deceive the machine learning detectors. We develop an automated tool to generate the adversarial examples without human intervention. In contrast to existing works, the adversarial examples crafted by our method can also deceive recent machine learning-based detectors that rely on semantic features such as control-flow-graph. The perturbations can also be implemented directly onto APK’s Dalvik bytecode rather than Android manifest to evade from recent detectors. We demonstrate our attack on two state-of-the-art Android malware detection schemes, MaMaDroid and Drebin. Our results show that the malware detection rates decreased from 96% to 0% in MaMaDroid, and from 97% to 0% in Drebin, with just a small number of codes to be inserted into the APK.","",""
102,"Peng Xu, Farbod Roosta-Khorasani, Michael W. Mahoney","Second-Order Optimization for Non-Convex Machine Learning: An Empirical Study",2017,"","","","",163,"2022-07-13 09:40:06","","10.1137/1.9781611976236.23","","",,,,,102,20.40,34,3,5,"While first-order optimization methods such as stochastic gradient descent (SGD) are popular in machine learning (ML), they come with well-known deficiencies, including relatively-slow convergence, sensitivity to the settings of hyper-parameters such as learning rate, stagnation at high training errors, and difficulty in escaping flat regions and saddle points. These issues are particularly acute in highly non-convex settings such as those arising in neural networks. Motivated by this, there has been recent interest in second-order methods that aim to alleviate these shortcomings by capturing curvature information. In this paper, we report detailed empirical evaluations of a class of Newton-type methods, namely sub-sampled variants of trust region (TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex ML problems. In doing so, we demonstrate that these methods not only can be computationally competitive with hand-tuned SGD with momentum, obtaining comparable or better generalization performance, but also they are highly robust to hyper-parameter settings. Further, in contrast to SGD with momentum, we show that the manner in which these Newton-type methods employ curvature information allows them to seamlessly escape flat regions and saddle points.","",""
80,"N. Ball, R. Brunner, A. Myers, N. E. Strand, S. Alberts, D. T. D. O. Astronomy, U. I. Urbana-Champaign, National Center for Supercomputing Applications, D. Physics","Robust Machine Learning Applied to Astronomical Data Sets. III. Probabilistic Photometric Redshifts for Galaxies and Quasars in the SDSS and GALEX",2008,"","","","",164,"2022-07-13 09:40:06","","10.1086/589646","","",,,,,80,5.71,9,9,14,"We apply machine learning in the form of a nearest neighbor instance-based algorithm (NN) to generate full photometric redshift probability density functions (PDFs) for objects in the Fifth Data Release of the Sloan Digital Sky Survey (SDSS DR5). We use a conceptually simple but novel application of NN to generate the PDFs, perturbing the object colors by their measurement error and using the resulting instances of nearest neighbor distributions to generate numerous individual redshifts. When the redshifts are compared to existing SDSS spectroscopic data, we find that the mean value of each PDF has a dispersion between the photometric and spectroscopic redshift consistent with other machine learning techniques, being σ = 0.0207 ± 0.0001 for main sample galaxies to r < 17.77 mag, σ = 0.0243 ± 0.0002 for luminous red galaxies to r 19.2 mag, and σ = 0.343 ± 0.005 for quasars to i < 20.3 mag. The PDFs allow the selection of subsets with improved statistics. For quasars, the improvement is dramatic: for those with a single peak in their probability distribution, the dispersion is reduced from 0.343 to σ = 0.117 ± 0.010, and the photometric redshift is within 0.3 of the spectroscopic redshift for 99.3% ± 0.1% of the objects. Thus, for this optical quasar sample, we can virtually eliminate ""catastrophic"" photometric redshift estimates. In addition to the SDSS sample, we incorporate ultraviolet photometry from the Third Data Release of the Galaxy Evolution Explorer All-Sky Imaging Survey (GALEX AIS GR3) to create PDFs for objects seen in both surveys. For quasars, the increased coverage of the observed-frame UV of the SED results in significant improvement over the full SDSS sample, with σ = 0.234 ± 0.010. We demonstrate that this improvement is genuine and not an artifact of the SDSS-GALEX matching process.","",""
8,"Issam Hammad, K. El-Sankary","Practical Considerations for Accuracy Evaluation in Sensor-Based Machine Learning and Deep Learning",2019,"","","","",165,"2022-07-13 09:40:06","","10.3390/s19163491","","",,,,,8,2.67,4,2,3,"Accuracy evaluation in machine learning is based on the split of data into a training set and a test set. This critical step is applied to develop machine learning models including models based on sensor data. For sensor-based problems, comparing the accuracy of machine learning models using the train/test split provides only a baseline comparison in ideal situations. Such comparisons won’t consider practical production problems that can impact the inference accuracy such as the sensors’ thermal noise, performance with lower inference quantization, and tolerance to sensor failure. Therefore, this paper proposes a set of practical tests that can be applied when comparing the accuracy of machine learning models for sensor-based problems. First, the impact of the sensors’ thermal noise on the models’ inference accuracy was simulated. Machine learning algorithms have different levels of error resilience to thermal noise, as will be presented. Second, the models’ accuracy using lower inference quantization was compared. Lowering inference quantization leads to lowering the analog-to-digital converter (ADC) resolution which is cost-effective in embedded designs. Moreover, in custom designs, analog-to-digital converters’ (ADCs) effective number of bits (ENOB) is usually lower than the ideal number of bits due to various design factors. Therefore, it is practical to compare models’ accuracy using lower inference quantization. Third, the models’ accuracy tolerance to sensor failure was evaluated and compared. For this study, University of California Irvine (UCI) ‘Daily and Sports Activities’ dataset was used to present these practical tests and their impact on model selection.","",""
519,"P. Blanchard, El Mahdi El Mhamdi, R. Guerraoui, J. Stainer","Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent",2017,"","","","",166,"2022-07-13 09:40:06","","","","",,,,,519,103.80,130,4,5,"We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of n workers, up to f being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite f Byzantine workers. We propose Krum, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum.","",""
3,"Arunkumar Vijayan, K. Chakrabarty, M. Tahoori","Machine Learning-Based Aging Analysis",2019,"","","","",167,"2022-07-13 09:40:06","","10.1007/978-3-030-04666-8_9","","",,,,,3,1.00,1,3,3,"","",""
6,"Marc-Oliver Pahl, Stefan Liebald, Lars Wustrich","Machine-Learning based IoT Data Caching",2019,"","","","",168,"2022-07-13 09:40:06","","","","",,,,,6,2.00,2,3,3,"The Internet of Things (IoT) continuously produces big amounts of data. Data-centric middleware can therefore help reducing the complexity when orchestrating distributed Things. With its heterogeneity and resource limitations, IoT applications can lack performance, scalability, or resilience. Caching can help overcoming the limitations.We are currently working on establishing data caching within IoT middleware. The paper presents fundamentals of caching, major challenges, relevant state of the art, and a description of our current approaches. We show directions of using machine learning for caching in the IoT.","",""
0,"Konstantinos Konstantinidis, A. Ramamoorthy","Aspis: A Robust Detection System for Distributed Learning",2021,"","","","",169,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,2,1,"State of the art machine learning models are routinely trained on large scale distributed clusters. Crucially, such systems can be compromised when some of the computing devices exhibit abnormal (Byzantine) behavior and return arbitrary results to the parameter server (PS). This behavior may be attributed to a plethora of reasons including system failures and orchestrated attacks. Existing work suggests robust aggregation and/or computational redundancy to alleviate the effect of distorted gradients. However, most of these schemes are ineffective when an adversary knows the task assignment and can judiciously choose the attacked workers to induce maximal damage. Our proposed method Aspis assigns gradient computations to worker nodes using a subset-based assignment which allows for multiple consistency checks on the behavior of a worker node. Examination of the calculated gradients and post-processing (clique-finding in an appropriately constructed graph) by the central node allows for efficient detection and subsequent exclusion of adversaries from the training process. We prove the Byzantine resilience and detection guarantees of Aspis under weak and strong attacks and extensively evaluate the system on various large-scale training scenarios. The main metric for our experiments is the test accuracy for which we demonstrate significant improvement of about 30% compared to many state-of-the-art approaches on the CIFAR-10 dataset. The corresponding reduction of the fraction of corrupted gradients ranges from 16% to 98%. Disciplines Artificial Intelligence and Robotics Comments This is a pre-print of the article Konstantinidis, Konstantinos, and Aditya Ramamoorthy. ""Aspis: A Robust Detection System for Distributed Learning."" arXiv preprint arXiv:2108.02416 (2021). Posted with permission. Creative Commons License This work is licensed under a Creative Commons Attribution 4.0 International License. This article is available at Iowa State University Digital Repository: https://lib.dr.iastate.edu/ece_pubs/314 Aspis: A Robust Detection System for Distributed Learning Konstantinos Konstantinidis Department of Electrical and Computer Engineering Iowa State University Ames, IA 50011 USA kostas@iastate.edu Aditya Ramamoorthy Department of Electrical and Computer Engineering Iowa State University Ames, IA 50011 USA adityar@iastate.edu","",""
1,"Ramkumar Harikrishnakumar, A. Dand, S. Nannapaneni, K. Krishnan","Supervised Machine Learning Approach for Effective Supplier Classification",2019,"","","","",170,"2022-07-13 09:40:06","","10.1109/ICMLA.2019.00045","","",,,,,1,0.33,0,4,3,"Supplier assessment plays a critical role in the supply chain management, which involves the flow of goods and services from the initial stage (raw material procurement) to the final stage (delivery). Supplier assessment is a multi-criteria decision-making (MCDM) approach that requires several criteria for the proper assessment of the suppliers. When there are several criteria involved, it makes the supplier assessment process more complicated. For a comprehensive and robust assessment process, we propose the use of supervised machine learning algorithms to classify various suppliers into four categories: excellent, good, satisfactory, and unsatisfactory. In this paper, supervised learning (classification) algorithms are applied for a supplier assessment problem where a model is trained based on the previous historical data and then tested on the new unseen data set. This method will provide an efficient way for supplier assessment that is more effective in terms of accuracy and time when compared to MCDM approach. Classification algorithms such as support vector machines (with linear, polynomial and radial basis kernels), logistic regression, k-nearest neighbors, and naïve Bayes methods are used to train the model and their performance is assessed against a test data. Finally, the performance measures from all the classification methods are used to assess the best supplier.","",""
104,"Yangkang Zhang","Automatic microseismic event picking via unsupervised machine learning",2020,"","","","",171,"2022-07-13 09:40:06","","10.1093/GJI/GGX420","","",,,,,104,52.00,104,1,2,"  Effective and efficient arrival picking plays an important role in microseismic and earthquake data processing and imaging. Widely used short-term-average long-term-average ratio (STA/LTA) based arrival picking algorithms suffer from the sensitivity to moderate-to-strong random ambient noise. To make the state-of-the-art arrival picking approaches effective, microseismic data need to be first pre-processed, for example, removing sufficient amount of noise, and second analysed by arrival pickers. To conquer the noise issue in arrival picking for weak microseismic or earthquake event, I leverage the machine learning techniques to help recognizing seismic waveforms in microseismic or earthquake data. Because of the dependency of supervised machine learning algorithm on large volume of well-designed training data, I utilize an unsupervised machine learning algorithm to help cluster the time samples into two groups, that is, waveform points and non-waveform points. The fuzzy clustering algorithm has been demonstrated to be effective for such purpose. A group of synthetic, real microseismic and earthquake data sets with different levels of complexity show that the proposed method is much more robust than the state-of-the-art STA/LTA method in picking microseismic events, even in the case of moderately strong background noise.","",""
74,"Monika A. Myszczynska, P. Ojamies, Alix M. B. Lacoste, Daniel Neil, Amir Saffari, R. Mead, G. Hautbergue, J. Holbrook, L. Ferraiuolo","Applications of machine learning to diagnosis and treatment of neurodegenerative diseases",2020,"","","","",172,"2022-07-13 09:40:06","","10.1038/s41582-020-0377-8","","",,,,,74,37.00,8,9,2,"","",""
78,"Xianfang Wang, Peng Gao, Yifeng Liu, Hongfei Li, Fan Lu","Predicting Thermophilic Proteins by Machine Learning",2020,"","","","",173,"2022-07-13 09:40:06","","10.2174/1574893615666200207094357","","",,,,,78,39.00,16,5,2,"  Thermophilic proteins can maintain good activity under high temperature, therefore, it is important to study thermophilic proteins for the thermal stability of proteins.    In order to solve the problem of low precision and low efficiency in predicting thermophilic proteins, a prediction method based on feature fusion and machine learning was proposed in this paper.    For the selected thermophilic data sets, firstly, the thermophilic protein sequence was characterized based on feature fusion by the combination of g-gap dipeptide, entropy density and autocorrelation coefficient. Then, Kernel Principal Component Analysis (KPCA) was used to reduce the dimension of the expressed protein sequence features in order to reduce the training time and improve efficiency. Finally, the classification model was designed by using the classification algorithm.    A variety of classification algorithms was used to train and test on the selected thermophilic dataset. By comparison, the accuracy of the Support Vector Machine (SVM) under the jackknife method was over 92%. The combination of other evaluation indicators also proved that the SVM performance was the best.     Because of choosing an effectively feature representation method and a robust classifier, the proposed method is suitable for predicting thermophilic proteins and is superior to most reported methods. ","",""
0,"Shuo Liu, Nirupam Gupta, N. Vaidya","Utilizing Redundancy in Cost Functions for Resilience in Distributed Optimization and Learning",2021,"","","","",174,"2022-07-13 09:40:06","","","","",,,,,0,0.00,0,3,1,"This paper considers the problem of resilient distributed optimization and stochastic machine learning in a server-based architecture. The system comprises a server and multiple agents, where each agent has a local cost function. The agents collaborate with the server to find a minimum of their aggregate cost functions. We consider the case when some of the agents may be asynchronous and/or Byzantine faulty. In this case, the classical algorithm of distributed gradient descent (DGD) is rendered ineffective. Our goal is to design techniques improving the efficacy of DGD with asynchrony and Byzantine failures. To do so, we start by proposing a way to model the agents’ cost functions by the generic notion of (f, r; )-redundancy where f and r are the parameters of Byzantine failures and asynchrony, respectively, and characterizes the closeness between agents’ cost functions. This allows us to quantify the level of redundancy present amongst the agents’ cost functions, for any given distributed optimization problem. We demonstrate, both theoretically and empirically, the merits of our proposed redundancy model in improving the robustness of DGD against asynchronous and Byzantine agents, and their extensions to distributed stochastic gradient descent (D-SGD) for robust distributed machine learning with asynchronous and Byzantine agents. This report supersedes our previous report [36] as it contains the most of the results in it. Georgetown University. Email: sl1539@georgetown.edu. École Polytechnique Fédérale de Lausanne (EPFL). Email: nirupam.gupta@epfl.ch. Georgetown University. Email: nitin.vaidya@georgetown.edu. 1 ar X iv :2 11 0. 10 85 8v 1 [ cs .D C ] 2 1 O ct 2 02 1","",""
0,"Muhammad Abdullah Hanif, R. Hafiz, M. Javed, Semeen Rehman, M. Shafique","Energy-Efficient Design of Advanced Machine Learning Hardware",2019,"","","","",175,"2022-07-13 09:40:06","","10.1007/978-3-030-04666-8_21","","",,,,,0,0.00,0,5,3,"","",""
46,"F. Yuan, S. A. Zargar, Qiuyi Chen, Shaohan Wang","Machine learning for structural health monitoring: challenges and opportunities",2020,"","","","",176,"2022-07-13 09:40:06","","10.1117/12.2561610","","",,,,,46,23.00,12,4,2,"A physics-based approach to structural health monitoring (SHM) has practical shortcomings which restrict its suitability to simple structures under well controlled environments. With the advances in information and sensing technology (sensors and sensor networks), it has become feasible to monitor large/diverse number of parameters in complex real-world structures either continuously or intermittently by employing large in-situ (wireless) sensor networks. The availability of this historical data has engendered a lot of interest in a data-driven approach as a natural and more viable option for realizing the goal of SHM in such structures. However, the lack of sensor data corresponding to different damage scenarios continues to remain a challenge. Most of the supervised machine-learning/deep-learning techniques, when trained using this inherently limited data, lack robustness and generalizability. Physics-informed learning, which involves the integration of domain knowledge into the learning process, is presented here as a potential remedy to this challenge. As a step towards the goal of automated damage detection (mathematically an inverse problem), preliminary results are presented from dynamic modelling of beam structures using physics-informed artificial neural networks. Forward and inverse problems involving partial differential equations are solved and comparisons reveal a clear superiority of physics-informed approach over one that is purely datadriven vis-à-vis overfitting/generalization. Other ways of incorporating domain knowledge into the machine learning pipeline are then presented through case-studies on various aspects of NDI/SHM (visual inspection, impact diagnosis). Lastly, as the final attribute of an optimal SHM approach, a sensing paradigm for non-contact full-field measurements for damage diagnosis is presented.","",""
47,"N. Ball, R. Brunner, A. Myers, N. E. Strand, S. Alberts, D. Tcheng, X. L. D. O. Astronomy, U. I. Urbana-Champaign, National Center for Supercomputing Applications, D. Physics","Robust Machine Learning Applied to Astronomical Data Sets. II. Quantifying Photometric Redshifts for Quasars Using Instance-based Learning",2006,"","","","",177,"2022-07-13 09:40:06","","10.1086/518362","","",,,,,47,2.94,5,10,16,"We apply instance-based machine learning in the form of a k-nearest neighbor algorithm to the task of estimating photometric redshifts for 55,746 objects spectroscopically classified as quasars in the Fifth Data Release of the Sloan Digital Sky Survey. We compare the results obtained to those from an empirical color-redshift relation (CZR). In contrast to previously published results using CZRs, we find that the instance-based photometric redshifts are assigned with no regions of catastrophic failure. Remaining outliers are simply scattered about the ideal relation, in a manner similar to the pattern seen in the optical for normal galaxies at redshifts z 1. The instance-based algorithm is trained on a representative sample of the data and pseudo-blind-tested on the remaining unseen data. The variance between the photometric and spectroscopic redshifts is σ2 = 0.123 ± 0.002 (compared to σ2 = 0.265 ± 0.006 for the CZR), and 54.9% ± 0.7%, 73.3% ± 0.6%, and 80.7% ± 0.3% of the objects are within Δz < 0.1, 0.2, and 0.3, respectively. We also match our sample to the Second Data Release of the Galaxy Evolution Explorer legacy data, and the resulting 7642 objects show a further improvement, giving a variance of σ2 = 0.054 ± 0.005, with 70.8% ± 1.2%, 85.8% ± 1.0%, and 90.8% ± 0.7% of objects within Δz < 0.1, 0.2, and 0.3. We show that the improvement is indeed due to the extra information provided by GALEX, by training on the same data set using purely SDSS photometry, which has a variance of σ2 = 0.090 ± 0.007. Each set of results represents a realistic standard for application to further data sets for which the spectra are representative.","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",178,"2022-07-13 09:40:06","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
29,"Yuhui Zheng, Le Sun, Shunfeng Wang, Jianwei Zhang, J. Ning","Spatially Regularized Structural Support Vector Machine for Robust Visual Tracking",2019,"","","","",179,"2022-07-13 09:40:06","","10.1109/TNNLS.2018.2855686","","",,,,,29,9.67,6,5,3,"Structural support vector machine (SSVM) is popular in the visual tracking field as it provides a consistent target representation for both learning and detection. However, the spatial distribution of feature is not considered in standard SSVM-based trackers, therefore leading to limited performance. To obtain a robust discriminative classifier, this paper proposes a novel tracking framework that spatially regularizes SSVM, which yields a new spatially regularized SSVM (SRSSVM). We utilize the spatial regularization prior to penalize the learning classifier with the same size as the target region. The location of classifier spatially located far from the center of region is assigned large weight and vice versa. Then, it is introduced into the SSVM model as a regularization factor to learn the robust discriminative model. Furthermore, an optimizing algorithm with dual coordination descent is presented to efficiently solve the SRSSVM tracking model. Our proposed SRSSVM tracking method has low computational cost like the traditional linear SSVM tracker while can significantly improve the robustness of the discriminative classifier. The experimental results on three popular tracking benchmark data sets show that the proposed SRSSVM tracking method performs favorably against the state-of-the-art trackers.","",""
161,"A. Bhagoji, Daniel Cullina, Chawin Sitawarin, Prateek Mittal","Enhancing robustness of machine learning systems via data transformations",2017,"","","","",180,"2022-07-13 09:40:06","","10.1109/CISS.2018.8362326","","",,,,,161,32.20,40,4,5,"We propose the use of data transformations as a defense against evasion attacks on ML classifiers. We present and investigate strategies for incorporating a variety of data transformations including dimensionality reduction via Principal Component Analysis to enhance the resilience of machine learning, targeting both the classification and the training phase. We empirically evaluate and demonstrate the feasibility of linear transformations of data as a defense mechanism against evasion attacks using multiple real-world datasets. Our key findings are that the defense is (i) effective against the best known evasion attacks from the literature, resulting in a two-fold increase in the resources required by a white-box adversary with knowledge of the defense for a successful attack, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification and human activity classification.","",""
142,"Hassan Rafique, Mingrui Liu, Qihang Lin, Tianbao Yang","Non-Convex Min-Max Optimization: Provable Algorithms and Applications in Machine Learning",2018,"","","","",181,"2022-07-13 09:40:06","","","","",,,,,142,35.50,36,4,4,"Min-max saddle-point problems have broad applications in many tasks in machine learning, e.g., distributionally robust learning, learning with non-decomposable loss, or learning with uncertain data. Although convex-concave saddle-point problems have been broadly studied with efficient algorithms and solid theories available, it remains a challenge to design provably efficient algorithms for non-convex saddle-point problems, especially when the objective function involves an expectation or a large-scale finite sum. Motivated by recent literature on non-convex non-smooth minimization, this paper studies a family of non-convex min-max problems where the minimization component is non-convex (weakly convex) and the maximization component is concave. We propose a proximally guided stochastic subgradient method and a proximally guided stochastic variance-reduced method for expected and finite-sum saddle-point problems, respectively. We establish the computation complexities of both methods for finding a nearly stationary point of the corresponding minimization problem.","",""
0,"W. Zong, Yang-Wai Chow, W. Susilo","Visual Analysis of Adversarial Examples in Machine Learning",2021,"","","","",182,"2022-07-13 09:40:06","","10.1007/978-981-33-6726-5_4","","",,,,,0,0.00,0,3,1,"","",""
0,"J. Figuerêdo, V. T. Sarinho, R. Calumby","Low-Cost Machine Learning for Effective and Efficient Bad Smells Detection",2021,"","","","",183,"2022-07-13 09:40:06","","10.5753/kdmile.2021.17468","","",,,,,0,0.00,0,3,1,"Bad smells are characteristics of software that indicate a code or design problem which can make information system hard to understand, evolve, and maintain. To address this problem, different approaches, manual and automated, have been proposed over the years, including more recently machine learning alternatives. However, despite the advances achieved, some machine learning techniques have not yet been effectively explored, such as the use of feature selection techniques. Moreover, it is not clear to what extent the use of numerous source-code features are necessary for reasonable bad smell detection success. Therefore, in this work we propose an approach using low-cost machine learning for effective and efficient detection of bad smells, through explicit feature selection. Our results showed that the selection allowed to statistically improve the effectiveness of the models. For some cases, the models achieved statistical equivalence, but relying on a highly reduced set of features. Indeed, by using explicit feature selection, simpler models such as Naive Bayes became statistically equivalent to robust models such as Random Forest. Therefore, the selection of features allowed keeping competitive or even superior effectiveness while also improving the efficiency of the models, demanding less computational resources for source-code preprocessing, model training and bad smell detection.","",""
0,"Tousif Rahman, R. Shafik, Ole-Christoffer Granmo, A. Yakovlev","Resilient Biomedical Systems Design Under Noise Using Logic-Based Machine Learning",2022,"","","","",184,"2022-07-13 09:40:06","","10.3389/fcteg.2021.778118","","",,,,,0,0.00,0,4,1,"Increased reliance on electronic health records and plethora of new sensor technologies has enabled the use of machine learning (ML) in medical diagnosis. This has opened up promising opportunities for faster and automated decision making, particularly in early and repetitive diagnostic routines. Nevertheless, there are also increased possibilities of data aberrance arising from environmentally induced noise. It is vital to create ML models that are resilient in the presence of data noise to minimize erroneous classifications that could be crucial. This study uses a recently proposed ML algorithm called the Tsetlin machine (TM) to study the robustness against noise-injected medical data. We test two different feature extraction methods, in conjunction with the TM, to explore how feature engineering can mitigate the impact of noise corruption. Our results show the TM is capable of effective classification even with a signal-to-noise ratio (SNR) of −15dB as its training parameters remain resilient to noise injection. We show that high testing data sensitivity can still be possible at very low SNRs through a balance of feature distribution–based discretization and a rule mining algorithm used as a noise filtering encoding method. Through this method we show how a smaller number of core features can be extracted from a noisy problem space resulting in reduced ML model complexity and memory footprint—in some cases up to 6x fewer training parameters while retaining equal or better performance. In addition, we investigate the cost of noise resilience in terms of energy when compared with recently proposed binarized neural networks.","",""
97,"M. Maniruzzaman, M. Rahman, Md. Al-MehediHasan, Harman S. Suri, M. Abedin, A. El-Baz, J. Suri","Accurate Diabetes Risk Stratification Using Machine Learning: Role of Missing Value and Outliers",2018,"","","","",185,"2022-07-13 09:40:06","","10.1007/s10916-018-0940-7","","",,,,,97,24.25,14,7,4,"","",""
141,"F. Thabtah","Machine learning in autistic spectrum disorder behavioral research: A review and ways forward",2019,"","","","",186,"2022-07-13 09:40:06","","10.1080/17538157.2017.1399132","","",,,,,141,47.00,141,1,3,"ABSTRACT Autistic Spectrum Disorder (ASD) is a mental disorder that retards acquisition of linguistic, communication, cognitive, and social skills and abilities. Despite being diagnosed with ASD, some individuals exhibit outstanding scholastic, non-academic, and artistic capabilities, in such cases posing a challenging task for scientists to provide answers. In the last few years, ASD has been investigated by social and computational intelligence scientists utilizing advanced technologies such as machine learning to improve diagnostic timing, precision, and quality. Machine learning is a multidisciplinary research topic that employs intelligent techniques to discover useful concealed patterns, which are utilized in prediction to improve decision making. Machine learning techniques such as support vector machines, decision trees, logistic regressions, and others, have been applied to datasets related to autism in order to construct predictive models. These models claim to enhance the ability of clinicians to provide robust diagnoses and prognoses of ASD. However, studies concerning the use of machine learning in ASD diagnosis and treatment suffer from conceptual, implementation, and data issues such as the way diagnostic codes are used, the type of feature selection employed, the evaluation measures chosen, and class imbalances in data among others. A more serious claim in recent studies is the development of a new method for ASD diagnoses based on machine learning. This article critically analyses these recent investigative studies on autism, not only articulating the aforementioned issues in these studies but also recommending paths forward that enhance machine learning use in ASD with respect to conceptualization, implementation, and data. Future studies concerning machine learning in autism research are greatly benefitted by such proposals.","",""
0,"Hsiao-Chi Li, Chang-Yu Cheng, Chia Chou, Chien-Chang Hsu, Meng-Lin Chang, Y. Chiu, J. Chai","Multi-Class Brain Age Discrimination Using Machine Learning Algorithm",2019,"","","","",187,"2022-07-13 09:40:06","","10.1109/ICMLC48188.2019.8949317","","",,,,,0,0.00,0,7,3,"Resting-state functional connectivity analyses have revealed a significant effect on the inter-regional interactions in brain. The brain age prediction based on resting-state functional magnetic resonance imaging has been proved as biomarkers to characterize the typical brain development and neuropsychiatric disorders. The brain age prediction model based on functional connectivity measurements derived from resting-state functional magnetic resonance imaging has received a lots of interest in recent years due to its great success in age prediction. However, some of the recent studies rely on experienced neuroscientist experts to select appropriate connectivity features in order to build a robust model for prediction while the others just selected the features based on trial-and-error test. Besides, the subjects used in this studies omitted some subjects that can be divided into two groups with less similarity which may confused the prediction model. In this study, we proposed a multi-class age categories discrimination method with the connectivity features selected via K-means clustering with no prior knowledge provided. The experimental results show that with K-means selected features the proposed model better discriminate multi-class age categories.","",""
82,"Qian Yang, Jina Suh, N. Chen, Gonzalo A. Ramos","Grounding Interactive Machine Learning Tool Design in How Non-Experts Actually Build Models",2018,"","","","",188,"2022-07-13 09:40:06","","10.1145/3196709.3196729","","",,,,,82,20.50,21,4,4,"Machine learning (ML) promises data-driven insights and solutions for people from all walks of life, but the skill of crafting these solutions is possessed by only a few. Emerging research addresses this issue by creating ML tools that are easy and accessible to people who are not formally trained in ML (non-experts). This work investigated how non-experts build ML solutions for themselves in real life. Our interviews and surveys revealed unique potentials of non-expert ML, as well several pitfalls that non-experts are susceptible to. For example, many perceived percentage accuracy as a sole measure of performance, thus problematic models proceeded to deployment. These observations suggested that, while challenging, making ML easy and robust should both be important goals of designing novice-facing ML tools. To advance on this insight, we discuss design implications and created a sensitizing concept to demonstrate how designers might guide non-experts to easily build robust solutions.","",""
65,"N. Ball, R. Brunner, A. Myers, D. E. U. O. I. A. Urbana-Champaign, National Center for Supercomputing Applications","Robust machine learning applied to astronomical data sets. I. Star-galaxy classification of the sloan digital sky survey DR3 using decision trees",2006,"","","","",189,"2022-07-13 09:40:06","","10.1086/507440","","",,,,,65,4.06,13,5,16,"We provide classifications for all 143 million nonrepeat photometric objects in the Third Data Release of the SDSS using decision trees trained on 477,068 objects with SDSS spectroscopic data. We demonstrate that these star/galaxy classifications are expected to be reliable for approximately 22 million objects with r 20. The general machine learning environment Data-to-Knowledge and supercomputing resources enabled extensive investigation of the decision tree parameter space. This work presents the first public release of objects classified in this way for an entire SDSS data release. The objects are classified as either galaxy, star, or nsng (neither star nor galaxy), with an associated probability for each class. To demonstrate how to effectively make use of these classifications, we perform several important tests. First, we detail selection criteria within the probability space defined by the three classes to extract samples of stars and galaxies to a given completeness and efficiency. Second, we investigate the efficacy of the classifications and the effect of extrapolating from the spectroscopic regime by performing blind tests on objects in the SDSS, 2dFGRS, and 2QZ surveys. Given the photometric limits of our spectroscopic training data, we effectively begin to extrapolate past our star-galaxy training set at r ~ 18. By comparing the number counts of our training sample with the classified sources, however, we find that our efficiencies appear to remain robust to r ~ 20. As a result, we expect our classifications to be accurate for 900,000 galaxies and 6.7 million stars and remain robust via extrapolation for a total of 8.0 million galaxies and 13.9 million stars.","",""
72,"Georgios Damaskinos, El Mahdi El Mhamdi, R. Guerraoui, Rhicheek Patra, Mahsa Taziki","Asynchronous Byzantine Machine Learning (the case of SGD)",2018,"","","","",190,"2022-07-13 09:40:06","","","","",,,,,72,18.00,14,5,4,"Asynchronous distributed machine learning solutions have proven very effective so far, but always assuming perfectly functioning workers. In practice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrupt data, or even malicious attacks. We introduce \emph{Kardam}, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of two complementary components: a filtering and a dampening component. The first is scalar-based and ensures resilience against $\frac{1}{3}$ Byzantine workers. Essentially, this filter leverages the Lipschitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attempt to corrupt the progress of SGD. The dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We prove that Kardam guarantees almost sure convergence in the presence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead with respect to non Byzantine-resilient solutions. We empirically show that Kardam does not introduce additional noise to the learning procedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and empirically show to be less than $f/n$, where $f$ is the number of Byzantine failures tolerated and $n$ the total number of workers. Interestingly, we also empirically observe that the dampening component is interesting in its own right for it enables to build an SGD algorithm that outperforms alternative staleness-aware asynchronous competitors in environments with honest workers.","",""
85,"Neoklis Polyzotis, Sudip Roy, S. E. Whang, Martin A. Zinkevich","Data Lifecycle Challenges in Production Machine Learning",2018,"","","","",191,"2022-07-13 09:40:06","","10.1145/3299887.3299891","","",,,,,85,21.25,21,4,4,"Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to explore how different constraints are imposed on the solutions depending on where in the lifecycle of a model the problems are encountered and who encounters them.","",""
83,"S. Kendale, Prathamesh Kulkarni, A. Rosenberg, Jing Wang","Supervised Machine-learning Predictive Analytics for Prediction of Postinduction Hypotension",2018,"","","","",192,"2022-07-13 09:40:06","","10.1097/ALN.0000000000002374","","",,,,,83,20.75,21,4,4,"What We Already Know about This Topic The ability to predict postinduction hypotension remains limited and challenging due to the multitude of data elements that may be considered Novel machine-learning algorithms may offer a systematic approach to predict postinduction hypotension, but are understudied What This Article Tells Us That Is New Among 13,323 patients undergoing a variety of surgical procedures, 8.9% experienced a mean arterial pressure less than 55 mmHg within 10 min of induction start While some machine-learning algorithms perform worse than logistic regression, several techniques may be superior Gradient boosting machine, with tuning, demonstrates a receiver operating characteristic area under the curve of 0.76, a negative predictive value of 19%, and positive predictive value of 96% Background: Hypotension is a risk factor for adverse perioperative outcomes. Machine-learning methods allow large amounts of data for development of robust predictive analytics. The authors hypothesized that machine-learning methods can provide prediction for the risk of postinduction hypotension. Methods: Data was extracted from the electronic health record of a single quaternary care center from November 2015 to May 2016 for patients over age 12 that underwent general anesthesia, without procedure exclusions. Multiple supervised machine-learning classification techniques were attempted, with postinduction hypotension (mean arterial pressure less than 55 mmHg within 10 min of induction by any measurement) as primary outcome, and preoperative medications, medical comorbidities, induction medications, and intraoperative vital signs as features. Discrimination was assessed using cross-validated area under the receiver operating characteristic curve. The best performing model was tuned and final performance assessed using split-set validation. Results: Out of 13,323 cases, 1,185 (8.9%) experienced postinduction hypotension. Area under the receiver operating characteristic curve using logistic regression was 0.71 (95% CI, 0.70 to 0.72), support vector machines was 0.63 (95% CI, 0.58 to 0.60), naive Bayes was 0.69 (95% CI, 0.67 to 0.69), k-nearest neighbor was 0.64 (95% CI, 0.63 to 0.65), linear discriminant analysis was 0.72 (95% CI, 0.71 to 0.73), random forest was 0.74 (95% CI, 0.73 to 0.75), neural nets 0.71 (95% CI, 0.69 to 0.71), and gradient boosting machine 0.76 (95% CI, 0.75 to 0.77). Test set area for the gradient boosting machine was 0.74 (95% CI, 0.72 to 0.77). Conclusions: The success of this technique in predicting postinduction hypotension demonstrates feasibility of machine-learning models for predictive analytics in the field of anesthesiology, with performance dependent on model selection and appropriate tuning.","",""
8,"Mustafa Anil Koçak, David Ramirez, E. Erkip, D. Shasha","SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness",2017,"","","","",193,"2022-07-13 09:40:06","","10.1109/TPAMI.2019.2932415","","",,,,,8,1.60,2,4,5,"<italic>SafePredict</italic> is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, <inline-formula><tex-math notation=""LaTeX"">$1-\epsilon$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""kocak-ieq1-2932415.gif""/></alternatives></inline-formula>, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm so that the error rate on non-refused predictions does not exceed <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq2-2932415.gif""/></alternatives></inline-formula>. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq3-2932415.gif""/></alternatives></inline-formula>, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the-art confidence-based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software is included in the supplementary material, which can be found on the Computer Society Digital Library at <uri>http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2932415</uri>.","",""
17,"Weina Zhang, Han Liu, V. Silenzio, Peiyuan Qiu, W. Gong","Machine Learning Models for the Prediction of Postpartum Depression: Application and Comparison Based on a Cohort Study",2020,"","","","",194,"2022-07-13 09:40:06","","10.2196/15516","","",,,,,17,8.50,3,5,2,"Background Postpartum depression (PPD) is a serious public health problem. Building a predictive model for PPD using data during pregnancy can facilitate earlier identification and intervention. Objective The aims of this study are to compare the effects of four different machine learning models using data during pregnancy to predict PPD and explore which factors in the model are the most important for PPD prediction. Methods Information on the pregnancy period from a cohort of 508 women, including demographics, social environmental factors, and mental health, was used as predictors in the models. The Edinburgh Postnatal Depression Scale score within 42 days after delivery was used as the outcome indicator. Using two feature selection methods (expert consultation and random forest-based filter feature selection [FFS-RF]) and two algorithms (support vector machine [SVM] and random forest [RF]), we developed four different machine learning PPD prediction models and compared their prediction effects. Results There was no significant difference in the effectiveness of the two feature selection methods in terms of model prediction performance, but 10 fewer factors were selected with the FFS-RF than with the expert consultation method. The model based on SVM and FFS-RF had the best prediction effects (sensitivity=0.69, area under the curve=0.78). In the feature importance ranking output by the RF algorithm, psychological elasticity, depression during the third trimester, and income level were the most important predictors. Conclusions In contrast to the expert consultation method, FFS-RF was important in dimension reduction. When the sample size is small, the SVM algorithm is suitable for predicting PPD. In the prevention of PPD, more attention should be paid to the psychological resilience of mothers.","",""
21,"El-Mahdi El-Mhamdi, R. Guerraoui, Arsany Guirguis, L. Hoang, Sébastien Rouault","Genuinely Distributed Byzantine Machine Learning",2019,"","","","",195,"2022-07-13 09:40:06","","10.1145/3382734.3405695","","",,,,,21,7.00,4,5,3,"Machine Learning (ML) solutions are nowadays distributed, according to the so-called server/worker architecture. One server holds the model parameters while several workers train the model. Clearly, such architecture is prone to various types of component failures, which can be all encompassed within the spectrum of a Byzantine behavior. Several approaches have been proposed recently to tolerate Byzantine workers. Yet all require trusting a central parameter server. We initiate in this paper the study of the ""general"" Byzantine-resilient distributed machine learning problem where no individual component is trusted. In particular, we distribute the parameter server computation on several nodes. We show that this problem can be solved in an asynchronous system, despite the presence of ⅓ Byzantine parameter servers and ⅓ Byzantine workers (which is optimal). We present a new algorithm, ByzSGD, which solves the general Byzantine-resilient distributed machine learning problem by relying on three major schemes. The first, Scatter/Gather, is a communication scheme whose goal is to bound the maximum drift among models on correct servers. The second, Distributed Median Contraction (DMC), leverages the geometric properties of the median in high dimensional spaces to bring parameters within the correct servers back close to each other, ensuring learning convergence. The third, Minimum-Diameter Averaging (MDA), is a statistically-robust gradient aggregation rule whose goal is to tolerate Byzantine workers. MDA requires loose bound on the variance of non-Byzantine gradient estimates, compared to existing alternatives (e.g., Krum [12]). Interestingly, ByzSGD ensures Byzantine resilience without adding communication rounds (on a normal path), compared to vanilla non-Byzantine alternatives. ByzSGD requires, however, a larger number of messages which, we show, can be reduced if we assume synchrony. We implemented ByzSGD on top of TensorFlow, and we report on our evaluation results. In particular, we show that ByzSGD achieves convergence in Byzantine settings with around 32% overhead compared to vanilla TensorFlow. Furthermore, we show that ByzSGD's throughput overhead is 24--176% in the synchronous case and 28--220% in the asynchronous case.","",""
0,"Sannasi Chakravarthy S R, H. Rajaguru","Deep Features with Improved Extreme Learning Machine for Breast Cancer Classification",2021,"","","","",196,"2022-07-13 09:40:06","","10.1109/ISCMI53840.2021.9654814","","",,,,,0,0.00,0,2,1,"Breast cancer classification problem is receiving more attention among researchers due to its global impact on women's healthcare. There is always a demand for research analysis in the earlier diagnosis of breast cancer. The paper proposes a new computer-aided diagnosis (CAD) framework which integrates deep learning and Extreme Learning Machine (ELM) for feature extrication and classification of breast cancer. The proposed CAD tool is very much helpful for radiologists in the earlier diagnosis of breast cancer using digital mammograms. Herein, the research uses the Sine-Cosine Crow-Search Optimization Algorithm (SC-CSOA) for improving the ELM’s classification performance. And to extricate the robust features from the input mammograms, the concept of transfer learning is applied. For that, the work adopts the three most efficient Residual Network (ResNet) families of CNN, namely ResNet18, ResNet50, and ResNet101 architectures. The input database used for evaluation is the INbreast dataset which comprises Full-Field Digital Mammogram (FFDM) images. At this point, the research compares the results obtained with the existing ELM and K-NN algorithms where it is found that the performance of the proposed framework provides the supreme classification (95.811% of accuracy) over others.","",""
40,"J. Kosaian, K. V. Rashmi, S. Venkataraman","Learning a Code: Machine Learning for Approximate Non-Linear Coded Computation",2018,"","","","",197,"2022-07-13 09:40:06","","","","",,,,,40,10.00,13,3,4,"Machine learning algorithms are typically run on large scale, distributed compute infrastructure that routinely face a number of unavailabilities such as failures and temporary slowdowns. Adding redundant computations using coding-theoretic tools called ""codes"" is an emerging technique to alleviate the adverse effects of such unavailabilities. A code consists of an encoding function that proactively introduces redundant computation and a decoding function that reconstructs unavailable outputs using the available ones. Past work focuses on using codes to provide resilience for linear computations and specific iterative optimization algorithms. However, computations performed for a variety of applications including inference on state-of-the-art machine learning algorithms, such as neural networks, typically fall outside this realm. In this paper, we propose taking a learning-based approach to designing codes that can handle non-linear computations. We present carefully designed neural network architectures and a training methodology for learning encoding and decoding functions that produce approximate reconstructions of unavailable computation results. We present extensive experimental results demonstrating the effectiveness of the proposed approach: we show that the our learned codes can accurately reconstruct $64 - 98\%$ of the unavailable predictions from neural-network based image classifiers on the MNIST, Fashion-MNIST, and CIFAR-10 datasets. To the best of our knowledge, this work proposes the first learning-based approach for designing codes, and also presents the first coding-theoretic solution that can provide resilience for any non-linear (differentiable) computation. Our results show that learning can be an effective technique for designing codes, and that learned codes are a highly promising approach for bringing the benefits of coding to non-linear computations.","",""
39,"Bin Nie, J. Xue, Saurabh Gupta, Tirthak Patel, C. Engelmann, E. Smirni, Devesh Tiwari","Machine Learning Models for GPU Error Prediction in a Large Scale HPC System",2018,"","","","",198,"2022-07-13 09:40:06","","10.1109/DSN.2018.00022","","",,,,,39,9.75,6,7,4,"GPUs are widely deployed on large-scale HPC systems to provide powerful computational capability for scientific applications from various domains. As those applications are normally long-running, investigating the characteristics of GPU errors becomes imperative for reliability. In this paper, we first study the system conditions that trigger GPU errors using six-month trace data collected from a large-scale, operational HPC system. Then, we use machine learning to predict the occurrence of GPU errors, by taking advantage of temporal and spatial dependencies of the trace data. The resulting machine learning prediction framework is robust and accurate under different workloads.","",""
90,"Nagdev Amruthnath, Tarun Gupta","A research study on unsupervised machine learning algorithms for early fault detection in predictive maintenance",2018,"","","","",199,"2022-07-13 09:40:06","","10.1109/IEA.2018.8387124","","",,,,,90,22.50,45,2,4,"The area of predictive maintenance has taken a lot of prominence in the last couple of years due to various reasons. With new algorithms and methodologies growing across different learning methods, it has remained a challenge for industries to adopt which method is fit, robust and provide most accurate detection. Fault detection is one of the critical components of predictive maintenance; it is very much needed for industries to detect faults early and accurately. In a production environment, to minimize the cost of maintenance, sometimes it is required to build a model with minimal or no historical data. In such cases, unsupervised learning would be a better option model building. In this paper, we have chosen a simple vibration data collected from an exhaust fan, and have fit different unsupervised learning algorithms such as PCA T2 statistic, Hierarchical clustering, K-Means, Fuzzy C-Means clustering and model-based clustering to test its accuracy, performance, and robustness. In the end, we have proposed a methodology to benchmark different algorithms and choosing the final model.","",""
0,"S. Pundir, M. Obaidat, M. Wazid, A. Das, D. P. Singh, J. Rodrigues","MADP-IIME: malware attack detection protocol in IoT-enabled industrial multimedia environment using machine learning approach",2021,"","","","",200,"2022-07-13 09:40:06","","10.1007/S00530-020-00743-9","","",,,,,0,0.00,0,6,1,"","",""
