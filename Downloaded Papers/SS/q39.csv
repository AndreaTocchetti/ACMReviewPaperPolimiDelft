Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
161,"A. Bhagoji, Daniel Cullina, Chawin Sitawarin, Prateek Mittal","Enhancing robustness of machine learning systems via data transformations",2017,"","","","",1,"2022-07-13 09:24:04","","10.1109/CISS.2018.8362326","","",,,,,161,32.20,40,4,5,"We propose the use of data transformations as a defense against evasion attacks on ML classifiers. We present and investigate strategies for incorporating a variety of data transformations including dimensionality reduction via Principal Component Analysis to enhance the resilience of machine learning, targeting both the classification and the training phase. We empirically evaluate and demonstrate the feasibility of linear transformations of data as a defense mechanism against evasion attacks using multiple real-world datasets. Our key findings are that the defense is (i) effective against the best known evasion attacks from the literature, resulting in a two-fold increase in the resources required by a white-box adversary with knowledge of the defense for a successful attack, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification and human activity classification.","",""
0,"Wenjie Huang, M. Ling","Machine Learning-Based Method for Urban Lifeline System Resilience Assessment in GIS*",2019,"","","","",2,"2022-07-13 09:24:04","","10.5772/INTECHOPEN.82748","","",,,,,0,0.00,0,2,3,"System resilience, the capability of a system to sustain and recover from deliberate attacks, accidents, or naturally occurring threats or incidents, is a key property to measure the degree of robustness and coupling effect of complex system. The systems of waste disposal, urban water supply, and electricity transmission are typical systems with complex and high coupling features. In this chapter, a methodology for measuring the system resilience of such systems is proposed. It is a process of integrated decision-making which contains two aspects: (1) a five-dimensional indicator framework of system resilience which includes attributes in infrastructural, economic, and social sectors and (2) a hybrid K-means algorithm, which combines entropy theory, bootstrapping, and analytic network process. Through utilizing real data, the methodology can assist to identify and classify the level of system resilience for different geographical regions which are sustained by lifeline systems. The calculation of algorithm, visualization of processed data, and classification of resilience level can be finally realized in geographic information system. Through utilizing by regional governments and local communities, the final result can serve to provide guideline for resource allocation and the prevention of huge economic loss in disasters.","",""
5,"A. Hussein, A. Chehab, A. Kayssi, I. Elhajj","Machine learning for network resilience: The start of a journey",2018,"","","","",3,"2022-07-13 09:24:04","","10.1109/SDS.2018.8370423","","",,,,,5,1.25,1,4,4,"Security is one of the main concerns facing the development of new projects in networking and communications. Another challenge is to verify that a system is working exactly as specified. On the other hand, advances in Artificial Intelligence (AI) technology have opened up new markets and opportunities for progress in critical areas such as network resiliency, health, education, energy, economic inclusion, social welfare, and the environment. AI is expected to play an increasing role in defensive and offensive measures to provide a rapid response to react to the landscape of evolving threats. Software Defined Networking (SDN), being centralized by nature, provides a global view of the network. It is the flexibility and robustness offered by programmable networking that lead us to consider the integration of these two concepts, SDN and AI. Inspired by the fascinating tactics of the human immunity system, we aim to design a general hybrid Artificial Intelligence Resiliency System (ARS) that strikes a good balance between centralized and distributed security systems that may be applicable to different network environments. In addition, we aim to investigate and leverage the latest AI techniques to improve network performance in general and resiliency in particular.","",""
5,"Onur Danaci, Sanjaya Lohani, B. Kirby, R. Glasser","Machine learning pipeline for quantum state estimation with incomplete measurements",2020,"","","","",4,"2022-07-13 09:24:04","","10.1088/2632-2153/abe5f5","","",,,,,5,2.50,1,4,2,"Two-qubit systems typically employ 36 projective measurements for high-fidelity tomographic estimation. The overcomplete nature of the 36 measurements suggests possible robustness of the estimation procedure to missing measurements. In this paper, we explore the resilience of machine-learning-based quantum state estimation techniques to missing measurements by creating a pipeline of stacked machine learning models for imputation, denoising, and state estimation. When applied to simulated noiseless and noisy projective measurement data for both pure and mixed states, we demonstrate quantum state estimation from partial measurement results that outperforms previously developed machine-learning-based methods in reconstruction fidelity and several conventional methods in terms of resource scaling. Notably, our developed model does not require training a separate model for each missing measurement, making it potentially applicable to quantum state estimation of large quantum systems where preprocessing is computationally infeasible due to the exponential scaling of quantum system dimension.","",""
0,"Stefano Calzavara, L. Cazzaro, C. Lucchese, Federico Marcuzzi, S. Orlando","Beyond Robustness: Resilience Verification of Tree-Based Classifiers",2021,"","","","",5,"2022-07-13 09:24:04","","","","",,,,,0,0.00,0,5,1,"In this paper we criticize the robustness measure traditionally employed to assess the performance of machine learning models deployed in adversarial settings. To mitigate the limitations of robustness, we introduce a new measure called resilience and we focus on its verification. In particular, we discuss how resilience can be verified by combining a traditional robustness verification technique with a data-independent stability analysis, which identifies a subset of the feature space where the model does not change its predictions despite adversarial manipulations. We then introduce a formally sound data-independent stability analysis for decision trees and decision tree ensembles, which we experimentally assess on public datasets and we leverage for resilience verification. Our results show that resilience verification is useful and feasible in practice, yielding a more reliable security assessment of both standard and robust decision tree models.","",""
6,"Liwei Song, Vikash Sehwag, A. Bhagoji, Prateek Mittal","A Critical Evaluation of Open-World Machine Learning",2020,"","","","",6,"2022-07-13 09:24:04","","","","",,,,,6,3.00,2,4,2,"Open-world machine learning (ML) combines closed-world models trained on in-distribution data with out-of-distribution (OOD) detectors, which aim to detect and reject OOD inputs. Previous works on open-world ML systems usually fail to test their reliability under diverse, and possibly adversarial conditions. Therefore, in this paper, we seek to understand how resilient are state-of-the-art open-world ML systems to changes in system components? With our evaluation across 6 OOD detectors, we find that the choice of in-distribution data, model architecture and OOD data have a strong impact on OOD detection performance, inducing false positive rates in excess of $70\%$. We further show that OOD inputs with 22 unintentional corruptions or adversarial perturbations render open-world ML systems unusable with false positive rates of up to $100\%$. To increase the resilience of open-world ML, we combine robust classifiers with OOD detection techniques and uncover a new trade-off between OOD detection and robustness.","",""
0,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, M. Teimoori, F. Kriebel, Jeff Zhang, Kang Liu, Semeen Rehman, T. Theocharides, A. Artusi, S. Garg, M. Shafique","Robust Computing for Machine Learning-Based Systems",2020,"","","","",7,"2022-07-13 09:24:04","","10.1007/978-3-030-52017-5_20","","",,,,,0,0.00,0,12,2,"","",""
2,"J. Sarkar, Cory Peterson","Operational Workload Impact on Robust Solid-State Storage Analyzed with Interpretable Machine Learning",2019,"","","","",8,"2022-07-13 09:24:04","","10.1109/IRPS.2019.8720510","","",,,,,2,0.67,1,2,3,"Solid-state storage technology is finding increasing adoption in enterprise and data center environments due to their high reliability and reducing cost. With high performance solid-state storage devices (SSDs) internally designed as distributed resilient systems, their operational behavior under materially different workloads is described in this research. Application of interpretable machine learning on internal parametric data of SSDs enables insights on workloads' interaction with the resilient system design. After prior research demonstrated significantly different accelerated workload stress, the analysis on resilience of the SSDs under random vs. pseudo-sequential workloads emphasize the efficacy and importance of their distributed resilience schemes. As such, these results provide causational insights on the mechanism of differential stress of the workloads impacting the resilience design principles. Moreover, the results elucidate guidelines strongly relevant from design robustness perspective for research on novel SSD architectures such as the proposed Open Channel SSD, towards deployment in hyperscale and virtualization environments.","",""
0,"F. Marulli, S. Marrone, Laura Verde","Sensitivity of Machine Learning Approaches to Fake and Untrusted Data in Healthcare Domain",2022,"","","","",9,"2022-07-13 09:24:04","","10.3390/jsan11020021","","",,,,,0,0.00,0,3,1,"Machine Learning models are susceptible to attacks, such as noise, privacy invasion, replay, false data injection, and evasion attacks, which affect their reliability and trustworthiness. Evasion attacks, performed to probe and identify potential ML-trained models’ vulnerabilities, and poisoning attacks, performed to obtain skewed models whose behavior could be driven when specific inputs are submitted, represent a severe and open issue to face in order to assure security and reliability to critical domains and systems that rely on ML-based or other AI solutions, such as healthcare and justice, for example. In this study, we aimed to perform a comprehensive analysis of the sensitivity of Artificial Intelligence approaches to corrupted data in order to evaluate their reliability and resilience. These systems need to be able to understand what is wrong, figure out how to overcome the resulting problems, and then leverage what they have learned to overcome those challenges and improve their robustness. The main research goal pursued was the evaluation of the sensitivity and responsiveness of Artificial Intelligence algorithms to poisoned signals by comparing several models solicited with both trusted and corrupted data. A case study from the healthcare domain was provided to support the pursued analyses. The results achieved with the experimental campaign were evaluated in terms of accuracy, specificity, sensitivity, F1-score, and ROC area.","",""
0,"Harsh Chaudhari, Matthew Jagielski, Alina Oprea","SafeNet: Mitigating Data Poisoning Attacks on Private Machine Learning",2022,"","","","",10,"2022-07-13 09:24:04","","10.48550/arXiv.2205.09986","","",,,,,0,0.00,0,3,1,"—Secure multiparty computation (MPC) has been proposed to allow multiple mutually distrustful data owners to jointly train machine learning (ML) models on their combined data. However, the datasets used for training ML models might be under the control of an adversary mounting a data poisoning attack, and MPC prevents inspecting training sets to detect poisoning. We show that multiple MPC frameworks for private ML training are susceptible to backdoor and targeted poisoning attacks. To mitigate this, we propose SafeNet, a framework for building ensemble models in MPC with formal guarantees of robustness to data poisoning attacks. We extend the security deﬁnition of private ML training to account for poisoning and prove that our SafeNet design satisﬁes the deﬁnition. We demonstrate SafeNet’s efﬁciency, accuracy, and resilience to poisoning on several machine learning datasets and models. For instance, SafeNet reduces backdoor attack success from 100% to 0% for a neural network model, while achieving 39 × faster training and 36 × less communication than the four-party MPC framework of Dalskov et al. [26].","",""
0,"A. Shamis, P. Pietzuch, Antoine Delignat-Lavaud, Andrew J. Paverd, Manuel Costa","Dropbear: Machine Learning Marketplaces made Trustworthy with Byzantine Model Agreement",2022,"","","","",11,"2022-07-13 09:24:04","","10.48550/arXiv.2205.15757","","",,,,,0,0.00,0,5,1,"Marketplaces for machine learning (ML) models are emerging as a way for organizations to monetize models. They allow model owners to retain control over hosted models by using cloud resources to execute ML inference requests for a fee, preserving model confidentiality. Clients that rely on hosted models require trustworthy inference results, even when models are managed by third parties. While the resilience and robustness of inference results can be improved by combining multiple independent models, such support is unavailable in today’s marketplaces. We describe Dropbear, the first ML model marketplace that provides clients with strong integrity guarantees by combining results from multiple models in a trustworthy fashion. Dropbear replicates inference computation across a model group, which consists of multiple cloud-based GPU nodes belonging to different model owners. Clients receive inference certificates that prove agreement using a Byzantine consensus protocol, even under model heterogeneity and concurrent model updates. To improve performance, Dropbear batches inference and consensus operations separately: it first performs the inference computation across a model group, before ordering requests and model updates. Despite its strong integrity guarantees, Dropbear’s performance matches that of state-ofthe-art ML inference systems: deployed across 3 cloud sites, it handles 800 requests/s with ImageNet models.","",""
0,"Naiara Escudero, P. Costas, Michael W. Hardt, G. Inalhan","Machine Learning Based Visual Navigation System Architecture for Aam Operations with A Discussion on its Certifiability",2022,"","","","",12,"2022-07-13 09:24:04","","10.1109/ICNS54818.2022.9771519","","",,,,,0,0.00,0,4,1,"Advanced Air Mobility (AAM) is expected to revolutionize the future of general transportation expanding the conventional notion of air traffic to include several services carried out by autonomous aerial platforms. However, the significant challenges associated with such complex scenarios require the introduction of sophisticated technologies able to deliver the resilience, robustness, and accuracy needed to achieve safe, autonomous operations [39]. In this context, solutions based on Artificial Intelligence (AI), able to overcome some limitations found in traditional approaches, are becoming a major opportunity for the aviation industry, but, at the same time, a significant challenge with respect to the certification standards.With the focal point on further proposing a certifiable architecture for AI-enhanced vision navigation in AAM operations, this paper first, summarizes the current technologies and fusion methods applied to date to navigation purposes, to later address the certification problem. Regarding certification, it explores three specific points: 1) traditional certification procedures; 2) current status of AI homologation recommendations; and 3) other certification factors to be considered for future discussion.","",""
41,"P. Blanchard, El Mahdi El Mhamdi, R. Guerraoui, J. Stainer","Byzantine-Tolerant Machine Learning",2017,"","","","",13,"2022-07-13 09:24:04","","","","",,,,,41,8.20,10,4,5,"The growth of data, the need for scalability and the complexity of models used in modern machine learning calls for distributed implementations. Yet, as of today, distributed machine learning frameworks have largely ignored the possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study the robustness to Byzantine failures at the fundamental level of stochastic gradient descent (SGD), the heart of most machine learning algorithms. Assuming a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can SGD be, without limiting the dimension, nor the size of the parameter space.  We first show that no gradient descent update rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the update rule capturing the basic requirements to guarantee convergence despite $f$ Byzantine workers. We finally propose Krum, an update rule that satisfies the resilience property aforementioned. For a $d$-dimensional learning problem, the time complexity of Krum is $O(n^2 \cdot (d + \log n))$.","",""
17,"L. Cornejo-Bueno, L. Cuadra, S. Jiménez-Fernández, J. Acevedo-Rodríguez, L. Prieto, S. Salcedo-Sanz","Wind Power Ramp Events Prediction with Hybrid Machine Learning Regression Techniques and Reanalysis Data",2017,"","","","",14,"2022-07-13 09:24:04","","10.3390/EN10111784","","",,,,,17,3.40,3,6,5,"Wind Power Ramp Events (WPREs) are large fluctuations of wind power in a short time interval, which lead to strong, undesirable variations in the electric power produced by a wind farm. Its accurate prediction is important in the effort of efficiently integrating wind energy in the electric system, without affecting considerably its stability, robustness and resilience. In this paper, we tackle the problem of predicting WPREs by applying Machine Learning (ML) regression techniques. Our approach consists of using variables from atmospheric reanalysis data as predictive inputs for the learning machine, which opens the possibility of hybridizing numerical-physical weather models with ML techniques for WPREs prediction in real systems. Specifically, we have explored the feasibility of a number of state-of-the-art ML regression techniques, such as support vector regression, artificial neural networks (multi-layer perceptrons and extreme learning machines) and Gaussian processes to solve the problem. Furthermore, the ERA-Interim reanalysis from the European Center for Medium-Range Weather Forecasts is the one used in this paper because of its accuracy and high resolution (in both spatial and temporal domains). Aiming at validating the feasibility of our predicting approach, we have carried out an extensive experimental work using real data from three wind farms in Spain, discussing the performance of the different ML regression tested in this wind power ramp event prediction problem.","",""
4,"F. Firouzi, Bahar Farahani, A. Kahng, J. Rabaey, N. Balac","Guest Editorial: Alternative Computing and Machine Learning for Internet of Things",2017,"","","","",15,"2022-07-13 09:24:04","","10.1109/TVLSI.2017.2742098","","",,,,,4,0.80,1,5,5,"The impending Internet of Things (IoT) wave is promising to affect every aspect of our daily lives, ranging from smart things to smart buildings, smart cities, and smart environments. A lot of attention has been devoted to the tsunami of data produced by IoT, and the related means of extracting useful actionable information from it, spawning efforts in Big Data processing and machine learning. Yet, all of this does little to address the need for IoT to capture, interpret, and act on this wall of (noisy) information at the right time, at the right place, and in the right form. Conventional computing systems are a poor match to the needs of this emerging massively distributed real-time system. Hence, alternative computing techniques present an attractive alternative, trading off computational resolution for significant gains in quality-of-service energy efficiency and robustness. This observation is based on the conjecture that most applications related to IoT have an inherent error resilience and are evolutionary (that is, learning-based). Alternative computing strategies may be conceived at every level of the design hierarchy, starting from the device level with novel 3-D nonvolatile memory/logic combinations, or at the architectural level by shifting away from the traditional von Neumann architecture to different computing paradigms such as neuromorphic and/or stochastic computation all the way up to the algorithmic and data representation levels.","",""
0,"A. Dvir, Yehonatan Zion, Jonathan Muehlstein, Ofir Pele, Chen Hajaj, Ran Dubin","Robust Machine Learning for Encrypted Traffic Classification",2016,"","","","",16,"2022-07-13 09:24:04","","","","",,,,,0,0.00,0,6,6,"Desktops and laptops can be maliciously exploited to violate privacy. In this paper, we consider the daily battle between the passive attacker who is targeting a specific user against a user that may be adversarial opponent. In this scenario, while the attacker tries to choose the best vector attack by surreptitiously monitoring the victims encrypted network traffic in order to identify users parameters such as the Operating System (OS), browser and apps. The user may use tools such as a Virtual Private Network (VPN) or even change protocols parameters to protect his/her privacy. We provide a large dataset of more than 20,000 examples for this task. We run a comprehensive set of experiments, that achieves high (above 85) classification accuracy, robustness and resilience to changes of features as a function of different network conditions at test time. We also show the effect of a small training set on the accuracy.","",""
3,"Ihsen Alouani, Anouar Ben Khalifa, Farhad Merchant, R. Leupers","An Investigation on Inherent Robustness of Posit Data Representation",2021,"","","","",17,"2022-07-13 09:24:04","","10.1109/VLSID51830.2021.00052","","",,,,,3,3.00,1,4,1,"As the dimensions and operating voltages of computer electronics shrink to cope with consumers' demand for higher performance and lower power consumption, circuit sensitivity to soft errors increases dramatically. Recently, a new data-type is proposed in the literature called posit data type. Posit arithmetic has absolute advantages such as higher numerical accuracy, speed, and simpler hardware design than IEEE 754–2008 technical standard-compliant arithmetic. In this paper, we propose a comparative robustness study between 32-bit posit and 32-bit IEEE 754–2008 compliant representations. At first, we propose a theoretical analysis for IEEE 754 compliant numbers and posit numbers for single bit flip and double bit flips. Then, we conduct exhaustive fault injection experiments that show a considerable inherent resilience in posit format compared to classical IEEE 754 compliant representation. To show a relevant use-case of fault-tolerant applications, we perform experiments on a set of machine-learning applications. In more than 95% of the exhaustive fault injection exploration, posit representation is less impacted by faults than the IEEE 754 compliant floating-point representation. Moreover, in 100% of the tested machine-learning applications, the accuracy of posit-implemented systems is higher than the classical floating-point-based ones.","",""
0,"A. Fontana","And/or trade-off in artificial neurons: impact on adversarial robustness",2021,"","","","",18,"2022-07-13 09:24:04","","","","",,,,,0,0.00,0,1,1,"Since its discovery in 2013, the phenomenon of adversarial examples has attracted a growing amount of attention from the machine learning community. A deeper understanding of the problem could lead to a better comprehension of how information is processed and encoded in neural networks and, more in general, could help to solve the issue of interpretability in machine learning. Our idea to increase adversarial resilience starts with the observation that artificial neurons can be divided in two broad categories: AND-like neurons and OR-like neurons. Intuitively, the former are characterised by a relatively low number of combinations of input values which trigger neuron activation, while for the latter the opposite is true. Our hypothesis is that the presence in a network of a sufficiently high number of OR-like neurons could lead to classification “brittleness” and increase the network’s susceptibility to adversarial attacks. After constructing an operational definition of a neuron AND-like behaviour, we proceed to introduce several measures to increase the proportion of AND-like neurons in the network: L1 norm weight normalisation; application of an input filter; comparison between the neuron output’s distribution obtained when the network is fed with the actual data set and the distribution obtained when the network is fed with a randomised version of the former called “scrambled data set”. Tests performed on the MNIST data set hint that the proposed measures could represent an interesting direction to explore.","",""
4,"Ahsan Morshed, R. Dutta","Machine Learning based Vocabulary Management Tool Assessment for the Linked Open Data",2012,"","","","",19,"2022-07-13 09:24:04","","10.5120/9724-4197","","",,,,,4,0.40,2,2,10,"domain vocabularies in the context of developing the knowledge based Linked Open data system is the most important discipline on the web. Many editors are available for developing and managing the vocabularies or Ontologies. However, selecting the most relevant editor is very difficult since each vocabulary construction initiative requires its own budget, time, resources. In this paper a novel unsupervised machine learning based comparative assessment mechanism has been proposed for selecting the most relevant editor. Defined evaluation criterions were functionality, reusability, data storage, complexity, association, maintainability, resilience, reliability, robustness, learnability, availability, flexibility, and visibility. Principal component analysis (PCA) was applied on the feedback data set collected from a survey involving sixty users. Focus was to identify the least correlated features carrying the most independent information variance to optimize the tool selection process. An automatic evaluation method based on Bagging Decision Trees has been used to identify the most suitable editor. Three tools namely Vocbench, TopBraid EVN and Pool Party Thesaurus Manager have been evaluated. Decision tree based analysis recommended the Vocbench and the Pool Party Thesaurus Manager are the better performer than the TopBraid EVN tool with very similar recommendation scores.","",""
0,"Moustafa Naiem Abdel-Mooty, W. El-Dakhakhni, P. Coulibaly","Data-Driven Community Flood Resilience Prediction",2022,"","","","",20,"2022-07-13 09:24:04","","10.3390/w14132120","","",,,,,0,0.00,0,3,1,"Climate change and the development of urban centers within flood-prone areas have significantly increased flood-related disasters worldwide. However, most flood risk categorization and prediction efforts have been focused on the hydrologic features of flood hazards, often not considering subsequent long-term losses and recovery trajectories (i.e., community’s flood resilience). In this study, a two-stage Machine Learning (ML)-based framework is developed to accurately categorize and predict communities’ flood resilience and their response to future flood hazards. This framework is a step towards developing comprehensive, proactive flood disaster management planning to further ensure functioning urban centers and mitigate the risk of future catastrophic flood events. In this framework, resilience indices are synthesized considering resilience goals (i.e., robustness and rapidity) using unsupervised ML, coupled with climate information, to develop a supervised ML prediction algorithm. To showcase the utility of the framework, it was applied on historical flood disaster records collected by the US National Weather Services. These disaster records were subsequently used to develop the resilience indices, which were then coupled with the associated historical climate data, resulting in high-accuracy predictions and, thus, utility in flood resilience management studies. To further demonstrate the utilization of the framework, a spatial analysis was developed to quantify communities’ flood resilience and vulnerability across the selected spatial domain. The framework presented in this study is employable in climate studies and patio-temporal vulnerability identification. Such a framework can also empower decision makers to develop effective data-driven climate resilience strategies.","",""
0,"Marie Alban, Desnos Karol, Morin Luce, Zhang Lu","Expert training: Enhancing AI resilience to image coding artifacts",2022,"","","","",21,"2022-07-13 09:24:04","","10.2352/ei.2022.34.10.ipas-392","","",,,,,0,0.00,0,4,1,"—In the Machine-to-Machine (M2M) transmission context, there is a great need to reduce the amount of transmitted information using lossy compression. However, commonly used image compression methods are designed for human perception, not for Artificial Intelligence (AI) algorithms performances. It is known that these compression distortions affect many deep learning based architectures on several computer vision tasks. In this paper, we focus on the classification task and propose a new approach, named expert training, to enhance Convolutional Neural Networks (CNNs) resilience to compression distortions. We validated our approach using MnasNet and ResNet50 ar- chitectures, against image compression distortions introduced by three commonly used methods (JPEG, J2K and BPG), on the ImageNet dataset. The results showed a better robustness of these two architectures against the tested coding artifacts using the proposed expert training approach. Our code is publicly available at https://github.com/albmarie/expert training.","",""
7,"Yulei Wu","Robust Learning-Enabled Intelligence for the Internet of Things: A Survey From the Perspectives of Noisy Data and Adversarial Examples",2021,"","","","",22,"2022-07-13 09:24:04","","10.1109/JIOT.2020.3018691","","",,,,,7,7.00,7,1,1,"The Internet of Things (IoT) has been widely adopted in a range of verticals, e.g., automation, health, energy, and manufacturing. Many of the applications in these sectors, such as self-driving cars and remote surgery, are critical and high stakes applications, calling for advanced machine learning (ML) models for data analytics. Essentially, the training and testing data that are collected by massive IoT devices may contain noise (e.g., abnormal data, incorrect labels, and incomplete information) and adversarial examples. This requires high robustness of ML models to make reliable decisions for IoT applications. The research of robust ML has received tremendous attention from both academia and industry in recent years. This article will investigate the state of the art and representative works of robust ML models that can enable high resilience and reliability of IoT intelligence. Two aspects of robustness will be focused on, i.e., when the training data of ML models contain noises and adversarial examples, which may typically happen in many real-world IoT scenarios. In addition, the reliability of both neural networks and reinforcement learning framework will be investigated. Both of these two ML paradigms have been widely used in handling data in IoT scenarios. The potential research challenges and open issues will be discussed to provide future research directions.","",""
16,"El Mahdi El Mhamdi, R. Guerraoui, Arsany Guirguis, Sébastien Rouault","SGD: Decentralized Byzantine Resilience",2019,"","","","",23,"2022-07-13 09:24:04","","","","",,,,,16,5.33,4,4,3,"The size of the datasets available today leads to distribute Machine Learning (ML) tasks. An SGD--based optimization is for instance typically carried out by two categories of participants: parameter servers and workers. Some of these nodes can sometimes behave arbitrarily (called \emph{Byzantine} and caused by corrupt/bogus data/machines), impacting the accuracy of the entire learning activity. Several approaches recently studied how to tolerate Byzantine workers, while assuming honest and trusted parameter servers. In order to achieve total ML robustness, we introduce GuanYu, the first algorithm (to the best of our knowledge) to handle Byzantine parameter servers as well as Byzantine workers. We prove that GuanYu ensures convergence against $\frac{1}{3}$ Byzantine parameter servers and $\frac{1}{3}$ Byzantine workers, which is optimal in asynchronous networks (GuanYu does also tolerate unbounded communication delays, i.e.\ asynchrony). To prove the Byzantine resilience of GuanYu, we use a contraction argument, leveraging geometric properties of the median in high dimensional spaces to prevent (with probability 1) any drift on the models within each of the non-Byzantine servers. % To convey its practicality, we implemented GuanYu using the low-level TensorFlow APIs and deployed it in a distributed setup using the CIFAR-10 dataset. The overhead of tolerating Byzantine participants, compared to a vanilla TensorFlow deployment that is vulnerable to a single Byzantine participant, is around 30\% in terms of throughput (model updates per second) - while maintaining the same convergence rate (model updates required to reach some accuracy).","",""
1,"Maria Khvalchik, Mikhail Galkin","Departamento de Nosotros: How Machine Translated Corpora Affects Language Models in MRC Tasks",2020,"","","","",24,"2022-07-13 09:24:04","","","","",,,,,1,0.50,1,2,2,"Pre-training large-scale language models (LMs) requires huge amounts of text corpora. LMs for English enjoy ever growing corpora of diverse language resources. However, less resourced languages and their mono- and multilingual LMs often struggle to obtain bigger datasets. A typical approach in this case implies using machine translation of English corpora to a target language. In this work, we study the caveats of applying directly translated corpora for fine-tuning LMs for downstream natural language processing tasks and demonstrate that careful curation along with post-processing lead to improved performance and overall LMs robustness. In the empirical evaluation, we perform a comparison of directly translated against curated Spanish SQuAD datasets on both user and system levels. Further experimental results on XQuAD and MLQA transfer-learning evaluation question answering tasks show that presumably multilingual LMs exhibit more resilience to machine translation artifacts in terms of the exact match score.","",""
0,"Sayat Mimar, G. Ghoshal","A sampling-guided unsupervised learning method to capture percolation in complex networks",2021,"","","","",25,"2022-07-13 09:24:04","","10.1038/s41598-022-07921-x","","",,,,,0,0.00,0,2,1,"","",""
11,"Ángel Alexander Cabrera, Fred Hohman, Jason Lin, Duen Horng Chau","Interactive Classification for Deep Learning Interpretation",2018,"","","","",26,"2022-07-13 09:24:04","","","","",,,,,11,2.75,3,4,4,"We present an interactive system enabling users to manipulate images to explore the robustness and sensitivity of deep learning image classifiers. Using modern web technologies to run in-browser inference, users can remove image features using inpainting algorithms and obtain new classifications in real time, which allows them to ask a variety of ""what if"" questions by experimentally modifying images and seeing how the model reacts. Our system allows users to compare and contrast what image regions humans and machine learning models use for classification, revealing a wide range of surprising results ranging from spectacular failures (e.g., a ""water bottle"" image becomes a ""concert"" when removing a person) to impressive resilience (e.g., a ""baseball player"" image remains correctly classified even without a glove or base). We demonstrate our system at The 2018 Conference on Computer Vision and Pattern Recognition (CVPR) for the audience to try it live. Our system is open-sourced at this https URL. A video demo is available at this https URL.","",""
6,"Dan Hendrycks, Andy Zou, Mantas Mazeika, Leonard Tang, Bo Li, D. Song, J. Steinhardt","PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures",2021,"","","","",27,"2022-07-13 09:24:04","","","","",,,,,6,6.00,1,7,1,"In real-world applications of machine learning, reliable and safe systems must consider measures of performance beyond standard test set accuracy. These other goals include out-of-distribution (OOD) robustness, prediction consistency, resilience to adversaries, calibrated uncertainty estimates, and the ability to detect anomalous inputs. How-ever, improving performance towards these goals is often a balancing act that today’s methods cannot achieve without sacriﬁcing performance on other safety axes. For instance, adversarial training improves adversarial robustness but sharply degrades other classiﬁer performance metrics. Similarly, strong data augmentation and regularization techniques often improve OOD robustness but harm anomaly detection, raising the question of whether a Pareto improvement on all existing safety measures is possible. To meet this challenge, we design a new data augmentation strategy utilizing the natural structural complexity of pictures such as fractals, which outperforms numerous baselines, is near Pareto-optimal, and roundly improves safety measures. state-of-the-art Outlier Exposure method without requiring a large, diverse dataset of known outliers. demonstrating that P IX M IX can improve uncertainty estimation under distribution shifts with unseen image corruptions.","",""
2,"D. Berrar","On the Noise Resilience of Ranking Measures",2016,"","","","",28,"2022-07-13 09:24:04","","10.1007/978-3-319-46672-9_6","","",,,,,2,0.33,2,1,6,"","",""
4,"A. Wheeldon, A. Yakovlev, R. Shafik, Jordan Morris","Low-Latency Asynchronous Logic Design for Inference at the Edge",2020,"","","","",29,"2022-07-13 09:24:04","","10.23919/DATE51398.2021.9474126","","",,,,,4,2.00,1,4,2,"Modern internet of things (IoT) devices leverage machine learning inference using sensed data on-device rather than offloading them to the cloud. Commonly known as inference at-the-edge, this gives many benefits to the users, including personalization and security. However, such applications demand high energy efficiency and robustness. In this paper we propose a method for reduced area and power overhead of self-timed early-propagative asynchronous inference circuits, designed using the principles of learning automata. Due to natural resilience to timing as well as logic underpinning, the circuits are tolerant to variations in environment and supply voltage whilst enabling the lowest possible latency. Our method is exemplified through an inference datapath for a low power machine learning application. The circuit builds on the Tsetlin machine algorithm further enhancing its energy efficiency. Average latency of the proposed circuit is reduced by 10× compared with the synchronous implementation whilst maintaining similar area. Robustness of the proposed circuit is proven through post-synthesis simulation with 0.25 V to 1.2 V supply. Functional correctness is maintained and latency scales with gate delay as voltage is decreased.","",""
2,"Zhiwei Gao, Michael Z. Q. Chen, M. A. Henson","Advances in Condition Monitoring, Optimization and Control for Complex Industrial Processes",2021,"","","","",30,"2022-07-13 09:24:04","","10.3390/books978-3-0365-0689-0","","",,,,,2,2.00,1,3,1,"Complex dynamic analysis of industrial processes Condition monitoring techniques Machine learning-based diagnosis and prognosis algorithms Advanced optimization methodologies Resilience of cyber–physical systems Advanced control algorithms and robustness analysis with applications Accelerating multiscale dynamic simulations of industrial processes Validation and real-time applications an Open Access Journal by MDPI","",""
1,"Sandhya Aneja, Nagender Aneja, Pg Emeroylariffion Abas, A. G. Naim","IAES International Journal of Artificial Intelligence (IJ-AI)",2021,"","","","",31,"2022-07-13 09:24:04","","","","",,,,,1,1.00,0,4,1,"Received Aug 22, 2021 Revised May 20, 2022 Accepted Jun 6, 2022 Despite substantial advances in network architecture performance, the susceptibility of adversarial attacks makes deep learning challenging to implement in safety-critical applications. This paper proposes a data-centric approach to addressing this problem. A nonlocal denoising method with different luminance values has been used to generate adversarial examples from the Modified National Institute of Standards and Technology database (MNIST) and Canadian Institute for Advanced Research (CIFAR-10) data sets. Under perturbation, the method provided absolute accuracy improvements of up to 9.3% in the MNIST data set and 13% in the CIFAR10 data set. Training using transformed images with higher luminance values increases the robustness of the classifier. We have shown that transfer learning is disadvantageous for adversarial machine learning. The results indicate that simple adversarial examples can improve resilience and make deep learning easier to apply in various applications.","",""
0,"Abdul Hannan, Christian Gruhl, B. Sick","Anomaly based Resilient Network Intrusion Detection using Inferential Autoencoders",2021,"","","","",32,"2022-07-13 09:24:04","","10.1109/CSR51186.2021.9527980","","",,,,,0,0.00,0,3,1,"This article focuses on the application of conditional variational autoencoders as anomaly detectors to identify emerging threats in computer networks. Autoencoders are machine learning techniques that are used to find lower-dimensional representations, i.e. an encoding in latent space, from input space. With variational Autoencoders (VAE) this representation is not a single code word or vector but a probability distribution – greatly improving the robustness of the coding scheme. In contrast to VAE, we present a conditional variational autoencoder (CVAE), which uses the latent representation to encode regular and malicious network traffic into a bimodal distribution. While regular autoencoders are unsupervised, we require some labeled data to tune the bimodal representations, thus turning the learning problem into a semi-supervised classification task. However, unknown threats (i.e. those not contained in labeled training data) can be detected as well. In our presented case study, based on available computer network datasets (KDD99 and CIC-IDS2017), we could improve the detection of unknown threats compared to conventional approaches. Our experiments are publicly available.","",""
0,"Adri'an P'erez-Salinas","Algorithmic Strategies for seizing Quantum Computing",2021,"","","","",33,"2022-07-13 09:24:04","","","","",,,,,0,0.00,0,1,1,"Quantum computing is a nascent technology with prospects to have a huge impact in the world. Its current status, however, only counts on small and noisy quantum computers whose performance is limited. In this thesis, two different strategies are explored to take advantage of inherently quantum properties and propose recipes to seize quantum computing since its advent. First, the re-uploading strategy is a variational algorithm related to machine learning. It consists in introducing data several times along a computation accompanied by tunable parameters. This process permits the circuit to learn and mimic any behavior. This capability emerges naturally from the quantum properties of the circuit. Second, the unary strategy aims to reduce the density of information stored in a quantum circuit to increase its resilience against noise. This trade-off between performance and robustness brings an advantage for noisy devices, where small but meaningful quantum speed-ups can be found.","",""
0,"Omri Armstrong, Ran Gilad-Bachrach","Robust Model Compression Using Deep Hypotheses",2021,"","","","",34,"2022-07-13 09:24:04","","","","",,,,,0,0.00,0,2,1,"Machine Learning models should ideally be compact and ro- bust. Compactness provides efﬁciency and comprehensibil-ity whereas robustness provides resilience. Both topics have been studied in recent years but in isolation. Here we present a robust model compression scheme which is independent of model types: it can compress ensembles, neural networks and other types of models into diverse types of small mod- els. The main building block is the notion of depth derived from robust statistics. Originally, depth was introduced as a measure of the centrality of a point in a sample such that the median is the deepest point. This concept was extended to classiﬁcation functions which makes it possible to deﬁne the depth of a hypothesis and the median hypothesis. Algo- rithms have been suggested to approximate the median but they have been limited to binary classiﬁcation. In this study, we present a new algorithm, the Multiclass Empirical Median Optimization (MEMO) algorithm that ﬁnds a deep hypothesis in multi-class tasks, and prove its correctness. This leads to our Compact Robust Estimated Median Belief Optimization (CREMBO) algorithm for robust model compres- sion. We demonstrate the success of this algorithm empirically by compressing neural networks and random forests into small decision trees, which are interpretable models, and show that they are more accurate and robust than other com- parable methods. In addition, our empirical study shows that our method outperforms Knowledge Distillation on DNN to DNN compression.","",""
0,"S. Mohr, Konstantina Drainas, J. Geist","Assessment of Neural Networks for Stream-Water-Temperature Prediction",2021,"","","","",35,"2022-07-13 09:24:04","","10.1109/ICMLA52953.2021.00147","","",,,,,0,0.00,0,3,1,"Climate change results in altered air and water temperatures. Increases affect physicochemical properties, such as oxygen concentration, and can shift species distribution and survival, with consequences for ecosystem functioning and services. These ecosystem services have integral value for humankind and are forecasted to alter under climate warming. A mechanistic understanding of the drivers and magnitude of expected changes is essential in identifying system resilience and mitigation measures. In this work, we present a selection of state-of-the-art Neural Networks (NN) for the prediction of water temperatures in six streams in Germany. We show that the use of methods that compare observed and predicted values, exemplified with the Root Mean Square Error (RMSE), is not sufficient for their assessment. Hence we introduce additional analysis methods for our models to complement the state-of-the-art metrics. These analyses evaluate the NN’s robustness, possible maximal and minimal values, and the impact of single input parameters on the output. We thus contribute to understanding the processes within the NN and help applicants choose architectures and input parameters for reliable water temperature prediction models.","",""
0,"Sandhya Aneja, Nagender Aneja, Pg Emeroylariffion Abas, A. G. Naim","Defense against adversarial attacks on deep convolutional neural networks through nonlocal denoising",2022,"","","","",36,"2022-07-13 09:24:04","","10.11591/ijai.v11.i3.pp961-968","","",,,,,0,0.00,0,4,1,"Despite substantial advances in network architecture performance, the susceptibility of adversarial attacks makes deep learning challenging to implement in safety-critical applications. This paper proposes a data-centric approach to addressing this problem. A nonlocal denoising method with different luminance values has been used to generate adversarial examples from the Modified National Institute of Standards and Technology database (MNIST) and Canadian Institute for Advanced Research (CIFAR-10) data sets. Under perturbation, the method provided absolute accuracy improvements of up to 9.3% in the MNIST data set and 13% in the CIFAR-10 data set. Training using transformed images with higher luminance values increases the robustness of the classifier. We have shown that transfer learning is disadvantageous for adversarial machine learning. The results indicate that simple adversarial examples can improve resilience and make deep learning easier to apply in various applications.","",""
0,"Şerban Vădineanu, M. Nasri","Robust and accurate regression-based techniques for period inference in real-time systems",2022,"","","","",37,"2022-07-13 09:24:04","","10.1007/s11241-022-09385-8","","",,,,,0,0.00,0,2,1,"","",""
2,"Şerban Vădineanu, M. Nasri","Robust and Accurate Period Inference using Regression-Based Techniques",2020,"","","","",38,"2022-07-13 09:24:04","","10.1109/RTSS49844.2020.00040","","",,,,,2,1.00,1,2,2,"With the growth in complexity of real-time embedded systems, there is an increasing need for tools and techniques to understand and compare the observed runtime behavior of a system with the expected one. Since many real-time applications require periodic interactions with the environment, one of the fundamental problems in guaranteeing their temporal correctness is to be able to infer the periodicity of certain events in the system. The practicability of a period inference tool, however, depends on both its accuracy and robustness (also its resilience) against noise in the output trace of the system, e.g., when the system trace is impacted by the presence of aperiodic tasks, release jitters, and runtime variations in the execution time of the tasks. This work (i) presents the first period inference framework that uses regression-based machine-learning (RBML) methods, and (ii) thoroughly investigates the accuracy and robustness of different families of RBML methods in the presence of uncertainties in the system parameters. We show, on both synthetically generated traces and traces from actual systems, that our solutions can reduce the error of period estimation by two to three orders of magnitudes w.r.t. state of the art.","",""
0,"Şerban Vădineanu","Deriving Timing Properties from System Traces using Data-driven Techniques",2020,"","","","",39,"2022-07-13 09:24:04","","","","",,,,,0,0.00,0,1,2,"With the growth in the complexity of real-time embedded systems, there is an increasing need for tools and techniques to understand and compare the observed runtime behavior of a system with the expected one. Since many realtime applications require periodic interactions with the environment, one of the fundamental problems in guaranteeing/monitoring their temporal correctness is to be able to infer the periodicity of certain events in the system. The practicability of a period inference tool, however, depends on both its accuracy and robustness (resilience) against noise in the output trace of the system, e.g., when the system trace is impacted by events that have a non-deterministic nature such as the presence of aperiodic tasks, release jitters, and runtime execution-time variations of the tasks. This work (i) presents a period inference framework that uses regressionbased machine-learning (RBML) methods, (ii) thoroughly investigates the accuracy and robustness of different families of RBML methods in the presence of uncertainties in the system parameters, and (iii) proposes further accuracy improvements by deriving candidate pruning rules based on the inherent properties of the underlying scheduling policies. We show, on both synthetically generated traces and traces from actual systems, that our solutions can reduce the error of period estimation by two to three orders of magnitudes w.r.t. state of the art. Also, our methods showed to be robust against most sources of disturbance.","",""
0,"Shadi Sheikhfaal, Steven D. Pyle, Soheil Salehi, R. Demara","An Ultra-Low Power Spintronic Stochastic Spiking Neuron with Self-Adaptive Discrete Sampling",2019,"","","","",40,"2022-07-13 09:24:04","","10.1109/MWSCAS.2019.8884915","","",,,,,0,0.00,0,4,3,"State-of-the-art machine learning models have achieved impressive feats of narrow intelligence, but have yet to realize the computational generality, adaptability, and power efficiency of biological brains. Thus, this work aims to improve current neural network models by leveraging the principle that the cortex consists of noisy and imprecise components in order to realize an ultra-low-power stochastic spiking neural circuit that resembles biological neuronal behavior. By utilizing probabilistic spintronics to provide true stochasticity in a compact CMOS-compatible device, an Adaptive Ring Oscillator for as-needed discrete sampling, and a homeostasis mechanism to reduce power consumption, provide additional biological characteristics, and improve process variation resilience, this subthreshold circuit is able to generate sub-nanosecond spiking behavior with biological characteristics at 200mV, using less than 80nW, along with behavioral robustness to process variation.","",""
1,"Erick Martínez, Bella Oh, Feng Li, Xiao Luo","Evading Deep Neural Network and Random Forest Classifiers by Generating Adversarial Samples",2018,"","","","",41,"2022-07-13 09:24:04","","10.1007/978-3-030-18419-3_10","","",,,,,1,0.25,0,4,4,"","",""
3,"Jonathan Muehlstein, Yehonatan Zion, Maor Bahumi, Itay Kirshenboim, Ran Dubin, A. Dvir, Ofir Pele","Analyzing HTTPS Traffic for a Robust Identification of Operating System, Browser and Application",2016,"","","","",42,"2022-07-13 09:24:04","","","","",,,,,3,0.50,0,7,6,"Desktops and laptops can be maliciously exploited to violate privacy. There are two main types of attack scenarios: active and passive. In this paper, we consider the passive scenario where the adversary does not interact actively with the device, but he is able to eavesdrop on the network traffic of the device from the network side. Most of the Internet traffic is encrypted and thus passive attacks are challenging. In this paper, we show that an external attacker can robustly identify the operating system, browser and application of HTTP encrypted traffic (HTTPS). We provide a large dataset of more than 20,000 examples for this task. We present a comprehensive evaluation of traffic features including new ones and machine learning algorithms. We run a comprehensive set of experiments, which shows that our classification accuracy is 96.06%. Due to the adaptive nature of the problem, we also investigate the robustness and resilience to changes of features due to different network conditions (e.g., VPN) at test time and the effect of small training set on the accuracy. We show that our proposed solution is robust to these changes.","",""
1,"Phuong T. Nguyen, Davide Di Ruscio, Juri Di Rocco, Claudio Di Sipio, M. Di Penta","Adversarial Machine Learning: On the Resilience of Third-party Library Recommender Systems",2021,"","","","",43,"2022-07-13 09:24:04","","10.1145/3463274.3463809","","",,,,,1,1.00,0,5,1,"In recent years, we have witnessed a dramatic increase in the application of Machine Learning algorithms in several domains, including the development of recommender systems for software engineering (RSSE). While researchers focused on the underpinning ML techniques to improve recommendation accuracy, little attention has been paid to make such systems robust and resilient to malicious data. By manipulating the algorithms’ training set, i.e., large open-source software (OSS) repositories, it would be possible to make recommender systems vulnerable to adversarial attacks. This paper presents an initial investigation of adversarial machine learning and its possible implications on RSSE. As a proof-of-concept, we show the extent to which the presence of manipulated data can have a negative impact on the outcomes of two state-of-the-art recommender systems which suggest third-party libraries to developers. Our work aims at raising awareness of adversarial techniques and their effects on the Software Engineering community. We also propose equipping recommender systems with the capability to learn to dodge adversarial activities.","",""
0,"Susmit Jha, Brian Jalaian, Anirban Roy, Gunjan Verma","Trinity: Trust, Resilience and Interpretability of Machine Learning Models",2021,"","","","",44,"2022-07-13 09:24:04","","10.1002/9781119723950.ch16","","",,,,,0,0.00,0,4,1,"","",""
3,"J. Xie, Inalvis Alvarez-Fernandez, Wei Sun","A Review of Machine Learning Applications in Power System Resilience",2020,"","","","",45,"2022-07-13 09:24:04","","10.1109/PESGM41954.2020.9282137","","",,,,,3,1.50,1,3,2,"The integration of power electronics enabled devices and the high penetration of renewable energy drastically increase the complexity of power system operation and control. Power systems are still vulnerable to large-scale blackouts caused by extreme natural events or man-made attacks. With the recent development in artificial intelligence technique, machine learning has shown a processing ability in computational, perceptual and cognitive intelligence. It is an urgent challenge to integrate the advanced machine learning technology and large amount of real-time data from wide area measurement systems and intelligent electronic devices, in order to effectively enhance power system resilience and ensure the reliable and secure operation of power systems. Therefore, this paper aims to systematically review the existing application of machine learning methods on power system resilience enhancement, to expand the interest of researchers and scholars in this topic, and to jointly promote the application of artificial intelligence in the field of power systems.","",""
0,"Lauren J. Wong, E. Altland, Joshua Detwiler, Paolo Fermin, Julia Mahon Kuzin, Nathan Moeliono, A. S. Abdalla, William C. Headley, Alan J. Michaels","Resilience Improvements for Space-Based Radio Frequency Machine Learning",2020,"","","","",46,"2022-07-13 09:24:04","","10.1109/ISNCC49221.2020.9297212","","",,,,,0,0.00,0,9,2,"Recent work has quantified the degradations that occur in convolutional neural nets (CNN) deployed in harsh environments like space-based image or radio frequency (RF) processing applications. Such degradations yield a robust correlation and causality between single-event upset (SEU) induced errors in memory weights of on-orbit CNN implementations. However, minimal considerations have been given to how the resilience of CNNs can be improved algorithmically as opposed to via enhanced hardware. This paper focuses on RF-processing CNNs and performs an in-depth analysis of applying software-based error detection and correction mechanisms, which may subsequently be combined with protections of radiation-hardened processor platforms. These techniques are more accessible for low cost smallsat platforms than ruggedized hardware. Additionally, methods for minimizing the memory and computational complexity of the resulting resilience techniques are identified. Combined with periodic scrubbing, the resulting techniques are shown to improve expected lifetimes of CNN-based RF-processing algorithms by several orders of magnitude.","",""
36,"K. Schultebraucks, I. Galatzer-Levy","Machine Learning for Prediction of Posttraumatic Stress and Resilience Following Trauma: An Overview of Basic Concepts and Recent Advances.",2019,"","","","",47,"2022-07-13 09:24:04","","10.1002/jts.22384","","",,,,,36,12.00,18,2,3,"Posttraumatic stress responses are characterized by a heterogeneity in clinical appearance and etiology. This heterogeneity impacts the field's ability to characterize, predict, and remediate maladaptive responses to trauma. Machine learning (ML) approaches are increasingly utilized to overcome this foundational problem in characterization, prediction, and treatment selection across branches of medicine that have struggled with similar clinical realities of heterogeneity in etiology and outcome, such as oncology. In this article, we review and evaluate ML approaches and applications utilized in the areas of posttraumatic stress, stress pathology, and resilience research, and present didactic information and examples to aid researchers interested in the relevance of ML to their own research. The examined studies exemplify the high potential of ML approaches to build accurate predictive and diagnostic models of posttraumatic stress and stress pathology risk based on diverse sources of available information. The use of ML approaches to integrate high-dimensional data demonstrates substantial gains in risk prediction even when the sources of data are the same as those used in traditional predictive models. This area of research will greatly benefit from collaboration and data sharing among researchers of posttraumatic stress disorder, stress pathology, and resilience.","",""
312,"Jonas Rauber, Wieland Brendel, M. Bethge","Foolbox: A Python toolbox to benchmark the robustness of machine learning models",2017,"","","","",48,"2022-07-13 09:24:04","","","","",,,,,312,62.40,104,3,5,"Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox. The most up-to-date documentation can be found at http://foolbox.readthedocs.io. In 2013, Szegedy et al. demonstrated that minimal perturbations, often almost imperceptible to humans, can have devastating effects on machine predictions. These so-called adversarial perturbations thus demonstrate a striking difference between human and machine perception. As a result, adversarial perturbations have been subject to many Equal contribution Centre for Integrative Neuroscience, University of Tübingen, Germany Bernstein Center for Computational Neuroscience, Tübingen, Germany International Max Planck Research School for Intelligent Systems, Tübingen, Germany Max Planck Institute for Biological Cybernetics, Tübingen, Germany Institute for Theoretical Physics, University of Tübingen, Germany. Correspondence to: Jonas Rauber <jonas.rauber@bethgelab.org>. Reliable Machine Learning in the Wild Workshop, 34 th International Conference on Machine Learning, Sydney, Australia, 2017. studies concerning the generation of such perturbations and strategies to protect machine learning models such as deep neural networks against them. A practical definition of the robustness R of a model, first used by Szegedy et al. (2013), is the average size of the minimum adversarial perturbation ρ(x) across many samples x, R = 〈ρ(x)〉 x where (1) ρ(x) = min δ d(x,x+ δ) s.t. x+ δ is adversarial (2) and d(·) is some distance measure. Unfortunately, finding the global minimum adversarial perturbation is close to impossible in any practical setting, and we thus employ heuristic attacks to find a suitable approximation. Such heuristics, however, can fail, in which case we could easily be mislead to believe that a model is robust (Brendel & Bethge, 2017). Our best strategy is thus to employ as many attacks as possible, and to use the minimal perturbation found across all attacks as an approximation to the true global minimum. At the moment, however, such a strategy is severely obstructed by two problems: first, the code for most known attack methods is either not available at all, or only available for one particular deep learning framework. Second, implementations of the same attack often differ in many details and are thus not directly comparable. Foolbox improves upon the existing Python package cleverhans by Papernot et al. (2016b) in three important aspects: 1. It interfaces with most popular machine learning frameworks such as PyTorch, Keras, TensorFlow, Theano, Lasagne and MXNet and provides a straight forward way to add support for other frameworks, 2. it provides reference implementations for more than 15 adversarial attacks with a simple and consistent API, and 3. it supports many different criteria for adversarial examples, including custom ones. This technical report is structured as follows: In section 1 we provide an overview over Foolbox and demonstrate Foolbox: A Python toolbox to benchmark the robustness of machine learning models how to benchmark a model and report the result. In section 2 we describe the adversarial attack methods that are implemented in Foolbox and explain the internal hyperparameter tuning.","",""
77,"A. Qayyum, Junaid Qadir, M. Bilal, A. Al-Fuqaha","Secure and Robust Machine Learning for Healthcare: A Survey",2020,"","","","",49,"2022-07-13 09:24:04","","10.1109/RBME.2020.3013489","","",,,,,77,38.50,19,4,2,"Recent years have witnessed widespread adoption of machine learning (ML)/deep learning (DL) techniques due to their superior performance for a variety of healthcare applications ranging from the prediction of cardiac arrest from one-dimensional heart signals to computer-aided diagnosis (CADx) using multi-dimensional medical images. Notwithstanding the impressive performance of ML/DL, there are still lingering doubts regarding the robustness of ML/DL in healthcare settings (which is traditionally considered quite challenging due to the myriad security and privacy issues involved), especially in light of recent results that have shown that ML/DL are vulnerable to adversarial attacks. In this paper, we present an overview of various application areas in healthcare that leverage such techniques from security and privacy point of view and present associated challenges. In addition, we present potential methods to ensure secure and privacy-preserving ML for healthcare applications. Finally, we provide insight into the current research challenges and promising directions for future research.","",""
1,"You-Cheng Lai, Chun-Yen Yao, Shao-Hong Yang, Ying-Wei Wu, Tsung-Te Liu","A Robust Area-Efficient Physically Unclonable Function With High Machine Learning Attack Resilience in 28-nm CMOS",2022,"","","","",50,"2022-07-13 09:24:04","","10.1109/tcsi.2021.3098018","","",,,,,1,1.00,0,5,1,"Strong physically unclonable function (PUF) offers a promising solution to low-cost hardware identification and authentication for Internet of Things (IoT) applications. The continuous advancement of machine learning (ML) technology makes the PUF resilience to ML attacks a major design priority. This paper presents a robust and area-efficient strong PUF design with high ML attack resilience. The proposed PUF architecture based on inverter amplifiers operating in the subthreshold region achieves both low energy consumption and high supply and temperature scalability. The proposed nonlinearity topology effectively enhances PUF resilience to various ML attacks with low implementation area and cost. The proposed strong PUF design was designed and implemented using a 28-nm CMOS process. The measurement results show that the proposed PUF design achieves a nearly ideal ML attack resilience of 49.96 % with a small area of 239,857 F<sup>2</sup>, and demonstrates a stable operation across a wide range of supply voltage from 0.5–1.4 V and temperature from −40–100 °C. This represents <inline-formula> <tex-math notation=""LaTeX"">$3\times $ </tex-math></inline-formula> improvement in area efficiency, <inline-formula> <tex-math notation=""LaTeX"">$2.25\times $ </tex-math></inline-formula> and <inline-formula> <tex-math notation=""LaTeX"">$1.08\times $ </tex-math></inline-formula> improvement in operating voltage and temperature range, respectively, compared to the state-of-the-art results.","",""
0,"S. Samadi, M. Taslimi","Develop a situation-based prioritization program as a road map to enhance the pre-resilience in flood management using machine learning methods",2022,"","","","",51,"2022-07-13 09:24:04","","10.1108/ijdrbe-12-2021-0161","","",,,,,0,0.00,0,2,1," Purpose This study aims to review the features and challenges of the flood relief chain, identifies administrative measures during and after the flood occurrence and prioritizes them using two machine learning (ML) and analytic hierarchy process (AHP) methods. This paper aims to provide a prioritization program based on flood conditions that optimize flood management and improves society’s resilience against flood occurrence.   Design/methodology/approach The collected database in this paper has been trained by using ML algorithms, including support vector machine (SVM), Naive Bayes (NB) and k-nearest neighbors (kNN), to create a prioritization program. Furthermore, the administrative measures in two phases of during and after the flood are prioritized by using the AHP method and questionnaires completed by experts and relief workers in flood management.   Findings Among the ML algorithms, the SVM method was selected with 91.37% accuracy. The prioritization program provided by the model, which distinguishes it from other existing models, considers five conditions of the flood occurrence to prioritize actions (season, population affected, area affected, damage to houses and human lives lost). Therefore, the model presents a specific plan for each flood with different occurrence conditions.   Research limitations/implications The main limitation is the lack of a comprehensive data set to determine the effect of all flood conditions on the prioritization program and the relief activities that have been done in previous flood disasters.   Originality/value The originality of this paper is the use of ML methods to prioritize administrative measures during and after the flood and presents a prioritization program based on each flood’s conditions. Therefore, through this program, the authority and society can control the adverse impacts of flood more effectively and help to reduce human and financial losses as much as possible. ","",""
0,"Junbo Wang, Amitangshu Pal, Qinglin Yang, K. Kant, Kaiming Zhu, Song Guo","Collaborative Machine Learning: Schemes, Robustness, and Privacy.",2022,"","","","",52,"2022-07-13 09:24:04","","10.1109/TNNLS.2022.3169347","","",,,,,0,0.00,0,6,1,"Distributed machine learning (ML) was originally introduced to solve a complex ML problem in a parallel way for more efficient usage of computation resources. In recent years, such learning has been extended to satisfy other objectives, namely, performing learning in situ on the training data at multiple locations and keeping the training datasets private while still allowing sharing of the model. However, these objectives have led to considerable research on the vulnerabilities of distributed learning both in terms of privacy concerns of the training data and the robustness of the learned overall model due to bad or maliciously crafted training data. This article provides a comprehensive survey of various privacy, security, and robustness issues in distributed ML.","",""
49,"Eric Wong, J. Z. Kolter","Learning perturbation sets for robust machine learning",2020,"","","","",53,"2022-07-13 09:24:04","","","","",,,,,49,24.50,25,2,2,"Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline spatial transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to train models which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at this https URL.","",""
270,"Jonas Rauber, Wieland Brendel, M. Bethge","Foolbox v0.8.0: A Python toolbox to benchmark the robustness of machine learning models",2017,"","","","",54,"2022-07-13 09:24:04","","","","",,,,,270,54.00,90,3,5,"Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at this https URL . The most up-to-date documentation can be found at this http URL .","",""
13,"M. Grassia, M. Domenico, G. Mangioni","Machine learning dismantling and early-warning signals of disintegration in complex systems",2021,"","","","",55,"2022-07-13 09:24:04","","10.1038/s41467-021-25485-8","","",,,,,13,13.00,4,3,1,"","",""
17,"Mathias Lécuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel J. Hsu, S. Jana","On the Connection between Differential Privacy and Adversarial Robustness in Machine Learning",2018,"","","","",56,"2022-07-13 09:24:04","","","","",,,,,17,4.25,3,5,4,"Adversarial examples in machine learning has been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best-effort, heuristic approaches that have all been shown to be vulnerable to sophisticated attacks. More recently, rigorous defenses that provide formal guarantees have emerged, but are hard to scale or generalize. A rigorous and general foundation for designing defenses is required to get us off this arms race trajectory. We propose leveraging differential privacy (DP) as a formal building block for robustness against adversarial examples. We observe that the semantic of DP is closely aligned with the formal definition of robustness to adversarial examples. We propose PixelDP, a strategy for learning robust deep neural networks based on formal DP guarantees. PixelDP networks give theoretical guarantees for a subset of their predictions regarding the robustness against adversarial perturbations of bounded size. Our evaluation with MNIST, CIFAR-10, and CIFAR-100 shows that PixelDP networks achieve accuracy under attack on par with the best-performing defense to date, but additionally certify robustness against meaningful-size 1-norm and 2-norm attacks for 40-60% of their predictions. Our experience points to DP as a rigorous, broadly applicable, and mechanism-rich foundation for robust machine learning.","",""
165,"S. Ardabili, A. Mosavi, Pedram Ghamisi, F. Ferdinand, A. Várkonyi-Kóczy, U. Reuter, T. Rabczuk, P. Atkinson","COVID-19 Outbreak Prediction with Machine Learning",2020,"","","","",57,"2022-07-13 09:24:04","","10.1101/2020.04.17.20070094","","",,,,,165,82.50,21,8,2,"Several outbreak prediction models for COVID-19 are being used by officials around the world to make informed-decisions and enforce relevant control measures. Among the standard models for COVID-19 global pandemic prediction, simple epidemiological and statistical models have received more attention by authorities, and they are popular in the media. Due to a high level of uncertainty and lack of essential data, standard models have shown low accuracy for long-term prediction. Although the literature includes several attempts to address this issue, the essential generalization and robustness abilities of existing models needs to be improved. This paper presents a comparative analysis of machine learning and soft computing models to predict the COVID-19 outbreak. Among a wide range of machine learning models investigated, two models showed promising results (i.e., multi-layered perceptron, MLP, and adaptive network-based fuzzy inference system, ANFIS). Based on the results reported here, and due to the highly complex nature of the COVID-19 outbreak and variation in its behavior from nation-to-nation, this study suggests machine learning as an effective tool to model the outbreak.","",""
18,"I. Galatzer-Levy, K. Ruggles, Zhe Chen","Data Science in the Research Domain Criteria Era: Relevance of Machine Learning to the Study of Stress Pathology, Recovery, and Resilience",2018,"","","","",58,"2022-07-13 09:24:04","","10.1177/2470547017747553","","",,,,,18,4.50,6,3,4,"Diverse environmental and biological systems interact to influence individual differences in response to environmental stress. Understanding the nature of these complex relationships can enhance the development of methods to (1) identify risk, (2) classify individuals as healthy or ill, (3) understand mechanisms of change, and (4) develop effective treatments. The Research Domain Criteria initiative provides a theoretical framework to understand health and illness as the product of multiple interrelated systems but does not provide a framework to characterize or statistically evaluate such complex relationships. Characterizing and statistically evaluating models that integrate multiple levels (e.g. synapses, genes, and environmental factors) as they relate to outcomes that are free from prior diagnostic benchmarks represent a challenge requiring new computational tools that are capable to capture complex relationships and identify clinically relevant populations. In the current review, we will summarize machine learning methods that can achieve these goals.","",""
37,"Ziv Katzir, Y. Elovici","Quantifying the resilience of machine learning classifiers used for cyber security",2018,"","","","",59,"2022-07-13 09:24:04","","10.1016/j.eswa.2017.09.053","","",,,,,37,9.25,19,2,4,"","",""
9,"Mehran Goli, Jannis Stoppe, R. Drechsler","Resilience Evaluation for Approximating SystemC Designs Using Machine Learning Techniques",2018,"","","","",60,"2022-07-13 09:24:04","","10.1109/RSP.2018.8631997","","",,,,,9,2.25,3,3,4,"As digital circuits have become more complicated than ever, abstract description languages such as SystemC have been introduced, allowing designers to work on more abstract levels during the design process. Design metrics such as performance and energy consumption are a central concern for designers at all levels of abstraction. Approximate computing is a promising way to optimize these criteria, sacrificing accuracy. Defining which parts of a design can be approximated (and to what degree) is a crucial and non-trivial design decision, which is usually connected to a larger programming effort, especially when exploring the design space manually. In this paper, we propose an automated approach based on machine learning techniques in order to detect the resilience of a given SystemC design's modules. This is used to identify components of the design that can be approximated. The effectiveness of the proposed method is evaluated using several SystemC benchmarks from various domains.","",""
4,"Vahid Behzadan, W. Hsu","RL-Based Method for Benchmarking the Adversarial Resilience and Robustness of Deep Reinforcement Learning Policies",2019,"","","","",61,"2022-07-13 09:24:04","","10.1007/978-3-030-26250-1_25","","",,,,,4,1.33,2,2,3,"","",""
1,"T. Klemas, Steve Chan","Harnessing Machine Learning, Data Analytics, and Computer-Aided Testing for Cyber Security Applications Achieving Sustained Cyber Resilience for Typical Attack Surface Configurations and Environments",2018,"","","","",62,"2022-07-13 09:24:04","","","","",,,,,1,0.25,1,2,4,"While media reports frequently highlight the exciting aspects of the cyber security field, many cyber security tasks are quite tedious and repetitive. At the same time, however, strong pattern recognition, deductive reasoning, and inference skills are required, as well as a high degree of situational awareness. As a direct consequence, the field of cyber security is replete with potential opportunities to apply data analytics, machine learning, computer aided testing, and other advanced approaches to reduce the frustration of cyber security operators by easing key challenges. In fact, given a typical range of cyber attack surfaces, leveraging these machine-enhanced analysis and decision approaches in conjunction with a robust defense-in-depth posture is a crucial step towards achieving sustained, predictable performance across typical cyber security tasks and promotes cyber resilience. This paper will both outline details for a near-term research effort and explore a variety of key opportunities to exploit these approaches with the objective of raising awareness, providing initial guidance to aid potential adopters, and developing effective strategies to incorporate them into existing cyber security constructs. Keywordsartificial intelligence; expert systems, machine learning; supervised learning, unsupervised learning, pattern recognition, spectral methods, k-means, modularity, Lagrange multiplier, optimization, anomaly detection, data analytics, data science, networks, cyber security operator, cyber defensive tools, cyber resilience.","",""
117,"Xiao Chen, Chaoran Li, Derui Wang, S. Wen, Jun Zhang, S. Nepal, Yang Xiang, K. Ren","Android HIV: A Study of Repackaging Malware for Evading Machine-Learning Detection",2018,"","","","",63,"2022-07-13 09:24:04","","10.1109/TIFS.2019.2932228","","",,,,,117,29.25,15,8,4,"Machine learning-based solutions have been successfully employed for the automatic detection of malware on Android. However, machine learning models lack robustness to adversarial examples, which are crafted by adding carefully chosen perturbations to the normal inputs. So far, the adversarial examples can only deceive detectors that rely on syntactic features (e.g., requested permissions, API calls, etc.), and the perturbations can only be implemented by simply modifying application’s manifest. While recent Android malware detectors rely more on semantic features from Dalvik bytecode rather than manifest, existing attacking/defending methods are no longer effective. In this paper, we introduce a new attacking method that generates adversarial examples of Android malware and evades being detected by the current models. To this end, we propose a method of applying optimal perturbations onto Android APK that can successfully deceive the machine learning detectors. We develop an automated tool to generate the adversarial examples without human intervention. In contrast to existing works, the adversarial examples crafted by our method can also deceive recent machine learning-based detectors that rely on semantic features such as control-flow-graph. The perturbations can also be implemented directly onto APK’s Dalvik bytecode rather than Android manifest to evade from recent detectors. We demonstrate our attack on two state-of-the-art Android malware detection schemes, MaMaDroid and Drebin. Our results show that the malware detection rates decreased from 96% to 0% in MaMaDroid, and from 97% to 0% in Drebin, with just a small number of codes to be inserted into the APK.","",""
15,"Shuyan Zhou, Xiangkai Zeng, Yingqi Zhou, Antonios Anastasopoulos, Graham Neubig","Improving Robustness of Neural Machine Translation with Multi-task Learning",2019,"","","","",64,"2022-07-13 09:24:04","","10.18653/v1/W19-5368","","",,,,,15,5.00,3,5,3,"While neural machine translation (NMT) achieves remarkable performance on clean, in-domain text, performance is known to degrade drastically when facing text which is full of typos, grammatical errors and other varieties of noise. In this work, we propose a multi-task learning algorithm for transformer-based MT systems that is more resilient to this noise. We describe our submission to the WMT 2019 Robustness shared task based on this method. Our model achieves a BLEU score of 32.8 on the shared task French to English dataset, which is 7.1 BLEU points higher than the baseline vanilla transformer trained with clean text.","",""
34,"Sebastian Banescu, C. Collberg, A. Pretschner","Predicting the Resilience of Obfuscated Code Against Symbolic Execution Attacks via Machine Learning",2017,"","","","",65,"2022-07-13 09:24:04","","","","",,,,,34,6.80,11,3,5,"Software obfuscation transforms code such that it is more  difficult to reverse engineer. However, it is known that  given enough resources, an attacker will successfully reverse  engineer an obfuscated program. Therefore, an  open challenge for software obfuscation is estimating the  time an obfuscated program is able to withstand a given  reverse engineering attack. This paper proposes a general  framework for choosing the most relevant software  features to estimate the effort of automated attacks. Our  framework uses these software features to build regression  models that can predict the resilience of different  software protection transformations against automated  attacks. To evaluate the effectiveness of our approach,  we instantiate it in a case-study about predicting the time  needed to deobfuscate a set of C programs, using an attack  based on symbolic execution. To train regression  models our system requires a large set of programs as  input. We have therefore implemented a code generator  that can generate large numbers of arbitrarily complex  random C functions. Our results show that features  such as the number of community structures in the graphrepresentation  of symbolic path-constraints, are far more  relevant for predicting deobfuscation time than other features  generally used to measure the potency of controlflow  obfuscation (e.g. cyclomatic complexity). Our best  model is able to predict the number of seconds of symbolic  execution-based deobfuscation attacks with over  90% accuracy for 80% of the programs in our dataset,  which also includes several realistic hash functions.","",""
44,"Pathum Chamikara Mahawaga Arachchige, P. Bertók, I. Khalil, Dongxi Liu, S. Çamtepe, Mohammed Atiquzzaman","A Trustworthy Privacy Preserving Framework for Machine Learning in Industrial IoT Systems",2020,"","","","",66,"2022-07-13 09:24:04","","10.1109/TII.2020.2974555","","",,,,,44,22.00,7,6,2,"Industrial Internet of Things (IIoT) is revolutionizing many leading industries such as energy, agriculture, mining, transportation, and healthcare. IIoT is a major driving force for Industry 4.0, which heavily utilizes machine learning (ML) to capitalize on the massive interconnection and large volumes of IIoT data. However, ML models that are trained on sensitive data tend to leak privacy to adversarial attacks, limiting its full potential in Industry 4.0. This article introduces a framework named PriModChain that enforces privacy and trustworthiness on IIoT data by amalgamating differential privacy, federated ML, Ethereum blockchain, and smart contracts. The feasibility of PriModChain in terms of privacy, security, reliability, safety, and resilience is evaluated using simulations developed in Python with socket programming on a general-purpose computer. We used Ganache_v2.0.1 local test network for the local experiments and Kovan test network for the public blockchain testing. We verify the proposed security protocol using Scyther_v1.1.3 protocol verifier.","",""
12,"A. N. Onodera, W. P. Gavião Neto, M. I. Roveri, W. R. Oliveira, I. Sacco","Immediate effects of EVA midsole resilience and upper shoe structure on running biomechanics: a machine learning approach",2017,"","","","",67,"2022-07-13 09:24:04","","10.7717/peerj.3026","","",,,,,12,2.40,2,5,5,"Background Resilience of midsole material and the upper structure of the shoe are conceptual characteristics that can interfere in running biomechanics patterns. Artificial intelligence techniques can capture features from the entire waveform, adding new perspective for biomechanical analysis. This study tested the influence of shoe midsole resilience and upper structure on running kinematics and kinetics of non-professional runners by using feature selection, information gain, and artificial neural network analysis. Methods Twenty-seven experienced male runners (63 ± 44 km/week run) ran in four-shoe design that combined two resilience-cushioning materials (low and high) and two uppers (minimalist and structured). Kinematic data was acquired by six infrared cameras at 300 Hz, and ground reaction forces were acquired by two force plates at 1,200 Hz. We conducted a Machine Learning analysis to identify features from the complete kinematic and kinetic time series and from 42 discrete variables that had better discriminate the four shoes studied. For that analysis, we built an input data matrix of dimensions 1,080 (10 trials × 4 shoes × 27 subjects) × 1,254 (3 joints × 3 planes of movement × 101 data points + 3 vectors forces × 101 data points + 42 discrete calculated kinetic and kinematic features). Results The applied feature selection by information gain and artificial neural networks successfully differentiated the two resilience materials using 200(16%) biomechanical variables with an accuracy of 84.8% by detecting alterations of running biomechanics, and the two upper structures with an accuracy of 93.9%. Discussion The discrimination of midsole resilience resulted in lower accuracy levels than did the discrimination of the shoe uppers. In both cases, the ground reaction forces were among the 25 most relevant features. The resilience of the cushioning material caused significant effects on initial heel impact, while the effects of different uppers were distributed along the stance phase of running. Biomechanical changes due to shoe midsole resilience seemed to be subject-dependent, while those due to upper structure seemed to be subject-independent.","",""
11,"Siddharth Gangadhar, J. Sterbenz","Machine learning aided traffic tolerance to improve resilience for software defined networks",2017,"","","","",68,"2022-07-13 09:24:04","","10.1109/RNDM.2017.8093035","","",,,,,11,2.20,6,2,5,"Software Defined Networks (SDNs) have gained prominence recently due to their flexible management and superior configuration functionality of the underlying network. SDNs, with OpenFlow as their primary implementation, allow for the use of a centralised controller to drive the decision making for all the supported devices in the network and manage traffic through routing table changes for incoming flows. In conventional networks, machine learning has been shown to detect malicious intrusion, and classify attacks such as DoS, user to root, and probe attacks. In this work, we extend the use of machine learning to improve traffic tolerance for SDNs. To achieve this, we extend the functionality of the controller to include a resilience framework, ReSDN, that incorporates machine learning to be able to distinguish DoS attacks, focussing on a neptune attack for our experiments. Our model is trained using the MIT KDD 1999 dataset. The system is developed as a module on top of the POX controller platform and evaluated using the Mininet simulator.","",""
74,"M. Bogojeski, Leslie Vogt-Maranto, M. Tuckerman, K. Müller, K. Burke","Quantum chemical accuracy from density functional approximations via machine learning",2019,"","","","",69,"2022-07-13 09:24:04","","10.1038/s41467-020-19093-1","","",,,,,74,24.67,15,5,3,"","",""
148,"Amedeo Sapio, M. Canini, Chen-Yu Ho, J. Nelson, Panos Kalnis, Changhoon Kim, A. Krishnamurthy, M. Moshref, Dan R. K. Ports, Peter Richtárik","Scaling Distributed Machine Learning with In-Network Aggregation",2019,"","","","",70,"2022-07-13 09:24:04","","","","",,,,,148,49.33,15,10,3,"Training complex machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide a robust, efficient solution that speeds up training by up to 300%, and at least by 20% for a number of real-world benchmark models.","",""
5,"Morgan Ekmefjord, Addi Ait-Mlouk, Sadi Alawadi, Mattias Åkesson, Desislava Stoyanova, O. Spjuth, S. Toor, A. Hellander","Scalable federated machine learning with FEDn",2021,"","","","",71,"2022-07-13 09:24:04","","","","",,,,,5,5.00,1,8,1,"to be production grade for industrial applications and a ﬂexible research tool to explore real-world performance of novel federated algorithms and the framework has been used in number of industrial and academic R&D projects. In this paper we present the architecture and implementation of FEDn. We demonstrate the framework’s scalability and efﬁciency in evaluations based on two case-studies representative for a cross-silo and a cross-device use-case respectively. recover robustly in the events of failures in the FEDn network. Resilience is vital for production grade federated machine learning. The horizontal component replication each","",""
106,"Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, V. Larivière, A. Beygelzimer, Florence d'Alché-Buc, E. Fox, H. Larochelle","Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",2020,"","","","",72,"2022-07-13 09:24:04","","","","",,,,,106,53.00,13,8,2,"One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.","",""
46,"F. Yuan, S. A. Zargar, Qiuyi Chen, Shaohan Wang","Machine learning for structural health monitoring: challenges and opportunities",2020,"","","","",73,"2022-07-13 09:24:04","","10.1117/12.2561610","","",,,,,46,23.00,12,4,2,"A physics-based approach to structural health monitoring (SHM) has practical shortcomings which restrict its suitability to simple structures under well controlled environments. With the advances in information and sensing technology (sensors and sensor networks), it has become feasible to monitor large/diverse number of parameters in complex real-world structures either continuously or intermittently by employing large in-situ (wireless) sensor networks. The availability of this historical data has engendered a lot of interest in a data-driven approach as a natural and more viable option for realizing the goal of SHM in such structures. However, the lack of sensor data corresponding to different damage scenarios continues to remain a challenge. Most of the supervised machine-learning/deep-learning techniques, when trained using this inherently limited data, lack robustness and generalizability. Physics-informed learning, which involves the integration of domain knowledge into the learning process, is presented here as a potential remedy to this challenge. As a step towards the goal of automated damage detection (mathematically an inverse problem), preliminary results are presented from dynamic modelling of beam structures using physics-informed artificial neural networks. Forward and inverse problems involving partial differential equations are solved and comparisons reveal a clear superiority of physics-informed approach over one that is purely datadriven vis-à-vis overfitting/generalization. Other ways of incorporating domain knowledge into the machine learning pipeline are then presented through case-studies on various aspects of NDI/SHM (visual inspection, impact diagnosis). Lastly, as the final attribute of an optimal SHM approach, a sensing paradigm for non-contact full-field measurements for damage diagnosis is presented.","",""
103,"Yunchao Liu, Srinivasan Arunachalam, K. Temme","A rigorous and robust quantum speed-up in supervised machine learning",2020,"","","","",74,"2022-07-13 09:24:04","","10.1038/s41567-021-01287-z","","",,,,,103,51.50,34,3,2,"","",""
1,"R. Guerraoui, Arsany Guirguis, Jérémy Plassmann, Anton Ragot, Sébastien Rouault","GARFIELD: System Support for Byzantine Machine Learning (Regular Paper)",2021,"","","","",75,"2022-07-13 09:24:04","","10.1109/DSN48987.2021.00021","","",,,,,1,1.00,0,5,1,"We present GARFIELD, a library to transparently make machine learning (ML) applications, initially built with popular (but fragile) frameworks, e.g., TensorFlow and PyTorch, Byzantine–resilient. GARFIELD relies on a novel object–oriented design, reducing the coding effort, and addressing the vulnerability of the shared–graph architecture followed by classical ML frameworks. GARFIELD encompasses various communication patterns and supports computations on CPUs and GPUs, allowing addressing the general question of the practical cost of Byzantine resilience in ML applications. We report on the usage of GARFIELD on three main ML architectures: (a) a single server with multiple workers, (b) several servers and workers, and (c) peer–to–peer settings. Using GARFIELD, we highlight interesting facts about the cost of Byzantine resilience. In particular, (a) Byzantine resilience, unlike crash resilience, induces an accuracy loss, (b) the throughput overhead comes more from communication than from robust aggregation, and (c) tolerating Byzantine servers costs more than tolerating Byzantine workers.","",""
0,"Yan Zhou, Murat Kantarcioglu, B. Xi","A Game Theoretic Perspective on Adversarial Machine Learning and Related Cybersecurity Applications",2021,"","","","",76,"2022-07-13 09:24:04","","10.1002/9781119723950.ch13","","",,,,,0,0.00,0,3,1,"In cybersecurity applications where machine learning algorithms are increasingly used to detect vulnerabilities, a somewhat unique challenge arises as exploits targeting machine learning models are constantly devised by the attackers. Traditional machine learning models are no longer robust and reliable when they are under attack. The action and reaction between machine learning systems and the adversary can be modeled as a game between two or more players. Under well‐defined attack models, game theory can provide robustness guarantee for machine learning models that are otherwise vulnerable to application‐time data corruption. We review two cases of game theory‐based machine learning techniques: in one case, players play a zero sum game by following a minimax strategy, while in the other case, players play a sequential game with one player as the leader and the rest as the followers. Experimental results on e‐mail spam and web spam datasets are presented. In the zero sum game, we demonstrate that an adversarial SVM model built upon the minimax strategy is much more resilient to adversarial attacks than standard SVM and one‐class SVM models. We also show that optimal learning strategies derived to counter overly pessimistic attack models can produce unsatisfactory results when the real attacks are much weaker. In the sequential game, we demonstrate that the mixed strategy, allowing a player to randomize over available strategies, is the best solution in general without knowing what types of adversaries machine learning applications are facing in the wild. We also discuss scenarios where players' behavior may derail rational decision making and models that consider such decision risks.","",""
0,"Yifan Zhou, Peng Zhang","Quantum Machine Learning for Power System Stability Assessment",2021,"","","","",77,"2022-07-13 09:24:04","","","","",,,,,0,0.00,0,2,1,"Transient stability assessment (TSA), a cornerstone for resilient operations of today’s interconnected power grids, is a grand challenge yet to be addressed since the genesis of electric power systems. This paper is a confluence of quantum computing, data science and machine learning to potentially resolve the aforementioned challenge caused by high dimensionality, nonlinearity and uncertainty. We devise a quantum TSA (qTSA) method, a low-depth, high expressibility quantum neural network, to enable scalable and efficient data-driven transient stability prediction for bulk power systems. qTSA renders the intractable TSA straightforward and effortless in the Hilbert space, and provides rich information that enables unprecedentedly resilient and secure power system operations. Extensive experiments on quantum simulators and real quantum computers verify the accuracy, noise-resilience, scalability and universality of qTSA. qTSA underpins a solid foundation of a quantum-enabled, ultra-resilient power grid which will benefit the people as well as various commercial and industrial sectors. Introduction Texas’ and California’s rolling outages [1, 2] in recent years signaled that our existing power infrastructures can hardly sustain the ever-expanding communities and deep integration of low-inertia renewables [3, 4]. The situations are rapidly deteriorating as our power grids are increasingly integrating massive DERs, such as intermittent rooftop solar photovoltaics (PVs), as well as solar farms and offshore wind systems, and have been subject to more frequent weather events [5, 6]. A key technology to secure today’s bulk power grids is transient stability assessment (TSA) which aims to determine the ability of the system to ride-through large disturbances (contingencies) and to reach the post-contingency steady-state [7]. Transient instability is a fast phenomenon typically taking only a few seconds for the bulk system to collapse after contingencies occur. Due to this very nature, system operators in the control center never have sufficient time to steer the power system away from instability upon the occurrence of contingencies. For this reason, we have to rely on computer-based TSA without any manual interaction from human operators to assess the transient stability of the system [8]. TSA, however, is a grand challenge yet to be addressed since the genesis of power systems in the era of Tesla, Edison and Westinghouse [9]. Interconnected power systems are the largest and most complicated man-made dynamical systems on this planet. Those bulk systems are highly nonlinear, exhibit multi-scale behaviors spatially and temporarily, and are increasingly stochastic and uncertain due to deep integration of renewable energy resources. The majority of the TSA methods being used in power industry rely on the explicit integration or implicit integration of differential equation models of the bulk power systems, which are known to be intractable to handle large power systems [10–12]. Discrete events such as frequent plug-and-plays of renewables, microgrids and loads in today’s power systems [13, 14] and unknown models due to data privacy issues [15] make existing TSA methods even more computationally formidable even if they are executed on the powerful and expensive real-time simulators. Even worse, a large number of power system TSA must be conducted to examine the stability of the system in relation to massive ‘N− k’ contingencies (k components have failed in a power system with N components), which further impedes the application of TSA in real-world power system operations. All the aforementioned challenges have made classical TSA prohibitively difficult for the online operation of large interconnected grids. Today’s power systems are undergoing an Enlightenment, where the confluence of big data, quantum computing and machine learning altogether is to drive a regime shift in the analysis and operation of our critical power infrastructures. Big data is the force behind the revolution: massive new types of intelligent electronic sensors such as synchronized phasor measurement units (PMUs), advanced metering infrastructure (AMI) meters and remote terminal units (RTUs) [16] are continuously generating gigantic volumes of data which allow for the development of data-driven power system analytics. Most recently, the successes in exploiting the potential of quantum supremacy [17, 18] shed lights on a ‘quantum leap’ of computing capabilities. The power of quantum computing is derived from its ability to prepare and maintain complex superpositions of ar X iv :2 10 4. 04 85 5v 2 [ qu an tph ] 2 3 M ay 2 02 1 quantum states across many quantum degrees of freedom. While classically the number of required physical resources N grows exponentially with the system complexity n, N grows linearly with n in a quantum computer, resulting in exponential speedups over classical computing. Furthermore, highly entangled states, very difficult to represent on classical computers, are easily represented on a quantum computer [19, 20]. Therefore, the intractable power system problems aforementioned, if formulated properly through programmable quantum circuits, can be executed efficiently on a quantum computer. Inheriting the exponential speedup of quantum computing in tensor manipulation [21], the swift growth in quantum machine learning (QML) techniques [22–24] ignites new hopes of developing unprecedentedly scalable and efficient data-driven power system analytics. QML is promisingly efficacious for data processing and model training in high-dimensional space that are intractable for classical algorithms [25, 26]. Ideally, unique quantum operators such as superposition and entanglement, which can not be represented by classical operators, enable a superior representation of complicated data relationships [27–29]. Nevertheless, the existing noisy quantum devices are still restrictive, hindering the implementing of QML if deep quantum circuits are needed. This paper is the first attempt to unlock the potential of QML for power system TSA. A low-depth, high expressibility quantum neural network (QNN)-based transient stability assessment (qTSA) method is devised to enable scalable, reliable and efficient data-driven transient stability prediction. In particular, we are focusing on designing an efficient qTSA circuit that is feasible to pursue on near-term devices, considering the noisy-intermediate-scale quantum (NISQ) era [30, 31], but general enough to be directly expandable to the noise-free quantum computer of a distant future (5-10 years). We have designed systematical studies which have demonstrated the robustness, accuracy and fidelity of qTSA on real-scale power systems. Our qTSA has shown consistently high performance on quantum computers at different noise levels. Stability assessment plays a central role in nearly every field of the life sciences, physics and engineering. Our qTSA therefore is promising to positively impact the modeling, analysis, controlling and securing various high-dimensional, nonlinear, hybrid dynamical systems. In particular, qTSA underpins a solid foundation of a quantum-enabled resilient power grid, i.e., tomorrow’s unprecedentedly autonomic power infrastructure towards self-configuration, self-healing, self-optimization and selfprotection against grid changes, renewable power injections, faults, disastrous events and cyberattacks. Such a quantum-enabled grid will benefit the people as well as various commercial and industrial sectors. Results Power system transients are generically modelled as a set of nonlinear differential algebraic equations: { Ẋ = FD(X ,Y ) 0 = FA(X ,Y ) (1a) (1b) where X and Y separately denote the differential variables and algebraic variables; (1a) formulates the nonlinear dynamics of power devices, such as generators (e.g., synchronous machines, distributed energy resources), controllers (e.g., governors, exciters, inverters), power loads, etc; (1b) formulates the instantaneous power flow of the entire power grid. TSA appraises a power system’s capability of resisting large disturbance [7]. Denote Z = (X ,Y ), and φ(t,Z) as the orbit of (1) starting from Z. An asymptotically stable equilibrium point (SEP) Zs of (1) satisfies that: (a) Zs is Lyapunov stable; (b) there exists an open neighborhood O of Zs such that ∀Z ∈O converges to Zs when t approaches infinity [32]. The stability region of Zs encloses all the states that can be attracted by Zs within an infinite time: A(Zs) = {Z ∈ Rn : lim t→∞ φ(t,Z) = Zs} (2) Stability region theory states that system stability after a large disturbance is determined by whether the post-disturbance state is within the stability region of an SEP [32]. Therefore, to formulate the data-driven TSA, the idea is to establish a direct mapping between the post-disturbance power system states and the stability results [33, 34]. The keystone of qTSA, different from classical machine learning techniques, is that the transient stability features in a Euclidean space Z ∈ E are embedded into quantum states in a Hilbert space |ψ〉 ∈H through a variational quantum circuit (VQC), which serves as a QNN to explicitly separate the stable and unstable samples. Our key innovation is a design of low-depth, high expressibility qTSA, as presented in Fig. 1 (see also Methods). Fig. 1(a) first visualizes the low-depth variational qTSA circuit which addresses the dimensionality and nonlinearity obstacles in TSA as well as the nonnegligible noise and source limitations on near-term quantum devices. Kernel ingredients in qTSA circuit include (see Methods): 1) Non-Gaussian feature encoding: |ψE〉=UE(pE ,Z) |0〉 which adopts parameterized, activation-enhanced quantum gates to enable a flexible, nonlinear and dimension-free encoding of power system stability features Z;","",""
0,"Tousif Rahman, R. Shafik, Ole-Christoffer Granmo, A. Yakovlev","Resilient Biomedical Systems Design Under Noise Using Logic-Based Machine Learning",2022,"","","","",78,"2022-07-13 09:24:04","","10.3389/fcteg.2021.778118","","",,,,,0,0.00,0,4,1,"Increased reliance on electronic health records and plethora of new sensor technologies has enabled the use of machine learning (ML) in medical diagnosis. This has opened up promising opportunities for faster and automated decision making, particularly in early and repetitive diagnostic routines. Nevertheless, there are also increased possibilities of data aberrance arising from environmentally induced noise. It is vital to create ML models that are resilient in the presence of data noise to minimize erroneous classifications that could be crucial. This study uses a recently proposed ML algorithm called the Tsetlin machine (TM) to study the robustness against noise-injected medical data. We test two different feature extraction methods, in conjunction with the TM, to explore how feature engineering can mitigate the impact of noise corruption. Our results show the TM is capable of effective classification even with a signal-to-noise ratio (SNR) of −15dB as its training parameters remain resilient to noise injection. We show that high testing data sensitivity can still be possible at very low SNRs through a balance of feature distribution–based discretization and a rule mining algorithm used as a noise filtering encoding method. Through this method we show how a smaller number of core features can be extracted from a noisy problem space resulting in reduced ML model complexity and memory footprint—in some cases up to 6x fewer training parameters while retaining equal or better performance. In addition, we investigate the cost of noise resilience in terms of energy when compared with recently proposed binarized neural networks.","",""
0,"W. Zong, Yang-Wai Chow, W. Susilo","Visual Analysis of Adversarial Examples in Machine Learning",2021,"","","","",79,"2022-07-13 09:24:04","","10.1007/978-981-33-6726-5_4","","",,,,,0,0.00,0,3,1,"","",""
61,"Diego Didona, F. Quaglia, P. Romano, Ennio Torre","Enhancing Performance Prediction Robustness by Combining Analytical Modeling and Machine Learning",2015,"","","","",80,"2022-07-13 09:24:04","","10.1145/2668930.2688047","","",,,,,61,8.71,15,4,7,"Classical approaches to performance prediction rely on two, typically antithetic, techniques: Machine Learning (ML) and Analytical Modeling (AM). ML takes a black box approach, whose accuracy strongly depends on the representativeness of the dataset used during the initial training phase. Specifically, it can achieve very good accuracy in areas of the features' space that have been sufficiently explored during the training process. Conversely, AM techniques require no or minimal training, hence exhibiting the potential for supporting prompt instantiation of the performance model of the target system. However, in order to ensure their tractability, they typically rely on a set of simplifying assumptions. Consequently, AM's accuracy can be seriously challenged in scenarios (e.g., workload conditions) in which such assumptions are not matched. In this paper we explore several hybrid/gray box techniques that exploit AM and ML in synergy in order to get the best of the two worlds. We evaluate the proposed techniques in case studies targeting two complex and widely adopted middleware systems: a NoSQL distributed key-value store and a Total Order Broadcast (TOB) service.","",""
2,"T. Ermolieva, Y. Ermoliev, M. Obersteiner, E. Rovenskaya","Chapter 4 Two-Stage Nonsmooth Stochastic Optimization and Iterative Stochastic Quasigradient Procedure for Robust Estimation, Machine Learning and Decision Making",2021,"","","","",81,"2022-07-13 09:24:04","","10.1007/978-3-030-70370-7_4","","",,,,,2,2.00,1,4,1,"","",""
0,"Murilo Cruz Lopes, Marília de Matos Amorim, V. S. Freitas, R. Calumby","Survival Prediction for Oral Cancer Patients: A Machine Learning Approach",2021,"","","","",82,"2022-07-13 09:24:04","","10.5753/kdmile.2021.17466","","",,,,,0,0.00,0,4,1,"There is a high incidence of oral cancer in Brazil, with 150,000 new cases estimated for 2020-2022. In most cases, it is diagnosed at an advanced stage and are related to many risk factors. The Registro Hospitalar de Câncer (RHC), managed by Instituto Nacional de Câncer (INCA), is a nation-wide database that integrates cancer registers from several hospitals in Brazil. RHC is mostly an administrative database but also include clinical, socioeconomic and hospitalization data for each patient with a cancer diagnostic in the country. For these patients, prognostication is always a difficult task a demand multi-dimensional analysis. Therefore, exploiting large-scale data and machine intelligence approaches emerge as promising tool for computer-aided decision support on death risk estimation. Given the importance of this context, some works have reported high prognostication effectiveness, however with extremely limited data collections, relying on weak validation protocols or simple robustness analysis. Hence, this work describes a detailed workflow and experimental analysis for oral cancer patient survival prediction considering careful data curation and strict validation procedures. By exploiting multiple machine learning algorithms and optimization techniques the proposed approach allowed promising survival prediction effectiveness with F1 and AuC-ROC over 0.78 and 0.80, respectively. Moreover, a detailed analysis have shown that the minimization of different types of prediction errors were achieved by different models, which highlights the importance of the rigour in this kind of validation.","",""
28,"Sicong Zhou, Huawei Huang, Wuhui Chen, Zibin Zheng, Song Guo","PiRATE: A Blockchain-Based Secure Framework of Distributed Machine Learning in 5G Networks",2019,"","","","",83,"2022-07-13 09:24:04","","10.1109/MNET.001.1900658","","",,,,,28,9.33,6,5,3,"in fifth-generation (5G) networks and beyond, communication latency and network bandwidth will be no longer be bottlenecks to mobile users. Thus, almost every mobile device can participate in distributed learning. That is, the availability issue of distributed learning can be eliminated. However, model safety will become a challenge. This is because the distributed learning system is prone to suffering from byzantine attacks during the stages of updating model parameters and aggregating gradients among multiple learning participants. Therefore, to provide the byzantine-resilience for distributed learning in the 5G era, this article proposes a secure computing framework based on the sharding technique of blockchain, namely PiRATE. To prove the feasibility of the proposed PiRATE, we implemented a prototype. A case study shows how the proposed PiRATE contributes to distributed learning. Finally, we also envision some open issues and challenges based on the proposed byzantine- resilient learning framework.","",""
0,"Phillip Williams, Haytham Idriss, M. Bayoumi","Mc-PUF: Memory-based and Machine Learning Resilient Strong PUF for Device Authentication in Internet of Things",2021,"","","","",84,"2022-07-13 09:24:04","","10.1109/CSR51186.2021.9527930","","",,,,,0,0.00,0,3,1,"Physically Unclonable Functions (PUFs) are hardware-based security primitives that utilize manufacturing process variations to realize binary keys (Weak PUFs) or binary functions (Strong PUFs). This primitive is desirable for key generation and authentication in constrained devices, due to its low power and low area overhead. However, in recent years many research papers are focused on the vulnerability of PUFs to modeling attacks. This attack is possible because the PUFs challenge and response exchanges are usually transmitted over communication channel without encryption. Thus, an attacker can collect challenge-response pairs and use it as input into a learning algorithm, to create a model that can predict responses given new challenges. In this paper we introduce a serial and a parallel novel 64-bits memory-based controlled PUF (Mc-PUF) architecture for device authentication that has high uniqueness and randomness, reliable, and resilient against modeling attacks. These architectures generate a response by utilizing bits extracted from the fingerprint of a synchronous random-access memory (SRAM) PUF with a control logic. The synthesis of the serial architecture yielded an area of 1.136K GE, while the parallel architecture was 3.013K GE. The best prediction accuracy obtained from the modeling attack was ~50%, which prevents an attacker from accurately predicting responses to future challenges. We also showcase the scalability of the design through XOR-ing several Mc-PUFs, further improving upon its security and performance. The remainder of the paper presents the proposed architectures, along with their hardware implementations, area and power consumption, and security resilience against modeling attacks. The 3-XOR Mc-PUF had the greatest overhead, but it produced the best randomness, uniqueness, and resilience against modeling attacks.","",""
21,"Lie He, Sai Praneeth Karimireddy, Martin Jaggi","Secure Byzantine-Robust Machine Learning",2020,"","","","",85,"2022-07-13 09:24:04","","","","",,,,,21,10.50,7,3,2,"Increasingly machine learning systems are being deployed to edge servers and devices (e.g. mobile phones) and trained in a collaborative manner. Such distributed/federated/decentralized training raises a number of concerns about the robustness, privacy, and security of the procedure. While extensive work has been done in tackling with robustness, privacy, or security individually, their combination has rarely been studied. In this paper, we propose a secure two-server protocol that offers both input privacy and Byzantine-robustness. In addition, this protocol is communication-efficient, fault-tolerant and enjoys local differential privacy.","",""
6,"Catherine D. Schuman, J. Parker Mitchell, J. T. Johnston, Maryam Parsa, Bill Kay, Prasanna Date, R. Patton","Resilience and Robustness of Spiking Neural Networks for Neuromorphic Systems",2020,"","","","",86,"2022-07-13 09:24:04","","10.1109/IJCNN48605.2020.9207560","","",,,,,6,3.00,1,7,2,"Though robustness and resilience are commonly quoted as features of neuromorphic computing systems, the expected performance of neuromorphic systems in the face of hardware failures is not clear. In this work, we study the effect of failures on the performance of four different training algo-rithms for spiking neural networks on neuromorphic systems: two back-propagation-based training approaches (Whetstone and SLAYER), a liquid state machine or reservoir computing approach, and an evolutionary optimization-based approach (EONS). We show that these four different approaches have very different resilience characteristics with respect to simulated hardware failures. We then analyze an approach for training more resilient spiking neural networks using the evolutionary optimization approach. We show how this approach produces more resilient networks and discuss how it can be extended to other spiking neural network training approaches as well.","",""
299,"J Zhang, M. Harman, Lei Ma, Yang Liu","Machine Learning Testing: Survey, Landscapes and Horizons",2019,"","","","",87,"2022-07-13 09:24:04","","10.1109/tse.2019.2962027","","",,,,,299,99.67,75,4,3,"This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.","",""
29,"Fahad Shabbir Ahmad, Liaqat Ali, Liaqat Ali, Raza-Ul-Mustafa, Hasan Ali Khattak, Tahir Hameed, Iram Wajahat, Seifedine Kadry, S. Bukhari","A hybrid machine learning framework to predict mortality in paralytic ileus patients using electronic health records (EHRs)",2020,"","","","",88,"2022-07-13 09:24:04","","10.1007/s12652-020-02456-3","","",,,,,29,14.50,3,9,2,"","",""
1,"Sam Yang, B. Vaagensmith, Deepika Patra","Power Grid Contingency Analysis with Machine Learning: A Brief Survey and Prospects",2020,"","","","",89,"2022-07-13 09:24:04","","10.1109/RWS50334.2020.9241293","","",,,,,1,0.50,0,3,2,"We briefly review previous applications of machine learning (ML) in power grid analyses and introduce our ongoing effort toward developing a generative-adversarial (GA) model for fast and reliable grid contingency analyses. According to our review, the persisting limitation of traditional ML techniques in grid analyses is the need for an exhaustive amount of training data for model generalization and accurate predictions. GA models overcome this limitation by first learning true data distribution from a small training set, from which new samples assimilating true data are generated with some variations. Subsequently, GA models can transfer learn or super-generalize with increased accuracy, that is, accurately predict n − (k + 2) contingencies from a small n − k training set and generated n − (k + 1) data. The joint effort between Idaho National Lab and Florida State University strives to develop a zero-shot and deep learning-based contingency analysis tool, named Smart Contingency Analysis Neural Network (SCANN), by leveraging the aforementioned advantages of GA models. The basic architecture of SCANN stems from the Latent Encoding of Atypical Perturbations network combined with an adversarial network, and it is designed to generate imbalanced power flow data from learned true data distributions for prediction purposes. Here we also introduce the abstract concept of resilience-chaos plots, a new resilience characterization tool proposed to complement SCANN by aiding in the assessment of large amounts of high-order contingency predictions.","",""
13,"D. Feldmeyer, C. Meisch, H. Sauter, J. Birkmann","Using OpenStreetMap Data and Machine Learning to Generate Socio-Economic Indicators",2020,"","","","",90,"2022-07-13 09:24:04","","10.3390/ijgi9090498","","",,,,,13,6.50,3,4,2,"Socio-economic indicators are key to understanding societal challenges. They disassemble complex phenomena to gain insights and deepen understanding. Specific subsets of indicators have been developed to describe sustainability, human development, vulnerability, risk, resilience and climate change adaptation. Nonetheless, insufficient quality and availability of data often limit their explanatory power. Spatial and temporal resolution are often not at a scale appropriate for monitoring. Socio-economic indicators are mostly provided by governmental institutions and are therefore limited to administrative boundaries. Furthermore, different methodological computation approaches for the same indicator impair comparability between countries and regions. OpenStreetMap (OSM) provides an unparalleled standardized global database with a high spatiotemporal resolution. Surprisingly, the potential of OSM seems largely unexplored in this context. In this study, we used machine learning to predict four exemplary socio-economic indicators for municipalities based on OSM. By comparing the predictive power of neural networks to statistical regression models, we evaluated the unhinged resources of OSM for indicator development. OSM provides prospects for monitoring across administrative boundaries, interdisciplinary topics, and semi-quantitative factors like social cohesion. Further research is still required to, for example, determine the impact of regional and international differences in user contributions on the outputs. Nonetheless, this database can provide meaningful insight into otherwise unknown spatial differences in social, environmental or economic inequalities.","",""
10,"Haoyu Zhuang, Xiaodan Xi, Nan Sun, M. Orshansky","A Strong Subthreshold Current Array PUF Resilient to Machine Learning Attacks",2020,"","","","",91,"2022-07-13 09:24:04","","10.1109/TCSI.2019.2945247","","",,,,,10,5.00,3,4,2,"This paper presents a strong silicon physical unclonable function (PUF) resistant to machine learning (ML) attacks. The PUF, termed the subthreshold current array PUF (SCA-PUF), consists of a pair of two-dimensional transistor arrays and a low-offset comparator. The proposed 65-bit SCA-PUF is fabricated in a 130nm process and allows 265 challenge-response pairs (CRPs). It consumes 68nW and 11pJ/bit while exhibiting high uniqueness, uniformity, and randomness. It achieves bit error rate (BER) of 5.8% for the temperature range of −20 to 80°C and supply voltage variation of ±10%. The calibration-based CRP selection method improves BER to 0.4% with a 42% loss of CRPs. When subjected to ML attacks, the prediction error stays over 40% on 104 training points, which shows negligible loss in PUF unpredictability and $\sim 100\times $ higher resilience than the 65-bit arbiter PUF, 3-XOR PUF, and 3-XOR lightweight (LW) PUF.","",""
87,"G. Peng, M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, Savador Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Multiscale modeling meets machine learning: What can we learn?",2019,"","","","",92,"2022-07-13 09:24:04","","10.1007/s11831-020-09405-5","","",,,,,87,29.00,9,12,3,"","",""
8,"Youness Arjoune, Fatima Salahdine, Md. Shoriful Islam, Elias Ghribi, N. Kaabouch","A Novel Jamming Attacks Detection Approach Based on Machine Learning for Wireless Communication",2020,"","","","",93,"2022-07-13 09:24:04","","10.1109/ICOIN48656.2020.9016462","","",,,,,8,4.00,2,5,2,"Jamming attacks target a wireless network creating an unwanted denial of service. 5G is vulnerable to these attacks despite its resilience prompted by the use of millimeter wave bands. Over the last decade, several types of jamming detection techniques have been proposed, including fuzzy logic, game theory, channel surfing, and time series. Most of these techniques are inefficient in detecting smart jammers. Thus, there is a great need for efficient and fast jamming detection techniques with high accuracy. In this paper, we compare the efficiency of several machine learning models in detecting jamming signals. We investigated the types of signal features that identify jamming signals, and generated a large dataset using these parameters. Using this dataset, the machine learning algorithms were trained, evaluated, and tested. These algorithms are random forest, support vector machine, and neural network. The performance of these algorithms was evaluated and compared using the probability of detection, probability of false alarm, probability of miss detection, and accuracy. The simulation results show that jamming detection based random forest algorithm can detect jammers with a high accuracy, high detection probability and low probability of false alarm.","",""
665,"Weihua Hu, Matthias Fey, M. Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, J. Leskovec","Open Graph Benchmark: Datasets for Machine Learning on Graphs",2020,"","","","",94,"2022-07-13 09:24:04","","","","",,,,,665,332.50,83,8,2,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .","",""
1,"M. Amini, Ahmed Imteaj, J. Mohammadi","Distributed Machine Learning for Resilient Operation of Electric Systems",2020,"","","","",95,"2022-07-13 09:24:04","","10.1109/SEST48500.2020.9203368","","",,,,,1,0.50,0,3,2,"Power system resilience is crucial to ensure secure energy delivery to electricity consumers. Power system outages lead to economical and societal burdens for the society and industries. To mitigate the socio-economical impacts of a power outage, we need to develop efficient algorithms to ensure resilient operation of the power system. In this paper, we first explain the notion of data-driven resilience. Then, we present a pathway of leveraging edge intelligence to improve resilience. To this end, we propose a novel distributed machine learning paradigm. Our proposed structure relies on local Resilience Management Systems (RMS) that serve as intelligent decision-making entities in each area, e.g. an autonomous micro-grid or a smart home can act as RMS. The RMS agents, which are available in different areas, can share their local data (i.e., a microgrid's operational data) with their neighboring RMS to coordinate their decisions in a distributed fashion. This will provide two major advantages: 1) distributed intelligence replaces centralized decision-making leading to robust decision-making and enhanced resilience; 2) since local data are locally shared among all entities within an RMS, if one of the RMS agents fails to communicate with the rest of network, we still can maintain a feasible solution (which is not necessarily optimal). Finally, we presents different scenarios in the simulation results section that showcases the system performance for two buildings under various outage scenarios.","",""
0,"E. Kondrateva, Polina Belozerova, M. Sharaev, Evgeny Burnaev, A. Bernstein, I. Samotaeva","Machine learning models reproducibility and validation for MR images recognition",2020,"","","","",96,"2022-07-13 09:24:04","","10.1117/12.2559525","","",,,,,0,0.00,0,6,2,"In the present work, we introduce a data processing and analysis pipeline, which ensures the reproducibility of machine learning models chosen for MR image recognition. The proposed pipeline is applied to solve the binary classification problems: epilepsy and depression diagnostics based on vectorized features from MR images. This model is then assessed in terms of classification performance, robustness and reliability of the results, including predictive accuracy on unseen data. The classification performance achieved with our approach compares favorably to ones reported in the literature, where usually no thorough model evaluation is performed.","",""
0,"El-Mahdi El-Mhamdi, R. Guerraoui, Arsany Guirguis, Sébastien Rouault","Garfield: System Support for Byzantine Machine Learning",2020,"","","","",97,"2022-07-13 09:24:04","","","","",,,,,0,0.00,0,4,2,"Byzantine Machine Learning (ML) systems are nowadays vulnerable for they require trusted machines and/or a synchronous network. We present Garfield, a system that provably achieves Byzantine resilience in ML applications without assuming any trusted component nor any bound on communication or computation delays. Garfield leverages ML specificities to make progress despite consensus being impossible in such an asynchronous, Byzantine environment. Following the classical server/worker architecture, Garfield replicates the parameter server while relying on the statistical properties of stochastic gradient descent to keep the models on the correct servers close to each other. On the other hand, Garfield uses statistically-robust gradient aggregation rules (GARs) to achieve resilience against Byzantine workers. We integrate Garfield with two widely-used ML frameworks, TensorFlow and PyTorch, while achieving transparency: applications developed with either framework do not need to change their interfaces to be made Byzantine resilient. Our implementation supports full-stack computations on both CPUs and GPUs. We report on our evaluation of Garfield with different (a) baselines, (b) ML models (e.g., ResNet-50 and VGG), and (c) hardware infrastructures (CPUs and GPUs). Our evaluation highlights several interesting facts about the cost of Byzantine resilience. In particular, (a) Byzantine resilience, unlike crash resilience, induces an accuracy loss, and (b) the throughput overhead comes much more from communication (70%) than from aggregation.","",""
0,"Mahmood Sharif","Practical Inference-Time Attacks Against Machine-Learning Systems and a Defense Against Them",2020,"","","","",98,"2022-07-13 09:24:04","","10.1184/R1/11568513.V1","","",,,,,0,0.00,0,1,2,"Prior work has shown that machine-learning algorithms are vulnerable to evasion by socalled adversarial examples. Nonetheless, the majority of the work on evasion attackshas mainly explored Lp-bounded perturbations that lead to misclassification. From a computer-security perspective, such attacks have limited practical implications. To fillthe gap, we propose evasion attacks that satisfy multiple objectives, and show that these attacks pose a practical threat to computer systems. In particular, we demonstrate how to produce adversarial examples against state-of-the-art face-recognition and malwaredetection systems that simultaneously satisfy multiple objectives (e.g., smoothness and robustness against changes in imaging conditions) to mislead the systems in practical settings. Against face recognition, we develop a systematic method to automatically generate attacks, which are realized through printing a pair of eyeglass frames. When worn by attackers, the eyeglasses allow them mislead face-recognition algorithms to evade recognition or impersonate other individuals. Against malware detection, we develop an attack that guides binary-diversification tools via optimization to transform binaries in a functionality preserving manner and mislead detection. The attacks that we initially demonstrate achieve the desired objectives via ad hoc optimizations. We extend these attacks via a general framework to train a generator neural network to emit adversarial examples satisfying desired objectives. We demonstratethe ability of the proposed framework to accommodate a wide range of objectives, including imprecise ones difficult to model, in two application domains. Specifically,we demonstrate how to produce adversarial eyeglass frames to mislead face recognition with better robustness, inconspicuousness, and scalability than previous approaches, as well as a new attack to fool a handwritten-digit classifier. Finally, to protect computer-systems from adversarial examples, we propose n-ML—a novel defense that is inspired by n-version programming. n-ML trains an ensemble of n classifiers, and classifies inputs by a vote. Unlike prior approaches, however, the classifiers are trained to classify adversarial examples differently than each other,rendering it very difficult for an adversarial example to obtain enough votes to be misclassified. In several application domains (including face and street-sign recognition),we show that n-ML roughly retains the benign classification accuracies of state-of-theart models, while simultaneously defending against adversarial examples (producedby our framework, or Lp-based attacks) with better resilience than the best defenses known to date and, in most cases, with lower inference-time overhead.","",""
105,"Zidong Du, K. Palem, L. Avinash, O. Temam, Yunji Chen, Chengyong Wu","Leveraging the error resilience of machine-learning applications for designing highly energy efficient accelerators",2014,"","","","",99,"2022-07-13 09:24:04","","10.1109/ASPDAC.2014.6742890","","",,,,,105,13.13,18,6,8,"In recent years, inexact computing has been increasingly regarded as one of the most promising approaches for reducing energy consumption in many applications that can tolerate a degree of inaccuracy. Driven by the principle of trading tolerable amounts of application accuracy in return for significant resource savings - the energy consumed, the (critical path) delay and the (silicon) area being the resources - this approach has been limited to certain application domains. In this paper, we propose to expand the application scope, error tolerance as well as the energy savings of inexact computing systems through neural network architectures. Such neural networks are fast emerging as popular candidate accelerators for future heterogeneous multi-core platforms, and have flexible error tolerance limits owing to their ability to be trained. Our results based on simulated 65nm technology designs demonstrate that the proposed inexact neural network accelerator could achieve 43.91%-62.49% savings in energy consumption (with corresponding delay and area savings being 18.79% and 31.44% respectively) when compared to existing baseline neural network implementation, at the cost of an accuracy loss (quantified as the Mean Square Error (MSE) which increases from 0.14 to 0.20 on average).","",""
1920,"A. Kurakin, Ian J. Goodfellow, Samy Bengio","Adversarial Machine Learning at Scale",2016,"","","","",100,"2022-07-13 09:24:04","","","","",,,,,1920,320.00,640,3,6,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.","",""
484,"Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, K. Ramchandran","Speeding Up Distributed Machine Learning Using Codes",2015,"","","","",101,"2022-07-13 09:24:04","","10.1109/TIT.2017.2736066","","",,,,,484,69.14,97,5,7,"Codes are widely used in many engineering applications to offer <italic>robustness</italic> against <italic>noise</italic>. In large-scale systems, there are several types of noise that can affect the performance of distributed machine learning algorithms—straggler nodes, system failures, or communication bottlenecks—but there has been little interaction cutting across codes, machine learning, and distributed systems. In this paper, we provide theoretical insights on how <italic>coded</italic> solutions can achieve significant gains compared with uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: <italic>matrix multiplication</italic> and <italic>data shuffling</italic>. For matrix multiplication, we use codes to alleviate the effect of stragglers and show that if the number of homogeneous workers is <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula>, and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of <inline-formula> <tex-math notation=""LaTeX"">$\log n$ </tex-math></inline-formula>. For data shuffling, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction <inline-formula> <tex-math notation=""LaTeX"">$\alpha $ </tex-math></inline-formula> of the data matrix can be cached at each worker, and <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> is the number of workers, <italic>coded shuffling</italic> reduces the communication cost by a factor of <inline-formula> <tex-math notation=""LaTeX"">$\left({\alpha + \frac {1}{n}}\right)\gamma (n)$ </tex-math></inline-formula> compared with uncoded shuffling, where <inline-formula> <tex-math notation=""LaTeX"">$\gamma (n)$ </tex-math></inline-formula> is the ratio of the cost of unicasting <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> messages to <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> users to multicasting a common message (of the same size) to <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> users. For instance, <inline-formula> <tex-math notation=""LaTeX"">$\gamma (n) \simeq n$ </tex-math></inline-formula> if multicasting a message to <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> users is as cheap as unicasting a message to one user. We also provide experimental results, corroborating our theoretical gains of the coded algorithms.","",""
84,"Y. Khourdifi, M. Bahaj","Heart Disease Prediction and Classification Using Machine Learning Algorithms Optimized by Particle Swarm Optimization and Ant Colony Optimization",2019,"","","","",102,"2022-07-13 09:24:04","","10.22266/IJIES2019.0228.24","","",,,,,84,28.00,42,2,3,"The prediction of heart disease is one of the areas where machine learning can be implemented. Optimization algorithms have the advantage of dealing with complex non-linear problems with a good flexibility and adaptability. In this paper, we exploited the Fast Correlation-Based Feature Selection (FCBF) method to filter redundant features in order to improve the quality of heart disease classification. Then, we perform a classification based on different classification algorithms such as K-Nearest Neighbour, Support Vector Machine, Naïve Bayes, Random Forest and a Multilayer Perception | Artificial Neural Network optimized by Particle Swarm Optimization (PSO) combined with Ant Colony Optimization (ACO) approaches. The proposed mixed approach is applied to heart disease dataset; the results demonstrate the efficacy and robustness of the proposed hybrid method in processing various types of data for heart disease classification. Therefore, this study examines the different machine learning algorithms and compares the results using different performance measures, i.e. accuracy, precision, recall, f1-score, etc. A maximum classification accuracy of 99.65% using the optimized model proposed by FCBF, PSO and ACO. The results show that the performance of the proposed system is superior to that of the classification technique presented above.","",""
33,"V. D. Florio","Antifragility = Elasticity + Resilience + Machine Learning: Models and Algorithms for Open System Fidelity",2014,"","","","",103,"2022-07-13 09:24:04","","10.1016/j.procs.2014.05.499","","",,,,,33,4.13,33,1,8,"","",""
42,"Hiromu Araki, T. Mizoguchi, Y. Hatsugai","Phase diagram of a disordered higher-order topological insulator: A machine learning study",2018,"","","","",104,"2022-07-13 09:24:04","","10.1103/PhysRevB.99.085406","","",,,,,42,10.50,14,3,4,"A higher-order topological insulator is a new concept of topological states of matter, which is characterized by the emergent boundary states whose dimensionality is lower by more than two compared with that of the bulk, and draws a considerable interest. Yet, its robustness against disorders is still unclear. In this work, we investigate a phase diagram of higher-order topological insulator phases in a breathing kagome model in the presence of disorders by using a state-of-the-art machine learning technique. We find that the corner states survive against the finite strength of disorder potential as long as the energy gap is not closed, indicating the stability of the higher-order topological phases against the disorders.","",""
45,"R. Roelofs, Vaishaal Shankar, B. Recht, Sara Fridovich-Keil, Moritz Hardt, John Miller, Ludwig Schmidt","A Meta-Analysis of Overfitting in Machine Learning",2019,"","","","",105,"2022-07-13 09:24:04","","","","",,,,,45,15.00,6,7,3,"We conduct the first large meta-analysis of overfitting due to test set reuse in the machine learning community. Our analysis is based on over one hundred machine learning competitions hosted on the Kaggle platform over the course of several years. In each competition, numerous practitioners repeatedly evaluated their progress against a holdout set that forms the basis of a public ranking available throughout the competition. Performance on a separate test set used only once determined the final ranking. By systematically comparing the public ranking with the final ranking, we assess how much participants adapted to the holdout set over the course of a competition. Our study shows, somewhat surprisingly, little evidence of substantial overfitting. These findings speak to the robustness of the holdout method across different data domains, loss functions, model classes, and human analysts.","",""
34,"O. Tuncer, E. Ates, Yijia Zhang, Ata Turk, J. Brandt, V. Leung, Manuel Egele, A. Coskun","Online Diagnosis of Performance Variation in HPC Systems Using Machine Learning",2019,"","","","",106,"2022-07-13 09:24:04","","10.1109/TPDS.2018.2870403","","",,,,,34,11.33,4,8,3,"As the size and complexity of high performance computing (HPC) systems grow in line with advancements in hardware and software technology, HPC systems increasingly suffer from performance variations due to shared resource contention as well as software- and hardware-related problems. Such performance variations can lead to failures and inefficiencies, which impact the cost and resilience of HPC systems. To minimize the impact of performance variations, one must quickly and accurately detect and diagnose the anomalies that cause the variations and take mitigating actions. However, it is difficult to identify anomalies based on the voluminous, high-dimensional, and noisy data collected by system monitoring infrastructures. This paper presents a novel machine learning based framework to automatically diagnose performance anomalies at runtime. Our framework leverages historical resource usage data to extract signatures of previously-observed anomalies. We first convert collected time series data into easy-to-compute statistical features. We then identify the features that are required to detect anomalies, and extract the signatures of these anomalies. At runtime, we use these signatures to diagnose anomalies with negligible overhead. We evaluate our framework using experiments on a real-world HPC supercomputer and demonstrate that our approach successfully identifies 98 percent of injected anomalies and consistently outperforms existing anomaly diagnosis techniques.","",""
30,"Zitao Chen, Guanpeng Li, K. Pattabiraman, Nathan Debardeleben","BinFI: an efficient fault injector for safety-critical machine learning systems",2019,"","","","",107,"2022-07-13 09:24:04","","10.1145/3295500.3356177","","",,,,,30,10.00,8,4,3,"As machine learning (ML) becomes pervasive in high performance computing, ML has found its way into safety-critical domains (e.g., autonomous vehicles). Thus the reliability of ML has grown in importance. Specifically, failures of ML systems can have catastrophic consequences, and can occur due to soft errors, which are increasing in frequency due to system scaling. Therefore, we need to evaluate ML systems in the presence of soft errors. In this work, we propose BinFI, an efficient fault injector (FI) for finding the safety-critical bits in ML applications. We find the widely-used ML computations are often monotonic. Thus we can approximate the error propagation behavior of a ML application as a monotonic function. BinFI uses a binary-search like FI technique to pinpoint the safety-critical bits (also measure the overall resilience). BinFI identifies 99.56% of safety-critical bits (with 99.63% precision) in the systems, which significantly outperforms random FI, with much lower costs.","",""
0,"Basel Halak, Mohd Syafiq Mispan","An Overview of Machine Learning Applications in Hardware Security",2022,"","","","",108,"2022-07-13 09:24:04","","10.1109/DTS55284.2022.9809857","","",,,,,0,0.00,0,2,1,"this study explores the uses of machine learning (ML) in the field of hardware security, in particular, two applications areas are considered, namely, hardware Trojan (HT) and IC counterfeits. These examples demonstrate how ML algorithms can be employed as a defense mechanism to detect forged or tampered-with circuits. Our analysis shows that the ML detection accuracy still has not reached 100%. The selection and size of the feature vectors greatly affect the performance of the learning models, however, increasing the number of features or their size can lead to large overheads. Therefore, a thorough analysis is required to only select the appropriate- ate several relevant features that significantly contribute to the accuracy of ML models. The study also highlighted the need for a more robust deployment of ML algorithms to enhance their resilience to adversarial attacks.","",""
11,"B. Celik, J. Vanschoren","Adaptation Strategies for Automated Machine Learning on Evolving Data",2020,"","","","",109,"2022-07-13 09:24:04","","10.1109/TPAMI.2021.3062900","","",,,,,11,5.50,6,2,2,"Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust to changes in the underlying data. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on a variety of AutoML approaches for building machine learning pipelines, including Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.","",""
10,"Sam Andersson, Deepti R. Bathula, S. Iliadis, M. Walter, A. Skalkidou","Predicting women with depressive symptoms postpartum with machine learning methods",2021,"","","","",110,"2022-07-13 09:24:04","","10.1038/s41598-021-86368-y","","",,,,,10,10.00,2,5,1,"","",""
12,"Lizon Maharjan, Mark Ditsworth, Manish Niraula, Carlos Caicedo Narvaez, B. Fahimi","Machine learning based energy management system for grid disaster mitigation",2019,"","","","",111,"2022-07-13 09:24:04","","10.1049/IET-STG.2018.0043","","",,,,,12,4.00,2,5,3,"The recent increase in infiltration of distributed resources has challenged the traditional operation of power systems. Simultaneously, devastating effects of recent natural disasters have questioned the resilience of power infrastructure for an electricity dependent community. In this study, a solution has been presented in the form of a resilient smart grid network which utilises distributed energy resources (DERs) and machine learning (ML) algorithms to improve the power availability during disastrous events. In addition to power electronics with load categorisation features, the presented system utilises ML tools to use the information from neighbouring units and external sources to make complicated logical decisions directed towards providing power to critical loads at all times. Furthermore, the provided model encourages consideration of ML tools as a part of smart grid design process together with power electronics and controls, rather than as an additional feature.","",""
12,"S. M. Rasel, Hsing-chung Chang, T. Ralph, N. Saintilan, I. J. Diti","Application of feature selection methods and machine learning algorithms for saltmarsh biomass estimation using Worldview-2 imagery",2019,"","","","",112,"2022-07-13 09:24:04","","10.1080/10106049.2019.1624988","","",,,,,12,4.00,2,5,3,"Abstract Assessing large scale plant productivity of coastal marshes is essential to understand the resilience of these systems to climate change. Two machine learning approaches, random forest (RF) and support vector machine (SVM) regression were tested to estimate biomass of a common saltmarshes species, salt couch grass (Sporobolus virginicus). Reflectance and vegetation indices derived from 8 bands of Worldview-2 multispectral data were used for four experiments to develop the biomass model. These four experiments were, Experiment-1: 8 bands of Worldview-2 image, Experiment-2: Possible combination of all bands of Worldview-2 for Normalized Difference Vegetation Index (NDVI) type vegetation indices, Experiment-3: Combination of bands and vegetation indices, Experiment-4: Selected variables derived from experiment-3 using variable selection methods. The main objectives of this study are (i) to recommend an affordable low cost data source to predict biomass of a common saltmarshes species, (ii) to suggest a variable selection method suitable for multispectral data, (iii) to assess the performance of RF and SVM for the biomass prediction model. Cross-validation of parameter optimizations for SVM showed that optimized parameter of ɛ-SVR failed to provide a reliable prediction. Hence, ν-SVR was used for the SVM model. Among the different variable selection methods, recursive feature elimination (RFE) selected a minimum number of variables (only 4) with an RMSE of 0.211 (kg/m2). Experiment-4 (only selected bands) provided the best results for both of the machine learning regression methods, RF (R 2= 0.72, RMSE= 0.166 kg/m2) and SVR (R 2= 0.66, RMSE = 0.200 kg/m2) to predict biomass. When a 10-fold cross validation of the RF model was compared with a 10-fold cross validation of SVR, a significant difference (p = <0.0001) was observed for RMSE. One to one comparisons of actual to predicted biomass showed that RF underestimates the high biomass values, whereas SVR overestimates the values; this suggests a need for further investigation and refinement.","",""
1,"J. Sierchio, L. Bookman, Emily Clark, Daniel Clymer, Tao Wang","Measuring robustness and resilience against counters on autonomous platforms",2021,"","","","",113,"2022-07-13 09:24:04","","10.1117/12.2587562","","",,,,,1,1.00,0,5,1,"Autonomous platforms are becoming ubiquitous in society, including UAVs, Roombas, and self-driving cars. With the increase in prevalence of autonomous platforms comes an increase in the threat of attacks against these platforms. These attacks can range from direct hacking to remotely take control of the platforms themselves [1], to attacks involving manipulation or deception such as spoofing or fooling sensor inputs [2, 3]. Ensuring autonomous systems are robust and resilient (R2) against these attacks will become an important challenge to overcome if they are to be trusted and widely adopted. This paper addresses the need to quantitatively define robustness and resilience against manipulation and deceptive attacks which are inherently harder to detect. We define a set of robust estimation metrics that are mathematically rigorous, can be applied to multiple algorithm use cases, and are easy to interpret. Since many of these functions are processed over time, the primary focus will be on process-based metrics. These metrics can be adapted over time by responding and reconfiguring at system runtime. This paper will: 1) provide background information on previous work in this area, including adversarial machine learning, robotics control, and engineering design. 2) Present the metrics and explain how to address our unique problem. 3) Apply these metrics to three different autonomy applications: target tracking, autonomous control, and automatic target recognition. 4) Discuss some additional caveats and potential areas for future work.","",""
9,"M. S. Sidhu, D. Ronanki, S. Williamson","State of Charge Estimation of Lithium-Ion Batteries Using Hybrid Machine Learning Technique",2019,"","","","",114,"2022-07-13 09:24:04","","10.1109/IECON.2019.8927066","","",,,,,9,3.00,3,3,3,"The pivotal features of low self-discharge, high energy density and long calendar life lead the Lithium-ion (Li-ion) batteries as being a mainstream energy storage source in electric vehicles (EVs). A meticulous estimation of the state of charge (SOC) is indispensable for ensuring safe and reliable operations in battery powered EVs. However, SOC estimation of Li-ion battery with high accuracy have become a major challenge in the automotive industry. To fulfill reliable operation in EVs, researchers have proposed numerous SOC estimators through model based or machine learning techniques. This paper presents an improved SOC estimation of Li-ion battery using random forest (RF) regression, which is robust and effective for controlling dynamic systems. To ensure good resilience and accuracy, a Gaussian filter is adopted at the final stage to minimize the variations in the SOC estimation. The proposed SOC estimator is verified on the experimental data of the Li-ion battery under Federal test driving schedules and different operating temperatures. Results show that the proposed SOC estimator displays sufficient accuracy and outperforms the traditional artificial intelligence based approaches.","",""
111,"Heinrich Jiang, Ofir Nachum","Identifying and Correcting Label Bias in Machine Learning",2019,"","","","",115,"2022-07-13 09:24:04","","","","",,,,,111,37.00,56,2,3,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.","",""
102,"Peng Xu, Farbod Roosta-Khorasani, Michael W. Mahoney","Second-Order Optimization for Non-Convex Machine Learning: An Empirical Study",2017,"","","","",116,"2022-07-13 09:24:04","","10.1137/1.9781611976236.23","","",,,,,102,20.40,34,3,5,"While first-order optimization methods such as stochastic gradient descent (SGD) are popular in machine learning (ML), they come with well-known deficiencies, including relatively-slow convergence, sensitivity to the settings of hyper-parameters such as learning rate, stagnation at high training errors, and difficulty in escaping flat regions and saddle points. These issues are particularly acute in highly non-convex settings such as those arising in neural networks. Motivated by this, there has been recent interest in second-order methods that aim to alleviate these shortcomings by capturing curvature information. In this paper, we report detailed empirical evaluations of a class of Newton-type methods, namely sub-sampled variants of trust region (TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex ML problems. In doing so, we demonstrate that these methods not only can be computationally competitive with hand-tuned SGD with momentum, obtaining comparable or better generalization performance, but also they are highly robust to hyper-parameter settings. Further, in contrast to SGD with momentum, we show that the manner in which these Newton-type methods employ curvature information allows them to seamlessly escape flat regions and saddle points.","",""
8,"Issam Hammad, K. El-Sankary","Practical Considerations for Accuracy Evaluation in Sensor-Based Machine Learning and Deep Learning",2019,"","","","",117,"2022-07-13 09:24:04","","10.3390/s19163491","","",,,,,8,2.67,4,2,3,"Accuracy evaluation in machine learning is based on the split of data into a training set and a test set. This critical step is applied to develop machine learning models including models based on sensor data. For sensor-based problems, comparing the accuracy of machine learning models using the train/test split provides only a baseline comparison in ideal situations. Such comparisons won’t consider practical production problems that can impact the inference accuracy such as the sensors’ thermal noise, performance with lower inference quantization, and tolerance to sensor failure. Therefore, this paper proposes a set of practical tests that can be applied when comparing the accuracy of machine learning models for sensor-based problems. First, the impact of the sensors’ thermal noise on the models’ inference accuracy was simulated. Machine learning algorithms have different levels of error resilience to thermal noise, as will be presented. Second, the models’ accuracy using lower inference quantization was compared. Lowering inference quantization leads to lowering the analog-to-digital converter (ADC) resolution which is cost-effective in embedded designs. Moreover, in custom designs, analog-to-digital converters’ (ADCs) effective number of bits (ENOB) is usually lower than the ideal number of bits due to various design factors. Therefore, it is practical to compare models’ accuracy using lower inference quantization. Third, the models’ accuracy tolerance to sensor failure was evaluated and compared. For this study, University of California Irvine (UCI) ‘Daily and Sports Activities’ dataset was used to present these practical tests and their impact on model selection.","",""
519,"P. Blanchard, El Mahdi El Mhamdi, R. Guerraoui, J. Stainer","Machine Learning with Adversaries: Byzantine Tolerant Gradient Descent",2017,"","","","",118,"2022-07-13 09:24:04","","","","",,,,,519,103.80,130,4,5,"We study the resilience to Byzantine failures of distributed implementations of Stochastic Gradient Descent (SGD). So far, distributed machine learning frameworks have largely ignored the possibility of failures, especially arbitrary (i.e., Byzantine) ones. Causes of failures include software bugs, network asynchrony, biases in local datasets, as well as attackers trying to compromise the entire system. Assuming a set of n workers, up to f being Byzantine, we ask how resilient can SGD be, without limiting the dimension, nor the size of the parameter space. We first show that no gradient aggregation rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the aggregation rule capturing the basic requirements to guarantee convergence despite f Byzantine workers. We propose Krum, an aggregation rule that satisfies our resilience property, which we argue is the first provably Byzantine-resilient algorithm for distributed SGD. We also report on experimental evaluations of Krum.","",""
3,"Arunkumar Vijayan, K. Chakrabarty, M. Tahoori","Machine Learning-Based Aging Analysis",2019,"","","","",119,"2022-07-13 09:24:04","","10.1007/978-3-030-04666-8_9","","",,,,,3,1.00,1,3,3,"","",""
6,"Marc-Oliver Pahl, Stefan Liebald, Lars Wustrich","Machine-Learning based IoT Data Caching",2019,"","","","",120,"2022-07-13 09:24:04","","","","",,,,,6,2.00,2,3,3,"The Internet of Things (IoT) continuously produces big amounts of data. Data-centric middleware can therefore help reducing the complexity when orchestrating distributed Things. With its heterogeneity and resource limitations, IoT applications can lack performance, scalability, or resilience. Caching can help overcoming the limitations.We are currently working on establishing data caching within IoT middleware. The paper presents fundamentals of caching, major challenges, relevant state of the art, and a description of our current approaches. We show directions of using machine learning for caching in the IoT.","",""
3,"Haoyu Yang, Wen Chen, P. Pathak, Frank Gennari, Ya-Chieh Lai, Bei Yu","Automatic Layout Generation with Applications in Machine Learning Engine Evaluation",2019,"","","","",121,"2022-07-13 09:24:04","","10.1109/MLCAD48534.2019.9142121","","",,,,,3,1.00,1,6,3,"Machine learning-based lithography hotspot detection has been deeply studied recently, from varies feature extraction techniques to efficient learning models. It has been observed that such machine learning-based frameworks are providing satisfactory metal layer hotspot prediction results on known public metal layer benchmarks. In this work, we seek to evaluate how these machine learning-based hotspot detectors generalize to complicated patterns. We first introduce a automatic layout generation tool that can synthesize varies layout patterns given a set of design rules. The tool currently supports both metal layer and via layer generation. As a case study, we conduct hotspot detection on the generated via layer layouts with representative machine learning-based hotspot detectors, which shows that continuous study on model robustness and generality is necessary to prototype and integrate the learning engines in DFM flows. The source code of the layout generation tool will be available at https://github.com/phdyang007/layout-generation.","",""
3,"Minsung Hong, R. Akerkar","Analytics and Evolving Landscape of Machine Learning for Emergency Response",2019,"","","","",122,"2022-07-13 09:24:04","","10.1007/978-3-030-15628-2_11","","",,,,,3,1.00,2,2,3,"","",""
1,"Yu Zhang","Post-Earthquake Performance Assessment and Decision-Making for Tall Buildings: Integrating Statistical Modeling, Machine Learning, Stochastic Simulation and Optimization",2019,"","","","",123,"2022-07-13 09:24:04","","","","",,,,,1,0.33,1,1,3,"Author(s): Zhang, Yu | Advisor(s): Burton, Henry V | Abstract: With the embrace of the seismic resilience concept as a measure of the ability of constructed facilities and communities to contain the effects of an earthquake and achieve a timely recovery, the critical role of tall buildings in supporting community functionality, has been brought to the forefront. This study presents a series of frameworks for performing post-earthquake assessment and optimal decision-making for tall buildings. A machine learning framework to assess structural safety is first proposed and applied to a low-rise frame building. A similar methodology is then adapted for tall buildings, while incorporating robust techniques to deal with the high-dimension feature space that arises because of the large number of structural components. Seismic risk assessment is then carried out for the tall building by comparing the time-dependent probability of exceeding various response demand limits over a pre-defined period considering both mainshock-only and mainshock-aftershock hazard. The risk-based consistency of the limit state acceptance criteria for engineering demand parameters is also examined. Finally, a methodology to support optimal decision-making following the mainshock is developed with the goal of minimizing expected financial losses and, at the same time, ensuring life safety. The proposed prediction model, risk assessment methodology and optimal decision-making strategy can provide critical insights into the seismic performance of mainshock-damaged tall buildings and inform the post-earthquake actions of occupants, structural engineers, insurance companies and policymakers.","",""
104,"Yangkang Zhang","Automatic microseismic event picking via unsupervised machine learning",2020,"","","","",124,"2022-07-13 09:24:04","","10.1093/GJI/GGX420","","",,,,,104,52.00,104,1,2,"  Effective and efficient arrival picking plays an important role in microseismic and earthquake data processing and imaging. Widely used short-term-average long-term-average ratio (STA/LTA) based arrival picking algorithms suffer from the sensitivity to moderate-to-strong random ambient noise. To make the state-of-the-art arrival picking approaches effective, microseismic data need to be first pre-processed, for example, removing sufficient amount of noise, and second analysed by arrival pickers. To conquer the noise issue in arrival picking for weak microseismic or earthquake event, I leverage the machine learning techniques to help recognizing seismic waveforms in microseismic or earthquake data. Because of the dependency of supervised machine learning algorithm on large volume of well-designed training data, I utilize an unsupervised machine learning algorithm to help cluster the time samples into two groups, that is, waveform points and non-waveform points. The fuzzy clustering algorithm has been demonstrated to be effective for such purpose. A group of synthetic, real microseismic and earthquake data sets with different levels of complexity show that the proposed method is much more robust than the state-of-the-art STA/LTA method in picking microseismic events, even in the case of moderately strong background noise.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",125,"2022-07-13 09:24:04","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",126,"2022-07-13 09:24:04","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
74,"Monika A. Myszczynska, P. Ojamies, Alix M. B. Lacoste, Daniel Neil, Amir Saffari, R. Mead, G. Hautbergue, J. Holbrook, L. Ferraiuolo","Applications of machine learning to diagnosis and treatment of neurodegenerative diseases",2020,"","","","",127,"2022-07-13 09:24:04","","10.1038/s41582-020-0377-8","","",,,,,74,37.00,8,9,2,"","",""
78,"Xianfang Wang, Peng Gao, Yifeng Liu, Hongfei Li, Fan Lu","Predicting Thermophilic Proteins by Machine Learning",2020,"","","","",128,"2022-07-13 09:24:04","","10.2174/1574893615666200207094357","","",,,,,78,39.00,16,5,2,"  Thermophilic proteins can maintain good activity under high temperature, therefore, it is important to study thermophilic proteins for the thermal stability of proteins.    In order to solve the problem of low precision and low efficiency in predicting thermophilic proteins, a prediction method based on feature fusion and machine learning was proposed in this paper.    For the selected thermophilic data sets, firstly, the thermophilic protein sequence was characterized based on feature fusion by the combination of g-gap dipeptide, entropy density and autocorrelation coefficient. Then, Kernel Principal Component Analysis (KPCA) was used to reduce the dimension of the expressed protein sequence features in order to reduce the training time and improve efficiency. Finally, the classification model was designed by using the classification algorithm.    A variety of classification algorithms was used to train and test on the selected thermophilic dataset. By comparison, the accuracy of the Support Vector Machine (SVM) under the jackknife method was over 92%. The combination of other evaluation indicators also proved that the SVM performance was the best.     Because of choosing an effectively feature representation method and a robust classifier, the proposed method is suitable for predicting thermophilic proteins and is superior to most reported methods. ","",""
0,"Shuo Liu, Nirupam Gupta, N. Vaidya","Utilizing Redundancy in Cost Functions for Resilience in Distributed Optimization and Learning",2021,"","","","",129,"2022-07-13 09:24:04","","","","",,,,,0,0.00,0,3,1,"This paper considers the problem of resilient distributed optimization and stochastic machine learning in a server-based architecture. The system comprises a server and multiple agents, where each agent has a local cost function. The agents collaborate with the server to find a minimum of their aggregate cost functions. We consider the case when some of the agents may be asynchronous and/or Byzantine faulty. In this case, the classical algorithm of distributed gradient descent (DGD) is rendered ineffective. Our goal is to design techniques improving the efficacy of DGD with asynchrony and Byzantine failures. To do so, we start by proposing a way to model the agents’ cost functions by the generic notion of (f, r; )-redundancy where f and r are the parameters of Byzantine failures and asynchrony, respectively, and characterizes the closeness between agents’ cost functions. This allows us to quantify the level of redundancy present amongst the agents’ cost functions, for any given distributed optimization problem. We demonstrate, both theoretically and empirically, the merits of our proposed redundancy model in improving the robustness of DGD against asynchronous and Byzantine agents, and their extensions to distributed stochastic gradient descent (D-SGD) for robust distributed machine learning with asynchronous and Byzantine agents. This report supersedes our previous report [36] as it contains the most of the results in it. Georgetown University. Email: sl1539@georgetown.edu. École Polytechnique Fédérale de Lausanne (EPFL). Email: nirupam.gupta@epfl.ch. Georgetown University. Email: nitin.vaidya@georgetown.edu. 1 ar X iv :2 11 0. 10 85 8v 1 [ cs .D C ] 2 1 O ct 2 02 1","",""
53,"M. Shafique, Mahum Naseer, T. Theocharides, C. Kyrkou, O. Mutlu, Lois Orosa, Jungwook Choi","Robust Machine Learning Systems: Challenges,Current Trends, Perspectives, and the Road Ahead",2020,"","","","",130,"2022-07-13 09:24:04","","10.1109/MDAT.2020.2971217","","",,,,,53,26.50,8,7,2,"Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPS) and Internet-of-Things (IoT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques. —Partha Pratim Pande, Washington State University","",""
41,"Sirui Lu, L. Duan, D. Deng","Quantum Adversarial Machine Learning",2019,"","","","",131,"2022-07-13 09:24:04","","10.1103/PHYSREVRESEARCH.2.033212","","",,,,,41,13.67,14,3,3,"Adversarial machine learning is an emerging field that focuses on studying vulnerabilities of machine learning approaches in adversarial settings and developing techniques accordingly to make learning robust to adversarial manipulations. It plays a vital role in various machine learning applications and has attracted tremendous attention across different communities recently. In this paper, we explore different adversarial scenarios in the context of quantum machine learning. We find that, similar to traditional classifiers based on classical neural networks, quantum learning systems are likewise vulnerable to crafted adversarial examples, independent of whether the input data is classical or quantum. In particular, we find that a quantum classifier that achieves nearly the state-of-the-art accuracy can be conclusively deceived by adversarial examples obtained via adding imperceptible perturbations to the original legitimate samples. This is explicitly demonstrated with quantum adversarial learning in different scenarios, including classifying real-life images (e.g., handwritten digit images in the dataset MNIST), learning phases of matter (such as, ferromagnetic/paramagnetic orders and symmetry protected topological phases), and classifying quantum data. Furthermore, we show that based on the information of the adversarial examples at hand, practical defense strategies can be designed to fight against a number of different attacks. Our results uncover the notable vulnerability of quantum machine learning systems to adversarial perturbations, which not only reveals a novel perspective in bridging machine learning and quantum physics in theory but also provides valuable guidance for practical applications of quantum classifiers based on both near-term and future quantum technologies.","",""
103,"Muhammad Attique Khan, I. Ashraf, M. Alhaisoni, Robertas Damaševičius, R. Scherer, A. Rehman, S. Bukhari","Multimodal Brain Tumor Classification Using Deep Learning and Robust Feature Selection: A Machine Learning Application for Radiologists",2020,"","","","",132,"2022-07-13 09:24:04","","10.3390/diagnostics10080565","","",,,,,103,51.50,15,7,2,"Manual identification of brain tumors is an error-prone and tedious process for radiologists; therefore, it is crucial to adopt an automated system. The binary classification process, such as malignant or benign is relatively trivial; whereas, the multimodal brain tumors classification (T1, T2, T1CE, and Flair) is a challenging task for radiologists. Here, we present an automated multimodal classification method using deep learning for brain tumor type classification. The proposed method consists of five core steps. In the first step, the linear contrast stretching is employed using edge-based histogram equalization and discrete cosine transform (DCT). In the second step, deep learning feature extraction is performed. By utilizing transfer learning, two pre-trained convolutional neural network (CNN) models, namely VGG16 and VGG19, were used for feature extraction. In the third step, a correntropy-based joint learning approach was implemented along with the extreme learning machine (ELM) for the selection of best features. In the fourth step, the partial least square (PLS)-based robust covariant features were fused in one matrix. The combined matrix was fed to ELM for final classification. The proposed method was validated on the BraTS datasets and an accuracy of 97.8%, 96.9%, 92.5% for BraTs2015, BraTs2017, and BraTs2018, respectively, was achieved.","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",133,"2022-07-13 09:24:04","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
2,"Sarath Shekkizhar, Antonio Ortega","Revisiting Local Neighborhood Methods in Machine Learning",2021,"","","","",134,"2022-07-13 09:24:04","","10.1109/DSLW51110.2021.9523409","","",,,,,2,2.00,1,2,1,"Several machine learning methods leverage the idea of locality by using k-nearest neighbor (KNN) techniques to design better pattern recognition models. However, the choice of KNN parameters such as k is often made experimentally, e.g., via cross-validation, leading to local neighborhoods without a clear geometric interpretation. In this paper, we replace KNN with our recently introduced polytope neighborhood scheme - Non Negative Kernel regression (NNK). NNK formulates neighborhood selection as a sparse signal approximation problem and is adaptive to the local distribution of samples in the neighborhood of the data point of interest. We analyze the benefits of local neighborhood construction based on NNK. In particular, we study the generalization properties of local interpolation using NNK and present data dependent bounds in the non asymptotic setting. The applicability of NNK in transductive few shot learning setting and for measuring distance between two datasets is demonstrated. NNK exhibits robust, superior performance in comparison to standard locally weighted neighborhood methods.","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",135,"2022-07-13 09:24:04","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
22,"Lal Hussain, I. Awan, W. Aziz, Sharjil Saeed, Amjad Ali, Farukh Zeeshan, K. Kwak","Detecting Congestive Heart Failure by Extracting Multimodal Features and Employing Machine Learning Techniques",2020,"","","","",136,"2022-07-13 09:24:04","","10.1155/2020/4281243","","",,,,,22,11.00,3,7,2,"The adaptability of heart to external and internal stimuli is reflected by the heart rate variability (HRV). Reduced HRV can be a predictor of negative cardiovascular outcomes. Based on the nonlinear, nonstationary, and highly complex dynamics of the controlling mechanism of the cardiovascular system, linear HRV measures have limited capability to accurately analyze the underlying dynamics. In this study, we propose an automated system to analyze HRV signals by extracting multimodal features to capture temporal, spectral, and complex dynamics. Robust machine learning techniques, such as support vector machine (SVM) with its kernel (linear, Gaussian, radial base function, and polynomial), decision tree (DT), k-nearest neighbor (KNN), and ensemble classifiers, were employed to evaluate the detection performance. Performance was evaluated in terms of specificity, sensitivity, positive predictive value (PPV), negative predictive value (NPV), and area under the receiver operating characteristic curve (AUC). The highest performance was obtained using SVM linear kernel (TA = 93.1%, AUC = 0.97, 95% CI [lower bound = 0.04, upper bound = 0.89]), followed by ensemble subspace discriminant (TA = 91.4%, AUC = 0.96, 95% CI [lower bound 0.07, upper bound = 0.81]) and SVM medium Gaussian kernel (TA = 90.5%, AUC = 0.95, 95% CI [lower bound = 0.07, upper bound = 0.86]). The results reveal that the proposed approach can provide an effective and computationally efficient tool for automatic detection of congestive heart failure patients.","",""
0,"Uma Gunasilan","Debate as a learning activity for teaching programming: a case in the subject of machine learning",2021,"","","","",137,"2022-07-13 09:24:04","","10.1108/heswbl-01-2021-0006","","",,,,,0,0.00,0,1,1,"PurposeDebates are well known to encompass a variety of skills we would like higher education candidates to embody when they graduate.Design/methodology/approachDebates in a classroom with computer science as the main subject has been popular in high schools particularly with emerging issues around the area, however it does not have as an extensive similar documented outreach in tertiary education, particularly in the area of hard computer sciences and more recent concentrations of computer science, such as machine learning, artificial intelligence and cloud computing.FindingsTo explore further, the debate dataset had more methodologies applied and was split into training and testing sets, whose results were then compared by a standardized measure: Root Mean Square Error (RMSE) which is currently standard in the industry. The rationale of the approach is to quantify that debate activities have an immensely positive impact towards both the teaching and learning in technical subjects and needs to be more often and robustly used within higher education.Originality/valueThe rationale of the approach is that classroom debate activities equip students with verbal and social learning styles and an opportunity to engage with content in a way that is more comfortable than working with traditional lecture-and-laboratory style learning.","",""
0,"Yinyihong Liu","Airbnb Pricing Based on Statistical Machine Learning Models",2021,"","","","",138,"2022-07-13 09:24:04","","10.1109/CONF-SPML54095.2021.00042","","",,,,,0,0.00,0,1,1,"Being one of the largest online accommodation booking platforms, Airbnb has many hosts who are seeking for more proper prices to increase their booking rate. To develop a good pricing prediction model, this paper has employed machine learning models including KNN, MLR, LASSO regression, Ridge regression, Random Forest, Gradient Boosting and XGBoost etc. While past studies on Airbnb pricing have applied quantitative pricing, some face the problems that the models are not robust enough and some face the problem of not training the model plentily. To fill this gap, we give careful consideration in exploratory data analysis to make the dataset more reasonable, apply many robust models ranging from regularized regression to ensemble models and use cross validation and random search to tune each parameter in each model. In this way, we not only select XGBoost as the best model for price prediction with R2 score 0.6321, but also uncover the features which have statistical significance with the target price.","",""
0,"J. Figuerêdo, V. T. Sarinho, R. Calumby","Low-Cost Machine Learning for Effective and Efficient Bad Smells Detection",2021,"","","","",139,"2022-07-13 09:24:04","","10.5753/kdmile.2021.17468","","",,,,,0,0.00,0,3,1,"Bad smells are characteristics of software that indicate a code or design problem which can make information system hard to understand, evolve, and maintain. To address this problem, different approaches, manual and automated, have been proposed over the years, including more recently machine learning alternatives. However, despite the advances achieved, some machine learning techniques have not yet been effectively explored, such as the use of feature selection techniques. Moreover, it is not clear to what extent the use of numerous source-code features are necessary for reasonable bad smell detection success. Therefore, in this work we propose an approach using low-cost machine learning for effective and efficient detection of bad smells, through explicit feature selection. Our results showed that the selection allowed to statistically improve the effectiveness of the models. For some cases, the models achieved statistical equivalence, but relying on a highly reduced set of features. Indeed, by using explicit feature selection, simpler models such as Naive Bayes became statistically equivalent to robust models such as Random Forest. Therefore, the selection of features allowed keeping competitive or even superior effectiveness while also improving the efficiency of the models, demanding less computational resources for source-code preprocessing, model training and bad smell detection.","",""
72,"Georgios Damaskinos, El Mahdi El Mhamdi, R. Guerraoui, Rhicheek Patra, Mahsa Taziki","Asynchronous Byzantine Machine Learning (the case of SGD)",2018,"","","","",140,"2022-07-13 09:24:04","","","","",,,,,72,18.00,14,5,4,"Asynchronous distributed machine learning solutions have proven very effective so far, but always assuming perfectly functioning workers. In practice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrupt data, or even malicious attacks. We introduce \emph{Kardam}, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of two complementary components: a filtering and a dampening component. The first is scalar-based and ensures resilience against $\frac{1}{3}$ Byzantine workers. Essentially, this filter leverages the Lipschitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attempt to corrupt the progress of SGD. The dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We prove that Kardam guarantees almost sure convergence in the presence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead with respect to non Byzantine-resilient solutions. We empirically show that Kardam does not introduce additional noise to the learning procedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and empirically show to be less than $f/n$, where $f$ is the number of Byzantine failures tolerated and $n$ the total number of workers. Interestingly, we also empirically observe that the dampening component is interesting in its own right for it enables to build an SGD algorithm that outperforms alternative staleness-aware asynchronous competitors in environments with honest workers.","",""
92,"Martin Rozycki, T. Satterthwaite, N. Koutsouleris, G. Erus, J. Doshi, D. Wolf, Yong Fan, R. Gur, R. Gur, E. Meisenzahl, C. Zhuo, Hong Yin, Hao Yan, W. Yue, Dai Zhang, C. Davatzikos","Multisite Machine Learning Analysis Provides a Robust Structural Imaging Signature of Schizophrenia Detectable Across Diverse Patient Populations and Within Individuals",2018,"","","","",141,"2022-07-13 09:24:04","","10.1093/schbul/sbx137","","",,,,,92,23.00,9,16,4,"Past work on relatively small, single-site studies using regional volumetry, and more recently machine learning methods, has shown that widespread structural brain abnormalities are prominent in schizophrenia. However, to be clinically useful, structural imaging biomarkers must integrate high-dimensional data and provide reproducible results across clinical populations and on an individual person basis. Using advanced multi-variate analysis tools and pooled data from case-control imaging studies conducted at 5 sites (941 adult participants, including 440 patients with schizophrenia), a neuroanatomical signature of patients with schizophrenia was found, and its robustness and reproducibility across sites, populations, and scanners, was established for single-patient classification. Analyses were conducted at multiple scales, including regional volumes, voxelwise measures, and complex distributed patterns. Single-subject classification was tested for single-site, pooled-site, and leave-site-out generalizability. Regional and voxelwise analyses revealed a pattern of widespread reduced regional gray matter volume, particularly in the medial prefrontal, temporolimbic and peri-Sylvian cortex, along with ventricular and pallidum enlargement. Multivariate classification using pooled data achieved a cross-validated prediction accuracy of 76% (AUC = 0.84). Critically, the leave-site-out validation of the detected schizophrenia signature showed accuracy/AUC range of 72-77%/0.73-0.91, suggesting a robust generalizability across sites and patient cohorts. Finally, individualized patient classifications displayed significant correlations with clinical measures of negative, but not positive, symptoms. Taken together, these results emphasize the potential for structural neuroimaging data to provide a robust and reproducible imaging signature of schizophrenia. A web-accessible portal is offered to allow the community to obtain individualized classifications of magnetic resonance imaging scans using the methods described herein.","",""
49,"Péter Horváth, T. Wild, U. Kutay, G. Csucs","Machine Learning Improves the Precision and Robustness of High-Content Screens",2011,"","","","",142,"2022-07-13 09:24:04","","10.1177/1087057111414878","","",,,,,49,4.45,12,4,11,"Imaging-based high-content screens often rely on single cell-based evaluation of phenotypes in large data sets of microscopic images. Traditionally, these screens are analyzed by extracting a few image-related parameters and use their ratios (linear single or multiparametric separation) to classify the cells into various phenotypic classes. In this study, the authors show how machine learning–based classification of individual cells outperforms those classical ratio-based techniques. Using fluorescent intensity and morphological and texture features, they evaluated how the performance of data analysis increases with increasing feature numbers. Their findings are based on a case study involving an siRNA screen monitoring nucleoplasmic and nucleolar accumulation of a fluorescently tagged reporter protein. For the analysis, they developed a complete analysis workflow incorporating image segmentation, feature extraction, cell classification, hit detection, and visualization of the results. For the classification task, the authors have established a new graphical framework, the Advanced Cell Classifier, which provides a very accurate high-content screen analysis with minimal user interaction, offering access to a variety of advanced machine learning methods.","",""
8,"Mustafa Anil Koçak, David Ramirez, E. Erkip, D. Shasha","SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness",2017,"","","","",143,"2022-07-13 09:24:04","","10.1109/TPAMI.2019.2932415","","",,,,,8,1.60,2,4,5,"<italic>SafePredict</italic> is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, <inline-formula><tex-math notation=""LaTeX"">$1-\epsilon$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""kocak-ieq1-2932415.gif""/></alternatives></inline-formula>, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm so that the error rate on non-refused predictions does not exceed <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq2-2932415.gif""/></alternatives></inline-formula>. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq3-2932415.gif""/></alternatives></inline-formula>, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the-art confidence-based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software is included in the supplementary material, which can be found on the Computer Society Digital Library at <uri>http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2932415</uri>.","",""
41,"Matthew Wicker, M. Kwiatkowska","Robustness of 3D Deep Learning in an Adversarial Setting",2019,"","","","",144,"2022-07-13 09:24:04","","10.1109/CVPR.2019.01204","","",,,,,41,13.67,21,2,3,"Understanding the spatial arrangement and nature of real-world objects is of paramount importance to many complex engineering tasks, including autonomous navigation. Deep learning has revolutionized state-of-the-art performance for tasks in 3D environments; however, relatively little is known about the robustness of these approaches in an adversarial setting. The lack of comprehensive analysis makes it difficult to justify deployment of 3D deep learning models in real-world, safety-critical applications. In this work, we develop an algorithm for analysis of pointwise robustness of neural networks that operate on 3D data. We show that current approaches presented for understanding the resilience of state-of-the-art models vastly overestimate their robustness. We then use our algorithm to evaluate an array of state-of-the-art models in order to demonstrate their vulnerability to occlusion attacks. We show that, in the worst case, these networks can be reduced to 0% classification accuracy after the occlusion of at most 6.5% of the occupied input space.","",""
17,"Weina Zhang, Han Liu, V. Silenzio, Peiyuan Qiu, W. Gong","Machine Learning Models for the Prediction of Postpartum Depression: Application and Comparison Based on a Cohort Study",2020,"","","","",145,"2022-07-13 09:24:04","","10.2196/15516","","",,,,,17,8.50,3,5,2,"Background Postpartum depression (PPD) is a serious public health problem. Building a predictive model for PPD using data during pregnancy can facilitate earlier identification and intervention. Objective The aims of this study are to compare the effects of four different machine learning models using data during pregnancy to predict PPD and explore which factors in the model are the most important for PPD prediction. Methods Information on the pregnancy period from a cohort of 508 women, including demographics, social environmental factors, and mental health, was used as predictors in the models. The Edinburgh Postnatal Depression Scale score within 42 days after delivery was used as the outcome indicator. Using two feature selection methods (expert consultation and random forest-based filter feature selection [FFS-RF]) and two algorithms (support vector machine [SVM] and random forest [RF]), we developed four different machine learning PPD prediction models and compared their prediction effects. Results There was no significant difference in the effectiveness of the two feature selection methods in terms of model prediction performance, but 10 fewer factors were selected with the FFS-RF than with the expert consultation method. The model based on SVM and FFS-RF had the best prediction effects (sensitivity=0.69, area under the curve=0.78). In the feature importance ranking output by the RF algorithm, psychological elasticity, depression during the third trimester, and income level were the most important predictors. Conclusions In contrast to the expert consultation method, FFS-RF was important in dimension reduction. When the sample size is small, the SVM algorithm is suitable for predicting PPD. In the prevention of PPD, more attention should be paid to the psychological resilience of mothers.","",""
21,"El-Mahdi El-Mhamdi, R. Guerraoui, Arsany Guirguis, L. Hoang, Sébastien Rouault","Genuinely Distributed Byzantine Machine Learning",2019,"","","","",146,"2022-07-13 09:24:04","","10.1145/3382734.3405695","","",,,,,21,7.00,4,5,3,"Machine Learning (ML) solutions are nowadays distributed, according to the so-called server/worker architecture. One server holds the model parameters while several workers train the model. Clearly, such architecture is prone to various types of component failures, which can be all encompassed within the spectrum of a Byzantine behavior. Several approaches have been proposed recently to tolerate Byzantine workers. Yet all require trusting a central parameter server. We initiate in this paper the study of the ""general"" Byzantine-resilient distributed machine learning problem where no individual component is trusted. In particular, we distribute the parameter server computation on several nodes. We show that this problem can be solved in an asynchronous system, despite the presence of ⅓ Byzantine parameter servers and ⅓ Byzantine workers (which is optimal). We present a new algorithm, ByzSGD, which solves the general Byzantine-resilient distributed machine learning problem by relying on three major schemes. The first, Scatter/Gather, is a communication scheme whose goal is to bound the maximum drift among models on correct servers. The second, Distributed Median Contraction (DMC), leverages the geometric properties of the median in high dimensional spaces to bring parameters within the correct servers back close to each other, ensuring learning convergence. The third, Minimum-Diameter Averaging (MDA), is a statistically-robust gradient aggregation rule whose goal is to tolerate Byzantine workers. MDA requires loose bound on the variance of non-Byzantine gradient estimates, compared to existing alternatives (e.g., Krum [12]). Interestingly, ByzSGD ensures Byzantine resilience without adding communication rounds (on a normal path), compared to vanilla non-Byzantine alternatives. ByzSGD requires, however, a larger number of messages which, we show, can be reduced if we assume synchrony. We implemented ByzSGD on top of TensorFlow, and we report on our evaluation results. In particular, we show that ByzSGD achieves convergence in Byzantine settings with around 32% overhead compared to vanilla TensorFlow. Furthermore, we show that ByzSGD's throughput overhead is 24--176% in the synchronous case and 28--220% in the asynchronous case.","",""
0,"Ransalu Senanayake, Daniel J. Fremont, Mykel J. Kochenderfer, A. Lomuscio, D. Margineantu, Cheng Soon Ong","Guest Editorial: Special issue on robust machine learning",2021,"","","","",147,"2022-07-13 09:24:04","","10.1007/s10994-021-06113-4","","",,,,,0,0.00,0,6,1,"","",""
40,"J. Kosaian, K. V. Rashmi, S. Venkataraman","Learning a Code: Machine Learning for Approximate Non-Linear Coded Computation",2018,"","","","",148,"2022-07-13 09:24:04","","","","",,,,,40,10.00,13,3,4,"Machine learning algorithms are typically run on large scale, distributed compute infrastructure that routinely face a number of unavailabilities such as failures and temporary slowdowns. Adding redundant computations using coding-theoretic tools called ""codes"" is an emerging technique to alleviate the adverse effects of such unavailabilities. A code consists of an encoding function that proactively introduces redundant computation and a decoding function that reconstructs unavailable outputs using the available ones. Past work focuses on using codes to provide resilience for linear computations and specific iterative optimization algorithms. However, computations performed for a variety of applications including inference on state-of-the-art machine learning algorithms, such as neural networks, typically fall outside this realm. In this paper, we propose taking a learning-based approach to designing codes that can handle non-linear computations. We present carefully designed neural network architectures and a training methodology for learning encoding and decoding functions that produce approximate reconstructions of unavailable computation results. We present extensive experimental results demonstrating the effectiveness of the proposed approach: we show that the our learned codes can accurately reconstruct $64 - 98\%$ of the unavailable predictions from neural-network based image classifiers on the MNIST, Fashion-MNIST, and CIFAR-10 datasets. To the best of our knowledge, this work proposes the first learning-based approach for designing codes, and also presents the first coding-theoretic solution that can provide resilience for any non-linear (differentiable) computation. Our results show that learning can be an effective technique for designing codes, and that learned codes are a highly promising approach for bringing the benefits of coding to non-linear computations.","",""
242,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",149,"2022-07-13 09:24:04","","10.1145/3308809.3308857","","",,,,,242,48.40,81,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server andm working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of them working machines suffer Byzantine faults - a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.","",""
79,"Taesik Na, J. Ko, S. Mukhopadhyay","Cascade Adversarial Machine Learning Regularized with a Unified Embedding",2017,"","","","",150,"2022-07-13 09:24:04","","","","",,,,,79,15.80,26,3,5,"Injecting adversarial examples during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we first show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. We also propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.","",""
52,"Sahrish Khan Tayyaba, Hasan Ali Khattak, Ahmad S. Almogren, M. A. Shah, Ikram Ud Din, Ibrahim Alkhalifa, M. Guizani","5G Vehicular Network Resource Management for Improving Radio Access Through Machine Learning",2020,"","","","",151,"2022-07-13 09:24:04","","10.1109/ACCESS.2020.2964697","","",,,,,52,26.00,7,7,2,"The current cellular technology and vehicular networks cannot satisfy the mighty strides of vehicular network demands. Resource management has become a complex and challenging objective to gain expected outcomes in a vehicular environment. The 5G cellular network promises to provide ultra-high-speed, reduced delay, and reliable communications. The development of new technologies such as the network function virtualization (NFV) and software defined networking (SDN) are critical enabling technologies leveraging 5G. The SDN-based 5G network can provide an excellent platform for autonomous vehicles because SDN offers open programmability and flexibility for new services incorporation. This separation of control and data planes enables centralized and efficient management of resources in a very optimized and secure manner by having a global overview of the whole network. The SDN also provides flexibility in communication administration and resource management, which are of critical importance when considering the ad-hoc nature of vehicular network infrastructures, in terms of safety, privacy, and security, in vehicular network environments. In addition, it promises the overall improved performance. In this paper, we propose a flow-based policy framework on the basis of two tiers virtualization for vehicular networks using SDNs. The vehicle to vehicle (V2V) communication is quite possible with wireless virtualization where different radio resources are allocated to V2V communications based on the flow classification, i.e., safety-related flow or non-safety flows, and the controller is responsible for managing the overall vehicular environment and V2X communications. The motivation behind this study is to implement a machine learning-enabled architecture to cater the sophisticated demands of modern vehicular Internet infrastructures. The inclination towards robust communications in 5G-enabled networks has made it somewhat tricky to manage network slicing efficiently. This paper also presents a proof of concept for leveraging machine learning-enabled resource classification and management through experimental evaluation of special-purpose testbed established in custom mininet setup. Furthermore, the results have been evaluated using Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Deep Neural Network (DNN). While concluding the paper, it is shown that the LSTM has outperformed the rest of classification techniques with promising results.","",""
90,"Nagdev Amruthnath, Tarun Gupta","A research study on unsupervised machine learning algorithms for early fault detection in predictive maintenance",2018,"","","","",152,"2022-07-13 09:24:04","","10.1109/IEA.2018.8387124","","",,,,,90,22.50,45,2,4,"The area of predictive maintenance has taken a lot of prominence in the last couple of years due to various reasons. With new algorithms and methodologies growing across different learning methods, it has remained a challenge for industries to adopt which method is fit, robust and provide most accurate detection. Fault detection is one of the critical components of predictive maintenance; it is very much needed for industries to detect faults early and accurately. In a production environment, to minimize the cost of maintenance, sometimes it is required to build a model with minimal or no historical data. In such cases, unsupervised learning would be a better option model building. In this paper, we have chosen a simple vibration data collected from an exhaust fan, and have fit different unsupervised learning algorithms such as PCA T2 statistic, Hierarchical clustering, K-Means, Fuzzy C-Means clustering and model-based clustering to test its accuracy, performance, and robustness. In the end, we have proposed a methodology to benchmark different algorithms and choosing the final model.","",""
0,"S. Pundir, M. Obaidat, M. Wazid, A. Das, D. P. Singh, J. Rodrigues","MADP-IIME: malware attack detection protocol in IoT-enabled industrial multimedia environment using machine learning approach",2021,"","","","",153,"2022-07-13 09:24:04","","10.1007/S00530-020-00743-9","","",,,,,0,0.00,0,6,1,"","",""
1,"Zhen Guo, J. Song, G. Barbastathis, M. Glinsky, C. Vaughan, K. Larson, B. Alpert, Z. Levine","Advantage of Machine Learning over Maximum Likelihood in Limited-Angle Low-Photon X-Ray Tomography",2021,"","","","",154,"2022-07-13 09:24:04","","","","",,,,,1,1.00,0,8,1,"Limited-angle X-ray tomography reconstruction is an illconditioned inverse problem in general. Especially when the projection angles are limited and the measurements are taken in a photon-limited condition, reconstructions from classical algorithms such as filtered backprojection may lose fidelity and acquire artifacts due to the missing-cone problem. To obtain satisfactory reconstruction results, prior assumptions, such as total variation minimization and nonlocal image similarity, are usually incorporated within the reconstruction algorithm. In this work, we introduce deep neural networks to determine and apply a prior distribution in the reconstruction process. Our neural networks learn the prior directly from synthetic training samples. The neural nets thus obtain a prior distribution that is specific to the class of objects we are interested in reconstructing. In particular, we used deep generative models with 3D convolutional layers and 3D attention layers which are trained on 3D synthetic integrated circuit (IC) data from a model dubbed CircuitFaker. We demonstrate that, when the projection angles and photon budgets are limited, the priors from our deep generative models can dramatically improve the IC reconstruction quality on synthetic data compared with maximum likelihood estimation. Training the deep generative models with synthetic IC data from CircuitFaker illustrates the capabilities of the learned prior from machine learning. We expect that if the process were reproduced with experimental data, the advantage of the machine learning would persist. The advantages of machine learning in limited angle X-ray tomography may further enable applications in low-photon nanoscale imaging. Background Limited-angle X-ray tomography has the ability to image the interior of three-dimensional (3D) objects non-invasively without collecting the measurements from a full set of rotation angles. It has drawn wide attention in practical nanoscale imaging due to its advantage in having a relatively short data acquisition time. Also, in some cases, not all angles are physically accessible. Typically, the tomographic imaging system consists of a sample holder, an objective zone plate, and a CCD camera, and the illumination is generated from the X-ray source. The measurements are taken from a series of rotational angles with respect to the sample of interest, where a cone-beam geometry is generally assumed to produce the ray projection from the source point to the sample, and then from sample to the center of each detector pixel. After the measurements, objects subsequently can be reconstructed based on 3D computed tomography algorithms. Theoretically, full-angle measurement is preferred to avoid the missing cone problem in the reconstruction process. In practice, however, limited-angle measurement is often used due to the time of acquiring the full angle measurement. For objects or samples that are radiation sensitive, a low-photon budget per scan is also preferred to minimize the total exposure and potential damage. Limited-angle X-ray tomography is an ill-conditioned inverse problem [1, 2], where the goal is to find a discrete representation of an object f based on the observations g taken on a digital camera at limited number of angles. When limited-angle tomography is applied where the illumination is limited to only a few photons, the noise sensitivity of poor conditioning becomes even more evident: not only do the collected Fourier slices cover the Fourier space unevenly due to the limited scans, they are also inaccurate due to the presence of shot noise in the measurements. Limited angle X-ray tomography therefore has had limited utility for radiation sensitive samples. To solve the limited angle X-ray tomography with photon scarcity, regularization is required during the reconstruction process. Some improvement can be obtained by extrapolating missing data [3]. Data consistency conditions, e.g., the Helgason-Ludwig consistency conditions, further improves the quality of extrapolation [4]. Still, extrapolating methods are struggling with complex structures and are not robust to noise in the experimental measurements. Iterative algorithms with constraints known a priori reconstruct the object using multiple steps. The algorithms start with an assumed object, simulate the measurement from the assumed object, compare the experimental measurements and simulated measurements, and then update the object based on the difference between measurements and simulations. The last step also includes discrepancy in the prior terms into the computation of the update. The process continues until a certain convergence criterion is achieved. Iterative algorithms with prior constraints such as total variation minimization and nonlocal image similarity often exhibit improved resilience to noise. Previously, we also have shown a Bayesian approach [5] based on the Bouman-Sauer formulation for the iterative reconstruction algorithm [6]. The regularization constrains the iterative optimization to a subdomain in which the object belongs, thereby effectively improving the reconstruction quality [7, 8]. Recently, machine learning has been applied successfully to ar X iv :2 11 1. 08 01 1v 2 [ ee ss .I V ] 1 8 D ec 2 02 1 limited-angle tomography to overcome the challenge in solving the inverse problem. Efforts have been made in using learned priors to provide information of missing data during reconstruction [9, 10, 11]. In particular, deep learning, a subset of machine learning that is based on artificial neural networks with representation learning, achieved promising results [11, 12, 13]. For tomographic inverse problems, U-net-like neural network architectures [14] have been widely used to produce reconstruction pixel-by-pixel. For example, U-net has been used to predict invisible singularities in the image object [12], to generate missing projections with a data-consistent reconstruction method [13], and to improve the reconstruction quality from the conventional FBP method [11]. Unlike the general priors in iterative algorithms, the learned prior from a deep learning method is conditioned on a large amount of paired training data. Therefore, the learned prior explores the statistical property of the training distribution. In this work, we extend the application of limited-angle Xray tomography to experimental conditions of low-photon incidence. To approximate the resulting ill-conditioned inverse problem and to obtain a satisfactory reconstruction quality, we apply machine learning to determine a prior distribution for the reconstruction process. In particular, we use deep generative models with 3D convolution and 3D attention which are trained on 3D synthetic integrated circuit (IC) data from a model dubbed CircuitFaker. We demonstrate that the priors from our deep generative models drastically improve the IC reconstruction quality on synthetic data from maximum likelihood estimation when the projection angles and photon budgets are limited. Beyond existing research that uses machine learning for limited angle X-ray tomography, our generative model exhibits improvements over maximum likelihood reconstructions under low-photon conditions. We further examine different neural network architectures to reveal some of the important network design choices for solving the inverse problem. Method The overall pipeline of our method is organized as follows: First, we generate synthetic integrated circuit (IC) layouts using CircuitFaker, an in-house model. Then, we simulate the limitedangle X-ray tomography radiographs. Next, we apply the maximum likelihood method to generate an initial IC reconstruction. Finally, we feed the initial IC reconstruction to the (trained) deep generative model and compare the reconstruction quality of the test set by calculating the bit error rate. CircuitFaker CircuitFaker is an algorithm that generates a random set of voxels with binary values which resembles an integrated circuit interconnect. The synthetic circuits from CircuitFaker is the class of artificial objects for tomographic reconstruction, and the implicit correlations in their spatial features are the priors to be assumed or to be learned for the inverse algorithms. A particular draw of CircuitFaker assigns a bit in each of N = NxNyNz locations. These locations are indexed as i` = 1, ...,N`, with ` = 1, 2, 3 for x, y, and z. All bits are initialized to 0. In the first round, there are wire seed points for all locations (i1, i2, i3) with i1, ..., i3 odd. Each seed point is set by a Bernoulli draw with probability pw of getting a 1. There are three kinds of layers, x, y, and via Figure 1. Selected 16× 16× 8 circuit from CircuitFaker. Each image is a slice of 2D layer in the z dimension. The value of z increases as a raster scan of the 8 slices shown. Black indicates copper and white indicates silicon. Here, x layers are the first and fifth layers in z, y layers are the third and seventh layers in z. Others are via layers. layers. The x wiring layers have index i3 = 1 mod 4. The y wiring layers have i3 = 3 mod 4. The via layers are the others, i.e., i3 even. In the second round, a point on an x wiring layer to the immediate right of a point with value 1 is set to 1 with probability px. A point on a y wiring layer immediately above in plan view a point with a value 1 is set to 1 with probability py. Similarly, a point on a via layer immediately above a point with a value 1 is set to 1 with probability pz. In this paper, we chose these parameters: Nx = Ny = 16, Nz = 8, pw = 0.75, px = py = 0.8, and pz = 0.5. Fig. 1 shows one of the generated circuits with size 16×16×8. Imaging geometry for X-ray tomography We assumed that each voxel in the circuit is in size of size 0.15 μm × 0.15 μm × 0.30 μm. Therefore, the total volume of the circuit is 2.4 μm× 2.4 μm× 2.4 μm. The detector is assumed to be in the x-z plane at a tilt angle of φ = 0◦. The rotation axis is ","",""
0,"T. Martin, S. Areibi, G. Grewal","Effective Machine-Learning Models for Predicting Routability During FPGA Placement",2021,"","","","",155,"2022-07-13 09:24:04","","10.1109/MLCAD52597.2021.9531243","","",,,,,0,0.00,0,3,1,"The ability to efficiently and accurately predict placement routability, while avoiding the large computational cost of performing routing, is an asset when seeking to reduce total placement and routing runtime. In this paper, we present a series of simple ML models and ensembles to predict the routability of a placement solution. Ensembles based on Bagging, Boosting and Stack of classifiers are introduced to produce more accurate and robust solutions than single/simple models. Our results show an improvement in prediction accuracy and runtime compared to the best published results in the literature.","",""
178,"J. Otterbach, R. Manenti, N. Alidoust, A. Bestwick, M. Block, B. Bloom, S. Caldwell, N. Didier, E. Fried, S. Hong, Peter J. Karalekas, C. Osborn, A. Papageorge, E. C. Peterson, G. Prawiroatmodjo, N. Rubin, C. Ryan, D. Scarabelli, M. Scheer, E. A. Sete, P. Sivarajah, Robert S. Smith, A. Staley, N. Tezak, W. Zeng, A. Hudson, Blake R. Johnson, M. Reagor, M. Silva, C. Rigetti","Unsupervised Machine Learning on a Hybrid Quantum Computer",2017,"","","","",156,"2022-07-13 09:24:04","","","","",,,,,178,35.60,18,30,5,"Machine learning techniques have led to broad adoption of a statistical model of computing. The statistical distributions natively available on quantum processors are a superset of those available classically. Harnessing this attribute has the potential to accelerate or otherwise improve machine learning relative to purely classical performance. A key challenge toward that goal is learning to hybridize classical computing resources and traditional learning techniques with the emerging capabilities of general purpose quantum processors. Here, we demonstrate such hybridization by training a 19-qubit gate model processor to solve a clustering problem, a foundational challenge in unsupervised learning. We use the quantum approximate optimization algorithm in conjunction with a gradient-free Bayesian optimization to train the quantum machine. This quantum/classical hybrid algorithm shows robustness to realistic noise, and we find evidence that classical optimization can be used to train around both coherent and incoherent imperfections.","",""
136,"A. Bhagoji, Daniel Cullina, Prateek Mittal","Dimensionality Reduction as a Defense against Evasion Attacks on Machine Learning Classifiers",2017,"","","","",157,"2022-07-13 09:24:04","","","","",,,,,136,27.20,45,3,5,"We propose the use of dimensionality reduction as a defense against evasion attacks on ML classifiers. We present and investigate a strategy for incorporating dimensionality reduction via Principal Component Analysis to enhance the resilience of machine learning, targeting both the classification and the training phase. We empirically evaluate and demonstrate the feasibility of dimensionality reduction of data as a defense mechanism against evasion attacks using multiple real-world datasets. Our key findings are that the defenses are (i) effective against strategic evasion attacks in the literature, increasing the resources required by an adversary for a successful attack by a factor of about two, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification, and human activity classification.","",""
297,"Andrius Vabalas, E. Gowen, E. Poliakoff, A. Casson","Machine learning algorithm validation with a limited sample size",2019,"","","","",158,"2022-07-13 09:24:04","","10.1371/journal.pone.0224365","","",,,,,297,99.00,74,4,3,"Advances in neuroimaging, genomic, motion tracking, eye-tracking and many other technology-based data collection methods have led to a torrent of high dimensional datasets, which commonly have a small number of samples because of the intrinsic high cost of data collection involving human participants. High dimensional data with a small number of samples is of critical importance for identifying biomarkers and conducting feasibility and pilot work, however it can lead to biased machine learning (ML) performance estimates. Our review of studies which have applied ML to predict autistic from non-autistic individuals showed that small sample size is associated with higher reported classification accuracy. Thus, we have investigated whether this bias could be caused by the use of validation methods which do not sufficiently control overfitting. Our simulations show that K-fold Cross-Validation (CV) produces strongly biased performance estimates with small sample sizes, and the bias is still evident with sample size of 1000. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. We also show that feature selection if performed on pooled training and testing data is contributing to bias considerably more than parameter tuning. In addition, the contribution to bias by data dimensionality, hyper-parameter space and number of CV folds was explored, and validation methods were compared with discriminable data. The results suggest how to design robust testing methodologies when working with small datasets and how to interpret the results of other studies based on what validation method was used.","",""
87,"Kexin Pei, Yinzhi Cao, Junfeng Yang, S. Jana","Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems",2017,"","","","",159,"2022-07-13 09:24:04","","","","",,,,,87,17.40,22,4,5,"Due to the increasing usage of machine learning (ML) techniques in security- and safety-critical domains, such as autonomous systems and medical diagnosis, ensuring correct behavior of ML systems, especially for different corner cases, is of growing importance. In this paper, we propose a generic framework for evaluating security and robustness of ML systems using different real-world safety properties. We further design, implement and evaluate VeriVis, a scalable methodology that can verify a diverse set of safety properties for state-of-the-art computer vision systems with only blackbox access. VeriVis leverage different input space reduction techniques for efficient verification of different safety properties. VeriVis is able to find thousands of safety violations in fifteen state-of-the-art computer vision systems including ten Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving system with thousands of neurons as well as five commercial third-party vision APIs including Google vision and Clarifai for twelve different safety properties. Furthermore, VeriVis can successfully verify local safety properties, on average, for around 31.7% of the test images. VeriVis finds up to 64.8x more violations than existing gradient-based methods that, unlike VeriVis, cannot ensure non-existence of any violations. Finally, we show that retraining using the safety violations detected by VeriVis can reduce the average number of violations up to 60.2%.","",""
7,"Md. Kowsher, A. Tahabilder, S. Murad","Impact-Learning: A Robust Machine Learning Algorithm",2020,"","","","",160,"2022-07-13 09:24:04","","10.1145/3411174.3411185","","",,,,,7,3.50,2,3,2,"The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth.","",""
6,"F. Farokhi","Regularization Helps with Mitigating Poisoning Attacks: Distributionally-Robust Machine Learning Using the Wasserstein Distance",2020,"","","","",161,"2022-07-13 09:24:04","","","","",,,,,6,3.00,6,1,2,"We use distributionally-robust optimization for machine learning to mitigate the effect of data poisoning attacks. We provide performance guarantees for the trained model on the original data (not including the poison records) by training the model for the worst-case distribution on a neighbourhood around the empirical distribution (extracted from the training dataset corrupted by a poisoning attack) defined using the Wasserstein distance. We relax the distributionally-robust machine learning problem by finding an upper bound for the worst-case fitness based on the empirical sampled-averaged fitness and the Lipschitz-constant of the fitness function (on the data for given model parameters) as regularizer. For regression models, we prove that this regularizer is equal to the dual norm of the model parameters. We use the Wine Quality dataset, the Boston Housing Market dataset, and the Adult dataset for demonstrating the results of this paper.","",""
38,"Jinyuan Jia, Xiaoyu Cao, N. Gong","Intrinsic Certified Robustness of Bagging against Data Poisoning Attacks",2020,"","","","",162,"2022-07-13 09:24:04","","","","",,,,,38,19.00,13,3,2,"In a \emph{data poisoning attack}, an attacker modifies, deletes, and/or inserts some training examples to corrupt the learnt machine learning model. \emph{Bootstrap Aggregating (bagging)} is a well-known ensemble learning method, which trains multiple base models on random subsamples of a training dataset using a base learning algorithm and uses majority vote to predict labels of testing examples. We prove the intrinsic certified robustness of bagging against data poisoning attacks. Specifically, we show that bagging with an arbitrary base learning algorithm provably predicts the same label for a testing example when the number of modified, deleted, and/or inserted training examples is bounded by a threshold. Moreover, we show that our derived threshold is tight if no assumptions on the base learning algorithm are made. We evaluate our method on MNIST and CIFAR10. For instance, our method achieves a certified accuracy of $91.1\%$ on MNIST when arbitrarily modifying, deleting, and/or inserting 100 training examples.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",163,"2022-07-13 09:24:04","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
5,"A. Raz, Paul C. Wood, L. Mockus, D. DeLaurentis","System of systems uncertainty quantification using machine learning techniques with smart grid application",2020,"","","","",164,"2022-07-13 09:24:04","","10.1002/sys.21561","","",,,,,5,2.50,1,4,2,"System‐of‐Systems capability is inherently tied to the participation and performance of the constituent systems and the network performance which connects the systems together. It is imperative for the SoS stakeholders to quantify the SoS capability and performance to any uncertain variations in the system participation and network outages so that the system participation is incentivized and network design optimized. However, given the independent operations, management, and objectives of constituent systems, along with an increasing number of systems that collectively become a part of SoS, it becomes difficult to obtain a closed analytical function for SoS performance characterization. In this paper, we investigate and compare two machine learning techniques, Artificial Neural Network and Parametric Bayesian Estimation, to obtain a predictive model of the SoS given the uncertainty in the constituent system participation and the network conditions. We demonstrate our approach on a smart grid SoS application example and describe how the two machine learning techniques enable SoS robustness and resilience analysis by quantifying the uncertainty in the model and SoS operations. The results of smart grid example establish the value of SoS uncertainty quantification (UQ) and show how smart grid operators can utilize UQ models to maintain the desired robustness as operating conditions evolve and how the designers can incorporate low‐cost networks into the SoS while maintaining high performance and resilience.","",""
81,"P. Komiske, E. Metodiev, B. Nachman, M. Schwartz","Pileup Mitigation with Machine Learning (PUMML)",2017,"","","","",165,"2022-07-13 09:24:04","","10.1007/JHEP12(2017)051","","",,,,,81,16.20,20,4,5,"","",""
71,"T. Cohen, M. Freytsis, B. Ostdiek","(Machine) learning to do more with less",2017,"","","","",166,"2022-07-13 09:24:04","","10.1007/JHEP02(2018)034","","",,,,,71,14.20,24,3,5,"","",""
208,"J. Blanchet, Yang Kang, M. KarthyekRajhaaA.","Robust Wasserstein profile inference and applications to machine learning",2016,"","","","",167,"2022-07-13 09:24:04","","10.1017/jpr.2019.49","","",,,,,208,34.67,69,3,6,"We show that several machine learning estimators, including square-root least absolute shrinkage and selection and regularized logistic regression, can be represented as solutions to distributionally robust optimization problems. The associated uncertainty regions are based on suitably defined Wasserstein distances. Hence, our representations allow us to view regularization as a result of introducing an artificial adversary that perturbs the empirical distribution to account for out-of-sample effects in loss estimation. In addition, we introduce RWPI (robust Wasserstein profile inference), a novel inference methodology which extends the use of methods inspired by empirical likelihood to the setting of optimal transport costs (of which Wasserstein distances are a particular case). We use RWPI to show how to optimally select the size of uncertainty regions, and as a consequence we are able to choose regularization parameters for these machine learning estimators without the use of cross validation. Numerical experiments are also given to validate our theoretical findings.","",""
4721,"Nicholas Carlini, D. Wagner","Towards Evaluating the Robustness of Neural Networks",2016,"","","","",168,"2022-07-13 09:24:04","","10.1109/SP.2017.49","","",,,,,4721,786.83,2361,2,6,"Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95% to 0.5%.In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.","",""
1,"Muhammad Abdullah Hanif, M. Shafique","Dependable Deep Learning: Towards Cost-Efficient Resilience of Deep Neural Network Accelerators against Soft Errors and Permanent Faults",2020,"","","","",169,"2022-07-13 09:24:04","","10.1109/IOLTS50870.2020.9159734","","",,,,,1,0.50,1,2,2,"Deep Learning has enabled machines to learn computational models (i.e., Deep Neural Networks – DNNs) that can perform certain complex tasks with claims to be close to human-level precision. This state-of-the-art performance offered by DNNs in many Artificial Intelligence (AI) applications has paved their way to being used in several safety-critical applications where even a single failure can lead to catastrophic results. Therefore, improving the robustness of these models to hardware-induced faults (such as soft errors, aging, and manufacturing defects) is of significant importance to avoid any disastrous event. Traditional redundancy-based fault mitigation techniques cannot be employed in a wide of applications due to their high overheads, which, when coupled with the compute-intensive nature of DNNs, lead to undesirable resource consumption. In this article, we present an overview of different low-cost fault-mitigation techniques that exploit the intrinsic characteristics of DNNs to limit their overheads. We discuss how each technique can contribute to the overall resilience of a DNN-based system, and how they can be integrated together to offer resilience against multiple diverse hardware-induced reliability threats. Towards the end, we highlight several key future directions that are envisioned to help in achieving highly dependable DL-based systems.","",""
103,"F. Granata, S. Papirio, G. Esposito, R. Gargano, G. D. Marinis","Machine Learning Algorithms for the Forecasting of Wastewater Quality Indicators",2017,"","","","",170,"2022-07-13 09:24:04","","10.3390/W9020105","","",,,,,103,20.60,21,5,5,"Stormwater runoff is often contaminated by human activities. Stormwater discharge into water bodies significantly contributes to environmental pollution. The choice of suitable treatment technologies is dependent on the pollutant concentrations. Wastewater quality indicators such as biochemical oxygen demand (BOD5), chemical oxygen demand (COD), total suspended solids (TSS), and total dissolved solids (TDS) give a measure of the main pollutants. The aim of this study is to provide an indirect methodology for the estimation of the main wastewater quality indicators, based on some characteristics of the drainage basin. The catchment is seen as a black box: the physical processes of accumulation, washing, and transport of pollutants are not mathematically described. Two models deriving from studies on artificial intelligence have been used in this research: Support Vector Regression (SVR) and Regression Trees (RT). Both the models showed robustness, reliability, and high generalization capability. However, with reference to coefficient of determination R2 and root‐mean square error, Support Vector Regression showed a better performance than Regression Tree in predicting TSS, TDS, and COD. As regards BOD5, the two models showed a comparable performance. Therefore, the considered machine learning algorithms may be useful for providing an estimation of the values to be considered for the sizing of the treatment units in absence of direct measures.","",""
9,"Wenjie Che, M. Martínez-Ramón, F. Saqib, J. Plusquellic","Delay model and machine learning exploration of a hardware-embedded delay PUF",2018,"","","","",171,"2022-07-13 09:24:04","","10.1109/HST.2018.8383905","","",,,,,9,2.25,2,4,4,"A special class of Physically Unclonable Functions (PUF) called strong PUFs are characterized as having an exponentially large challenge-response pair (CRP) space. However, model-building attacks with machine learning algorithms have shown that the CRP space of most strong PUFs can be predicted using a relatively small subset of training samples. In this paper, we investigate the delay model of the Hardware-Embedded deLay PUF (HELP) and apply machine learning algorithms to determine its resilience to model-building attacks. The delay model for HELP possesses significant differences when compared with other delay-based PUFs such as the Arbiter PUF, particularly with respect to the composition of the paths which are tested to generate response bits. We show that the complexity of the delay model in combination with a set of delay post processing operations carried out within the HELP algorithm significantly reduce the effectiveness of model-building attacks.","",""
9,"Georgios Damaskinos, El Mahdi El Mhamdi, R. Guerraoui, Rhicheek Patra, Mahsa Taziki","Asynchronous Byzantine Machine Learning",2018,"","","","",172,"2022-07-13 09:24:04","","","","",,,,,9,2.25,2,5,4,"Asynchronous distributed machine learning solutions have proven very effective so far, but always assuming perfectly functioning workers. In practice, some of the workers can however exhibit Byzantine behavior, caused by hardware failures, software bugs, corrupt data, or even malicious attacks. We introduce \emph{Kardam}, the first distributed asynchronous stochastic gradient descent (SGD) algorithm that copes with Byzantine workers. Kardam consists of two complementary components: a filtering and a dampening component. The first is scalar-based and ensures resilience against $\frac{1}{3}$ Byzantine workers. Essentially, this filter leverages the Lipschitzness of cost functions and acts as a self-stabilizer against Byzantine workers that would attempt to corrupt the progress of SGD. The dampening component bounds the convergence rate by adjusting to stale information through a generic gradient weighting scheme. We prove that Kardam guarantees almost sure convergence in the presence of asynchrony and Byzantine behavior, and we derive its convergence rate. We evaluate Kardam on the CIFAR-100 and EMNIST datasets and measure its overhead with respect to non Byzantine-resilient solutions. We empirically show that Kardam does not introduce additional noise to the learning procedure but does induce a slowdown (the cost of Byzantine resilience) that we both theoretically and empirically show to be less than $f/n$, where $f$ is the number of Byzantine failures tolerated and $n$ the total number of workers. Interestingly, we also empirically observe that the dampening component is interesting in its own right for it enables to build an SGD algorithm that outperforms alternative staleness-aware asynchronous competitors in environments with honest workers.","",""
15,"Ezekiel O. Soremekun, Sakshi Udeshi, Sudipta Chattopadhyay, A. Zeller","Exposing Backdoors in Robust Machine Learning Models",2020,"","","","",173,"2022-07-13 09:24:04","","","","",,,,,15,7.50,4,4,2,"The introduction of robust optimisation has pushed the state-of-the-art in defending against adversarial attacks. However, the behaviour of such optimisation has not been studied in the light of a fundamentally different class of attacks called backdoors. In this paper, we demonstrate that adversarially robust models are susceptible to backdoor attacks. Subsequently, we observe that backdoors are reflected in the feature representation of such models. Then, this observation is leveraged to detect backdoor-infected models via a detection technique called AEGIS. Specifically, AEGIS uses feature clustering to effectively detect backdoor-infected robust Deep Neural Networks (DNNs).  In our evaluation of major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS effectively detects robust DNNs infected with backdoors. Overall, AEGIS has 97% (70/72) detection accuracy and 0.3% (2/648) false positive rate, for all configurations. Our investigation reveals that salient features of adversarially robust DNNs break the stealthy nature of backdoor attacks.","",""
165,"W. Huggins, Piyush Patel, K. B. Whaley, E. M. Stoudenmire","Towards Quantum Machine Learning with Tensor Networks",2018,"","","","",174,"2022-07-13 09:24:04","","10.1088/2058-9565/aaea94","","",,,,,165,41.25,41,4,4,"Author(s): Huggins, W; Patil, P; Mitchell, B; Birgitta Whaley, K; Miles Stoudenmire, E | Abstract: © 2019 IOP Publishing Ltd. Machine learning is a promising application of quantum computing, but challenges remain for implementation today because near-term devices have a limited number of physical qubits and high error rates. Motivated by the usefulness of tensor networks for machine learning in the classical context, we propose quantum computing approaches to both discriminative and generative learning, with circuits based on tree and matrix product state tensor networks, that could already have benefits with such near-term devices. The result is a unified framework in which classical and quantum computing can benefit from the same theoretical and algorithmic developments, and the same model can be trained classically then transferred to the quantum setting for additional optimization. Tensor network circuits can also provide qubit-efficient schemes in which, depending on the architecture, the number of physical qubits required scales only logarithmically with, or independently of the input or output data sizes. We demonstrate our proposals with numerical experiments, training a discriminative model to perform handwriting recognition using a hybrid quantum-classical optimization procedure that could be carried out on quantum hardware today, and testing the noise resilience of the trained model.","",""
174,"Andrew F. Zahrt, J. Henle, Brennan T Rose, Yang Wang, William T. Darrow, S. Denmark","Prediction of higher-selectivity catalysts by computer-driven workflow and machine learning",2019,"","","","",175,"2022-07-13 09:24:04","","10.1126/science.aau5631","","",,,,,174,58.00,29,6,3,"Predicting catalyst selectivity Asymmetric catalysis is widely used in chemical research and manufacturing to access just one of two possible mirror-image products. Nonetheless, the process of tuning catalyst structure to optimize selectivity is still largely empirical. Zahrt et al. present a framework for more efficient, predictive optimization. As a proof of principle, they focused on a known coupling reaction of imines and thiols catalyzed by chiral phosphoric acid compounds. By modeling multiple conformations of more than 800 prospective catalysts, and then training machine-learning algorithms on a subset of experimental results, they achieved highly accurate predictions of enantioselectivities. Science, this issue p. eaau5631 A model encompassing multiple conformations of chiral phosphoric acid catalysts accurately predicts enantioselectivities. INTRODUCTION The development of new synthetic methods in organic chemistry is traditionally accomplished through empirical optimization. Catalyst design, wherein experimentalists attempt to qualitatively identify correlations between catalyst structure and catalyst efficiency, is no exception. However, this approach is plagued by numerous deficiencies, including the lack of mechanistic understanding of a new transformation, the inherent limitations of human cognitive abilities to find patterns in large collections of data, and the lack of quantitative guidelines to aid catalyst identification. Chemoinformatics provides an attractive alternative to empiricism for several reasons: Mechanistic information is not a prerequisite, catalyst structures can be characterized by three-dimensional (3D) descriptors (numerical representations of molecular properties derived from the 3D molecular structure) that quantify the steric and electronic properties of thousands of candidate molecules, and the suitability of a given catalyst candidate can be quantified by comparing its properties with a computationally derived model trained on experimental data. The ability to accurately predict a selective catalyst by using a set of less than optimal data remains a major goal for machine learning with respect to asymmetric catalysis. We report a method to achieve this goal and propose a more efficient alternative to traditional catalyst design. RATIONALE The workflow we have created consists of the following components: (i) construction of an in silico library comprising a large collection of conceivable, synthetically accessible catalysts derived from a particular scaffold; (ii) calculation of relevant chemical descriptors for each scaffold; (iii) selection of a representative subset of the catalysts [this subset is termed the universal training set (UTS) because it is agnostic to reaction or mechanism and thus can be used to optimize any reaction catalyzed by that scaffold]; (iv) collection of the training data; and (v) application of machine learning methods to generate models that predict the enantioselectivity of each member of the in silico library. These models are evaluated with an external test set of catalysts (predicting selectivities of catalysts outside of the training data). The validated models can then be used to select the optimal catalyst for a given reaction. RESULTS To demonstrate the viability of our method, we predicted reaction outcomes with substrate combinations and catalysts different from the training data and simulated a situation in which highly selective reactions had not been achieved. In the first demonstration, a model was constructed by using support vector machines and validated with three different external test sets. The first test set evaluated the ability of the model to predict the selectivity of only reactions forming new products with catalysts from the training set. The model performed well, with a mean absolute deviation (MAD) of 0.161 kcal/mol. Next, the same model was used to predict the selectivity of an external test set of catalysts with substrate combinations from the training set. The performance of the model was still highly accurate, with a MAD of 0.211 kcal/mol. Lastly, reactions forming new products with the external test catalysts were predicted with a MAD of 0.236 kcal/mol. In the second study, no reactions with selectivity above 80% enantiomeric excess were used as training data. Deep feed-forward neural networks accurately reproduced the experimental selectivity data, successfully predicting the most selective reactions. More notably, the general trends in selectivity, on the basis of average catalyst selectivity, were correctly identified. Despite omitting about half of the experimental free energy range from the training data, we could still make accurate predictions in this region of selectivity space. CONCLUSION The capability to predict selective catalysts has the potential to change the way chemists select and optimize chiral catalysts from an empirically guided to a mathematically guided approach. Chemoinformatics-guided optimization protocol. (A) Generation of a large in silico library of catalyst candidates. (B) Calculation of robust chemical descriptors. (C) Selection of a UTS. (D) Acquisition of experimental selectivity data. (E) Application of machine learning to use moderate- to low-selectivity reactions to predict high-selectivity reactions. R, any group; X, O or S; Y, OH, SH, or NHTf; PC, principal component; ΔΔG, mean selectivity. Catalyst design in asymmetric reaction development has traditionally been driven by empiricism, wherein experimentalists attempt to qualitatively recognize structural patterns to improve selectivity. Machine learning algorithms and chemoinformatics can potentially accelerate this process by recognizing otherwise inscrutable patterns in large datasets. Herein we report a computationally guided workflow for chiral catalyst selection using chemoinformatics at every stage of development. Robust molecular descriptors that are agnostic to the catalyst scaffold allow for selection of a universal training set on the basis of steric and electronic properties. This set can be used to train machine learning methods to make highly accurate predictive models over a broad range of selectivity space. Using support vector machines and deep feed-forward neural networks, we demonstrate accurate predictive modeling in the chiral phosphoric acid–catalyzed thiol addition to N-acylimines.","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",176,"2022-07-13 09:24:04","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
166,"M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, S. Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Integrating machine learning and multiscale modeling—perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences",2019,"","","","",177,"2022-07-13 09:24:04","","10.1038/s41746-019-0193-y","","",,,,,166,55.33,17,11,3,"","",""
99,"Liwei Song, R. Shokri, Prateek Mittal","Privacy Risks of Securing Machine Learning Models against Adversarial Examples",2019,"","","","",178,"2022-07-13 09:24:04","","10.1145/3319535.3354211","","",,,,,99,33.00,33,3,3,"The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.","",""
2,"Ziqiang Shi, Chaoliang Zhong, Yasuto Yokota, Wensheng Xia, Jun Sun","Robustness Evaluation of Deep Learning Models Based on Local Prediction Consistency",2019,"","","","",179,"2022-07-13 09:24:04","","10.1109/ICMLA.2019.00224","","",,,,,2,0.67,0,5,3,"It is important to estimate the performance gap of a given deep learning model on the target data set, since discrepancy or bias between source and target domains is a common and fundamental problem in the practice of machine learning techniques. Without any assumptions on data bias, such as label shift or covariate shift and without target data labels, we propose a robustness estimation method based on prediction consistency evaluation between source and target data in the neighborhood of the source samples. Considering outliers and whether the user provided model is fully trained, a variety of variant methods are also tried, including setting neighborhood threshold to average intra-class distance for each category and relative robustness. Furthermore, the time complexity of this method is O(nlogn), which is applicable for large datasets. Experiments on the handwritten digit recognition and Japanese handwriting recognition show that the proposed methods are effective.","",""
87,"J. Collins, K. Howe, B. Nachman","Extending the search for new resonances with machine learning",2019,"","","","",180,"2022-07-13 09:24:04","","10.1103/physrevd.99.014038","","",,,,,87,29.00,29,3,3,"The oldest and most robust technique to search for new particles is to look for ``bumps'' in invariant mass spectra over smoothly falling backgrounds. We present a new extension of the bump hunt that naturally benefits from modern machine learning algorithms while remaining model agnostic. This approach is based on the classification without labels (CWoLa) method where the invariant mass is used to create two potentially mixed samples, one with little or no signal and one with a potential resonance. Additional features that are uncorrelated with the invariant mass can be used for training the classifier. Given the lack of new physics signals at the Large Hadron Collider (LHC), such model-agnostic approaches are critical for ensuring full coverage to fully exploit the rich datasets from the LHC experiments. In addition to illustrating how the new method works in simple test cases, we demonstrate the power of the extended bump hunt on a realistic all-hadronic resonance search in a channel that would not be covered with existing techniques.","",""
101,"Andreas K Triantafyllidis, A. Tsanas","Applications of Machine Learning in Real-Life Digital Health Interventions: Review of the Literature",2019,"","","","",181,"2022-07-13 09:24:04","","10.2196/12286","","",,,,,101,33.67,51,2,3,"Background Machine learning has attracted considerable research interest toward developing smart digital health interventions. These interventions have the potential to revolutionize health care and lead to substantial outcomes for patients and medical professionals. Objective Our objective was to review the literature on applications of machine learning in real-life digital health interventions, aiming to improve the understanding of researchers, clinicians, engineers, and policy makers in developing robust and impactful data-driven interventions in the health care domain. Methods We searched the PubMed and Scopus bibliographic databases with terms related to machine learning, to identify real-life studies of digital health interventions incorporating machine learning algorithms. We grouped those interventions according to their target (ie, target condition), study design, number of enrolled participants, follow-up duration, primary outcome and whether this had been statistically significant, machine learning algorithms used in the intervention, and outcome of the algorithms (eg, prediction). Results Our literature search identified 8 interventions incorporating machine learning in a real-life research setting, of which 3 (37%) were evaluated in a randomized controlled trial and 5 (63%) in a pilot or experimental single-group study. The interventions targeted depression prediction and management, speech recognition for people with speech disabilities, self-efficacy for weight loss, detection of changes in biopsychosocial condition of patients with multiple morbidity, stress management, treatment of phantom limb pain, smoking cessation, and personalized nutrition based on glycemic response. The average number of enrolled participants in the studies was 71 (range 8-214), and the average follow-up study duration was 69 days (range 3-180). Of the 8 interventions, 6 (75%) showed statistical significance (at the P=.05 level) in health outcomes. Conclusions This review found that digital health interventions incorporating machine learning algorithms in real-life studies can be useful and effective. Given the low number of studies identified in this review and that they did not follow a rigorous machine learning evaluation methodology, we urge the research community to conduct further studies in intervention settings following evaluation principles and demonstrating the potential of machine learning in clinical practice.","",""
107,"Hyunjun Kim, Eunjong Ahn, Myoungsu Shin, S. Sim","Crack and Noncrack Classification from Concrete Surface Images Using Machine Learning",2019,"","","","",182,"2022-07-13 09:24:04","","10.1177/1475921718768747","","",,,,,107,35.67,27,4,3,"In concrete structures, surface cracks are important indicators of structural durability and serviceability. Generally, concrete cracks are visually monitored by inspectors who record crack information such as the existence, location, and width. Manual visual inspection is often considered ineffective in terms of cost, safety, assessment accuracy, and reliability. Digital image processing has been introduced to more accurately obtain crack information from images. A critical challenge is to automatically identify cracks from an image containing actual cracks and crack-like noise patterns (e.g. dark shadows, stains, lumps, and holes), which are often seen in concrete structures. This article presents a methodology for identifying concrete cracks using machine learning. The method helps in determining the existence and location of cracks from surface images. The proposed approach is particularly designed for classifying cracks and noncrack noise patterns that are otherwise difficult to distinguish using existing image processing algorithms. In the training stage of the proposed approach, image binarization is used to extract crack candidate regions; subsequently, classification models are constructed based on speeded-up robust features and convolutional neural network. The obtained crack identification methods are quantitatively and qualitatively compared using new concrete surface images containing cracks and noncracks.","",""
50,"Megha Byali, Harsh Chaudhari, A. Patra, A. Suresh","FLASH: Fast and Robust Framework for Privacy-preserving Machine Learning",2020,"","","","",183,"2022-07-13 09:24:04","","10.2478/popets-2020-0036","","",,,,,50,25.00,13,4,2,"Abstract Privacy-preserving machine learning (PPML) via Secure Multi-party Computation (MPC) has gained momentum in the recent past. Assuming a minimal network of pair-wise private channels, we propose an efficient four-party PPML framework over rings ℤ2ℓ, FLASH, the first of its kind in the regime of PPML framework, that achieves the strongest security notion of Guaranteed Output Delivery (all parties obtain the output irrespective of adversary’s behaviour). The state of the art ML frameworks such as ABY3 by Mohassel et.al (ACM CCS’18) and SecureNN by Wagh et.al (PETS’19) operate in the setting of 3 parties with one malicious corruption but achieve the weaker security guarantee of abort. We demonstrate PPML with real-time efficiency, using the following custom-made tools that overcome the limitations of the aforementioned state-of-the-art– (a) dot product, which is independent of the vector size unlike the state-of-the-art ABY3, SecureNN and ASTRA by Chaudhari et.al (ACM CCSW’19), all of which have linear dependence on the vector size. (b) Truncation and MSB Extraction, which are constant round and free of circuits like Parallel Prefix Adder (PPA) and Ripple Carry Adder (RCA), unlike ABY3 which uses these circuits and has round complexity of the order of depth of these circuits. We then exhibit the application of our FLASH framework in the secure server-aided prediction of vital algorithms– Linear Regression, Logistic Regression, Deep Neural Networks, and Binarized Neural Networks. We substantiate our theoretical claims through improvement in benchmarks of the aforementioned algorithms when compared with the current best framework ABY3. All the protocols are implemented over a 64-bit ring in LAN and WAN. Our experiments demonstrate that, for MNIST dataset, the improvement (in terms of throughput) ranges from 24 × to 1390 × over LAN and WAN together.","",""
52,"Hana Dureckova, M. Krykunov, M. Z. Aghaji, T. Woo","Robust Machine Learning Models for Predicting High CO2 Working Capacity and CO2/H2 Selectivity of Gas Adsorption in Metal Organic Frameworks for Precombustion Carbon Capture",2019,"","","","",184,"2022-07-13 09:24:04","","10.1021/ACS.JPCC.8B10644","","",,,,,52,17.33,13,4,3,"This work is devoted to the development of quantitative structure–property relationship (QSPR) models using machine learning to predict CO2 working capacity and CO2/H2 selectivity for precombustion carbon capture using a topologically diverse database of hypothetical metal–organic framework (MOF) structures (358 400 MOFs, 1166 network topologies). Such a diversity of the networks topology is much higher than previously used (<20 network topologies) for rapid and accurate recognition of high-performing MOFs for other gas-separation applications. The gradient boosted trees regression method allowed us to use 80% of the database as a training set, while the rest was used for the validation and test set. The QSPR models are first built using purely geometric descriptors of MOFs such as gravimetric surface area and void fraction. Additional models which account for chemical features of MOFs are constructed using atomic property weighted radial distribution functions (AP-RDFs) with a novel normalization to acco...","",""
50,"A. Romagnoni, S. Jégou, K. Van Steen, G. Wainrib, J. Hugot, L. Peyrin-Biroulet, M. Chamaillard, J. Colombel, M. Cottone, M. D’Amato, R. D'Incà, J. Halfvarson, P. Henderson, A. Karban, N. Kennedy, M. Khan, M. Lémann, A. Levine, D. Massey, M. Milla, S. M. Ng, I. Oikonomou, H. Peeters, D. Proctor, J. Rahier, P. Rutgeerts, F. Seibold, L. Stronati, K. Taylor, L. Törkvist, Kullak Ublick, J. V. van Limbergen, A. van Gossum, M. Vatn, Hu Zhang, Wei Zhang, J. Andrews, P. Bampton, M. Barclay, T. Florin, R. Gearry, K. Krishnaprasad, I. Lawrance, G. Mahy, G. Montgomery, G. Radford-Smith, R. Roberts, L. Simms, K. Hanigan, A. Croft","Comparative performances of machine learning methods for classifying Crohn Disease patients using genome-wide genotyping data",2019,"","","","",185,"2022-07-13 09:24:04","","10.1038/s41598-019-46649-z","","",,,,,50,16.67,5,50,3,"","",""
34,"Xiaodan Xi, Haoyu Zhuang, Nan Sun, M. Orshansky","Strong subthreshold current array PUF with 265 challenge-response pairs resilient to machine learning attacks in 130nm CMOS",2017,"","","","",186,"2022-07-13 09:24:04","","10.23919/VLSIC.2017.8008503","","",,,,,34,6.80,9,4,5,"This paper presents a strong silicon physically unclonable function (PUF) immune to machine learning (ML) attacks. The PUF, termed the subthreshold current array (SCA) PUF, is composed of a pair of two-dimensional transistor arrays and a low-offset comparator. The fabricated PUF chip allows 265 challenge-response pairs (CRPs) and achieves high reliability with average bit error rate (BER) of 5.8% for temperatures −20 to 80°C and Vdd + 10%. The calibration-based CRPs filtering method effectively improves BER to 2.6% with a 10% loss of CRPs. When subjected to ML attacks, the PUF shows resilience that is 100X higher than known alternatives, with negligible loss in PUF unpredictability.","",""
2,"S. Siltanen, Takanori Ide","Electrical Impedance Tomography, Enclosure Method and Machine Learning",2020,"","","","",187,"2022-07-13 09:24:04","","10.1109/MLSP49062.2020.9231717","","",,,,,2,1.00,1,2,2,"Electrical impedance tomography (EIT) is a non-destructive imaging method, where a physical body is probed with electric measurements at the boundary, and information about the internal conductivity is extracted from the data. The enclosure method of Ikehata [J. Inv. III-Posed Prob. 8(2000)] recovers the convex hull of an inclusion of unknown conductivity embedded in known background conductivity. Practical implementations of the enclosure method are based on least-squares (LS) fitting of lines to noise-robust values of the so-called indicator function. It is shown how a convolutional neural network instead of LS fitting improves the accuracy of the enclosure method significantly while retaining interpretability.","",""
39,"Hyunil Kim, Seung-Hyun Kim, J. Hwang, Changho Seo","Efficient Privacy-Preserving Machine Learning for Blockchain Network",2019,"","","","",188,"2022-07-13 09:24:04","","10.1109/ACCESS.2019.2940052","","",,,,,39,13.00,10,4,3,"A blockchain as a trustworthy and secure decentralized and distributed network has been emerged for many applications such as in banking, finance, insurance, healthcare and business. Recently, many communities in blockchain networks want to deploy machine learning models to get meaningful knowledge from geographically distributed large-scale data owned by each participant. To run a learning model without data centralization, distributed machine learning (DML) for blockchain networks has been studied. While several works have been proposed, privacy and security have not been sufficiently addressed, and as we show later, there are vulnerabilities in the architecture and limitations in terms of efficiency. In this paper, we propose a privacy-preserving DML model for a permissioned blockchain to resolve the privacy, security, and performance issues in a systematic way. We develop a differentially private stochastic gradient descent method and an error-based aggregation rule as core primitives. Our model can treat any type of differentially private learning algorithm where non-deterministic functions should be defined. The proposed error-based aggregation rule is effective to prevent attacks by an adversarial node that tries to deteriorate the accuracy of DML models. Our experiment results show that our proposed model provides stronger resilience against adversarial attacks than other aggregation rules under a differentially private scenario. Finally, we show that our proposed model has high usability because it has low computational complexity and low transaction latency.","",""
37,"Mariam Nassar, M. Doan, A. Filby, O. Wolkenhauer, D. Fogg, J. Piasecka, C. Thornton, Anne E Carpenter, H. Summers, P. Rees, H. Hennig","Label‐Free Identification of White Blood Cells Using Machine Learning",2019,"","","","",189,"2022-07-13 09:24:04","","10.1002/cyto.a.23794","","",,,,,37,12.33,4,11,3,"White blood cell (WBC) differential counting is an established clinical routine to assess patient immune system status. Fluorescent markers and a flow cytometer are required for the current state‐of‐the‐art method for determining WBC differential counts. However, this process requires several sample preparation steps and may adversely disturb the cells. We present a novel label‐free approach using an imaging flow cytometer and machine learning algorithms, where live, unstained WBCs were classified. It achieved an average F1‐score of 97% and two subtypes of WBCs, B and T lymphocytes, were distinguished from each other with an average F1‐score of 78%, a task previously considered impossible for unlabeled samples. We provide an open‐source workflow to carry out the procedure. We validated the WBC analysis with unstained samples from 85 donors. The presented method enables robust and highly accurate identification of WBCs, minimizing the disturbance to the cells and leaving marker channels free to answer other biological questions. It also opens the door to employing machine learning for liquid biopsy, here, using the rich information in cell morphology for a wide range of diagnostics of primary blood. © 2019 The Authors. Cytometry Part A published by Wiley Periodicals, Inc. on behalf of International Society for Advancement of Cytometry.","",""
217,"Ian J. Goodfellow, Nicolas Papernot, P. Mcdaniel","Cleverhans V0.1: an Adversarial Machine Learning Library",2016,"","","","",190,"2022-07-13 09:24:04","","","","",,,,,217,36.17,72,3,6,"cleverhans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models’ performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the cleverhans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system.","",""
0,"B. Monga","Model-Based and Machine Learning-Based Control of Biological Oscillators",2020,"","","","",191,"2022-07-13 09:24:04","","","","",,,,,0,0.00,0,1,2,"Author(s): Monga, Bharat | Advisor(s): Moehlis, Jeff | Abstract: Nonlinear oscillators - dynamical systems with stable periodic orbits - arise in many systems of physical, technological, and biological interest. This dissertation investigates the dynamics of such oscillators arising in biology, and develops several control algorithms to modify their collective behavior. We demonstrate that these control algorithms have potential in devising treatments for Parkinson's disease, cardiac alternans, and jet lag. Phase reduction, a classical reduction technique, has been instrumental in understanding such biological oscillators. In this dissertation, we investigate a new reduction technique called augmented phase reduction, and calculate its associated analytical expressions for six dynamically different planar systems: This helps us to understand the dynamical regimes for which the use of augmented phase reduction is advantageous over the standard phase reduction. We further this study by developing a novel optimal control algorithm based on the augmented phase reduction to change the phase of a single oscillator using a minimum energy input. We show that our control algorithm is effective even when a large phase change is required or when the nontrivial Floquet multiplier of the oscillator is close to unity; in such cases, the previously proposed control algorithm based on the standard phase reduction fails.We then devise a novel framework to control a population of biological oscillators as a whole, and change their collective behavior. Our first two control algorithms are Lyapunov-based, and our third is an optimal control algorithm which minimizes the control energy consumption while achieving the desired collective behavior of an oscillator population. We show that the developed control algorithms can synchronize, desynchronize, cluster, and phase shift the population.We continue this investigation by developing two novel machine learning control algorithms, which have a simple and intelligent structure that makes them effective even with a sparse data set. We show that these algorithms are powerful enough to control a wide variety of dynamical systems and not just biological oscillators. We conclude this study by understanding how the developed machine learning algorithms work in terms of phase reduction.In this dissertation, we have developed all these algorithms with the goal of ease of experimental implementation, for which the model parameters/training data can be measured experimentally. We close the loop on this dissertation by carrying out robustness analysis for the developed algorithms; demonstrating their resilience to noise, and thus their suitability for controlling living biological tissue. They truly hold great potential in devising treatments for Parkinson's disease, cardiac alternans, and jet lag.","",""
30,"Giovanni Apruzzese, M. Colajanni, Luca Ferretti, Mirco Marchetti","Addressing Adversarial Attacks Against Security Systems Based on Machine Learning",2019,"","","","",192,"2022-07-13 09:24:04","","10.23919/CYCON.2019.8756865","","",,,,,30,10.00,8,4,3,"Machine-learning solutions are successfully adopted in multiple contexts but the application of these techniques to the cyber security domain is complex and still immature. Among the many open issues that affect security systems based on machine learning, we concentrate on adversarial attacks that aim to affect the detection and prediction capabilities of machine-learning models. We consider realistic types of poisoning and evasion attacks targeting security solutions devoted to malware, spam and network intrusion detection. We explore the possible damages that an attacker can cause to a cyber detector and present some existing and original defensive techniques in the context of intrusion detection systems. This paper contains several performance evaluations that are based on extensive experiments using large traffic datasets. The results highlight that modern adversarial attacks are highly effective against machine-learning classifiers for cyber detection, and that existing solutions require improvements in several directions. The paper paves the way for more robust machine-learning-based techniques that can be integrated into cyber security platforms.","",""
657,"Wieland Brendel, Jonas Rauber, M. Bethge","Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",2017,"","","","",193,"2022-07-13 09:24:04","","","","",,,,,657,131.40,219,3,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .","",""
33,"V. Chernozhukov, W. Newey, Rahul Singh","Automatic Debiased Machine Learning of Causal and Structural Effects",2018,"","","","",194,"2022-07-13 09:24:04","","10.3982/ecta18515","","",,,,,33,8.25,11,3,4,"Many causal and structural effects depend on regressions. Examples include policy effects, average derivatives, regression decompositions, average treatment effects, causal mediation, and parameters of economic structural models. The regressions may be high‐dimensional, making machine learning useful. Plugging machine learners into identifying equations can lead to poor inference due to bias from regularization and/or model selection. This paper gives automatic debiasing for linear and nonlinear functions of regressions. The debiasing is automatic in using Lasso and the function of interest without the full form of the bias correction. The debiasing can be applied to any regression learner, including neural nets, random forests, Lasso, boosting, and other high‐dimensional methods. In addition to providing the bias correction, we give standard errors that are robust to misspecification, convergence rates for the bias correction, and primitive conditions for asymptotic inference for estimators of a variety of estimators of structural and causal effects. The automatic debiased machine learning is used to estimate the average treatment effect on the treated for the NSW job training data and to estimate demand elasticities from Nielsen scanner data while allowing preferences to be correlated with prices and income.","",""
52,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",195,"2022-07-13 09:24:04","","10.1145/3219617.3219655","","",,,,,52,10.40,17,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q łe m for an arbitrarily small but fixed constant ε>0. The parameter estimate converges in O(łog N) rounds with an estimation error on the order of max √dq/N, ~√d/N , which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q . The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log 3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
168,"D. Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, Soroosh Shafieezadeh-Abadeh","Wasserstein Distributionally Robust Optimization: Theory and Applications in Machine Learning",2019,"","","","",196,"2022-07-13 09:24:04","","10.1287/EDUC.2019.0198","","",,,,,168,56.00,42,4,3,"Many decision problems in science, engineering and economics are affected by uncertain parameters whose distribution is only indirectly observable through samples. The goal of data-driven decision-making is to learn a decision from finitely many training samples that will perform well on unseen test samples. This learning task is difficult even if all training and test samples are drawn from the same distribution---especially if the dimension of the uncertainty is large relative to the training sample size. Wasserstein distributionally robust optimization seeks data-driven decisions that perform well under the most adverse distribution within a certain Wasserstein distance from a nominal distribution constructed from the training samples. In this tutorial we will argue that this approach has many conceptual and computational benefits. Most prominently, the optimal decisions can often be computed by solving tractable convex optimization problems, and they enjoy rigorous out-of-sample and asymptotic consistency guarantees. We will also show that Wasserstein distributionally robust optimization has interesting ramifications for statistical learning and motivates new approaches for fundamental learning tasks such as classification, regression, maximum likelihood estimation or minimum mean square error estimation, among others.","",""
23,"J. Zhang, Kang Liu, Faiq Khalid, Muhammad Abdullah Hanif, Semeen Rehman, T. Theocharides, Alessandro Artussi, M. Shafique, S. Garg","Building Robust Machine Learning Systems: Current Progress, Research Challenges, and Opportunities",2019,"","","","",197,"2022-07-13 09:24:04","","10.1145/3316781.3323472","","",,,,,23,7.67,3,9,3,"Machine learning, in particular deep learning, is being used in almost all the aspects of life to facilitate humans, specifically in mobile and Internet of Things (IoT)-based applications. Due to its state-of-the-art performance, deep learning is also being employed in safety-critical applications, for instance, autonomous vehicles. Reliability and security are two of the key required characteristics for these applications because of the impact they can have on human's life. Towards this, in this paper, we highlight the current progress, challenges and research opportunities in the domain of robust systems for machine learning-based applications.","",""
87,"Joseph Gardiner, Shishir Nagaraja","On the Security of Machine Learning in Malware C&C Detection",2016,"","","","",198,"2022-07-13 09:24:04","","10.1145/3003816","","",,,,,87,14.50,44,2,6,"One of the main challenges in security today is defending against malware attacks. As trends and anecdotal evidence show, preventing these attacks, regardless of their indiscriminate or targeted nature, has proven difficult: intrusions happen and devices get compromised, even at security-conscious organizations. As a consequence, an alternative line of work has focused on detecting and disrupting the individual steps that follow an initial compromise and are essential for the successful progression of the attack. In particular, several approaches and techniques have been proposed to identify the command and control (C8C) channel that a compromised system establishes to communicate with its controller. A major oversight of many of these detection techniques is the design’s resilience to evasion attempts by the well-motivated attacker. C8C detection techniques make widespread use of a machine learning (ML) component. Therefore, to analyze the evasion resilience of these detection techniques, we first systematize works in the field of C8C detection and then, using existing models from the literature, go on to systematize attacks against the ML components used in these approaches.","",""
103,"Baibhab Chatterjee, D. Das, Shovan Maity, Shreyas Sen","RF-PUF: Enhancing IoT Security Through Authentication of Wireless Nodes Using In-Situ Machine Learning",2018,"","","","",199,"2022-07-13 09:24:04","","10.1109/JIOT.2018.2849324","","",,,,,103,25.75,26,4,4,"Traditional authentication in radio-frequency (RF) systems enable secure data communication within a network through techniques such as digital signatures and hash-based message authentication codes (HMAC), which suffer from key-recovery attacks. State-of-the-art Internet of Things networks such as Nest also use open authentication (OAuth 2.0) protocols that are vulnerable to cross-site-recovery forgery (CSRF), which shows that these techniques may not prevent an adversary from copying or modeling the secret IDs or encryption keys using invasive, side channel, learning or software attacks. Physical unclonable functions (PUFs), on the other hand, can exploit manufacturing process variations to uniquely identify silicon chips which makes a PUF-based system extremely robust and secure at low cost, as it is practically impossible to replicate the same silicon characteristics across dies. Taking inspiration from human communication, which utilizes inherent variations in the voice signatures to identify a certain speaker, we present RF-PUF: a deep neural network-based framework that allows real-time authentication of wireless nodes, using the effects of inherent process variation on RF properties of the wireless transmitters (Tx), detected through in-situ machine learning at the receiver (Rx) end. The proposed method utilizes the already-existing asymmetric RF communication framework and does not require any additional circuitry for PUF generation or feature extraction. The burden of device identification is completely shifted to the gateway Rx, similar to the operation of a human listener’s brain. Simulation results involving the process variations in a standard 65-nm technology node, and features such as local oscillator offset and  ${I}$ – ${Q}$  imbalance detected with a neural network having 50 neurons in the hidden layer indicate that the framework can distinguish up to 4800 Tx(s) with an accuracy of 99.9% [≈99% for 10000 Tx(s)] under varying channel conditions, and without the need for traditional preambles. The proposed scheme can be used as a stand-alone security feature, or as a part of traditional multifactor authentication.","",""
28,"Jiuwen Cao, K. Zhang, Hongwei Yong, Xiaoping Lai, Badong Chen, Zhiping Lin","Extreme Learning Machine With Affine Transformation Inputs in an Activation Function",2019,"","","","",200,"2022-07-13 09:24:04","","10.1109/TNNLS.2018.2877468","","",,,,,28,9.33,5,6,3,"The extreme learning machine (ELM) has attracted much attention over the past decade due to its fast learning speed and convincing generalization performance. However, there still remains a practical issue to be approached when applying the ELM: the randomly generated hidden node parameters without tuning can lead to the hidden node outputs being nonuniformly distributed, thus giving rise to poor generalization performance. To address this deficiency, a novel activation function with an affine transformation (AT) on its input is introduced into the ELM, which leads to an improved ELM algorithm that is referred to as an AT-ELM in this paper. The scaling and translation parameters of the AT activation function are computed based on the maximum entropy principle in such a way that the hidden layer outputs approximately obey a uniform distribution. Application of the AT-ELM algorithm in nonlinear function regression shows its robustness to the range scaling of the network inputs. Experiments on nonlinear function regression, real-world data set classification, and benchmark image recognition demonstrate better performance for the AT-ELM compared with the original ELM, the regularized ELM, and the kernel ELM. Recognition results on benchmark image data sets also reveal that the AT-ELM outperforms several other state-of-the-art algorithms in general.","",""
