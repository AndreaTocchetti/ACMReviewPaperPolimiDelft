Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
3,"Jean-Jacques Ohana, Steve Ohana, E. Benhamou, D. Saltiel, B. Guez","Explainable AI Models of Stock Crashes: A Machine-Learning Explanation of the Covid March 2020 Equity Meltdown",2021,"","","","",1,"2022-07-13 09:38:01","","10.2139/ssrn.3809308","","",,,,,3,3.00,1,5,1,"We consider a gradient boosting decision trees (GBDT) approach to predict large S&P 500 price drops from a set of 150 technical, fundamental and macroeconomic features. We report an improved accuracy of GBDT over other machine learning (ML) methods on the S&P 500 futures prices. We show that retaining fewer and carefully selected features provides improvements across all ML approaches. Shapley values have recently been introduced from game theory to the field of ML. They allow for a robust identification of the most important variables predicting stock market crises, and of a local explanation of the crisis probability at each date, through a consistent features attribution. We apply this methodology to analyze in detail the March 2020 financial meltdown, for which the model offered a timely out of sample prediction. This analysis unveils in particular the contrarian predictive role of the tech equity sector before and after the crash.","",""
10,"T. Botari, Frederik Hvilshøj, Rafael Izbicki, A. Carvalho","MeLIME: Meaningful Local Explanation for Machine Learning Models",2020,"","","","",2,"2022-07-13 09:38:01","","","","",,,,,10,5.00,3,4,2,"Most state-of-the-art machine learning algorithms induce black-box models, preventing their application in many sensitive domains. Hence, many methodologies for explaining machine learning models have been proposed to address this problem. In this work, we introduce strategies to improve local explanations taking into account the distribution of the data used to train the black-box models. We show that our approach, MeLIME, produces more meaningful explanations compared to other techniques over different ML models, operating on various types of data. MeLIME generalizes the LIME method, allowing more flexible perturbation sampling and the use of different local interpretable models. Additionally, we introduce modifications to standard training algorithms of local interpretable models fostering more robust explanations, even allowing the production of counterfactual examples. To show the strengths of the proposed approach, we include experiments on tabular data, images, and text; all showing improved explanations. In particular, MeLIME generated more meaningful explanations on the MNIST dataset than methods such as GuidedBackprop, SmoothGrad, and Layer-wise Relevance Propagation. MeLIME is available on this https URL.","",""
37,"Leif Hancox-Li","Robustness in machine learning explanations: does it matter?",2020,"","","","",3,"2022-07-13 09:38:01","","10.1145/3351095.3372836","","",,,,,37,18.50,37,1,2,"The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.","",""
3,"F. Biessmann, D. Refiano","Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated",2021,"","","","",4,"2022-07-13 09:38:01","","","","",,,,,3,3.00,2,2,1,"The field explainable artificial intelligence (XAI) has brought about an arsenal of methods to render Machine Learning (ML) predictions more interpretable. But how useful explanations provided by transparent ML methods are for humans remains difficult to assess. Here we investigate the quality of interpretable computer vision algorithms using techniques from psychophysics. In crowdsourced annotation tasks we study the impact of different interpretability approaches on annotation accuracy and task time. We compare these quality metrics with classical XAI, automated quality metrics. Our results demonstrate that psychophysical experiments allow for robust quality assessment of transparency in machine learning. Interestingly the quality metrics computed without humans in the loop did not provide a consistent ranking of interpretability methods nor were they representative for how useful an explanation was for humans. These findings highlight the potential of methods from classical psychophysics for modern machine learning applications. We hope that our results provide convincing arguments for evaluating interpretability in its natural habitat, human-ML interaction, if the goal is to obtain an authentic assessment of interpretability.","",""
1,"Quang-Vinh Dang","Improving the performance of the intrusion detection systems by the machine learning explainability",2021,"","","","",5,"2022-07-13 09:38:01","","10.1108/ijwis-03-2021-0022","","",,,,,1,1.00,1,1,1," Purpose This study aims to explain the state-of-the-art machine learning models that are used in the intrusion detection problem for human-being understandable and study the relationship between the explainability and the performance of the models.   Design/methodology/approach The authors study a recent intrusion data set collected from real-world scenarios and use state-of-the-art machine learning algorithms to detect the intrusion. The authors apply several novel techniques to explain the models, then evaluate manually the explanation. The authors then compare the performance of model post- and prior-explainability-based feature selection.   Findings The authors confirm our hypothesis above and claim that by forcing the explainability, the model becomes more robust, requires less computational power but achieves a better predictive performance.   Originality/value The authors draw our conclusions based on their own research and experimental works. ","",""
10,"Saeid Tizpaz-Niari, Pavol Cern'y, A. Trivedi","Detecting and understanding real-world differential performance bugs in machine learning libraries",2020,"","","","",6,"2022-07-13 09:38:01","","10.1145/3395363.3404540","","",,,,,10,5.00,3,3,2,"Programming errors that degrade the performance of systems are widespread, yet there is very little tool support for finding and diagnosing these bugs. We present a method and a tool based on differential performance analysis---we find inputs for which the performance varies widely, despite having the same size. To ensure that the differences in the performance are robust (i.e. hold also for large inputs), we compare the performance of not only single inputs, but of classes of inputs, where each class has similar inputs parameterized by their size. Thus, each class is represented by a performance function from the input size to performance. Importantly, we also provide an explanation for why the performance differs in a form that can be readily used to fix a performance bug. The two main phases in our method are discovery with fuzzing and explanation with decision tree classifiers, each of which is supported by clustering. First, we propose an evolutionary fuzzing algorithm to generate inputs that characterize different performance functions. For this fuzzing task, the unique challenge is that we not only need the input class with the worst performance, but rather a set of classes exhibiting differential performance. We use clustering to merge similar input classes which significantly improves the efficiency of our fuzzer. Second, we explain the differential performance in terms of program inputs and internals (e.g., methods and conditions). We adapt discriminant learning approaches with clustering and decision trees to localize suspicious code regions. We applied our techniques on a set of micro-benchmarks and real-world machine learning libraries. On a set of micro-benchmarks, we show that our approach outperforms state-of-the-art fuzzers in finding inputs to characterize differential performance. On a set of case-studies, we discover and explain multiple performance bugs in popular machine learning frameworks, for instance in implementations of logistic regression in scikit-learn. Four of these bugs, reported first in this paper, have since been fixed by the developers.","",""
0,"Wencan Zhang, Mariella Dimiccoli, Brian Y. Lim","Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning",2022,"","","","",7,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,3,1,"Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias). Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.","",""
0,"Wencan Zhang, Mariella Dimiccoli, Brian Y. Lim","Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning",2020,"","","","",8,"2022-07-13 09:38:01","","10.1145/3491102.3517522","","",,,,,0,0.00,0,3,2,"Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias) by perturbations and corruptions. Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.","",""
1,"Jieyu Lu, Yingkai Zhang","Unified Deep Learning Model for Multitask Reaction Predictions with Explanation",2022,"","","","",9,"2022-07-13 09:38:01","","10.1021/acs.jcim.1c01467","","",,,,,1,1.00,1,2,1,"There is significant interest and importance to develop robust machine learning models to assist organic chemistry synthesis. Typically, task-specific machine learning models for distinct reaction prediction tasks have been developed. In this work, we develop a unified deep learning model, T5Chem, for a variety of chemical reaction predictions tasks by adapting the ""Text-to-Text Transfer Transformer"" (T5) framework in natural language processing (NLP). On the basis of self-supervised pretraining with PubChem molecules, the T5Chem model can achieve state-of-the-art performances for four distinct types of task-specific reaction prediction tasks using four different open-source data sets, including reaction type classification on USPTO_TPL, forward reaction prediction on USPTO_MIT, single-step retrosynthesis on USPTO_50k, and reaction yield prediction on high-throughput C-N coupling reactions. Meanwhile, we introduced a new unified multitask reaction prediction data set USPTO_500_MT, which can be used to train and test five different types of reaction tasks, including the above four as well as a new reagent suggestion task. Our results showed that models trained with multiple tasks are more robust and can benefit from mutual learning on related tasks. Furthermore, we demonstrated the use of SHAP (SHapley Additive exPlanations) to explain T5Chem predictions at the functional group level, which provides a way to demystify sequence-based deep learning models in chemistry. T5Chem is accessible through https://yzhang.hpc.nyu.edu/T5Chem.","",""
21,"Cheng-Yu Hsieh, Chih-Kuan Yeh, Xuanqing Liu, Pradeep Ravikumar, Seungyeon Kim, Sanjiv Kumar, Cho-Jui Hsieh","Evaluations and Methods for Explanation through Robustness Analysis",2019,"","","","",10,"2022-07-13 09:38:01","","","","",,,,,21,7.00,3,7,3,"Among multiple ways of interpreting a machine learning model, measuring the importance of a set of features tied to a prediction is probably one of the most intuitive ways to explain a model. In this paper, we establish the link between a set of features to a prediction with a new evaluation criterion, robustness analysis, which measures the minimum distortion distance of adversarial perturbation. By measuring the tolerance level for an adversarial attack, we can extract a set of features that provides the most robust support for a prediction, and also can extract a set of features that contrasts the current prediction to a target class by setting a targeted adversarial attack. By applying this methodology to various prediction tasks across multiple domains, we observe the derived explanations are indeed capturing the significant feature set qualitatively and quantitatively.","",""
0,"Shihao Gu","Predicting Stock Returns with Firm Characteristics by Machine Learning Techniques",2017,"","","","",11,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,1,5,"Author(s): Gu, Shihao | Advisor(s): Zhu, Song-Chun | Abstract: We propose multiple advanced learning methods to deal with the ""curse of dimensionality""challenge in the cross-sectional stock returns. Our purpose is to predict the one-month-aheadstock returns by the rm characteristics which are so-called ""anomalies"". Compared withthe traditional methods like portfolio sorting and Fama Factor models, we focus on usingall existing machine learning methods to do the prediction rather than the explanation. Toalleviate the concern of excessive data mining, we use several regularization penalties thatcan lead to a sparse and robust model. Our method can identify the return predictors withincremental pricing information and learn the interaction effects by applying to a hierarchicalstructure. Our best method can achieve much higher out of sample R2 and portfolio SharpRatios than traditional linear regression method.","",""
8,"David Alvarez-Melis, Harmanpreet Kaur, Hal Daum'e, H. Wallach, Jennifer Wortman Vaughan","From Human Explanation to Model Interpretability: A Framework Based on Weight of Evidence",2021,"","","","",12,"2022-07-13 09:38:01","","","","",,,,,8,8.00,2,5,1,"We take inspiration from the study of human explanation to inform the design and evaluation of interpretability methods in machine learning. First, we survey the literature on human explanation in philosophy, cognitive science, and the social sciences, and propose a list of design principles for machine- generated explanations that are meaningful to humans. Using the concept of weight of evidence from information theory, we develop a method for generating explanations that adhere to these principles. We show that this method can be adapted to handle high-dimensional, multi-class settings, yielding a ﬂexible framework for generating explanations. We demon- strate that these explanations can be estimated accurately from ﬁnite samples and are robust to small perturbations of the inputs. We also evaluate our method through a qualitative user study with machine learning practitioners, where we observe that the resulting explanations are usable despite some participants struggling with background concepts like prior class probabilities. Finally, we conclude by surfacing design implications for interpretability tools in general.","",""
7,"Emanuele La Malfa, A. Zbrzezny, Rhiannon Michelmore, Nicola Paoletti, M. Kwiatkowska","On Guaranteed Optimal Robust Explanations for NLP Models",2021,"","","","",13,"2022-07-13 09:38:01","","10.24963/366","","",,,,,7,7.00,1,5,1,"We build on abduction-based explanations for machine learning and develop a method for computing local explanations for neural network models in natural language processing (NLP). Our explanations comprise a subset of the words of the input text that satisfies two key features: optimality w.r.t. a user-defined cost function, such as the length of explanation, and robustness, in that they ensure prediction invariance for any bounded perturbation in the embedding space of the left-out words. We present two solution algorithms, respectively based on implicit hitting sets and maximum universal subsets, introducing a number of algorithmic improvements to speed up convergence of hard instances. We show how our method can be configured with different perturbation sets in the embedded space and used to detect bias in predictions by enforcing include/exclude constraints on biased terms, as well as to enhance existing heuristic-based NLP explanation frameworks such as Anchors. We evaluate our framework on three widely used sentiment analysis tasks and texts of up to 100 words from SST, Twitter and IMDB datasets, demonstrating the effectiveness of the derived explanations.","",""
32,"Himabindu Lakkaraju, Nino Arsov, Osbert Bastani","Robust and Stable Black Box Explanations",2020,"","","","",14,"2022-07-13 09:38:01","","","","",,,,,32,16.00,11,3,2,"As machine learning black boxes are increasingly being deployed in real-world applications, there has been a growing interest in developing post hoc explanations that summarize the behaviors of these black boxes. However, existing algorithms for generating such explanations have been shown to lack stability and robustness to distribution shifts. We propose a novel framework for generating robust and stable explanations of black box models based on adversarial training. Our framework optimizes a minimax objective that aims to construct the highest fidelity explanation with respect to the worst-case over a set of adversarial perturbations. We instantiate this algorithm for explanations in the form of linear models and decision sets by devising the required optimization procedures. To the best of our knowledge, this work makes the first attempt at generating post hoc explanations that are robust to a general class of adversarial perturbations that are of practical interest. Experimental evaluation with real-world and synthetic datasets demonstrates that our approach substantially improves robustness of explanations without sacrificing their fidelity on the original data distribution.","",""
1,"R. Vardhan, Ninghao Liu, Phakpoom Chinprutthiwong, Weijie Fu, Zhen Hu, Xia Hu, G. Gu","ExAD: An Ensemble Approach for Explanation-based Adversarial Detection",2021,"","","","",15,"2022-07-13 09:38:01","","","","",,,,,1,1.00,0,7,1,"Recent research has shown Deep Neural Networks (DNNs) to be vulnerable to adversarial examples that induce desired misclassifications in the models. Such risks impede the application of machine learning in security-sensitive domains. Several defense methods have been proposed against adversarial attacks to detect adversarial examples at test time or to make machine learning models more robust. However, while existing methods are quite effective under blackbox threat model, where the attacker is not aware of the defense, they are relatively ineffective under whitebox threat model, where the attacker has full knowledge of the defense. In this paper, we propose ExAD, a framework to detect adversarial examples using an ensemble of explanation techniques. Each explanation technique in ExAD produces an explanation map identifying the relevance of input variables for the model’s classification. For every class in a dataset, the system includes a detector network, corresponding to each explanation technique, which is trained to distinguish between normal and abnormal explanation maps. At test time, if the explanation map of an input is detected as abnormal by any detector model of the classified class, then we consider the input to be an adversarial example. We evaluate our approach using six state-of-the-art adversarial attacks on three image datasets. Our extensive evaluation shows that our mechanism can effectively detect these attacks under blackbox threat model with limited false-positives. Furthermore, we find that our approach achieves promising results in limiting the success rate of whitebox attacks.","",""
6,"Olivier Deiss, S. Biswal, Jing Jin, Haoqi Sun, M. Westover, Jimeng Sun","HAMLET: Interpretable Human And Machine co-LEarning Technique",2018,"","","","",16,"2022-07-13 09:38:01","","","","",,,,,6,1.50,1,6,4,"Efficient label acquisition processes are key to obtaining robust classifiers. However, data labeling is often challenging and subject to high levels of label noise. This can arise even when classification targets are well defined, if instances to be labeled are more difficult than the prototypes used to define the class, leading to disagreements among the expert community. Here, we enable efficient training of deep neural networks. From low-confidence labels, we iteratively improve their quality by simultaneous learning of machines and experts. We call it Human And Machine co-LEarning Technique (HAMLET). Throughout the process, experts become more consistent, while the algorithm provides them with explainable feedback for confirmation. HAMLET uses a neural embedding function and a memory module filled with diverse reference embeddings from different classes. Its output includes classification labels and highly relevant reference embeddings as explanation. We took the study of brain monitoring at intensive care unit (ICU) as an application of HAMLET on continuous electroencephalography (cEEG) data. Although cEEG monitoring yields large volumes of data, labeling costs and difficulty make it hard to build a classifier. Additionally, while experts agree on the labels of clear-cut examples of cEEG patterns, labeling many real-world cEEG data can be extremely challenging. Thus, a large minority of sequences might be mislabeled. HAMLET has shown significant performance gain against deep learning and other baselines, increasing accuracy from 7.03% to 68.75% on challenging inputs. Besides improved performance, clinical experts confirmed the interpretability of those reference embeddings in helping explaining the classification results by HAMLET.","",""
0,"Joe Hays, S. Ramamoorthy, Christian Tetzlaff","Editorial: Robust Artificial Intelligence for Neurorobotics",2021,"","","","",17,"2022-07-13 09:38:01","","10.3389/fnbot.2021.809903","","",,,,,0,0.00,0,3,1,"Neural computing is a powerful paradigm that has revolutionized machine learning. Building from early roots in the study of adaptive behavior and attempts to understand information processing in parallel and distributed neural architectures, modern neural networks have convincingly demonstrated successes in numerous areas—transforming the practice of computer vision, natural language processing, and even computational biology. Applications in robotics bring stringent constraints on size, weight and power constraints (SWaP), which challenge the developers of these technologies in new ways. Indeed, these requirements take us back to the roots of the field of neural computing, forcing us to ask how it could be that the human brain achieves with as little as 12 watts of power what seems to require entire server farms with state of the art computational and numerical methods. Likewise, even lowly insects demonstrate a degree of adaptivity and resilience that still defy easy explanation or computational replication. In this Research Topic, we have compiled the latest research addressing several aspects of these broadly defined challenge questions. As illustrated in Figure 1, the articles are organized into four prevailing themes: Sense, Think, Act, and Tools.","",""
12,"Laura Rieger, L. K. Hansen","IROF: a low resource evaluation metric for explanation methods",2020,"","","","",18,"2022-07-13 09:38:01","","","","",,,,,12,6.00,6,2,2,"The adoption of machine learning in health care hinges on the transparency of the used algorithms, necessitating the need for explanation methods. However, despite a growing literature on explaining neural networks, no consensus has been reached on how to evaluate those explanation methods. We propose IROF, a new approach to evaluating explanation methods that circumvents the need for manual evaluation. Compared to other recent work, our approach requires several orders of magnitude less computational resources and no human input, making it accessible to lower resource groups and robust to human bias.","",""
154,"Hyun-Jae Choi, Eric Jang, Alexander A. Alemi","WAIC, but Why? Generative Ensembles for Robust Anomaly Detection",2018,"","","","",19,"2022-07-13 09:38:01","","","","",,,,,154,38.50,51,3,4,"Machine learning models encounter Out-of-Distribution (OoD) errors when the data seen at test time are generated from a different stochastic generator than the one used to generate the training data. One proposal to scale OoD detection to high-dimensional data is to learn a tractable likelihood approximation of the training distribution, and use it to reject unlikely inputs. However, likelihood models on natural data are themselves susceptible to OoD errors, and even assign large likelihoods to samples from other datasets. To mitigate this problem, we propose Generative Ensembles, which robustify density-based OoD detection by way of estimating epistemic uncertainty of the likelihood model. We present a puzzling observation in need of an explanation -- although likelihood measures cannot account for the typical set of a distribution, and therefore should not be suitable on their own for OoD detection, WAIC performs surprisingly well in practice.","",""
0,"Sayan Bandyapadhyay, F. Fomin, P. Golovach, W. Lochet, Nidhi Purohit, Kirill Simonov","How to Find a Good Explanation for Clustering?",2021,"","","","",20,"2022-07-13 09:38:01","","10.1609/aaai.v36i4.20306","","",,,,,0,0.00,0,6,1,"k-means and k-median clustering are powerful unsupervised machine learning techniques. However, due to complicated dependences on all the features, it is challenging to interpret the resulting cluster assignments. Moshkovitz, Dasgupta, Rashtchian, and Frost proposed an elegant model of explainable k-means and k-median clustering in ICML 2020. In this model, a decision tree with k leaves provides a straightforward characterization of the data set into clusters.       We study two natural algorithmic questions about explainable clustering. (1) For a given clustering, how to find the ``best explanation'' by using a decision tree with k leaves? (2) For a given set of points, how to find a decision tree with k leaves minimizing the k-means/median objective of the resulting explainable clustering? To address the first question, we introduce a new model of explainable clustering. Our model, inspired by the notion of outliers in robust statistics, is the following. We are seeking a small number of points (outliers) whose removal makes the existing clustering well-explainable. For addressing the second question, we initiate the study of the model of Moshkovitz et al. from the perspective of multivariate complexity. Our rigorous algorithmic analysis sheds some light on the influence of parameters like the input size, dimension of the data, the number of outliers, the number of clusters, and the approximation ratio, on the computational complexity of explainable clustering.","",""
0,"Hangzhi Guo, Feiran Jia, Jinghui Chen, A. Squicciarini, A. Yadav","RoCourseNet: Distributionally Robust Training of a Prediction Aware Recourse Model",2022,"","","","",21,"2022-07-13 09:38:01","","10.48550/arXiv.2206.00700","","",,,,,0,0.00,0,5,1,"Counterfactual (CF) explanations for machine learning (ML) models are preferred by end-users, as they explain the predictions of ML models by providing a recourse case to individuals who are adversely impacted by predicted outcomes. Existing CF explanation methods generate recourses under the assumption that the underlying target ML model remains stationary over time. However, due to commonly occurring distributional shifts in training data, ML models constantly get updated in practice, which might render previously generated recourses invalid and diminish end-users trust in our algorithmic framework. To address this problem, we propose RoCourseNet, a training framework that jointly optimizes for predictions and robust recourses to future data shifts. We have three main contributions: (i) We propose a novel virtual data shift (VDS) algorithm to ﬁnd worst-case shifted ML models by explicitly considering the worst-case data shift in the training dataset. (ii) We leverage adversarial training to solve a novel tri-level optimization problem inside RoCourseNet, which simultaneously generates predictions and corresponding robust recourses. (iii) Finally, we evaluate RoCourseNet’s performance on three real-world datasets and show that RoCourseNet outperforms state-of-the-art baselines by ∼ 10% in generating robust CF explanations.","",""
0,"Gaur Loveleen, Bhandari Mohan, Bhadwal Singh Shikhar, Jhanjhi Nz, Mohammad Shorfuzzaman, Mehedi Masud","Explanation-driven HCI Model to Examine the Mini-Mental State for Alzheimer’s Disease",2022,"","","","",22,"2022-07-13 09:38:01","","10.1145/3527174","","",,,,,0,0.00,0,6,1,"Directing research on Alzheimer’s towards only early prediction and accuracy cannot be considered a feasible approach towards tackling a ubiquitous degenerative disease today. Applying deep learning (DL), Explainable artificial intelligence(XAI) and advancing towards the human-computer interface(HCI) model can be a leap forward in medical research. This research aims to propose a robust explainable HCI model using shapley additive explanation (SHAP), local interpretable model-agnostic explanations (LIME) and DL algorithms. The use of DL algorithms: logistic regression(80.87%), support vector machine (85.8%), k-nearest neighbour(87.24%), multilayer perceptron(91.94%), decision tree(100%) and explainability can help exploring untapped avenues for research in medical sciences that can mould the future of HCI models. The outcomes of the proposed model depict higher prediction accuracy bringing efficient computer interface in decision making, and suggests a high level of relevance in the field of medical and clinical research.","",""
0,"D. Bzdok, J. Ioannidis","Deep ] neural networks are elaborate regression methods aimed solely at prediction , not estimation or explanation",2019,"","","","",23,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,2,3,"The last decades saw dramatic progress in brain research. These advances were often buttressed by probing single variables to make circumscribed discoveries, typically through null hypothesis significance testing. New ways for generating massive data fueled tension between the traditional methodology, used to infer statistically relevant effects in carefully-chosen variables, and pattern-learning algorithms, used to identify predictive signatures by searching through abundant information. In this article, we detail the antagonistic philosophies behind two quantitative approaches: certifying robust effects in understandable variables, and evaluating how accurately a built model can forecast future outcomes. We discourage choosing analysis tools via categories like ‘statistics’ or ‘machine learning’. Rather, to establish reproducible knowledge about the brain, we advocate prioritizing tools in view of the core motivation of each quantitative analysis: aiming towards mechanistic insight, or optimizing predictive accuracy.","",""
0,"I. Cone, C. Clopath, H. Shouval","Learning and Expression of Dopaminergic Reward Prediction Error via Plastic Representations of Time",2022,"","","","",24,"2022-07-13 09:38:01","","10.1101/2022.04.06.487298","","",,,,,0,0.00,0,3,1,"Dopamine (DA) releasing neurons in the midbrain learn response patterns that represent reward prediction error (RPE). Typically, models proposing a mechanistic explanation for how dopamine neurons learn to exhibit RPE are based on temporal difference (TD) learning, a machine learning algorithm. However, mechanistic models motivated by TD learning face two significant hurdles. First, TD based models typically require rather unrealistic components, such as long and robust temporal chains of feature specific neurons which tile, a priori, each interval from a given stimulus to a given reward. Secondly, various predictions of TD clash with experimental observations of how RPE evolves over learning. Here, we present a biophysically plausible plastic network model of spiking neurons, that learns RPEs and can replicate results observed in multiple experiments. This model, coined FLEX (Flexibly Learned Errors in Expected Reward), learns feature specific representations of time, allowing for neural representations of stimuli to adjust their timing and relation to rewards in an online manner. Following learning, model dopamine neurons in FLEX report a distribution of response types, as observed experimentally and as used in machine learning. Dopamine firing in our model reflects an RPE before and after learning but not necessarily during learning, allowing the model to reconcile seemingly inconsistent experiments and make unique predictions that contrast those of TD.","",""
3,"A. Preece, Daniel Harborne, R. Raghavendra, Richard J. Tomsett, Dave Braines","Provisioning Robust and Interpretable AI/ML-Based Service Bundles",2018,"","","","",25,"2022-07-13 09:38:01","","10.1109/MILCOM.2018.8599838","","",,,,,3,0.75,1,5,4,"Coalition operations environments are characterised by the need to share intelligence, surveillance and reconnaissance services. Increasingly, such services are based on artificial intelligence (AI)and machine learning (ML)technologies. Two key issues in the exploitation of AI/ML services are robustness and interpretability. Employing a diverse portfolio of services can make a system robust to ‘unknown unknowns’. Interpretability - the need for services to offer explanation facilities to engender user trust - can be addressed by a variety of methods to generate either transparent or post hoc explanations according to users' requirements. This paper shows how a service-provisioning framework for coalition operations can be extended to address specific requirements for robustness and interpretability, allowing automatic selection of service bundles for intelligence, surveillance and reconnaissance tasks. The approach is demonstrated in a case study on traffic monitoring featuring a diverse set of AI/ML services based on deep neural networks and heuristic reasoning approaches.","",""
1,"Klemen Pečnik, V. Todorovic, M. Bošnjak, M. Čemažar, I. Kononenko, G. Serša, J. Plavec","The General Explanation Method with NMR Spectroscopy Enables the Identification of Metabolite Profiles Specific for Normal and Tumor Cell Lines",2018,"","","","",26,"2022-07-13 09:38:01","","10.1002/cbic.201800392","","",,,,,1,0.25,0,7,4,"Machine learning models in metabolomics, despite their great prediction accuracy, are still not widely adopted owing to the lack of an efficient explanation for their predictions. In this study, we propose the use of the general explanation method to explain the predictions of a machine learning model to gain detailed insight into metabolic differences between biological systems. The method was tested on a dataset of 1H NMR spectra acquired on normal lung and mesothelial cell lines and their tumor counterparts. Initially, the random forests and artificial neural network models were applied to the dataset, and excellent prediction accuracy was achieved. The predictions of the models were explained with the general explanation method, which enabled identification of discriminating metabolic concentration differences between individual cell lines and enabled the construction of their specific metabolic concentration profiles. This intuitive and robust method holds great promise for in‐depth understanding of the mechanisms that underline phenotypes as well as for biomarker discovery in complex diseases.","",""
21,"Hongfu Liu, Haiyi Mao, Y. Fu","Robust Multi-View Feature Selection",2016,"","","","",27,"2022-07-13 09:38:01","","10.1109/ICDM.2016.0039","","",,,,,21,3.50,7,3,6,"High-throughput technologies have enabled us to rapidly accumulate a wealth of diverse data types. These multi-view data contain much more information to uncover the cluster structure than single-view data, which draws raising attention in data mining and machine learning areas. On one hand, many features are extracted to provide enough information for better representations, on the other hand, such abundant features might result in noisy, redundant and irrelevant information, which harms the performance of the learning algorithms. In this paper, we focus on a new topic, multi-view unsupervised feature selection, which aims to discover the discriminative features in each view for better explanation and representation. Although there are some exploratory studies along this direction, most of them employ the traditional feature selection by putting the features in different views together and fail to evaluate the performance in the multi-view setting. The features selected in this way are difficult to explain due to the meaning of different views, which disobeys the goal of feature selection as well. In light of this, we intend to give a correct understanding of multi-view feature selection. Different from the existing work, which either incorrectly concatenates the features from different views, or takes huge time complexity to learn the pseudo labels, we propose a novel algorithm, Robust Multi-view Feature Selection (RMFS), which applies robust multi-view K-means to obtain the robust and high quality pseudo labels for sparse feature selection in an efficient way. Nontrivially we give the solution by taking the derivatives and further provide a K-means-like optimization to update several variables in a unified framework with the convergence guarantee. We demonstrate extensive experiments on three real-world multi-view data sets, which illustrate the effectiveness and efficiency of RMFS in terms of both single-view and multi-view evaluations by a large margin.","",""
8,"Johannes Fürnkranz, T. Scheffer, M. Spiliopoulou","Machine Learning: ECML 2006, 17th European Conference on Machine Learning, Berlin, Germany, September 18-22, 2006, Proceedings",2006,"","","","",28,"2022-07-13 09:38:01","","10.1007/11871842","","",,,,,8,0.50,3,3,16,"","",""
3,"G. DeJong, S. Hutchinson, M. Spong","Integration of Machine Learning and Sensor-Based Control in Intelligent Robotic Systems",1993,"","","","",29,"2022-07-13 09:38:01","","10.23919/ACC.1993.4792873","","",,,,,3,0.10,1,3,29,"This paper discusses the integration of machine learning and sensor-based control in intelligent robotic systems. Our research is interdisciplinary and combines techniques of explanation-based control with robust and adaptive nonlinear control, computer vision, and motion planning. Our intent in this research is to go beyond the strict hierarchical control architectures typically used in robotic systems by integrating modeling, dynamics, and control across traditional levels of planning and control at all levels of intelligence. Our ultimate goal is to combine analytical techniques of nonlinear dynamics and control with artificial intelligence into a single new paradigm in which symbolic reasoning holds an equal place with differential equation based modeling and control.","",""
11,"Daniel Harborne, C. Willis, Richard J. Tomsett, A. Preece","Integrating learning and reasoning services for explainable information fusion",2018,"","","","",30,"2022-07-13 09:38:01","","","","",,,,,11,2.75,3,4,4,"—We present a distributed information fusion system  able to integrate heterogeneous information processing services  based on machine learning and reasoning approaches. We focus  on higher (semantic) levels of information fusion, and highlight  the requirement for the component services, and the system as  a whole, to generate explanations of its outputs. Using a case  study approach in the domain of traffic monitoring, we introduce  component services based on (i) deep neural network approaches  and (ii) heuristic-based reasoning. We examine methods for  explanation generation in each case, including both transparency  (e.g, saliency maps, reasoning traces) and post-hoc methods  (e.g, explanation in terms of similar examples, identification of  relevant semantic objects). We consider trade-offs in terms of  the classification performance of the services and the kinds of  available explanations, and show how service integration offers  more robust performance and explainability.","",""
146,"Chih-Kuan Yeh, Cheng-Yu Hsieh, Arun Sai Suggala, David I. Inouye, Pradeep Ravikumar","On the (In)fidelity and Sensitivity of Explanations",2019,"","","","",31,"2022-07-13 09:38:01","","","","",,,,,146,48.67,29,5,3,"We consider objective evaluation measures of saliency explanations for complex black-box machine learning models. We propose simple robust variants of two notions that have been considered in recent literature: (in)fidelity, and sensitivity. We analyze optimal explanations with respect to both these measures, and while the optimal explanation for sensitivity is a vacuous constant explanation, the optimal explanation for infidelity is a novel combination of two popular explanation methods. By varying the perturbation distribution that defines infidelity, we obtain novel explanations by optimizing infidelity, which we show to out-perform existing explanations in both quantitative and qualitative measurements. Another salient question given these measures is how to modify any given explanation to have better values with respect to these measures. We propose a simple modification based on lowering sensitivity, and moreover show that when done appropriately, we could simultaneously improve both sensitivity as well as fidelity.","",""
1,"Wang Xue-song","Q Learning Based on Probability Support Vector Classification Machine",2010,"","","","",32,"2022-07-13 09:38:01","","","","",,,,,1,0.08,1,1,12,"The state-action space of a Q learning system was divided into positive and negative classes according to TD error criterion.In order to describe the uncertainty of classification and to solve the problem of low learning precision resulted from simple classification,a probability support vector classification machine(PSVCM) was used to make the classification of samples both have qualitative explanation and quantitative evaluation.The inputs of PSVCM are continuous states and discrete actions,while its output is a class label with a probability value.A Q learning control strategy for continuous action space can be obtained based on a weighted operation of the positive actions with their probability values.The simulations results of a boat problem show that the proposed method is suitable for Q learning control for nonlinear systems with continuous states and continuous actions compared with Q learning based on traditional SVCM and the control performance is robust with respect to the setting of initial action.","",""
7,"Kuang-Huei Lee, Anurag Arnab, S. Guadarrama, J. Canny, Ian S. Fischer","Compressive Visual Representations",2021,"","","","",33,"2022-07-13 09:38:01","","","","",,,,,7,7.00,1,5,1,"Learning effective visual representations that generalize well without human supervision is a fundamental problem in order to apply Machine Learning to a wide variety of tasks. Recently, two families of self-supervised methods, contrastive learning and latent bootstrapping, exempliﬁed by SimCLR and BYOL respec-tively, have made signiﬁcant progress. In this work, we hypothesize that adding explicit information compression to these algorithms yields better and more robust representations. We verify this by developing SimCLR and BYOL formulations compatible with the Conditional Entropy Bottleneck (CEB) objective, allowing us to both measure and control the amount of compression in the learned representation, and observe their impact on downstream tasks. Furthermore, we explore the relationship between Lipschitz continuity and compression, showing a tractable lower bound on the Lipschitz constant of the encoders we learn. As Lipschitz continuity is closely related to robustness, this provides a new explanation for why compressed models are more robust. Our experiments conﬁrm that adding compression to SimCLR and BYOL signiﬁcantly improves linear evaluation accuracies and model robustness across a wide range of domain shifts. In particular, the compressed version of BYOL achieves 76.0% Top-1 linear evaluation accuracy on ImageNet with ResNet-50, and 78.8% with ResNet-50 2x. 1","",""
5,"Mohammed Bany Muhammad, M. Yeasin","Eigen-CAM: Visual Explanations for Deep Convolutional Neural Networks",2021,"","","","",34,"2022-07-13 09:38:01","","10.1007/s42979-021-00449-3","","",,,,,5,5.00,3,2,1,"","",""
1,"Aparna Balagopalan, Haoran Zhang, Kimia Hamidieh, Thomas Hartvigsen, F. Rudzicz, M. Ghassemi","The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations",2022,"","","","",35,"2022-07-13 09:38:01","","10.1145/3531146.3533179","","",,,,,1,1.00,0,6,1,"Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.","",""
0,"J. Balajee, M. A. Saleem Durai","Smart survey on recent trends in water level, drought and water quality analysis system",2021,"","","","",36,"2022-07-13 09:38:01","","10.1088/1742-6596/1964/4/042052","","",,,,,0,0.00,0,2,1,"Over 200 million yearly reports of diseases identified with scarce water and sanitation conditions, 5-10 million deaths occurred worldwide. Water quality checking has subsequently gotten important to supply clean and safe water. This survey work depicts the fundamental explanation behind the requirement for robust and productive Water level, Drought, and water quality control in the level framework, which will keep human assets healthy, sustainable and diminish water use for household purposes. Climate change and variability have so many significant impacts caused by the natural environment’s water system. Incredible methods, collection of water samples are tested alone and analyzed in water laboratories. However, it is not always easy to capture, analyze, and rapidly disseminate information to relevant users to make timely and well-informed decisions. The review work encompasses traditional methods based on Machine Learning (ML), and Deep Learning (DL) approaches.","",""
0,"Joao Marques-Silva","Automated Reasoning in Explainable AI",2021,"","","","",37,"2022-07-13 09:38:01","","10.3233/faia210109","","",,,,,0,0.00,0,1,1,"The envisioned applications of machine learning (ML) in high-risk and safetycritical applications hinge on systems that are robust in their operation and that can be trusted. Automated reasoning offers the solution to ensure robustness and to guarantee trust. This talk overviews recent efforts on applying automated reasoning tools in explaining black-box (and so non-interpretable) ML models [6], and relates such efforts with past work on reasoning about inconsistent logic formulas [11]. Moreover, the talk details the computation of rigorous explanations of black-box models, and how these serve for assessing the quality of widely used heuristic explanation approaches. The talk also covers important properties of rigorous explanations, including duality relationships between different kinds of explanations [7,5,4]. Finally, the talk briefly overviews ongoing work on mapping practical efficient [8,3] but also tractable explainability [9,10,2,1].","",""
0,"Vladislav Zhuzhel, Rodrigo Rivera-Castro, Nina Kaploukhaya, Liliya Mironova, A. Zaytsev, E. Burnaev","COHORTNEY: Non-Parametric Clustering of Event Sequences",2021,"","","","",38,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,6,1,"Cohort analysis is a pervasive activity in web analytics. One divides users into groups according to specific criteria and tracks their behavior over time. Despite its extensive use, academic circles do not discuss cohort analysis to evaluate user behavior online. This work introduces an unsupervised non-parametric approach to group Internet users based on their activities. In comparison, canonical methods in marketing and engineering-based techniques underperform. COHORTNEY is the first machine learning-based cohort analysis algorithm with a robust theoretical explanation.","",""
0,"David Alvarez-Melis, Harmanpreet Kaur, Hal Daum'e, H. Wallach, Jennifer Wortman Vaughan","A Human-Centered Interpretability Framework Based on Weight of Evidence",2021,"","","","",39,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,5,1,"In this paper, we take a human-centered approach to interpretable machine learning. First, drawing inspiration from the study of explanation in philosophy, cognitive science, and the social sciences, we propose a list of design principles for machinegenerated explanations that are meaningful to humans. Using the concept of weight of evidence from information theory, we develop a method for producing explanations that adhere to these principles. We show that this method can be adapted to handle high-dimensional, multi-class settings, yielding a flexible meta-algorithm for generating explanations. We demonstrate that these explanations can be estimated accurately from finite samples and are robust to small perturbations of the inputs. We also evaluate our method through a qualitative user study with machine learning practitioners, where we observe that the resulting explanations are usable despite some participants struggling with background concepts like prior class probabilities. Finally, we conclude by surfacing design implications for interpretability tools.","",""
0,"A. Gupta, Biplav Srivastava","Towards Developing Better Object Detectors for Real-World Use",2021,"","","","",40,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,2,1,"Deep visual models are fast surpassing human-level performance for various vision tasks, including object detection, increasing their use in day-to-day life applications. It is often the case that standard models that perform well when evaluated on the validation dataset—usually collected from the same source as the training dataset, often perform poorly on data different from that of the training data. Recent works also prove that adversarial examples can easily fool deep learning models and are primarily opaque. To address the issue of making object detectors more compatible for real-world use, we propose some steps to make them more reliable and robust for deployment. Proposed methods include the explanation method and data augmentation techniques. Data augmentation improves the performance and outcomes of machine learning models by generalizing them and explanation methods for getting new insights into black-box detectors. Such understanding can also help improve resistance to a wide range of adversarial attacks. ACM Reference Format: Akshay Gupta and Biplav Srivastava. 2022. Towards Developing Better Object Detectors for Real-World Use. In 5th Joint International Conference on Data Science & Management of Data (9th ACM IKDD CODS and 27th COMAD) (CODS-COMAD 2022), January 8–10, 2022, Bangalore, India. ACM, New York, NY, USA, 2 pages. https://doi.org/10.1145/3493700.3493741","",""
0,"E. Benhamou, Jean-Jacques Ohana, D. Saltiel, B. Guez, Steve Ohana","Explainable AI (XAI) Models Applied to Planning in Financial Markets",2021,"","","","",41,"2022-07-13 09:38:01","","10.2139/ssrn.3862437","","",,,,,0,0.00,0,5,1,"Regime changes planning in financial markets is well known to be hard to explain and interpret. Can an asset manager ex-plain clearly the intuition of his regime changes prediction on equity market ? To answer this question, we consider a gradi-ent boosting decision trees (GBDT) approach to plan regime changes on S&P 500 from a set of 150 technical, fundamen-tal and macroeconomic features. We report an improved ac-curacy of GBDT over other machine learning (ML) methods on the S&P 500 futures prices. We show that retaining fewer and carefully selected features provides improvements across all ML approaches. Shapley values have recently been intro-duced from game theory to the field of ML. This approach allows a robust identification of the most important variables planning stock market crises, and of a local explanation of the crisis probability at each date, through a consistent features attribution. We apply this methodology to analyse in detail the March 2020 financial meltdown, for which the model of-fered a timely out of sample prediction. This analysis unveils in particular the contrarian predictive role of the tech equity sector before and after the crash.","",""
0,"P. Rasouli, Ingrid Chieh Yu","Analyzing and Improving the Robustness of Tabular Classifiers using Counterfactual Explanations",2021,"","","","",42,"2022-07-13 09:38:01","","10.1109/ICMLA52953.2021.00209","","",,,,,0,0.00,0,2,1,"Recent studies have revealed that Machine Learning (ML) models are vulnerable to adversarial perturbations. Such perturbations can be intentionally or accidentally added to the original inputs, evading the classifier’s behavior to misclassify the crafted samples. A widely-used solution is to retrain the model using data points generated by various attack strategies. However, this creates a classifier robust to some particular evasions and can not defend unknown or universal perturbations. Counterfactual explanations are a specific class of post-hoc explanation methods that provide minimal modification to the input features in order to obtain a particular outcome from the model. In addition to the resemblance of counterfactual explanations to the universal perturbations, the possibility of generating instances from specific classes makes such approaches suitable for analyzing and improving the model’s robustness. Rather than explaining the model’s decisions in the deployment phase, we utilize the distance information obtained from counterfactuals and propose novel metrics to analyze the robustness of tabular classifiers. Further, we introduce a decision boundary modification approach using customized counterfactual data points to improve the robustness of the models without compromising their accuracy. Our framework addresses the robustness of black-box classifiers in the tabular setting, which is considered an under-explored research area. Through several experiments and evaluations, we demonstrate the efficacy of our approach in analyzing and improving the robustness of black-box tabular classifiers.","",""
0,"Ana Sofia Carmo, Mariana Abreu, A. Fred, Hugo Silva","EpiBOX: An Automated Platform for Long-Term Biosignal Collection",2022,"","","","",43,"2022-07-13 09:38:01","","10.3389/fninf.2022.837278","","",,,,,0,0.00,0,4,1,"Biosignals represent a first-line source of information to understand the behavior and state of human biological systems, often used in machine learning problems. However, the development of healthcare-related algorithms that are both personalized and robust requires the collection of large volumes of data to capture representative instances of all possible states. While the rise of flexible biosignal acquisition solutions has enabled the expedition of data collection, they often require complicated frameworks or do not provide the customization required in some research contexts. As such, EpiBOX was developed as an open-source, standalone, and automated platform that enables the long-term acquisition of biosignals, passable to be operated by individuals with low technological proficiency. In particular, in this paper, we present an in-depth explanation of the framework, methods for the evaluation of its performance, and the corresponding findings regarding the perspective of the end-user. The impact of the network connection on data transfer latency was studied, demonstrating innocuous latency values for reasonable signal strengths and manageable latency values even when the connection was unstable. Moreover, performance profiling of the EpiBOX user interface (mobile application) indicates a suitable performance in all aspects, providing an encouraging outlook on adherence to the system. Finally, the experience of our research group is described as a use case, indicating a promising outlook regarding the use of the EpiBOX framework within similar contexts. As a byproduct of these features, our hope is that by empowering physicians, technicians, and monitored subjects to supervise the biosignal collection process, we enable researchers to scale biosignal collection.","",""
0,"Debanjan Datta, F. Chen, Naren Ramakrishnan","Framing Algorithmic Recourse for Anomaly Detection",2022,"","","","",44,"2022-07-13 09:38:01","","10.1145/3534678.3539344","","",,,,,0,0.00,0,3,1,"The problem of algorithmic recourse has been explored for supervised machine learning models, to provide more interpretable, transparent and robust outcomes from decision support systems. An unexplored area is that of algorithmic recourse for anomaly detection, specifically for tabular data with only discrete feature values. Here the problem is to present a set of counterfactuals that are deemed normal by the underlying anomaly detection model so that applications can utilize this information for explanation purposes or to recommend countermeasures. We present an approach— C ontext preserving A lgorithmic R ecourse for A nomalies in T abular data ( CARAT ), that is effective, scalable, and agnostic to the underlying anomaly detection model. CARAT uses a transformer based encoder-decoder model to explain an anomaly by finding features with low likelihood. Subsequently semantically coherent counterfactuals are generated by modifying the highlighted features, using the overall context of features in the anomalous instance(s). Extensive experiments help demonstrate the efficacy of CARAT .","",""
8,"Laura Rieger, L. K. Hansen","A simple defense against adversarial attacks on heatmap explanations",2020,"","","","",45,"2022-07-13 09:38:01","","","","",,,,,8,4.00,4,2,2,"With machine learning models being used for more sensitive applications, we rely on interpretability methods to prove that no discriminating attributes were used for classification. A potential concern is the so-called ""fair-washing"" - manipulating a model such that the features used in reality are hidden and more innocuous features are shown to be important instead.  In our work we present an effective defence against such adversarial attacks on neural networks. By a simple aggregation of multiple explanation methods, the network becomes robust against manipulation. This holds even when the attacker has exact knowledge of the model weights and the explanation methods used.","",""
7,"M. Singh, Nupur Kumari, Puneet Mangla, Abhishek Sinha, V. Balasubramanian, Balaji Krishnamurthy","Attributional Robustness Training Using Input-Gradient Spatial Alignment",2019,"","","","",46,"2022-07-13 09:38:01","","10.1007/978-3-030-58583-9_31","","",,,,,7,2.33,1,6,3,"","",""
8,"R. Taheri, R. Javidan, Zahra Pooranian","Adversarial android malware detection for mobile multimedia applications in IoT environments",2020,"","","","",47,"2022-07-13 09:38:01","","10.1007/s11042-020-08804-x","","",,,,,8,4.00,3,3,2,"","",""
7,"Sean Saito, Eugene Chua, Nicholas Capel, Rocco Hu","Improving LIME Robustness with Smarter Locality Sampling",2020,"","","","",48,"2022-07-13 09:38:01","","","","",,,,,7,3.50,2,4,2,"Explainability algorithms such as LIME have enabled machine learning systems to adopt transparency and fairness, which are important qualities in commercial use cases. However, recent work has shown that LIME's naive sampling strategy can be exploited by an adversary to conceal biased, harmful behavior. We propose to make LIME more robust by training a generative adversarial network to sample more realistic synthetic data which the explainer uses to generate explanations. Our experiments demonstrate that our proposed method demonstrates an increase in accuracy across three real-world datasets in detecting biased, adversarial behavior compared to vanilla LIME. This is achieved while maintaining comparable explanation quality, with up to 99.94\% in top-1 accuracy in some cases.","",""
13,"Y. Yufik","The Understanding Capacity and Information Dynamics in the Human Brain",2019,"","","","",49,"2022-07-13 09:38:01","","10.3390/e21030308","","",,,,,13,4.33,13,1,3,"This article proposes a theory of neuronal processes underlying cognition, focusing on the mechanisms of understanding in the human brain. Understanding is a product of mental modeling. The paper argues that mental modeling is a form of information production inside the neuronal system extending the reach of human cognition “beyond the information given” (Bruner, J.S., Beyond the Information Given, 1973). Mental modeling enables forms of learning and prediction (learning with understanding and prediction via explanation) that are unique to humans, allowing robust performance under unfamiliar conditions having no precedents in the past history. The proposed theory centers on the notions of self-organization and emergent properties of collective behavior in the neuronal substrate. The theory motivates new approaches in the design of intelligent artifacts (machine understanding) that are complementary to those underlying the technology of machine learning.","",""
8,"M. Singh, Nupur Kumari, Puneet Mangla, Abhishek Sinha, V. Balasubramanian, Balaji Krishnamurthy","On the Benefits of Attributional Robustness",2019,"","","","",50,"2022-07-13 09:38:01","","","","",,,,,8,2.67,1,6,3,"Interpretability is an emerging area of research in trustworthy machine learning. Safe deployment of machine learning system mandates that the prediction and its explanation be reliable and robust. Recently, it was shown that one could craft perturbations that produce perceptually indistinguishable inputs having the same prediction, yet very different interpretations. We tackle the problem of attributional robustness (i.e. models having robust explanations) by maximizing the alignment between the input image and its saliency map using soft-margin triplet loss. We propose a robust attribution training methodology that beats the state-of-the-art attributional robustness measure by a margin of approximately 6-18% on several standard datasets, ie. SVHN, CIFAR-10 and GTSRB. We further show the utility of the proposed robust model in the domain of weakly supervised object localization and segmentation. Our proposed robust model also achieves a new state-of-the-art object localization accuracy on the CUB-200 dataset.","",""
8,"J. Pfau, Albert T. Young, Maria L. Wei, Michael J. Keiser","Global Saliency: Aggregating Saliency Maps to Assess Dataset Artefact Bias",2019,"","","","",51,"2022-07-13 09:38:01","","","","",,,,,8,2.67,2,4,3,"In high-stakes applications of machine learning models, interpretability methods provide guarantees that models are right for the right reasons. In medical imaging, saliency maps have become the standard tool for determining whether a neural model has learned relevant robust features, rather than artefactual noise. However, saliency maps are limited to local model explanation because they interpret predictions on an image-by-image basis. We propose aggregating saliency globally, using semantic segmentation masks, to provide quantitative measures of model bias across a dataset. To evaluate global saliency methods, we propose two metrics for quantifying the validity of saliency explanations. We apply the global saliency method to skin lesion diagnosis to determine the effect of artefacts, such as ink, on model bias.","",""
0,"","Figure 1: Combining Inductive and Analytical Learn- Ing: in the Ideal Case, a Learning System Deals with All Levels",,"","","","",52,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,0,,"of domain theories, i.e., it is robust with respect to severe errors therein. It operates purely inductively if no domain theory is available or the domain theory is random, and purely analytically if the domain theory is perfect. form they require correct and complete prior knowledge of the domain. In contrast, inductive learning methods require no such prior knowledge, but rely instead on many more training examples to guide generalization , together with some syntactic inductive bias. One of the major open problems in machine learning is to combine analytical and inductive learning in order to gain the beneets of both approaches: reduced requirement for training data, and robustness with respect to poor prior knowledge. Figure 1 illustrates the spectrum of domain theories over which a general learning system should be able to operate. At present, we h a ve inductive learning methods that operate well at the leftmost point on the spectrum , in which no domain theory is available. We also have explanation-based methods that operate well on the right under certain assumptions about the character of potential errors in the domain knowledge. We seek a single uniied method, which i s Robust with respect to severe errors in the domain theory, i.e., it should operate across the entire spectrum. In particular, if no domain theory is available or one that is even worse than random, we desire that the system learns as well as a purely inductive system. At the other extreme, if perfect knowledge is available, the system should perform comparably to current explanation-based methods. domain knowledge that it has previously learned from scratch, as well as knowledge provided by the designer. In particular, we are interested in methods that can operate under a broad variety of domain theory errors , such as those typical of inductively learned do","",""
16,"Pei-Hsuan Lu, Pin-Yu Chen, Kang-Cheng Chen, Chia-Mu Yu","On the Limitation of MagNet Defense Against L1-Based Adversarial Examples",2018,"","","","",53,"2022-07-13 09:38:01","","10.1109/DSN-W.2018.00065","","",,,,,16,4.00,4,4,4,"In recent years, defending adversarial perturbations to natural examples in order to build robust machine learning models trained by deep neural networks (DNNs) has become an emerging research field in the conjunction of deep learning and security. In particular, MagNet consisting of an adversary detector and a data reformer is by far one of the strongest defenses in the black-box oblivious attack setting, where the attacker aims to craft transferable adversarial examples from an undefended DNN model to bypass an unknown defense module deployed on the same DNN model. Under this setting, MagNet can successfully defend a variety of attacks in DNNs, including the high-confidence adversarial examples generated by the Carlini and Wagner's attack based on the L2 distortion metric. However, in this paper, under the same attack setting we show that adversarial examples crafted based on the L1 distortion metric can easily bypass MagNet and mislead the target DNN image classifiers on MNIST and CIFAR-10. We also provide explanations on why the considered approach can yield adversarial examples with superior attack performance and conduct extensive experiments on variants of MagNet to verify its lack of robustness to L1 distortion based attacks. Notably, our results substantially weaken the assumption of effective threat models on MagNet that require knowing the deployed defense technique when attacking DNNs (i.e., the gray-box attack setting).","",""
2,"P. Luszczek, I. Yamazaki, J. Dongarra","Increasing Accuracy of Iterative Refinement in Limited Floating-Point Arithmetic on Half-Precision Accelerators",2019,"","","","",54,"2022-07-13 09:38:01","","10.1109/HPEC.2019.8916392","","",,,,,2,0.67,1,3,3,"The emergence of deep learning as a leading computational workload for machine learning tasks on large-scale cloud infrastructure installations has led to plethora of accelerator hardware releases. However, the reduced precision and range of the floating-point numbers on these new platforms makes it a non-trivial task to leverage these unprecedented advances in computational power for numerical linear algebra operations that come with a guarantee of robust error bounds. In order to address these concerns, we present a number of strategies that can be used to increase the accuracy of limited-precision iterative refinement. By limited precision, we mean 16-bit floating-point formats implemented in modern hardware accelerators and are not necessarily compliant with the IEEE half-precision specification. We include the explanation of a broader context and connections to established IEEE floating-point standards and existing high-performance computing (HPC) benchmarks. We also present a new formulation of LU factorization that we call signed square root LU which produces more numerically balanced L and U factors which directly address the problems of limited range of the low-precision storage formats. The experimental results indicate that it is possible to recover substantial amounts of the accuracy in the system solution that would otherwise be lost. Previously, this could only be achieved by using iterative refinement based on single-precision floating-point arithmetic. The discussion will also explore the numerical stability issues that are important for robust linear solvers on these new hardware platforms.","",""
8,"G. Perry, David O'Sullivan","Identifying Narrative Descriptions in Agent-Based Models Representing Past Human-Environment Interactions",2018,"","","","",55,"2022-07-13 09:38:01","","10.1007/S10816-017-9355-X","","",,,,,8,2.00,4,2,4,"","",""
62,"Michael Lebowitz","Not the Path to Perdition: The Utility of Similarity-Based Learning",1986,"","","","",56,"2022-07-13 09:38:01","","","","",,,,,62,1.72,62,1,36,"A large portion of the research in machine learning has involved a paradigm of comparing many examples and analyzing them in terms of similarities and differences, assuming that the resulting generalizations will have applicability to new examples. While such research has been very successful, it is by no means obvious why similarity-based generalizations should be useful, since they may simply reflect coincidences. Proponents of explanation-based learning, a new, knowledge-intensive method of examining single examples to derive generalizations based on underlying causal models, could contend that their methods are more fundamentally grounded, and that there is no need to look for similarities across examples. In this paper, we present the issues, and then show why similarity-based methods are important. We present four reasons why robust machine learning must involve the integration of similarity-based and explanation-based methods. We argue that: 1) it may not always be practical or even possible to determine a causal explanation; 2) similarity usually implies causality; 3) similarity-based generalizations can be refined over time; 4) similarity-based and explanation-based methods complement each other in important ways.","",""
0,"A. Bhamidipaty, D. Gruen, S. Patel, D. Soroker","Towards a Generalized Similarity Service",2018,"","","","",57,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,4,4,"Similarity is an integral part of learning, both human learning and machine learning. Similarity-driven reasoning, analogy, learning and explanation are critical for AI to become truly robust. The complexity in learning a measure of similarity particularly stems from the fact that there is no single notion of similarity for a set of objects. The definition of similarity is critically dependent on the impression of the user performing the task. To truly provide a similarity service, the system has to be able to learn the notion of similarity in real-time, interacting with the user. In this paper, we introduce a vision of generalized similarity service that attempts to learn an individual’s similarity function. A conceptual framework describing the system capabilities for such a service is presented. Implementation of this framework is applied to the domain of company similarity. A preliminary user study highlights the importance of generalized similarity service.","",""
36,"P. Boinee, A. Angelis, G. Foresti","Meta Random Forests",2008,"","","","",58,"2022-07-13 09:38:01","","","","",,,,,36,2.57,12,3,14,"Leo Breimans Random Forests (RF) is a recent development in tree based classifiers and quickly proven to be one of the most important algorithms in the machine learning literature. It has shown robust and improved results of classifications on standard data sets. Ensemble learning algorithms such as AdaBoost and Bagging have been in active research and shown improvements in classification results for several benchmarking data sets with mainly decision trees as their base classifiers. In this paper we experiment to apply these Meta learning techniques to the random forests. We experiment the working of the ensembles of random forests on the standard data sets available in UCI data sets. We compare the original random forest algorithm with their ensemble counterparts and discuss the results. Keywords— Random Forests [RF], ensembles, UCI. I. PROBLEM DOMAIN ANDOM Forests (RF) [1] are one of the most successful tree based classifiers. It has proven to be fast, robust to noise, and offers possibilities for explanation and visualization of its output. In the random forest method, a large number of classification trees are grown and combined. Statistically speaking two elements serve to obtain a random forest resampling and random split selection. Resampling is done here by sampling multiple times with replacement from the original training data set. Thus in the resulting samples, a certain event may appear several times, and other events not at all. About 2/3 of the data in the training sample are taken for each bootstrap sample and the remaining one-third of the cases are left out of the sample. This oob (out-of-bag) data is used to get a running unbiased estimate of the classification error as trees are added to the forest. It is also used to get estimates of variable importance. The design of random forests is to give the user a good deal of information about the data besides an accurate prediction. Much of this information comes from using the oob cases in the training set that have been left out of the bootstrapped training set. Random split selection is used in each trees growing process. It is computationally effective and offer good prediction performance. It generates an internal unbiased estimate of the generalization. It has an effective method for Manuscript received September 30, 2005. Praveen Boinee is the PhD Student in Computer Science in Udine University, Udine, 33100, Italy (phone: 0039-0432-558231; e-mail: boinee@fisica.uniud.it). Alessandro De Angelis is the Professor in Experimental and Computational Physics at Udine University, Udine, 33100, Italy (e-mail: deangelis@fisica.uniud.it). Gian Luca Foresti is the Professor in Computer Science at Udine University, Udine, Italy (email: foresti@dimi.uniud.it). estimating missing data and maintains accuracy when a large proportion of the data are missing. It generates an internal unbiased estimate of the generalization error as the forest building progresses and thus does not over fit. These capabilities of RF can be extended to unlabeled data, leading to unsupervised clustering, data views and outlier detection. Several authors have noted that constructing ensembles of base learners can significantly improve the performance of learning. Bagging, boosting, are the most popular examples of this methodology. The success of ensemble methods is usually explained with the margin and correlation of base classifiers [13]. To have a good ensemble one needs base classifiers which are diverse (in a sense that they predict differently), yet accurate. The ensemble mechanism which operates on the top of base learners then ensures highly accurate predictions. Here we experiment with random forests as themselves as the base classifiers for making ensembles and test the performance of the model. The ensembles are applied on UCI standard data sets and compared with the original random forest algorithm. The paper is organized as follows. In section II we introduce the decision tress the bases for constructing the random forests. Section III introduces the actual random forests algorithm. Section IV discusses the ensemble learning and making of bagged and boosted random forests. The experiments with UCI data sets are described in section V. Results are discussed in Section VI. II. DECISION TREES – A BASE FOR RANDOM FORESTS The decision-tree representation is the most widely used logic method for efficiently producing classifiers from the data. There is a large number of decision-tree induction algorithms described primarily in the machine-learning and applied-statistics literature. The decision tree algorithm is well known for its robustness and learning efficiency with its learning time complexity of O(nlog2n). The output of the algorithm is a decision tree, which can be easily represented as a set of symbolic rules (IF...THEN). The symbolic rules can be directly interpreted and compared with the existing domain knowledge, providing the useful information for the domain experts. A typical decision-tree learning system adopts a top-down strategy that searches for a solution in a part of the search space. It guarantees that a simple, but not necessarily the simplest, tree will be found. A decision tree consists of nodes that where attributes are tested. The outgoing branches of a node correspond to all the possible outcomes of the test at the Meta Random Forests Praveen Boinee, Alessandro De Angelis, and Gian Luca Foresti R World Academy of Science, Engineering and Technology International Journal of Computer, Electrical, Automation, Control and Information Engineering Vol:2, No:6, 2008 2246 International Scholarly and Scientific Research & Innovation 2(6) 2008 scholar.waset.org/1999.4/3799 In te rn at io na l S ci en ce I nd ex , C om pu te r an d In fo rm at io n E ng in ee ri ng V ol :2 , N o: 6, 2 00 8 w as et .o rg /P ub lic at io n/ 37 99 node. A simple decision tree for classification of samples with two input attributes X and Y is given Fig. 1. Fig. 1 A simple decision tree with the tests on attributes X and Y All samples with feature values X>1 and Y=B belong to Class2, while the samples with values X<1 belong to Class1, whatever the value for feature Y. The samples, at a nonleaf node in the tree structure, are thus partitioned along the branches and each child node gets its corresponding subset of samples. Decision trees that use univariate splits have a simple representational form, making it relatively easy for the user to understand the inferred model; at the same time, they represent a restriction on the expressiveness of the model. In general, any restriction on a particular tree representation can significantly restrict the functional form and thus the approximation power of the model. A well-known treegrowing algorithm for generating decision trees based on univariate splits is Quinlan's ID3 with an extended version called C4.5 [6]. Greedy search methods, which involve growing and pruning decision-tree structures, are typically employed in these algorithms to explore the exponential space of possible models and to remove unnecessary preconditions and duplication. C4.5 applies a divide and conquers strategy to construct the tree. The sets of instances are accompanied by a set of properties. A decision tree is a tree where each node is a test on the values of an attribute, and the leaves represent the class of an instance that satisfies the tests. The tree will return a ‘yes’ or ‘no’ decision when the sets of instances are tested on it. Rules can be derived from the tree by following a path from the root to a leaf and using the nodes along the path as preconditions for the rule, to predict the class at the leaf. For developing random forests, we use the trees that randomly choose a subset of attributes at each mode. III. RANDOM FORESTS A random forest is a classifier consisting of a collection of tree structures classifiers ( ) { } ,... 1 , , = Θ k x h k where the { } k Θ are independent identically distributed random vectors and each tree casts a unit vote for the most popular class at input x. The forest chooses the classification having the most votes over all the trees in the forest. Each tree is grown as follows: 1. If the number of cases in the training set is N, sample N cases at random but with replacement, from the original data. This sample will be the training set for growing the tree. 2. If there are M input variables, a number m<<M is specified such that at each node, m variables are selected at random out of the M and the best split on these m is used to split the node. The value of m is held constant during the forest growing. 3. Each tree is grown to the largest extent possible. There is no pruning. and the overall forest error rate depends on two things: • The correlation between any two trees in the forest. Increasing the correlation increases the forest error rate. • The strength of each individual tree in the forest. A tree with a low error rate is a strong classifier. Increasing the strength of the individual trees decreases the forest error rate. After each tree is built, all of the data are run down the tree, and proximities are computed for each pair of cases. If two cases occupy the same terminal node, their proximity is increased by one. At the end of the run, the proximities are normalized by dividing by the number of trees. Proximities are used in replacing missing data, locating outliers, and producing illuminating low-dimensional views of the data. To formalize the working of the random forests, Let the forest contain K classifier trees ( ) ( ) ( ) x h x h x h K ,..., , 2 1 and the joint classifier be ( ) x h . Each learning instance is represented by an ordered pair (x,y), where each vector of attributes x consists of individual attributes a i Ai ,..., 1 , = (a is the number of attributes) and is labeled with the target value c j y j ,..., 1 , = (c is the number of class values). The correct class is denoted as y, without index","",""
1,"Laxmi S. Patil, S. Bewoor, Prof. M. S. Bewoor, Dr. S. H. Patil","Query Specific ROCK Clustering Algorithm for Text Summarization",2012,"","","","",59,"2022-07-13 09:38:01","","","","",,,,,1,0.10,0,4,10,"The idea of Data Mining has become very popular in recent years. Data Mining is the notion of all methods and techniques, which allow analyzing very large data sets to extract and discover previously unknown structures and relations out of such huge heaps of details. Data clustering is an important technique for exploratory data analysis. Clustering is a data mining (machine learning) technique used to place data elements into related groups without advance knowledge of the group definitions. Clustering is nothing but Grouping of objects into different sets, or the partitioning of a data set into subsets (clusters), In this paper we provides in depth explanation of implementation adopted for ROCK (RObust Clustering using linKs) clustering algorithm. We propose a novel concept of links to measure the similarity/proximity between a pair of data points. We develop a robust hierarchical clustering algorithm ROCK that employs links and not distances when merging clusters.","",""
13,"S. Kedar-Cabelli","Formulating concepts and analogies according to purpose",1988,"","","","",60,"2022-07-13 09:38:01","","","","",,,,,13,0.38,13,1,34,"The dissertation investigates open problems in machine learning, a subarea of artificial intelligence (AI). In particular, we focus on issues within the explanation-based generalization (EBG) framework, a framework for producing deductive generalizations from single examples. We begin by presenting a domain independent EBG method, and an associated algorithm and implementation (PROLOG-EBG). The algorithm demonstrates the close relationship between EBG and resolution theorem-proving.  Despite recent progress, EBG methods exhibit an important limitation: they are incapable of determining which target concepts are useful ones to acquire. More robust generalizers must be able to automatically determine which concepts to acquire based on the purpose of the learning, since concepts acquired for one purpose may not be appropriate for another.  Our notion of the purpose of the learning is to acquire concepts which will benefit an associated performance system. Two open issues become apparent once EBG is associated with a performance system: How can EBG acquire target concepts and definitions appropriate for the performance system? Further, could the acquired target concept definitions be used to improve subsequent performance?  We focus and investigate these issues in the context of a specific type of performance system--a state-space planner. Our approach is to provide EBG with explicit knowledge of the planner and specific planning task. The purposive concept formulation and purposive explanation replay methods, respectively, provide solutions to the open problems.  We provide experimental support for these methods in the form of prototype systems. The results confirm that a learning system can formulate concepts and analogies sensitive to the purpose of the learning in restricted planning situations. We describe further extensions suggested by these results.","",""
0,"Jean-Arcady Meyer, Stewart W. Wilson","An Adaptation Anomaly of a Genetic Algorithm",1991,"","","","",61,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,2,31,"The purpose of this paper is threefold. To analyze an adaptation anomaly observed in a specific genetic algorithm designed to optimize robot trajectories. to propose an explanation for this unusual adaptive behaviour by drawing an analogy with some elementary mechanisms in nature, and to suggest that a much more robust adaptive strategy is to allow concurrent adaptation of both the information content and the representation structure by the genetic plan. This latter strategy results in the optimization of the ability to adapt. This paper suggests that when artificial life or machine learning applications attempt to c apture the essence of natural adaptation. it is important they allow selection to operate on all levels of the system. Furthermore, it is essential to expose both the structure of the system and its information content to selection.","",""
77,"A. Qayyum, Junaid Qadir, M. Bilal, A. Al-Fuqaha","Secure and Robust Machine Learning for Healthcare: A Survey",2020,"","","","",62,"2022-07-13 09:38:01","","10.1109/RBME.2020.3013489","","",,,,,77,38.50,19,4,2,"Recent years have witnessed widespread adoption of machine learning (ML)/deep learning (DL) techniques due to their superior performance for a variety of healthcare applications ranging from the prediction of cardiac arrest from one-dimensional heart signals to computer-aided diagnosis (CADx) using multi-dimensional medical images. Notwithstanding the impressive performance of ML/DL, there are still lingering doubts regarding the robustness of ML/DL in healthcare settings (which is traditionally considered quite challenging due to the myriad security and privacy issues involved), especially in light of recent results that have shown that ML/DL are vulnerable to adversarial attacks. In this paper, we present an overview of various application areas in healthcare that leverage such techniques from security and privacy point of view and present associated challenges. In addition, we present potential methods to ensure secure and privacy-preserving ML for healthcare applications. Finally, we provide insight into the current research challenges and promising directions for future research.","",""
15,"Eunji Lee, Dave Braines, Mitchell Stiffler, Adam Adam Hudler, Daniel Harborne","Developing the sensitivity of LIME for better machine learning explanation",2019,"","","","",63,"2022-07-13 09:38:01","","10.1117/12.2520149","","",,,,,15,5.00,3,5,3,"Machine learning systems can provide outstanding results, but their black-box nature means that it’s hard to understand how the conclusion has been reached. Understanding how the results are determined is especially important in military and security contexts due to the importance of the decisions that may be made as a result. In this work, the reliability of LIME (Local Interpretable Model Agnostic Explanations), a method of interpretability, was analyzed and developed. A simple Convolutional Neural Network (CNN) model was trained using two classes of images of “gun-wielder” and “non-wielder"". The sensitivity of LIME improved when multiple output weights for individual images were averaged and visualized. The resultant averaged images were compared to the individual images to analyze the variability and reliability of the two LIME methods. Without techniques such as those explored in this paper, LIME appears to be unstable because of the simple binary coloring and the ease with which colored regions flip when comparing different analyses. A closer inspection reveals that the significantly weighted regions are consistent, and the lower weighted regions flip states due to inherent randomness of the method. This suggests that improving the weighting methods for explanation techniques, which can then be used in the visualization of the results, is important to improve perceived stability and therefore better enable human interpretation and trust.","",""
53,"M. Shafique, Mahum Naseer, T. Theocharides, C. Kyrkou, O. Mutlu, Lois Orosa, Jungwook Choi","Robust Machine Learning Systems: Challenges,Current Trends, Perspectives, and the Road Ahead",2020,"","","","",64,"2022-07-13 09:38:01","","10.1109/MDAT.2020.2971217","","",,,,,53,26.50,8,7,2,"Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPS) and Internet-of-Things (IoT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques. —Partha Pratim Pande, Washington State University","",""
0,"Ransalu Senanayake, Daniel J. Fremont, Mykel J. Kochenderfer, A. Lomuscio, D. Margineantu, Cheng Soon Ong","Guest Editorial: Special issue on robust machine learning",2021,"","","","",65,"2022-07-13 09:38:01","","10.1007/s10994-021-06113-4","","",,,,,0,0.00,0,6,1,"","",""
0,"Jirong Yi, R. Mudumbai, Weiyu Xu","Derivation of Information-Theoretically Optimal Adversarial Attacks with Applications to Robust Machine Learning",2020,"","","","",66,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,3,2,"We consider the theoretical problem of designing an optimal adversarial attack on a decision system that maximally degrades the achievable performance of the system as measured by the mutual information between the degraded signal and the label of interest. This problem is motivated by the existence of adversarial examples for machine learning classifiers. By adopting an information theoretic perspective, we seek to identify conditions under which adversarial vulnerability is unavoidable i.e. even optimally designed classifiers will be vulnerable to small adversarial perturbations. We present derivations of the optimal adversarial attacks for discrete and continuous signals of interest, i.e., finding the optimal perturbation distributions to minimize the mutual information between the degraded signal and a signal following a continuous or discrete distribution. In addition, we show that it is much harder to achieve adversarial attacks for minimizing mutual information when multiple redundant copies of the input signal are available. This provides additional support to the recently proposed ``feature compression"" hypothesis as an explanation for the adversarial vulnerability of deep learning classifiers. We also report on results from computational experiments to illustrate our theoretical results.","",""
49,"Ran Xin, S. Kar, U. Khan","Decentralized Stochastic Optimization and Machine Learning: A Unified Variance-Reduction Framework for Robust Performance and Fast Convergence",2020,"","","","",67,"2022-07-13 09:38:01","","10.1109/MSP.2020.2974267","","",,,,,49,24.50,16,3,2,"Decentralized methods to solve finite-sum minimization problems are important in many signal processing and machine learning tasks where the data samples are distributed across a network of nodes, and raw data sharing is not permitted due to privacy and/or resource constraints. In this article, we review decentralized stochastic first-order methods and provide a unified algorithmic framework that combines variance reduction with gradient tracking to achieve robust performance and fast convergence. We provide explicit theoretical guarantees of the corresponding methods when the objective functions are smooth and strongly convex and show their applicability to nonconvex problems via numerical experiments. Throughout the article, we provide intuitive illustrations of the main technical ideas by casting appropriate tradeoffs and comparisons among the methods of interest and by highlighting applications to decentralized training of machine learning models.","",""
4,"Abderrahmen Amich, Birhanu Eshete","Explanation-Guided Diagnosis of Machine Learning Evasion Attacks",2021,"","","","",68,"2022-07-13 09:38:01","","10.1007/978-3-030-90019-9_11","","",,,,,4,4.00,2,2,1,"","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",69,"2022-07-13 09:38:01","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
25,"David Watson, L. Floridi","The explanation game: a formal framework for interpretable machine learning",2019,"","","","",70,"2022-07-13 09:38:01","","10.1007/s11229-020-02629-9","","",,,,,25,8.33,13,2,3,"","",""
57,"Lal Hussain","Detecting epileptic seizure with different feature extracting strategies using robust machine learning classification techniques by applying advance parameter optimization approach",2018,"","","","",71,"2022-07-13 09:38:01","","10.1007/s11571-018-9477-1","","",,,,,57,14.25,57,1,4,"","",""
101,"G. Lecu'e, M. Lerasle","Robust machine learning by median-of-means: Theory and practice",2017,"","","","",72,"2022-07-13 09:38:01","","10.1214/19-AOS1828","","",,,,,101,20.20,51,2,5,"We introduce new estimators for robust machine learning based on median-of-means (MOM) estimators of the mean of real valued random variables. These estimators achieve optimal rates of convergence under minimal assumptions on the dataset. The dataset may also have been corrupted by outliers on which no assumption is granted. We also analyze these new estimators with standard tools from robust statistics. In particular, we revisit the concept of breakdown point. We modify the original definition by studying the number of outliers that a dataset can contain without deteriorating the estimation properties of a given estimator. This new notion of breakdown number, that takes into account the statistical performances of the estimators, is non-asymptotic in nature and adapted for machine learning purposes. We proved that the breakdown number of our estimator is of the order of (number of observations)*(rate of convergence). For instance, the breakdown number of our estimators for the problem of estimation of a d-dimensional vector with a noise variance sigma^2 is sigma^2d and it becomes sigma^2 s log(d/s) when this vector has only s non-zero component. Beyond this breakdown point, we proved that the rate of convergence achieved by our estimator is (number of outliers) divided by (number of observation).  Besides these theoretical guarantees, the major improvement brought by these new estimators is that they are easily computable in practice. In fact, basically any algorithm used to approximate the standard Empirical Risk Minimizer (or its regularized versions) has a robust version approximating our estimators. As a proof of concept, we study many algorithms for the classical LASSO estimator. A byproduct of the MOM algorithms is a measure of depth of data that can be used to detect outliers.","",""
34,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, Semeen Rehman, M. Shafique","Robust Machine Learning Systems: Reliability and Security for Deep Neural Networks",2018,"","","","",73,"2022-07-13 09:38:01","","10.1109/IOLTS.2018.8474192","","",,,,,34,8.50,7,5,4,"Machine learning is commonly being used in almost all the areas that involve advanced data analytics and intelligent control. From applications like Natural Language Processing (NLP) to autonomous driving are based upon machine learning algorithms. An increasing trend is observed in the use of Deep Neural Networks (DNNs) for such applications. While the slight inaccuracy in applications like NLP does not have any severe consequences, it is not the same for other safety-critical applications, like autonomous driving and smart healthcare, where a small error can lead to catastrophic effects. Apart from high-accuracy DNN algorithms, there is a significant need for robust machine learning systems and hardware architectures that can generate reliable and trustworthy results in the presence of hardware-level faults while also preserving security and privacy. This paper provides an overview of the challenges being faced in ensuring reliable and secure execution of DNNs. To address the challenges, we present several techniques for analyzing and mitigating the reliability and security threats in machine learning systems.","",""
19,"Navdeep Gill, Patrick Hall, Kim Montgomery, Nicholas Schmidt","A Responsible Machine Learning Workflow with Focus on Interpretable Models, Post-hoc Explanation, and Discrimination Testing",2020,"","","","",74,"2022-07-13 09:38:01","","10.3390/info11030137","","",,,,,19,9.50,5,4,2,"This manuscript outlines a viable approach for training and evaluating machine learning systems for high-stakes, human-centered, or regulated applications using common Python programming tools. The accuracy and intrinsic interpretability of two types of constrained models, monotonic gradient boosting machines and explainable neural networks, a deep learning architecture well-suited for structured data, are assessed on simulated data and publicly available mortgage data. For maximum transparency and the potential generation of personalized adverse action notices, the constrained models are analyzed using post-hoc explanation techniques including plots of partial dependence and individual conditional expectation and with global and local Shapley feature importance. The constrained model predictions are also tested for disparate impact and other types of discrimination using measures with long-standing legal precedents, adverse impact ratio, marginal effect, and standardized mean difference, along with straightforward group fairness measures. By combining interpretable models, post-hoc explanations, and discrimination testing with accessible software tools, this text aims to provide a template workflow for machine learning applications that require high accuracy and interpretability and that mitigate risks of discrimination.","",""
3,"Christos Kokkotis, S. Moustakidis, V. Baltzopoulos, G. Giakas, D. Tsaopoulos","Identifying Robust Risk Factors for Knee Osteoarthritis Progression: An Evolutionary Machine Learning Approach",2021,"","","","",75,"2022-07-13 09:38:01","","10.3390/healthcare9030260","","",,,,,3,3.00,1,5,1,"Knee osteoarthritis (KOA) is a multifactorial disease which is responsible for more than 80% of the osteoarthritis disease’s total burden. KOA is heterogeneous in terms of rates of progression with several different phenotypes and a large number of risk factors, which often interact with each other. A number of modifiable and non-modifiable systemic and mechanical parameters along with comorbidities as well as pain-related factors contribute to the development of KOA. Although models exist to predict the onset of the disease or discriminate between asymptotic and OA patients, there are just a few studies in the recent literature that focused on the identification of risk factors associated with KOA progression. This paper contributes to the identification of risk factors for KOA progression via a robust feature selection (FS) methodology that overcomes two crucial challenges: (i) the observed high dimensionality and heterogeneity of the available data that are obtained from the Osteoarthritis Initiative (OAI) database and (ii) a severe class imbalance problem posed by the fact that the KOA progressors class is significantly smaller than the non-progressors’ class. The proposed feature selection methodology relies on a combination of evolutionary algorithms and machine learning (ML) models, leading to the selection of a relatively small feature subset of 35 risk factors that generalizes well on the whole dataset (mean accuracy of 71.25%). We investigated the effectiveness of the proposed approach in a comparative analysis with well-known FS techniques with respect to metrics related to both prediction accuracy and generalization capability. The impact of the selected risk factors on the prediction output was further investigated using SHapley Additive exPlanations (SHAP). The proposed FS methodology may contribute to the development of new, efficient risk stratification strategies and identification of risk phenotypes of each KOA patient to enable appropriate interventions.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",76,"2022-07-13 09:38:01","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
0,"K.-R. Muller","Towards robust machine learning methods for the analysis of brain data",2018,"","","","",77,"2022-07-13 09:38:01","","10.1109/IWW-BCI.2018.8311495","","",,,,,0,0.00,0,1,4,"In this short abstract I will discuss recent directions that machine learning and BCI efforts of the BBCI team and coworkers have taken. It is the nature of this short text that many pointers to research are given all of which show a high overlap to prior own contributions; this is not only unavoidable but intentional. When analysing Brain Data, it is challenging to combine data streams stemming from various modalities (see e.g. Biessmann et al., 2011, Sui et al., 2012, Fazli et al., 2015, Dahne et al., 2015). Hybrid BCIs are a successful example in this direction (Pfurtscheller et al., 2010, Muller-Putz et al. 2015, Dahne et al. 2015, Fazli et al. 2012, 2015). These techniques are firmly rooted in modern machine learning and signal processing that are now readily in use for analysing EEG, for decoding cognitive states etc. (Nikulin et al. 2007, and see Dornhege et al. 2004, Muller et al. 2008, Bunau et al. 2009, Tomioka and Muller, 2010, Blankertz et al., 2008, 2011, 2016, Lemm et al., 2011, for recent reviews and contributions to Machine Learning for BCI). Note that fusing information has also been a very common practice in the sciences and engineering (W altz and Llinas, 1990). The talk will discuss challenges for BCIs that are to be applied outside controlled lab spaces. Such complex and highly artifactual scenarios demand robust signal processing methods; see e.g. Samek et al. 2014, 2017b for recent reviews on robust methods for BCI. In addition I may expand on technical advances on the explanation framework for deep neural networks (Baehrens et al. 2010, Bach et al. 2015, Lapuschkin et al. 2016a and 2016b, Samek et al. 2017a, Montavon et al. 2017, 2018) to BCI data is given (Sturm et al. 2016). Furthermore, time permitting, I will revisit co-adaptive BCI systems (Vidaurre et al. 2011, Muller et al. 2017) and report on an upcoming study connecting fMRI and EEG data for co-adaptive training (Nierhaus et al. 2017). This abstract is based on joint work with Wojciech Samek, Benjamin Blankertz, Gabriel Curio, Michael Tangermann, Siamac Fazli, Vadim Nikulin, Gregoire Montavon, Sebastian Bach/Lapuschkin, Irene Sturm, Arno Villringer, Carmen Vidaurre, Till Nierhaus and many other members of the Berlin Brain Computer Interface team, the machine learning groups and many more esteemed collaborators. We greatly acknowledge funding by BMBF, EU, DFG and NRF.","",""
0,"K. Müller","Towards robust machine learning methods for the analysis of brain data",2018,"","","","",78,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,1,4,"In this short abstract I will discuss recent directions that machine learning and BCI efforts of the BBCI team and coworkers have taken. It is the nature of this short text that many pointers to research are given all of which show a high overlap to prior own contributions; this is not only unavoidable but intentional. When analysing Brain Data, it is challenging to combine data streams stemming from various modalities (see e.g. Biessmann et al., 2011, Sui et al., 2012, Fazli et al., 2015, Dähne et al., 2015). Hybrid BCIs are a successful example in this direction (Pfurtscheller et al., 2010, Müller-Putz et al. 2015, Dähne et al. 2015, Fazli et al. 2012, 2015). These techniques are firmly rooted in modern machine learning and signal processing that are now readily in use for analysing EEG, for decoding cognitive states etc. (Nikulin et al. 2007, and see Dornhege et al. 2004, Müller et al. 2008, Bünau et al. 2009, Tomioka and Müller, 2010, Blankertz et al., 2008, 2011, 2016, Lemm et al., 2011, for recent reviews and contributions to Machine Learning for BCI). Note that fusing information has also been a very common practice in the sciences and engineering (W altz and Llinas, 1990). The talk will discuss challenges for BCIs that are to be applied outside controlled lab spaces. Such complex and highly artifactual scenarios demand robust signal processing methods; see e.g. Samek et al. 2014, 2017b for recent reviews on robust methods for BCI. In addition I may expand on technical advances on the explanation framework for deep neural networks (Baehrens et al. 2010, Bach et al. 2015, Lapuschkin et al. 2016a and 2016b, Samek et al. 2017a, Montavon et al. 2017, 2018) to BCI data is given (Sturm et al. 2016). Furthermore, time permitting, I will revisit co-adaptive BCI systems (Vidaurre et al. 2011, Müller et al. 2017) and report on an upcoming study connecting fMRI and EEG data for co-adaptive training (Nierhaus et al. 2017). This abstract is based on joint work with Wojciech Samek, Benjamin Blankertz, Gabriel Curio, Michael Tangermann, Siamac Fazli, Vadim Nikulin, Gregoire Montavon, Sebastian Bach/Lapuschkin, Irene Sturm, Arno Villringer, Carmen Vidaurre, Till Nierhaus and many other members of the Berlin Brain Computer Interface team, the machine learning groups and many more esteemed collaborators. We greatly acknowledge funding by BMBF, EU, DFG and NRF.","",""
65,"Jianlong Zhou, A. Gandomi, Fang Chen, Andreas Holzinger","Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics",2021,"","","","",79,"2022-07-13 09:38:01","","10.3390/ELECTRONICS10050593","","",,,,,65,65.00,16,4,1,"The most successful Machine Learning (ML) systems remain complex black boxes to end-users, and even experts are often unable to understand the rationale behind their decisions. The lack of transparency of such systems can have severe consequences or poor uses of limited valuable resources in medical diagnosis, financial decision-making, and in other high-stake domains. Therefore, the issue of ML explanation has experienced a surge in interest from the research community to application domains. While numerous explanation methods have been explored, there is a need for evaluations to quantify the quality of explanation methods to determine whether and to what extent the offered explainability achieves the defined objective, and compare available explanation methods and suggest the best explanation from the comparison for a specific task. This survey paper presents a comprehensive overview of methods proposed in the current literature for the evaluation of ML explanations. We identify properties of explainability from the review of definitions of explainability. The identified properties of explainability are used as objectives that evaluation metrics should achieve. The survey found that the quantitative metrics for both model-based and example-based explanations are primarily used to evaluate the parsimony/simplicity of interpretability, while the quantitative metrics for attribution-based explanations are primarily used to evaluate the soundness of fidelity of explainability. The survey also demonstrated that subjective measures, such as trust and confidence, have been embraced as the focal point for the human-centered evaluation of explainable systems. The paper concludes that the evaluation of ML explanations is a multidisciplinary research topic. It is also not possible to define an implementation of evaluation metrics, which can be applied to all explanation methods.","",""
33,"J. Schneider, J. Handali","Personalized Explanation for Machine Learning: a Conceptualization",2019,"","","","",80,"2022-07-13 09:38:01","","","","",,,,,33,11.00,17,2,3,"Explanation in machine learning and related fields such as artificial intelligence aims at making machine learning models and their decisions understandable to humans. Existing work suggests that personalizing explanations might help to improve understandability. In this work, we derive a conceptualization of personalized explanation by defining and structuring the problem based on prior work on machine learning explanation, personalization (in machine learning) and concepts and techniques from other domains such as privacy and knowledge elicitation. We perform a categorization of explainee data used in the process of personalization as well as describing means to collect this data. We also identify three key explanation properties that are amendable to personalization: complexity, decision information and presentation. We also enhance existing work on explanation by introducing additional desiderata and measures to quantify the quality of personalized explanations.","",""
37,"Fan Yang, Mengnan Du, Xia Hu","Evaluating Explanation Without Ground Truth in Interpretable Machine Learning",2019,"","","","",81,"2022-07-13 09:38:01","","","","",,,,,37,12.33,12,3,3,"Interpretable Machine Learning (IML) has become increasingly important in many real-world applications, such as autonomous cars and medical diagnosis, where explanations are significantly preferred to help people better understand how machine learning systems work and further enhance their trust towards systems. However, due to the diversified scenarios and subjective nature of explanations, we rarely have the ground truth for benchmark evaluation in IML on the quality of generated explanations. Having a sense of explanation quality not only matters for assessing system boundaries, but also helps to realize the true benefits to human users in practical settings. To benchmark the evaluation in IML, in this article, we rigorously define the problem of evaluating explanations, and systematically review the existing efforts from state-of-the-arts. Specifically, we summarize three general aspects of explanation (i.e., generalizability, fidelity and persuasibility) with formal definitions, and respectively review the representative methodologies for each of them under different tasks. Further, a unified evaluation framework is designed according to the hierarchical needs from developers and end-users, which could be easily adopted for different scenarios in practice. In the end, open problems are discussed, and several limitations of current evaluation techniques are raised for future explorations.","",""
665,"Weihua Hu, Matthias Fey, M. Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, J. Leskovec","Open Graph Benchmark: Datasets for Machine Learning on Graphs",2020,"","","","",82,"2022-07-13 09:38:01","","","","",,,,,665,332.50,83,8,2,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .","",""
792,"T. Yarkoni, Jacob Westfall","Choosing Prediction Over Explanation in Psychology: Lessons From Machine Learning",2017,"","","","",83,"2022-07-13 09:38:01","","10.1177/1745691617693393","","",,,,,792,158.40,396,2,5,"Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.","",""
12,"J. Schneider, J. Handali","Personalized explanation in machine learning",2019,"","","","",84,"2022-07-13 09:38:01","","","","",,,,,12,4.00,6,2,3,"Explanation in machine learning and related fields such as artificial intelligence aims at making machine learning models and their decisions understandable to humans. Existing work suggests that personalizing explanations might help to improve understandability. In this work, we derive a conceptualization of personalized explanation by defining and structuring the problem based on prior work on machine learning explanation, personalization (in machine learning) and concepts and techniques from other domains such as privacy and knowledge elicitation. We perform a categorization of explainee information used in the process of personalization as well as describing means to collect this information. We also identify three key explanation properties that are amendable to personalization: complexity, decision information and presentation. We also enhance existing work on explanation by introducing additional desiderata and measures to quantify the quality of personalized explanations.","",""
296,"R. Mothilal, Amit Sharma, Chenhao Tan","Explaining machine learning classifiers through diverse counterfactual explanations",2019,"","","","",85,"2022-07-13 09:38:01","","10.1145/3351095.3372850","","",,,,,296,98.67,99,3,3,"Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.","",""
0,"H. Anh, Cao Van Kien","Robust extreme learning machine neural approach for uncertain nonlinear hyper‐chaotic system identification",2021,"","","","",86,"2022-07-13 09:38:01","","10.1002/rnc.5756","","",,,,,0,0.00,0,2,1,"This paper proposes a novel nonlinearly parameterized advanced single‐hidden layer neural extreme learning machine (ASHLN‐ELM) model in which the hidden and output weighting values are simultaneously updated using adaptively robust rules that are implemented based on Lyapunov stability principle. The proposed scheme guarantees the fast convergence speed of the state‐estimation residual errors bounded to null regarding to the influence of time‐varied disturbances. Additionally, proposed method needs no any knowledge related to desired weighting values or required approximating error. Typical uncertain hyper‐chaotic benchmark systems are used as to verify the new ASHLN‐ELM approach and to demonstrate the efficiency and the robustness of proposed method.","",""
215,"M. Narayanan, Emily Chen, Jeffrey He, Been Kim, S. Gershman, Finale Doshi-Velez","How do Humans Understand Explanations from Machine Learning Systems? An Evaluation of the Human-Interpretability of Explanation",2018,"","","","",87,"2022-07-13 09:38:01","","","","",,,,,215,53.75,36,6,4,"Recent years have seen a boom in interest in machine learning systems that can provide a human-understandable rationale for their predictions or decisions. However, exactly what kinds of explanation are truly human-interpretable remains poorly understood. This work advances our understanding of what makes explanations interpretable in the specific context of verification. Suppose we have a machine learning system that predicts X, and we provide rationale for this prediction X. Given an input, an explanation, and an output, is the output consistent with the input and the supposed rationale? Via a series of user-studies, we identify what kinds of increases in complexity have the greatest effect on the time it takes for humans to verify the rationale, and which seem relatively insensitive.","",""
3,"F. Lécué, B. Abeloos, Jonathan Anctil, Manuel Bergeron, Damien Dalla-Rosa, Simon Corbeil-Letourneau, Florian Martet, Tanguy Pommellet, L. Salvan, Simon Veilleux, M. Ziaeefard","Thales XAI Platform: Adaptable Explanation of Machine Learning Systems - A Knowledge Graphs Perspective",2019,"","","","",88,"2022-07-13 09:38:01","","","","",,,,,3,1.00,0,11,3,"Explanation in Machine Learning systems has been identified to be the main asset to have for large scale deployment of Artificial Intelligence (AI) in critical systems. Explanations could be example-, features-, semantics-based or even counterfactual to potentially action on an AI system; they could be represented in many different ways e.g., textual, graphical, or visual. All representations serve different means, purpose and operators. We built the first-of-its-kind XAI (eXplainable AI) platform for critical systems i.e., Thales XAI Platform which aims at serving explanations through various forms. This paper emphasizes on the semantics-based explanations for Machine Learning systems. 1 Explainable AI in Critical Systems Motivation: The current hype of Artificial Intelligence (AI) mostly refers to the success of Machine Learning (ML) and its sub-domain of deep learning. However industries operating with critical systems are either highly regulated, or require high level of certification and robustness. Therefore, such industry constraints do limit the adoption of non deterministic and ML systems. Answers to the question of explainability will be intrinsically connected to the adoption of AI in industry at scale. Indeed explanation, which could be used for debugging intelligent systems or deciding to follow a recommendation in real-time, will increase acceptance and (business) user trust. Explainable AI (XAI) is now referring to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. Focus: Thales XAI Platform is designed to provide explanation for a ML task (classification, regression, object detection, segmentation). Although Thales XAI Platform does provide different levels of explanation e.g., example-based, featuresbased, counterfactual using textual and visual representations, we emphasis only on the semantics-based explanation through knowledge graphs. ? Copyright c © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Critical Applications: From adapting a plane trajectory, stopping a train, refitting a boat to reconfiguring a satellite, all are examples of critical situations where explanation is a must-to-have to follow an AI system decision. 2 Why Knowledge Graphs for Explainable AI? State-of-the-Art Limitations: Most approaches limits explanation of ML systems to features involved in the data and model, or at best to examples, prototypes or counterfactuals. Explanation should go beyond correlation (features importance) and numerical similarity (local explanation). Opportunity: By expanding and linking initial (training, validation and test) data with entities in knowledge graphs, (i) context is encoded, (ii) connections and relations are exposed, and (iii) inference and causation are natively supported. Knowledge graphs are used for encoding better representation of data, structuring a ML model in a more interpretable way, and adopt a semantic similarity for local (instance-based) and global (model-based) explanation. 3 Thales XAI Platform: A Knowledge Graph Perspective (Semantic) Perspective: The platform is combining ML and reasoning functionalities to expose a human-like rational as explanation when (i) recognizing an object (in a raw image) of any class in a knowledge graph, (ii) predicting a link in a knowledge graph. Thales XAI Platform is using state-of-the-art Semantic Web tools for enriching input, output (class) data with DBpedia (4, 233, 000 resources) and domain-specific knowledge graphs, usually enterprise knowledge graphs. This is a crucial step for contextualizing training, validation, test data. Explainable ML Classifications: Starting from raw images, as unstructured data, but with class labels augmented with a domain knowledge graph, Thales XAI Platform relies on existing neural network architectures to build the most appropriate models. All confidence scores of output classes on any input image are updated based on the semantic description of the output classes. For instance, an input classified as a car will have a higher overall confidence score in case some properties of car in the knowledge graph are retrieved e.g., having wheels, being on a road. In addition the platform is embedding naturally explanation i.e., properties of the objects retrieved in both the raw data and knowledge graph. Explainable Relational Learning: Starting from relational data, structured as graph, and augmented with a domain knowledge graph, Thales XAI Platform relies on existing knowledge graph embeddings frameworks to build the most appropriate models. Explanation of any link prediction is retrieved by identifying representative hotspots in the knowledge graph i.e., connected parts of the graphs that negatively impact prediction accuracy when removed.","",""
220,"Ian J. Goodfellow, P. Mcdaniel, Nicolas Papernot","Making machine learning robust against adversarial inputs",2018,"","","","",89,"2022-07-13 09:38:01","","10.1145/3134599","","",,,,,220,55.00,73,3,4,"Such inputs distort how machine-learning-based systems are able to function in the world as it is.","",""
349,"Or Biran, Courtenay V. Cotton","Explanation and Justification in Machine Learning : A Survey Or",2017,"","","","",90,"2022-07-13 09:38:01","","","","",,,,,349,69.80,175,2,5,"We present a survey of the research concerning explanation and justification in the Machine Learning literature and several adjacent fields. Within Machine Learning, we differentiate between two main branches of current research: interpretable models, and prediction interpretation and justification.","",""
0,"Mengdi Huai","Fostering Trustworthiness in Machine Learning via Robust and Automated Model Interpretation",2022,"","","","",91,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,1,1,"Machine learning models have been widely applied in real world to build intelligent systems (e.g., selfdriving cars, intelligent recommendation systems, and clinical decision support systems). However, traditional machine learning models mainly focus on optimizing accuracy and efficiency, and they fail to consider how to foster trustworthiness in their design. In practice, machine learning models are suffering a crisis of trust when they are applied in real-world applications due to the lack of transparency behind their behaviors. The concern about the “black box” nature of machine learning models makes decision makers reluctant to trust the predicted results, especially when these models are used for making critical decisions (e.g., medical disease diagnosis). In this talk, I will introduce my research efforts towards the goal of making machine learning trustworthy. Specifically, I will discuss how to foster trustworthiness in machine learning via robust and automated model interpretation. I will first describe my recent research on the security vulnerability of model interpretation methods for deep reinforcement learning (DRL) and introduce two malicious attack frameworks that can significantly alter the interpretation results while incurring minor damage to the performance of the original DRL model. Then, I will present an automated and robust model interpretation framework, which can not only automatically generate the concept-based explanations for the predicted results but also provide certified robustness guarantees for the generated explanations. Bio: Mengdi Huai is a Ph.D. candidate in the Department of Computer Science at the University of Virginia. Her research interests lie in the areas of data mining and machine learning, with a current focus on developing novel techniques to build trustworthy learning systems that are explainable, robust, private, and fair. Mengdi is also interested in designing effective data mining and machine learning algorithms to deal with complex data with both strong empirical performance and theoretical guarantees. Her research work has been published in various top-tier venues, such as KDD, AAAI, IJCAI, NeurIPS, and TKDD. Mengdi received multiple prestigious awards from the University of Virginia for her excellence in research, including the Sture G. Olsson Fellowship in Engineering and the John A. Stankovic Research Award. Her recent work on malicious attacks against model interpretation won the Best Paper Runner-up of KDD2020. Mengdi was selected as one of the Rising Stars in EECS at MIT. She was also selected as one of the Rising Stars in Data Science at UChicago.","",""
19,"D. Gupta, B. B. Hazarika, M. Berlin","Robust regularized extreme learning machine with asymmetric Huber loss function",2020,"","","","",92,"2022-07-13 09:38:01","","10.1007/s00521-020-04741-w","","",,,,,19,9.50,6,3,2,"","",""
1146,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Efficient and Robust Automated Machine Learning",2015,"","","","",93,"2022-07-13 09:38:01","","","","",,,,,1146,163.71,191,6,7,"The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.","",""
154,"Sahil Verma, John P. Dickerson, Keegan E. Hines","Counterfactual Explanations for Machine Learning: A Review",2020,"","","","",94,"2022-07-13 09:38:01","","","","",,,,,154,77.00,51,3,2,"Machine learning plays a role in many deployed decision systems, often in ways that are difficult or impossible to understand by human stakeholders. Explaining, in a human-understandable way, the relationship between the input and output of machine learning models is essential to the development of trustworthy machine-learning-based systems. A burgeoning body of research seeks to define the goals and methods of explainability in machine learning. In this paper, we seek to review and categorize research on counterfactual explanations, a specific class of explanation that provides a link between what could have happened had input to a model been changed in a particular way. Modern approaches to counterfactual explainability in machine learning draw connections to the established legal doctrine in many countries, making them appealing to fielded systems in high-impact areas such as finance and healthcare. Thus, we design a rubric with desirable properties of counterfactual explanation algorithms and comprehensively evaluate all currently-proposed algorithms against that rubric. Our rubric provides easy comparison and comprehension of the advantages and disadvantages of different approaches and serves as an introduction to major research themes in this field. We also identify gaps and discuss promising research directions in the space of counterfactual explainability.","",""
11,"B. Celik, J. Vanschoren","Adaptation Strategies for Automated Machine Learning on Evolving Data",2020,"","","","",95,"2022-07-13 09:38:01","","10.1109/TPAMI.2021.3062900","","",,,,,11,5.50,6,2,2,"Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust to changes in the underlying data. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on a variety of AutoML approaches for building machine learning pipelines, including Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.","",""
64,"W. Samek, G. Montavon, S. Lapuschkin, Christopher J. Anders, K. Müller","Toward Interpretable Machine Learning: Transparent Deep Neural Networks and Beyond",2020,"","","","",96,"2022-07-13 09:38:01","","","","",,,,,64,32.00,13,5,2,"With the broader and highly successful usage of machine learning in industry and the sciences, there has been a growing demand for explainable AI. Interpretability and explanation methods for gaining a better understanding about the problem solving abilities and strategies of nonlinear Machine Learning such as Deep Learning (DL), LSTMs, and kernel methods are therefore receiving increased attention. In this work we aim to (1) provide a timely overview of this active emerging field and explain its theoretical foundations, (2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations, (3) outline best practice aspects i.e. how to best include interpretation methods into the standard usage of machine learning and (4) demonstrate successful usage of explainable AI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of machine learning.","",""
57,"Fumeng Yang, Zhuanyi Huang, J. Scholtz, Dustin L. Arendt","How do visual explanations foster end users' appropriate trust in machine learning?",2020,"","","","",97,"2022-07-13 09:38:01","","10.1145/3377325.3377480","","",,,,,57,28.50,14,4,2,"We investigated the effects of example-based explanations for a machine learning classifier on end users' appropriate trust. We explored the effects of spatial layout and visual representation in an in-person user study with 33 participants. We measured participants' appropriate trust in the classifier, quantified the effects of different spatial layouts and visual representations, and observed changes in users' trust over time. The results show that each explanation improved users' trust in the classifier, and the combination of explanation, human, and classification algorithm yielded much better decisions than the human and classification algorithm separately. Yet these visual explanations lead to different levels of trust and may cause inappropriate trust if an explanation is difficult to understand. Visual representation and performance feedback strongly affect users' trust, and spatial layout shows a moderate effect. Our results do not support that individual differences (e.g., propensity to trust) affect users' trust in the classifier. This work advances the state-of-the-art in trust-able machine learning and informs the design and appropriate use of automated systems.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",98,"2022-07-13 09:38:01","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",99,"2022-07-13 09:38:01","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
2,"Yifeng Gao, Hosein Mohammadi Makrani, Mehrdad Aliasgari, Amin Rezaei, Jessica Lin, H. Homayoun, H. Sayadi","Adaptive-HMD: Accurate and Cost-Efficient Machine Learning-Driven Malware Detection using Microarchitectural Events",2021,"","","","",100,"2022-07-13 09:38:01","","10.1109/IOLTS52814.2021.9486701","","",,,,,2,2.00,0,7,1,"To address the high complexity and computational overheads of conventional software-based detection techniques, Hardware Malware Detection (HMD) has shown promising results as an alternative anomaly detection solution. HMD methods apply Machine Learning (ML) classifiers on microarchitectural events monitored by built-in Hardware Performance Counter (HPC) registers available in modern microprocessors to recognize the patterns of anomalies (e.g., signatures of malicious applications). Existing hardware malware detection solutions have mainly focused on utilizing standard ML algorithms to detect the existence of malware without considering an adaptive and cost-efficient approach for online malware detection. Our comprehensive analysis across a wide range of malicious software applications and different branches of machine learning algorithms indicates that the type of adopted ML algorithm to detect malicious applications at the hardware level highly correlates with the type of the examined malware, and the ultimate performance evaluation metric (F-measure, robustness, latency, detection rate/cost, etc.) to select the most efficient ML model for distinguishing the target malware from benign program. Therefore, in this work we propose Adaptive-HMD, an accurate and cost-efficient machine learning-driven framework for online malware detection using low-level microarchitectural events collected from HPC registers. Adaptive-HMD is equipped with a lightweight tree-based decision-making algorithm that accurately selects the most efficient ML model to be used for the inference in online malware detection according to the users' preference and optimal performance vs. cost (hardware overhead and latency) criteria. The experimental results demonstrate that Adaptive-HMD achieves up to 94% detection rate (F-measure) while improving the cost-efficiency of ML-based malware detection by more than 5X as compared to existing ensemble-based malware detection methods.","",""
28,"Ricards Marcinkevics, Julia E. Vogt","Interpretability and Explainability: A Machine Learning Zoo Mini-tour",2020,"","","","",101,"2022-07-13 09:38:01","","10.3929/ETHZ-B-000454597","","",,,,,28,14.00,14,2,2,"In this review, we examine the problem of designing interpretable and explainable machine learning models. Interpretability and explainability lie at the core of many machine learning and statistical applications in medicine, economics, law, and natural sciences. Although interpretability and explainability have escaped a clear universal definition, many techniques motivated by these properties have been developed over the recent 30 years with the focus currently shifting towards deep learning methods. In this review, we emphasise the divide between interpretability and explainability and illustrate these two different research directions with concrete examples of the state-of-the-art. The review is intended for a general machine learning audience with interest in exploring the problems of interpretation and explanation beyond logistic regression or random forest variable importance. This work is not an exhaustive literature survey, but rather a primer focusing selectively on certain lines of research which the authors found interesting or informative.","",""
41,"Ashraf Abdul, C. von der Weth, Mohan S. Kankanhalli, Brian Y. Lim","COGAM: Measuring and Moderating Cognitive Load in Machine Learning Model Explanations",2020,"","","","",102,"2022-07-13 09:38:01","","10.1145/3313831.3376615","","",,,,,41,20.50,10,4,2,"Interpretable machine learning models trade -off accuracy for simplicity to make explanations more readable and easier to comprehend. Drawing from cognitive psychology theories in graph comprehension, we formalize readability as visual cognitive chunks to measure and moderate the cognitive load in explanation visualizations. We present Cognitive-GAM (COGAM) to generate explanations with desired cognitive load and accuracy by combining the expressive nonlinear generalized additive models (GAM) with simpler sparse linear models. We calibrated visual cognitive chunks with reading time in a user study, characterized the trade-off between cognitive load and accuracy for four datasets in simulation studies, and evaluated COGAM against baselines with users. We found that COGAM can decrease cognitive load without decreasing accuracy and/or increase accuracy without increasing cognitive load. Our framework and empirical measurement instruments for cognitive load will enable more rigorous assessment of the human interpretability of explainable AI.","",""
44,"Min Zhang, Haoxuan You, Pranav Kadam, Shan Liu, C.-C. Jay Kuo","PointHop: An Explainable Machine Learning Method for Point Cloud Classification",2019,"","","","",103,"2022-07-13 09:38:01","","10.1109/TMM.2019.2963592","","",,,,,44,14.67,9,5,3,"An explainable machine learning method for point cloud classification, called the PointHop method, is proposed in this work. The PointHop method consists of two stages: 1) local-to-global attribute building through iterative one-hop information exchange and 2) classification and ensembles. In the attribute building stage, we address the problem of unordered point cloud data using a space partitioning procedure and developing a robust descriptor that characterizes the relationship between a point and its one-hop neighbor in a PointHop unit. When we put multiple PointHop units in cascade, the attributes of a point will grow by taking its relationship with one-hop neighbor points into account iteratively. Furthermore, to control the rapid dimension growth of the attribute vector associated with a point, we use the Saab transform to reduce the attribute dimension in each PointHop unit. In the classification and ensemble stage, we feed the feature vector obtained from multiple PointHop units to a classifier. We explore ensemble methods to improve the classification performance furthermore. It is shown by experimental results that the PointHop method offers classification performance that is comparable with state-of-the-art methods while demanding much lower training complexity.","",""
3,"D. Rengasamy, Benjamin Rothwell, G. Figueredo","Towards a More Reliable Interpretation of Machine Learning Outputs for Safety-Critical Systems using Feature Importance Fusion",2020,"","","","",104,"2022-07-13 09:38:01","","10.3390/app112411854","","",,,,,3,1.50,1,3,2,"When machine learning supports decision-making in safety-critical systems, it is important to verify and understand the reasons why a particular output is produced. Although feature importance calculation approaches assist in interpretation, there is a lack of consensus regarding how features’ importance is quantified, which makes the explanations offered for the outcomes mostly unreliable. A possible solution to address the lack of agreement is to combine the results from multiple feature importance quantifiers to reduce the variance in estimates and to improve the quality of explanations. Our hypothesis is that this leads to more robust and trustworthy explanations of the contribution of each feature to machine learning predictions. To test this hypothesis, we propose an extensible model-agnostic framework divided in four main parts: (i) traditional data pre-processing and preparation for predictive machine learning models, (ii) predictive machine learning, (iii) feature importance quantification, and (iv) feature importance decision fusion using an ensemble strategy. Our approach is tested on synthetic data, where the ground truth is known. We compare different fusion approaches and their results for both training and test sets. We also investigate how different characteristics within the datasets affect the quality of the feature importance ensembles studied. The results show that, overall, our feature importance ensemble framework produces 15% less feature importance errors compared with existing methods. Additionally, the results reveal that different levels of noise in the datasets do not affect the feature importance ensembles’ ability to accurately quantify feature importance, whereas the feature importance quantification error increases with the number of features and number of orthogonal informative features. We also discuss the implications of our findings on the quality of explanations provided to safety-critical systems.","",""
2,"Artur Movsessian, D. Cava, D. Tcherniak","Interpretable machine learning in damage detection using Shapley Additive Explanations",2021,"","","","",105,"2022-07-13 09:38:01","","10.31224/osf.io/96yf5","","",,,,,2,2.00,1,3,1,"In recent years, Machine Learning (ML) techniques have gained popularity in Structural Health Monitoring (SHM). These have been particularly used for damage detection in a wide range of engineering applications such as wind turbine blades. The outcomes of previous research studies in this area have demonstrated the capabilities of ML for robust damage detection. However, the primary challenge facing ML in SHM is the lack of interpretability of the prediction models hindering the broader implementation of these techniques. For this purpose, this study integrates the novel Shapley Additive exPlanations (SHAP) method into a ML-based damage detection process as a tool for introducing interpretability and, thus, build evidence for reliable decision-making in SHM applications. The SHAP method is based on coalitional game theory and adds global and local interpretability to ML-based models by computing the marginal contribution of each feature. The contribution is used to understand the nature of damage indices (DIs). The applicability of the SHAP method is first demonstrated on a simple lumped mass-spring-damper system with simulated temperature variabilities. Later, the SHAP method has been evaluated on data from an in-operation V27 wind turbine with artificially introduced damage in one of its blades. The results show the relationship between the environmental and operational variabilities (EOVs) and their direct influence on the damage indices. This ultimately helps to understand the difference between false positives caused by EOVs and true positives resulting from damage in the structure.","",""
1,"Anupam Datta, Matt Fredrikson, Klas Leino, Kaiji Lu, S. Sen, Zifan Wang","Machine Learning Explainability and Robustness: Connected at the Hip",2021,"","","","",106,"2022-07-13 09:38:01","","10.1145/3447548.3470806","","",,,,,1,1.00,0,6,1,"This tutorial examines the synergistic relationship between explainability methods for machine learning and a significant problem related to model quality: robustness against adversarial perturbations. We begin with a broad overview of approaches to explainable AI, before narrowing our focus to post-hoc explanation methods for predictive models. We discuss perspectives on what constitutes a ""good'' explanation in various settings, with an emphasis on axiomatic justifications for various explanation methods. In doing so, we will highlight the importance of an explanation method's faithfulness to the target model, as this property allows one to distinguish between explanations that are unintelligible because of the method used to produce them, and cases where a seemingly poor explanation points to model quality issues. Next, we introduce concepts surrounding adversarial robustness, including adversarial attacks as well as a range of corresponding state-of-the-art defenses. Finally, building on the knowledge presented thus far, we present key insights from the recent literature on the connections between explainability and robustness, showing that many commonly-perceived explainability issues may be caused by non-robust model behavior. Accordingly, a careful study of adversarial examples and robustness can lead to models whose explanations better appeal to human intuition and domain knowledge.","",""
2,"Sarath Shekkizhar, Antonio Ortega","Revisiting Local Neighborhood Methods in Machine Learning",2021,"","","","",107,"2022-07-13 09:38:01","","10.1109/DSLW51110.2021.9523409","","",,,,,2,2.00,1,2,1,"Several machine learning methods leverage the idea of locality by using k-nearest neighbor (KNN) techniques to design better pattern recognition models. However, the choice of KNN parameters such as k is often made experimentally, e.g., via cross-validation, leading to local neighborhoods without a clear geometric interpretation. In this paper, we replace KNN with our recently introduced polytope neighborhood scheme - Non Negative Kernel regression (NNK). NNK formulates neighborhood selection as a sparse signal approximation problem and is adaptive to the local distribution of samples in the neighborhood of the data point of interest. We analyze the benefits of local neighborhood construction based on NNK. In particular, we study the generalization properties of local interpolation using NNK and present data dependent bounds in the non asymptotic setting. The applicability of NNK in transductive few shot learning setting and for measuring distance between two datasets is demonstrated. NNK exhibits robust, superior performance in comparison to standard locally weighted neighborhood methods.","",""
0,"Yan Zhou, Murat Kantarcioglu, B. Xi","A Game Theoretic Perspective on Adversarial Machine Learning and Related Cybersecurity Applications",2021,"","","","",108,"2022-07-13 09:38:01","","10.1002/9781119723950.ch13","","",,,,,0,0.00,0,3,1,"In cybersecurity applications where machine learning algorithms are increasingly used to detect vulnerabilities, a somewhat unique challenge arises as exploits targeting machine learning models are constantly devised by the attackers. Traditional machine learning models are no longer robust and reliable when they are under attack. The action and reaction between machine learning systems and the adversary can be modeled as a game between two or more players. Under well‐defined attack models, game theory can provide robustness guarantee for machine learning models that are otherwise vulnerable to application‐time data corruption. We review two cases of game theory‐based machine learning techniques: in one case, players play a zero sum game by following a minimax strategy, while in the other case, players play a sequential game with one player as the leader and the rest as the followers. Experimental results on e‐mail spam and web spam datasets are presented. In the zero sum game, we demonstrate that an adversarial SVM model built upon the minimax strategy is much more resilient to adversarial attacks than standard SVM and one‐class SVM models. We also show that optimal learning strategies derived to counter overly pessimistic attack models can produce unsatisfactory results when the real attacks are much weaker. In the sequential game, we demonstrate that the mixed strategy, allowing a player to randomize over available strategies, is the best solution in general without knowing what types of adversaries machine learning applications are facing in the wild. We also discuss scenarios where players' behavior may derail rational decision making and models that consider such decision risks.","",""
0,"P. Rasouli, Ingrid Chieh Yu","Explainable Debugger for Black-box Machine Learning Models",2021,"","","","",109,"2022-07-13 09:38:01","","10.1109/IJCNN52387.2021.9533944","","",,,,,0,0.00,0,2,1,"The research around developing methods for debugging and refining Machine Learning (ML) models is still in its infancy. We believe employing tailored tools in the development process can help developers in creating more trustworthy and reliable models. This is particularly essential for creating black-box models such as deep neural networks and random forests, as their opaque decision-making and complex structure prevent detailed investigations. Although many explanation techniques provide interpretability in terms of predictive features for a mispredicted instance, it would be beneficial for a developer to find a partition of the training data that significantly influences the anomaly. Such responsible partitions can be subjected to data visualization and data engineering in the development phase to improve the model's accuracy. In this paper, we propose a systematic debugging framework for the development of ML models that guides the data engineering process using the model's decision boundary. Our approach finds the influential neighborhood of anomalous data points using observation-level feature importance and explains them via a novel quasi-global explanation technique. It is also equipped with a robust global explanation approach to reveal general trends and expose potential biases in the neighborhoods. We demonstrate the efficacy of the devised framework through several experiments on standard data sets and black-box models and propose various guidelines on how the framework's components can be practically useful from a developer's perspective.","",""
0,"Wei Chen, Xiangkui Li, Lu Ma, Dong Li","Enhancing Robustness of Machine Learning Integration With Routine Laboratory Blood Tests to Predict Inpatient Mortality After Intracerebral Hemorrhage",2022,"","","","",110,"2022-07-13 09:38:01","","10.3389/fneur.2021.790682","","",,,,,0,0.00,0,4,1,"Objective: The accurate evaluation of outcomes at a personalized level in patients with intracerebral hemorrhage (ICH) is critical clinical implications. This study aims to evaluate how machine learning integrates with routine laboratory tests and electronic health records (EHRs) data to predict inpatient mortality after ICH. Methods: In this machine learning-based prognostic study, we included 1,835 consecutive patients with acute ICH between October 2010 and December 2018. The model building process incorporated five pre-implant ICH score variables (clinical features) and 13 out of 59 available routine laboratory parameters. We assessed model performance according to a range of learning metrics, such as the mean area under the receiver operating characteristic curve [AUROC]. We also used the Shapley additive explanation algorithm to explain the prediction model. Results: Machine learning models using laboratory data achieved AUROCs of 0.71–0.82 in a split-by-year development/testing scheme. The non-linear eXtreme Gradient Boosting model yielded the highest prediction accuracy. In the held-out validation set of development cohort, the predictive model using comprehensive clinical and laboratory parameters outperformed those using clinical alone in predicting in-hospital mortality (AUROC [95% bootstrap confidence interval], 0.899 [0.897–0.901] vs. 0.875 [0.872–0.877]; P <0.001), with over 81% accuracy, sensitivity, and specificity. We observed similar performance in the testing set. Conclusions: Machine learning integrated with routine laboratory tests and EHRs could significantly promote the accuracy of inpatient ICH mortality prediction. This multidimensional composite prediction strategy might become an intelligent assistive prediction for ICH risk reclassification and offer an example for precision medicine.","",""
0,"Yinyihong Liu","Airbnb Pricing Based on Statistical Machine Learning Models",2021,"","","","",111,"2022-07-13 09:38:01","","10.1109/CONF-SPML54095.2021.00042","","",,,,,0,0.00,0,1,1,"Being one of the largest online accommodation booking platforms, Airbnb has many hosts who are seeking for more proper prices to increase their booking rate. To develop a good pricing prediction model, this paper has employed machine learning models including KNN, MLR, LASSO regression, Ridge regression, Random Forest, Gradient Boosting and XGBoost etc. While past studies on Airbnb pricing have applied quantitative pricing, some face the problems that the models are not robust enough and some face the problem of not training the model plentily. To fill this gap, we give careful consideration in exploratory data analysis to make the dataset more reasonable, apply many robust models ranging from regularized regression to ensemble models and use cross validation and random search to tune each parameter in each model. In this way, we not only select XGBoost as the best model for price prediction with R2 score 0.6321, but also uncover the features which have statistical significance with the target price.","",""
0,"Yuchao Chen, Qian Huang, Jiannan Zhao, X. Hu","Unsupervised Machine Learning on Domes in the Lunar Gardner Region: Implications for Dome Classification and Local Magmatic Activities on the Moon",2021,"","","","",112,"2022-07-13 09:38:01","","10.3390/rs13050845","","",,,,,0,0.00,0,4,1,"Lunar volcanic domes are essential windows into the local magmatic activities on the Moon. Classification of domes is a useful way to figure out the relationship between dome appearances and formation processes. Previous studies of dome classification were manually or semi-automatically carried out either qualitatively or quantitively. We applied an unsupervised machine-learning method to domes that are annularly or radially distributed around Gardner, a unique central-vent volcano located in the northern part of the Mare Tranquillitatis. High-resolution lunar imaging and spectral data were used to extract morphometric and spectral properties of domes in both the Gardner volcano and its surrounding region in the Mare Tranquillitatis. An integrated robust Fuzzy C-Means clustering algorithm was performed on 120 combinations of five morphometric (diameter, area, height, surface volume, and slope) and two elemental features (FeO and TiO2 contents) to find the optimum combination. Rheological features of domes and their dike formation parameters were calculated for dome-forming lava explanations. Results show that diameter, area, surface volume, and slope are the selected optimum features for dome clustering. 54 studied domes can be grouped into four dome clusters (DC1 to DC4). DC1 domes are relatively small, steep, and close to the Gardner volcano, with forming lavas of high viscosities and low effusion rates, representing the latest Eratosthenian dome formation stage of the Gardner volcano. Domes of DC2 to DC4 are relatively large, smooth, and widely distributed, with forming lavas of low viscosities and high effusion rates, representing magmatic activities varying from Imbrian to Eratosthenian in the northern Mare Tranquillitatis. The integrated algorithm provides a new and independent way to figure out the representative properties of lunar domes and helps us further clarify the relationship between dome clusters and local magma activities of the Moon.","",""
0,"Uma Gunasilan","Debate as a learning activity for teaching programming: a case in the subject of machine learning",2021,"","","","",113,"2022-07-13 09:38:01","","10.1108/heswbl-01-2021-0006","","",,,,,0,0.00,0,1,1,"PurposeDebates are well known to encompass a variety of skills we would like higher education candidates to embody when they graduate.Design/methodology/approachDebates in a classroom with computer science as the main subject has been popular in high schools particularly with emerging issues around the area, however it does not have as an extensive similar documented outreach in tertiary education, particularly in the area of hard computer sciences and more recent concentrations of computer science, such as machine learning, artificial intelligence and cloud computing.FindingsTo explore further, the debate dataset had more methodologies applied and was split into training and testing sets, whose results were then compared by a standardized measure: Root Mean Square Error (RMSE) which is currently standard in the industry. The rationale of the approach is to quantify that debate activities have an immensely positive impact towards both the teaching and learning in technical subjects and needs to be more often and robustly used within higher education.Originality/valueThe rationale of the approach is that classroom debate activities equip students with verbal and social learning styles and an opportunity to engage with content in a way that is more comfortable than working with traditional lecture-and-laboratory style learning.","",""
0,"N. Brandenstein","Going beyond simplicity: Using machine learning to predict belief in conspiracy theories",2021,"","","","",114,"2022-07-13 09:38:01","","10.31234/OSF.IO/FJ3MZ","","",,,,,0,0.00,0,1,1,"Public and scientific interest in why people believe in conspiracy theories (CT) surged in the past years. To come up with a theoretical explanation, researchers investigated relationships of CT belief with psychological factors such as political attitudes, emotions or personality (van Prooijen & Douglas, 2018). However, recent studies put the robustness of these relationships into question (e.g., Stojanov & Halberstadt, 2020). In this study, the analysis of a representative dataset with 2025 adults uncovered that the simplicity of the current analysis routine, exhibiting high sample-specificity and neglecting complex associations of psychological factors and belief in CTs, may obscure these relationships. Further, poor replicability of CT belief associations can be detected and remedied by using a prediction-based modeling approach and machine learning models, which proposes a timely shift in the field’s analysis routine. Conceptual and theoretical implications for CT belief research and theory building are derived.","",""
242,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",115,"2022-07-13 09:38:01","","10.1145/3308809.3308857","","",,,,,242,48.40,81,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server andm working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of them working machines suffer Byzantine faults - a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.","",""
0,"T. Martin, S. Areibi, G. Grewal","Effective Machine-Learning Models for Predicting Routability During FPGA Placement",2021,"","","","",116,"2022-07-13 09:38:01","","10.1109/MLCAD52597.2021.9531243","","",,,,,0,0.00,0,3,1,"The ability to efficiently and accurately predict placement routability, while avoiding the large computational cost of performing routing, is an asset when seeking to reduce total placement and routing runtime. In this paper, we present a series of simple ML models and ensembles to predict the routability of a placement solution. Ensembles based on Bagging, Boosting and Stack of classifiers are introduced to produce more accurate and robust solutions than single/simple models. Our results show an improvement in prediction accuracy and runtime compared to the best published results in the literature.","",""
1973,"Finale Doshi-Velez, Been Kim","Towards A Rigorous Science of Interpretable Machine Learning",2017,"","","","",117,"2022-07-13 09:38:01","","","","",,,,,1973,394.60,987,2,5,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.","",""
13,"Dennis Collaris, J. V. Wijk","ExplainExplore: Visual Exploration of Machine Learning Explanations",2020,"","","","",118,"2022-07-13 09:38:01","","10.1109/PacificVis48177.2020.7090","","",,,,,13,6.50,7,2,2,"Machine learning models often exhibit complex behavior that is difficult to understand. Recent research in explainable AI has produced promising techniques to explain the inner workings of such models using feature contribution vectors. These vectors are helpful in a wide variety of applications. However, there are many parameters involved in this process and determining which settings are best is difficult due to the subjective nature of evaluating interpretability. To this end, we introduce EXPLAINEXPLORE: an interactive explanation system to explore explanations that fit the subjective preference of data scientists. We leverage the domain knowledge of the data scientist to find optimal parameter settings and instance perturbations, and enable the discussion of the model and its explanation with domain experts. We present a use case on a real-world dataset to demonstrate the effectiveness of our approach for the exploration and tuning of machine learning explanations.","",""
85,"P. Graff, F. Feroz, M. Hobson, A. Lasenby","SKYNET: an efficient and robust neural network training tool for machine learning in astronomy",2013,"","","","",119,"2022-07-13 09:38:01","","10.1093/mnras/stu642","","",,,,,85,9.44,21,4,9,"We present the first public release of our generic neural network training algorithm, called SKYNET. This efficient and robust machine-learning tool is able to train large and deep feedforward neural networks, including autoencoders, for use in a wide range of supervised and unsupervised learning applications, such as regression, classification, density estimation, clustering and dimensionality reduction. SKYNET uses a powerful ‘pre-training’ method, to obtain a set of network parameters close to the true global maximum of the training objective function, followed by further optimisation using an automatically-regularised variant of Newton’s method; the latter uses second-order derivative information to improve convergence, but without the need to evaluate or store the full Hessian matrix, by using a fast approximate method to calculate Hessian-vector products. This combination of methods allows for the training of complicated networks that are difficult to optimise using standard backpropagation techniques. SKYNET employs convergence criteria that naturally prevent overfitting, and also includes a fast algorithm for estimating the accuracy of network outputs. The utility and flexibility of SKYNET are demonstrated by application to a number of toy problems, and to astronomical problems focusing on the recovery of structure from blurred and noisy images, the identification of gamma-ray bursters, and the compression and denoising of galaxy images. The SKYNET software, which is implemented in standard ANSI C and fully parallelised using MPI, is available at http://www.mrao.cam.ac.uk/software/skynet/.","",""
404,"D. V. Carvalho, E. M. Pereira, Jaime S. Cardoso","Machine Learning Interpretability: A Survey on Methods and Metrics",2019,"","","","",120,"2022-07-13 09:38:01","","10.3390/ELECTRONICS8080832","","",,,,,404,134.67,135,3,3,"Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.","",""
5,"W. Tsai, K. Fang, X. Ji, Kathryn Lawson, Chaopeng Shen","Revealing Causal Controls of Storage-Streamflow Relationships With a Data-Centric Bayesian Framework Combining Machine Learning and Process-Based Modeling",2020,"","","","",121,"2022-07-13 09:38:01","","10.3389/frwa.2020.583000","","",,,,,5,2.50,1,5,2,"Some machine learning (ML) methods such as classification trees are useful tools to generate hypotheses about how hydrologic systems function. However, data limitations dictate that ML alone often cannot differentiate between causal and associative relationships. For example, previous ML analysis suggested that soil thickness is the key physiographic factor determining the storage-streamflow correlations in the eastern US. This conclusion is not robust, especially if data are perturbed, and there were alternative, competing explanations including soil texture and terrain slope. However, typical causal analysis based on process-based models (PBMs) is inefficient and susceptible to human bias. Here we demonstrate a more efficient and objective analysis procedure where ML is first applied to generate data-consistent hypotheses, and then a PBM is invoked to verify these hypotheses. We employed a surface-subsurface processes model and conducted perturbation experiments to implement these competing hypotheses and assess the impacts of the changes. The experimental results strongly support the soil thickness hypothesis as opposed to the terrain slope and soil texture ones, which are co-varying and coincidental factors. Thicker soil permits larger saturation excess and longer system memory that carries wet season water storage to influence dry season baseflows. We further suggest this analysis could be formulated into a data-centric Bayesian framework. This study demonstrates that PBM present indispensable value for problems that ML cannot solve alone, and is meant to encourage more synergies between ML and PBM in the future.","",""
0,"Ali Farzane, M. Akbarzadeh, Reza Ferdousi, M. Rashidi, R. Safdari","Potential biomarker detection for liver cancer stem cell by machine learning approach",2020,"","","","",122,"2022-07-13 09:38:01","","10.22317/JCMS.V6I6.898","","",,,,,0,0.00,0,5,2,"Objectives: In this study, we aimed to identify putative biomarkers for identification and characterization of these cells in liver cancer.  Methods: We employed a supervised machine learning method, XGBoost, to data from 13 GEO data series to classify samples using gene expression data.  Results.  Across the 376 samples (129 CSCs and 247 non-CSCs cases), XGBoost displayed high performance in the classification of data. XGBoost feature importance scores and SHAP (Shapley Additive explanation) values were used for the interpretation of results and analysis of individual gene importance. We confirmed that expression levels of a 10-gene set (PTGER3, AURKB, C15orf40, IDI2, OR8D1, NACA2, SERPINB6, L1CAM, SMC1A, and RASGRF1) were predictive. The results showed that these 10 genes can detect CSCs robustly with accuracy, sensitivity, and specificity of 97 %, 100 %, and 95 %, respectively.  Conclusions. We suggest that the ten-gene set may be used as a biomarker set for detecting and characterizing CSCs using gene expression data.","",""
0,"M. Hind, Dennis Wei, Yunfeng Zhang","Consumer-Driven Explanations for Machine Learning Decisions: An Empirical Study of Robustness",2020,"","","","",123,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,3,2,"Many proposed methods for explaining machine learning predictions are in fact challenging to understand for nontechnical consumers. This paper builds upon an alternative consumer-driven approach called TED that asks for explanations to be provided in training data, along with target labels. Using semi-synthetic data from credit approval and employee retention applications, experiments are conducted to investigate some practical considerations with TED, including its performance with different classification algorithms, varying numbers of explanations, and variability in explanations. A new algorithm is proposed to handle the case where some training examples do not have explanations. Our results show that TED is robust to increasing numbers of explanations, noisy explanations, and large fractions of missing explanations, thus making advances toward its practical deployment.","",""
1920,"A. Kurakin, Ian J. Goodfellow, Samy Bengio","Adversarial Machine Learning at Scale",2016,"","","","",124,"2022-07-13 09:38:01","","","","",,,,,1920,320.00,640,3,6,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.","",""
2,"S. Siltanen, Takanori Ide","Electrical Impedance Tomography, Enclosure Method and Machine Learning",2020,"","","","",125,"2022-07-13 09:38:01","","10.1109/MLSP49062.2020.9231717","","",,,,,2,1.00,1,2,2,"Electrical impedance tomography (EIT) is a non-destructive imaging method, where a physical body is probed with electric measurements at the boundary, and information about the internal conductivity is extracted from the data. The enclosure method of Ikehata [J. Inv. III-Posed Prob. 8(2000)] recovers the convex hull of an inclusion of unknown conductivity embedded in known background conductivity. Practical implementations of the enclosure method are based on least-squares (LS) fitting of lines to noise-robust values of the so-called indicator function. It is shown how a convolutional neural network instead of LS fitting improves the accuracy of the enclosure method significantly while retaining interpretability.","",""
0,"D. Efremenko, Himani Jain, Jian Xu","Two Machine Learning Based Schemes for Solving Direct and Inverse Problems of Radiative Transfer Theory",2020,"","","","",126,"2022-07-13 09:38:01","","10.51130/graphicon-2020-2-3-45","","",,,,,0,0.00,0,3,2,"Artificial neural networks (ANNs) are used to substitute computationally expensive radiative transfer models (RTMs) and inverse operators (IO) for retrieving optical parameters of the medium. However, the direct parametrization of RTMs and IOs by means of ANNs has certain drawbacks, such as loss of generality, computations of huge training datasets, robustness issues etc. This paper provides an analysis of different ANN-related methods, based on our results and those published by other authors. In particular, two techniques are proposed. In the first method, the ANN substitutes the eigenvalue solver in the discrete ordinate RTM, thereby reducing the computational time. Unlike classical RTM parametrization schemes based on ANN, in this method the resulting ANN can be used for arbitrary geometry and layer optical thicknesses. In the second method, the IO is trained by using the real measurements (preprocessed Level-2 TROPOMI data) to improve the stability of the inverse operator. This method provides robust results even without applying the Tikhonov regularization method.","",""
123,"Jiawei Zhang, Yang Wang, Piero Molino, Lezhi Li, D. Ebert","Manifold: A Model-Agnostic Framework for Interpretation and Diagnosis of Machine Learning Models",2018,"","","","",127,"2022-07-13 09:38:01","","10.1109/TVCG.2018.2864499","","",,,,,123,30.75,25,5,4,"Interpretation and diagnosis of machine learning models have gained renewed interest in recent years with breakthroughs in new approaches. We present Manifold, a framework that utilizes visual analysis techniques to support interpretation, debugging, and comparison of machine learning models in a more transparent and interactive manner. Conventional techniques usually focus on visualizing the internal logic of a specific model type (i.e., deep neural networks), lacking the ability to extend to a more complex scenario where different model types are integrated. To this end, Manifold is designed as a generic framework that does not rely on or access the internal logic of the model and solely observes the input (i.e., instances or features) and the output (i.e., the predicted result and probability distribution). We describe the workflow of Manifold as an iterative process consisting of three major phases that are commonly involved in the model development and diagnosis process: inspection (hypothesis), explanation (reasoning), and refinement (verification). The visual components supporting these tasks include a scatterplot-based visual summary that overviews the models' outcome and a customizable tabular view that reveals feature discrimination. We demonstrate current applications of the framework on the classification and regression tasks and discuss other potential machine learning use scenarios where Manifold can be applied.","",""
657,"Wieland Brendel, Jonas Rauber, M. Bethge","Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",2017,"","","","",128,"2022-07-13 09:38:01","","","","",,,,,657,131.40,219,3,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .","",""
41,"B. Dimanov, Umang Bhatt, M. Jamnik, Adrian Weller","You Shouldn't Trust Me: Learning Models Which Conceal Unfairness From Multiple Explanation Methods",2020,"","","","",129,"2022-07-13 09:38:01","","10.3233/FAIA200380","","",,,,,41,20.50,10,4,2,"Transparency of algorithmic systems has been discussed as a way for end-users and regulators to develop appropriate trust in machine learning models. One popular approach, LIME [26], even suggests that model explanations can answer the question “Why should I trust you?” Here we show a straightforward method for modifying a pre-trained model to manipulate the output of many popular feature importance explanation methods with little change in accuracy, thus demonstrating the danger of trusting such explanation methods. We show how this explanation attack can mask a model’s discriminatory use of a sensitive feature, raising strong concerns about using such explanation methods to check model fairness.","",""
243,"Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, R. Puri, J. Moura, P. Eckersley","Explainable machine learning in deployment",2019,"","","","",130,"2022-07-13 09:38:01","","10.1145/3351095.3375624","","",,,,,243,81.00,24,10,3,"Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.","",""
0,"Zhang Jing, Ren Yong-gong","Robust Multi-feature Extreme Learning Machine",2017,"","","","",131,"2022-07-13 09:38:01","","10.1007/978-3-030-01520-6_13","","",,,,,0,0.00,0,2,5,"","",""
37,"Radwa El Shawi, Youssef Mohamed, M. Al-mallah, S. Sakr","Interpretability in HealthCare A Comparative Study of Local Machine Learning Interpretability Techniques",2019,"","","","",132,"2022-07-13 09:38:01","","10.1109/CBMS.2019.00065","","",,,,,37,12.33,9,4,3,"Although complex machine learning models (e.g., Random Forest, Neural Networks) are commonly outperforming the traditional simple interpretable models (e.g., Linear Regression, Decision Tree), in the healthcare domain, clinicians find it hard to understand and trust these complex models due to the lack of intuition and explanation of their predictions. With the new General Data Protection Regulation (GDPR), the importance for plausibility and verifiability of the predictions made by machine learning models has become essential. To tackle this challenge, recently, several machine learning interpretability techniques have been developed and introduced. In general, the main aim of these interpretability techniques is to shed light and provide insights into the predictions process of the machine learning models and explain how the model predictions have resulted. However, in practice, assessing the quality of the explanations provided by the various interpretability techniques is still questionable. In this paper, we present a comprehensive experimental evaluation of three recent and popular local model agnostic interpretability techniques, namely, LIME, SHAP and Anchors on different types of real-world healthcare data. Our experimental evaluation covers different aspects for its comparison including identity, stability, separability, similarity, execution time and bias detection. The results of our experiments show that LIME achieves the lowest performance for the identity metric and the highest performance for the separability metric across all datasets included in this study. On average, SHAP has the smallest average time to output explanation across all datasets included in this study. For detecting the bias, SHAP enables the participants to better detect the bias.","",""
27,"Alexander Warnecke, Dan Arp, Christian Wressnegger, K. Rieck","Evaluating Explanation Methods for Deep Learning in Security",2019,"","","","",133,"2022-07-13 09:38:01","","10.1109/EuroSP48549.2020.00018","","",,,,,27,9.00,7,4,3,"Deep learning is increasingly used as a building block of security systems. Unfortunately, neural networks are hard to interpret and typically opaque to the practitioner. The machine learning community has started to address this problem by developing methods for explaining the predictions of neural networks. While several of these approaches have been successfully applied in the area of computer vision, their application in security has received little attention so far. It is an open question which explanation methods are appropriate for computer security and what requirements they need to satisfy. In this paper, we introduce criteria for comparing and evaluating explanation methods in the context of computer security. These cover general properties, such as the accuracy of explanations, as well as security-focused aspects, such as the completeness, efficiency, and robustness. Based on our criteria, we investigate six popular explanation methods and assess their utility in security systems for malware detection and vulnerability discovery. We observe significant differences between the methods and build on these to derive general recommendations for selecting and applying explanation methods in computer security.","",""
76,"Stefano Teso, K. Kersting","Explanatory Interactive Machine Learning",2019,"","","","",134,"2022-07-13 09:38:01","","10.1145/3306618.3314293","","",,,,,76,25.33,38,2,3,"Although interactive learning puts the user into the loop, the learner remains mostly a black box for the user. Understanding the reasons behind predictions and queries is important when assessing how the learner works and, in turn, trust. Consequently, we propose the novel framework of explanatory interactive learning where, in each step, the learner explains its query to the user, and the user interacts by both answering the query and correcting the explanation. We demonstrate that this can boost the predictive and explanatory powers of, and the trust into, the learned model, using text (e.g. SVMs) and image classification (e.g. neural networks) experiments as well as a user study.","",""
27,"Nina Narodytska, Aditya A. Shrotri, Kuldeep S. Meel, Alexey Ignatiev, Joao Marques-Silva","Assessing Heuristic Machine Learning Explanations with Model Counting",2019,"","","","",135,"2022-07-13 09:38:01","","10.1007/978-3-030-24258-9_19","","",,,,,27,9.00,5,5,3,"","",""
6,"Rudrasis Chakraborty, Liu Yang, Søren Hauberg, B. Vemuri","Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear Subspace Learning",2017,"","","","",136,"2022-07-13 09:38:01","","10.1109/tpami.2020.2992392","","",,,,,6,1.20,2,4,5,"Principal component analysis (PCA) and Kernel principal component analysis (KPCA) are fundamental methods in machine learning for dimensionality reduction. The former is a technique for finding this approximation in finite dimensions and the latter is often in an infinite dimensional reproducing Kernel Hilbert-space (RKHS). In this paper, we present a geometric framework for computing the principal linear subspaces in both (finite and infinite) situations as well as for the robust PCA case, that amounts to computing the intrinsic average on the space of all subspaces: the Grassmann manifold. Points on this manifold are defined as the subspaces spanned by <inline-formula><tex-math notation=""LaTeX"">$K$</tex-math><alternatives><mml:math><mml:mi>K</mml:mi></mml:math><inline-graphic xlink:href=""chakraborty-ieq1-2992392.gif""/></alternatives></inline-formula>-tuples of observations. The intrinsic Grassmann average of these subspaces are shown to coincide with the principal components of the observations when they are drawn from a Gaussian distribution. We show similar results in the RKHS case and provide an efficient algorithm for computing the projection onto the this average subspace. The result is a method akin to KPCA which is substantially faster. Further, we present a novel online version of the KPCA using our geometric framework. Competitive performance of all our algorithms are demonstrated on a variety of real and synthetic data sets.","",""
5,"Gideon A. Lyngdoh, Mohd Zaki, N. Krishnan, Sumanta","Prediction of Concrete Strengths Enabled by Missing Data Imputation and Interpretable Machine Learning",2022,"","","","",137,"2022-07-13 09:38:01","","","","",,,,,5,5.00,1,4,1,"Machine learning (ML)-based prediction of non-linear composition-strength relationship in concretes requires a large, complete, and consistent dataset. However, the availability of such datasets is limited as the datasets often suffer from incompleteness because of missing data corresponding to different input features, which makes the development of robust ML-based predictive models challenging. Besides, as the degree of complexity in these ML models increases, the interpretation of the results becomes challenging. These interpretations of results are critical towards the development of efficient materials design strategies for enhanced materials performance. To address these challenges, this paper implements different data imputation approaches for enhanced dataset completeness. The imputed dataset is leveraged to predict the compressive and tensile strength of concrete using various hyperparameteroptimized ML approaches. Among all the approaches, Extreme Gradient Boosted Decision Trees (XGBoost) showed the highest prediction efficacy when the dataset is imputed using k-nearest neighbors (kNN) with a 10-neighbor configuration. To interpret the predicted results, SHapley Additive exPlanations (SHAP) is employed. Overall, by implementing efficient combinations of data imputation approach, machine learning, and data interpretation, this paper develops an efficient approach to evaluate the compositionstrength relationship in concrete. This work, in turn, can be used as a starting point toward the design and development of various performance-enhanced and sustainable concretes.","",""
2,"Jiyuan Tu, Weidong Liu, Xiaojun Mao","Byzantine-robust distributed sparse learning for M-estimation",2021,"","","","",138,"2022-07-13 09:38:01","","10.1007/S10994-021-06001-X","","",,,,,2,2.00,1,3,1,"","",""
25,"Jiaoyan Chen, F. Lécué, Jeff Z. Pan, I. Horrocks, Huajun Chen","Knowledge-based Transfer Learning Explanation",2018,"","","","",139,"2022-07-13 09:38:01","","","","",,,,,25,6.25,5,5,4,"Machine learning explanation can significantly boost machine learning's application in decision making, but the usability of current methods is limited in human-centric explanation, especially for transfer learning, an important machine learning branch that aims at utilizing knowledge from one learning domain (i.e., a pair of dataset and prediction task) to enhance prediction model training in another learning domain. In this paper , we propose an ontology-based approach for human-centric explanation of transfer learning. Three kinds of knowledge-based explanatory evidence, with different granularities, including general factors, particular narrators and core contexts are first proposed and then inferred with both local ontologies and external knowledge bases. The evaluation with US flight data and DB-pedia has presented their confidence and availability in explaining the transferability of feature representation in flight departure delay forecasting.","",""
0,"Yuan Wang, Liping Yang, Jun Wu, Zisheng Song, Li-nan Shi","Mining Campus Big Data: Prediction of Career Choice Using Interpretable Machine Learning Method",2022,"","","","",140,"2022-07-13 09:38:01","","10.3390/math10081289","","",,,,,0,0.00,0,5,1,"The issue of students’ career choice is the common concern of students themselves, parents, and educators. However, students’ behavioral data have not been thoroughly studied for understanding their career choice. In this study, we used eXtreme Gradient Boosting (XGBoost), a machine learning (ML) technique, to predict the career choice of college students using a real-world dataset collected in a specific college. Specifically, the data include information on the education and career choice of 18,000 graduates during their college years. In addition, SHAP (Shapley Additive exPlanation) was employed to interpret the results and analyze the importance of individual features. The results show that XGBoost can predict students’ career choice robustly with a precision, recall rate, and an F1 value of 89.1%, 85.4%, and 0.872, respectively. Furthermore, the interaction of features among four different choices of students (i.e., choose to study in China, choose to work, difficulty in finding a job, and choose to study aboard) were also explored. Several educational features, especially differences in grade point average (GPA) during their college studying, are found to have relatively larger impact on the final choice of career. These results can be of help in the planning, design, and implementation of higher educational institutions’ (HEIs) events.","",""
0,"Jing-Jing Liu, Jian-chao Liu","Permeability Predictions for Tight Sandstone Reservoir Using Explainable Machine Learning and Particle Swarm Optimization",2022,"","","","",141,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,2,1,"High-precision permeability prediction is of great significance to tight sandstone reservoirs. However, while considerable progress has recently been made in the machine learning based prediction of reservoir permeability, the generalization of this approach is limited by weak interpretability. Hence, an interpretable XGBoost model is proposed herein based on particle swarm optimization to predict the permeability of tight sandstone reservoirs with higher accuracy and robust interpretability. The porosity and permeability of 202 core plugs and 6 logging curves (namely, the gamma-ray (GR) curve, the acoustic curve (AC), the spontaneous potential (SP) curve, the caliper (CAL) curve, the deep lateral resistivity (RILD) curve, and eight lateral resistivity (RFOC) curve) are extracted along with three derived variables (i.e., the shale content, the AC slope, and the GR slope) as data sets. Based on the data preprocessing, global and local interpretations are performed according to the Shapley additive explanations (SHAP) analysis, and the redundant features in the data set are screened to identify the porosity, AC, CAL, and GR slope as the four most important features. The particle swarm optimization algorithm is then used to optimize the hyperparameters of the XGBoost model. The prediction results of the PSO-XGBoost model indicate a superior performance compared with that of the benchmark XGBoost model. In addition, the reliable application of the interpretable PSO-XGBoost model in the prediction of tight sandstone reservoir permeability is examined by comparing the results with those of two traditional mathematical regression models, five machine learning models, and three deep learning models. Thus, the interpretable PSO-XGBoost model is shown to have more advantages in permeability prediction along with the lowest root mean square error, thereby confirming the effectiveness and practicability of this method.","",""
0,"Yiyang Chen, Wei Jiang, Themistoklis Charalambous","Machine learning based iterative learning control for non-repetitive time-varying systems",2021,"","","","",142,"2022-07-13 09:38:01","","10.1002/rnc.6272","","",,,,,0,0.00,0,3,1,"The repetitive tracking task for time-varying systems (TVSs) with non-repetitive time-varying parameters, which is also called non-repetitive TVSs, is realized in this paper using iterative learning control (ILC). A machine learning (ML) based nominal model update mechanism, which utilizes the linear regression technique to update the nominal model at each ILC trial only using the current trial information, is proposed for non-repetitive TVSs in order to enhance the ILC performance. Given that the ML mechanism forces the model uncertainties to remain within the ILC robust tolerance, an ILC update law is proposed to deal with non-repetitive TVSs. How to tune parameters inside ML and ILC algorithms to achieve the desired aggregate performance is also provided. The robustness and reliability of the proposed method are verified by simulations. Comparison with current state-of-the-art demonstrates its superior control performance in terms of controlling precision. This paper broadens ILC applications from time-invariant systems to non-repetitive TVSs, adopts ML regression technique to estimate non-repetitive time-varying parameters between two ILC trials and proposes a detailed parameter tuning mechanism to achieve desired performance, which are the main contributions.","",""
0,"Yulu Zheng, Zheng Guo, Yanbo Zhang, Jianjing Shang, Leilei Yu, Ping Fu, Yizhi Liu, Xingang Li, Hao Wang, Ling Ren, Wei Zhang, H. Hou, Xuerui Tan, Weiqi Wang","Rapid triage for ischemic stroke: a machine learning-driven approach in the context of predictive, preventive and personalised medicine",2022,"","","","",143,"2022-07-13 09:38:01","","10.1007/s13167-022-00283-4","","",,,,,0,0.00,0,14,1,"","",""
0,"J. Kernbach, V. Staartjes","Foundations of Machine Learning-Based Clinical Prediction Modeling: Part I-Introduction and General Principles.",2021,"","","","",144,"2022-07-13 09:38:01","","10.1007/978-3-030-85292-4_2","","",,,,,0,0.00,0,2,1,"","",""
0,"C. Steriade","Entering the Era of Personalized Medicine in Epilepsy Through Neuroimaging Machine Learning",2022,"","","","",145,"2022-07-13 09:38:01","","10.1177/15357597221081627","","",,,,,0,0.00,0,1,1,"In drug-resistant temporal lobe epilepsy (TLE), precise predictions of drug response, surgical outcome, and cognitive dysfunction at an individual level remain challenging. A possible explanation may lie in the dominant “one-size-fits-all” group-level analytical approaches that do not allow parsing interindividual variations along the disease spectrum. Conversely, analyzing interpatient heterogeneity is increasingly recognized as a step toward person-centered care. Here, we utilized unsupervised machine learning to estimate latent relations (or disease factors) from 3T multimodal MRI features (cortical thickness, hippocampal volume, FLAIR, T1/FLAIR, and diffusion parameters) representing whole-brain patterns of structural pathology in 82 TLE patients. We assessed the specificity of our approach against ageand sex-matched healthy individuals and a cohort of frontal lobe epilepsy patients with histologically verified focal cortical dysplasia. We identified four latent disease factors variably coexpressed within each patient and characterized by ipsilateral hippocampal microstructural alterations, loss of myelin and atrophy (Factor-1), bilateral paralimbic and hippocampal gliosis (Factor-2), bilateral neocortical atrophy (Factor-3), and bilateral white matter microstructural alterations (Factor-4). Bootstrap analysis and parameter variations supported high stability and robustness of these factors. Moreover, they were not expressed in healthy controls and only negligibly in disease controls, supporting specificity. Supervised classifiers trained on latent disease factors could predict patient-specific drug response in 76 ± 3% and postsurgical seizure outcome in 88 ± 2%, outperforming classifiers that did not operate on latent factor information. Latent factor models predicted inter-patient variability in cognitive dysfunction (verbal IQ: r = 0.40 ± 0.03; memory: r = 0.35 ± 0.03; sequential motor tapping: r = 0.36 ± 0.04), again outperforming baseline learners. Data-driven analysis of disease factors provides a novel appraisal of the continuum of interindividual variability, which is likely determined by multiple interacting pathological processes. Incorporating interindividual variability is likely to improve clinical prognostics.","",""
13,"Thibault Laugel, Marie-Jeanne Lesot, C. Marsala, X. Renard, Marcin Detyniecki","Unjustified Classification Regions and Counterfactual Explanations in Machine Learning",2019,"","","","",146,"2022-07-13 09:38:01","","10.1007/978-3-030-46147-8_3","","",,,,,13,4.33,3,5,3,"","",""
78,"Raha Moraffah, Mansooreh Karami, Ruocheng Guo, A. Raglin, Huan Liu","Causal Interpretability for Machine Learning - Problems, Methods and Evaluation",2020,"","","","",147,"2022-07-13 09:38:01","","10.1145/3400051.3400058","","",,,,,78,39.00,16,5,2,"Machine learning models have had discernible achievements in a myriad of applications. However, most of these models are black-boxes, and it is obscure how the decisions are made by them. This makes the models unreliable and untrustworthy. To provide insights into the decision making processes of these models, a variety of traditional interpretable models have been proposed. Moreover, to generate more humanfriendly explanations, recent work on interpretability tries to answer questions related to causality such as ""Why does this model makes such decisions?"" or ""Was it a specific feature that caused the decision made by the model?"". In this work, models that aim to answer causal questions are referred to as causal interpretable models. The existing surveys have covered concepts and methodologies of traditional interpretability. In this work, we present a comprehensive survey on causal interpretable models from the aspects of the problems and methods. In addition, this survey provides in-depth insights into the existing evaluation metrics for measuring interpretability, which can help practitioners understand for what scenarios each evaluation metric is suitable.","",""
44,"Sijia Liu, Pin-Yu Chen, B. Kailkhura, Gaoyuan Zhang, A. Hero III, P. Varshney","A Primer on Zeroth-Order Optimization in Signal Processing and Machine Learning: Principals, Recent Advances, and Applications",2020,"","","","",148,"2022-07-13 09:38:01","","10.1109/MSP.2020.3003837","","",,,,,44,22.00,7,6,2,"Zeroth-order (ZO) optimization is a subset of gradient-free optimization that emerges in many signal processing and machine learning (ML) applications. It is used for solving optimization problems similarly to gradient-based methods. However, it does not require the gradient, using only function evaluations. Specifically, ZO optimization iteratively performs three major steps: gradient estimation, descent direction computation, and the solution update. In this article, we provide a comprehensive review of ZO optimization, with an emphasis on showing the underlying intuition, optimization principles, and recent advances in convergence analysis. Moreover, we demonstrate promising applications of ZO optimization, such as evaluating robustness and generating explanations from black-box deep learning (DL) models and efficient online sensor management.","",""
38,"Oscar Gomez, Steffen Holter, Jun Yuan, E. Bertini","ViCE: visual counterfactual explanations for machine learning models",2020,"","","","",149,"2022-07-13 09:38:01","","10.1145/3377325.3377536","","",,,,,38,19.00,10,4,2,"The continued improvements in the predictive accuracy of machine learning models have allowed for their widespread practical application. Yet, many decisions made with seemingly accurate models still require verification by domain experts. In addition, end-users of a model also want to understand the reasons behind specific decisions. Thus, the need for interpretability is increasingly paramount. In this paper we present an interactive visual analytics tool, ViCE, that generates counterfactual explanations to contextualize and evaluate model decisions. Each sample is assessed to identify the minimal set of changes needed to flip the model's output. These explanations aim to provide end-users with personalized actionable insights with which to understand, and possibly contest or improve, automated decisions. The results are effectively displayed in a visual interface where counterfactual explanations are highlighted and interactive methods are provided for users to explore the data and model. The functionality of the tool is demonstrated by its application to a home equity line of credit dataset.","",""
3,"N. Radziwill","Machine Learning with R, Third Edition (Book Review)",2019,"","","","",150,"2022-07-13 09:38:01","","10.1080/10686967.2019.1648086","","",,,,,3,1.00,3,1,3,"This book is highly recommended for anyone with previous programing experience who seeks a solid, grounded introduction to basic machine learning using the R statistical software. With nearly 100 additional pages added since the first edition in 2013, this update to Brett Lantz’s excellent text is well worth the purchase, even for those who already have an earlier copy on their shelf. Clear writing, robust explanations, and compelling examples appear throughout, and most chapters explain the math underlying the methods in as simple and easy a manner as possible. I liked the first edition so much, I used it as the primary textbook for my applied machine learning class for undergraduate juniors and seniors in science and engineering. Chapter 1 provides an overview of the main concepts associated with developing and using ML models for decision making. It includes discussions of traditional topics like overfitting and emerging issues like bias and artificial intelligence (AI) ethics. The chapter structure follows the same pattern as previous editions, so knn, Naive Bayes, decision trees, four neural networks and SVMs, association rules, k-means, and performance are all covered. Chapter 12 on specialized machine learning topics is significantly updated from previous editions and now covers tidyverse, domain-specific data, and brief examinations of performance optimization techniques like parallelization, MapReduce, Hadoop, and Spark. In most chapters, there are fully reproducible examples clearly broken down into steps. Within those steps, subtasks (for example, transformation, data preparation, model specification) are also clearly specified, making it clear how to structure different types of problems. This book is excellent for beginners and others who want to use R to learn how to skillfully address ML problems using their own data.","",""
4,"S. Nomm, Alejandro Guerra-Manzanares, Hayretdin Bahsi","Towards the Integration of a Post-Hoc Interpretation Step into the Machine Learning Workflow for IoT Botnet Detection",2019,"","","","",151,"2022-07-13 09:38:01","","10.1109/ICMLA.2019.00193","","",,,,,4,1.33,1,3,3,"The analysis of the interplay between the feature selection and the post-hoc local interpretation steps in a machine learning workflow followed for IoT botnet detection constitutes the research scope of the present paper. While the application of machine learning-based techniques has become a trend in cyber security, the main focus has been almost on detection accuracy. However, providing the relevant explanation for a detection decision is a vital requirement in a tiered incident handling processes of the contemporary security operations centers. Moreover, the design of intrusion detection systems in IoT networks has to take the limitations of the computational resources into consideration. Therefore, resource limitations in addition to human element of incident handling necessitate considering feature selection and interpretability at the same time in machine learning workflows. In this paper, first, we analyzed the selection of features and its implication on the data accuracy. Second, we investigated the impact of feature selection on the explanations generated at the post-hoc interpretation phase. We utilized a filter method, Fisher's Score and Local Interpretable Model-Agnostic Explanation (LIME) at feature selection and post-hoc interpretation phases, respectively. To evaluate the quality of explanations, we proposed a metric that reflects the need of the security analysts. It is demonstrated that the application of both steps for the particular case of IoT botnet detection may result in highly accurate and interpretable learning models induced by fewer features. Our metric enables us to evaluate the detection accuracy and interpretability in an integrated way.","",""
1,"Ramkumar Harikrishnakumar, A. Dand, S. Nannapaneni, K. Krishnan","Supervised Machine Learning Approach for Effective Supplier Classification",2019,"","","","",152,"2022-07-13 09:38:01","","10.1109/ICMLA.2019.00045","","",,,,,1,0.33,0,4,3,"Supplier assessment plays a critical role in the supply chain management, which involves the flow of goods and services from the initial stage (raw material procurement) to the final stage (delivery). Supplier assessment is a multi-criteria decision-making (MCDM) approach that requires several criteria for the proper assessment of the suppliers. When there are several criteria involved, it makes the supplier assessment process more complicated. For a comprehensive and robust assessment process, we propose the use of supervised machine learning algorithms to classify various suppliers into four categories: excellent, good, satisfactory, and unsatisfactory. In this paper, supervised learning (classification) algorithms are applied for a supplier assessment problem where a model is trained based on the previous historical data and then tested on the new unseen data set. This method will provide an efficient way for supplier assessment that is more effective in terms of accuracy and time when compared to MCDM approach. Classification algorithms such as support vector machines (with linear, polynomial and radial basis kernels), logistic regression, k-nearest neighbors, and naïve Bayes methods are used to train the model and their performance is assessed against a test data. Finally, the performance measures from all the classification methods are used to assess the best supplier.","",""
1,"Leila Etaati","Overview of Microsoft Machine Learning Tools",2019,"","","","",153,"2022-07-13 09:38:01","","10.1007/978-1-4842-3658-1_20","","",,,,,1,0.33,1,1,3,"","",""
8,"Dongqi Han, Zhiliang Wang, Ying Zhong, Wenqi Chen, Jiahai Yang, Shuqiang Lu, Xingang Shi, Xia Yin","Evaluating and Improving Adversarial Robustness of Machine Learning-Based Network Intrusion Detectors",2020,"","","","",154,"2022-07-13 09:38:01","","10.1109/JSAC.2021.3087242","","",,,,,8,4.00,1,8,2,"Machine learning (ML), especially deep learning (DL) techniques have been increasingly used in anomaly-based network intrusion detection systems (NIDS). However, ML/DL has shown to be extremely vulnerable to adversarial attacks, especially in such security-sensitive systems. Many adversarial attacks have been proposed to evaluate the robustness of ML-based NIDSs. Unfortunately, existing attacks mostly focused on feature-space and/or white-box attacks, which make impractical assumptions in real-world scenarios, leaving the study on practical gray/black-box attacks largely unexplored. To bridge this gap, we conduct the first systematic study of the gray/black-box traffic-space adversarial attacks to evaluate the robustness of ML-based NIDSs. Our work outperforms previous ones in the following aspects: (i) practical —the proposed attack can automatically mutate original traffic with extremely limited knowledge and affordable overhead while preserving its functionality; (ii) generic —the proposed attack is effective for evaluating the robustness of various NIDSs using diverse ML/DL models and non-payload-based features; (iii) explainable —we propose an explanation method for the fragile robustness of ML-based NIDSs. Based on this, we also propose a defense scheme against adversarial attacks to improve system robustness. We extensively evaluate the robustness of various NIDSs using diverse feature sets and ML/DL models. Experimental results show our attack is effective (e.g., >97% evasion rate in half cases for Kitsune, a state-of-the-art NIDS) with affordable execution cost and the proposed defense method can effectively mitigate such attacks (evasion rate is reduced by >50% in most cases).","",""
0,"Mohit Thakkar","Introduction to Machine Learning",2019,"","","","",155,"2022-07-13 09:38:01","","10.1007/978-1-4842-4297-1_1","","",,,,,0,0.00,0,1,3,"","",""
0,"Muhammad Abdullah Hanif, R. Hafiz, M. Javed, Semeen Rehman, M. Shafique","Energy-Efficient Design of Advanced Machine Learning Hardware",2019,"","","","",156,"2022-07-13 09:38:01","","10.1007/978-3-030-04666-8_21","","",,,,,0,0.00,0,5,3,"","",""
33,"P. Pan, Yichao Li, Yongjiu Xiao, B. Han, L. Su, M. Su, Yansheng Li, Siqi Zhang, D. Jiang, Xia Chen, Fuquan Zhou, Ling Ma, Pengtao Bao, Lixin Xie","Prognostic Assessment of COVID-19 in the Intensive Care Unit by Machine Learning Methods: Model Development and Validation",2020,"","","","",157,"2022-07-13 09:38:01","","10.2196/23128","","",,,,,33,16.50,3,14,2,"Background Patients with COVID-19 in the intensive care unit (ICU) have a high mortality rate, and methods to assess patients’ prognosis early and administer precise treatment are of great significance. Objective The aim of this study was to use machine learning to construct a model for the analysis of risk factors and prediction of mortality among ICU patients with COVID-19. Methods In this study, 123 patients with COVID-19 in the ICU of Vulcan Hill Hospital were retrospectively selected from the database, and the data were randomly divided into a training data set (n=98) and test data set (n=25) with a 4:1 ratio. Significance tests, correlation analysis, and factor analysis were used to screen 100 potential risk factors individually. Conventional logistic regression methods and four machine learning algorithms were used to construct the risk prediction model for the prognosis of patients with COVID-19 in the ICU. The performance of these machine learning models was measured by the area under the receiver operating characteristic curve (AUC). Interpretation and evaluation of the risk prediction model were performed using calibration curves, SHapley Additive exPlanations (SHAP), Local Interpretable Model-Agnostic Explanations (LIME), etc, to ensure its stability and reliability. The outcome was based on the ICU deaths recorded from the database. Results Layer-by-layer screening of 100 potential risk factors finally revealed 8 important risk factors that were included in the risk prediction model: lymphocyte percentage, prothrombin time, lactate dehydrogenase, total bilirubin, eosinophil percentage, creatinine, neutrophil percentage, and albumin level. Finally, an eXtreme Gradient Boosting (XGBoost) model established with the 8 important risk factors showed the best recognition ability in the training set of 5-fold cross validation (AUC=0.86) and the verification queue (AUC=0.92). The calibration curve showed that the risk predicted by the model was in good agreement with the actual risk. In addition, using the SHAP and LIME algorithms, feature interpretation and sample prediction interpretation algorithms of the XGBoost black box model were implemented. Additionally, the model was translated into a web-based risk calculator that is freely available for public usage. Conclusions The 8-factor XGBoost model predicts risk of death in ICU patients with COVID-19 well; it initially demonstrates stability and can be used effectively to predict COVID-19 prognosis in ICU patients.","",""
29,"Yuhui Zheng, Le Sun, Shunfeng Wang, Jianwei Zhang, J. Ning","Spatially Regularized Structural Support Vector Machine for Robust Visual Tracking",2019,"","","","",158,"2022-07-13 09:38:01","","10.1109/TNNLS.2018.2855686","","",,,,,29,9.67,6,5,3,"Structural support vector machine (SSVM) is popular in the visual tracking field as it provides a consistent target representation for both learning and detection. However, the spatial distribution of feature is not considered in standard SSVM-based trackers, therefore leading to limited performance. To obtain a robust discriminative classifier, this paper proposes a novel tracking framework that spatially regularizes SSVM, which yields a new spatially regularized SSVM (SRSSVM). We utilize the spatial regularization prior to penalize the learning classifier with the same size as the target region. The location of classifier spatially located far from the center of region is assigned large weight and vice versa. Then, it is introduced into the SSVM model as a regularization factor to learn the robust discriminative model. Furthermore, an optimizing algorithm with dual coordination descent is presented to efficiently solve the SRSSVM tracking model. Our proposed SRSSVM tracking method has low computational cost like the traditional linear SSVM tracker while can significantly improve the robustness of the discriminative classifier. The experimental results on three popular tracking benchmark data sets show that the proposed SRSSVM tracking method performs favorably against the state-of-the-art trackers.","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",159,"2022-07-13 09:38:01","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
2,"A. Ayobi, Katarzyna Stawarz, Dmitri S. Katz, P. Marshall, Taku Yamagata, Raúl Santos-Rodríguez, Peter A. Flach, A. O'Kane","Machine Learning Explanations as Boundary Objects: How AI Researchers Explain and Non-Experts Perceive Machine Learning",2021,"","","","",160,"2022-07-13 09:38:01","","","","",,,,,2,2.00,0,8,1,"Understanding artificial intelligence (AI) and machine learning (ML) approaches is becoming increasingly important for people with a wide range of professional backgrounds. However, it is unclear how ML concepts can be effectively explained as part of human-centred and multidisciplinary design processes. We provide a qualitative account of how AI researchers explained and non-experts perceived ML concepts as part of a co-design project that aimed to inform the design of ML applications for diabetes self-care. We identify benefits and challenges of explaining ML concepts with analogical narratives, information visualisations, and publicly available videos. Co-design participants reported not only gaining an improved understanding of ML concepts but also highlighted challenges of understanding ML explanations, including misalignments between scientific models and their lived self-care experiences and individual information needs. We frame our findings through the lens of Stars and Griesemer’s concept of boundary objects to discuss how the presentation of user-centred ML explanations could strike a balance between being plastic and robust enough to support design objectives and people’s individual information needs.","",""
1,"Khansa Rasheed, A. Qayyum, M. Ghaly, A. Al-Fuqaha, A. Razi, Junaid Qadir","Explainable, Trustworthy, and Ethical Machine Learning for Healthcare: A Survey",2021,"","","","",161,"2022-07-13 09:38:01","","10.36227/TECHRXIV.14376179.V1","","",,,,,1,1.00,0,6,1,"With the advent of machine learning (ML) applications in daily life, the questions about liability, trust, and interpretability of their outputs are raising, especially for healthcare applications. The black-box nature of ML models is a roadblock for clinical utilization. Therefore, to gain the trust of clinicians and patients, researchers need to provide explanations of how and why the model is making a specific decision. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the field of eXplainable ML (XML). In this paper, we provide a comprehensive review of explainable and interpretable ML techniques implemented for providing the reasons behind their decisions for various healthcare applications. Along with highlighting various security, safety, and robustness challenges that hinder the trustworthiness of ML we also discussed the ethical issues of healthcare ML and describe how explainable and trustworthy ML can resolve these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.","",""
97,"M. Maniruzzaman, M. Rahman, Md. Al-MehediHasan, Harman S. Suri, M. Abedin, A. El-Baz, J. Suri","Accurate Diabetes Risk Stratification Using Machine Learning: Role of Missing Value and Outliers",2018,"","","","",162,"2022-07-13 09:38:01","","10.1007/s10916-018-0940-7","","",,,,,97,24.25,14,7,4,"","",""
0,"V. Zhukovska, Oleksandr O. Mosiiuk","STATISTICAL SOFTWARE R IN CORPUS-DRIVEN RESEARCH AND MACHINE LEARNING",2021,"","","","",163,"2022-07-13 09:38:01","","10.33407/itlt.v86i6.4627","","",,,,,0,0.00,0,2,1,"The rapid development of computer software and network technologies has facilitated the intensive application of specialized statistical software not only in the traditional information technology spheres (i.e., statistics, engineering, artificial intelligence) but also in linguistics. The statistical software R is one of the most popular analytical tools for statistical processing a huge array of digitalized language data, especially in quantitative corpus linguistic studies of Western Europe and North America. This article discusses the functionality of the software package R, focusing on its advantages in performing complex statistical analyses of linguistic data in corpus-driven studies and creating linguistic classifiers in machine learning. With this in mind, a three-stage strategy of computer-statistical analysis of linguistic corpus data is elaborated: 1) data processing and preparing to be subjected to a statistical procedure, 2) utilizing statistical hypothesis testing methods (MANOVA, ANOVA) and the Tukey post-hoc test, and 3) developing a model of a linguistic classifier and analyzing its effectiveness. The strategy is implemented on 11 000 tokens of English detached nonfinite constructions with an explicit subject extracted from the BNC-BYU corpus. The statistical analysis indicates significant differences in the realization of the factors of the parameter “Part of speech of the subject”. The analyzed linguistic data are employed to build a machine model for the classification of the given constructions. Particular attention is devoted to the methodological perspectives of interdisciplinary research in the fields of linguistics and computer studies. The potential application of the elaborated case study in training undergraduate, master, and postgraduate students of Applied Linguistics is indicated. The article provides all the statistical data and codes written in the R script with comprehensive descriptions and explanations. The concluding part of the article summarizes the obtained results and highlights the issues for further research connected with the popularization of the statistical software complex R and raising the awareness of specialists in this statistical analysis system.","",""
0,"C. Carpenter","Machine-Learning Approach Optimizes Well Spacing",2021,"","","","",164,"2022-07-13 09:38:01","","10.2118/0921-0044-jpt","","",,,,,0,0.00,0,1,1,"This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 201698, “Finding a Trend Out of Chaos: A Machine-Learning Approach for Well-Spacing Optimization,” by Zheren Ma, Ehsan Davani, SPE, and Xiaodan Ma, SPE, Quantum Reservoir Impact, et al., prepared for the 2020 SPE Annual Technical Conference and Exhibition, originally scheduled to be held in Denver, Colorado, 5–7 October. The paper has not been peer reviewed.  Data-driven decisions powered by machine-learning (ML) methods are increasing in popularity when optimizing field development in unconventional reservoirs. However, because well performance is affected by many factors, the challenge is to uncover trends within all the noise. By leveraging basin-level knowledge captured by big data sculpting, integrating private and public data with the use of uncertainty quantification, a process the authors describe as augmented artificial intelligence (AI) can provide quick, science-based answers for well spacing and fracturing optimization and can assess the full potential of an asset in unconventional reservoirs. A case study in the Midland Basin is detailed in the complete paper.      Augmented AI is a process wherein ML and human expertise are coupled to improve solutions. The augmented AI work flow (Fig. 1) starts with data sculpting, which includes information retrieval; data cleaning and standardization; and smart, deep, and systematic data quality control (QC). Feature engineering generates all relevant parameters entering the ML model. More than 50 features have been generated for this work and categorized. The final step is to perform model tuning and ensemble, evaluating model robustness and generating model explanation and uncertainty quantification.        The complete paper provides a detailed geological background of the Permian Basin and its Wolfcamp unconventional layer, an organic-rich shale formation with tight reservoir properties.  To find a solution for the multidimensional well-spacing problem in the Permian Basin, multiple sources and types of data were gathered using publicly available sources. The detailed geological attributes, including structure, petrophysics, geochemistry, basin-level features, and cultural information (such as counties or lease boundaries) have been combined in an integrated database to extract and generate features for the ML algorithm. Most attributes are available either in a limited number of wells, mostly vertical, or through the low number of available cored wells across the basin. Therefore, a significant amount of data imputation has been processed with mapping exercises using geostatistical modeling techniques.  The mapping process augmented the ML attribute-generation step because these features were distributed in both vertical and lateral dimensions. All horizontal wells within the area of interest across the Permian Basin have been resampled with the logged and mapped information.  The geological features also are reengineered into multiple indices to reduce the number of labeled features to include in the ML process. This feature-reduction process also has helped in ranking and selecting the most-important parameters relevant to the well-spacing problem. Here, a key attribute called the shale-oil index was introduced, which is generated for the ML-driven process and is used in understanding the level of contribution of geological sweet spots to well-spacing optimization. In addition, the initial well, reservoir, or laboratory data, including logs, have been normalized before mapping and modeling to eliminate potential bias. This study has focused on Wolfcamp layers; however, both geological and engineering attribute generation work flows used for this practical ML methodology to find optimization solutions for common problems are highly applicable to other unconventional layers, such as Bone Spring or Spraberry. ","",""
0,"Z. Yin, Yin‐Fu Jin, F. Gao","Intelligent modelling of clay compressibility using hybrid meta-heuristic and machine learning algorithms",2021,"","","","",165,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,3,1,"Compression index C c is an essential parameter in geotechnical design for which the effectiveness of correlation is still a challenge. This paper suggests a novel modelling approach using machine learning (ML) technique. The performance of ﬁ ve commonly used machine learning (ML) algorithms, i.e. back-propagation neural network (BPNN), extreme learning machine (ELM), support vector machine (SVM), random forest (RF) and evolutionary polynomial regression (EPR) in predicting C c is comprehensively investigated. A database with a total number of 311 datasets including three input variables, i.e. initial void ratio e 0 , liquid limit water content w L , plasticity index I p , and one output variable C c is ﬁ rst established. Genetic algorithm (GA) is used to optimize the hyper-parameters in ﬁ ve ML algorithms, and the average prediction error for the 10-fold cross-validation (CV) sets is set as the ﬁ tness function in the GA for enhancing the robustness of ML models. The results indicate that ML models outperform empirical prediction formulations with lower prediction error. RF yields the lowest error followed by BPNN, ELM, EPR and SVM. If the ranges of input variables in the database are large enough, BPNN and RF models are recommended to predict C c . Furthermore, if the distribution of input variables is continuous, RF model is the best one. Otherwise, EPR model is recommended if the ranges of input variables are small. The predicted cor- relations between input and output variables using ﬁ ve ML models show great agreement with the physical explanation.","",""
0,"Cong Cao","A machine learning-based study of the impact of traffic flow changes on air pollution combined with meteorological conditions: the case of Oslo",2021,"","","","",166,"2022-07-13 09:38:01","","10.31235/osf.io/yp4gn","","",,,,,0,0.00,0,1,1,"In this paper, we explore the impact of changes in traffic flow on local air pollution under specific meteorological conditions by integrating hourly traffic flow data, air pollution data and meteorological data, using generalized linear regression models and advanced machine learning algorithms: support vector machines and decision trees. The geographical location is Oslo, the capital of Norway, and the time we selected is from February 2020 to September 2020; We also selected 24-hour data for May 11 and 16 of the same year, representing weekday and holiday traffic flow, respectively, as a subset to further explore. Finally, we selected data from July 2020 for robustness testing, and algorithm performance verification.We found that: the maximum traffic flow on holidays is significantly higher than that on weekdays, but the holidays produce less concentration of {NO}_x throughout the month; the peak arrival time of {NO}_x,\ {NO}_2and NO concentrations is later than the peak arrival time of traffic flow. Among them, {NO}_x has a very significant variation, so we choose {NO}_x concentration as an air pollution indicator to measure the effect of traffic flow variation on air pollution; we also find that {NO}_xconcentration is negatively correlated with hourly precipitation, and the variation trend is like that of minimum air temperature. We used multiple imputation methods to interpolate the missing values. The decision tree results yield that when traffic volumes are high (>81%), low temperatures generate more concentrations of {NO}_x than high temperatures (an increase of 3.1%). Higher concentrations of {NO}_x (2.4%) are also generated when traffic volumes are low (no less than 22%) but there is some precipitation ≥ 0.27%.In the evaluation of the prediction accuracy of the machine learning algorithms, the support vector machine has the best prediction performance with high R-squared and small MAE, MSE and RMSE, indicating that the support vector machine has a better explanation for air pollution caused by traffic flow, while the decision tree is the second best, and the generalized linear regression model is the worst.The selected data for July 2020 obtained results consistent with the overall dataset.","",""
0,"W. Zong, Yang-Wai Chow, W. Susilo","Visual Analysis of Adversarial Examples in Machine Learning",2021,"","","","",167,"2022-07-13 09:38:01","","10.1007/978-981-33-6726-5_4","","",,,,,0,0.00,0,3,1,"","",""
0,"Chao An, Hongcai Yang, Xiaoling Yu, Zhi-yu Han, Zhigang Cheng, Fang-yi Liu, J. Dou, Bin Li, Yichao Li, Yansheng Li, Jie Yu, P. Liang","A Machine Learning Model Based on Electronic Health Records for Predicting Recurrence after Microwave Ablation of Hepatocellular Carcinoma",2021,"","","","",168,"2022-07-13 09:38:01","","10.2139/ssrn.3901789","","",,,,,0,0.00,0,12,1,"Purpose: To investigate the accuracy and robustness of machine learning(ML) model using clinical data extracted directly from electronic health records for predicting the recurrence risk after microwave ablation(MWA).    Methods: Between August 2005 and December 2019, 1574 HCC patients who subsequently underwent MWA from four hospitals were reviewed. Data were assigned to the training, internal and external validation set, respectively. Apart from traditional logistic regression(LR), three ML models including (Random Forest, Support Vector Machine and eXtreme Gradient Boosting[XGBoost]) were built and validated in the predictive ability with area under receiving operator characteristic (AUC). SHapley Additive exPlanations (SHAP) and Local Interpretable Model-Agnostic Explanations (LIME) algorithm were performed to realize their interpretability.    Results: After 26.2 months of median follow-up period(interquartile range, 6.3, 160.7 months), 51.9%(817/1574) patients occur recurrence. The predictive abilities of three ML models outperformed LR (P < 0.05 for all). When 9 variables were trained simultaneouly using recursive feature elimination with cross-validation, the XGBoost model achieved top predictive ability with AUC value (0.75, 95% CI [confidence interval]: 0.72-0.78) in training set, (0.74, 95% CI: 0.69-0.80) in internal set and (0.76, 95% CI: 0.70-0.82) in external validation set among all models, and it was interpreted depending on the visualization of risk factors by the SHAP and LIME algorithms. The predictive system of post-ablation recurrence risk stratification was provided on online (http://114.251.235.51:8001/) based on XGboost analysis.    Conclusions: The XGBoost model based on clinical data can effectively predict recurrence risk after MWA, which can contribute to surveillance, prevention and treatment strategies for HCC.    Funding: None to declare.    Declaration of Interest: None to declare.    Ethical Approval: This retrospective, multi-center study protocol was approved by the Ethics Committee of all participating institutions","",""
0,"J. Figuerêdo, V. T. Sarinho, R. Calumby","Low-Cost Machine Learning for Effective and Efficient Bad Smells Detection",2021,"","","","",169,"2022-07-13 09:38:01","","10.5753/kdmile.2021.17468","","",,,,,0,0.00,0,3,1,"Bad smells are characteristics of software that indicate a code or design problem which can make information system hard to understand, evolve, and maintain. To address this problem, different approaches, manual and automated, have been proposed over the years, including more recently machine learning alternatives. However, despite the advances achieved, some machine learning techniques have not yet been effectively explored, such as the use of feature selection techniques. Moreover, it is not clear to what extent the use of numerous source-code features are necessary for reasonable bad smell detection success. Therefore, in this work we propose an approach using low-cost machine learning for effective and efficient detection of bad smells, through explicit feature selection. Our results showed that the selection allowed to statistically improve the effectiveness of the models. For some cases, the models achieved statistical equivalence, but relying on a highly reduced set of features. Indeed, by using explicit feature selection, simpler models such as Naive Bayes became statistically equivalent to robust models such as Random Forest. Therefore, the selection of features allowed keeping competitive or even superior effectiveness while also improving the efficiency of the models, demanding less computational resources for source-code preprocessing, model training and bad smell detection.","",""
0,"Jaehun Kim","Increasing trust in complex machine learning systems",2021,"","","","",170,"2022-07-13 09:38:01","","10.1145/3476415.3476435","","",,,,,0,0.00,0,1,1,"Machine learning (ML) has become a core technology for many real-world applications. Modern ML models are applied to unprecedentedly complex and difficult challenges, including very large and subjective problems. For instance, applications towards multimedia understanding have been advanced substantially. Here, it is already prevalent that cultural/artistic objects such as music and videos are analyzed and served to users according to their preference, enabled through ML techniques. One of the most recent breakthroughs in ML is Deep Learning (DL), which has been immensely adopted to tackle such complex problems. DL allows for higher learning capacity, making end-to-end learning possible, which reduces the need for substantial engineering effort, while achieving high effectiveness. At the same time, this also makes DL models more complex than conventional ML models. Reports in several domains indicate that such more complex ML models may have potentially critical hidden problems: various biases embedded in the training data can emerge in the prediction, extremely sensitive models can make unaccountable mistakes. Furthermore, the black-box nature of the DL models hinders the interpretation of the mechanisms behind them. Such unexpected drawbacks result in a significant impact on the trustworthiness of the systems in which the ML models are equipped as the core apparatus. In this thesis, a series of studies investigates aspects of trustworthiness for complex ML applications, namely the reliability and explainability. Specifically, we focus on music as the primary domain of interest, considering its complexity and subjectivity. Due to this nature of music, ML models for music are necessarily complex for achieving meaningful effectiveness. As such, the reliability and explainability of music ML models are crucial in the field. The first main chapter of the thesis investigates the transferability of the neural network in the Music Information Retrieval (MIR) context. Transfer learning, where the pre-trained ML models are used as off-the-shelf modules for the task at hand, has become one of the major ML practices. It is helpful since a substantial amount of the information is already encoded in the pre-trained models, which allows the model to achieve high effectiveness even when the amount of the dataset for the current task is scarce. However, this may not always be true if the ""source"" task which pre-trained the model shares little commonality with the ""target"" task at hand. An experiment including multiple ""source"" tasks and ""target"" tasks was conducted to examine the conditions which have a positive effect on the transferability. The result of the experiment suggests that the number of source tasks is a major factor of transferability. Simultaneously, it is less evident that there is a single source task that is universally effective on multiple target tasks. Overall, we conclude that considering multiple pre-trained models or pre-training a model employing heterogeneous source tasks can increase the chance for successful transfer learning. The second major work investigates the robustness of the DL models in the transfer learning context. The hypothesis is that the DL models can be susceptible to imperceptible noise on the input. This may drastically shift the analysis of similarity among inputs, which is undesirable for tasks such as information retrieval. Several DL models pre-trained in MIR tasks are examined for a set of plausible perturbations in a real-world setup. Based on a proposed sensitivity measure, the experimental results indicate that all the DL models were substantially vulnerable to perturbations, compared to a traditional feature encoder. They also suggest that the experimental framework can be used to test the pre-trained DL models for measuring robustness. In the final main chapter, the explainability of black-box ML models is discussed. In particular, the chapter focuses on the evaluation of the explanation derived from model-agnostic explanation methods. With black-box ML models having become common practice, model-agnostic explanation methods have been developed to explain a prediction. However, the evaluation of such explanations is still an open problem. The work introduces an evaluation framework that measures the quality of the explanations employing fidelity and complexity. Fidelity refers to the explained mechanism's coherence to the black-box model, while complexity is the length of the explanation. Throughout the thesis, we gave special attention to the experimental design, such that robust conclusions can be reached. Furthermore, we focused on delivering machine learning framework and evaluation frameworks. This is crucial, as we intend that the experimental design and results will be reusable in general ML practice. As it implies, we also aim our findings to be applicable beyond the music applications such as computer vision or natural language processing. Trustworthiness in ML is not a domain-specific problem. Thus, it is vital for both researchers and practitioners from diverse problem spaces to increase awareness of complex ML systems' trustworthiness. We believe the research reported in this thesis provides meaningful stepping stones towards the trustworthiness of ML.","",""
0,"Hsiao-Chi Li, Chang-Yu Cheng, Chia Chou, Chien-Chang Hsu, Meng-Lin Chang, Y. Chiu, J. Chai","Multi-Class Brain Age Discrimination Using Machine Learning Algorithm",2019,"","","","",171,"2022-07-13 09:38:01","","10.1109/ICMLC48188.2019.8949317","","",,,,,0,0.00,0,7,3,"Resting-state functional connectivity analyses have revealed a significant effect on the inter-regional interactions in brain. The brain age prediction based on resting-state functional magnetic resonance imaging has been proved as biomarkers to characterize the typical brain development and neuropsychiatric disorders. The brain age prediction model based on functional connectivity measurements derived from resting-state functional magnetic resonance imaging has received a lots of interest in recent years due to its great success in age prediction. However, some of the recent studies rely on experienced neuroscientist experts to select appropriate connectivity features in order to build a robust model for prediction while the others just selected the features based on trial-and-error test. Besides, the subjects used in this studies omitted some subjects that can be divided into two groups with less similarity which may confused the prediction model. In this study, we proposed a multi-class age categories discrimination method with the connectivity features selected via K-means clustering with no prior knowledge provided. The experimental results show that with K-means selected features the proposed model better discriminate multi-class age categories.","",""
3,"Alexander Warnecke, Dan Arp, Christian Wressnegger, K. Rieck","Evaluating Explanation Methods for Deep Learning in Computer Security",2020,"","","","",172,"2022-07-13 09:38:01","","","","",,,,,3,1.50,1,4,2,"Deep learning is increasingly used as a building block of security systems. Unfortunately, neural networks are hard to interpret and typically opaque to the practitioner. The machine learning community has started to address this problem by developing methods for explaining the predictions of neural networks. While several of these approaches have been successfully applied in the area of computer vision, their application in security has received little attention so far. It is an open question which explanation methods are appropriate for computer security and what requirements they need to satisfy.  In this paper, we introduce criteria for comparing and evaluating explanation methods in the context of computer security. These cover general properties, such as the accuracy of explanations, as well as security-focused aspects, such as the completeness, efficiency, and robustness. Based on our criteria, we investigate six popular explanation methods and assess their utility in security systems for malware detection and vulnerability discovery. We observe significant differences between the methods and build on these to derive general recommendations for selecting and applying explanation methods in computer security.","",""
82,"Qian Yang, Jina Suh, N. Chen, Gonzalo A. Ramos","Grounding Interactive Machine Learning Tool Design in How Non-Experts Actually Build Models",2018,"","","","",173,"2022-07-13 09:38:01","","10.1145/3196709.3196729","","",,,,,82,20.50,21,4,4,"Machine learning (ML) promises data-driven insights and solutions for people from all walks of life, but the skill of crafting these solutions is possessed by only a few. Emerging research addresses this issue by creating ML tools that are easy and accessible to people who are not formally trained in ML (non-experts). This work investigated how non-experts build ML solutions for themselves in real life. Our interviews and surveys revealed unique potentials of non-expert ML, as well several pitfalls that non-experts are susceptible to. For example, many perceived percentage accuracy as a sole measure of performance, thus problematic models proceeded to deployment. These observations suggested that, while challenging, making ML easy and robust should both be important goals of designing novice-facing ML tools. To advance on this insight, we discuss design implications and created a sensitizing concept to demonstrate how designers might guide non-experts to easily build robust solutions.","",""
78,"Finale Doshi-Velez, Been Kim","Considerations for Evaluation and Generalization in Interpretable Machine Learning",2018,"","","","",174,"2022-07-13 09:38:01","","10.1007/978-3-319-98131-4_1","","",,,,,78,19.50,39,2,4,"","",""
8,"Mustafa Anil Koçak, David Ramirez, E. Erkip, D. Shasha","SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness",2017,"","","","",175,"2022-07-13 09:38:01","","10.1109/TPAMI.2019.2932415","","",,,,,8,1.60,2,4,5,"<italic>SafePredict</italic> is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, <inline-formula><tex-math notation=""LaTeX"">$1-\epsilon$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""kocak-ieq1-2932415.gif""/></alternatives></inline-formula>, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm so that the error rate on non-refused predictions does not exceed <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq2-2932415.gif""/></alternatives></inline-formula>. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq3-2932415.gif""/></alternatives></inline-formula>, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the-art confidence-based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software is included in the supplementary material, which can be found on the Computer Society Digital Library at <uri>http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2932415</uri>.","",""
0,"Sannasi Chakravarthy S R, H. Rajaguru","Deep Features with Improved Extreme Learning Machine for Breast Cancer Classification",2021,"","","","",176,"2022-07-13 09:38:01","","10.1109/ISCMI53840.2021.9654814","","",,,,,0,0.00,0,2,1,"Breast cancer classification problem is receiving more attention among researchers due to its global impact on women's healthcare. There is always a demand for research analysis in the earlier diagnosis of breast cancer. The paper proposes a new computer-aided diagnosis (CAD) framework which integrates deep learning and Extreme Learning Machine (ELM) for feature extrication and classification of breast cancer. The proposed CAD tool is very much helpful for radiologists in the earlier diagnosis of breast cancer using digital mammograms. Herein, the research uses the Sine-Cosine Crow-Search Optimization Algorithm (SC-CSOA) for improving the ELM’s classification performance. And to extricate the robust features from the input mammograms, the concept of transfer learning is applied. For that, the work adopts the three most efficient Residual Network (ResNet) families of CNN, namely ResNet18, ResNet50, and ResNet101 architectures. The input database used for evaluation is the INbreast dataset which comprises Full-Field Digital Mammogram (FFDM) images. At this point, the research compares the results obtained with the existing ELM and K-NN algorithms where it is found that the performance of the proposed framework provides the supreme classification (95.811% of accuracy) over others.","",""
42,"Sina Mohseni, E. Ragan","A Human-Grounded Evaluation Benchmark for Local Explanations of Machine Learning",2018,"","","","",177,"2022-07-13 09:38:01","","","","",,,,,42,10.50,21,2,4,"In order for people to be able to trust and take advantage of the results of advanced machine learning and artificial intelligence solutions for real decision making, people need to be able to understand the machine rationale for given output. Research in explain artificial intelligence (XAI) addresses the aim, but there is a need for evaluation of human relevance and understandability of explanations. Our work contributes a novel methodology for evaluating the quality or human interpretability of explanations for machine learning models. We present an evaluation benchmark for instance explanations from text and image classifiers. The explanation meta-data in this benchmark is generated from user annotations of image and text samples. We describe the benchmark and demonstrate its utility by a quantitative evaluation on explanations generated from a recent machine learning algorithm. This research demonstrates how human-grounded evaluation could be used as a measure to qualify local machine-learning explanations.","",""
6,"Kei Nakagawa, Masaya Abe, Junpei Komiyama","RIC-NN: A Robust Transferable Deep Learning Framework for Cross-sectional Investment Strategy",2019,"","","","",178,"2022-07-13 09:38:01","","10.1109/DSAA49011.2020.00051","","",,,,,6,2.00,2,3,3,"Stock return predictability is an important research theme as it reflects our economic and social organization, and significant efforts are made to explain the dynamism therein. Statistics of strong explanative power, called ""factor"", have been proposed to summarize the essence of predictive stock returns. The challenge here is to make a multi-factor investment strategy that is consistent over a reasonably long period based on supervised machine learning. Although machine learning methods are increasingly popular in stock return prediction, an inference of the stock return is highly elusive, and naive use of complex machine learning methods easily overfits the current data and results in poor performance on future data. We propose a principled stock return prediction framework that we call Ranked Information Coefficient Neural Network (RIC-NN) that alleviates the overfitting. RIC-NN addresses the difficulty that arises in nonconvex machine learning: Namely, initialization and the stopping of the training model and the transfer among several different tasks (markets). RIC-NN is a deep learning approach and includes the following three novel ideas: (1) nonlinear multi-factor approach, (2) stopping criteria with ranked information coefficient (rank IC), and (3) deep transfer learning among multiple regions. Experimental comparison with the stocks in the Morgan Stanley Capital International indices shows that RIC-NN outperforms not only off-the-shelf machine learning methods but also the average return of major equity investment funds in the last fourteen years.","",""
0,"Murilo Cruz Lopes, Marília de Matos Amorim, V. S. Freitas, R. Calumby","Survival Prediction for Oral Cancer Patients: A Machine Learning Approach",2021,"","","","",179,"2022-07-13 09:38:01","","10.5753/kdmile.2021.17466","","",,,,,0,0.00,0,4,1,"There is a high incidence of oral cancer in Brazil, with 150,000 new cases estimated for 2020-2022. In most cases, it is diagnosed at an advanced stage and are related to many risk factors. The Registro Hospitalar de Câncer (RHC), managed by Instituto Nacional de Câncer (INCA), is a nation-wide database that integrates cancer registers from several hospitals in Brazil. RHC is mostly an administrative database but also include clinical, socioeconomic and hospitalization data for each patient with a cancer diagnostic in the country. For these patients, prognostication is always a difficult task a demand multi-dimensional analysis. Therefore, exploiting large-scale data and machine intelligence approaches emerge as promising tool for computer-aided decision support on death risk estimation. Given the importance of this context, some works have reported high prognostication effectiveness, however with extremely limited data collections, relying on weak validation protocols or simple robustness analysis. Hence, this work describes a detailed workflow and experimental analysis for oral cancer patient survival prediction considering careful data curation and strict validation procedures. By exploiting multiple machine learning algorithms and optimization techniques the proposed approach allowed promising survival prediction effectiveness with F1 and AuC-ROC over 0.78 and 0.80, respectively. Moreover, a detailed analysis have shown that the minimization of different types of prediction errors were achieved by different models, which highlights the importance of the rigour in this kind of validation.","",""
18,"Li Fu, Lu Liu, Zhi-Jiang Yang, Pan Li, Jun-Jie Ding, Yong-Huan Yun, Aiping Lu, Tingjun Hou, Dongsheng Cao","Systematic Modeling of log D7.4 Based on Ensemble Machine Learning, Group Contribution, and Matched Molecular Pair Analysis",2019,"","","","",180,"2022-07-13 09:38:01","","10.1021/acs.jcim.9b00718","","",,,,,18,6.00,2,9,3,"Lipophilicity, as evaluated by the n-octanol/buffer solution distribution coefficient at pH = 7.4 (logD7.4), is a major determinant of various absorption, distribution, metabolism, elimination and toxicology (ADMET) parameters of drug candidates. In this study, we developed several quantitative structure-property relationship (QSPR) models to predict logD7.4 based on a large and structurally diverse data set. Eight popular machine learning algorithms were employed to build the prediction models with 43 molecular descriptors selected by a wrapper feature selection method. The results demonstrated XGBoost yielded better prediction performance than any other single model (RT2 = 0.906 and RMSET = 0.395). However, the consensus model from the top three models could continue to improve the prediction performance (RT2 = 0.922 and RMSET=0.359). The robustness, reliability, and generalization ability of the models were strictly evaluated by the Y-randomization test and applicability domain analysis. Moreover, the group contribution model based on 110 atom types and the local models for different ionization states were also established and compared with the global models. The results demonstrated that the descriptor-based consensus model is superior to the group contribution method, and the local models have no advantage over the global models. Finally, matched molecular pair (MMP) analysis and descriptor importance analysis were performed to extract transformation rules and give some explanations related to logD7.4. In conclusion, we believe that the consensus model developed in this study can be used as a reliable and promising tool to evaluate logD7.4 in drug discovery.","",""
33,"B. Abdollahi, O. Nasraoui","Transparency in Fair Machine Learning: the Case of Explainable Recommender Systems",2018,"","","","",181,"2022-07-13 09:38:01","","10.1007/978-3-319-90403-0_2","","",,,,,33,8.25,17,2,4,"","",""
33,"Ved P. Kafle, Y. Fukushima, P. Martinez-Julia, T. Miyazawa","Consideration On Automation of 5G Network Slicing with Machine Learning",2018,"","","","",182,"2022-07-13 09:38:01","","10.23919/ITU-WT.2018.8597639","","",,,,,33,8.25,8,4,4,"Machine learning has the capability to provide simpler solutions to complex problems by analyzing a huge volume of data in a short time, learning for adapting its functionality to dynamically changing environments, and predicting near future events with reasonably good accuracy. The 5G communication networks are getting complex due to emergence of unprecedentedly huge number of new connected devices and new types of services. Moreover, the requirements of creating virtual network slices suitable to provide optimal services for diverse users and applications are posing challenges to the efficient management of network resources, processing information about a huge volume of traffic, staying robust against all potential security threats, and adaptively adjustment of network functionality for time-varying workload. In this paper, we introduce about the envisioned 5G network slicing and elaborate the necessity of automation of network functions for the design, construction, deployment, operation, control and management of network slices. We then revisit the machine learning techniques that can be applied for the automation of network functions. We also discuss the status of artificial intelligence and machine learning related activities being progressed in standards development organizations and industrial forums.","",""
299,"J Zhang, M. Harman, Lei Ma, Yang Liu","Machine Learning Testing: Survey, Landscapes and Horizons",2019,"","","","",183,"2022-07-13 09:38:01","","10.1109/tse.2019.2962027","","",,,,,299,99.67,75,4,3,"This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.","",""
241,"B. Goldstein, A. Navar, R. Carter","Moving beyond regression techniques in cardiovascular risk prediction: applying machine learning to address analytic challenges",2016,"","","","",184,"2022-07-13 09:38:01","","10.1093/eurheartj/ehw302","","",,,,,241,40.17,80,3,6,"Abstract Risk prediction plays an important role in clinical cardiology research. Traditionally, most risk models have been based on regression models. While useful and robust, these statistical methods are limited to using a small number of predictors which operate in the same way on everyone, and uniformly throughout their range. The purpose of this review is to illustrate the use of machine-learning methods for development of risk prediction models. Typically presented as black box approaches, most machine-learning methods are aimed at solving particular challenges that arise in data analysis that are not well addressed by typical regression approaches. To illustrate these challenges, as well as how different methods can address them, we consider trying to predicting mortality after diagnosis of acute myocardial infarction. We use data derived from our institution's electronic health record and abstract data on 13 regularly measured laboratory markers. We walk through different challenges that arise in modelling these data and then introduce different machine-learning approaches. Finally, we discuss general issues in the application of machine-learning methods including tuning parameters, loss functions, variable importance, and missing data. Overall, this review serves as an introduction for those working on risk modelling to approach the diffuse field of machine learning.","",""
27,"Kacper Sokol, Peter A. Flach","Conversational Explanations of Machine Learning Predictions Through Class-contrastive Counterfactual Statements",2018,"","","","",185,"2022-07-13 09:38:01","","10.24963/ijcai.2018/836","","",,,,,27,6.75,14,2,4,"Machine learning models have become pervasive in our everyday life; they decide on important matters influencing our education, employment and judicial system. Many of these predictive systems are commercial products protected by trade secrets, hence their decision-making is opaque. Therefore, in our research we address interpretability and explainability of predictions made by machine learning models. Our work draws heavily on human explanation research in social sciences: contrastive and exemplar explanations provided through a dialogue. This user-centric design, focusing on a lay audience rather than domain experts, applied to machine learning allows explainees to drive the explanation to suit their needs instead of being served a precooked template.","",""
130,"A. Subasi, Jasmin Kevric, Muhammed Abdullah Canbaz","Epileptic seizure detection using hybrid machine learning methods",2017,"","","","",186,"2022-07-13 09:38:01","","10.1007/s00521-017-3003-y","","",,,,,130,26.00,43,3,5,"","",""
327,"Nicolas Papernot, P. Mcdaniel","Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning",2018,"","","","",187,"2022-07-13 09:38:01","","","","",,,,,327,81.75,164,2,4,"Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.","",""
9,"E. Ates, Burak Aksar, V. Leung, A. Coskun","Counterfactual Explanations for Machine Learning on Multivariate Time Series Data",2020,"","","","",188,"2022-07-13 09:38:01","","10.1109/ICAPAI49758.2021.9462056","","",,,,,9,4.50,2,4,2,"Applying machine learning (ML) on multivariate time series data has growing popularity in many application domains, including in computer system management. For example, recent high performance computing (HPC) research proposes a variety of ML frameworks that use system telemetry data in the form of multivariate time series so as to detect performance variations, perform intelligent scheduling or node allocation, and improve system security. Common barriers for adoption for these ML frameworks include the lack of user trust and the difficulty of debugging. These barriers need to be overcome to enable the widespread adoption of ML frameworks in production systems. To address this challenge, this paper proposes a novel explainability technique for providing counterfactual explanations for supervised ML frameworks that use multivariate time series data. The proposed method outperforms state-of-the-art explainability methods on several different ML frameworks and data sets in metrics such as faithfulness and robustness. The paper also demonstrates how the proposed method can be used to debug ML frameworks and gain a better understanding of HPC system telemetry data.","",""
0,"T. Sumers, Mark K. Ho, T. Griffiths","Show or Tell? Demonstration is More Robust to Changes in Shared Perception than Explanation",2020,"","","","",189,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,3,2,"Successful teaching entails a complex interaction between a teacher and a learner. The teacher must select and convey information based on what they think the learner perceives and believes. Teaching always involves misaligned beliefs, but studies of pedagogy often focus on situations where teachers and learners share perceptions. Nonetheless, a teacher and learner may not always experience or attend to the same aspects of the environment. Here, we study how misaligned perceptions influence communication. We hypothesize that the efficacy of different forms of communication depends on the shared perceptual state between teacher and learner. We develop a cooperative teaching game to test whether concrete mediums (demonstrations, or ""showing"") are more robust than abstract ones (language, or ""telling"") when the teacher and learner are not perceptually aligned. We find evidence that (1) language-based teaching is more affected by perceptual misalignment, but (2) demonstration-based teaching is less likely to convey nuanced information. We discuss implications for human pedagogy and machine learning.","",""
5,"Tengyang Wang, Guanghua Liu, Hongye Lin","A machine learning approach to predict intravenous immunoglobulin resistance in Kawasaki disease patients: A study based on a Southeast China population",2020,"","","","",190,"2022-07-13 09:38:01","","10.1371/journal.pone.0237321","","",,,,,5,2.50,2,3,2,"Kawasaki disease is the leading cause of pediatric acquired heart disease. Coronary artery abnormalities are the main complication of Kawasaki disease. Kawasaki disease patients with intravenous immunoglobulin resistance are at a greater risk of developing coronary artery abnormalities. Several scoring models have been established to predict resistance to intravenous immunoglobulin, but clinicians usually do not apply those models in patients because of their poor performance. To find a better model, we retrospectively collected data including 753 observations and 82 variables. A total of 644 observations were included in the analysis, and 124 of the patients observed were intravenous immunoglobulin resistant (19.25%). We considered 7 different linear and nonlinear machine learning algorithms, including logistic regression (L1 and L1 regularized), decision tree, random forest, AdaBoost, gradient boosting machine (GBM), and lightGBM, to predict the class of intravenous immunoglobulin resistance (binary classification). Data from patients who were discharged before Sep 2018 were included in the training set (n = 497), while all the data collected after 9/1/2018 were included in the test set (n = 147). We used the area under the ROC curve, accuracy, sensitivity, and specificity to evaluate the performances of each model. The gradient GBM had the best performance (area under the ROC curve 0.7423, accuracy 0.8844, sensitivity 0.3043, specificity 0.9919). Additionally, the feature importance was evaluated with SHapley Additive exPlanation (SHAP) values, and the clinical utility was assessed with decision curve analysis. We also compared our model with the Kobayashi score, Egami score, Formosa score and Kawamura score. Our machine learning model outperformed all of the aforementioned four scoring models. Our study demonstrates a novel and robust machine learning method to predict intravenous immunoglobulin resistance in Kawasaki disease patients. We believe this approach could be implemented in an electronic health record system as a form of clinical decision support in the near future.","",""
2,"Markus Jaeger, Stephan Krügel, D. Marinelli, J. Papenbrock, P. Schwendner","Understanding Machine Learning for Diversified Portfolio Construction by Explainable AI",2020,"","","","",191,"2022-07-13 09:38:01","","10.2139/ssrn.3528616","","",,,,,2,1.00,0,5,2,"In this paper, we construct a pipeline to investigate heuristic diversification strategies in asset allocation. We use machine learning concepts (""explainable AI"") to compare the robustness of different strategies and back out implicit rules for decision making.    In a first step, we augment the asset universe (the empirical dataset) with a range of scenarios generated with a block bootstrap from the empirical dataset.    Second, we backtest the candidate strategies over a long period of time, checking their performance variability. Third, we use XGBoost as a regression model to connect the difference between the measured performances between two strategies to a pool of statistical features of the portfolio universe tailored to the investigated strategy.    Finally, we employ the concept of Shapley values to extract the relationships that the model could identify between the portfolio characteristics and the statistical properties of the asset universe.    We test this pipeline for studying risk-parity strategies with a volatility target, and in particular, comparing the machine learning-driven Hierarchical Risk Parity (HRP) to the classical Equal Risk Contribution (ERC) strategy.    In the augmented dataset built from a multi-asset investment universe of commodities, equities and fixed income futures, we find that HRP better matches the volatility target, and shows better risk-adjusted performances. Finally, we train XGBoost to learn the difference between the realized Calmar ratios of HRP and ERC and extract explanations.    The explanations provide fruitful ex-post indications of the connection between the statistical properties of the universe and the strategy performance in the training set. For example, the model confirms that features addressing the hierarchical properties of the universe are connected to the relative performance of HRP respect to ERC.","",""
50,"Amir Feder, Nadav Oved, Uri Shalit, Roi Reichart","CausaLM: Causal Model Explanation Through Counterfactual Language Models",2020,"","","","",192,"2022-07-13 09:38:01","","10.1162/coli_a_00404","","",,,,,50,25.00,13,4,2,"Abstract Understanding predictions made by deep neural networks is notoriously difficult, but also crucial to their dissemination. As all machine learning–based methods, they are as good as their training data, and can also capture unwanted biases. While there are tools that can help understand whether such biases exist, they do not distinguish between correlation and causation, and might be ill-suited for text-based models and for reasoning about high-level language concepts. A key problem of estimating the causal effect of a concept of interest on a given model is that this estimation requires the generation of counterfactual examples, which is challenging with existing generation technology. To bridge that gap, we propose CausaLM, a framework for producing causal model explanations using counterfactual language representation models. Our approach is based on fine-tuning of deep contextualized embedding models with auxiliary adversarial tasks derived from the causal graph of the problem. Concretely, we show that by carefully choosing auxiliary adversarial pre-training tasks, language representation models such as BERT can effectively learn a counterfactual representation for a given concept of interest, and be used to estimate its true causal effect on model performance. A byproduct of our method is a language representation model that is unaffected by the tested concept, which can be useful in mitigating unwanted bias ingrained in the data.1","",""
1,"Zheren Ma, E. Davani, Xiaodan Ma, Hanna Lee, I. Arslan, Xiang Zhai, H. Darabi, D. Castineira","Finding a Trend Out of Chaos, A Machine Learning Approach for Well Spacing Optimization",2020,"","","","",193,"2022-07-13 09:38:01","","10.2118/201698-ms","","",,,,,1,0.50,0,8,2,"  Data-driven decisions powered by machine-learning methods are increasing in popularity when it comes to optimizing field development in unconventional reservoirs. However, since well performance is impacted by many factors (e.g., geological characteristics, completion design, well design, etc.), the challenge is uncovering trends from all the noise.  By leveraging basin-level knowledge captured by big data sculpting, integrating private and public data with the use of uncertainty quantification, Augmented AI (a combination of expert-based knowledge and advanced AI frameworks) can provide quick and science-based answers for well spacing and fracking optimization and assess the full potential of an asset in unconventional reservoirs.  Augmented AI is artificial intelligence powered by engineering wisdom. The Augmented AI workflow starts with data sculpting, which includes information retrieval, data cleaning and standardization, and finally a smart, deep and systematic data QC. Feature engineering generates all the relevant parameters going into the machine learning model—over 50 features have been generated for this work and categorized. The final step is to perform model tuning and ensemble, evaluating the model robustness, generating model explanation and uncertainty quantification. Augmented AI adopts an iterative machine learning modeling approach. This approach combines new and innovative engineering and G&G workflows with data-driven models so that a deep understanding of the field behavior can be developed. Loops from feature selection to model tuning are used until good model results are achieved. The loop is automated using Bayesians optimization. All machine learning models have different strengths and weaknesses for prediction. Instead of manually determining which machine learning model to use, this approach uses an adaptive ensemble machine learning approach that is a stacking algorithm that combines multiple regression models via a second level machine learning model. It smartly aggregates opinions from different models with reduced variance and better robustness.  Augmented AI has been applied in unconventional reservoirs with great results. A case study in Midland Basin is presented in this paper. Domain-induced feature engineering was performed to obtain important features for predicting well performance, and initial feature selection was conducted using feature correlation analysis. A trusted and explainable ML model was built and enhanced with uncertainty quantification. After running several sensitivity analyses, Augmented AI optimized the attributes of interest, then vetted the outcome, generating a report and visualizing the results.  In addition, further information about the direct impact of well spacing on EUR was deconvoluted from other parameters using an ML explanation technique for Wolfcamp Formation in Permian Basin and subsequently well spacing optimization was presented for the case study in Midland Basin.  An innovative model was created using Augmented AI to optimize well spacing, leveraging big data sculpting, domain and physics-induced feature engineering, and machine learning. The learning was transferred from the basin model to the specific region of interest. Augmented AI provides efficient and systematic private data organization, an explainable machine learning model, robust production forecast with quantified uncertainty and well spacing and frac parameters optimization.  Augmented AI models are already built for major basins such as Midland and Delaware basins. The learning and knowledge of the model can be transferred to any region in a basin and can be refined using more accurate private data. This allows conclusions to be drawn even with a limited number of wells.","",""
0,"E. Kondrateva, Polina Belozerova, M. Sharaev, Evgeny Burnaev, A. Bernstein, I. Samotaeva","Machine learning models reproducibility and validation for MR images recognition",2020,"","","","",194,"2022-07-13 09:38:01","","10.1117/12.2559525","","",,,,,0,0.00,0,6,2,"In the present work, we introduce a data processing and analysis pipeline, which ensures the reproducibility of machine learning models chosen for MR image recognition. The proposed pipeline is applied to solve the binary classification problems: epilepsy and depression diagnostics based on vectorized features from MR images. This model is then assessed in terms of classification performance, robustness and reliability of the results, including predictive accuracy on unseen data. The classification performance achieved with our approach compares favorably to ones reported in the literature, where usually no thorough model evaluation is performed.","",""
0,"G. Montavon, W. Samek","Statistics meets Machine Learning 5 Abstracts Explaining the decisions of deep neural networks and beyond",2020,"","","","",195,"2022-07-13 09:38:01","","","","",,,,,0,0.00,0,2,2,"Explaining the decisions of deep neural networks and beyond Grégoire Montavon and Wojciech Samek (joint work with Klaus-Robert Müller, Sebastian Lapuschkin, Alexander Binder, Jacob Kauffmann) Machine learning models have become increasingly complex and this complexity has allowed them to reach high prediction accuracy on challenging datasets. In some cases, improved predictivity has come at the expense of interpretability, in particular, complex models tend to be perceived as black-boxes. A lack of interpretability is problematic, not only because interpretability is desirable in itself (e.g. to extract useful insights from a model or from the modeled data), but also because common measurements of prediction accuracy can become strongly unreliable when certain assumptions about the training data are not met. Real-world datasets are typically not representative of all possible cases and the truly relevant variables may correlate with other irrelevant variables. In such circumstances, one would need to ensure that the machine learning model does not rely on these irrelevant variables. An assessment based purely on test set accuracy would be oblivious to the exact decision strategy and could overestimate the true prediction performance. This phenomenon has been referred to as the ‘Clever Hans’ effect [9]. Only an extension of the dataset with specific test cases, or an inspection of the model, e.g. via interpretability techniques [3, 16, 12], is capable of highlighting the improper decision structure. In this talk, we look at the question of explaining the predictions of deep neural networks, a successful machine learning approach that has been used increasingly in real-world applications. A challenge for getting these explanations is the complexity of the decision function, which makes it hard to apply simple explanation methods developed in the context of linear models, e.g. based on first-order Taylor expansions. In particular, DNN decision functions are highly nonlinear and multiscale, with a gradient that is highly varying or ‘shattered’ [4]. Also, local searches in the input space easily result in ‘adversarial examples’ [13] where the prediction no longer corresponds to the observed pattern in the input. Layer-wise relevance propagation (LRP) [3] is a technique that was proposed to robustly explain the neural network decision in terms of input features. It was shown to work on numerous models in a wide range of applications [14, 5, 15]. LRP departs from the neural network’s function representation to consider instead its graph structure. Specifically, the LRP algorithm performs an iterative redistribution of the neural network output to the lower layers. Redistribution from each layer to the layer below is achieved by means of propagation rules that satisfy a conservation property analogous to Kirchoff’s conservation laws in electrical circuits. The LRP algorithm terminates once the input layer has been reached. The LRP algorithm can be motivated as decomposing a complex problem 6 Oberwolfach Report 4/2020 (analyzing a highly nonlinear function) into a collection of simpler subproblems (treating each neuron individually). Furthermore, it was shown that the LRP algorithm can be interpreted as a collection of Taylor expansions performed at each layer and neuron of the neural network [11]. Specifically, the ‘relevance’ received by a given neuron is approximately the product of the neuron activation and a locally constant term. In turn, the LRP redistribution step can be interpreted as (1) identifying the linear terms of a Taylor expansion of the relevance expressed as a function of activations in the lower layer, and (2) propagating to the lower layer accordingly. A connection can be made between different proposed LRP propagation rules and the choice of reference point at which the Taylor expansion is performed [11, 10]. This Taylor-based view on the LRP algorithm allows in particular to verify that the corresponding reference points are meaningful, for example, that they satisfy domain membership constraints. This interpretation of LRP as a collection of Taylor expansions is referred to as “deep Taylor decomposition” [11]. The LRP algorithm has been successfully applied to various data types and problems, ranging from computer vision and natural language processing tasks such as classification of concepts in images [3], age prediction [8] or categorization of text documents [2], over reinforcement learning tasks such as playing computer games [9], to various medical data analysis tasks, e.g., decoding of fMRI signals [14] or therapy outcome prediction [15]. In these diverse applications, LRP explanations provide additional insights into the decision strategies used by the model, which not only help to better understand the data, including its biases and artifacts [8, 9], but also help to analyze the learning processes and model’s decision strategies [9]. In the second part of the talk, two recent advances that broaden the usefulness of explanation methods are discussed. First, Spectral Relevance Analysis (SpRAy) [9], a dataset-wide analysis of individual explanations that summarizes the overall decision structure of the model into a finite and easily interpretable set of prototypical decision strategies. This analysis allows to systematically investigate complex models on large datasets. It has unveiled in commonly used datasets, artifacts, that tend to systematically induce flaws into the decision structure of ML models trained on them. For example, a website logo was found in some images of the class ‘truck’ of the ImageNet dataset, which the state-of-the-art VGG-16 neural network would then use for its predictions [1]. Another advance brings successful explanation techniques to non-neural network architectures such as kernel-based models. The approach that we term ‘neuralization’ [6] finds for these non-neural network architectures a functionally equivalent neural network so that state-of-the-art explanation techniques such as LRP can be applied. The approach was successfully applied to various unsupervised models, in particular, kernel one-class SVMs [7] and various k-means clustering models [6], thereby shedding light into what input features make a data point anomalous or member of a given cluster. Statistics meets Machine Learning 7 Although significant progress has been made to improve the transparency of ML models such as deep neural networks, numerous challenges still need to be addressed both on the methods and theory side. In particular, there is a need for standardized and unbiased evaluation benchmarks for assessing the quality and usefulness of an explanation. Furthermore, an important future work will be to adopt a more holistic view on the problem of explanation, that considers how to make best use of the user’s interpretation and feedback capabilities, and that also integrates the end goal of the explanation method, for example, achieving better and more informed decisions, or systematically improving and robustifying a machine learning model.","",""
12,"N. Khoa, M. M. Alamdari, T. Rakotoarivelo, Ali Anaissi, Yang Wang","Structural Health Monitoring Using Machine Learning Techniques and Domain Knowledge Based Features",2018,"","","","",196,"2022-07-13 09:38:01","","10.1007/978-3-319-90403-0_20","","",,,,,12,3.00,2,5,4,"","",""
4,"Jungang Lou, Yunliang Jiang, Qing Shen, Ruiqin Wang, Zechao Li","Probabilistic Regularized Extreme Learning for Robust Modeling of Traffic Flow Forecasting.",2020,"","","","",197,"2022-07-13 09:38:01","","10.1109/TNNLS.2020.3027822","","",,,,,4,2.00,1,5,2,"The adaptive neurofuzzy inference system (ANFIS) is a structured multioutput learning machine that has been successfully adopted in learning problems without noise or outliers. However, it does not work well for learning problems with noise or outliers. High-accuracy real-time forecasting of traffic flow is extremely difficult due to the effect of noise or outliers from complex traffic conditions. In this study, a novel probabilistic learning system, probabilistic regularized extreme learning machine combined with ANFIS (probabilistic R-ELANFIS), is proposed to capture the correlations among traffic flow data and, thereby, improve the accuracy of traffic flow forecasting. The new learning system adopts a fantastic objective function that minimizes both the mean and the variance of the model bias. The results from an experiment based on real-world traffic flow data showed that, compared with some kernel-based approaches, neural network approaches, and conventional ANFIS learning systems, the proposed probabilistic R-ELANFIS achieves competitive performance in terms of forecasting ability and generalizability.","",""
8,"Teodora Popordanoska, Mohit Kumar, Stefano Teso","Machine Guides, Human Supervises: Interactive Learning with Global Explanations",2020,"","","","",198,"2022-07-13 09:38:01","","","","",,,,,8,4.00,3,3,2,"We introduce explanatory guided learning (XGL), a novel interactive learning strategy in which a machine guides a human supervisor toward selecting informative examples for a classifier. The guidance is provided by means of global explanations, which summarize the classifier's behavior on different regions of the instance space and expose its flaws. Compared to other explanatory interactive learning strategies, which are machine-initiated and rely on local explanations, XGL is designed to be robust against cases in which the explanations supplied by the machine oversell the classifier's quality. Moreover, XGL leverages global explanations to open up the black-box of human-initiated interaction, enabling supervisors to select informative examples that challenge the learned model. By drawing a link to interactive machine teaching, we show theoretically that global explanations are a viable approach for guiding supervisors. Our simulations show that explanatory guided learning avoids overselling the model's quality and performs comparably or better than machine- and human-initiated interactive learning strategies in terms of model quality.","",""
80,"Xiaoqin Zhang, Mingyu Fan, Di Wang, Peng Zhou, D. Tao","Top-k Feature Selection Framework Using Robust 0–1 Integer Programming",2020,"","","","",199,"2022-07-13 09:38:01","","10.1109/TNNLS.2020.3009209","","",,,,,80,40.00,16,5,2,"Feature selection (FS), which identifies the relevant features in a data set to facilitate subsequent data analysis, is a fundamental problem in machine learning and has been widely studied in recent years. Most FS methods rank the features in order of their scores based on a specific criterion and then select the <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> top-ranked features, where <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> is the number of desired features. However, these features are usually not the top-<inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> features and may present a suboptimal choice. To address this issue, we propose a novel FS framework in this article to select the exact top-<inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> features in the unsupervised, semisupervised, and supervised scenarios. The new framework utilizes the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm as the matrix sparsity constraint rather than its relaxations, such as the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{1,2}$ </tex-math></inline-formula>-norm. Since the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm constrained problem is difficult to solve, we transform the discrete <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm-based constraint into an equivalent 0–1 integer constraint and replace the 0–1 integer constraint with two continuous constraints. The obtained top-<inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> FS framework with two continuous constraints is theoretically equivalent to the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm constrained problem and can be optimized by the alternating direction method of multipliers (ADMM). Unsupervised and semisupervised FS methods are developed based on the proposed framework, and extensive experiments on real-world data sets are conducted to demonstrate the effectiveness of the proposed FS framework.","",""
8,"Xiaoxuan Lu, Yushen Long, Han Zou, Chengpu Yu, Lihua Xie","Robust extreme learning machine for regression problems with its application to wifi based indoor positioning system",2014,"","","","",200,"2022-07-13 09:38:01","","10.1109/MLSP.2014.6958903","","",,,,,8,1.00,2,5,8,"We propose two kinds of robust extreme learning machines (RELMs) based on the close-to-mean constraint and the small-residual constraint respectively to solve the problem of noisy measurements in indoor positioning systems (IPSs). We formulate both RELMs as second order cone programming problems. The fact that feature mapping in ELM is known to users is exploited to give the needed information for robust constraints. Real-world indoor localization experimental results show that, the proposed algorithms can not only improve the accuracy and repeatability, but also reduce the deviations and worst case errors of IPSs compared with basic ELM and OPT-ELM based IPSs.","",""
