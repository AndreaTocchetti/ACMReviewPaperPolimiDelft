Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
0,"R. Shokri","Trusting Machine Learning: Privacy, Robustness, and Transparency Challenges",2019,"","","","",1,"2022-07-13 09:23:05","","10.1145/3335203.3335728","","",,,,,0,0.00,0,1,3,"Machine learning algorithms have shown an unprecedented predictive power for many complex learning tasks. As they are increasingly being deployed in large scale critical applications for processing various types of data, new questions related to their trustworthiness would arise. Can machine learning algorithms be trusted to have access to individuals’ sensitive data? Can they be robust against noisy or adversarially perturbed data? Can we reliably interpret their learning process, and explain their predictions? In this talk, I will go over the challenges of building trustworthy machine learning algorithms in centralized and distributed (federated) settings, and will discuss the inter-relation between privacy, robustness, and interpretability.","",""
13,"Xinlei Mi, Baiming Zou, F. Zou, J. Hu","Permutation-based identification of important biomarkers for complex diseases via machine learning models",2021,"","","","",2,"2022-07-13 09:23:05","","10.1038/s41467-021-22756-2","","",,,,,13,13.00,3,4,1,"","",""
1,"Khansa Rasheed, A. Qayyum, M. Ghaly, A. Al-Fuqaha, A. Razi, Junaid Qadir","Explainable, Trustworthy, and Ethical Machine Learning for Healthcare: A Survey",2021,"","","","",3,"2022-07-13 09:23:05","","10.36227/TECHRXIV.14376179.V1","","",,,,,1,1.00,0,6,1,"With the advent of machine learning (ML) applications in daily life, the questions about liability, trust, and interpretability of their outputs are raising, especially for healthcare applications. The black-box nature of ML models is a roadblock for clinical utilization. Therefore, to gain the trust of clinicians and patients, researchers need to provide explanations of how and why the model is making a specific decision. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the field of eXplainable ML (XML). In this paper, we provide a comprehensive review of explainable and interpretable ML techniques implemented for providing the reasons behind their decisions for various healthcare applications. Along with highlighting various security, safety, and robustness challenges that hinder the trustworthiness of ML we also discussed the ethical issues of healthcare ML and describe how explainable and trustworthy ML can resolve these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.","",""
0,"Eike Petersen, Yannik Potdevin, Esfandiar Mohammadi, S. Zidowitz, Sabrina Breyer, Dirk Nowotka, Sandra Henn, Ludwig Pechmann, M. Leucker, P. Rostalski, C. Herzog","Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Technical Challenges and Solutions",2021,"","","","",4,"2022-07-13 09:23:05","","","","",,,,,0,0.00,0,11,1,"Machine learning is expected to fuel significant improvements in medical care. To ensure that fundamental principles such as beneficence, respect for human autonomy, prevention of harm, justice, privacy, and transparency are respected, medical machine learning applications must be developed responsibly. A large number of high-level declarations of ethical principles have been put forth for this purpose, but there is a severe lack of technical guidelines explicating the practical consequences for medical machine learning. Similarly, there is currently considerable uncertainty regarding the exact regulatory requirements placed upon medical machine learning systems. In this paper, we survey the technical challenges involved in creating medical machine learning systems responsibly and in conformity with existing regulations, as well as possible solutions to address these challenges. We begin by providing a brief overview of existing regulations affecting medical machine learning, showing that properties such as safety, robustness, reliability, privacy, security, transparency, explainability, and nondiscrimination are all demanded already by existing law and regulations — albeit, in many cases, to an uncertain degree. Next, we discuss the key technical obstacles to achieving these desirable properties, and important techniques to overcome those barriers in the medical context. Since most of the technical challenges are very young and new problems frequently emerge, the scientific discourse is rapidly evolving and has not yet converged on clear best-practice solutions. Nevertheless, we aim to illuminate the underlying technical challenges, possible ways for addressing them, and their respective merits and drawbacks. In particular, we notice that distribution shift, spurious correlations, model underspecification, and data scarcity represent severe challenges in the medical context (and others) that are very difficult to solve with classical black-box deep neural networks. Important measures that may help to address these challenges include the use of large and representative datasets and federated learning as a means to that end, the careful exploitation of domain knowledge wherever feasible, the use of inherently transparent models, comprehensive model testing and verification, as well as stakeholder inclusion. ar X iv :2 10 7. 09 54 6v 1 [ cs .L G ] 2 0 Ju l 2 02 1","",""
0,"Zhixin Pan, P. Mishra","Automated Detection of Spectre and Meltdown Attacks Using Explainable Machine Learning",2021,"","","","",5,"2022-07-13 09:23:05","","10.1109/HOST49136.2021.9702278","","",,,,,0,0.00,0,2,1,"Spectre and Meltdown attacks exploit security vulnerabilities of advanced architectural features to access inherently concealed memory data without authorization. Existing defense mechanisms have three major drawbacks: (i) they can be fooled by obfuscation techniques, (ii) the lack of transparency severely limits their applicability, and (iii) it can introduce unacceptable performance degradation. In this paper, we propose a novel detection scheme based on explainable machine learning to address these fundamental challenges. Specifically, this paper makes three important contributions. (1) Our work is the first attempt in applying explainable machine learning for Spectre and Meltdown attack detection. (2) Our proposed method utilizes the temporal differences of hardware events in sequential timestamps instead of overall statistics, which contributes to the robustness of ML models against evasive attacks. (3) Extensive experimental evaluation demonstrates that our approach can significantly improve detection efficiency (38.4% on average) compared to state-of-the-art techniques.","",""
5,"P. Santhanam","Quality Management of Machine Learning Systems",2020,"","","","",6,"2022-07-13 09:23:05","","10.1007/978-3-030-62144-5_1","","",,,,,5,2.50,5,1,2,"","",""
1,"Michael T. Kiley","Financial Conditions and Economic Activity: Insights from Machine Learning",2020,"","","","",7,"2022-07-13 09:23:05","","10.17016/FEDS.2020.095","","",,,,,1,0.50,1,1,2,"Machine learning (ML) techniques are used to construct a financial conditions index (FCI). The components of the ML-FCI are selected based on their ability to predict the unemployment rate one-year ahead. Three lessons for macroeconomics and variable selection/dimension reduction with large datasets emerge. First, variable transformations can drive results, emphasizing the need for transparency in selection of transformations and robustness to a range of reasonable choices. Second, there is strong evidence of nonlinearity in the relationship between financial variables and economic activityâ€”tight financial conditions are associated with sharp deteriorations in economic activity and accommodative conditions are associated with only modest improvements in activity. Finally, the ML-FCI places sizable weight on equity prices and term spreads, in contrast to other measures. These lessons yield an ML-FCI showing tightening in financial conditions before the early 1990s and early 2000s recessions, in contrast to the National Financial Conditions Index (NFCI).","",""
0,"J. Filipe, Ashish Ghosh, R. Prates, O. Shehory, E. Farchi, Guy Barash","Engineering Dependable and Secure Machine Learning Systems: Third International Workshop, EDSMLS 2020, New York City, NY, USA, February 7, 2020, Revised Selected Papers",2020,"","","","",8,"2022-07-13 09:23:05","","10.1007/978-3-030-62144-5","","",,,,,0,0.00,0,6,2,"","",""
51,"Shubham Sharma, Jette Henderson, Joydeep Ghosh","CERTIFAI: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models",2019,"","","","",9,"2022-07-13 09:23:05","","10.1145/3375627.3375812","","",,,,,51,17.00,17,3,3,"As artificial intelligence plays an increasingly important role in our society, there are ethical and moral obligations for both businesses and researchers to ensure that their machine learning models are designed, deployed, and maintained responsibly. These models need to be rigorously audited for fairness, robustness, transparency, and interpretability. A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming. In this paper, we introduce a unified and model-agnostic approach to address these issues: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models (CERTIFAI). Unlike previous methods in this domain, CERTIFAI is a general tool that can be applied to any black-box model and any type of input data. Given a model and an input instance, CERTIFAI uses a custom genetic algorithm to generate counterfactuals: instances close to the input that change the prediction of the model. We demonstrate how these counterfactuals can be used to examine issues of robustness, interpretability, transparency, and fairness. Additionally, we introduce CERScore, the first black-box model robustness score that performs comparably to methods that have access to model internals.","",""
0,"A. Aksjonov, V. Kyrki","A Safety-Critical Decision Making and Control Framework Combining Machine Learning and Rule-based Algorithms",2022,"","","","",10,"2022-07-13 09:23:05","","","","",,,,,0,0.00,0,2,1,"While artificial-intelligence-based methods suffer from lack of transparency, rule-based methods dominate in safety-critical systems. Yet, the latter cannot compete with the first ones in robustness to multiple requirements, for instance, simultaneously addressing safety, comfort, and efficiency. Hence, to benefit from both methods they must be joined in a single system. This paper proposes a decision making and control framework, which profits from advantages of both the ruleand machine-learning-based techniques while compensating for their disadvantages. The proposed method embodies two controllers operating in parallel, called Safety and Learned. A rule-based switching logic selects one of the actions transmitted from both controllers. The Safety controller is prioritized every time, when the Learned one does not meet the safety constraint, and also directly participates in the safe Learned controller training. Decision making and control in autonomous driving is chosen as the system case study, where an autonomous vehicle learns a multi-task policy to safely cross an unprotected intersection. Multiple requirements (i.e., safety, efficiency, and comfort) are set for vehicle operation. A numerical simulation is performed for the proposed framework validation, where its ability to satisfy the requirements and robustness to changing environment is successfully demonstrated.","",""
7,"Sean Saito, Eugene Chua, Nicholas Capel, Rocco Hu","Improving LIME Robustness with Smarter Locality Sampling",2020,"","","","",11,"2022-07-13 09:23:05","","","","",,,,,7,3.50,2,4,2,"Explainability algorithms such as LIME have enabled machine learning systems to adopt transparency and fairness, which are important qualities in commercial use cases. However, recent work has shown that LIME's naive sampling strategy can be exploited by an adversary to conceal biased, harmful behavior. We propose to make LIME more robust by training a generative adversarial network to sample more realistic synthetic data which the explainer uses to generate explanations. Our experiments demonstrate that our proposed method demonstrates an increase in accuracy across three real-world datasets in detecting biased, adversarial behavior compared to vanilla LIME. This is achieved while maintaining comparable explanation quality, with up to 99.94\% in top-1 accuracy in some cases.","",""
0,"A. Schenk, U. Clausen","Creating Transparency in the Finished Vehicles Transportation Process Through the Implementation of a Real-Time Decision Support System",2020,"","","","",12,"2022-07-13 09:23:05","","10.1109/IEEM45057.2020.9309978","","",,,,,0,0.00,0,2,2,"The complexity of global distribution networks in the automotive industry and likewise the number of disruptions significantly increased throughout the last years. In order to monitor relevant processes and to optimize decision-making in case of disruptions, a concept for a decision support system (DSS) was introduced. For this purpose, the distribution process weaknesses of the German premium automotive company BMW were identified. The method used was a Failure Mode and Effect Analysis with operational managers and relevant process partners interviews. Based on the findings, performance indicators, thresholds, early warnings and options for action were specified. A big data platform supports the processing of the growing number of relevant data in real-time. In the long-term decision-making can be automated using machine learning algorithms. This paper proves that negative impacts of disruptions can be minimized, and the robustness of the process improved by anticipating and identifying deviations beforehand and in real-time. Hence, companies save money while strengthening customer satisfaction. The DSS can be seen as a necessary precursor of a digital twin.","",""
1,"G. Vouros","Explainable Deep Reinforcement Learning: State of the Art and Challenges",2022,"","","","",13,"2022-07-13 09:23:05","","10.1145/3527448","","",,,,,1,1.00,1,1,1,"Interpretability, explainability and transparency are key issues to introducing Artificial Intelligence methods in many critical domains: This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability and fairness, and has important consequences towards keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. While the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article we aim to provide a review of state of the art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators - i.e., of those that take the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state of the art methods, categorizing them in classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes identifying open questions and important challenges.","",""
4,"Haw-Shiuan Chang, Shankar Vembu, Sunil Mohan, Rheeya Uppaal, A. McCallum","Using error decay prediction to overcome practical issues of deep active learning for named entity recognition",2019,"","","","",14,"2022-07-13 09:23:05","","10.1007/s10994-020-05897-1","","",,,,,4,1.33,1,5,3,"","",""
0,"Juanjuan Tian, Li Li","Digital Universal Financial Credit Risk Analysis Using Particle Swarm Optimization Algorithm with Structure Decision Tree Learning-Based Evaluation Model",2022,"","","","",15,"2022-07-13 09:23:05","","10.1155/2022/4060256","","",,,,,0,0.00,0,2,1,"Predictions of credit risk, model reliability, monitoring, and efficient loan processing are all important factors in decision-making and transparency. Machine learning method is providing these people new hope. However, it is up to banking or nonbanking institutions to determine how they will implement this advanced method in order to decrease human biases in loan decision-making. Objective. This paper proposed the novel machine learning-based credit risk analysis in the digital banking evaluation model. The purpose of this research is to compare various ML algorithms in order to develop an accurate model for credit risk assessment utilising data from a genuine credit registry dataset. Aim is to design the classification-based model using particle swarm optimization (PSO) algorithm with structure decision tree learning (SDTL) in predicting credit risk. This system has the potential to improve quality criteria such as dependability, robustness, extensibility, and scalability. Features have been extracted and classified by the proposed PSO_SL model. Experimental Results. The data have been collected based on real time for credit analysis. Simulation is carried out in Python and optimal results are obtained in comparative analysis with existing techniques. The accuracy obtained by proposed technique is enhanced and the error rate of the design is minimized.","",""
0,"Joeri Lenaerts, Hannah Pinson, V. Ginis","Deep learning the design of optical components",2020,"","","","",16,"2022-07-13 09:23:05","","10.1117/12.2568664","","",,,,,0,0.00,0,3,2,"In addition to the celebrated numerical techniques, such as Finite-Element and Finite-Difference methods, it is also possible to predict the scattering properties of optical components using artificial neural networks. However, these machine-learning models typically suffer from a simplicity versus accuracy trade-off. In our work, we overcome this trade-off. We train several neural networks with an indirect goal. Instead of training the net to predict scattering, we try to train it the laws of Optics on a more fundamental level. In this way, we can increase the predictive power and robustness while maintaining a high degree of transparency in the system.","",""
2,"Rainer Schulz, Martin Wersing","Automated Valuation Services: A case study for Aberdeen in Scotland",2021,"","","","",17,"2022-07-13 09:23:05","","10.1080/09599916.2020.1861066","","",,,,,2,2.00,1,2,1,"ABSTRACT Automated valuation services (AVSs) offered by listings platforms predict market values based on property characteristics supplied by users. We investigate the implementation of such a service for the City of Aberdeen. We fit different market value models with machine learning methods and assess them in a rolling windows procedure that mimics an AVS setting. We also investigate the ease and robustness with which the models can be implemented. We discuss how prediction uncertainty can be measured and reported to users. If implemented in the future, such a service has the potential to improve the transparency of the local housing market.","",""
1,"Michael Tsang, James Enouen, Yan Liu","Interpretable Artificial Intelligence through the Lens of Feature Interaction",2021,"","","","",18,"2022-07-13 09:23:05","","","","",,,,,1,1.00,0,3,1,"Deep learning, alongside other modern machine learning techniques, has become the state of the art solution for a diverse range of real-world tasks. These include a variety of sensitive applications such as healthcare, finance, autonomous driving, criminal justice, and others which all pose significant concerns for fairness, robustness, safety, and trustworthiness. Despite these applications to critical tasks, deep networks are infamously referred to as black-box models because of their total lack of transparency in decision-making. If we are able to gain insight into how a model is coming to its conclusions, we are able to more clearly assess the trustworthiness and validity of its decisions. Consequently, an abundance of ongoing research is attempting to address model interpretability as the key problem to resolving these issues. There are many methods which are currently used to provide explanations of complex model predictions. LIME (Ribeiro et al., 2016) fits a local linear model around a data point, showing which features positively and negatively influence the prediction results. Despite the overall model being nonlinear, the local model gets an interpretable picture of how the model looks at small scales around the data point. Extensions of this method use other interpretable models like small decision tress. Shapley Values and SHAP follows a similar idea to assign a score to each feature, using a gametheoretic formulation which treats each feature as a player causing the final prediction (Lundberg and Lee, 2017). Its more rigorous formulation yields guarantees of its explanations summing up to the prediction score, but practically it usually must be estimated because of its high computational cost. Shuffle-based feature importance permutes the data of each feature to ascertain its importance in the final prediction in comparison to its normal prediction (Fisher et al., 2018). IG uses the fundamental theorem of calculus to provide additive explanations of a prediction (Sundararajan et al., 2017). This method is very popular in computer vision where its computational efficiency and saliency are prized, even though other work has exposed some of its shortcomings in providing an interpretation (Adebayo et al., 2018). Other methods are specifically designed for computer vision like TCAV (Kim et al., 2018) which finds a ‘concept direction’ corresponding to a large sample of concept images from the user. Surprisingly, all of these most popular interpretability methods share the same one limitation. None of these methods consider the shared importance of groups of two or more features; they only look at the effects had by each of the features individually. A feature interaction between two variables broadly describes a situation where both of the features/ variables are simultaneously important for a model’s prediction. In text applications for sentiment, ”not good” is a very simple example of two words strongly interacting with one another to create a negative sentiment. In modern-day applications, neural networks are usually hailed as amazing function approximators exactly because of their incredible ability to automatically uncover these kinds of complex relationships between the variables of the dataset. In many ways, however,","",""
0,"A. Nesen, K. Solaiman, Bharat Bhargava","Dataset Augmentation with Generated Novelties",2021,"","","","",19,"2022-07-13 09:23:05","","10.1109/TransAI51903.2021.00015","","",,,,,0,0.00,0,3,1,"As machine learning models take over an increasingly larger number of domains in our lives, their accuracy, fairness, transparency and adaptability become of greater importance. In the everchanging environments, the resulting ability of the models to perform accurately depends on whether they are able to handle novel, unpredicted and unforeseen instances, examples and classes or any other novel changes in the world of model operation, such as environmental, contextual, distributional changes. The proper handling of novelties sustains the model’s usefulness and adeptness in the long run. The efficiency of response to the encounter of novelties depends on the efforts that were invested at the model training, design and data collection stages. In this work, we propose a variety of approaches and methods which can be incorporated into the novelty generation techniques at the earliest stages of creating the machine learning dataset and the model to assure its robustness and reduce the bias. We revisit distinctions between novelties and anomalies to define a formal novelty generation framework that is domain-agnostic and budget efficient. Then we propose a video-specific use case and evaluate the result of the chosen methods on the video dataset. Our methods aim at making the machine learning solutions adaptable, responsible and show improvement in the accuracy and ability of the models to detect novelties.","",""
0,"G. El-khawaga, Mervat Abu-Elkheir, M. Reichert","XAI in the Context of Predictive Process Monitoring: An Empirical Analysis Framework",2022,"","","","",20,"2022-07-13 09:23:05","","10.3390/a15060199","","",,,,,0,0.00,0,3,1,"Predictive Process Monitoring (PPM) has been integrated into process mining use cases as a value-adding task. PPM provides useful predictions on the future of the running business processes with respect to different perspectives, such as the upcoming activities to be executed next, the final execution outcome, and performance indicators. In the context of PPM, Machine Learning (ML) techniques are widely employed. In order to gain trust of stakeholders regarding the reliability of PPM predictions, eXplainable Artificial Intelligence (XAI) methods have been increasingly used to compensate for the lack of transparency of most of predictive models. Multiple XAI methods exist providing explanations for almost all types of ML models. However, for the same data, as well as, under the same preprocessing settings or same ML models, generated explanations often vary significantly. Corresponding variations might jeopardize the consistency and robustness of the explanations and, subsequently, the utility of the corresponding model and pipeline settings. This paper introduces a framework that enables the analysis of the impact PPM-related settings and ML-model-related choices may have on the characteristics and expressiveness of the generated explanations. Our framework provides a means to examine explanations generated either for the whole reasoning process of an ML model, or for the predictions made on the future of a certain business process instance. Using well-defined experiments with different settings, we uncover how choices made through a PPM workflow affect and can be reflected through explanations. This framework further provides the means to compare how different characteristics of explainability methods can shape the resulting explanations and reflect on the underlying model reasoning process.","",""
1,"M. Bagheri, Majid Mohrekesh, N. Karimi, S. Samavi, S. Shirani, P. Khadivi","Image Watermarking with Region of Interest Determination Using Deep Neural Networks",2020,"","","","",21,"2022-07-13 09:23:05","","10.1109/ICMLA51294.2020.00172","","",,,,,1,0.50,0,6,2,"Watermarking is a popular technique used in various applications, such as copyright protection of digital media, including audio, video, and image files. Proper watermarking should satisfy multiple criteria, such as robustness and transparency. While a successful watermarking needs to meet these criteria, there is a tradeoff between the two opposing criteria of robustness and transparency. This paper proposes a method for determining the appropriate locations for embedding watermarks with high strength factors. For this purpose, a deep neural network, known as Mask R-CNN, is used, which is pre-trained on the COCO dataset. This neural network finds a good strength factor for those sub-blocks of the host image selected for embedding. The proposed technique can be used in conjunction with most DWT and DCT based semi-blind watermarking approaches. Experiments show that the proposed method is robust against different attacks and demonstrates good transparency.","",""
0,"Xinlei Mi, Baiming Zou, F. Zou, J. Hu","Permutation-based Identification of Important Biomarkers for Complex Diseases via Black-box Models",2020,"","","","",22,"2022-07-13 09:23:05","","10.1101/2020.04.27.064170","","",,,,,0,0.00,0,4,2,"Study of human disease remains challenging due to convoluted disease etiologies and complex molecular mechanisms at genetic, genomic, and proteomic levels. Many machine learning-based methods, including deep learning and random forest, have been developed and widely used to alleviate some analytic challenges in complex human disease studies. While enjoying the modeling flexibility and robustness, these model frameworks suffer from non-transparency and difficulty in interpreting the role of each individual feature due to their intrinsic black-box natures. However, identifying important biomarkers associated with complex human diseases is a critical pursuit towards assisting researchers to establish novel hypotheses regarding prevention, diagnosis and treatment of complex human diseases. Herein, we propose a Permutation-based Feature Importance Test (PermFIT) for estimating and testing the feature importance, and for assisting interpretation of individual feature in various black-box frameworks, including deep neural networks, random forests, and support vector machines. PermFIT (available at https://github.com/SkadiEye/deepTL) is implemented in a computationally efficient manner, without model refitting for each permuted data. We conduct extensive numerical studies under various scenarios, and show that PermFIT not only yields valid statistical inference, but also helps to improve the prediction accuracy of black-box models with top selected features. With the application to the Cancer Genome Atlas (TCGA) kidney tumor data and the HITChip atlas BMI data, PermFIT clearly demonstrates its practical usage in identifying important biomarkers and boosting performance of black-box predictive models.","",""
2,"Jun Zhang, Xiaoyi Zhou, Jilin Yang, Chunjie Cao, Jixin Ma","Adaptive Robust Blind Watermarking Scheme Improved by Entropy-Based SVM and Optimized Quantum Genetic Algorithm",2019,"","","","",23,"2022-07-13 09:23:05","","10.1155/2019/7817809","","",,,,,2,0.67,0,5,3,"With the intensive study of machine learning in digital watermarking, its ability to balance the robustness and transparency of watermarking technology has attracted researchers’ attention. Therefore, quantum genetic algorithm, which serves as an intelligent optimized scheme combined with biological genetic mechanism and quantum computing, is widely used in various fields. In this study, an adaptive robust blind watermarking algorithm by means of optimized quantum genetics (OQGA) and entropy classification-based SVM (support vector machine) is proposed. The host image was divided into two parts according to the odd and even rows of the host image. One part was transformed by DCT (discrete cosine transform), and then the embedding intensity and position were separately trained by entropy-based SVM and OQGA; the other part was by DWT (discrete wavelet transform), in which the key fusion was achieved by an ergodic matrix to embed the watermark. Simulation results indicate the proposed algorithm ensures the watermark scheme transparency as well as having better resistance to common attacks such as lossy JPEG compression, image darken, Gaussian low-pass filtering, contrast decreasing, salt-pepper noise, and geometric attacks such as rotation and cropping.","",""
16,"Andree Thieltges, F. Schmidt, Simon Hegelich","The Devil's Triangle: Ethical Considerations on Developing Bot Detection Methods",2016,"","","","",24,"2022-07-13 09:23:05","","","","",,,,,16,2.67,5,3,6,"Social media is increasingly populated with bots. To protect the authenticity of the user, experience machine learning algorithms are used to detect these bots. Ethical dimensions of these methods have not been thoroughly considered yet. Taking histogram analysis of Twitter users' profile images as example, the paper demonstrates the trade-offs of accuracy, transparency, and robustness. Because there is no general optimum in ethical considerations, these dimensions form a ""devil's triangle"".","",""
1,"G. Stevenson","An approach to situation recognition based on learned semantic models",2015,"","","","",25,"2022-07-13 09:23:05","","","","",,,,,1,0.14,1,1,7,"A key enabler of pervasive computing is the ability to drive service delivery through the analysis of situations: Semantically meaningful classifications of system state, identified through analysing the readings from sensors attached to the everyday objects that people interact with. Situation recognition is a mature area of research, with techniques primarily falling into two categories. Knowledge-based techniques use inference rules crafted by experts; however often they compensate poorly for sensing peculiarities. Learning-based approaches excel at extracting patterns from noisy training data, however their lack of transparency can make it difficult to diagnose errors. In this thesis we propose a novel hybrid approach to situation recognition that combines both techniques. This offers improvements over each used individually, through not sacrificing the intelligibility of the decision processes that the use of machine learning alone often implies, and through providing better recognition accuracy through robustness to noise typically unattainable when developers use knowledge-based techniques in isolation. We present an ontology model and reasoning framework that supports the uniform modelling of pervasive environments, and infers additional knowledge from that which is specified, in a principled way. We use this as a basis from which to learn situation recognition models that exhibit comparable performance with more complex machine learning techniques, while retaining intelligibility. Finally, we extend the approach to construct ensemble classifiers with either improved recognition accuracy, intelligibility or both. To validate our approach, we apply the techniques to real-world data sets collected in smart-office and smart-home environments. We analyse the situation recognition performance and intelligibility of the decision processes, and compare the results to standard machine learning techniques and results published in the literature.","",""
5,"Hua Lian, Bo-Ning Hu, Rui-Mei Zhao, Yan-Li Hou","Design of digital watermarking algorithm based on wavelet transform",2010,"","","","",26,"2022-07-13 09:23:05","","10.1109/ICMLC.2010.5580639","","",,,,,5,0.42,1,4,12,"A blind watermarking algorithm based on wavelet transform domain is proposed. This algorithm use two-level wavelet transform on the original image. Reference the coefficients of level detail sub-band in one-level wavelet transform and adjust the two-level wavelet coefficients in the same direction adaptive to achieve embedded watermark information. The result shows that the embedded watermark has good transparency and robustness, and the watermark can be extracted from the watermarked image without the original image.","",""
18,"D. Suc, I. Bratko","Skill modeling through symbolic reconstruction of operator's trajectories",2000,"","","","",27,"2022-07-13 09:23:05","","10.1109/3468.895885","","",,,,,18,0.82,9,2,22,"Controlling a complex dynamic system, such as a plane or a crane, usually requires a skilled operator. Such control skill is typically hard to reconstruct through introspection. Therefore an attractive approach to the reconstruction of control skill involves machine learning from operator's control traces, also known as behavioral cloning. In the most common approach to behavioral cloning, a controller is induced as a direct mapping from system states to actions. Unfortunately, such controllers usually suffer from lack of robustness and lack typical elements of human control strategies, such as subgoals and substages of the control plan. We investigate a novel approach. We apply the GoldHorn program to induce from the operator's trajectories a set of symbolic constraints. These are then used together with a locally weighted regression model to determine the next action. Using the Acrobot problem in a case study, this approach showed significant improvements both in terms of control performance and transparency of induced clones.","",""
4,"Ming-Shing Hsieh","Image watermarking based on fuzzy inference filter",2009,"","","","",28,"2022-07-13 09:23:05","","10.1109/ICMLC.2009.5212603","","",,,,,4,0.31,4,1,13,"In this paper, an efficiently DWT-based watermarking technique is proposed to embed signatures in images to attest the owner identification and discourage the unauthorized copying. This paper deals with a fuzzy inference filter to choose the larger entropy of coefficients to embed watermarks. Unlike most previous watermarking frameworks which embedded watermarks in the larger coefficients of inner coarser subbands, the proposed technique is based on utilizing a context model and fuzzy inference filter by embedding watermarks in the larger-entropy coefficients of coarser DWT subbands.Our schemes have been shown to provide very good results in both image transparency and robustness.","",""
0,"Peer-Olaf Siebers","Hongmei He (intelligent System Lab, University of Bristol): Soft Computing Approaches under the Framework of Hierarchical Decision Making or Classification System",2010,"","","","",29,"2022-07-13 09:23:05","","","","",,,,,0,0.00,0,1,12,"Hongmei He (Intelligent System Lab, University of Bristol): Soft Computing Approaches Under the Framework of Hierarchical Decision Making or Classification System With the development of AI, we can see there are a surprising number of the brain functions of the human Intelligent System (IS) are quite similar to those of an artificial IS, since most artificial ISs are modelled through naturally emulating human intelligence. A wide variety of approaches have been utilised in the functional design of artificial ISs. For example, fuzzy logic for robustness, decision trees for the transparency of reasoning, machine learning for knowledge learning, semantics for understandability, probabilistic reasoning, and neural computing, etc.","",""
0,"Ying-Jian Qi, Ying Li, Lei-Fan Yan, Yi Yuan, Qing Li","The application of grey model in 3D model blind watermark",2010,"","","","",30,"2022-07-13 09:23:05","","10.1109/ICMLC.2010.5580583","","",,,,,0,0.00,0,5,12,"In order to solve the copyright issues brought by the distributed rendering system for three-dimensional model, we present this blind watermarking algorithm with high transparency and robustness. In the generation step, we compressed the original watermark information using the UGM model of gray system, for reducing the amount of information embedded in watermark. In the embedding step, we replace some bits in the model feature field with the watermarking signal so as to extract watermark without the original data. Experimental results show good transparency and robustness against attacks.","",""
9,"Liang Xiao, Huizhong Wu, Zhihui Wei, Y. Bao","Perceptual digital watermark of images using ridgelet transform",2004,"","","","",31,"2022-07-13 09:23:05","","10.1109/ICMLC.2004.1380539","","",,,,,9,0.50,2,4,18,"A perceptually based watermarking technique for image is proposed. It provides a new visual model, which can exactly evaluate the just noticeable distortion (JND) tolerance of human visual system. The watermarking algorithm is designed to address the issue of how to improve the robustness and the security of the embedded watermark according to the image content In order to improve the robustness and transparency, the method is first developed to determine significant ridgelet subbands and to select a couple of significant ridgelet coefficients in these subbands automatically, then the watermark bits are inserted in the visual significant ridgelet coefficients without exceeding the maximum strength of JND. With the spread of spectrum technique and the secret key protection, the security of the embedded watermark is also guaranteed very well. Experiments show that the proposed method can achieve a better tradeoff between the robustness and transparency.","",""
9,"Kacper Sokol, Peter A. Flach","One Explanation Does Not Fit All: The Promise of Interactive Explanations for Machine Learning Transparency",2020,"","","","",32,"2022-07-13 09:23:05","","","","",,,,,9,4.50,5,2,2,"The need for transparency of predictive systems based on Machine Learning algorithms arises as a consequence of their ever-increasing proliferation in the industry. Whenever black-box algorithmic predictions influence human affairs, the inner workings of these algorithms should be scrutinised and their decisions explained to the relevant stakeholders, including the system engineers, the system’s operators and the individuals whose case is being decided. While a variety of interpretability and explainability methods is available, none of them is a panacea that can satisfy all diverse expectations and competing objectives that might be required by the parties involved. We address this challenge in this paper by discussing the promises of Interactive Machine Learning for improved transparency of black-box systems using the example of contrastive explanations – a state-of-the-art approach to Interpretable Machine Learning. Specifically, we show how to personalise counterfactual explanations by interactively adjusting their conditional statements and extract additional explanations by asking follow-up “What if?” questions. Our experience in building, deploying and presenting this type of system allowed us to list desired properties as well as potential limitations, which can be used to guide the development of interactive explainers. While customising the medium of interaction, i.e., the user interface comprising of various communication channels, may give an Kacper Sokol Department of Computer Science, University of Bristol Bristol, United Kingdom E-mail: K.Sokol@bristol.ac.uk Peter Flach Department of Computer Science, University of Bristol Bristol, United Kingdom E-mail: Peter.Flach@bristol.ac.uk impression of personalisation, we argue that adjusting the explanation itself and its content is more important. To this end, properties such as breadth, scope, context, purpose and target of the explanation have to be considered, in addition to explicitly informing the explainee about its limitations and caveats. Furthermore, we discuss the challenges of mirroring the explainee’s mental model, which is the main building block of intelligible human-machine interactions. We also deliberate on the risks of allowing the explainee to freely manipulate the explanations and thereby extracting information about the underlying predictive model, which might be leveraged by malicious actors to steal or game the model. Finally, building an end-to-end interactive explainability system is a challenging engineering task; unless the main goal is its deployment, we recommend “Wizard of Oz” studies as a proxy for testing and evaluating standalone interactive explainability algorithms.","",""
13,"Michael Bücker, G. Szepannek, Alicja Gosiewska, P. Biecek","Transparency, auditability, and explainability of machine learning models in credit scoring",2020,"","","","",33,"2022-07-13 09:23:05","","10.1080/01605682.2021.1922098","","",,,,,13,6.50,3,4,2,"Abstract A major requirement for credit scoring models is to provide a maximally accurate risk prediction. Additionally, regulators demand these models to be transparent and auditable. Thus, in credit scoring, very simple predictive models such as logistic regression or decision trees are still widely used and the superior predictive power of modern machine learning algorithms cannot be fully leveraged. Significant potential is therefore missed, leading to higher reserves or more credit defaults. This article works out different dimensions that have to be considered for making credit scoring models understandable and presents a framework for making “black box” machine learning models transparent, auditable, and explainable. Following this framework, we present an overview of techniques, demonstrate how they can be applied in credit scoring and how results compare to the interpretability of scorecards. A real world case study shows that a comparable degree of interpretability can be achieved while machine learning techniques keep their ability to improve predictive power.","",""
129,"S. Vollmer, B. Mateen, G. Bohner, F. Király, R. Ghani, P. Jónsson, Sarah Cumbers, Adrian Jonas, K. McAllister, P. Myles, David Grainger, M. Birse, Richard Branson, K. Moons, G. Collins, J. Ioannidis, C. Holmes, H. Hemingway","Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness",2020,"","","","",34,"2022-07-13 09:23:05","","10.1136/bmj.l6927","","",,,,,129,64.50,13,18,2,"Machine learning, artificial intelligence, and other modern statistical methods are providing new opportunities to operationalise previously untapped and rapidly growing sources of data for patient benefit. Despite much promising research currently being undertaken, particularly in imaging, the literature as a whole lacks transparency, clear reporting to facilitate replicability, exploration for potential ethical concerns, and clear demonstrations of effectiveness. Among the many reasons why these problems exist, one of the most important (for which we provide a preliminary solution here) is the current lack of best practice guidance specific to machine learning and artificial intelligence. However, we believe that interdisciplinary groups pursuing research and impact projects involving machine learning and artificial intelligence for health would benefit from explicitly addressing a series of questions concerning transparency, reproducibility, ethics, and effectiveness (TREE). The 20 critical questions proposed here provide a framework for research groups to inform the design, conduct, and reporting; for editors and peer reviewers to evaluate contributions to the literature; and for patients, clinicians and policy makers to critically appraise where new findings may deliver patient benefit.","",""
9,"Tian Zhang, Qi Liu, Yihang Dan, Shuai Yu, Xu Han, Jian Dai, Kun Xu","Machine learning and evolutionary algorithm studies of graphene metamaterials for optimized plasmon-induced transparency.",2019,"","","","",35,"2022-07-13 09:23:05","","10.1364/OE.389231","","",,,,,9,3.00,1,7,3,"Machine learning and optimization algorithms have been widely applied in the design and optimization for photonics devices. We briefly review recent progress of this field of research and show data-driven applications, including spectrum prediction, inverse design and performance optimization, for novel graphene metamaterials (GMs). The structure of the GMs is well-designed to achieve the wideband plasmon induced transparency (PIT) effect, which can be theoretically demonstrated by using the transfer matrix method. Some traditional machine learning algorithms, including k nearest neighbour, decision tree, random forest and artificial neural networks, are utilized to equivalently substitute the numerical simulation in the forward spectrum prediction and complete the inverse design for the GMs. The calculated results demonstrate that all algorithms are effective and the random forest has advantages in terms of accuracy and training speed. Moreover, evolutionary algorithms, including single-objective (genetic algorithm) and multi-objective optimization (NSGA-II), are used to achieve the steep transmission characteristics of PIT effect by synthetically taking many different performance metrics into consideration. The maximum difference between the transmission peaks and dips in the optimized transmission spectrum reaches 0.97. In comparison to previous works, we provide a guidance for intelligent design of photonics devices based on machine learning and evolutionary algorithms and a reference for the selection of machine learning algorithms for simple inverse design problems.","",""
16,"","Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness",2020,"","","","",36,"2022-07-13 09:23:05","","10.1136/bmj.m1312","","",,,,,16,8.00,0,0,2,"","",""
87,"B. Hutchinson, A. Smart, A. Hanna, Emily L. Denton, Christina Greer, Oddur Kjartansson, Parker Barnes, Margaret Mitchell","Towards Accountability for Machine Learning Datasets: Practices from Software Engineering and Infrastructure",2020,"","","","",37,"2022-07-13 09:23:05","","10.1145/3442188.3445918","","",,,,,87,43.50,11,8,2,"Datasets that power machine learning are often used, shared, and reused with little visibility into the processes of deliberation that led to their creation. As artificial intelligence systems are increasingly used in high-stakes tasks, system development and deployment practices must be adapted to address the very real consequences of how model development data is constructed and used in practice. This includes greater transparency about data, and accountability for decisions made when developing it. In this paper, we introduce a rigorous framework for dataset development transparency that supports decision-making and accountability. The framework uses the cyclical, infrastructural and engineering nature of dataset development to draw on best practices from the software development lifecycle. Each stage of the data development lifecycle yields documents that facilitate improved communication and decision-making, as well as drawing attention to the value and necessity of careful data work. The proposed framework makes visible the often overlooked work and decisions that go into dataset creation, a critical step in closing the accountability gap in artificial intelligence and a critical/necessary resource aligned with recent work on auditing processes.","",""
65,"Jianlong Zhou, A. Gandomi, Fang Chen, Andreas Holzinger","Evaluating the Quality of Machine Learning Explanations: A Survey on Methods and Metrics",2021,"","","","",38,"2022-07-13 09:23:05","","10.3390/ELECTRONICS10050593","","",,,,,65,65.00,16,4,1,"The most successful Machine Learning (ML) systems remain complex black boxes to end-users, and even experts are often unable to understand the rationale behind their decisions. The lack of transparency of such systems can have severe consequences or poor uses of limited valuable resources in medical diagnosis, financial decision-making, and in other high-stake domains. Therefore, the issue of ML explanation has experienced a surge in interest from the research community to application domains. While numerous explanation methods have been explored, there is a need for evaluations to quantify the quality of explanation methods to determine whether and to what extent the offered explainability achieves the defined objective, and compare available explanation methods and suggest the best explanation from the comparison for a specific task. This survey paper presents a comprehensive overview of methods proposed in the current literature for the evaluation of ML explanations. We identify properties of explainability from the review of definitions of explainability. The identified properties of explainability are used as objectives that evaluation metrics should achieve. The survey found that the quantitative metrics for both model-based and example-based explanations are primarily used to evaluate the parsimony/simplicity of interpretability, while the quantitative metrics for attribution-based explanations are primarily used to evaluate the soundness of fidelity of explainability. The survey also demonstrated that subjective measures, such as trust and confidence, have been embraced as the focal point for the human-centered evaluation of explainable systems. The paper concludes that the evaluation of ML explanations is a multidisciplinary research topic. It is also not possible to define an implementation of evaluation metrics, which can be applied to all explanation methods.","",""
55,"Qianwen Wang, Yao Ming, Zhihua Jin, Qiaomu Shen, Dongyu Liu, Micah J. Smith, K. Veeramachaneni, Huamin Qu","ATMSeer: Increasing Transparency and Controllability in Automated Machine Learning",2019,"","","","",39,"2022-07-13 09:23:05","","10.1145/3290605.3300911","","",,,,,55,18.33,7,8,3,"To relieve the pain of manually selecting machine learning algorithms and tuning hyperparameters, automated machine learning (AutoML) methods have been developed to automatically search for good models. Due to the huge model search space, it is impossible to try all models. Users tend to distrust automatic results and increase the search budget as much as they can, thereby undermining the efficiency of AutoML. To address these issues, we design and implement ATMSeer, an interactive visualization tool that supports users in refining the search space of AutoML and in analyzing the results. To guide the design of ATMSeer, we derive a workflow of using AutoML based on interviews with machine learning experts. A multi-granularity visualization is proposed to enable users to monitor the AutoML process, analyze the searched models, and refine the search space in real time. We demonstrate the utility and usability of ATMSeer through two case studies, expert interviews, and a user study with 13 end users.","",""
23,"Inioluwa Deborah Raji, Jingyi Yang","ABOUT ML: Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles",2019,"","","","",40,"2022-07-13 09:23:05","","","","",,,,,23,7.67,12,2,3,"We present the ""Annotation and Benchmarking on Understanding and Transparency of Machine Learning Lifecycles"" (ABOUT ML) project as an initiative to operationalize ML transparency and work towards a standard ML documentation practice. We make the case for the project's relevance and effectiveness in consolidating disparate efforts across a variety of stakeholders, as well as bringing in the perspectives of currently missing voices that will be valuable in shaping future conversations. We describe the details of the initiative and the gaps we hope this project will help address.","",""
15,"Hubert Baniecki, Wojciech Kretowicz, Piotr Piatyszek, J. Wiśniewski, P. Biecek","dalex: Responsible Machine Learning with Interactive Explainability and Fairness in Python",2020,"","","","",41,"2022-07-13 09:23:05","","","","",,,,,15,7.50,3,5,2,"The increasing amount of available data, computing power, and the constant pursuit for higher performance results in the growing complexity of predictive models. Their black-box nature leads to opaqueness debt phenomenon inflicting increased risks of discrimination, lack of reproducibility, and deflated performance due to data drift. To manage these risks, good MLOps practices ask for better validation of model performance and fairness, higher explainability, and continuous monitoring. The necessity of deeper model transparency appears not only from scientific and social domains, but also emerging laws and regulations on artificial intelligence. To facilitate the development of responsible machine learning models, we showcase dalex, a Python package which implements the model-agnostic interface for interactive model exploration. It adopts the design crafted through the development of various tools for responsible machine learning; thus, it aims at the unification of the existing solutions. This library's source code and documentation are available under open license at this https URL","",""
77,"A. Qayyum, Junaid Qadir, M. Bilal, A. Al-Fuqaha","Secure and Robust Machine Learning for Healthcare: A Survey",2020,"","","","",42,"2022-07-13 09:23:05","","10.1109/RBME.2020.3013489","","",,,,,77,38.50,19,4,2,"Recent years have witnessed widespread adoption of machine learning (ML)/deep learning (DL) techniques due to their superior performance for a variety of healthcare applications ranging from the prediction of cardiac arrest from one-dimensional heart signals to computer-aided diagnosis (CADx) using multi-dimensional medical images. Notwithstanding the impressive performance of ML/DL, there are still lingering doubts regarding the robustness of ML/DL in healthcare settings (which is traditionally considered quite challenging due to the myriad security and privacy issues involved), especially in light of recent results that have shown that ML/DL are vulnerable to adversarial attacks. In this paper, we present an overview of various application areas in healthcare that leverage such techniques from security and privacy point of view and present associated challenges. In addition, we present potential methods to ensure secure and privacy-preserving ML for healthcare applications. Finally, we provide insight into the current research challenges and promising directions for future research.","",""
312,"Jonas Rauber, Wieland Brendel, M. Bethge","Foolbox: A Python toolbox to benchmark the robustness of machine learning models",2017,"","","","",43,"2022-07-13 09:23:05","","","","",,,,,312,62.40,104,3,5,"Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at https://github.com/bethgelab/foolbox. The most up-to-date documentation can be found at http://foolbox.readthedocs.io. In 2013, Szegedy et al. demonstrated that minimal perturbations, often almost imperceptible to humans, can have devastating effects on machine predictions. These so-called adversarial perturbations thus demonstrate a striking difference between human and machine perception. As a result, adversarial perturbations have been subject to many Equal contribution Centre for Integrative Neuroscience, University of Tübingen, Germany Bernstein Center for Computational Neuroscience, Tübingen, Germany International Max Planck Research School for Intelligent Systems, Tübingen, Germany Max Planck Institute for Biological Cybernetics, Tübingen, Germany Institute for Theoretical Physics, University of Tübingen, Germany. Correspondence to: Jonas Rauber <jonas.rauber@bethgelab.org>. Reliable Machine Learning in the Wild Workshop, 34 th International Conference on Machine Learning, Sydney, Australia, 2017. studies concerning the generation of such perturbations and strategies to protect machine learning models such as deep neural networks against them. A practical definition of the robustness R of a model, first used by Szegedy et al. (2013), is the average size of the minimum adversarial perturbation ρ(x) across many samples x, R = 〈ρ(x)〉 x where (1) ρ(x) = min δ d(x,x+ δ) s.t. x+ δ is adversarial (2) and d(·) is some distance measure. Unfortunately, finding the global minimum adversarial perturbation is close to impossible in any practical setting, and we thus employ heuristic attacks to find a suitable approximation. Such heuristics, however, can fail, in which case we could easily be mislead to believe that a model is robust (Brendel & Bethge, 2017). Our best strategy is thus to employ as many attacks as possible, and to use the minimal perturbation found across all attacks as an approximation to the true global minimum. At the moment, however, such a strategy is severely obstructed by two problems: first, the code for most known attack methods is either not available at all, or only available for one particular deep learning framework. Second, implementations of the same attack often differ in many details and are thus not directly comparable. Foolbox improves upon the existing Python package cleverhans by Papernot et al. (2016b) in three important aspects: 1. It interfaces with most popular machine learning frameworks such as PyTorch, Keras, TensorFlow, Theano, Lasagne and MXNet and provides a straight forward way to add support for other frameworks, 2. it provides reference implementations for more than 15 adversarial attacks with a simple and consistent API, and 3. it supports many different criteria for adversarial examples, including custom ones. This technical report is structured as follows: In section 1 we provide an overview over Foolbox and demonstrate Foolbox: A Python toolbox to benchmark the robustness of machine learning models how to benchmark a model and report the result. In section 2 we describe the adversarial attack methods that are implemented in Foolbox and explain the internal hyperparameter tuning.","",""
9,"Jake Goldenfein","Algorithmic Transparency and Decision-Making Accountability: Thoughts for Buying Machine Learning Algorithms",2019,"","","","",44,"2022-07-13 09:23:05","","","","",,,,,9,3.00,9,1,3,"There has been a great deal of research on how to achieve algorithmic accountability and transparency in automated decision-making systems - especially for those used in public governance. However, good accountability in the implementation and use of automated decision-making systems is far from simple. It involves multiple overlapping institutional, technical, and political considerations, and becomes all the more complex in the context of machine learning based, rather than rule based, decision systems. This chapter argues that relying on human oversight of automated systems, so called ‘human-in-the-loop’ approaches, is entirely deficient, and suggests addressing transparency and accountability during the procurement phase of machine learning systems - during their specification and parameterisation - is absolutely critical. In a machine learning based automated decision system, the accountability typically associated with a public official making a decision has already been displaced into the actions and decisions of those creating the system - the bureaucrats and engineers involved in building the relevant models, curating the datasets, and implementing a system institutionally. But what should those system designers be thinking about and asking for when specifying those systems?    There are a lot of accountability mechanisms available for system designers to consider, including new computational transparency mechanisms, ‘fairness’ and non-discrimination, and ‘explainability’ of decisions. If an official specifies for a system to be transparent, fair, or explainable, however, it is important that they understand the limitations of such a specification in the context of machine learning. Each of these approaches is fraught with risks, limitations, and the challenging political economy of technology platforms in government. Without understand the complexities and limitations of those accountability and transparency ideas, they risk disempowering public officials in the face of private industry technology vendors, who use trade secrets and market power in deeply problematic ways, as well as producing deficient accountability outcomes. This chapter therefore outlines the risks associated with corporate cooption of those transparency and accountability mechanisms, and suggests that significant resources must be invested in developing the necessary skills in the public sector for deciding whether a machine learning system is useful and desirable, and how it might be made as accountable and transparent as possible.","",""
3,"Martin Strobel","Aspects of Transparency in Machine Learning",2019,"","","","",45,"2022-07-13 09:23:05","","","","",,,,,3,1.00,3,1,3,"The fact that machine learning is growing more and more entrenched in almost every aspect of society, combined with the opacity of various of its algorithms has induced the relatively young research area of transparent machine learning. The aim of this domain is to provide explanations for automated decisions to increase public trust. In my thesis, I am going to consider certain problems that arise from this research agenda. Particularly, I so far considered, the dilemma of conflicting explanations and the issue of privacy concerns arising from transparency.","",""
23,"Sina Shaham, Ming Ding, Bo Liu, Shuping Dang, Zihuai Lin, Jun Li","Privacy Preserving Location Data Publishing: A Machine Learning Approach",2019,"","","","",46,"2022-07-13 09:23:05","","10.1109/tkde.2020.2964658","","",,,,,23,7.67,4,6,3,"Publishing datasets plays an essential role in open data research and promoting transparency of government agencies. However, such data publication might reveal users’ private information. One of the most sensitive sources of data is spatiotemporal trajectory datasets. Unfortunately, merely removing unique identifiers cannot preserve the privacy of users. Adversaries may know parts of the trajectories or be able to link the published dataset to other sources for the purpose of user identification. Therefore, it is crucial to apply privacy preserving techniques before the publication of spatiotemporal trajectory datasets. In this paper, we propose a robust framework for the anonymization of spatiotemporal trajectory datasets termed as machine learning based anonymization (MLA). By introducing a new formulation of the problem, we are able to apply machine learning algorithms for clustering the trajectories and propose to use <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=""shaham-ieq1-2964658.gif""/></alternatives></inline-formula>-means algorithm for this purpose. A variation of <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=""shaham-ieq2-2964658.gif""/></alternatives></inline-formula>-means algorithm is also proposed to preserve the privacy in overly sensitive datasets. Moreover, we improve the alignment process by considering multiple sequence alignment as part of the MLA. The framework and all the proposed algorithms are applied to T-Drive, Geolife, and Gowalla location datasets. The experimental results indicate a significantly higher utility of datasets by anonymization based on MLA framework.","",""
16,"Milena Pavlović, Lonneke Scheffer, Keshav Motwani, Chakravarthi Kanduri, Radmila Kompova, Nikolay Vazov, K. Waagan, Fabian L. M. Bernal, A. Costa, Brian D Corrie, R. Akbar, Ghadi S. Al Hajj, Gabriel Balaban, T. Brusko, Maria Chernigovskaya, S. Christley, L. Cowell, R. Frank, Ivar Grytten, Sveinung Gundersen, Ingrid Hobæk Haff, S. Hochreiter, E. Hovig, Ping-Han Hsieh, G. Klambauer, M. Kuijjer, Christin Lund-Andersen, A. Martini, Thomas Minotto, J. Pensar, K. Rand, E. Riccardi, Philippe A. Robert, Artur Rocha, Andrei Slabodkin, I. Snapkov, L. Sollid, Dmytro Titov, Cédric R. Weber, Michael Widrich, G. Yaari, V. Greiff, G. K. Sandve","immuneML: an ecosystem for machine learning analysis of adaptive immune receptor repertoires",2021,"","","","",47,"2022-07-13 09:23:05","","10.1101/2021.03.08.433891","","",,,,,16,16.00,2,43,1,"Adaptive immune receptor repertoires (AIRR) are key targets for biomedical research as they record past and ongoing adaptive immune responses. The capacity of machine learning (ML) to identify complex discriminative sequence patterns renders it an ideal approach for AIRR-based diagnostic and therapeutic discovery. To date, widespread adoption of AIRR ML has been inhibited by a lack of reproducibility, transparency, and interoperability. immuneML (immuneml.uio.no) addresses these concerns by implementing each step of the AIRR ML process in an extensible, open-source software ecosystem that is based on fully specified and shareable workflows. To facilitate widespread user adoption, immuneML is available as a command-line tool and through an intuitive Galaxy web interface, and extensive documentation of workflows is provided. We demonstrate the broad applicability of immuneML by (i) reproducing a large-scale study on immune state prediction, (ii) developing, integrating, and applying a novel method for antigen specificity prediction, and (iii) showcasing streamlined interpretability-focused benchmarking of AIRR ML.","",""
22,"A. Antoniadi, Yuhan Du, Yasmine Guendouz, Lan Wei, C. Mazo, Brett A. Becker, C. Mooney","Current Challenges and Future Opportunities for XAI in Machine Learning-Based Clinical Decision Support Systems: A Systematic Review",2021,"","","","",48,"2022-07-13 09:23:05","","10.3390/APP11115088","","",,,,,22,22.00,3,7,1,"Machine Learning and Artificial Intelligence (AI) more broadly have great immediate and future potential for transforming almost all aspects of medicine. However, in many applications, even outside medicine, a lack of transparency in AI applications has become increasingly problematic. This is particularly pronounced where users need to interpret the output of AI systems. Explainable AI (XAI) provides a rationale that allows users to understand why a system has produced a given output. The output can then be interpreted within a given context. One area that is in great need of XAI is that of Clinical Decision Support Systems (CDSSs). These systems support medical practitioners in their clinic decision-making and in the absence of explainability may lead to issues of under or over-reliance. Providing explanations for how recommendations are arrived at will allow practitioners to make more nuanced, and in some cases, life-saving decisions. The need for XAI in CDSS, and the medical field in general, is amplified by the need for ethical and fair decision-making and the fact that AI trained with historical data can be a reinforcement agent of historical actions and biases that should be uncovered. We performed a systematic literature review of work to-date in the application of XAI in CDSS. Tabular data processing XAI-enabled systems are the most common, while XAI-enabled CDSS for text analysis are the least common in literature. There is more interest in developers for the provision of local explanations, while there was almost a balance between post-hoc and ante-hoc explanations, as well as between model-specific and model-agnostic techniques. Studies reported benefits of the use of XAI such as the fact that it could enhance decision confidence for clinicians, or generate the hypothesis about causality, which ultimately leads to increased trustworthiness and acceptability of the system and potential for its incorporation in the clinical workflow. However, we found an overall distinct lack of application of XAI in the context of CDSS and, in particular, a lack of user studies exploring the needs of clinicians. We propose some guidelines for the implementation of XAI in CDSS and explore some opportunities, challenges, and future research needs.","",""
18,"H. Wallach","Big Data, Machine Learning, and the Social Sciences: Fairness, Accountability, and Transparency",2019,"","","","",49,"2022-07-13 09:23:05","","","","",,,,,18,6.00,18,1,3,"","",""
161,"A. Bhagoji, Daniel Cullina, Chawin Sitawarin, Prateek Mittal","Enhancing robustness of machine learning systems via data transformations",2017,"","","","",50,"2022-07-13 09:23:05","","10.1109/CISS.2018.8362326","","",,,,,161,32.20,40,4,5,"We propose the use of data transformations as a defense against evasion attacks on ML classifiers. We present and investigate strategies for incorporating a variety of data transformations including dimensionality reduction via Principal Component Analysis to enhance the resilience of machine learning, targeting both the classification and the training phase. We empirically evaluate and demonstrate the feasibility of linear transformations of data as a defense mechanism against evasion attacks using multiple real-world datasets. Our key findings are that the defense is (i) effective against the best known evasion attacks from the literature, resulting in a two-fold increase in the resources required by a white-box adversary with knowledge of the defense for a successful attack, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification and human activity classification.","",""
0,"Junbo Wang, Amitangshu Pal, Qinglin Yang, K. Kant, Kaiming Zhu, Song Guo","Collaborative Machine Learning: Schemes, Robustness, and Privacy.",2022,"","","","",51,"2022-07-13 09:23:05","","10.1109/TNNLS.2022.3169347","","",,,,,0,0.00,0,6,1,"Distributed machine learning (ML) was originally introduced to solve a complex ML problem in a parallel way for more efficient usage of computation resources. In recent years, such learning has been extended to satisfy other objectives, namely, performing learning in situ on the training data at multiple locations and keeping the training datasets private while still allowing sharing of the model. However, these objectives have led to considerable research on the vulnerabilities of distributed learning both in terms of privacy concerns of the training data and the robustness of the learned overall model due to bad or maliciously crafted training data. This article provides a comprehensive survey of various privacy, security, and robustness issues in distributed ML.","",""
90,"Paul B. de Laat","Algorithmic Decision-Making Based on Machine Learning from Big Data: Can Transparency Restore Accountability?",2017,"","","","",52,"2022-07-13 09:23:05","","10.1007/s13347-017-0293-z","","",,,,,90,18.00,90,1,5,"","",""
8,"Björn Lütjens, Lucas Liebenwein, K. Kramer","Machine Learning-based Estimation of Forest Carbon Stocks to increase Transparency of Forest Preservation Efforts",2019,"","","","",53,"2022-07-13 09:23:05","","","","",,,,,8,2.67,3,3,3,"An increasing amount of companies and cities plan to become CO2-neutral, which requires them to invest in renewable energies and carbon emission offsetting solutions. One of the cheapest carbon offsetting solutions is preventing deforestation in developing nations, a major contributor in global greenhouse gas emissions. However, forest preservation projects historically display an issue of trust and transparency, which drives companies to invest in transparent, but expensive air carbon capture facilities. Preservation projects could conduct accurate forest inventories (tree diameter, species, height etc.) to transparently estimate the biomass and amount of stored carbon. However, current rainforest inventories are too inaccurate, because they are often based on a few expensive ground-based samples and/or low-resolution satellite imagery. LiDAR-based solutions, used in US forests, are accurate, but cost-prohibitive, and hardly-accessible in the Amazon rainforest. We propose accurate and cheap forest inventory analyses through Deep Learning-based processing of drone imagery. The more transparent estimation of stored carbon will create higher transparency towards clients and thereby increase trust and investment into forest preservation projects.","",""
49,"Eric Wong, J. Z. Kolter","Learning perturbation sets for robust machine learning",2020,"","","","",54,"2022-07-13 09:23:05","","","","",,,,,49,24.50,25,2,2,"Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline spatial transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to train models which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at this https URL.","",""
73,"P. D. Laat","Algorithmic Decision-Making Based on Machine Learning from Big Data: Can Transparency Restore Accountability?",2018,"","","","",55,"2022-07-13 09:23:05","","10.1007/S13347-017-0293-Z","","",,,,,73,18.25,73,1,4,"","",""
270,"Jonas Rauber, Wieland Brendel, M. Bethge","Foolbox v0.8.0: A Python toolbox to benchmark the robustness of machine learning models",2017,"","","","",56,"2022-07-13 09:23:05","","","","",,,,,270,54.00,90,3,5,"Even todays most advanced machine learning models are easily fooled by almost imperceptible perturbations of their inputs. Foolbox is a new Python package to generate such adversarial perturbations and to quantify and compare the robustness of machine learning models. It is build around the idea that the most comparable robustness measure is the minimum perturbation needed to craft an adversarial example. To this end, Foolbox provides reference implementations of most published adversarial attack methods alongside some new ones, all of which perform internal hyperparameter tuning to find the minimum adversarial perturbation. Additionally, Foolbox interfaces with most popular deep learning frameworks such as PyTorch, Keras, TensorFlow, Theano and MXNet and allows different adversarial criteria such as targeted misclassification and top-k misclassification as well as different distance measures. The code is licensed under the MIT license and is openly available at this https URL . The most up-to-date documentation can be found at this http URL .","",""
243,"Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, R. Puri, J. Moura, P. Eckersley","Explainable machine learning in deployment",2019,"","","","",57,"2022-07-13 09:23:05","","10.1145/3351095.3375624","","",,,,,243,81.00,24,10,3,"Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.","",""
33,"B. Abdollahi, O. Nasraoui","Transparency in Fair Machine Learning: the Case of Explainable Recommender Systems",2018,"","","","",58,"2022-07-13 09:23:05","","10.1007/978-3-319-90403-0_2","","",,,,,33,8.25,17,2,4,"","",""
224,"R. Roscher, B. Bohn, Marco F. Duarte, J. Garcke","Explainable Machine Learning for Scientific Insights and Discoveries",2019,"","","","",59,"2022-07-13 09:23:05","","10.1109/ACCESS.2020.2976199","","",,,,,224,74.67,56,4,3,"Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.","",""
19,"S. Vollmer, B. Mateen, G. Bohner, F. Király, R. Ghani, P. Jónsson, Sarah Cumbers, Adrian Jonas, K. McAllister, P. Myles, David Granger, M. Birse, Richard Branson, K. Moons, G. Collins, J. Ioannidis, C. Holmes, H. Hemingway","Machine learning and AI research for Patient Benefit: 20 Critical Questions on Transparency, Replicability, Ethics and Effectiveness",2018,"","","","",60,"2022-07-13 09:23:05","","","","",,,,,19,4.75,2,18,4,"Machine learning (ML), artificial intelligence (AI) and other modern statistical methods are providing new opportunities to operationalize previously untapped and rapidly growing sources of data for patient benefit. Whilst there is a lot of promising research currently being undertaken, the literature as a whole lacks: transparency; clear reporting to facilitate replicability; exploration for potential ethical concerns; and, clear demonstrations of effectiveness. There are many reasons for why these issues exist, but one of the most important that we provide a preliminary solution for here is the current lack of ML/AI- specific best practice guidance. Although there is no consensus on what best practice looks in this field, we believe that interdisciplinary groups pursuing research and impact projects in the ML/AI for health domain would benefit from answering a series of questions based on the important issues that exist when undertaking work of this nature. Here we present 20 questions that span the entire project life cycle, from inception, data analysis, and model evaluation, to implementation, as a means to facilitate project planning and post-hoc (structured) independent evaluation. By beginning to answer these questions in different settings, we can start to understand what constitutes a good answer, and we expect that the resulting discussion will be central to developing an international consensus framework for transparent, replicable, ethical and effective research in artificial intelligence (AI-TREE) for health.","",""
17,"David V. Pynadath, M. Barnes, Ning Wang, Jessie Y.C. Chen","Transparency Communication for Machine Learning in Human-Automation Interaction",2018,"","","","",61,"2022-07-13 09:23:05","","10.1007/978-3-319-90403-0_5","","",,,,,17,4.25,4,4,4,"","",""
17,"E. Vorm","Assessing Demand for Transparency in Intelligent Systems Using Machine Learning",2018,"","","","",62,"2022-07-13 09:23:05","","10.1109/INISTA.2018.8466328","","",,,,,17,4.25,17,1,4,"Intelligent systems offering decision support can lessen cognitive load and improve the efficiency of decision making in a variety of contexts. These systems assist users by evaluating multiple courses of action and recommending the right action at the right time. Modern intelligent systems using machine learning introduce new capabilities in decision support, but they can come at a cost. Machine learning models provide little explanation of their outputs or reasoning process, making it difficult to determine when it is appropriate to trust, or if not, what went wrong. In order to improve trust and ensure appropriate reliance on these systems, users must be afforded increased transparency, enabling an understanding of the systems reasoning, and an explanation of its predictions or classifications. Here we discuss the salient factors in designing transparent intelligent systems using machine learning, and present the results of a user-centered design study. We propose design guidelines derived from our study, and discuss next steps for designing for intelligent system transparency.","",""
17,"Mathias Lécuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel J. Hsu, S. Jana","On the Connection between Differential Privacy and Adversarial Robustness in Machine Learning",2018,"","","","",63,"2022-07-13 09:23:05","","","","",,,,,17,4.25,3,5,4,"Adversarial examples in machine learning has been a topic of intense research interest, with attacks and defenses being developed in a tight back-and-forth. Most past defenses are best-effort, heuristic approaches that have all been shown to be vulnerable to sophisticated attacks. More recently, rigorous defenses that provide formal guarantees have emerged, but are hard to scale or generalize. A rigorous and general foundation for designing defenses is required to get us off this arms race trajectory. We propose leveraging differential privacy (DP) as a formal building block for robustness against adversarial examples. We observe that the semantic of DP is closely aligned with the formal definition of robustness to adversarial examples. We propose PixelDP, a strategy for learning robust deep neural networks based on formal DP guarantees. PixelDP networks give theoretical guarantees for a subset of their predictions regarding the robustness against adversarial perturbations of bounded size. Our evaluation with MNIST, CIFAR-10, and CIFAR-100 shows that PixelDP networks achieve accuracy under attack on par with the best-performing defense to date, but additionally certify robustness against meaningful-size 1-norm and 2-norm attacks for 40-60% of their predictions. Our experience points to DP as a rigorous, broadly applicable, and mechanism-rich foundation for robust machine learning.","",""
165,"S. Ardabili, A. Mosavi, Pedram Ghamisi, F. Ferdinand, A. Várkonyi-Kóczy, U. Reuter, T. Rabczuk, P. Atkinson","COVID-19 Outbreak Prediction with Machine Learning",2020,"","","","",64,"2022-07-13 09:23:05","","10.1101/2020.04.17.20070094","","",,,,,165,82.50,21,8,2,"Several outbreak prediction models for COVID-19 are being used by officials around the world to make informed-decisions and enforce relevant control measures. Among the standard models for COVID-19 global pandemic prediction, simple epidemiological and statistical models have received more attention by authorities, and they are popular in the media. Due to a high level of uncertainty and lack of essential data, standard models have shown low accuracy for long-term prediction. Although the literature includes several attempts to address this issue, the essential generalization and robustness abilities of existing models needs to be improved. This paper presents a comparative analysis of machine learning and soft computing models to predict the COVID-19 outbreak. Among a wide range of machine learning models investigated, two models showed promising results (i.e., multi-layered perceptron, MLP, and adaptive network-based fuzzy inference system, ANFIS). Based on the results reported here, and due to the highly complex nature of the COVID-19 outbreak and variation in its behavior from nation-to-nation, this study suggests machine learning as an effective tool to model the outbreak.","",""
13,"Nuno Antunes, Leandro Balby, F. Figueiredo, Nuno Lourenço, Wagner Meira Jr, W. Santos","Fairness and Transparency of Machine Learning for Trustworthy Cloud Services",2018,"","","","",65,"2022-07-13 09:23:05","","10.1109/DSN-W.2018.00063","","",,,,,13,3.25,2,6,4,"Machine learning is nowadays ubiquitous, providing mechanisms for supporting decision making that leverages big data analytics. However, this recent rise in importance of machine learning also raises societal concerns about the dependability and trustworthiness of systems which depend on such automated predictions. Within this context, the new general data protection regulation (GDPR) demands that organizations take the appropriate measures to protect individuals' data, and use it in a privacy-preserving, fair and transparent fashion. In this paper we present how fairness and transparency are supported in the ATMOSPHERE ecosystem for trustworthy clouds. For this, we present the scope of fairness and transparency concerns in the project and then discuss the techniques that are being developed to address each of these concerns. Furthermore, we discuss how fairness and transparency are used with other quality attributes to characterize the trustworthiness of cloud systems.","",""
11,"Jianlong Zhou, Fang Chen","2D Transparency Space - Bring Domain Users and Machine Learning Experts Together",2018,"","","","",66,"2022-07-13 09:23:05","","10.1007/978-3-319-90403-0_1","","",,,,,11,2.75,6,2,4,"","",""
9,"C. Gretton","Trust and Transparency in Machine Learning-Based Clinical Decision Support",2018,"","","","",67,"2022-07-13 09:23:05","","10.1007/978-3-319-90403-0_14","","",,,,,9,2.25,9,1,4,"","",""
10,"J. Pearl","Radical empiricism and machine learning research",2021,"","","","",68,"2022-07-13 09:23:05","","10.1515/jci-2021-0006","","",,,,,10,10.00,10,1,1,"Abstract I contrast the “data fitting” vs “data interpreting” approaches to data science along three dimensions: Expediency, Transparency, and Explainability. “Data fitting” is driven by the faith that the secret to rational decisions lies in the data itself. In contrast, the data-interpreting school views data, not as a sole source of knowledge but as an auxiliary means for interpreting reality, and “reality” stands for the processes that generate the data. I argue for restoring balance to data science through a task-dependent symbiosis of fitting and interpreting, guided by the Logic of Causation.","",""
128,"E. Jo, Timnit Gebru","Lessons from archives: strategies for collecting sociocultural data in machine learning",2019,"","","","",69,"2022-07-13 09:23:05","","10.1145/3351095.3372829","","",,,,,128,42.67,64,2,3,"A growing body of work shows that many problems in fairness, accountability, transparency, and ethics in machine learning systems are rooted in decisions surrounding the data collection and annotation process. In spite of its fundamental nature however, data collection remains an overlooked part of the machine learning (ML) pipeline. In this paper, we argue that a new specialization should be formed within ML that is focused on methodologies for data collection and annotation: efforts that require institutional frameworks and procedures. Specifically for sociocultural data, parallels can be drawn from archives and libraries. Archives are the longest standing communal effort to gather human information and archive scholars have already developed the language and procedures to address and discuss many challenges pertaining to data collection such as consent, power, inclusivity, transparency, and ethics & privacy. We discuss these five key approaches in document collection practices in archives that can inform data collection in sociocultural ML. By showing data collection practices from another field, we encourage ML research to be more cognizant and systematic in data collection and draw from interdisciplinary expertise.","",""
117,"Xiao Chen, Chaoran Li, Derui Wang, S. Wen, Jun Zhang, S. Nepal, Yang Xiang, K. Ren","Android HIV: A Study of Repackaging Malware for Evading Machine-Learning Detection",2018,"","","","",70,"2022-07-13 09:23:05","","10.1109/TIFS.2019.2932228","","",,,,,117,29.25,15,8,4,"Machine learning-based solutions have been successfully employed for the automatic detection of malware on Android. However, machine learning models lack robustness to adversarial examples, which are crafted by adding carefully chosen perturbations to the normal inputs. So far, the adversarial examples can only deceive detectors that rely on syntactic features (e.g., requested permissions, API calls, etc.), and the perturbations can only be implemented by simply modifying application’s manifest. While recent Android malware detectors rely more on semantic features from Dalvik bytecode rather than manifest, existing attacking/defending methods are no longer effective. In this paper, we introduce a new attacking method that generates adversarial examples of Android malware and evades being detected by the current models. To this end, we propose a method of applying optimal perturbations onto Android APK that can successfully deceive the machine learning detectors. We develop an automated tool to generate the adversarial examples without human intervention. In contrast to existing works, the adversarial examples crafted by our method can also deceive recent machine learning-based detectors that rely on semantic features such as control-flow-graph. The perturbations can also be implemented directly onto APK’s Dalvik bytecode rather than Android manifest to evade from recent detectors. We demonstrate our attack on two state-of-the-art Android malware detection schemes, MaMaDroid and Drebin. Our results show that the malware detection rates decreased from 96% to 0% in MaMaDroid, and from 97% to 0% in Drebin, with just a small number of codes to be inserted into the APK.","",""
48,"Jaimie Drozdal, Justin D. Weisz, Dakuo Wang, Gaurav Dass, Bingsheng Yao, Changruo Zhao, Michael J. Muller, Lin Ju, Hui Su","Trust in AutoML: exploring information needs for establishing trust in automated machine learning systems",2020,"","","","",71,"2022-07-13 09:23:05","","10.1145/3377325.3377501","","",,,,,48,24.00,5,9,2,"We explore trust in a relatively new area of data science: Automated Machine Learning (AutoML). In AutoML, AI methods are used to generate and optimize machine learning models by automatically engineering features, selecting models, and optimizing hyperparameters. In this paper, we seek to understand what kinds of information influence data scientists' trust in the models produced by AutoML? We operationalize trust as a willingness to deploy a model produced using automated methods. We report results from three studies - qualitative interviews, a controlled experiment, and a card-sorting task - to understand the information needs of data scientists for establishing trust in AutoML systems. We find that including transparency features in an AutoML tool increased user trust and understandability in the tool; and out of all proposed features, model performance metrics and visualizations are the most important information to data scientists when establishing their trust with an AutoML tool.","",""
3,"","Moving towards reproducible machine learning",2021,"","","","",72,"2022-07-13 09:23:05","","10.1038/s43588-021-00152-6","","",,,,,3,3.00,0,0,1,"","",""
11,"Michael Veale","Logics and practices of transparency and opacity in real-world applications of public sector machine learning",2017,"","","","",73,"2022-07-13 09:23:05","","10.31235/osf.io/6cdhe","","",,,,,11,2.20,11,1,5,"Machine learning systems are increasingly used to support public sector decision-making across a variety of sectors. Given concerns around accountability in these domains, and amidst accusations of intentional or unintentional bias, there have been increased calls for transparency of these technologies. Few, however, have considered how logics and practices concerning transparency have been understood by those involved in the machine learning systems already being piloted and deployed in public bodies today. This short paper distils insights about transparency on the ground from interviews with 27 such actors, largely public servants and relevant contractors, across 5 OECD countries. Considering transparency and opacity in relation to trust and buy-in, better decision-making, and the avoidance of gaming, it seeks to provide useful insights for those hoping to develop socio-technical approaches to transparency that might be useful to practitioners on-the-ground.","",""
74,"M. Bogojeski, Leslie Vogt-Maranto, M. Tuckerman, K. Müller, K. Burke","Quantum chemical accuracy from density functional approximations via machine learning",2019,"","","","",74,"2022-07-13 09:23:05","","10.1038/s41467-020-19093-1","","",,,,,74,24.67,15,5,3,"","",""
148,"Amedeo Sapio, M. Canini, Chen-Yu Ho, J. Nelson, Panos Kalnis, Changhoon Kim, A. Krishnamurthy, M. Moshref, Dan R. K. Ports, Peter Richtárik","Scaling Distributed Machine Learning with In-Network Aggregation",2019,"","","","",75,"2022-07-13 09:23:05","","","","",,,,,148,49.33,15,10,3,"Training complex machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide a robust, efficient solution that speeds up training by up to 300%, and at least by 20% for a number of real-world benchmark models.","",""
106,"Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, V. Larivière, A. Beygelzimer, Florence d'Alché-Buc, E. Fox, H. Larochelle","Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",2020,"","","","",76,"2022-07-13 09:23:05","","","","",,,,,106,53.00,13,8,2,"One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.","",""
35,"S. Lo Piano","Ethical principles in machine learning and artificial intelligence: cases from the field and possible ways forward",2020,"","","","",77,"2022-07-13 09:23:05","","10.1057/s41599-020-0501-9","","",,,,,35,17.50,35,1,2,"","",""
46,"F. Yuan, S. A. Zargar, Qiuyi Chen, Shaohan Wang","Machine learning for structural health monitoring: challenges and opportunities",2020,"","","","",78,"2022-07-13 09:23:05","","10.1117/12.2561610","","",,,,,46,23.00,12,4,2,"A physics-based approach to structural health monitoring (SHM) has practical shortcomings which restrict its suitability to simple structures under well controlled environments. With the advances in information and sensing technology (sensors and sensor networks), it has become feasible to monitor large/diverse number of parameters in complex real-world structures either continuously or intermittently by employing large in-situ (wireless) sensor networks. The availability of this historical data has engendered a lot of interest in a data-driven approach as a natural and more viable option for realizing the goal of SHM in such structures. However, the lack of sensor data corresponding to different damage scenarios continues to remain a challenge. Most of the supervised machine-learning/deep-learning techniques, when trained using this inherently limited data, lack robustness and generalizability. Physics-informed learning, which involves the integration of domain knowledge into the learning process, is presented here as a potential remedy to this challenge. As a step towards the goal of automated damage detection (mathematically an inverse problem), preliminary results are presented from dynamic modelling of beam structures using physics-informed artificial neural networks. Forward and inverse problems involving partial differential equations are solved and comparisons reveal a clear superiority of physics-informed approach over one that is purely datadriven vis-à-vis overfitting/generalization. Other ways of incorporating domain knowledge into the machine learning pipeline are then presented through case-studies on various aspects of NDI/SHM (visual inspection, impact diagnosis). Lastly, as the final attribute of an optimal SHM approach, a sensing paradigm for non-contact full-field measurements for damage diagnosis is presented.","",""
103,"Yunchao Liu, Srinivasan Arunachalam, K. Temme","A rigorous and robust quantum speed-up in supervised machine learning",2020,"","","","",79,"2022-07-13 09:23:05","","10.1038/s41567-021-01287-z","","",,,,,103,51.50,34,3,2,"","",""
3,"F. Biessmann, D. Refiano","Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated",2021,"","","","",80,"2022-07-13 09:23:05","","","","",,,,,3,3.00,2,2,1,"The field explainable artificial intelligence (XAI) has brought about an arsenal of methods to render Machine Learning (ML) predictions more interpretable. But how useful explanations provided by transparent ML methods are for humans remains difficult to assess. Here we investigate the quality of interpretable computer vision algorithms using techniques from psychophysics. In crowdsourced annotation tasks we study the impact of different interpretability approaches on annotation accuracy and task time. We compare these quality metrics with classical XAI, automated quality metrics. Our results demonstrate that psychophysical experiments allow for robust quality assessment of transparency in machine learning. Interestingly the quality metrics computed without humans in the loop did not provide a consistent ranking of interpretability methods nor were they representative for how useful an explanation was for humans. These findings highlight the potential of methods from classical psychophysics for modern machine learning applications. We hope that our results provide convincing arguments for evaluating interpretability in its natural habitat, human-ML interaction, if the goal is to obtain an authentic assessment of interpretability.","",""
30,"S. L. Piano","Ethical principles in machine learning and artificial intelligence: cases from the field and possible ways forward",2020,"","","","",81,"2022-07-13 09:23:05","","10.1057/S41599-020-0501-9","","",,,,,30,15.00,30,1,2,"","",""
0,"W. Zong, Yang-Wai Chow, W. Susilo","Visual Analysis of Adversarial Examples in Machine Learning",2021,"","","","",82,"2022-07-13 09:23:05","","10.1007/978-981-33-6726-5_4","","",,,,,0,0.00,0,3,1,"","",""
0,"Yan Zhou, Murat Kantarcioglu, B. Xi","A Game Theoretic Perspective on Adversarial Machine Learning and Related Cybersecurity Applications",2021,"","","","",83,"2022-07-13 09:23:05","","10.1002/9781119723950.ch13","","",,,,,0,0.00,0,3,1,"In cybersecurity applications where machine learning algorithms are increasingly used to detect vulnerabilities, a somewhat unique challenge arises as exploits targeting machine learning models are constantly devised by the attackers. Traditional machine learning models are no longer robust and reliable when they are under attack. The action and reaction between machine learning systems and the adversary can be modeled as a game between two or more players. Under well‐defined attack models, game theory can provide robustness guarantee for machine learning models that are otherwise vulnerable to application‐time data corruption. We review two cases of game theory‐based machine learning techniques: in one case, players play a zero sum game by following a minimax strategy, while in the other case, players play a sequential game with one player as the leader and the rest as the followers. Experimental results on e‐mail spam and web spam datasets are presented. In the zero sum game, we demonstrate that an adversarial SVM model built upon the minimax strategy is much more resilient to adversarial attacks than standard SVM and one‐class SVM models. We also show that optimal learning strategies derived to counter overly pessimistic attack models can produce unsatisfactory results when the real attacks are much weaker. In the sequential game, we demonstrate that the mixed strategy, allowing a player to randomize over available strategies, is the best solution in general without knowing what types of adversaries machine learning applications are facing in the wild. We also discuss scenarios where players' behavior may derail rational decision making and models that consider such decision risks.","",""
61,"Diego Didona, F. Quaglia, P. Romano, Ennio Torre","Enhancing Performance Prediction Robustness by Combining Analytical Modeling and Machine Learning",2015,"","","","",84,"2022-07-13 09:23:05","","10.1145/2668930.2688047","","",,,,,61,8.71,15,4,7,"Classical approaches to performance prediction rely on two, typically antithetic, techniques: Machine Learning (ML) and Analytical Modeling (AM). ML takes a black box approach, whose accuracy strongly depends on the representativeness of the dataset used during the initial training phase. Specifically, it can achieve very good accuracy in areas of the features' space that have been sufficiently explored during the training process. Conversely, AM techniques require no or minimal training, hence exhibiting the potential for supporting prompt instantiation of the performance model of the target system. However, in order to ensure their tractability, they typically rely on a set of simplifying assumptions. Consequently, AM's accuracy can be seriously challenged in scenarios (e.g., workload conditions) in which such assumptions are not matched. In this paper we explore several hybrid/gray box techniques that exploit AM and ML in synergy in order to get the best of the two worlds. We evaluate the proposed techniques in case studies targeting two complex and widely adopted middleware systems: a NoSQL distributed key-value store and a Total Order Broadcast (TOB) service.","",""
0,"Murilo Cruz Lopes, Marília de Matos Amorim, V. S. Freitas, R. Calumby","Survival Prediction for Oral Cancer Patients: A Machine Learning Approach",2021,"","","","",85,"2022-07-13 09:23:05","","10.5753/kdmile.2021.17466","","",,,,,0,0.00,0,4,1,"There is a high incidence of oral cancer in Brazil, with 150,000 new cases estimated for 2020-2022. In most cases, it is diagnosed at an advanced stage and are related to many risk factors. The Registro Hospitalar de Câncer (RHC), managed by Instituto Nacional de Câncer (INCA), is a nation-wide database that integrates cancer registers from several hospitals in Brazil. RHC is mostly an administrative database but also include clinical, socioeconomic and hospitalization data for each patient with a cancer diagnostic in the country. For these patients, prognostication is always a difficult task a demand multi-dimensional analysis. Therefore, exploiting large-scale data and machine intelligence approaches emerge as promising tool for computer-aided decision support on death risk estimation. Given the importance of this context, some works have reported high prognostication effectiveness, however with extremely limited data collections, relying on weak validation protocols or simple robustness analysis. Hence, this work describes a detailed workflow and experimental analysis for oral cancer patient survival prediction considering careful data curation and strict validation procedures. By exploiting multiple machine learning algorithms and optimization techniques the proposed approach allowed promising survival prediction effectiveness with F1 and AuC-ROC over 0.78 and 0.80, respectively. Moreover, a detailed analysis have shown that the minimization of different types of prediction errors were achieved by different models, which highlights the importance of the rigour in this kind of validation.","",""
23,"Chelsea Chandler, P. Foltz, B. Elvevåg","Using Machine Learning in Psychiatry: The Need to Establish a Framework That Nurtures Trustworthiness.",2019,"","","","",86,"2022-07-13 09:23:05","","10.1093/schbul/sbz105","","",,,,,23,7.67,8,3,3,"The rapid embracing of artificial intelligence in psychiatry has a flavor of being the current ""wild west""; a multidisciplinary approach that is very technical and complex, yet seems to produce findings that resonate. These studies are hard to review as the methods are often opaque and it is tricky to find the suitable combination of reviewers. This issue will only get more complex in the absence of a rigorous framework to evaluate such studies and thus nurture trustworthiness. Therefore, our paper discusses the urgency of the field to develop a framework with which to evaluate the complex methodology such that the process is done honestly, fairly, scientifically, and accurately. However, evaluation is a complicated process and so we focus on three issues, namely explainability, transparency, and generalizability, that are critical for establishing the viability of using artificial intelligence in psychiatry. We discuss how defining these three issues helps towards building a framework to ensure trustworthiness, but show how difficult definition can be, as the terms have different meanings in medicine, computer science, and law. We conclude that it is important to start the discussion such that there can be a call for policy on this and that the community takes extra care when reviewing clinical applications of such models..","",""
21,"Lie He, Sai Praneeth Karimireddy, Martin Jaggi","Secure Byzantine-Robust Machine Learning",2020,"","","","",87,"2022-07-13 09:23:05","","","","",,,,,21,10.50,7,3,2,"Increasingly machine learning systems are being deployed to edge servers and devices (e.g. mobile phones) and trained in a collaborative manner. Such distributed/federated/decentralized training raises a number of concerns about the robustness, privacy, and security of the procedure. While extensive work has been done in tackling with robustness, privacy, or security individually, their combination has rarely been studied. In this paper, we propose a secure two-server protocol that offers both input privacy and Byzantine-robustness. In addition, this protocol is communication-efficient, fault-tolerant and enjoys local differential privacy.","",""
0,"Yixuan Cheng, Qiuran Li, Fengge Wan","Financial Risk Management using Machine Learning Method",2021,"","","","",88,"2022-07-13 09:23:05","","10.1109/MLBDBI54094.2021.00034","","",,,,,0,0.00,0,3,1,"AI in banking industries could solve the dilemma, including fraud, credit crisis, and financial risk management, faced by the recent market structure. In addition, AI could reduce the rates of errors committed by humans and enhance the total transparency of each dealing, replacing the routine and monotonous tasks performed by humans in the early stage of financial activities. In this study, both unsupervised and supervised learning, including SVM, logistic regression, random forest, and K-means, are performed for credit rating and loan decision issues. Through the experimental result, we conclude that the SVM algorithm, whose accuracy is up to 78.4%, has the best performance among these supervised learning models. Moreover, the three clustering results produced by K-means are more desired and provide a constructive suggestion to deal with SME credit rating. This study gives an AI-based strategy for loan decisions and credit rating, which can help bank industries solve financial issues.","",""
18,"N. Jacobson, K. Bentley, Ashley Walton, Shirley B. Wang, R. Fortgang, A. Millner, G. Coombs, A. Rodman, Daniel D. L. Coppersmith","Ethical dilemmas posed by mobile health and machine learning in psychiatry research",2020,"","","","",89,"2022-07-13 09:23:05","","10.2471/BLT.19.237107","","",,,,,18,9.00,2,9,2,"Abstract The application of digital technology to psychiatry research is rapidly leading to new discoveries and capabilities in the field of mobile health. However, the increase in opportunities to passively collect vast amounts of detailed information on study participants coupled with advances in statistical techniques that enable machine learning models to process such information has raised novel ethical dilemmas regarding researchers’ duties to: (i) monitor adverse events and intervene accordingly; (ii) obtain fully informed, voluntary consent; (iii) protect the privacy of participants; and (iv) increase the transparency of powerful, machine learning models to ensure they can be applied ethically and fairly in psychiatric care. This review highlights emerging ethical challenges and unresolved ethical questions in mobile health research and provides recommendations on how mobile health researchers can address these issues in practice. Ultimately, the hope is that this review will facilitate continued discussion on how to achieve best practice in mobile health research within psychiatry.","",""
29,"Fahad Shabbir Ahmad, Liaqat Ali, Liaqat Ali, Raza-Ul-Mustafa, Hasan Ali Khattak, Tahir Hameed, Iram Wajahat, Seifedine Kadry, S. Bukhari","A hybrid machine learning framework to predict mortality in paralytic ileus patients using electronic health records (EHRs)",2020,"","","","",90,"2022-07-13 09:23:05","","10.1007/s12652-020-02456-3","","",,,,,29,14.50,3,9,2,"","",""
299,"J Zhang, M. Harman, Lei Ma, Yang Liu","Machine Learning Testing: Survey, Landscapes and Horizons",2019,"","","","",91,"2022-07-13 09:23:05","","10.1109/tse.2019.2962027","","",,,,,299,99.67,75,4,3,"This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.","",""
13,"E. Akyol, C. Langbort, T. Başar","Price of Transparency in Strategic Machine Learning",2016,"","","","",92,"2022-07-13 09:23:05","","","","",,,,,13,2.17,4,3,6,"Based on the observation that the transparency of an algorithm comes with a cost for the algorithm designer when the users (data providers) are strategic, this paper studies the impact of strategic intent of the users on the design and performance of transparent ML algorithms. We quantitatively study the {\bf price of transparency} in the context of strategic classification algorithms, by modeling the problem as a nonzero-sum game between the users and the algorithm designer. The cost of having a transparent algorithm is measured by a quantity, named here as price of transparency which is the ratio of the designer cost at the Stackelberg equilibrium, when the algorithm is transparent (which allows users to be strategic) to that of the setting where the algorithm is not transparent.","",""
10,"F. Zerka, V. Urovi, A. Vaidyanathan, S. Barakat, R. Leijenaar, S. Walsh, H. Gabrani-Juma, B. Miraglio, H. Woodruff, M. Dumontier, P. Lambin","Blockchain for Privacy Preserving and Trustworthy Distributed Machine Learning in Multicentric Medical Imaging (C-DistriM)",2020,"","","","",93,"2022-07-13 09:23:05","","10.1109/ACCESS.2020.3029445","","",,,,,10,5.00,1,11,2,"The utility of Artificial Intelligence (AI) in healthcare strongly depends upon the quality of the data used to build models, and the confidence in the predictions they generate. Access to sufficient amounts of high-quality data to build accurate and reliable models remains problematic owing to substantive legal and ethical constraints in making clinically relevant research data available offsite. New technologies such as distributed learning offer a pathway forward, but unfortunately tend to suffer from a lack of transparency, which undermines trust in what data are used for the analysis. To address such issues, we hypothesized that, a novel distributed learning that combines sequential distributed learning with a blockchain-based platform, namely Chained Distributed Machine learning C-DistriM, would be feasible and would give a similar result as a standard centralized approach. C-DistriM enables health centers to dynamically participate in training distributed learning models. We demonstrate C-DistriM using the NSCLC-Radiomics open data to predict two-year lung-cancer survival. A comparison of the performance of this distributed solution, evaluated in six different scenarios, and the centralized approach, showed no statistically significant difference (AUCs between central and distributed models), all DeLong tests yielded $p$ -val >0.05. This methodology removes the need to blindly trust the computation in one specific server on a distributed learning network. This fusion of blockchain and distributed learning serves as a proof-of-concept to increase transparency, trust, and ultimately accelerate the adoption of AI in multicentric studies. We conclude that our blockchain-based model for sequential training on distributed datasets is a feasible approach, provides equivalent performance to the centralized approach.","",""
87,"G. Peng, M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, Savador Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Multiscale modeling meets machine learning: What can we learn?",2019,"","","","",94,"2022-07-13 09:23:05","","10.1007/s11831-020-09405-5","","",,,,,87,29.00,9,12,3,"","",""
75,"K. Kashinath, M. Mustafa, A. Albert, J.-L. Wu, C. Jiang, S. Esmaeilzadeh, K. Azizzadenesheli, R. Wang, A. Chattopadhyay, A. Singh, A. Manepalli, D. Chirila, R. Yu, R. Walters, B. White, H. Xiao, H. Tchelepi, P. Marcus, A. Anandkumar, P. Hassanzadeh, Prabhat","Physics-informed machine learning: case studies for weather and climate modelling",2021,"","","","",95,"2022-07-13 09:23:05","","10.1098/rsta.2020.0093","","",,,,,75,75.00,8,21,1,"Machine learning (ML) provides novel and powerful ways of accurately and efficiently recognizing complex patterns, emulating nonlinear dynamics, and predicting the spatio-temporal evolution of weather and climate processes. Off-the-shelf ML models, however, do not necessarily obey the fundamental governing laws of physical systems, nor do they generalize well to scenarios on which they have not been trained. We survey systematic approaches to incorporating physics and domain knowledge into ML models and distill these approaches into broad categories. Through 10 case studies, we show how these approaches have been used successfully for emulating, downscaling, and forecasting weather and climate processes. The accomplishments of these studies include greater physical consistency, reduced training time, improved data efficiency, and better generalization. Finally, we synthesize the lessons learned and identify scientific, diagnostic, computational, and resource challenges for developing truly robust and reliable physics-informed ML models for weather and climate processes. This article is part of the theme issue ‘Machine learning for weather and climate modelling’.","",""
6,"Max Scheerer, Jonas Klamroth, Ralf H. Reussner, Bernhard Beckert","Towards classes of architectural dependability assurance for machine-learning-based systems",2020,"","","","",96,"2022-07-13 09:23:05","","10.1145/3387939.3388613","","",,,,,6,3.00,2,4,2,"Advances in Machine Learning (ML) have brought previously hard to handle problems within arm's reach. However, this power comes at the cost of unassured reliability and lacking transparency. Overcoming this drawback is very hard due to the probabilistic nature of ML. Current approaches mainly tackle this problem by developing more robust learning procedures. Such algorithmic approaches, however, are limited to certain types of uncertainties and cannot deal with all of them, e.g., hardware failure. This paper discusses how this problem can be addressed at architectural rather than algorithmic level to assess systems dependability properties in early development stages. Moreover, we argue that Self-Adaptive Systems (SAS) are more suited to safeguard ML w.r.t. various uncertainties. As a step towards this we propose classes of dependability in which ML-based systems may be categorized and discuss which and how assurances can be made for each class.","",""
665,"Weihua Hu, Matthias Fey, M. Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, J. Leskovec","Open Graph Benchmark: Datasets for Machine Learning on Graphs",2020,"","","","",97,"2022-07-13 09:23:05","","","","",,,,,665,332.50,83,8,2,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .","",""
2,"M. Mapundu, C. Kabudula, E. Musenge, T. Celik","Overview of Statistical and Machine Learning Techniques for Determining Causes of Death from Verbal Autopsies: A Systematic Literature Review",2020,"","","","",98,"2022-07-13 09:23:05","","10.21203/rs.3.rs-95087/v1","","",,,,,2,1.00,1,4,2,"  Background: The process of determining causes of death in areas where there is limited clinical services using verbal autopsies has become a key issue in terms of accuracy on cause of death (prone to errors and subjective), quality of data among many drawbacks. This is mainly because there is no proper standard available in performing verbal autopsy, even though it is important for civil registration systems and strengthening of health priorities. Physician diagnosis is the only gold standard in reviewing verbal autopsy narratives. In practice, conventional statistical methods are used to perform verbal autopsies due to their simplicity and transparency. However, in literature complex machine learning models can be found that can replace the traditional statistical methods. There has not been much application of machine learning techniques in verbal autopsy to determine cause of death, despite the advances in technology. As such, there is a need for a thorough survey of recent literature on statistical and machine learning approaches applied in verbal autopsy to determine cause of death. Methods: A systematic review was conducted and included a search from six databases. Our study only included scientiﬁc articles published in last decade that reported on verbal autopsy and: (1) algorithms; (2) statistical techniques; (3) machine learning and (4) deep learning. The search yielded 110 articles, after meta analysis, we identiﬁed 85 articles as being relevant and discarded the other 25. We investigated and compared the most commonly used statistical and machine learning techniques in VAs, identiﬁed limitations of each of these techniques, proposed a guiding machine learning framework and pointed to future directions. Results: Eighty ﬁve studies met the inclusion criteria. Apart from physician diagnosis, statistical methods are the most currently applied tools to determine cause of death from verbal autopsies. However, there has been little application of traditional machine learning and emerging techniques, even though they have shown promising results in other domains. Conclusions: Technological application of machine learning to determine cause of death, should focus on effective ideal strategies of pre-processing, transparency, robust feature engineering techniques and data balancing in order to attain optimal model performance.","",""
3,"Numair Sani, Jaron J. R. Lee, Razieh Nabi, I. Shpitser","A Semiparametric Approach to Interpretable Machine Learning",2020,"","","","",99,"2022-07-13 09:23:05","","","","",,,,,3,1.50,1,4,2,"Black box models in machine learning have demonstrated excellent predictive performance in complex problems and high-dimensional settings. However, their lack of transparency and interpretability restrict the applicability of such models in critical decision-making processes. In order to combat this shortcoming, we propose a novel approach to trading off interpretability and performance in prediction models using ideas from semiparametric statistics, allowing us to combine the interpretability of parametric regression models with performance of nonparametric methods. We achieve this by utilizing a two-piece model: the first piece is interpretable and parametric, to which a second, uninterpretable residual piece is added. The performance of the overall model is optimized using methods from the sufficient dimension reduction literature. Influence function based estimators are derived and shown to be doubly robust. This allows for use of approaches such as double Machine Learning in estimating our model parameters. We illustrate the utility of our approach via simulation studies and a data application based on predicting the length of stay in the intensive care unit among surgery patients.","",""
0,"El-Mahdi El-Mhamdi, R. Guerraoui, Arsany Guirguis, Sébastien Rouault","Garfield: System Support for Byzantine Machine Learning",2020,"","","","",100,"2022-07-13 09:23:05","","","","",,,,,0,0.00,0,4,2,"Byzantine Machine Learning (ML) systems are nowadays vulnerable for they require trusted machines and/or a synchronous network. We present Garfield, a system that provably achieves Byzantine resilience in ML applications without assuming any trusted component nor any bound on communication or computation delays. Garfield leverages ML specificities to make progress despite consensus being impossible in such an asynchronous, Byzantine environment. Following the classical server/worker architecture, Garfield replicates the parameter server while relying on the statistical properties of stochastic gradient descent to keep the models on the correct servers close to each other. On the other hand, Garfield uses statistically-robust gradient aggregation rules (GARs) to achieve resilience against Byzantine workers. We integrate Garfield with two widely-used ML frameworks, TensorFlow and PyTorch, while achieving transparency: applications developed with either framework do not need to change their interfaces to be made Byzantine resilient. Our implementation supports full-stack computations on both CPUs and GPUs. We report on our evaluation of Garfield with different (a) baselines, (b) ML models (e.g., ResNet-50 and VGG), and (c) hardware infrastructures (CPUs and GPUs). Our evaluation highlights several interesting facts about the cost of Byzantine resilience. In particular, (a) Byzantine resilience, unlike crash resilience, induces an accuracy loss, and (b) the throughput overhead comes much more from communication (70%) than from aggregation.","",""
0,"Matthias Mühlbauer, Hubert Würschinger, Dominik Polzer, N. Hanenkamp","Energy Profile Prediction of Milling Processes Using Machine Learning Techniques",2020,"","","","",101,"2022-07-13 09:23:05","","10.1007/978-3-662-62746-4_1","","",,,,,0,0.00,0,4,2,"","",""
0,"El-Mahdi El-Mhamdi, R. Guerraoui, Arsany Guirguis, Sébastien Rouault","Garfield: System Support for Byzantine Machine Learning",2020,"","","","",102,"2022-07-13 09:23:05","","","","",,,,,0,0.00,0,4,2,"Byzantine Machine Learning (ML) systems are nowadays vulnerable for they require trusted machines and/or a synchronous network. We present Garfield, a system that provably achieves Byzantine resilience in ML applications without assuming any trusted component nor any bound on communication or computation delays. Garfield leverages ML specificities to make progress despite consensus being impossible in such an asynchronous, Byzantine environment. Following the classical server/worker architecture, Garfield replicates the parameter server while relying on the statistical properties of stochastic gradient descent to keep the models on the correct servers close to each other. On the other hand, Garfield uses statistically-robust gradient aggregation rules (GARs) to achieve resilience against Byzantine workers. We integrate Garfield with two widely-used ML frameworks, TensorFlow and PyTorch, while achieving transparency: applications developed with either framework do not need to change their interfaces to be made Byzantine resilient. Our implementation supports full-stack computations on both CPUs and GPUs. We report on our evaluation of Garfield with different (a) baselines, (b) ML models (e.g., ResNet-50 and VGG), and (c) hardware infrastructures (CPUs and GPUs). Our evaluation highlights several interesting facts about the cost of Byzantine resilience. In particular, (a) Byzantine resilience, unlike crash resilience, induces an accuracy loss, and (b) the throughput overhead comes much more from communication (70%) than from aggregation.","",""
0,"E. Kondrateva, Polina Belozerova, M. Sharaev, Evgeny Burnaev, A. Bernstein, I. Samotaeva","Machine learning models reproducibility and validation for MR images recognition",2020,"","","","",103,"2022-07-13 09:23:05","","10.1117/12.2559525","","",,,,,0,0.00,0,6,2,"In the present work, we introduce a data processing and analysis pipeline, which ensures the reproducibility of machine learning models chosen for MR image recognition. The proposed pipeline is applied to solve the binary classification problems: epilepsy and depression diagnostics based on vectorized features from MR images. This model is then assessed in terms of classification performance, robustness and reliability of the results, including predictive accuracy on unseen data. The classification performance achieved with our approach compares favorably to ones reported in the literature, where usually no thorough model evaluation is performed.","",""
1920,"A. Kurakin, Ian J. Goodfellow, Samy Bengio","Adversarial Machine Learning at Scale",2016,"","","","",104,"2022-07-13 09:23:05","","","","",,,,,1920,320.00,640,3,6,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.","",""
0,"D. Efremenko, Himani Jain, Jian Xu","Two Machine Learning Based Schemes for Solving Direct and Inverse Problems of Radiative Transfer Theory",2020,"","","","",105,"2022-07-13 09:23:05","","10.51130/graphicon-2020-2-3-45","","",,,,,0,0.00,0,3,2,"Artificial neural networks (ANNs) are used to substitute computationally expensive radiative transfer models (RTMs) and inverse operators (IO) for retrieving optical parameters of the medium. However, the direct parametrization of RTMs and IOs by means of ANNs has certain drawbacks, such as loss of generality, computations of huge training datasets, robustness issues etc. This paper provides an analysis of different ANN-related methods, based on our results and those published by other authors. In particular, two techniques are proposed. In the first method, the ANN substitutes the eigenvalue solver in the discrete ordinate RTM, thereby reducing the computational time. Unlike classical RTM parametrization schemes based on ANN, in this method the resulting ANN can be used for arbitrary geometry and layer optical thicknesses. In the second method, the IO is trained by using the real measurements (preprocessed Level-2 TROPOMI data) to improve the stability of the inverse operator. This method provides robust results even without applying the Tikhonov regularization method.","",""
0,"Marco Matarese, A. Sciutti, F. Rea, Silvia Rossi","Toward Robots’ Behavioral Transparency of Temporal Difference Reinforcement Learning With a Human Teacher",2021,"","","","",106,"2022-07-13 09:23:05","","10.1109/THMS.2021.3116119","","",,,,,0,0.00,0,4,1,"The high request for autonomous human–robot interaction (HRI), combined with the potential of machine learning (ML) techniques, allow us to deploy ML mechanisms in robot control. However, the use of ML can make robots’ behavior unclear to the observer during the learning phase. Recently, transparency in HRI has been investigated to make such interactions more comprehensible. In this work, we propose a model to improve the transparency during reinforcement learning (RL) tasks for HRI scenarios: the model supports transparency by having the robot show nonverbal emotional-behavioral cues. Our model considered human feedback as the reward of the RL algorithm and it presents emotional-behavioral responses based on the progress of the robot learning. The model is managed only by the temporal-difference error. We tested the architecture in a teaching scenario with the iCub humanoid robot. The results highlight that when the robot expresses its emotional-behavioral response, the human teacher is able to understand its learning process better. Furthermore, people prefer to interact with an expressive robot as compared to a mechanical one. Movement-based signals proved to be more effective in revealing the internal state of the robot than facial expressions. In particular, gaze movements were effective in showing the robot's next intentions. In contrast, communicating uncertainty through robot movements sometimes led to action misinterpretation, highlighting the importance of balancing transparency and the legibility of the robot goal. We also found a reliable temporal window in which to register teachers’ feedback that can be used by the robot as a reward.","",""
189,"A. Mosavi, M. Salimi, Sina Faizollahzadeh Ardabili, T. Rabczuk, Shahaboddin Shamshirband, A. Várkonyi-Kóczy","State of the Art of Machine Learning Models in Energy Systems, a Systematic Review",2019,"","","","",107,"2022-07-13 09:23:05","","10.3390/EN12071301","","",,,,,189,63.00,32,6,3,"Machine learning (ML) models have been widely used in the modeling, design and prediction in energy systems. During the past two decades, there has been a dramatic increase in the advancement and application of various types of ML models for energy systems. This paper presents the state of the art of ML models used in energy systems along with a novel taxonomy of models and applications. Through a novel methodology, ML models are identified and further classified according to the ML modeling technique, energy type, and application area. Furthermore, a comprehensive review of the literature leads to an assessment and performance evaluation of the ML models and their applications, and a discussion of the major challenges and opportunities for prospective research. This paper further concludes that there is an outstanding rise in the accuracy, robustness, precision and generalization ability of the ML models in energy systems using hybrid ML models. Hybridization is reported to be effective in the advancement of prediction models, particularly for renewable energy systems, e.g., solar energy, wind energy, and biofuels. Moreover, the energy demand prediction using hybrid models of ML have highly contributed to the energy efficiency and therefore energy governance and sustainability.","",""
93,"Mark O. Riedl","Human-Centered Artificial Intelligence and Machine Learning",2019,"","","","",108,"2022-07-13 09:23:05","","10.1002/HBE2.117","","",,,,,93,31.00,93,1,3,"Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.","",""
484,"Kangwook Lee, Maximilian Lam, Ramtin Pedarsani, Dimitris Papailiopoulos, K. Ramchandran","Speeding Up Distributed Machine Learning Using Codes",2015,"","","","",109,"2022-07-13 09:23:05","","10.1109/TIT.2017.2736066","","",,,,,484,69.14,97,5,7,"Codes are widely used in many engineering applications to offer <italic>robustness</italic> against <italic>noise</italic>. In large-scale systems, there are several types of noise that can affect the performance of distributed machine learning algorithms—straggler nodes, system failures, or communication bottlenecks—but there has been little interaction cutting across codes, machine learning, and distributed systems. In this paper, we provide theoretical insights on how <italic>coded</italic> solutions can achieve significant gains compared with uncoded ones. We focus on two of the most basic building blocks of distributed learning algorithms: <italic>matrix multiplication</italic> and <italic>data shuffling</italic>. For matrix multiplication, we use codes to alleviate the effect of stragglers and show that if the number of homogeneous workers is <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula>, and the runtime of each subtask has an exponential tail, coded computation can speed up distributed matrix multiplication by a factor of <inline-formula> <tex-math notation=""LaTeX"">$\log n$ </tex-math></inline-formula>. For data shuffling, we use codes to reduce communication bottlenecks, exploiting the excess in storage. We show that when a constant fraction <inline-formula> <tex-math notation=""LaTeX"">$\alpha $ </tex-math></inline-formula> of the data matrix can be cached at each worker, and <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> is the number of workers, <italic>coded shuffling</italic> reduces the communication cost by a factor of <inline-formula> <tex-math notation=""LaTeX"">$\left({\alpha + \frac {1}{n}}\right)\gamma (n)$ </tex-math></inline-formula> compared with uncoded shuffling, where <inline-formula> <tex-math notation=""LaTeX"">$\gamma (n)$ </tex-math></inline-formula> is the ratio of the cost of unicasting <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> messages to <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> users to multicasting a common message (of the same size) to <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> users. For instance, <inline-formula> <tex-math notation=""LaTeX"">$\gamma (n) \simeq n$ </tex-math></inline-formula> if multicasting a message to <inline-formula> <tex-math notation=""LaTeX"">$n$ </tex-math></inline-formula> users is as cheap as unicasting a message to one user. We also provide experimental results, corroborating our theoretical gains of the coded algorithms.","",""
84,"Y. Khourdifi, M. Bahaj","Heart Disease Prediction and Classification Using Machine Learning Algorithms Optimized by Particle Swarm Optimization and Ant Colony Optimization",2019,"","","","",110,"2022-07-13 09:23:05","","10.22266/IJIES2019.0228.24","","",,,,,84,28.00,42,2,3,"The prediction of heart disease is one of the areas where machine learning can be implemented. Optimization algorithms have the advantage of dealing with complex non-linear problems with a good flexibility and adaptability. In this paper, we exploited the Fast Correlation-Based Feature Selection (FCBF) method to filter redundant features in order to improve the quality of heart disease classification. Then, we perform a classification based on different classification algorithms such as K-Nearest Neighbour, Support Vector Machine, Naïve Bayes, Random Forest and a Multilayer Perception | Artificial Neural Network optimized by Particle Swarm Optimization (PSO) combined with Ant Colony Optimization (ACO) approaches. The proposed mixed approach is applied to heart disease dataset; the results demonstrate the efficacy and robustness of the proposed hybrid method in processing various types of data for heart disease classification. Therefore, this study examines the different machine learning algorithms and compares the results using different performance measures, i.e. accuracy, precision, recall, f1-score, etc. A maximum classification accuracy of 99.65% using the optimized model proposed by FCBF, PSO and ACO. The results show that the performance of the proposed system is superior to that of the classification technique presented above.","",""
48,"N. M. Barbosa, Mon-Chu Chen","Rehumanized Crowdsourcing: A Labeling Framework Addressing Bias and Ethics in Machine Learning",2019,"","","","",111,"2022-07-13 09:23:05","","10.1145/3290605.3300773","","",,,,,48,16.00,24,2,3,"The increased use of machine learning in recent years led to large volumes of data being manually labeled via crowdsourcing microtasks completed by humans. This brought about dehumanization effects, namely, when task requesters overlook the humans behind the task, leading to issues of ethics (e.g., unfair payment) and amplification of human biases, which are transferred into training data and affect machine learning in the real world. We propose a framework that allocates microtasks considering human factors of workers such as demographics and compensation. We deployed our framework to a popular crowdsourcing platform and conducted experiments with 1,919 workers collecting 160,345 human judgments. By routing microtasks to workers based on demographics and appropriate pay, our framework mitigates biases in the contributor sample and increases the hourly pay given to contributors. We discuss potential extensions and how it can promote transparency in crowdsourcing.","",""
64,"Alexander H S Harris, A. Kuo, Yingjie Weng, A. Trickey, Thomas R Bowe, N. Giori","Can Machine Learning Methods Produce Accurate and Easy-to-use Prediction Models of 30-day Complications and Mortality After Knee or Hip Arthroplasty?",2019,"","","","",112,"2022-07-13 09:23:05","","10.1097/CORR.0000000000000601","","",,,,,64,21.33,11,6,3,"Background Existing universal and procedure-specific surgical risk prediction models of death and major complications after elective total joint arthroplasty (TJA) have limitations including poor transparency, poor to modest accuracy, and insufficient validation to establish performance across diverse settings. Thus, the need remains for accurate and validated prediction models for use in preoperative management, informed consent, shared decision-making, and risk adjustment for reimbursement. Questions/purposes The purpose of this study was to use machine learning methods and large national databases to develop and validate (both internally and externally) parsimonious risk-prediction models for mortality and complications after TJA. Methods Preoperative demographic and clinical variables from all 107,792 nonemergent primary THAs and TKAs in the 2013 to 2014 American College of Surgeons-National Surgical Quality Improvement Program (ACS-NSQIP) were evaluated as predictors of 30-day death and major complications. The NSQIP database was chosen for its high-quality data on important outcomes and rich characterization of preoperative demographic and clinical predictors for demographically and geographically diverse patients. Least absolute shrinkage and selection operator (LASSO) regression, a type of machine learning that optimizes accuracy and parsimony, was used for model development. Tenfold validation was used to produce C-statistics, a measure of how well models discriminate patients who experience an outcome from those who do not. External validation, which evaluates the generalizability of the models to new data sources and patient groups, was accomplished using data from the Veterans Affairs Surgical Quality Improvement Program (VASQIP). Models previously developed from VASQIP data were also externally validated using NSQIP data to examine the generalizability of their performance with a different group of patients outside the VASQIP context. Results The models, developed using LASSO regression with diverse clinical (for example, American Society of Anesthesiologists classification, comorbidities) and demographic (for example, age, gender) inputs, had good accuracy in terms of discriminating the likelihood a patient would experience, within 30 days of arthroplasty, a renal complication (C-statistic, 0.78; 95% confidence interval [CI], 0.76-0.80), death (0.73; 95% CI, 0.70-0.76), or a cardiac complication (0.73; 95% CI, 0.71-0.75) from one who would not. By contrast, the models demonstrated poor accuracy for venous thromboembolism (C-statistic, 0.61; 95% CI, 0.60-0.62) and any complication (C-statistic, 0.64; 95% CI, 0.63-0.65). External validation of the NSQIP- derived models using VASQIP data found them to be robust in terms of predictions about mortality and cardiac complications, but not for predicting renal complications. Models previously developed with VASQIP data had poor accuracy when externally validated with NSQIP data, suggesting they should not be used outside the context of the Veterans Health Administration. Conclusions Moderately accurate predictive models of 30-day mortality and cardiac complications after elective primary TJA were developed as well as internally and externally validated. To our knowledge, these are the most accurate and rigorously validated TJA-specific prediction models currently available (http://med.stanford.edu/s-spire/Resources/clinical-tools-.html). Methods to improve these models, including the addition of nonstandard inputs such as natural language processing of preoperative clinical progress notes or radiographs, should be pursued as should the development and validation of models to predict longer term improvements in pain and function. Level of Evidence Level III, diagnostic study.","",""
73,"M. Molina, F. Garip","Machine Learning for Sociology",2019,"","","","",113,"2022-07-13 09:23:05","","10.1146/ANNUREV-SOC-073117-041106","","",,,,,73,24.33,37,2,3,"Machine learning is a field at the intersection of statistics and computer science that uses algorithms to extract information and knowledge from data. Its applications increasingly find their way into economics, political science, and sociology. We offer a brief introduction to this vast toolbox and illustrate its current uses in the social sciences, including distilling measures from new data sources, such as text and images; characterizing population heterogeneity; improving causal inference; and offering predictions to aid policy decisions and theory development. We argue that, in addition to serving similar purposes in sociology, machine learning tools can speak to long-standing questions on the limitations of the linear modeling framework, the criteria for evaluating empirical findings, transparency around the context of discovery, and the epistemological core of the discipline.","",""
58,"S. Ardabili, A. Mosavi, Majid Dehghani, A. Várkonyi-Kóczy","Deep Learning and Machine Learning in Hydrological Processes Climate Change and Earth Systems a Systematic Review",2019,"","","","",114,"2022-07-13 09:23:05","","10.1007/978-3-030-36841-8_5","","",,,,,58,19.33,15,4,3,"","",""
45,"R. Roelofs, Vaishaal Shankar, B. Recht, Sara Fridovich-Keil, Moritz Hardt, John Miller, Ludwig Schmidt","A Meta-Analysis of Overfitting in Machine Learning",2019,"","","","",115,"2022-07-13 09:23:05","","","","",,,,,45,15.00,6,7,3,"We conduct the first large meta-analysis of overfitting due to test set reuse in the machine learning community. Our analysis is based on over one hundred machine learning competitions hosted on the Kaggle platform over the course of several years. In each competition, numerous practitioners repeatedly evaluated their progress against a holdout set that forms the basis of a public ranking available throughout the competition. Performance on a separate test set used only once determined the final ranking. By systematically comparing the public ranking with the final ranking, we assess how much participants adapted to the holdout set over the course of a competition. Our study shows, somewhat surprisingly, little evidence of substantial overfitting. These findings speak to the robustness of the holdout method across different data domains, loss functions, model classes, and human analysts.","",""
42,"Hiromu Araki, T. Mizoguchi, Y. Hatsugai","Phase diagram of a disordered higher-order topological insulator: A machine learning study",2018,"","","","",116,"2022-07-13 09:23:05","","10.1103/PhysRevB.99.085406","","",,,,,42,10.50,14,3,4,"A higher-order topological insulator is a new concept of topological states of matter, which is characterized by the emergent boundary states whose dimensionality is lower by more than two compared with that of the bulk, and draws a considerable interest. Yet, its robustness against disorders is still unclear. In this work, we investigate a phase diagram of higher-order topological insulator phases in a breathing kagome model in the presence of disorders by using a state-of-the-art machine learning technique. We find that the corner states survive against the finite strength of disorder potential as long as the energy gap is not closed, indicating the stability of the higher-order topological phases against the disorders.","",""
38,"S. Saria, Adarsh Subbaswamy","Tutorial: Safe and Reliable Machine Learning",2019,"","","","",117,"2022-07-13 09:23:05","","","","",,,,,38,12.67,19,2,3,"This document serves as a brief overview of the ""Safe and Reliable Machine Learning"" tutorial given at the 2019 ACM Conference on Fairness, Accountability, and Transparency (FAT* 2019). The talk slides can be found here: this https URL, while a video of the talk is available here: this https URL, and a complete list of references for the tutorial here: this https URL.","",""
30,"Jennifer Horkoff","Non-Functional Requirements for Machine Learning: Challenges and New Directions",2019,"","","","",118,"2022-07-13 09:23:05","","10.1109/RE.2019.00050","","",,,,,30,10.00,30,1,3,"Machine Learning (ML) provides approaches which use big data to enable algorithms to ""learn"", producing outputs which would be difficult to obtain otherwise. Despite the advances allowed by ML, much recent attention has been paid to certain qualities of ML solutions, particularly fairness and transparency, but also qualities such as privacy, security, and testability. From a requirements engineering (RE) perspective, such qualities are also known as non-functional requirements (NFRs). In RE, the meaning of certain NFRs, how to refine those NFRs, and how to use NFRs for design and runtime decision making over traditional software is relatively well established and understood. However, in a context where the solution involves ML, much of our knowledge about NFRs no longer applies. First, the types of NFRs we are concerned with undergo a shift: NFRs like fairness and transparency become prominent, whereas other NFRs such as modularity may become less relevant. The meanings and interpretations of NFRs in an ML context (e.g., maintainability, interoperability, and usability) must be rethought, including how these qualities are decomposed into sub-qualities. Trade-offs between NFRs in an ML context must be re-examined. Beyond the changing landscape of NFRs, we can ask if our known approaches to understanding, formalizing, modeling, and reasoning over NFRs at design and runtime must also be adjusted, or can be applied as-is to this new area? Given these questions, this work outlines challenges and a proposed research agenda for the exploration of NFRs for ML-based solutions.","",""
28,"R. Shokri, Martin Strobel, Yair Zick","Privacy Risks of Explaining Machine Learning Models",2019,"","","","",119,"2022-07-13 09:23:05","","","","",,,,,28,9.33,9,3,3,"Can we trust black-box machine learning with its decisions? Can we trust algorithms to train machine learning models on sensitive data? Transparency and privacy are two fundamental elements of trust for adopting machine learning. In this paper, we investigate the relation between interpretability and privacy. In particular we analyze if an adversary can exploit transparent machine learning to infer sensitive information about its training set. To this end, we perform membership inference as well as reconstruction attacks on two popular classes of algorithms for explaining machine learning models: feature-based and record-based influence measures. We empirically show that an attacker, that only observes the feature-based explanations, has the same power as the state of the art membership inference attacks on model predictions. We also demonstrate that record-based explanations can be effectively exploited to reconstruct significant parts of the training set. Finally, our results indicate that minorities and special cases are more vulnerable to these type of attacks than majority groups.","",""
23,"N. Kreif, K. DiazOrdaz","Machine learning in policy evaluation: new tools for causal inference",2019,"","","","",120,"2022-07-13 09:23:05","","10.1093/ACREFORE/9780190625979.013.256","","",,,,,23,7.67,12,2,3,"While machine learning (ML) methods have received a lot of attention in recent years, these methods are primarily for prediction. Empirical researchers conducting policy evaluations are, on the other hand, preoccupied with causal problems, trying to answer counterfactual questions: what would have happened in the absence of a policy? Because these counterfactuals can never be directly observed (described as the “fundamental problem of causal inference”) prediction tools from the ML literature cannot be readily used for causal inference. In the last decade, major innovations have taken place incorporating supervised ML tools into estimators for causal parameters such as the average treatment effect (ATE). This holds the promise of attenuating model misspecification issues, and increasing of transparency in model selection. One particularly mature strand of the literature include approaches that incorporate supervised ML approaches in the estimation of the ATE of a binary treatment, under the unconfoundedness and positivity assumptions (also known as exchangeability and overlap assumptions).  This article begins by reviewing popular supervised machine learning algorithms, including trees-based methods and the lasso, as well as ensembles, with a focus on the Super Learner. Then, some specific uses of machine learning for treatment effect estimation are introduced and illustrated, namely (1) to create balance among treated and control groups, (2) to estimate so-called nuisance models (e.g., the propensity score, or conditional expectations of the outcome) in semi-parametric estimators that target causal parameters (e.g., targeted maximum likelihood estimation or the double ML estimator), and (3) the use of machine learning for variable selection in situations with a high number of covariates.  Since there is no universal best estimator, whether parametric or data-adaptive, it is best practice to incorporate a semi-automated approach than can select the models best supported by the observed data, thus attenuating the reliance on subjective choices.","",""
32,"A. Ata, Muhammad Adnan Khan, Sagheer Abbas, Gulzar Ahmad, A. Fatima","MODELLING SMART ROAD TRAFFIC CONGESTION CONTROL SYSTEM USING MACHINE LEARNING TECHNIQUES",2019,"","","","",121,"2022-07-13 09:23:05","","10.14311/NNW.2019.29.008","","",,,,,32,10.67,6,5,3,"By the dramatic growth of the population in cities requires the traffic systems to be designed efficiently and sustainably by taking full advantage of modern-day technology. Dynamic traffic flow is a significant issue which brings about a block of traffic movement. Thus, for tackling this issue, this paper aims to provide a mechanism to predict the traffic congestion with the help of Artificial Neural Networks (ANN) which shall control or minimize the blockage and result in the smoothening of road traffic. Proposed Modeling Smart Road Traffic Congestion Control using Artificial Back Propagation Neural Networks (MSR2C-ABPNN) for road traffic increase transparency, availability and efficiency in services offered to the citizens. In this paper, the prediction of congestion is operationalized by using the algorithm of backpropagation to train the neural network. The proposed system aims to provide a solution that will increase the comfort level of travellers to make intelligent and better transportation decision, and the neural network is a plausible approach to find traffic situations. Proposed MSR2C-ABPNN with Time series gives attractive results concerning MSE as compared to the fitting approach.","",""
14,"Carlos Vladimiro Gonzalez Zelaya","Towards Explaining the Effects of Data Preprocessing on Machine Learning",2019,"","","","",122,"2022-07-13 09:23:05","","10.1109/icde.2019.00245","","",,,,,14,4.67,14,1,3,"Ensuring the explainability of machine learning models is an active research topic, naturally associated with notions of algorithmic transparency and fairness. While most approaches focus on the problem of making the model itself explainable, we note that many of the decisions that affect the model's predictive behaviour are made during data preprocessing, and are encoded as specific data transformation steps as part of pre-learning pipelines. Our research explores metrics to quantify the effect of some of these steps. In this initial work we define a simple metric, which we call volatility, to measure the effect of including/excluding a specific step on predictions made by the resulting model. Using training set rebalancing as a concrete example, we report on early experiments on measuring volatility in two public benchmark datasets, Students' Academic Performance and German Credit, with the ultimate goal of identifying predictors for volatility that are independent of the dataset and of the specific preprocessing step.","",""
29,"H. Kan, Hadi Kharrazi, Hsien-Yen Chang, D. Bodycombe, K. Lemke, J. Weiner","Exploring the use of machine learning for risk adjustment: A comparison of standard and penalized linear regression models in predicting health care costs in older adults",2019,"","","","",123,"2022-07-13 09:23:05","","10.1371/journal.pone.0213258","","",,,,,29,9.67,5,6,3,"Background Payers and providers still primarily use ordinary least squares (OLS) to estimate expected economic and clinical outcomes for risk adjustment purposes. Penalized linear regression represents a practical and incremental step forward that provides transparency and interpretability within the familiar regression framework. This study conducted an in-depth comparison of prediction performance of standard and penalized linear regression in predicting future health care costs in older adults. Methods and findings This retrospective cohort study included 81,106 Medicare Advantage patients with 5 years of continuous medical and pharmacy insurance from 2009 to 2013. Total health care costs in 2013 were predicted with comorbidity indicators from 2009 to 2012. Using 2012 predictors only, OLS performed poorly (e.g., R2 = 16.3%) compared to penalized linear regression models (R2 ranging from 16.8 to 16.9%); using 2009–2012 predictors, the gap in prediction performance increased (R2:15.0% versus 18.0–18.2%). OLS with a reduced set of predictors selected by lasso showed improved performance (R2 = 16.6% with 2012 predictors, 17.4% with 2009–2012 predictors) relative to OLS without variable selection but still lagged behind the prediction performance of penalized regression. Lasso regression consistently generated prediction ratios closer to 1 across different levels of predicted risk compared to other models. Conclusions This study demonstrated the advantages of using transparent and easy-to-interpret penalized linear regression for predicting future health care costs in older adults relative to standard linear regression. Penalized regression showed better performance than OLS in predicting health care costs. Applying penalized regression to longitudinal data increased prediction accuracy. Lasso regression in particular showed superior prediction ratios across low and high levels of predicted risk. Health care insurers, providers and policy makers may benefit from adopting penalized regression such as lasso regression for cost prediction to improve risk adjustment and population health management and thus better address the underlying needs and risk of the populations they serve.","",""
0,"Mengdi Huai","Fostering Trustworthiness in Machine Learning via Robust and Automated Model Interpretation",2022,"","","","",124,"2022-07-13 09:23:05","","","","",,,,,0,0.00,0,1,1,"Machine learning models have been widely applied in real world to build intelligent systems (e.g., selfdriving cars, intelligent recommendation systems, and clinical decision support systems). However, traditional machine learning models mainly focus on optimizing accuracy and efficiency, and they fail to consider how to foster trustworthiness in their design. In practice, machine learning models are suffering a crisis of trust when they are applied in real-world applications due to the lack of transparency behind their behaviors. The concern about the “black box” nature of machine learning models makes decision makers reluctant to trust the predicted results, especially when these models are used for making critical decisions (e.g., medical disease diagnosis). In this talk, I will introduce my research efforts towards the goal of making machine learning trustworthy. Specifically, I will discuss how to foster trustworthiness in machine learning via robust and automated model interpretation. I will first describe my recent research on the security vulnerability of model interpretation methods for deep reinforcement learning (DRL) and introduce two malicious attack frameworks that can significantly alter the interpretation results while incurring minor damage to the performance of the original DRL model. Then, I will present an automated and robust model interpretation framework, which can not only automatically generate the concept-based explanations for the predicted results but also provide certified robustness guarantees for the generated explanations. Bio: Mengdi Huai is a Ph.D. candidate in the Department of Computer Science at the University of Virginia. Her research interests lie in the areas of data mining and machine learning, with a current focus on developing novel techniques to build trustworthy learning systems that are explainable, robust, private, and fair. Mengdi is also interested in designing effective data mining and machine learning algorithms to deal with complex data with both strong empirical performance and theoretical guarantees. Her research work has been published in various top-tier venues, such as KDD, AAAI, IJCAI, NeurIPS, and TKDD. Mengdi received multiple prestigious awards from the University of Virginia for her excellence in research, including the Sture G. Olsson Fellowship in Engineering and the John A. Stankovic Research Award. Her recent work on malicious attacks against model interpretation won the Best Paper Runner-up of KDD2020. Mengdi was selected as one of the Rising Stars in EECS at MIT. She was also selected as one of the Rising Stars in Data Science at UChicago.","",""
0,"Eike Petersen, Yannik Potdevin, Esfandiar Mohammadi, Stephan Zidowitz, Sabrina Breyer, Dirk Nowotka, Sandra Henn, Ludwig Pechmann, M. Leucker, P. Rostalski, Christian Herzog","Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Challenges and Solutions",2021,"","","","",125,"2022-07-13 09:23:05","","10.1109/ACCESS.2022.3178382","","",,,,,0,0.00,0,11,1,"Machine learning is expected to fuel significant improvements in medical care. To ensure that fundamental principles such as beneficence, respect for human autonomy, prevention of harm, justice, privacy, and transparency are respected, medical machine learning systems must be developed responsibly. Many high-level declarations of ethical principles have been put forth for this purpose, but there is a severe lack of technical guidelines explicating the practical consequences for medical machine learning. Similarly, there is currently considerable uncertainty regarding the exact regulatory requirements placed upon medical machine learning systems. This survey provides an overview of the technical and procedural challenges involved in creating medical machine learning systems responsibly and in conformity with existing regulations, as well as possible solutions to address these challenges. First, a brief review of existing regulations affecting medical machine learning is provided, showing that properties such as safety, robustness, reliability, privacy, security, transparency, explainability, and nondiscrimination are all demanded already by existing law and regulations—albeit, in many cases, to an uncertain degree. Next, the key technical obstacles to achieving these desirable properties are discussed, as well as important techniques to overcome these obstacles in the medical context. We notice that distribution shift, spurious correlations, model underspecification, uncertainty quantification, and data scarcity represent severe challenges in the medical context. Promising solution approaches include the use of large and representative datasets and federated learning as a means to that end, the careful exploitation of domain knowledge, the use of inherently transparent models, comprehensive out-of-distribution model testing and verification, as well as algorithmic impact assessments.","",""
0,"S. Al-Zaiti, Alaa A. Alghwiri, Xiao Hu, G. Clermont, A. Peace, P. Macfarlane, R. Bond","A clinician’s guide to understanding and critically appraising machine learning studies: a checklist for Ruling Out Bias Using Standard Tools in Machine Learning (ROBUST-ML)",2022,"","","","",126,"2022-07-13 09:23:05","","10.1093/ehjdh/ztac016","","",,,,,0,0.00,0,7,1,"  Developing functional machine learning (ML)-based models to address unmet clinical needs requires unique considerations for optimal clinical utility. Recent debates about the rigours, transparency, explainability, and reproducibility of ML models, terms which are defined in this article, have raised concerns about their clinical utility and suitability for integration in current evidence-based practice paradigms. This featured article focuses on increasing the literacy of ML among clinicians by providing them with the knowledge and tools needed to understand and critically appraise clinical studies focused on ML. A checklist is provided for evaluating the rigour and reproducibility of the four ML building blocks: data curation, feature engineering, model development, and clinical deployment. Checklists like this are important for quality assurance and to ensure that ML studies are rigourously and confidently reviewed by clinicians and are guided by domain knowledge of the setting in which the findings will be applied. Bridging the gap between clinicians, healthcare scientists, and ML engineers can address many shortcomings and pitfalls of ML-based solutions and their potential deployment at the bedside.","",""
11,"B. Celik, J. Vanschoren","Adaptation Strategies for Automated Machine Learning on Evolving Data",2020,"","","","",127,"2022-07-13 09:23:05","","10.1109/TPAMI.2021.3062900","","",,,,,11,5.50,6,2,2,"Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust to changes in the underlying data. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on a variety of AutoML approaches for building machine learning pipelines, including Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.","",""
0,"S. V. Sanghami, John J. Lee, Qin Hu","Machine Learning Enhanced Blockchain Consensus with Transaction Prioritization for Smart Cities",2021,"","","","",128,"2022-07-13 09:23:05","","10.1109/jiot.2022.3175208","","",,,,,0,0.00,0,3,1,"In the given technology-driven era, smart cities are the next frontier of technology, aiming at improving the quality of people’s lives. Many research works focus on future smart cities with holistic approach towards smart city development and realising an overarching smart city vision. In this paper, we introduce such future smart cities that leverage completely on blockchain technology in areas like data security, energy and waste management, urban management, governance, transport, supply chain, including emergency events and environmental monitoring. Blockchain, being a decentralized immutable ledger, has the potential to promote the development of smart cities by guaranteeing transparency, data security, reliability, efficiency, interoperability and privacy which makes it a promising fit for smart cities. Particularly, using blockchain in emergency events will ensure multiple parties to coordinate resources in an emergency, coordinate more efficient disaster responses, enable rescue teams to perform their jobs more efficiently, provide interoperability between many parties involved in response, increase timeliness of services and provide transparency. In that case, if a current fee-based or first-come-first-serve-based processing is used, emergency events may get delayed in being processed due to competition, and thus, threatening people’s lives. Thus, there is a need for transaction prioritization based on the priority of information and quick creation of blocks (i.e., variable interval block creation mechanism). Also, since the leader will ensure transaction prioritization while generating blocks, leader rotation and proper election procedure becomes important for the transaction prioritization process to take place honestly and efficiently in our consortium blockchain. In our consensus protocol, we deploy a machine learning (ML) algorithm to achieve efficient leader election and design a novel dynamic block creation algorithm. Also, to ensure honest assessment from the followers on the leaders’ generated blocks, a peer-predictionbased verification mechanism is proposed. Both security analysis and simulation experiments are carried out to demonstrate the robustness, accuracy, and efficiency of our proposed scheme.","",""
9,"T. Botari, Rafael Izbicki, A. Carvalho","Local Interpretation Methods to Machine Learning Using the Domain of the Feature Space",2019,"","","","",129,"2022-07-13 09:23:05","","10.1007/978-3-030-43823-4_21","","",,,,,9,3.00,3,3,3,"","",""
43,"Fatimah Ishowo-Oloko, Jean‐François Bonnefon, Zakariyah Shoroye, J. Crandall, I. Rahwan, Talal Rahwan","Behavioural evidence for a transparency-efficiency tradeoff in human-machine cooperation",2019,"","","","",130,"2022-07-13 09:23:05","","10.1038/s42256-019-0113-5","","",,,,,43,14.33,7,6,3,"","",""
111,"Heinrich Jiang, Ofir Nachum","Identifying and Correcting Label Bias in Machine Learning",2019,"","","","",131,"2022-07-13 09:23:05","","","","",,,,,111,37.00,56,2,3,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.","",""
102,"Peng Xu, Farbod Roosta-Khorasani, Michael W. Mahoney","Second-Order Optimization for Non-Convex Machine Learning: An Empirical Study",2017,"","","","",132,"2022-07-13 09:23:05","","10.1137/1.9781611976236.23","","",,,,,102,20.40,34,3,5,"While first-order optimization methods such as stochastic gradient descent (SGD) are popular in machine learning (ML), they come with well-known deficiencies, including relatively-slow convergence, sensitivity to the settings of hyper-parameters such as learning rate, stagnation at high training errors, and difficulty in escaping flat regions and saddle points. These issues are particularly acute in highly non-convex settings such as those arising in neural networks. Motivated by this, there has been recent interest in second-order methods that aim to alleviate these shortcomings by capturing curvature information. In this paper, we report detailed empirical evaluations of a class of Newton-type methods, namely sub-sampled variants of trust region (TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex ML problems. In doing so, we demonstrate that these methods not only can be computationally competitive with hand-tuned SGD with momentum, obtaining comparable or better generalization performance, but also they are highly robust to hyper-parameter settings. Further, in contrast to SGD with momentum, we show that the manner in which these Newton-type methods employ curvature information allows them to seamlessly escape flat regions and saddle points.","",""
8,"Fabian Horst, D. Slijepcevic, S. Lapuschkin, Anna-Maria Raberger, M. Zeppelzauer, W. Samek, C. Breiteneder, W. Schöllhorn, B. Horsak","On the Understanding and Interpretation of Machine Learning Predictions in Clinical Gait Analysis Using Explainable Artificial Intelligence",2019,"","","","",133,"2022-07-13 09:23:05","","","","",,,,,8,2.67,1,9,3,"Systems incorporating Artificial Intelligence (AI) and machine learning (ML) techniques are increasingly used to guide decision-making in the healthcare sector. While AI-based systems provide powerful and promising results with regard to their classification and prediction accuracy (e.g., in differentiating between different disorders in human gait), most share a central limitation, namely their black-box character. Understanding which features classification models learn, whether they are meaningful and consequently whether their decisions are trustworthy is difficult and often impossible to comprehend. This severely hampers their applicability as decisionsupport systems in clinical practice. There is a strong need for AI-based systems to provide transparency and justification of predictions, which are necessary also for ethical and legal compliance. As a consequence, in recent years the field of explainable AI (XAI) has gained increasing importance. XAI focuses on the development of methods that enhance transparency and interpretability of complex ML models, such as Deep (Convolutional) Neural Networks. The primary aim of this article is to investigate whether XAI methods can enhance transparency, explainability and interpretability of predictions in automated clinical gait classification. We utilize a dataset comprising bilateral three-dimensional ground reaction force measurements from 132 patients with different lower-body gait disorders and 62 healthy controls. In our experiments, 1 ar X iv :1 91 2. 07 73 7v 1 [ cs .L G ] 1 6 D ec 2 01 9 Horst and Slijepcevic et al. Explainable AI in Clinical Gait Analysis we included several gait classification tasks, employed a representative set of classification methods, and a well-established XAI method – Layer-wise Relevance Propagation (LRP) – to explain decisions at the signal (input) level. The classification results are analyzed, compared and interpreted in terms of classification accuracy and relevance of input values for specific decisions. The decomposed input relevance information are evaluated from a statistical (using Statistical Parameter Mapping) and clinical (by an expert) viewpoint. There are three dimensions in our comparison: (i) different classification tasks, (ii) different classification methods, and (iii) data normalization. The presented approach exemplifies how XAI can be used to understand and interpret state-of-the-art ML models trained for gait classification tasks, and shows that the features that are considered relevant for machine learning models can be attributed to meaningful and clinically relevant biomechanical gait characteristics.","",""
7,"C. Seifert, Stefanie Scherzinger, L. Wiese","Towards Generating Consumer Labels for Machine Learning Models",2019,"","","","",134,"2022-07-13 09:23:05","","10.1109/CogMI48466.2019.00033","","",,,,,7,2.33,2,3,3,"Machine learning (ML) based decision making is becoming commonplace. For persons affected by ML-based decisions, a certain level of transparency regarding the properties of the underlying ML model can be fundamental. In this vision paper, we propose to issue consumer labels for trained and published ML models. These labels primarily target machine learning lay persons, such as the operators of an ML system, the executors of decisions, and the decision subjects themselves. Provided that consumer labels comprehensively capture the characteristics of the trained ML model, consumers are enabled to recognize when human intelligence should supersede artificial intelligence. In the long run, we envision a service that generates these consumer labels (semi-)automatically. In this paper, we survey the requirements that an ML system should meet, and correspondingly, the properties that an ML consumer label could capture. We further discuss the feasibility of operationalizing and benchmarking these requirements in the automated generation of ML consumer labels.","",""
3,"Minsung Hong, R. Akerkar","Analytics and Evolving Landscape of Machine Learning for Emergency Response",2019,"","","","",135,"2022-07-13 09:23:05","","10.1007/978-3-030-15628-2_11","","",,,,,3,1.00,2,2,3,"","",""
3,"Haoyu Yang, Wen Chen, P. Pathak, Frank Gennari, Ya-Chieh Lai, Bei Yu","Automatic Layout Generation with Applications in Machine Learning Engine Evaluation",2019,"","","","",136,"2022-07-13 09:23:05","","10.1109/MLCAD48534.2019.9142121","","",,,,,3,1.00,1,6,3,"Machine learning-based lithography hotspot detection has been deeply studied recently, from varies feature extraction techniques to efficient learning models. It has been observed that such machine learning-based frameworks are providing satisfactory metal layer hotspot prediction results on known public metal layer benchmarks. In this work, we seek to evaluate how these machine learning-based hotspot detectors generalize to complicated patterns. We first introduce a automatic layout generation tool that can synthesize varies layout patterns given a set of design rules. The tool currently supports both metal layer and via layer generation. As a case study, we conduct hotspot detection on the generated via layer layouts with representative machine learning-based hotspot detectors, which shows that continuous study on model robustness and generality is necessary to prototype and integrate the learning engines in DFM flows. The source code of the layout generation tool will be available at https://github.com/phdyang007/layout-generation.","",""
8,"S. Ke, J. L. M. Olea, J. Nesbit","Robust Machine Learning Algorithms for Text Analysis∗",2019,"","","","",137,"2022-07-13 09:23:05","","","","",,,,,8,2.67,3,3,3,"We study the Latent Dirichlet Allocation model, a popular Bayesian algorithm for text analysis. Our starting point is the generic lack of identification of the model’s parameters, which suggests that the choice of prior matters. We then characterize by how much the posterior mean of a given functional of the model’s parameters varies in response to a change in the prior, and we suggest two algorithms to approximate this range. Both of our algorithms rely on obtaining multiple Nonnegative Matrix Factorizations of either the posterior draws of the corpus’ population term-document frequency matrix or of its sample analogue. The key idea is to maximize/minimize the functional of interest over all these nonnegative matrix factorizations. To illustrate the applicability of our results, we revisit recent work on the effects of increased transparency on discussions regarding monetary policy decisions in the United States.","",""
104,"Yangkang Zhang","Automatic microseismic event picking via unsupervised machine learning",2020,"","","","",138,"2022-07-13 09:23:05","","10.1093/GJI/GGX420","","",,,,,104,52.00,104,1,2,"  Effective and efficient arrival picking plays an important role in microseismic and earthquake data processing and imaging. Widely used short-term-average long-term-average ratio (STA/LTA) based arrival picking algorithms suffer from the sensitivity to moderate-to-strong random ambient noise. To make the state-of-the-art arrival picking approaches effective, microseismic data need to be first pre-processed, for example, removing sufficient amount of noise, and second analysed by arrival pickers. To conquer the noise issue in arrival picking for weak microseismic or earthquake event, I leverage the machine learning techniques to help recognizing seismic waveforms in microseismic or earthquake data. Because of the dependency of supervised machine learning algorithm on large volume of well-designed training data, I utilize an unsupervised machine learning algorithm to help cluster the time samples into two groups, that is, waveform points and non-waveform points. The fuzzy clustering algorithm has been demonstrated to be effective for such purpose. A group of synthetic, real microseismic and earthquake data sets with different levels of complexity show that the proposed method is much more robust than the state-of-the-art STA/LTA method in picking microseismic events, even in the case of moderately strong background noise.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",139,"2022-07-13 09:23:05","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",140,"2022-07-13 09:23:05","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
78,"Xianfang Wang, Peng Gao, Yifeng Liu, Hongfei Li, Fan Lu","Predicting Thermophilic Proteins by Machine Learning",2020,"","","","",141,"2022-07-13 09:23:05","","10.2174/1574893615666200207094357","","",,,,,78,39.00,16,5,2,"  Thermophilic proteins can maintain good activity under high temperature, therefore, it is important to study thermophilic proteins for the thermal stability of proteins.    In order to solve the problem of low precision and low efficiency in predicting thermophilic proteins, a prediction method based on feature fusion and machine learning was proposed in this paper.    For the selected thermophilic data sets, firstly, the thermophilic protein sequence was characterized based on feature fusion by the combination of g-gap dipeptide, entropy density and autocorrelation coefficient. Then, Kernel Principal Component Analysis (KPCA) was used to reduce the dimension of the expressed protein sequence features in order to reduce the training time and improve efficiency. Finally, the classification model was designed by using the classification algorithm.    A variety of classification algorithms was used to train and test on the selected thermophilic dataset. By comparison, the accuracy of the Support Vector Machine (SVM) under the jackknife method was over 92%. The combination of other evaluation indicators also proved that the SVM performance was the best.     Because of choosing an effectively feature representation method and a robust classifier, the proposed method is suitable for predicting thermophilic proteins and is superior to most reported methods. ","",""
74,"Monika A. Myszczynska, P. Ojamies, Alix M. B. Lacoste, Daniel Neil, Amir Saffari, R. Mead, G. Hautbergue, J. Holbrook, L. Ferraiuolo","Applications of machine learning to diagnosis and treatment of neurodegenerative diseases",2020,"","","","",142,"2022-07-13 09:23:05","","10.1038/s41582-020-0377-8","","",,,,,74,37.00,8,9,2,"","",""
53,"M. Shafique, Mahum Naseer, T. Theocharides, C. Kyrkou, O. Mutlu, Lois Orosa, Jungwook Choi","Robust Machine Learning Systems: Challenges,Current Trends, Perspectives, and the Road Ahead",2020,"","","","",143,"2022-07-13 09:23:05","","10.1109/MDAT.2020.2971217","","",,,,,53,26.50,8,7,2,"Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPS) and Internet-of-Things (IoT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques. —Partha Pratim Pande, Washington State University","",""
233,"E. Vayena, A. Blasimme, I. Cohen","Machine learning in medicine: Addressing ethical challenges",2018,"","","","",144,"2022-07-13 09:23:05","","10.1371/journal.pmed.1002689","","",,,,,233,58.25,78,3,4,"Effy Vayena and colleagues argue that machine learning in medicine must offer data protection, algorithmic transparency, and accountability to earn the trust of patients and clinicians.","",""
41,"Sirui Lu, L. Duan, D. Deng","Quantum Adversarial Machine Learning",2019,"","","","",145,"2022-07-13 09:23:05","","10.1103/PHYSREVRESEARCH.2.033212","","",,,,,41,13.67,14,3,3,"Adversarial machine learning is an emerging field that focuses on studying vulnerabilities of machine learning approaches in adversarial settings and developing techniques accordingly to make learning robust to adversarial manipulations. It plays a vital role in various machine learning applications and has attracted tremendous attention across different communities recently. In this paper, we explore different adversarial scenarios in the context of quantum machine learning. We find that, similar to traditional classifiers based on classical neural networks, quantum learning systems are likewise vulnerable to crafted adversarial examples, independent of whether the input data is classical or quantum. In particular, we find that a quantum classifier that achieves nearly the state-of-the-art accuracy can be conclusively deceived by adversarial examples obtained via adding imperceptible perturbations to the original legitimate samples. This is explicitly demonstrated with quantum adversarial learning in different scenarios, including classifying real-life images (e.g., handwritten digit images in the dataset MNIST), learning phases of matter (such as, ferromagnetic/paramagnetic orders and symmetry protected topological phases), and classifying quantum data. Furthermore, we show that based on the information of the adversarial examples at hand, practical defense strategies can be designed to fight against a number of different attacks. Our results uncover the notable vulnerability of quantum machine learning systems to adversarial perturbations, which not only reveals a novel perspective in bridging machine learning and quantum physics in theory but also provides valuable guidance for practical applications of quantum classifiers based on both near-term and future quantum technologies.","",""
103,"Muhammad Attique Khan, I. Ashraf, M. Alhaisoni, Robertas Damaševičius, R. Scherer, A. Rehman, S. Bukhari","Multimodal Brain Tumor Classification Using Deep Learning and Robust Feature Selection: A Machine Learning Application for Radiologists",2020,"","","","",146,"2022-07-13 09:23:05","","10.3390/diagnostics10080565","","",,,,,103,51.50,15,7,2,"Manual identification of brain tumors is an error-prone and tedious process for radiologists; therefore, it is crucial to adopt an automated system. The binary classification process, such as malignant or benign is relatively trivial; whereas, the multimodal brain tumors classification (T1, T2, T1CE, and Flair) is a challenging task for radiologists. Here, we present an automated multimodal classification method using deep learning for brain tumor type classification. The proposed method consists of five core steps. In the first step, the linear contrast stretching is employed using edge-based histogram equalization and discrete cosine transform (DCT). In the second step, deep learning feature extraction is performed. By utilizing transfer learning, two pre-trained convolutional neural network (CNN) models, namely VGG16 and VGG19, were used for feature extraction. In the third step, a correntropy-based joint learning approach was implemented along with the extreme learning machine (ELM) for the selection of best features. In the fourth step, the partial least square (PLS)-based robust covariant features were fused in one matrix. The combined matrix was fed to ELM for final classification. The proposed method was validated on the BraTS datasets and an accuracy of 97.8%, 96.9%, 92.5% for BraTs2015, BraTs2017, and BraTs2018, respectively, was achieved.","",""
3,"Kai R. T. Larsen, Daniel S. Becker","Why Use Automated Machine Learning?",2021,"","","","",147,"2022-07-13 09:23:05","","10.1093/oso/9780190941659.003.0001","","",,,,,3,3.00,2,2,1,"Machine learning is involved in search, translation, detecting depression, likelihood of college dropout, finding lost children, and to sell all kinds of products. While barely beyond its inception, the current machine learning revolution will affect people and organizations no less than the Industrial Revolution’s effect on weavers and many other skilled laborers. Machine learning will automate hundreds of millions of jobs that were considered too complex for machines ever to take over even a decade ago, including driving, flying, painting, programming, and customer service, as well as many of the jobs previously reserved for humans in the fields of finance, marketing, operations, accounting, and human resources. This section explains how automated machine learning addresses exploratory data analysis, feature engineering, algorithm selection, hyperparameter tuning, and model diagnostics. The section covers the eight criteria considered essential for AutoML to have significant impact: accuracy, productivity, ease of use, understanding and learning, resource availability, process transparency, generalization, and recommended actions. ","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",148,"2022-07-13 09:23:05","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",149,"2022-07-13 09:23:05","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
2,"Sarath Shekkizhar, Antonio Ortega","Revisiting Local Neighborhood Methods in Machine Learning",2021,"","","","",150,"2022-07-13 09:23:05","","10.1109/DSLW51110.2021.9523409","","",,,,,2,2.00,1,2,1,"Several machine learning methods leverage the idea of locality by using k-nearest neighbor (KNN) techniques to design better pattern recognition models. However, the choice of KNN parameters such as k is often made experimentally, e.g., via cross-validation, leading to local neighborhoods without a clear geometric interpretation. In this paper, we replace KNN with our recently introduced polytope neighborhood scheme - Non Negative Kernel regression (NNK). NNK formulates neighborhood selection as a sparse signal approximation problem and is adaptive to the local distribution of samples in the neighborhood of the data point of interest. We analyze the benefits of local neighborhood construction based on NNK. In particular, we study the generalization properties of local interpolation using NNK and present data dependent bounds in the non asymptotic setting. The applicability of NNK in transductive few shot learning setting and for measuring distance between two datasets is demonstrated. NNK exhibits robust, superior performance in comparison to standard locally weighted neighborhood methods.","",""
1,"C. Betancourt, S. Stadtler, T. Stomberg, Ann-Kathrin Edrich, Ankit Patnala, R. Roscher, J. Kowalski, M. Schultz","Global fine resolution mapping of ozone metrics through explainable machine learning",2021,"","","","",151,"2022-07-13 09:23:05","","10.5194/EGUSPHERE-EGU21-7596","","",,,,,1,1.00,0,8,1,"<p>Through the availability of multi-year ground based ozone observations on a global scale, substantial geospatial meta data, and high performance computing capacities, it is now possible to use machine learning for a global data-driven ozone assessment. In this presentation, we will show a novel, completely data-driven approach to map tropospheric ozone globally.</p><p>Our goal is to interpolate ozone metrics and aggregated statistics from the database of the Tropospheric Ozone Assessment Report (TOAR) onto a global 0.1&#176; x 0.1&#176; resolution grid. &#160;It is challenging to interpolate ozone, a toxic greenhouse gas because its formation depends on many interconnected environmental factors on small scales. We conduct the interpolation with various machine learning methods trained on aggregated hourly ozone data from five years at more than 5500 locations worldwide. We use several geospatial datasets as training inputs to provide proxy input for environmental factors controlling ozone formation, such as precursor emissions and climate. The resulting maps contain different ozone metrics, i.e. statistical aggregations which are widely used to assess air pollution impacts on health, vegetation, and climate.</p><p>The key aspects of this contribution are twofold: First, we apply explainable machine learning methods to the data-driven ozone assessment. Second, we discuss dominant uncertainties relevant to the ozone mapping and quantify their impact whenever possible. Our methods include a thorough a-priori uncertainty estimation of the various data and methods, assessment of scientific consistency, finding critical model parameters, using ensemble methods, and performing error modeling.</p><p>Our work aims to increase the reliability and integrity of the derived ozone maps through the provision of scientific robustness to a data-centric machine learning task. This study hence represents a blueprint for how to formulate an environmental machine learning task scientifically, gather the necessary data, and develop a data-driven workflow that focuses on optimizing transparency and applicability of its product to maximize its scientific knowledge return.</p>","",""
22,"Lal Hussain, I. Awan, W. Aziz, Sharjil Saeed, Amjad Ali, Farukh Zeeshan, K. Kwak","Detecting Congestive Heart Failure by Extracting Multimodal Features and Employing Machine Learning Techniques",2020,"","","","",152,"2022-07-13 09:23:05","","10.1155/2020/4281243","","",,,,,22,11.00,3,7,2,"The adaptability of heart to external and internal stimuli is reflected by the heart rate variability (HRV). Reduced HRV can be a predictor of negative cardiovascular outcomes. Based on the nonlinear, nonstationary, and highly complex dynamics of the controlling mechanism of the cardiovascular system, linear HRV measures have limited capability to accurately analyze the underlying dynamics. In this study, we propose an automated system to analyze HRV signals by extracting multimodal features to capture temporal, spectral, and complex dynamics. Robust machine learning techniques, such as support vector machine (SVM) with its kernel (linear, Gaussian, radial base function, and polynomial), decision tree (DT), k-nearest neighbor (KNN), and ensemble classifiers, were employed to evaluate the detection performance. Performance was evaluated in terms of specificity, sensitivity, positive predictive value (PPV), negative predictive value (NPV), and area under the receiver operating characteristic curve (AUC). The highest performance was obtained using SVM linear kernel (TA = 93.1%, AUC = 0.97, 95% CI [lower bound = 0.04, upper bound = 0.89]), followed by ensemble subspace discriminant (TA = 91.4%, AUC = 0.96, 95% CI [lower bound 0.07, upper bound = 0.81]) and SVM medium Gaussian kernel (TA = 90.5%, AUC = 0.95, 95% CI [lower bound = 0.07, upper bound = 0.86]). The results reveal that the proposed approach can provide an effective and computationally efficient tool for automatic detection of congestive heart failure patients.","",""
0,"Uma Gunasilan","Debate as a learning activity for teaching programming: a case in the subject of machine learning",2021,"","","","",153,"2022-07-13 09:23:05","","10.1108/heswbl-01-2021-0006","","",,,,,0,0.00,0,1,1,"PurposeDebates are well known to encompass a variety of skills we would like higher education candidates to embody when they graduate.Design/methodology/approachDebates in a classroom with computer science as the main subject has been popular in high schools particularly with emerging issues around the area, however it does not have as an extensive similar documented outreach in tertiary education, particularly in the area of hard computer sciences and more recent concentrations of computer science, such as machine learning, artificial intelligence and cloud computing.FindingsTo explore further, the debate dataset had more methodologies applied and was split into training and testing sets, whose results were then compared by a standardized measure: Root Mean Square Error (RMSE) which is currently standard in the industry. The rationale of the approach is to quantify that debate activities have an immensely positive impact towards both the teaching and learning in technical subjects and needs to be more often and robustly used within higher education.Originality/valueThe rationale of the approach is that classroom debate activities equip students with verbal and social learning styles and an opportunity to engage with content in a way that is more comfortable than working with traditional lecture-and-laboratory style learning.","",""
0,"Yinyihong Liu","Airbnb Pricing Based on Statistical Machine Learning Models",2021,"","","","",154,"2022-07-13 09:23:05","","10.1109/CONF-SPML54095.2021.00042","","",,,,,0,0.00,0,1,1,"Being one of the largest online accommodation booking platforms, Airbnb has many hosts who are seeking for more proper prices to increase their booking rate. To develop a good pricing prediction model, this paper has employed machine learning models including KNN, MLR, LASSO regression, Ridge regression, Random Forest, Gradient Boosting and XGBoost etc. While past studies on Airbnb pricing have applied quantitative pricing, some face the problems that the models are not robust enough and some face the problem of not training the model plentily. To fill this gap, we give careful consideration in exploratory data analysis to make the dataset more reasonable, apply many robust models ranging from regularized regression to ensemble models and use cross validation and random search to tune each parameter in each model. In this way, we not only select XGBoost as the best model for price prediction with R2 score 0.6321, but also uncover the features which have statistical significance with the target price.","",""
0,"J. Figuerêdo, V. T. Sarinho, R. Calumby","Low-Cost Machine Learning for Effective and Efficient Bad Smells Detection",2021,"","","","",155,"2022-07-13 09:23:05","","10.5753/kdmile.2021.17468","","",,,,,0,0.00,0,3,1,"Bad smells are characteristics of software that indicate a code or design problem which can make information system hard to understand, evolve, and maintain. To address this problem, different approaches, manual and automated, have been proposed over the years, including more recently machine learning alternatives. However, despite the advances achieved, some machine learning techniques have not yet been effectively explored, such as the use of feature selection techniques. Moreover, it is not clear to what extent the use of numerous source-code features are necessary for reasonable bad smell detection success. Therefore, in this work we propose an approach using low-cost machine learning for effective and efficient detection of bad smells, through explicit feature selection. Our results showed that the selection allowed to statistically improve the effectiveness of the models. For some cases, the models achieved statistical equivalence, but relying on a highly reduced set of features. Indeed, by using explicit feature selection, simpler models such as Naive Bayes became statistically equivalent to robust models such as Random Forest. Therefore, the selection of features allowed keeping competitive or even superior effectiveness while also improving the efficiency of the models, demanding less computational resources for source-code preprocessing, model training and bad smell detection.","",""
0,"Carlo Mehli, K. Hinkelmann, S. Jüngling","Decision Support combining Machine Learning, Knowledge Representation and Case-Based Reasoning",2021,"","","","",156,"2022-07-13 09:23:05","","","","",,,,,0,0.00,0,3,1,"Knowledge and knowledge work are essential for the success of companies nowadays. Decisions are based on knowledge and better knowledge leads to more informed decisions. Therefore, the management of knowledge and support of decision making has increasingly become a source of competitive advantage for organizations. The current research uses a design science research approach (DSR) with the aim to improve the decision making of a knowledge intensive process such as the student admission process, which is done manually until now. In the awareness phase of the DSR process, the case study research method is applied to analyze the decision making and the knowledge that is needed to derive the decisions. Based on the analysis of the application scenario, suitable methods to support decision making were identified. The resulting system design is based on a combination of Case-Based Reasoning (CBR) and Machine Learning (ML). The proposed system design and prototype has been validated using triangulation evaluation, to assess the impact of the proposed system on the application scenario. The evaluation revealed that the additional hints from CBR and ML can assist the deans of the study program to improve the knowledge management and increase the quality, transparency and consistency of the decision-making process in the student application process. Furthermore, the proposed approach fosters the exchange of knowledge among the different process participants involved and codifies previously tacit knowledge to some extent and provides relevant externalized knowledge to decision makers at the required moment. The designed prototype showcases how ML and CBR methodologies can be combined to support decision making in knowledge intensive processes and finally concludes with potential recommendations for future research.","",""
28,"Maonan Wang, K. Zheng, Yanqing Yang, Xiujuan Wang","An Explainable Machine Learning Framework for Intrusion Detection Systems",2020,"","","","",157,"2022-07-13 09:23:05","","10.1109/ACCESS.2020.2988359","","",,,,,28,14.00,7,4,2,"In recent years, machine learning-based intrusion detection systems (IDSs) have proven to be effective; especially, deep neural networks improve the detection rates of intrusion detection models. However, as models become more and more complex, people can hardly get the explanations behind their decisions. At the same time, most of the works about model interpretation focuses on other fields like computer vision, natural language processing, and biology. This leads to the fact that in practical use, cybersecurity experts can hardly optimize their decisions according to the judgments of the model. To solve these issues, a framework is proposed in this paper to give an explanation for IDSs. This framework uses SHapley Additive exPlanations (SHAP), and combines local and global explanations to improve the interpretation of IDSs. The local explanations give the reasons why the model makes certain decisions on the specific input. The global explanations give the important features extracted from IDSs, present the relationships between the feature values and different types of attacks. At the same time, the interpretations between two different classifiers, one-vs-all classifier and multiclass classifier, are compared. NSL-KDD dataset is used to test the feasibility of the framework. The framework proposed in this paper leads to improve the transparency of any IDS, and helps the cybersecurity staff have a better understanding of IDSs’ judgments. Furthermore, the different interpretations between different kinds of classifiers can also help security experts better design the structures of the IDSs. More importantly, this work is unique in the intrusion detection field, presenting the first use of the SHAP method to give explanations for IDSs.","",""
121,"Andreas Holzinger, Peter Kieseberg, E. Weippl, A. Tjoa","Current Advances, Trends and Challenges of Machine Learning and Knowledge Extraction: From Machine Learning to Explainable AI",2018,"","","","",158,"2022-07-13 09:23:05","","10.1007/978-3-319-99740-7_1","","",,,,,121,30.25,30,4,4,"","",""
49,"Péter Horváth, T. Wild, U. Kutay, G. Csucs","Machine Learning Improves the Precision and Robustness of High-Content Screens",2011,"","","","",159,"2022-07-13 09:23:05","","10.1177/1087057111414878","","",,,,,49,4.45,12,4,11,"Imaging-based high-content screens often rely on single cell-based evaluation of phenotypes in large data sets of microscopic images. Traditionally, these screens are analyzed by extracting a few image-related parameters and use their ratios (linear single or multiparametric separation) to classify the cells into various phenotypic classes. In this study, the authors show how machine learning–based classification of individual cells outperforms those classical ratio-based techniques. Using fluorescent intensity and morphological and texture features, they evaluated how the performance of data analysis increases with increasing feature numbers. Their findings are based on a case study involving an siRNA screen monitoring nucleoplasmic and nucleolar accumulation of a fluorescently tagged reporter protein. For the analysis, they developed a complete analysis workflow incorporating image segmentation, feature extraction, cell classification, hit detection, and visualization of the results. For the classification task, the authors have established a new graphical framework, the Advanced Cell Classifier, which provides a very accurate high-content screen analysis with minimal user interaction, offering access to a variety of advanced machine learning methods.","",""
92,"Martin Rozycki, T. Satterthwaite, N. Koutsouleris, G. Erus, J. Doshi, D. Wolf, Yong Fan, R. Gur, R. Gur, E. Meisenzahl, C. Zhuo, Hong Yin, Hao Yan, W. Yue, Dai Zhang, C. Davatzikos","Multisite Machine Learning Analysis Provides a Robust Structural Imaging Signature of Schizophrenia Detectable Across Diverse Patient Populations and Within Individuals",2018,"","","","",160,"2022-07-13 09:23:05","","10.1093/schbul/sbx137","","",,,,,92,23.00,9,16,4,"Past work on relatively small, single-site studies using regional volumetry, and more recently machine learning methods, has shown that widespread structural brain abnormalities are prominent in schizophrenia. However, to be clinically useful, structural imaging biomarkers must integrate high-dimensional data and provide reproducible results across clinical populations and on an individual person basis. Using advanced multi-variate analysis tools and pooled data from case-control imaging studies conducted at 5 sites (941 adult participants, including 440 patients with schizophrenia), a neuroanatomical signature of patients with schizophrenia was found, and its robustness and reproducibility across sites, populations, and scanners, was established for single-patient classification. Analyses were conducted at multiple scales, including regional volumes, voxelwise measures, and complex distributed patterns. Single-subject classification was tested for single-site, pooled-site, and leave-site-out generalizability. Regional and voxelwise analyses revealed a pattern of widespread reduced regional gray matter volume, particularly in the medial prefrontal, temporolimbic and peri-Sylvian cortex, along with ventricular and pallidum enlargement. Multivariate classification using pooled data achieved a cross-validated prediction accuracy of 76% (AUC = 0.84). Critically, the leave-site-out validation of the detected schizophrenia signature showed accuracy/AUC range of 72-77%/0.73-0.91, suggesting a robust generalizability across sites and patient cohorts. Finally, individualized patient classifications displayed significant correlations with clinical measures of negative, but not positive, symptoms. Taken together, these results emphasize the potential for structural neuroimaging data to provide a robust and reproducible imaging signature of schizophrenia. A web-accessible portal is offered to allow the community to obtain individualized classifications of magnetic resonance imaging scans using the methods described herein.","",""
8,"Mustafa Anil Koçak, David Ramirez, E. Erkip, D. Shasha","SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness",2017,"","","","",161,"2022-07-13 09:23:05","","10.1109/TPAMI.2019.2932415","","",,,,,8,1.60,2,4,5,"<italic>SafePredict</italic> is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, <inline-formula><tex-math notation=""LaTeX"">$1-\epsilon$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""kocak-ieq1-2932415.gif""/></alternatives></inline-formula>, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm so that the error rate on non-refused predictions does not exceed <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq2-2932415.gif""/></alternatives></inline-formula>. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq3-2932415.gif""/></alternatives></inline-formula>, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the-art confidence-based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software is included in the supplementary material, which can be found on the Computer Society Digital Library at <uri>http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2932415</uri>.","",""
44,"Spencer Chang, T. Cohen, B. Ostdiek","What is the Machine Learning",2017,"","","","",162,"2022-07-13 09:23:05","","10.1103/PhysRevD.97.056009","","",,,,,44,8.80,15,3,5,"Applications of machine learning tools to problems of physical interest are often criticized for producing sensitivity at the expense of transparency. To address this concern, we explore a data planing procedure for identifying combinations of variables -- aided by physical intuition -- that can discriminate signal from background. Weights are introduced to smooth away the features in a given variable(s). New networks are then trained on this modified data. Observed decreases in sensitivity diagnose the variable's discriminating power. Planing also allows the investigation of the linear versus non-linear nature of the boundaries between signal and background. We demonstrate the efficacy of this approach using a toy example, followed by an application to an idealized heavy resonance scenario at the Large Hadron Collider. By unpacking the information being utilized by these algorithms, this method puts in context what it means for a machine to learn.","",""
0,"Ransalu Senanayake, Daniel J. Fremont, Mykel J. Kochenderfer, A. Lomuscio, D. Margineantu, Cheng Soon Ong","Guest Editorial: Special issue on robust machine learning",2021,"","","","",163,"2022-07-13 09:23:05","","10.1007/s10994-021-06113-4","","",,,,,0,0.00,0,6,1,"","",""
41,"Saqib Aziz, M. Dowling","Machine Learning and AI for Risk Management",2018,"","","","",164,"2022-07-13 09:23:05","","10.1007/978-3-030-02330-0_3","","",,,,,41,10.25,21,2,4,"","",""
242,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",165,"2022-07-13 09:23:05","","10.1145/3308809.3308857","","",,,,,242,48.40,81,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server andm working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of them working machines suffer Byzantine faults - a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.","",""
38,"Cynthia C. S. Liem, Markus Langer, Andrew M. Demetriou, A. Hiemstra, Achmadnoer Sukma Wicaksana, M. Born, Cornelius J. König","Psychology Meets Machine Learning: Interdisciplinary Perspectives on Algorithmic Job Candidate Screening",2018,"","","","",166,"2022-07-13 09:23:05","","10.1007/978-3-319-98131-4_9","","",,,,,38,9.50,5,7,4,"","",""
79,"Taesik Na, J. Ko, S. Mukhopadhyay","Cascade Adversarial Machine Learning Regularized with a Unified Embedding",2017,"","","","",167,"2022-07-13 09:23:05","","","","",,,,,79,15.80,26,3,5,"Injecting adversarial examples during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we first show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. We also propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.","",""
52,"Sahrish Khan Tayyaba, Hasan Ali Khattak, Ahmad S. Almogren, M. A. Shah, Ikram Ud Din, Ibrahim Alkhalifa, M. Guizani","5G Vehicular Network Resource Management for Improving Radio Access Through Machine Learning",2020,"","","","",168,"2022-07-13 09:23:05","","10.1109/ACCESS.2020.2964697","","",,,,,52,26.00,7,7,2,"The current cellular technology and vehicular networks cannot satisfy the mighty strides of vehicular network demands. Resource management has become a complex and challenging objective to gain expected outcomes in a vehicular environment. The 5G cellular network promises to provide ultra-high-speed, reduced delay, and reliable communications. The development of new technologies such as the network function virtualization (NFV) and software defined networking (SDN) are critical enabling technologies leveraging 5G. The SDN-based 5G network can provide an excellent platform for autonomous vehicles because SDN offers open programmability and flexibility for new services incorporation. This separation of control and data planes enables centralized and efficient management of resources in a very optimized and secure manner by having a global overview of the whole network. The SDN also provides flexibility in communication administration and resource management, which are of critical importance when considering the ad-hoc nature of vehicular network infrastructures, in terms of safety, privacy, and security, in vehicular network environments. In addition, it promises the overall improved performance. In this paper, we propose a flow-based policy framework on the basis of two tiers virtualization for vehicular networks using SDNs. The vehicle to vehicle (V2V) communication is quite possible with wireless virtualization where different radio resources are allocated to V2V communications based on the flow classification, i.e., safety-related flow or non-safety flows, and the controller is responsible for managing the overall vehicular environment and V2X communications. The motivation behind this study is to implement a machine learning-enabled architecture to cater the sophisticated demands of modern vehicular Internet infrastructures. The inclination towards robust communications in 5G-enabled networks has made it somewhat tricky to manage network slicing efficiently. This paper also presents a proof of concept for leveraging machine learning-enabled resource classification and management through experimental evaluation of special-purpose testbed established in custom mininet setup. Furthermore, the results have been evaluated using Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Deep Neural Network (DNN). While concluding the paper, it is shown that the LSTM has outperformed the rest of classification techniques with promising results.","",""
90,"Nagdev Amruthnath, Tarun Gupta","A research study on unsupervised machine learning algorithms for early fault detection in predictive maintenance",2018,"","","","",169,"2022-07-13 09:23:05","","10.1109/IEA.2018.8387124","","",,,,,90,22.50,45,2,4,"The area of predictive maintenance has taken a lot of prominence in the last couple of years due to various reasons. With new algorithms and methodologies growing across different learning methods, it has remained a challenge for industries to adopt which method is fit, robust and provide most accurate detection. Fault detection is one of the critical components of predictive maintenance; it is very much needed for industries to detect faults early and accurately. In a production environment, to minimize the cost of maintenance, sometimes it is required to build a model with minimal or no historical data. In such cases, unsupervised learning would be a better option model building. In this paper, we have chosen a simple vibration data collected from an exhaust fan, and have fit different unsupervised learning algorithms such as PCA T2 statistic, Hierarchical clustering, K-Means, Fuzzy C-Means clustering and model-based clustering to test its accuracy, performance, and robustness. In the end, we have proposed a methodology to benchmark different algorithms and choosing the final model.","",""
0,"Iksung Kang, A. Goy, G. Barbastathis","Correction: Dynamical machine learning volumetric reconstruction of objects’ interiors from limited angular views",2021,"","","","",170,"2022-07-13 09:23:05","","10.1038/s41377-021-00615-5","","",,,,,0,0.00,0,3,1,"","",""
0,"T. Martin, S. Areibi, G. Grewal","Effective Machine-Learning Models for Predicting Routability During FPGA Placement",2021,"","","","",171,"2022-07-13 09:23:05","","10.1109/MLCAD52597.2021.9531243","","",,,,,0,0.00,0,3,1,"The ability to efficiently and accurately predict placement routability, while avoiding the large computational cost of performing routing, is an asset when seeking to reduce total placement and routing runtime. In this paper, we present a series of simple ML models and ensembles to predict the routability of a placement solution. Ensembles based on Bagging, Boosting and Stack of classifiers are introduced to produce more accurate and robust solutions than single/simple models. Our results show an improvement in prediction accuracy and runtime compared to the best published results in the literature.","",""
163,"Michael Veale, R. Binns","Fairer machine learning in the real world: Mitigating discrimination without collecting sensitive data",2017,"","","","",172,"2022-07-13 09:23:05","","10.1177/2053951717743530","","",,,,,163,32.60,82,2,5,"Decisions based on algorithmic, machine learning models can be unfair, reproducing biases in historical data used to train them. While computational techniques are emerging to address aspects of these concerns through communities such as discrimination-aware data mining (DADM) and fairness, accountability and transparency machine learning (FATML), their practical implementation faces real-world challenges. For legal, institutional or commercial reasons, organisations might not hold the data on sensitive attributes such as gender, ethnicity, sexuality or disability needed to diagnose and mitigate emergent indirect discrimination-by-proxy, such as redlining. Such organisations might also lack the knowledge and capacity to identify and manage fairness issues that are emergent properties of complex sociotechnical systems. This paper presents and discusses three potential approaches to deal with such knowledge and information deficits in the context of fairer machine learning. Trusted third parties could selectively store data necessary for performing discrimination discovery and incorporating fairness constraints into model-building in a privacy-preserving manner. Collaborative online platforms would allow diverse organisations to record, share and access contextual and experiential knowledge to promote fairness in machine learning systems. Finally, unsupervised learning and pedagogically interpretable algorithms might allow fairness hypotheses to be built for further selective testing and exploration. Real-world fairness challenges in machine learning are not abstract, constrained optimisation problems, but are institutionally and contextually grounded. Computational fairness tools are useful, but must be researched and developed in and with the messy contexts that will shape their deployment, rather than just for imagined situations. Not doing so risks real, near-term algorithmic harm.","",""
26,"Jianlong Zhou, Fang Chen","Human and Machine Learning",2018,"","","","",173,"2022-07-13 09:23:05","","10.1007/978-3-319-90403-0","","",,,,,26,6.50,13,2,4,"","",""
178,"J. Otterbach, R. Manenti, N. Alidoust, A. Bestwick, M. Block, B. Bloom, S. Caldwell, N. Didier, E. Fried, S. Hong, Peter J. Karalekas, C. Osborn, A. Papageorge, E. C. Peterson, G. Prawiroatmodjo, N. Rubin, C. Ryan, D. Scarabelli, M. Scheer, E. A. Sete, P. Sivarajah, Robert S. Smith, A. Staley, N. Tezak, W. Zeng, A. Hudson, Blake R. Johnson, M. Reagor, M. Silva, C. Rigetti","Unsupervised Machine Learning on a Hybrid Quantum Computer",2017,"","","","",174,"2022-07-13 09:23:05","","","","",,,,,178,35.60,18,30,5,"Machine learning techniques have led to broad adoption of a statistical model of computing. The statistical distributions natively available on quantum processors are a superset of those available classically. Harnessing this attribute has the potential to accelerate or otherwise improve machine learning relative to purely classical performance. A key challenge toward that goal is learning to hybridize classical computing resources and traditional learning techniques with the emerging capabilities of general purpose quantum processors. Here, we demonstrate such hybridization by training a 19-qubit gate model processor to solve a clustering problem, a foundational challenge in unsupervised learning. We use the quantum approximate optimization algorithm in conjunction with a gradient-free Bayesian optimization to train the quantum machine. This quantum/classical hybrid algorithm shows robustness to realistic noise, and we find evidence that classical optimization can be used to train around both coherent and incoherent imperfections.","",""
52,"Umang Bhatt, Yunfeng Zhang, Javier Antorán, Q. Liao, P. Sattigeri, Riccardo Fogliato, Gabrielle Gauthier Melançon, R. Krishnan, Jason Stanley, Omesh Tickoo, L. Nachman, R. Chunara, Adrian Weller, Alice Xiang","Uncertainty as a Form of Transparency: Measuring, Communicating, and Using Uncertainty",2020,"","","","",175,"2022-07-13 09:23:05","","10.1145/3461702.3462571","","",,,,,52,26.00,5,14,2,"Algorithmic transparency entails exposing system properties to various stakeholders for purposes that include understanding, improving, and contesting predictions. Until now, most research into algorithmic transparency has predominantly focused on explainability. Explainability attempts to provide reasons for a machine learning model's behavior to stakeholders. However, understanding a model's specific behavior alone might not be enough for stakeholders to gauge whether the model is wrong or lacks sufficient knowledge to solve the task at hand. In this paper, we argue for considering a complementary form of transparency by estimating and communicating the uncertainty associated with model predictions. First, we discuss methods for assessing uncertainty. Then, we characterize how uncertainty can be used to mitigate model unfairness, augment decision-making, and build trustworthy systems. Finally, we outline methods for displaying uncertainty to stakeholders and recommend how to collect information required for incorporating uncertainty into existing ML pipelines. This work constitutes an interdisciplinary review drawn from literature spanning machine learning, visualization/HCI, design, decision-making, and fairness. We aim to encourage researchers and practitioners to measure, communicate, and use uncertainty as a form of transparency.","",""
10,"J. Watson, C. Holmes","Machine learning analysis plans for randomised controlled trials: detecting treatment effect heterogeneity with strict control of type I error",2020,"","","","",176,"2022-07-13 09:23:05","","10.1186/s13063-020-4076-y","","",,,,,10,5.00,5,2,2,"","",""
297,"Andrius Vabalas, E. Gowen, E. Poliakoff, A. Casson","Machine learning algorithm validation with a limited sample size",2019,"","","","",177,"2022-07-13 09:23:05","","10.1371/journal.pone.0224365","","",,,,,297,99.00,74,4,3,"Advances in neuroimaging, genomic, motion tracking, eye-tracking and many other technology-based data collection methods have led to a torrent of high dimensional datasets, which commonly have a small number of samples because of the intrinsic high cost of data collection involving human participants. High dimensional data with a small number of samples is of critical importance for identifying biomarkers and conducting feasibility and pilot work, however it can lead to biased machine learning (ML) performance estimates. Our review of studies which have applied ML to predict autistic from non-autistic individuals showed that small sample size is associated with higher reported classification accuracy. Thus, we have investigated whether this bias could be caused by the use of validation methods which do not sufficiently control overfitting. Our simulations show that K-fold Cross-Validation (CV) produces strongly biased performance estimates with small sample sizes, and the bias is still evident with sample size of 1000. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. We also show that feature selection if performed on pooled training and testing data is contributing to bias considerably more than parameter tuning. In addition, the contribution to bias by data dimensionality, hyper-parameter space and number of CV folds was explored, and validation methods were compared with discriminable data. The results suggest how to design robust testing methodologies when working with small datasets and how to interpret the results of other studies based on what validation method was used.","",""
7,"Md. Kowsher, A. Tahabilder, S. Murad","Impact-Learning: A Robust Machine Learning Algorithm",2020,"","","","",178,"2022-07-13 09:23:05","","10.1145/3411174.3411185","","",,,,,7,3.50,2,3,2,"The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth.","",""
87,"Kexin Pei, Yinzhi Cao, Junfeng Yang, S. Jana","Towards Practical Verification of Machine Learning: The Case of Computer Vision Systems",2017,"","","","",179,"2022-07-13 09:23:05","","","","",,,,,87,17.40,22,4,5,"Due to the increasing usage of machine learning (ML) techniques in security- and safety-critical domains, such as autonomous systems and medical diagnosis, ensuring correct behavior of ML systems, especially for different corner cases, is of growing importance. In this paper, we propose a generic framework for evaluating security and robustness of ML systems using different real-world safety properties. We further design, implement and evaluate VeriVis, a scalable methodology that can verify a diverse set of safety properties for state-of-the-art computer vision systems with only blackbox access. VeriVis leverage different input space reduction techniques for efficient verification of different safety properties. VeriVis is able to find thousands of safety violations in fifteen state-of-the-art computer vision systems including ten Deep Neural Networks (DNNs) such as Inception-v3 and Nvidia's Dave self-driving system with thousands of neurons as well as five commercial third-party vision APIs including Google vision and Clarifai for twelve different safety properties. Furthermore, VeriVis can successfully verify local safety properties, on average, for around 31.7% of the test images. VeriVis finds up to 64.8x more violations than existing gradient-based methods that, unlike VeriVis, cannot ensure non-existence of any violations. Finally, we show that retraining using the safety violations detected by VeriVis can reduce the average number of violations up to 60.2%.","",""
6,"F. Farokhi","Regularization Helps with Mitigating Poisoning Attacks: Distributionally-Robust Machine Learning Using the Wasserstein Distance",2020,"","","","",180,"2022-07-13 09:23:05","","","","",,,,,6,3.00,6,1,2,"We use distributionally-robust optimization for machine learning to mitigate the effect of data poisoning attacks. We provide performance guarantees for the trained model on the original data (not including the poison records) by training the model for the worst-case distribution on a neighbourhood around the empirical distribution (extracted from the training dataset corrupted by a poisoning attack) defined using the Wasserstein distance. We relax the distributionally-robust machine learning problem by finding an upper bound for the worst-case fitness based on the empirical sampled-averaged fitness and the Lipschitz-constant of the fitness function (on the data for given model parameters) as regularizer. For regression models, we prove that this regularizer is equal to the dual norm of the model parameters. We use the Wine Quality dataset, the Boston Housing Market dataset, and the Adult dataset for demonstrating the results of this paper.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",181,"2022-07-13 09:23:05","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
147,"Drew A. Hudson, Christopher D. Manning","Learning by Abstraction: The Neural State Machine",2019,"","","","",182,"2022-07-13 09:23:05","","","","",,,,,147,49.00,74,2,3,"We introduce the Neural State Machine, seeking to bridge the gap between the neural and symbolic views of AI and integrate their complementary strengths for the task of visual reasoning. Given an image, we first predict a probabilistic graph that represents its underlying semantics and serves as a structured world model. Then, we perform sequential reasoning over the graph, iteratively traversing its nodes to answer a given question or draw a new inference. In contrast to most neural architectures that are designed to closely interact with the raw sensory data, our model operates instead in an abstract latent space, by transforming both the visual and linguistic modalities into semantic concept-based representations, thereby achieving enhanced transparency and modularity. We evaluate our model on VQA-CP and GQA, two recent VQA datasets that involve compositionality, multi-step inference and diverse reasoning skills, achieving state-of-the-art results in both cases. We provide further experiments that illustrate the model's strong generalization capacity across multiple dimensions, including novel compositions of concepts, changes in the answer distribution, and unseen linguistic structures, demonstrating the qualities and efficacy of our approach.","",""
81,"P. Komiske, E. Metodiev, B. Nachman, M. Schwartz","Pileup Mitigation with Machine Learning (PUMML)",2017,"","","","",183,"2022-07-13 09:23:05","","10.1007/JHEP12(2017)051","","",,,,,81,16.20,20,4,5,"","",""
71,"T. Cohen, M. Freytsis, B. Ostdiek","(Machine) learning to do more with less",2017,"","","","",184,"2022-07-13 09:23:05","","10.1007/JHEP02(2018)034","","",,,,,71,14.20,24,3,5,"","",""
19,"Rimpal R. Popat, Jayesh Chaudhary","A Survey on Credit Card Fraud Detection Using Machine Learning",2018,"","","","",185,"2022-07-13 09:23:05","","10.1109/ICOEI.2018.8553963","","",,,,,19,4.75,10,2,4,"Nowadays digitalization gaining popularity because of seamless, easy and convenience use of e-commerce. It became very rampant and easy mode of payment. People choose online payment and e-shopping; because of time convenience, transport convenience, etc. As the result of huge amount of e-commerce use, there is a vast increment in credit card fraud also. Fraudsters try to misuse the card and transparency of online payments. Thus to overcome with the fraudsters activity become very essential. The main aim is to secure credit card transactions; so people can use e-banking safely and easily. To detecting the credit card fraud there are various techniques which are based on Deep learning, Logistic Regression, Naive Bayesian, Support Vector Machine (SVM), Neural Network, Artificial Immune System, K Nearest Neighbor, Data Mining, Decision Tree, Fuzzy logic based System, Genetic Algorithm etc.","",""
208,"J. Blanchet, Yang Kang, M. KarthyekRajhaaA.","Robust Wasserstein profile inference and applications to machine learning",2016,"","","","",186,"2022-07-13 09:23:05","","10.1017/jpr.2019.49","","",,,,,208,34.67,69,3,6,"We show that several machine learning estimators, including square-root least absolute shrinkage and selection and regularized logistic regression, can be represented as solutions to distributionally robust optimization problems. The associated uncertainty regions are based on suitably defined Wasserstein distances. Hence, our representations allow us to view regularization as a result of introducing an artificial adversary that perturbs the empirical distribution to account for out-of-sample effects in loss estimation. In addition, we introduce RWPI (robust Wasserstein profile inference), a novel inference methodology which extends the use of methods inspired by empirical likelihood to the setting of optimal transport costs (of which Wasserstein distances are a particular case). We use RWPI to show how to optimally select the size of uncertainty regions, and as a consequence we are able to choose regularization parameters for these machine learning estimators without the use of cross validation. Numerical experiments are also given to validate our theoretical findings.","",""
439,"Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin","Model-Agnostic Interpretability of Machine Learning",2016,"","","","",187,"2022-07-13 09:23:05","","","","",,,,,439,73.17,146,3,6,"Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.","",""
4721,"Nicholas Carlini, D. Wagner","Towards Evaluating the Robustness of Neural Networks",2016,"","","","",188,"2022-07-13 09:23:05","","10.1109/SP.2017.49","","",,,,,4721,786.83,2361,2,6,"Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classification t, it is possible to find a new input x' that is similar to x but classified as t. This makes it difficult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks' ability to find adversarial examples from 95% to 0.5%.In this paper, we demonstrate that defensive distillation does not significantly increase the robustness of neural networks by introducing three new attack algorithms that are successful on both distilled and undistilled neural networks with 100% probability. Our attacks are tailored to three distance metrics used previously in the literature, and when compared to previous adversarial example generation algorithms, our attacks are often much more effective (and never worse). Furthermore, we propose using high-confidence adversarial examples in a simple transferability test we show can also be used to break defensive distillation. We hope our attacks will be used as a benchmark in future defense attempts to create neural networks that resist adversarial examples.","",""
0,"G. Montavon, W. Samek","Statistics meets Machine Learning 5 Abstracts Explaining the decisions of deep neural networks and beyond",2020,"","","","",189,"2022-07-13 09:23:05","","","","",,,,,0,0.00,0,2,2,"Explaining the decisions of deep neural networks and beyond Grégoire Montavon and Wojciech Samek (joint work with Klaus-Robert Müller, Sebastian Lapuschkin, Alexander Binder, Jacob Kauffmann) Machine learning models have become increasingly complex and this complexity has allowed them to reach high prediction accuracy on challenging datasets. In some cases, improved predictivity has come at the expense of interpretability, in particular, complex models tend to be perceived as black-boxes. A lack of interpretability is problematic, not only because interpretability is desirable in itself (e.g. to extract useful insights from a model or from the modeled data), but also because common measurements of prediction accuracy can become strongly unreliable when certain assumptions about the training data are not met. Real-world datasets are typically not representative of all possible cases and the truly relevant variables may correlate with other irrelevant variables. In such circumstances, one would need to ensure that the machine learning model does not rely on these irrelevant variables. An assessment based purely on test set accuracy would be oblivious to the exact decision strategy and could overestimate the true prediction performance. This phenomenon has been referred to as the ‘Clever Hans’ effect [9]. Only an extension of the dataset with specific test cases, or an inspection of the model, e.g. via interpretability techniques [3, 16, 12], is capable of highlighting the improper decision structure. In this talk, we look at the question of explaining the predictions of deep neural networks, a successful machine learning approach that has been used increasingly in real-world applications. A challenge for getting these explanations is the complexity of the decision function, which makes it hard to apply simple explanation methods developed in the context of linear models, e.g. based on first-order Taylor expansions. In particular, DNN decision functions are highly nonlinear and multiscale, with a gradient that is highly varying or ‘shattered’ [4]. Also, local searches in the input space easily result in ‘adversarial examples’ [13] where the prediction no longer corresponds to the observed pattern in the input. Layer-wise relevance propagation (LRP) [3] is a technique that was proposed to robustly explain the neural network decision in terms of input features. It was shown to work on numerous models in a wide range of applications [14, 5, 15]. LRP departs from the neural network’s function representation to consider instead its graph structure. Specifically, the LRP algorithm performs an iterative redistribution of the neural network output to the lower layers. Redistribution from each layer to the layer below is achieved by means of propagation rules that satisfy a conservation property analogous to Kirchoff’s conservation laws in electrical circuits. The LRP algorithm terminates once the input layer has been reached. The LRP algorithm can be motivated as decomposing a complex problem 6 Oberwolfach Report 4/2020 (analyzing a highly nonlinear function) into a collection of simpler subproblems (treating each neuron individually). Furthermore, it was shown that the LRP algorithm can be interpreted as a collection of Taylor expansions performed at each layer and neuron of the neural network [11]. Specifically, the ‘relevance’ received by a given neuron is approximately the product of the neuron activation and a locally constant term. In turn, the LRP redistribution step can be interpreted as (1) identifying the linear terms of a Taylor expansion of the relevance expressed as a function of activations in the lower layer, and (2) propagating to the lower layer accordingly. A connection can be made between different proposed LRP propagation rules and the choice of reference point at which the Taylor expansion is performed [11, 10]. This Taylor-based view on the LRP algorithm allows in particular to verify that the corresponding reference points are meaningful, for example, that they satisfy domain membership constraints. This interpretation of LRP as a collection of Taylor expansions is referred to as “deep Taylor decomposition” [11]. The LRP algorithm has been successfully applied to various data types and problems, ranging from computer vision and natural language processing tasks such as classification of concepts in images [3], age prediction [8] or categorization of text documents [2], over reinforcement learning tasks such as playing computer games [9], to various medical data analysis tasks, e.g., decoding of fMRI signals [14] or therapy outcome prediction [15]. In these diverse applications, LRP explanations provide additional insights into the decision strategies used by the model, which not only help to better understand the data, including its biases and artifacts [8, 9], but also help to analyze the learning processes and model’s decision strategies [9]. In the second part of the talk, two recent advances that broaden the usefulness of explanation methods are discussed. First, Spectral Relevance Analysis (SpRAy) [9], a dataset-wide analysis of individual explanations that summarizes the overall decision structure of the model into a finite and easily interpretable set of prototypical decision strategies. This analysis allows to systematically investigate complex models on large datasets. It has unveiled in commonly used datasets, artifacts, that tend to systematically induce flaws into the decision structure of ML models trained on them. For example, a website logo was found in some images of the class ‘truck’ of the ImageNet dataset, which the state-of-the-art VGG-16 neural network would then use for its predictions [1]. Another advance brings successful explanation techniques to non-neural network architectures such as kernel-based models. The approach that we term ‘neuralization’ [6] finds for these non-neural network architectures a functionally equivalent neural network so that state-of-the-art explanation techniques such as LRP can be applied. The approach was successfully applied to various unsupervised models, in particular, kernel one-class SVMs [7] and various k-means clustering models [6], thereby shedding light into what input features make a data point anomalous or member of a given cluster. Statistics meets Machine Learning 7 Although significant progress has been made to improve the transparency of ML models such as deep neural networks, numerous challenges still need to be addressed both on the methods and theory side. In particular, there is a need for standardized and unbiased evaluation benchmarks for assessing the quality and usefulness of an explanation. Furthermore, an important future work will be to adopt a more holistic view on the problem of explanation, that considers how to make best use of the user’s interpretation and feedback capabilities, and that also integrates the end goal of the explanation method, for example, achieving better and more informed decisions, or systematically improving and robustifying a machine learning model.","",""
41,"P. Blanchard, El Mahdi El Mhamdi, R. Guerraoui, J. Stainer","Byzantine-Tolerant Machine Learning",2017,"","","","",190,"2022-07-13 09:23:05","","","","",,,,,41,8.20,10,4,5,"The growth of data, the need for scalability and the complexity of models used in modern machine learning calls for distributed implementations. Yet, as of today, distributed machine learning frameworks have largely ignored the possibility of arbitrary (i.e., Byzantine) failures. In this paper, we study the robustness to Byzantine failures at the fundamental level of stochastic gradient descent (SGD), the heart of most machine learning algorithms. Assuming a set of $n$ workers, up to $f$ of them being Byzantine, we ask how robust can SGD be, without limiting the dimension, nor the size of the parameter space.  We first show that no gradient descent update rule based on a linear combination of the vectors proposed by the workers (i.e, current approaches) tolerates a single Byzantine failure. We then formulate a resilience property of the update rule capturing the basic requirements to guarantee convergence despite $f$ Byzantine workers. We finally propose Krum, an update rule that satisfies the resilience property aforementioned. For a $d$-dimensional learning problem, the time complexity of Krum is $O(n^2 \cdot (d + \log n))$.","",""
10,"Andreas Holzinger, Peter Kieseberg, A. Tjoa, E. Weippl","Machine Learning and Knowledge Extraction",2018,"","","","",191,"2022-07-13 09:23:05","","10.1007/978-3-319-99740-7","","",,,,,10,2.50,3,4,4,"","",""
103,"F. Granata, S. Papirio, G. Esposito, R. Gargano, G. D. Marinis","Machine Learning Algorithms for the Forecasting of Wastewater Quality Indicators",2017,"","","","",192,"2022-07-13 09:23:05","","10.3390/W9020105","","",,,,,103,20.60,21,5,5,"Stormwater runoff is often contaminated by human activities. Stormwater discharge into water bodies significantly contributes to environmental pollution. The choice of suitable treatment technologies is dependent on the pollutant concentrations. Wastewater quality indicators such as biochemical oxygen demand (BOD5), chemical oxygen demand (COD), total suspended solids (TSS), and total dissolved solids (TDS) give a measure of the main pollutants. The aim of this study is to provide an indirect methodology for the estimation of the main wastewater quality indicators, based on some characteristics of the drainage basin. The catchment is seen as a black box: the physical processes of accumulation, washing, and transport of pollutants are not mathematically described. Two models deriving from studies on artificial intelligence have been used in this research: Support Vector Regression (SVR) and Regression Trees (RT). Both the models showed robustness, reliability, and high generalization capability. However, with reference to coefficient of determination R2 and root‐mean square error, Support Vector Regression showed a better performance than Regression Tree in predicting TSS, TDS, and COD. As regards BOD5, the two models showed a comparable performance. Therefore, the considered machine learning algorithms may be useful for providing an estimation of the values to be considered for the sizing of the treatment units in absence of direct measures.","",""
15,"Ezekiel O. Soremekun, Sakshi Udeshi, Sudipta Chattopadhyay, A. Zeller","Exposing Backdoors in Robust Machine Learning Models",2020,"","","","",193,"2022-07-13 09:23:05","","","","",,,,,15,7.50,4,4,2,"The introduction of robust optimisation has pushed the state-of-the-art in defending against adversarial attacks. However, the behaviour of such optimisation has not been studied in the light of a fundamentally different class of attacks called backdoors. In this paper, we demonstrate that adversarially robust models are susceptible to backdoor attacks. Subsequently, we observe that backdoors are reflected in the feature representation of such models. Then, this observation is leveraged to detect backdoor-infected models via a detection technique called AEGIS. Specifically, AEGIS uses feature clustering to effectively detect backdoor-infected robust Deep Neural Networks (DNNs).  In our evaluation of major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS effectively detects robust DNNs infected with backdoors. Overall, AEGIS has 97% (70/72) detection accuracy and 0.3% (2/648) false positive rate, for all configurations. Our investigation reveals that salient features of adversarially robust DNNs break the stealthy nature of backdoor attacks.","",""
174,"Andrew F. Zahrt, J. Henle, Brennan T Rose, Yang Wang, William T. Darrow, S. Denmark","Prediction of higher-selectivity catalysts by computer-driven workflow and machine learning",2019,"","","","",194,"2022-07-13 09:23:05","","10.1126/science.aau5631","","",,,,,174,58.00,29,6,3,"Predicting catalyst selectivity Asymmetric catalysis is widely used in chemical research and manufacturing to access just one of two possible mirror-image products. Nonetheless, the process of tuning catalyst structure to optimize selectivity is still largely empirical. Zahrt et al. present a framework for more efficient, predictive optimization. As a proof of principle, they focused on a known coupling reaction of imines and thiols catalyzed by chiral phosphoric acid compounds. By modeling multiple conformations of more than 800 prospective catalysts, and then training machine-learning algorithms on a subset of experimental results, they achieved highly accurate predictions of enantioselectivities. Science, this issue p. eaau5631 A model encompassing multiple conformations of chiral phosphoric acid catalysts accurately predicts enantioselectivities. INTRODUCTION The development of new synthetic methods in organic chemistry is traditionally accomplished through empirical optimization. Catalyst design, wherein experimentalists attempt to qualitatively identify correlations between catalyst structure and catalyst efficiency, is no exception. However, this approach is plagued by numerous deficiencies, including the lack of mechanistic understanding of a new transformation, the inherent limitations of human cognitive abilities to find patterns in large collections of data, and the lack of quantitative guidelines to aid catalyst identification. Chemoinformatics provides an attractive alternative to empiricism for several reasons: Mechanistic information is not a prerequisite, catalyst structures can be characterized by three-dimensional (3D) descriptors (numerical representations of molecular properties derived from the 3D molecular structure) that quantify the steric and electronic properties of thousands of candidate molecules, and the suitability of a given catalyst candidate can be quantified by comparing its properties with a computationally derived model trained on experimental data. The ability to accurately predict a selective catalyst by using a set of less than optimal data remains a major goal for machine learning with respect to asymmetric catalysis. We report a method to achieve this goal and propose a more efficient alternative to traditional catalyst design. RATIONALE The workflow we have created consists of the following components: (i) construction of an in silico library comprising a large collection of conceivable, synthetically accessible catalysts derived from a particular scaffold; (ii) calculation of relevant chemical descriptors for each scaffold; (iii) selection of a representative subset of the catalysts [this subset is termed the universal training set (UTS) because it is agnostic to reaction or mechanism and thus can be used to optimize any reaction catalyzed by that scaffold]; (iv) collection of the training data; and (v) application of machine learning methods to generate models that predict the enantioselectivity of each member of the in silico library. These models are evaluated with an external test set of catalysts (predicting selectivities of catalysts outside of the training data). The validated models can then be used to select the optimal catalyst for a given reaction. RESULTS To demonstrate the viability of our method, we predicted reaction outcomes with substrate combinations and catalysts different from the training data and simulated a situation in which highly selective reactions had not been achieved. In the first demonstration, a model was constructed by using support vector machines and validated with three different external test sets. The first test set evaluated the ability of the model to predict the selectivity of only reactions forming new products with catalysts from the training set. The model performed well, with a mean absolute deviation (MAD) of 0.161 kcal/mol. Next, the same model was used to predict the selectivity of an external test set of catalysts with substrate combinations from the training set. The performance of the model was still highly accurate, with a MAD of 0.211 kcal/mol. Lastly, reactions forming new products with the external test catalysts were predicted with a MAD of 0.236 kcal/mol. In the second study, no reactions with selectivity above 80% enantiomeric excess were used as training data. Deep feed-forward neural networks accurately reproduced the experimental selectivity data, successfully predicting the most selective reactions. More notably, the general trends in selectivity, on the basis of average catalyst selectivity, were correctly identified. Despite omitting about half of the experimental free energy range from the training data, we could still make accurate predictions in this region of selectivity space. CONCLUSION The capability to predict selective catalysts has the potential to change the way chemists select and optimize chiral catalysts from an empirically guided to a mathematically guided approach. Chemoinformatics-guided optimization protocol. (A) Generation of a large in silico library of catalyst candidates. (B) Calculation of robust chemical descriptors. (C) Selection of a UTS. (D) Acquisition of experimental selectivity data. (E) Application of machine learning to use moderate- to low-selectivity reactions to predict high-selectivity reactions. R, any group; X, O or S; Y, OH, SH, or NHTf; PC, principal component; ΔΔG, mean selectivity. Catalyst design in asymmetric reaction development has traditionally been driven by empiricism, wherein experimentalists attempt to qualitatively recognize structural patterns to improve selectivity. Machine learning algorithms and chemoinformatics can potentially accelerate this process by recognizing otherwise inscrutable patterns in large datasets. Herein we report a computationally guided workflow for chiral catalyst selection using chemoinformatics at every stage of development. Robust molecular descriptors that are agnostic to the catalyst scaffold allow for selection of a universal training set on the basis of steric and electronic properties. This set can be used to train machine learning methods to make highly accurate predictive models over a broad range of selectivity space. Using support vector machines and deep feed-forward neural networks, we demonstrate accurate predictive modeling in the chiral phosphoric acid–catalyzed thiol addition to N-acylimines.","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",195,"2022-07-13 09:23:05","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
166,"M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, S. Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Integrating machine learning and multiscale modeling—perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences",2019,"","","","",196,"2022-07-13 09:23:05","","10.1038/s41746-019-0193-y","","",,,,,166,55.33,17,11,3,"","",""
99,"Liwei Song, R. Shokri, Prateek Mittal","Privacy Risks of Securing Machine Learning Models against Adversarial Examples",2019,"","","","",197,"2022-07-13 09:23:05","","10.1145/3319535.3354211","","",,,,,99,33.00,33,3,3,"The arms race between attacks and defenses for machine learning models has come to a forefront in recent years, in both the security community and the privacy community. However, one big limitation of previous research is that the security domain and the privacy domain have typically been considered separately. It is thus unclear whether the defense methods in one domain will have any unexpected impact on the other domain. In this paper, we take a step towards resolving this limitation by combining the two domains. In particular, we measure the success of membership inference attacks against six state-of-the-art defense methods that mitigate the risk of adversarial examples (i.e., evasion attacks). Membership inference attacks determine whether or not an individual data record has been part of a model's training set. The accuracy of such attacks reflects the information leakage of training algorithms about individual members of the training set. Adversarial defense methods against adversarial examples influence the model's decision boundaries such that model predictions remain unchanged for a small area around each input. However, this objective is optimized on training data. Thus, individual data records in the training set have a significant influence on robust models. This makes the models more vulnerable to inference attacks. To perform the membership inference attacks, we leverage the existing inference methods that exploit model predictions. We also propose two new inference methods that exploit structural properties of robust models on adversarially perturbed data. Our experimental evaluation demonstrates that compared with the natural training (undefended) approach, adversarial defense methods can indeed increase the target model's risk against membership inference attacks. When using adversarial defenses to train the robust models, the membership inference advantage increases by up to 4.5 times compared to the naturally undefended models. Beyond revealing the privacy risks of adversarial defenses, we further investigate the factors, such as model capacity, that influence the membership information leakage.","",""
2,"Ziqiang Shi, Chaoliang Zhong, Yasuto Yokota, Wensheng Xia, Jun Sun","Robustness Evaluation of Deep Learning Models Based on Local Prediction Consistency",2019,"","","","",198,"2022-07-13 09:23:05","","10.1109/ICMLA.2019.00224","","",,,,,2,0.67,0,5,3,"It is important to estimate the performance gap of a given deep learning model on the target data set, since discrepancy or bias between source and target domains is a common and fundamental problem in the practice of machine learning techniques. Without any assumptions on data bias, such as label shift or covariate shift and without target data labels, we propose a robustness estimation method based on prediction consistency evaluation between source and target data in the neighborhood of the source samples. Considering outliers and whether the user provided model is fully trained, a variety of variant methods are also tried, including setting neighborhood threshold to average intra-class distance for each category and relative robustness. Furthermore, the time complexity of this method is O(nlogn), which is applicable for large datasets. Experiments on the handwritten digit recognition and Japanese handwriting recognition show that the proposed methods are effective.","",""
87,"J. Collins, K. Howe, B. Nachman","Extending the search for new resonances with machine learning",2019,"","","","",199,"2022-07-13 09:23:05","","10.1103/physrevd.99.014038","","",,,,,87,29.00,29,3,3,"The oldest and most robust technique to search for new particles is to look for ``bumps'' in invariant mass spectra over smoothly falling backgrounds. We present a new extension of the bump hunt that naturally benefits from modern machine learning algorithms while remaining model agnostic. This approach is based on the classification without labels (CWoLa) method where the invariant mass is used to create two potentially mixed samples, one with little or no signal and one with a potential resonance. Additional features that are uncorrelated with the invariant mass can be used for training the classifier. Given the lack of new physics signals at the Large Hadron Collider (LHC), such model-agnostic approaches are critical for ensuring full coverage to fully exploit the rich datasets from the LHC experiments. In addition to illustrating how the new method works in simple test cases, we demonstrate the power of the extended bump hunt on a realistic all-hadronic resonance search in a channel that would not be covered with existing techniques.","",""
107,"Hyunjun Kim, Eunjong Ahn, Myoungsu Shin, S. Sim","Crack and Noncrack Classification from Concrete Surface Images Using Machine Learning",2019,"","","","",200,"2022-07-13 09:23:05","","10.1177/1475921718768747","","",,,,,107,35.67,27,4,3,"In concrete structures, surface cracks are important indicators of structural durability and serviceability. Generally, concrete cracks are visually monitored by inspectors who record crack information such as the existence, location, and width. Manual visual inspection is often considered ineffective in terms of cost, safety, assessment accuracy, and reliability. Digital image processing has been introduced to more accurately obtain crack information from images. A critical challenge is to automatically identify cracks from an image containing actual cracks and crack-like noise patterns (e.g. dark shadows, stains, lumps, and holes), which are often seen in concrete structures. This article presents a methodology for identifying concrete cracks using machine learning. The method helps in determining the existence and location of cracks from surface images. The proposed approach is particularly designed for classifying cracks and noncrack noise patterns that are otherwise difficult to distinguish using existing image processing algorithms. In the training stage of the proposed approach, image binarization is used to extract crack candidate regions; subsequently, classification models are constructed based on speeded-up robust features and convolutional neural network. The obtained crack identification methods are quantitatively and qualitatively compared using new concrete surface images containing cracks and noncracks.","",""
