Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",1,"2022-07-13 09:33:26","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
0,"Canan Tiftik","Investigation of Human Resources Dimension in Management and Organization Structure of the Effects of Artificial Intelligence",2021,"","","","",2,"2022-07-13 09:33:26","","10.21733/IBAD.833256","","",,,,,0,0.00,0,1,1,"In the competitive time, there has been a great deal of progress in the industry. It is one of the most serious obstacles to the industry in many industries that adopt contemporary technologies to manage continuous development and faster than ordinary jobs. Many of the scientists and researchers recommend using AI tools and digital technologies for industries. Machine language and artificial intelligence are used by many organizations in the human resources unit, where it undertakes an integrated task in recruiting, performance analysis, personnel selection, data collection for employees, providing real-time information and obtaining the right information. Artificial intelligence-based Human Resources (HR) applications have a solid potential to increase employee productivity and support HR experts to become knowledge and trained consultants that increase the success of the employee. HR applications authorized by artificial intelligence have the ability to analyze, predict, diagnose and seek and find more robust and capable resources.","",""
2,"Aidan Murphy, Gráinne Murphy, Jorge Amaral, D. M. Dias, Enrique Naredo, C. Ryan","Towards Incorporating Human Knowledge in Fuzzy Pattern Tree Evolution",2021,"","","","",3,"2022-07-13 09:33:26","","10.1007/978-3-030-72812-0_5","","",,,,,2,2.00,0,6,1,"","",""
0,"Chengbing Tan, Qun Chen","Application of an artificial intelligence algorithm model of memory retrieval and roaming in sorting Chinese medicinal materials",2021,"","","","",4,"2022-07-13 09:33:26","","10.3233/jcm-215477","","",,,,,0,0.00,0,2,1,"In order to capture autobiographical memory, inspired by the development of human intelligence, a computational AM model for autobiographical memory is proposed in this paper, which is a three-layer network structure, in which the bottom layer encodes the event-specific knowledge comprising 5W1H, and provides retrieval clues to the middle layer, encodes the related events, and the top layer encodes the event set. According to the bottom-up memory search process, the corresponding events and event sets can be identified in the middle layer and the top layer respectively; At the same time, AM model can simulate human memory roaming through the process of rule-based memory retrieval. The computational AM model proposed in this paper not only has robust and flexible memory retrieval, but also has better response performance to noisy memory retrieval cues than the commonly used memory retrieval model based on keyword query method, and can also imitate the roaming phenomenon in memory.","",""
1,"Alicia Lai","Artificial Intelligence, LLC: Corporate Personhood as Tort Reform",2020,"","","","",5,"2022-07-13 09:33:26","","10.2139/ssrn.3677360","","",,,,,1,0.50,1,1,2,"Our legal system has long tried to fit the square peg of artificial intelligence (AI) technologies into the round hole of the current tort regime, overlooking the inability of traditional liability schemes to address the nuances of how AI technology creates harms. The current tort regime deals out rough justice—using strict liability for some AI products and using the negligence rule for other AI services—both of which are insufficiently tailored to achieve public policy objectives.    Under a strict liability regime where manufacturers are always held liable for the faults of their technology regardless of knowledge or precautionary measures, firms are incentivized to play it safe and stifle innovation. But even with this cautionary stance, the goals of strict liability cannot be met due to the unique nature of AI technology: its mistakes are merely “efficient errors”—they appropriately surpass the human baseline, they are game theory problems intended for a jury, they are necessary to train a robust system, or they are harmless but misclassified.    Under a negligence liability regime where the onus falls entirely on consumers to prove the element of causation, victimized consumers are left without sufficient recourse or compensation. Many critiques have been leveled against the “black-box” nature of algorithms.    This paper proposes a new framework to regulate artificial intelligence technologies: bestowing corporate personhood to AI systems. First, the corporate personality trait of “limited liability” strikes an optimal balance in determining liability—it would both compensate victims (for instance, through obligations to carry insurance and a straightforward burden of causation) while holding manufacturers responsible only when the infraction is egregious (for instance, through veil-piercing). Second, corporate personhood is “divisible”—meaning not all corporate personality traits need to be granted—which circumvents many of the philosophical criticisms of giving AI the complete set of rights of full legal personhood.","",""
0,"Bukhoree Sahoh, Kanjana Haruehansapong, Mallika Kliangkhlao","Causal Artificial Intelligence for High-Stakes Decisions: The Design and Development of a Causal Machine Learning Model",2022,"","","","",6,"2022-07-13 09:33:26","","10.1109/access.2022.3155118","","",,,,,0,0.00,0,3,1,"A high-stakes decision requires deep thought to understand the complex factors that stop a situation from becoming worse. Such decisions are carried out under high pressure, with a lack of information, and in limited time. This research applies Causal Artificial Intelligence to high-stakes decisions, aiming to encode causal assumptions based on human-like intelligence, and thereby produce interpretable and argumentative knowledge. We develop a Causal Bayesian Networks model based on causal science using $d$ -separation and do-operations to discover the causal graph aligned with cognitive understanding. Causal odd ratios are used to measure the causal assumptions integrated with the real-world data to prove the proposed causal model compatibility. Causal effect relationships in the model are verified based on causal P-values and causal confident intervals and approved less than 1% by random chance. It shows that the causal model can encode cognitive understanding as precise, robust relationships. The concept of model design allows software agents to imitate human intelligence by inferring potential knowledge and be employed in high-stakes decision applications.","",""
0,"Chris Yang","Explainable Artificial Intelligence for Predictive Modeling in Healthcare",2022,"","","","",7,"2022-07-13 09:33:26","","10.1007/s41666-022-00114-1","","",,,,,0,0.00,0,1,1,"","",""
7,"David K. Spencer, Stephen Duncan, Adam Taliaferro","Operationalizing artificial intelligence for multi-domain operations: a first look",2019,"","","","",8,"2022-07-13 09:33:26","","10.1117/12.2524227","","",,,,,7,2.33,2,3,3,"Artificial Intelligence / Machine Learning (AI/ML) is a foundational requirement for Multi-Domain Operations (MDO). To solve some of MDO’s most critical problems, for example, penetrating and dis-integrating an adversary’s antiaccess/area denial (A2/AD) systems, the future force requires the ability to converge capabilities from across multiple domains at speeds and scales beyond human cognitive abilities. This requires robust, interoperable AI/ML that operates across multiple layers: from optimizing technologies and platforms, to fusing data from multiple sources, to transferring knowledge across joint functions to accomplish critical MDO tactical tasks. This paper provides an overview of ongoing work from the Unified Quest Future Study Plan and other events with the Army’s Futures and Concepts Center to operationalize AI/ML to address MDO problems with this layered approach. It includes insights and required AI/ML capabilities determined with subject matter experts from various organizations at these learning events over the past two years, as well as vignettes that illustrate how AI/ML can be operationalized to enable successful Multi-Domain Operations against a near peer adversary.","",""
0,"Yaxin Peng, S. Du, T. Zeng","Preface: Special Issue on Optimization Models and Algorithms in Artificial Intelligence",2019,"","","","",9,"2022-07-13 09:33:26","","10.1007/s40305-019-00278-5","","",,,,,0,0.00,0,3,3,"","",""
15,"Yun-he Pan","Special issue on artificial intelligence 2.0",2017,"","","","",10,"2022-07-13 09:33:26","","10.1631/FITEE.1710000","","",,,,,15,3.00,15,1,5,"With the ever-growing popularization of the Internet, universal existence of sensors, emergence of big data, development of e-commerce, rise of the information community, and interconnection and fusion of data and knowledge in human society, physical space, and cyberspace, the information environment surrounding artificial intelligence (AI) development has changed profoundly, leading to a new evolutionary stage: AI 2.0. The emergence of new technologies also promotes AI to a new stage (Pan, 2016). The next-generation AI, namely AI 2.0, is a more explainable, robust, open, and general AI with the following attractive merits: It effectively integrates data-driven machine learning approaches (bottom-up) with knowledge-guided methods (top-down). In addition, it can employ data with different modalities (e.g., visual, auditory, and natural language processing) to perform cross-media learning and inference. Furthermore, there will be a step from the pursuit of an intelligent machine to the hybridaugmented intelligence (i.e., high-level man-machine collaboration and fusion). AI 2.0 will also promote crowd-based intelligence and autonomous-intelligent systems. In the next decades, AI2.0 will probably achieve remarkable progress in aforementioned trends, and therefore significantly change our cities, products, services, economics, environments, even how we advance our society. This special issue aims at reporting recent re-thinking of AI 2.0 from aforementioned aspects as well as practical methodologies, efficient implementations, and applications of AI 2.0. The papers in this special issue can be categorized into two groups. The first group consists of six review papers and the second group five research papers. In the first group, Zhuang et al. (2017) reviewed recent emerging theoretical and technological advances of AI in big data settings. The authors concluded that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI. Li W et al. (2017) described the concepts of crowd intelligence, and explained its relationship to the existing related concepts, e.g., crowdsourcing and human computation. In addition, the authors introduced four categories of representative crowd intelligence platforms. Peng et al. (2017) presented approaches, advances, and future directions in cross-media analysis and reasoning. This paper covers cross-media representation, mining, reasoning, and cross-media knowledge evolution. Tian et al. (2017) reviewed the state-of-the-art research of the perception in terms of visual perception, auditory perception, and speech perception. It also covered perceptual information processing and learning engines. Zhang et al. (2017) introduced the trends in the development of intelligent unmanned autonomous systems. It covered unmanned vehicles, unmanned aerial vehicles, service robots, space robots, marine robots, and unmanned Editorial: Frontiers of Information Technology & Electronic Engineering www.zju.edu.cn/jzus; engineering.cae.cn; www.springerlink.com ISSN 2095-9184 (print); ISSN 2095-9230 (online) E-mail: jzus@zju.edu.cn","",""
1,"M. Moradi, Kathrin Blagec, M. Samwald","Deep learning models are not robust against noise in clinical text",2021,"","","","",11,"2022-07-13 09:33:26","","","","",,,,,1,1.00,0,3,1,"Artificial Intelligence (AI) systems are attracting increasing interest in the medical domain due to their ability to learn complicated tasks that require human intelligence and expert knowledge. AI systems that utilize high-performance Natural Language Processing (NLP) models have achieved state-of-the-art results on a wide variety of clinical text processing benchmarks. They have even outperformed human accuracy on some tasks. However, performance evaluation of such AI systems have been limited to accuracy measures on curated and clean benchmark datasets that may not properly reflect how robustly these systems can operate in real-world situations. In order to address this challenge, we introduce and implement a wide variety of perturbation methods that simulate different types of noise and variability in clinical text data. While noisy samples produced by these perturbation methods can often be understood by humans, they may cause AI systems to make erroneous decisions. Conducting extensive experiments on several clinical text processing tasks, we evaluated the robustness of high-performance NLP models against various types of character-level and word-level noise. The results revealed that the NLP models performance degrades when the input contains small amounts of noise. This study is a significant step towards exposing vulnerabilities of AI models utilized in clinical text processing systems. The proposed perturbation methods can be used in performance evaluation tests to assess how robustly clinical NLP models can operate on noisy data, in real-world settings.","",""
3,"Cheikh Brahim El Vaigh, François Torregrossa, Robin Allesiardo, G. Gravier, P. Sébillot","A correlation-based entity embedding approach for robust entity linking",2020,"","","","",12,"2022-07-13 09:33:26","","10.1109/ICTAI50040.2020.00148","","",,,,,3,1.50,1,5,2,"Entity alignment is a crucial tool in knowledge discovery to reconcile knowledge from different sources. Recent state-of-the-art approaches leverage joint embedding of knowledge graphs (KGs) so that similar entities from different KGs are close in the embedded space. Whatever the joint embedding technique used, a seed set of aligned entities, often provided by (time-consuming) human expertise, is required to learn the joint KG embedding and/or a mapping between KG embeddings. In this context, a key issue is to limit the size and quality requirement for the seed. State-of-the-art methods usually learn the embedding by explicitly minimizing the distance between aligned entities from the seed and uniformly maximizing the distance for entities not in the seed. In contrast, we design a less restrictive optimization criterion that indirectly minimizes the distance between aligned entities in the seed by globally maximizing the dimension-wise correlation among all the embeddings of seed entities. Within an iterative entity alignment system, the correlation-based entity embedding function achieves state-of-the-art results and is shown to significantly increase robustness to the seed's size and accuracy. It ultimately enables fully unsupervised entity alignment using a seed automatically generated with a symbolic alignment method based on entities' names.","",""
26,"Giuseppe Futia, Antonio Vetrò","On the Integration of Knowledge Graphs into Deep Learning Models for a More Comprehensible AI - Three Challenges for Future Research",2020,"","","","",13,"2022-07-13 09:33:26","","10.3390/info11020122","","",,,,,26,13.00,13,2,2,"Deep learning models contributed to reaching unprecedented results in prediction and classification tasks of Artificial Intelligence (AI) systems. However, alongside this notable progress, they do not provide human-understandable insights on how a specific result was achieved. In contexts where the impact of AI on human life is relevant (e.g., recruitment tools, medical diagnoses, etc.), explainability is not only a desirable property, but it is -or, in some cases, it will be soon-a legal requirement. Most of the available approaches to implement eXplainable Artificial Intelligence (XAI) focus on technical solutions usable only by experts able to manipulate the recursive mathematical functions in deep learning algorithms. A complementary approach is represented by symbolic AI, where symbols are elements of a lingua franca between humans and deep learning. In this context, Knowledge Graphs (KGs) and their underlying semantic technologies are the modern implementation of symbolic AI—while being less flexible and robust to noise compared to deep learning models, KGs are natively developed to be explainable. In this paper, we review the main XAI approaches existing in the literature, underlying their strengths and limitations, and we propose neural-symbolic integration as a cornerstone to design an AI which is closer to non-insiders comprehension. Within such a general direction, we identify three specific challenges for future research—knowledge matching, cross-disciplinary explanations and interactive explanations.","",""
5,"Sutta Sornmayura","Robust FOREX Trading with Deep Q Network (DQN)",2019,"","","","",14,"2022-07-13 09:33:26","","","","",,,,,5,1.67,5,1,3,"Financial trading is one of the most attractive areas in finance. Trading systems development is not an easy task because it requires extensive knowledge in several areas such as quantitative analysis, financial skills, and computer programming. A trading systems expert, as a human, also brings in their own bias when developing the system. There should be another, more effective way to develop the system using artificial intelligence. The aim of this study was to compare the performance of AI agents to the performance of the buy-and-hold strategy and the expert trader. The tested market consisted of 15 years of the Forex data market, from two currency pairs (EURUSD, USDJPY) obtained from Dukascopy Bank SA Switzerland. Both hypotheses were tested with a paired t-Test at the 0.05 significance level. The findings showed that AI can beat the buy & hold strategy with significant superiority, in FOREX for both currency pairs (EURUSD, USDJPY), and that AI can also significantly outperform CTA (experienced trader) for trading in EURUSD. However, the AI could not significantly outperform CTA for USDJPY trading. Limitations, contributions, and further research were recommended.","",""
1,"J. Wolff","The SP Theory of Intelligence as a Foundation for the Development of a General, Human-Level Thinking Machine",2016,"","","","",15,"2022-07-13 09:33:26","","","","",,,,,1,0.17,1,1,6,"This paper summarises how the ""SP theory of intelligence"" and its realisation in the ""SP computer model"" simplifies and integrates concepts across artificial intelligence and related areas, and thus provides a promising foundation for the development of a general, human-level thinking machine, in accordance with the main goal of research in artificial general intelligence.  The key to this simplification and integration is the powerful concept of ""multiple alignment"", borrowed and adapted from bioinformatics. This concept has the potential to be the ""double helix"" of intelligence, with as much significance for human-level intelligence as has DNA for biological sciences.  Strengths of the SP system include: versatility in the representation of diverse kinds of knowledge; versatility in aspects of intelligence (including: strengths in unsupervised learning; the processing of natural language; pattern recognition at multiple levels of abstraction that is robust in the face of errors in data; several kinds of reasoning (including: one-step `deductive' reasoning; chains of reasoning; abductive reasoning; reasoning with probabilistic networks and trees; reasoning with 'rules'; nonmonotonic reasoning and reasoning with default values; Bayesian reasoning with 'explaining away'; and more); planning; problem solving; and more); seamless integration of diverse kinds of knowledge and diverse aspects of intelligence in any combination; and potential for application in several areas (including: helping to solve nine problems with big data; helping to develop human-level intelligence in autonomous robots; serving as a database with intelligence and with versatility in the representation and integration of several forms of knowledge; serving as a vehicle for medical knowledge and as an aid to medical diagnosis; and several more).","",""
73,"Yueting Zhuang, Fei Wu, Chun Chen, Yunhe Pan","Challenges and opportunities: from big data to knowledge in AI 2.0",2017,"","","","",16,"2022-07-13 09:33:26","","10.1631/FITEE.1601883","","",,,,,73,14.60,18,4,5,"In this paper, we review recent emerging theoretical and technological advances of artificial intelligence (AI) in the big data settings. We conclude that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI, as follows: from shallow computation to deep neural reasoning; from merely data-driven model to data-driven with structured logic rules models; from task-oriented (domain-specific) intelligence (adherence to explicit instructions) to artificial general intelligence in a general context (the capability to learn from experience). Motivated by such endeavors, the next generation of AI, namely AI 2.0, is positioned to reinvent computing itself, to transform big data into structured knowledge, and to enable better decision-making for our society.","",""
18,"P. Bock","The Emergence of Artificial Intelligence: Learning to Learn",1985,"","","","",17,"2022-07-13 09:33:26","","10.1609/AIMAG.V6I3.498","","",,,,,18,0.49,18,1,37,"The classical approach to the acquisition of knowledge and reason in artificial intelligence is to program the facts and rules into the machine. Unfortunately, the amount of time required to program the equivalent of human intelligence is prohibitively large. An alternative approach allows an automaton to learn to solve problems through iterative trial-and-error interaction with its environment, much as humans do. To solve a problem posed by the environment, the automaton generates a sequence or collection of responses based on its experience. The environment evaluates the effectiveness of this collection, and reports its evaluation to the automaton. The automaton modifies its strategy accordingly, and then generates a new collection of responses. This process is repeated until the automaton converges to the correct collection of responses. The principles underlying this paradigm, known as collective learning systems theory, are explained and applied to a simple game, demonstrating robust learning and dynamic adaptivity.","",""
0,"G. McCalla","Artificial Intelligence and Educational Technology: A Natural Synergy. Extended Abstract.",1994,"","","","",18,"2022-07-13 09:33:26","","","","",,,,,0,0.00,0,1,28,"Educational technology and artificial intelligence (AI) are natural partners in the development of environments to support human learning. Designing systems with the characteristics of a rich learning environment is the long term goal of research in intelligent tutoring systems (ITS). Building these characteristics into a system is extremely difficult: each requires the use of techniques from AI, including knowledge representation, diagnosis and user modeling, planning, machine learning, and natural language understanding. Artificial intelligence techniques are usable now in practical systems. To illustrate this, several working systems that use artificial intelligence and that have been developed in the ARIES Laboratory (University of Saskatchewan, Canada) are discussed. The SCENT advisor can be used to provide robust diagnosis in a wide variety of problem solving domains. The learning by teaching system inverts the usual instructional paradigm: the system acts as an inquisitive learner, thus stimulating the human learner to refine and extend his/her knowledge. G.E.N.I.U.S. takes advantage of the credibility invested in a programming advisor by human learners in order to provide ""ignorance-based"" advice on programming errors. Finally, the VCR Tutor provides help to learners on how to program a video cassette recorder. The general lesson is that AI and educational technology can interact in a natural synergy to the mutual benefit of both. (Contains 11 references.) (MAS) *********************************************************************** Reproductions supplied by EDRS are the best that can be made from the original document. *********************************************************************** Artificial Intelligence and Educational Technology: A Natural Synergy","",""
1,"Y. Xiang, B. Chaib-draa","Advances in artificial intelligence : 16th Conference of the Canadian Society for Computational Studies of Intelligence, AI 2003, Halifax, Canada, June 11-13, 2003 : proceedings",2003,"","","","",19,"2022-07-13 09:33:26","","","","",,,,,1,0.05,1,2,19,"Experiences Building a Distributed Sensor Network.- Artificial Intelligence and Human Brain Imaging.- Machine Learning Methods for Computational Proteomics and Beyond.- The Structure Model Interpretation of Wright's NESS Test.- Answer Formulation for Question-Answering.- Patttern-Based AI Scripting Using ScriptEase.- Enumerating the Preconditions of Agent Message Types.- Monadic Memoization towards Correctness-Preserving Reduction of Search.- Searching Solutions in the Crypto-arithmetic Problems: An Adaptive Parallel Genetic Algorithm Approach.- Stochastic Local Search for Multiprocessor Scheduling for Minimum Total Tardiness.- A Graph Based Backtracking Algorithm for Solving General CSPs.- Iterated Robust Tabu Search for MAX-SAT.- Scaling and Probabilistic Smoothing: Dynamic Local Search for Unweighted MAX-SAT.- A Comparison of Consistency Propagation Algorithms in Constraint Optimization.- Discovering Temporal/Causal Rules: A Comparison of Methods.- Selective Transfer of Task Knowledge Using Stochastic Noise.- Efficient Mining of Indirect Associations Using HI-Mine.- Case Authoring from Text and Historical Experiences.- Session Boundary Detection for Association Rule Learning Using n-Gram Language Models.- Negotiating Exchanges of Private Information for Web Service Eligibility.- Post-supervised Template Induction for Dynamic Web Sources.- Summarizing Web Sites Automatically.- Cycle-Cutset Sampling for Bayesian Networks.- Learning First-Order Bayesian Networks.- AUC: A Better Measure than Accuracy in Comparing Learning Algorithms.- Model-Based Least-Squares Policy Evaluation.- DIAGAL: A Tool for Analyzing and Modelling Commitment-Based Dialogues between Agents.- Situation Event Logic for Early Validation of Multi-Agent Systems.- Understanding ""Not-Understood"": Towards an Ontology of Error Conditions for Agent Communication.- An Improved Ant Colony Optimisation Algorithm for the 2D HP Protein Folding Problem.- Hybrid Randomised Neighbourhoods Improve Stochastic Local Search for DNA Code Design.- A Strategy for Improved Satisfaction of Selling Software Agents in E-Commerce.- Pre-negotiations over Services - A Framework for Evaluation.- Formal Theory for Describing Action Concepts in Terminological Knowledge Bases.- Improving User-Perceived QoS in Mobile Ad Hoc Networks Using Decision Rules Induction.- Risk Neutral Calibration of Classifiers.- Search Bound Strategies for Rule Mining by Iterative Deepening.- Methods for Mining Frequent Sequential Patterns.- Learning by Discovering Conflicts.- Enhancing Caching in Distributed Databases Using Intelligent Polytree Representations.- Feature Selection Strategies for Text Categorization.- Learning General Graphplan Memos through Static Domain Analysis.- Classification Automaton and Its Construction Using Learning.- A Genetic K-means Clustering Algorithm Applied to Gene Expression Data.- Explanation-Oriented Association Mining Using a Combination of Unsupervised and Supervised Learning Algorithms.- Motion Recognition from Video Sequences.- Noun Sense Disambiguation with WordNet for Software Design Retrieval.- Not as Easy as It Seems: Automating the Construction of Lexical Chains Using Roget's Thesaurus.- The Importance of Fine-Grained Cue Phrases in Scientific Citations.- Fuzzy C-Means Clustering of Web Users for Educational Sites.- Re-using Web Information for Building Flexible Domain Knowledge.- A New Inference Axiom for Probabilistic Conditional Independence.- Probabilistic Reasoning for Meal Planning in Intelligent Fridges.- Probabilistic Reasoning in Bayesian Networks: A Relational Database Approach.- Fundamental Issue of Naive Bayes.- The Virtual Driving Instructor Creating Awareness in a Multiagent System.- Multi-attribute Exchange Market: Theory and Experiments.- Agent-Based Online Trading System.- On the Applicability of L-systems and Iterated Function Systems for Grammatical Synthesis of 3D Models.- An Unsupervised Clustering Algorithm for Intrusion Detection.- Dueling CSP Representations: Local Search in the Primal versus Dual Constraint Graph.- A Quick Look at Methods for Mining Long Subsequences.- Back to the Future: Changing the Direction of Time to Discover Causality.- Learning Coordination in RoboCupRescue.- Accent Classification Using Support Vector Machine and Hidden Markov Model.- A Neural Network Based Approach to the Artificial Aging of Facial Images.- Adaptive Negotiation for Agent Based Distributed Manufacturing Scheduling.- Multi-agent System Architecture for Tracking Moving Objects.","",""
2,"Moonis Ali, B. Whitehead, U. Gupta, H. Ferber","Identification and interpretation of patterns in rocket engine data: Artificial intelligence and neural network approaches",1995,"","","","",20,"2022-07-13 09:33:26","","","","",,,,,2,0.07,1,4,27,"This paper describes an expert system which is designed to perform automatic data analysis, identify anomalous events, and determine the characteristic features of these events. We have employed both artificial intelligence and neural net approaches in the design of this expert system. The artificial intelligence approach is useful because it provides (1) the use of human experts' knowledge of sensor behavior and faulty engine conditions in interpreting data; (2) the use of engine design knowledge and physical sensor locations in establishing relationships among the events of multiple sensors; (3) the use of stored analysis of past data of faulty engine conditions; and (4) the use of knowledge-based reasoning in distinguishing sensor failure from actual faults. The neural network approach appears promising because neural nets (1) can be trained on extremely noisy data and produce classifications which are more robust under noisy conditions than other classification techniques; (2) avoid the necessity of noise removal by digital filtering and therefore avoid the need to make assumptions about frequency bands or other signal characteristics of anomalous behavior; (3) can, in effect, generate their own feature detectors based on the characteristics of the sensor data used in training; and (4) are inherently parallel and therefore are potentially implementable in special-purpose parallel hardware.","",""
0,"Qinyun Liu","Solution Generation through Hybrid Intelligence and Creativity based on Investment Portfolio",2018,"","","","",21,"2022-07-13 09:33:26","","10.23940/IJPE.18.07.P29.16411650","","",,,,,0,0.00,0,1,4,"Artificial Intelligence (AI) has been developed to be robust on computing. Learning can be achieved by connecting to heterogeneous data using AI algorithms, such as the Artificial Neural Network. Knowledge can be learned, and rules in the database can be discovered by machines through heuristic algorithms. However, creativity has not been achieved by computers like the human brain by using AI algorithms individually. This research serves to explore a method to achieve creative solution generation by utilizing a relationship between intelligence and creativity, assuming intelligence is the subset of creativity. Under this relationship, the computing can be fulfilled using AI algorithms. The theories of achieving creativity is the guidance of this method.","",""
2,"Mathieu Lelerre, A. Mouaddib, L. Jeanpierre","Robust Inverse Planning Approaches for Policy Estimation of Semi-autonomous Agents",2017,"","","","",22,"2022-07-13 09:33:26","","10.1109/ICTAI.2017.00146","","",,,,,2,0.40,1,3,5,"Most of existing coordination techniques for autonomous agents assume the knowledge or the estimation of the other agents’ policy. However, this assumption is not valid in semi-autonomous agents because an external entity can take the control and modify the behavior of the agent. We face this problem in applications where an operator can take the control of the system (Robot/UAV). Many human factors may affect this behavior, such as stress, hesitations and preferences. Estimating the policy in such contexts is a difficult problem. Many existing algorithms using Inverse Reinforcement learning or imitation have been developed. However most of them have weak performance when non-optimal policy is followed. In this paper, we investigate techniques for estimating the followed policies of semi-autonomous agents that could be nonoptimal due to critical situations We extend some prediction methods and algorithms based on Factored MDPs and Inverse Reinforcement Learning to improve their stability and their efficiency during the execution of a mission. Then, we develop various experiments showing the performance on efficiency and stability of our approach in different conditions and comparing with it.","",""
4,"Fatmah Baothman","An Intelligent Big Data Management System Using Haar Algorithm-Based Nao Agent Multisensory Communication",2021,"","","","",23,"2022-07-13 09:33:26","","10.1155/2021/9977751","","",,,,,4,4.00,4,1,1,"Artificial intelligence (AI) is progressively changing techniques of teaching and learning. In the past, the objective was to provide an intelligent tutoring system without intervention from a human teacher to enhance skills, control, knowledge construction, and intellectual engagement. This paper proposes a definition of AI focusing on enhancing the humanoid agent Nao’s learning capabilities and interactions. The aim is to increase Nao intelligence using big data by activating multisensory perceptions such as visual and auditory stimuli modules and speech-related stimuli, as well as being in various movements. The method is to develop a toolkit by enabling Arabic speech recognition and implementing the Haar algorithm for robust image recognition to improve the capabilities of Nao during interactions with a child in a mixed reality system using big data. The experiment design and testing processes were conducted by implementing an AI principle design, namely, the three-constituent principle. Four experiments were conducted to boost Nao’s intelligence level using 100 children, different environments (class, lab, home, and mixed reality Leap Motion Controller (LMC). An objective function and an operational time cost function are developed to improve Nao’s learning experience in different environments accomplishing the best results in 4.2 seconds for each number recognition. The experiments’ results showed an increase in Nao’s intelligence from 3 to 7 years old compared with a child’s intelligence in learning simple mathematics with the best communication using a kappa ratio value of 90.8%, having a corpus that exceeded 390,000 segments, and scoring 93% of success rate when activating both auditory and vision modules for the agent Nao. The developed toolkit uses Arabic speech recognition and the Haar algorithm in a mixed reality system using big data enabling Nao to achieve a 94% success learning rate at a distance of 0.09 m; when using LMC in mixed reality, the hand sign gestures recorded the highest accuracy of 98.50% using Haar algorithm. The work shows that the current work enabled Nao to gradually achieve a higher learning success rate as the environment changes and multisensory perception increases. This paper also proposes a cutting-edge research work direction for fostering child-robots education in real time.","",""
3,"Sijie Yang, Fei Zhu, Xinghong Ling, QUAN LIU, Peiyao Zhao","Intelligent Health Care: Applications of Deep Learning in Computational Medicine",2021,"","","","",24,"2022-07-13 09:33:26","","10.3389/fgene.2021.607471","","",,,,,3,3.00,1,5,1,"With the progress of medical technology, biomedical field ushered in the era of big data, based on which and driven by artificial intelligence technology, computational medicine has emerged. People need to extract the effective information contained in these big biomedical data to promote the development of precision medicine. Traditionally, the machine learning methods are used to dig out biomedical data to find the features from data, which generally rely on feature engineering and domain knowledge of experts, requiring tremendous time and human resources. Different from traditional approaches, deep learning, as a cutting-edge machine learning branch, can automatically learn complex and robust feature from raw data without the need for feature engineering. The applications of deep learning in medical image, electronic health record, genomics, and drug development are studied, where the suggestion is that deep learning has obvious advantage in making full use of biomedical data and improving medical health level. Deep learning plays an increasingly important role in the field of medical health and has a broad prospect of application. However, the problems and challenges of deep learning in computational medical health still exist, including insufficient data, interpretability, data privacy, and heterogeneity. Analysis and discussion on these problems provide a reference to improve the application of deep learning in medical health.","",""
0,"F. Ventura, Salvatore Greco, D. Apiletti, T. Cerquitelli","Trusting deep learning natural-language models via local and global explanations",2022,"","","","",25,"2022-07-13 09:33:26","","10.1007/s10115-022-01690-9","","",,,,,0,0.00,0,4,1,"","",""
0,"V. Noel Jeygar Robert, K. Vidya","Effective cooperative spectrum sensing using deep recurrent reinforced learning‐based Q‐routing in multihop cognitive radio networks",2021,"","","","",26,"2022-07-13 09:33:26","","10.1002/dac.4982","","",,,,,0,0.00,0,2,1,"Cognitive radio network (CRN) is a promising technology that mitigates the scarcity of spectrum. The main challenge faced in the opportunistic CRN is the spectrum sensing (SS). The traditional methods of SS use detectors to find the available bands, which appear to provide unreliable performance in the case of actual environment where noise is predominant. Literature has proved that employing an artificial intelligence (AI) model for routing overcomes the lack of network knowledge and lack of human intervention and helps to learn robust patterns. Reinforced learning (RL) is a dynamically learning process that selects the actions based on continuous feedback received from the dynamic environment to maximize the reward. Deep reinforcement learning (DRL) models have proven to be successful at learning control policies image inputs. However, they struggle with learning policies that require longer term information. Recurrent neural network (RNN) architectures have been used in tasks dealing with longer term dependencies between data points. Motivated by the performance of AI models, these architectures are investigated in this work to overcome the difficulties arising from learning policies with long‐term dependencies. Thus, a deep recurrent reinforced learning‐based Q‐routing (DRRL‐based Q‐routing) is developed. The proposed study suggests a multihop CRN operated in an interweave mode. This algorithm finds optimal routing path between the secondary user transmitter (SUT) and the secondary user destination (SUD), optimal SS duration, and individual secondary user (SU) power requirements for SS and data transmission process while minimizing the end‐to‐end outage under the constraints of energy causality, SS reliability, interference threshold, and individual link throughput.","",""
21,"Phillip Odom, Tushar Khot, R. Porter, Sriraam Natarajan","Knowledge-Based Probabilistic Logic Learning",2015,"","","","",27,"2022-07-13 09:33:26","","10.1609/aaai.v29i1.9690","","",,,,,21,3.00,5,4,7,"    Advice giving has been long explored in artificial intelligence to build robust learning algorithms. We consider advice giving in relational domains where the noise is systematic. The advice is provided as logical statements that are then explicitly considered by the learning algorithm at every update. Our empirical evidence proves that human advice can effectively accelerate learning in noisy structured domains where so far humans have been merely used as labelers or as designers of initial structure of the model.   ","",""
24,"D. Miller","The medical AI insurgency: what physicians must know about data to practice with intelligent machines",2019,"","","","",28,"2022-07-13 09:33:26","","10.1038/s41746-019-0138-5","","",,,,,24,8.00,24,1,3,"","",""
1,"S. Tran","Propositional Knowledge Representation and Reasoning in Restricted Boltzmann Machines.",2017,"","","","",29,"2022-07-13 09:33:26","","","","",,,,,1,0.20,1,1,5,"While knowledge representation and reasoning are considered the keys for human-level artificial intelligence, connectionist networks have been shown successful in a broad range of applications due to their capacity for robust learning and flexible inference under uncertainty. The idea of representing symbolic knowledge in connectionist networks has been well-received and attracted much attention from research community as this can establish a foundation for integration of scalable learning and sound reasoning. In previous work, there exist a number of approaches that map logical inference rules with feed-forward propagation of artificial neural networks (ANN). However, the discriminative structure of an ANN requires the separation of input/output variables which makes it difficult for general reasoning where any variables should be inferable. Other approaches address this issue by employing generative models such as symmetric connectionist networks, however, they are difficult and convoluted. In this paper we propose a novel method to represent propositional formulas in restricted Boltzmann machines which is less complex, especially in the cases of logical implications and Horn clauses. An integration system is then developed and evaluated in real datasets which shows promising results.","",""
14,"Ndapandula Nakashole","Automatic extraction of facts, relations, and entities for web-scale knowledge base population",2012,"","","","",30,"2022-07-13 09:33:26","","10.22028/D291-26412","","",,,,,14,1.40,14,1,10,"Equipping machines with knowledge, through the construction of machinereadable knowledge bases, presents a key asset for semantic search, machine translation, question answering, and other formidable challenges in artificial intelligence. However, human knowledge predominantly resides in books and other natural language text forms. This means that knowledge bases must be extracted and synthesized from natural language text. When the source of text is the Web, extraction methods must cope with ambiguity, noise, scale, and updates. The goal of this dissertation is to develop knowledge base population methods that address the afore mentioned characteristics of Web text. The dissertation makes three contributions. The first contribution is a method for mining high-quality facts at scale, through distributed constraint reasoning and a pattern representation model that is robust against noisy patterns. The second contribution is a method for mining a large comprehensive collection of relation types beyond those commonly found in existing knowledge bases. The third contribution is a method for extracting facts from dynamic Web sources such as news articles and social media where one of the key challenges is the constant emergence of new entities. All methods have been evaluated through experiments involving Web-scale text collections.","",""
2,"C. Lim, C. Abeynayake, M. Sato-Ilic, L. Jain","Special issue: Computational intelligence models for image processing and information reasoning",2013,"","","","",31,"2022-07-13 09:33:26","","10.3233/IFS-2012-0546","","",,,,,2,0.22,1,4,9,"Computational Intelligence CI models comprise robust computing methodologies with a high level of machine learning quotient. CI models, in general, are useful for designing computerized intelligent systems/machines that possess useful characteristics mimicking human behaviors and capabilities in solving complex tasks, e.g., learning, adaptation, and evolution. Examples of some popular CI models include fuzzy systems, artificial neural networks, evolutionary algorithms, multi-agent systems, decision trees, rough set theory, knowledge-based systems, and hybrid of these models. This special issue highlights how different computational intelligence models, coupled with other complementary techniques, can be used to handle problems encountered in image processing and information reasoning.","",""
51,"Jun He, Shixi Yang, C. Gan","Unsupervised Fault Diagnosis of a Gear Transmission Chain Using a Deep Belief Network",2017,"","","","",32,"2022-07-13 09:33:26","","10.3390/s17071564","","",,,,,51,10.20,17,3,5,"Artificial intelligence (AI) techniques, which can effectively analyze massive amounts of fault data and automatically provide accurate diagnosis results, have been widely applied to fault diagnosis of rotating machinery. Conventional AI methods are applied using features selected by a human operator, which are manually extracted based on diagnostic techniques and field expertise. However, developing robust features for each diagnostic purpose is often labour-intensive and time-consuming, and the features extracted for one specific task may be unsuitable for others. In this paper, a novel AI method based on a deep belief network (DBN) is proposed for the unsupervised fault diagnosis of a gear transmission chain, and the genetic algorithm is used to optimize the structural parameters of the network. Compared to the conventional AI methods, the proposed method can adaptively exploit robust features related to the faults by unsupervised feature learning, thus requires less prior knowledge about signal processing techniques and diagnostic expertise. Besides, it is more powerful at modelling complex structured data. The effectiveness of the proposed method is validated using datasets from rolling bearings and gearbox. To show the superiority of the proposed method, its performance is compared with two well-known classifiers, i.e., back propagation neural network (BPNN) and support vector machine (SVM). The fault classification accuracies are 99.26% for rolling bearings and 100% for gearbox when using the proposed method, which are much higher than that of the other two methods.","",""
0,"NeuroData, C. Priebe, R. Burns, R. J. Vogelstein","Measuring and Reconstructing the Brain at the Synaptic Scale: Towards a Bioﬁdelic Human Brain in silico",2015,"","","","",33,"2022-07-13 09:33:26","","10.6084/M9.FIGSHARE.1285813.V1","","",,,,,0,0.00,0,4,7,"The ability to construct a biofidelic human brain in silico has potentially transformative implications for artificial intelligence, medical diagnostics and therapeutics, and our basic understanding of the brain and the mind. Previous large-scale brain simulations were built from well studied parts, but lacked detailed knowledge of connectivity [1]. We are developing a complete pipeline to construct the first biofidelic human brain emulation. These tools are all designed to be high-throughput, mostly automated, and robust. 11 Step 1: The Automatic Tape Collecting UltraMicrotome (ATUM) cuts large areas of brain tissue (3x3 mm2) generating thousands of 25 nm thick serial sections and cubic mms of volume with no loss [2]. The sections are collected on a firm plastic tape which is then cut into strips and placed on silicon wafers. Step 2: The wafers are imaged automatically with a scanning electron microscope so that thousands of two dimensional images are generated with lateral resolutions of 3 nm [3]. New advances in imaging technology will accelerate this process from 1M pixels per second to speeds of 1-10G pixels per second over the next 5 years. At these speeds whole mammalian brains can be imaged in a few years. Depending on the resolution with which one wants to image the white matter tracks, it is for the first time possible to consider imaging entire human brains at a resolution where all the synapses are visible. Step 3: Threedimensional image processing tools generate a “clean” volumetric image from the collection of twodimensional images. The data is stored to facilitate efficient machine annotation, and simultaneous access by thousands of users.","",""
2,"Fabio Catania, Pietro Crovari, Micol Spitale, F. Garzotto","Automatic Speech Recognition: Do Emotions Matter?",2019,"","","","",34,"2022-07-13 09:33:26","","10.1109/CDKE46621.2019.00009","","",,,,,2,0.67,1,4,3,"By leveraging the advancements in Natural Language Processing and Cognitive Computing, conversational artificial intelligence (AI) has become more mature over the last years. It serves humans in a broad range of applications in business enterprises, government, health-care, and entertaining, and it is getting more embedded into peoples' lives. However, despite the recent improvements, we are still far away from a robust or general AI comparable to human intelligence, especially when it comes to adaptive intelligence able to settle into non-standard and noisy environments. In this paper, we bring to light that emotion in speech negatively affects automatic speech recognition and automatic human inputs understanding. Therefore, emotion is to be considered as a noise compromising the understanding of what the user says and consequently messing up the whole interaction with conversational technologies. For this study, Google Cloud Speech-to-Text and IBM Watson Speech-to-Text have been used.","",""
3,"Neda Nasiriani, A. Squicciarini, Zara Saldanha, Sanchit Goel, Nicola Zannone","Hierarchical Clustering for Discrimination Discovery: A Top-Down Approach",2019,"","","","",35,"2022-07-13 09:33:26","","10.1109/AIKE.2019.00041","","",,,,,3,1.00,1,5,3,"Today, data is an essential part of many decision-making processes in businesses and social life through the use of various machine learning techniques. These methods can easily perpetuate human bias in the data and result in discrimination. Despite a growing interest in data discrimination discovery and removal, to date there is a lack of a general and robust framework to distinguish discriminatory decision-making processes from non-discriminatory ones. In this work, we present a generic framework that helps detect possible discrimination by analyzing historical data and associated decisions using a top-down unsupervised approach, which we refer to as hierarchical clustering. Our approach is highly adaptive as it gradually ""learns"" users' inherent groups, and clusters their records using cohesiveness and density of points in the dataset. Moreover, we propose a progressive attribute-selection method to choose statistically relevant attributes, thus reducing the effect of noise. Finally, we adopt a recursive notion of cluster profile that is homogeneous w.r.t. decision labels. This allows for deeper insights on the data and on the decision-making underlying the final user classification. Our framework is able to identify both positive and negative bias resulting in discrimination. We also highlight patterns of discrimination revealed by the homogeneous cluster centroids, which otherwise could not be captured.","",""
144,"A. Garcez, L. Lamb, D. Gabbay","Neural-Symbolic Cognitive Reasoning",2008,"","","","",36,"2022-07-13 09:33:26","","10.1007/978-3-540-73246-4","","",,,,,144,10.29,48,3,14,"","",""
29,"A. Gordon","Commonsense Interpretation of Triangle Behavior",2016,"","","","",37,"2022-07-13 09:33:26","","10.1609/aaai.v30i1.9881","","",,,,,29,4.83,29,1,6,"    The ability to infer intentions, emotions, and other unobservable psychological states from people's behavior is a hallmark of human social cognition, and an essential capability for future Artificial Intelligence systems. The commonsense theories of psychology and sociology necessary for such inferences have been a focus of logic-based knowledge representation research, but have been difficult to employ in robust automated reasoning architectures. In this paper we model behavior interpretation as a process of logical abduction, where the reasoning task is to identify the most probable set of assumptions that logically entail the observable behavior of others, given commonsense theories of psychology and sociology. We evaluate our approach using Triangle-COPA, a benchmark suite of 100 challenge problems based on an early social psychology experiment by Fritz Heider and Marianne Simmel. Commonsense knowledge of actions, social relationships, intentions, and emotions are encoded as defeasible axioms in first-order logic. We identify sets of assumptions that logically entail observed behaviors by backchaining with these axioms to a given depth, and order these sets by their joint probability assuming conditional independence. Our approach solves almost all (91) of the 100 questions in Triangle-COPA, and demonstrates a promising approach to robust behavior interpretation that integrates both logical and probabilistic reasoning.   ","",""
5,"Justin W. Hart, B. Scassellati","Robotic Self-Models Inspired by Human Development",2010,"","","","",38,"2022-07-13 09:33:26","","","","",,,,,5,0.42,3,2,12,"Traditionally, in the fields of artificial intelligence and robotics, representations of the self have been conspicuously absent. Capabilities of systems are listed explicitly by developers during construction and choices between behavioral options are decided based on search, inference, and planning. In robotics, while knowledge of the external world has often been acquired through experience, knowledge about the robot itself has generally been built in by the designer. Built-in models of the robot's kinematics, physical and sensory capabilities, and other equipment have stood in the place of self-knowledge, but none of these representations offer the flexibility, robustness, and functionality that are present in people. In this work, we seek to emulate forms of self-awareness developed during human infancy in our humanoid robot, Nico. In particular, we are interested in the ability to reason about the robot's embodiment and physical capabilities, with the robot building a model of itself through its experiences.","",""
1,"F. Kirchner, Sebastian Bartsch, José DeGea","Experiments on Embodied Cognition: A Bio-Inspired Approach for Robust Biped Locomotion",2007,"","","","",39,"2022-07-13 09:33:26","","10.5772/4883","","",,,,,1,0.07,0,3,15,"Recently, the psychological point of view that grants the body a more significant role in cognition has also gained attention in artificial intelligence. Proponents of this approach would claim that instead of a ‘mind that works on abstract problems’ we have to deal with and understand ‘a body that needs a mind to make it function’ (Wilson, 2002). These ideas differ quite radically from the traditional approach that describes a cognitive process as an abstract information processing task where the real physical connections to the outside world are of only sub-critical importance, sometimes discarded as mere ‘informational encapsulated plug-ins’ (Fodor, 1983). Thus most theories in cognitive psychology have tried to describe the process of human thinking in terms of propositional knowledge. At the same time, artificial intelligence research has been dominated by methods of abstract symbolic processing, even if researchers often used robotic systems to implement them (Nilsson, 1984). Ignoring sensor-motor influences on cognitive ability is in sharp contrast to research by William James (James, 1890) and others (see (Prinz, 1987) for a review) that describe theories of cognition based on motor acts, or a theory of cognitive function emerging from seminal research on sensor-motor abilities by Jean Piaget (Wilson, 2002) and the theory of affordances by (Gibson, 1977). In the 1980s the linguist Lakoff and the philosopher Johnson (Lakoff & Johnson, 1980) put forward the idea of abstract concepts based on metaphors for bodily, physical concepts; around the same time, Brooks (Brooks, 1986) made a major impact on artificial intelligence research by his concepts of behavior based robotics and interaction with the environment without internal representation instead of the sensereason-act cycle. This approach has gained wide attention ever since and there appears to be a growing sense of commitment to the idea that cognitive ability in a system (natural or artificial) has to be studied in the context of its relation to a ‘kinematically competent’ physical body. Among the most competent (in a multi functional sense) physical bodies around are certainly humans, so the study of humanoid robots appears to be a promising field for","",""
62,"C. Lo, Y. Wong, A. Rad","Intelligent system for process supervision and fault diagnosis in dynamic physical systems",2006,"","","","",40,"2022-07-13 09:33:26","","10.1109/TIE.2006.870707","","",,,,,62,3.88,21,3,16,"In recent years, the increasing complexity of process plants and other engineered systems has extended the scope of interest in control engineering, which was previously focused on the development of controllers for specified performance criteria such as stability and precision. Modern industrial systems require a higher demand of system reliability, safety, and low-cost operation, which in turn call for sophisticated and elegant fault-detection and isolation algorithms. This paper develops an intelligent supervisory coordinator (ISC) for process supervision and fault diagnosis in dynamic physical systems. A qualitative bond graph modeling scheme, integrating artificial-intelligence techniques with control engineering, is used to construct the knowledge base of the ISC. A supervisor provided by the ISC utilizes the knowledge in the knowledge base to classify various system behaviors, coordinates different control tasks (e.g., fault diagnosis), and communicates system states to human operators. The ISC provides a robust semiautonomous system to assist human operators in managing dynamic physical systems. The proposed ISC has been successfully applied to supervise a laboratory-scale servo-tank liquid process rig.","",""
1,"L. E. Souza","Flexible Planning Knowledge Acquisition for Industrial Processes",2008,"","","","",41,"2022-07-13 09:33:26","","","","",,,,,1,0.07,1,1,14,"The acquisition of planning knowledge for industrial processes has been shown to be particularly hard in terms of acquiring robust task knowledge from human experts. Furthermore, this knowledge, when acquired, is rather domain dependent and the acquisition effort is very seldom amortized over other applications. In this paper, we report on our work on flexibly representing a functional model of a chemical process in an Artificial Intelligence (AI) planning system. We show the generality and flexibility of our new planning operator representation. Operators are designed for general functions which can be defined for hierarchical levels of functional detail. We present how the planning domain can be used with little or no modification, depending on the level of abstraction, to other engineering systems. This planning domain is fully implemented in a domain-independent AI planning system.","",""
1,"Nicholas Fung","Toward Real Time Autonomous Robotics Through Integration of Hierarchical Goal Network Planning and Low Level Control Libraries",2017,"","","","",42,"2022-07-13 09:33:26","","","","",,,,,1,0.20,1,1,5,"Automated planning has become an increasingly influential area of research in the realm of artificial intelligence. Task based planning algorithms provide a number of advantages including the ease of human readability when creating mission length plans. However, task based planning algorithms are rarely implement on real world robotic systems because they require additional domain specific knowledge to define tasks and are generally not as flexible as other planning techniques. This paper documents work to integrate a hierarchical goal network planning algorithm with low level path planning. The system utilizes the Goal Decomposition with Landmarks (GoDeL) planner for plan generation at an abstract level and the Searched Based Planning Library (SBPL) for low level control. The system is used to direct a robot through an office setting within a simulation environment. We then discuss incorporating an ”in the now” approach to the GoDeL algorithm to make the system more robust to a dynamic environment. The resulting algorithm is more suited for use in real time applications such as autonomous robotics.","",""
0,"Wenqin Wu","Simple Dynamic Coattention Networks",2017,"","","","",43,"2022-07-13 09:33:26","","","","",,,,,0,0.00,0,1,5,"Reading comprehension (RC), or the capability to process document texts and answer questions about them is a difficult task for machines, as human language understanding and real-world knowledge are needed [4]. This can serve a wide range of applications, from simplifying information retrieval processes to building more robust artificial intelligence. Previously, most natural language processing was done with classical probabilistic models. With the recent progress in deep learning, more researchers are switching to using neural networks as they are proven to produce better results.","",""
18,"M. Klenk, Kenneth D. Forbus, E. Tomai, Hyeonkyeong Kim","Using analogical model formulation with sketches to solve Bennett Mechanical Comprehension Test problems",2011,"","","","",44,"2022-07-13 09:33:26","","10.1080/0952813X.2010.502312","","",,,,,18,1.64,5,4,11,"One of the central problems of artificial intelligence is capturing the breadth and flexibility of human common sense reasoning. One way to evaluate common sense is to use versions of human tests that rely on everyday reasoning. The Bennett Mechanical Comprehension Test consists of everyday reasoning problems posed via pictures and is used to evaluate technicians. This test is challenging because it requires conceptual knowledge spanning a broad range of domains, experience with a wide variety of everyday situations, and spatial reasoning. This article describes how we have extended our Companion Cognitive Architecture, which treats analogical processing as central, to perform well over a subset of the Bennett test. We introduce analogical model formulation as a robust method for reasoning about everyday scenarios, by analogy with cases that represent prior experiences. This enables a companion to perform qualitative reasoning (QR) without a complete domain theory, as typically required for QR. We introduce sketch annotations to communicate linkages between visual and conceptual properties in sketches. We introduce analogical reference frames to enable comparative analysis to operate over a broader range of problems than prior techniques. We show that these techniques enable a companion to score reasonably well on a difficult subset of the Bennett test.","",""
5,"Quanshi Zhang, Xuan Song, Xiaowei Shao, Huijing Zhao, R. Shibasaki","From RGB-D Images to RGB Images",2015,"","","","",45,"2022-07-13 09:33:26","","10.1145/2629701","","",,,,,5,0.71,1,5,7,"Mining object-level knowledge, that is, building a comprehensive category model base, from a large set of cluttered scenes presents a considerable challenge to the field of artificial intelligence. How to initiate model learning with the least human supervision (i.e., manual labeling) and how to encode the structural knowledge are two elements of this challenge, as they largely determine the scalability and applicability of any solution. In this article, we propose a model-learning method that starts from a single-labeled object for each category, and mines further model knowledge from a number of informally captured, cluttered scenes. However, in these scenes, target objects are relatively small and have large variations in texture, scale, and rotation. Thus, to reduce the model bias normally associated with less supervised learning methods, we use the robust 3D shape in RGB-D images to guide our model learning, then apply the properly trained category models to both object detection and recognition in more conventional RGB images. In addition to model training for their own categories, the knowledge extracted from the RGB-D images can also be transferred to guide model learning for a new category, in which only RGB images without depth information in the new category are provided for training. Preliminary testing shows that the proposed method performs as well as fully supervised learning methods.","",""
0,"Dimitris Spathis, A. Tefas","Learning to interact with high-dimensional data",2017,"","","","",46,"2022-07-13 09:33:26","","","","",,,,,0,0.00,0,2,5,"Artificial intelligence techniques and humans have skills that complement each other; the first being good in computation at lowest level (e.g. matrix multiplications) whereas people are better at abstracting and transferring knowledge from their experience. This thesis examines how we can combine the two aforementioned strengths in order to create robust, efficient, visual and interpretable machine learning. In particular, we research on dimensionality reduction techniques, which provide ways of projecting high-dimensional data in 2D. While dimensionality reduction is used for many reasons such as to reduce storage space and processing time, we focus on its usage for visualization. By visualizing the features in a two dimensional (2D) or three dimensional (3D) space, we make it easier for the human perception to understand the structure of data in a manner that feels natural. In layman terms, we examine what happens when a user interacts (moves, drags etc.) with some data points in 2D, which correspond to high-dimensional data. Most modern dimensionality reduction techniques are based in distance metrics, which are prone to outliers and crowding issues. By extending a recently proposed generic framework called Similarity Embedding Framework (SEF) which minimizes the objective function of the difference between the projection and a target, we define that target similarity matrix as the outcome of user interaction. Then, the framework learns iteratively the optimal projection with gradient descent. In essence, we just optimize two similarity matrices. Fast linear and kernel versions are proposed. The experimental procedure covers two interaction scenarios. The first one is a quite common in multi-dimensional projection literature, so that users are provided with a subset of data points (also known as control points) and they are free to rearrange it as they wish, usually to cluster them better. Based, on that interaction, techniques have been proposed that perform interpolation so that they are able to project the rest of unseen dataset, in a kind of semi-supervised learning. While most techniques of this kind rely on feeding coordinates to least-squares, bayesian modeling, eigendecomposition, or other kinds of solvers, this thesis suggests an end-toend optimization algorithm that models user interaction as a target similarity matrix. Extensive evaluations are performed where we report results that outperform competitive baselines in a wide range of datasets (numerical, image, text). The second interaction scenario involves questions like ""what happens if I move that class away?"". This scenario involves a modification of SEF in which the target matrix of user interaction is enriched with the information of high-dimensional neighbors. In essence, when a whole dataset is projected, we drag a class away and we set the target matrix so as that the points we moved to be similar with their high-dimensional neighbors. This procedure results in some surprising observations. Apart from improving classification precision and clustering, the new emerging structure of the projection unveils semantic manifolds. For example, on a Head Pose dataset, by just dragging the faces looking far left to the left and those looking far right to the right, all faces are re-arranged on a continuum even on the vertical axis (face up and down). This methodology could be used in domain adaptation of dense embeddings and transfer learning. Αριστοτέλειο Πανεπιστήμιο Θεσσαλονίκης","",""
5,"Beom-Jin Lee, Jung-Woo Ha, Kyung-Min Kim, Byoung-Tak Zhang","Evolutionary concept learning from cartoon videos by multimodal hypernetworks",2013,"","","","",47,"2022-07-13 09:33:26","","10.1109/CEC.2013.6557700","","",,,,,5,0.56,1,4,9,"Concepts have been widely used for categorizing and representing knowledge in artificial intelligence. Previous researches on concept learning have focused on unimodal data, usually on linguistic domains in a static environment. Concept learning from multimodal stream data, such as videos, remains a challenge due to their dynamic change and high-dimensionality. Here we propose an evolutionary method that simulates the process of human concept learning from multimodal video streams. Two key ideas on evolutionary concept learning are representing concepts in a large collection (population) of hyperedges or a hypergraph and to incrementally learning from video streams based on an evolutionary approach. The hypergraph is learned ""evolutionarily"" by repeating the generation and selection process of hyperedge concepts from the video data. The advantage of this evolutionary learning process is that the population-based distributed coding allows flexible and robust trace of the change of concept relations as the video story unfolds. We evaluate the proposed method on a suite of children's cartoon videos for 517 minutes of total playing time. Experimental results show that the proposed method effectively represents visual-textual concept relations and our evolutionary concept learning method effectively models the conceptual change as an evolutionary process. We also investigate the structure properties of the constructed concept networks.","",""
5,"M. Garza, M. Maher","A KNOWLEDGE-LEAN STRUCTURAL ENGINEERING DESIGN EXPERT SYSTEM Andrs",1998,"","","","",48,"2022-07-13 09:33:26","","","","",,,,,5,0.21,3,2,24,"In this paper we present the GENCAD expert system which helps solve problems involving the structural design of tall buildings, pecifically focusing on the ways that the buildings resist lateral movements. One of the features of this type of design problem is that there doesn’t seem to be a lot of knowledge that can be made explicit about solving problems in the domain in order to program an expert system with it. Whether this lack of explicit knowledge is due to the complexity of the designs involved; to the implicit, intuitive, nature of the knowledge used by human experts; or to other factors, it still hinders the construction of a robust knowledgebased expert system. Our methodology has therefore been to make GENCAD a knowledge-lean expert system. This has been achieved by performing a careful task analysis of the problem-solving task and domain, and by implementing the required subtasks as much as possible using artificial intelligence methods that require little or no explicitly-coded domainand task-specific knowledge. We present here our analysis and our implementation of GENCAD.","",""
3,"M. E. Alami, F. Arriaga","Intelligent E-learning Systems and the Transfer of Novices into Experts",2013,"","","","",49,"2022-07-13 09:33:26","","10.5120/14140-2280","","",,,,,3,0.33,2,2,9,"In our knowledge society learning is a very important and broad topic that includes several unsolved questions. Among them the transfer of novices into experts remains elusive. The paper shows that the cognitive elements and mental models needed for the expert execution of a task or skill can be used in cooperation with suitable exercises and intelligent elearning systems to obtain a faster and more robust transfer of novices into experts. The paper also discusses the role of mental models and how can they be obtained from human experts. It also includes a state-space methodology to know the obtained place for the apprentice within the transfer process and how to move him optimally towards the final or expert state. A particular example for decision making by using an intelligent e-learning system simulating a private Medical Centre is included and the obtained results for more than five years are assessed. General Terms Artificial Intelligence, Intelligent E-learning Systems, Multiagent Systems, Human Learning.","",""
1,"H. Burns","Knowledge-based educational systems.",1988,"","","","",50,"2022-07-13 09:33:26","","","","",,,,,1,0.03,1,1,34,"In knowledge-based educational systems, the key concept is that information and procedures are represented in the same data structure. These structures can search for each other in flexible and, consequently, very robust ways. At the Air Force Human Resources Laboratory (AFHRL), our researchers are building computer environments that know what they know, know how people can best use them, and know how to draw inferences about their state--self-referential electronic tutors. In September 1986, artificial intelligence researchers participated in AFHRL's Research Planning Forum for Intelligent Tutorial Systems (ITS). This essay reviews the state of the philosophy, art, and science of artificial intelligence (AI) approaches to education. Then it summarizes the research issues which were presented, discussed, and better defined in this Forum--namely the nature and representation of 1) expertise modules, 2) student diagnostic modules, 3) adaptive instructional and curriculum modules, 4) instructional environments, and 5) man-machine interfaces. Advances in artificial intelligence, cognitive science, and instructional discourse have provided a means for investigating human learning, for representing an individual's own ""knowledge processing."" Research and development in knowledge-based educational systems seems promising, not only for helping people learn how to perform complex tasks, but also for explicitly expressing how people learn to learn. Therefore, would it not be wise to establish a scientific legacy for the development of effective knowledge-based tutorial systems which is informed by the best studies of mind and meaning, language and thought, purpose and paradox?","",""
0,"Saud Ben Mahmoud Mandourah","DEVELOPING A WEB-BASED ONTOLOGY FOR EXPERT DECISION SUPPORT SYSTEMS",2015,"","","","",51,"2022-07-13 09:33:26","","","","",,,,,0,0.00,0,1,7,"Expert systems were originally developed to solve illdefined problems and well-defined problems that are not efficiently solved with algorithmic approaches. This technology provides an innovative and robust techniques to capture and package knowledge. Its strength lies in its ability to be put to practical use when an expert is not available. This technology has proven to be especially effective when the task is in a rapidly changing environment. On the other side, ontology is the foundation of describing a domain of interest and it consists in a collection of terms organized in a hierarchical structure that shape the reality. The main objective of using ontologies is to share knowledge between computers or computers and human. Most of the usages of ontologies in the field of artificial intelligence are related to knowledge based systems and intelligent systems. These types of ontologies include a small number of concepts and their main objective is to facilitate reasoning tasks. This paper presents the developing of web-based ontology for expert systems technology. The developed ontology was encoded in OWL-DL format using the Protégé-OWL editing environment. Keywords-Ontological Engineering, Expert Systems, Artificial Intelligence, Knowledge Engineering, Web Technology","",""
23,"S. Nwankwo, Benjamin Obidigbo, F. Ekwulugo","Allying for quality excellence: scope for expert systems in supplier quality management",2002,"","","","",52,"2022-07-13 09:33:26","","10.1108/02656710210413516","","",,,,,23,1.15,8,3,20,"Over the past decade, quality managers and scholars have focused increased attention on supplier quality as a key resource for organisations. This paper presents the results of an exploratory study into how organisations rank supplier selection attributes and the extent to which use is made of decision support systems (expert systems in particular) in supplier quality management. Overall, quality was ranked the most important attribute. Paradoxically, decision‐support/knowledge‐based systems are not being utilized in solving the multi‐criteria decision problem inherent in supplier quality management. It is speculated that the lack of robust strategy for combining both human and artificial intelligence in supplier quality integration means that many organisations are making themselves vulnerable as out‐sourcing and strategic partnerships become important determinants of competitive advantage. Consequently, this paper assesses the scope for expert systems, a branch of artificial intelligence that is capable of helping organisations to co‐ordinate and harness potentially diverse sources of input resources in supplier quality management.","",""
11,"S. Salhi","Applications of Fuzzy Set Methodologies in Industrial Engineering",1991,"","","","",53,"2022-07-13 09:33:26","","10.1057/JORS.1991.59","","",,,,,11,0.35,11,1,31,"","",""
0,"J. Ortiz-Rodríguez, M. Martinez-Blanco, H. Vega-Carrillo, Ramón López Velarde","Development of a new software tool , based on ANN technology , in neutron spectrometry and dosimetry research",2007,"","","","",54,"2022-07-13 09:33:26","","","","",,,,,0,0.00,0,4,15,"Artificial Intelligence is a branch of study which enhances the capability of computers by giving them human-like intelligence. The brain architecture has been extensively studied and attempts have been made to emulate it as in the Artificial Neural Network technology. A large variety of neural network architectures have been developed and they have gained wide-spread popularity over the last few decades. Their application is considered as a substitute for many classical techniques that have been used for many years, as in the case of neutron spectrometry and dosimetry research areas. In previous works, a new approach called Robust Design of Artificial Neural network was applied to build an ANN topology capable to solve the neutron spectrometry and dosimetry problems within the Matlab® programming environment. In this work, the knowledge stored at Matlab® ANN’s synaptic weights was extracted in order to develop for first time a customized software application based on ANN technology, which is proposed to be used in the neutron spectrometry and simultaneous dosimetry fields.","",""
3,"S. Tardiff","Self-organizing event maps",2004,"","","","",55,"2022-07-13 09:33:26","","","","",,,,,3,0.17,3,1,18,"To take further steps along the path toward true artificial intelligence, systems must be built that are capable of learning about the world around them through observation and explanation. These systems should be flexible and robust in the style of the human brain and little precompiled knowledge should be given initially. As a step toward achieving this lofty goal, this thesis presents the self-organizing event map (SOEM) architcture. The SOEM architecture seeks to provide a way in which computers can be taught, through simple observation of the world, about typical events in a way that is flexible and robust. The self-organizing event map, as a data structure, stores a plane of event models that are continually updated and organized according to events that are observed by the system. In this manner, the event map produces clusters of similar events and provides an implicit representation of the regularity within the event space to which the system has been exposed. As part of this thesis, a test system that makes use of self-organizing event map architecture has been developed in conjunction with the Genesis Project at the Computer Science and Artificial Intelligence Laboratory (CSAIL) at MIT. This system receives input through a natural-language text interface and, through repreated training cycles, becomes capable of discerning between typical and exceptional events. Clusters of similar events develop within the map and these clusters act as an implicit form of the more commonly used (and explicit) notion of scripts and capability lists. For example., a trained map may recognize that dogs often run, but never fly. Therefore if a new input is received that describes a flying dog, the map would be capable of identifying the event as exceptional (or simply erroneous) and that further attention should be paid. By using clusters of similarity as an implicit representation, the self-organizing event maps presented here more accurately mimic natural memory systems and do not suffer from being tied to the limitations of a specific explicit representation of regularity. Thesis Supervisor: Patrick Henry Winston Title: Ford Professor of Artificial Intelligence and Computer Science","",""
5,"K. Monta, J. Itoh, M. Makino","An advanced man-machine system for BWR nuclear power plants",1992,"","","","",56,"2022-07-13 09:33:26","","10.1109/HFPP.1992.283393","","",,,,,5,0.17,2,3,30,"Developmental efforts to improve man-machine systems for BWR nuclear power plants have been carried out. A computerized operator support system has been developed based on the operators' role from the plant operational safety point of view. The support of human operators in their knowledge-based behaviour to cope with unanticipated abnormal events has been developed, considering the advance in artificial intelligence and cognitive system engineering. A top down design approach has been adopted based on cognitive work analysis, and the following main functions have been established: an ecological interface-support of the operator's direct preception and analytical reasoning; an cognitive model-based advisor-support of the operators' cognitive resources; and a robust automatic sequence controller.<<ETX>>","",""
5,"D. Wile, R. Balzer, N. Goldman","Automated derivation of program control structure from natural language program descriptions",1977,"","","","",57,"2022-07-13 09:33:26","","10.1145/800228.806935","","",,,,,5,0.11,2,3,45,"This paper describes a system which organizes a natural language description of a program into a conventional program control structure, as a part of a larger system for converting informal natural language program specifications into running programs. Analysis of the input program fragments using a model of a human “reader” of specifications has been found to be a very successful adjunct to conventional “planning” methodologies.  Natural language descriptions of programs can frequently be characterized as “rubble”—a very loosely organized set of almost independent description fragments [Schwartz]. Such specifications are often quite robust, due to a large degree of redundancy; they are also frequently quite concise, due to reliance on the readers' innate knowledge and their knowledge of the application domain. This paper discusses a paradigm for structuring the portion of “rubble” program descriptions which maps into conventional programming language control constructs and definition facilities.","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",58,"2022-07-13 09:33:26","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
19,"Thomas G. Dietterich","Robust artificial intelligence and robust human organizations",2018,"","","","",59,"2022-07-13 09:33:26","","10.1007/s11704-018-8900-4","","",,,,,19,4.75,19,1,4,"","",""
1,"","The Future of Human Knowledge and Artificial Intelligence",2019,"","","","",60,"2022-07-13 09:33:26","","10.34190/km.19.129","","",,,,,1,0.33,0,0,3,"","",""
1,"Elizabeth Real de Oliveira, P. Rodrigues","A Review of Literature on Human Behaviour and Artificial Intelligence: Contributions Towards Knowledge Management",2021,"","","","",61,"2022-07-13 09:33:26","","10.34190/ejkm.19.2.2459","","",,,,,1,1.00,1,2,1,"The main purpose of this research paper is to understand how artificial intelligence and machine learning applied to human behaviour has been treated, both theoretically and empirically, over the last twenty years, regarding predictive analytics and human organizational behaviour analysis. To achieve this goal, the authors performed a systematic literature review, as proposed by Tranfield, Denyer and Smart (2003), on selected databases and followed the PRISMA framework (Preferred Reporting Items for Systematic reviews and Meta-Analyses). The method is particularly suited for assessing emerging trends within multiple disciplines and therefore deemed the most suitable method for the purposes of this paper, which intends to survey and select papers according to their contribute towards theory building. By mapping what is known, this review will lay the groundwork, providing a timely insight into the current state of research on human organisational behaviour and its applications. A total of 17795 papers resulted from the application of the search equations. The papers’ abstracts were screened according to the inclusion / exclusion criterions which resulted in 199 papers for analysis. The authors have analysed the papers through VOSviewer software and R programming statistical computing software. This review showed that 60% of the research undertaken in the field has been done in the last three and a half years and there is no prominent author or academic journal, showing the emergence and the novelty of this research. The other key finds of the research relate to the evolution of the concept, from data-driven (hard) towards emotions-driven (soft) organisations.","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",62,"2022-07-13 09:33:26","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
5985,"David Silver, Julian Schrittwieser, K. Simonyan, Ioannis Antonoglou, Aja Huang, A. Guez, T. Hubert, Lucas baker, Matthew Lai, A. Bolton, Yutian Chen, T. Lillicrap, Fan Hui, L. Sifre, George van den Driessche, T. Graepel, D. Hassabis","Mastering the game of Go without human knowledge",2017,"","","","",63,"2022-07-13 09:33:26","","10.1038/nature24270","","",,,,,5985,1197.00,599,17,5,"","",""
8,"M. Peters, P. Jandrić","Artificial Intelligence, Human Evolution, and the Speed of Learning",2019,"","","","",64,"2022-07-13 09:33:26","","10.1007/978-981-13-8161-4_12","","",,,,,8,2.67,4,2,3,"","",""
0,"F. LeRon Shults","Progress in simulating human geography: Assemblage theory and the practice of multi-agent artificial intelligence modeling",2021,"","","","",65,"2022-07-13 09:33:26","","10.1177/03091325211059567","","",,,,,0,0.00,0,1,1,"Over the last few years, there has been an explosion of interest in assemblage theory among human geographers. During this same period, a growing number of scholars in the field have utilized computational methodologies to simulate the complex adaptive systems they study. However, very little attention has been paid to the connections between these two developments. This article outlines those connections and argues that more explicitly integrating assemblage theory and computer modeling can encourage a more robust philosophical understanding of both and facilitate progress in scientific research on the ways in which complex socio-material systems form and transform.","",""
40,"B. Cope, M. Kalantzis, Duane Searsmith","Artificial intelligence for education: Knowledge and its assessment in AI-enabled learning ecologies",2020,"","","","",66,"2022-07-13 09:33:26","","10.1080/00131857.2020.1728732","","",,,,,40,20.00,13,3,2,"Abstract Over the past ten years, we have worked in a collaboration between educators and computer scientists at the University of Illinois to imagine futures for education in the context of what is loosely called “artificial intelligence.” Unhappy with the first generation of digital learning environments, our agenda has been to design alternatives and research their implementation. Our starting point has been to ask, what is the nature of machine intelligence, and what are its limits and potentials in education? This paper offers some tentative answers, first conceptually, and then practically in an overview of the results of a number of experimental implementations documented in greater detail elsewhere. Our key finding is that artificial intelligence—in the context of the practices of electronic computing developing over the past three quarters of a century—will never in any sense “take over” the role of teacher, because how it works and what it does are so profoundly different from human intelligence. However, within the limits that we describe in this paper, it offers the potential to transform education in ways that—counterintuitively perhaps—make education more human, not less.","",""
0,"A. D. W. Sumari, I. Syamsiana","A Simple Introduction to Cognitive Artificial Intelligence’s Knowledge Growing System",2021,"","","","",67,"2022-07-13 09:33:26","","10.1109/DATABIA53375.2021.9650179","","",,,,,0,0.00,0,2,1,"Knowledge Growing System (KGS) since its introduction in 2009, has been stated as the foundation of Cognitive Artificial Intelligence (CAI). Because of its computation mechanism simplicity and it does not burden the computation resources, various use-cases have applied KGS to solve their problems. The KGS development was inspired by the growing of knowledge within human brain when thinking during carrying out interactions to a phenomenon in its environment. KGS learns to the data received at that time and at the next series of time that are sensed and perceived by its sensory organs, and uses them to generate knowledge. By combining approaches and techniques from cognitive psychology, mathematics, social science, and AI fields, we created simple mathematics formulas called ASSA2010 (Arwin Sumari-Suwandi Ahmad year 2010) information-inferencing fusion method for KGS’ knowledge growing mechanism. In this article, we deliver a simple introduction to KGS and also some of its utilizations for humankind.","",""
0,"Bushra Rasheed, M. Usama, Asmara Safdar","Robust Artificial Intelligence Approach to Stabilize and Control Propeller Driven Hybrid UGV",2022,"","","","",68,"2022-07-13 09:33:26","","10.1109/ICAI55435.2022.9773375","","",,,,,0,0.00,0,3,1,"Hybrid Unmanned Ground Vehicle (HUGV) can drive on any terrain including walls and fly as well, using the multi directional thrust force of propellers. In the era of industrial revolution, hybrid UGVs need to be autonomous with intelligent decision making capabilities. During wall climbing of hybrid UGVs, stability is essential and depends on real time feedback from multiple sensors. To increase stability and control, it is proposed that PID control loops should be replaced by AI based algorithms that reduce the decision time and mathematical complexity. For autonomous movement in any terrain using the proposed model, intelligent UGVs can map and localize simultaneously.They can make intelligent decisions about mode of movement i.e. driving on ground or wall, steering on ground or wall, flying and maneuvering by using real time sensor readings. Integration of the proposed AI models with HUGV can be applied to many areas which are hard for humans to access, for instance; inspection of large structures, bio & nuclear hazard environments, planetary exploration & magnetic fields detection.","",""
1,"Noris Binti Mohd Norowi","Human-Centred Artificial Intelligence in Concatenative Sound Synthesis",2021,"","","","",69,"2022-07-13 09:33:26","","10.1007/978-3-030-72116-9_21","","",,,,,1,1.00,1,1,1,"","",""
13,"Francesca Iandolo, F. Loia, Irene Fulco, Chiara Nespoli, F. Caputo","Combining Big Data and Artificial Intelligence for Managing Collective Knowledge in Unpredictable Environment—Insights from the Chinese Case in Facing COVID-19",2020,"","","","",70,"2022-07-13 09:33:26","","10.1007/s13132-020-00703-8","","",,,,,13,6.50,3,5,2,"","",""
8,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor, A. Siraj, Mike Rogers","Domain Knowledge Aided Explainable Artificial Intelligence for Intrusion Detection and Response",2019,"","","","",71,"2022-07-13 09:33:26","","","","",,,,,8,2.67,2,5,3,"Artificial Intelligence (AI) has become an integral part of modern-day security solutions for its ability to learn very complex functions and handling ""Big Data"". However, the lack of explainability and interpretability of successful AI models is a key stumbling block when trust in a model's prediction is critical. This leads to human intervention, which in turn results in a delayed response or decision. While there have been major advancements in the speed and performance of AI-based intrusion detection systems, the response is still at human speed when it comes to explaining and interpreting a specific prediction or decision. In this work, we infuse popular domain knowledge (i.e., CIA principles) in our model for better explainability and validate the approach on a network intrusion detection test case. Our experimental results suggest that the infusion of domain knowledge provides better explainability as well as a faster decision or response. In addition, the infused domain knowledge generalizes the model to work well with unknown attacks, as well as opens the path to adapt to a large stream of network traffic from numerous IoT devices.","",""
3,"C. Kolski, G. Boy, G. Melançon, M. Ochs, J. Vanderdonckt","Cross-Fertilisation Between Human-Computer Interaction and Artificial Intelligence",2020,"","","","",72,"2022-07-13 09:33:26","","10.1007/978-3-030-06170-8_11","","",,,,,3,1.50,1,5,2,"","",""
0,"Patrick Lambe","AI: Artificial Intelligence or Autistic Intelligence? Keeping knowledge organisation human",2019,"","","","",73,"2022-07-13 09:33:26","","10.5771/9783956505508-255","","",,,,,0,0.00,0,1,3,"","",""
63,"M. VerMilyea, J. Hall, S. Diakiw, A. Johnston, T. Nguyen, D. Perugini, A. Miller, A. Picou, A. P. Murphy, M. Perugini","Development of an artificial intelligence-based assessment model for prediction of embryo viability using static images captured by optical light microscopy during IVF",2020,"","","","",74,"2022-07-13 09:33:26","","10.1093/humrep/deaa013","","",,,,,63,31.50,6,10,2,"Abstract STUDY QUESTION Can an artificial intelligence (AI)-based model predict human embryo viability using images captured by optical light microscopy? SUMMARY ANSWER We have combined computer vision image processing methods and deep learning techniques to create the non-invasive Life Whisperer AI model for robust prediction of embryo viability, as measured by clinical pregnancy outcome, using single static images of Day 5 blastocysts obtained from standard optical light microscope systems. WHAT IS KNOWN ALREADY Embryo selection following IVF is a critical factor in determining the success of ensuing pregnancy. Traditional morphokinetic grading by trained embryologists can be subjective and variable, and other complementary techniques, such as time-lapse imaging, require costly equipment and have not reliably demonstrated predictive ability for the endpoint of clinical pregnancy. AI methods are being investigated as a promising means for improving embryo selection and predicting implantation and pregnancy outcomes. STUDY DESIGN, SIZE, DURATION These studies involved analysis of retrospectively collected data including standard optical light microscope images and clinical outcomes of 8886 embryos from 11 different IVF clinics, across three different countries, between 2011 and 2018. PARTICIPANTS/MATERIALS, SETTING, METHODS The AI-based model was trained using static two-dimensional optical light microscope images with known clinical pregnancy outcome as measured by fetal heartbeat to provide a confidence score for prediction of pregnancy. Predictive accuracy was determined by evaluating sensitivity, specificity and overall weighted accuracy, and was visualized using histograms of the distributions of predictions. Comparison to embryologists’ predictive accuracy was performed using a binary classification approach and a 5-band ranking comparison. MAIN RESULTS AND THE ROLE OF CHANCE The Life Whisperer AI model showed a sensitivity of 70.1% for viable embryos while maintaining a specificity of 60.5% for non-viable embryos across three independent blind test sets from different clinics. The weighted overall accuracy in each blind test set was >63%, with a combined accuracy of 64.3% across both viable and non-viable embryos, demonstrating model robustness and generalizability beyond the result expected from chance. Distributions of predictions showed clear separation of correctly and incorrectly classified embryos. Binary comparison of viable/non-viable embryo classification demonstrated an improvement of 24.7% over embryologists’ accuracy (P = 0.047, n = 2, Student’s t test), and 5-band ranking comparison demonstrated an improvement of 42.0% over embryologists (P = 0.028, n = 2, Student’s t test). LIMITATIONS, REASONS FOR CAUTION The AI model developed here is limited to analysis of Day 5 embryos; therefore, further evaluation or modification of the model is needed to incorporate information from different time points. The endpoint described is clinical pregnancy as measured by fetal heartbeat, and this does not indicate the probability of live birth. The current investigation was performed with retrospectively collected data, and hence it will be of importance to collect data prospectively to assess real-world use of the AI model. WIDER IMPLICATIONS OF THE FINDINGS These studies demonstrated an improved predictive ability for evaluation of embryo viability when compared with embryologists’ traditional morphokinetic grading methods. The superior accuracy of the Life Whisperer AI model could lead to improved pregnancy success rates in IVF when used in a clinical setting. It could also potentially assist in standardization of embryo selection methods across multiple clinical environments, while eliminating the need for complex time-lapse imaging equipment. Finally, the cloud-based software application used to apply the Life Whisperer AI model in clinical practice makes it broadly applicable and globally scalable to IVF clinics worldwide. STUDY FUNDING/COMPETING INTEREST(S) Life Whisperer Diagnostics, Pty Ltd is a wholly owned subsidiary of the parent company, Presagen Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation and Startup Fund (RCSF). ‘In kind’ support and embryology expertise to guide algorithm development were provided by Ovation Fertility. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. Presagen has filed a provisional patent for the technology described in this manuscript (52985P pending). A.P.M. owns stock in Life Whisperer, and S.M.D., A.J., T.N. and A.P.M. are employees of Life Whisperer.","",""
41,"Anja Bechmann, G. Bowker","Unsupervised by any other name: Hidden layers of knowledge production in artificial intelligence on social media",2019,"","","","",75,"2022-07-13 09:33:26","","10.1177/2053951718819569","","",,,,,41,13.67,21,2,3,"Artificial Intelligence (AI) in the form of different machine learning models is applied to Big Data as a way to turn data into valuable knowledge. The rhetoric is that ensuing predictions work well—with a high degree of autonomy and automation. We argue that we need to analyze the process of applying machine learning in depth and highlight at what point human knowledge production takes place in seemingly autonomous work. This article reintroduces classification theory as an important framework for understanding such seemingly invisible knowledge production in the machine learning development and design processes. We suggest a framework for studying such classification closely tied to different steps in the work process and exemplify the framework on two experiments with machine learning applied to Facebook data from one of our labs. By doing so we demonstrate ways in which classification and potential discrimination take place in even seemingly unsupervised and autonomous models. Moving away from concepts of non-supervision and autonomy enable us to understand the underlying classificatory dispositifs in the work process and that this form of analysis constitutes a first step towards governance of artificial intelligence.","",""
94,"A. Zappone, M. Di Renzo, M. Debbah, T. T. Lam, Xuewen Qian","Model-Aided Wireless Artificial Intelligence: Embedding Expert Knowledge in Deep Neural Networks for Wireless System Optimization",2018,"","","","",76,"2022-07-13 09:33:26","","10.1109/MVT.2019.2921627","","",,,,,94,23.50,19,5,4,"Deep learning based on artificial neural networks (ANNs) is a powerful machine-learning method that, in recent years, has been successfully used to realize tasks such as image classification, speech recognition, and language translation, among others, that are usually simple for human beings but extremely difficult for machines. This is one reason deep learning is considered one of the main enablers for realizing artificial intelligence (AI). The current methodology in deep learning consists of employing a data-driven approach to identify the best architecture of an ANN that allows input-output data pairs to be fitted. Once the ANN is trained, it is capable of responding to never-observed inputs by providing the optimum output based on past acquired knowledge. In this context, a recent trend in the deep-learning community complements purely data-driven approaches with prior information based on expert knowledge. In this article, we describe two methods that implement this strategy to optimize wireless communication networks. In addition, we provide numerical results to assess the performance of the proposed approaches compared with purely data-driven implementations.","",""
66,"Fabio Massimo Zanzotto","Viewpoint: Human-in-the-loop Artificial Intelligence",2017,"","","","",77,"2022-07-13 09:33:26","","10.1613/jair.1.11345","","",,,,,66,13.20,66,1,5,"Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future may have a possible dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers may need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Many learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, many of these workers are shooting themselves in the feet.  In this paper, we propose Human-in-the-loop Artificial Intelligence (HitAI) as a fairer paradigm for AI systems. Recognizing that any AI system has humans in the loop, HitAI will reward these aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Merry Men, HitAI researchers should fight for a fairer Robin Hood Artificial Intelligence that gives back what it steals.      This article is part of the special track on AI and Society.    ","",""
44,"Chiara Longoni, Luca Cian","Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect",2020,"","","","",78,"2022-07-13 09:33:26","","10.1177/0022242920957347","","",,,,,44,22.00,22,2,2,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).","",""
1,"C. Bormann, M. Kanakasabapathy, Prudhvi Thirumalaraju, I. Dimitriadis, I. Souter, K. Hammer, H. Shafiee","O-125 Development of an artificial intelligence embryo witnessing system to accurately track and identify patient specific embryos in a human IVF laboratory",2021,"","","","",79,"2022-07-13 09:33:26","","10.1093/humrep/deab126.050","","",,,,,1,1.00,0,7,1,"      Can convolutional neural networks (CNN) be used as a witnessing system to accurately track and identify patient specific embryos at the cleavage stage of development?        We developed the first artificial intelligence driven witnessing system to accurately track cleavage and blastocyst stage embryos in a human ART laboratory.        There are reports of human errors in embryo tracking that have led to the births of children with different genetic makeup than their birth parents. Clinical practices rely on manual identification, barcodes or radio-frequency identification technology to track embryos. These systems are designed to track culture dishes but are unable to monitor developing embryos within the dish to help ensure an error-free patient match. Previously, we developed an AI witnessing system to track blastocysts with 100% accuracy. The goal of this study was to determine whether an AI witnessing system could be developed that accurately tracks cleavage stage embryos.        A pre-developed deep neural network technology was first trained and tested on 4944 embryos images. The algorithm processed embryo images for each patient and produced a unique key that was associated with the patient ID at 60 hpi, which formed our library. When the algorithm evaluated embryos at 64 hpi it generated another key that was matched with the patient’s unique key available in the library.        A total of 3068 embryos from 412 patients were examined by the CNN at both 60 hpi and 64 hpi. These timepoints were chosen as they reflect the time our laboratory evaluates Day 3 embryos (60 hpi) and the time we move them to another dish and prepare them for transfer (64 hpi). The patient cohorts ranged from 3-12 embryos per patient.        The accuracy of the CNN in correctly matching the patient identification with the patient embryo cohort was 100% (CI: 99.1% to 100.0%, n = 412).        Limitations of this study include that all embryos were imaged under identical conditions and within the same EmbryoScope. Additionally, this study only examined fresh Day 3 embryos cultured over a span of 4 hours. Future studies should include images of fresh and frozen/thawed embryos captured using different imaging systems.        This study describes the first artificial intelligence-based approach for cleavage stage embryo tracking and patient specimen identification in the IVF laboratory. This technology offers a robust witnessing step based on unique morphological features that are specific to each individual embryo.        This work was partially supported by the Brigham Precision Medicine Developmental Award (Brigham Precision Medicine Program, Brigham and Women’s Hospital), Partners Innovation Discovery Grant (Partners Healthcare), and R01AI118502, and R01AI138800. ","",""
2,"Frank Guerin","Projection: A Mechanism for Human-like Reasoning in Artificial Intelligence",2021,"","","","",80,"2022-07-13 09:33:26","","10.1080/0952813x.2022.2078889","","",,,,,2,2.00,2,1,1,"Artiﬁcial Intelligence systems cannot yet match human abilities to apply knowledge to situations that vary from what they have been programmed for, or trained for. In visual object recognition, methods of inference exploiting top-down information (from a model) have been shown to be eﬀective for recognising entities in diﬃcult conditions. Here a component of this type of inference, called ‘projection’, is shown to be a key mechanism to solve the problem of applying knowledge to varied or challenging situations, across a range of AI domains, such as vision, robotics, or language. Finally the relevance of projection to tackling the commonsense knowledge problem is discussed.","",""
0,"Iván Manuel De la Vega Hernández, Angel Serrano Urdaneta, E. Carayannis","Global bibliometric mapping of the frontier of knowledge in the field of artificial intelligence for the period 1990–2019",2022,"","","","",81,"2022-07-13 09:33:26","","10.1007/s10462-022-10206-4","","",,,,,0,0.00,0,3,1,"","",""
0,"A. Chavez Badiola, A. Flores-Saiffe, R. Valencia, G. Mendizabal-Ruiz, J. Villavicencio, D. Gonzalez, D. Griffin, A. Drakeley, J. Cohen","P-241 ‘Augmented intelligence’ to possibly shorten euploid identification time: A human-machine interaction study for euploid identification using ERICA, an Artificial Intelligence software to assist embryo ranking",2022,"","","","",82,"2022-07-13 09:33:26","","10.1093/humrep/deac107.231","","",,,,,0,0.00,0,9,1,"      What is the mean number of transfers needed to achieve a euploid transfer selected by embryologists plus ERICA’s assistance?        Augmented intelligence (ERICA plus human collaboration) outperforms both the embryologists and artificial intelligence's individual performance alone.        Euploid embryos are more likely to implant successfully. Artificial intelligence (AI) could improve embryo selection over current techniques, but scepticism exists. Augmented intelligence (AuI) combines both the mathematical reproducibility of machine learning and the knowledge and experience of humans. This approach employs AI tools as an assistant, where the user shall learn to interpret the AI. A recent study suggested that embryologists assisted by AI improved the embryo selection of euploid transfers. ERICA (IVF2.0 Limited, UK) was designed to rank blastocysts according to their probability of euploidy.        We prospectively studied embryo selection for ERICA alone, embryologists only and when interacting (embryologists and ERICA) in 150 synthetically generated (reconstructed on real-data) embryo transfer cycles. Embryos were ranked in order, and performance was assessed by time to identify a euploid embryo within each cycle cohort correctly. Embryologists were allowed to rank a maximum of 10 cycles per day for three weeks starting in January 2022, using a mobile phone application designed for this purpose.        Using real-life cycle distributions of euploid/aneuploid blastocysts and the number of embryos in a cycle (according to ERICA’s database), we created 150 synthetic cycles, 30 for each age bracket (< 35, 35-37, 38-40, 41-42, and >42). These were randomly populated with blastocyst images preserving their actual ploidy status correspondingly. Each synthetic cycle contained between 2 to 6 authentic embryo images with at least one euploid and one aneuploid.        The total database had a euploid rate of 37.4% (n = 513), and by age brackets from 1 to 5 were 45.7% (n = 116), 43.8% (n = 105), 35.9% (n = 92), 31.2% (n = 96), and 28.8% (n = 104) respectively.  The mean number of cycles analysed by each participant was 113.5 (CI: 100.8-126.2). The mean time-to-euploid transfer for embryologists alone was 2.07 (CI:2.00-2.13); for the ERICA alone was 1.86 (CI:1.82-1.91); and for embryologists assisted by ERICA was 1.62 (CI:1.55-1.68). All study groups compared to each other were statistically significant using a paired two-tailed student’s t-test (p < 0.001).  The proportion of euploid transfer at the first try for embryologists alone was 0.40 (CI:0.37-0.43), for ERICA alone was 0.54 (CI:0.53-0.54), and for embryologists assisted by ERICA was 0.47 (CI:0.44-0.50). All study groups compared with each other were statistically significant with a paired two-tailed student’s t-test (p < 0.01).        Although our findings suggest that Aul outperforms both AI and humans alone, this study needs to be replicated with a larger cohort of embryologists with different experience levels in different countries to confirm these results.        Combining machine-human interaction through a well-designed process could improve embryo selection and reduce inter-operator variability amongst staff with different experience levels. It could also set a frame for adequate agency and accountability, and enhance trust and adoption.        NA ","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",83,"2022-07-13 09:33:26","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
3,"R. Kreutzer, Marie Sirrenberg","Fields of Application of Artificial Intelligence—Health Care, Education and Human Resource Management",2019,"","","","",84,"2022-07-13 09:33:26","","10.1007/978-3-030-25271-7_6","","",,,,,3,1.00,2,2,3,"","",""
34,"C. Thongprayoon, W. Kaewput, K. Kovvuru, P. Hansrivijit, S. Kanduri, T. Bathini, A. Chewcharat, N. Leeaphorn, M. Gonzalez-Suarez, W. Cheungpasitporn","Promises of Big Data and Artificial Intelligence in Nephrology and Transplantation",2020,"","","","",85,"2022-07-13 09:33:26","","10.3390/jcm9041107","","",,,,,34,17.00,3,10,2,"Kidney diseases form part of the major health burdens experienced all over the world. Kidney diseases are linked to high economic burden, deaths, and morbidity rates. The great importance of collecting a large quantity of health-related data among human cohorts, what scholars refer to as “big data”, has increasingly been identified, with the establishment of a large group of cohorts and the usage of electronic health records (EHRs) in nephrology and transplantation. These data are valuable, and can potentially be utilized by researchers to advance knowledge in the field. Furthermore, progress in big data is stimulating the flourishing of artificial intelligence (AI), which is an excellent tool for handling, and subsequently processing, a great amount of data and may be applied to highlight more information on the effectiveness of medicine in kidney-related complications for the purpose of more precise phenotype and outcome prediction. In this article, we discuss the advances and challenges in big data, the use of EHRs and AI, with great emphasis on the usage of nephrology and transplantation.","",""
32,"W. Villegas-Ch., Adrián Arias-Navarrete, Xavier Palacios-Pacheco","Proposal of an Architecture for the Integration of a Chatbot with Artificial Intelligence in a Smart Campus for the Improvement of Learning",2020,"","","","",86,"2022-07-13 09:33:26","","10.3390/su12041500","","",,,,,32,16.00,11,3,2,"Traditional teaching based on masterclasses or techniques where the student develops a passive role has proven to be inefficient methods in the learning process. The use of technology in universities helps to generate active learning where the student’s interest improves making him the main actor in his education. However, implementing an environment where active learning takes place requires a great deal of effort given the number of variables involved in this objective. To identify these variables, it is necessary to analyze the data generated by the students in search of patterns that allow them to be classified according to their needs. Once these needs are identified, it is possible to make decisions that contribute to the learning of each student; for this, the use of artificial intelligence is considered. These techniques emulate the processes of human thought using structures that contain knowledge and experience of human experts.","",""
31,"I. Habli, T. Lawton, Zoe Porter","Artificial intelligence in health care: accountability and safety",2020,"","","","",87,"2022-07-13 09:33:26","","10.2471/BLT.19.237487","","",,,,,31,15.50,10,3,2,"Abstract The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed.","",""
2,"Davis K. Cope","Music, Artificial Intelligence and Neuroscience",2021,"","","","",88,"2022-07-13 09:33:26","","10.1007/978-3-030-72116-9_7","","",,,,,2,2.00,2,1,1,"","",""
1,"Alaa Amjed Abdulateef, Alaa Hamid Mohammed, Ihsan Amjad Abdulateef","The Avoidance And Detection Function Of Artificial Intelligence In Covid-19",2021,"","","","",89,"2022-07-13 09:33:26","","10.1109/HORA52670.2021.9461280","","",,,,,1,1.00,0,3,1,"The whole planet today is having to fight COVID-19 with big obstacles. The COVID-19 influenced several countries around the world between December 2019 and the present day. Many organisations and scientists seek to find a vaccine and to minimize the spread of COVID-19. Artificial Intelligence is one technology that can successfully address this virus (AI). In the case of other pathogens, artificial intelligence performed very well and could help us cope with the virus COVID-19, too. It is the imagination and the information of the people who use it which will help to overcome this dilemma. In some previous instances AI played a major role in virus prevention and identification. We have an ability to detect certain aspects of the AI because of the COVID-19 crisis. Machine learning that an AI subclass is used to identify patterns and to plan valuable knowledge based on recorded data sets. At the point where used entirely, AI can exceed human efforts by speed and differentiate designs from knowledge previously ignored. However many correct and appropriate data are needed for effective implementation of AI systems. This paper discusses the AI's role in COVID-19 prevention and detection and examines numerous technological aspects of AI. This paper would also clarify where AI will contribute with likely solutions to stop the spread of COVID-19.","",""
1,"Y. Sheng, Jiahan Zhang, Y. Ge, Xinyi Li, Wentao Wang, H. Stephens, F. Yin, Qiuwen Wu, Q. Wu","Artificial intelligence applications in intensity modulated radiation treatment planning: an overview.",2021,"","","","",90,"2022-07-13 09:33:26","","10.21037/qims-21-208","","",,,,,1,1.00,0,9,1,"Artificial intelligence (AI) refers to methods that improve and automate challenging human tasks by systematically capturing and applying relevant knowledge in these tasks. Over the past decades, a number of approaches have been developed to address different types and needs of system intelligence ranging from search strategies to knowledge representation and inference to robotic planning. In the context of radiation treatment planning, multiple AI approaches may be adopted to improve the planning quality and efficiency. For example, knowledge representation and inference methods may improve dose prescription by integrating and reasoning about the domain knowledge described in many clinical guidelines and clinical trials reports. In this review, we will focus on the most studied AI approach in intensity modulated radiation therapy (IMRT)/volumetric modulated arc therapy (VMAT)-machine learning (ML) and describe our recent efforts in applying ML to improve the quality, consistency, and efficiency of IMRT/VMAT planning. With the available high-quality data, we can build models to accurately predict critical variables for each step of the planning process and thus automate and improve its outcomes. Specific to the IMRT/VMAT planning process, we can build models for each of the four critical components in the process: dose-volume histogram (DVH), Dose, Fluence, and Human Planner. These models can be divided into two general groups. The first group focuses on encoding prior experience and knowledge through ML and more recently deep learning (DL) from prior clinical plans and using these models to predict the optimal DVH (DVH prediction model), or 3D dose distribution (dose prediction model), or fluence map (fluence map model). The goal of these models is to reduce or remove the trial-and-error process and guarantee consistently high-quality plans. The second group of models focuses on mimicking human planners' decision-making process (planning strategy model) during the iterative adjustments/guidance of the optimization engine. Each critical step of the IMRT/VMAT treatment planning process can be improved and automated by AI methods. As more training data becomes available and more sophisticated models are developed, we can expect that the AI methods in treatment planning will continue to improve accuracy, efficiency, and robustness.","",""
1,"Lana Sinapayen","Perspective: Purposeful Failure in Artificial Life and Artificial Intelligence",2021,"","","","",91,"2022-07-13 09:33:26","","","","",,,,,1,1.00,1,1,1,"Complex systems fail. I argue that failures can be a blueprint characterizing living organisms and biological intelligence, a control mechanism to increase complexity in evolutionary simulations, and an alternative to classical fitness optimization. Imitating biological successes in Artificial Life and Artificial Intelligence can be misleading; imitating failures offers a path towards understanding and emulating life it in artificial systems. Failure is Knowledge, Knowledge is Power You are handed a mysterious box containing the most complex object in the universe, and must find how the object works. Where do you start? “The human brain is the most complex object in the universe” is a well worn cliche (Constable (1918)). While the claim might not be true, the human brain is definitely very complex. In neuroscience and psychology, one of the most compelling ways to understand how the brain works is to study how it fails. Brain damage, irrational decisions, sensory illusions: internal or external changes that make the brain fail are how we find how the brain succeeds. Failure is used to understand complex systems beyond neuroscience: reverse-engineering computer software, understanding animal behavior, identifying solid materials... Failure even defines Science itself. an hypothesis is considered scientific if and only if it is “falsifiable”: if it can reproducibly fail (Popper (1934)). Why default to observing failures when we don’t know what is going on? Because the success-failure boundary is full of information. Let me define “failure” in the context of this discussion. Imagine being an ant dropped somewhere on top of Fig.1-(a). What is the fastest way to map your surroundings? Rather than walking every inch of the surface, find boundaries. When you are investigating a complex system that is working as expected, you are an ant dropped on Mount Success. To find the boundary, you have to push the system into failure mode. Staying inside the success space can inform you about the robustness of the system to perturbations (at best the system recovered from the perturbation, at worst your perturbation was irrelevant), but it is not explanatory. Neither is going from failure to failure. You can only investigate causes and effects if your intervention actually changes something: the failure boundary is not just more informative, it is a different kind of information altogether. Boundaries and failures are not exactly the same. If you are observing a function of the system that does not change when it crosses the failure boundary, you will not notice the transition. If you are observing the right function, you might see the system performance on that function become better or worse. Let us call “failure points” abrupt transitions from “some performance” to 0: they are the most salient of transitions. Going back to Fig.1, the ant might not notice the transition from a gentle slope to a flat terrain, but a cliff will be noticeable. Ideally, you would want to map the entire failure boundary; in practice, you will focus on failure points. ? Mount Success Failure Bog","",""
1,"Pranav Gupta, A. Woolley","Articulating the Role of Artificial Intelligence in Collective Intelligence: A Transactive Systems Framework",2021,"","","","",92,"2022-07-13 09:33:26","","10.1177/1071181321651354c","","",,,,,1,1.00,1,2,1,"Human society faces increasingly complex problems that require coordinated collective action. Artificial intelligence (AI) holds the potential to bring together the knowledge and associated action needed to find solutions at scale. In order to unleash the potential of human and AI systems, we need to understand the core functions of collective intelligence. To this end, we describe a socio-cognitive architecture that conceptualizes how boundedly rational individuals coordinate their cognitive resources and diverse goals to accomplish joint action. Our transactive systems framework articulates the inter-member processes underlying the emergence of collective memory, attention, and reasoning, which are fundamental to intelligence in any system. Much like the cognitive architectures that have guided the development of artificial intelligence, our transactive systems framework holds the potential to be formalized in computational terms to deepen our understanding of collective intelligence and pinpoint roles that AI can play in enhancing it.","",""
167,"Max Tegmark","Life 3.0: Being Human in the Age of Artificial Intelligence",2017,"","","","",93,"2022-07-13 09:33:26","","","","",,,,,167,33.40,167,1,5,"New York Times Best Seller How will Artificial Intelligence affect crime, war, justice, jobs, society and our very sense of being human? The rise of AI has the potential to transform our future more than any other technologyand theres nobody better qualified or situated to explore that future than Max Tegmark, an MIT professor whos helped mainstream research on how to keep AI beneficial. How can we grow our prosperity through automation without leaving people lacking income or purpose? What career advice should we give todays kids? How can we make future AI systems more robust, so that they do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will machines eventually outsmart us at all tasks, replacing humans on the job market and perhaps altogether? Will AI help life flourish like never before or give us more power than we can handle? What sort of future do you want? This book empowers you to join what may be the most important conversation of our time. It doesnt shy away from the full range of viewpoints or from the most controversial issuesfrom superintelligence to meaning, consciousness and the ultimate physical limits on life in the cosmos.","",""
3,"Edirlei Soares de Lima, B. Feijó","Artificial Intelligence in Human-Robot Interaction",2019,"","","","",94,"2022-07-13 09:33:26","","10.1007/978-3-319-96722-6_11","","",,,,,3,1.00,2,2,3,"","",""
0,"Jingjing Li, Hao Wang","Application of Artificial Intelligence in Libraries",2021,"","","","",95,"2022-07-13 09:33:26","","10.1109/AIAM54119.2021.00072","","",,,,,0,0.00,0,2,1,"Artificial intelligence (AI) has shown great promise for applications with large breakthroughs in pattern recognition, knowledge mapping, computer vision, and human-machine interaction. In this paper, firstly, the key technical features of AI in library application are studied, and its application in library stack building, stack management and information retrieval service is analyzed; then the existing problems of artificial intelligence in library application are analyzed; and finally the corresponding application service countermeasures are put forward, which can provide reference for related researchers and staff.","",""
0,"Ewa Szewczyk","Artificial intelligence in administrative law and procedure",2021,"","","","",96,"2022-07-13 09:33:26","","10.13166/wsge//krcl6757","","",,,,,0,0.00,0,1,1,"Introduction Since J. McCarthy used the term ‘artificial intelligence’ (AI) in the 1950s, it has become a key concept in the technological development of all mankind. It has appeared in every area of life and science. AI has become established in areas of life that were previously thought to be reserved for decision-making by human beings. Artificial intelligence is based on the analysis of large volumes of data, used in algorithms. According to the modern definition, artificial intelligence encompasses the area of knowledge that includes fuzzy logic, evolutionary computation, neural networks, artificial life, and robotics, and one of its essential features is the ability to learn1 and take into account new circumstances when solving a given problem2. In other words, artificial intelligence is the ability of a machine to mimic or imitate human intelligence3. Algorithms are nothing new. They have been used in computer programmes for decades. Today, however, advanced algorithms have become digital robots – often sophisticated computer programmes (rather than physical entities) with the ability to adapt and ‘learn’. However, there is no denying that the unhindered development of AI technologies is marred with public concern and is by no means universally embraced, even though the Covid-19 pandemic has","",""
0,"Junxi Chen, Xiying Zhan, Yuping Wang, Xuping Huang","Medical Robots based on Artificial Intelligence in the Medical Education",2021,"","","","",97,"2022-07-13 09:33:26","","10.1109/ICAIE53562.2021.00008","","",,,,,0,0.00,0,4,1,"With the rapid development of artificial intelligence, more and more scientific and technological achievements in the information age have been widely used in various fields of society. At the same time, the rapid development of intelligence has also continuously promoted the progress of medical education, and more and more robots have been applied to the field of medical education. The main purpose of this research is to analyze the current situation of using artificial intelligence-based medical robots in medical education to determine its effect on improving medical education. We have proposed a method for using medical robots based on artificial intelligence technology to help medical students conduct more efficient skills training and professional knowledge acquisition. Medical education based on AI use the robots to act as a human body model, perform scene simulation, act as a ""medical encyclopedia"", perform pharmacological simulation reactions, and finally automatically evaluate students’ medical knowledge mastery level. All in all, integrating various artificial intelligence technologies into medical education, aroused the interest of medical students in learning, and greatly improved the results of teaching.","",""
0,"V. Maphosa, M. Maphosa","The Trajectory of Artificial Intelligence Research in Higher Education: A Bibliometric Analysis and Visualisation",2021,"","","","",98,"2022-07-13 09:33:26","","10.1109/icABCD51485.2021.9519368","","",,,,,0,0.00,0,2,1,"The rapid development of artificial intelligence (AI) and its applications is gaining global attention and promises to revolutionise every aspect of human life, including education. AI will transform higher education (HE) institutions through improved adaptation and competitiveness. The application of AI in HE is an emerging research area with limited review. Our paper seeks to provide a comprehensive bibliometric analysis and visualisation of research on AI application in HE in the past two decades. We evaluated 283 articles published by researchers from 59 countries in the Scopus database over the past two decades based on explicit inclusion and exclusion criteria. The study applied various bibliometric indicators and word analysis to examine emergent trends. VOSviewer was used for visualisations to map a knowledge base by uncovering keywords used within AI in HE. The results show the number of AI articles published per year, their geographic distribution, analysis by subject area and keywords, and research trends. Research in AI is interdisciplinary, dominated by computer science and engineering fields. AI research in HE is growing, the first 15 years contributed 22%, and the last five years yielded 78%. Countries with a high investment in research dominated AI research, with China and the United States leading. There is very little research from developing countries. Our paper highlights current and future research directions in AI in education, and its limitations.","",""
0,"Qingchu Jiang, Tianmei Mao","Research on Future Education Development under the trend of Information Technology and Artificial Intelligence in the Sixth Scientific and Technological Revolution",2021,"","","","",99,"2022-07-13 09:33:26","","10.1109/ICAIE53562.2021.00131","","",,,,,0,0.00,0,2,1,"With the rapid rise of big data, cloud computing, artificial intelligence and other emerging technologies and their wide application in various fields of society, the Sixth Scientific and Technological Revolution is quietly infiltrating human habitat, which Includes the field of education. At present, China's education development is also facing the challenges of education reform. Based on the influence of technological revolution on the future, this paper investigates the development of educational technology for China's educational reform. Our education should change the teaching form through multimedia and online classroom tools, and expand the new teaching content under the analysis of Artificial Intelligence (AI). On the one hand, informatization, internationalization and knowledge economy bring severe external challenges to education. On the other hand, China's education is facing such problems as educational injustice, lack of innovation and lack of lifelong education system. Traditional educational concepts, models and methods are no longer able to adapt to the new development situation, and there is an urgent need to promote systematic changes in education as a whole. Based on the future impact of the Sixth Scientific and Technological Revolution, this study aims to clarify the scientific trend and law of education in the future. In view of these problems, this paper puts forward concrete analysis and countermeasures for China's educational reform in combination with the technological development in the field of education.","",""
23,"S. Jamil","Artificial Intelligence and Journalistic Practice: The Crossroads of Obstacles and Opportunities for the Pakistani Journalists",2020,"","","","",100,"2022-07-13 09:33:26","","10.1080/17512786.2020.1788412","","",,,,,23,11.50,23,1,2,"ABSTRACT In the past two decades, technological advancement has reshaped the news media landscape in many developed and developing countries of the world. Journalistic practice, especially in large-scale economies, is experiencing transformations due to the introduction of automatic artificial intelligence processes into different aspects of news production and dissemination. Now devices and machines are taking on the role of communicator, replacing journalists as communicators. It is important to analyse whether journalism in low income countries has begun to feeling the ripple of artificial intelligence for technologically advanced journalistic practices and newsrooms. Using the Human-Machine Communication framework, this study advances knowledge about what AI can do to transform journalism practice in low-income countries, which is vital to compare the implications of AI in other similar socioeconomic contexts and news media ecologies. Thus, this study specifically reflects upon the case of Pakistan. It investigates how the Pakistani journalists perceive AI technology in the role of communicator and how they view the human-machine communication process. This study identifies constraints and explores opportunities for the use of artificial intelligence in Pakistan’s mainstream news media. To address these aims, this study uses the qualitative method of in-depth interviews, and presents the findings using thematic analysis.","",""
53,"Fabio Massimo Zanzotto","Human-in-the-loop Artificial Intelligence",2017,"","","","",101,"2022-07-13 09:33:26","","","","",,,,,53,10.60,53,1,5,"Little by little, newspapers are revealing the bright future that Artificial Intelligence (AI) is building. Intelligent machines will help everywhere. However, this bright future has a dark side: a dramatic job market contraction before its unpredictable transformation. Hence, in a near future, large numbers of job seekers will need financial support while catching up with these novel unpredictable jobs. This possible job market crisis has an antidote inside. In fact, the rise of AI is sustained by the biggest knowledge theft of the recent years. Learning AI machines are extracting knowledge from unaware skilled or unskilled workers by analyzing their interactions. By passionately doing their jobs, these workers are digging their own graves.  In this paper, we propose Human-in-the-loop Artificial Intelligence (HIT-AI) as a fairer paradigm for Artificial Intelligence systems. HIT-AI will reward aware and unaware knowledge producers with a different scheme: decisions of AI systems generating revenues will repay the legitimate owners of the knowledge used for taking those decisions. As modern Robin Hoods, HIT-AI researchers should fight for a fairer Artificial Intelligence that gives back what it steals.","",""
0,"M. Boughanem, I. Akermi, G. Pasi, Karam Abdulahhad","Information Retrieval and Artificial Intelligence",2020,"","","","",102,"2022-07-13 09:33:26","","10.1007/978-3-030-06170-8_5","","",,,,,0,0.00,0,4,2,"","",""
0,"E. Nikitos, T. Triantafillou, K. Dimitropoulos, V. Kallergi, P. Psathas, I. Erlich, A. Ben-Meir, N. Bergelson","P-271 Challenges with comparing different commercially available Artificial Intelligence (AI) systems on the same data set of time-lapse selected euploid blastocysts",2022,"","","","",103,"2022-07-13 09:33:26","","10.1093/humrep/deac107.260","","",,,,,0,0.00,0,8,1,"      To identify challenges in choosing a robust AI following comparative validation with data already pre-selected with established embryos selection tools: blastulation, morphology, time-lapse, PGTA.        Challenges included: bias; assessment against outcomes AI models were not trained on; performance metrics prioritisation; statistical methodology; continuous data cutoffs for binary clinical decision making.        AI is commercially available to be incorporated into routine practice to support embryo selection decision-making. Different clinical practices and demographics are used to train AI models, potentially impacting the prediction efficacy of the same model when used in different clinics. Fertility professionals require robust methods of validation to responsibly implement AI-based tools. Unbiased and robust frameworks for comparing AI systems in the same dataset are needed. Validating AI in a dataset of time-lapse selected euploid blastocysts using all the current methods of embryo selection currently available is the toughest assessment possible and has not previously been performed.        This study uses a retrospectively timelapse dataset collected from 2018-2021 at a single private fertility clinic. The dataset included 915 blastocysts which underwent PGTA (913 results: 381 euploids, 528 aneuploids, 4 mosaics) and 46 euploids transferred with known bhcg and ongoing clinical outcome (of which 40 resulted to live birth).  Following a prospective, comparative, observational, cohort study design, blastocysts were blindly scored using the CHLOE(FAIRTILITY) and another commercially available AI system, referred to as ‘AI-2’.        Patients aged 24-47years (average 35.4). Blastocysts selected for biopsy and transfer based on morphology and KIDScore(Vitrolife). Both AI systems were tested in the data set blindly, without any training. Correlation Regression analysis assessed correlation with KIDSCORE and relative to each AI system. Efficacy of prediction (using metrics AUC, Accuracy, Sensitivity, Specificity and Informedness) of outcomes (ploidy, biochemical and clinical pregnancy) were assessed for both AI models (CHLOEvsAI-2) by two independent statisticians to establish significance.        Regression analysis demonstrated no correlation between KIDSCORE and AI-2(r2=0.3%,p=0.5) or between CHLOE(FAIRTILITY) and AI-2(r2=0.03%,p=0.9). CHLOE(FAIRTILITY) correlated with KIDSCORE(r2=29%,p<0.001).  AI-2 was not predictive of ploidy (Euploids vs Aneuploids+mosaic: AUC=0.5,p=0.6). CHLOE(Fairtility) was predictive of ploidy(AUC=0.66, p<0.001).  Neither AI-2 or CHLOE(Fairtility) predicted which embryo the human embryologist prioritised for transfer (AI-2 vs CHLOE:accuracy:0.31vs0.49, p<0.00001). Neither AI-2 nor CHLOE(Fairtility) predicted which embryo the human embryologist prioritised for transfer (AI-2 vs CHLOE:accuracy: 0.31vs0.49, p<0.00001). There was no difference detected in efficacy of prediction of biochemical (accuracy:0.52vs0.67,NS) and ongoing clinical pregnancy (accuracy:0.53 vs 0.78,NS) by AI-2 or CHLOE. This is partly due to the low number of euploid transfers assessed (n = 46), and partly due to the fact that neither of these algorithms are trained specifically on predicting outcome of euploid transfers.  CHLOE(Fairtility) was more specific than AI-2 for predicting selection for transfer(0.44/0.80vs0.17/0.93,p<0.05/NS) and ploidy(0.54/0.77vs0.23/0.87,p<0.05/NS), and they were equally as sensitive. CHLOE(Fairtility) was more sensitive, and less specific than AI-2 for predicting biochemical pregnancy(0.36/0.81vs0.86/0.38,p<0.05) and more sensitive but equally as specific for predicting clinical pregnancy(0.33/0.88vs0.83/0.46,NS/p<0.05).  Informedness was positive for both CHLOE(Fairtility) and AI-2 in predicting all outcomes assessed. Informedness was greater for AI-2 for predicting morphology(AI-2vsCHLOE:0.16vs0.31,p<0.05), transfer(0.11vs0.24,p<0.05), ploidy(0.10vs0.31,p<0.05) and equivalent for predicting biochemical (0.23vs0.17,NS) and clinical pregnancy(0.29vs0.22,NS).        In this single clinic study, both algorithms were assessed against outcomes (live birth following transfer of time-lapse cultured euploid blastocysts) for which they were not trained on: AI-2(designed for ploidy prediction) and CHLOE(FAIRTILITY, implantation prediction of non-PGTA embryos) and no clinic data was used for training.        The only way to decide which AI model is more useful is by a direct comparison of two or more models on the same dataset with same outcomes and metrics, as recommended by TRIPOD. To date, this is the first publication comparing multiple commercial AI models on the same dataset.        NA ","",""
1,"A. Admin, Dr.P Dr.P.Kavitha2, A. Akshaya, P. P.Shalin, R. R.Ramya","A Survey on Cyber Security Meets Artificial Intelligence: AI– Driven Cyber Security",2022,"","","","",104,"2022-07-13 09:33:26","","10.54216/jchci.020202","","",,,,,1,1.00,0,5,1,"The computerized version of human intelligence is Artificial Intelligence(AI). Artificial Intelligence systems combine large sets of data with intelligent and iterative processing algorithms in order to make predictions, based on patterns and features in the data that they analyse. With the booming technologies such as IOT and Cloud Computing, huge amounts of data are generated and collected that require cyber security protection today. There is a growing need for cyber security methods which are both robust and intelligent due to the ever-increasing complexity of cyber crimes. While data can be used to benefit business interests, it poses a number of challenges in terms of security and privacy protection. Artificial Intelligence (AI) based technologies, such as machine learning statistics, big data analysis, deep learning and so on, have been used to deal with cyber security threats. These technologies are used for intrusion detection systems, malicious software detection, and encrypted communications. In the rapidly growing field of AI driven security, scientists from multiple disciplines work together to combat cyber threats. AI models require unique cyber security defence and protection technologies. This survey provides various method, different datasets and methodologies that may be used for the proposed IA enabled cyber security technologies. This study aims to classify the AI-based cyber security solutions gathered and describe how they can help solve problems in the field of cyber security.","",""
17,"V. Gromyko, V. Kazaryan, N. Vasilyev, A. Simakin, S. Anosov","Artificial Intelligence as Tutoring Partner for Human Intellect",2017,"","","","",105,"2022-07-13 09:33:26","","10.1007/978-3-319-67349-3_22","","",,,,,17,3.40,3,5,5,"","",""
169,"Amisha, Paras Malik, Monika Pathania, V. Rathaur","Overview of artificial intelligence in medicine",2019,"","","","",106,"2022-07-13 09:33:26","","10.4103/jfmpc.jfmpc_440_19","","",,,,,169,56.33,42,4,3,"Background: Artificial intelligence (AI) is the term used to describe the use of computers and technology to simulate intelligent behavior and critical thinking comparable to a human being. John McCarthy first described the term AI in 1956 as the science and engineering of making intelligent machines. Objective: This descriptive article gives a broad overview of AI in medicine, dealing with the terms and concepts as well as the current and future applications of AI. It aims to develop knowledge and familiarity of AI among primary care physicians. Materials and Methods: PubMed and Google searches were performed using the key words 'artificial intelligence'. Further references were obtained by cross-referencing the key articles. Results: Recent advances in AI technology and its current applications in the field of medicine have been discussed in detail. Conclusions: AI promises to change the practice of medicine in hitherto unknown ways, but many of its practical applications are still in their infancy and need to be explored and developed better. Medical professionals also need to understand and acclimatize themselves with these advances for better healthcare delivery to the masses.","",""
167,"C. Langlotz, Bibb Allen, B. Erickson, Jayashree Kalpathy-Cramer, K. Bigelow, T. Cook, A. Flanders, M. Lungren, D. Mendelson, J. Rudie, Ge Wang, K. Kandarpa","A Roadmap for Foundational Research on Artificial Intelligence in Medical Imaging: From the 2018 NIH/RSNA/ACR/The Academy Workshop.",2019,"","","","",107,"2022-07-13 09:33:26","","10.1148/radiol.2019190613","","",,,,,167,55.67,17,12,3,"Imaging research laboratories are rapidly creating machine learning systems that achieve expert human performance using open-source methods and tools. These artificial intelligence systems are being developed to improve medical image reconstruction, noise reduction, quality assurance, triage, segmentation, computer-aided detection, computer-aided classification, and radiogenomics. In August 2018, a meeting was held in Bethesda, Maryland, at the National Institutes of Health to discuss the current state of the art and knowledge gaps and to develop a roadmap for future research initiatives. Key research priorities include: 1, new image reconstruction methods that efficiently produce images suitable for human interpretation from source data; 2, automated image labeling and annotation methods, including information extraction from the imaging report, electronic phenotyping, and prospective structured image reporting; 3, new machine learning methods for clinical imaging data, such as tailored, pretrained model architectures, and federated machine learning methods; 4, machine learning methods that can explain the advice they provide to human users (so-called explainable artificial intelligence); and 5, validated methods for image de-identification and data sharing to facilitate wide availability of clinical imaging data sets. This research roadmap is intended to identify and prioritize these needs for academic research laboratories, funding agencies, professional societies, and industry.","",""
0,"D. Anderlini, Luigi F. Agnati, D. Guidolin, M. Marcoli, Amina S. Woods, G. Maura","From Gilgamesh’s quest for immortality to everlasting cloud hyper-collective mind: ethical implications for artificial intelligence",2022,"","","","",108,"2022-07-13 09:33:26","","10.1108/gkmc-08-2021-0130","","",,,,,0,0.00,0,6,1," Purpose This conceptual paper aims to explore the possibility of human beings reaching a virtual form of immortality.   Design/methodology/approach The paper is an investigation of the path from an early example of human knowledge to the birth of artificial intelligence (AI) and robots. A critical analysis of different point of views, from philosophers to scientists, is presented.   Findings From ancient rock art paintings to the moon landing, human knowledge has made a huge progress to the point of creating robots resembling human features. While these humanoid robots can successfully undertake risky tasks, they also generate ethical issues for the society they interact with.   Research limitations/implications The paper is conceptual, and it does attempt to provide one theory by which human beings can achieve the dream of immortality. It is part of a work in progress on the use of AI and the issues related to the creation/use of humanoid robots in society.   Originality/value This paper provides an overview of some of the key issues and themes impacting our modern society. Its originality resides in the linking of human knowledge to collective knowledge and then of collective mind to the hyper-collective mind. The idea of humans reaching immortality is burdened by the imperative need to define ethical guidelines for the field of AI and its uses. ","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",109,"2022-07-13 09:33:26","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
0,"Beilei Wang, Jie Jing, Xiaochun Huang, Cheng Hua, Qin Qin, Y. Jia, Zhiyong Wang, Lei Jiang, Bai Gao, Les J. Wu, Xianfei Zeng, Fubo Wang, Chuanbin Mao, Shanrong Liu","Establishment of a Knowledge‐and‐Data‐Driven Artificial Intelligence System with Robustness and Interpretability in Laboratory Medicine",2022,"","","","",110,"2022-07-13 09:33:26","","10.1002/aisy.202100204","","",,,,,0,0.00,0,14,1,"Laboratory medicine plays an important role in clinical diagnosis. However, no laboratory‐based artificial intelligence (AI) diagnostic system has been applied in current clinical practice due to the lack of robustness and interpretability. Although many attempts have been made, it is still difficult for doctors to adopt the existing machine learning (ML) patterns in interpreting laboratory (lab) big data. Here, a knowledge‐and‐data‐driven laboratory diagnostic system is developed, termed AI‐based Lab tEst tO diagNosis (AI LEON), by integrating an innovative knowledge graph analysis framework and “mixed XGboost and Genetic Algorithm (MiXG)” technique to simulate the doctor's laboratory‐based diagnosis. To establish AI LEON, we included 89 116 949 laboratory data and 10 423 581 diagnosis data points from 730 113 participants. Among them, 686 626 participants were recruited for training and validating purposes with the remaining for testing purposes. AI LEON automatically identified and analyzed 2071 lab indexes, resulting in multiple disease recommendations that involved 441 common diseases in ten organ systems. AI LEON exhibited outstanding transparency and interpretability in three universal clinical application scenarios and outperformed human physicians in interpreting lab reports. AI LEON is an advanced intelligent system that enables a comprehensive interpretation of lab big data, which substantially improves the clinical diagnosis.","",""
0,"A. Campbell, R. Smith, B. Petersen, L. Moore, A. Khan, A. Barrie","O-125 Application of artificial intelligence using big data to devise and train a machine learning model on over 63,000 human embryos to automate time-lapse embryo annotation",2022,"","","","",111,"2022-07-13 09:33:26","","10.1093/humrep/deac105.025","","",,,,,0,0.00,0,6,1,"      Can a machine learning (ML) model, developed using modern neural network architecture produce comparable annotation data; utilisable for algorithmic outcome prediction, to manual time-lapse annotations?        The model automatically annotated unseen embryos with comparable results to manual methods, generating morphokinetic data to enable comparably predictive outputs from an embryo selection algorithm.        The application of artificial intelligence across healthcare industries, including fertility, is increasing. Several ML models are available that seek to generate or analyse embryo images and morphokinetic data, and to determine embryo viability potential. Along with photographic images, the use of time-lapse in IVF laboratories has amassed numeric data, resulting predominantly from annotated manual assessment of images over time. Embryo annotation practice is variable in quality, can be subjective and is time-consuming; commonly taking several minutes per embryo. The development of rapid, accurate automatic annotation would represent a significant time-saving as well as an increase in reproducibility and accuracy.        Multicentre quality assured annotation data from 63,383 time-lapse monitored embryos (EmbryoScope®), comprising over 400 million individual images, were used to train a ML model to automatically generate morphokinetic annotations. Data was derived from 8 UK clinics within a cohesive group between 2012-2021. Accuracy was assessed using 900 unseen embryos (with live birth outcome) by comparing the output of an established in-house, prospectively validated embryo selection model when the input was either ML-automated, or manual annotations.        Multi-focal plane images were processed on the Azure cloud (Microsoft) and resampled to 300x300 pixels. A Laplacian-based focal stacking algorithm merged frames into a single image. The model consisted of an EfficientNetB4 Convolutional Neural Network classifier to extract features and classify the stage of embryo images. A Temporal Convolutional Network  interpreted a time-series of image features; producing annotations from pronuclear fading through to blastocyst. Soft localisation loss function used QA data to integrate annotation subjectivities.        The ML model rapidly and automatically generated annotations. Efficacy and comparability of the ML model to automate reliable, utilisable annotations was demonstrated by comparison with manual annotation data and the ML model’s ability to auto-generate annotations which could be used to predict live birth by providing annotation data to an established, validated in house embryo selection model. Live birth-predictive capability was measured, and benchmarked against manual annotation, using the area under the receiver operating characteristic curve (AUC).  When tested on time-lapse images, collected from pronuclear fading to full blastulation, representing 900 previously unseen, transferred blastocysts where live birth outcomes were blinded, the in-house developed auto-annotation ML model resulted in an AUC of 0.686 compared with 0.661 for manual annotations, for live birth prediction.  Auto annotation using the developed model took only milliseconds to complete per embryo. The developed auto-annotation model, built and tested on large data, is considered suitable for productionisation with the aim of being validated and integrated into an application to support IVF laboratory practice.        Whilst this model was trained to recognise key morphokinetic events, there are other morphokinetic variables that may be useful in the prediction of live birth and further improve embryo selection, or deselection, ability. Akin to manual interpretation, some embryos may fail to be annotated or need second opinion.        There is increasing evidence supporting the application of ML to utilise big data from time-lapse imaging and fertility care generally. Whilst promising benefits to IVF clinics and patients, responsible use of data is required alongside large high-quality datasets, and rigorous validation, to ensure safe and robust applications.        N/A ","",""
6,"A. D. W. Sumari, A. S. Ahmad","Knowledge-growing system: The origin of the cognitive artificial intelligence",2017,"","","","",112,"2022-07-13 09:33:26","","10.1109/ICEEI.2017.8312382","","",,,,,6,1.20,3,2,5,"Knowledgde-Growing System (KGS) has been there for a while since invented in 2009. Since some real-life problems have been approached by applying KGS and prospective results show the benefits of using it especially for cases in decision making and estimating or predicting the probability of occurrence of a phenomenon being observed by the system. It is also good to be applied to obtain second opinion or alternative decision for decision maker regarding a phenomenon being observed. The excellence of KGS is its mechanism which mimics how human thinks, in this case is how brain grows the knowledge based on the information it receives from the sensory organs. Thinking is a cognitive process which is not easy to be emulated to a system. In this paper we share our endeavor in building new perspective in Artificial Intelligence called Cognitive Artificial Intelligence (CAI) where KGS is the primary example of our achievement.","",""
0,"S. Sadeghi, M. Amiri, Farzaneh Mansoori Mooseloo","Artificial Intelligence and Its Application in Optimization under Uncertainty",2021,"","","","",113,"2022-07-13 09:33:26","","10.5772/intechopen.98628","","",,,,,0,0.00,0,3,1,"Nowadays, the increase in data acquisition and availability and complexity around optimization make it imperative to jointly use artificial intelligence (AI) and optimization for devising data-driven and intelligent decision support systems (DSS). A DSS can be successful if large amounts of interactive data proceed fast and robustly and extract useful information and knowledge to help decision-making. In this context, the data-driven approach has gained prominence due to its provision of insights for decision-making and easy implementation. The data-driven approach can discover various database patterns without relying on prior knowledge while also handling flexible objectives and multiple scenarios. This chapter reviews recent advances in data-driven optimization, highlighting the promise of data-driven optimization that integrates mathematical programming and machine learning (ML) for decision-making under uncertainty and identifies potential research opportunities. This chapter provides guidelines and implications for researchers, managers, and practitioners in operations research who want to advance their decision-making capabilities under uncertainty concerning data-driven optimization. Then, a comprehensive review and classification of the relevant publications on the data-driven stochastic program, data-driven robust optimization, and data-driven chance-constrained are presented. This chapter also identifies fertile avenues for future research that focus on deep-data-driven optimization, deep data-driven models, as well as online learning-based data-driven optimization. Perspectives on reinforcement learning (RL)-based data-driven optimization and deep RL for solving NP-hard problems are discussed. We investigate the application of data-driven optimization in different case studies to demonstrate improvements in operational performance over conventional optimization methodology. Finally, some managerial implications and some future directions are provided.","",""
1,"Shengtao Dong, J. Li, Haozong Zhao, Yu-Zhen Zheng, Yaoning Chen, Junxi Shen, Huan Yang, Jieyang Zhu","Risk Factor Analysis for Predicting the Onset of Rotator Cuff Calcific Tendinitis Based on Artificial Intelligence",2022,"","","","",114,"2022-07-13 09:33:26","","10.1155/2022/8978878","","",,,,,1,1.00,0,8,1,"Background Symptomatic rotator cuff calcific tendinitis (RCCT) is a common shoulder disorder, and approaches combined with artificial intelligence greatly facilitate the development of clinical practice. Current scarce knowledge of the onset suggests that clinicians may need to explore this disease thoroughly. Methods Clinical data were retrospectively collected from subjects diagnosed with RCCT at our institution within the period 2008 to 2020. A standardized questionnaire related to shoulder symptoms was completed in all cases, and standardized radiographs of both shoulders were extracted using a human-computer interactive electronic medical system (EMS) to clarify the clinical diagnosis of symptomatic RCCT. Based on the exclusion of asymptomatic subjects, risk factors in the baseline characteristics significantly associated with the onset of symptomatic RCCT were assessed via stepwise logistic regression analysis. Results Of the 1,967 consecutive subjects referred to our academic institution for shoulder discomfort, 237 were diagnosed with symptomatic RCCT (12.05%). The proportion of women and the prevalence of clinical comorbidities were significantly higher in the RCCT cohort than those in the non-RCCT cohort. Stepwise logistic regression analysis confirmed that female gender, hyperlipidemia, diabetes mellitus, and hypothyroidism were independent risk factors for the entire cohort. Stratified by gender, the study found a partial overlap of risk factors contributing to morbidity in men and women. Diagnosis of hyperlipidemia, diabetes mellitus, and hypothyroidism in male cases and diabetes mellitus in female cases were significantly associated with symptomatic RCCT. Conclusion Independent predictors of symptomatic RCCT are female, hyperlipidemia, diabetes mellitus, and hypothyroidism. Men diagnosed with hyperlipidemia, diabetes mellitus, and hypothyroidism are at high risk for symptomatic RCCT, while more medical attention is required for women with diabetes mellitus. Artificial intelligence offers pioneering innovations in the diagnosis and treatment of musculoskeletal disorders, and careful assessment through individualized risk stratification can help predict onset and targeted early stage treatment.","",""
0,"Alexandre Ardichvili","The Impact of Artificial Intelligence on Expertise Development: Implications for HRD",2022,"","","","",115,"2022-07-13 09:33:26","","10.1177/15234223221077304","","",,,,,0,0.00,0,1,1,"Problem The implementation of artificial intelligence (AI) is assumed to lead to increased productivity of knowledge workers. However, AI could also have negative effects on the development of professional expertise. Solution A review of the literature on expertise development is provided, followed by examples of AI implementation in a knowledge-intensive profession, accounting. The analysis of these examples suggests that automation can result in the loss of expertise due to reduced opportunities for learning from deliberate practice and experienced colleagues, and from working on progressively more complex tasks. Implications for human resource development (HRD) include creating alternative individual development opportunities and promoting organizational cultures conducive to expertise development in human-machine interaction modes. Stakeholders The results of this study will be of interest to scholars of HRD, accounting education, and human-machine interaction. Practical implications will be of relevance to HRD professionals and managers responsible for the implementation of artificial intelligence solutions.","",""
0,"R. Philipsen, P. Brauner, Hannah Biermann, M. Ziefle","I Am What I Am – Roles for Artificial Intelligence from the Users’ Perspective",2022,"","","","",116,"2022-07-13 09:33:26","","10.54941/ahfe1001453","","",,,,,0,0.00,0,4,1,"With increasing digitization, intelligent software systems are taking over more tasks in everyday human life, both in private and professional contexts. So-called artificial intelligence (AI) ranges from subtle and often unnoticed improvements in daily life, optimizations in data evaluation, assistance systems with which the people interact directly, to perhaps artificial anthropomorphic entities in the future. How-ever, no etiquette yet exists for integrating AI into the human living environment, which has evolved over millennia for human interaction. This paper addresses what roles AI may take, what knowledge AI may have, and how this is influenced by user characteristics. The results show that roles with personal relationships, such as an AI as a friend or partner, are not preferred by users. The higher the confidence in an AI's handling of data, the more likely personal roles are seen as an option for the AI, while the preference for subordinate roles, such as an AI as a servant or a subject, depends on general technology acceptance and belief in a dangerous world. The role attribution is independent from the usage intention and the semantic perception of artificial intelligence, which differs only slightly, e.g., in terms of morality and controllability, from the perception of human intelligence.","",""
81,"M. Coeckelbergh","Artificial Intelligence, Responsibility Attribution, and a Relational Justification of Explainability",2019,"","","","",117,"2022-07-13 09:33:26","","10.1007/s11948-019-00146-8","","",,,,,81,27.00,81,1,3,"","",""
27,"N. S. Saravana Kumar","IMPLEMENTATION OF ARTIFICIAL INTELLIGENCE IN IMPARTING EDUCATION AND EVALUATING STUDENT PERFORMANCE",2019,"","","","",118,"2022-07-13 09:33:26","","10.36548/jaicn.2019.1.001","","",,,,,27,9.00,27,1,3,"Simulation of human intelligence process is made possible with the help of artificial intelligence. The learning, reasoning and self-correction properties are made possible in computer systems. Along with AI, other technologies are combined effectively in order to create remarkable applications. We apply the changing role of AI and its techniques in new educational paradigms to create a personalised teaching-learning environment. Features like recognition, pattern matching, decision making, reasoning, problem solving and so on are applied along with knowledge based system and supervised machine learning for a complete learning and assessment process.","",""
1,"D. Handelman, Corban G. Rivera, R. St. Amant, Emma Holmes, Andrew R. Badger, Bryanna Y. Yeh","Adaptive human-robot teaming through integrated symbolic and subsymbolic artificial intelligence: preliminary results",2022,"","","","",119,"2022-07-13 09:33:26","","10.1117/12.2618686","","",,,,,1,1.00,0,6,1,"As the autonomy of intelligent systems continues to increase, the ability of humans to maintain control over machine behavior, work effectively in concert with them, and trust them, becomes paramount. Ideally, a machine’s plan of action would be accessible to and understandable by human team members, and machine behavior would be modifiable in real time, in the field, to accommodate unanticipated situations. The ability of machines to adapt to new situations quickly and reliably based on both human input and autonomous learning has the potential to enhance numerous human-machine teaming scenarios. Our research focuses on the question, “Can robots become competent and adaptive teammates by emulating human skill acquisition strategies?” In this paper we describe the Robotic Skill Acquisition (RSA) cognitive architecture and show preliminary results of teaming experiments involving a human wearing an augmented reality headset and a quadruped robot performing tasks related to reconnaissance. The goal is to combine instruction and discovery by integrating declarative symbolic AI and reflexive neural network learning to produce robust, explainable and trusted robot behavior, adjustable autonomy, and adaptive human-robot teaming. Humans and robots start with a playbook of modifiable hierarchical task descriptions that encode explicit task knowledge. Neural network based feedback error learning enables human-directed behavior shaping, and reinforcement learning enables discovery of novel subtask control strategies. It is anticipated that modifications to and transitions between symbolic and subsymbolic processing will enable highly adaptive behavior in support of enhanced situational awareness and operational effectiveness of human-robot teams.","",""
52,"R. Fjelland","Why general artificial intelligence will not be realized",2020,"","","","",120,"2022-07-13 09:33:26","","10.1057/s41599-020-0494-4","","",,,,,52,26.00,52,1,2,"","",""
27,"R. Abdullah, Bahjat Fakieh","Health Care Employees’ Perceptions of the Use of Artificial Intelligence Applications: Survey Study",2020,"","","","",121,"2022-07-13 09:33:26","","10.2196/17620","","",,,,,27,13.50,14,2,2,"Background The advancement of health care information technology and the emergence of artificial intelligence has yielded tools to improve the quality of various health care processes. Few studies have investigated employee perceptions of artificial intelligence implementation in Saudi Arabia and the Arabian world. In addition, limited studies investigated the effect of employee knowledge and job title on the perception of artificial intelligence implementation in the workplace. Objective The aim of this study was to explore health care employee perceptions and attitudes toward the implementation of artificial intelligence technologies in health care institutions in Saudi Arabia. Methods An online questionnaire was published, and responses were collected from 250 employees, including doctors, nurses, and technicians at 4 of the largest hospitals in Riyadh, Saudi Arabia. Results The results of this study showed that 3.11 of 4 respondents feared artificial intelligence would replace employees and had a general lack of knowledge regarding artificial intelligence. In addition, most respondents were unaware of the advantages and most common challenges to artificial intelligence applications in the health sector, indicating a need for training. The results also showed that technicians were the most frequently impacted by artificial intelligence applications due to the nature of their jobs, which do not require much direct human interaction. Conclusions The Saudi health care sector presents an advantageous market potential that should be attractive to researchers and developers of artificial intelligence solutions.","",""
38,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases.",2020,"","","","",122,"2022-07-13 09:33:26","","10.1038/s41746-020-0229-3","","",,,,,38,19.00,6,6,2,"","",""
45,"Tom Kamiel Magda Vercauteren, M. Unberath, N. Padoy, N. Navab","CAI4CAI: The Rise of Contextual Artificial Intelligence in Computer-Assisted Interventions",2019,"","","","",123,"2022-07-13 09:33:26","","10.1109/JPROC.2019.2946993","","",,,,,45,15.00,11,4,3,"Data-driven computational approaches have evolved to enable extraction of information from medical images with reliability, accuracy, and speed, which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theaters are extremely complex and typically rely on poorly integrated intraoperative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer-assisted interventions, we highlight the crucial need to take the context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer-assisted intervention (CAI4CAI) arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors, and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human–AI actor team; and how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision-making ultimately producing more precise and reliable interventions.","",""
19,"M. Komorowski","Clinical management of sepsis can be improved by artificial intelligence: yes",2019,"","","","",124,"2022-07-13 09:33:26","","10.1007/s00134-019-05898-2","","",,,,,19,6.33,19,1,3,"","",""
32,"André Renz, Romy Hilbig","Prerequisites for artificial intelligence in further education: identification of drivers, barriers, and business models of educational technology companies",2020,"","","","",125,"2022-07-13 09:33:26","","10.1186/s41239-020-00193-3","","",,,,,32,16.00,16,2,2,"","",""
3,"T. Kaur, Anirudra Diwakar, Kirandeep, Pranav Mirpuri, M. Tripathi, P. Chandra, T. Gandhi","Artificial Intelligence in Epilepsy",2021,"","","","",126,"2022-07-13 09:33:26","","10.4103/0028-3886.317233","","",,,,,3,3.00,0,7,1,"Background: The study of seizure patterns in electroencephalography (EEG) requires several years of intensive training. In addition, inadequate training and human error may lead to misinterpretation and incorrect diagnosis. Artificial intelligence (AI)-based automated seizure detection systems hold an exciting potential to create paradigms for proper diagnosis and interpretation. AI holds the promise to transform healthcare into a system where machines and humans can work together to provide an accurate, timely diagnosis, and treatment to the patients. Objective: This article presents a brief overview of research on the use of AI systems for pattern recognition in EEG for clinical diagnosis. Material and Methods: The article begins with the need for understanding nonstationary signals such as EEG and simplifying their complexity for accurate pattern recognition in medical diagnosis. It also explains the core concepts of AI, machine learning (ML), and deep learning (DL) methods. Results and Conclusions: In this present context of epilepsy diagnosis, AI may work in two ways; first by creating visual representations (e.g., color-coded paradigms), which allow persons with limited training to make a diagnosis. The second is by directly explaining a complete automated analysis, which of course requires more complex paradigms than the previous one. We also clarify that AI is not about replacing doctors and strongly emphasize the need for domain knowledge in building robust AI models that can work in real-time scenarios rendering good detection accuracy in a minimum amount of time.","",""
0,"Lauren M. Sanders, Jason H. Yang, Ryan T Scott, A. Qutub, H. Martín, D. Berrios, Jaden J. A. Hastings, J. Rask, Graham Mackintosh, A. Hoarfrost, Stuart Chalk, John Kalantari, K. Khezeli, E. Antonsen, Joel Babdor, Richard Barker, S. Baranzini, A. Beheshti, Guillermo M. Delgado-Aparicio, B. Glicksberg, C. Greene, M. Haendel, Arif A. Hamid, P. Heller, Daniel Jamieson, K. Jarvis, S. Komarova, M. Komorowski, P. Kothiyal, A. Mahabal, U. Manor, Christopher E. Mason, Mona Matar, G. Mias, Jack M. Miller, J. Myers, Charlotte A Nelson, Jonathan Oribello, Seung-min Park, P. Parsons-Wingerter, R. Prabhu, R. Reynolds, Amanda M. Saravia-Butler, S. Saria, A. Sawyer, N. Singh, Frank Soboczenski, Michael Snyder, Karthik Soman, C. Theriot","Beyond Low Earth Orbit: Biological Research, Artificial Intelligence, and Self-Driving Labs",2021,"","","","",127,"2022-07-13 09:33:26","","","","",,,,,0,0.00,0,50,1,"Space biology research aims to understand fundamental effects of spaceflight on organisms, develop foundational knowledge to support deep space exploration, and ultimately bioengineer spacecraft and habitats to stabilize the ecosystem of plants, crops, microbes, animals, and humans for sustained multi-planetary life. To advance these aims, the field leverages experiments, platforms, data, and model organisms from both spaceborne and ground-analog studies. As research is extended beyond low Earth orbit, experiments and platforms must be maximally autonomous, light, agile, and intelligent to expedite knowledge discovery. Here we present a summary of recommendations from a workshop organized by the National Aeronautics and Space Administration on artificial intelligence, machine learning, and modeling applications which offer key solutions toward these space biology challenges. In the next decade, the synthesis of artificial intelligence into the field of space biology will deepen the biological understanding of spaceflight effects, facilitate predictive modeling and analytics, support maximally autonomous and reproducible experiments, and efficiently manage spaceborne data and metadata, all with the goal to enable life to thrive in deep space.","",""
0,"Nigamanth Sridhar, Li Yang, J. Joshi, Victor P. Piotrowski","Cybersecurity Education in the Age of Artificial Intelligence",2021,"","","","",128,"2022-07-13 09:33:26","","10.1145/3408877.3439525","","",,,,,0,0.00,0,4,1,"The 2019 Federal Cybersecurity Research and Development Strategic Plan highlighted the mutual needs and benefits of artificial intelligence (AI) and cybersecurity. AI techniques are expected to enhance cybersecurity by assisting human system managers with automated monitoring, analysis, and responses to cybersecurity attacks. Conversely, it is essential to guard AI technologies from unintended uses and hostile exploitation by leveraging cybersecurity practices. Research results at the intersection of AI and cybersecurity can help us to be better equipped with tools and techniques to tackle the growing cybersecurity challenges, while also presenting an opportunity to devise fundamentally new ways to motivate and educate students about cybersecurity in the age of AI. Likewise, a June 2019 technical workshop on 'Artificial Intelligence and Cybersecurity: Opportunities and Challenges' noted how the interplay between AI, machine learning, and cybersecurity will continue to introduce new opportunities and challenges in the security of AI as well as AI for cybersecurity. Basic research at the intersection of AI, cybersecurity, and education has the potential to expand existing AI opportunities and resources in cybersecurity education and workforce development. Education efforts are needed to foster workforce knowledge and skills about applying AI expertise to cybersecurity as well as building robust and trustworthy AI. This BOF session will bring together researchers who are interested in these collaborative explorations.","",""
0,"R. Hariharan, P. He, C. Hickman, J. Chambost, C. Jacques, M. Hentschke, B. Cunegatto, C. Dutra, A. Drakeley, Q. Zhan, R. Miller, G. Verheyen, M. Rosselot, S. Loubersac, K. Kelley","P–165 Using Artificial Intelligence to Classify Embryo Shape: An International Perspective",2021,"","","","",129,"2022-07-13 09:33:26","","10.1093/humrep/deab130.164","","",,,,,0,0.00,0,15,1,"      Is a pre-trained machine learning algorithm able to accurately detect cellular arrangement in 4-cell embryos from a different continent?        Artificial Intelligence (AI) analysis of 4-cell embryo classification is transferable across clinics globally with 79% accuracy.        Previous studies observing four-cell human embryo configurations have demonstrated that non-tetrahedral embryos (embryos in which cells make contact with fewer than 3 other cells) are associated with compromised blastulation and implantation potential. Previous research by this study group has indicated the efficacy of AI models in classification of tetrahedral and non-tetrahedral embryos with 87% accuracy, with a database comprising 2 clinics both from the same country (Brazil). This study aims to evaluate the transferability and robustness of this model on blind test data from a different country (France).        The study was a retrospective cohort analysis in which 909 4-cell embryo images (“tetrahedral”, n = 749; “non-tetrahedral”, n = 160) were collected from 3 clinics (2 Brazilian, 1 French). All embryos were captured at the central focal plane using Embryoscope™ time-lapse incubators. The training data consisted solely of embryo images captured in Brazil (586 tetrahedral; 87 non-tetrahedral) and the test data consisted exclusively of embryo images captured in France (163 tetrahedral; 72 non-tetrahedral).        The embryo images were labelled as either “tetrahedral” or “non-tetrahedral” at their respective clinics. Annotations were then validated by three operators. A ResNet–50 neural network model pretrained on ImageNet was fine-tuned on the training dataset to predict the correct annotation for each image. We used the cross entropy loss function and the RMSprop optimiser (lr = 1e–5). Simple data augmentations (flips and rotations) were used during the training process to help counteract class imbalances.        Our model was capable of classifying embryos in the blind French test set with 79% accuracy when trained with the Brazilian data. The model had sensitivity of 91% and 51% for tetrahedral and non-tetrahedral embryos respectively; precision was 81% and 73%; F1 score was 86% and 60%; and AUC was 0.61 and 0.64. This represents a 10% decrease in accuracy compared to when the model both trained and tested on different data from the same clinics.        Although strict inclusion and exclusion criteria were used, inter-operator variability may affect the pre-processing stage of the algorithm. Moreover, as only one focal plane was used, ambiguous cases were interpoloated and further annotated. Analysing embryos at multiple focal planes may prove crucial in improving the accuracy of the model.  Wider implications of the findings: Though the use of machine learning models in the analysis of embryo imagery has grown in recent years, there has been concern over their robustness and transferability. While previous results have demonstrated the utility of locally-trained models, our results highlight the potential for models to be implemented across different clinics.        Not applicable ","",""
26,"Yingxu Wang, W. Kinsner, S. Kwong, Henry Leung, Jianhua Lu, Michael H. Smith, L. Trajković, E. Tunstel, K. Plataniotis, G. Yen","Brain-Inspired Systems: A Transdisciplinary Exploration on Cognitive Cybernetics, Humanity, and Systems Science Toward Autonomous Artificial Intelligence",2020,"","","","",130,"2022-07-13 09:33:26","","10.1109/MSMC.2018.2889502","","",,,,,26,13.00,3,10,2,"Brain-inspired cognitive systems (BCSs) are an emerging field of cybernetics, cognitive science, and system science. BCSs study not only the intelligence science foundations of artificial intelligence (AI) and cognitive systems, but also formal models of the brain embodied by computational intelligence. This article presents the brain and intelligence science foundations of BCS toward hybrid intelligent systems and the symbiotic intelligence of humanity. It explores the transdisciplinary theoretical foundations of system, brain, intelligence, knowledge, cybernetic, and cognitive sciences toward the next generation of knowledge processors beyond classic data processors for autonomous computing systems. A BCS provides an overarching platform for cognitive cybernetics, humanity, and systems to enable emerging hybrid societies shared by humans and intelligent machines.","",""
27,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases",2020,"","","","",131,"2022-07-13 09:33:26","","10.1038/s41746-020-0229-3","","",,,,,27,13.50,5,6,2,"","",""
63,"S. Strohmeier, F. Piazza","Artificial Intelligence Techniques in Human Resource Management - A Conceptual Exploration",2015,"","","","",132,"2022-07-13 09:33:26","","10.1007/978-3-319-17906-3_7","","",,,,,63,9.00,32,2,7,"","",""
11,"K. Mudgal, Neelanjan Das","The ethical adoption of artificial intelligence in radiology",2019,"","","","",133,"2022-07-13 09:33:26","","10.1259/bjro.20190020","","",,,,,11,3.67,6,2,3,"Artificial intelligence (AI) is rapidly transforming healthcare—with radiology at the pioneering forefront. To be trustfully adopted, AI needs to be lawful, ethical and robust. This article covers the different aspects of a safe and sustainable deployment of AI in radiology during: training, integration and regulation. For training, data must be appropriately valued, and deals with AI companies must be centralized. Companies must clearly define anonymization and consent, and patients must be well-informed about their data usage. Data fed into algorithms must be made AI-ready by refining, purification, digitization and centralization. Finally, data must represent various demographics. AI needs to be safely integrated with radiologists-in-the-loop: guiding forming concepts of AI solutions and supervising training and feedback. To be well-regulated, AI systems must be approved by a health authority and agreements must be made upon liability for errors, roles of supervised and unsupervised AI and fair workforce distribution (between AI and radiologists), with a renewal of policy at regular intervals. Any errors made must have a root-cause analysis, with outcomes fedback to companies to close the loop—thus enabling a dynamic best prediction system. In the distant future, AI may act autonomously with little human supervision. Ethical training and integration can ensure a ""transparent"" technology that will allow insight: helping us reflect on our current understanding of imaging interpretation and fill knowledge gaps, eventually moulding radiological practice. This article proposes recommendations for ethical practise that can guide a nationalized framework to build a sustainable and transparent system.","",""
484,"M. Komorowski, L. Celi, O. Badawi, A. Gordon, Aldo A. Faisal","The Artificial Intelligence Clinician learns optimal treatment strategies for sepsis in intensive care",2018,"","","","",134,"2022-07-13 09:33:26","","10.1038/s41591-018-0213-5","","",,,,,484,121.00,97,5,4,"","",""
1,"L. Bori, M. Valera, D. Gilboa, R. Maor, I. Kottel, J. Remohi, D. Seidman, M. Meseguer","O-084 Computer vision can distinguish between euploid and aneuploid embryos. A novel artificial intelligence (AI) approach to measure cell division activity associated with chromosomal status",2021,"","","","",135,"2022-07-13 09:33:26","","10.1093/humrep/deab125.014","","",,,,,1,1.00,0,8,1,"      Can we distinguish between top-grade euploid and aneuploid embryos by AI measurement of cell edges in time-lapse videos?        Aneuploid embryos can be distinguished from euploid embryos by AI determination of a longer time to blastulation and higher cell activity.        Continuous monitoring of the embryo development has brought out morphokinetic parameters that are used to predict pre-implantation genetic testing (PGT) results. Previous publications showed that euploid embryos reach blastulation earlier than non-euploid embryos. However, time-lapse data are currently under-utilized in making predictions about embryo chromosomal content. AI and computer vision could take advantage of the massive amount of data embedded in the images of embryo development. This is the first attempt to distinguish between euploid and aneuploid embryos by computer vision in an objective and indirect way based on the measurement of cell edges as a proxy for cell activity.        We performed a retrospective analysis of 1,314 time-lapse videos from embryos cultured to the blastocyst stage with PGT results. This single-center study involved two phases; a comparison of the start time of blastulation between euploid (n = 544) and aneuploid embryos (n = 797). In phase two, we designed a novel methodology to examine whether precise measurement of cell edges over time could reflect cell activity differences in blastulation.        We assumed that the delay in blastulation is reflected by higher cell activity that could be determined accurately for the first time using computer vision and machine learning to measure the length of the edges (from t2 to t8). We compared computer vision based measurements of cell edges, reflecting cell number and size, in videos of 231 top-grade euploid (n = 111) and aneuploid (n = 120) embryos.        The mean and standard deviation of blastulation start time was 100.1±6.8 h for euploid embryos and 101.8±8.2 h for aneuploid embryos (p < 0.001). Regarding the measurement of cell activity, a computer vision algorithm identified the edges and provided a certainty score for each edge, higher when the algorithm is more certain that this is a cell edge (as opposed to noise in the images). A threshold was set to distinguish cell edges from noise using this score. The following results for top-grade embryos are shown as the sum of the edge lengths (µm) average of 160 pictures per embryo (frames between t2 and t8). The total length of the cell edges increased from two cells (420±85 µm) to eight cells (861±237 µm), in line with the mitosis events. Both the average total edge measured (450±162 µm for euploid embryos and 489±215 µm for aneuploid embryos, p < 0.01) and the average total of the difference between consecutive frames (135±47 µm for euploid embryos and 153±64 µm for aneuploid embryos, p < 0.01) were higher for aneuploid embryos than for euploid embryos. A regression model to differentiate between the two classes achieved 73% sensitivity and 73% specificity on this dataset.        The main limitation of this study is the difficulty to correlate our findings to other measure of cell activity. A more robust AI function (using not only cell edges lengths) would be required for future analysis to measure the cell activity in cell division up to the blastocyst stage.        Our results show for the first time that an AI based system can precisely measure microscopic cell edges in the dividing embryo. Using this novel method, we could distinguish between euploid and aneuploid embryos. This non-invasive method could further enhance our knowledge of the developing embryo.        Not Applicable ","",""
0,"","Neural Network Training Using Genetic Algorithms Series In Machine Perception And Artificial Intelligence",2021,"","","","",136,"2022-07-13 09:33:26","","","","",,,,,0,0.00,0,0,1,"Knowledge-Based Intelligent Information and Engineering Systems 2Nature-inspired Methods in Chemometrics: Genetic Algorithms and Artificial Neural NetworksParallel Implementations of Backpropagation Neural Networks on TransputersEvolutionary Algorithms and Neural NetworksTraining Neural Networks Using Hybrids with Genetic AlgorithmsNeural Network Training Using Genetic AlgorithmsGene Expression ProgrammingTraining a Neural Network with a Genetic AlgorithmMethods and Applications of Artificial IntelligenceClassification and Learning Using Genetic AlgorithmsPractical Computer Vision Applications Using Deep Learning with CNNsIntelligent Hybrid SystemsNeurogenetic LearningAdvances in Neural Networks ISNN 2007Hybrid Intelligent SystemsEncyclopedia of Computer Science and TechnologyMachine LearningUsing a Genetic Algorithm in Training an Artificial Neural Network to Implement the XOR FunctionGenetic and Evolutionary Computation — GECCO 2004Handbook of Fuzzy ComputationNeural Network Data Analysis Using SimulnetTMArtificial Neural Nets and Genetic AlgorithmsApplied Soft Computing Technologies: The Challenge of ComplexityGenetic Algorithm for Artificial Neural Network Training for the Purpose of Automated Part RecognitionNEURAL NETWORKS, FUZZY LOGIC AND GENETIC ALGORITHMAutomatic Generation of Neural Network Architecture Using Evolutionary ComputationPGANETThe Sixth International Symposium on Neural Networks (ISNN 2009)Evolutionary Machine Learning TechniquesModeling Decisions for Artificial IntelligenceEmpirical Studies on the Utility of Genetic Algorithms for Training and Designing of Neural NetworksTraining Neural Networks Using Genetic AlgorithmsTraining feedforward neural networks using genetic algorithmsMetaheuristic Procedures for Training Neural NetworksArtificial Neural Nets and Genetic AlgorithmsNature-Inspired Computing: Concepts, Methodologies, Tools, and ApplicationsApplications of Evolutionary ComputingArtificial Intelligence and CreativityArtificial Neural Nets and Genetic AlgorithmsDeep Learning Using Genetic Algorithms Creativity is one of the least understood aspects of intelligence and is often seen as `intuitive' and not susceptible to rational enquiry. Recently, however, there has been a resurgence of interest in the area, principally in artificial intelligence and cognitive science, but also in psychology, philosophy, computer science, logic, mathematics, sociology, and architecture and design. This volume brings this work together and provides an overview of this rapidly developing field. It addresses a range of issues. Can computers be creative? Can they help us to understand human creativity? How can artificial intelligence (AI) enhance human creativity? How, in particular, can it contribute to the `sciences of the artificial', such as design? Does the new wave of AI (connectionism, geneticism and artificial life) offer more promise in these areas than classical, symbol-handling AI? What would the implications be for AI and cognitive science if computers could not be creative? These issues are explored in five interrelated parts, each of which is introducted and explained by a leading figure in the field. Prologue (Margaret Boden) Part I: Foundational Issues (Terry Dartnall) Part II: Creativity and Cognition (Graeme S. Halford and Robert Levinson) Part III: Creativity and Connectionism (Chris Thornton) Part IV: Creativity and Design (John Gero) Part V: Human Creativity Enhancement (Ernest Edmonds) Epilogue (Douglas Hofstadter) For researchers in AI, cognitive science, computer science, philosophy, psychology, mathematics, logic, sociology, and architecture and design; and anyone interested in the rapidly growing field of artificial intelligence and creativity.From the contents: Neural networks – theory and applications: NNs (= neural networks) classifier on continuous data domains– quantum associative memory – a new class of neuron-like discrete filters to image processing – modular NNs for improving generalisation properties – presynaptic inhibition modelling for image processing application – NN recognition system for a curvature primal sketch – NN based nonlinear temporalspatial noise rejection system – relaxation rate for improving Hopfield network – Oja's NN and influence of the learning gain on its dynamics Genetic algorithms – theory and applications: transposition: a biological-inspired mechanism to use with GAs (= genetic algorithms) – GA for decision tree induction – optimising decision classifications using GAs – scheduling tasks with intertask communication onto multiprocessors by GAs – design of robust networks with GA – effect of degenerate coding on GAs – multiple traffic signal control using a GA – evolving musical harmonisation – niched-penalty approach for constraint handling in GAs – GA with dynamic population size – GA with dynamic niche clustering for multimodal function optimisation Soft computing and uncertainty: self-adaptation of evolutionary constructed decision trees by information spreading – evolutionary programming of near optimal NNsArtificial neural networks and genetic algorithms both are areas of research","",""
0,"S. Bandyopadhyay, R. Mukherjee, S. Sarkar","A Report on the First Workshop on Software Engineering for Artificial Intelligence (SE4AI 2020)",2020,"","","","",137,"2022-07-13 09:33:26","","10.1145/3385032.3385055","","",,,,,0,0.00,0,3,2,"With advancement in technology-driven decision making, the software-intensive systems for decisions have become more robust, dynamic, adaptive, context-aware, dependable. Architectural designs of such systems crave for new approaches where the data-driven decision making has to be incorporated in the solution. Methods for recommendation mechanism, prediction of operation failures, dealing with unsafe conditions etc are going to be part of the solution itself. Integrating such features to conceive an intelligent system that will directly influence the business solution is mostly appreciated. This would not have been possible without the direct interference of Artificial Intelligence which has been a standard procedure of industrial repertoire since 1980s. The direct impact of AI on social and economic life has been been felt mostly in last decade (since 2007) with the advent of smart phone, which contribute largely to ""big data"". The era of ""big data"" has witnessed the efficacy of Machine Learning and there is a need of the hour to combine data-driven machine intelligence with human intelligence (insights and domain knowledge) to effectively make the software development (requirement, design, testing, deployment and operation management) intelligent. The research community has shown a keen interest in this emerging field. In this report, we present a pre-organization summary of the workshop to be held on February 27, 2020, at IIIT Jabbalpur (India), co-located with the 13th Innovations in Software Engineering Conference (ISEC 2020).","",""
7,"D. G. Harkut, K. Kasat","Introductory Chapter: Artificial Intelligence - Challenges and Applications",2019,"","","","",138,"2022-07-13 09:33:26","","10.5772/INTECHOPEN.84624","","",,,,,7,2.33,4,2,3,"Artificial intelligence (AI) is any task performed by program or machine, which otherwise human needs to apply intelligence to accomplish it. It is the science and engineering of making machines to demonstrate intelligence especially visual perception, speech recognition, decision-making, and translation between languages like human beings. AI is the simulation of human intelligence processes by machines, especially computer systems. This includes learning, reasoning, planning, self-correction, problem solving, knowledge representation, perception, motion, manipulation, and creativity. It is a science and a set of computational techniques that are inspired by the way in which human beings use their nervous system and their body to feel, learn, reason, and act. AI is related to machine learning and deep learning wherein machine learning makes use of algorithms to discover patterns and generate insights from the data they are working on. Deep learning is a subset of machine learning, one that brings AI closer to the goal of enabling machines to think and work as human as possible. AI is a debatable topic and is often represented in a negative way; some would call it a blessing in disguise for businesses, while for some it is a technology that endangers the mere existence of humankind as it is potentially capable of taking over and dominating human being, but in reality artificial intelligence has affected our lifestyle either directly or indirectly and shaping the future of tomorrow. AI has already become an intrinsic part of our daily life and has greatly impacted our lifestyle despite the imperative uses of digital assistants of mobile phones, driverassistance systems, the bots, texts and speech translators, and systems that assist in recommending products and services and customized learning. Every emerging technology is a source of both enthusiasm and skepticism. AI is a source of both advantages and disadvantages in different perspectives. However, we need to overcome certain challenges before we can realize the true potential and immense transformational capabilities of this emerging technology. Some of the challenges related to artificial intelligence are:","",""
4,"T. Schmid","Deconstructing the Final Frontier of Artificial Intelligence: Five Theses for a Constructivist Machine Learning",2019,"","","","",139,"2022-07-13 09:33:26","","","","",,,,,4,1.33,4,1,3,"Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subirón 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world – and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (Méhaut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as “hypothetical constructs” (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n","",""
2,"Małgorzata Suchacka, N. Horáková","Towards Artificial Intelligence. Sociological Reflections on the Relationship Man - Organization - Device",2019,"","","","",140,"2022-07-13 09:33:26","","10.2478/czoto-2019-0116","","",,,,,2,0.67,1,2,3,"Abstract The main goal of the study will be to pay attention to technologization of the learning process and its social dimensions in the context of artificial intelligence. The reflection will mainly cover selected theories of learning and knowledge management in the organization and its broadly understood environment. Considering the sociological dimensions of these phenomena is supposed to lead to the emphasis on the importance of the security of the human-organization-device relationship. Due to the interdisciplinary nature of the issue, the article will include references to the concept of artificial intelligence and machine learning. Difficult questions will arise around the ideas and will become the conclusion of the considerations.","",""
0,"E. Cambouropoulos, Maximos A. Kaliakatsos-Papakostas","Cognitive Musicology and Artificial Intelligence: Harmonic Analysis, Learning, and Generation",2021,"","","","",141,"2022-07-13 09:33:26","","10.1007/978-3-030-72116-9_10","","",,,,,0,0.00,0,2,1,"","",""
0,"D. Mareschal, Sam Blakeman","Fast and Slow Learning in Human-Like Intelligence",2021,"","","","",142,"2022-07-13 09:33:26","","10.1093/oso/9780198862536.003.0016","","",,,,,0,0.00,0,2,1,"In this chapter we review the extent to which rapid one-short learning or fast-mapping exists in human learning. We find that it exists in both children and adults, but that it is almost always accompanied by slow consolidated learning in which new knowledge is integrated with existing knowledge-bases. Rapid learning is also present in a broad range of non-human species, particularly in the context of high reward values. We argue that reward prediction errors guide the extent to which fast or slow learning dominates, and present a Complementary Learning Systems neural network model (CTDL) of cortical/hippocampal learning that uses reward prediction errors to adjudicate between learning in the two systems. Developing human-like artificial intelligence will require implementing multiple learning and inference systems governed by a flexible control system with an equal capacity to that of human control systems.","",""
152,"P. Khosravi, Ehsan Kazemi, Q. Zhan, J. Malmsten, M. Toschi, Pantelis Zisimopoulos, Alexandros Sigaras, S. Lavery, L. Cooper, C. Hickman, M. Meseguer, Z. Rosenwaks, O. Elemento, N. Zaninovic, I. Hajirasouliha","Deep learning enables robust assessment and selection of human blastocysts after in vitro fertilization",2019,"","","","",143,"2022-07-13 09:33:26","","10.1038/s41746-019-0096-y","","",,,,,152,50.67,15,15,3,"","",""
6,"Herut Uzan, Shira Sardi, A. Goldental, R. Vardi, I. Kanter","Biological learning curves outperform existing ones in artificial intelligence algorithms",2019,"","","","",144,"2022-07-13 09:33:26","","10.1038/s41598-019-48016-4","","",,,,,6,2.00,1,5,3,"","",""
2,"Á. Alberich-Bayarri, A. Pastor, Rafael López González, Fabio García Castro","How to Develop Artificial Intelligence Applications",2019,"","","","",145,"2022-07-13 09:33:26","","10.1007/978-3-319-94878-2_5","","",,,,,2,0.67,1,4,3,"","",""
1,"B. A.","Informational Linguistics: Computer, Internet, Artificial Intelligence and Language",2019,"","","","",146,"2022-07-13 09:33:26","","","","",,,,,1,0.33,1,1,3,"Modern technological progress is clearly mediated via the informational and communicational conceptualizations, which are first of all of language nature. Interdependence and syncretism of human cognitive activity create unlimited demand for knowledge interpretation of certain semantic format – information. Informational Linguistics is a discipline, dedicated to the interdisciplinary investigation of the specifics of communication contents.","",""
9,"K. Goodman, Diana Zandi, A. Reis, E. Vayena","Balancing risks and benefits of artificial intelligence in the health sector",2020,"","","","",147,"2022-07-13 09:33:26","","10.2471/blt.20.253823","","",,,,,9,4.50,2,4,2,"230 During the last decade, enhanced computing power and the availability of large amounts of data have prompted the practical use of artificial intelligence in health care. Health and medical journals now commonly include reports on machine learning and big data, and descriptions of the risks posed by, and the governance required to manage, this technology. Machine learning algorithms are used to make diagnoses, identify treatments and analyse public health threats, and these systems can learn and improve continuously in response to new data. The tension between risks and concerns on one hand versus potential and opportunity on the other has shaped this issue of the Bulletin of the World Health Organization on the new ethical challenges of artificial intelligence in public health. Data-driven discovery and analysis in health care can increase knowledge and efficiency as well as challenge social values related to privacy, data control and the monetization of personal information. In India, for example, the adoption of a system for assigning all citizens a unique identification number, linking it to individual health records and several health-related schemes, raises ethical, legal and social issues, and the need for an appropriate ethical framework and data governance.1 These issues might be particularly challenging in lowand middle-income countries. Trust is perhaps the overarching theme of the contributions to this issue, and it is indeed one of the central values in digital health. One article explores opportunities for a human-centric ethical and regulatory environment to support the evolution of trust-based artificial intelligence with special regard to health insurance.2 Likewise, trust plays a role along with empathy and compassion in the humane side of care, the importance of which must be preserved in exploring the kind of health care society ought to promote.3 Similarly, European Union guidance might be too context-specific and as such leaves too much room for local, contextualized discretion for it to foster trustworthy artificial intelligence globally.4 In the context of population health research, researchers propose a post-research review model for ethics governance of research using artificial intelligence.5 For mobile health research in behavioural science, machine learning tools pose novel challenges for transparency, privacy, consent and the management of adverse events, all of which point to the need for consensusbased guidelines.6 As use of artificial intelligence systems expands, accountability for harm to patients and responsibility for their safety entail the need for human control and understanding of these systems.7 Other safeguards will require deliberate investments in data quality, access to care and processes to minimize bias, all in the service of trustworthiness.8 Success in integrating artificial intelligence into everyday patient care, as for instance in the United Kingdom of Great Britain and Northern Ireland’s National Health Service, is dependent on transparency, accountability and trust.9 In addition to trust, the values of fairness, justice and equity are seen as posing challenges even if other ethical duties are met. If artificial intelligence systems can explicitly improve equity, it is also a requirement that they do not worsen inequity.10 Thus, the case of neglected tropical diseases in low-resource settings illustrates opportunities for improved public health, as well as new challenges.11 Globally, the potential to help address some shortages and unmet needs in public health and care services might be realized by artificial intelligence-controlled conversational agents or chatbots that give health advice. However, realizing this potential will require the collaborative establishment of best practices and international ethics guidelines for technologies that replace humans.12 The field of bioethics emerged and grew in response to the development of new technologies and, sometimes, related wrongdoing. Ensuring adequate education, governance and ongoing ethical scrutiny will be essential if we are to realize the benefits and minimize the risks of this new technology. Questions of artificial intelligence accountability, equity and inclusiveness remain. The field is quickly evolving, and more artificial intelligence-based applications and services are becoming available in high-income countries. Identifying better tools for benefit-sharing and, simultaneously, evidence-based safeguards and criteria for appropriate uses and users to benefit everyone, including those in middleand lowincome countries, is essential. The World Health Organization (WHO) has made a commitment to addressing ethics, governance and regulation of artificial intelligence for health. In late 2019, WHO established an expert group to help develop a global framework for ethics and governance in artificial intelligence. The goal of this initiative is to ensure that these technologies are aligned with the overarching aims of promoting fair and equitable global health, meeting human rights standards and supporting Member States’ commitments to achieve universal health coverage. ■ Balancing risks and benefits of artificial intelligence in the health sector Kenneth Goodman, Diana Zandi, Andreas Reis & Effy Vayena","",""
0,"S. Cuddy","THE BENEFITS AND DANGERS OF USING ARTIFICIAL INTELLIGENCE IN PETROPHYSICS",2020,"","","","",148,"2022-07-13 09:33:26","","10.30632/spwla-5066","","",,,,,0,0.00,0,1,2,"Abstract Artificial Intelligence, or AI, is a method of data analysis that learns from data, identify patterns and makes predictions with the minimal human intervention. AI is bringing many benefits to petrophysical evaluation. Using case studies, this paper describes several successful applications. The future of AI has even more potential. However, if used carelessly there are potentially grave consequences. A complex Middle East Carbonate field needed a bespoke shaly water saturation equation. AI was used to ‘evolve’ an ideal equation, together with field specific saturation and cementation exponents. One UKCS gas field had an ‘oil problem’. Here, AI was used to unlock the hidden fluid information in the NMR T1 and T2 spectra and successfully differentiate oil and gas zones in real time. A North Sea field with 30 wells had shear velocity data (Vs) in only 4 wells. Vs was required for reservoir modelling and well bore stability prediction. AI was used to predict Vs in all 30 wells. Incorporating high vertical resolution data, the Vs predictions were even better than the recorded logs. As it is not economic to take core data on every well, AI is used to discover the relationships between logs, core, litho-facies and permeability in multi-dimensional data space. As a consequence, all wells in a field were populated with these data to build a robust reservoir model. In addition, the AI predicted data upscaled correctly unlike many conventional techniques. AI gives impressive results when automatically log quality controlling (LQC) and repairing electrical logs for bad hole and sections of missing data. AI doesn’t require prior knowledge of the petrophysical response equations and is self-calibrating. There are no parameters to pick or cross-plots to make. There is very little user intervention and AI avoids the problem of ‘garbage in, garbage out’ (GIGO), by ignoring noise and outliers. AI programs work with an unlimited number of electrical logs, core and gas chromatography data; and don’t ‘fall-over’ if some of those inputs are missing. AI programs currently being developed include ones where their machine code evolves using similar rules used by life’s DNA code. These AI programs pose considerable dangers far beyond the oil industry as described in this paper. A ‘risk assessment’ is essential on all AI programs so that all hazards and risk factors, that could cause harm, are identified and mitigated.","",""
0,"A. Paic","Policies for Artificial Intelligence in Science and Innovation",2020,"","","","",149,"2022-07-13 09:33:26","","10.22323/1.372.0045","","",,,,,0,0.00,0,1,2,"This contribution synthesizes the discussions of the special session on policies for Artificial Intelligence in Science and Innovation, organized by the OECD’s Directorate for Science, Technology and Innovation. The session was opened by Dr Judith Arrieta, Minister of the Foreign Service at the Chief of Staff’s Office of the Secretary of Foreign Affairs of Mexico, and the two panels included speakers from governments, industry and civil society from European countries, USA,Canada China and Australia. Participants discussed the disruptive nature of AI and the formidable challenges it poses. Most of the discussion focused under the umbrella title of ethics, but they span very different issues of human-centered values, fairness, transparency, explainability, and many more. Other challenges include employment, education, SME policy, enabling environment, access to data and computing technology. Responses by governments were also discussed with a particular focus on national strategies, whose main pillars are oriented toward knowledge creation through AI research, knowledge diffusion through linkages to the private sector, development of human capital which will underpin the development of the sector, and a strong values, ethical and regulatory framework to create the conditions for the development of trustworthy AI.  In a world of finite resources, discussants concluded that one cannot apply very stringent requirements to all AI decisions, and there is clearly a need to require more transparency, explainability and robustness from systems which have the greatest impact on human lives. Therefore an approach based on algorithmic impact assessment seems reasonable. Such an approach needs to be further developed and standardized.","",""
0,"Shivali Agarwal, Jayachandu Bandlamudi, Atri Mandal, Anupama Ray, G. Sridhara","Automated Assignment of Helpdesk Email Tickets: An Artificial Intelligence Life-Cycle Case Study",2020,"","","","",150,"2022-07-13 09:33:26","","","","",,,,,0,0.00,0,5,2,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 Fall 2020 45 The landscape of modern information technology service delivery is changing, with increased focus on automation and optimization. Most information technology vendors today have service platforms aimed toward end-to-end automation for carrying out mundane, repetitive labor-intensive tasks and even for tasks requiring human cognizance. One such task is ticket assignment and dispatch, where the service requests submitted by the end-users to the vendor in the form of tickets are reviewed by a centralized dispatch team and assigned to the appropriate service team and resolver group. The dispatch of a ticket to the correct group of practitioners is a critical step in the speedy resolution of a ticket. Incorrect dispatch decisions can significantly increase the total turnaround time for ticket resolution, as observed in a study of an actual production system (agarwal, Sindhgatta, and Sengupta 2012). When such delays occur, it causes customer dissatisfaction as well as monetary penalties for the vendor due to service-level-agreement breaches. Several factors make the dispatcher’s job challenging, namely the need for in-depth knowledge of the roles and responsibilities of various groups, the heterogeneous and informal nature of email text, and the high attrition rate in service delivery teams (Mandal et al. 2018). Given the fact that inefficiencies in dispatch have serious business consequences, there has been a lot of interest in automating the assignment process. a number of different approaches have been proposed for automating ticket dispatch (agarwal, Sindhgatta, and Sengupta 2012; Shao et al. 2008a, 2008b; Parvin, Bose, and Van Oyen 2009).  In this article, we present an endto-end automated helpdesk email ticket assignment system driven by high accuracy, coverage, business continuity, scalability, and optimal usage of computational resources. The primary objective of the system is to determine the problem mentioned in an incoming email ticket and then automatically dispatch it to an appropriate resolver group with high accuracy. While meeting this objective, it should also meet the objective of being able to operate at desired accuracy levels in the face of changing business needs by automatically adapting to the changes. The proposed system uses a system of classifiers with separate strategies for handling frequent and sparse resolver groups augmented with a semiautomatic rule engine and retraining strategies to ensure that it is accurate, robust, and adaptive to changing business needs. Our system has been deployed in the production of six major service providers in diverse service domains and currently assigns 100,000 emails per month, on an average, with an accuracy close to ninety percent and covering at least ninety percent of email tickets. This translates to achieving human-level accuracy and results in a net savings of more than 50,000 man-hours of effort per annum. To date, our deployed system has already served more than two million tickets in production. Automated Assignment of Helpdesk Email Tickets: An Artificial Intelligence Life-Cycle Case Study","",""
43,"Stuart J. Russell, Thomas G. Dietterich, Eric Horvitz, B. Selman, F. Rossi, D. Hassabis, S. Legg, Mustafa Suleyman, D. George, D. Phoenix","Letter to the Editor: Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter",2015,"","","","",151,"2022-07-13 09:33:26","","10.1609/aimag.v36i4.2621","","",,,,,43,6.14,4,10,7,"Artificial intelligence (AI) research has explored a variety of problems and approaches since its inception, but for the last 20 years or so has been focused on the problems surrounding the construction of intelligent agents — systems that perceive and act in some environment. In this context, ""intelligence"" is related to statistical and economic notions of rationality — colloquially, the ability to make good decisions, plans, or inferences. The adoption of probabilistic and decision-theoretic representations and statistical learning methods has led to a large degree of integration and cross-fertilization among AI, machine learning, statistics, control theory, neuroscience, and other fields. The establishment of shared theoretical frameworks, combined with the availability of data and processing power, has yielded remarkable successes in various component tasks such as speech recognition, image classification, autonomous vehicles, machine translation, legged locomotion, and question-answering systems. As capabilities in these areas and others cross the threshold from laboratory research to economically valuable technologies, a virtuous cycle takes hold whereby even small improvements in performance are worth large sums of money, prompting greater investments in research. There is now a broad consensus that AI research is progressing steadily, and that its impact on society is likely to increase. The potential benefits are huge, since everything that civilization has to offer is a product of human intelligence; we cannot predict what we might achieve when this intelligence is magnified by the tools AI may provide, but the eradication of disease and poverty are not unfathomable. Because of the great potential of AI, it is important to research how to reap its benefits while avoiding potential pitfalls. The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the AAAI 2008–09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do. The attached research priorities document [see page X] gives many examples of such research directions that can help maximize the societal benefit of AI. This research is by necessity interdisciplinary, because it involves both society and AI. It ranges from economics, law and philosophy to computer security, formal methods and, of course, various branches of AI itself. In summary, we believe that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today.","",""
36,"William R. Frey, D. Patton, M. Gaskell, K. McGregor","Artificial Intelligence and Inclusion: Formerly Gang-Involved Youth as Domain Experts for Analyzing Unstructured Twitter Data",2018,"","","","",152,"2022-07-13 09:33:26","","10.1177/0894439318788314","","",,,,,36,9.00,9,4,4,"Mining social media data for studying the human condition has created new and unique challenges. When analyzing social media data from marginalized communities, algorithms lack the ability to accurately interpret off-line context, which may lead to dangerous assumptions about and implications for marginalized communities. To combat this challenge, we hired formerly gang-involved young people as domain experts for contextualizing social media data in order to create inclusive, community-informed algorithms. Utilizing data from the Gang Intervention and Computer Science Project—a comprehensive analysis of Twitter data from gang-involved youth in Chicago—we describe the process of involving formerly gang-involved young people in developing a new part-of-speech tagger and content classifier for a prototype natural language processing system that detects aggression and loss in Twitter data. We argue that involving young people as domain experts leads to more robust understandings of context, including localized language, culture, and events. These insights could change how data scientists approach the development of corpora and algorithms that affect people in marginalized communities and who to involve in that process. We offer a contextually driven interdisciplinary approach between social work and data science that integrates domain insights into the training of qualitative annotators and the production of algorithms for positive social impact.","",""
0,"Jacob Pettigrew, Gideon Woo, Herbert H. Tsang","Computational Intelligence in Human Feature Analysis and Pose Selection",2020,"","","","",153,"2022-07-13 09:33:26","","10.1109/SSCI47803.2020.9308270","","",,,,,0,0.00,0,3,2,"Using computers to detect a human’s features is a difficult problem. The solution to this problem can be used in applications such as facial detection and gesture recognition. These applications require fast computation and high accuracy. In our research, we are trying to detect human poses by examining humans’ features such as the arms and legs. Artificial Neural Networks have been successfully used in feature analysis and are popular for use in human pose selection. In this paper, we present the results from our research comparing various computational intelligent approaches such as Convolutional Neural Networks (CNN), Multi-Layer Perceptrons (MLP), Support Vector Machines (SVM), and K-Nearest Neighbour (KNN). Among the four algorithms examined in this paper, we found that CNNs outperformed other algorithms in terms of prediction accuracy and calculation speed. Our main contribution is therefore a CNN specifically designed for learning a human’s body structure and limb articulation, producing high accuracy while being robust against different body types and variation in limb articulation.","",""
0,"A. Barrie, R. Smith, C. Hickman, I. Erlich, A. Campbell","P-287 An assessment of agreement between automated embryo annotation, through artificial intelligence, and manual embryo annotation",2022,"","","","",154,"2022-07-13 09:33:26","","10.1093/humrep/deac107.276","","",,,,,0,0.00,0,5,1,"      How strong is the agreement between embryo morphokinetic annotations performed by experienced embryologists compared to an automated embryo annotation system based on artificial intelligence (AI)?        Agreement between manual and automated annotation as determined by the interclass correlation coefficient (ICC) revealed strong or very strong agreement for all analysed morphokinetic variables.        Transitioning from time-lapse imaging to embryo selection for transfer, freezing or discard involves annotation; the action of converting images to numerical data. Numerical data can be used as input to selection models quantifying embryo viability. Currently, embryos are manually annotated by the embryologist which can be subjective and time-consuming. As such, clinics prioritise a manageable number of variables to annotate, leading to a range of clinic practices. There is the additional challenge of operator variation, despite the development of standardised definitions and quality assurance schemes. AI may help resolve these challenges.        Retrospective comparative analysis, including 2442 embryos from IVF and ICSI cycles, from four private fertility clinics belonging to the same group in the UK. All the embryos cultured in a time-lapse incubator (EmbryoScope,Vitrolife) between January 2016 and 2019 were included in the study. Manual annotations (MA) versus automated annotations (AA) were compared using a two-way, mixed interclass correlation coefficient (ICC), which produced five categories of agreement, very weak(0-0.20), weak(0.21-0.40), moderate(0.41-0.60), strong(0.61-0.80), very strong(0.81-1.00).        Videos were manually annotated by experienced embryologists from pronuclei fading (tPNf) to time of expanded blastocyst (tEB) with all cell stages annotated in between (time to two-cell (t2), three-cell (t3), four-cell (t4), five-cell (t5), six-cell (t6), seven-cell (t7), eight-cell (t8), nine-cell (t9), morula (tM), start of blastulation (tSB) and full blastocyst (tB)). Blind to human annotations, and without any training, the same videos were annotated by CHLOE (Fairtility) to produce automated annotation data.        Of the expected annotations, AA did not provide a result for 15.4% of the MA(3235/21008). Very strong agreement(0.81-1.00) between MA and AA was found for tPNf, t2, t3, t5, t6, tM, tSB, tB, tEB. Strong agreement(0.61-0.80) was found for t4, t7, t8 and t9+. Outliers in the AA data, defined as one standard deviation from the MA, were interrogated further for five key morphokinetic parameters; t2, t5, t8, tSB and tB. A total of 269 outliers were identified.  For t2 outliers(n = 14,6%), the average time difference was 5.97h(range;5.50-24.44h). All embryos with a t2 outlier were classed as either poor(PQ) or average quality(AQ).  The t5 outliers(n = 45,19%) had an average time difference of 2.84h(range;9.33-36.69h). 96%(n = 43) of these embryos were classed as PQ(n = 25,56%) or AQ(n = 18,40%).  Outliers for t8(138,58%) were, on average, 17.53h different between MA and AA(range;12.68-40.35h). 94%(n = 130)of these embryos were classed as PQ(n = 77,56%) or AQ(n = 53,38%).  The tSB outliers(n = 28,12%) had an average time difference of 3.58h(range;0.71-14.39h). 89%(n = 25) of these embryos were classed as PQ(n = 16,57%) or AQ(n = 9,32%).  Finally, outliers associated with tB(n = 44,18%) had an average time difference of 6.39h(range;0.02-33.67h). 95%(n = 42) of these embryos were classed as PQ(n = 38,86%) or AQ(n = 4,9%).  Almost 15%(n = 40) of the embryos had outliers in more than one of the five morphokinetic parameters.        The findings for this study reflect the capabilities of a specific AI-based annotation algorithm against the practice in multiple clinics in the same group and country. The automated annotation algorithm was not trained on this dataset prior to validation, which is encouraging for generalisability.        AI is ideally suited to resolve annotation challenges. This study demonstrates that where embryo quality is poor, annotation could be skewed both when performed manually and automatically. Once robustness is demonstrated, AI tools such as CHLOE, may allow clinics to process clinical data efficiently, objectively and consistently.        None ","",""
82,"T. Davenport","The AI Advantage: How to Put the Artificial Intelligence Revolution to Work",2018,"","","","",155,"2022-07-13 09:33:26","","","","",,,,,82,20.50,82,1,4,"Cutting through the hype, a practical guide to using artificial intelligence for business benefits and competitive advantage.In The AI Advantage, Thomas Davenport offers a guide to using artificial intelligence in business. He describes what technologies are available and how companies can use them for business benefits and competitive advantage. He cuts through the hype of the AI craze?remember when it seemed plausible that IBM's Watson could cure cancer??to explain how businesses can put artificial intelligence to work now, in the real world. His key recommendation: don't go for the ?moonshot? (curing cancer, or synthesizing all investment knowledge); look for the ?low-hanging fruit? to make your company more efficient.Davenport explains that the business value AI offers is solid rather than sexy or splashy. AI will improve products and processes and make decisions better informed?important but largely invisible tasks. AI technologies won't replace human workers but augment their capabilities, with smart machines to work alongside smart people. AI can automate structured and repetitive work; provide extensive analysis of data through machine learning (?analytics on steroids?), and engage with customers and employees via chatbots and intelligent agents. Companies should experiment with these technologies and develop their own expertise.Davenport describes the major AI technologies and explains how they are being used, reports on the AI work done by large commercial enterprises like Amazon and Google, and outlines strategies and steps to becoming a cognitive corporation. This book provides an invaluable guide to the real-world future of business AI.A book in the Management on the Cutting Edge series, published in cooperation with MIT Sloan Management Review.","",""
125,"M. Matheny, D. Whicher, Sonoo Thadaney Israni","Artificial Intelligence in Health Care: A Report From the National Academy of Medicine.",2019,"","","","",156,"2022-07-13 09:33:26","","10.1001/jama.2019.21579","","",,,,,125,41.67,42,3,3,"The promise of artificial intelligence (AI) in health care offers substantial opportunities to improve patient and clinical team outcomes, reduce costs, and influence population health. Current data generation greatly exceeds human cognitive capacity to effectively manage information, and AI is likely to have an important and complementary role to human cognition to support delivery of personalized health care.1 For example, recent innovations in AI have shown high levels of accuracy in imaging and signal detection tasks and are considered among the most mature tools in this domain.2 However, there are challenges in realizing the potential for AI in health care. Disconnects between reality and expectations have led to prior precipitous declines in use of the technology, termed AI winters, and another such event is possible, especially in health care.3 Today, AI has outsized market expectations and technology sector investments. Current challenges include using biased data for AI model development, applying AI outside of populations represented in the training and validation data sets, disregarding the effects of possible unintended consequences on care or the patientclinician relationship, and limited data about actual effects on patient outcomes and cost of care. AI in Healthcare: The Hope, The Hype, The Promise, The Peril, a publication by the National Academy of Medicine (NAM), synthesizes current knowledge and offers a reference document for the responsible development, implementation, and maintenance of AI in the clinical enterprise.4 The publication outlines current and near-term AI solutions; highlights the challenges, limi-","",""
45,"L. Lynn","Artificial intelligence systems for complex decision-making in acute care medicine: a review",2019,"","","","",157,"2022-07-13 09:33:26","","10.1186/s13037-019-0188-2","","",,,,,45,15.00,45,1,3,"","",""
0,"I. Zvereva, K. Dmitry","P-256 The influence of artificial intelligence embryo scoring on male-female sex ratio",2022,"","","","",158,"2022-07-13 09:33:26","","10.1093/humrep/deac107.246","","",,,,,0,0.00,0,2,1,"      Are there correlations among male-female sex ratio, human blastocyst ploidy status and artificial intelligence (AI)-based morphokinetics embryo selection?        Embryo selection based on morphological evaluation by time-lapse system (TLS) with AI technology could lead to a female-biased sex ratio of resulting newborns.        As of now, there have been only limited attempts to evaluate how AI-based TLS embryo selection for priority transfer could affect male-to-female sex ratio in human population, and the results of different publications were contradicting. However, the morphokinetic assessment was made without calculating the embryos KID Score (Embryos with Known Implantation Data), which significantly improves and make faster the decision-making process.        This is a monocentric, retrospective study from October 2019 to December 2021 including 251 blastocysts with PGT-A results. Embryos were cultured in time-lapse incubator (EmbryoScope, Vitrolife) up to the time of trophectoderm biopsy. All embryos were evaluated based on the KIDscoreTM D5 algorithm (Vitrolife) under routine supervision by experienced embryologists. The PGT-A results were obtained by using next-generation sequencing (NGS) platform from Medical Genomics LLC laboratory (Illumina MiSeq, Illumina).        Sample size was 251 embryos from 101 women (mean female age was 36.0 ± 5.6 years). All embryos were divided in four groups in accordance with their final KID score: <2.5 ( n = 7), 2.6-5.0 ( n = 33), 5.1-7.5 ( n = 123) and >7.5 ( n = 88). The embryos with sex chromosome abnormalities were also included in research to assess the frequency of occurrence in embryos with low and high KID score.        As expected, the percentage of aneuploid blastocysts, as well as the rate of sex chromosome abnormalities, decreased with increasing the embryo KID score. The highest male-female sex ratio among all embryos was observed for the group with KID score <2.5 (1.33), and gradually decreased to values of 0.92 and 0.74 in groups with KID score 5.1-7.5 and >7.5, respectively. At the same time, the highest male-female sex ratio among euploid blastocysts was maximal in the group with KID score 2.6-5.0. The obtained data contradict results of some other studies, which reported faster development of male embryos (which should mean their higher KID score). However, the KID score was not evaluated in them, and thus these results cannot be directly compared to ours.        Most patients in this study had complicated reproductive history, with repeated failures in IVF programs, often with a stop in embryo development. Also, the present investigation is retrospective. A following multicenter researches with larger sample size and cross-centered validation of embryologist-performed annotation is considered in our future approach.        Obtained data doesn’t allow to establish the female-gender prevalence among embryos. Nevertheless, further accumulation of knowledge about relation between KID Embryo Score and embryo gender can be used for presumptive sex determination in special cases with sex-linked diseases, where poor embryo morphology doesn’t allow to perform biopsy for genetic analysis.        - ","",""
0,"F. Renna, Miguel L. Martins, Alexandre Neto, António Cunha, D. Libânio, M. Dinis-Ribeiro, M. Coimbra","Artificial Intelligence for Upper Gastrointestinal Endoscopy: A Roadmap from Technology Development to Clinical Practice",2022,"","","","",159,"2022-07-13 09:33:26","","10.3390/diagnostics12051278","","",,,,,0,0.00,0,7,1,"Stomach cancer is the third deadliest type of cancer in the world (0.86 million deaths in 2017). In 2035, a 20% increase will be observed both in incidence and mortality due to demographic effects if no interventions are foreseen. Upper GI endoscopy (UGIE) plays a paramount role in early diagnosis and, therefore, improved survival rates. On the other hand, human and technical factors can contribute to misdiagnosis while performing UGIE. In this scenario, artificial intelligence (AI) has recently shown its potential in compensating for the pitfalls of UGIE, by leveraging deep learning architectures able to efficiently recognize endoscopic patterns from UGIE video data. This work presents a review of the current state-of-the-art algorithms in the application of AI to gastroscopy. It focuses specifically on the threefold tasks of assuring exam completeness (i.e., detecting the presence of blind spots) and assisting in the detection and characterization of clinical findings, both gastric precancerous conditions and neoplastic lesion changes. Early and promising results have already been obtained using well-known deep learning architectures for computer vision, but many algorithmic challenges remain in achieving the vision of AI-assisted UGIE. Future challenges in the roadmap for the effective integration of AI tools within the UGIE clinical practice are discussed, namely the adoption of more robust deep learning architectures and methods able to embed domain knowledge into image/video classifiers as well as the availability of large, annotated datasets.","",""
0,"K. Sfakianoudis, E. Maziotis, S. Grigoriadis, A. Pantou, G. Kokkini, A. Trypidi, I. Angeli, T. Vaxevanoglou, K. Pantos, M. Simopoulou","O-122 Reporting on the value of Artificial Intelligence in predicting the optimal embryo for transfer: A systematic review and meta-analysis",2022,"","","","",160,"2022-07-13 09:33:26","","10.1093/humrep/deac105.022","","",,,,,0,0.00,0,10,1,"      Are Artificial Intelligence (AI) based models effective in robustly predicting in vitro fertilization (IVF) outcome by assessing embryo quality?        The majority of the AI-based models could provide an accurate prediction regarding live birth, clinical pregnancy, clinical pregnancy with fetal heartbeat and embryo ploidy status.        Precision and consistency in embryo quality evaluation are of paramount importance regarding the outcome of an IVF cycle. Numerous embryo grading and evaluation systems, employing morphological and morphokinetical assessment, have been proposed but without reaching a consensus yet. The main limitation of the aforementioned assessment systems is that they depend on human evaluation, which may be subject to subjectivity and interobserver variation. Thus, automated prediction models may be essential to optimize objectivity and reliability of embryo grading. Artificial neural network models may process microscopy images or time-lapse videos as input to predict the embryos’ potential competency.        A systematic review and meta-analysis including 18 published studies. The population consists of preimplantation embryos suitable for embryo transfer in IVF/ICSI cycles following employment of an AI-based prediction model. The outcome measures are prediction of live birth, clinical pregnancy, clinical pregnancy with heartbeat and ploidy status.        A systematic search of the literature was performed in the databases of Pubmed/Medline, Embase, and Cochrane Central Library limited to articles published in English up to August 2021. The initial search yielded a total of 694 studies with 97 of them being duplicates and other 579 being excluded on the grounds of not fulfilling inclusion criteria. Following full-text screening and citation mining a total of 18 studies were identified to be eligible for inclusion.        Four studies reported on prediction of live birth. The sensitivity was 70.6% (95%C.I.: 38.1-90.4%) and specificity was 90.6% (95%C.I.:79.3-96.1%).  The Area Under the Curve (AUC) of the Summary Receiver Operating Characteristics (SROC) curve was 0.905, while the partial AUC (pAUC) was 0.755. Employing the Bayesian approach, the total Observed:Expected ratio (O:E) was 1.12 (95%CI: 0.26–2.37; 95%PI:0.02-6.54). Ten studies reported on prediction of clinical pregnancy. The sensitivity and the specificity were 71% (95%C.I.: 58.1-81.2%) and 62.5% (95%C.I.: 47.4-75.5%) respectively. The AUC was 0.716, while pAUC was 0.693. Moreover, the total O:E ratio was 0.92 (95%CI: 0.61–1.28; 95%PI:0.13-2.43). Eight studies reported on prediction of clinical pregnancy with fetal heartbeat the sensitivity was 75.2% (95%C.I.: 66.8-82%) and the specificity was 55.3% (95%C.I.: 41.2-68.7%). The AUC was 0.722, while the pAUC was 0.774. The O:E ratio was 0.77 (95%CI: 0.54 – 1.05; 95%PI: 0.21-1.62). Four studies reported on the ploidy status of the embryo. The sensitivity and specificity were 59.4% (95%C.I.: 45.0-73.1%) and 79.2% (95%C.I.: 70.1-86.1%) respectively. The AUC was 0.751 and the pAUC was 0.585. The total O:E ratio was 0.86 (95%CI: 0.42 – 1.27; 95%PI: 0.03-1.83).        The limited number of studies fulfilling inclusion criteria, along with the different designs applied when developing AI models which may lead to increased heterogeneity, stand as limitations. Inclusion of women regardless of their age presents as another limitation, as advanced maternal age has been associated with diminished IVF outcomes.        Albeit, our findings support that AI is a highly promising tool in the era of personalized medicine providing precise predictions it does not appear to considerably surpass human prediction capabilities. More studies and more collaborations between the developers are of paramount importance prior to AI becoming the gold standard.        Not applicable ","",""
37,"C. Kulikowski","Beginnings of Artificial Intelligence in Medicine (AIM): Computational Artifice Assisting Scientific Inquiry and Clinical Art – with Reflections on Present AIM Challenges",2019,"","","","",161,"2022-07-13 09:33:26","","10.1055/s-0039-1677895","","",,,,,37,12.33,37,1,3,"Summary Background : The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970’s led to a major change in the paradigm that affected all of artificial intelligence (AI) research. Since then, AI has evolved, surviving several “winters”, as it has oscillated between relying on expensive and hard-to-validate knowledge-based approaches, and the alternative of using machine learning methods for inferring classification rules from labelled datasets. In the past couple of decades, we are seeing a gradual but progressive intertwining of the two. Objectives : To give an overview of early directions in AI in medicine and threads of some subsequent developments motivated by the very different goals of scientific inquiry for biomedical research, and for computational modeling of clinical reasoning and more general healthcare problem solving from the perspective of today’s “AI-Deep Learning Boom”. To show how, from the beginning, AI was central to Biomedical and Health Informatics (BMHI), as a field investigating how to understand intelligent thinking in dealing professionally with the practice for healthcare, developing mathematical models, technology, and software tools to aid human experts in biomedicine, despite many previous bouts of “exuberant optimism” about the methodologies deployed. Methods : An overview and commentary on some of the early research and publications in AI in biomedicine, emphasizing the different approaches to the modeling of problems involved in clinical practice in contrast to those of biomedical science. A concluding reflection of a few current challenges and pitfalls of AI in some biomedical applications. Conclusion : While biomedical knowledge-based systems played a critical role in influencing AI in its early days, 50 years later they have taken a back seat behind “Deep Learning” which promises to discover knowledge structures for inference and prediction, both in science and for clinical decision-support. Early work on AI for medical consultation turned out to be more useful for explanation and teaching than for clinical practice, as had been originally intended. Today, despite the many reported successes of deep learning, fundamental scientific challenges arise in drawing on models of brain science, cognition, and language, if AI is to augment and complement rather than replace human judgment and expertise in biomedicine while also incorporating these advances for translational medicine. Understanding clinical phenotypes and how they relate to precision and personalization of care requires not only scientific inquiry, but also humanistic models of treatment that respond to patient and practitioner narrative exchanges, since it is the stories and insights of human experts which encourage what Norbert Weiner termed the ethical “human use of human beings”, so central to adherence to the Hippocratic Oath","",""
21,"Li-Qi Shu, Yi-Kan Sun, L. Tan, Q. Shu, A. Chang","Application of artificial intelligence in pediatrics: past, present and future",2019,"","","","",162,"2022-07-13 09:33:26","","10.1007/s12519-019-00255-1","","",,,,,21,7.00,4,5,3,"","",""
23,"B. Chin-Yee, Ross E. G. Upshur","Three Problems with Big Data and Artificial Intelligence in Medicine",2019,"","","","",163,"2022-07-13 09:33:26","","10.1353/pbm.2019.0012","","",,,,,23,7.67,12,2,3,"ABSTRACT:The rise of big data and artificial intelligence (AI) in health care has engendered considerable excitement, claiming to improve approaches to diagnosis, prognosis, and treatment. Amidst the enthusiasm, the philosophical assumptions that underlie the big data and AI movement in medicine are rarely examined. This essay outlines three philosophical challenges faced by this movement: (1) the epistemological-ontological problem arising from the theory-ladenness of big data and measurement; (2) the epistemological-logical problem resulting from the inherent limitations of algorithms and attendant issues of reliability and interpretability; and (3) the phenomenological problem concerning the irreducibility of human experience to quantitative data. These philosophical issues demonstrate several important challenges for these technologies that must be considered prior to their integration into clinical care. Our article aims to initiate a critical dialogue on the impact of big data and AI in health care in order to allow for more robust evaluation of these technologies and to aid in the development of approaches to clinical care that better serve clinicians and their patients.","",""
11,"C. E. Kahn","Artificial Intelligence, Real Radiology.",2019,"","","","",164,"2022-07-13 09:33:26","","10.1148/RYAI.2019184001","","",,,,,11,3.67,11,1,3,"Welcome to this inaugural issue of Radiology: Artificial Intelligence. Our journal’s mission is to publish highquality scientific work that advances our understanding of artificial intelligence (AI) in radiology. AI has become a topic of great interest—especially the application of machine learning techniques to medical images—but AI itself is not new. The term artificial intelligence was proposed in 1956 to describe efforts to understand, simulate, and improve upon human qualities such as reasoning, learning, solving problems, understanding verbal and written language, processing visual information, and playing games like chess and poker. What is new is a resurgence of interest in AI, particularly in the use of machine learning to recognize patterns in images. And, curiously, it is game playing that has opened this new frontier—but not the games of chess, checkers, or Go. Rather, think Xbox and PlayStation. Video games require a rapidly changing three-dimensional scene to be transformed into two-dimensional images shown in real time. The need to compute images efficiently spurred the development of highly parallelized graphics processing units. These specialized processors, in turn, have powered software for increasingly complex and sophisticated “deep” artificial neural network models. Whereas neural networks developed 10 years ago typically had three or four layers, today’s deep networks comprise hundreds of layers (1). Deep learning models have engendered both great excitement and a great deal of hyperbole. After all, if AI systems can pick out pictures of cats on the web, then surely such systems are ready to replace radiologists, right? Well, perhaps not, at least not right now. There is much work to be done to build and validate systems that can detect and characterize the thousands of imaging findings and their associated diseases that can be seen across a panoply of radiology studies. And that brings us to the quote from Shakespeare. Anyone can claim to build an AI system, but that doesn’t mean that the system will do their bidding as imagined. Our journal is here to assure that the science and applications of AI in radiology are built on thoughtful, innovative, and well-validated research. What sorts of topics will this journal publish? We will bring you the same high caliber of research that is found in RSNA’s flagship scientific journal, Radiology, but focused here on AI, machine learning, and data science in radiology. In particular, we seek to publish first-rate work that provides rigorous evaluation of AI’s applications to clinical problems in radiology. We invite manuscripts that show the impact of AI to extract information, diagnose and manage disease in patients, streamline radiology workflow, or improve health care outcomes. We’re interested in image segmentation, image reconstruction, automated detection of abnormalities, diagnostic reasoning, natural language processing, clinical workflow analysis, radiomics, and radiogenomics. We also invite manuscripts that demonstrate novel applications of AI in radiology or highlight innovative AI methodologies. Developers of publicly available sets of radiologic images, image annotations, radiology reports, or algorithms can present their work as a Data Resources report. AI and radiology do not exist in isolation: they are part of broad endeavors to advance knowledge and improve health. As such, this journal will feature articles on the ethical, legal, social, and economic implications of AI in radiology. AI is and must be a human—and humane—activity (2). We must engage in this work with an eye to how these technologies will help us care for our patients more effectively and humanely. Our goal is not to replace, but rather to extend our human abilities to provide medical care— and to improve the lives of those we are privileged to serve. All RSNA members receive access to this online bimonthly journal. We invite all readers (RSNA members or not) to sign up for our Editor’s Blog, The Vasty Deep (https://pubs.rsna.org/page/ai/blog) and to follow us on Twitter (@Radiology_AI). These social media platforms will augment the journal and offer innovative online features. Again, welcome!","",""
52,"Serge-Lopez Wamba-Taguimdje, S. Wamba, J. K. Kamdjoug, C. Wanko","Influence of artificial intelligence (AI) on firm performance: the business value of AI-based transformation projects",2020,"","","","",165,"2022-07-13 09:33:26","","10.1108/bpmj-10-2019-0411","","",,,,,52,26.00,13,4,2,"The main purpose of our study is to analyze the influence of Artificial Intelligence (AI) on firm performance, notably by building on the business value of AI-based transformation projects. This study was conducted using a four-step sequential approach: (1) analysis of AI and AI concepts/technologies; (2) in-depth exploration of case studies from a great number of industrial sectors; (3) data collection from the databases (websites) of AI-based solution providers; and (4) a review of AI literature to identify their impact on the performance of organizations while highlighting the business value of AI-enabled projects transformation within organizations.,This study has called on the theory of IT capabilities to seize the influence of AI business value on firm performance (at the organizational and process levels). The research process (responding to the research question, making discussions, interpretations and comparisons, and formulating recommendations) was based on a review of 500 case studies from IBM, AWS, Cloudera, Nvidia, Conversica, Universal Robots websites, etc. Studying the influence of AI on the performance of organizations, and more specifically, of the business value of such organizations’ AI-enabled transformation projects, required us to make an archival data analysis following the three steps, namely the conceptual phase, the refinement and development phase, and the assessment phase.,AI covers a wide range of technologies, including machine translation, chatbots and self-learning algorithms, all of which can allow individuals to better understand their environment and act accordingly. Organizations have been adopting AI technological innovations with a view to adapting to or disrupting their ecosystem while developing and optimizing their strategic and competitive advantages. AI fully expresses its potential through its ability to optimize existing processes and improve automation, information and transformation effects, but also to detect, predict and interact with humans. Thus, the results of our study have highlighted such AI benefits in organizations, and more specifically, its ability to improve on performance at both the organizational (financial, marketing and administrative) and process levels. By building on these AI attributes, organizations can, therefore, enhance the business value of their transformed projects. The same results also showed that organizations achieve performance through AI capabilities only when they use their features/technologies to reconfigure their processes.,AI obviously influences the way businesses are done today. Therefore, practitioners and researchers need to consider AI as a valuable support or even a pilot for a new business model. For the purpose of our study, we adopted a research framework geared toward a more inclusive and comprehensive approach so as to better account for the intangible benefits of AI within organizations. In terms of interest, this study nurtures a scientific interest, which aims at proposing a model for analyzing the influence of AI on the performance of organizations, and at the same time, filling the associated gap in the literature. As for the managerial interest, our study aims to provide managers with elements to be reconfigured or added in order to take advantage of the full benefits of AI, and therefore improve organizations’ performance, the profitability of their investments in AI transformation projects, and some competitive advantage. This study also allows managers to consider AI not as a single technology but as a set/combination of several different configurations of IT in the various company’s business areas because multiple key elements must be brought together to ensure the success of AI: data, talent mix, domain knowledge, key decisions, external partnerships and scalable infrastructure.,This article analyses case studies on the reuse of secondary data from AI deployment reports in organizations. The transformation of projects based on the use of AI focuses mainly on business process innovations and indirectly on those occurring at the organizational level. Thus, 500 case studies are being examined to provide significant and tangible evidence about the business value of AI-based projects and the impact of AI on firm performance. More specifically, this article, through these case studies, exposes the influence of AI at both the organizational and process performance levels, while considering it not as a single technology but as a set/combination of the several different configurations of IT in various industries.","",""
11,"Wen-qian Sun, Xing Gao","The Construction of Undergraduate Machine Learning Course in the Artificial Intelligence Era",2018,"","","","",166,"2022-07-13 09:33:26","","10.1109/ICCSE.2018.8468758","","",,,,,11,2.75,6,2,4,"Machine learning technology has been greatly developed in the last decade, which makes artificial intelligence reach a revolutionary breakthrough and lets us really perceive the potential of artificial intelligence in changing human life. In order to improve the understanding and application ability of artificial intelligence, carrying out the corresponding machine learning course is of significance for the students during the undergraduate period. This paper probes into the teaching content, teaching form and other aspects of the undergraduate machine learning course based on this issue and proposes a teaching method driven by application scenarios to guide the undergraduate students to understand the development, current situation and frontier technology of machine learning. In the experimental design, the students' theoretical knowledge is fully considered, the practical questions are simplified, and the students' ability to think and solve problems is also raised, so as to lay a theoretical and practical basis for further study of machine learning.","",""
42,"George Gadanidis","Artificial intelligence, computational thinking, and mathematics education",2017,"","","","",167,"2022-07-13 09:33:26","","10.1108/IJILT-09-2016-0048","","",,,,,42,8.40,42,1,5,"Purpose          The purpose of this paper is to examine the intersection of artificial intelligence (AI), computational thinking (CT), and mathematics education (ME) for young students (K-8). Specifically, it focuses on three key elements that are common to AI, CT and ME: agency, modeling of phenomena and abstracting concepts beyond specific instances.          Design/methodology/approach          The theoretical framework of this paper adopts a sociocultural perspective where knowledge is constructed in interactions with others (Vygotsky, 1978). Others also refers to the multiplicity of technologies that surround us, including both the digital artefacts of our new media world, and the human methods and specialized processes acting in the world. Technology is not simply a tool for human intention. It is an actor in the cognitive ecology of immersive humans-with-technology environments (Levy, 1993, 1998) that supports but also disrupts and reorganizes human thinking (Borba and Villarreal, 2005).          Findings          There is fruitful overlap between AI, CT and ME that is of value to consider in mathematics education.          Originality/value          Seeing ME through the lenses of other disciplines and recognizing that there is a significant overlap of key elements reinforces the importance of agency, modeling and abstraction in ME and provides new contexts and tools for incorporating them in classroom practice.","",""
0,"Jinyun Tang, W. Riley, Qing Zhu, T. Keenan","Using machine learning and artificial intelligence to improve model-data integrated earth system model predictions of water and carbon cycle extremes",2021,"","","","",168,"2022-07-13 09:33:26","","10.2172/1769794","","",,,,,0,0.00,0,4,1,"Jinyun Tang 1, William J Riley 1, Qing Zhu 1, Trevor Keenan 1, 2 1Lawrence Berkeley National Laboratory 2 University of California, Berkeley Focal Area(s) The research proposed here focuses on improving the predictive power of the land component of earth system models (ESMs) using (1) model-data fusion enabled by machine learning (ML) and artificial intelligence (AI), (2) predictive modeling through the combination of ML, AI, and big-data (comprising both model output and observations), and (3) insight of ESM structure and process mechanisms gleaned from complex data using ML and AI. Science Challenge Current efforts on water and carbon cycle benchmarking and improving ESM predictions focus on how well models capture (1) snapshots of climatology (e.g., the spatial pattern of land surface evapotranspiration), (2) time series of aggregated variables (e.g., interannual variability of net land carbon fluxes), and (3) one-vs-one variable correlations (e.g., the relationship between precipitation and evapotranspiration), all of which are less than three dimensional. However, ESM predictions are by nature of high dimension, beyond those spanned by space and time, when variables like vegetation diversity and human water use are considered. Moreover, in 10 years, with improved spatial-temporal resolution and the inclusion of more mechanistic processes, ESMs will very likely output more diverse data streams at much larger volume. Meanwhile, thanks to technological advancements, the amount of observations will also increase dramatically, in both type and spatial-temporal coverage. Current benchmark and model-data integration paradigms and methods are insufficient to address this big-data challenge. Further, current approaches are not of sufficient specificity or fidelity for evaluation at fine spatial resolutions (e.g., 1 km), nor do they provide comprehensive understanding of the casualty relationships that affect climate extremes. To address these challenges, research is proposed here to (1) make better uses of multiple scales of observations to concurrently analyze, evaluate, and reduce ESM uncertainty, and generate process knowledge of terrestrial processes, (2) achieve the ability to clearly integrate diverse observations, ML and AI, theory, and model predictive capabilities, (3) obtain robust quantification of multivariate functional relationships (e.g., net primary productivity to precipitation, temperature, and radiation) under a wide range of environmental conditions, and (4) provide high fidelity prediction and understanding of climate extreme events at fine spatial resolutions. Rationale ESM predictions are uncertain because (1) the earth system comprises many components, including atmosphere, land, ocean, biosphere, cryosphere, human activities, etc., each of which is insufficiently monitored and understood; (2) when the insufficient understanding of these earth system components are combined with the limited spatial-temporal resolution of ESMs,","",""
25,"Feng Liu, Yong Shi, Y. Liu","Intelligence Quotient and Intelligence Grade of Artificial Intelligence",2017,"","","","",169,"2022-07-13 09:33:26","","10.1007/s40745-017-0109-0","","",,,,,25,5.00,8,3,5,"","",""
95,"S. Dilek, Hüseyin Çakir, Mustafa Aydin","Applications of Artificial Intelligence Techniques to Combating Cyber Crimes: A Review",2015,"","","","",170,"2022-07-13 09:33:26","","10.5121/ijaia.2015.6102","","",,,,,95,13.57,32,3,7,"With the advances in information technology (IT) criminals are using cyberspace to commit numerous cyber crimes. Cyber infrastructures are highly vulnerable to intrusions and other threats. Physical devices and human intervention are not sufficient for monitoring and protection of these infrastructures; hence, there is a need for more sophisticated cyber defense systems that need to be flexible, adaptable and robust, and able to detect a wide variety of threats and make intelligent real-time decisions. Numerous bio-inspired computing methods of Artificial Intelligence have been increasingly playing an important role in cyber crime detection and prevention. The purpose of this study is to present advances made so far in the field of applying AI techniques for combating cyber crimes, to demonstrate how these techniques can be an effective tool for detection and prevention of cyber attacks, as well as to give the scope for future work.","",""
8,"Amaan Anwar, S. Hassan","Applying Artificial Intelligence Techniques to Prevent Cyber Assaults",2017,"","","","",171,"2022-07-13 09:33:26","","","","",,,,,8,1.60,4,2,5,"Cyber security ostensibly is the discipline that could profit most from the introduction of Artificial Intelligence (AI). It is tough to make software for defending against the powerfully developing assaults in systems. It can be cured by applying techniques of artificial intelligence. Where conventional security systems may be slow and deficient, artificial intelligence techniques can enhance their overall security execution and give better security from an expanding number of complex cyber threats. Beside the great opportunities attributed to AI inside cyber security, its utilization has legitimized risks and concerns. To promote increment the development of cyber security, a holistic perspective of associations cyber environment is required in which AI is consolidated with human knowledge, since neither individuals nor AI alone has proven overall success in this sphere. In this manner, socially mindful utilization of AI techniques will be needed to further mitigate related risks and concerns.","",""
159,"V. Özdemir, N. Hekim","Birth of Industry 5.0: Making Sense of Big Data with Artificial Intelligence, ""The Internet of Things"" and Next-Generation Technology Policy.",2018,"","","","",172,"2022-07-13 09:33:26","","10.1089/omi.2017.0194","","",,,,,159,39.75,80,2,4,"Driverless cars with artificial intelligence (AI) and automated supermarkets run by collaborative robots (cobots) working without human supervision have sparked off new debates: what will be the impacts of extreme automation, turbocharged by the Internet of Things (IoT), AI, and the Industry 4.0, on Big Data and omics implementation science? The IoT builds on (1) broadband wireless internet connectivity, (2) miniaturized sensors embedded in animate and inanimate objects ranging from the house cat to the milk carton in your smart fridge, and (3) AI and cobots making sense of Big Data collected by sensors. Industry 4.0 is a high-tech strategy for manufacturing automation that employs the IoT, thus creating the Smart Factory. Extreme automation until ""everything is connected to everything else"" poses, however, vulnerabilities that have been little considered to date. First, highly integrated systems are vulnerable to systemic risks such as total network collapse in the event of failure of one of its parts, for example, by hacking or Internet viruses that can fully invade integrated systems. Second, extreme connectivity creates new social and political power structures. If left unchecked, they might lead to authoritarian governance by one person in total control of network power, directly or through her/his connected surrogates. We propose Industry 5.0 that can democratize knowledge coproduction from Big Data, building on the new concept of symmetrical innovation. Industry 5.0 utilizes IoT, but differs from predecessor automation systems by having three-dimensional (3D) symmetry in innovation ecosystem design: (1) a built-in safe exit strategy in case of demise of hyperconnected entrenched digital knowledge networks. Importantly, such safe exists are orthogonal-in that they allow ""digital detox"" by employing pathways unrelated/unaffected by automated networks, for example, electronic patient records versus material/article trails on vital medical information; (2) equal emphasis on both acceleration and deceleration of innovation if diminishing returns become apparent; and (3) next generation social science and humanities (SSH) research for global governance of emerging technologies: ""Post-ELSI Technology Evaluation Research"" (PETER). Importantly, PETER considers the technology opportunity costs, ethics, ethics-of-ethics, framings (epistemology), independence, and reflexivity of SSH research in technology policymaking. Industry 5.0 is poised to harness extreme automation and Big Data with safety, innovative technology policy, and responsible implementation science, enabled by 3D symmetry in innovation ecosystem design.","",""
10,"A. S. Ahmad, A. D. W. Sumari","Cognitive artificial intelligence: Brain-inspired intelligent computation in artificial intelligence",2017,"","","","",173,"2022-07-13 09:33:26","","10.1109/SAI.2017.8252094","","",,,,,10,2.00,5,2,5,"Computation occurred within human brain is very much awesome and is not possible to be emulated 100% exactly in Artificial Intelligence (AI) method-based machines. What scientists did and have been done so far up to now are to try to model it as close as to what exactly occurs within the brain. Human brain has an awesome mechanism in performing computation with the end result is new knowledge and human uses the knowledge to actuate his organs. In this paper we will show a new approach for emulating the computation occured within human brain to obtain new knowledge based on the inputs sensed by the system's sensory system taken from the environment. When this process is carried out recursively, the system's knowledge becomes newer and newer, and it is called as knowledge growing. This approach is designed for an agent that has ability to think and act rationally like human. Our cognitive modelling approach is resulted in a model of human information processing and a technique to obtain the most maximum performance should be taken by the cognitive agent. This method is called as A3S (Arwin-Adang-Aciek-Sembiring), the agent is called as Knowledge-Growing System (KGS) and this brain-inspired method opens a new perspective in AI that we call as Cognitive Artificial Intelligence (CAI).","",""
23,"J. C. Alvarado-Pérez, Diego Hernán Peluffo-Ordóñez, Roberto Therón","Bridging the gap between human knowledge and machine learning",2015,"","","","",174,"2022-07-13 09:33:26","","10.14201/ADCAIJ2015415464","","",,,,,23,3.29,8,3,7,"Nowadays, great amount of data is being created by several sources from academic, scientific, business and industrial activities. Such data intrinsically contains meaningful information allowing for developing techniques, and have scientific validity to explore the information thereof. In this connection, the aim of artificial intelligence (AI) is getting new knowledge to make decisions properly. AI has taken an important place in scientific and technology development communities, and recently develops computer-based processing devices for modern machines. Under the premise, the premise that the feedback provided by human reasoning -which is holistic, flexible and parallel- may enhance the data analysis, the need for the integration of natural and artificial intelligence has emerged. Such an integration makes the process of knowledge discovery more effective, providing the ability to easily find hidden trends and patterns belonging to the database predictive model. As well, allowing for new observations and considerations from beforehand known data by using both data analysis methods and knowledge and skills from human reasoning. In this work, we review main basics and recent works on artificial and natural intelligence integration in order to introduce users and researchers on this emergent field. As well, key aspects to conceptually compare them are provided.","",""
0,"R. Brachman, David Gunning, Murray Burke","Integrated Artificial Intelligence Systems",2020,"","","","",175,"2022-07-13 09:33:26","","","","",,,,,0,0.00,0,3,2,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 66 AI MAGAZINE When one thinks about what it might take to build an intelligent system, it is evident that multiple capabilities will be required. Intelligence is generally considered to be reflected in the ability of a system to learn and understand the world around it, and to deal successfully with new or challenging situations. A closer look at what it might take to accomplish this reveals a surprisingly complex set of abilities that must work together. There are many variations on these themes, but roughly speaking, a robustly intelligent, autonomous agent embedded in the real world will need perceptual capabilities to sense and help interpret external signals and phenomena; a set of beliefs about the world, including itself and other agents, cause and effect, and a host of other things relevant to its survival and success in achieving its goals; a variety of reasoning capabilities to determine implications of its beliefs, understand its environment, plan ahead, solve problems, and so forth; a wide array of learning and adaptation capabilities; the ability to affect the world through action; and, some kind of rich communication mechanism along the lines of natural human language generation and understanding.  From Shakey the Robot to self-driving cars, from the personal computer to personal assistants on our phones, the Defense Advanced Research Projects Agency (DARPA) has led the development of integrated artificial intelligence (AI) systems for more than half a century. From the earliest days of AI, it was apparent that a robust, generally intelligent system should include a complete set of capabilities: perception, memory, reasoning, learning, planning, and action; and when DARPA initiated AI research in the 1960s, ambitious projects such as Shakey the Robot went after the complete package. As DARPA realized the challenges, they backed away from the ultimate goal of integrated AI and tried to make progress on the individual problems of image understanding, speech and language understanding, knowledge representation and reasoning, planning and decision aids, machine learning, and robotic manipulation. Yet, even as researchers struggled to make progress in these subdisciplines, DARPA periodically resurrected the challenge of integrated intelligent systems and pushed the community to try again. In the 1980s, DARPA’s Strategic Computing Initiative took on challenges of integrated AI projects such as the Autonomous Land Vehicle and the Pilot’s Associate. These did not succeed, but instead set the stage for the several decades of more siloed research that followed, until it was time to try again. In the 2000s, DARPA took on the integrated AI problem again with its Grand Challenges, which led to the first self-driving cars, and projects such as the Personalized Assistant that Learns, which produced Apple’s Siri. These efforts created complex, richlyintegrated systems that represented quantum leaps ahead in machine intelligence. The integration of sophisticated capabilities in a fundamental way is the key to general intelligence. This is the story of DARPA’s persistent long-term support for this essential premise of AI. Integrated Artificial Intelligence Systems","",""
96,"S. Wartman, C. Combs","Medical Education Must Move From the Information Age to the Age of Artificial Intelligence",2017,"","","","",176,"2022-07-13 09:33:26","","10.1097/ACM.0000000000002044","","",,,,,96,19.20,48,2,5,"Noteworthy changes coming to the practice of medicine require significant medical education reforms. While proposals for such reforms abound, they are insufficient because they do not adequately address the most fundamental change—the practice of medicine is rapidly transitioning from the information age to the age of artificial intelligence. Increasingly, future medical practice will be characterized by: the delivery of care wherever the patient happens to be; the provision of care by newly constituted health care teams; the use of a growing array of data from multiple sources and artificial intelligence applications; and the skillful management of the interface between medicine and machines. To be effective in this environment, physicians must work at the top of their license, have knowledge spanning the health professions and care continuum, effectively leverage data platforms, focus on analyzing outcomes and improving performance, and communicate the meaning of the probabilities generated by massive amounts of data to patients, given their unique human complexities. The authors believe that a “reboot” of medical education is required that makes better use of the findings of cognitive psychology and pays more attention to the alignment of humans and machines in education and practice. Medical education needs to move beyond the foundational biomedical and clinical sciences. Systematic curricular attention must focus on the organization of professional effort among health professionals, the use of intelligence tools involving large data sets, and machine learning and robots, all the while assuring the mastery of compassionate care.","",""
90,"M. Alsharqi, W. Woodward, J. Mumith, D. C. Markham, R. Upton, P. Leeson","Artificial intelligence and echocardiography",2018,"","","","",177,"2022-07-13 09:33:26","","10.1530/ERP-18-0056","","",,,,,90,22.50,15,6,4,"Echocardiography plays a crucial role in the diagnosis and management of cardiovascular disease. However, interpretation remains largely reliant on the subjective expertise of the operator. As a result inter-operator variability and experience can lead to incorrect diagnoses. Artificial intelligence (AI) technologies provide new possibilities for echocardiography to generate accurate, consistent and automated interpretation of echocardiograms, thus potentially reducing the risk of human error. In this review, we discuss a subfield of AI relevant to image interpretation, called machine learning, and its potential to enhance the diagnostic performance of echocardiography. We discuss recent applications of these methods and future directions for AI-assisted interpretation of echocardiograms. The research suggests it is feasible to apply machine learning models to provide rapid, highly accurate and consistent assessment of echocardiograms, comparable to clinicians. These algorithms are capable of accurately quantifying a wide range of features, such as the severity of valvular heart disease or the ischaemic burden in patients with coronary artery disease. However, the applications and their use are still in their infancy within the field of echocardiography. Research to refine methods and validate their use for automation, quantification and diagnosis are in progress. Widespread adoption of robust AI tools in clinical echocardiography practice should follow and have the potential to deliver significant benefits for patient outcome.","",""
7,"Arwin Datumaya Wahyudi Sumari, A. S. Ahmad, Cognitive Artificial","The application of cognitive artificial intelligence within C4ISR framework for national resilience",2017,"","","","",178,"2022-07-13 09:33:26","","10.1109/ACDTJ.2017.8259600","","",,,,,7,1.40,2,3,5,"Cognitive Artificial Intelligence (CAI) is a new perspective in Artificial Intelligence (AI) which is aimed to emulate how human brain works in generating knowledge. Human becomes intelligent because of knowledge which grows over time in his brain. With comprehensive knowledge, he can understand the world (environment) and is able to make decision and or action on it. On the other hand, strategic decision which impacts to the continuance of having a nation and having state is a critical and crucial matter, and it should be done in precise and quick manner especially in the case of contingency and faced to mutiple-data multiple-decision-alternative problems. The most precise decision has to be based on the knowledge from extracted comprehensive information. In this paper we show you the application of CAI for National Security with Knowledge-Growing System (KGS) as the engine of decision making system. We apply the CAI to a framework called Cognitive Command, Control, Communications, Computers, Intelligence, Surveillance and Reconnaissance (C4ISR) with examples taken from a simulated of real-life case in the Defense-Security domain.","",""
78,"H. Kitano","Artificial Intelligence to Win the Nobel Prize and Beyond: Creating the Engine for Scientific Discovery",2016,"","","","",179,"2022-07-13 09:33:26","","10.1609/aimag.v37i1.2642","","",,,,,78,13.00,78,1,6,"This article proposes a new grand challenge for AI reasearch: to develop AI system to make major scientific discoveries in biomedical sciences that worth Nobel Prize. There are a series of human cognitive limitations that prevents us from making accerlated scientific discoveries, particularity in biomedical sciences. As a result, scientific discoveries are left behind at the level of cottage industry. AI systems can transform scientific discoveries into highly efficient practice, thereby enable us to expand our knowledge in unprecedented way. Such system may out-compute all possible hypotheses and may redefine the nature of scientific intuition, hence scientific discovery process.","",""
51,"M. Rigla, Gema García-Sáez, B. Pons, M. Hernando","Artificial Intelligence Methodologies and Their Application to Diabetes",2018,"","","","",180,"2022-07-13 09:33:26","","10.1177/1932296817710475","","",,,,,51,12.75,13,4,4,"In the past decade diabetes management has been transformed by the addition of continuous glucose monitoring and insulin pump data. More recently, a wide variety of functions and physiologic variables, such as heart rate, hours of sleep, number of steps walked and movement, have been available through wristbands or watches. New data, hydration, geolocation, and barometric pressure, among others, will be incorporated in the future. All these parameters, when analyzed, can be helpful for patients and doctors’ decision support. Similar new scenarios have appeared in most medical fields, in such a way that in recent years, there has been an increased interest in the development and application of the methods of artificial intelligence (AI) to decision support and knowledge acquisition. Multidisciplinary research teams integrated by computer engineers and doctors are more and more frequent, mirroring the need of cooperation in this new topic. AI, as a science, can be defined as the ability to make computers do things that would require intelligence if done by humans. Increasingly, diabetes-related journals have been incorporating publications focused on AI tools applied to diabetes. In summary, diabetes management scenarios have suffered a deep transformation that forces diabetologists to incorporate skills from new areas. This recently needed knowledge includes AI tools, which have become part of the diabetes health care. The aim of this article is to explain in an easy and plane way the most used AI methodologies to promote the implication of health care providers—doctors and nurses—in this field.","",""
38,"J. Yun, Doo Seok Lee, Heungju Ahn, Kyungbae Park, Tan Yigitcanlar","Not Deep Learning but Autonomous Learning of Open Innovation for Sustainable Artificial Intelligence",2016,"","","","",181,"2022-07-13 09:33:26","","10.3390/SU8080797","","",,,,,38,6.33,8,5,6,"What do we need for sustainable artificial intelligence that is not harmful but beneficial human life? This paper builds up the interaction model between direct and autonomous learning from the human’s cognitive learning process and firms’ open innovation process. It conceptually establishes a direct and autonomous learning interaction model. The key factor of this model is that the process to respond to entries from external environments through interactions between autonomous learning and direct learning as well as to rearrange internal knowledge is incessant. When autonomous learning happens, the units of knowledge determinations that arise from indirect learning are separated. They induce not only broad autonomous learning made through the horizontal combinations that surpass the combinations that occurred in direct learning but also in-depth autonomous learning made through vertical combinations that appear so that new knowledge is added. The core of the interaction model between direct and autonomous learning is the variability of the boundary between proven knowledge and hypothetical knowledge, limitations in knowledge accumulation, as well as complementarity and conflict between direct and autonomous learning. Therefore, these should be considered when introducing the interaction model between direct and autonomous learning into navigations, cleaning robots, search engines, etc. In addition, we should consider the relationship between direct learning and autonomous learning when building up open innovation strategies and policies.","",""
44,"A. Vellido","Societal Issues Concerning the Application of Artificial Intelligence in Medicine",2018,"","","","",182,"2022-07-13 09:33:26","","10.1159/000492428","","",,,,,44,11.00,44,1,4,"Background: Medicine is becoming an increasingly data-centred discipline and, beyond classical statistical approaches, artificial intelligence (AI) and, in particular, machine learning (ML) are attracting much interest for the analysis of medical data. It has been argued that AI is experiencing a fast process of commodification. This characterization correctly reflects the current process of industrialization of AI and its reach into society. Therefore, societal issues related to the use of AI and ML should not be ignored any longer and certainly not in the medical domain. These societal issues may take many forms, but they all entail the design of models from a human-centred perspective, incorporating human-relevant requirements and constraints. In this brief paper, we discuss a number of specific issues affecting the use of AI and ML in medicine, such as fairness, privacy and anonymity, explainability and interpretability, but also some broader societal issues, such as ethics and legislation. We reckon that all of these are relevant aspects to consider in order to achieve the objective of fostering acceptance of AI- and ML-based technologies, as well as to comply with an evolving legislation concerning the impact of digital technologies on ethically and privacy sensitive matters. Our specific goal here is to reflect on how all these topics affect medical applications of AI and ML. This paper includes some of the contents of the “2nd Meeting of Science and Dialysis: Artificial Intelligence,” organized in the Bellvitge University Hospital, Barcelona, Spain. Summary and Key Messages: AI and ML are attracting much interest from the medical community as key approaches to knowledge extraction from data. These approaches are increasingly colonizing ambits of social impact, such as medicine and healthcare. Issues of social relevance with an impact on medicine and healthcare include (although they are not limited to) fairness, explainability, privacy, ethics and legislation.","",""
38,"H. Jaeger","Artificial intelligence: Deep neural reasoning",2016,"","","","",183,"2022-07-13 09:33:26","","10.1038/nature19477","","",,,,,38,6.33,38,1,6,"","",""
38,"D. Bruckner, H. Zeilinger, D. Dietrich","Cognitive Automation—Survey of Novel Artificial General Intelligence Methods for the Automation of Human Technical Environments",2012,"","","","",184,"2022-07-13 09:33:26","","10.1109/TII.2011.2176741","","",,,,,38,3.80,13,3,10,"Automation, the utilization of control and information technologies for reducing the need for human intervention in the production process is about to meet Cognition-the science concerned with human thinking-and related sciences. More and more processes require analysis and insights that allow controlling them beyond the mere execution of rules and beyond prefitted controllers in order to automatically keep them within the desired conditions. Automatic and flexible decision making based on challenging conditions such as increasing amounts of information, lacking prior knowledge of data, incomplete, missing or contradicting data, becomes the key challenges for future automation technologies.","",""
0,"John Kalantari","A general purpose artificial intelligence framework for the analysis of complex biological systems",2017,"","","","",185,"2022-07-13 09:33:26","","10.17077/ETD.4ESKIJ3M","","",,,,,0,0.00,0,1,5,"This thesis encompasses research on Artificial Intelligence in support of automating scientific discovery in the fields of biology and medicine. At the core of this research is the ongoing development of a general-purpose artificial intelligence framework emulating various facets of human-level intelligence necessary for building cross-domain knowledge that may lead to new insights and discoveries. To learn and buildmodels in a data-drivenmanner, we develop a general-purpose learning framework called Syntactic Nonparametric Analysis of Complex Systems (SYNACX), which uses tools from Bayesian nonparametric inference to learn the statistical and syntactic properties of biological phenomena from sequence data. We show that the models learned by SYNACX offer performance comparable to that of standard neural network architectures. For complex biological systems or processes consisting of several heterogeneous components with spatio-temporal interdependencies across multiple scales, learning frameworks like SYNACX can become unwieldy due to the the resultant combinatorial complexity. Thus we also investigate ways to robustly reduce data dimensionality by introducing a new data abstraction. In particular, we extend traditional string and graph grammars in a new modeling formalism which we call Simplicial Grammar. This formalism integrates the topological properties of the simplicial complex with the expressive power of stochastic grammars in a computation abstraction with which we can decompose complex system behavior, into a finite set of modular grammar rules which parsimoniously describe the spatial/temporal structure and dynamics of patterns inferred from sequence data.","",""
14,"A. Antonov","From Artificial Intelligence to Human Super- Intelligence",2011,"","","","",186,"2022-07-13 09:33:26","","","","",,,,,14,1.27,14,1,11,"The problem of artificial intelligence defined over 60 years ago aims at teaching computers to solve intellectual tasks without a human and instead of a human, i.e., basically, creates the preconditions for intellectual degradation of the mankind. Moreover, the definition of the artificial intelligence problem itself has now become obsolete and does not take account of the new knowledge on human intelligence. It sets the almost needless and intractable problem of solving complex multi-factor tasks, successfully solved by the human sub-consciousness, with the help of computer simulation of the low-factor human rational thinking. At the same time, the technological singularity concept reads that after the research of the artificial intelligence problem is successfully completed in early 21 st century the emergence of computer civilization is inevitable, followed by the possible extinction of the human civilization. Thus, we conclude that it is necessary to stop further research of the artificial intelligence problem. Instead, we suggest developing the problem of human super-intelligence, which will be able to solve successfully much more pressing multi-factor problems and will pose no threat to the humanity.","",""
10,"R. Smith","Idealizations of Uncertainty, and Lessons from Artificial Intelligence",2016,"","","","",187,"2022-07-13 09:33:26","","10.5018/ECONOMICS-EJOURNAL.JA.2016-7","","",,,,,10,1.67,10,1,6,"Abstract At a time when economics is giving intense scrutiny to the likely impact of artificial intelligence (AI) on the global economy, this paper suggests the two disciplines face a common problem when it comes to uncertainty. It is argued that, despite the enormous achievements of AI systems, it would be a serious mistake to suppose that such systems, unaided by human intervention, are as yet any nearer to providing robust solutions to the problems posed by Keynesian uncertainty. Under the radically uncertain conditions, human decision-making (for all its problems) has proved relatively robust, while decision making relying solely on deterministic rules or probabilistic models is bound to be brittle. AI remains dependent on techniques that are seldom seen in human decision-making, including assumptions of fully enumerable spaces of future possibilities, which are rigorously computed over, and extensively searched. Discussion of alternative models of human decision making under uncertainty follows, suggesting a future research agenda in this area of common interest to AI and economics.","",""
17,"Reid G. Hoffman","Using artificial intelligence to set information free",2016,"","","","",188,"2022-07-13 09:33:26","","10.7551/mitpress/11645.003.0007","","",,,,,17,2.83,17,1,6,"Artificial intelligence (AI) is about to transform management from an art into a combination of art and science. Specialized AI will allow us to apply data science to our human interactions at work in a way that earlier management theorists like Drucker could only imagine. These specialized forms of AI can process and manipulate enormous quantities of data at a rate our biological brains can't match. Therein lies the applicability to management: within the next five years, forward-thinking organizations will be using specialized forms of AI to build a complex and comprehensive corporate knowledge graph. Specialized AI will be ubiquitous throughout the organization, indexing every document, folder, and file. AI will also be sitting in the middle of the communication stream, collecting all of the work products, from emails to files shared to chat messages.","",""
8,"Luteshna Bishnoi, Shailendra Narayan Singh","Artificial Intelligence Techniques Used In Medical Sciences: A Review",2018,"","","","",189,"2022-07-13 09:33:26","","10.1109/CONFLUENCE.2018.8442729","","",,,,,8,2.00,4,2,4,"Artificial intelligence(AI) is more augmented intelligence and that is a fundamental philosophical direction it took where it is complementing people's intelligence with machines that have enough aspects of the intelligent problem solving capabilities that together the person and machine can do better than one or the other. Medical diagnosis is hard for humans it actually takes a lot of time without the help of intelligent machines. AIM (Artificial Intelligence in Medical) systems have been used in health care system mainly for diagnosis tasks. AI is the branch of computer science which deals with the machine intelligence and maximising chances of success and accuracy. Medical is the field where technology is much needed. Our expanding desires of the highest quality of health care and the quick development of ever more detailed medical knowledge leave the doctor without sufficient time to give to each case and attempting to stay aware of the advancements in his field. The main aim of writing this paper is to review the effectiveness of AI techniques in medical sciences and compare them.","",""
135,"Khalid Colchester, H. Hagras, D. Alghazzawi, G. Aldabbagh","A Survey of Artificial Intelligence Techniques Employed for Adaptive Educational Systems within E-Learning Platforms",2017,"","","","",190,"2022-07-13 09:33:26","","10.1515/jaiscr-2017-0004","","",,,,,135,27.00,34,4,5,"Abstract The adaptive educational systems within e-learning platforms are built in response to the fact that the learning process is different for each and every learner. In order to provide adaptive e-learning services and study materials that are tailor-made for adaptive learning, this type of educational approach seeks to combine the ability to comprehend and detect a person’s specific needs in the context of learning with the expertise required to use appropriate learning pedagogy and enhance the learning process. Thus, it is critical to create accurate student profiles and models based upon analysis of their affective states, knowledge level, and their individual personality traits and skills. The acquired data can then be efficiently used and exploited to develop an adaptive learning environment. Once acquired, these learner models can be used in two ways. The first is to inform the pedagogy proposed by the experts and designers of the adaptive educational system. The second is to give the system dynamic self-learning capabilities from the behaviors exhibited by the teachers and students to create the appropriate pedagogy and automatically adjust the e-learning environments to suit the pedagogies. In this respect, artificial intelligence techniques may be useful for several reasons, including their ability to develop and imitate human reasoning and decision-making processes (learning-teaching model) and minimize the sources of uncertainty to achieve an effective learning-teaching context. These learning capabilities ensure both learner and system improvement over the lifelong learning mechanism. In this paper, we present a survey of raised and related topics to the field of artificial intelligence techniques employed for adaptive educational systems within e-learning, their advantages and disadvantages, and a discussion of the importance of using those techniques to achieve more intelligent and adaptive e-learning environments.","",""
189,"Lawrence B. Solum","Legal Personhood for Artificial Intelligences",2008,"","","","",191,"2022-07-13 09:33:26","","10.4324/9781003074991-37","","",,,,,189,13.50,189,1,14,"Could an artificial intelligence become a legal person? As of today, this question is only theoretical. No existing computer program currently possesses the sort of capacities that would justify serious judicial inquiry into the question of legal personhood. The question is nonetheless of some interest. Cognitive science begins with the assumption that the nature of human intelligence is computational, and therefore, that the human mind can, in principle, be modelled as a program that runs on a computer. Artificial intelligence (AI) research attempts to develop such models. But even as cognitive science has displaced behavioralism as the dominant paradigm for investigating the human mind, fundamental questions about the very possibility of artificial intelligence continue to be debated. This Essay explores those questions through a series of thought experiments that transform the theoretical question whether artificial intelligence is possible into legal questions such as, ""Could an artificial intelligence serve as a trustee?"" What is the relevance of these legal thought experiments for the debate over the possibility of artificial intelligence? A preliminary answer to this question has two parts. First, putting the AI debate in a concrete legal context acts as a pragmatic Occam's razor. By reexamining positions taken in cognitive science or the philosophy of artificial intelligence as legal arguments, we are forced to see them anew in a relentlessly pragmatic context. Philosophical claims that no program running on a digital computer could really be intelligent are put into a context that requires us to take a hard look at just what practical importance the missing reality could have for the way we speak and conduct our affairs. In other words, the legal context provides a way to ask for the ""cash value"" of the arguments. The hypothesis developed in this Essay is that only some of the claims made in the debate over the possibility of AI do make a pragmatic difference, and it is pragmatic differences that ought to be decisive. Second, and more controversially, we can view the legal system as a repository of knowledge-a formal accumulation of practical judgments. The law embodies core insights about the way the world works and how we evaluate it. Moreover, in common-law systems judges strive to decide particular cases in a way that best fits the legal landscape-the prior cases, the statutory law, and the constitution. Hence, transforming the abstract debate over the possibility of AI into an imagined hard case forces us to check our intuitions and arguments against the assumptions that underlie social decisions made in many other contexts. By using a thought experiment that explicitly focuses on wide coherence, we increase the chance that the positions we eventually adopt will be in reflective equilibrium with our views about related matters. In addition, the law embodies practical knowledge in a form that is subject to public examination and discussion. Legal materials are published and subject to widespread public scrutiny and discussion. Some of the insights gleaned in the law may clarify our approach to the artificial intelligence debate.","",""
168,"Steve Omohundro","Cryptocurrencies, smart contracts, and artificial intelligence",2014,"","","","",192,"2022-07-13 09:33:26","","10.1145/2685328.2685334","","",,,,,168,21.00,168,1,8,"Recent developments in ""cryptocurrencies"" and ""smart contracts"" are creating new opportunities for applying AI techniques. These economic technologies would benefit from greater real world knowledge and reasoning as they become integrated with everyday commerce. Cryptocurrencies and smart contracts may also provide an infrastructure for ensuring that AI systems follow specified legal and safety regulations as they become more integrated into human society.","",""
3,"E.V. Blagodarny, A.A Vedyakhin, A.M. Raygorodsky","Development of Educational Projects on the Basis of Technological Platforms with Artificial Intelligence: The Experience of MIPT on the Use of High Vox-Platform",2018,"","","","",193,"2022-07-13 09:33:26","","10.1109/IC-AIAI.2018.8674452","","",,,,,3,0.75,1,3,4,"The MIPT School of Applied Mathematics and Computer Science conducts research on artificial intelligence and develops education in this field in Russia. Modern science and technology are developing so quickly that a person needs to constantly learn and acquire new skills. Therefore, MIPT develops educational courses at the school, academic and corporate levels. Employers and scientific laboratories are more interested in the practical skills of employees in AI, than just theoretical knowledge. For this reason, the MIPT School of Applied Mathematics and Computer Science conducts and develop new practice-oriented educational courses. Moreover, the Laboratory of Innovation at MIPT creates the HighVox platform, which will allow MIPT students to gain experience in solving real problems from Russian companies during their studies. The platform creates the digital trace of each student: competences, scientific interests, courses taken, completed projects, soft skills, etc. Based on the digital trace of each participant, the platform automatically creates recommendations for projects and teams most suitable for the student. In the future, HighVox will become a place where technical specialists search for work, get an education (lifelong learning), communicate with colleagues on specialized topics and offer their ideas for startups. As part of the creation of this platform, the laboratory of innovation conducts research in two directions: a model of human competence and the formation of effective teams based on hard & soft skills using artificial intelligence.","",""
0,"","ACTIVITY REPORT Project-Team Models and Algorithms for Artiﬁcial Intelligence",2022,"","","","",194,"2022-07-13 09:33:26","","","","",,,,,0,0.00,0,0,1,"The expectation-maximization (EM) algorithm is a powerful computational technique for maximum likelihood estimation in incomplete data models. When the expectation step cannot be performed in closed form, a stochastic approximation of EM (SAEM) can be used. The convergence of the SAEM toward critical points of the observed likelihood has been proved and its numerical efﬁciency has been demonstrated. However, sampling from the posterior distribution may be intractable or have a high computational cost. Moreover, despite appealing features, the limit position of this algorithm can strongly depend on its starting one. To cope with this two issues, we propose in [11] new stochastic approximation version of the EM in which we do not sample from the exact distribution in the expectation phase of the procedure. We ﬁrst prove the convergence of this algorithm toward critical points of the observed likelihood. Then, we propose an instantiation of this general procedure to favor convergence toward global maxima. Experiments on synthetic and real data highlight the performance of this algorithm in comparison to the SAEM and the EM when feasible. of subject-speciﬁc weights characterizing partial membership across clusters. With this ﬂexibility come challenges in uniquely identifying, estimating, and interpreting the parameters. In [40], we propose a new class of Dimension-Grouped MMMs (Gro-M 3 s) for multivariate categorical data, which improve parsimony and interpretability. In Gro-M 3 s, observed variables are partitioned into groups such that the latent membership is constant for variables within a group but can differ across groups. Traditional latent class models are obtained when all variables are in one group, while traditional MMMs are obtained when each variable is in its own group. The new model corresponds to a novel decomposition of probability tensors. Theoretically, we derive transparent identiﬁability conditions for both the unknown grouping structure and model parameters in general settings. Methodologically, we propose a Bayesian approach for Dirichlet Gro-M3 s to inferring the variable grouping structure and estimating model parameters. Simulation results demonstrate good computational performance and empirically conﬁrm the identiﬁability results. We illustrate the new methodology through an application to a functional disability dataset. from this natural partition. In a Bayesian context, this is achieved by considering the Dirichlet cluster proportion prior parameter α as a regularisation term controlling the granularity of the clustering. This second step allows the exploration of the clustering at coarser scales and the ordering of the clusters an important output for the visual representations of the clustering results. The clustering results obtained with the proposed approach, on simulated as well as real settings, are compared with existing strategies and are shown to be particularly relevant. This work is implemented in the R package greed and Figure 2 illustrates the main idea of the method. In this applied work [19], we use the Fisher-EM algorithm for clustering for the unsupervised classiﬁcation of 702, 248 spectra of galaxies and quasars with resdshifts smaller than 0.25 that were retrieved from the Sloan Digital Sky Survey (SDSS) database, release 7. The spectra were ﬁrst corrected for the redshift, then wavelet-ﬁltered to reduce the noise, and ﬁnally binned to obtain about 1437 wavelengths per spectrum. Fisher-EM, an unsupervised clustering discriminative latent mixture model algorithm, was applied on these corrected spectra, considering the full set as well as several subsets of 100,000 and 300,000 spectra. The optimum number of classes given by a penalized likelihood criterion is 86 classes, the 37 most populated ones gathering 99% of the sample. These classes are established from a subset of 302144 spectra. Using several cross-validation techniques we ﬁnd that this classiﬁcation is in agreement with the results obtained on the other subsets with an average misclassiﬁcation error of about 15%. The large number of very small classes tends to increase this error rate. This is the ﬁrst time that an automatic, objective and robust unsupervised classiﬁcation is established on such a large amount of spectra of galaxies. The mean spectra of the classes can be used as templates for a large majority of galaxies in our Universe. Figure 7 illustrates the obtained results. Recurrent Neural Networks, Deep linguistic patterns the of a of of to the is this linguistic that becomes valuable for our descriptive approach through deep as it allows us to observe complex lexico-grammatical structures, that potentially associate several levels of text representation in the same structure. The convolutional model used until now must therefore be adapted to integrate this additional information in order to obtain an even ﬁner description of the textual salience of a corpus. the relevant features used by the CNN to perform the classiﬁcation task. We empirically demonstrate the efﬁciency of our approach on corpora from two different languages: English and French. On all datasets, wTDS automatically encodes complex linguistic objects based on co-occurrences and possibly on grammatical and syntax analysis. relationships between the concepts in the metadata by analyzing the contrast between the concepts similarities in the Joconde’s semantic model and other vocabularies and we tried to improve the model prediction scores based on the semantic relations. Our results show that cross-fertilization between symbolic AI and machine learning can indeed provide the tools to address the challenges of the museum curators work describing the artwork pieces and searching for the relevant images. that combines a geometric approach for decision rules with existing post hoc solutions for machine learning models to generate an intuitive feature ranking tailored to the end user. We show that established model-agnostic approaches produce poor results in this framework. Figure 13 illustrates this work. Algorithms involving Gaussian processes or determinantal point processes typically require computing the determinant of a kernel matrix. Frequently, the latter is computed from the Cholesky decomposition, an algorithm of cubic complexity in the size of the matrix. We show that, under mild assumptions, it is possible to estimate the determinant from only a sub-matrix, with probabilistic guarantee on the relative error. In [37], we present an augmentation of the Cholesky decomposition that stops under certain conditions before processing the whole matrix. Experiments demonstrate that this can save a considerable amount of time while having an overhead of less than 5% when not stopping early. More generally, we present a probabilistic stopping strategy for the approximation of a sum of known length where addends are revealed sequentially. We do not assume independence between addends, only that they are bounded from below and decrease in conditional expectation. of there is a signiﬁcant from combining and audio data in detecting active speakers. either of the modalities can potentially mislead audiovisual fusion by inducing unreliable or deceptive information. outlines active speaker detection as a multi-objective learning problem to leverage best of each modalities using a novel self-attention, uncertainty-based multimodal fusion scheme. Results obtained show that the proposed multi-objective learning architecture outperforms traditional approaches in improving both mAP and AUC scores. We further demonstrate that our fusion strategy surpasses, in active speaker detection, other modality fusion methods reported in various disciplines. We ﬁnally show that the proposed method signiﬁcantly improves the state-of-the-art on the AVA-ActiveSpeaker dataset. This paper explores the problem of summarizing professional soccer matches as automatically as possible using both the event-stream data collected from the ﬁeld and the content broadcasted on TV. We have designed an architecture, introducing ﬁrst (1) a Multiple Instance Learning method that takes into account the sequential dependency among events and then (2) a hierarchical multimodal attention layer that grasps the importance of each event in an action [31]. We evaluate our approach on matches from two professional European soccer leagues, showing its capability to identify the best actions for automatic summarization by comparing with real summaries made by human operators. Figure 18 illustrates the general schema of the approach. We a coherent framework for studying longitudinal manifold-valued data. We introduce a Bayesian mixed-effects model which allows estimating both a group-representative piecewise-geodesic creating clusters of similar sentences. The ideal practice is to obtain a cluster with only positive blocks and another with only negative ones. Comparing to the supervised approach (Bag of words + Logistic Regression Classiﬁer) with its f1-score as 0.8234 and f2-score as 0.8316, we found that both S-Bert [58] (with a f1-score of 0.6250 and f2-score of 0.6192) and BioBert [57] (f1-score as 0.7004 and f2 as 0.6955) can achieves relatively good results and latter even outperformed the former due to its domain speciﬁc knowledge. around 13 billion euros per year to European citizens [52]. In the ﬁeld of healthcare insurance, in France the compulsory scheme detected over 261.2 million euros of fraudulent services in 2018, mainly due to healthcare professionals and healthcare establishments [50]. In the United States, according to the FBI, medicare fraud costs insurance companies between 21 billion and 71 billion US dollars per year [55]. In a context where reducing management costs is a real issue for healthcare insurers, the ﬁght against fraud is a real expectation of the customers of professionals in the sector so that everyone receives a fair return for their contributions. This stud","",""
1,"Arunaben Prahladbhai Gurjar, S. Patel","Fundamental Categories of Artificial Neural Networks",2021,"","","","",195,"2022-07-13 09:33:26","","10.4018/978-1-7998-4042-8.CH003","","",,,,,1,1.00,1,2,1,"The new era of the world uses artificial intelligence (AI) and machine learning. The combination of AI and machine learning is called artificial neural network (ANN). Artificial neural network can be used as hardware or software-based components. Different topology and learning algorithms are used in artificial neural networks. Artificial neural network works similarly to the functionality of the human nervous system. ANN is working as a nonlinear computing model based on activities performed by human brain such as classification, prediction, decision making, visualization just by considering previous experience. ANN is used to solve complex, hard-to-manage problems by accruing knowledge about the environment. There are different types of artificial neural networks available in machine learning. All types of artificial neural networks work based of mathematical operation and require a set of parameters to get results. This chapter gives overview on the various types of neural networks like feed forward, recurrent, feedback, classification-predication.","",""
20,"M. Stampar, K. Fertalj","Artificial intelligence in network intrusion detection",2015,"","","","",196,"2022-07-13 09:33:26","","10.1109/MIPRO.2015.7160479","","",,,,,20,2.86,10,2,7,"In past, detection of network attacks has been almost solely done by human operators. They anticipated network anomalies in front of consoles, where based on their expert knowledge applied necessary security measures. With the exponential growth of network bandwidth, this task slowly demanded substantial improvements in both speed and accuracy. One proposed way how to achieve this is the usage of artificial intelligence (AI), progressive and promising computer science branch, particularly one of its sub-fields - machine learning (ML) - where main idea is learning from data. In this paper authors will try to give a general overview of AI algorithms, with main focus on their usage for network intrusion detection.","",""
69,"L. Saitta, Jean-Daniel Zucker","Abstraction in Artificial Intelligence and Complex Systems",2013,"","","","",197,"2022-07-13 09:33:26","","10.1007/978-1-4614-7052-6","","",,,,,69,7.67,35,2,9,"","",""
1,"Massoud Sokouti, B. Sokouti","Applying the Science of Systematic Review and Meta-Analysis to Retrospective Artificial Intelligence Based Studies: The importance of performance evaluation",2019,"","","","",198,"2022-07-13 09:33:26","","","","",,,,,1,0.33,1,2,3,"The rationale behind the meta-analysis goes back to the 17th century studies of astronomy which then Karl Pearson performed a study based on meta-analysis using the data for typhoid inoculation in 1904. After, William Cochran applied this type of analysis to medical researches by taking the advantage of multiple previous studies. For more information and details on the history, the readers are referred to. To emerge the important role of systematic and metaanalysis studies even in the area of artificial intelligence systems, it is an anticipated that more reliable results can be driven from previous research studies alongside a simple review of such studies from which most of them may be ignored or not included as a matter of their nonsystematical type of reviews. The meta-analysis technique uses various types of statistics tools and methodologies to commonly derive a predictive diagnostic or non-diagnostic performance result of their compared corresponding approaches on the target defined disorders using information included in different datasets of previous studies. Although, a meta-analysis study can be regarded as a review of previous studies, however, it thoroughly targets not only the achieving results of those studies but also determine the in-common and non-commonpatterns of those researches as well as biases of the performance results whether they have been inserted intentionally or unintentionally. The importance of meta-analysis has been vastly discussed in medical sciences and therefore, been conducted rigorously through various studies, mostly on clinical trial ones. However, this technique is one of those less valued tools imported in to biomedical engineering studies and hence, their related algorithms mostly on the performance of artificial intelligence approaches. One of those studies to mention is the one performed on classification algorithms for pattern recognition by So Young Sohn in 1999 based on some in-house implemented statistics tools without considering the meta-analysis software. Moreover, in 2015,Horn et al have conducted a systematic review on functional brain imaging studies on assessing the familiarity of artificial neural networks and discussed their pros and cons in terms of their experimental conflicting results based on a meta-analysis on 68 publishedarticles. In another recent study, the role of real-time biomedical systems has been evaluated by a meta-analysis approach on 134 real-times papers in terms of computational complexity, delay and speed up considering various types of algorithms and hardware implementation. Recently, two types of systematic review and analysis have been performed which shows the potential non-mature trends of this approach in artificial intelligence based researches.In the first one the authors studied the performance of different machine learning algorithms for heart disease diagnosis; however, the metaanalysis part was not performed due to the existence of heterogeneity in the final included studies through the PRISMA (Preferred reporting items for systematic reviews and meta-analyses) checklist. And in the second one, the performance of several DNA based encryption algorithms based according to the results obtained from previous publications has been proposed where, it has been found out that there were no improvements in the proposed algorithms and it has been suggested that a dataset of images should be available in order to test and evaluate the performance of methodologies. However, the methodologies should also be available for public use. Moreover, the analyses section can be carried out through a simple statistical student’s t test analysisor the metaanalysis procedure using available tools such as MetaDisc, MIX, and Meta-Analyst. While comparing the two environments (i.e., clinical and computational), there are in-common units for decision making in diagnosing symptoms which are human (brain system and some data) and computer (artificial intelligence systems and some data). This outstanding feature and the abovementioned examples makes the meta-analysis studies applicable to the researches performed based on artificial intelligence systems, too. This will open a new view on interactions between the results obtained from previous studies while considering their special algorithms, different datasets, and possible biases. One more thing to emphasize for the future research studies is on publicizing the datasets and the implemented algorithms in terms of web servers, Java, C++ and Matlab libraries or R packages to make the results re-generable using new datasets which make them more comparable with new designed methodologies to ease the metaanalysis robust studies. As, it is also clear, most of the webservers and datasets in the medical parts coupled with data derived from bioscience knowledge are publicly.","",""
45,"Ashok K. Goel","A 30-year case study and 15 principles: Implications of an artificial intelligence methodology for functional modeling",2013,"","","","",199,"2022-07-13 09:33:26","","10.1017/S0890060413000218","","",,,,,45,5.00,45,1,9,"Abstract Research on design and analysis of complex systems has led to many functional representations with several meanings of function. This work on conceptual design uses a family of representations called structure–behavior–function (SBF) models. The SBF family ranges from behavior–function models of abstract design patterns to drawing–shape–SBF models that couple SBF models with visuospatial knowledge of technological systems. Development of SBF modeling is an instance of cognitively oriented artificial intelligence research that seeks to understand human cognition and build intelligent agents for addressing complex tasks such as design. This paper first traces the development of SBF modeling as our perspective on design evolved from that of problem solving to that of memory and learning. Next, the development of SBF modeling as a case study is used to abstract some of the core principles of an artificial intelligence methodology for functional modeling. Finally, some implications of the artificial intelligence methodology for different meanings of function are examined.","",""
0,"María del Rocío Soto Flores, Ingrid Yadibel Cuevas Zuñiga, Susana Garduño Román","The Formation of Human Capital and Its Relationship With the Knowledge Society in Mexico",2019,"","","","",200,"2022-07-13 09:33:26","","10.4018/978-1-5225-8461-2.CH003","","",,,,,0,0.00,0,3,3,"The processes of economic globalization and accelerating technological change have led to changes in economic and social life at a global level. New technologies, such as the TICs, systems of artificial intelligence, scanning, connectivity, nanotechnology, and biotechnology, among others, have transformed the national productive structures and human capital that require technologies disruptive today. In this context, education has become the main element of the knowledge society and training of human capital that demands a knowledge-based economy. The objective of the chapter is to analyze the relationship between human capital formations in the construction of a society of knowledge in Mexico. The structure is organized in three sections: 1) an analysis of the knowledge society, 2) the formation of human capital and the institutions of higher education in the knowledge society, and 3) human capital formation and its relationship in the construction of a society of knowledge in Mexico.","",""
