Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
2,"Sunny Raj","Towards Robust Artificial Intelligence Systems",2020,"","","","",1,"2022-07-13 09:32:18","","","","",,,,,2,1.00,2,1,2,"Adoption of deep neural networks (DNNs) into safety-critical and high-assurance systems has been hindered by the inability of DNNs to handle adversarial and out-of-distribution input. State-ofthe-art DNNs misclassify adversarial input and give high confidence output for out-of-distribution input. We attempt to solve this problem by employing two approaches, first, by detecting adversarial input and, second, by developing a confidence metric that can indicate when a DNN system has reached its limits and is not performing to the desired specifications. The effectiveness of our method at detecting adversarial input is demonstrated against the popular DeepFool adversarial image generation method. On a benchmark of 50,000 randomly chosen ImageNet adversarial images generated for CaffeNet and GoogLeNet DNNs, our method can recover the correct label with 95.76% and 97.43% accuracy, respectively. The proposed attribution-based confidence (ABC) metric utilizes attributions used to explain DNN output to characterize whether an output corresponding to an input to the DNN can be trusted. The attribution based approach removes the need to store training or test data or to train an ensemble of models to obtain confidence scores. Hence, the ABC metric can be used when only the trained DNN is available during inference. We test the effectiveness of the ABC metric against both adversarial and out-of-distribution input. We experimental demonstrate that the ABC metric is high for ImageNet input and low for adversarial input generated by FGSM, PGD, DeepFool, CW, and adversarial patch methods. For a DNN trained on MNIST images, ABC metric is high for in-distribution MNIST input and low for out-of-distribution Fashion-MNIST and notMNIST input.","",""
0,"A. S. Rakin, Ye Wang, S. Aeron, T. Koike-Akino, P. Moulin, K. Parsons","Towards Universal Adversarial Examples and Defenses /Author=Rakin, Adnan S; Wang, Ye; Aeron, Shuchin; Koike-Akino, Toshiaki; Moulin, Pierre; Parsons, Kieran /CreationDate=October 21, 2021 /Subject=Artificial Intelligence, Machine Learning, Signal Processing",2021,"","","","",2,"2022-07-13 09:32:18","","","","",,,,,0,0.00,0,6,1,"Adversarial example attacks have recently exposed the severe vulnerability of neural network models. However, most of the existing attacks require some form of target model information (i.e., weights/model inquiry/architecture) to improve the efficacy of the attack. We leverage the information-theoretic connections between robust learning and generalized rate-distortion theory to formulate a universal adversarial example (UAE) generation algorithm. Our algorithm trains an offline adversarial generator to minimize the mutual information of a given data distribution. At the inference phase, our UAE can efficiently generate effective adversary examples without high computation cost.These adversarial examples in turn allow for developing universal defense responses through adversarial training. Our experiments demonstrate promising gains in improving the training efficiency of conventional adversarial training IEEE Information Theory Workshop 2021 c © 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Mitsubishi Electric Research Laboratories, Inc. 201 Broadway, Cambridge, Massachusetts 02139 Towards Universal Adversarial Examples and Defenses Adnan Siraj Rakin, Ye Wang, Shuchin Aeron, Toshiaki Koike-Akino, Pierre Moulin, Kieran Parsons ∗Arizona State University, †Mitsubishi Electric Research Laboratories (MERL), ‡Tufts University, §University of Illinois at Urbana-Champaign ∗asrakin@asu.edu, †{yewang, koike, parsons}@merl.com, ‡shuchin@ece.tufts.edu, §pmoulin@illinois.edu Abstract—Adversarial examples have recently exposed the severe vulnerability of neural network models. However, most of the existing attacks require some form of target model information (i.e., weights/model inquiry/architecture) to improve the efficacy of the attack. We leverage the information-theoretic connections between robust learning and generalized rate-distortion theory to formulate a universal adversarial example (UAE) generation algorithm. Our algorithm trains an offline adversarial generator to minimize the mutual information between the label and perturbed data. At the inference phase, our UAE method can efficiently generate effective adversarial examples without high computation cost. These adversarial examples in turn allow for developing universal defenses through adversarial training. Our experiments demonstrate promising gains in improving the training efficiency of conventional adversarial training.Adversarial examples have recently exposed the severe vulnerability of neural network models. However, most of the existing attacks require some form of target model information (i.e., weights/model inquiry/architecture) to improve the efficacy of the attack. We leverage the information-theoretic connections between robust learning and generalized rate-distortion theory to formulate a universal adversarial example (UAE) generation algorithm. Our algorithm trains an offline adversarial generator to minimize the mutual information between the label and perturbed data. At the inference phase, our UAE method can efficiently generate effective adversarial examples without high computation cost. These adversarial examples in turn allow for developing universal defenses through adversarial training. Our experiments demonstrate promising gains in improving the training efficiency of conventional adversarial training.","",""
5,"Shen Wang, Zhuobiao Qiao","Robust Pervasive Detection for Adversarial Samples of Artificial Intelligence in IoT Environments",2019,"","","","",3,"2022-07-13 09:32:18","","10.1109/ACCESS.2019.2919695","","",,,,,5,1.67,3,2,3,"Nowadays, artificial intelligence technologies (e.g., deep neural networks) have been used widely in the Internet of Things (IoT) to provide smart services and sensing data processing. The evolving neural network even exceeds the human cognitive level. However, the accuracy of these structures depends to some extent on the accuracy of the training data. Some well-designed generated antagonistic disturbances are sufficient to deceive model when added to images. Such attacks cause the classifiers trained by the neural network to misidentify the object and thus completely fail. On the other hand, the various existing defensive methods that have been proposed suffer from two criticisms. The first thing that bears the brunt is unsatisfactory detection rate due to low robustness toward the adversarial sample. Second, the excessive dependence on the output of specific network structure layers hinders the emergence of universal schemes. In this paper, we propose the large margin cosine estimation (LMCE) detection scheme to overcome the above shortcomings, making the detection independent and universal. We illustrate the principle of our approach and demonstrate the significance and analysis of some important parameters. Moreover, we model various types of adversarial attacks and establish proposed defense mechanisms against them and evaluate our approach from different aspects. This method has been clearly validated on a range of standard datasets including MNIST, CIFAR-10, and SVHN. The assessment strongly reflects the robustness and pervasive of this approach in the face of various white and semi-white box attacks.","",""
2,"Erik Hemberg, Linda Zhang, Una-May O’Reilly","Exploring Adversarial Artificial Intelligence for Autonomous Adaptive Cyber Defense",2020,"","","","",4,"2022-07-13 09:32:18","","10.1007/978-3-030-33432-1_3","","",,,,,2,1.00,1,3,2,"","",""
2,"Charles Rogers, John Bugg, C. Nyheim, Will Gebhardt, Brian Andris, Evan Heitman, C. Fleming","Adversarial Artificial Intelligence for Overhead Imagery Classification Models",2019,"","","","",5,"2022-07-13 09:32:18","","10.1109/SIEDS.2019.8735608","","",,,,,2,0.67,0,7,3,"In overhead object detection, computers are increasingly replacing humans at spotting and identifying specific items within images through the use of machine learning (ML). These ML programs must be both accurate and robust. Accuracy means the results must be trusted enough to substitute for the manual deduction process. Robustness is the magnitude to which the network can handle discrepancies within the images. One way to gauge the robustness is through the use of adversarial networks. Adversarial algorithms are trained using perturbations of the image to reduce the accuracy of an existing classification model. The greater degree of perturbations a model can withstand, the more robust it is. In this paper, comparisons of existing deep neural network models and the advancement of adversarial AI are explored. While there is some published research about AI and adversarial networks, very little discusses this particular utilization for overhead imagery. This paper focuses on overhead imagery, specifically that of ships. Using a public Kaggle dataset, we developed multiple models to detect ships in overhead imagery, specifically ResNet50, DenseNet201, and InceptionV3. The goal of the adversarial works is to manipulate an image so that its contents are misclassified. This paper focuses specifically on producing perturbations that can be recreated in the physical world. This serves to account for physical conditions, whether intentional or not, that could reduce accuracy within our network. While there are military applications for this specific research, the general findings can be applied to all AI overhead image classification topics. This work will explore both the vulnerabilities of existing classifier neural net models and the visualization of these vulnerabilities.","",""
5,"Ayodeji Oseni, Nour Moustafa, H. Janicke, Peng Liu, Z. Tari, A. Vasilakos","Security and Privacy for Artificial Intelligence: Opportunities and Challenges",2021,"","","","",6,"2022-07-13 09:32:18","","","","",,,,,5,5.00,1,6,1,"The increased adoption of Artificial Intelligence (AI) presents an opportunity to solve many socio-economic and environmental challenges; however, this cannot happen without securing AI-enabled technologies. In recent years, most AI models are vulnerable to advanced and sophisticated hacking techniques. This challenge has motivated concerted research efforts into adversarial AI, with the aim of developing robust machine and deep learning models that are resilient to different types of adversarial scenarios. In this paper, we present a holistic cyber security review that demonstrates adversarial attacks against AI applications, including aspects such as adversarial knowledge and capabilities, as well as existing methods for generating adversarial examples and existing cyber defence models. We explain mathematical AI models, especially new variants of reinforcement and federated learning, to demonstrate how attack vectors would exploit vulnerabilities of AI models. We also propose a systematic framework for demonstrating attack techniques against AI applications, and reviewed several cyber defences that would protect the AI applications against those attacks. We also highlight the importance of understanding the adversarial goals and their capabilities, especially the recent attacks against industry applications, to develop adaptive defences that assess to secure AI applications. Finally, we describe the main challenges and future research directions in the domain of security and privacy of AI technologies.","",""
0,"Chuan Zhou, Duohe Ma, Tianwei Zhang, Liming Wang","Generating Adversarial Examples for Robust Deception against Image Transfer and Reloading",2021,"","","","",7,"2022-07-13 09:32:18","","10.1109/icaice54393.2021.00159","","",,,,,0,0.00,0,4,1,"Adversarial examples play an irreplaceable role in evaluating DNNs' security and robustness. It's essential to understanding the adversarial examples' effectiveness to utilize them for model improvement. In this paper, we explore the impact of input transformation on adversarial examples. First, we discover a new phenomenon. The process of RELOAD or TRANSFER may deactivate adversarial examples' malicious functionality. The reason is that these processes would reduce pixel precision, which counters the perturbation added by the adversary. We validate this finding on different mainstream adversarial algorithms. Second, we propose a novel Confidence Iteration method, which can generate more robust adversarial examples. The key idea is to set the confidence threshold and add the pixel loss caused by image reloading or transferring into the calculation. We integrate our solution with existing adversarial algorithms. Experiments indicate that such integration can significantly increase the adversarial attacks' success rate.","",""
2,"Marie Franziska Thomas, F. Kofler, Lioba Grundl, T. Finck, Hongwei Li, C. Zimmer, Bjorn H. Menze, B. Wiestler","Improving Automated Glioma Segmentation in Routine Clinical Use Through Artificial Intelligence-Based Replacement of Missing Sequences With Synthetic Magnetic Resonance Imaging Scans.",2021,"","","","",8,"2022-07-13 09:32:18","","10.1097/RLI.0000000000000828","","",,,,,2,2.00,0,8,1,"OBJECTIVES Although automated glioma segmentation holds promise for objective assessment of tumor biology and response, its routine clinical use is impaired by missing sequences, for example, due to motion artifacts. The aim of our study was to develop and validate a generative adversarial network for synthesizing missing sequences to allow for a robust automated segmentation.   MATERIALS AND METHODS Our model was trained on data from The Cancer Imaging Archive (n = 238 WHO II-IV gliomas) to synthesize either missing FLAIR, T2-weighted, T1-weighted (T1w), or contrast-enhanced T1w images from available sequences, using a novel tumor-targeting loss to improve synthesis of tumor areas. We validated performance in a test set from both the REMBRANDT repository and our local institution (n = 68 WHO II-IV gliomas), using qualitative image appearance metrics, but also segmentation performance with state-of-the-art segmentation models. Segmentation of synthetic images was compared with 2 commonly used strategies for handling missing input data, entering a blank mask or copying an existing sequence.   RESULTS Across tumor areas and missing sequences, synthetic images generally outperformed both conventional approaches, in particular when FLAIR was missing. Here, for edema and whole tumor segmentation, we improved the Dice score, a common metric for evaluation of segmentation performance, by 12% and 11%, respectively, over the best conventional method. No method was able to reliably replace missing contrast-enhanced T1w images.   DISCUSSION Replacing missing nonenhanced magnetic resonance sequences via synthetic images significantly improves segmentation quality over most conventional approaches. This model is freely available and facilitates more widespread use of automated segmentation in routine clinical use, where missing sequences are common.","",""
0,"Yufeng Zhang, Zhuoran Yang, Zhaoran Wang","Provably Efficient Actor-Critic for Risk-Sensitive and Robust Adversarial RL: A Linear-Quadratic Case",2021,"","","","",9,"2022-07-13 09:32:18","","","","",,,,,0,0.00,0,3,1,"Risk-sensitivity plays a central role in artificial intelligence safety. In this paper, we study the global convergence of the actor-critic algorithm for risk-sensitive reinforcement learning (RSRL) with exponential utility, which remains challenging for policy optimization as it lacks the linearity needed to formulate policy gradient. To bypass such an issue of nonlinearity, we resort to the equivalence between RSRL and robust adversarial reinforcement learning (RARL), which is formulated as a zero-sum Markov game with a hypothetical adversary. In particular, the Nash equilibrium (NE) of such a game yields the optimal policy for RSRL, which is provably robust. We focus on a simple yet fundamental setting known as linear-quadratic (LQ) game. To attain the optimal policy, we develop a nested natural actor-critic algorithm, which provably converges to the NE of the LQ game at a sublinear rate, thus solving both RSRL and RARL. To the best knowledge, the proposed nested actor-critic algorithm appears to be the first model-free policy optimization algorithm that provably attains the optimal policy for RSRL and RARL in the LQ setting, which sheds light on more general settings.","",""
6,"N. Elkin-Koren","Contesting algorithms: Restoring the public interest in content filtering by artificial intelligence",2020,"","","","",10,"2022-07-13 09:32:18","","10.1177/2053951720932296","","",,,,,6,3.00,6,1,2,"In recent years, artificial intelligence has been deployed by online platforms to prevent the upload of allegedly illegal content or to remove unwarranted expressions. These systems are trained to spot objectionable content and to remove it, block it, or filter it out before it is even uploaded. Artificial intelligence filters offer a robust approach to content moderation which is shaping the public sphere. This dramatic shift in norm setting and law enforcement is potentially game-changing for democracy. Artificial intelligence filters carry censorial power, which could bypass traditional checks and balances secured by law. Their opaque and dynamic nature creates barriers to oversight, and conceals critical value choices and tradeoffs. Currently, we lack adequate tools to hold them accountable. This paper seeks to address this gap by introducing an adversarial procedure— – Contesting Algorithms. It proposes to deliberately introduce friction into the dominant removal systems governed by artificial intelligence. Algorithmic content moderation often seeks to optimize a single goal, such as removing copyright-infringing materials or blocking hate speech, while other values in the public interest, such as fair use or free speech, are often neglected. Contesting algorithms introduce an adversarial design which reflects conflicting values, and thereby may offer a check on dominant removal systems. Facilitating an adversarial intervention may promote democratic principles by keeping society in the loop. An adversarial public artificial intelligence system could enhance dynamic transparency, facilitate an alternative public articulation of social values using machine learning systems, and restore societal power to deliberate and determine social tradeoffs.","",""
2,"Simon Delecourt, Li Guo","Building a Robust Mobile Payment Fraud Detection System with Adversarial Examples",2019,"","","","",11,"2022-07-13 09:32:18","","10.1109/AIKE.2019.00026","","",,,,,2,0.67,1,2,3,"Mobile payment is becoming a major payment method in many countries. However, the rate of payment fraud with mobile is higher than with credit card. One potential reason is that mobile data is easier to be modified than credit card data by fraudsters, which degrades our data-driven fraud detection system. Supervised learning methods are pervasively used in fraud detection. However, these supervised learning methods used in fraud detection have traditionally been developed following the assumption that the environment is benign; there are no adversaries trying to evade fraud detection system. In this paper, we took potential reactions of fraudsters into consideration to build a robust mobile fraud detection system using adversarial examples. Experimental results showed that the performance of our proposed method was improved in both benign and adversarial environments.","",""
2,"D. Vos, S. Verwer","Robust Optimal Classification Trees Against Adversarial Examples",2021,"","","","",12,"2022-07-13 09:32:18","","10.1609/aaai.v36i8.20829","","",,,,,2,2.00,1,2,1,"Decision trees are a popular choice of explainable model, but just like neural networks, they suffer from adversarial examples. Existing algorithms for fitting decision trees robust against adversarial examples are greedy heuristics and lack approximation guarantees. In this paper we propose ROCT, a collection of methods to train decision trees that are optimally robust against user-specified attack models. We show that the min-max optimization problem that arises in adversarial learning can be solved using a single minimization formulation for decision trees with 0-1 loss. We propose such formulations in Mixed-Integer Linear Programming and Maximum Satisfiability, which widely available solvers can optimize. We also present a method that determines the upper bound on adversarial accuracy for any model using bipartite matching. Our experimental results demonstrate that the existing heuristics achieve close to optimal scores while ROCT achieves state-of-the-art scores.","",""
1,"S. Silva, Arun Das, I. Scarff, Peyman Najafirad","Adaptive Clustering of Robust Semantic Representations for Adversarial Image Purification",2021,"","","","",13,"2022-07-13 09:32:18","","","","",,,,,1,1.00,0,4,1,"Advances in Artificial Intelligence (AI) have made it possible to automate human-level visual search and perception tasks on the massive sets of image data shared on social media on a daily basis. However, AI-based automated filters are highly susceptible to deliberate image attacks that can lead to content misclassification of cyberbulling, child sexual abuse material (CSAM), adult content, and deepfakes. One of the most effective methods to defend against such disturbances is adversarial training, but this comes at the cost of generalization for unseen attacks and transferability across models. In this article, we propose a robust defense against adversarial image attacks, which is model agnostic and generalizable to unseen adversaries. We begin with a baseline model, extracting the latent representations for each class and adaptively clustering the latent representations that share a semantic similarity. Next, we obtain the distributions for these clustered latent representations along with their originating images. We then learn semantic reconstruction dictionaries (SRD). We adversarially train a new model constraining the latent space representation to minimize the distance between the adversarial latent representation and the true cluster distribution. To purify the image, we decompose the input into low and highfrequency components. The high-frequency component is reconstructed based on the best SRD from the clean dataset. In order to evaluate the best SRD, we rely on the distance between the robust latent representations and semantic cluster distributions. The output is a purified image with no perturbations. Evaluations using comprehensive datasets including image benchmarks and social media images demonstrate that our proposed purification approach guards and enhances the accuracy of AI-based image filters for unlawful and harmful perturbed images considerably.","",""
5,"Nag Mani, M. Moh, T. Moh","Towards Robust Ensemble Defense Against Adversarial Examples Attack",2019,"","","","",14,"2022-07-13 09:32:18","","10.1109/GLOBECOM38437.2019.9013408","","",,,,,5,1.67,2,3,3,"With recent advancements in the field of artificial intelligence, deep learning has created a niche in the technology space and is being actively used in autonomous and IoT systems globally. Unfortunately, these deep learning models have become susceptible to adversarial attacks that can severely impact its integrity. Research has shown that many state-of-the-art models are vulnerable to attacks by well- crafted adversarial examples. These adversarial examples are perturbed versions of clean data with a small amount of noise added to it. These adversarial samples are imperceptible to the human eye yet they can easily fool the targeted model. The exposed vulnerabilities of these models raise the question of their usability in safety-critical real-world applications such as autonomous driving and medical applications. In this work, we have documented the effectiveness of six different gradient-based adversarial attacks on ResNet image recognition model. Defending against these adversaries is challenging. Adversarial re-training has been one of the widely used defense technique. It aims at training a more robust model capable of handling the adversarial examples attack by itself. We showcase the limitations of traditional adversarial-retraining techniques that could be effective against some adversaries but does not protect against more sophisticated attacks. We present a new ensemble defense strategy using adversarial retraining technique that is capable of withstanding six adversarial attacks on cifar10 dataset with a minimum accuracy of 89.31%.","",""
0,"Seok-Hwan Choi, Jinmyeong Shin, Peng Liu, Yoon-Ho Choi","ARGAN: Adversarially Robust Generative Adversarial Networks for Deep Neural Networks Against Adversarial Examples",2022,"","","","",15,"2022-07-13 09:32:18","","10.1109/access.2022.3160283","","",,,,,0,0.00,0,4,1,"An adversarial example, which is an input instance with small, intentional feature perturbations to machine learning models, represents a concrete problem in Artificial intelligence safety. As an emerging defense method to defend against adversarial examples, generative adversarial networks-based defense methods have recently been studied. However, the performance of the state-of-the-art generative adversarial networks-based defense methods is limited because the target deep neural network models with generative adversarial networks-based defense methods are robust against adversarial examples but make a false decision for legitimate input data. To solve the accuracy degradation of the generative adversarial networks-based defense methods for legitimate input data, we propose a new generative adversarial networks-based defense method, which is called Adversarially Robust Generative Adversarial Networks(ARGAN). While converting input data to machine learning models using the two-step transformation architecture, ARGAN learns the generator model to reflect the vulnerability of the target deep neural network model against adversarial examples and optimizes parameter values of the generator model for a joint loss function. From the experimental results under various datasets collected from diverse applications, we show that the accuracy of ARGAN for legitimate input data is good-enough while keeping the target deep neural network model robust against adversarial examples. We also show that the accuracy of ARGAN outperforms the accuracy of the state-of-the-art generative adversarial networks-based defense methods.","",""
0,"D. Lange","Robustness of artificial intelligence in the face of novelty",2022,"","","","",16,"2022-07-13 09:32:18","","10.1117/12.2622912","","",,,,,0,0.00,0,1,1,"A critical factor in utilizing agents with Artificial Intelligence (AI) is their robustness to novelty. AI agents include models that are either engineered or trained. Engineered models include knowledge of those aspects of the environment that are known and considered important by the engineers. Learned models form embeddings of aspects of the environment based on connections made through the training data. In operation, however, a rich environment is likely to present challenges not seen in training sets or accounted for in engineered models. Worse still, adversarial environments are subject to change by opponents. A program at the Defense Advanced Research Project Agency (DARPA) seeks to develop the science necessary to develop and evaluate agents that are robust to novelty. This capability will be required, before AI has the role envisioned within mission critical environments.","",""
0,"Konstantinos G. Liakos, G. Georgakilas, F. Plessas, P. Kitsos","GAINESIS: Generative Artificial Intelligence NEtlists SynthesIS",2022,"","","","",17,"2022-07-13 09:32:18","","10.3390/electronics11020245","","",,,,,0,0.00,0,4,1,"A significant problem in the field of hardware security consists of hardware trojan (HT) viruses. The insertion of HTs into a circuit can be applied for each phase of the circuit chain of production. HTs degrade the infected circuit, destroy it or leak encrypted data. Nowadays, efforts are being made to address HTs through machine learning (ML) techniques, mainly for the gate-level netlist (GLN) phase, but there are some restrictions. Specifically, the number and variety of normal and infected circuits that exist through the free public libraries, such as Trust-HUB, are based on the few samples of benchmarks that have been created from circuits large in size. Thus, it is difficult, based on these data, to develop robust ML-based models against HTs. In this paper, we propose a new deep learning (DL) tool named Generative Artificial Intelligence Netlists SynthesIS (GAINESIS). GAINESIS is based on the Wasserstein Conditional Generative Adversarial Network (WCGAN) algorithm and area–power analysis features from the GLN phase and synthesizes new normal and infected circuit samples for this phase. Based on our GAINESIS tool, we synthesized new data sets, different in size, and developed and compared seven ML classifiers. The results demonstrate that our new generated data sets significantly enhance the performance of ML classifiers compared with the initial data set of Trust-HUB.","",""
0,"M. Florez, M. Caporale, Pakpoom Buabthong, Z. Ross, D. Asimaki, Men‐Andrin Meier","Data-Driven Synthesis of Broadband Earthquake Ground Motions Using Artificial Intelligence",2022,"","","","",18,"2022-07-13 09:32:18","","10.1785/0120210264","","",,,,,0,0.00,0,6,1,"  Robust estimation of ground motions generated by scenario earthquakes is critical for many engineering applications. We leverage recent advances in generative adversarial networks (GANs) to develop a new framework for synthesizing earthquake acceleration time histories. Our approach extends the Wasserstein GAN formulation to allow for the generation of ground motions conditioned on a set of continuous physical variables. Our model is trained to approximate the intrinsic probability distribution of a massive set of strong-motion recordings from Japan. We show that the trained generator model can synthesize realistic three-component accelerograms conditioned on magnitude, distance, and VS30. Our model captures most of the relevant statistical features of the acceleration spectra and waveform envelopes. The output seismograms display clear P- and S-wave arrivals with the appropriate energy content and relative onset timing. The synthesized peak ground acceleration estimates are also consistent with observations. We develop a set of metrics that allow us to assess the training process’s stability and to tune model hyperparameters. We further show that the trained generator network can interpolate to conditions in which no earthquake ground-motion recordings exist. Our approach allows for the on-demand synthesis of accelerograms for engineering purposes.","",""
0,"Hatma Suryotrisongko, Y. Musashi, A. Tsuneda, K. Sugitani","Robust Botnet DGA Detection: Blending XAI and OSINT for Cyber Threat Intelligence Sharing",2022,"","","","",19,"2022-07-13 09:32:18","","10.1109/ACCESS.2022.3162588","","",,,,,0,0.00,0,4,1,"We investigated 12 years DNS query logs of our campus network and identified phenomena of malicious botnet domain generation algorithm (DGA) traffic. DGA-based botnets are difficult to detect using cyber threat intelligence (CTI) systems based on blocklists. Artificial intelligence (AI)/machine learning (ML)-based CTI systems are required. This study (1) proposed a model to detect DGA-based traffic based on statistical features with datasets comprising 55 DGA families, (2) discussed how CTI can be expanded with computable CTI paradigm, and (3) described how to improve the explainability of the model outputs by blending explainable AI (XAI) and open-source intelligence (OSINT) for trust problems, an antidote for skepticism to the shared models and preventing automation bias. We define the XAI-OSINT blending as aggregations of OSINT for AI/ML model outcome validation. Experimental results show the effectiveness of our models (96.3% accuracy). Our random forest model provides better robustness against three state-of-the-art DGA adversarial attacks (CharBot, DeepDGA, MaskDGA) compared with character-based deep learning models (Endgame, CMU, NYU, MIT). We demonstrate the sharing mechanism and confirm that the XAI-OSINT blending improves trust for CTI sharing as evidence to validate our proposed computable CTI paradigm to assist security analysts in security operations centers using an automated, explainable OSINT approach (for second opinion). Therefore, the computable CTI reduces manual intervention in critical cybersecurity decision-making.","",""
4,"D. Yang, Jay Xiong, Xincheng Li, Xu Yan, J. Raiti, Yuntao Wang, Huaqiang Wu, Zhenyu Zhong","Building Towards ""Invisible Cloak"": Robust Physical Adversarial Attack on YOLO Object Detector",2018,"","","","",20,"2022-07-13 09:32:18","","10.1109/UEMCON.2018.8796670","","",,,,,4,1.00,1,8,4,"Deep learning based object detection algorithms like R-CNN, SSD, YOLO have been applied to many scenarios, including video surveillance, autonomous vehicle, intelligent robotics et al. With more and more application and autonomy left to deep learning based artificial intelligence, humans want to ensure that the machine does the best for them under their control. However, deep learning algorithms are known to be vulnerable to carefully crafted input known as adversarial examples which makes it possible for an attacker to fool an AI system. In this work, we explored the mechanism behind the YOLO object detector and proposed an optimization method to craft adversarial examples to attack the YOLO model. The experiment shows that this white box attack method is effective and has a success rate of 100% in crafting digital adversarial examples to fool the YOLO model. We also proposed a robust physical adversarial sticker generation method based on an extended Expectation Over Transformation (EOT) method(a method to craft adversarial example in the physical world). We conduct experiments to find the most effective approach to generate adversarial stickers. We tested the stickers both digitally as a watermark and physically showing it on an electronic screen on the front surface of a person. Our result shows that the sticker attack as a watermark has a success rate of 90% and 45% on photos taken indoors and on random 318 pictures from ImageNet. Our physical attack also has a success rate of 72% on photos taken indoors. We shared our project source code on the Github and our work is reproducible.","",""
20,"Joseph Clements, Yuzhe Yang, Ankur A Sharma, Hongxin Hu, Yingjie Lao","Rallying Adversarial Techniques against Deep Learning for Network Security",2019,"","","","",21,"2022-07-13 09:32:18","","10.1109/SSCI50451.2021.9660011","","",,,,,20,6.67,4,5,3,"Recent advances in artificial intelligence and the increasing need for robust defensive measures in network security have led to the adoption of deep learning approaches for network intrusion detection systems (NIDS). These methods have achieved superior performance against conventional network attacks, enabling unique and dynamic security systems in realworld applications. Adversarial machine learning, unfortunately, has recently shown that deep learning models are inherently vulnerable to adversarial modifications on their input data. In this work, we explore the potential of adversarial entities to compromise such vulnerabilities to compromise deep learning-based NIDS systems. Specifically, we show that by modifying on average as little as 1.38 of an observed packet's input features, an adversary can generate malicious inputs that effectively fool a target deep learning-based NIDS. Therefore, it is crucial to consider the performance from the conventional network security perspective and the adversarial machine learning domain when designing such systems.","",""
55,"Sayak Paul, Pin-Yu Chen","Vision Transformers are Robust Learners",2021,"","","","",22,"2022-07-13 09:32:18","","10.1609/aaai.v36i2.20103","","",,,,,55,55.00,28,2,1,"Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy. What remains largely unexplored is their robustness evaluation and attribution. In this work, we study the robustness of the Vision Transformer (ViT) (Dosovitskiy et al. 2021) against common corruptions and perturbations, distribution shifts, and natural adversarial examples. We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT(Dosovitskiy et al. 2021) models and SOTA convolutional neural networks (CNNs), Big-Transfer (Kolesnikov et al. 2020). Through a series of six systematically designed experiments, we then present analyses that provide both quantitative andqualitative indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1accuracy of 28.10% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness. Code for reproducing our experiments is available at https://git.io/J3VO0.","",""
9,"G. Nneji, Jingye Cai, Deng Jianhua, H. N. Monday, I. A. Chikwendu, Ariyo Oluwasanmi, E. C. James, Goodness Temofe Mgbejime","Enhancing Low Quality in Radiograph Datasets Using Wavelet Transform Convolutional Neural Network and Generative Adversarial Network for COVID-19 Identification",2021,"","","","",23,"2022-07-13 09:32:18","","10.1109/PRAI53619.2021.9551043","","",,,,,9,9.00,1,8,1,"The coronavirus disease of 2019 (COVID-19) pandemic has caused a global public health epidemic since there is no 100% vaccine to cure or prevent the further spread of the virus. With the ever-increasing number of new infections, creating automated methods for COVID-19 identification of Chest X-ray images is critical to aiding clinical diagnosis and reducing the time-consumption for image interpretation. This paper proposes a novel joint framework for accurate COVID-19 identification by integrating an enhanced super-resolution generative adversarial network with a noise reduction filter bank of wavelet transform convolutional neural network on both Chest X-ray and Chest Tomography images for COVID-19 identification. The super-resolution utilized in this study is to enhance the image quality while the wavelet transform Convolutional Neural Network architecture is used to accurately identify COVID-19. Our proposed architecture is very robust to noise and vanishing gradient problem. We used public domain datasets of Chest x-ray images and Chest Tomography to train and check the performance of our COVID-19 identification task. This experiment shows that our system is consistently efficient by accuracy of 0.988, sensitivity of 0.994, and specificity of 0.987, AUC of 0.99, F1-score of 0.982 and 0.989 for precision using the Chest X-ray dataset while for Chest Tomography dataset, an accuracy of 0.978, sensitivity of 0.981, and specificity of 0.979, AUC of 0.985, F1-score of 0.961 and precision of 0.980. These performances have also outweighed other established state-of-the-art learning methods.","",""
6,"Ke Wang, Peng Xu, Chien-Ming Chen, S. Kumari, M. Shojafar, M. Alazab","Neural Architecture Search for Robust Networks in 6G-Enabled Massive IoT Domain",2020,"","","","",24,"2022-07-13 09:32:18","","10.1109/JIOT.2020.3040281","","",,,,,6,3.00,1,6,2,"6G technology enables artificial intelligence (AI)-based massive IoT to manage network resources and data with ultra high speed, responsive network, and wide coverage. However, many AI-enabled Internet-of-Things (AIoT) systems are vulnerable to adversarial example attacks. Therefore, designing robust deep learning models that can be deployed on resource-constrained devices has become an important research topic in the field of 6G-enabled AIoT. In this article, we propose a method for automatically searching for robust and efficient neural network structures for AIoT systems. By introducing a skip connection structure, a feature map with reduced front-end influence can be used for calculations during the classification process. Additionally, a novel type of densely connected search space is proposed. By relaxing this space, it is possible to search for network structures efficiently. In addition, combined with adversarial training and model delay constraints, we propose a multiobjective gradient optimization method to realize the automatic searching of network structures. Experimental results demonstrate that our method is effective for AIoT systems and superior to state-of-the-art neural architecture search algorithms.","",""
1,"Wenzhao Xiang, Hang Su, Chang Liu, Yandong Guo, Shibao Zheng","Improving Robustness of Adversarial Attacks Using an Affine-Invariant Gradient Estimator",2021,"","","","",25,"2022-07-13 09:32:18","","","","",,,,,1,1.00,0,5,1,"As designers of artiﬁcial intelligence try to outwit hack-ers, both sides continue to hone in on AI’s inherent vulnera-bilities. Designed and trained from certain statistical distributions of data, AI’s deep neural networks (DNNs) remain vulnerable to deceptive inputs that violate a DNN’s statistical, predictive assumptions. Before being fed into a neural network, however, most existing adversarial examples cannot maintain malicious functionality when applied to an afﬁne transformation. For practical purposes, maintaining that malicious functionality serves as an important measure of the robustness of adversarial attacks. To help DNNs learn to defend themselves more thoroughly against attacks, we propose an afﬁne-invariant adversarial attack, which can consistently produce more robust adversarial examples over afﬁne transformations. For efﬁciency, we propose to dis-entangle current afﬁne-transformation strategies from the Euclidean geometry coordinate plane with its geometric translations, rotations and dilations; we reformulate the lat-ter two in polar coordinates. Afterwards, we construct an afﬁne-invariant gradient estimator by convolving the gradient at the original image with derived kernels, which can be integrated with any gradient-based attack methods. Extensive experiments on ImageNet, including some experiments under physical condition, demonstrate that our method can signiﬁcantly improve the afﬁne invariance of adversarial examples and, as a byproduct, improve the transferability of adversarial examples, compared with alternative state-of-the-art methods. 1","",""
4,"Gauri Jagatap, Animesh Basak Chowdhury, S. Garg, C. Hegde","Adversarially Robust Learning via Entropic Regularization",2020,"","","","",26,"2022-07-13 09:32:18","","10.3389/frai.2021.780843","","",,,,,4,2.00,1,4,2,"In this paper we propose a new family of algorithms, ATENT, for training adversarially robust deep neural networks. We formulate a new loss function that is equipped with an additional entropic regularization. Our loss function considers the contribution of adversarial samples that are drawn from a specially designed distribution in the data space that assigns high probability to points with high loss and in the immediate neighborhood of training samples. Our proposed algorithms optimize this loss to seek adversarially robust valleys of the loss landscape. Our approach achieves competitive (or better) performance in terms of robust classification accuracy as compared to several state-of-the-art robust learning approaches on benchmark datasets such as MNIST and CIFAR-10.","",""
0,"Efi Kafali, T. Semertzidis, P. Daras","Delayed Adversarial Training with Non-Sequential Adversarial Epochs",2021,"","","","",27,"2022-07-13 09:32:18","","10.1109/ICTAI52525.2021.00217","","",,,,,0,0.00,0,3,1,"Adversarial Training (AT) has so far been considered as the most effective approach for building robust computer vision architectures that can adequately handle adversarial attacks. However, its high complexity, rising from the need to repeatedly generate attacks against image batches over training epochs, has been a controlling factor to its generalized use. Therefore, to date, AT has only been used for training on vanilla datasets, casting doubt on how soon it can be applied to real world computer vision systems in safety critical domains. Few researchers have to a certain extent addressed the issue by proposing AT variations which have moderately improved the complexity. Extending these advances, this work explores simple yet effective ways to further improve the complexity of AT in terms of training time. Specifically, it is explored through this work whether a) training sequential adversarial epochs and b) attacking the entire dataset during adversarial epochs are necessary for a robust learned model.","",""
0,"Mehmet Melih Arıcı, A. Sen","Improving Robustness of Deep Learning Systems with Fast and Customizable Adversarial Data Generation",2021,"","","","",28,"2022-07-13 09:32:18","","10.1109/AITEST52744.2021.00017","","",,,,,0,0.00,0,2,1,"Deep Learning (DL) is the force behind the success of solving many complicated tasks in recent years. With the use of DL systems in safety-critical applications, it has become of great importance to make these systems robust against adversarial attacks. Adversarial data generation is an effective tool to make DL systems robust against such attacks, with the help of adversarial training. Recent studies focus on gradient-based adversarial attacks. Although they can successfully generate adversarial samples, high computation cost and lack of flexibility over input generation arise the need for an efficient and flexible adversarial attack methodology. In this paper, we present DeepCustom, a fast and customizable adversarial data generation framework towards bridging this gap. Convolutional autoencoders with custom loss functions, enable user-configurable data generation within a much shorter time compared to the state-of-the-art attack method called PGD. Experiments show that our technique produces adversarial samples faster than PGD and using these samples in adversarial training, allows comparable robustness against adversarial attacks.","",""
0,"Adaku Uchendu, Daniel Campoy, Christopher Menart, Alexandra Hildenbrandt","Robustness of Bayesian Neural Networks to White-Box Adversarial Attacks",2021,"","","","",29,"2022-07-13 09:32:18","","10.1109/AIKE52691.2021.00017","","",,,,,0,0.00,0,4,1,"Bayesian Neural Networks (BNNs), unlike Traditional Neural Networks (TNNs) are robust and adept at handling adversarial attacks by incorporating randomness. This randomness improves the estimation of uncertainty, a feature lacking in TNNs. Thus, we investigate the robustness of BNNs to white-box attacks using multiple Bayesian neural architectures. Furthermore, we create our BNN model, called BNN-DenseNet, by fusing Bayesian inference (i.e., variational Bayes) to the DenseNet architecture, and BDAV, by combining this intervention with adversarial training. Experiments are conducted on the CIFAR-10 and FGVC-Aircraft datasets. We attack our models with strong white-box attacks (l∞-FGSM, l∞-PGD, l2-PGD, EOT l∞-FGSM, and EOT l∞-PGD). In all experiments, at least one BNN outperforms traditional neural networks during adversarial attack scenarios. An adversarially-trained BNN outperforms its non-Bayesian, adversarially-trained counterpart in most experiments, and often by significant margins. These experimental results suggest that the dense nature of DenseNet provides robustness advantages that are further amplified by fusing Bayesian Inference with the architecture. Lastly, we investigate network calibration and find that BNNs do not make overconfident predictions, providing evidence that BNNs are also better at measuring uncertainty.","",""
0,"Weijie Kang, Junjie Xue, Jiyang Xiao, Haizhen Zhu, Jianfeng Li, Changjun Li","Multimode Generative Adversarial Networks for Sequence Data Generation",2021,"","","","",30,"2022-07-13 09:32:18","","10.1088/1742-6596/1827/1/012209","","",,,,,0,0.00,0,6,1,"As a new type of artificial intelligence technology, generative adversarial network (GAN) has good data understanding and generation capabilities, and has a wide range of application prospects in the fields of image and speech. However, due to the lack of prior knowledge, its training process is less robust and prone to occur the pattern ignore. Its development is restricted to a certain extent, and its application scope still needs to be expanded. To solve the above problems, this paper introduces a knowledge confidence multimode GAN (KC-MGAN) algorithm, calculates the confidence of the input data through the reasoning method, and then puts the confidence and the input data into the GAN system to generate new sample data. During the training process, the confidence of the input data is continuously calculated, while the generated data samples are continuously evaluated. The training process will end until the GAN system reaches a stable condition. Finally, this paper takes the generation of UAV flight trajectory data as an example to verify the effectiveness of the proposed method. Some explorations have been made for the application of data generation and GAN’s training mode with the prior knowledge.","",""
0,"Shuo Liu, Liwen Xu","Anomaly Detection with Dual Adversarial Training",2021,"","","","",31,"2022-07-13 09:32:18","","10.1109/ICDMW53433.2021.00063","","",,,,,0,0.00,0,2,1,"Anomaly detection is of paramount importance in data mining and artificial intelligence. Deep generative models have been widely used in anomaly detection as a dominant paradigm to model complex and high-dimensional data distribution. However, developing effective and robust anomaly detection systems for complex and high-dimensional data using generative models remains a challenge. In this paper, we propose a novel Dual Adversarial Training method for Anomaly Detection (DAT-AD), which uses the adversarial training idea in Generative Adversarial Network (GAN) and the Virtual Adversarial Training (VAT) idea to improve the effectiveness and robustness of anomaly detector, respectively. In addition, we have also carefully designed the network architecture and the loss function of our method to ensure that the trained network can be utilized to the greatest extent. We demonstrate the superiority of our method by conducting various experiments on tabular and image data.","",""
0,"Mahmoud Ahmed Hossameldeen Mohammad Ahmed Ibrahim","Deep Sequence Models: Learning to Generate Data and Adversarial Attacks",2021,"","","","",32,"2022-07-13 09:32:18","","10.26180/14909385.V1","","",,,,,0,0.00,0,1,1,"Understanding sequential data like natural language sentences and learning to model it with generative models are fundamental research problems in artificial intelligence. Solving them helps to create machines that are imaginative and which can perform human-like reasoning and robust decision making. Advanced sequence models will have a significant impact on key areas including drug discovery, autonomous vehicles, and robotics. This thesis advances research in sequence models in two ways: by introducing controlling mechanisms into generative models, and by learning to efficiently generate attacks on natural language models.","",""
3,"Gerardo Ibarra-Vázquez, G. Olague, Cesar Puente, Mariana Chan-Ley, C. Soubervielle-Montalvo","Automated design of accurate and robust image classifiers with brain programming",2021,"","","","",33,"2022-07-13 09:32:18","","10.1145/3449726.3463179","","",,,,,3,3.00,1,5,1,"Foster the mechanical design of artificial vision requires a delicate balance between high-level analytical methods and the discovery through metaheuristics of near-optimal functions working towards complex visual problems. Evolutionary computation and swarm intelligence have developed strategies that automatically design meaningful deep convolutional neural network architectures to create better image classifiers. However, these architectures have not surpassed hand-craft models working with outdated problems with datasets of icon images. Nowadays, recent concerns about deep convolutional neural networks to adversarial attacks in the form of modifications to the input image can manipulate their output to make them untrustworthy. Brain programming is a hyper-heuristic whose aim is to work at a higher level of abstraction to develop automatically artificial visual cortex algorithms for a problem domain like image classification. This work's primary goal is to employ brain programming to design an artificial visual cortex to produce accurate and robust image classifiers in two problems. We analyze the final models designed by brain programming with the assumption of fooling the system using two adversarial attacks. In both experiments, brain programming constructed artificial brain models capable of competing with hand-crafted deep convolutional neural networks without any influence in the predictions when an adversarial attack is present.","",""
2,"Shubham Sharma, A. Gee, D. Paydarfar, J. Ghosh","FaiR-N: Fair and Robust Neural Networks for Structured Data",2020,"","","","",34,"2022-07-13 09:32:18","","10.1145/3461702.3462559","","",,,,,2,1.00,1,4,2,"Fairness and robustness in machine learning are crucial when individuals are subject to automated decisions made by models in high-stake domains. To promote ethical artificial intelligence, fairness metrics that rely on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias. However, fairness measures that rely on comparing the ability to achieve recourse have been relatively unexplored. In this paper, we present a novel formulation for training neural networks that considers the distance of data observations to the decision boundary such that the new objective: (1) reduces the disparity in the average ability of recourse between individuals in each protected group, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that models trained with this new objective are more fair and adversarially robust neural networks, with similar accuracies, when compared to models without it. We also investigate a trade-off between the recourse-based fairness and robustness objectives. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across protected groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse disparity across groups are considered to train fairer neural networks.","",""
1,"Javier Maroto, Gérôme Bovet, P. Frossard","On the benefits of robust models in modulation recognition",2021,"","","","",35,"2022-07-13 09:32:18","","10.1117/12.2587156","","",,,,,1,1.00,0,3,1,"Given the rapid changes in telecommunication systems and their higher dependence on artificial intelligence, it is increasingly important to have models that can perform well under different, possibly adverse, conditions. Deep Neural Networks (DNNs) using convolutional layers are state-of-the-art in many tasks in communications. However, in other domains, like image classification, DNNs have been shown to be vulnerable to adversarial perturbations, which consist of imperceptible crafted noise that when added to the data fools the model into misclassification. This puts into question the security of DNNs in communication tasks, and in particular in modulation recognition. We propose a novel framework to test the robustness of current state-of-the-art models where the adversarial perturbation strength is dependent on the signal strength and measured with the “signal to perturbation ratio” (SPR). We show that current state-of-the-art models are susceptible to these perturbations. In contrast to current research on the topic of image classification, modulation recognition allows us to have easily accessible insights on the usefulness of the features learned by DNNs by looking at the constellation space. When analyzing these vulnerable models we found that adversarial perturbations do not shift the symbols towards the nearest classes in constellation space. This shows that DNNs do not base their decisions on signal statistics that are important for the Bayes-optimal modulation recognition model, but spurious correlations in the training data. Our feature analysis and proposed framework can help in the task of finding better models for communication systems.","",""
0,"A. Papandreou, A. Kloukiniotis, A. Lalos, K. Moustakas","Deep multi-modal data analysis and fusion for robust scene understanding in CAVs",2021,"","","","",36,"2022-07-13 09:32:18","","10.1109/MMSP53017.2021.9733604","","",,,,,0,0.00,0,4,1,"Deep learning (DL) tends to be the integral part of Autonomous Vehicles (AVs). Therefore the development of scene analysis modules that are robust to various vulnerabilities such as adversarial inputs or cyber-attacks is becoming an imperative need for the future AV perception systems. In this paper, we deal with this issue by exploring the recent progress in Artificial Intelligence (AI) and Machine Learning (ML) to provide holistic situational awareness and eliminate the effect of the previous attacks on the scene analysis modules. We propose novel multi-modal approaches against which achieve robustness to adversarial attacks, by appropriately modifying the analysis Neural networks and by utilizing late fusion methods. More specifically, we propose a holistic approach by adding new layers to a 2D segmentation DL model enhancing its robustness to adversarial noise. Then, a novel late fusion technique has been applied, by extracting direct features from the 3D space and project them into the 2D segmented space for identifying inconsistencies. Extensive evaluation studies using the KITTI odometry dataset provide promising performance results under various types of noise.","",""
0,"Alakananda Mitra, S. Mohanty, P. Corcoran, E. Kougianos","EasyDeep: An IoT Friendly Robust Detection Method for GAN Generated Deepfake Images in Social Media",2021,"","","","",37,"2022-07-13 09:32:18","","10.1007/978-3-030-96466-5_14","","",,,,,0,0.00,0,4,1,"","",""
3,"DonghuaWang, Tingsong Jiang, Jialiang Sun, Weien Zhou, Xiaoya Zhang, Zhiqiang Gong, Wen Yao, Xiaoqian Chen","FCA: Learning a 3D Full-coverage Vehicle Camouflage for Multi-view Physical Adversarial Attack",2021,"","","","",38,"2022-07-13 09:32:18","","10.1609/aaai.v36i2.20141","","",,,,,3,3.00,0,8,1,"Physical adversarial attacks in object detection have attracted increasing attention. However, most previous works focus on hiding the objects from the detector by generating an individual adversarial patch, which only covers the planar part of the vehicle’s surface and fails to attack the detector in physical scenarios for multi-view, long-distance and partially occluded objects. To bridge the gap between digital attacks and physical attacks, we exploit the full 3D vehicle surface to propose a robust Full-coverage Camouflage Attack (FCA) to fool detectors. Specifically, we first try rendering the nonplanar camouflage texture over the full vehicle surface. To mimic the real-world environment conditions, we then introduce a transformation function to transfer the rendered camouflaged vehicle into a photo-realistic scenario. Finally, we design an efficient loss function to optimize the camouflage texture. Experiments show that the full-coverage camouflage attack can not only outperform state-of-the-art methods under various test cases but also generalize to different environments, vehicles, and object detectors.","",""
0,"Lujun Li, Ludwig Kurzinger, Tobias Watzel, G. Rigoll","A Global Discriminant Joint Training Framework for Robust Speech Recognition",2021,"","","","",39,"2022-07-13 09:32:18","","10.1109/ICTAI52525.2021.00088","","",,,,,0,0.00,0,4,1,"Robustness in adverse acoustic conditions is critical for practical human-machine interaction. A common solution for this problem is adding an independent speech enhancement front-end. Nonetheless, due to being trained separately from the automatic speech recognition (ASR) module, the independent enhancement front-end falls into the sub-optimum easily. Besides, the handcrafted loss function of the enhancement module tends to introduce unseen distortions, which even degrade the ASR performance. To address this concern, a promising idea of the joint training is progressively drawing more interests. Nevertheless, none of the previously proposed joint-training frameworks is built on the increasingly popular self-attention mechanism or generative adversarial architecture. This paper proposes a novel joint-training framework, concatenating a speech enhancement generative adversarial network as the front-end and a self-attention based ASR module as the back-end to be jointly trained as an extensive network, to boost the noise robustness of the end-to-end ASR system. A Sinc convolution layer is usefully merged into the speech enhancement front-end for more representative features extraction. Moreover, a discriminant component plays the role of the local guide of the enhancement module and the global guide in the joint training simultaneously, which guides the enhancement front-end to output more desirable features for the subsequent ASR module and thereby offsets the limitation of the separate training and handcrafted loss functions.Systematic experiments reveal that the proposed framework significantly overtakes other competitive solutions, especially in challenging environments.","",""
1,"Wooju Lee, H. Myung","Adversarial Attack for Asynchronous Event-based Data",2021,"","","","",40,"2022-07-13 09:32:18","","10.1609/aaai.v36i2.20010","","",,,,,1,1.00,1,2,1,"Deep neural networks (DNNs) are vulnerable to adversarial examples that are carefully designed to cause the deep learning model to make mistakes. Adversarial examples of 2D images and 3D point clouds have been extensively studied, but studies on event-based data are limited. Event-based data can be an alternative to a 2D image under high-speed movements, such as autonomous driving. However, the given adversarial events make the current deep learning model vulnerable to safety issues. In this work, we generate adversarial examples and then train the robust models for event-based data, for the first time. Our algorithm shifts the time of the original events and generates additional adversarial events. Additional adversarial events are generated in two stages. First, null events are added to the event-based data to generate additional adversarial events. The perturbation size can be controlled with the number of null events. Second, the location and time of additional adversarial events are set to mislead DNNs in a gradient-based attack. Our algorithm achieves an attack success rate of 97.95% on the N-Caltech101 dataset. Furthermore, the adversarial training model improves robustness on the adversarial event data compared to the original model.","",""
1,"Hanjie Chen, Yangfeng Ji","Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation",2022,"","","","",41,"2022-07-13 09:32:18","","10.48550/arXiv.2203.12709","","",,,,,1,1.00,1,2,1,"Neural language models show vulnerability to adversarial examples which are semantically similar to their original counterparts with a few words replaced by their synonyms. A common way to improve model robustness is adversarial training which follows two steps—collecting adversarial examples by attacking a target model, and fine-tuning the model on the augmented dataset with these adversarial examples. The objective of traditional adversarial training is to make a model produce the same correct predictions on an original/adversarial example pair. However, the consistency between model decision-makings on two similar texts is ignored. We argue that a robust model should behave consistently on original/adversarial example pairs, that is making the same predictions (what) based on the same reasons (how) which can be reflected by consistent interpretations. In this work, we propose a novel feature-level adversarial training method named FLAT. FLAT aims at improving model robustness in terms of both predictions and interpretations. FLAT incorporates variational word masks in neural networks to learn global word importance and play as a bottleneck teaching the model to make predictions based on important words. FLAT explicitly shoots at the vulnerability problem caused by the mismatch between model understandings on the replaced words and their synonyms in original/adversarial example pairs by regularizing the corresponding global word importance scores. Experiments show the effectiveness of FLAT in improving the robustness with respect to both predictions and interpretations of four neural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks on four text classification tasks. The models trained via FLAT also show better robustness than baseline models on unforeseen adversarial examples across different attacks.","",""
10,"Wei Liu, S. Chawla, J. Bailey, C. Leckie, K. Ramamohanarao","An Efficient Adversarial Learning Strategy for Constructing Robust Classification Boundaries",2012,"","","","",42,"2022-07-13 09:32:18","","10.1007/978-3-642-35101-3_55","","",,,,,10,1.00,2,5,10,"","",""
7,"Flávio Luis de Mello","A Survey on Machine Learning Adversarial Attacks",2020,"","","","",43,"2022-07-13 09:32:18","","10.17648/jisc.v7i1.76","","",,,,,7,3.50,7,1,2,"It is becoming notorious several types of adversaries based on their threat model leverage vulnerabilities to compromise a machine learning system. Therefore, it is important to provide robustness to machine learning algorithms and systems against these adversaries. However, there are only a few strong countermeasures, which can be used in all types of attack scenarios to design a robust artificial intelligence system. This paper is structured and comprehensive overview of the research on attacks to machine learning systems and it tries to call the attention from developers and software houses to the security issues concerning machine learning.","",""
3,"Ezgi Korkmaz","Deep Reinforcement Learning Policies Learn Shared Adversarial Features Across MDPs",2021,"","","","",44,"2022-07-13 09:32:18","","10.1609/aaai.v36i7.20684","","",,,,,3,3.00,3,1,1,"The use of deep neural networks as function approximators has led to striking progress for reinforcement learning algorithms and applications. Yet the knowledge we have on decision boundary geometry and the loss landscape of neural policies is still quite limited. In this paper, we propose a framework to investigate the decision boundary and loss landscape similarities across states and across MDPs. We conduct experiments in various games from Arcade Learning Environment, and discover that high sensitivity directions for neural policies are correlated across MDPs. We argue that these high sensitivity directions support the hypothesis that non-robust features are shared across training environments of reinforcement learning agents. We believe our results reveal fundamental properties of the environments used in deep reinforcement learning training, and represent a tangible step towards building robust and reliable deep reinforcement learning agents.","",""
3,"Richard Evan Sutanto, Sukho Lee","Real-Time Adversarial Attack Detection with Deep Image Prior Initialized as a High-Level Representation Based Blurring Network",2020,"","","","",45,"2022-07-13 09:32:18","","10.3390/electronics10010052","","",,,,,3,1.50,2,2,2,"Several recent studies have shown that artificial intelligence (AI) systems can malfunction due to intentionally manipulated data coming through normal channels. Such kinds of manipulated data are called adversarial examples. Adversarial examples can pose a major threat to an AI-led society when an attacker uses them as means to attack an AI system, which is called an adversarial attack. Therefore, major IT companies such as Google are now studying ways to build AI systems which are robust against adversarial attacks by developing effective defense methods. However, one of the reasons why it is difficult to establish an effective defense system is due to the fact that it is difficult to know in advance what kind of adversarial attack method the opponent is using. Therefore, in this paper, we propose a method to detect the adversarial noise without knowledge of the kind of adversarial noise used by the attacker. For this end, we propose a blurring network that is trained only with normal images and also use it as an initial condition of the Deep Image Prior (DIP) network. This is in contrast to other neural network based detection methods, which require the use of many adversarial noisy images for the training of the neural network. Experimental results indicate the validity of the proposed method.","",""
3,"Da Xu, Yuting Ye, Chuanwei Ruan, Bo Yang","Towards Robust Off-policy Learning for Runtime Uncertainty",2022,"","","","",46,"2022-07-13 09:32:18","","10.1609/aaai.v36i9.21249","","",,,,,3,3.00,1,4,1,"Off-policy learning plays a pivotal role in optimizing and evaluating policies prior to the online deployment. However, during the real-time serving, we observe varieties of interventions and constraints that cause inconsistency between the online and offline setting, which we summarize and term as runtime uncertainty. Such uncertainty cannot be learned from the logged data due to its abnormality and rareness nature. To assert a certain level of robustness, we perturb the off-policy estimators along an adversarial direction in view of the runtime uncertainty. It allows the resulting estimators to be robust not only to observed but also unexpected runtime uncertainties. Leveraging this idea, we bring runtime-uncertainty robustness to three major off-policy learning methods: the inverse propensity score method, reward-model method, and doubly robust method. We theoretically justify the robustness of our methods to runtime uncertainty, and demonstrate their effectiveness using both the simulation and the real-world online experiments.","",""
0,"Seungyong Moon, Gaon An, Hyun Oh Song","Preemptive Image Robustification for Protecting Users against Man-in-the-Middle Adversarial Attacks",2021,"","","","",47,"2022-07-13 09:32:18","","10.1609/aaai.v36i7.20751","","",,,,,0,0.00,0,3,1,"Deep neural networks have become the driving force of modern image recognition systems. However, the vulnerability of neural networks against adversarial attacks poses a serious threat to the people affected by these systems. In this paper, we focus on a real-world threat model where a Man-in-the-Middle adversary maliciously intercepts and perturbs images web users upload online. This type of attack can raise severe ethical concerns on top of simple performance degradation. To prevent this attack, we devise a novel bi-level optimization algorithm that finds points in the vicinity of natural images that are robust to adversarial perturbations. Experiments on CIFAR-10 and ImageNet show our method can effectively robustify natural images within the given modification budget. We also show the proposed method can improve robustness when jointly used with randomized smoothing.","",""
0,"Xuanyu Zhang, Shi-You Xu, Jun Hu, Zhi-Yuan Xie","Optimized L2 Norm Loss for Adversarial Robustness",2022,"","","","",48,"2022-07-13 09:32:18","","10.1109/CCAI55564.2022.9807767","","",,,,,0,0.00,0,4,1,"Although adversarial training is the most common method to make models obtain better adversarial robustness, its drawback of leading to reduced accuracy has been plaguing the academic community. In recent years, many articles have pointed out that good Lipschitz continuity helps models obtain better robustness and standard accuracy, and argued that models that are both robust and accurate exist. However, many methods still perform less well with models even with the addition of Lipschitz continuity constraints. Therefore, we discuss the drawbacks of existing Lipschitz continuity metric in deep learning in terms of Lipschitz continuity, and propose a counteracting Lipschitz continuity metric that is more suitable for deep learning. We demonstrate theoretically and experimentally that Mixup can significantly enhance the local Lipschitz continuity of the model. Using this property, we generate a large number of mix confrontation samples using Target attack to fill the entire neighborhood space. Our method gives the model a smoother localization and significantly improves the adversarial robustness of the model beyond most existing adversarial training methods.","",""
2,"Chih-Ling Chang, Jui-Lung Hung, Chin-Wei Tien, Chia-Wei Tien, S. Kuo","Evaluating Robustness of AI Models against Adversarial Attacks",2020,"","","","",49,"2022-07-13 09:32:18","","10.1145/3385003.3410920","","",,,,,2,1.00,0,5,2,"Recently developed adversarial attacks on neural networks have become more aggressive and dangerous, because of which Artificial Intelligence (AI) models are no longer sufficiently robust against them. It is important to have a set of effective and reliable methods to detect malicious attacks to ensure the security of AI models. Such standardized methods can also serve as a reference for researchers to develop robust models and new kinds of attacks. This study proposes a method to assess the robustness of AI models. Six commonly used image classification CNN models were evaluated when subjected to 13 types of adversarial attacks. The robustness of the models is calculated unbiased and can be used as a reference for further improvement. It is distinguished from prior related works that our algorithm is attack-agnostic and is applicable to neural network model.","",""
4,"Jiarong Xu, Junru Chen, Yang Yang, Yizhou Sun, Chunping Wang, Jiangang Lu","Unsupervised Adversarially-Robust Representation Learning on Graphs",2020,"","","","",50,"2022-07-13 09:32:18","","10.1609/aaai.v36i4.20349","","",,,,,4,2.00,1,6,2,"Unsupervised/self-supervised pre-training methods for graph representation learning have recently attracted increasing research interests, and they are shown to be able to generalize to various downstream applications. Yet, the adversarial robustness of such pre-trained graph learning models remains largely unexplored. More importantly, most existing defense techniques designed for end-to-end graph representation learning methods require pre-specified label definitions, and thus cannot be directly applied to the pre-training methods. In this paper, we propose an unsupervised defense technique to robustify pre-trained deep graph models, so that the perturbations on the input graph can be successfully identified and blocked before the model is applied to different downstream tasks. Specifically, we introduce a mutual information-based measure, graph representation vulnerability (GRV), to quantify the robustness of graph encoders on the representation space. We then formulate an optimization problem to learn the graph representation by carefully balancing the trade-off between the expressive power and the robustness (i.e., GRV) of the graph encoder. The discrete nature of graph topology and the joint space of graph data make the optimization problem intractable to solve. To handle the above difficulty and to reduce computational expense, we further relax the problem and thus provide an approximate solution. Additionally, we explore a provable connection between the robustness of the unsupervised graph encoder and that of models on downstream tasks. Extensive experiments demonstrate that even without access to labels and tasks, our model is still able to enhance robustness against adversarial attacks on three downstream tasks (node classification, link prediction, and community detection) by an average of +16.5% compared with existing methods.","",""
1,"Mahdi Soleymani, Ramy E. Ali, Hessam Mahdavifar, A. Avestimehr","ApproxIFER: A Model-Agnostic Approach to Resilient and Robust Prediction Serving Systems",2021,"","","","",51,"2022-07-13 09:32:18","","10.1609/aaai.v36i8.20809","","",,,,,1,1.00,0,4,1,"Due to the surge of cloud-assisted AI services, the problem of designing resilient prediction serving systems that can effectively cope with stragglers and minimize response delays has attracted much interest. The common approach for tackling this problem is replication which assigns the same prediction task to multiple workers. This approach, however, is inefficient and incurs significant resource overheads. Hence, a learning-based approach known as parity model (ParM) has been recently proposed which learns models that can generate ``parities’’ for a group of predictions to reconstruct the predictions of the slow/failed workers. While this learning-based approach is more resource-efficient than replication, it is tailored to the specific model hosted by the cloud and is particularly suitable for a small number of queries (typically less than four) and tolerating very few stragglers (mostly one). Moreover, ParM does not handle Byzantine adversarial workers. We propose a different approach, named Approximate Coded Inference (ApproxIFER), that does not require training any parity models, hence it is agnostic to the model hosted by the cloud and can be readily applied to different data domains and model architectures. Compared with earlier works, ApproxIFER can handle a general number of stragglers and scales significantly better with the number of queries. Furthermore, ApproxIFER is robust against Byzantine workers. Our extensive experiments on a large number of datasets and model architectures show significant degraded mode accuracy improvement by up to 58% over ParM.","",""
0,"Mohammad Khalooei, M. Homayounpour, M. Amirmazlaghani","Layer-wise Regularized Adversarial Training using Layers Sustainability Analysis (LSA) framework",2022,"","","","",52,"2022-07-13 09:32:18","","","","",,,,,0,0.00,0,3,1,"Deep neural network models are used today in various applications of artificial intelligence, the strengthening of which, in the face of adversarial attacks is of particular importance. An appropriate solution to adversarial attacks is adversarial training, which reaches a trade-off between robustness and generalization. This paper introduces a novel framework (Layer Sustainability Analysis (LSA)) for the analysis of layer vulnerability in an arbitrary neural network in the scenario of adversarial attacks. LSA can be a helpful toolkit to assess deep neural networks and to extend the adversarial training approaches towards improving the sustainability of model layers via layer monitoring and analysis. The LSA framework identifies a list of Most Vulnerable Layers (MVL list) of the given network. The relative error, as a comparison measure, is used to evaluate representation sustainability of each layer against adversarial inputs. The proposed approach for obtaining robust neural networks to fend off adversarial attacks is based on a layer-wise regularization (LR) over LSA proposal(s) for adversarial training (AT); i.e. the AT-LR procedure. AT-LR could be used with any benchmark adversarial attack to reduce the vulnerability of network layers and to improve conventional adversarial training approaches. The proposed idea performs well theoretically and experimentally for state-of-the-art multilayer perceptron and convolutional neural network architectures. Compared with the AT-LR and its corresponding base adversarial training, the classification accuracy of more significant perturbations increased by 16.35%, 21.79%, and 10.730% on Moon, MNIST, and CIFAR-10 benchmark datasets, respectively. The LSA framework is available and published at https://github.com/khalooei/LSA.","",""
0,"Anirudh Yadav, Ashutosh Upadhyay, S. Sharanya","An integrated Auto Encoder-Block Switching defense approach to prevent adversarial attacks",2022,"","","","",53,"2022-07-13 09:32:18","","10.48550/arXiv.2203.10930","","",,,,,0,0.00,0,3,1,"According to the recent studies, the vulnerability of state of the art Neural Networks to adversarial input samples has increased drastically. Neural network is an intermediate path or technique by which a computer learns to perform tasks using Machine learning algorithms. Machine Learning and Artificial Intelligence model has become fundamental aspect of life, such as self-driving cars [1], smart home devices, so any vulnerability is a significant concern. The smallest input deviations can fool these extremely literal systems and deceive their users as well as administrator into precarious situations. This article proposes a defense algorithm which utilizes the combination of an auto-encoder [3] and block-switching architecture. Auto-coder is intended to remove any perturbations found in input images whereas block switching method is used to make it more robust against White-box attack. Attack is planned using FGSM [9] model, and the subsequent counter-attack by the proposed architecture will take place thereby demonstrating the feasibility and security delivered by the algorithm.","",""
0,"Nyee Thoang Lim, Meng Yi Kuan, Muxin Pu, Mei Kuan Lim, Chun Yong Chong","Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors",2022,"","","","",54,"2022-07-13 09:32:18","","10.48550/arXiv.2204.08612","","",,,,,0,0.00,0,5,1,"—Deepfakes utilise Artiﬁcial Intelligence (AI) tech- niques to create synthetic media where the likeness of one person is replaced with another. There are growing concerns that deepfakes can be maliciously used to create misleading and harmful digital contents. As deepfakes become more common, there is a dire need for deepfake detection technology to help spot deepfake media. Present deepfake detection models are able to achieve outstanding accuracy ( > 90%). However, most of them are limited to within-dataset scenario. Most models do not gener- alise well enough in cross-dataset scenario. Furthermore, state-of-the-art deepfake detection models rely on neural network- based classiﬁcation models that are known to be vulnerable to adversarial attacks. Motivated by the need for a robust deepfake detection model, this study adapts metamorphic testing (MT) principles to help identify potential factors that could inﬂuence the robustness of the examined model, while overcoming the test oracle problem in this domain. Metamorphic testing is speciﬁcally chosen as the testing technique as it ﬁts our demand to address learning-based system testing with probabilistic outcomes from largely black-box components, based on potentially large input domains. We performed our evaluations on MesoInception-4 and TwoStreamNet models, which are the state-of-the-art deepfake detection models. This study identiﬁed makeup application as an adversarial attack that could fool deepfake detectors. Our experimental results demonstrate that both the MesoInception-4 and TwoStreamNet models degrade in their performance by up to 30% when the input data is perturbed with makeup.","",""
0,"Narmin Ghaffari Laleh, D. Truhn, Gregory Patrick Veldhuizen, Tianyu Han, Marko van Treeck, R. D. Bülow, R. Langer, B. Dislich, P. Boor, V. Schulz, J. Kather","Adversarial attacks and adversarial robustness in computational pathology",2022,"","","","",55,"2022-07-13 09:32:18","","10.1101/2022.03.15.484515","","",,,,,0,0.00,0,11,1,"Artificial Intelligence (AI) can support diagnostic workflows in oncology by aiding diagnosis and providing biomarkers. AI applications are therefore expected to evolve from academic prototypes to commercial products in the coming years. However, AI applications are vulnerable to adversarial attacks, such as malicious interference with test data aiming to cause misclassifications. Therefore, it is essential for the use of AI-based diagnostic devices to secure them against such attacks before widespread use. Unfortunately, no resistant systems exist in computational pathology so far. To address this problem, we investigate the susceptibility of convolutional neural networks (CNNs) to multiple types of white- and black-box attacks. We demonstrate that both attacks can easily confuse CNNs in clinically relevant pathology tasks and impair classification performance. Classical adversarially robust training and dual batch normalization (DBN) are possible mitigation strategies but require precise knowledge of the type of attack used in the inference. We demonstrate that vision transformers (ViTs) perform equally well compared to CNNs at baseline and are orders of magnitude more robust to different types of white-box and black-box attacks. At a mechanistic level, we show that this is associated with a more robust latent representation of clinically relevant categories in ViTs compared to CNNs. Our results are in line with previous theoretical studies. We show that ViTs are robust learners in computational pathology. This implies that large-scale rollout of AI models in computational pathology should rely on ViTs rather than CNN-based classifiers to provide inherent protection against adversaries.","",""
0,"S. Asha, P. Vinod","Evaluation of adversarial machine learning tools for securing AI systems",2021,"","","","",56,"2022-07-13 09:32:18","","10.1007/s10586-021-03421-1","","",,,,,0,0.00,0,2,1,"","",""
0,"Lujia Bao, K. Zheng","RCC: A Paradigm for Training a Robust Chinese Text Classification Model",2022,"","","","",57,"2022-07-13 09:32:18","","10.1109/iwecai55315.2022.00090","","",,,,,0,0.00,0,2,1,"Adversarial attacks against language models have gained more and more attention in recent years, and various adversarial text generation models have been proposed. Chinese language models are more vulnerable to character-level tampering attacks due to the language nature. In this paper, we implement a set of white-box attack algorithms against Chinese text classification models, which significantly reduce the accuracy of multiple baseline models on multiple classification tasks while ensuring that one can recover the original utterance. We utilize follow strategies, and propose a paradigm to enhance the robustness of Chinese classification models: 1) generating adversarial text during training as a dynamic data augmentation, 2) introducing extra glyph and phonology information into Chinses language models.","",""
0,"Deepak Ravikumar, Sangamesh Kodge, Isha Garg, K. Roy","TREND: Transferability based Robust ENsemble Design",2020,"","","","",58,"2022-07-13 09:32:18","","10.1109/tai.2022.3175172","","",,,,,0,0.00,0,4,2,"Deep Learning models hold state-of-the-art performance in many fields, but their vulnerability to adversarial examples poses a threat to their ubiquitous deployment in practical settings. Additionally, adversarial inputs generated on one classifier have been shown to transfer to other classifiers trained on similar data, which makes the attacks possible even if the model parameters are not revealed to the adversary. This property of transferability has not yet been systematically studied, leading to a gap in our understanding of robustness of neural networks to adversarial inputs. In this work, we study the effect of network architecture, initialization, input, weight and activation quantization on transferability. Our experiments reveal that transferability is significantly hampered by input quantization and architectural mismatch between source and target, is unaffected by initialization and is architecture-dependent for both weight and activation quantization. To quantify transferability, we propose a simple metric, which is a function of the attack strength. We demonstrate the utility of the proposed metric in designing a methodology to build ensembles with improved adversarial robustness. Finally, we show that an ensemble consisting of carefully chosen input quantized networks achieves better adversarial robustness than would otherwise be possible with a single network.","",""
0,"Timothy Zee, Alexander Ororbia, A. Mali, Ifeoma Nwogu","A Robust Backpropagation-Free Framework for Images",2022,"","","","",59,"2022-07-13 09:32:18","","10.48550/arXiv.2206.01820","","",,,,,0,0.00,0,4,1,"While current deep learning algorithms have been successful for a wide variety of artiﬁcial intelligence (AI) tasks, including those involving structured image data, they present deep neurophysiological conceptual issues due to their reliance on the gradients computed by backpropagation of errors (backprop) to obtain synaptic weight adjustments; hence are biologically implausible. We present a more biologically plausible approach, the error-kernel driven activation alignment (EKDAA) algorithm, to train convolution neural networks (CNNs) using locally derived error transmission kernels and error maps. We demonstrate the efﬁcacy of EKDAA by performing the task of visual-recognition on the Fashion MNIST, CIFAR-10 and SVHN benchmarks as well as conducting blackbox robustness tests on adversarial examples derived from these datasets. Furthermore, we also present results for a CNN trained using a non-differentiable activation function. All recognition results nearly matches that of backprop and exhibit greater adversarial robustness compared to backprop.","",""
3,"Seetarama Raju Pericherla, Nithish Duvvuru, D. Jayagopi","Improving Adversarial Images Using Activation Maps",2019,"","","","",60,"2022-07-13 09:32:18","","10.1109/ITAIC.2019.8785543","","",,,,,3,1.00,1,3,3,"Deep Neural Networks are currently gaining a lot of attention for their near human-level performances in tasks such as image classification, object detection, etc. As a result, they are also being deployed in security critical and real time systems such as face recognition and autonomous cars. This requires models to be robust to changes to the input. However, recent literature has showed that they are easily fooled when human imperceptible noise, also known as adversarial noise, is added to the input. By exploiting this adversarial nature, various adversarial attacks and defences against these attacks have been introduced so far. In this paper, we propose a new approach which can be used alongside any existing adversarial attack to further reduce the L2 distance between the generated adversarial image and the original image. Our approach can also be thought of as a new adversarial attack built on top of an existing attack. We evaluated our approach on the ImageNet dataset. Using our approach, we were able to reduce the L2 distance for around 60-70% of the images sampled from the ImageNet dataset.","",""
5,"A. Fournaris, A. Lalos, D. Serpanos","Generative Adversarial Networks in AI-Enabled Safety-Critical Systems: Friend or Foe?",2019,"","","","",61,"2022-07-13 09:32:18","","10.1109/MC.2019.2924546","","",,,,,5,1.67,2,3,3,"Generative adversarial networks can be exploited to launch attacks against detection systems that rely on artificial intelligence (AI). To build effective cyberphysical systems that are operationally robust and socially accepted, we must expend significant effort to develop novel AI-based safety-critical systems.","",""
0,"Di Jin, Zhigang Li, Liang Yang, Dongxiao He, Pengfei Jiao, Lu Zhai","Adversarial Capsule Learning for Network Embedding",2019,"","","","",62,"2022-07-13 09:32:18","","10.1109/ICTAI.2019.00038","","",,,,,0,0.00,0,6,3,"The purpose of network embedding is to learn a low-dimensional representation for each node in the network. One can then use this low-dimensional representation to solve some network analysis tasks, such as node classification and node clustering. At present, there are several network embedding learning methods based on GAN (Generative Adversarial Networks) to enhance the robustness of representations. However, these methods have two drawbacks. First, they are often too difficult to be trained stably. Second, they only learn the robust representations by matching the posterior distribution of the latent representations to the given priors. On the contrary, Capsule Networks can learn a more equivariant representation of images that is more robust to the changes in pose and spatial relationships of parts of objects in images. However, there is still no research using Capsule Network for network embedding since the social network is essentially different from images. For this problem, we propose a new approach of adversarial capsule learning (ACL) for network embedding, which is the first time to use Capsule Network in the network analysis tasks. To be specific, the new model consists of two parts, a generator and discriminator. We use Graph Convolutional Networks (GCN) as the generator to learn the embedding of nodes, and use Capsule Network as the discriminator to distinguish between the real and fake samples as accurately as possible. The experimental results demonstrate the effectiveness of the proposed new method.","",""
0,"Bi Fukun, Lei Mingyang, Sun Jiayi","A Robust and Effective Tracking Method in Remote Sensing Video Sequences",2019,"","","","",63,"2022-07-13 09:32:18","","10.1145/3341016.3341022","","",,,,,0,0.00,0,3,3,"With the popularization of high resolution imaging technology and the progress of artificial intelligence, remote sensing target tracking in the aerial video plays a very important role in public security, such as antiterrorism efforts and military reconnaissance. As aerial video has rapid changes in orientations, low resolution, and multiple similar disruptors, and the main tracking methods generally have relatively low tracking performance in this research field, we develop a robust tracking method for remote sensing videos based on a saliency enhanced multi-domain convolutional neural network (SEMD). The process can be divided into two main stages: (1) in the offline pretraining stage, we combine the Least Squares Generative Adversarial Networks (LSGANs) with a rotation strategy to augment typical easily confused negative samples, which can improve the capacity to distinguish between target and the background. (2) in the online tracking process, a saliency module is embedded between convolutional layers and we optimize the arrangement of its functional sub-modules to boost the saliency of the feature map, which improve the network representation power for rapid dynamic changes in the target. Comprehensive evaluations of homemade datasets demonstrate that the proposed method can achieve high efficiency and accuracy results compared to state-of-the-art methods.","",""
5,"Y. Malhotra","AI, Machine Learning & Deep Learning Risk Management & Controls: Beyond Deep Learning and Generative Adversarial Networks: Model Risk Management in AI, Machine Learning & Deep Learning",2018,"","","","",64,"2022-07-13 09:32:18","","10.2139/SSRN.3193693","","",,,,,5,1.25,5,1,4,"The current paper proposes how model risk management in operationalizing machine learning for algorithm deployment can be applied in national C4I and Cyber projects such as Project Maven. It builds upon recent leadership of global Management and Leadership industry executives for AI and Machine Learning Executive Education for MIT Sloan School of Management and the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and invited presentations at Princeton University. After building understanding about why model risk management is most crucial to robust AI, Machine Learning, Deep Learning, and, Neural Networks deployment, it introduces a Knowledge Management Framework for Model Risk Management to advance beyond ‘AI Automation’ to ‘AI Augmentation.’","",""
0,"Yuxing Peng, J. Hsu","Named Entity Filters for Robust Machine Reading Comprehension",2018,"","","","",65,"2022-07-13 09:32:18","","10.1109/TAAI.2018.00048","","",,,,,0,0.00,0,2,4,"The machine reading comprehension problem aims to extract crucial information from the given document to answer the relevant questions. Although many methods regarding the problem have been proposed, the similarity distraction problem inside remains unsolved. The similarity distraction problem addresses the error caused by some sentences being very similar to the question but not containing the answer. Named entities have the uniqueness which can be utilized to distinguish similar sentences to prevent models from being distracted. In this paper, named entity filters (NE filters) are proposed. NE filters can utilize the information of named entities to alleviate the similarity distraction problem. Experiment results in this paper show that the NE filter can enhance the robustness of the used model. The baseline model increases 5% to 10% F1 score on two adversarial SQuAD datasets without decreasing the F1 score on the original SQuAD dataset. Besides, by adding the NE filter, other existing models increases 5% F1 score on the adversarial datasets with less than 1% loss on the original one.","",""
11,"Monireh Ebrahimi, Aaron Eberhart, Federico Bianchi, P. Hitzler","Towards bridging the neuro-symbolic gap: deep deductive reasoners",2021,"","","","",66,"2022-07-13 09:32:18","","10.1007/S10489-020-02165-6","","",,,,,11,11.00,3,4,1,"","",""
23,"Linyi Yang, Eoin M. Kenny, T. L. J. Ng, Y Yang, K.Z. Zhang, P.K. Kannan, Barry Smyth, Ruihai Dong","Generating Plausible Counterfactual Explanations for Deep Transformers in Financial Text Classification",2020,"","","","",67,"2022-07-13 09:32:18","","10.18653/V1/2020.COLING-MAIN.541","","",,,,,23,11.50,4,6,2,"Corporate mergers and acquisitions (M&A) account for billions of dollars of investment globally every year and offer an interesting and challenging domain for artificial intelligence. However, in these highly sensitive domains, it is crucial to not only have a highly robust/accurate model, but be able to generate useful explanations to garner a user’s trust in the automated system. Regrettably, the recent research regarding eXplainable AI (XAI) in financial text classification has received little to no attention, and many current methods for generating textual-based explanations result in highly implausible explanations, which damage a user’s trust in the system. To address these issues, this paper proposes a novel methodology for producing plausible counterfactual explanations, whilst exploring the regularization benefits of adversarial training on language models in the domain of FinTech. Exhaustive quantitative experiments demonstrate that not only does this approach improve the model accuracy when compared to the current state-of-the-art and human performance, but it also generates counterfactual explanations which are significantly more plausible based on human trials.","",""
5,"Xinqiao Zhang, Huili Chen, F. Koushanfar","TAD: Trigger Approximation based Black-box Trojan Detection for AI",2021,"","","","",68,"2022-07-13 09:32:18","","","","",,,,,5,5.00,2,3,1,"An emerging amount of intelligent applications have been developed with the surge of Machine Learning (ML). Deep Neural Networks (DNNs) have demonstrated unprecedented performance across various fields such as medical diagnosis and autonomous driving. While DNNs are widely employed in security-sensitive fields, they are identified to be vulnerable to Neural Trojan (NT) attacks that are controlled and activated by the stealthy trigger. We call this vulnerable model as adversarial artificial intelligence (AI). In this paper, we target to design a robust Trojan detection scheme that inspects whether a pre-trained AI model has been Trojaned before its deployment. Prior works are oblivious of the intrinsic property of trigger distribution and try to reconstruct the trigger pattern using simple heuristics, i.e., stimulating the given model to incorrect outputs. As a result, their detection time and effectiveness are limited. We leverage the observation that the pixel trigger typically features spatial dependency and propose TAD, the first trigger approximation based Trojan detection framework that enables fast and scalable search of the trigger in the input space. Furthermore, TAD can also detect Trojans embedded in the feature space where certain filter transformations are used to activate the Trojan. We perform extensive experiments to investigate the performance of the TAD across various datasets and ML models. Empirical results show that TAD achieves a ROC-AUC score of 0.91 on the public TrojAI dataset 1 and the average detection time per model is 7.1 minutes.","",""
5,"Jing Ke, Yiqing Shen, Yizhou Lu","Style Normalization In Histology With Federated Learning",2021,"","","","",69,"2022-07-13 09:32:18","","10.1109/ISBI48211.2021.9434078","","",,,,,5,5.00,2,3,1,"The global cancer burden is on the rise, and Artificial Intelligence (AI) has become increasingly crucial to achieve more objective and efficient diagnosis in digital pathology. Current AI-assisted histopathology analysis methods need to address the following two issues. First, the color variations due to use of different stains need to be tackled such as with stain style transfer technique. Second, in parallel with heterogeneity, datasets from individual clinical institutions are characterized by privacy regulations, and thus need to be addressed such as with robust data-private collaborative training. In this paper, to address the color heterogeneity problem, we propose a novel generative adversarial network with one orchestrating generator and multiple distributed discriminators for stain style transfer. We also incorporate Federated Learning (FL) to further preserve data privacy and security from multiple data centers. We use a large cohort of histopathology datasets as a case study.","",""
1,"Francisco Jáñez-Martino, R. Alaíz-Rodríguez, V. González-Castro, Eduardo FIDALGO, Enrique Alegre","A review of spam email detection: analysis of spammer strategies and the dataset shift problem",2022,"","","","",70,"2022-07-13 09:32:18","","10.1007/s10462-022-10195-4","","",,,,,1,1.00,0,5,1,"","",""
0,"Taehoon Lee","Robust Feature Learning with Deep Neural Networks",2016,"","","","",71,"2022-07-13 09:32:18","","","","",,,,,0,0.00,0,1,6,"Recent advances in machine learning continue to bring us closer to artificial intelligence. In particular, deep learning plays a key role in cutting-edge frameworks such as autonomous driving and game playing. Deep learning refers to a class of multi-layered neural networks, which is rapidly evolving as the amount of data increases, prior knowledge builds up, efficient training schemes are being developed, and high-end hardwares are being build. Currently, deep learning is a state-of-the-art technique for most recognition tasks. As deep neural networks learn many parameters, there has been a variety of attempts to obtain reasonable solutions over a wide search space. In this dissertation, three issues in deep learning are discussed and approaches to solve them with regularization techniques are suggested. First, deep neural networks expose the problem of intrinsic blind spots called adversarial perturbations. Thus, we must construct neural networks that resist the directions of adversarial perturbations by introducing an explicit loss term to minimize the differences between the original and adversarial samples. Second, training restricted Boltzmann machines show limited performance when handling minority samples in class-imbalanced datasets. Our approach addresses this limitation and is combined with a new regularization concept for datasets that have categorical features. Lastly, insufficient data handling is required to be more sophisticated when deep networks learn numerous parameters. Given high-dimensional samples, we must augment datasets with adequate prior knowledge to estimate a high-dimensional distribution. Furthermore, this dissertation shows the first application of deep belief networks to identifying junction splicing signals. Junction prediction is one of the major problems in the field of bioinformatics, and is a starting point to understanding the entire gene expression process. In summary, this dissertation proposes a set of deep learning regularization schemes that can learn the meaningful representation underlying large-scale genomic datasets and image datasets. The effectiveness of these methods was confirmed with a number of experimental studies.","",""
0,"Carson Kent, Jiajin Li, J. Blanchet, P. Glynn","Modified Frank Wolfe in Probability Space",2021,"","","","",72,"2022-07-13 09:32:18","","","","",,,,,0,0.00,0,4,1,"We propose a novel Frank-Wolfe (FW) procedure for the optimization of infinitedimensional functionals of probability measures a task which arises naturally in a wide range of areas including statistical learning (e.g. variational inference) and artificial intelligence (e.g. generative adversarial networks). Our FW procedure takes advantage of Wasserstein gradient flows and strong duality results recently developed in Distributionally Robust Optimization so that gradient steps (in the Wasserstein space) can be efficiently computed using finite-dimensional, convex optimization methods. We show how to choose the step sizes in order to guarantee exponentially fast iteration convergence, under mild assumptions on the functional to optimize. We apply our algorithm to a range of functionals arising from applications in nonparametric estimation.","",""
0,"Alexander Bastounis, Anders, Christian Hansen, D. Higham, I. Tyukin","Uniform-in-diffusivity Chaotic Mixing and the Batchelor Spectrum",2021,"","","","",73,"2022-07-13 09:32:18","","","","",,,,,0,0.00,0,5,1,"In a field of research where algorithms can misinterpret stop signs as speed limit signs with the addition of minimal graffiti [3], many commentators are wondering whether current artificial intelligence (AI) solutions are sufficiently robust, resilient, and trustworthy. How can the research community quantify and address such issues? Many empirical approaches investigate the generation of adversarial attacks: small, deliberate perturbations to an input that cause dramatic changes in a system’s output. Changes that are essentially imperceptible to the human eye may alter predictions in the field of image classification, which has implications in many high-stakes and safety-critical settings. The rise of algorithms that construct attacks—and heuristic techniques that identify or guard against them—has led to a version of conflict escalation wherein attack and defense strategies become increasingly ingenious [10]. These issues concern the conditioning of the underlying problem and stability of the algorithms in use. Recent research has utilized mathematical tools—notably from numerical analysis, applied probability, and high-dimensional geometry— to shed light on this field. However, many open problems remain.","",""
0,"D. Vaishnavi","An Enhanced Deep Network for Recognizing the Coronavirus Disease Using X-ray Images",2021,"","","","",74,"2022-07-13 09:32:18","","10.1080/03772063.2021.1997355","","",,,,,0,0.00,0,1,1,"The spreading of Coronavirus (covid-19) is pushing the healthcare organizations under exceptional over the universe and increasing pressure concurring to the World Health Organization (WHO). With advancement of Artificial Intelligence, the discovery of this type of infection during the initial stage will offers assistance in quick recuperation and in discharging the pressure on healthcare organizations. This paper presents the deep convolution network to detect the coronavirus in chest x-ray images. Due to the lack of benchmark datasets for covid-19 specifically in chest x-ray images, this work presents the framework that adopts the Generative Adversarial Network of Deep Convolution (DC-GAN). The proposed model of DC-GAN can generate a maximum of 30 different patterns for a single image and thus increases the size of dataset. It also offers assistance in overwhelming the overfitting issue and building the proposed framework more robust. The proposed model implements the transfer learning updated by the loss function using the ImageNet to attain the finest parameters from the pre-trained model. The publically available dataset is utilized to validate the proposed work. This work is validated by conducting the various experiments in various perspectives and also its performances are recorded using the measures namely accuracy, recall, precision and, F1score. These recorded results compared with the various existing methods.. [ FROM AUTHOR] Copyright of IETE Journal of Research is the property of Taylor & Francis Ltd and its content may not be copied or emailed to multiple sites or posted to a listserv without the copyright holder's express written permission. However, users may print, download, or email articles for individual use. This may be abridged. No warranty is given about the accuracy of the copy. Users should refer to the original published version of the material for the full . (Copyright applies to all Abstracts.)","",""
0,"Xueyang Wang, Xiaozhang Liu","Enhancing Robustness of Classifiers Based on PCA",2021,"","","","",75,"2022-07-13 09:32:18","","10.1109/PRAI53619.2021.9550807","","",,,,,0,0.00,0,2,1,"To date, deep learning techniques have been widely used. However, deep neural networks (DNNs) are vulnerable to adversarial attacks, which has become one of the hidden risks issues affecting system security. The adversarial sample is a perturbation input to fool the deep learning model. The inherent weakness of DNNs that lacks robustness to adversarial samples brings security problems, especially for tasks that require high reliability. This paper proposed a robustness enhancing method based on principal component analysis (PCA) and applied it to deep networks, which enhanced the ability of DNNs to resist adversarial attacks. Specifically, the proposed method firstly used PCA to downscale the clean samples, and then, chose two non-target attacks, DeepFool and FGSM, to craft adversarial samples pre-and-post downscale. Finally, by evaluating the changes in the robustness of the classifier, we draw the corresponding analytical conclusions. Experimental results on MNIST show that the proposed method makes deep networks more robust against white-box attacks.","",""
0,"E. Agliari, A. Fachechi, Chiara Marullo","Non-linear PDEs approach to statistical mechanics of Dense Associative Memories",2022,"","","","",76,"2022-07-13 09:32:18","","","","",,,,,0,0.00,0,3,1,": Dense associative memories (DAM), are widespread models in artiﬁcial intelligence used for pattern recognition tasks; computationally, they have been proven to be robust against adversarial input and theoretically, leveraging their analogy with spin-glass systems, they are usually treated by means of statistical-mechanics tools. Here we develop analytical methods, based on nonlinear PDEs, to investigate their functioning. In particular, we prove diﬀerential identities involving DAM’s partition function and macroscopic observables useful for a qualitative and quantitative analysis of the system. These results allow for a deeper comprehension of the mechanisms underlying DAMs and provide interdisciplinary tools for their study.","",""
0,"Xin Qiu, R. Miikkulainen","Detecting Misclassification Errors in Neural Networks with a Gaussian Process Model",2020,"","","","",77,"2022-07-13 09:32:18","","10.1609/aaai.v36i7.20773","","",,,,,0,0.00,0,2,2,"As neural network classifiers are deployed in real-world applications, it is crucial that their failures can be detected reliably. One practical solution is to assign confidence scores to each prediction, then use these scores to filter out possible misclassifications. However, existing confidence metrics are not yet sufficiently reliable for this role. This paper presents a new framework that produces a quantitative metric for detecting misclassification errors. This framework, RED, builds an error detector on top of the base classifier and estimates uncertainty of the detection scores using Gaussian Processes. Experimental comparisons with other error detection methods on 125 UCI datasets demonstrate that this approach is effective. Further implementations on two probabilistic base classifiers and two large deep learning architecture in vision tasks further confirm that the method is robust and scalable. Third, an empirical analysis of RED with out-of-distribution and adversarial samples shows that the method can be used not only to detect errors but also to understand where they come from. RED can thereby be used to improve trustworthiness of neural network classifiers more broadly in the future.","",""
0,"Dongfang Li, Dongfang Li, Baotian Hu, Qingcai Chen, Tujie Xu, Jingcong Tao, Yunan Zhang","Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction",2021,"","","","",78,"2022-07-13 09:32:18","","10.1609/aaai.v36i10.21342","","",,,,,0,0.00,0,7,1,"Recent works have shown explainability and robustness are two crucial ingredients of trustworthy and reliable text classification. However, previous works usually address one of two aspects: i) how to extract accurate rationales for explainability while being beneficial to prediction; ii) how to make the predictive model robust to different types of adversarial attacks. Intuitively, a model that produces helpful explanations should be more robust against adversarial attacks, because we cannot trust the model that outputs explanations but changes its prediction under small perturbations. To this end, we propose a joint classification and rationale extraction model named AT-BMC. It includes two key mechanisms: mixed Adversarial Training (AT) is designed to use various perturbations in discrete and embedding space to improve the model’s robustness, and Boundary Match Constraint (BMC) helps to locate rationales more precisely with the guidance of boundary information. Performances on benchmark datasets demonstrate that the proposed AT-BMC outperforms baselines on both classification and rationale extraction by a large margin. Robustness analysis shows that the proposed AT-BMC decreases the attack success rate effectively by up to 69%. The results indicate that there are connections between robust models and better explanations.","",""
0,"Emanuele La Malfa, M. Kwiatkowska","The King is Naked: on the Notion of Robustness for Natural Language Processing",2021,"","","","",79,"2022-07-13 09:32:18","","10.1609/aaai.v36i10.21353","","",,,,,0,0.00,0,2,1,"There is growing evidence that the classical notion of adversarial robustness originally introduced for images has been adopted as a de facto standard by a large part of the NLP research community.  We show that this notion is problematic in the context of NLP as it considers a narrow spectrum of linguistic phenomena. In this paper, we argue for semantic robustness, which is better aligned with the human concept of linguistic fidelity. We characterize semantic robustness in terms of biases that it is expected to induce in a model. We study semantic robustness of a range of vanilla and robustly trained architectures using a template-based generative test bed. We complement the analysis with empirical evidence that, despite being harder to implement, semantic robustness can improve performance %gives guarantees for on complex linguistic phenomena where models robust in the classical sense fail.","",""
0,"Pei Huang, Yuting Yang, Fuqi Jia, Minghao Liu, Feifei Ma, Jian Zhang","Word Level Robustness Enhancement: Fight Perturbation with Perturbation",2022,"","","","",80,"2022-07-13 09:32:18","","10.1609/aaai.v36i10.21324","","",,,,,0,0.00,0,6,1,"State-of-the-art deep NLP models have achieved impressive improvements on many tasks. However, they are found to be vulnerable to some perturbations. Before they are widely adopted, the fundamental issues of robustness need to be addressed. In this paper, we design a robustness enhancement method to defend against word substitution perturbation, whose basic idea is to fight perturbation with perturbation. We find that: although many well-trained deep models are not robust in the setting of the presence of adversarial samples, they satisfy weak robustness. That means they can handle most non-crafted perturbations well. Taking advantage of the weak robustness property of deep models, we utilize non-crafted perturbations to resist the adversarial perturbations crafted by attackers. Our method contains two main stages. The first stage is using randomized perturbation to conform the input to the data distribution. The second stage is using randomized perturbation to eliminate the instability of prediction results and enhance the robustness guarantee. Experimental results show that our method can significantly improve the ability of deep models to resist the state-of-the-art adversarial attacks while maintaining the prediction performance on the original clean data.","",""
140,"Xiaoyu Cao, N. Gong","Mitigating Evasion Attacks to Deep Neural Networks via Region-based Classification",2017,"","","","",81,"2022-07-13 09:32:18","","10.1145/3134600.3134606","","",,,,,140,28.00,70,2,5,"Deep neural networks (DNNs) have transformed several artificial intelligence research areas including computer vision, speech recognition, and natural language processing. However, recent studies demonstrated that DNNs are vulnerable to adversarial manipulations at testing time. Specifically, suppose we have a testing example, whose label can be correctly predicted by a DNN classifier. An attacker can add a small carefully crafted noise to the testing example such that the DNN classifier predicts an incorrect label, where the crafted testing example is called adversarial example. Such attacks are called evasion attacks. Evasion attacks are one of the biggest challenges for deploying DNNs in safety and security critical applications such as self-driving cars. In this work, we develop new DNNs that are robust to state-of-the-art evasion attacks. Our key observation is that adversarial examples are close to the classification boundary. Therefore, we propose region-based classification to be robust to adversarial examples. Specifically, for a benign/adversarial testing example, we ensemble information in a hypercube centered at the example to predict its label. In contrast, traditional classifiers are point-based classification, i.e., given a testing example, the classifier predicts its label based on the testing example alone. Our evaluation results on MNIST and CIFAR-10 datasets demonstrate that our region-based classification can significantly mitigate evasion attacks without sacrificing classification accuracy on benign examples. Specifically, our region-based classification achieves the same classification accuracy on testing benign examples as point-based classification, but our region-based classification is significantly more robust than point-based classification to state-of-the-art evasion attacks.","",""
0,"Daniel Lowd, Brenton Lessley, Mino De Raj","Towards Adversarial Reasoning in Statistical Relational Domains",2014,"","","","",82,"2022-07-13 09:32:18","","","","",,,,,0,0.00,0,3,8,"Statistical relational artificial intelligence combines first-order logic and probability in order to handle the complexity and uncertainty present in many real-world domains. However, many real-world domains also include multiple agents that cooperate or compete according to their diverse goals. In order to handle such domains, an autonomous agent must also consider the actions of other agents. In this paper, we show that existing statistical relational modeling and inference techniques can be readily adapted to certain adversarial or non-cooperative scenarios. We also discuss how learning methods can be adapted to be robust to the behavior of adversaries. Extending and applying these methods to real-world problems will extend the scope and impact of statistical relational artificial intelligence.","",""
11,"L. Guarnera, O. Giudice, S. Battiato","Fighting Deepfake by Exposing the Convolutional Traces on Images",2020,"","","","",83,"2022-07-13 09:32:18","","10.1109/ACCESS.2020.3023037","","",,,,,11,5.50,4,3,2,"Advances in Artificial Intelligence and Image Processing are changing the way people interacts with digital images and video. Widespread mobile apps like FACEAPP make use of the most advanced Generative Adversarial Networks (GAN) to produce extreme transformations on human face photos such gender swap, aging, etc. The results are utterly realistic and extremely easy to be exploited even for non-experienced users. This kind of media object took the name of Deepfake and raised a new challenge in the multimedia forensics field: the Deepfake detection challenge. Indeed, discriminating a Deepfake from a real image could be a difficult task even for human eyes but recent works are trying to apply the same technology used for generating images for discriminating them with preliminary good results but with many limitations: employed Convolutional Neural Networks are not so robust, demonstrate to be specific to the context and tend to extract semantics from images. In this paper, a new approach aimed to extract a Deepfake fingerprint from images is proposed. The method is based on the Expectation-Maximization algorithm trained to detect and extract a fingerprint that represents the Convolutional Traces (CT) left by GANs during image generation. The CT demonstrates to have high discriminative power achieving better results than state-of-the-art in the Deepfake detection task also proving to be robust to different attacks. Achieving an overall classification accuracy of over 98%, considering Deepfakes from 10 different GAN architectures not only involved in images of faces, the CT demonstrates to be reliable and without any dependence on image semantic. Finally, tests carried out on Deepfakes generated by FACEAPP achieving 93% of accuracy in the fake detection task, demonstrated the effectiveness of the proposed technique on a real-case scenario.","",""
0,"Sudheer Pullagura, S. V. N. Srinivasu","Monitoring & Controlling of Information against Unethical Hacking using Effective Machine Learning Techniques",2020,"","","","",84,"2022-07-13 09:32:18","","10.35940/ijeat.e9928.069520","","",,,,,0,0.00,0,2,2,"Many services are currently utilizing AI estimates to pick high-stake options. Determining the proper selection unequivocally relies on the rightness of the relevant information. This fact offers encouraging motivators to hackers to attempt to mislead Artificial Intelligence estimations through managing the relevant information that is taken care of to the estimates. But at that point, standard AI computations are certainly not wanted to become protected while encountering surprising details resources. At the moment, deal with the concern of ill-disposed AI; i.e., our experts will most likely generate risk-free AI calculations robust within the attraction of a loud or an adversely managed information. Ill-disposed Artificial Intelligence will be even more screening when the perfect turnout has a mind-boggling framework. At this moment, noteworthy limelight gets on adversarial AI for preparing for organized returns. To begin with, our team build up yet another calculation that dependably carries out accumulated collection, which is an organized expectation concern. Our discovering approach works and also is described as a curved square system. This method is sure about the desire calculation in both the closeness as well as the absence of an opponent. Next off, our team looks into the problem of criterion learning for strenuous, coordinated projection models. This technique develops regularization capacities dependent on the restrictions of the adversary. Now, illustrate that durability to the command of details corresponds to some regularization for a tremendous edge arranged assumption and the other way around.A typical device commonly either requires more computational capability to structure a clear-cut best assault, or it doesn't have adequate records about the trainee's design to accomplish, therefore. Consequently, it routinely tries to use many unnatural changes to the payment to a desire to bring in an accomplishment. This reality advises that on the occasion that our experts confine the usual lousy luck job under ill-disposed commotion, we will get vitality against ordinary opponents. Failure preparing seems like such an outcry mixture circumstance. Our experts calculate a regularization technique for an enormous edge parameter, discovering depending on the failure system. We stretch out dropout regularization to non-straight parts in a handful of oneof-a-kind means. Empirical analyses show that our systems reliably pounded the standards on a variety of datasets. This proposition integrates a recently dispersed and individual coauthored component.","",""
0,"Nikhil Parab","RecycleNet Final Report",2020,"","","","",85,"2022-07-13 09:32:18","","","","",,,,,0,0.00,0,1,2,"Waste production and management pose a large ecological and existential risk to our planet. The use of artificial intelligence (AI) to develop an accurate, low-cost, and lightweight waste classifier garners significant environmental and economic benefits. However, most of the AI research in this area have identified data limitation as a key challenge. In turn, this paper explores to augment the strength of these existing solutions by using Generative Adversarial Network (GAN) to synthesize training data, in order to better capture recyclable waste. A robust data-set will ultimately improve the accuracy and recall of existing deep classifiers, and warrant further research and investment in this area.","",""
10,"R. Vinayakumar, K. Soman, P. Poornachandran, S. Akarsh","Application of Deep Learning Architectures for Cyber Security",2019,"","","","",86,"2022-07-13 09:32:18","","10.1007/978-3-030-16837-7_7","","",,,,,10,3.33,3,4,3,"","",""
0,"Haote Yang, Shikui Tu","GLmser: A GAN-Lmser Network for Image-to-Image Translation",2019,"","","","",87,"2022-07-13 09:32:18","","10.1109/ICTAI.2019.00087","","",,,,,0,0.00,0,2,3,"We present a GAN-Lmser network for the problem of transforming an image from one domain A to another B. The proposed network is based on CNN-Lmser, a recent further extension to deep convolutional layers from least mean square error reconstruction (Lmser) network, which was originally proposed in 1991. Specifically, in GAN-Lmser, the two directions, A-to-B and B-to-A, share the same architecture and symmetrically the same weights, by following the duality in bidirectional architecture (DBA) and duality connection weights (DCW) of Lmser, and an adversarial loss from GAN(generative adversarial network) was added to Lmser. Compared with the famous image-to-image translation model CycleGAN, the GAN-Lmser is compact with a significantly reduced number of parameters and is able to transfer learning through weight sharing between the two directions. Experiments demonstrate that GAN-Lmser is at least comparable to CycleGAN in benchmark datasets, and is robust when the training sample size is small.","",""
2,"Victoria Resources","Work experience and internships",2015,"","","","",88,"2022-07-13 09:32:18","","","","",,,,,2,0.29,2,1,7,"Oct’19-Present University of Cambridge Research Assistant Working on semi-supervised techniques for image to image translation. Sep’18-Sep’19 Inception Institute of Artificial Intelligence, Abu Dhabi Research Intern Designed a novel training scheme for image classification task making the model robust against adversarial attacks, by restricting the hidden space of deep networks.| Paper| Code Designed a non-di erentiable defense mechanisms by selectively adding high frequency components to an image which nullify the effect of adversarial perturbations. | Paper| Code","",""
18,"E. Winner, M. Veloso","Multi-Fidelity Robotic Behaviors: Acting with Variable State Information",2000,"","","","",89,"2022-07-13 09:32:18","","","","",,,,,18,0.82,9,2,22,"Our work is driven by one of the core purposes of artificial intelligence: to develop real r obotic agents that achieve complex high-level goals in real-time environments. Robotic behaviors select actions as a function of the state of the robot and of the world. Designing robust and appropriate robotic behaviors is a difficult because of noise, uncertainty and the cost of acquiring the necessary state information. We addressed this challenge within the concrete domain of robotic soccer with fully autonomous legged robots provided by Sony. In this paper, we present one of the outcomes of this research: the introduction of multi-fidelity behaviors to explicitly adapt to different levels of state information accuracy. The paper motivates and introduces our general approach and then reports on our concrete work with the Sony robots. The multi-fidelity behaviors we developed allow the r obots to successfully achieve their goals in a dynamic and adversarial environment. A robot acts according to a set of behaviors that aggressively balance the cost of acquiring state information with the value of that information to the robot’s ab ility to achieve its high-level goals. The paper includes empirical experiments which support our method of balancing the cost and benefit of the incrementally-accurate state information.","",""
109,"Shilin Qiu, Qihe Liu, Shijie Zhou, Chunjiang Wu","Review of Artificial Intelligence Adversarial Attack and Defense Technologies",2019,"","","","",90,"2022-07-13 09:32:18","","10.3390/APP9050909","","",,,,,109,36.33,27,4,3,"In recent years, artificial intelligence technologies have been widely used in computer vision, natural language processing, automatic driving, and other fields. However, artificial intelligence systems are vulnerable to adversarial attacks, which limit the applications of artificial intelligence (AI) technologies in key security fields. Therefore, improving the robustness of AI systems against adversarial attacks has played an increasingly important role in the further development of AI. This paper aims to comprehensively summarize the latest research progress on adversarial attack and defense technologies in deep learning. According to the target model’s different stages where the adversarial attack occurred, this paper expounds the adversarial attack methods in the training stage and testing stage respectively. Then, we sort out the applications of adversarial attack technologies in computer vision, natural language processing, cyberspace security, and the physical world. Finally, we describe the existing adversarial defense methods respectively in three main categories, i.e., modifying data, modifying models and using auxiliary tools.","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",91,"2022-07-13 09:32:18","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
10,"M. Alomar, M. Hameed, N. Al‐Ansari, M. Alsaadi","Data-Driven Model for the Prediction of Total Dissolved Gas: Robust Artificial Intelligence Approach",2020,"","","","",92,"2022-07-13 09:32:18","","10.1155/2020/6618842","","",,,,,10,5.00,3,4,2,"Saturated total dissolved gas (TDG) is recently considered as a serious issue in the environmental engineering field since it stands behind the reasons for increasing the mortality rates of fish and aquatic organisms. The accurate and more reliable prediction of TDG has a very significant role in preserving the diversity of aquatic organisms and reducing the phenomenon of fish deaths. Herein, two machine learning approaches called support vector regression (SVR) and extreme learning machine (ELM) have been applied to predict the saturated TDG% at USGS 14150000 and USGS 14181500 stations which are located in the USA. For the USGS 14150000 station, the recorded samples from 13 October 2016 to 14 March 2019 (75%) were used for training set, and the rest from 15 March 2019 to 13 October 2019 (25%) were used for testing requirements. Similarly, for USGS 14181500 station, the hourly data samples which covered the period from 9 June 2017 till 11 March 2019 were used for calibrating the models and from 12 March 2019 until 9 October 2019 were used for testing the predictive models. Eight input combinations based on different parameters have been established as well as nine statistical performance measures have been used for evaluating the accuracy of adopted models, for instance, not limited, correlation of determination (        R      2        ), mean absolute relative error (MAE), and uncertainty at 95% (        U      95        ). The obtained results of the study for both stations revealed that the ELM managed efficiently to estimate the TDG in comparison to SVR technique. For USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.986 (0.986), MAE of 0.316 (0.441), and         U      95        of 3.592 (3.869). Lastly, for USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.991 (0.991), MAE of 0.338 (0.396), and         U      95        of 0.832 (0.837). In addition, ELM’s training process computational time is stated to be much shorter than that of SVM. The results also showed that the temperature parameter was the most significant variable that influenced TDG relative to the other parameters. Overall, the proposed model (ELM) proved to be an appropriate and efficient computer-assisted technology for saturated TDG modeling that will contribute to the basic knowledge of environmental considerations.","",""
120,"Hoang Nguyen, X. Bui","Predicting Blast-Induced Air Overpressure: A Robust Artificial Intelligence System Based on Artificial Neural Networks and Random Forest",2018,"","","","",93,"2022-07-13 09:32:18","","10.1007/s11053-018-9424-1","","",,,,,120,30.00,60,2,4,"","",""
0,"Bushra Rasheed, M. Usama, Asmara Safdar","Robust Artificial Intelligence Approach to Stabilize and Control Propeller Driven Hybrid UGV",2022,"","","","",94,"2022-07-13 09:32:18","","10.1109/ICAI55435.2022.9773375","","",,,,,0,0.00,0,3,1,"Hybrid Unmanned Ground Vehicle (HUGV) can drive on any terrain including walls and fly as well, using the multi directional thrust force of propellers. In the era of industrial revolution, hybrid UGVs need to be autonomous with intelligent decision making capabilities. During wall climbing of hybrid UGVs, stability is essential and depends on real time feedback from multiple sensors. To increase stability and control, it is proposed that PID control loops should be replaced by AI based algorithms that reduce the decision time and mathematical complexity. For autonomous movement in any terrain using the proposed model, intelligent UGVs can map and localize simultaneously.They can make intelligent decisions about mode of movement i.e. driving on ground or wall, steering on ground or wall, flying and maneuvering by using real time sensor readings. Integration of the proposed AI models with HUGV can be applied to many areas which are hard for humans to access, for instance; inspection of large structures, bio & nuclear hazard environments, planetary exploration & magnetic fields detection.","",""
8,"Xinyi Li, C. Wang, Y. Sheng, Jiahan Zhang, Wentao Wang, F. Yin, Qiuwen Wu, Q. Wu, Y. Ge","An Artificial Intelligence-Driven Agent for Real-Time Head-and-Neck IMRT Plan Generation using Conditional Generative Adversarial Network (cGAN).",2020,"","","","",95,"2022-07-13 09:32:18","","10.1002/mp.14770","","",,,,,8,4.00,1,9,2,"PURPOSE To develop an Artificial-Intelligence (AI) agent for fully automated rapid head-and-neck IMRT plan generation without time-consuming dose-volume-based inverse planning.   METHODS This AI agent was trained via implementing a conditional Generative Adversarial Network (cGAN) architecture. The generator, PyraNet, is a novel Deep Learning network that implements 28 classic ResNet blocks in pyramid-like concatenations. The discriminator is a customized 4-layer DenseNet. The AI agent first generates multiple customized 2D projections at 9 template beam angles from a patient's 3D CT volume and structures. These projections are then stacked as 4D inputs of PyraNet, from which 9 radiation fluence maps of the corresponding template beam angles are generated simultaneously. Finally, the predicted fluence maps are automatically post-processed by Gaussian deconvolution operations and imported into a commercial treatment planning system (TPS) for plan integrity check and visualization. The AI agent was built and tested upon 231 oropharyngeal IMRT plans from a TPS plan library. 200/16/15 plans were assigned for training/validation/testing, respectively. Only the primary plans in the sequential boost regime were studied. All plans were normalized to 44Gy prescription (2Gy/fx). A customized Harr wavelet loss was adopted for fluence map comparison during the training of the PyraNet. For test cases, isodose distributions in AI plans and TPS plans were qualitatively evaluated for overall dose distributions. Key dosimetric metrics were compared by Wilcoxson Signed Rank tests with a significance level of 0.05.   RESULTS All 15 AI plans were successfully generated. Isodose gradients outside of PTV in AI plans were comparable to those of the TPS plans. After PTV coverage normalization, Dmean of left parotid (DAI =23.1±2.4Gy; DTPS =23.1±2.0Gy), right parotid (DAI =23.8±3.0Gy; DTPS =23.9±2.3Gy), and oral cavity (DAI =24.7±6.0Gy; DTPS =23.9±4.3Gy) in the AI plans and the TPS plans were comparable without statistical significance. AI plans achieved comparable results for maximum dose at 0.01cc of brainstem (DAI =15.0±2.1Gy; DTPS =15.5±2.7Gy) and cord+5mm (DAI =27.5±2.3Gy; DTPS =25.8±1.9Gy) without clinically-relevant differences, but body Dmax results (DAI =121.1±3.9Gy; DTPS =109.0±0.9Gy) were higher than the TPS plan results. The AI agent needed ~3s for predicting fluence maps of an IMRT plan.   CONCLUSIONS With rapid and fully automated execution, the developed AI agent can generate complex head-and-neck IMRT plans with acceptable dosimetry quality. This approach holds great potential for clinical applications in pre-planning decision-making and real-time planning.","",""
6,"Rowan T. Hughes, Liming Zhu, Tomasz Bednarz","Generative Adversarial Networks–Enabled Human–Artificial Intelligence Collaborative Applications for Creative and Design Industries: A Systematic Review of Current Approaches and Trends",2021,"","","","",96,"2022-07-13 09:32:18","","10.3389/frai.2021.604234","","",,,,,6,6.00,2,3,1,"The future of work and workplace is very much in flux. A vast amount has been written about artificial intelligence (AI) and its impact on work, with much of it focused on automation and its impact in terms of potential job losses. This review will address one area where AI is being added to creative and design practitioners’ toolbox to enhance their creativity, productivity, and design horizons. A designer’s primary purpose is to create, or generate, the most optimal artifact or prototype, given a set of constraints. We have seen AI encroaching into this space with the advent of generative networks and generative adversarial networks (GANs) in particular. This area has become one of the most active research fields in machine learning over the past number of years, and a number of these techniques, particularly those around plausible image generation, have garnered considerable media attention. We will look beyond automatic techniques and solutions and see how GANs are being incorporated into user pipelines for design practitioners. A systematic review of publications indexed on ScienceDirect, SpringerLink, Web of Science, Scopus, IEEExplore, and ACM DigitalLibrary was conducted from 2015 to 2020. Results are reported according to PRISMA statement. From 317 search results, 34 studies (including two snowball sampled) are reviewed, highlighting key trends in this area. The studies’ limitations are presented, particularly a lack of user studies and the prevalence of toy-examples or implementations that are unlikely to scale. Areas for future study are also identified.","",""
6,"Zixiao Kong, Jingfeng Xue, Yong Wang, Lu Huang, Zequn Niu, Feng Li","A Survey on Adversarial Attack in the Age of Artificial Intelligence",2021,"","","","",97,"2022-07-13 09:32:18","","10.1155/2021/4907754","","",,,,,6,6.00,1,6,1,"With the rapid evolution of the Internet, the application of artificial intelligence fields is more and more extensive, and the era of AI has come. At the same time, adversarial attacks in the AI field are also frequent. Therefore, the research into adversarial attack security is extremely urgent. An increasing number of researchers are working in this field. We provide a comprehensive review of the theories and methods that enable researchers to enter the field of adversarial attack. This article is according to the “Why? →What? → How?” research line for elaboration. Firstly, we explain the significance of adversarial attack. Then, we introduce the concepts, types, and hazards of adversarial attack. Finally, we review the typical attack algorithms and defense techniques in each application area. Facing the increasingly complex neural network model, this paper focuses on the fields of image, text, and malicious code and focuses on the adversarial attack classifications and methods of these three data types, so that researchers can quickly find their own type of study. At the end of this review, we also raised some discussions and open issues and compared them with other similar reviews.","",""
7,"Yulei Wu","Robust Learning-Enabled Intelligence for the Internet of Things: A Survey From the Perspectives of Noisy Data and Adversarial Examples",2021,"","","","",98,"2022-07-13 09:32:18","","10.1109/JIOT.2020.3018691","","",,,,,7,7.00,7,1,1,"The Internet of Things (IoT) has been widely adopted in a range of verticals, e.g., automation, health, energy, and manufacturing. Many of the applications in these sectors, such as self-driving cars and remote surgery, are critical and high stakes applications, calling for advanced machine learning (ML) models for data analytics. Essentially, the training and testing data that are collected by massive IoT devices may contain noise (e.g., abnormal data, incorrect labels, and incomplete information) and adversarial examples. This requires high robustness of ML models to make reliable decisions for IoT applications. The research of robust ML has received tremendous attention from both academia and industry in recent years. This article will investigate the state of the art and representative works of robust ML models that can enable high resilience and reliability of IoT intelligence. Two aspects of robustness will be focused on, i.e., when the training data of ML models contain noises and adversarial examples, which may typically happen in many real-world IoT scenarios. In addition, the reliability of both neural networks and reinforcement learning framework will be investigated. Both of these two ML paradigms have been widely used in handling data in IoT scenarios. The potential research challenges and open issues will be discussed to provide future research directions.","",""
3,"Vibekananda Dutta, T. Zielińska","An Adversarial Explainable Artificial Intelligence (XAI) Based Approach for Action Forecasting",2021,"","","","",99,"2022-07-13 09:32:18","","10.14313/JAMRIS/4-2020/38","","",,,,,3,3.00,2,2,1,"Abstract: Despite the growing popularity of machine learning technology, vision‐based action recognition/forecasting systems are seen as black‐boxes by the user. The effecti‐ veness of such systems depends on the machine learning algorithms, it is difficult (or impossible) to explain the de‐ cisions making processes to the users. In this context, an approach that offers the user understanding of these re‐ asoning models is significant. To do this, we present an Explainable Artificial Intelligence (XAI) based approach to action forecasting using structured database and object affordances definition. The structured database is sup‐ porting the prediction process. The method allows to vi‐ sualize the components of the structured database. Later, the components of the base are used for forecasting the nominally possible motion goals. The object affordance explicated by the probability functions supports the se‐ lection of possiblemotion goals. The presentedmethodo‐ logy allows satisfactory explanations of the reasoning be‐ hind the inference mechanism. Experimental evaluation was conducted using the WUT‐18 dataset, the efficiency of the presented solution was compared to the other ba‐ seline algorithms.","",""
81,"Thomas G. Dietterich","Steps Toward Robust Artificial Intelligence",2017,"","","","",100,"2022-07-13 09:32:18","","10.1609/aimag.v38i3.2756","","",,,,,81,16.20,81,1,5,"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world","",""
81,"Thomas G. Dietterich","Steps Toward Robust Artificial Intelligence",2017,"","","","",101,"2022-07-13 09:32:18","","10.1609/aimag.v38i3.2756","","",,,,,81,16.20,81,1,5,"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world","",""
0,"Massimiliano Lupo Pasini, Junqi Yin","Stable Parallel Training of Wasserstein Conditional Generative Adversarial Neural Networks : *Full/Regular Research Paper submission for the symposium CSCI-ISAI: Artificial Intelligence",2021,"","","","",102,"2022-07-13 09:32:18","","10.1109/CSCI54926.2021.00026","","",,,,,0,0.00,0,2,1,"We use a stable parallel approach to train Wasserstein Conditional Generative Adversarial Neural Networks (W-CGANs). The parallel training reduces the risk of mode collapse and enhances scalability by using multiple generators that are concurrently trained, each one of them focusing on a single data label. The use of the Wasserstein metric reduces the risk of cycling by stabilizing the training of each generator. We apply the approach on the CIFAR10 and the CIFAR100 datasets, two standard benchmark datasets with images of the same resolution, but different number of classes. Performance is assessed using the inception score, the Fréchet inception distance, and image quality. An improvement in inception score and Fréchet inception distance is shown in comparison to previous results obtained by performing the parallel approach on deep convolutional conditional generative adversarial neural networks (DC-CGANs). Weak scaling is attained on both datasets using up to 100 NVIDIA V100 GPUs on the OLCF supercomputer Summit.","",""
19,"Thomas G. Dietterich","Robust artificial intelligence and robust human organizations",2018,"","","","",103,"2022-07-13 09:32:18","","10.1007/s11704-018-8900-4","","",,,,,19,4.75,19,1,4,"","",""
14,"I. Tyukin, D. Higham, A. Gorban","On Adversarial Examples and Stealth Attacks in Artificial Intelligence Systems",2020,"","","","",104,"2022-07-13 09:32:18","","10.1109/IJCNN48605.2020.9207472","","",,,,,14,7.00,5,3,2,"In this work we present a formal theoretical framework for assessing and analyzing two classes of malevolent action towards generic Artificial Intelligence (AI) systems. Our results apply to general multi-class classifiers that map from an input space into a decision space, including artificial neural networks used in deep learning applications. Two classes of attacks are considered. The first class involves adversarial examples and concerns the introduction of small perturbations of the input data that cause misclassification. The second class, introduced here for the first time and named stealth attacks, involves small perturbations to the AI system itself. Here the perturbed system produces whatever output is desired by the attacker on a specific small data set, perhaps even a single input, but performs as normal on a validation set (which is unknown to the attacker).We show that in both cases, i.e., in the case of an attack based on adversarial examples and in the case of a stealth attack, the dimensionality of the AI’s decision-making space is a major contributor to the AI’s susceptibility. For attacks based on adversarial examples, a second crucial parameter is the absence of local concentrations in the data probability distribution, a property known as Smeared Absolute Continuity. According to our findings, robustness to adversarial examples requires either (a) the data distributions in the AI’s feature space to have concentrated probability density functions or (b) the dimensionality of the AI’s decision variables to be sufficiently small. We also show how to construct stealth attacks on high-dimensional AI systems that are hard to spot unless the validation set is made exponentially large.","",""
14,"Gaolei Li, K. Ota, M. Dong, Jun Wu, Jianhua Li","DeSVig: Decentralized Swift Vigilance Against Adversarial Attacks in Industrial Artificial Intelligence Systems",2020,"","","","",105,"2022-07-13 09:32:18","","10.1109/TII.2019.2951766","","",,,,,14,7.00,3,5,2,"Individually reinforcing the robustness of a single deep learning model only gives limited security guarantees especially when facing adversarial examples. In this article, we propose DeSVig, a decentralized swift vigilance framework to identify adversarial attacks in an industrial artificial intelligence systems (IAISs), which enables IAISs to correct the mistake in a few seconds. The DeSVig is highly decentralized, which improves the effectiveness of recognizing abnormal inputs. We try to overcome the challenges on ultralow latency caused by dynamics in industries using peculiarly designated mobile edge computing and generative adversarial networks. The most important advantage of our work is that it can significantly reduce the failure risks of being deceived by adversarial examples, which is critical for safety-prioritized and delay-sensitive environments. In our experiments, adversarial examples of industrial electronic components are generated by several classical attacking models. Experimental results demonstrate that the DeSVig is more robust, efficient, and scalable than some state-of-art defenses.","",""
7,"Kwonsang Sohn, Christine Sung, Gukwon Koo, O. Kwon","Artificial intelligence in the fashion industry: consumer responses to generative adversarial network (GAN) technology",2020,"","","","",106,"2022-07-13 09:32:18","","10.1108/IJRDM-03-2020-0091","","",,,,,7,3.50,2,4,2,"This study examines consumers' evaluations of product consumption values, purchase intentions and willingness to pay for fashion products designed using generative adversarial network (GAN), an artificial intelligence technology. This research investigates differences between consumers' evaluations of a GAN-generated product and a non-GAN-generated product and tests whether disclosing the use of GAN technology affects consumers' evaluations.,Sample products were developed as experimental stimuli using cycleGAN. Data were collected from 163 members of Generation Y. Participants were assigned to one of the three experimental conditions (i.e. non-GAN-generated images, GAN-generated images with disclosure and GAN-generated images without disclosure). Regression analysis and ANOVA were used to test the hypotheses.,Functional, social and epistemic consumption values positively affect willingness to pay in the GAN-generated products. Relative to non-GAN-generated products, willingness to pay is significantly higher for GAN-generated products. Moreover, evaluations of functional value, emotional value and willingness to pay are highest when GAN technology is used, but not disclosed.,This study evaluates the utility of GANs from consumers' perspective based on the perceived value of GAN-generated product designs. Findings have practical implications for firms that are considering using GANs to develop products for the retail fashion market.","",""
5,"Jing Qiu, Lei Du, Yuanyuan Chen, Zhihong Tian, Xiaojiang Du, M. Guizani","Artificial Intelligence Security in 5G Networks: Adversarial Examples for Estimating a Travel Time Task",2020,"","","","",107,"2022-07-13 09:32:18","","10.1109/mvt.2020.3002487","","",,,,,5,2.50,1,6,2,"With the rapid development of the Internet, the nextgeneration network (5G) has emerged. 5G can support a variety of new applications, such as the Internet of Things (IoT), virtual reality (VR), and the Internet of Vehicles. Most of these new applications depend on deep learning algorithms, which have made great advances in many areas of artificial intelligence (AI). However, researchers have found that AI algorithms based on deep learning pose numerous security problems. For example, deep learning is susceptible to a well-designed input sample formed by adding small perturbations to the original sample. This well-designed input with small perturbations, which are imperceptible to humans, is called an adversarial example. An adversarial example is similar to a truth example, but it can render the deep learning model invalid. In this article, we generate adversarial examples for spatiotemporal data. Based on the travel time estimation (TTE) task, we use two methods-white-box and blackbox attacks-to invalidate deep learning models. Experiment results show that the adversarial examples successfully attack the deep learning model and thus that AI security is a big challenge of 5G.","",""
0,"Charlie T. Veal, Marshall Lindsay, S. Kovaleski, Derek T. Anderson, Stanton R. Price","Evolutionary Algorithm Driven Explainable Adversarial Artificial Intelligence",2020,"","","","",108,"2022-07-13 09:32:18","","10.1109/SSCI47803.2020.9308361","","",,,,,0,0.00,0,5,2,"It is well-known that machine learning algorithms can be susceptible to undesirable effects when exposed to conditions that are not expressed adequately in the training dataset. This leads to a growing interest throughout many communities; where do algorithms and trained models break? Recently, methods such as generative adversarial neural networks and variational autoencoders were proposed to create adversarial examples that challenge algorithms. This results in artificial intelligence having higher false detections or completely losing recognition. The problem is that existing solutions, are for the most part, black boxes. Current gaps include how do we better control and understand adversarial algorithms. Herein, we propose the concept of an adversarial modifier set as an understandable and controlled way to generate adversarial examples. This is achieved by exploiting the improved evolution-constructed algorithm to identify ideal features that a victim algorithm values in imagery. These features are combined to realize a tuple library that preserves spatial relations. Last, a set of algorithmically controlled modifiers that generate the imagery are found by examining the content of the false imagery. Preliminary results are encouraging and demonstrate that this approach has benefits in both generating explainable adversarial examples, as well as shedding some insight into victim algorithm decision making.","",""
18,"Hwiyoung Kim, D. Jung, B. Choi","Exploiting the Vulnerability of Deep Learning-Based Artificial Intelligence Models in Medical Imaging: Adversarial Attacks",2019,"","","","",109,"2022-07-13 09:32:18","","10.3348/JKSR.2019.80.2.259","","",,,,,18,6.00,6,3,3,"Due to rapid developments in the deep learning model, artificial intelligence (AI) models are expected to enhance clinical diagnostic ability and work efficiency by assisting physicians. Therefore, many hospitals and private companies are competing to develop AI-based automatic diagnostic systems using medical images. In the near future, many deep learning-based automatic diagnostic systems would be used clinically. However, the possibility of adversarial attacks exploiting certain vulnerabilities of the deep learning algorithm is a major obstacle to deploying deep learning-based systems in clinical practice. In this paper, we will examine in detail the kinds of principles and methods of adversarial attacks that can be made to deep learning models dealing with medical images, the problems that can arise, and the preventive measures that can be taken against them.","",""
1,"O. Jenkins, D. Lopresti, M. Mitchell","Next Wave Artificial Intelligence: Robust, Explainable, Adaptable, Ethical, and Accountable",2020,"","","","",110,"2022-07-13 09:32:18","","","","",,,,,1,0.50,0,3,2,"The history of AI has included several ""waves"" of ideas. The first wave, from the mid-1950s to the 1980s, focused on logic and symbolic hand-encoded representations of knowledge, the foundations of so-called ""expert systems"". The second wave, starting in the 1990s, focused on statistics and machine learning, in which, instead of hand-programming rules for behavior, programmers constructed ""statistical learning algorithms"" that could be trained on large datasets. In the most recent wave research in AI has largely focused on deep (i.e., many-layered) neural networks, which are loosely inspired by the brain and trained by ""deep learning"" methods. However, while deep neural networks have led to many successes and new capabilities in computer vision, speech recognition, language processing, game-playing, and robotics, their potential for broad application remains limited by several factors.  A concerning limitation is that even the most successful of today's AI systems suffer from brittleness-they can fail in unexpected ways when faced with situations that differ sufficiently from ones they have been trained on. This lack of robustness also appears in the vulnerability of AI systems to adversarial attacks, in which an adversary can subtly manipulate data in a way to guarantee a specific wrong answer or action from an AI system. AI systems also can absorb biases-based on gender, race, or other factors-from their training data and further magnify these biases in their subsequent decision-making. Taken together, these various limitations have prevented AI systems such as automatic medical diagnosis or autonomous vehicles from being sufficiently trustworthy for wide deployment. The massive proliferation of AI across society will require radically new ideas to yield technology that will not sacrifice our productivity, our quality of life, or our values.","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",111,"2022-07-13 09:32:18","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
67,"Zhiwen Deng, Chuan He, Yingzheng Liu, Kyung Chun Kim","Super-resolution reconstruction of turbulent velocity fields using a generative adversarial network-based artificial intelligence framework",2019,"","","","",112,"2022-07-13 09:32:18","","10.1063/1.5127031","","",,,,,67,22.33,17,4,3,"","",""
2,"Utku Kose","Techniques for Adversarial Examples Threatening the Safety of Artificial Intelligence Based Systems",2019,"","","","",113,"2022-07-13 09:32:18","","","","",,,,,2,0.67,2,1,3,"Artificial intelligence is known as the most effective technological field for rapid developments shaping the future of the world. Even today, it is possible to see intense use of intelligence systems in all fields of the life. Although advantages of the Artificial Intelligence are widely observed, there is also a dark side employing efforts to design hacking oriented techniques against Artificial Intelligence. Thanks to such techniques, it is possible to trick intelligent systems causing directed results for unsuccessful outputs. That is critical for also cyber wars of the future as it is predicted that the wars will be done unmanned, autonomous intelligent systems. Moving from the explanations, objective of this study is to provide information regarding adversarial examples threatening the Artificial Intelligence and focus on details of some techniques, which are used for creating adversarial examples. Adversarial examples are known as training data, which can trick a Machine Learning technique to learn incorrectly about the target problem and cause an unsuccessful or maliciously directed intelligent system at the end. The study enables the readers to learn enough about details of recent techniques for creating adversarial examples.","",""
44,"A. Goli, H. Zare, R. Tavakkoli-Moghaddam, A. Sadeghieh","Hybrid artificial intelligence and robust optimization for a multi-objective product portfolio problem Case study: The dairy products industry",2019,"","","","",114,"2022-07-13 09:32:18","","10.1016/j.cie.2019.106090","","",,,,,44,14.67,11,4,3,"","",""
26,"M. Kuzlu, Corinne Fair, Ozgur Guler","Role of Artificial Intelligence in the Internet of Things (IoT) cybersecurity",2021,"","","","",115,"2022-07-13 09:32:18","","10.1007/S43926-020-00001-4","","",,,,,26,26.00,9,3,1,"","",""
16,"A. Amritphale, Ranojoy Chatterjee, Suvo Chatterjee, N. Amritphale, Ali Rahnavard, G. Awan, B. Omar, G. Fonarow","Predictors of 30-Day Unplanned Readmission After Carotid Artery Stenting Using Artificial Intelligence",2021,"","","","",116,"2022-07-13 09:32:18","","10.1007/s12325-021-01709-7","","",,,,,16,16.00,2,8,1,"","",""
0,"Yajie Wang, Yu-an Tan, T. Baker, Neeraj Kumar, Quanxin Zhang","Deep Fusion: Crafting Transferable Adversarial Examples and Improving Robustness of Industrial Artificial Intelligence of Things",2022,"","","","",117,"2022-07-13 09:32:18","","10.1109/tii.2022.3168874","","",,,,,0,0.00,0,5,1,"","",""
1,"Karunanithi, O. AlHeyasat, D. Thomas, G. Kavitha","Attacks on Artificial Intelligence Applications through Adversarial Image",2018,"","","","",118,"2022-07-13 09:32:18","","","","",,,,,1,0.25,0,4,4,"Artificial Intelligence (AI) is a subset of Machine Learning. Cyber Security is always a game between defender and attacker. AI in cyber security plays a vital role in automating the security process which reduces human monitoring round the clock.Cyber attacks on AI applications are discussed in this paper. Adversarial image attack is a way to fool AI and humans in cyber world. Adversarial Image is altering the original image with algorithm which makes difficult to identify or classify the image by AI through Convolutional Neural Network. Even if both image looks similar to human eye, internal bits are altered to misclassify the image. Several methodologies are adopted to create these adversarial images attack by the attackers. Impacts of these Adversarial image attacks cause severe damages in various domains like social networking, Medical Image Diagnosis, Text Extraction from images, Security Cameras image analysis etc., This Adversarial attacks is future challenging aspect of the AI Cyber Security mechanism.","",""
164,"M. Jamshidi, A. Lalbakhsh, J. Talla, Z. Peroutka, F. Hadjilooei, Pedram Lalbakhsh, M. Jamshidi, L. Spada, M. Mirmozafari, Mojgan Dehghani, Asal Sabet, Saeed Roshani, S. Roshani, Nima Bayat-Makou, B. Mohamadzade, Zahra Malek, A. Jamshidi, S. Kiani, H. Hashemi‐Dezaki, Wahab Mohyuddin","Artificial Intelligence and COVID-19: Deep Learning Approaches for Diagnosis and Treatment",2020,"","","","",119,"2022-07-13 09:32:18","","10.1109/ACCESS.2020.3001973","","",,,,,164,82.00,16,20,2,"COVID-19 outbreak has put the whole world in an unprecedented difficult situation bringing life around the world to a frightening halt and claiming thousands of lives. Due to COVID-19’s spread in 212 countries and territories and increasing numbers of infected cases and death tolls mounting to 5,212,172 and 334,915 (as of May 22 2020), it remains a real threat to the public health system. This paper renders a response to combat the virus through Artificial Intelligence (AI). Some Deep Learning (DL) methods have been illustrated to reach this goal, including Generative Adversarial Networks (GANs), Extreme Learning Machine (ELM), and Long/Short Term Memory (LSTM). It delineates an integrated bioinformatics approach in which different aspects of information from a continuum of structured and unstructured data sources are put together to form the user-friendly platforms for physicians and researchers. The main advantage of these AI-based platforms is to accelerate the process of diagnosis and treatment of the COVID-19 disease. The most recent related publications and medical reports were investigated with the purpose of choosing inputs and targets of the network that could facilitate reaching a reliable Artificial Neural Network-based tool for challenges associated with COVID-19. Furthermore, there are some specific inputs for each platform, including various forms of the data, such as clinical data and medical imaging which can improve the performance of the introduced approaches toward the best responses in practical applications.","",""
10,"Qiang Zhang, M. Burrage, E. Lukaschuk, M. Shanmuganathan, Iulia A. Popescu, C. Nikolaidou, Rebecca Mills, K. Werys, Evan Hann, A. Barutcu, Suleyman D Polat, M. Salerno, M. Jerosch-Herold, R. Kwong, H. Watkins, C. Kramer, S. Neubauer, V. Ferreira, S. Piechnik","Toward Replacing Late Gadolinium Enhancement With Artificial Intelligence Virtual Native Enhancement for Gadolinium-Free Cardiovascular Magnetic Resonance Tissue Characterization in Hypertrophic Cardiomyopathy",2021,"","","","",120,"2022-07-13 09:32:18","","10.1161/CIRCULATIONAHA.121.054432","","",,,,,10,10.00,1,19,1,"Supplemental Digital Content is available in the text. Background: Late gadolinium enhancement (LGE) cardiovascular magnetic resonance (CMR) imaging is the gold standard for noninvasive myocardial tissue characterization but requires intravenous contrast agent administration. It is highly desired to develop a contrast agent–free technology to replace LGE for faster and cheaper CMR scans. Methods: A CMR virtual native enhancement (VNE) imaging technology was developed using artificial intelligence. The deep learning model for generating VNE uses multiple streams of convolutional neural networks to exploit and enhance the existing signals in native T1 maps (pixel-wise maps of tissue T1 relaxation times) and cine imaging of cardiac structure and function, presenting them as LGE-equivalent images. The VNE generator was trained using generative adversarial networks. This technology was first developed on CMR datasets from the multicenter Hypertrophic Cardiomyopathy Registry, using hypertrophic cardiomyopathy as an exemplar. The datasets were randomized into 2 independent groups for deep learning training and testing. The test data of VNE and LGE were scored and contoured by experienced human operators to assess image quality, visuospatial agreement, and myocardial lesion burden quantification. Image quality was compared using a nonparametric Wilcoxon test. Intra- and interobserver agreement was analyzed using intraclass correlation coefficients (ICC). Lesion quantification by VNE and LGE were compared using linear regression and ICC. Results: A total of 1348 hypertrophic cardiomyopathy patients provided 4093 triplets of matched T1 maps, cines, and LGE datasets. After randomization and data quality control, 2695 datasets were used for VNE method development and 345 were used for independent testing. VNE had significantly better image quality than LGE, as assessed by 4 operators (n=345 datasets; P<0.001 [Wilcoxon test]). VNE revealed lesions characteristic of hypertrophic cardiomyopathy in high visuospatial agreement with LGE. In 121 patients (n=326 datasets), VNE correlated with LGE in detecting and quantifying both hyperintensity myocardial lesions (r=0.77–0.79; ICC=0.77–0.87; P<0.001) and intermediate-intensity lesions (r=0.70–0.76; ICC=0.82–0.85; P<0.001). The native CMR images (cine plus T1 map) required for VNE can be acquired within 15 minutes and producing a VNE image takes less than 1 second. Conclusions: VNE is a new CMR technology that resembles conventional LGE but without the need for contrast administration. VNE achieved high agreement with LGE in the distribution and quantification of lesions, with significantly better image quality.","",""
10,"T. Penzkofer, A. Padhani, B. Turkbey, M. Haider, H. Huisman, J. Walz, G. Salomon, I. Schoots, J. Richenberg, G. Villeirs, V. Panebianco, O. Rouvière, V. Løgager, J. Barentsz","ESUR/ESUI position paper: developing artificial intelligence for precision diagnosis of prostate cancer using magnetic resonance imaging",2021,"","","","",121,"2022-07-13 09:32:18","","10.1007/s00330-021-08021-6","","",,,,,10,10.00,1,14,1,"","",""
2,"B. Nair, Yakov Diskin, V. Asari","Multi-modal low cost mobile indoor surveillance system on the Robust Artificial Intelligence-based Defense Electro Robot (RAIDER)",2012,"","","","",122,"2022-07-13 09:32:18","","10.1117/12.930353","","",,,,,2,0.20,1,3,10,"We present an autonomous system capable of performing security check routines. The surveillance machine, the Clearpath Husky robotic platform, is equipped with three IP cameras with different orientations for the surveillance tasks of face recognition, human activity recognition, autonomous navigation and 3D reconstruction of its environment. Combining the computer vision algorithms onto a robotic machine has given birth to the Robust Artificial Intelligencebased Defense Electro-Robot (RAIDER). The end purpose of the RAIDER is to conduct a patrolling routine on a single floor of a building several times a day. As the RAIDER travels down the corridors off-line algorithms use two of the RAIDER's side mounted cameras to perform a 3D reconstruction from monocular vision technique that updates a 3D model to the most current state of the indoor environment. Using frames from the front mounted camera, positioned at the human eye level, the system performs face recognition with real time training of unknown subjects. Human activity recognition algorithm will also be implemented in which each detected person is assigned to a set of action classes picked to classify ordinary and harmful student activities in a hallway setting.The system is designed to detect changes and irregularities within an environment as well as familiarize with regular faces and actions to distinguish potentially dangerous behavior. In this paper, we present the various algorithms and their modifications which when implemented on the RAIDER serves the purpose of indoor surveillance.","",""
139,"O. Méndez-Lucio, B. Baillif, Djork-Arné Clevert, D. Rouquié, J. Wichard","De novo generation of hit-like molecules from gene expression signatures using artificial intelligence",2018,"","","","",123,"2022-07-13 09:32:18","","10.1038/s41467-019-13807-w","","",,,,,139,34.75,28,5,4,"","",""
462,"Stuart J. Russell, Dan Dewey, Max Tegmark","Research Priorities for Robust and Beneficial Artificial Intelligence",2015,"","","","",124,"2022-07-13 09:32:18","","10.1609/aimag.v36i4.2577","","",,,,,462,66.00,154,3,7,"Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.","",""
9,"B. N. Manjunatha Reddy, S. K. Pramada, T. Roshni","Monthly surface runoff prediction using artificial intelligence: A study from a tropical climate river basin",2021,"","","","",125,"2022-07-13 09:32:18","","10.1007/s12040-020-01508-8","","",,,,,9,9.00,3,3,1,"","",""
10,"A. C. Horta, A. Silva, C. Sargo, V. M. Gonçalves, T. C. Zangirolami, Roberto Campos Giordano","Robust artificial intelligence tool for automatic start-up of the supplementary medium feeding in recombinant E. coli cultivations",2011,"","","","",126,"2022-07-13 09:32:18","","10.1007/s00449-011-0540-0","","",,,,,10,0.91,2,6,11,"","",""
8,"Linbo Liu, Mingcheng Bi, Yunhua Wang, Junfeng Liu, Xiwen Jiang, Zhongbin Xu, Xingcai Zhang","Artificial intelligence-powered microfluidics for nanomedicine and materials synthesis.",2021,"","","","",127,"2022-07-13 09:32:18","","10.1039/d1nr06195j","","",,,,,8,8.00,1,7,1,"Artificial intelligence (AI) is an emerging technology with great potential, and its robust calculation and analysis capabilities are unmatched by traditional calculation tools. With the promotion of deep learning and open-source platforms, the threshold of AI has also become lower. Combining artificial intelligence with traditional fields to create new fields of high research and application value has become a trend. AI has been involved in many disciplines, such as medicine, materials, energy, and economics. The development of AI requires the support of many kinds of data, and microfluidic systems can often mine object data on a large scale to support AI. Due to the excellent synergy between the two technologies, excellent research results have emerged in many fields. In this review, we briefly review AI and microfluidics and introduce some applications of their combination, mainly in nanomedicine and material synthesis. Finally, we discuss the development trend of the combination of the two technologies.","",""
5,"Thulsiram Gantala, K. Balasubramaniam","Automated Defect Recognition for Welds Using Simulation Assisted TFM Imaging with Artificial Intelligence",2021,"","","","",128,"2022-07-13 09:32:18","","10.1007/s10921-021-00761-1","","",,,,,5,5.00,3,2,1,"","",""
51,"Mohammad Behdad Mohammad Behdad Jamshidi Jamshidi, Ali Ali Lalbakhsh Lalbakhsh, Jakub Jakub Talla Talla, Zdeněk Zdenek Peroutka Peroutka, Farimah Farimah Hadjilooei Hadjilooei, Pedram Pedram Lalbakhsh Lalbakhsh, Morteza Morteza Jamshidi Jamshidi, Luigi La Luigi La Spada Spada, Mirhamed Mirhamed Mirmozafari Mirmozafari, Mojgan Mojgan Dehghani Dehghani, Asal Asal Sabet Sabet, Saeed Saeed Roshani Roshani, Sobhan Sobhan Roshani Roshani, Nima Nima Bayat-Makou Bayat-Makou, Bahare Bahare Mohamadzade Mohamadzade, Zahra Zahra Malek Malek, Alireza Alireza Jamshidi Jamshidi, Sarah Sarah Kiani Kiani, Hamed Hamed Hashemi-Dezaki Hashemi-Dezaki, Wahab Wahab Mohyuddin Mohyuddin","Artificial Intelligence and COVID-19: Deep Learning Approaches for Diagnosis and Treatment",2020,"","","","",129,"2022-07-13 09:32:18","","10.1109/ACCESS.2020.3001973","","",,,,,51,25.50,5,20,2,"COVID-19 outbreak has put the whole world in an unprecedented difficult situation bringing life around the world to a frightening halt and claiming thousands of lives. Due to COVID-19’s spread in 212 countries and territories and increasing numbers of infected cases and death tolls mounting to 5,212,172 and 334,915 (as of May 22 2020), it remains a real threat to the public health system. This paper renders a response to combat the virus through Artificial Intelligence (AI). Some Deep Learning (DL) methods have been illustrated to reach this goal, including Generative Adversarial Networks (GANs), Extreme Learning Machine (ELM), and Long/Short Term Memory (LSTM). It delineates an integrated bioinformatics approach in which different aspects of information from a continuum of structured and unstructured data sources are put together to form the user-friendly platforms for physicians and researchers. The main advantage of these AI-based platforms is to accelerate the process of diagnosis and treatment of the COVID-19 disease. The most recent related publications and medical reports were investigated with the purpose of choosing inputs and targets of the network that could facilitate reaching a reliable Artificial Neural Network-based tool for challenges associated with COVID-19. Furthermore, there are some specific inputs for each platform, including various forms of the data, such as clinical data and medical imaging which can improve the performance of the introduced approaches toward the best responses in practical applications.","",""
42,"M. Nassar, K. Salah, M. H. Rehman, D. Svetinovic","Blockchain for explainable and trustworthy artificial intelligence",2019,"","","","",130,"2022-07-13 09:32:18","","10.1002/widm.1340","","",,,,,42,14.00,11,4,3,"The increasing computational power and proliferation of big data are now empowering Artificial Intelligence (AI) to achieve massive adoption and applicability in many fields. The lack of explanation when it comes to the decisions made by today's AI algorithms is a major drawback in critical decision‐making systems. For example, deep learning does not offer control or reasoning over its internal processes or outputs. More importantly, current black‐box AI implementations are subject to bias and adversarial attacks that may poison the learning or the inference processes. Explainable AI (XAI) is a new trend of AI algorithms that provide explanations of their AI decisions. In this paper, we propose a framework for achieving a more trustworthy and XAI by leveraging features of blockchain, smart contracts, trusted oracles, and decentralized storage. We specify a framework for complex AI systems in which the decision outcomes are reached based on decentralized consensuses of multiple AI and XAI predictors. The paper discusses how our proposed framework can be utilized in key application areas with practical use cases.","",""
50,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello","Artificial Intelligence, Algorithmic Pricing, and Collusion",2020,"","","","",131,"2022-07-13 09:32:18","","10.1257/AER.20190623","","",,,,,50,25.00,13,4,2,"Increasingly, pricing algorithms are supplanting human decision making in real marketplaces. To inform the competition policy debate on the possible consequences of this development, we experiment with pricing algorithms powered by Artificial Intelligence (AI) in controlled environments (computer simulations), studying the interaction among a number of Q-learning algorithms in a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. In this setting the algorithms consistently learn to charge supra-competitive prices, without communicating with one another. The high prices are sustained by classical collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.","",""
3,"Yupeng Hu, Wenxin Kuang, Zheng Qin, Kenli Li, Jiliang Zhang, Yansong Gao, Wenjia Li, Keqin Li","Artificial Intelligence Security: Threats and Countermeasures",2021,"","","","",132,"2022-07-13 09:32:18","","10.1145/3487890","","",,,,,3,3.00,0,8,1,"In recent years, with rapid technological advancement in both computing hardware and algorithm, Artificial Intelligence (AI) has demonstrated significant advantage over human being in a wide range of fields, such as image recognition, education, autonomous vehicles, finance, and medical diagnosis. However, AI-based systems are generally vulnerable to various security threats throughout the whole process, ranging from the initial data collection and preparation to the training, inference, and final deployment. In an AI-based system, the data collection and pre-processing phase are vulnerable to sensor spoofing attacks and scaling attacks, respectively, while the training and inference phases of the model are subject to poisoning attacks and adversarial attacks, respectively. To address these severe security threats against the AI-based systems, in this article, we review the challenges and recent research advances for security issues in AI, so as to depict an overall blueprint for AI security. More specifically, we first take the lifecycle of an AI-based system as a guide to introduce the security threats that emerge at each stage, which is followed by a detailed summary for corresponding countermeasures. Finally, some of the future challenges and opportunities for the security issues in AI will also be discussed.","",""
96,"Eduardo H. B. Maia, L. Assis, Tiago Alves de Oliveira, Alisson Marques da Silva, A. Taranto","Structure-Based Virtual Screening: From Classical to Artificial Intelligence",2020,"","","","",133,"2022-07-13 09:32:18","","10.3389/fchem.2020.00343","","",,,,,96,48.00,19,5,2,"The drug development process is a major challenge in the pharmaceutical industry since it takes a substantial amount of time and money to move through all the phases of developing of a new drug. One extensively used method to minimize the cost and time for the drug development process is computer-aided drug design (CADD). CADD allows better focusing on experiments, which can reduce the time and cost involved in researching new drugs. In this context, structure-based virtual screening (SBVS) is robust and useful and is one of the most promising in silico techniques for drug design. SBVS attempts to predict the best interaction mode between two molecules to form a stable complex, and it uses scoring functions to estimate the force of non-covalent interactions between a ligand and molecular target. Thus, scoring functions are the main reason for the success or failure of SBVS software. Many software programs are used to perform SBVS, and since they use different algorithms, it is possible to obtain different results from different software using the same input. In the last decade, a new technique of SBVS called consensus virtual screening (CVS) has been used in some studies to increase the accuracy of SBVS and to reduce the false positives obtained in these experiments. An indispensable condition to be able to utilize SBVS is the availability of a 3D structure of the target protein. Some virtual databases, such as the Protein Data Bank, have been created to store the 3D structures of molecules. However, sometimes it is not possible to experimentally obtain the 3D structure. In this situation, the homology modeling methodology allows the prediction of the 3D structure of a protein from its amino acid sequence. This review presents an overview of the challenges involved in the use of CADD to perform SBVS, the areas where CADD tools support SBVS, a comparison between the most commonly used tools, and the techniques currently used in an attempt to reduce the time and cost in the drug development process. Finally, the final considerations demonstrate the importance of using SBVS in the drug development process.","",""
2,"Jeong Yeop Ryu, H. Chung, K. Choi","Potential role of artificial intelligence in craniofacial surgery",2021,"","","","",134,"2022-07-13 09:32:18","","10.7181/acfs.2021.00507","","",,,,,2,2.00,1,3,1,"The field of artificial intelligence (AI) is rapidly advancing, and AI models are increasingly applied in the medical field, especially in medical imaging, pathology, natural language processing, and biosignal analysis. On the basis of these advances, telemedicine, which allows people to receive medical services outside of hospitals or clinics, is also developing in many countries. The mechanisms of deep learning used in medical AI include convolutional neural networks, residual neural networks, and generative adversarial networks. Herein, we investigate the possibility of using these AI methods in the field of craniofacial surgery, with potential applications including craniofacial trauma, congenital anomalies, and cosmetic surgery.","",""
4,"Shivam Mehta, Y. Suhail, J. Nelson, M. Upadhyay","Artificial Intelligence for radiographic image analysis",2021,"","","","",135,"2022-07-13 09:32:18","","10.1053/J.SODO.2021.05.007","","",,,,,4,4.00,1,4,1,"Abstract Automated identification of landmarks on lateral cephalogram and cone-beam computed tomography (CBCT) scans can save time for the clinicians and act as a second set of eyes for analysis of radiographic images in diagnosis and treatment planning. Several machine-learning techniques have been utilized for this purpose with varying accuracies. However, high degree of variability in the clinical presentation of orthodontic patients, limitations of the algorithms, lack of labelled data, high compute power, etc. are some drawbacks that have limited robust clinical application of such techniques. In recent years, artificial neural networks like deep learning and more specifically deep neural networks are making significant inroads in the true adoption of this technology. YOLOv3 and Single Shot Multibox Detector are some of the deep learning algorithms that have shown promising results. This paper is a theoretical review of the evolution of these technologies and the current state of the art in orthodontic image analysis.","",""
0,"J. Curzon, T. A. Kosa, R. Akalu, K. El-Khatib","Privacy and Artificial Intelligence",2021,"","","","",136,"2022-07-13 09:32:18","","10.1109/TAI.2021.3088084","","",,,,,0,0.00,0,4,1,"Artificial intelligence is a rapidly developing field of research with many practical applications. Congruent to advances in technologies that enable big data, deep learning, and neural networks to train, learn, and predict, artificial intelligence creates new risks that are difficult to predict and manage. Such risks include economic turmoil, existential crises, and the dissolution of individual privacy. If unchecked, the capabilities of artificially intelligent systems could pose a fundamental threat to privacy in their operation or these systems may leak information under adversarial conditions. In this article, we survey the literature and provide various scenarios for the use of artificial intelligence, highlighting potential risks to privacy and offering various mitigating strategies. For the purpose of this research, a North American perspective of privacy is adopted. Impact statement—While an appreciation of the privacy risks associated with artificial intelligence is important, a thorough understanding of the assortment of different technologies that comprise artificial intelligence better prepares those implementing such systems in assessing privacy impacts. This can be achieved through the independent consideration of each constituent of an artificially intelligent system and its interactions. Under individual consideration, privacy-enhancing tools can be applied in a targeted manner to reduce the risk associated with specific components of an artificially intelligent system. A generalized North American approach to assess privacy risks in such systems is proposed that will retain applicability as the field of research evolves and can be adapted to account for various sociopolitical influences. With such an approach, privacy risks in artificial intelligent systems can be well understood, measured, and reduced.","",""
0,"A. Di Ieva, C. Russo, Abdulla Al Suman, Sidong Liu","IOTG-01. Computational Neurosurgery in Brain Tumors: A paradigm shift on the use of Artificial Intelligence and Connectomics in pre- and intra-operative imaging",2021,"","","","",137,"2022-07-13 09:32:18","","10.1093/neuonc/noab196.910","","",,,,,0,0.00,0,4,1,"  Computational Neurosurgery is a novel field where computational modeling and artificial intelligence (AI) are used to analyze diseases of neurosurgical interest. Our aim is to apply AI models to brain tumor (BT) images to a) automatically segment BTs on pre-operative MRI, b) predict the genetic subtype of glioma on intra- and post-operative histological specimens; and c) predict the extent of resection according to connectomics data. For the segmentation task, we used 510 BT images to train a deep learning (DL) model for automatic segmentation of the tumors’ edges and comparison of the AI-generated masks versus experts’ consensus (quantified by means of the dice score). For the histopathology task, we digitalized 266 hematoxylin/eosin slides of gliomas (including 130 IDH-wildtype and 136 IDH-mutant) and applied a DL architecture to predict the IDH genetic status, then validated by immunohistochemistry and genetic sequencing. The datasets were also augmented by generating synthetic glioma images by means of a Generative Adversarial Network methodology. The resection of 10 BTs was also customized according to connectomics data. In the segmentation experiment, we reached a dice score of ~0.9 (out of 1.0), while further demonstrating that only the T1, T1 after gadolinium, and FLAIR sequences are necessary for accurate automatic segmentation. In the histopathology task, we were able to predict the genetic status with accuracy between 76% and 95% using the DL model. The machine learning-based connectome analysis allowed us to perform safe supramaximal resection. We have shown the robustness of applying AI methodology or the automatic segmentation of BTs in MR imaging. Moreover, we have also shown that AI can be used to predict the genetic status, specifically, IDH, in histopathology images of gliomas. Our results support the use of AI in the clinical scenario for a fast and objective computerized characterization of patients affected by BTs.","",""
0,"S. Bhasin, S. Garg, F. Regazzoni","Special Section on Attacking and Protecting Artificial Intelligence",2021,"","","","",138,"2022-07-13 09:32:18","","10.1049/CIT2.12023","","",,,,,0,0.00,0,3,1,"Modern Artificial Intelligence (AI) systems largely rely on advanced algorithms, including machine learning techniques such as deep learning. The research community has invested significant efforts in understanding these algorithms, optimally tuning them, and improving their performance, but it has mostly neglected the security facet of the problem. Recent attacks and exploits demonstrated that machine learning‐based algorithms are susceptible to attacks targeting computer systems, including backdoors, hardware Trojans and fault attacks, but are also susceptible to a range of attacks specifically targeting them, such as adversarial input perturbations. Implementations of machine learning algorithms are often crucial proprietary assets for companies and thus are required to be protected. It follows that implementations of AI‐based algorithms are an attractive target for piracy and illegitimate use and, as such, they need to be protected as all other IPs. This is equally important for machine learning algorithms running on remote servers vulnerable to micro‐architectural exploits. Protecting AI algorithms from all these attacks is not a trivial task. While vast research in hardware and software security have established several sound countermeasures, the specificity of the algorithms used in AI could make such countermeasures ineffective (or simply inapplicable), given the complex and resource intensive nature of the algorithms. The task of protection will become even more difficult in the near future, given the trend where part of the intelligence will be deployed directly into resource constrained cyber‐physical systems and IoT devices. AI models themselves should be protected against illegitimate and unauthorized use and distribution. Because of this, IP protection techniques such as watermarking, fingerprinting and attestation have been proposed, but, especially the last two, should be studied more in depth. To address all these security challenges, two actions are needed. First, we need a complete understanding of the attackers' capabilities. Second, novel and lightweight approaches for protecting AI algorithms, given the distributed level of intelligence, should be conceived and developed, including (but not limited to) obfuscation, finger‐printing, homomorphic encryption, and a new set of countermeasures to protect AI algorithms from adversarial input, backdooring and physical attacks. This Special Section covers problems related to attacking and protecting implementations of AI algorithms, and the use of AI to improve state‐of‐the‐art attacks such as physical attacks. It consists of three articles which are selected for publication after multiple rounds of peer review and scrutiny. An overview of these articles is discussed in the following. The first article reports different types of adversarial attacks, considering various threat models, followed by a discussion on the efficiency and challenges of state‐of‐the‐art countermeasures against them. It also provides a taxonomy for adversarial learning which can help future research to correctly categorize discovered vulnerabilities and plan protection mechanisms accordingly. The article concludes discussing open problems that can trigger further research on the topic. The second article takes a step towards disseminating knowledge about the widely popular and critical threat of side‐ channel attacks on neural networks. This survey considers and categorizes the most relevant threat models and corresponding attacks with different objectives including recovery of hyper‐ parameters, secret weights and inputs. The article differentiates between types of side‐channel attacks like physical, local or remote to highlight the applicability of various attacks and concludes with a discussion of countermeasures. The third article surveys AI model ownership protection techniques, the majority of them being based on watermarking, reporting advantages and disadvantage of them and highlighting possible research directions. The authors identified that, to date, the most studied technique is watermarking, that has been proposed in white box and black box settings. The articles also survey existing attacks aiming at removing or making ineffective IP protection techniques, and identify fingerprinting and attestation as two approaches are not yet studied in depth. Overall, the articles accepted cover a wide spectrum of problem providing readers with a perspective on the underlying","",""
0,"Xiaohong W. Gao, B. Braden","Artificial intelligence in endoscopy: The challenges and future directions",2021,"","","","",139,"2022-07-13 09:32:18","","10.37126/aige.v2.i4.117","","",,,,,0,0.00,0,2,1,"Artificial intelligence based approaches, in particular deep learning, have achieved state-of-the-art performance in medical fields with increasing number of software systems being approved by both Europe and United States. This paper reviews their applications to early detection of oesophageal cancers with a focus on their advantages and pitfalls. The paper concludes with future recommendations towards the development of a real-time, clinical implementable, interpretable and robust diagnosis support systems.","",""
29,"Grayson W. Armstrong, A. Lorch","A(eye): A Review of Current Applications of Artificial Intelligence and Machine Learning in Ophthalmology",2019,"","","","",140,"2022-07-13 09:32:18","","10.1097/IIO.0000000000000298","","",,,,,29,9.67,15,2,3,"Artificial intelligence (AI) is a branch of computer science that aims to enable computers to perform human-like tasks. Although AI is a broad discipline, machine learning is a specific branch of AI that uses computer algorithms capable of “learning” through the simulation of human intelligence. Machine learning algorithms have been applied to the medical field since the 1970s,1 and since that time have proven useful in computerassisted diagnosis, screening, and prognostication of disease.2–7 Ophthalmology is uniquely capable of capitalizing on the promise of AI. Ophthalmologists, during routine clinical encounters, generate robust data sources capable of supporting machine learning algorithms including multimodal ophthalmic images and quantifiable metrics such as visual acuity (VA), intraocular pressure, and cup to disk ratio. To date, AI techniques have been applied to ophthalmology to screen for and diagnose diseases, such as diabetic retinopathy (DR), age-related macular degeneration (AMD), macular edema (ME), glaucoma, keratoconus, postlaserassisted in situ keratomileusis corneal ectasia, retinopathy of prematurity (ROP), and cataracts, as well as predict the prognosis of various ophthalmic diseases. Advances in ophthalmology-specific AI stand to increase patient access to clinical screening and diagnosis as well as decrease health care costs, especially when applied to high-risk populations, low-resource communities, or when combined with telemedicine initiatives. This review provides an introduction to AI andmachine learning, as well as an overview of current applications in the field of ophthalmology.","",""
29,"Ajay-Vikram Singh, Daniel Rosenkranz, M. Ansari, Rishabh Singh, Anurag Kanase, Shubham Pratap Singh, Blair Johnston, J. Tentschert, P. Laux, A. Luch","Artificial Intelligence and Machine Learning Empower Advanced Biomedical Material Design to Toxicity Prediction",2020,"","","","",141,"2022-07-13 09:32:18","","10.1002/aisy.202000084","","",,,,,29,14.50,3,10,2,"Materials at the nanoscale exhibit specific physicochemical interactions with their environment. Therefore, evaluating their toxic potential is a primary requirement for regulatory purposes and for the safer development of nanomedicines. In this review, to aid the understanding of nano–bio interactions from environmental and health and safety perspectives, the potential, reality, challenges, and future advances that artificial intelligence (AI) and machine learning (ML) present are described. Herein, AI and ML algorithms that assist in the reporting of the minimum information required for biomaterial characterization and aid in the development and establishment of standard operating procedures are focused. ML tools and ab initio simulations adopted to improve the reproducibility of data for robust quantitative comparisons and to facilitate in silico modeling and meta‐analyses leading to a substantial contribution to safe‐by‐design development in nanotoxicology/nanomedicine are mainly focused. In addition, future opportunities and challenges in the application of ML in nanoinformatics, which is particularly well‐suited for the clinical translation of nanotherapeutics, are highlighted. This comprehensive review is believed that it will promote an unprecedented involvement of AI research in improvements in the field of nanotoxicology and nanomedicine.","",""
20,"L. McCoy, Sujay Nagaraj, F. Morgado, V. Harish, Sunit Das, L. Celi","What do medical students actually need to know about artificial intelligence?",2020,"","","","",142,"2022-07-13 09:32:18","","10.1038/s41746-020-0294-7","","",,,,,20,10.00,3,6,2,"","",""
28,"H. Alami, L. Rivard, P. Lehoux, S. Hoffman, Stephanie B. M. Cadeddu, Mathilde Savoldelli, M. A. Samri, M. A. Ag Ahmed, R. Fleet, J. Fortin","Artificial intelligence in health care: laying the Foundation for Responsible, sustainable, and inclusive innovation in low- and middle-income countries",2020,"","","","",143,"2022-07-13 09:32:18","","10.1186/s12992-020-00584-1","","",,,,,28,14.00,3,10,2,"","",""
44,"Chiara Longoni, Luca Cian","Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect",2020,"","","","",144,"2022-07-13 09:32:18","","10.1177/0022242920957347","","",,,,,44,22.00,22,2,2,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).","",""
25,"D. Schiff","Out of the laboratory and into the classroom: the future of artificial intelligence in education",2020,"","","","",145,"2022-07-13 09:32:18","","10.1007/s00146-020-01033-8","","",,,,,25,12.50,25,1,2,"","",""
23,"M. Rohaim, E. Clayton, I. Sahin, J. Vilela, M. Khalifa, M. Al-Natour, M. Bayoumi, A. Poirier, M. Branavan, M. Tharmakulasingam, N. S. Chaudhry, R. Sodi, A. Brown, P. Burkhart, W. Hacking, J. Botham, J. Boyce, H. Wilkinson, Craig Williams, Jayde Whittingham-Dowd, E. Shaw, Matt D. Hodges, L. Butler, M. Bates, R. L. La Ragione, W. Balachandran, A. Fernando, M. Munir","Artificial Intelligence-Assisted Loop Mediated Isothermal Amplification (AI-LAMP) for Rapid Detection of SARS-CoV-2",2020,"","","","",146,"2022-07-13 09:32:18","","10.3390/v12090972","","",,,,,23,11.50,2,28,2,"Until vaccines and effective therapeutics become available, the practical solution to transit safely out of the current coronavirus disease 19 (CoVID-19) lockdown may include the implementation of an effective testing, tracing and tracking system. However, this requires a reliable and clinically validated diagnostic platform for the sensitive and specific identification of SARS-CoV-2. Here, we report on the development of a de novo, high-resolution and comparative genomics guided reverse-transcribed loop-mediated isothermal amplification (LAMP) assay. To further enhance the assay performance and to remove any subjectivity associated with operator interpretation of results, we engineered a novel hand-held smart diagnostic device. The robust diagnostic device was further furnished with automated image acquisition and processing algorithms and the collated data was processed through artificial intelligence (AI) pipelines to further reduce the assay run time and the subjectivity of the colorimetric LAMP detection. This advanced AI algorithm-implemented LAMP (ai-LAMP) assay, targeting the RNA-dependent RNA polymerase gene, showed high analytical sensitivity and specificity for SARS-CoV-2. A total of ~200 coronavirus disease (CoVID-19)-suspected NHS patient samples were tested using the platform and it was shown to be reliable, highly specific and significantly more sensitive than the current gold standard qRT-PCR. Therefore, this system could provide an efficient and cost-effective platform to detect SARS-CoV-2 in resource-limited laboratories.","",""
1,"Sandhya Aneja, Nagender Aneja, Pg Emeroylariffion Abas, A. G. Naim","IAES International Journal of Artificial Intelligence (IJ-AI)",2021,"","","","",147,"2022-07-13 09:32:18","","","","",,,,,1,1.00,0,4,1,"Received Aug 22, 2021 Revised May 20, 2022 Accepted Jun 6, 2022 Despite substantial advances in network architecture performance, the susceptibility of adversarial attacks makes deep learning challenging to implement in safety-critical applications. This paper proposes a data-centric approach to addressing this problem. A nonlocal denoising method with different luminance values has been used to generate adversarial examples from the Modified National Institute of Standards and Technology database (MNIST) and Canadian Institute for Advanced Research (CIFAR-10) data sets. Under perturbation, the method provided absolute accuracy improvements of up to 9.3% in the MNIST data set and 13% in the CIFAR10 data set. Training using transformed images with higher luminance values increases the robustness of the classifier. We have shown that transfer learning is disadvantageous for adversarial machine learning. The results indicate that simple adversarial examples can improve resilience and make deep learning easier to apply in various applications.","",""
0,"Fadheela Hussain, Riadh Ksantini, M. Hammad","A Review of Malicious Altering Healthcare Imagery using Artificial Intelligence",2021,"","","","",148,"2022-07-13 09:32:18","","10.1109/3ICT53449.2021.9582068","","",,,,,0,0.00,0,3,1,"During the second half of 2020, healthcare is and has been the number one target for cybercrime, enormous amount of cyberattacks on hospitals and health systems increased, and specialists trust there are more to come. Attackers who can get the way to reach the electronic health record would exploit it and will use it for their own interest like deal or vend it on the underground economy, hostage the systems and the sensitive data, that has a significant impact on operations. This review tried to analyze how cyber attacker employ Generative Adversarial Networks (GANs) to alter the evidences of patient's medical conditions from image scans and reports. Cyber attacker has different purposes in order to obstruct a political applicant, lockup investigations, obligate insurance scam, execute an act of violence, or even commit homicide. Numerous correlated works constructed on gan in medical images practices had been reviews in the period between 2000 to 2021. Many papers showed how hospital system, physicians and radiology's specialists and the most recent researches showed an extremely exposed to different types of intrusion gan attacks.","",""
0,"Igor Kunjavskij","Cybersecurity in Machine Learning and Artificial Intelligence for Self Driving Vehicles A proposal for an unified and open source test framework",2021,"","","","",149,"2022-07-13 09:32:18","","","","",,,,,0,0.00,0,1,1,"This report summarizes the key methodologies applicable for attacks on Convolutional Neural Networks deployed in self driving vehicles. Furthemore, the need for an open source framework specifically designed to test such networks for robustness against these attacks is demonstrated. This need stems from the fact that solutions available so far only cover very generic scenarios of adversarial attacks. Hence these tools are not suited to specifically secure Convolutional Neural Networks deployed in in self driving vehicles. All in all the development of such a framework would benefit all parties that are involved in the market of autonomously driving vehicles.","",""
27,"Omar Alshorman, Muhammad Irfan, N. Saad, D. Zhen, Noman Haider, A. Głowacz, Ahmad M. Alshorman","A Review of Artificial Intelligence Methods for Condition Monitoring and Fault Diagnosis of Rolling Element Bearings for Induction Motor",2020,"","","","",150,"2022-07-13 09:32:18","","10.1155/2020/8843759","","",,,,,27,13.50,4,7,2,"The fault detection and diagnosis (FDD) along with condition monitoring (CM) and of rotating machinery (RM) have critical importance for early diagnosis to prevent severe damage of infrastructure in industrial environments. Importantly, valuable industrial equipment needs continuous monitoring to enhance the safety, reliability, and availability and to decrease the cost of maintenance of modern industrial systems and applications. However, induction motor (IM) has been extensively used in several industrial processes because it is cheap, reliable, and robust. Rolling bearings are considered to be the main component of IM. Undoubtedly, any failure of this basic component can lead to a serious breakdown of IM and for whole industrial system. Thus, many current methods based on different techniques are employed as a fault prognosis and diagnosis of rolling elements bearing of IM. Moreover, these techniques include signal/image processing, intelligent diagnostics, data fusion, data mining, and expert systems for time and frequency as well as time-frequency domains. Artificial intelligence (AI) techniques have proven their significance in every field of digital technology. Industrial machines, automation, and processes are the net frontiers of AI adaptation. There are quite developed literatures that have been approaching the issues using signals and data processing techniques. However, the key contribution of this work is to present an extensive review of CM and FDD of the IM, especially for rolling elements bearings, based on artificial intelligent (AI) methods. This study highlights the advantages and performance limitations of each method. Finally, challenges and future trends are also highlighted.","",""
0,"Lucas Mendes Lima, Victor Calebe Cavalcante, Mariana Guimarães de Sousa, Cláudio Afonso Fleury, D. Oliveira, Eduardo Noronha de Andrade Freitas","Artificial Intelligence in Support of Welfare Monitoring of Dairy Cattle: A Systematic Literature Review",2021,"","","","",151,"2022-07-13 09:32:18","","10.1109/CSCI54926.2021.00324","","",,,,,0,0.00,0,6,1,"Context: Although agribusiness corresponded to more than 20% of Brazil’s Gross Domestic Product (GDP), most livestock is under manual control and manual monitoring. Additionally, alternative technologies are either uncomfortable and stressful, or expensive. Now, despite the great scientific advances in the area, there is still a pressing need for an automated robust, inexpensive and (sub)optimal technology to monitor animal behavior in a cost-effective, contact-less and stress-free fashion. Overall, this niche can leverage the benefits of Deep Learning schemes.Objective: This review aims to provide a systematic overview of most current projects in the area of comfort monitoring dairy cattle, as well as their corresponding image recognition-based techniques and technologies.Methods: First, a systematic review planning was carried out, and objectives, research questions, search strings, among others, were defined. Subsequently,a broad survey was conducted to extract, analyze and compile the data, to generate a easy-to-read visual source of information (tables and graphics).Results: Information was extracted from the reviewed papers. Among this data collected from the papers are techniques utilized, target behaviors, cow bodyparts identified in visual computational, besides their paper source font, the publication date, and localization. For example, the papers present are mostly recent. China has had a larger number of relevant papers in the area. The back was the body region most analyzed by the papers and the behaviors most analyzed were body condition score, lameness, cow’s body position and feeding/drinking behavior. Among the methods used is RCNN Inception V3 with the best accuracy for cow’s back region.Conclusion: The aim of this work is to present some of the papers that are being carried out in the area of dairy cow behavior monitoring, using techniques of Artifical Intelligence. It is expected that the information collected and presented in the present systematic review paper contribute to the future researches and projects of the area and the application of new techniques.","",""
0,"Keeley A. Crockett, Edwin Colyer, A. Latham","The Ethical Landscape of Data and Artificial Intelligence: Citizen Perspectives",2021,"","","","",152,"2022-07-13 09:32:18","","10.1109/SSCI50451.2021.9660153","","",,,,,0,0.00,0,3,1,"Globally, there is growing acknowledgement that those involved in the development and deployment of AI products and services should act responsibly and conduct their work within robust ethical frameworks. Many of the ethical guidelines now published highlight a requirement for citizens to have greater voice and involvement in this process and to hold actors to account regarding compliance and the impacts of their AI innovations. For citizens to participate in co-creation activities they need to be representative of the diverse communities of society and have an appropriate level of understanding of basic AI concepts. This paper presents the preliminary results of a longitudinal survey designed to capture citizen perspectives of the ethical landscape of data and AI. Forty participants were asked to participate in a survey and results were analyzed based on gender, age range and educational attainment. Results have shown that participant perception of AI, trust, bias and fairness is different but related to specific AI applications, and the context in which is applied. Citizens also are also very receptive to undertaking free courses/workshops on a wide range of AI concepts, ranging from family workshops to work-based training.","",""
43,"Stuart J. Russell, Thomas G. Dietterich, Eric Horvitz, B. Selman, F. Rossi, D. Hassabis, S. Legg, Mustafa Suleyman, D. George, D. Phoenix","Letter to the Editor: Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter",2015,"","","","",153,"2022-07-13 09:32:18","","10.1609/aimag.v36i4.2621","","",,,,,43,6.14,4,10,7,"Artificial intelligence (AI) research has explored a variety of problems and approaches since its inception, but for the last 20 years or so has been focused on the problems surrounding the construction of intelligent agents — systems that perceive and act in some environment. In this context, ""intelligence"" is related to statistical and economic notions of rationality — colloquially, the ability to make good decisions, plans, or inferences. The adoption of probabilistic and decision-theoretic representations and statistical learning methods has led to a large degree of integration and cross-fertilization among AI, machine learning, statistics, control theory, neuroscience, and other fields. The establishment of shared theoretical frameworks, combined with the availability of data and processing power, has yielded remarkable successes in various component tasks such as speech recognition, image classification, autonomous vehicles, machine translation, legged locomotion, and question-answering systems. As capabilities in these areas and others cross the threshold from laboratory research to economically valuable technologies, a virtuous cycle takes hold whereby even small improvements in performance are worth large sums of money, prompting greater investments in research. There is now a broad consensus that AI research is progressing steadily, and that its impact on society is likely to increase. The potential benefits are huge, since everything that civilization has to offer is a product of human intelligence; we cannot predict what we might achieve when this intelligence is magnified by the tools AI may provide, but the eradication of disease and poverty are not unfathomable. Because of the great potential of AI, it is important to research how to reap its benefits while avoiding potential pitfalls. The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the AAAI 2008–09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do. The attached research priorities document [see page X] gives many examples of such research directions that can help maximize the societal benefit of AI. This research is by necessity interdisciplinary, because it involves both society and AI. It ranges from economics, law and philosophy to computer security, formal methods and, of course, various branches of AI itself. In summary, we believe that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today.","",""
18,"Martin Ragot, Nicolas Martin, Salomé Cojean","AI-generated vs. Human Artworks. A Perception Bias Towards Artificial Intelligence?",2020,"","","","",154,"2022-07-13 09:32:18","","10.1145/3334480.3382892","","",,,,,18,9.00,6,3,2,"Via generative adversarial networks (GANs), artificial intelligence (AI) has influenced many areas, especially the artistic field, as symbol of a human task. In human-computer interaction (HCI) studies, perception biases against AI, machines, or computers are generally cited. However, experimental evidence is still lacking. This paper presents a wide-scale experiment in which 565 participants are asked to evaluate paintings (which were created by humans or AI) on four dimensions: liking, perceived beauty, novelty, and meaning. A priming effect is evaluated using two between-subject conditions: Artworks presented as created by an AI, and artworks presented as created by a human artist. Finally, the paintings perceived as being drawn by human are evaluated significantly more highly than those perceived as being made by AI. Thus, using such a methodology and sample in an unprecedented way, the results show a negative bias of perception towards AI and a preference bias towards human systems.","",""
18,"Ahmed Gowida, Salaheldin Elkatatny, Saad F. K. Al-Afnan, A. Abdulraheem","New Computational Artificial Intelligence Models for Generating Synthetic Formation Bulk Density Logs While Drilling",2020,"","","","",155,"2022-07-13 09:32:18","","10.3390/su12020686","","",,,,,18,9.00,5,4,2,"Synthetic well log generation using artificial intelligence tools is a robust solution for situations in which logging data are not available or are partially lost. Formation bulk density (RHOB) logging data greatly assist in identifying downhole formations. These data are measured in the field while drilling by using a density log tool in the form of either a logging while drilling (LWD) technique or (more often) by wireline logging after the formations are drilled. This is due to operational limitations during the drilling process. Therefore, the objective of this study was to develop a predictive tool for estimating RHOB while drilling using an adaptive network-based fuzzy interference system (ANFIS), functional network (FN), and support vector machine (SVM). The proposed model uses the mechanical drilling constraints as feeding input parameters, and the conventional RHOB log data as an output parameter. These mechanical drilling parameters are usually measured while drilling, and their responses vary with different formations. A dataset of 2400 actual datapoints, obtained from a horizontal well in the Middle East, were used to build the proposed models. The obtained dataset was divided into a 70/30 ratio for model training and testing, respectively. The optimized ANFIS-based model outperformed the FN- and SVM-based models with a correlation coefficient (R) of 0.93, and average absolute percentage error (AAPE) of 0.81% between the predicted and measured RHOB values. These results demonstrate the reliability of the developed ANFIS model for predicting RHOB while drilling, based on the mechanical drilling parameters. Subsequently, the ANFIS-based model was validated using unseen data from another well within the same field. The validation process yielded an AAPE of 0.97% between the predicted and actual RHOB values, which confirmed the robustness of the developed model as an effective predictive tool for RHOB.","",""
5,"G. Olague, Gerardo Ibarra-Vázquez, Mariana Chan-Ley, Cesar Puente, C. Soubervielle-Montalvo, Axel Martinez","A Deep Genetic Programming based Methodology for Art Media Classification Robust to Adversarial Perturbations",2020,"","","","",156,"2022-07-13 09:32:18","","10.1007/978-3-030-64556-4_6","","",,,,,5,2.50,1,6,2,"","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",157,"2022-07-13 09:32:18","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
14,"G. Coskuner, Majeed S Jassim, M. Zontul, Seda Karateke","Application of artificial intelligence neural network modeling to predict the generation of domestic, commercial and construction wastes",2020,"","","","",158,"2022-07-13 09:32:18","","10.1177/0734242X20935181","","",,,,,14,7.00,4,4,2,"Reliable prediction of municipal solid waste (MSW) generation rates is a significant element of planning and implementation of sustainable solid waste management strategies. In this study, the multi-layer perceptron artificial neural network (MLP-ANN) is applied to verify the prediction of annual generation rates of domestic, commercial and construction and demolition (C&D) wastes from the year 1997 to 2016 in Askar Landfill site in the Kingdom of Bahrain. The proposed robust predictive models incorporated selected explanatory variables to reflect the influence of social, demographical, economic, geographical and touristic factors upon waste generation rates (WGRs). The Mean Squared Error (MSE) and coefficient of determination (R2) are used as performance indicators to evaluate effectiveness of the developed models. MLP-ANN models exhibited strong accuracy in predictions with high R2 and low MSE values. The R2 values for domestic, commercial and C&D wastes are 0.95, 0.99 and 0.91, respectively. Our results show that the developed MLP-ANN models are effective for the prediction of WGRs from different sources and could be considered as a cost-effective approach for planning integrated MSW management systems.","",""
11,"K. Mudgal, Neelanjan Das","The ethical adoption of artificial intelligence in radiology",2019,"","","","",159,"2022-07-13 09:32:18","","10.1259/bjro.20190020","","",,,,,11,3.67,6,2,3,"Artificial intelligence (AI) is rapidly transforming healthcare—with radiology at the pioneering forefront. To be trustfully adopted, AI needs to be lawful, ethical and robust. This article covers the different aspects of a safe and sustainable deployment of AI in radiology during: training, integration and regulation. For training, data must be appropriately valued, and deals with AI companies must be centralized. Companies must clearly define anonymization and consent, and patients must be well-informed about their data usage. Data fed into algorithms must be made AI-ready by refining, purification, digitization and centralization. Finally, data must represent various demographics. AI needs to be safely integrated with radiologists-in-the-loop: guiding forming concepts of AI solutions and supervising training and feedback. To be well-regulated, AI systems must be approved by a health authority and agreements must be made upon liability for errors, roles of supervised and unsupervised AI and fair workforce distribution (between AI and radiologists), with a renewal of policy at regular intervals. Any errors made must have a root-cause analysis, with outcomes fedback to companies to close the loop—thus enabling a dynamic best prediction system. In the distant future, AI may act autonomously with little human supervision. Ethical training and integration can ensure a ""transparent"" technology that will allow insight: helping us reflect on our current understanding of imaging interpretation and fill knowledge gaps, eventually moulding radiological practice. This article proposes recommendations for ethical practise that can guide a nationalized framework to build a sustainable and transparent system.","",""
0,"Yusen Xie, Ting Sun, Xinglong Cui, Shuixin Deng, Lei Deng, Baohua Chen","Fast-robust book information extraction system for automated intelligence library",2021,"","","","",160,"2022-07-13 09:32:18","","10.1109/AIID51893.2021.9456499","","",,,,,0,0.00,0,6,1,"At present, in the large-scale book management scene, book sorting, daily maintenance and book retrieval are very common, but the book information is complicated and the efficiency of relying on manual management is extremely poor. Although there have been many self-service book systems based on optics or vision, they are mostly based on traditional computer vision algorithms such as boundary extraction. Due to the fact that there are more artificial experience thresholds, some shortcomings such as low detection accuracy, poor robustness, and inability to systematically deploy on a large scale, which lack of insufficient intelligence. Therefore, we proposed a book information extraction algorithm based on object detection and optical character recognition (OCR) that is suitable for multiple book information recognition, multiple book image angles and multiple book postures. It can be applied to scenes such as book sorting, bookshelf management and book retrieval. The system we designed includes the classification of book covers and back covers, the classification of books upright and inverted, the detection of book pages side and spine side, the recognition of book pricing. In terms of accuracy, the classification accuracy of the front cover and the back cover is 99.9%, the upright classification accuracy of book front covers is 98.8%, the back cover reaches 99.9%, the accuracy of book price recognition get 94.5%, and the book spine/page side detection mAP reaches 99.6%; in terms of detection speed, Yolov5 detection model was improved and the statistical-based pre-pruning strategy was adopted, support by our algorithm the system reaches 2.09 FPS in book price recognition, which improves the detection speed to meet actual needs.","",""
2,"Mikel Arbiza Goenaga","A critique of contemporary artificial intelligence art: Who is Edmond de Belamy?",2020,"","","","",161,"2022-07-13 09:32:18","","10.1387/ausart.21490","","",,,,,2,1.00,2,1,2,"Edmond de Belamy is a 2018 painting made by french collective Obvious, created using a type of Artificial Intelligence algorithms called Generative Adversarial Networks, which was sold at Christie's auction house in New York for $432,500. This historic event the so-called auction of the ""first artwork made by an AI"" raises 3 interesting questions about authorship, originality, and the arts as a space for scientific inquiry. While some think that the current deployment of Machine Learning algorithms and Artificial Intelligence techniques that we are seeing in the art world today may be seen as the ultimate ""Gesamtkunstwerk"" or total artwork, other points of view express that not only we need this type of cultural artifacts as a critique of industrialized use of Artificial Intelligence, but also a strict criteria has to be delimited in order to review contemporary art made with Machine Learning techniques.","",""
2,"Tyler J. Shipp, Daniel Clouse, Michael J. De Lucia, Metin B. Ahiskali, Kai Steverson, Jonathan Mullin, Nathaniel D. Bastian","Advancing the Research and Development of Assured Artificial Intelligence and Machine Learning Capabilities",2020,"","","","",162,"2022-07-13 09:32:18","","","","",,,,,2,1.00,0,7,2,"Artificial intelligence (AI) and machine learning (ML) have become increasingly vital in the development of novel defense and intelligence capabilities across all domains of warfare. An adversarial AI (A2I) and adversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is imperative that AI/ML models can defend against these attacks. A2I/AML defenses will help provide the necessary assurance of these advanced capabilities that use AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research and development of assured AI/ML capabilities via new A2I/AML defenses by fostering a collaborative environment across the U.S. Department of Defense and U.S. Intelligence Community. The A2IWG aims to identify specific challenges that it can help solve or address more directly, with initial focus on three topics: AI Trusted Robustness, AI System Security, and AI/ML Architecture Vulnerabilities.","",""
8,"I. Wiafe, F. N. Koranteng, Emmanuel Nyarko Obeng, Nana Assyne, Abigail Wiafe, S. Gulliver","Artificial Intelligence for Cybersecurity: A Systematic Mapping of Literature",2020,"","","","",163,"2022-07-13 09:32:18","","10.1109/ACCESS.2020.3013145","","",,,,,8,4.00,1,6,2,"Due to the ever-increasing complexities in cybercrimes, there is the need for cybersecurity methods to be more robust and intelligent. This will make defense mechanisms to be capable of making real-time decisions that can effectively respond to sophisticated attacks. To support this, both researchers and practitioners need to be familiar with current methods of ensuring cybersecurity (CyberSec). In particular, the use of artificial intelligence for combating cybercrimes. However, there is lack of summaries on artificial intelligent methods for combating cybercrimes. To address this knowledge gap, this study sampled 131 articles from two main scholarly databases (ACM digital library and IEEE Xplore). Using a systematic mapping, the articles were analyzed using quantitative and qualitative methods. It was observed that artificial intelligent methods have made remarkable contributions to combating cybercrimes with significant improvement in intrusion detection systems. It was also observed that there is a reduction in computational complexity, model training times and false alarms. However, there is a significant skewness within the domain. Most studies have focused on intrusion detection and prevention systems, and the most dominant technique used was support vector machines. The findings also revealed that majority of the studies were published in two journal outlets. It is therefore suggested that to enhance research in artificial intelligence for CyberSec, researchers need to adopt newer techniques and also publish in other related outlets.","",""
5,"Cathy O'Neil, H. Gunn","Near-Term Artificial Intelligence and the Ethical Matrix",2020,"","","","",164,"2022-07-13 09:32:18","","10.1093/oso/9780190905033.003.0009","","",,,,,5,2.50,3,2,2,"This chapter takes up the issue of near-term artificial intelligence, or the algorithms that are already in place in a variety of public and private sectors, guiding decisions from advertising and to credit ratings to sentencing in the justice system. There is a pressing need to recognize and evaluate the ways that structural racism, sexism, classism, and ableism may be embedded in and amplified by these systems. The chapter proposes a framework for ethical analysis that can be used to facilitate more robust ethical reflection in AI development and implementation. It presents an ethical matrix that incorporates the language of data science as a tool that data scientists can build themselves in order to integrate ethical analysis into the design process, addressing the need for immediate analysis and accountability over the design and deployment of near-term AI.","",""
496,"Christopher J. Kelly, A. Karthikesalingam, Mustafa Suleyman, Greg Corrado, Dominic King","Key challenges for delivering clinical impact with artificial intelligence",2019,"","","","",165,"2022-07-13 09:32:18","","10.1186/s12916-019-1426-2","","",,,,,496,165.33,99,5,3,"","",""
5,"Lindong Zhao, Xuguang Zhang, Jianxin Chen, Liang Zhou","Physical Layer Security in the Age of Artificial Intelligence and Edge Computing",2020,"","","","",166,"2022-07-13 09:32:18","","10.1109/MWC.001.2000044","","",,,,,5,2.50,1,4,2,"Physical layer security (PLS) is emerging as an attractive security paradigm to complement or even replace complex cryptography. Although information-theoretical transmission optimization and physical-layer key generation have been thoroughly researched, there still exist many critical issues to be tackled before PLS is extensively applied. In this article, we investigate the prospect for exploiting artificial intelligent (AI) and edge computing (EC) to facilitate the practical application of PLS. First, two outstanding challenges facing PLS designers are identified by analyzing the fundamental assumptions regarding eavesdroppers and wireless channels. Accordingly, two enhancement schemes are designed by reaping the benefits offered by AI and EC. Specifically, a novel secure resource management framework is developed to enhance the adaptability of an optimization-based PLS paradigm, and a robust physical-layer key generation method is designed to cope with reciprocity failure. Finally, we discuss a coordinated defense architecture with multi-layer, multi-domain, and multi-dimension, which is expected to exploit the compatibility and complementarity of the existing PLS methods.","",""
0,"Katanosh Morovat, B. Panda","A Survey of Artificial Intelligence in Cybersecurity",2020,"","","","",167,"2022-07-13 09:32:18","","10.1109/CSCI51800.2020.00026","","",,,,,0,0.00,0,2,2,"During the last decades, not only the number of cyberattacks have increased significantly, they have also become more sophisticated. Hence designing a cyber-resilient approach is of paramount importance. Traditional security methods are not adequate to prevent data breaches in case of cyberattacks. Cybercriminals have learned how to use new techniques and robust tools to hack, attack, and breach data. Fortunately, Artificial Intelligence (AI) technologies have been introduced into cyberspace to construct smart models for defending systems from attacks. Since AI technologies can rapidly evolve to address complex situations, they can be used as fundamental tools in the field of cybersecurity. Al-based techniques can provide efficient and powerful cyber defense tools to recognize malware attacks, network intrusions, phishing and spam emails, and data breaches, to name a few, and to alert security incidents when they occur. In this paper, we review the impact of AI in cybersecurity and summarize existing research in terms of benefits of AI in cybersecurity.","",""
63,"Rohan Gupta, Devesh Srivastava, Mehar Sahu, Swati Tiwari, R. K. Ambasta, Pravir Kumar","Artificial intelligence to deep learning: machine intelligence approach for drug discovery",2021,"","","","",168,"2022-07-13 09:32:18","","10.1007/s11030-021-10217-3","","",,,,,63,63.00,11,6,1,"","",""
54,"G. Collins, P. Dhiman, Constanza L. Andaur Navarro, Jie Ma, L. Hooft, J. Reitsma, P. Logullo, Andrew Beam, Lily Peng, B. van Calster, M. van Smeden, R. Riley, K. Moons","Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence",2021,"","","","",169,"2022-07-13 09:32:18","","10.1136/bmjopen-2020-048008","","",,,,,54,54.00,5,13,1,"Introduction The Transparent Reporting of a multivariable prediction model of Individual Prognosis Or Diagnosis (TRIPOD) statement and the Prediction model Risk Of Bias ASsessment Tool (PROBAST) were both published to improve the reporting and critical appraisal of prediction model studies for diagnosis and prognosis. This paper describes the processes and methods that will be used to develop an extension to the TRIPOD statement (TRIPOD-artificial intelligence, AI) and the PROBAST (PROBAST-AI) tool for prediction model studies that applied machine learning techniques. Methods and analysis TRIPOD-AI and PROBAST-AI will be developed following published guidance from the EQUATOR Network, and will comprise five stages. Stage 1 will comprise two systematic reviews (across all medical fields and specifically in oncology) to examine the quality of reporting in published machine-learning-based prediction model studies. In stage 2, we will consult a diverse group of key stakeholders using a Delphi process to identify items to be considered for inclusion in TRIPOD-AI and PROBAST-AI. Stage 3 will be virtual consensus meetings to consolidate and prioritise key items to be included in TRIPOD-AI and PROBAST-AI. Stage 4 will involve developing the TRIPOD-AI checklist and the PROBAST-AI tool, and writing the accompanying explanation and elaboration papers. In the final stage, stage 5, we will disseminate TRIPOD-AI and PROBAST-AI via journals, conferences, blogs, websites (including TRIPOD, PROBAST and EQUATOR Network) and social media. TRIPOD-AI will provide researchers working on prediction model studies based on machine learning with a reporting guideline that can help them report key details that readers need to evaluate the study quality and interpret its findings, potentially reducing research waste. We anticipate PROBAST-AI will help researchers, clinicians, systematic reviewers and policymakers critically appraise the design, conduct and analysis of machine learning based prediction model studies, with a robust standardised tool for bias evaluation. Ethics and dissemination Ethical approval has been granted by the Central University Research Ethics Committee, University of Oxford on 10-December-2020 (R73034/RE001). Findings from this study will be disseminated through peer-review publications. PROSPERO registration number CRD42019140361 and CRD42019161764.","",""
21,"Adrien Bécue, Isabel Praça, J. Gama","Artificial intelligence, cyber-threats and Industry 4.0: challenges and opportunities",2021,"","","","",170,"2022-07-13 09:32:18","","10.1007/S10462-020-09942-2","","",,,,,21,21.00,7,3,1,"","",""
19,"Ruhhee Tabbussum, A. Q. Dar","Performance evaluation of artificial intelligence paradigms—artificial neural networks, fuzzy logic, and adaptive neuro-fuzzy inference system for flood prediction",2021,"","","","",171,"2022-07-13 09:32:18","","10.1007/s11356-021-12410-1","","",,,,,19,19.00,10,2,1,"","",""
15,"S. Ebrahimian, Fatemeh Homayounieh, M. Rockenbach, Preetham Putha, T. Raj, I. Dayan, B. Bizzo, Varun Buch, Dufan Wu, Kyungsang Kim, Quanzheng Li, S. Digumarthy, M. Kalra","Artificial intelligence matches subjective severity assessment of pneumonia for prediction of patient outcome and need for mechanical ventilation: a cohort study",2021,"","","","",172,"2022-07-13 09:32:18","","10.1038/s41598-020-79470-0","","",,,,,15,15.00,2,13,1,"","",""
0,"Cyril Goutte, Xiao-Dan Zhu, R. Goebel, Yuzuru Tanaka","Advances in Artificial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON, Canada, May 13–15, 2020, Proceedings",2020,"","","","",173,"2022-07-13 09:32:18","","10.1007/978-3-030-47358-7","","",,,,,0,0.00,0,4,2,"","",""
755,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xisheng Fang, Shiqin Zhang, J. Xia, Jun Xia","Artificial Intelligence Distinguishes COVID-19 from Community Acquired Pneumonia on Chest CT",2020,"","","","",174,"2022-07-13 09:32:18","","10.1148/radiol.2020200905","","",,,,,755,377.50,76,18,2,"Background Coronavirus disease has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performances. Materials and Methods In this retrospective and multi-center study, a deep learning model, COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT exams for the detection of COVID-19. Community acquired pneumonia (CAP) and other non-pneumonia CT exams were included to test the robustness of the model. The datasets were collected from 6 hospitals between August 2016 and February 2020. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity and specificity. Results The collected dataset consisted of 4356 chest CT exams from 3,322 patients. The average age is 49±15 years and there were slightly more male patients than female (1838 vs 1484; p-value=0.29). The per-exam sensitivity and specificity for detecting COVID-19 in the independent test set was 114 of 127 (90% [95% CI: 83%, 94%]) and 294 of 307 (96% [95% CI: 93%, 98%]), respectively, with an AUC of 0.96 (p-value<0.001). The per-exam sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175) and 92% (239 of 259), respectively, with an AUC of 0.95 (95% CI: 0.93, 0.97). Conclusions A deep learning model can accurately detect COVID-19 and differentiate it from community acquired pneumonia and other lung diseases.","",""
52,"Jian-hua Li","Cyber security meets artificial intelligence: a survey",2018,"","","","",175,"2022-07-13 09:32:18","","10.1631/FITEE.1800573","","",,,,,52,13.00,52,1,4,"There is a wide range of interdisciplinary intersections between cyber security and artificial intelligence (AI). On one hand, AI technologies, such as deep learning, can be introduced into cyber security to construct smart models for implementing malware classification and intrusion detection and threating intelligence sensing. On the other hand, AI models will face various cyber threats, which will disturb their sample, learning, and decisions. Thus, AI models need specific cyber security defense and protection technologies to combat adversarial machine learning, preserve privacy in machine learning, secure federated learning, etc. Based on the above two aspects, we review the intersection of AI and cyber security. First, we summarize existing research efforts in terms of combating cyber attacks using AI, including adopting traditional machine learning methods and existing deep learning solutions. Then, we analyze the counterattacks from which AI itself may suffer, dissect their characteristics, and classify the corresponding defense methods. Finally, from the aspects of constructing encrypted neural network and realizing a secure federated deep learning, we expatiate the existing research on how to build a secure AI system.","",""
99,"R. Colling, Helen Pitman, K. Oien, N. Rajpoot, P. Macklin, D. Snead, Tony Sackville, C. Verrill","Artificial intelligence in digital pathology: a roadmap to routine use in clinical practice",2019,"","","","",176,"2022-07-13 09:32:18","","10.1002/path.5310","","",,,,,99,33.00,12,8,3,"The use of artificial intelligence will transform clinical practice over the next decade and the early impact of this will likely be the integration of image analysis and machine learning into routine histopathology. In the UK and around the world, a digital revolution is transforming the reporting practice of diagnostic histopathology and this has sparked a proliferation of image analysis software tools. While this is an exciting development that could discover novel predictive clinical information and potentially address international pathology workforce shortages, there is a clear need for a robust and evidence‐based framework in which to develop these new tools in a collaborative manner that meets regulatory approval. With these issues in mind, the NCRI Cellular Molecular Pathology (CM‐Path) initiative and the British In Vitro Diagnostics Association (BIVDA) have set out a roadmap to help academia, industry, and clinicians develop new software tools to the point of approved clinical use. © 2019 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.","",""
85,"A. Grzybowski, Piotr Brona, Gilbert Lim, P. Ruamviboonsuk, G. Tan, M. Abràmoff, D. Ting","Artificial intelligence for diabetic retinopathy screening: a review",2019,"","","","",177,"2022-07-13 09:32:18","","10.1038/s41433-019-0566-0","","",,,,,85,28.33,12,7,3,"","",""
73,"Valentina Bellemo, Gilbert Lim, T. Rim, G. Tan, C. Cheung, S. Sadda, M. He, A. Tufail, M. Lee, W. Hsu, D. Ting","Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application",2019,"","","","",178,"2022-07-13 09:32:18","","10.1007/s11892-019-1189-3","","",,,,,73,24.33,7,11,3,"","",""
67,"Yonghui Shang, Hoang Nguyen, X. Bui, Quang-Hieu Tran, H. Moayedi","A Novel Artificial Intelligence Approach to Predict Blast-Induced Ground Vibration in Open-Pit Mines Based on the Firefly Algorithm and Artificial Neural Network",2019,"","","","",179,"2022-07-13 09:32:18","","10.1007/s11053-019-09503-7","","",,,,,67,22.33,13,5,3,"","",""
51,"Xiaohang Wu, Yelin Huang, Zhenzhen Liu, Weiyi Lai, Erping Long, Kai Zhang, Jiewei Jiang, Duoru Lin, Kexin Chen, Tongyong Yu, Dongxuan Wu, Cong Li, Yanyi Chen, Minjie Zou, Chuan Chen, Yi Zhu, Chong Guo, Xiayin Zhang, Ruixin Wang, Yahan Yang, Yifan Xiang, Lijian Chen, Congxin Liu, J. Xiong, Z. Ge, Ding-ding Wang, Guihua Xu, Shao-lin Du, Chi Xiao, Jianghao Wu, Ke Zhu, Dan-yao Nie, Fan Xu, Jian Lv, Weirong Chen, Yizhi Liu, Haotian Lin","Universal artificial intelligence platform for collaborative management of cataracts",2019,"","","","",180,"2022-07-13 09:32:18","","10.1136/bjophthalmol-2019-314729","","",,,,,51,17.00,5,37,3,"Purpose To establish and validate a universal artificial intelligence (AI) platform for collaborative management of cataracts involving multilevel clinical scenarios and explored an AI-based medical referral pattern to improve collaborative efficiency and resource coverage. Methods The training and validation datasets were derived from the Chinese Medical Alliance for Artificial Intelligence, covering multilevel healthcare facilities and capture modes. The datasets were labelled using a three-step strategy: (1) capture mode recognition; (2) cataract diagnosis as a normal lens, cataract or a postoperative eye and (3) detection of referable cataracts with respect to aetiology and severity. Moreover, we integrated the cataract AI agent with a real-world multilevel referral pattern involving self-monitoring at home, primary healthcare and specialised hospital services. Results The universal AI platform and multilevel collaborative pattern showed robust diagnostic performance in three-step tasks: (1) capture mode recognition (area under the curve (AUC) 99.28%–99.71%), (2) cataract diagnosis (normal lens, cataract or postoperative eye with AUCs of 99.82%, 99.96% and 99.93% for mydriatic-slit lamp mode and AUCs >99% for other capture modes) and (3) detection of referable cataracts (AUCs >91% in all tests). In the real-world tertiary referral pattern, the agent suggested 30.3% of people be ‘referred’, substantially increasing the ophthalmologist-to-population service ratio by 10.2-fold compared with the traditional pattern. Conclusions The universal AI platform and multilevel collaborative pattern showed robust diagnostic performance and effective service for cataracts. The context of our AI-based medical referral pattern will be extended to other common disease conditions and resource-intensive situations.","",""
15,"J. Janet, Chenru Duan, A. Nandy, Fang Liu, H. Kulik","Navigating Transition-Metal Chemical Space: Artificial Intelligence for First-Principles Design.",2021,"","","","",181,"2022-07-13 09:32:18","","10.1021/acs.accounts.0c00686","","",,,,,15,15.00,3,5,1,"ConspectusThe variability of chemical bonding in open-shell transition-metal complexes not only motivates their study as functional materials and catalysts but also challenges conventional computational modeling tools. Here, tailoring ligand chemistry can alter preferred spin or oxidation states as well as electronic structure properties and reactivity, creating vast regions of chemical space to explore when designing new materials atom by atom. Although first-principles density functional theory (DFT) remains the workhorse of computational chemistry in mechanism deduction and property prediction, it is of limited use here. DFT is both far too computationally costly for widespread exploration of transition-metal chemical space and also prone to inaccuracies that limit its predictive performance for localized d electrons in transition-metal complexes. These challenges starkly contrast with the well-trodden regions of small-organic-molecule chemical space, where the analytical forms of molecular mechanics force fields and semiempirical theories have for decades accelerated the discovery of new molecules, accurate DFT functional performance has been demonstrated, and gold-standard methods from correlated wavefunction theory can predict experimental results to chemical accuracy.The combined promise of transition-metal chemical space exploration and lack of established tools has mandated a distinct approach. In this Account, we outline the path we charted in exploration of transition-metal chemical space starting from the first machine learning (ML) models (i.e., artificial neural network and kernel ridge regression) and representations for the prediction of open-shell transition-metal complex properties. The distinct importance of the immediate coordination environment of the metal center as well as the lack of low-level methods to accurately predict structural properties in this coordination environment first motivated and then benefited from these ML models and representations. Once developed, the recipe for prediction of geometric, spin state, and redox potential properties was straightforwardly extended to a diverse range of other properties, including in catalysis, computational ""feasibility"", and the gas separation properties of periodic metal-organic frameworks. Interpretation of selected features most important for model prediction revealed new ways to encapsulate design rules and confirmed that models were robustly mapping essential structure-property relationships. Encountering the special challenge of ensuring that good model performance could generalize to new discovery targets motivated investigation of how to best carry out model uncertainty quantification. Distance-based approaches, whether in model latent space or in carefully engineered feature space, provided intuitive measures of the domain of applicability. With all of these pieces together, ML can be harnessed as an engine to tackle the large-scale exploration of transition-metal chemical space needed to satisfy multiple objectives using efficient global optimization methods. In practical terms, bringing these artificial intelligence tools to bear on the problems of transition-metal chemical space exploration has resulted in ML-model assessments of large, multimillion compound spaces in minutes and validated new design leads in weeks instead of decades.","",""
6,"Yaguan Qian, Qiqi Shao, Jiamin Wang, Xiangyuan Lin, Yankai Guo, Zhaoquan Gu, Bin Wang, Chunming Wu","EI-MTD: Moving Target Defense for Edge Intelligence against Adversarial Attacks",2020,"","","","",182,"2022-07-13 09:32:18","","10.1145/3517806","","",,,,,6,3.00,1,8,2,"Edge intelligence has played an important role in constructing smart cities, but the vulnerability of edge nodes to adversarial attacks becomes an urgent problem. A so-called adversarial example can fool a deep learning model on an edge node for misclassification. Due to the transferability property of adversarial examples, an adversary can easily fool a black-box model by a local substitute model. Edge nodes in general have limited resources, which cannot afford a complicated defense mechanism like that on a cloud data center. To address the challenge, we propose a dynamic defense mechanism, namely EI-MTD. The mechanism first obtains robust member models of small size through differential knowledge distillation from a complicated teacher model on a cloud data center. Then, a dynamic scheduling policy, which builds on a Bayesian Stackelberg game, is applied to the choice of a target model for service. This dynamic defense mechanism can prohibit the adversary from selecting an optimal substitute model for black-box attacks. We also conduct extensive experiments to evaluate the proposed mechanism, and results show that EI-MTD could protect edge intelligence effectively against adversarial attacks in black-box settings.","",""
32,"Matt Taddy","The Technological Elements of Artificial Intelligence",2018,"","","","",183,"2022-07-13 09:32:18","","10.3386/W24301","","",,,,,32,8.00,32,1,4,"We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together – each targeting a straightforward prediction task – to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.","",""
32,"Jun-Ho Huh, Yeong-Seok Seo","Understanding Edge Computing: Engineering Evolution With Artificial Intelligence",2019,"","","","",184,"2022-07-13 09:32:18","","10.1109/ACCESS.2019.2945338","","",,,,,32,10.67,16,2,3,"The key to the explosion of the Internet of Things and the ability to collect, analyze, and provide big data in the cloud is edge computing, which is a new computing paradigm in which data is processed from edges. Edge Computing has been attracting attention as one of the top 10 strategic technology trends in the past two years and has innovative potential. It provides shorter response times, lower bandwidth costs, and more robust data safety and privacy protection than cloud computing. In particular, artificial intelligence technologies are rapidly incorporating edge computing. In this paper, we introduce the concepts, backgrounds, and pros and cons of edge computing, explain how it operates and its structure hierarchically with artificial intelligence concepts, list examples of its applications in various fields, and finally suggest some improvements and discuss the challenges of its application in three representative technological fields. We intend to clarify various analyses and opinions regarding edge computing and artificial intelligence.","",""
21,"N. Anantrasirichai, D. Bull","Artificial intelligence in the creative industries: a review",2020,"","","","",185,"2022-07-13 09:32:18","","10.1007/s10462-021-10039-7","","",,,,,21,10.50,11,2,2,"","",""
0,"","A Novel Approach to Adopt Explainable Artificial Intelligence in X-ray Image Classification",2022,"","","","",186,"2022-07-13 09:32:18","","10.33140/amlai.03.01.01","","",,,,,0,0.00,0,0,1,"Robust “Blackbox” algorithms such as Convolutional Neural Networks (CNNs) are known for making high prediction performance. However, the ability to explain and interpret these algorithms still require innovation in the understanding of influential and, more importantly, explainable features that directly or indirectly impact the performance of predictivity. In view of the above needs, this study proposes an interaction- based methodology – Influence Score (I-score) – to screen out the noisy and non-informative variables in the images hence it nourishes an environment with explainable and interpretable features that are directly associated to feature predictivity. We apply the proposed method on a real-world application in Pneumonia Chest X-ray Image data set and produced state- of-the-art results. We demonstrate how to apply the proposed approach for more general big data problems by improving the explain ability and interpretability without sacrificing the prediction performance. The contribution of this paper opens a novel angle that moves the community closer to the future pipelines of XAI problems.","",""
22,"Rushikesh S. Joshi, Alexander F. Haddad, Darryl Lau, C. Ames","Artificial Intelligence for Adult Spinal Deformity",2019,"","","","",187,"2022-07-13 09:32:18","","10.14245/ns.1938414.207","","",,,,,22,7.33,6,4,3,"Adult spinal deformity (ASD) is a complex disease that significantly affects the lives of many patients. Surgical correction has proven to be effective in achieving improvement of spinopelvic parameters as well as improving quality of life (QoL) for these patients. However, given the relatively high complication risk associated with ASD correction, it is of paramount importance to develop robust prognostic tools for predicting risk profile and outcomes. Historically, statistical models such as linear and logistic regression models were used to identify preoperative factors associated with postoperative outcomes. While these tools were useful for looking at simple associations, they represent generalizations across large populations, with little applicability to individual patients. More recently, predictive analytics utilizing artificial intelligence (AI) through machine learning for comprehensive processing of large amounts of data have become available for surgeons to implement. The use of these computational techniques has given surgeons the ability to leverage far more accurate and individualized predictive tools to better inform individual patients regarding predicted outcomes after ASD correction surgery. Applications range from predicting QoL measures to predicting the risk of major complications, hospital readmission, and reoperation rates. In addition, AI has been used to create a novel classification system for ASD patients, which will help surgeons identify distinct patient subpopulations with unique risk-benefit profiles. Overall, these tools will help surgeons tailor their clinical practice to address patients’ individual needs and create an opportunity for personalized medicine within spine surgery.","",""
0,"K. Hemachandran, Priti Verma, Purvi Pareek, Nidhi Arora, Korupalli V. Rajesh Kumar, T. Ahanger, Anil Audumbar Pise, R. Ratna","Artificial Intelligence: A Universal Virtual Tool to Augment Tutoring in Higher Education",2022,"","","","",188,"2022-07-13 09:32:18","","10.1155/2022/1410448","","",,,,,0,0.00,0,8,1,"Artificial intelligence is an emerging technology that revolutionizes human lives. Despite the fact that this technology is used in higher education, many professors are unaware of it. In this current scenario, there is a huge need to arise, implement information bridge technology, and enhance communication in the classroom. Through this paper, the authors try to predict the future of higher education with the help of artificial intelligence. This research article throws light on the current education system the problems faced by the subject faculties, students, changing government rules, and regulations in the educational sector. Various arguments and challenges on the implementation of artificial intelligence are prevailing in the educational sector. In this concern, we have built a use case model by using a student assessment data of our students and then built a synthesized using generative adversarial network (GAN). The dataset analyzed, visualized, and fed to different machine learning algorithms such as logistic Regression (LR), linear discriminant analysis (LDA), K-nearest neighbors (KNN), classification and regression trees (CART), naive Bayes (NB), support vector machines (SVM), and finally random forest (RF) algorithm and achieved a maximum accuracy of 58%. This article aims to bridge the gap between human lecturers and the machine. We are also concerned about the psychological emotions of the faculty and the students when artificial intelligence takes control.","",""
0,"Tianjun Mo, Qifan Hu","The analysis of artificial intelligence technology: based on neural network",2022,"","","","",189,"2022-07-13 09:32:18","","10.1117/12.2628491","","",,,,,0,0.00,0,2,1,"Contemporarily, deep Learning has achieved remarkable achievement in the field of artificial intelligence technology. Comparing with traditional machine learning methods, deep learning creates its model by constructing the neural network. This investigation reviews the neural network's development history and describes classical neural network methods, e.g., convolutional neural networks and recurrent neural networks. Besides, the shortcomings and limitations that the neural network is currently facing, including aspects in accuracy, stability, and robustness, are discussed. Meanwhile, the solutions towards these limitations are also mentioned, e.g., capsule network and adversarial attack. These results shed light for the future developments of Neural Network.","",""
21,"D. Ting, M. Ang, J. Mehta, D. Ting","Artificial intelligence-assisted telemedicine platform for cataract screening and management: a potential model of care for global eye health",2019,"","","","",190,"2022-07-13 09:32:18","","10.1136/bjophthalmol-2019-315025","","",,,,,21,7.00,5,4,3,"Artificial intelligence (AI) is the fourth industrial revolution.1 Deep learning is a robust machine learning technique that uses convolutional neural network to perform multilevel data abstraction without the need for manual feature engineering.2 In ophthalmology, many studies showed comparable, if not better, diagnostic performance in using AI to screen, diagnose, predict and monitor various eye conditions on fundus photographs and optical coherence tomography,3 4 including diabetic retinopathy (DR),5 age-related macular degeneration,6 glaucoma,7 retinopathy of prematurity (ROP).8   To date, many countries have reported well-established telemedicine programme to screen for DR and ROP,9–12 but limited for cataracts. Cataract is the leading cause of reversible blindness, affecting approximately 12.6 million (3.4–28.7 million) worldwide.13 14 The prevalence of cataract-related visual impairment also varies between high-income and low-income countries, with the latter having poorer access to tertiary care.13 In this issue, Wu et al 15 reported an AI-integrated telemedicine platform to screen and refer patients with cataract. This article consists of two parts: (1) the first part focusing on the AI system in detection of three tasks (capture mode, cataract diagnosis and referable cataract) and (2) the second part describing how these AI algorithms could be integrated in the telemedicine platform for real-world operational use. In this study, the referable cases were defined as: (1) grade 3 and grade 4 nuclear sclerotic …","",""
16,"K. Denecke, E. Gabarron, R. Grainger, S. Konstantinidis, A. Lau, O. Rivera-Romero, T. Miron-Shatz, M. Merolli","Artificial Intelligence for Participatory Health: Applications, Impact, and Future Implications",2019,"","","","",191,"2022-07-13 09:32:18","","10.1055/s-0039-1677902","","",,,,,16,5.33,2,8,3,"Summary Objective : Artificial intelligence (AI) provides people and professionals working in the field of participatory health informatics an opportunity to derive robust insights from a variety of online sources. The objective of this paper is to identify current state of the art and application areas of AI in the context of participatory health. Methods : A search was conducted across seven databases (PubMed, Embase, CINAHL, PsychInfo, ACM Digital Library, IEEExplore, and SCOPUS), covering articles published since 2013. Additionally, clinical trials involving AI in participatory health contexts registered at clinicaltrials.gov were collected and analyzed. Results : Twenty-two articles and 12 trials were selected for review. The most common application of AI in participatory health was the secondary analysis of social media data: self-reported data including patient experiences with healthcare facilities, reports of adverse drug reactions, safety and efficacy concerns about over-the-counter medications, and other perspectives on medications. Other application areas included determining which online forum threads required moderator assistance, identifying users who were likely to drop out from a forum, extracting terms used in an online forum to learn its vocabulary, highlighting contextual information that is missing from online questions and answers, and paraphrasing technical medical terms for consumers. Conclusions : While AI for supporting participatory health is still in its infancy, there are a number of important research priorities that should be considered for the advancement of the field. Further research evaluating the impact of AI in participatory health informatics on the psychosocial wellbeing of individuals would help in facilitating the wider acceptance of AI into the healthcare ecosystem.","",""
0,"N. Rafie, J. Jentzer, P. Noseworthy, A. Kashou","Mortality Prediction in Cardiac Intensive Care Unit Patients: A Systematic Review of Existing and Artificial Intelligence Augmented Approaches",2022,"","","","",192,"2022-07-13 09:32:18","","10.3389/frai.2022.876007","","",,,,,0,0.00,0,4,1,"The medical complexity and high acuity of patients in the cardiac intensive care unit make for a unique patient population with high morbidity and mortality. While there are many tools for predictions of mortality in other settings, there is a lack of robust mortality prediction tools for cardiac intensive care unit patients. The ongoing advances in artificial intelligence and machine learning also pose a potential asset to the advancement of mortality prediction. Artificial intelligence algorithms have been developed for application of electrocardiogram interpretation with promising accuracy and clinical application. Additionally, artificial intelligence algorithms applied to electrocardiogram interpretation have been developed to predict various variables such as structural heart disease, left ventricular systolic dysfunction, and atrial fibrillation. These variables can be used and applied to new mortality prediction models that are dynamic with the changes in the patient's clinical course and may lead to more accurate and reliable mortality prediction. The application of artificial intelligence to mortality prediction will fill the gaps left by current mortality prediction tools.","",""
19,"E. O. Kontis, T. Papadopoulos, M. Syed, E. Guillo‐Sansano, G. Burt, G. Papagiannis","Artificial-Intelligence Method for the Derivation of Generic Aggregated Dynamic Equivalent Models",2019,"","","","",193,"2022-07-13 09:32:18","","10.1109/TPWRS.2019.2894185","","",,,,,19,6.33,3,6,3,"Aggregated equivalent models for the dynamic analysis of active distribution networks (ADNs) can be efficiently developed using dynamic responses recorded through field measurements. However, equivalent model parameters are highly affected from the time-varying composition of power system loads and the stochastic behavior of distributed generators. Thus, equivalent models, developed through in situ measurements, are valid only for the operating conditions from which they have been derived. To overcome this issue, in this paper, a new method is proposed for the derivation of generic aggregated dynamic equivalent models, i.e., for equivalent models that can be used for the dynamic analysis of a wide range of network conditions. The method incorporates clustering and artificial neural network techniques to derive robust sets of parameters for a variable-order dynamic equivalent model. The effectiveness of the proposed method is evaluated using measurements recorded on a laboratory-scale ADN, while its performance is compared with a conventional technique. The corresponding results reveal the applicability of the proposed approach for the analysis and simulation of a wide range of distinct network conditions.","",""
11,"Siqi Liu, A. Setio, Florin C. Ghesu, E. Gibson, Sasa Grbic, B. Georgescu, D. Comaniciu","No Surprises: Training Robust Lung Nodule Detection for Low-Dose CT Scans by Augmenting With Adversarial Attacks",2020,"","","","",194,"2022-07-13 09:32:18","","10.1109/TMI.2020.3026261","","",,,,,11,5.50,2,7,2,"Detecting malignant pulmonary nodules at an early stage can allow medical interventions which may increase the survival rate of lung cancer patients. Using computer vision techniques to detect nodules can improve the sensitivity and the speed of interpreting chest CT for lung cancer screening. Many studies have used CNNs to detect nodule candidates. Though such approaches have been shown to outperform the conventional image processing based methods regarding the detection accuracy, CNNs are also known to be limited to generalize on under-represented samples in the training set and prone to imperceptible noise perturbations. Such limitations can not be easily addressed by scaling up the dataset or the models. In this work, we propose to add adversarial synthetic nodules and adversarial attack samples to the training data to improve the generalization and the robustness of the lung nodule detection systems. To generate hard examples of nodules from a differentiable nodule synthesizer, we use projected gradient descent (PGD) to search the latent code within a bounded neighbourhood that would generate nodules to decrease the detector response. To make the network more robust to unanticipated noise perturbations, we use PGD to search for noise patterns that can trigger the network to give over-confident mistakes. By evaluating on two different benchmark datasets containing consensus annotations from three radiologists, we show that the proposed techniques can improve the detection performance on real CT data. To understand the limitations of both the conventional networks and the proposed augmented networks, we also perform stress-tests on the false positive reduction networks by feeding different types of artificially produced patches. We show that the augmented networks are more robust to both under-represented nodules as well as resistant to noise perturbations.","",""
27,"Óscar Álvarez-Machancoses, J. Fernández-Martínez","Using artificial intelligence methods to speed up drug discovery",2019,"","","","",195,"2022-07-13 09:32:18","","10.1080/17460441.2019.1621284","","",,,,,27,9.00,14,2,3,"ABSTRACT Introduction: Drug discovery is the process through which potential new compounds are identified by means of biology, chemistry, and pharmacology. Due to the high complexity of genomic data, AI techniques are increasingly needed to help reduce this and aid the adoption of optimal decisions. Phenotypic prediction is of particular use to drug discovery and precision medicine where sets of genes that predict a given phenotype are determined. Phenotypic prediction is an undetermined problem given that the number of monitored genetic probes markedly exceeds the number of collected samples (from patients). This imbalance creates ambiguity in the characterization of the biological pathways that are responsible for disease development. Areas covered: In this paper, the authors present AI methodologies that perform a robust deep sampling of altered genetic pathways to locate new therapeutic targets, assist in drug repurposing and speed up and optimize the drug selection process. Expert opinion: AI is a potential solution to a number of drug discovery problems, though one should, bear in mind that the quality of data predicts the overall quality of the prediction, as in any modeling task in data science. The use of transparent methodologies is crucial, particularly in drug repositioning/repurposing in rare diseases.","",""
10,"Zihao Chen, Long Hu, Baoting Zhang, Aiping Lu, Yaofeng Wang, Yuanyuan Yu, Ge Zhang","Artificial Intelligence in Aptamer–Target Binding Prediction",2021,"","","","",196,"2022-07-13 09:32:18","","10.3390/ijms22073605","","",,,,,10,10.00,1,7,1,"Aptamers are short single-stranded DNA, RNA, or synthetic Xeno nucleic acids (XNA) molecules that can interact with corresponding targets with high affinity. Owing to their unique features, including low cost of production, easy chemical modification, high thermal stability, reproducibility, as well as low levels of immunogenicity and toxicity, aptamers can be used as an alternative to antibodies in diagnostics and therapeutics. Systematic evolution of ligands by exponential enrichment (SELEX), an experimental approach for aptamer screening, allows the selection and identification of in vitro aptamers with high affinity and specificity. However, the SELEX process is time consuming and characterization of the representative aptamer candidates from SELEX is rather laborious. Artificial intelligence (AI) could help to rapidly identify the potential aptamer candidates from a vast number of sequences. This review discusses the advancements of AI pipelines/methods, including structure-based and machine/deep learning-based methods, for predicting the binding ability of aptamers to targets. Structure-based methods are the most used in computer-aided drug design. For this part, we review the secondary and tertiary structure prediction methods for aptamers, molecular docking, as well as molecular dynamic simulation methods for aptamer–target binding. We also performed analysis to compare the accuracy of different secondary and tertiary structure prediction methods for aptamers. On the other hand, advanced machine-/deep-learning models have witnessed successes in predicting the binding abilities between targets and ligands in drug discovery and thus potentially offer a robust and accurate approach to predict the binding between aptamers and targets. The research utilizing machine-/deep-learning techniques for prediction of aptamer–target binding is limited currently. Therefore, perspectives for models, algorithms, and implementation strategies of machine/deep learning-based methods are discussed. This review could facilitate the development and application of high-throughput and less laborious in silico methods in aptamer selection and characterization.","",""
427,"D. Ting, L. Pasquale, L. Peng, J. P. Campbell, Aaron Y. Lee, R. Raman, G. Tan, L. Schmetterer, P. Keane, T. Wong","Artificial intelligence and deep learning in ophthalmology",2018,"","","","",197,"2022-07-13 09:32:18","","10.1136/bjophthalmol-2018-313173","","",,,,,427,106.75,43,10,4,"Artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest in recent years. DL has been widely adopted in image recognition, speech recognition and natural language processing, but is only beginning to impact on healthcare. In ophthalmology, DL has been applied to fundus photographs, optical coherence tomography and visual fields, achieving robust classification performance in the detection of diabetic retinopathy and retinopathy of prematurity, the glaucoma-like disc, macular oedema and age-related macular degeneration. DL in ocular imaging may be used in conjunction with telemedicine as a possible solution to screen, diagnose and monitor major eye diseases for patients in primary care and community settings. Nonetheless, there are also potential challenges with DL application in ophthalmology, including clinical and technical challenges, explainability of the algorithm results, medicolegal issues, and physician and patient acceptance of the AI ‘black-box’ algorithms. DL could potentially revolutionise how ophthalmology is practised in the future. This review provides a summary of the state-of-the-art DL systems described for ophthalmic applications, potential challenges in clinical deployment and the path forward.","",""
0,"Sandro González-González, L. Serpa-Andrade","Development of a virtual assistant chatbot based on Artificial Intelligence to control and supervise a process of 4 tanks which are interconnected",2022,"","","","",198,"2022-07-13 09:32:18","","10.54941/ahfe1001464","","",,,,,0,0.00,0,2,1,"This article presents the gathering of works related to the usage of virtual assistants into the 4.0 industry in order to stablish the parameters and essential characteristics to define the creation of a ‘chatbot’ virtual assistant. This device should be applicable to a process of 4 tanks which are interconnected with a robust multivariable PID control with the aim of controlling and supervising this process using a mobile messaging application from a smartphone by sending key words in text messages which will be interpreted by the chatbot and this will be capable of acting depending on the message it receives; it can be either a consultation of the status of the process and the tanks which will be answered with a text message with the required information, or a command which will make it work starting or stopping the process. This system is proposed as a solution in the case of long-distance supervision and control during different processes. With this, an option to optimize the execution of actions such as security, speed, reliability of data, and resource maximization can be implemented, which leads to a better general performance of an industry","",""
822,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xi Fang, Shiqin Zhang, J. Xia, Jun Xia","Using Artificial Intelligence to Detect COVID-19 and Community-acquired Pneumonia Based on Pulmonary CT: Evaluation of the Diagnostic Accuracy",2020,"","","","",199,"2022-07-13 09:32:18","","10.1148/RADIOL.2020200905","","",,,,,822,411.00,82,18,2,"Background Coronavirus disease 2019 (COVID-19) has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performance. Materials and Methods In this retrospective and multicenter study, a deep learning model, the COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT scans for the detection of COVID-19. CT scans of community-acquired pneumonia (CAP) and other non-pneumonia abnormalities were included to test the robustness of the model. The datasets were collected from six hospitals between August 2016 and February 2020. Diagnostic performance was assessed with the area under the receiver operating characteristic curve, sensitivity, and specificity. Results The collected dataset consisted of 4352 chest CT scans from 3322 patients. The average patient age (±standard deviation) was 49 years ± 15, and there were slightly more men than women (1838 vs 1484, respectively; P = .29). The per-scan sensitivity and specificity for detecting COVID-19 in the independent test set was 90% (95% confidence interval [CI]: 83%, 94%; 114 of 127 scans) and 96% (95% CI: 93%, 98%; 294 of 307 scans), respectively, with an area under the receiver operating characteristic curve of 0.96 (P < .001). The per-scan sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175 scans) and 92% (239 of 259 scans), respectively, with an area under the receiver operating characteristic curve of 0.95 (95% CI: 0.93, 0.97). Conclusion A deep learning model can accurately detect coronavirus 2019 and differentiate it from community-acquired pneumonia and other lung conditions. © RSNA, 2020 Online supplemental material is available for this article.","",""
9,"David Manheim","Multiparty Dynamics and Failure Modes for Machine Learning and Artificial Intelligence",2018,"","","","",200,"2022-07-13 09:32:18","","10.3390/BDCC3020021","","",,,,,9,2.25,9,1,4,"An important challenge for safety in machine learning and artificial intelligence systems is a set of related failures involving specification gaming, reward hacking, fragility to distributional shifts, and Goodhart’s or Campbell’s law. This paper presents additional failure modes for interactions within multi-agent systems that are closely related. These multi-agent failure modes are more complex, more problematic, and less well understood than the single-agent case, and are also already occurring, largely unnoticed. After motivating the discussion with examples from poker-playing artificial intelligence (AI), the paper explains why these failure modes are in some senses unavoidable. Following this, the paper categorizes failure modes, provides definitions, and cites examples for each of the modes: accidental steering, coordination failures, adversarial misalignment, input spoofing and filtering, and goal co-option or direct hacking. The paper then discusses how extant literature on multi-agent AI fails to address these failure modes, and identifies work which may be useful for the mitigation of these failure modes.","",""
