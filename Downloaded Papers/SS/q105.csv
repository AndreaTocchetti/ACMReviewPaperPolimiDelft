Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
1,"Quang-Vinh Dang","Improving the performance of the intrusion detection systems by the machine learning explainability",2021,"","","","",1,"2022-07-13 09:37:52","","10.1108/ijwis-03-2021-0022","","",,,,,1,1.00,1,1,1," Purpose This study aims to explain the state-of-the-art machine learning models that are used in the intrusion detection problem for human-being understandable and study the relationship between the explainability and the performance of the models.   Design/methodology/approach The authors study a recent intrusion data set collected from real-world scenarios and use state-of-the-art machine learning algorithms to detect the intrusion. The authors apply several novel techniques to explain the models, then evaluate manually the explanation. The authors then compare the performance of model post- and prior-explainability-based feature selection.   Findings The authors confirm our hypothesis above and claim that by forcing the explainability, the model becomes more robust, requires less computational power but achieves a better predictive performance.   Originality/value The authors draw our conclusions based on their own research and experimental works. ","",""
1,"M. Staron, Helena Odenstedt Herg'es, S. Naredi, L. Block, Ali El-Merhi, Richard Vithal, M. Elam","Robust Machine Learning in Critical Care — Software Engineering and Medical Perspectives",2021,"","","","",2,"2022-07-13 09:37:52","","10.1109/WAIN52551.2021.00016","","",,,,,1,1.00,0,7,1,"Using machine learning in clinical practice poses hard requirements on explainability, reliability, replicability and robustness of these systems. Therefore, developing reliable software for monitoring critically ill patients requires close collaboration between physicians and software engineers. However, these two different disciplines need to find own research perspectives in order to contribute to both the medical and the software engineering domain. In this paper, we address the problem of how to establish a collaboration where software engineering and medicine meets to design robust machine learning systems to be used in patient care. We describe how we designed software systems for monitoring patients under carotid endarterectomy, in particular focusing on the process of knowledge building in the research team. Our results show what to consider when setting up such a collaboration, how it develops over time and what kind of systems can be constructed based on it. We conclude that the main challenge is to find a good research team, where different competences are committed to a common goal.","",""
0,"Véronique M. Gomes, P. Melo-Pinto","Towards robust Machine Learning models for grape ripeness assessment",2021,"","","","",3,"2022-07-13 09:37:52","","10.1109/JCSSE53117.2021.9493822","","",,,,,0,0.00,0,2,1,"Artificial intelligence methods need to be more transparent for wider acceptance by the industry. In particular deep neural networks (DNN) are not explainable, due to the complex processes the input undergo. The present work addresses model explainability for wine grapes quality assessment through 1D-CNN, using regression activation maps (RAM) to show the contribution score of each wavelength for the prediction of sugar content. This way we identify the relevant regions related to this enological parameter. The results obtained indicate that the proposed approach can successfully highlight important spectral regions related to sugars absorption, improving the current state of the art, and opening way to dimensionality reduction methods and further model interpretation.","",""
0,"Christos Kokkotis, S. Moustakidis, E. Papageorgiou, G. Giakas, D. Tsaopoulos","A Machine Learning workflow for Diagnosis of Knee Osteoarthritis with a focus on post-hoc explainability",2020,"","","","",4,"2022-07-13 09:37:52","","10.1109/IISA50023.2020.9284354","","",,,,,0,0.00,0,5,2,"Knee Osteoarthritis (KOA) is a multifactorial disease-causing joint pain, deformity and dysfunction. The aim of this paper is to provide a data mining approach that could identify important risk factors which contribute to the diagnosis of KOA and their impact on model output, with a focus on posthoc explainability. Data were obtained from the osteoarthritis initiative (OAI) database enrolling people, with nonsymptomatic KOA and symptomatic KOA or being at high risk of developing KOA. The current study considered multidisciplinary data from heterogeneous sources such as questionnaire data, physical activity indexes, self-reported data about joint symptoms, disability and function as well as general health and physical exams’ data from individuals with or without KOA from the baseline visit. For the data mining part, a robust feature selection methodology was employed consisting of filter, wrapper and embedded techniques whereas feature ranking was decided on the basis of a majority vote scheme. The validation of the extracted factors was performed in subgroups employing seven well-known classifiers. A 77.88 % classification accuracy was achieved by Logistic Regression on the group of the first forty selected (40) risk factors. We investigated the behavior of the best model, with respect to classification errors and the impact of used features, to confirm their clinical relevance. The interpretation of the model output was performed by SHAP. The results are the basis for the development of easy-to-use diagnostic tools for clinicians for the early detection of KOA.","",""
0,"S. Al-Zaiti, Alaa A. Alghwiri, Xiao Hu, G. Clermont, A. Peace, P. Macfarlane, R. Bond","A clinician’s guide to understanding and critically appraising machine learning studies: a checklist for Ruling Out Bias Using Standard Tools in Machine Learning (ROBUST-ML)",2022,"","","","",5,"2022-07-13 09:37:52","","10.1093/ehjdh/ztac016","","",,,,,0,0.00,0,7,1,"  Developing functional machine learning (ML)-based models to address unmet clinical needs requires unique considerations for optimal clinical utility. Recent debates about the rigours, transparency, explainability, and reproducibility of ML models, terms which are defined in this article, have raised concerns about their clinical utility and suitability for integration in current evidence-based practice paradigms. This featured article focuses on increasing the literacy of ML among clinicians by providing them with the knowledge and tools needed to understand and critically appraise clinical studies focused on ML. A checklist is provided for evaluating the rigour and reproducibility of the four ML building blocks: data curation, feature engineering, model development, and clinical deployment. Checklists like this are important for quality assurance and to ensure that ML studies are rigourously and confidently reviewed by clinicians and are guided by domain knowledge of the setting in which the findings will be applied. Bridging the gap between clinicians, healthcare scientists, and ML engineers can address many shortcomings and pitfalls of ML-based solutions and their potential deployment at the bedside.","",""
2,"V. Kulkarni, M. Gawali, A. Kharat","Key Technology Considerations in Developing and Deploying Machine Learning Models in Clinical Radiology Practice",2021,"","","","",6,"2022-07-13 09:37:52","","10.2196/28776","","",,,,,2,2.00,1,3,1,"The use of machine learning to develop intelligent software tools for the interpretation of radiology images has gained widespread attention in recent years. The development, deployment, and eventual adoption of these models in clinical practice, however, remains fraught with challenges. In this paper, we propose a list of key considerations that machine learning researchers must recognize and address to make their models accurate, robust, and usable in practice. We discuss insufficient training data, decentralized data sets, high cost of annotations, ambiguous ground truth, imbalance in class representation, asymmetric misclassification costs, relevant performance metrics, generalization of models to unseen data sets, model decay, adversarial attacks, explainability, fairness and bias, and clinical validation. We describe each consideration and identify the techniques used to address it. Although these techniques have been discussed in prior research, by freshly examining them in the context of medical imaging and compiling them in the form of a laundry list, we hope to make them more accessible to researchers, software developers, radiologists, and other stakeholders.","",""
2,"A. Smart, Larry James, B. Hutchinson, Simone Wu, Shannon Vallor","Why Reliabilism Is not Enough: Epistemic and Moral Justification in Machine Learning",2020,"","","","",7,"2022-07-13 09:37:52","","10.1145/3375627.3375866","","",,,,,2,1.00,0,5,2,"In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of \em justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed wide adoption of machine learning? We argue that, in general, people implicitly adoptreliabilism regarding machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method \citegoldman2012reliabilism. We argue that, in cases where model deployments require \em moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral ""wrapper'' around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification---moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.","",""
0,"Tae-Hyun Chun, Yong Yu","Enhancing CHF Prediction of AECL Look-Up Table Along with Machine Learning",2020,"","","","",8,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,2,2,"A critical heat flux (CHF) is a key safety parameter. For the CHF prediction, artificial neural network has been also applied and showed good performances [1, 2]. However, it is hardly accepted in the nuclear community due to a drawback of ‘Explainability’. A machine learning, as a subset of the artificial intelligence, can play a supplementary role for a more robust domain knowledge-based model. AECL Look-up Table (LUT) is widely used for the CHF prediction in reactor thermal-hydraulic design and safety analyses [3]. This domain knowledge model can predict the CHF by two schemes such as DSM (Direct Substitute Method) and HBM (Heat Balance Method). The uncertainty is much large in the DSM relative to the HBM. But the DSM is practically used in the nuclear engineering since HBM requires iterations to reach the heat balance in the CHF prediction. The purpose of this study is to show a feasibility that a machine learning-aided CHF LUT model enhances considerably the accuracy of the CHF prediction.","",""
0,"Alireza Rezazadeh, Yasamin Jafarian, A. Kord","Explainable Ensemble Machine Learning for Breast Cancer Diagnosis based on Ultrasound Image Texture Features",2022,"","","","",9,"2022-07-13 09:37:52","","10.3390/forecast4010015","","",,,,,0,0.00,0,3,1,"Image classification is widely used to build predictive models for breast cancer diagnosis. Most existing approaches overwhelmingly rely on deep convolutional networks to build such diagnosis pipelines. These model architectures, although remarkable in performance, are black-box systems that provide minimal insight into the inner logic behind their predictions. This is a major drawback as the explainability of prediction is vital for applications such as cancer diagnosis. In this paper, we address this issue by proposing an explainable machine learning pipeline for breast cancer diagnosis based on ultrasound images. We extract first- and second-order texture features of the ultrasound images and use them to build a probabilistic ensemble of decision tree classifiers. Each decision tree learns to classify the input ultrasound image by learning a set of robust decision thresholds for texture features of the image. The decision path of the model predictions can then be interpreted by decomposing the learned decision trees. Our results show that our proposed framework achieves high predictive performance while being explainable.","",""
1,"Aparna Balagopalan, Haoran Zhang, Kimia Hamidieh, Thomas Hartvigsen, F. Rudzicz, M. Ghassemi","The Road to Explainability is Paved with Bias: Measuring the Fairness of Explanations",2022,"","","","",10,"2022-07-13 09:37:52","","10.1145/3531146.3533179","","",,,,,1,1.00,0,6,1,"Machine learning models in safety-critical settings like healthcare are often “blackboxes”: they contain a large number of parameters which are not transparent to users. Post-hoc explainability methods where a simple, human-interpretable model imitates the behavior of these blackbox models are often proposed to help users trust model predictions. In this work, we audit the quality of such explanations for different protected subgroups using real data from four settings in finance, healthcare, college admissions, and the US justice system. Across two different blackbox model architectures and four popular explainability methods, we find that the approximation quality of explanation models, also known as the fidelity, differs significantly between subgroups. We also demonstrate that pairing explainability methods with recent advances in robust machine learning can improve explanation fairness in some settings. However, we highlight the importance of communicating details of non-zero fidelity gaps to users, since a single solution might not exist across all settings. Finally, we discuss the implications of unfair explanation models as a challenging and understudied problem facing the machine learning community.","",""
0,"Hemant Rathore","Designing Adversarial Robust and Explainable Malware Detection System for Android based Smartphones: PhD Forum Abstract",2021,"","","","",11,"2022-07-13 09:37:52","","10.1145/3412382.3459209","","",,,,,0,0.00,0,1,1,"Android smartphones and malware have grown exponentially in the last decade. Literature suggests that the current malware detection systems cannot cope with the present security challenges. Thus researchers are developing next-generation malware detection systems/models using the machine and deep learning. However, the proposed systems/models have poor explainability and are vulnerable against adversarial attacks, which will jeopardize their adoption in the future security ecosystem. Thus, we aim to construct adversarial robust malware detection models by first acting as an adversary to find vulnerabilities in models and then proposing preventive countermeasures. We also aim to improve models' explain-ability to win security community confidence before real-world implementation.","",""
0,"Olivia M. Brown, B. Dillman","Proceedings of the Robust Artificial Intelligence System Assurance (RAISA) Workshop 2022",2022,"","","","",12,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,2,1,"The Robust Artificial Intelligence System Assurance (RAISA) workshop will focus on research, development and application of robust artificial intelligence (AI) and machine learning (ML) systems. Rather than studying robustness with respect to particular ML algorithms, our approach will be to explore robustness assurance at the system architecture level, during both development and deployment, and within the human-machine teaming context. While the research community is converging on robust solutions for individual AI models in specific scenarios, the problem of evaluating and assuring the robustness of an AI system across its entire life cycle is much more complex. Moreover, the operational context in which AI systems are deployed necessitates consideration of robustness and its relation to principles of fairness, privacy, and explainability.","",""
0,"Hatma Suryotrisongko, Y. Musashi, A. Tsuneda, K. Sugitani","Robust Botnet DGA Detection: Blending XAI and OSINT for Cyber Threat Intelligence Sharing",2022,"","","","",13,"2022-07-13 09:37:52","","10.1109/ACCESS.2022.3162588","","",,,,,0,0.00,0,4,1,"We investigated 12 years DNS query logs of our campus network and identified phenomena of malicious botnet domain generation algorithm (DGA) traffic. DGA-based botnets are difficult to detect using cyber threat intelligence (CTI) systems based on blocklists. Artificial intelligence (AI)/machine learning (ML)-based CTI systems are required. This study (1) proposed a model to detect DGA-based traffic based on statistical features with datasets comprising 55 DGA families, (2) discussed how CTI can be expanded with computable CTI paradigm, and (3) described how to improve the explainability of the model outputs by blending explainable AI (XAI) and open-source intelligence (OSINT) for trust problems, an antidote for skepticism to the shared models and preventing automation bias. We define the XAI-OSINT blending as aggregations of OSINT for AI/ML model outcome validation. Experimental results show the effectiveness of our models (96.3% accuracy). Our random forest model provides better robustness against three state-of-the-art DGA adversarial attacks (CharBot, DeepDGA, MaskDGA) compared with character-based deep learning models (Endgame, CMU, NYU, MIT). We demonstrate the sharing mechanism and confirm that the XAI-OSINT blending improves trust for CTI sharing as evidence to validate our proposed computable CTI paradigm to assist security analysts in security operations centers using an automated, explainable OSINT approach (for second opinion). Therefore, the computable CTI reduces manual intervention in critical cybersecurity decision-making.","",""
0,"P. Mahato, Apurva Narayan","Robust Supply Chains with Gradient Boosted Trees",2020,"","","","",14,"2022-07-13 09:37:52","","10.1109/SSCI47803.2020.9308150","","",,,,,0,0.00,0,2,2,"Supply chain networks often experience various internal and external events that lead to shipment failures. Despite advancements in various machine learning models, the problem of avoiding service level failures remains intricate and hard to solve. While multiple attempts have been made by various researchers to make supply chains resilient, this is still an open problem. Moreover, explainability in the field of machine learning is a challenging task that assists in decision formation along with transparency.We develop a machine learning pipeline with gradient boosted decision trees to mitigate service level failures in supply chains. Our framework is simple, easy to implement, and provides a promising result. It provides explainability to prevent service level failure in time sensitive supply chains such as food manufacturing. Our model can be used for rapid deployment with state-of-the-art prediction accuracy while establishing trust within the decision-makers.","",""
76,"Amir-Hossein Karimi, G. Barthe, B. Schölkopf, I. Valera","A survey of algorithmic recourse: definitions, formulations, solutions, and prospects",2020,"","","","",15,"2022-07-13 09:37:52","","","","",,,,,76,38.00,19,4,2,"Machine learning is increasingly used to inform decision-making in sensitive situations where decisions have consequential effects on individuals' lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role for the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavourably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions towards which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.","",""
11,"Daniel Harborne, C. Willis, Richard J. Tomsett, A. Preece","Integrating learning and reasoning services for explainable information fusion",2018,"","","","",16,"2022-07-13 09:37:52","","","","",,,,,11,2.75,3,4,4,"—We present a distributed information fusion system  able to integrate heterogeneous information processing services  based on machine learning and reasoning approaches. We focus  on higher (semantic) levels of information fusion, and highlight  the requirement for the component services, and the system as  a whole, to generate explanations of its outputs. Using a case  study approach in the domain of traffic monitoring, we introduce  component services based on (i) deep neural network approaches  and (ii) heuristic-based reasoning. We examine methods for  explanation generation in each case, including both transparency  (e.g, saliency maps, reasoning traces) and post-hoc methods  (e.g, explanation in terms of similar examples, identification of  relevant semantic objects). We consider trade-offs in terms of  the classification performance of the services and the kinds of  available explanations, and show how service integration offers  more robust performance and explainability.","",""
51,"A. Garcez, L. Lamb","Neurosymbolic AI: The 3rd Wave",2020,"","","","",17,"2022-07-13 09:37:52","","","","",,,,,51,25.50,26,2,2,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.","",""
3,"A. Karimi, Gilles Barthe, B. Schölkopf, I. Valera","A survey of algorithmic recourse:contrastive explanations and consequential recommendations",2022,"","","","",18,"2022-07-13 09:37:52","","10.1145/3527848","","",,,,,3,3.00,1,4,1,"Machine learning is increasingly used to inform decision-making in sensitive situations where decisions have consequential effects on individuals’ lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role in the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavorably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions towards which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.","",""
2,"Melissa D. McCradden","When is accuracy off-target?",2021,"","","","",19,"2022-07-13 09:37:52","","10.1038/s41398-021-01479-4","","",,,,,2,2.00,2,1,1,"","",""
2,"Ayoub El Qadi, Natalia Díaz Rodríguez, M. Trocan, Thomas Frossard","Explaining Credit Risk Scoring through Feature Contribution Alignment with Expert Risk Analysts",2021,"","","","",20,"2022-07-13 09:37:52","","","","",,,,,2,2.00,1,4,1,"Credit assessments activities are essential for financial institutions and allow the global economy to grow. Building robust, solid and accurate models that estimate the probability of a default of a company is mandatory for credit insurance companies, specially when it comes to bridging the trade finance gap. Automating the risk assessment process will allow credit risk experts to reduce their workload and focus on the critical and complex cases, as well as to improve the loan approval process by reducing the time to process the application. The recent developments in Artificial Intelligence are offering new powerful opportunities. However, most AI techniques are labelled as blackbox models due to their lack of explainability. For both users and regulators, in order to deploy such technologies at scale, being able to understand the model logic is a must to grant accurate and ethical decision making. In this study, we focus on companies credit scoring and we benchmark different machine learning models. The aim is to build a model to predict whether a company will experience financial problems in a given time horizon. We address the black box problem using eXplainable Artificial Techniques –in particular, post-hoc explanations using SHapley Additive exPlanations. We bring light by providing an expertaligned feature relevance score highlighting the disagreement between a credit risk expert and a model feature attribution explanation in order to better quantify the convergence towards a better human-aligned decision making.","",""
1,"Najah F. Ghalyan, C. Bhattacharya, I. F. Ghalyan, A. Ray","Spectral invariants of ergodic symbolic systems for pattern recognition and anomaly detection",2022,"","","","",21,"2022-07-13 09:37:52","","10.1098/rsta.2021.0196","","",,,,,1,1.00,0,4,1,"Despite tangible advances in machine learning (ML) over the last few decades, many of the ML techniques still suffer from fundamental issues like overfitting and lack of explainability. These issues mandate requirements for mathematical rigor to ensure robust learning from observed data. In this context, topological invariants in data manifolds provide a rich representation of the underlying dynamical system, which can be utilized for developing a mathematically rigorous ML tool to characterize the dynamical behaviour and operational phases of the underlying process. This paper aims to investigate spectral invariants of symbolic systems for detecting changes in topological characteristics of data manifolds. A novel ML approach is proposed, where commutator norms are used on sequences of endomorphisms to symbolically describe dynamical systems on probability spaces with ergodic measures. The objective here is to detect topological invariants of data manifolds that can be used for signal processing, pattern recognition, and anomaly detection. The proposed ML approach is validated on models of selected chaotic dynamical systems for prompt detection of phase transitions. This article is part of the theme issue ‘Data-driven prediction in dynamical systems’.","",""
0,"Joao Marques-Silva","Automated Reasoning in Explainable AI",2021,"","","","",22,"2022-07-13 09:37:52","","10.3233/faia210109","","",,,,,0,0.00,0,1,1,"The envisioned applications of machine learning (ML) in high-risk and safetycritical applications hinge on systems that are robust in their operation and that can be trusted. Automated reasoning offers the solution to ensure robustness and to guarantee trust. This talk overviews recent efforts on applying automated reasoning tools in explaining black-box (and so non-interpretable) ML models [6], and relates such efforts with past work on reasoning about inconsistent logic formulas [11]. Moreover, the talk details the computation of rigorous explanations of black-box models, and how these serve for assessing the quality of widely used heuristic explanation approaches. The talk also covers important properties of rigorous explanations, including duality relationships between different kinds of explanations [7,5,4]. Finally, the talk briefly overviews ongoing work on mapping practical efficient [8,3] but also tractable explainability [9,10,2,1].","",""
0,"Gaur Loveleen, Bhandari Mohan, Bhadwal Singh Shikhar, Jhanjhi Nz, Mohammad Shorfuzzaman, Mehedi Masud","Explanation-driven HCI Model to Examine the Mini-Mental State for Alzheimer’s Disease",2022,"","","","",23,"2022-07-13 09:37:52","","10.1145/3527174","","",,,,,0,0.00,0,6,1,"Directing research on Alzheimer’s towards only early prediction and accuracy cannot be considered a feasible approach towards tackling a ubiquitous degenerative disease today. Applying deep learning (DL), Explainable artificial intelligence(XAI) and advancing towards the human-computer interface(HCI) model can be a leap forward in medical research. This research aims to propose a robust explainable HCI model using shapley additive explanation (SHAP), local interpretable model-agnostic explanations (LIME) and DL algorithms. The use of DL algorithms: logistic regression(80.87%), support vector machine (85.8%), k-nearest neighbour(87.24%), multilayer perceptron(91.94%), decision tree(100%) and explainability can help exploring untapped avenues for research in medical sciences that can mould the future of HCI models. The outcomes of the proposed model depict higher prediction accuracy bringing efficient computer interface in decision making, and suggests a high level of relevance in the field of medical and clinical research.","",""
13,"M. Namaki, Avrilia Floratou, Fotis Psallidas, Subru Krishnan, Ashvin Agrawal, Yinghui Wu, Yiwen Zhu, Markus Weimer","Vamsa: Automated Provenance Tracking in Data Science Scripts",2020,"","","","",24,"2022-07-13 09:37:52","","10.1145/3394486.3403205","","",,,,,13,6.50,2,8,2,"There has recently been a lot of ongoing research in the areas of fairness, bias and explainability of machine learning (ML) models due to the self-evident or regulatory requirements of various ML applications. We make the following observation: All of these approaches require a robust understanding of the relationship between ML models and the data used to train them. In this work, we introduce the ML provenance tracking problem: the fundamental idea is to automatically track which columns in a dataset have been used to derive the features/labels of an ML model. We discuss the challenges in capturing such information in the context of Python, the most common language used by data scientists. We then present Vamsa, a modular system that extracts provenance from Python scripts without requiring any changes to the users' code. Using 26K real data science scripts, we verify the effectiveness of Vamsa in terms of coverage, and performance. We also evaluate Vamsa's accuracy on a smaller subset of manually labeled data. Our analysis shows that Vamsa's precision and recall range from 90.4% to 99.1% and its latency is in the order of milliseconds for average size scripts. Drawing from our experience in deploying ML models in production, we also present an example in which Vamsa helps automatically identify models that are affected by data corruption issues.","",""
4,"Kevin Fauvel, É. Fromont, Véronique Masson, P. Faverdin, A. Termier","Local Cascade Ensemble for Multivariate Data Classification",2020,"","","","",25,"2022-07-13 09:37:52","","","","",,,,,4,2.00,1,5,2,"We present LCE, a Local Cascade Ensemble for traditional (tabular) multivariate data classification, and its extension LCEM for Multivariate Time Series (MTS) classification. LCE is a new hybrid ensemble method that combines an explicit boosting-bagging approach to handle the usual bias-variance tradeoff faced by machine learning models and an implicit divide-and-conquer approach to individualize classifier errors on different parts of the training data. Our evaluation firstly shows that the hybrid ensemble method LCE outperforms the state-of-the-art classifiers on the UCI datasets and that LCEM outperforms the state-of-the-art MTS classifiers on the UEA datasets. Furthermore, LCEM provides explainability by design and manifests robust performance when faced with challenges arising from continuous data collection (different MTS length, missing data and noise).","",""
7,"Sean Saito, Eugene Chua, Nicholas Capel, Rocco Hu","Improving LIME Robustness with Smarter Locality Sampling",2020,"","","","",26,"2022-07-13 09:37:52","","","","",,,,,7,3.50,2,4,2,"Explainability algorithms such as LIME have enabled machine learning systems to adopt transparency and fairness, which are important qualities in commercial use cases. However, recent work has shown that LIME's naive sampling strategy can be exploited by an adversary to conceal biased, harmful behavior. We propose to make LIME more robust by training a generative adversarial network to sample more realistic synthetic data which the explainer uses to generate explanations. Our experiments demonstrate that our proposed method demonstrates an increase in accuracy across three real-world datasets in detecting biased, adversarial behavior compared to vanilla LIME. This is achieved while maintaining comparable explanation quality, with up to 99.94\% in top-1 accuracy in some cases.","",""
2,"A. Garcez, L. Lamb","A I ] 1 0 D ec 2 02 0 Neurosymbolic AI : The 3 rd Wave",2020,"","","","",27,"2022-07-13 09:37:52","","","","",,,,,2,1.00,1,2,2,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.","",""
1,"Kevin Fauvel, 'Elisa Fromont, Véronique Masson, P. Faverdin, A. Termier","XEM: An Explainable Ensemble Method for Multivariate Time Series Classification.",2020,"","","","",28,"2022-07-13 09:37:52","","","","",,,,,1,0.50,0,5,2,"We present XEM, an eXplainable Ensemble method for Multivariate time series classification. XEM relies on a new hybrid ensemble method that combines an explicit boosting-bagging approach to handle the bias-variance trade-off faced by machine learning models and an implicit divide-and-conquer approach to individualize classifier errors on different parts of the training data. Our evaluation shows that XEM outperforms the state-of-the-art MTS classifiers on the UEA datasets. Furthermore, XEM provides faithful explainability by design and manifests robust performance when faced with challenges arising from continuous data collection (different MTS length, missing data and noise).","",""
0,"Andreas Holzinger","From Explainable AI to Human-Centered AI",2020,"","","","",29,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,1,2,"The problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. Their weakness was in dealing with non-linearities and the intrinsic uncertainties of medical data. Advances in data-driven statistical machine learning have led to the current renaissance of AI, but the solutions are becoming increasingly complex and opaque. Due to increasing social, ethical, and legal aspects of AI in medicine, explainable AI (xAI) is attracting much interest within the international research community. While xAI deals with the implementation of transparency and traceability of statistical blackbox machine learning methods, there is a pressing need to go beyond xAI, e.g. to extent explainability with causability. The integrative backbone for this approach is in interactive machine learning with the human-inthe-loop because a human domain expert complements AI with implicit knowledge. Humans are robust, can generalize from few examples, understand relevant representations and concepts and are able to explain causal links between them. Consequently, more research is needed on how human experts explain their decisions by examining their strategies, as they are (but not always) able to describe the underlying explanatory factors. Formalized, these can be used to build structural causal models of human decision making and characteristics can be mapped back to train AI. Finally, such an AI-ecosystem needs advanced Human-AI interfaces, that allow to ask questions of why, but also to ask for counterfactuals, i.e. what-if. This interactivity between human and AI will contribute to enhance robustness, reliability, accountability, fairness and trust in AI and foster ethical responsible machine learning with the human-in-control.","",""
0,"T. Johnsen, J. Gao","Explainable Elastic Net using Time Series to Forecast Number of Daily COVID-19 Cases by Generic Region (Preprint)",2020,"","","","",30,"2022-07-13 09:37:52","","10.2196/preprints.23301","","",,,,,0,0.00,0,2,2,"  BACKGROUND  Current time series models such as Long Short-Term Memory Network (LSTM), Autoregressive Integrated Moving Average Model (ARIMA), and machine learning ensembles have been used to forecast the number of novel daily COVID-19 cases by specific region. The most common are ARIMA models. Lacking in the models are explainability by intuitively showing why and how the forecasts are being made, and the ability to generalize the models to be used in any country or region. Other models that use more novel approaches, lack in ease of use and access to data.      OBJECTIVE  To fill this gap by providing a robust method that is explainable, generic, and easy to use.      METHODS  The presented model, Elastic Net COVID-19 Forecaster or EN-CoF for short, uses a linear regressor trained using elastic net to learn meaningful weights of 17 previous 5-day averages to forecast the next 5-day average for any country.      RESULTS  EN-CoF made novel predictions for 124 countries, up to two weeks in advance, with a coefficient of determination, r2, value of 0.9860. For comparisons, the most commonly use model ARIMA was used on the same data and scored a comparable r2 value of 0.9872.      CONCLUSIONS  EN-CoF results, r2 values, are on par with state-of-the-art methods like ARIMA. The advantages of EN-CoF is that it is generic enough to be used for any country or region, provides intuitive static weights making it more explainable and easier to apply, and can be used without extensive machine learning or other technical knowledge. ","",""
0,"","Edinburgh Research Explorer Why reliabilism is not enough",,"","","","",31,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,0,,"In this paper we argue that standard calls for explainability that focus on the epistemic inscrutability of black-box machine learning models may be misplaced. If we presume, for the sake of this paper, that machine learning can be a source of knowledge, then it makes sense to wonder what kind of justification it involves. How do we rationalize on the one hand the seeming justificatory black box with the observed widespread adoption of machine learning? We argue that, in general, people implicitly adopt reliabilism regard-ing machine learning. Reliabilism is an epistemological theory of epistemic justification according to which a belief is warranted if it has been produced by a reliable process or method [18]. We argue that, in cases where model deployments require moral justification, reliabilism is not sufficient, and instead justifying deployment requires establishing robust human processes as a moral “wrap-per” around machine outputs. We then suggest that, in certain high-stakes domains with moral consequences, reliabilism does not provide another kind of necessary justification—moral justification. Finally, we offer cautions relevant to the (implicit or explicit) adoption of the reliabilist interpretation of machine learning.","",""
1,"Anupam Datta, Matt Fredrikson, Klas Leino, Kaiji Lu, S. Sen, Zifan Wang","Machine Learning Explainability and Robustness: Connected at the Hip",2021,"","","","",32,"2022-07-13 09:37:52","","10.1145/3447548.3470806","","",,,,,1,1.00,0,6,1,"This tutorial examines the synergistic relationship between explainability methods for machine learning and a significant problem related to model quality: robustness against adversarial perturbations. We begin with a broad overview of approaches to explainable AI, before narrowing our focus to post-hoc explanation methods for predictive models. We discuss perspectives on what constitutes a ""good'' explanation in various settings, with an emphasis on axiomatic justifications for various explanation methods. In doing so, we will highlight the importance of an explanation method's faithfulness to the target model, as this property allows one to distinguish between explanations that are unintelligible because of the method used to produce them, and cases where a seemingly poor explanation points to model quality issues. Next, we introduce concepts surrounding adversarial robustness, including adversarial attacks as well as a range of corresponding state-of-the-art defenses. Finally, building on the knowledge presented thus far, we present key insights from the recent literature on the connections between explainability and robustness, showing that many commonly-perceived explainability issues may be caused by non-robust model behavior. Accordingly, a careful study of adversarial examples and robustness can lead to models whose explanations better appeal to human intuition and domain knowledge.","",""
0,"A. Agogino, Ritchie Lee, D. Giannakopoulou","Machine Learning Explainability and Transferability for Path Navigation",2020,"","","","",33,"2022-07-13 09:37:52","","10.2514/6.2021-1885","","",,,,,0,0.00,0,3,2,"Deep neural networks are powerful tools for machine perception. Unfortunately their decisions are difficult to explain due to the complexity and size of the networks. Previously we have alleviated this issue by using the representational portion of a deep neural network and combining it with a :-nearest neighbor (KNN) classifier. Through inspection of the decisions made by the KNN, we can directly see the training data responsible for the decisions, allowing us to determine the quality of the overall decision and the quality of the representational layer of the deep NN. While the technique worked well, it requires tens of thousands of latent vectors to be stored for classification. In addition, it lacks the ability to show how parts of an image influence the classification decision. Here we address these issues by 1) Using a radial basis function network (RBFN) in place of the KNN allowing far fewer images to be used in deployment and 2) Using an autoencoder network for explainability. In addition to these techniques, we examine the effects of transfer learning to determine that results are robust. All results are tested on a domain where an unmanned aerial vehicle (UAV) navigates a forest trail through a single camera.","",""
102,"Nadia Burkart, M. Huber","A Survey on the Explainability of Supervised Machine Learning",2020,"","","","",34,"2022-07-13 09:37:52","","10.1613/jair.1.12228","","",,,,,102,51.00,51,2,2,"Predictions obtained by, e.g., artificial neural networks have a high accuracy but humans often perceive the models as black boxes. Insights about the decision making are mostly opaque for humans. Particularly understanding the decision making in highly sensitive areas such as healthcare or finance, is of paramount importance. The decision-making behind the black boxes requires it to be more transparent, accountable, and understandable for humans. This survey paper provides essential definitions, an overview of the different principles and methodologies of explainable Supervised Machine Learning (SML). We conduct a state-of-the-art survey that reviews past and recent explainable SML approaches and classifies them according to the introduced definitions. Finally, we illustrate principles by means of an explanatory case study and discuss important future directions.","",""
0,"Ou Wu, Weiyao Zhu, Yingjun Deng, Haixiang Zhang, Qinghu Hou","A Mathematical Foundation for Robust Machine Learning based on Bias-Variance Trade-off",2021,"","","","",35,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,5,1,"A common assumption in machine learning is that samples are independently and identically distributed (i.i.d). However, the contributions of different samples are not identical in training. Some samples are difficult to learn and some samples are noisy. The unequal contributions of samples has a considerable effect on training performances. Studies focusing on unequal sample contributions (e.g., easy, hard, noisy) in learning usually refer to these contributions as robust machine learning (RML). Weighing and regularization are two common techniques in RML. Numerous learning algorithms have been proposed but the strategies for dealing with easy/hard/noisy samples differ or even contradict with different learning algorithms. For example, some strategies take the hard samples first, whereas some strategies take easy first. Conducting a clear comparison for existing RML algorithms in dealing with different samples is difficult due to lack of a unified theoretical framework for RML. This study attempts to construct a mathematical foundation for RML based on the bias-variance trade-off theory. A series of definitions and properties are presented and proved. Several classical learning algorithms are also explained and compared. Improvements of existing methods are obtained based on the comparison. A unified method that combines two classical learning strategies is proposed.","",""
55,"Samantha Joel, Paul W. Eastwick, C. J. Allison, X. Arriaga, Zachary G. Baker, E. Bar-Kalifa, S. Bergeron, G. Birnbaum, R. Brock, C. Brumbaugh, Cheryl L. Carmichael, Serena Chen, Jennifer A. Clarke, Rebecca J. Cobb, Michael K. Coolsen, Jody L. Davis, David C de Jong, Anik Debrot, E. DeHaas, Jaye L. Derrick, Jami Eller, Marie-Joelle Estrada, Ruddy Faure, E. Finkel, R. C. Fraley, S. Gable, Reuma Gadassi-Polack, Yuthika U. Girme, Amie M. Gordon, Courtney Gosnell, Matthew D. Hammond, P. Hannon, Cheryl Harasymchuk, W. Hofmann, A. Horn, E. Impett, Jeremy P Jamieson, D. Keltner, James J Kim, Jeffrey L. Kirchner, E. Kluwer, M. Kumashiro, Grace M Larson, Gal Lazarus, Jill M. Logan, Laura B. Luchies, G. Macdonald, Laura V. Machia, Michael R Maniaci, J. Maxwell","Machine learning uncovers the most robust self-report predictors of relationship quality across 43 longitudinal couples studies",2020,"","","","",36,"2022-07-13 09:37:52","","10.1073/pnas.1917036117","","",,,,,55,27.50,6,50,2,"Significance What predicts how happy people are with their romantic relationships? Relationship science—an interdisciplinary field spanning psychology, sociology, economics, family studies, and communication—has identified hundreds of variables that purportedly shape romantic relationship quality. The current project used machine learning to directly quantify and compare the predictive power of many such variables among 11,196 romantic couples. People’s own judgments about the relationship itself—such as how satisfied and committed they perceived their partners to be, and how appreciative they felt toward their partners—explained approximately 45% of their current satisfaction. The partner’s judgments did not add information, nor did either person’s personalities or traits. Furthermore, none of these variables could predict whose relationship quality would increase versus decrease over time. Given the powerful implications of relationship quality for health and well-being, a central mission of relationship science is explaining why some romantic relationships thrive more than others. This large-scale project used machine learning (i.e., Random Forests) to 1) quantify the extent to which relationship quality is predictable and 2) identify which constructs reliably predict relationship quality. Across 43 dyadic longitudinal datasets from 29 laboratories, the top relationship-specific predictors of relationship quality were perceived-partner commitment, appreciation, sexual satisfaction, perceived-partner satisfaction, and conflict. The top individual-difference predictors were life satisfaction, negative affect, depression, attachment avoidance, and attachment anxiety. Overall, relationship-specific variables predicted up to 45% of variance at baseline, and up to 18% of variance at the end of each study. Individual differences also performed well (21% and 12%, respectively). Actor-reported variables (i.e., own relationship-specific and individual-difference variables) predicted two to four times more variance than partner-reported variables (i.e., the partner’s ratings on those variables). Importantly, individual differences and partner reports had no predictive effects beyond actor-reported relationship-specific variables alone. These findings imply that the sum of all individual differences and partner experiences exert their influence on relationship quality via a person’s own relationship-specific experiences, and effects due to moderation by individual differences and moderation by partner-reports may be quite small. Finally, relationship-quality change (i.e., increases or decreases in relationship quality over the course of a study) was largely unpredictable from any combination of self-report variables. This collective effort should guide future models of relationships.","",""
1,"Hali Lindsay, J. Tröger, A. König","Language Impairment in Alzheimer’s Disease—Robust and Explainable Evidence for AD-Related Deterioration of Spontaneous Speech Through Multilingual Machine Learning",2021,"","","","",37,"2022-07-13 09:37:52","","10.3389/fnagi.2021.642033","","",,,,,1,1.00,0,3,1,"Alzheimer’s disease (AD) is a pervasive neurodegenerative disease that affects millions worldwide and is most prominently associated with broad cognitive decline, including language impairment. Picture description tasks are routinely used to monitor language impairment in AD. Due to the high amount of manual resources needed for an in-depth analysis of thereby-produced spontaneous speech, advanced natural language processing (NLP) combined with machine learning (ML) represents a promising opportunity. In this applied research field though, NLP and ML methodology do not necessarily ensure robust clinically actionable insights into cognitive language impairment in AD and additional precautions must be taken to ensure clinical-validity and generalizability of results. In this study, we add generalizability through multilingual feature statistics to computational approaches for the detection of language impairment in AD. We include 154 participants (78 healthy subjects, 76 patients with AD) from two different languages (106 English speaking and 47 French speaking). Each participant completed a picture description task, in addition to a battery of neuropsychological tests. Each response was recorded and manually transcribed. From this, task-specific, semantic, syntactic and paralinguistic features are extracted using NLP resources. Using inferential statistics, we determined language features, excluding task specific features, that are significant in both languages and therefore represent “generalizable” signs for cognitive language impairment in AD. In a second step, we evaluated all features as well as the generalizable ones for English, French and both languages in a binary discrimination ML scenario (AD vs. healthy) using a variety of classifiers. The generalizable language feature set outperforms the all language feature set in English, French and the multilingual scenarios. Semantic features are the most generalizable while paralinguistic features show no overlap between languages. The multilingual model shows an equal distribution of error in both English and French. By leveraging multilingual statistics combined with a theory-driven approach, we identify AD-related language impairment that generalizes beyond a single corpus or language to model language impairment as a clinically-relevant cognitive symptom. We find a primary impairment in semantics in addition to mild syntactic impairment, possibly confounded by additional impaired cognitive functions.","",""
34,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, Semeen Rehman, M. Shafique","Robust Machine Learning Systems: Reliability and Security for Deep Neural Networks",2018,"","","","",38,"2022-07-13 09:37:52","","10.1109/IOLTS.2018.8474192","","",,,,,34,8.50,7,5,4,"Machine learning is commonly being used in almost all the areas that involve advanced data analytics and intelligent control. From applications like Natural Language Processing (NLP) to autonomous driving are based upon machine learning algorithms. An increasing trend is observed in the use of Deep Neural Networks (DNNs) for such applications. While the slight inaccuracy in applications like NLP does not have any severe consequences, it is not the same for other safety-critical applications, like autonomous driving and smart healthcare, where a small error can lead to catastrophic effects. Apart from high-accuracy DNN algorithms, there is a significant need for robust machine learning systems and hardware architectures that can generate reliable and trustworthy results in the presence of hardware-level faults while also preserving security and privacy. This paper provides an overview of the challenges being faced in ensuring reliable and secure execution of DNNs. To address the challenges, we present several techniques for analyzing and mitigating the reliability and security threats in machine learning systems.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",39,"2022-07-13 09:37:52","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
8,"Daniil Bash, Yongqiang Cai, Vijila Chellappan, S. L. Wong, Yang Xu, Pawan Kumar, J. Tan, Anas Abutaha, J. Cheng, Y. Lim, S. Tian, D. Ren, Flore Mekki-Barrada, W. Wong, J. Kumar, Saif A. Khan, Qianxiao Li, T. Buonassisi, K. Hippalgaonkar","Machine Learning and High-Throughput Robust Design of P3HT-CNT Composite Thin Films for High Electrical Conductivity",2020,"","","","",40,"2022-07-13 09:37:52","","10.26434/chemrxiv.13265288.v1","","",,,,,8,4.00,1,19,2,"Combining high-throughput experiments with machine learning allows quick optimization of parameter spaces towards achieving target properties. In this study, we demonstrate that machine learning, combined with multi-labeled datasets, can additionally be used for scientific understanding and hypothesis testing. We introduce an automated flow system with high-throughput drop-casting for thin film preparation, followed by fast characterization of optical and electrical properties, with the capability to complete one cycle of learning of fully labeled ~160 samples in a single day. We combine regio-regular poly-3-hexylthiophene with various carbon nanotubes to achieve electrical conductivities as high as 1200 S/cm. Interestingly, a non-intuitive local optimum emerges when 10% of double-walled carbon nanotubes are added with long single wall carbon nanotubes, where the conductivity is seen to be as high as 700 S/cm, which we subsequently explain with high fidelity optical characterization. Employing dataset resampling strategies and graph-based regressions allows us to account for experimental cost and uncertainty estimation of correlated multi-outputs, and supports the proving of the hypothesis linking charge delocalization to electrical conductivity. We therefore present a robust machine-learning driven high-throughput experimental scheme that can be applied to optimize and understand properties of composites, or hybrid organic-inorganic materials.","",""
5,"J. Papenbrock, P. Schwendner, Markus Jaeger, Stephan Krügel","Matrix Evolutions: Synthetic Correlations and Explainable Machine Learning for Constructing Robust Investment Portfolios",2020,"","","","",41,"2022-07-13 09:37:52","","10.2139/ssrn.3663220","","",,,,,5,2.50,1,4,2,"In this article, the authors present a novel and highly flexible concept to simulate correlation matrixes of financial markets. It produces realistic outcomes regarding stylized facts of empirical correlation matrixes and requires no asset return input data. The matrix generation is based on a multiobjective evolutionary algorithm, so the authors call the approach matrix evolutions. It is suitable for parallel implementation and can be accelerated by graphics processing units and quantum-inspired algorithms. The approach is useful for backtesting, pricing, and hedging correlation-dependent investment strategies and financial products. Its potential is demonstrated in a machine learning case study for robust portfolio construction in a multi-asset universe: An explainable machine learning program links the synthetic matrixes to the portfolio volatility spread of hierarchical risk parity versus equal risk contribution. TOPICS: Statistical methods, big data/machine learning, portfolio construction, performance measurement Key Findings ▪ The authors introduce the matrix evolutions concept based on an evolutionary algorithm to simulate correlation matrixes useful for financial market applications. ▪ They apply the resulting synthetic correlation matrixes to benchmark hierarchical risk parity (HRP) and equal risk contribution allocations of a multi-asset futures portfolio and find HRP to show lower portfolio risk. ▪ The authors evaluate three competing machine learning methods to regress the portfolio risk spread between both allocation methods against statistical features of the synthetic correlation matrixes and then discuss the local and global feature importance using the SHAP framework by Lundberg and Lee (2017).","",""
1,"M. Quade, Thomas Isele, Markus Abel","Explainable Machine Learning Control - robust control and stability analysis",2020,"","","","",42,"2022-07-13 09:37:52","","","","",,,,,1,0.50,0,3,2,"Recently, the term explainable AI became known as an approach to produce models from artificial intelligence which allow interpretation. Since a long time, there are models of symbolic regression in use that are perfectly explainable and mathematically tractable: in this contribution we demonstrate how to use symbolic regression methods to infer the optimal control of a dynamical system given one or several optimization criteria, or cost functions. In previous publications, network control was achieved by automatized machine learning control using genetic programming. Here, we focus on the subsequent analysis of the analytical expressions which result from the machine learning. In particular, we use AUTO to analyze the stability properties of the controlled oscillator system which served as our model. As a result, we show that there is a considerable advantage of explainable models over less accessible neural networks.","",""
0,"Carolina Natel de Moura, J. Seibert, Miriam Rita Moro Mine, Ricardo Carvalho de Almeida","Are Machine Learning methods robust enough for hydrological modeling under changing conditions?",2020,"","","","",43,"2022-07-13 09:37:52","","10.5194/egusphere-egu2020-690","","",,,,,0,0.00,0,4,2,"  <p>The advancement of big data and increased computational power have contributed to an increased use of Machine Learning (ML) approaches in hydrological modelling. These approaches are powerful tools for modeling non-linear systems. However, the applicability of ML in non-stationary conditions needs to be studied further. As climate change will change hydrological patterns, testing ML approaches for non-stationary conditions is essential. Here, we used the Differential Split-Sample Test (DSST) to test the climate transposability of ML approaches (e.g., calibrating in a wet period and validating in a dry one, and vice-versa).&#160; We applied five ML approaches using daily precipitation and temperature as input for the prediction of the daily discharge in six snow-dominated Swiss catchments. Lower and upper benchmarks were used to evaluate performances through a relative performance measure. The lower benchmark is the average of the bucket-type HBV model runs from 1000 random parameter sets. The upper benchmark is the automatically calibrated HBV model. In comparison with the stationary condition, the models performed slightly poorer in the non-stationary condition. The performance of simple ML approaches was poor for non-stationary conditions with an underestimation of peak flows, as well as a poor representation of the snow-melting period. On the other hand, a more complex ML approach (deep learning), the Long Short -Term Memory (LSTM), showed a good performance when compared with the lower and upper benchmarks. This might be explained by the fact that the so-called memory cell allowed to simulate the storage effects.&#160;</p> ","",""
11,"M. Kovalev, L. Utkin","A robust algorithm for explaining unreliable machine learning survival models using the Kolmogorov-Smirnov bounds",2020,"","","","",44,"2022-07-13 09:37:52","","10.1016/j.neunet.2020.08.007","","",,,,,11,5.50,6,2,2,"","",""
34,"A. Binder, M. Bockmayr, M. Hägele, S. Wienert, D. Heim, Katharina Hellweg, M. Ishii, A. Stenzinger, A. Hocke, C. Denkert, K. Müller, F. Klauschen","Morphological and molecular breast cancer profiling through explainable machine learning",2021,"","","","",45,"2022-07-13 09:37:52","","10.1038/S42256-021-00303-4","","",,,,,34,34.00,3,12,1,"","",""
0,"Supatcha Lertampaiporn, A. Hongsthong, Warin Wattanapornprom, C. Thammarongtham","Ensemble-AHTPpred: A Robust Ensemble Machine Learning Model Integrated With a New Composite Feature for Identifying Antihypertensive Peptides",2022,"","","","",46,"2022-07-13 09:37:52","","10.3389/fgene.2022.883766","","",,,,,0,0.00,0,4,1,"Hypertension or elevated blood pressure is a serious medical condition that significantly increases the risks of cardiovascular disease, heart disease, diabetes, stroke, kidney disease, and other health problems, that affect people worldwide. Thus, hypertension is one of the major global causes of premature death. Regarding the prevention and treatment of hypertension with no or few side effects, antihypertensive peptides (AHTPs) obtained from natural sources might be useful as nutraceuticals. Therefore, the search for alternative/novel AHTPs in food or natural sources has received much attention, as AHTPs may be functional agents for human health. AHTPs have been observed in diverse organisms, although many of them remain underinvestigated. The identification of peptides with antihypertensive activity in the laboratory is time- and resource-consuming. Alternatively, computational methods based on robust machine learning can identify or screen potential AHTP candidates prior to experimental verification. In this paper, we propose Ensemble-AHTPpred, an ensemble machine learning algorithm composed of a random forest (RF), a support vector machine (SVM), and extreme gradient boosting (XGB), with the aim of integrating diverse heterogeneous algorithms to enhance the robustness of the final predictive model. The selected feature set includes various computed features, such as various physicochemical properties, amino acid compositions (AACs), transitions, n-grams, and secondary structure-related information; these features are able to learn more information in terms of analyzing or explaining the characteristics of the predicted peptide. In addition, the tool is integrated with a newly proposed composite feature (generated based on a logistic regression function) that combines various feature aspects to enable improved AHTP characterization. Our tool, Ensemble-AHTPpred, achieved an overall accuracy above 90% on independent test data. Additionally, the approach was applied to novel experimentally validated AHTPs, obtained from recent studies, which did not overlap with the training and test datasets, and the tool could precisely predict these AHTPs.","",""
0,"Mengdi Huai","Fostering Trustworthiness in Machine Learning via Robust and Automated Model Interpretation",2022,"","","","",47,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,1,1,"Machine learning models have been widely applied in real world to build intelligent systems (e.g., selfdriving cars, intelligent recommendation systems, and clinical decision support systems). However, traditional machine learning models mainly focus on optimizing accuracy and efficiency, and they fail to consider how to foster trustworthiness in their design. In practice, machine learning models are suffering a crisis of trust when they are applied in real-world applications due to the lack of transparency behind their behaviors. The concern about the “black box” nature of machine learning models makes decision makers reluctant to trust the predicted results, especially when these models are used for making critical decisions (e.g., medical disease diagnosis). In this talk, I will introduce my research efforts towards the goal of making machine learning trustworthy. Specifically, I will discuss how to foster trustworthiness in machine learning via robust and automated model interpretation. I will first describe my recent research on the security vulnerability of model interpretation methods for deep reinforcement learning (DRL) and introduce two malicious attack frameworks that can significantly alter the interpretation results while incurring minor damage to the performance of the original DRL model. Then, I will present an automated and robust model interpretation framework, which can not only automatically generate the concept-based explanations for the predicted results but also provide certified robustness guarantees for the generated explanations. Bio: Mengdi Huai is a Ph.D. candidate in the Department of Computer Science at the University of Virginia. Her research interests lie in the areas of data mining and machine learning, with a current focus on developing novel techniques to build trustworthy learning systems that are explainable, robust, private, and fair. Mengdi is also interested in designing effective data mining and machine learning algorithms to deal with complex data with both strong empirical performance and theoretical guarantees. Her research work has been published in various top-tier venues, such as KDD, AAAI, IJCAI, NeurIPS, and TKDD. Mengdi received multiple prestigious awards from the University of Virginia for her excellence in research, including the Sture G. Olsson Fellowship in Engineering and the John A. Stankovic Research Award. Her recent work on malicious attacks against model interpretation won the Best Paper Runner-up of KDD2020. Mengdi was selected as one of the Rising Stars in EECS at MIT. She was also selected as one of the Rising Stars in Data Science at UChicago.","",""
1,"A. Rafay, M. Suleman, Affan Alim","Robust Review Rating Prediction Model based on Machine and Deep Learning: Yelp Dataset",2020,"","","","",48,"2022-07-13 09:37:52","","10.1109/ICETST49965.2020.9080713","","",,,,,1,0.50,0,3,2,"Public reviews for a business are very important and help the business to measure the quality and excellence in different directions which leads to predict the worth of a business in the market. In other words, reviews have a very high impact on business revenue. In this paper, we focus on reviews for all kinds of restaurants business and have proposed a sentiment analysis and opinion mining model to perform the classification on business reviews. In order to achieve robust results both binary and multilabel classification are used used by using a large and rich text reviews dataset provided by Yelp Dataset Challenge round -13. Extensive and series of experiments have been done and compare the results of a machine learning based algorithm “Multinomial Naive Bayes” and deep learning algorithm “convolution Long Short Term Memory'” (CLSTM) with word2vec and Global Vector (Glove). After analyzing the performance of each model with different metrics, it has been observed that the best model for classifying the review ratings is CLSTM. We have also found the role of bias in the machine and its importance in explaining the performance differences observed on specific problems.","",""
243,"Umang Bhatt, Alice Xiang, Shubham Sharma, Adrian Weller, Ankur Taly, Yunhan Jia, Joydeep Ghosh, R. Puri, J. Moura, P. Eckersley","Explainable machine learning in deployment",2019,"","","","",49,"2022-07-13 09:37:52","","10.1145/3351095.3375624","","",,,,,243,81.00,24,10,3,"Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.","",""
224,"R. Roscher, B. Bohn, Marco F. Duarte, J. Garcke","Explainable Machine Learning for Scientific Insights and Discoveries",2019,"","","","",50,"2022-07-13 09:37:52","","10.1109/ACCESS.2020.2976199","","",,,,,224,74.67,56,4,3,"Machine learning methods have been remarkably successful for a wide range of application areas in the extraction of essential information from data. An exciting and relatively recent development is the uptake of machine learning in the natural sciences, where the major goal is to obtain novel scientific insights and discoveries from observational or simulated data. A prerequisite for obtaining a scientific outcome is domain knowledge, which is needed to gain explainability, but also to enhance scientific consistency. In this article, we review explainable machine learning in view of applications in the natural sciences and discuss three core elements that we identified as relevant in this context: transparency, interpretability, and explainability. With respect to these core elements, we provide a survey of recent scientific works that incorporate machine learning and the way that explainable machine learning is used in combination with domain knowledge from the application areas.","",""
11,"B. Celik, J. Vanschoren","Adaptation Strategies for Automated Machine Learning on Evolving Data",2020,"","","","",51,"2022-07-13 09:37:52","","10.1109/TPAMI.2021.3062900","","",,,,,11,5.50,6,2,2,"Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust to changes in the underlying data. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on a variety of AutoML approaches for building machine learning pipelines, including Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.","",""
0,"Adrià Soldevila Coma","Robust leak localization in water distribution networks using machine learning techniques",2018,"","","","",52,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,1,4,"This PhD thesis presents a methodology to detect, estimate and localize water leaks (with the main focus in the localization problem) in water distribution networks using hydraulic models and machine learning techniques. The actual state of the art is introduced, the theoretical basis of the machine learning techniques applied are explained and the hydraulic model is also detailed. The whole methodology is presented and tested into different water distribution networks and district metered areas based on simulated and real case studies and compared with published methods. The focus of the contributions is to bring more robust methods against the uncertainties that effects the problem of leak detection, by dealing with them using the self-similarity to create features monitored by the change detection technique intersection-of-confidence-interval, and the leak localization where the problem is tackled using machine learning techniques. By using those techniques, it is expected to learn the leak behavior considering their uncertainty to be used in the diagnosis stage after the training phase. One method for the leak detection problem is presented that is able to estimate the leak size and the time that the leak has been produced. This method captures the normal, leak-free, behavior and contrast it with the new measurements in order to evaluate the state of the network. If the behavior is not normal check if it is due to a leak. To have a more robust leak detection method, a specific validation is designed to operate specifically with leaks and in the temporal region where the leak is most apparent. A methodology to extent the current model-based approach to localize water leaks by means of classifiers is proposed where the non-parametric k-nearest neighbors classifier and the parametric multi-class Bayesian classifier are proposed. A new data-driven approach to localize leaks using a multivariate regression technique without the use of hydraulic models is also introduced. This method presents a clear benefit over the model-based technique by removing the need of the hydraulic model despite of the topological information is still required. Also, the information of the expected leaks is not required since information of the expected hydraulic behavior with leak is exploited to find the place where the leak is more suitable. This method has a good performance in practice, but is very sensitive to the number of sensor in the network and their sensor placement. The proposed sensor placement techniques reduce the computational load required to take into account the amount of data needed to model the uncertainty compared with other optimization approaches while are designed to work with the leak localization problem. More precisely, the proposed hybrid feature selection technique for sensor placement is able to work with any method that can be evaluated with confusion matrix and still being specialized for the leak localization task. This last method is good for a few sensors, but lacks of precision when the number of sensors to place is large. To overcome this problem an incremental sensor placement is proposed which is better for a larger number of sensors to place but worse when the number is small.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",53,"2022-07-13 09:37:52","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
7,"D. Jacob","CATE meets ML - Conditional Average Treatment Effect and Machine Learning",2021,"","","","",54,"2022-07-13 09:37:52","","10.2139/ssrn.3816558","","",,,,,7,7.00,7,1,1,"For treatment effects - one of the core issues in modern econometric analysis - prediction and estimation are two sides of the same coin. As it turns out, machine learning methods are the tool for generalized prediction models. Combined with econometric theory, they allow us to estimate not only the average but a personalized treatment effect - the conditional average treatment effect (CATE). In this tutorial, we give an overview of novel methods, explain them in detail, and apply them via Quantlets in real data applications. We study the effect that microcredit availability has on the amount of money borrowed and if 401(k) pension plan eligibility has an impact on net financial assets, as two empirical examples. The presented toolbox of methods contains meta-learners, like the Doubly-Robust, R-, T- and X-learner, and methods that are specially designed to estimate the CATE like the causal BART and the generalized random forest. In both, the microcredit and 401(k) example, we find a positive treatment effect for all observations but conflicting evidence of treatment effect heterogeneity. An additional simulation study, where the true treatment effect is known, allows us to compare the different methods and to observe patterns and similarities.","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",55,"2022-07-13 09:37:52","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
4,"Guillaume Vidot, Christophe Gabreau, I. Ober, Iulian Ober","Certification of embedded systems based on Machine Learning: A survey",2021,"","","","",56,"2022-07-13 09:37:52","","","","",,,,,4,4.00,1,4,1,"Advances in machine learning (ML) open the way to innovating functions in the avionic domain, such as navigation/surveillance assistance (e.g. vision-based navigation, obstacle sensing, virtual sensing), speechto-text applications, autonomous flight, predictive maintenance or cockpit assistance. Current certification standards and practices, which were defined and refined decades over decades with classical programming in mind, do not however support this new development paradigm. This article provides an overview of the main challenges raised by the use ML in the demonstration of compliance with regulation requirements, and a survey of literature relevant to these challenges, with particular focus on the issues of robustness and explainability of ML results.","",""
4,"Alexandra Renouard, A. Maggi, M. Grunberg, C. Doubre, C. Hibert","Toward False Event Detection and Quarry Blast versus Earthquake Discrimination in an Operational Setting Using Semiautomated Machine Learning",2021,"","","","",57,"2022-07-13 09:37:52","","10.1785/0220200305","","",,,,,4,4.00,1,5,1,"  Small-magnitude earthquakes shed light on the spatial and magnitude distribution of natural seismicity, as well as its rate and occurrence, especially in stable continental regions where natural seismicity remains difficult to explain under slow strain-rate conditions. However, capturing them in catalogs is strongly hindered by signal-to-noise ratio issues, resulting in high rates of false and man-made events also being detected. Accurate and robust discrimination of these events is critical for optimally detecting small earthquakes. This requires uncovering recurrent salient features that can rapidly distinguish first false events from real events, then earthquakes from man-made events (mainly quarry blasts), despite high signal variability and noise content. In this study, we combined the complementary strengths of human and interpretable rule-based machine-learning algorithms for solving this classification problem. We used human expert knowledge to co-create two reliable machine-learning classifiers through human-assisted selection of classification features and review of events with uncertain classifier predictions. The two classifiers are integrated into the SeisComP3 operational monitoring system. The first one discards false events from the set of events obtained with a low short-term average/long-term average threshold; the second one labels the remaining events as either earthquakes or quarry blasts. When run in an operational setting, the first classifier correctly detected more than 99% of false events and just over 93% of earthquakes; the second classifier correctly labeled 95% of quarry blasts and 96% of earthquakes. After a manual review of the second classifier low-confidence outputs, the final catalog contained fewer than 2% of misclassified events. These results confirm that machine learning strengthens the quality of earthquake catalogs and that the performance of machine-learning classifiers can be improved through human expertise. Our study promotes a broader implication of hybrid intelligence monitoring within seismological observatories.","",""
4,"S. Newman, R. Furbank","Explainable machine learning models of major crop traits from satellite-monitored continent-wide field trial data.",2021,"","","","",58,"2022-07-13 09:37:52","","10.1038/s41477-021-01001-0","","",,,,,4,4.00,2,2,1,"","",""
1,"M. A. Al Janabi","Optimization algorithms and investment portfolio analytics with machine learning techniques under time-varying liquidity constraints",2021,"","","","",59,"2022-07-13 09:37:52","","10.1108/JM2-10-2020-0259","","",,,,,1,1.00,1,1,1," Purpose This paper aims to examine from commodity portfolio managers’ perspective the performance of liquidity adjusted risk modeling in assessing the market risk parameters of a large commodity portfolio and in obtaining efficient and coherent portfolios under different market circumstances.   Design/methodology/approach The implemented market risk modeling algorithm and investment portfolio analytics using reinforcement machine learning techniques can simultaneously handle risk-return characteristics of commodity investments under regular and crisis market settings besides considering the particular effects of the time-varying liquidity constraints of the multiple-asset commodity portfolios.   Findings In particular, the paper implements a robust machine learning method to commodity optimal portfolio selection and within a liquidity-adjusted value-at-risk (LVaR) framework. In addition, the paper explains how the adapted LVaR modeling algorithms can be used by a commodity trading unit in a dynamic asset allocation framework for estimating risk exposure, assessing risk reduction alternates and creating efficient and coherent market portfolios.   Originality/value The optimization parameters subject to meaningful operational and financial constraints, investment portfolio analytics and empirical results can have important practical uses and applications for commodity portfolio managers particularly in the wake of the 2007–2009 global financial crisis. In addition, the recommended reinforcement machine learning optimization algorithms can aid in solving some real-world dilemmas under stressed and adverse market conditions (e.g. illiquidity, switching in correlations factors signs, nonlinear and non-normal distribution of assets’ returns) and can have key applications in machine learning, expert systems, smart financial functions, internet of things (IoT) and financial technology (FinTech) in big data ecosystems. ","",""
44,"Min Zhang, Haoxuan You, Pranav Kadam, Shan Liu, C.-C. Jay Kuo","PointHop: An Explainable Machine Learning Method for Point Cloud Classification",2019,"","","","",60,"2022-07-13 09:37:52","","10.1109/TMM.2019.2963592","","",,,,,44,14.67,9,5,3,"An explainable machine learning method for point cloud classification, called the PointHop method, is proposed in this work. The PointHop method consists of two stages: 1) local-to-global attribute building through iterative one-hop information exchange and 2) classification and ensembles. In the attribute building stage, we address the problem of unordered point cloud data using a space partitioning procedure and developing a robust descriptor that characterizes the relationship between a point and its one-hop neighbor in a PointHop unit. When we put multiple PointHop units in cascade, the attributes of a point will grow by taking its relationship with one-hop neighbor points into account iteratively. Furthermore, to control the rapid dimension growth of the attribute vector associated with a point, we use the Saab transform to reduce the attribute dimension in each PointHop unit. In the classification and ensemble stage, we feed the feature vector obtained from multiple PointHop units to a classifier. We explore ensemble methods to improve the classification performance furthermore. It is shown by experimental results that the PointHop method offers classification performance that is comparable with state-of-the-art methods while demanding much lower training complexity.","",""
0,"A. Nair, F. Yu, P. C. Jost, P. DeMott, E. Levin, J. Jimenez, J. Peischl, I. Pollack, C. Fredrickson, A. Beyersdorf, B. Nault, Minsu Park, S. Yum, B. Palm, Lu Xu, I. Bourgeois, B. Anderson, A. Nenes, L. Ziemba, R. Moore, Taehyoung Lee, T. Park, C. Thompson, F. Flocke, L. Huey, Michelle J. Kim, Q. Peng","Machine learning uncovers aerosol size information from chemistry and meteorology to quantify potential cloud-forming particles",2021,"","","","",61,"2022-07-13 09:37:52","","10.21203/rs.3.rs-244416/v1","","",,,,,0,0.00,0,27,1,"  Cloud condensation nuclei (CCN) are mediators of aerosol–cloud interactions, which contribute to the largest uncertainty in climate change prediction. Here, we present a machine learning/artificial intelligence model that quantifies CCN from variables of aerosol composition, atmospheric trace gases, and meteorology. Comprehensive multi-campaign airborne measurements, covering varied physicochemical regimes in the troposphere, confirm the validity of and help probe the inner workings of this machine learning model: revealing for the first time that different ranges of atmospheric aerosol composition and mass correspond to distinct aerosol number size distributions. Machine learning extracts this information, important for accurate quantification of CCN, additionally from both chemistry and meteorology. This can provide a physicochemically explainable, computationally efficient, robust machine learning pathway in global climate models that only resolve aerosol composition; potentially mitigating the uncertainty of effective radiative forcing due to aerosol–cloud interactions (ERFaci) and improving confidence in assessment of anthropogenic contributions and climate change projections.","",""
3,"Jean-Jacques Ohana, Steve Ohana, E. Benhamou, D. Saltiel, B. Guez","Explainable AI Models of Stock Crashes: A Machine-Learning Explanation of the Covid March 2020 Equity Meltdown",2021,"","","","",62,"2022-07-13 09:37:52","","10.2139/ssrn.3809308","","",,,,,3,3.00,1,5,1,"We consider a gradient boosting decision trees (GBDT) approach to predict large S&P 500 price drops from a set of 150 technical, fundamental and macroeconomic features. We report an improved accuracy of GBDT over other machine learning (ML) methods on the S&P 500 futures prices. We show that retaining fewer and carefully selected features provides improvements across all ML approaches. Shapley values have recently been introduced from game theory to the field of ML. They allow for a robust identification of the most important variables predicting stock market crises, and of a local explanation of the crisis probability at each date, through a consistent features attribution. We apply this methodology to analyze in detail the March 2020 financial meltdown, for which the model offered a timely out of sample prediction. This analysis unveils in particular the contrarian predictive role of the tech equity sector before and after the crash.","",""
3,"F. Biessmann, D. Refiano","Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated",2021,"","","","",63,"2022-07-13 09:37:52","","","","",,,,,3,3.00,2,2,1,"The field explainable artificial intelligence (XAI) has brought about an arsenal of methods to render Machine Learning (ML) predictions more interpretable. But how useful explanations provided by transparent ML methods are for humans remains difficult to assess. Here we investigate the quality of interpretable computer vision algorithms using techniques from psychophysics. In crowdsourced annotation tasks we study the impact of different interpretability approaches on annotation accuracy and task time. We compare these quality metrics with classical XAI, automated quality metrics. Our results demonstrate that psychophysical experiments allow for robust quality assessment of transparency in machine learning. Interestingly the quality metrics computed without humans in the loop did not provide a consistent ranking of interpretability methods nor were they representative for how useful an explanation was for humans. These findings highlight the potential of methods from classical psychophysics for modern machine learning applications. We hope that our results provide convincing arguments for evaluating interpretability in its natural habitat, human-ML interaction, if the goal is to obtain an authentic assessment of interpretability.","",""
2,"Zifan Xu, Xuesu Xiao, Garrett A. Warnell, Anirudh Nair, P. Stone","Machine Learning Methods for Local Motion Planning: A Study of End-to-End vs. Parameter Learning",2021,"","","","",64,"2022-07-13 09:37:52","","10.1109/SSRR53300.2021.9597689","","",,,,,2,2.00,0,5,1,"While decades of research efforts have been devoted to developing classical autonomous navigation systems to move robots from one point to another in a collision-free manner, machine learning approaches to navigation have been recently proposed to learn navigation behaviors from data. Two representative paradigms are end-to-end learning (directly from perception to motion) and parameter learning (from perception to parameters used by a classical underlying planner). These two types of methods are believed to have complementary pros and cons: parameter learning is expected to be robust to different scenarios, have provable guarantees, and exhibit explainable behaviors; end-to-end learning does not require extensive engineering and has the potential to outperform approaches that rely on classical systems. However, these beliefs have not been verified through real-world experiments in a comprehensive way. In this paper, we report on an extensive study to compare end-to-end and parameter learning for local motion planners in a large suite of simulated and physical experiments. In particular, we test the performance of end-to-end motion policies, which directly compute raw motor commands, and parameter policies, which compute parameters to be used by classical planners, with different inputs (e.g., raw sensor data, costmaps), and provide an analysis of the results.","",""
2,"S. Newman, R. Furbank","Explainable machine learning models of major crop traits from satellite-monitored continent-wide field trial data",2021,"","","","",65,"2022-07-13 09:37:52","","10.1101/2021.03.08.434495","","",,,,,2,2.00,1,2,1,"Four species of grass generate half of all human-consumed calories1. However, abundant biological data on species that produce our food remains largely inaccessible, imposing direct barriers to understanding crop yield and fitness traits. Here, we assemble and analyse a continent-wide database of field experiments spanning ten years and hundreds of thousands of machine-phenotyped populations of ten major crop species. Training an ensemble of machine learning models, using thousands of variables capturing weather, ground-sensor, soil, chemical and fertiliser dosage, management, and satellite data, produces robust cross-continent yield models exceeding R2 = 0.8 prediction accuracy. In contrast to ‘black box’ analytics, detailed interrogation of these models reveals fundamental drivers of crop behaviour and complex interactions predicting yield and agronomic traits. These results demonstrate the capacity of machine learning models to build unified, interpretable, and explainable models of crop behaviour, and highlight the powerful role of data in the future of food.","",""
1,"Alina Oprea","Machine Learning Integrity and Privacy in Adversarial Environments",2021,"","","","",66,"2022-07-13 09:37:52","","10.1145/3450569.3462164","","",,,,,1,1.00,1,1,1,"Machine learning is increasingly being used for automated decisions in applications such as health care, finance, autonomous vehicles, and personalized recommendations. These critical applications require strong guarantees on both the integrity of the machine learning models and the privacy of the user data used to train these models. The area of adversarial machine learning studies the effect of adversarial attacks against machine learning models and aims to design robust defense algorithms. The main challenges in this space are the development of realistic adversarial models that consider the specifics of real-world applications, and the design of machine learning algorithms resilient against a wide range of threats. In this talk, we describe our work on creating a taxonomy of poisoning attacks against machine learning systems at training time. In light of recent software supply chain vulnerabilities revealed by the SolarWinds attack, the supply chain of machine learning development needs to be protected. First, we introduce our optimization approach to create poisoning availability attacks against linear regression and discuss robust defenses based on techniques from robust statistics [1]. Then, we discuss how an attacker with minimal knowledge of a machine learning classifier can inject backdoor poisoning attacks, by leveraging techniques from machine learning explainability [4]. We demonstrate these methods on several malware classifiers and show the challenges of designing robust defenses to protect against these attacks. We also define a new attack model called subpopulation poisoning, that requires a small set of poisoning points to impact the accuracy of the model on a targeted subpopulation [2]. We evaluate our poisoning attacks on multiple data modalities, including image, text, and tabular data. Finally, we highlight a surprising connection between machine learning integrity and privacy attacks, and show how poisoning attacks can be used for auditing the privacy of machine learning algorithms such as differentially private stochastic gradient descent [3].","",""
2,"D. Rawat","Secure and trustworthy machine learning/artificial intelligence for multi-domain operations",2021,"","","","",67,"2022-07-13 09:37:52","","10.1117/12.2592860","","",,,,,2,2.00,2,1,1,"Machine Learning (ML) algorithms and Artificial Intelligence (AI) are now regarded as very useful for data-driven applications including resilient multi-domain operations. However, ML algorithms and AI systems can be controlled, dodged, biased, and misled through flawed learning models and input data, they need robust security features and trust. Furthermore, ML algorithms and AI systems add challenges when we have (unlabeled/labeled) sparse/small data or big data for training and evaluation. It is very important to design, evaluate and test ML algorithms and AI systems that produce reliable, robust, trustworthy, explainable, and fair/unbiased outcomes to make them acceptable and reliable in mission critical multi-domain operations. ML algorithms rely on data and work on the principle of ``Garbage In, Garbage Out,"" which means that if the input data to learning model is corrupted or compromised, the outcomes of the ML/AI would not be optimal, reliable and trustworthy.","",""
242,"Pantelis Linardatos, Vasilis Papastefanopoulos, S. Kotsiantis","Explainable AI: A Review of Machine Learning Interpretability Methods",2020,"","","","",68,"2022-07-13 09:37:52","","10.3390/e23010018","","",,,,,242,121.00,81,3,2,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.","",""
0,"S. Saha, H. Singh, A. Soliman, S. Rajasekaran","A novel computational methodology for GWAS multi-locus analysis based on graph theory and machine learning",2021,"","","","",69,"2022-07-13 09:37:52","","10.1101/2021.10.22.21265388","","",,,,,0,0.00,0,4,1,"Background: Current form of genome-wide association studies (GWAS) is inadequate to accurately explain the genetics of complex traits due to the lack of sufficient statistical power. It explores each variant individually, but current studies show that multiple variants with varying effect sizes actually act in a concerted way to develop a complex disease. To address this issue, we have developed an algorithmic framework that can effectively solve the multi-locus problem in GWAS with a very high level of confidence. Our methodology consists of three novel algorithms based on graph theory and machine learning. It identifies a set of highly discriminating variants that are stable and robust with little (if any) spuriousness. Consequently, likely these variants should be able to interpret missing heritability of a convoluted disease as an entity. Results: To demonstrate the efficacy of our proposed algorithms, we have considered astigmatism case-control GWAS dataset. Astigmatism is a common eye condition that causes blurred vision because of an error in the shape of the cornea. The cause of astigmatism is not entirely known but a sizable inheritability is assumed. Clinical studies show that developmental disorders (such as, autism) and astigmatism co-occur in a statistically significant number of individuals. By performing classical GWAS analysis, we didn't find any genome-wide statistically significant variants. Conversely, we have identified a set of stable, robust, and highly predictive variants that can together explain the genetics of astigmatism. We have performed a set of biological enrichment analyses based on gene ontology (GO) terms, disease ontology (DO) terms, biological pathways, network of pathways, and so forth to manifest the accuracy and novelty of our findings. Conclusions: Rigorous experimental evaluations show that our proposed methodology can solve GWAS multi-locus problem effectively and efficiently. It can identify signals from the GWAS dataset having small number of samples with a high level of accuracy. We believe that the proposed methodology based on graph theory and machine learning is the most comprehensive one compared to any other machine learning based tools in this domain.","",""
0,"J. Ding, Chi-Hsiang Chu, Mong-Na Lo Huang, Chien-Ching Hsu","Dopamine Transporter SPECT Image Classification for Neurodegenerative Parkinsonism via Diffusion Maps and Machine Learning Classifiers",2021,"","","","",70,"2022-07-13 09:37:52","","10.1007/978-3-030-80432-9_29","","",,,,,0,0.00,0,4,1,"","",""
0,"R. Arya, Jyoti Sharma, R. Shrivastava, Devyani Thapliyal, G. Verros","Modeling of Surfactant-Enhanced Drying of Poly(styrene)-p-xylene Polymeric Coatings Using Machine Learning Technique",2021,"","","","",71,"2022-07-13 09:37:52","","10.3390/coatings11121529","","",,,,,0,0.00,0,5,1,"In this work, a machine learning technique based on a regression tree model was used to model the surfactant enhanced drying of poly(styrene)-p-xylene coatings. The predictions of the developed model based on regression trees are in excellent agreement with the experimental data. A total of 16,258 samples were obtained through experimentation. These samples were separated into two parts: 12,960 samples were used for the training of the regression tree, and the remaining 3298 samples were used to test the tree’s prediction accuracy. MATLAB software was used to grow the regression tree. The mean squared error between the model-predicted values and actual outputs was calculated to be 8.8415 × 10−6. This model has good generalizing ability; predicts weight loss for given values of time, thickness, and triphenyl phosphate; and has a maximum error of 1%. It is robust and for this system, can be used for any composition and thickness for this system, which will drastically reduce the need for further experimentations to explain diffusion and drying.","",""
0,"S. E. Whang, Ki Hyun Tae, Yuji Roh, Geon Heo","Responsible AI Challenges in End-to-end Machine Learning",2021,"","","","",72,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,4,1,"Responsible AI is becoming critical as AI is widely used in our everyday lives. Many companies that deploy AI publicly state that when training a model, we not only need to improve its accuracy, but also need to guarantee that the model does not discriminate against users (fairness), is resilient to noisy or poisoned data (robustness), is explainable, and more. In addition, these objectives are not only relevant to model training, but to all steps of end-to-end machine learning, which include data collection, data cleaning and validation, model training, model evaluation, and model management and serving. Finally, responsible AI is conceptually challenging, and supporting all the objectives must be as easy as possible. We thus propose three key research directions towards this vision – depth, breadth, and usability – to measure progress and introduce our ongoing research. First, responsible AI must be deeply supported where multiple objectives like fairness and robust must be handled together. To this end, we propose FR-Train, a holistic framework for fair and robust model training in the presence of data bias and poisoning. Second, responsible AI must be broadly supported, preferably in all steps of machine learning. Currently we focus on the data pre-processing steps and propose Slice Tuner, a selective data acquisition framework for training fair and accurate models, and MLClean, a data cleaning framework that also improves fairness and robustness. Finally, responsible AI must be usable where the techniques must be easy to deploy and actionable. We propose FairBatch, a batch selection approach for fairness that is effective and simple to use, and Slice Finder, a model evaluation tool that automatically finds problematic slices. We believe we scratched the surface of responsible AI for end-to-end machine learning and suggest research challenges moving forward.","",""
0,"İrem ERSÖZ KAYA, Oya Korkmaz","Machine Learning Approach for Predicting Employee Attrition and Factors Leading to Attrition",2021,"","","","",73,"2022-07-13 09:37:52","","10.21605/cukurovaumfd.1040487","","",,,,,0,0.00,0,2,1,"In this study that aims to prevent the attrition of human resource which is so important for enterprises, as well as to prevent the leave of employment which is the natural result of such attrition, employee attrition and factors causing attrition are tried to be determined by predictive analytics approaches. The sample dataset which contains 30 different attributes of 1470 employees was obtained for the analysis from a database provided by IBM Watson Analytics. In the study, seven different machine learning algorithms were used to evaluate the prediction achievements. The gain ratio approach was preferred in determining the factors causing attrition. The key point of the study was to cope with the imbalanced data through resampling with bootstrapping. Thereby, even in the blind test, prospering prediction performances reaching up to 80% accuracy were achieved in robust specificity without sacrificing sensitivity. Therewithal, the effective factors causing attrition were investigated in the study and it was concluded that the first 20 attributes ranked according to their gain ratio were sufficient in explaining attrition.","",""
0,"G. Truda","Quantified Sleep: Machine learning techniques for observational n-of-1 studies",2021,"","","","",74,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,1,1,"This paper applies statistical learning techniques to an observational Quantified-Self (QS) study to build a descriptive model of sleep quality. A total of 472 days of my sleep data was collected with an Oura ring. This was combined with a variety of lifestyle, environmental, and psychological data, harvested from multiple sensors and manual logs. Such n-of-1 QS projects pose a number of specific challenges: heterogeneous data sources with many missing values; few observations and many features; dynamic feedback loops; and human biases. This paper directly addresses these challenges with an end-to-end QS pipeline for observational studies that combines techniques from statistics and machine learning to produce robust descriptive models. Sleep quality is one of the most difficult modelling targets in QS research, due to high noise and a large number of weakly-contributing factors. Sleep quality was selected so that approaches from this paper would generalise to most other n-of-1 QS projects. Techniques are presented for combining and engineering features for the different classes of data types, sample frequencies, and schema. This includes manually-tracked event logs and automatically-sampled weather and geo-spatial data. Relevant statistical analyses for outliers, normality, (auto)correlation, stationarity, and missing data are detailed, along with a proposed method for hierarchical clustering to identify correlated groups of features. The missing data was overcome using a combination of knowledge-based and statistical techniques, including several multivariate imputation algorithms. “Markov unfolding” is presented for collapsing the time series into a collection of independent observations, whilst incorporating historical information. The final model was interpreted in two key ways: by inspecting the internal β-parameters, and using the SHAP framework, which can explain any “black box” model. These two interpretation techniques were combined to produce a list of the 16 most-predictive features, demonstrating that an observational study can greatly narrow down the number of features that need to be considered when designing interventional QS studies.","",""
0,"Elizaveta Felsche, R. Ludwig","Applying machine learning for drought prediction using a large ensemble of climate simulations",2021,"","","","",75,"2022-07-13 09:37:52","","10.5194/EGUSPHERE-EGU21-1305","","",,,,,0,0.00,0,2,1,"<p>There is strong scientific and social interest to understand the factors leading to extreme events in order to improve the management of risks associated with hazards like droughts. Recent events like the summer 2018 drought in Germany already had severe und unexpected impacts, e.g. forest fires and crop failures; in order to increase preparedness robust prediction tools are &#160;urgently required. In this study, machine learning methods are applied to predict the occurrence of a drought with lead times of one to three months. The approach takes into account a list of thirty atmospheric and soil variables<strong> </strong>as predictor input parameters from a single regional climate model initial condition large ensemble (CRCM5-LE). The data was produced the context of the ClimEx project by Ouranos with the Canadian Regional Climate Model (CRCM5) driven by 50 members of the Canadian Earth System Model (CanESM2) for the Bavarian and Quebec domains.</p><p>Drought occurrence was defined using the Standardized Precipitation Index. The training and test datasets were chosen from the current climatology (1955-2005) for the Munich and Lisbon subdomain within the CRCM5-LE. The best performing machine learning algorithms managed to obtain a correct classification of drought or no drought for a lead time of one month for around 60 % of the events of each class for the both domains. Explainable AI methods like feature importance and shapley values were applied to gain a better understanding of the trained algorithms. Physical variables like the North Atlantic Oscillation Index and air pressure one month before the event proved to be of high importance for the prediction. The study showed that better accuracies can be obtained for the Lisbon domain, due to the stronger influence of the North Atlantic Oscillation Index on Portugal&#8217;s climate.</p>","",""
0,"Safa Omri, C. Sinz","Machine Learning Techniques for Software Quality Assurance: A Survey",2021,"","","","",76,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,2,1,"Over the last years, machine learning techniques have been applied to more and more application domains, including software engineering and, especially, software quality assurance. Important application domains have been, e.g., software defect prediction or test case selection and prioritization. The ability to predict which components in a large software system are most likely to contain the largest numbers of faults in the next release helps to better manage projects, including early estimation of possible release delays, and affordably guide corrective actions to improve the quality of the software. However, developing robust fault prediction models is a challenging task and many techniques have been proposed in the literature. Closely related to estimating defect-prone parts of a software system is the question of how to select and prioritize test cases, and indeed test case prioritization has been extensively researched as a means for reducing the time taken to discover regressions in software. In this survey, we discuss various approaches in both fault prediction and test case prioritization, also explaining how in recent studies deep learning algorithms for fault prediction help to bridge the gap between programs’ semantics and fault prediction features. We also review recently proposed machine learning methods for test case prioritization (TCP), and their ability to reduce the cost of regression testing without negatively affecting fault detection capabilities.","",""
0,"Yang Kang","Distributionally Robust Optimization and its Applications in Machine Learning",2017,"","","","",77,"2022-07-13 09:37:52","","10.7916/D8WD4C1R","","",,,,,0,0.00,0,1,5,"Distributionally Robust Optimization and its Applications in Machine Learning Yang Kang The goal of Distributionally Robust Optimization (DRO) is to minimize the cost of running a stochastic system, under the assumption that an adversary can replace the underlying baseline stochastic model by another model within a family known as the distributional uncertainty region. This dissertation focuses on a class of DRO problems which are data-driven, which generally speaking means that the baseline stochastic model corresponds to the empirical distribution of a given sample. One of the main contributions of this dissertation is to show that the class of data-driven DRO problems that we study unify many successful machine learning algorithms, including square root Lasso, support vector machines, and generalized logistic regression, among others. A key distinctive feature of the class of DRO problems that we consider here is that our distributional uncertainty region is based on optimal transport costs. In contrast, most of the DRO formulations that exist to date take advantage of a likelihood based formulation (such as Kullback-Leibler divergence, among others). Optimal transport costs include as a special case the so-called Wasserstein distance, which is popular in various statistical applications. The use of optimal transport costs is advantageous relative to the use of divergencebased formulations because the region of distributional uncertainty contains distributions which explore samples outside of the support of the empirical measure, therefore explaining why many machine learning algorithms have the ability to improve generalization. Moreover, the DRO representations that we use to unify the previously mentioned machine learning algorithms, provide a clear interpretation of the so-called regularization parameter, which is known to play a crucial role in controlling generalization error. As we establish, the regularization parameter corresponds exactly to the size of the distributional uncertainty region. Another contribution of this dissertation is the development of statistical methodology to study data-driven DRO formulations based on optimal transport costs. Using this theory, for example, we provide a sharp characterization of the optimal selection of regularization parameters in machine learning settings such as square-root Lasso and regularized logistic regression. Our statistical methodology relies on the construction of a key object which we call the robust Wasserstein profile function (RWP function). The RWP function similar in spirit to the empirical likelihood profile function in the context of empirical likelihood (EL). But the asymptotic analysis of the RWP function is different because of a certain lack of smoothness which arises in a suitable Lagrangian formulation. Optimal transport costs have many advantages in terms of statistical modeling. For example, we show how to define a class of novel semi-supervised learning estimators which are natural companions of the standard supervised counterparts (such as square root Lasso, support vector machines, and logistic regression). We also show how to define the distributional uncertainty region in a purely data-driven way. Precisely, the optimal transport formulation allows us to inform the shape of the distributional uncertainty, not only its center (which given by the empirical distribution). This shape is informed by establishing connections to the metric learning literature. We develop a class of metric learning algorithms which are based on robust optimization. We use the robust-optimization-based metric learning algorithms to inform the distributional uncertainty region in our data-driven DRO problem. This means that we endow the adversary with additional which force him to spend effort on regions of importance to further improve generalization properties of machine learning algorithms. In summary, we explain how the use of optimal transport costs allow constructing what we call double-robust statistical procedures. We test all of the procedures proposed in this paper in various data sets, showing significant improvement in generalization ability over a wide range of state-of-the-art procedures. Finally, we also discuss a class of stochastic optimization algorithms of independent interest which are particularly useful to solve DRO problems, especially those which arise when the distributional uncertainty region is based on optimal transport costs.","",""
37,"Leif Hancox-Li","Robustness in machine learning explanations: does it matter?",2020,"","","","",78,"2022-07-13 09:37:52","","10.1145/3351095.3372836","","",,,,,37,18.50,37,1,2,"The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.","",""
1,"Guanyu Wang, T. Fearn, Tengyao Wang, K. Choy","Insight Gained from Using Machine Learning Techniques to Predict the Discharge Capacities of Doped Spinel Cathode Materials for Lithium‐Ion Batteries Applications",2021,"","","","",79,"2022-07-13 09:37:52","","10.1002/ENTE.202100053","","",,,,,1,1.00,0,4,1,"Rechargeable lithium-ion batteries (LIBs) are known as the most promising energy storage technology due to their high energy density, high power density, and long charge/discharge life cycle. Presently, an extensive amount of research has been devoted to boosting their performance as to complement the applications in sustainable energy power and this demands for the improvements in storage capacity, steadiness and safety, toxicity, eco-friendliness, and material cost. For this, discovering new cathode materials has become the key as they both contribute to 33% of the total battery cost and compared with the anode, have much lower storage capacity, and therefore greatly limit on the battery discharging capacities. Among all cathode materials, spinel cathode materials (LiMn2O4) are preferred over the widely commercialized lithium cobalt oxide (LiCoO2) material for its nontoxic nature, robust 3D structure (high Li-ion diffusion), and low cost (manganese metal is more abundant than cobalt metal). However, the issues of drastic capacity fading and limited rate performance have restricted their use in large-scale commercial applications. These inferior properties can be explained by two underlying chemical phenomena. The first is the dissolution of the manganese ions Mn3þ from the material surface into various forms of Mn4þ(solid) and Mn2þ(sol), which reduces the Li-ions site energy and eventually lowers the rate of reversible electrochemical reactions. The second reason is the Jahn–Teller distortion (JRD) effects initialized from the high spin electrons interactions from the d-orbital electrons of manganese ion (Mn3þ), which destabilize the overall crystal structure and hence reduce the respective cycle life. Doping the manganese (III) sites with lower valence (lower than 3þ) dopants (Figure 1a) seems to be an effective approach to this problem as it increases the average Mn valence in LiMn2O4 to suppress the JTD effect through reducing the concentration of Mn (III) and eventually decrease the rate of dissolution reaction. Indeed, promising results have been seen in capacity improvement for lower valence dopants such as Al, Cr, Fe, Gd, Ga, Mg, Nd, Ni, Ru, Sc, and Zn; however, the use of higher valence dopants such as Si and Sn are also shown to be effective. Figure 1b shows G. Wang Institute for Materials Discovery University College London Roberts Building, London WC1E 7JE, UK Prof. T. Fearn, Dr. T. Wang Department of Statistical Science University College London 1-19 Torrington Place, London WC1R 7HB, UK E-mail: tengyao.wang@ucl.ac.UK Prof. K.-L. Choy Institute for Materials Discovery Faculty of Maths and Physical Sciences University College London Roberts Building, London WC1E 7JE, UK E-mail: k.choy@ucl.ac.uk","",""
2,"S. Meister, Mahdieu A. M. Wermes, J. Stüve, R. Groves","Explainability of deep learning classifier decisions for optical detection of manufacturing defects in the automated fiber placement process",2021,"","","","",80,"2022-07-13 09:37:52","","10.1117/12.2592584","","",,,,,2,2.00,1,4,1,"Automated fibre layup techniques are commonly used composite manufacturing processes in the aviation sector and require a manual visual inspection. Neural Network classification of defects has the potential to automate this visual inspection, however, the machine decision-making processes are hard to verify. Thus, we present an approach for visualising Convolutional Neural Network (CNN) based classifications of manufacturing defects and quantifying its robustness suitably. Our investigations have shown that especially Smoothed Integrated Gradients and DeepSHAP are particularly well suited for the visualisation of CNN classifications. The Smoothed Integrated Gradients technique also reveals advantages in robustness when evaluating degraded input images.","",""
17,"E. Casiraghi, D. Malchiodi, G. Trucco, M. Frasca, L. Cappelletti, T. Fontana, A. Esposito, E. Avola, A. Jachetti, J. Reese, A. Rizzi, P. Robinson, G. Valentini","Explainable Machine Learning for Early Assessment of COVID-19 Risk Prediction in Emergency Departments",2020,"","","","",81,"2022-07-13 09:37:52","","10.1109/ACCESS.2020.3034032","","",,,,,17,8.50,2,13,2,"Between January and October of 2020, the severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2) virus has infected more than 34 million persons in a worldwide pandemic leading to over one million deaths worldwide (data from the Johns Hopkins University). Since the virus begun to spread, emergency departments were busy with COVID-19 patients for whom a quick decision regarding in- or outpatient care was required. The virus can cause characteristic abnormalities in chest radiographs (CXR), but, due to the low sensitivity of CXR, additional variables and criteria are needed to accurately predict risk. Here, we describe a computerized system primarily aimed at extracting the most relevant radiological, clinical, and laboratory variables for improving patient risk prediction, and secondarily at presenting an explainable machine learning system, which may provide simple decision criteria to be used by clinicians as a support for assessing patient risk. To achieve robust and reliable variable selection, Boruta and Random Forest (RF) are combined in a 10-fold cross-validation scheme to produce a variable importance estimate not biased by the presence of surrogates. The most important variables are then selected to train a RF classifier, whose rules may be extracted, simplified, and pruned to finally build an associative tree, particularly appealing for its simplicity. Results show that the radiological score automatically computed through a neural network is highly correlated with the score computed by radiologists, and that laboratory variables, together with the number of comorbidities, aid risk prediction. The prediction performance of our approach was compared to that that of generalized linear models and shown to be effective and robust. The proposed machine learning-based computational system can be easily deployed and used in emergency departments for rapid and accurate risk prediction in COVID-19 patients.","",""
81,"Vaishak Belle, I. Papantonis","Principles and Practice of Explainable Machine Learning",2020,"","","","",82,"2022-07-13 09:37:52","","10.3389/fdata.2021.688969","","",,,,,81,40.50,41,2,2,"Artificial intelligence (AI) provides many opportunities to improve private and public life. Discovering patterns and structures in large troves of data in an automated manner is a core component of data science, and currently drives applications in diverse areas such as computational biology, law and finance. However, such a highly positive impact is coupled with a significant challenge: how do we understand the decisions suggested by these systems in order that we can trust them? In this report, we focus specifically on data-driven methods—machine learning (ML) and pattern recognition models in particular—so as to survey and distill the results and observations from the literature. The purpose of this report can be especially appreciated by noting that ML models are increasingly deployed in a wide range of businesses. However, with the increasing prevalence and complexity of methods, business stakeholders in the very least have a growing number of concerns about the drawbacks of models, data-specific biases, and so on. Analogously, data science practitioners are often not aware about approaches emerging from the academic literature or may struggle to appreciate the differences between different methods, so end up using industry standards such as SHAP. Here, we have undertaken a survey to help industry practitioners (but also data scientists more broadly) understand the field of explainable machine learning better and apply the right tools. Our latter sections build a narrative around a putative data scientist, and discuss how she might go about explaining her models by asking the right questions. From an organization viewpoint, after motivating the area broadly, we discuss the main developments, including the principles that allow us to study transparent models vs. opaque models, as well as model-specific or model-agnostic post-hoc explainability approaches. We also briefly reflect on deep learning models, and conclude with a discussion about future research directions.","",""
9,"E. Ates, Burak Aksar, V. Leung, A. Coskun","Counterfactual Explanations for Machine Learning on Multivariate Time Series Data",2020,"","","","",83,"2022-07-13 09:37:52","","10.1109/ICAPAI49758.2021.9462056","","",,,,,9,4.50,2,4,2,"Applying machine learning (ML) on multivariate time series data has growing popularity in many application domains, including in computer system management. For example, recent high performance computing (HPC) research proposes a variety of ML frameworks that use system telemetry data in the form of multivariate time series so as to detect performance variations, perform intelligent scheduling or node allocation, and improve system security. Common barriers for adoption for these ML frameworks include the lack of user trust and the difficulty of debugging. These barriers need to be overcome to enable the widespread adoption of ML frameworks in production systems. To address this challenge, this paper proposes a novel explainability technique for providing counterfactual explanations for supervised ML frameworks that use multivariate time series data. The proposed method outperforms state-of-the-art explainability methods on several different ML frameworks and data sets in metrics such as faithfulness and robustness. The paper also demonstrates how the proposed method can be used to debug ML frameworks and gain a better understanding of HPC system telemetry data.","",""
10,"T. Botari, Frederik Hvilshøj, Rafael Izbicki, A. Carvalho","MeLIME: Meaningful Local Explanation for Machine Learning Models",2020,"","","","",84,"2022-07-13 09:37:52","","","","",,,,,10,5.00,3,4,2,"Most state-of-the-art machine learning algorithms induce black-box models, preventing their application in many sensitive domains. Hence, many methodologies for explaining machine learning models have been proposed to address this problem. In this work, we introduce strategies to improve local explanations taking into account the distribution of the data used to train the black-box models. We show that our approach, MeLIME, produces more meaningful explanations compared to other techniques over different ML models, operating on various types of data. MeLIME generalizes the LIME method, allowing more flexible perturbation sampling and the use of different local interpretable models. Additionally, we introduce modifications to standard training algorithms of local interpretable models fostering more robust explanations, even allowing the production of counterfactual examples. To show the strengths of the proposed approach, we include experiments on tabular data, images, and text; all showing improved explanations. In particular, MeLIME generated more meaningful explanations on the MNIST dataset than methods such as GuidedBackprop, SmoothGrad, and Layer-wise Relevance Propagation. MeLIME is available on this https URL.","",""
5,"P. Santhanam","Quality Management of Machine Learning Systems",2020,"","","","",85,"2022-07-13 09:37:52","","10.1007/978-3-030-62144-5_1","","",,,,,5,2.50,5,1,2,"","",""
2,"Yousef Sheikhi Garjan, Mehdi Ghaneezabadi","Machine Learning Interpretability Application to Optimize Well Completion in Montney",2020,"","","","",86,"2022-07-13 09:37:52","","10.2118/200019-ms","","",,,,,2,1.00,1,2,2,"Recently machine learning has being extensively deployed for oil and gas industry for improving result and expedite process. However, the black box models do not explain their prediction which considered as a barrier to adopt machine learning. This paper is about optimizing hydraulic fracture with machine learning methods and making informative decision with interpreting machine learning model. The solution can show that it could save over million dollars per well and improve well performance significantly. Interestingly, the machine leaning explainability approach was utilized to explain and measure the reason behind of why some wells are performing better than other and vice versa.Hydraulic fracturing modeling and optimization in tight oil and unconventional reservoir requires substantial geological modeling, fracture design, post-fracture production simulation with excessive sensitivity analysis due to complexity and uncertainty in the nature of data. These types of studies are computationally and monetarily expensive. Furthermore, digital oil technology has facilitated the process of data gathering enabled operators to have access to huge amount of data. Common approaches are no longer suitable to handle this pile of data but machine learning methods could be successfully utilized for this purpose.In this paper, a variety types of advanced machine learning methods including linear regression, Random forest, Gradient Boost, XGBoost, Bagging, ExtraTrees and neural network were employed to optimize well completion in Montney formation. The objective was to create a robust predictive model capturing all the effective operational well parameters (features) capable of optimizing the first 12 months cumulative of equivalent well production.Special Individual Conditional Expectation (ICE) plots and Partial Dependency plots(PDP) were used to depict how HF completion features influence the prediction of a machine learning model. Furthermore, a novel approach was employed to explain the model prediction of an existing well by computing the contribution of each feature to the prediction.Over 1838 hydraulically fractured (HF) wells producing from 2008 till 2019 in Montney formation have been considered for this analysis. The outcome of Explanatory Data Analysis (EDA) revealed that well production performance has not been improved despite of continues enhancement of hydraulic fracture parameters such as proppant injected volume, length of stimulated horizontal wells, and number of stages per well in the course of two years. This finding raises the concern of whether operators are properly optimizing completion design. After comparing all machine learning methods, Random Forest method was chosen as the most appropriate and accurate method to proceed for further analysis. ICE and PDP plots helped to understand the impacts of different fracturing features on production for individual well in addition to define optimum operation features on Montney Formation. Furthermore, quantifying of each feature’s impact on individual well production and linking it to an economic model, we were able to demonstrate potential profit and loss for each well. The model suggests that some wells could have achieved over $1 million extra profit during the first 12-months of production.In this study, not only a reliable predictive data-driven model has been built for hydraulically-fractured wells in Montney formation, but also a comprehensive workflow of sensitivity and explainatability analysis has been introduced to obtain an optimized fit-to-purpose well completion design.","",""
8,"C. Rea, K. Montes, A. Pau, R. Granetz, O. Sauter","Progress Toward Interpretable Machine Learning–Based Disruption Predictors Across Tokamaks",2020,"","","","",87,"2022-07-13 09:37:52","","10.1080/15361055.2020.1798589","","",,,,,8,4.00,2,5,2,"Abstract In this paper we lay the groundwork for a robust cross-device comparison of data-driven disruption prediction algorithms on DIII-D and JET tokamaks. In order to consistently carry on a comparative analysis, we define physics-based indicators of disruption precursors based on temperature, density, and radiation profiles that are currently not used in many other machine learning predictors for DIII-D data. These profile-based indicators are shown to well-describe impurity accumulation events in both DIII-D and JET discharges that eventually disrupt. The univariate analysis of the features used as input signals in the data-driven algorithms applied on the data of both tokamaks statistically highlights the differences in the dominant disruption precursors. JET with its ITER-like wall is more prone to impurity accumulation events, while DIII-D is more subject to edge-cooling mechanisms that destabilize dangerous magnetohydrodynamic modes. Even though the analyzed data sets are characterized by such intrinsic differences, we show through a few examples that the inclusion of physics-based disruption markers in data-driven algorithms is a promising path toward the realization of a uniform framework to predict and interpret disruptive scenarios across different tokamaks. As long as the destabilizing precursors are diagnosed in a device-independent way, the knowledge that data-driven algorithms learn on one device can be re-used to explain a disruptive behavior on another device.","",""
6,"Anika Gebauer, Monja Ellinger, Victor M. Brito Gómez, Mareike Ließ","Development of pedotransfer functions for water retention in tropical mountain soil landscapes: spotlight on parameter tuning in machine learning",2020,"","","","",88,"2022-07-13 09:37:52","","10.5194/soil-6-215-2020","","",,,,,6,3.00,2,4,2,"Abstract. Machine-learning algorithms are good at computing non-linear problems and fitting complex composite functions, which makes them an adequate tool for addressing multiple environmental research questions. One important application is the development of pedotransfer functions (PTFs). This study aims to develop water retention PTFs for two remote tropical mountain regions with rather different soil landscapes: (1) those dominated by peat soils and soils under volcanic influence with high organic matter contents and (2) those dominated by tropical mineral soils. Two tuning procedures were compared to fit boosted regression tree models: (1) tuning with grid search, which is the standard approach in pedometrics; and (2) tuning with differential evolution optimization. A nested cross-validation approach was applied to generate robust models. The area-specific PTFs developed outperform other more general PTFs. Furthermore, the first PTF for typical soils of Paramo landscapes (Ecuador), i.e., organic soils under volcanic influence, is presented. Overall, the results confirmed the differential evolution algorithm's high potential for tuning machine-learning models. While models based on tuning with grid search roughly predicted the response variables' mean for both areas, models applying the differential evolution algorithm for parameter tuning explained up to 25 times more of the response variables' variance.","",""
0,"Dhilsath Fathima.M, S. Samuel, R. Haran","AN EFFICIENT MACHINE LEARNING MODEL FOR PREDICTION OF ACUTE MYOCARDIAL INFARCTION",2020,"","","","",89,"2022-07-13 09:37:52","","10.2174/2666255813666200325104317","","",,,,,0,0.00,0,3,2,"   This proposed work is used to develop an improved and robust machine learning model for predicting Myocardial Infarction (MI) could have substantial clinical impact.     This paper explains how to build machine learning based computer-aided analysis system for an early and accurate prediction of Myocardial Infarction (MI) which utilizes framingham heart study dataset for validation and evaluation. This proposed computer-aided analysis model will support medical professionals to predict myocardial infarction proficiently.     The proposed model utilize the mean imputation to remove the missing values from the data set, then applied principal component analysis to extract the optimal features from the data set to enhance the performance of the classifiers. After PCA, the reduced features are partitioned into training dataset and testing dataset where 70% of the training dataset are given as an input to the four well-liked classifiers as support vector machine, k-nearest neighbor, logistic regression and decision tree to train the classifiers and 30% of test dataset is used to evaluate an output of machine learning model using performance metrics as confusion matrix, classifier accuracy, precision, sensitivity, F1-score, AUC-ROC curve.    Output of the classifiers are evaluated using performance measures and we observed that logistic regression provides high accuracy than K-NN, SVM, decision tree classifiers and PCA performs sound as a good feature extraction method to enhance the performance of proposed model. From these analyses, we conclude that logistic regression having good mean accuracy level and standard deviation accuracy compared with the other three algorithms. AUC-ROC curve of the proposed classifiers is analyzed from the output figure.4, figure.5 that logistic regression exhibits good AUC-ROC score, i.e. around 70% compared to k-NN and decision tree algorithm.     From the result analysis, we infer that this proposed machine learning model will act as an optimal decision making system to predict the acute myocardial infarction at an early stage than an existing machine learning based prediction models and it is capable to predict the presence of an acute myocardial Infarction with human using the heart disease risk factors, in order to decide when to start lifestyle modification and medical treatment to prevent the heart disease. ","",""
2,"L. Israel, Felix D. Schönbrodt","Predicting affective appraisals from facial expressions and physiology using machine learning",2020,"","","","",90,"2022-07-13 09:37:52","","10.3758/s13428-020-01435-y","","",,,,,2,1.00,1,2,2,"","",""
0,"E. Ates, Burak Aksar, V. Leung, A. Coskun","Explainable Machine Learning Frameworks for Managing HPC Systems.",2020,"","","","",91,"2022-07-13 09:37:52","","10.2172/1829224","","",,,,,0,0.00,0,4,2,"Recent research on supercomputing proposes a variety of machine learning frameworks that are able to detect performance variations, find optimum application configurations, perform intelligent scheduling or node allocation, and improve system security. Although these goals align well with HPC systems’ needs, barriers such as the lack of user trust or the difficulty of debugging need to be overcome to enable the widespread adoption of such frameworks in production systems. This paper evaluates a new counterfactual time series explainability method and compares it against state-of-the-art explainability methods for supervised machine learning frameworks that use multivariate HPC system telemetry data. The counterfactual time series explainability method outperforms existing methods in terms of comprehensibility and robustness. We also show how explainability techniques can be used to debug machine learning frameworks and gain a better understanding of HPC system telemetry data.","",""
0,"J. Filipe, Ashish Ghosh, R. Prates, O. Shehory, E. Farchi, Guy Barash","Engineering Dependable and Secure Machine Learning Systems: Third International Workshop, EDSMLS 2020, New York City, NY, USA, February 7, 2020, Revised Selected Papers",2020,"","","","",92,"2022-07-13 09:37:52","","10.1007/978-3-030-62144-5","","",,,,,0,0.00,0,6,2,"","",""
1,"Korn Sooksatra, Pablo Rivas","A Review of Machine Learning and Cryptography Applications",2020,"","","","",93,"2022-07-13 09:37:52","","10.1109/CSCI51800.2020.00105","","",,,,,1,0.50,1,2,2,"Adversarially robust neural cryptography deals with the training of a neural-based model using an adversary to leverage the learning process in favor of reliability and trustworthiness. The adversary can be a neural network or a strategy guided by a neural network. These mechanisms are proving successful in finding secure means of data protection. Similarly, machine learning benefits significantly from the cryptography area by protecting models from being accessible to malicious users. This paper is a literature review on the symbiotic relationship between machine learning and cryptography. We explain cryptographic algorithms that have been successfully applied in machine learning problems and, also, deep learning algorithms that have been used in cryptography. We pay special attention to the exciting and relatively new area of adversarial robustness.","",""
0,"Kanan Mukhtarli","Machine learning for homogeneous grouping of pavements",2020,"","","","",94,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,1,2,"Abstract    Machine learning for homogeneous grouping of pavements.  Kanan Mukhtarli    Rapid pavement deterioration is a major problem in areas with harsh weather conditions or high traffic loading. Despite many studies focused on the pavement management systems, there is not, to the date, a robust method explaining how to process large amounts of pavement data to create homogeneous groups for rehabilitation-related decision making. This thesis employs machine learning to develop an approach capable of partitioning pavement data with a close response to casual factors like traffic and weather conditions and considering its performance through international roughness index and deflections. Two different methods: K-means and Self Organizing Maps (SOM) clustering techniques were tested to understand the correlation between daily factors and pavements deterioration. The goodness of clustering was tested using extrinsic and intrinsic evaluation methods. It was concluded from the results that SOM clustering provided better results as it relies on a soft clustering method where one point can represent two clusters at the same time. Moreover, it became obvious from the methodology that including the previous year’s data has very little to no effect on homogeneous groups. Techniques discussed and developed in this study can help road asset managers with decision making for the maintenance and rehabilitation of pavement. Moreover, future researchers can use the results of this study to further develop the idea of building decision support systems for pavement rehabilitation.","",""
0,"A. Arslan","RETHINKING ROBUSTNESS IN MACHINE LEARNING: USE OF GENERATIVE ADVERSARIAL NETWORKS FOR ENHANCED ROBUSTNESS",2020,"","","","",95,"2022-07-13 09:37:52","","10.26483/ijarcs.v13i1.6801","","",,,,,0,0.00,0,1,2,"Machine learning (ML) is increasingly being used in real-world applications, so understanding the uncertainty and robustness of a model is necessary to ensure performance in practice. This paper explores approximations for robustness which can meaningfully explain the behavior of any black box model. Starting with a discussion on components of a robust model this paper offers some techniques based on the Generative Adversarial Network (GAN) approach to improve the robustness of a model. The study concludes that a clear understanding of robust models for ML allows improving information for practitioners, and helps to develop tools that assess the robustness of ML. Also, ML tools and libraries could benefit from a clear understanding on how information should be presented and how these tools are used.","",""
0,"M. Hind, Dennis Wei, Yunfeng Zhang","Consumer-Driven Explanations for Machine Learning Decisions: An Empirical Study of Robustness",2020,"","","","",96,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,3,2,"Many proposed methods for explaining machine learning predictions are in fact challenging to understand for nontechnical consumers. This paper builds upon an alternative consumer-driven approach called TED that asks for explanations to be provided in training data, along with target labels. Using semi-synthetic data from credit approval and employee retention applications, experiments are conducted to investigate some practical considerations with TED, including its performance with different classification algorithms, varying numbers of explanations, and variability in explanations. A new algorithm is proposed to handle the case where some training examples do not have explanations. Our results show that TED is robust to increasing numbers of explanations, noisy explanations, and large fractions of missing explanations, thus making advances toward its practical deployment.","",""
0,"Gabriel D. Patrón, D. León, Edwin Lopez, G. Hernández","An Interpretable Automated Machine Learning Credit Risk Model",2020,"","","","",97,"2022-07-13 09:37:52","","10.1007/978-3-030-61834-6_2","","",,,,,0,0.00,0,4,2,"","",""
932,"Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael A. Specter, Lalana Kagal","Explaining Explanations: An Overview of Interpretability of Machine Learning",2018,"","","","",98,"2022-07-13 09:37:52","","10.1109/DSAA.2018.00018","","",,,,,932,233.00,155,6,4,"There has recently been a surge of work in explanatory artificial intelligence (XAI). This research area tackles the important problem that complex machines and algorithms often cannot provide insights into their behavior and thought processes. XAI allows users and parts of the internal system to be more transparent, providing explanations of their decisions in some level of detail. These explanations are important to ensure algorithmic fairness, identify potential bias/problems in the training data, and to ensure that the algorithms perform as expected. However, explanations produced by these systems is neither standardized nor systematically assessed. In an effort to create best practices and identify open challenges, we describe foundational concepts of explainability and show how they can be used to classify existing literature. We discuss why current approaches to explanatory methods especially for deep neural networks are insufficient. Finally, based on our survey, we conclude with suggested future research directions for explanatory artificial intelligence.","",""
10,"Saeid Tizpaz-Niari, Pavol Cern'y, A. Trivedi","Detecting and understanding real-world differential performance bugs in machine learning libraries",2020,"","","","",99,"2022-07-13 09:37:52","","10.1145/3395363.3404540","","",,,,,10,5.00,3,3,2,"Programming errors that degrade the performance of systems are widespread, yet there is very little tool support for finding and diagnosing these bugs. We present a method and a tool based on differential performance analysis---we find inputs for which the performance varies widely, despite having the same size. To ensure that the differences in the performance are robust (i.e. hold also for large inputs), we compare the performance of not only single inputs, but of classes of inputs, where each class has similar inputs parameterized by their size. Thus, each class is represented by a performance function from the input size to performance. Importantly, we also provide an explanation for why the performance differs in a form that can be readily used to fix a performance bug. The two main phases in our method are discovery with fuzzing and explanation with decision tree classifiers, each of which is supported by clustering. First, we propose an evolutionary fuzzing algorithm to generate inputs that characterize different performance functions. For this fuzzing task, the unique challenge is that we not only need the input class with the worst performance, but rather a set of classes exhibiting differential performance. We use clustering to merge similar input classes which significantly improves the efficiency of our fuzzer. Second, we explain the differential performance in terms of program inputs and internals (e.g., methods and conditions). We adapt discriminant learning approaches with clustering and decision trees to localize suspicious code regions. We applied our techniques on a set of micro-benchmarks and real-world machine learning libraries. On a set of micro-benchmarks, we show that our approach outperforms state-of-the-art fuzzers in finding inputs to characterize differential performance. On a set of case-studies, we discover and explain multiple performance bugs in popular machine learning frameworks, for instance in implementations of logistic regression in scikit-learn. Four of these bugs, reported first in this paper, have since been fixed by the developers.","",""
296,"R. Mothilal, Amit Sharma, Chenhao Tan","Explaining machine learning classifiers through diverse counterfactual explanations",2019,"","","","",100,"2022-07-13 09:37:52","","10.1145/3351095.3372850","","",,,,,296,98.67,99,3,3,"Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.","",""
0,"Mimansa Jaiswal","Interpreting Multimodal Machine Learning Models Trained for Emotion Recognition to Address Robustness and Privacy Concerns",2020,"","","","",101,"2022-07-13 09:37:52","","10.1609/aaai.v34i10.7130","","",,,,,0,0.00,0,1,2,"Many mobile applications and virtual conversational agents now aim to recognize and adapt to emotions. These predicted emotions are used in variety of downstream applications: (a) generating more human like dialogues, (b) predicting mental health issues, and (c) hate speech detection and intervention. To enable this, data are transmitted from users' devices and stored on central servers. These data are then processed further, either annotated or used as inputs for training a model for a specific task. Yet, these data contain sensitive information that could be used by mobile applications without user's consent or, maliciously, by an eavesdropping adversary. My work focuses on two major issues that are faced while training emotion recognition algorithms: (a) privacy of the generated representations and, (b) explaining and ensuring that the predictions are robust to various situations. Tackling these issues would lead to emotion based algorithms that are deployable and helpful at a larger scale, thus enabling more human like experience when interacting with AI.","",""
166,"M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, S. Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Integrating machine learning and multiscale modeling—perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences",2019,"","","","",102,"2022-07-13 09:37:52","","10.1038/s41746-019-0193-y","","",,,,,166,55.33,17,11,3,"","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",103,"2022-07-13 09:37:52","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
1981,"C. Rudin","Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",2018,"","","","",104,"2022-07-13 09:37:52","","10.1038/S42256-019-0048-X","","",,,,,1981,495.25,1981,1,4,"","",""
5,"Florian Tambon, G. Laberge, Le An, Amin Nikanjam, Paulina Stevia Nouwou Mindom, Yann Pequignot, F. Khomh, G. Antoniol, E. Merlo, Franccois Laviolette","How to Certify Machine Learning Based Safety-critical Systems? A Systematic Literature Review",2021,"","","","",105,"2022-07-13 09:37:52","","10.1007/s10515-022-00337-x","","",,,,,5,5.00,1,10,1,"","",""
21,"F. Grando, L. Granville, L. Lamb","Machine Learning in Network Centrality Measures",2018,"","","","",106,"2022-07-13 09:37:52","","10.1145/3237192","","",,,,,21,5.25,7,3,4,"Complex networks are ubiquitous to several computer science domains. Centrality measures are an important analysis mechanism to uncover vital elements of complex networks. However, these metrics have high computational costs and requirements that hinder their applications in large real-world networks. In this tutorial, we explain how the use of neural network learning algorithms can render the application of the metrics in complex networks of arbitrary size. Moreover, the tutorial describes how to identify the best configuration for neural network training and learning such for tasks, besides presenting an easy way to generate and acquire training data. We do so by means of a general methodology, using complex network models adaptable to any application. We show that a regression model generated by the neural network successfully approximates the metric values and therefore is a robust, effective alternative in real-world applications. The methodology and proposed machine-learning model use only a fraction of time with respect to other approximation algorithms, which is crucial in complex network applications.","",""
0,"Shuai Zhao, Yingzhou Peng, Yi Zhang, Huai Wang","Physics-informed Machine Learning for Parameter Estimation of DC-DC Converter",2022,"","","","",107,"2022-07-13 09:37:52","","10.1109/APEC43599.2022.9773482","","",,,,,0,0.00,0,4,1,"Although various machine learning-based methods have been proposed for condition monitoring in power elec-tronics, they are challenging to be implemented in practice due to the accuracy, data availability, computation burden, explainability, etc. Physics-informed machine learning (PIML) has been emerging as a promising direction where the above challenges can be mitigated by incorporating domain knowledge. In this paper, we propose a PIML- based parameter estimation method for a DC-DC Buck converter, as an exemplary application of PIML in power electronics. By seamlessly integrating a deep neural network and the converter physical model, it can estimate multiple component parameters simultaneously with high accuracy and robustness, while based on a limited dataset. It expects to provide a new perspective to tailor existing ML tools for power electronic applications.","",""
2,"Pedro Salas-Rojo, Juan Gabriel Rodríguez","Inheritances and wealth inequality: a machine learning approach",2022,"","","","",108,"2022-07-13 09:37:52","","10.1007/s10888-022-09528-8","","",,,,,2,2.00,1,2,1,"","",""
0,"David Melching, Tobias Strohmann, Guillermo Requena, E. Breitbarth","Explainable machine learning for precise fatigue crack tip detection",2022,"","","","",109,"2022-07-13 09:37:52","","10.1038/s41598-022-13275-1","","",,,,,0,0.00,0,4,1,"","",""
1,"Daniel Gutierrez-Rojas, I. Christou, Daniel Dantas, A. Narayanan, P. Nardelli, Yongheng Yang","Performance evaluation of machine learning for fault selection in power transmission lines",2022,"","","","",110,"2022-07-13 09:37:52","","10.1007/s10115-022-01657-w","","",,,,,1,1.00,0,6,1,"","",""
0,"Jun Su, Pengcheng Zhou","Machine Learning-based Modeling and Prediction of the Intrinsic Relationship between Human Emotion and Music",2022,"","","","",111,"2022-07-13 09:37:52","","10.1145/3534966","","",,,,,0,0.00,0,2,1,"Human emotion is one of the most complex psychophysiological phenomena and has been reported to be affected significantly by music listening. It is supposed that there is an intrinsic relationship between human emotion and music, which can be modeled and predicted quantitatively in a supervised manner. Here, a heuristic clustering analysis is carried out on large-scale free music archive to derive a genre-diverse music library, to which the emotional response of participants is measured using a standard protocol, consequently resulting in a systematic emotion-to-music profile. Eight machine learning methods are employed to statistically correlate the basic sound features of music audio tracks in the library with the measured emotional response of tested people to the music tracks in a training set and to blindly predict the emotional response from sound features in a test set. This study found that nonlinear methods are more robust and predictable but considerably time-consuming than linear approaches. The neural networks have strong internal fittability but are associated with a significant overfitting issue. The support vector machine and Gaussian process exhibit both high internal stability and satisfactory external predictability in all used methods; they are considered as promising tools to model, predict and explain the intrinsic relationship between human emotion and music. The psychological basis and perceptional implication underlying the built machine learning models are also discussed to find out the key music factors that affect human emotion.","",""
0,"Mitchell Solomon, Micah Billouin, Anthony O. Smith, Jad Zeineddine, Kevin Chow, A. Rangarajan, A. Peter","Regional infrasonic and seismic event classification with machine learning",2022,"","","","",112,"2022-07-13 09:37:52","","10.1117/12.2623291","","",,,,,0,0.00,0,7,1,"The present work details how convolutional and recurrent deep learning networks can be used to classify infrasonic and seismic events of regional-field rocket demolitions, rocket motor burns, quarry blasts, and earthquakes. To accelerate machine learning adoption within the geophysical sciences, we illustrate the full machine learning pipeline: data acquisition and cleaning, preprocessing, model construction and training, and model understanding using feature-space analysis. Multiple deep learning architectures are evaluated to provide practical lessons learned and insights. The LSTM-RNN suffers from degraded learning on long geophysical signals, while a CNN is more robust to variance in data quality and length. Geophysical time-series signals should be learned with the instrument response deconvolved to avoid gross resampling or decimation of the signal. Frequency-domain feature inputs like spectrograms exhibit improved classification performance, and mapping event-based attributes to the learned feature space of deep networks can provide explainable physical context. All network configurations are validated on geophysical data collected in the Utah region, with experimental results including ablation studies examining different input types, preprocessing strategies, and hyperparameter settings.","",""
0,"J. Escanciano, Joel Robert Terschuur","Debiased Semiparametric U-Statistics: Machine Learning Inference on Inequality of Opportunity",2022,"","","","",113,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,2,1,"We construct locally robust/orthogonal moments in a semiparametric U-statistics setting. These are quadratic moments in the distribution of the data with a zero derivative with respect to first steps at their limit, which reduces model selection bias with machine learning first steps. We use orthogonal moments to propose new debiased estimators and valid inferences in a variety of applications ranging from Inequality of Opportunity (IOp) to distributional treatment effects. U-statistics with machine learning first steps arise naturally in these and many other applications. A leading example in IOp is the Gini coefficient of machine learning fitted values. We introduce a novel U-moment representation of the First Step Influence Function (U-FSIF) to take into account the effect of the first step estimation on an identifying quadratic moment. Adding the U-FISF to the identifying quadratic moment gives rise to an orthogonal quadratic moment. Our leading and motivational application is to measuring IOp, for which we propose a simple debiased estimator, and the first available inferential methods. We give general and simple regularity conditions for asymptotic theory, and demonstrate an improved finite sample performance in simulations for our debiased measures of IOp. In an empirical application, we find that standard measures of IOp are about six times more sensitive to first step machine learners than our debiased measures, and that between 42% and 46% of income inequality in Spain is explained by circumstances out of the control of the individual. JEL Classification: C13; C14; C21; D31; D63 ∗Research founded by Ministerio de Ciencia e Innovación, grant ECO2017-86675-P, MCI/AEI/FEDER/UE, grant PGC 2018-096732-B-100, and Comunidad de Madrid, grants EPUC3M11 (VPRICIT) and H2019/HUM589. 1 ar X iv :2 20 6. 05 23 5v 1 [ ec on .E M ] 1 0 Ju n 20 22","",""
0,"Jing-Jing Liu, Jian-chao Liu","Permeability Predictions for Tight Sandstone Reservoir Using Explainable Machine Learning and Particle Swarm Optimization",2022,"","","","",114,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,2,1,"High-precision permeability prediction is of great significance to tight sandstone reservoirs. However, while considerable progress has recently been made in the machine learning based prediction of reservoir permeability, the generalization of this approach is limited by weak interpretability. Hence, an interpretable XGBoost model is proposed herein based on particle swarm optimization to predict the permeability of tight sandstone reservoirs with higher accuracy and robust interpretability. The porosity and permeability of 202 core plugs and 6 logging curves (namely, the gamma-ray (GR) curve, the acoustic curve (AC), the spontaneous potential (SP) curve, the caliper (CAL) curve, the deep lateral resistivity (RILD) curve, and eight lateral resistivity (RFOC) curve) are extracted along with three derived variables (i.e., the shale content, the AC slope, and the GR slope) as data sets. Based on the data preprocessing, global and local interpretations are performed according to the Shapley additive explanations (SHAP) analysis, and the redundant features in the data set are screened to identify the porosity, AC, CAL, and GR slope as the four most important features. The particle swarm optimization algorithm is then used to optimize the hyperparameters of the XGBoost model. The prediction results of the PSO-XGBoost model indicate a superior performance compared with that of the benchmark XGBoost model. In addition, the reliable application of the interpretable PSO-XGBoost model in the prediction of tight sandstone reservoir permeability is examined by comparing the results with those of two traditional mathematical regression models, five machine learning models, and three deep learning models. Thus, the interpretable PSO-XGBoost model is shown to have more advantages in permeability prediction along with the lowest root mean square error, thereby confirming the effectiveness and practicability of this method.","",""
0,"Desalegn Aweke, Assefa Senbato Genale, B. Sundaram, Amit Pandey, Vijaykumar Janga, P. Karthika","Machine Learning based Network Security in Healthcare System",2022,"","","","",115,"2022-07-13 09:37:52","","10.1109/ICSCDS53736.2022.9760977","","",,,,,0,0.00,0,6,1,"The world is filled with exciting technologies and ideas; scientists build machines to avoid human intervention in completing work. It is highly challenging to complete the task without the Machine Learning (ML) Technology intervention. With the technological development, certain processes or consultations are performed with the aid of doctors available around the world. In this scenario, it could be noticed that health care is one of the world's expected domains that require the most incredible attention in data security while performing data transfer. Nodes in the network are considered based on the weakest link to overcome the cyber attacker's issues. Besides building the software for data storage, a better mechanism has to be incorporated to provide security to the stored data. This process is a delicate task for every network engineer. This paper will explain such concepts related to health prediction and health care by building the most robust network security systems. Finally, the discussion would cross over human-looping systems, which act as one of the common problems that are affected mentally for a person. According to the results, the suggested model achieved the accuracy of 98.89%, that is 4.76% greater than the previous model.","",""
0,"B. Krishnamurthy, S. Shiva","Scalable Hindsight Experience Replay based Q-learning Framework with Explainability for Big Data Applications in Fog Computing",2022,"","","","",116,"2022-07-13 09:37:52","","10.1109/CCWC54503.2022.9720835","","",,,,,0,0.00,0,2,1,"Nowadays Internet of Things (IoT) applications are proliferating with explosive growth and their computational load is very high. Fog computing is an important structure used for processing the IoT devices data at cloud proximity. Big data applications demand scalable computing with stringent performance requirement. The currently available machine learning models do not match the growing scale of big data applications and lack explainability. In this paper an explainable Q-learning framework with hindsight experience replay (Q-HER) is developed to provide holistic scalability solution for big data applications using minimum number of fog nodes. The reward engineering process is streamlined and each episode with original goal and subset of other goals is repeated to yield high quality scalability policies. The mathematical modeling reveals that the generated scalability decisions satisfy the quality assurance parameters like correctness, robustness, model relevance, and ∊ -Differential Data privacy. The performance of the proposed Q-HER is found to be good towards the performance metrics like accuracy, latency, cost, and average resource wastage under two different scenarios of limited and unlimited processor fog nodes.","",""
5,"Zied Ftiti, Kais Tissaoui, S. Boubaker","On the relationship between oil and gas markets: a new forecasting framework based on a machine learning approach",2020,"","","","",117,"2022-07-13 09:37:52","","10.1007/s10479-020-03652-2","","",,,,,5,2.50,2,3,2,"","",""
0,"Wencan Zhang, Mariella Dimiccoli, Brian Y. Lim","Debiased-CAM to mitigate systematic error with faithful visual explanations of machine learning",2022,"","","","",118,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,3,1,"Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias). Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.","",""
0,"Wencan Zhang, Mariella Dimiccoli, Brian Y. Lim","Debiased-CAM to mitigate image perturbations with faithful visual explanations of machine learning",2020,"","","","",119,"2022-07-13 09:37:52","","10.1145/3491102.3517522","","",,,,,0,0.00,0,3,2,"Model explanations such as saliency maps can improve user trust in AI by highlighting important features for a prediction. However, these become distorted and misleading when explaining predictions of images that are subject to systematic error (bias) by perturbations and corruptions. Furthermore, the distortions persist despite model fine-tuning on images biased by different factors (blur, color temperature, day/night). We present Debiased-CAM to recover explanation faithfulness across various bias types and levels by training a multi-input, multi-task model with auxiliary tasks for explanation and bias level predictions. In simulation studies, the approach not only enhanced prediction accuracy, but also generated highly faithful explanations about these predictions as if the images were unbiased. In user studies, debiased explanations improved user task performance, perceived truthfulness and perceived helpfulness. Debiased training can provide a versatile platform for robust performance and explanation faithfulness for a wide range of applications with data biases.","",""
0,"Christos Kokkotis, C. Ntakolia, S. Moustakidis, G. Giakas, D. Tsaopoulos","Explainable machine learning for knee osteoarthritis diagnosis based on a novel fuzzy feature selection methodology",2021,"","","","",120,"2022-07-13 09:37:52","","10.1007/s13246-022-01106-6","","",,,,,0,0.00,0,5,1,"","",""
0,"Kleanthis Avramidis, M. Rostami, Melinda Chang, Shrikanth Narayanan","Automating Detection of Papilledema in Pediatric Fundus Images with Explainable Machine Learning",2022,"","","","",121,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,4,1,"Papilledema is an ophthalmic neurologic disorder in which increased intracranial pressure leads to swelling of the optic nerves. Undiag-nosed papilledema in children may lead to blindness and may be a sign of life-threatening conditions, such as brain tumors. Robust and accurate clinical diagnosis of this syndrome can be facilitated by automated analysis of fundus images using deep learning, especially in the presence of challenges posed by pseudopapilledema that has similar fundus appearance but distinct clinical implications. We present a deep learning-based algorithm for the automatic detection of pediatric papilledema. Our approach is based on optic disc localization and detection of explainable papilledema indicators through data augmentation. Experiments on real-world clinical data demon-strate that our proposed method is effective with a diagnostic accuracy comparable to expert ophthalmologists 1 .","",""
11,"Markus Jaeger, Stephan Krügel, D. Marinelli, J. Papenbrock, P. Schwendner","Interpretable Machine Learning for Diversified Portfolio Construction",2020,"","","","",122,"2022-07-13 09:37:52","","10.2139/ssrn.3730144","","",,,,,11,5.50,2,5,2,"In this article, the authors construct a pipeline to benchmark hierarchical risk parity (HRP) relative to equal risk contribution (ERC) as examples of diversification strategies allocating to liquid multi-asset futures markets with dynamic leverage (volatility target). The authors use interpretable machine learning concepts (explainable AI) to compare the robustness of the strategies and to back out implicit rules for decision-making. The empirical dataset consists of 17 equity index, government bond, and commodity futures markets across 20 years. The two strategies are back tested for the empirical dataset and for about 100,000 bootstrapped datasets. XGBoost is used to regress the Calmar ratio spread between the two strategies against features of the bootstrapped datasets. Compared to ERC, HRP shows higher Calmar ratios and better matches the volatility target. Using Shapley values, the Calmar ratio spread can be attributed especially to univariate drawdown measures of the asset classes. TOPICS: Quantitative methods, statistical methods, big data/machine learning, portfolio construction, performance measurement Key Findings ▪ The authors introduce a procedure to benchmark rule-based investment strategies and to explain the differences in path-dependent risk-adjusted performance measures using interpretable machine learning. ▪ They apply the procedure to the Calmar ratio spread between hierarchical risk parity (HRP) and equal risk contribution (ERC) allocations of a multi-asset futures portfolio and find HRP to have superior risk-adjusted performance. ▪ The authors regress the Calmar ratio spread against statistical features of bootstrapped futures return datasets using XGBoost and apply the SHAP framework by Lundberg and Lee (2017) to discuss the local and global feature importance.","",""
8,"K. Szielasko, B. Wolter, R. Tschuncky, Sargon Youssef","Micromagnetic materials characterization using machine learning",2019,"","","","",123,"2022-07-13 09:37:52","","10.1515/teme-2019-0099","","",,,,,8,2.67,2,4,3,"Abstract Micromagnetic materials characterization is a nondestructive means of predicting mechanical properties and stress of steel and iron products. The method is based on the circumstance that both mechanical and magnetic behaviour relate to microstructure over similar interaction mechanisms, which leads to characteristic correlations between mechanical and magnetic properties of ferromagnetic materials. The prediction of mechanical properties or stress from micromagnetic parameters represents an inverse problem commonly addressed by regression and classification approaches. Challenges for the industrial application of micromagnetic methods lie in the development of robust sensors, definition of significant features, and implementation of powerful machine learning algorithms for a reliable quantitative target value prediction by processing of the micromagnetic features. This contribution briefly explains the background of micromagnetics, describes the typical challenges experienced in practice and provides insight into latest progress in the application of machine learning to micromagnetic data.","",""
3,"N. Radziwill","Machine Learning with R, Third Edition (Book Review)",2019,"","","","",124,"2022-07-13 09:37:52","","10.1080/10686967.2019.1648086","","",,,,,3,1.00,3,1,3,"This book is highly recommended for anyone with previous programing experience who seeks a solid, grounded introduction to basic machine learning using the R statistical software. With nearly 100 additional pages added since the first edition in 2013, this update to Brett Lantz’s excellent text is well worth the purchase, even for those who already have an earlier copy on their shelf. Clear writing, robust explanations, and compelling examples appear throughout, and most chapters explain the math underlying the methods in as simple and easy a manner as possible. I liked the first edition so much, I used it as the primary textbook for my applied machine learning class for undergraduate juniors and seniors in science and engineering. Chapter 1 provides an overview of the main concepts associated with developing and using ML models for decision making. It includes discussions of traditional topics like overfitting and emerging issues like bias and artificial intelligence (AI) ethics. The chapter structure follows the same pattern as previous editions, so knn, Naive Bayes, decision trees, four neural networks and SVMs, association rules, k-means, and performance are all covered. Chapter 12 on specialized machine learning topics is significantly updated from previous editions and now covers tidyverse, domain-specific data, and brief examinations of performance optimization techniques like parallelization, MapReduce, Hadoop, and Spark. In most chapters, there are fully reproducible examples clearly broken down into steps. Within those steps, subtasks (for example, transformation, data preparation, model specification) are also clearly specified, making it clear how to structure different types of problems. This book is excellent for beginners and others who want to use R to learn how to skillfully address ML problems using their own data.","",""
2,"J. Prešern, M. S. Smodiš Škerl","Parameters influencing queen body mass and their importance as determined by machine learning in honey bees (Apis mellifera carnica)",2019,"","","","",125,"2022-07-13 09:37:52","","10.1007/s13592-019-00683-y","","",,,,,2,0.67,1,2,3,"","",""
9,"F. Maes, D. Robben, D. Vandermeulen, P. Suetens","The Role of Medical Image Computing and Machine Learning in Healthcare",2019,"","","","",126,"2022-07-13 09:37:52","","10.1007/978-3-319-94878-2_2","","",,,,,9,3.00,2,4,3,"","",""
1,"Quico Spaen","Applications and Advances in Similarity-based Machine Learning",2019,"","","","",127,"2022-07-13 09:37:52","","","","",,,,,1,0.33,1,1,3,"Author(s): Spaen, Quico Pepijn | Advisor(s): Hochbaum, Dorit S | Abstract: Similarity-based machine learning methods differ from traditional machine learning methods in that they also use pairwise similarity relations between objects to infer the labels of unlabeled objects. A recent comparative study for classification problems by Baumann et al. [2019] demonstrated that similarity-based techniques have superior performance and robustness when compared to well-established machine learning techniques. Similarity-based machine learning methods benefit from two advantages that could explain superior their performance: They can make use of the pairwise relations between unlabeled objects, and they are robust due to the transitive property of pairwise similarities. A challenge for similarity-based machine learning methods on large datasets is that the number of pairwise similarity grows quadratically in the size of the dataset. For large datasets, it thus becomes practically impossible to compute all possible pairwise similarities. In 2016, Hochbaum and Baumann proposed the technique of sparse computation to address this growth by computing only those pairwise similarities that are relevant. Their proposed implementation of sparse computation is still difficult to scale to millions objects. This dissertation focuses on advancing the practical implementations of sparse computation to larger datasets and on two applications for which similarity-based machine learning was particularly effective. The applications that are studied here are cell identification in calcium-imaging movies and detecting aberrant linking behavior in directed networks. For sparse computation we present faster, geometric algorithms and a technique, named sparse-reduced computation, that combines sparse computation with compression. The geometric algorithms compute the exact same output as the original implementation of sparse computation, but identify the relevant pairwise similarities faster by using the concept of data shifting for identifying objects in the same or neighboring blocks. Empirical results on datasets with up to 10 million objects show a significant reduction in running time. Sparse-reduced computation combines sparse computation with a technique for compressing highly-similar or identical objects, enabling the use of similarity-based machine learning on massively-large datasets. The computational results demonstrate that sparse-reduced computation provides a significant reduction in running time with a minute loss in accuracy.A major problem facing neuroscientists today is cell identification in calcium-imaging movies. These movies are in-vivo recordings of thousands of neurons at cellular resolution. There is a great need for automated approaches to extract the activity of single neurons from these movies since manual post-processing takes tens of hours per dataset. We present the HNCcorr algorithm for cell identification in calcium-imaging movies. The name HNCcorr is derived from its use of the similarity-based Hochbaum's Normalized Cut (HNC) model with pairwise similarities derived from correlation. In HNCcorr, the task of cell detection is approached as a clustering problem. HNCcorr utilizes HNC to detect cells in these movies as coherent clusters of pixels that are highly distinct from the remaining pixels. HNCcorr guarantees, unlike existing methodologies for cell identification, a globally optimal solution to the underlying optimization problem. Of independent interest is a novel method, named similarity-squared, that we devised for measuring similarity between pixels. We provide an experimental study and demonstrate that HNCcorr is a top performer on the Neurofinder cell identification benchmark and that it improves over algorithms based on matrix factorization.The second application is detecting aberrant agents, such as fake news sources or spam websites, based on their link behavior in networks. Across contexts, a distinguishing characteristic between normal and aberrant agents is that normal agents rarely link to aberrant ones. We refer to this phenomenon as aberrant linking behavior. We present an Markov Random Fields (MRF) formulation, with links as the pairwise similarities, that detects aberrant agents based on aberrant linking behavior and any prior information (if given). This MRF formulation is solved optimally and in polynomial time. We compare the optimal solution for the MRF formulation to well-known algorithms based on random walks. In our empirical experiment with twenty-three different datasets, the MRF method outperforms the other detection algorithms. This work represents the first use of optimization methods for detecting aberrant agents as well as the first time that MRF is applied to directed graphs.","",""
1,"C. Coglianese","Deploying Machine Learning for a Sustainable Future",2020,"","","","",128,"2022-07-13 09:37:52","","10.2307/j.ctvqc6gcq.26","","",,,,,1,0.50,1,1,2,"To meet the environmental challenges of a warming planet and an increasingly complex, high tech economy, government must become smarter about how it makes policies and deploys its limited resources. It specifically needs to build a robust capacity to analyze large volumes of environmental and economic data by using machine-learning algorithms to improve regulatory oversight, monitoring, and decision-making. Three challenges can be expected to drive the need for algorithmic environmental governance: more problems, less funding, and growing public demands. This paper explains why algorithmic governance will prove pivotal in meeting these challenges, but it also presents four likely obstacles that environmental agencies will need to surmount if they are to take full advantage of big data and predictive analytics. First, agencies must invest in upgrading their information technology infrastructure to take advantage of computational advances. Relatively modest technology investments, if made wisely, could support the use of algorithmic tools that could yield substantial savings in other administrative costs. Second, agencies will need to confront emerging concerns about privacy, fairness, and transparency associated with its reliance on Big Data and algorithmic analyses. Third, government agencies will need to strengthen their human capital so that they have the personnel who understand how to use machine learning responsibly. Finally, to work well, algorithms will need clearly defined objectives. Environmental officials will need to continue to engage with elected officials, members of the public, environmental groups, and industry representatives to forge clarity and consistency over how various risk and regulatory objectives should be specified in machine learning tools. Overall, with thoughtful planning, adequate resources, and responsible management, governments should be able to overcome the obstacles that stand in the way of the use of artificial intelligence to improve environmental sustainability. If policy makers and the public will recognize the need for smarter governance, they can then start to tackle obstacles that stand in its way and better position society for a more sustainable future.","",""
1,"Anika Gebauer, Monja Ellinger, Victor M. Brito Gómez, Mareike Ließ","Development of pedotransfer functions for tropical mountain soilscapes: Spotlight on parameter tuning in machine learning",2019,"","","","",129,"2022-07-13 09:37:52","","10.5194/soil-2019-72","","",,,,,1,0.33,0,4,3,"Abstract. Machine learning algorithms are good in computing non-linear problems and fitting complex composite functions, which makes them an adequate tool to address multiple environmental research questions. One important application is the development of pedotransfer functions (PTF). This study aims to develop water retention PTFs for two remote tropical mountain regions of rather different soil-landscapes, dominated by (1) organic soils under volcanic influence, and (2) tropical mineral soils. Two tuning procedures were compared to fit boosted regression tree models: (1) tuning by grid search, which is the standard approach in pedometrics and (2) tuning by differential evolution optimization. A nested cross-validation approach was applied to generate robust models. The developed area-specific PTFs outrival other more general PTFs. Furthermore, the first PTF for typical soils of Páramo landscapes, i.e. organic soils under volcanic influence, is presented. Overall, results confirmed the differential evolution algorithm’s high potential for tuning machine learning models. While models based on tuning by grid search roughly predicted the response variables' mean for both areas, models applying the differential evolution algorithm for parameter tuning explained up to 22 times more of the response variables' variance. ","",""
1,"William Briguglio, Sherif Saad","Interpreting Machine Learning Malware Detectors Which Leverage N-gram Analysis",2019,"","","","",130,"2022-07-13 09:37:52","","10.1007/978-3-030-45371-8_6","","",,,,,1,0.33,1,2,3,"","",""
1,"Sicheng Jiang, Sirui Lu, D. Deng","Adversarial Machine Learning Phases of Matter",2019,"","","","",131,"2022-07-13 09:37:52","","","","",,,,,1,0.33,0,3,3,"We study the robustness of machine learning approaches to adversarial perturbations, with a focus on supervised learning scenarios. We find that typical phase classifiers based on deep neural networks are extremely vulnerable to adversarial perturbations: adding a tiny amount of carefully crafted noises into the original legitimate examples will cause the classifiers to make incorrect predictions at a notably high confidence level. Through the lens of activation maps, we find that some important underlying physical principles and symmetries remain to be adequately captured for classifiers with even near-perfect performance. This explains why adversarial perturbations exist for fooling these classifiers. In addition, we find that, after adversarial training the classifiers will become more consistent with physical laws and consequently more robust to certain kinds of adversarial perturbations. Our results provide valuable guidance for both theoretical and experimental future studies on applying machine learning techniques to condensed matter physics.","",""
0,"D. Ting, L. Peng, A. Varadarajan, Pearse Keane FRCOphth, P. Burlina, M. Chiang, L. Schmetterer, L. Pasquale, N. Bressler, D. Webster, M. Abràmoff, T. Y. Wong","Artificial Intelligence, Machine Learning and Deep Learning in Ophthalmology: Current Clinical Relevance",2019,"","","","",132,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,12,3,"With the advent of computer graphic processing units, improvement in mathematical models and availability of big data, artificial intelligence (AI) using machine learning (ML) and deep learning (DL) techniques have achieved robust performance for potential application across many industries, including social-media, the internet of things, the automotive industry and healthcare. DL systems provide capability in image, speech and motion recognition as well as in natural language processing. In medicine, most of the progress of AI, ML and DL systems has been demonstrated in image-centric specialties such as radiology, dermatology and pathology. There is increasing interest in AI in ophthalmology. New studies, including pre-registered prospective clinical trials, have shown DL systems are effective in detecting diabetic retinopathy (DR), glaucoma, age-related macular degeneration, retinopathy of prematurity, refractive error and in identifying cardiovascular risk factors and diseases, using image based data such as fundus photographs and optical coherence tomography. Additionally, the application of ML to Humphrey visual fields may be useful in detecting glaucoma progression. There are fewer studies that incorporate clinical data in AL algorithms and no prospective studies to demonstrate that AI algorithms can predict the development of eye disease. This article describes the current global eye disease burden, clinical unmet needs and selected common ophthalmic conditions of public health importance for which AI and DL systems may be applicable. Technical and clinical aspects to build a DL system to address those gaps, and the potential challenges for clinical adoption are discussed. AI, ML and DL likely will play a crucial role in clinical ophthalmology practice, with implications for screening, diagnosis and follow up of the major causes of vision impairment, in the setting of the ageing population globally. Introduction With the advent of graphic processing units (GPUs), advances in mathematical models, the availability of big datasets and low cost sensors, artificial intelligence (AI) using machine learning (ML) techniques initially and deep learning (DL) techniques subsequently, has sparked tremendous interest in many industries. These include application of AI in social-media, the internet of things, finance and banking, the automotive industry and healthcare. AI systems can be designed not only for image, speech and motion recognition, but also in natural language processing. In medicine, the most robust AI algorithms have been demonstrated in image-centric specialties, including radiology, dermatology, pathology and increasingly so in ophthalmology. For example, Lakhani et al demonstrated excellent performance in detecting pulmonary tuberculosis from chest radiographs, while Esteva et al was able to differentiate malignant melanoma from benign lesions on skin photographs. In ophthalmology, there have been two major areas in which AI and new DL systems have been applied. First, AI systems have been shown in new studies, including preregistered prospective clinical trials, to accurately detect diabetic retinopathy (DR), 13 glaucoma, age-related macular degeneration (AMD), retinopathy of prematurity (ROP), and refractive error, from digital fundus photographs. A range of cardiovascular risk factors have also been accurately predicted from fundus photographs. Second, several retinal conditions [e.g., neovascular AMD, earlier stages of AMD, and diabetic macular edema (DME)] has also be detected accurately using optical coherence tomography (OCT). There are relatively fewer AI studies using other data, such as studies which show good performance in detecting glaucoma progression from serial Humphrey visual fields (HVFs). However, there are fewer studies that incorporate clinical and imaging data in AL algorithms, and no prospective studies to demonstrate that AI algorithms can predict the development of eye diseases over time. Furthermore, the implementation and adoption of AI into routine clinical care remains extremely challenging. These remain significant goals of AI research in ophthalmology This article describes basic concepts of AI, ML and DL and how such systems might address some of the global burdens created by common eye conditions. Furthermore, the technical and clinical aspects of developing and validating an AI/DL system, potential challenges and future directions are also discussed in this article. Artificial Intelligence, Machine Learning and Deep Learning AI was conceptualized in 1956, after a workshop at Dartmouth College (Figure 1). In the workshop, many AI groups showed promising results in computer learning of checkers strategies, solving word problems in algebra and proving logical theorems. These tasks involved mostly pattern recognition and computational learning. All AI systems were designed to execute and maximise its chance of ‘winning’ within a constructed environment. The term ‘machine learning’ (ML) was subsequently coined by Arthur Samuel in 1959 and stated that “the computer should have the ability to learn using various statistical techniques, without being explicitly programmed”. Using ML, the algorithm can learn and make predictions based on the data that has been fed into the training phase, using either a supervised or unsupervised approach. ML has been widely adopted in applications such as computer vision and predictive analytics using complex mathematical models. In supervised learning, the computer is trained with labelled examples, also known as ground truth, whereas for unsupervised learning, no labelling is required for the algorithm to find its own structure in the input. The majority of AI application in biomedical research uses supervised learning. DL utilizes multiple processing layers to learn representation of data with multiple levels of abstraction. Although some forms of deep neural networks have already been investigated in the past, the advent of graphic processing units (GPU) with improved processing power, larger annotated datasets, and other factors, have recently boosted its diagnostic performance in many domains. Using learning approaches such as backpropagation, a ML or DL system is able to discover intricate structure in large data sets, then changing its internal parameters that are used to compute the representation in each layer from the previous one. These approaches permit the use of regional samples to allow the network to learn to detect biomarkers; furthermore these approaches use complete images, and associate the entire image with a diagnostic output, thereby eliminating the use of “hand-engineered” image features. Given the much improved performance, DL has been widely adopted in image recognition, speech recognition and natural language processing. General Approach in Building a Robust AI system This section explains some common terminologies, software framework, network architectures, datasets selection, assistive vs. autonomous AI system, consideration factors to ensure the robustness of these algorithms (Table 1). In order to build a robust DL system, it is important to have 2 main components – the ‘brain’ (technical networks – Convolutional Neural Network (CNN) and the ‘dictionary’ (the datasets). 1. What is a CNN? A CNN is a deep neural network consisting of a cascade of processing layers that resemble the biological processes of the animal visual cortex. It transforms the input volume into an output volume via a differentiable function. Inspired by Hubel and Weisel, each neuron in the visual cortex will respond to the stimulus that is specific to a region within an image, similar to how the brain neuron would respond to the visual stimuli, that will activate a particular region of the visual space, known as the receptive field. These receptive fields are tiled together to cover the entire visual field. Two classes of cells are found in this region – simple vs complex cells. The simple cells active when they detect edge-like patterns, while the more complex cells activate when they have a larger receptive field and are invariant to the position of the pattern. Broadly, the CNN can be divided into the input, hidden (also known as featureextraction layers) and output layers (Figure 2A). The hidden layers usually consist of convolutional, pooling, fully connected and normalization layers, and the number of hidden layers will differ for different CNNs. The input layer specifies the width, height and the number of channels (usually 3 channels – red, green and blue). The convolutional layer is the core building block of a CNN, transforming the input data by applying a set of filters (also known as kernels) that acts as the feature detectors. The filter will slide over the input image to produce a feature map (as the output). A CNN learns the values of these filters weights on its own during the training process, although the specific parameters such as number of filters, filter size, network architecture still need to be set prior to that. Additional operations called activations (for example ReLU or Rectified Linear Unit) are used after every convolution operation. For pooling, the aim is to reduce the dimensionality of each feature map and make it somewhat spatially invariant, and retain the most important information. Pooling can be divided into different types: maximum, average and minimum. In the case of maximum pooling, the largest element from the rectified feature map will be taken (Figure 2B). The output from the convolutional and pooling layers represent the high-level features of the input image. The purpose of the fully connected layer is to use these high-level features to classify the input image into various classes based on the training dataset. Following which, backpropagation is conducted to compute the network weights and uses the gradient descent to update all filters and parameter values to minimize the output error. T","",""
0,"R. Shokri","Trusting Machine Learning: Privacy, Robustness, and Transparency Challenges",2019,"","","","",133,"2022-07-13 09:37:52","","10.1145/3335203.3335728","","",,,,,0,0.00,0,1,3,"Machine learning algorithms have shown an unprecedented predictive power for many complex learning tasks. As they are increasingly being deployed in large scale critical applications for processing various types of data, new questions related to their trustworthiness would arise. Can machine learning algorithms be trusted to have access to individuals’ sensitive data? Can they be robust against noisy or adversarially perturbed data? Can we reliably interpret their learning process, and explain their predictions? In this talk, I will go over the challenges of building trustworthy machine learning algorithms in centralized and distributed (federated) settings, and will discuss the inter-relation between privacy, robustness, and interpretability.","",""
6,"A. Raef, M. Totten, Aria Linares, A. Kamari","Lithofacies Control on Reservoir Quality of the Viola Limestone in Southwest Kansas and Unsupervised Machine Learning Approach of Seismic Attributes Facies-Classification",2019,"","","","",134,"2022-07-13 09:37:52","","10.1007/s00024-019-02205-4","","",,,,,6,2.00,2,4,3,"","",""
34,"D. Simester, Artem Timoshenko, S. Zoumpoulis","Targeting Prospective Customers: Robustness of Machine-Learning Methods to Typical Data Challenges",2020,"","","","",135,"2022-07-13 09:37:52","","10.1287/mnsc.2019.3308","","",,,,,34,17.00,11,3,2,"We investigate how firms can use the results of field experiments to optimize the targeting of promotions when prospecting for new customers. We evaluate seven widely used machine-learning methods using a series of two large-scale field experiments. The first field experiment generates a common pool of training data for each of the seven methods. We then validate the seven optimized policies provided by each method together with uniform benchmark policies in a second field experiment. The findings not only compare the performance of the targeting methods, but also demonstrate how well the methods address common data challenges. Our results reveal that when the training data are ideal, model-driven methods perform better than distance-driven methods and classification methods. However, the performance advantage vanishes in the presence of challenges that affect the quality of the training data, including the extent to which the training data captures details of the implementation setting. The challenges we study are covariate shift, concept shift, information loss through aggregation, and imbalanced data. Intuitively, the model-driven methods make better use of the information available in the training data, but the performance of these methods is more sensitive to deterioration in the quality of this information. The classification methods we tested performed relatively poorly. We explain the poor performance of the classification methods in our setting and describe how the performance of these methods could be improved. This paper was accepted by Matthew Shum, marketing.","",""
29,"Yuhui Zheng, Le Sun, Shunfeng Wang, Jianwei Zhang, J. Ning","Spatially Regularized Structural Support Vector Machine for Robust Visual Tracking",2019,"","","","",136,"2022-07-13 09:37:52","","10.1109/TNNLS.2018.2855686","","",,,,,29,9.67,6,5,3,"Structural support vector machine (SSVM) is popular in the visual tracking field as it provides a consistent target representation for both learning and detection. However, the spatial distribution of feature is not considered in standard SSVM-based trackers, therefore leading to limited performance. To obtain a robust discriminative classifier, this paper proposes a novel tracking framework that spatially regularizes SSVM, which yields a new spatially regularized SSVM (SRSSVM). We utilize the spatial regularization prior to penalize the learning classifier with the same size as the target region. The location of classifier spatially located far from the center of region is assigned large weight and vice versa. Then, it is introduced into the SSVM model as a regularization factor to learn the robust discriminative model. Furthermore, an optimizing algorithm with dual coordination descent is presented to efficiently solve the SRSSVM tracking model. Our proposed SRSSVM tracking method has low computational cost like the traditional linear SSVM tracker while can significantly improve the robustness of the discriminative classifier. The experimental results on three popular tracking benchmark data sets show that the proposed SRSSVM tracking method performs favorably against the state-of-the-art trackers.","",""
8,"Dongqi Han, Zhiliang Wang, Ying Zhong, Wenqi Chen, Jiahai Yang, Shuqiang Lu, Xingang Shi, Xia Yin","Evaluating and Improving Adversarial Robustness of Machine Learning-Based Network Intrusion Detectors",2020,"","","","",137,"2022-07-13 09:37:52","","10.1109/JSAC.2021.3087242","","",,,,,8,4.00,1,8,2,"Machine learning (ML), especially deep learning (DL) techniques have been increasingly used in anomaly-based network intrusion detection systems (NIDS). However, ML/DL has shown to be extremely vulnerable to adversarial attacks, especially in such security-sensitive systems. Many adversarial attacks have been proposed to evaluate the robustness of ML-based NIDSs. Unfortunately, existing attacks mostly focused on feature-space and/or white-box attacks, which make impractical assumptions in real-world scenarios, leaving the study on practical gray/black-box attacks largely unexplored. To bridge this gap, we conduct the first systematic study of the gray/black-box traffic-space adversarial attacks to evaluate the robustness of ML-based NIDSs. Our work outperforms previous ones in the following aspects: (i) practical —the proposed attack can automatically mutate original traffic with extremely limited knowledge and affordable overhead while preserving its functionality; (ii) generic —the proposed attack is effective for evaluating the robustness of various NIDSs using diverse ML/DL models and non-payload-based features; (iii) explainable —we propose an explanation method for the fragile robustness of ML-based NIDSs. Based on this, we also propose a defense scheme against adversarial attacks to improve system robustness. We extensively evaluate the robustness of various NIDSs using diverse feature sets and ML/DL models. Experimental results show our attack is effective (e.g., >97% evasion rate in half cases for Kitsune, a state-of-the-art NIDS) with affordable execution cost and the proposed defense method can effectively mitigate such attacks (evasion rate is reduced by >50% in most cases).","",""
4,"Abderrahmen Amich, Birhanu Eshete","Explanation-Guided Diagnosis of Machine Learning Evasion Attacks",2021,"","","","",138,"2022-07-13 09:37:52","","10.1007/978-3-030-90019-9_11","","",,,,,4,4.00,2,2,1,"","",""
4,"E. Glaab, Armin Rauschenberger, R. Banzi, C. Gerardi, Paula Garcia, J. Demotes","Biomarker discovery studies for patient stratification using machine learning analysis of omics data: a scoping review",2021,"","","","",139,"2022-07-13 09:37:52","","10.1136/bmjopen-2021-053674","","",,,,,4,4.00,1,6,1,"Objective To review biomarker discovery studies using omics data for patient stratification which led to clinically validated FDA-cleared tests or laboratory developed tests, in order to identify common characteristics and derive recommendations for future biomarker projects. Design Scoping review. Methods We searched PubMed, EMBASE and Web of Science to obtain a comprehensive list of articles from the biomedical literature published between January 2000 and July 2021, describing clinically validated biomarker signatures for patient stratification, derived using statistical learning approaches. All documents were screened to retain only peer-reviewed research articles, review articles or opinion articles, covering supervised and unsupervised machine learning applications for omics-based patient stratification. Two reviewers independently confirmed the eligibility. Disagreements were solved by consensus. We focused the final analysis on omics-based biomarkers which achieved the highest level of validation, that is, clinical approval of the developed molecular signature as a laboratory developed test or FDA approved tests. Results Overall, 352 articles fulfilled the eligibility criteria. The analysis of validated biomarker signatures identified multiple common methodological and practical features that may explain the successful test development and guide future biomarker projects. These include study design choices to ensure sufficient statistical power for model building and external testing, suitable combinations of non-targeted and targeted measurement technologies, the integration of prior biological knowledge, strict filtering and inclusion/exclusion criteria, and the adequacy of statistical and machine learning methods for discovery and validation. Conclusions While most clinically validated biomarker models derived from omics data have been developed for personalised oncology, first applications for non-cancer diseases show the potential of multivariate omics biomarker design for other complex disorders. Distinctive characteristics of prior success stories, such as early filtering and robust discovery approaches, continuous improvements in assay design and experimental measurement technology, and rigorous multicohort validation approaches, enable the derivation of specific recommendations for future studies.","",""
45,"H. Escalante, S. Escalera, I. Guyon, Xavier Baró, Yağmur Güçlütürk, Umut Güçlü, M. V. Gerven","Explainable and Interpretable Models in Computer Vision and Machine Learning",2018,"","","","",140,"2022-07-13 09:37:52","","10.1007/978-3-319-98131-4","","",,,,,45,11.25,6,7,4,"","",""
3,"Kai R. T. Larsen, Daniel S. Becker","Why Use Automated Machine Learning?",2021,"","","","",141,"2022-07-13 09:37:52","","10.1093/oso/9780190941659.003.0001","","",,,,,3,3.00,2,2,1,"Machine learning is involved in search, translation, detecting depression, likelihood of college dropout, finding lost children, and to sell all kinds of products. While barely beyond its inception, the current machine learning revolution will affect people and organizations no less than the Industrial Revolution’s effect on weavers and many other skilled laborers. Machine learning will automate hundreds of millions of jobs that were considered too complex for machines ever to take over even a decade ago, including driving, flying, painting, programming, and customer service, as well as many of the jobs previously reserved for humans in the fields of finance, marketing, operations, accounting, and human resources. This section explains how automated machine learning addresses exploratory data analysis, feature engineering, algorithm selection, hyperparameter tuning, and model diagnostics. The section covers the eight criteria considered essential for AutoML to have significant impact: accuracy, productivity, ease of use, understanding and learning, resource availability, process transparency, generalization, and recommended actions. ","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",142,"2022-07-13 09:37:52","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
2,"A. Ayobi, Katarzyna Stawarz, Dmitri S. Katz, P. Marshall, Taku Yamagata, Raúl Santos-Rodríguez, Peter A. Flach, A. O'Kane","Machine Learning Explanations as Boundary Objects: How AI Researchers Explain and Non-Experts Perceive Machine Learning",2021,"","","","",143,"2022-07-13 09:37:52","","","","",,,,,2,2.00,0,8,1,"Understanding artificial intelligence (AI) and machine learning (ML) approaches is becoming increasingly important for people with a wide range of professional backgrounds. However, it is unclear how ML concepts can be effectively explained as part of human-centred and multidisciplinary design processes. We provide a qualitative account of how AI researchers explained and non-experts perceived ML concepts as part of a co-design project that aimed to inform the design of ML applications for diabetes self-care. We identify benefits and challenges of explaining ML concepts with analogical narratives, information visualisations, and publicly available videos. Co-design participants reported not only gaining an improved understanding of ML concepts but also highlighted challenges of understanding ML explanations, including misalignments between scientific models and their lived self-care experiences and individual information needs. We frame our findings through the lens of Stars and Griesemer’s concept of boundary objects to discuss how the presentation of user-centred ML explanations could strike a balance between being plastic and robust enough to support design objectives and people’s individual information needs.","",""
1,"Nicolas Jourdan, S. Sen, E. J. Husom, Enrique Garcia-Ceja, Tobias Biegel, J. Metternich","On The Reliability Of Machine Learning Applications In Manufacturing Environments",2021,"","","","",144,"2022-07-13 09:37:52","","","","",,,,,1,1.00,0,6,1,"The increasing deployment of advanced digital technologies such as Internet of Things (IoT) devices and Cyber-Physical Systems (CPS) in industrial environments is enabling the productive use of machine learning (ML) algorithms in the manufacturing domain. As ML applications transcend from research to productive use in real-world industrial environments, the question of reliability arises. Since the majority of ML models are trained and evaluated on static datasets, continuous online monitoring of their performance is required to build reliable systems. Furthermore, concept and sensor drift can lead to degrading accuracy of the algorithm over time, thus compromising safety, acceptance and economics if undetected and not properly addressed. In this work, we exemplarily highlight the severity of the issue on a publicly available industrial dataset which was recorded over the course of 36 months and explain possible sources of drift. We assess the robustness of ML algorithms commonly used in manufacturing and show, that the accuracy strongly declines with increasing drift for all tested algorithms. We further investigate how uncertainty estimation may be leveraged for online performance estimation as well as drift detection as a first step towards continually learning applications. The results indicate, that ensemble algorithms like random forests show the least decay of confidence calibration under drift.","",""
1,"C. Betancourt, S. Stadtler, T. Stomberg, Ann-Kathrin Edrich, Ankit Patnala, R. Roscher, J. Kowalski, M. Schultz","Global fine resolution mapping of ozone metrics through explainable machine learning",2021,"","","","",145,"2022-07-13 09:37:52","","10.5194/EGUSPHERE-EGU21-7596","","",,,,,1,1.00,0,8,1,"<p>Through the availability of multi-year ground based ozone observations on a global scale, substantial geospatial meta data, and high performance computing capacities, it is now possible to use machine learning for a global data-driven ozone assessment. In this presentation, we will show a novel, completely data-driven approach to map tropospheric ozone globally.</p><p>Our goal is to interpolate ozone metrics and aggregated statistics from the database of the Tropospheric Ozone Assessment Report (TOAR) onto a global 0.1&#176; x 0.1&#176; resolution grid. &#160;It is challenging to interpolate ozone, a toxic greenhouse gas because its formation depends on many interconnected environmental factors on small scales. We conduct the interpolation with various machine learning methods trained on aggregated hourly ozone data from five years at more than 5500 locations worldwide. We use several geospatial datasets as training inputs to provide proxy input for environmental factors controlling ozone formation, such as precursor emissions and climate. The resulting maps contain different ozone metrics, i.e. statistical aggregations which are widely used to assess air pollution impacts on health, vegetation, and climate.</p><p>The key aspects of this contribution are twofold: First, we apply explainable machine learning methods to the data-driven ozone assessment. Second, we discuss dominant uncertainties relevant to the ozone mapping and quantify their impact whenever possible. Our methods include a thorough a-priori uncertainty estimation of the various data and methods, assessment of scientific consistency, finding critical model parameters, using ensemble methods, and performing error modeling.</p><p>Our work aims to increase the reliability and integrity of the derived ozone maps through the provision of scientific robustness to a data-centric machine learning task. This study hence represents a blueprint for how to formulate an environmental machine learning task scientifically, gather the necessary data, and develop a data-driven workflow that focuses on optimizing transparency and applicability of its product to maximize its scientific knowledge return.</p>","",""
1,"Tonni Das","Performance Analysis of Quantum Machine Learning Classifiers",2021,"","","","",146,"2022-07-13 09:37:52","","","","",,,,,1,1.00,1,1,1,"In recent years, researchers have started looking into data transformations in quantum computation. They want to see how quantum computing affects the robustness and performance of machine learning methods. Quantum mechanics succeed in explaining some phenomena where classical formulas failed in the past. Thus, it expanded in analytical research fields such as Quantum Machine Learning (QML) over the years. The developing QML discipline has proven solutions to issues that are equivalent (or comparable) to those addressed by classical machine learning, including classification and prediction problems using quantum classifiers. As a result of these factors, quantum classifier analysis has become one of the most important topics in QML. This paper studies four quantum classifiers: Support Vector Classification with Quantum Kernel (SVCQK), Quantum Support Vector Classifier (QSVC), Variational Quantum Classifier (VQC), and Circuit Quantum Neural Network Classifier (CQNNC). We also report case study outcomes and results analysis utilizing linearly and non-linearly separable datasets generated. Our research is to explore if quantum information may aid learning or convergence. 35th Conference on Neural Information Processing Systems (NeurIPS 2021), Sydney, Australia.","",""
2,"Xin-Yi Song, Ya Gao, Yubo Peng, Sen Huang, Chao Liu, Zhong-ren Peng","A machine learning approach to modelling the spatial variations in the daily fine particulate matter (PM2.5) and nitrogen dioxide (NO2) of Shanghai, China",2021,"","","","",147,"2022-07-13 09:37:52","","10.1177/2399808320975031","","",,,,,2,2.00,0,6,1,"It is challenging to forecast high-resolution spatial-temporal patterns of intra-urban air pollution and identify impacting factors at the regional scale. Studies have attempted to capture features of air pollutants such as fine particulate matter (PM2.5) and nitrogen dioxide (NO2) using land use regression models, but this method overlooks the multi-collinearity of factors, non-linear correlations between factors and air pollutants, and it fails to perform well when processing daily data. However, machine learning is a feasible approach for establishing persuasive intra-urban air pollution daily variation models. In this article, random forest is utilised to establish intra-urban PM2.5 and NO2 spatial-temporal variation models and is compared to the traditional land use regression method. Taking the city of Shanghai, China as the case area, 36 station-measured daily records in two and a half years of PM2.5 and NO2 concentrations were collected. And over 80 different predictors associated with meteorological and geographical conditions, transportation, community population density, land use and points of interest are used to construct the land use regression and random forest models. Results from the two methods are compared and impacting factors identified. Explained variance (R2) is used to quantify and compare model performance. The final land use regression model explains 49.3% and 42.2% of the spatial variation in ambient PM2.5 and NO2, respectively, whereas the random forest model explains 78.1% and 60.5% of the variance. Regression mappings for unsampled sites on a grid pattern of 1 km × 1 km are also implemented. The random forest model is shown to perform much better than the land use regression model. In general, the findings suggest that the random forest approach offers a robust improvement in predicting performance compared to the land use regression model in estimating daily spatial variations in ambient PM2.5 and NO2.","",""
2,"K. Morik, Helena Kotthaus, Lukas Heppe, Danny Heinrich, Raphael Fischer, Andrea Pauly, N. Piatkowski","The Care Label Concept: A Certification Suite for Trustworthy and Resource-Aware Machine Learning",2021,"","","","",148,"2022-07-13 09:37:52","","","","",,,,,2,2.00,0,7,1,"Machine learning applications have become ubiquitous. This has led to an increased effort of making machine learning trustworthy. Explainable and fair AI have already matured. They address knowledgeable users and application engineers. For those who do not want to invest time into understanding the method or the learned model, we offer care labels: easy to understand at a glance, allowing for method or model comparisons, and, at the same time, scientifically well-based. On one hand, this transforms descriptions as given by, e.g., Fact Sheets or Model Cards, into a form that is well-suited for end-users. On the other hand, care labels are the result of a certification suite that tests whether stated guarantees hold. In this paper, we present two experiments with our certification suite. One shows the care labels for configurations of Markov random fields (MRFs). Based on the underlying theory of MRFs, each choice leads to its specific rating of static properties like, e.g., expressivity and reliability. In addition, the implementation is tested and resource consumption is measured yielding dynamic properties. This two-level procedure is followed by another experiment certifying deep neural network (DNN) models. There, we draw the static properties from literature on a particular model and data set. At the second level, experiments are generated that deliver measurements of robustness against certain attacks. We illustrate this by ResNet-18 and MobileNetV3 applied to ImageNet.","",""
1,"Ivan Ny Hanitra, F. Criscuolo, S. Carrara, G. De Micheli","Multi-Ion-Sensing Emulator and Multivariate Calibration Optimization by Machine Learning Models",2021,"","","","",149,"2022-07-13 09:37:52","","10.1109/ACCESS.2021.3065754","","",,,,,1,1.00,0,4,1,"One paramount challenge in multi-ion-sensing arises from ion interference that degrades the accuracy of sensor calibration. Machine learning models are here proposed to optimize such multivariate calibration. However, the acquisition of big experimental data is time and resource consuming in practice, necessitating new paradigms and efficient models for these data-limited frameworks. Therefore, a novel approach is presented in this work, where a multi-ion-sensing emulator is designed to explain the response of an ion-sensing array in a mixed-ion environment. A case study is performed emulating the concurrent monitoring of sodium, potassium, lithium, and lead ions, in a medium representative of sweat samples. These analytes are relevant examples of sweat ion-sensing applications for physiology, therapeutic drug monitoring, and heavy metal contamination. It is demonstrated that calibration datasets output by the emulator explain accurately the experimental response of polymeric solid-contact ion-selective electrodes, where root-mean-squared error of 1.37, 1.44, 1.78, $2\,mV$ are obtained, respectively, for Na+, K+, Li+, Pb2+ sensor calibration in artificial sweat. Besides, synthetic datasets of custom size are generated to train, validate, and evaluate different types of multivariate regressors. A Multi-Output Support Vector Regressor (M-SVR) is proposed as a compact, accurate, robust, and efficient multivariate calibration model. It features 13.22% normalized root mean squares, and 20.29% mean root squares improvement compared to a simple linear regression model. It is an unbiased estimator for medium to large datasets, and its average generalization error is of 3.22%. Besides, M-SVR models have a lower computational complexity than single-output SVR or neural network models, making them a suitable solution for memory and energy-constrained edge devices used for continuous and real-time multi-ion monitoring.","",""
1,"Khansa Rasheed, A. Qayyum, M. Ghaly, A. Al-Fuqaha, A. Razi, Junaid Qadir","Explainable, Trustworthy, and Ethical Machine Learning for Healthcare: A Survey",2021,"","","","",150,"2022-07-13 09:37:52","","10.36227/TECHRXIV.14376179.V1","","",,,,,1,1.00,0,6,1,"With the advent of machine learning (ML) applications in daily life, the questions about liability, trust, and interpretability of their outputs are raising, especially for healthcare applications. The black-box nature of ML models is a roadblock for clinical utilization. Therefore, to gain the trust of clinicians and patients, researchers need to provide explanations of how and why the model is making a specific decision. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the field of eXplainable ML (XML). In this paper, we provide a comprehensive review of explainable and interpretable ML techniques implemented for providing the reasons behind their decisions for various healthcare applications. Along with highlighting various security, safety, and robustness challenges that hinder the trustworthiness of ML we also discussed the ethical issues of healthcare ML and describe how explainable and trustworthy ML can resolve these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.","",""
0,"Bernhard Kühn, Marc H. Taylor, A. Kempf","Using machine learning to link spatiotemporal information to biological processes in the ocean: a case study for North Sea cod recruitment",2021,"","","","",151,"2022-07-13 09:37:52","","10.3354/MEPS13689","","",,,,,0,0.00,0,3,1,"Marine organisms are subject to environmental variability on various temporal and spatial scales, which affect processes related to growth and mortality of different life stages. Marine scientists are often faced with the challenge of identifying environmental variables that best explain these processes, which, given the complexity of the interactions, can be like searching for a needle in the proverbial haystack. Even after initial hypothesisbased variable selection, a large number of potential candidate variables can remain if different lagged and seasonal influences are considered. To tackle this problem, we propose a machine learning framework that incorporates important steps in model building, ranging from environmental signal extraction to automated variable selection and model validation. Its modular structure allows for the inclusion of both parametric and machine learning models, like random forest. Unsupervised feature extractions via empirical orthogonal functions (EOFs) or self-organising maps (SOMs) are demonstrated as a way to summarize spatiotemporal fields for inclusion in predictive models. The proposed framework offers a robust way to reduce model complexity through a multi-objective genetic algorithm (NSGAII) combined with rigorous cross-validation. We ap plied the framework to recruitment of the North Sea cod stock and investigated the effects of sea surface temperature (SST), salinity and currents on the stock via a modified version of random forest. The best model (5-fold CV r2 = 0.69) incorporated spawning stock biomass and EOF-derived time series of SST and salinity anomalies acting through different seasons, likely relating to differing environmental effects on specific life-history stages during the recruitment year. OPEN ACCESS Linking spatiotemporal information to North Sea cod recruitment via machine learning. Image: B. Kühn, M. Taylor; Gears from https:// commons. wikimedia.org/wiki/File:Gear_7.svg (CC-BY-SA license)","",""
0,"Jaehun Kim","Increasing trust in complex machine learning systems",2021,"","","","",152,"2022-07-13 09:37:52","","10.1145/3476415.3476435","","",,,,,0,0.00,0,1,1,"Machine learning (ML) has become a core technology for many real-world applications. Modern ML models are applied to unprecedentedly complex and difficult challenges, including very large and subjective problems. For instance, applications towards multimedia understanding have been advanced substantially. Here, it is already prevalent that cultural/artistic objects such as music and videos are analyzed and served to users according to their preference, enabled through ML techniques. One of the most recent breakthroughs in ML is Deep Learning (DL), which has been immensely adopted to tackle such complex problems. DL allows for higher learning capacity, making end-to-end learning possible, which reduces the need for substantial engineering effort, while achieving high effectiveness. At the same time, this also makes DL models more complex than conventional ML models. Reports in several domains indicate that such more complex ML models may have potentially critical hidden problems: various biases embedded in the training data can emerge in the prediction, extremely sensitive models can make unaccountable mistakes. Furthermore, the black-box nature of the DL models hinders the interpretation of the mechanisms behind them. Such unexpected drawbacks result in a significant impact on the trustworthiness of the systems in which the ML models are equipped as the core apparatus. In this thesis, a series of studies investigates aspects of trustworthiness for complex ML applications, namely the reliability and explainability. Specifically, we focus on music as the primary domain of interest, considering its complexity and subjectivity. Due to this nature of music, ML models for music are necessarily complex for achieving meaningful effectiveness. As such, the reliability and explainability of music ML models are crucial in the field. The first main chapter of the thesis investigates the transferability of the neural network in the Music Information Retrieval (MIR) context. Transfer learning, where the pre-trained ML models are used as off-the-shelf modules for the task at hand, has become one of the major ML practices. It is helpful since a substantial amount of the information is already encoded in the pre-trained models, which allows the model to achieve high effectiveness even when the amount of the dataset for the current task is scarce. However, this may not always be true if the ""source"" task which pre-trained the model shares little commonality with the ""target"" task at hand. An experiment including multiple ""source"" tasks and ""target"" tasks was conducted to examine the conditions which have a positive effect on the transferability. The result of the experiment suggests that the number of source tasks is a major factor of transferability. Simultaneously, it is less evident that there is a single source task that is universally effective on multiple target tasks. Overall, we conclude that considering multiple pre-trained models or pre-training a model employing heterogeneous source tasks can increase the chance for successful transfer learning. The second major work investigates the robustness of the DL models in the transfer learning context. The hypothesis is that the DL models can be susceptible to imperceptible noise on the input. This may drastically shift the analysis of similarity among inputs, which is undesirable for tasks such as information retrieval. Several DL models pre-trained in MIR tasks are examined for a set of plausible perturbations in a real-world setup. Based on a proposed sensitivity measure, the experimental results indicate that all the DL models were substantially vulnerable to perturbations, compared to a traditional feature encoder. They also suggest that the experimental framework can be used to test the pre-trained DL models for measuring robustness. In the final main chapter, the explainability of black-box ML models is discussed. In particular, the chapter focuses on the evaluation of the explanation derived from model-agnostic explanation methods. With black-box ML models having become common practice, model-agnostic explanation methods have been developed to explain a prediction. However, the evaluation of such explanations is still an open problem. The work introduces an evaluation framework that measures the quality of the explanations employing fidelity and complexity. Fidelity refers to the explained mechanism's coherence to the black-box model, while complexity is the length of the explanation. Throughout the thesis, we gave special attention to the experimental design, such that robust conclusions can be reached. Furthermore, we focused on delivering machine learning framework and evaluation frameworks. This is crucial, as we intend that the experimental design and results will be reusable in general ML practice. As it implies, we also aim our findings to be applicable beyond the music applications such as computer vision or natural language processing. Trustworthiness in ML is not a domain-specific problem. Thus, it is vital for both researchers and practitioners from diverse problem spaces to increase awareness of complex ML systems' trustworthiness. We believe the research reported in this thesis provides meaningful stepping stones towards the trustworthiness of ML.","",""
0,"I. Olier, Oghenejokpeme I. Orhobor, T. Dash, Andy M. Davis, L. Soldatova, J. Vanschoren, R. King","Transformational machine learning: Learning how to learn from many related scientific problems",2021,"","","","",153,"2022-07-13 09:37:52","","10.1073/pnas.2108013118","","",,,,,0,0.00,0,7,1,"Significance Machine learning (ML) is the branch of artificial intelligence (AI) that develops computational systems that learn from experience. In supervised ML, the ML system generalizes from labelled examples to learn a model that can predict the labels of unseen examples. Examples are generally represented using features that directly describe the examples. For instance, in drug design, ML uses features that describe molecular shape and so on. In cases where there are multiple related ML problems, it is possible to use a different type of feature: predictions made about the examples by ML models learned on other problems. We call this transformational ML. We show that this results in better predictions and improved understanding when applied to scientific problems. Almost all machine learning (ML) is based on representing examples using intrinsic features. When there are multiple related ML problems (tasks), it is possible to transform these features into extrinsic features by first training ML models on other tasks and letting them each make predictions for each example of the new task, yielding a novel representation. We call this transformational ML (TML). TML is very closely related to, and synergistic with, transfer learning, multitask learning, and stacking. TML is applicable to improving any nonlinear ML method. We tested TML using the most important classes of nonlinear ML: random forests, gradient boosting machines, support vector machines, k-nearest neighbors, and neural networks. To ensure the generality and robustness of the evaluation, we utilized thousands of ML problems from three scientific domains: drug design, predicting gene expression, and ML algorithm selection. We found that TML significantly improved the predictive performance of all the ML methods in all the domains (4 to 50% average improvements) and that TML features generally outperformed intrinsic features. Use of TML also enhances scientific understanding through explainable ML. In drug design, we found that TML provided insight into drug target specificity, the relationships between drugs, and the relationships between target proteins. TML leads to an ecosystem-based approach to ML, where new tasks, examples, predictions, and so on synergistically interact to improve performance. To contribute to this ecosystem, all our data, code, and our ∼50,000 ML models have been fully annotated with metadata, linked, and openly published using Findability, Accessibility, Interoperability, and Reusability principles (∼100 Gbytes).","",""
0,"Wenxiu Xie, Christine Ji, Tianyong Hao, Chi-Yin Chow","Predicting the Easiness and Complexity of English Health Materials for International Tertiary Students With Linguistically Enhanced Machine Learning Algorithms: Development and Validation Study.",2021,"","","","",154,"2022-07-13 09:37:52","","10.2196/25110","","",,,,,0,0.00,0,4,1,"BACKGROUND There is an increasing body of research on the development of machine learning algorithms in the evaluation of online health educational resources for specific readerships. Machine learning algorithms are known for their lack of interpretability compared with statistics. Given their high predictive precision, improving the interpretability of these algorithms can help increase their applicability and replicability in health educational research and applied linguistics, as well as in the development and review of new health education resources for effective and accessible health education.   OBJECTIVE Our study aimed to develop a linguistically enriched machine learning model to predict binary outcomes of online English health educational resources in terms of their easiness and complexity for international tertiary students.   METHODS Logistic regression emerged as the best performing algorithm compared with support vector machine (SVM) (linear), SVM (radial basis function), random forest, and extreme gradient boosting on the transformed data set using L2 normalization. We applied recursive feature elimination with SVM to perform automatic feature selection. The automatically selected features (n=67) were then further streamlined through expert review. The finalized feature set of 22 semantic features achieved a similar area under the curve, sensitivity, specificity, and accuracy compared with the initial (n=115) and automatically selected feature sets (n=67). Logistic regression with the linguistically enhanced feature set (n=22) exhibited important stability and robustness on the training data of different sizes (20%, 40%, 60%, and 80%), and showed consistently high performance when compared with the other 4 algorithms (SVM [linear], SVM [radial basis function], random forest, and extreme gradient boosting).   RESULTS We identified semantic features (with positive regression coefficients) contributing to the prediction of easy-to-understand online health texts and semantic features (with negative regression coefficients) contributing to the prediction of hard-to-understand health materials for readers with nonnative English backgrounds. Language complexity was explained by lexical difficulty (rarity and medical terminology), verbs typical of medical discourse, and syntactic complexity. Language easiness of online health materials was associated with features such as common speech act verbs, personal pronouns, and familiar reasoning verbs. Successive permutation of features illustrated the interaction between these features and their impact on key performance indicators of the machine learning algorithms.   CONCLUSIONS The new logistic regression model developed exhibited consistency, scalability, and, more importantly, interpretability based on existing health and linguistic research. It was found that low and high linguistic accessibilities of online health materials were explained by 2 sets of distinct semantic features. This revealed the inherent complexity of effective health communication beyond current readability analyses, which were limited to syntactic complexity and lexical difficulty.","",""
0,"Eike Petersen, Yannik Potdevin, Esfandiar Mohammadi, S. Zidowitz, Sabrina Breyer, Dirk Nowotka, Sandra Henn, Ludwig Pechmann, M. Leucker, P. Rostalski, C. Herzog","Responsible and Regulatory Conform Machine Learning for Medicine: A Survey of Technical Challenges and Solutions",2021,"","","","",155,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,11,1,"Machine learning is expected to fuel significant improvements in medical care. To ensure that fundamental principles such as beneficence, respect for human autonomy, prevention of harm, justice, privacy, and transparency are respected, medical machine learning applications must be developed responsibly. A large number of high-level declarations of ethical principles have been put forth for this purpose, but there is a severe lack of technical guidelines explicating the practical consequences for medical machine learning. Similarly, there is currently considerable uncertainty regarding the exact regulatory requirements placed upon medical machine learning systems. In this paper, we survey the technical challenges involved in creating medical machine learning systems responsibly and in conformity with existing regulations, as well as possible solutions to address these challenges. We begin by providing a brief overview of existing regulations affecting medical machine learning, showing that properties such as safety, robustness, reliability, privacy, security, transparency, explainability, and nondiscrimination are all demanded already by existing law and regulations — albeit, in many cases, to an uncertain degree. Next, we discuss the key technical obstacles to achieving these desirable properties, and important techniques to overcome those barriers in the medical context. Since most of the technical challenges are very young and new problems frequently emerge, the scientific discourse is rapidly evolving and has not yet converged on clear best-practice solutions. Nevertheless, we aim to illuminate the underlying technical challenges, possible ways for addressing them, and their respective merits and drawbacks. In particular, we notice that distribution shift, spurious correlations, model underspecification, and data scarcity represent severe challenges in the medical context (and others) that are very difficult to solve with classical black-box deep neural networks. Important measures that may help to address these challenges include the use of large and representative datasets and federated learning as a means to that end, the careful exploitation of domain knowledge wherever feasible, the use of inherently transparent models, comprehensive model testing and verification, as well as stakeholder inclusion. ar X iv :2 10 7. 09 54 6v 1 [ cs .L G ] 2 0 Ju l 2 02 1","",""
0,"P. Rasouli, Ingrid Chieh Yu","Explainable Debugger for Black-box Machine Learning Models",2021,"","","","",156,"2022-07-13 09:37:52","","10.1109/IJCNN52387.2021.9533944","","",,,,,0,0.00,0,2,1,"The research around developing methods for debugging and refining Machine Learning (ML) models is still in its infancy. We believe employing tailored tools in the development process can help developers in creating more trustworthy and reliable models. This is particularly essential for creating black-box models such as deep neural networks and random forests, as their opaque decision-making and complex structure prevent detailed investigations. Although many explanation techniques provide interpretability in terms of predictive features for a mispredicted instance, it would be beneficial for a developer to find a partition of the training data that significantly influences the anomaly. Such responsible partitions can be subjected to data visualization and data engineering in the development phase to improve the model's accuracy. In this paper, we propose a systematic debugging framework for the development of ML models that guides the data engineering process using the model's decision boundary. Our approach finds the influential neighborhood of anomalous data points using observation-level feature importance and explains them via a novel quasi-global explanation technique. It is also equipped with a robust global explanation approach to reveal general trends and expose potential biases in the neighborhoods. We demonstrate the efficacy of the devised framework through several experiments on standard data sets and black-box models and propose various guidelines on how the framework's components can be practically useful from a developer's perspective.","",""
0,"W. Zong, Yang-Wai Chow, W. Susilo","Visual Analysis of Adversarial Examples in Machine Learning",2021,"","","","",157,"2022-07-13 09:37:52","","10.1007/978-981-33-6726-5_4","","",,,,,0,0.00,0,3,1,"","",""
0,"Wenxiu Xie, Christine Ji, Tianyong Hao, Chi-Yin Chow","Predicting the Easiness and Complexity of English Health Materials for International Tertiary Students With Linguistically Enhanced Machine Learning Algorithms: Development and Validation Study (Preprint)",2021,"","","","",158,"2022-07-13 09:37:52","","10.2196/preprints.25110","","",,,,,0,0.00,0,4,1,"  BACKGROUND  There is an increasing body of research on the development of machine learning algorithms in the evaluation of online health educational resources for specific readerships. Machine learning algorithms are known for their lack of interpretability compared with statistics. Given their high predictive precision, improving the interpretability of these algorithms can help increase their applicability and replicability in health educational research and applied linguistics, as well as in the development and review of new health education resources for effective and accessible health education.      OBJECTIVE  Our study aimed to develop a linguistically enriched machine learning model to predict binary outcomes of online English health educational resources in terms of their easiness and complexity for international tertiary students.      METHODS  Logistic regression emerged as the best performing algorithm compared with support vector machine (SVM) (linear), SVM (radial basis function), random forest, and extreme gradient boosting on the transformed data set using L2 normalization. We applied recursive feature elimination with SVM to perform automatic feature selection. The automatically selected features (n=67) were then further streamlined through expert review. The finalized feature set of 22 semantic features achieved a similar area under the curve, sensitivity, specificity, and accuracy compared with the initial (n=115) and automatically selected feature sets (n=67). Logistic regression with the linguistically enhanced feature set (n=22) exhibited important stability and robustness on the training data of different sizes (20%, 40%, 60%, and 80%), and showed consistently high performance when compared with the other 4 algorithms (SVM [linear], SVM [radial basis function], random forest, and extreme gradient boosting).      RESULTS  We identified semantic features (with positive regression coefficients) contributing to the prediction of easy-to-understand online health texts and semantic features (with negative regression coefficients) contributing to the prediction of hard-to-understand health materials for readers with nonnative English backgrounds. Language complexity was explained by lexical difficulty (rarity and medical terminology), verbs typical of medical discourse, and syntactic complexity. Language easiness of online health materials was associated with features such as common speech act verbs, personal pronouns, and familiar reasoning verbs. Successive permutation of features illustrated the interaction between these features and their impact on key performance indicators of the machine learning algorithms.      CONCLUSIONS  The new logistic regression model developed exhibited consistency, scalability, and, more importantly, interpretability based on existing health and linguistic research. It was found that low and high linguistic accessibilities of online health materials were explained by 2 sets of distinct semantic features. This revealed the inherent complexity of effective health communication beyond current readability analyses, which were limited to syntactic complexity and lexical difficulty. ","",""
0,"Zhixin Pan, P. Mishra","Automated Detection of Spectre and Meltdown Attacks Using Explainable Machine Learning",2021,"","","","",159,"2022-07-13 09:37:52","","10.1109/HOST49136.2021.9702278","","",,,,,0,0.00,0,2,1,"Spectre and Meltdown attacks exploit security vulnerabilities of advanced architectural features to access inherently concealed memory data without authorization. Existing defense mechanisms have three major drawbacks: (i) they can be fooled by obfuscation techniques, (ii) the lack of transparency severely limits their applicability, and (iii) it can introduce unacceptable performance degradation. In this paper, we propose a novel detection scheme based on explainable machine learning to address these fundamental challenges. Specifically, this paper makes three important contributions. (1) Our work is the first attempt in applying explainable machine learning for Spectre and Meltdown attack detection. (2) Our proposed method utilizes the temporal differences of hardware events in sequential timestamps instead of overall statistics, which contributes to the robustness of ML models against evasive attacks. (3) Extensive experimental evaluation demonstrates that our approach can significantly improve detection efficiency (38.4% on average) compared to state-of-the-art techniques.","",""
0,"Wei Chen, Xiangkui Li, Lu Ma, Dong Li","Enhancing Robustness of Machine Learning Integration With Routine Laboratory Blood Tests to Predict Inpatient Mortality After Intracerebral Hemorrhage",2022,"","","","",160,"2022-07-13 09:37:52","","10.3389/fneur.2021.790682","","",,,,,0,0.00,0,4,1,"Objective: The accurate evaluation of outcomes at a personalized level in patients with intracerebral hemorrhage (ICH) is critical clinical implications. This study aims to evaluate how machine learning integrates with routine laboratory tests and electronic health records (EHRs) data to predict inpatient mortality after ICH. Methods: In this machine learning-based prognostic study, we included 1,835 consecutive patients with acute ICH between October 2010 and December 2018. The model building process incorporated five pre-implant ICH score variables (clinical features) and 13 out of 59 available routine laboratory parameters. We assessed model performance according to a range of learning metrics, such as the mean area under the receiver operating characteristic curve [AUROC]. We also used the Shapley additive explanation algorithm to explain the prediction model. Results: Machine learning models using laboratory data achieved AUROCs of 0.71–0.82 in a split-by-year development/testing scheme. The non-linear eXtreme Gradient Boosting model yielded the highest prediction accuracy. In the held-out validation set of development cohort, the predictive model using comprehensive clinical and laboratory parameters outperformed those using clinical alone in predicting in-hospital mortality (AUROC [95% bootstrap confidence interval], 0.899 [0.897–0.901] vs. 0.875 [0.872–0.877]; P <0.001), with over 81% accuracy, sensitivity, and specificity. We observed similar performance in the testing set. Conclusions: Machine learning integrated with routine laboratory tests and EHRs could significantly promote the accuracy of inpatient ICH mortality prediction. This multidimensional composite prediction strategy might become an intelligent assistive prediction for ICH risk reclassification and offer an example for precision medicine.","",""
6,"Kei Nakagawa, Masaya Abe, Junpei Komiyama","RIC-NN: A Robust Transferable Deep Learning Framework for Cross-sectional Investment Strategy",2019,"","","","",161,"2022-07-13 09:37:52","","10.1109/DSAA49011.2020.00051","","",,,,,6,2.00,2,3,3,"Stock return predictability is an important research theme as it reflects our economic and social organization, and significant efforts are made to explain the dynamism therein. Statistics of strong explanative power, called ""factor"", have been proposed to summarize the essence of predictive stock returns. The challenge here is to make a multi-factor investment strategy that is consistent over a reasonably long period based on supervised machine learning. Although machine learning methods are increasingly popular in stock return prediction, an inference of the stock return is highly elusive, and naive use of complex machine learning methods easily overfits the current data and results in poor performance on future data. We propose a principled stock return prediction framework that we call Ranked Information Coefficient Neural Network (RIC-NN) that alleviates the overfitting. RIC-NN addresses the difficulty that arises in nonconvex machine learning: Namely, initialization and the stopping of the training model and the transfer among several different tasks (markets). RIC-NN is a deep learning approach and includes the following three novel ideas: (1) nonlinear multi-factor approach, (2) stopping criteria with ranked information coefficient (rank IC), and (3) deep transfer learning among multiple regions. Experimental comparison with the stocks in the Morgan Stanley Capital International indices shows that RIC-NN outperforms not only off-the-shelf machine learning methods but also the average return of major equity investment funds in the last fourteen years.","",""
33,"B. Abdollahi, O. Nasraoui","Transparency in Fair Machine Learning: the Case of Explainable Recommender Systems",2018,"","","","",162,"2022-07-13 09:37:52","","10.1007/978-3-319-90403-0_2","","",,,,,33,8.25,17,2,4,"","",""
10,"David Zabala-Blanco, M. Mora, César A. Azurdia-Meza, A. Firoozabadi, Pablo Palacios Játiva, I. Soto","Relaxation of the Radio-Frequency Linewidth for Coherent-Optical Orthogonal Frequency-Division Multiplexing Schemes by Employing the Improved Extreme Learning Machine",2020,"","","","",163,"2022-07-13 09:37:52","","10.3390/sym12040632","","",,,,,10,5.00,2,6,2,"A coherent optical (CO) orthogonal frequency division multiplexing (OFDM) scheme gives a scalable and flexible solution for increasing the transmission rate, being extremely robust to chromatic dispersion as well as polarization mode dispersion. Nevertheless, as any coherent-detection OFDM system, the overall system performance is limited by laser phase noises. On the other hand, extreme learning machines (ELMs) have gained a lot of attention from the machine learning community owing to good generalization performance, negligible learning speed, and minimum human intervention. In this manuscript, a phase-error mitigation method based on the single-hidden layer feedforward network prone to the improved ELM algorithm for CO-OFDM systems is introduced for the first time. In the training step, two steps are distinguished. Firstly, pilots are used, which is very common in OFDM-based systems, to diminish laser phase noises as well as to correct frequency-selective impairments and, therefore, the bandwidth efficiency can be maximized. Secondly, the regularization parameter is included in the ELM to balance the empirical and structural risks, namely to minimize the root mean square error in the test stage and, consequently, the bit error rate (BER) metric. The operational principle of the real-complex (RC) ELM is analytically explained, and then, its sub-parameters (number of hidden neurons, regularization parameter, and activation function) are numerically found in order to enhance the system performance. For binary and quadrature phase-shift keying modulations, the RC-ELM outperforms the benchmark pilot-assisted equalizer as well as the fully-real ELM, and almost matches the common phase error (CPE) compensation and the ELM defined in the complex domain (C-ELM) in terms of the BER over an additive white Gaussian noise channel and different laser oscillators. However, both techniques are characterized by the following disadvantages: the CPE compensator reduces the transmission rate since an additional preamble is mandatory for channel estimation purposes, while the C-ELM requires a bounded and differentiable activation function in the complex domain and can not follow semi-supervised training. In the same context, the novel ELM algorithm can not compete with the CPE compensator and C-ELM for the 16-ary quadrature amplitude modulation. On the other hand, the novel ELM exposes a negligible computational cost with respect to the C-ELM and PAE methods.","",""
33,"Ved P. Kafle, Y. Fukushima, P. Martinez-Julia, T. Miyazawa","Consideration On Automation of 5G Network Slicing with Machine Learning",2018,"","","","",164,"2022-07-13 09:37:52","","10.23919/ITU-WT.2018.8597639","","",,,,,33,8.25,8,4,4,"Machine learning has the capability to provide simpler solutions to complex problems by analyzing a huge volume of data in a short time, learning for adapting its functionality to dynamically changing environments, and predicting near future events with reasonably good accuracy. The 5G communication networks are getting complex due to emergence of unprecedentedly huge number of new connected devices and new types of services. Moreover, the requirements of creating virtual network slices suitable to provide optimal services for diverse users and applications are posing challenges to the efficient management of network resources, processing information about a huge volume of traffic, staying robust against all potential security threats, and adaptively adjustment of network functionality for time-varying workload. In this paper, we introduce about the envisioned 5G network slicing and elaborate the necessity of automation of network functions for the design, construction, deployment, operation, control and management of network slices. We then revisit the machine learning techniques that can be applied for the automation of network functions. We also discuss the status of artificial intelligence and machine learning related activities being progressed in standards development organizations and industrial forums.","",""
33,"Zebin Yang, Aijun Zhang, A. Sudjianto","Enhancing Explainability of Neural Networks Through Architecture Constraints",2019,"","","","",165,"2022-07-13 09:37:52","","10.1109/TNNLS.2020.3007259","","",,,,,33,11.00,11,3,3,"Prediction accuracy and model explainability are the two most important objectives when developing machine learning algorithms to solve real-world problems. Neural networks are known to possess good prediction performance but suffer from a lack of model interpretability. In this article, we propose to enhance the explainability of neural networks through the following architecture constraints: 1) sparse additive subnetworks; 2) projection pursuit with orthogonality constraint; and 3) smooth function approximation. It leads to an enhanced explainable neural network (ExNN) with a superior balance between prediction performance and model interpretability. We derive sufficient identifiability conditions for the proposed ExNN model. The multiple parameters are simultaneously estimated by a modified minibatch gradient descent method based on the backpropagation algorithm for calculating the derivatives and the Cayley transform for preserving the projection orthogonality. Through simulation study under six different scenarios, we compare the proposed method to several benchmarks, including least absolute shrinkage and selection operator, support vector machine, random forest, extreme learning machine, and multilayer perceptron. It is shown that the proposed ExNN model keeps the flexibility of pursuing high prediction accuracy while attaining improved interpretability. Finally, a real data example is employed as a showcase application.","",""
4,"Himaghna Bhattacharjee, Nikolaos Anesiadis, D. Vlachos","Regularized machine learning on molecular graph model explains systematic error in DFT enthalpies",2021,"","","","",166,"2022-07-13 09:37:52","","10.1038/s41598-021-93854-w","","",,,,,4,4.00,1,3,1,"","",""
18,"G. Crowley, Sophia Kwon, S. Haider, E. Caraher, R. Lam, D. St-Jules, Mengling Liu, D. Prezant, A. Nolan","Metabolomics of World Trade Center-Lung Injury: a machine learning approach",2018,"","","","",167,"2022-07-13 09:37:52","","10.1136/bmjresp-2017-000274","","",,,,,18,4.50,2,9,4,"Introduction Biomarkers of metabolic syndrome expressed soon after World Trade Center (WTC) exposure predict development of WTC Lung Injury (WTC-LI). The metabolome remains an untapped resource with potential to comprehensively characterise many aspects of WTC-LI. This case–control study identified a clinically relevant, robust subset of metabolic contributors of WTC-LI through comprehensive high-dimensional metabolic profiling and integration of machine learning techniques. Methods Never-smoking, male, WTC-exposed firefighters with normal pre-9/11 lung function were segregated by post-9/11 lung function. Cases of WTC-LI (forced expiratory volume in 1s <lower limit of normal, n=15) and controls (n=15) were identified from previous cohorts. The metabolome of serum drawn within 6 months of 9/11 was quantified. Machine learning was used for dimension reduction to identify metabolites associated with WTC-LI. Results 580 metabolites qualified for random forests (RF) analysis to identify a refined metabolite profile that yielded maximal class separation. RF of the refined profile correctly classified subjects with a 93.3% estimated success rate. 5 clusters of metabolites emerged within the refined profile. Prominent subpathways include known mediators of lung disease such as sphingolipids (elevated in cases of WTC-LI), and branched-chain amino acids (reduced in cases of WTC-LI). Principal component analysis of the refined profile explained 68.3% of variance in five components, demonstrating class separation. Conclusion Analysis of the metabolome of WTC-exposed 9/11 rescue workers has identified biologically plausible pathways associated with loss of lung function. Since metabolites are proximal markers of disease processes, metabolites could capture the complexity of past exposures and better inform treatment. These pathways warrant further mechanistic research.","",""
18,"M. Rege","Machine Learning for Cyber Defense and Attack",2018,"","","","",168,"2022-07-13 09:37:52","","","","",,,,,18,4.50,18,1,4,"The exponential advancements in processing, storage and network technologies have led to the recent explosive growth in big data, connectivity and machine learning. The world is becoming increasingly digitalized raising security concerns and the desperate need for robust and advanced security technologies and techniques to combat the increasing complex nature of cyber-attacks. This paper discusses how machine learning is being used in cyber security in both defense and offense activities, including discussions on cyber-attacks targeted at machine learning models. Specifically, we discuss the applications of machine learning in carrying out cyber-attacks, such as in smart botnets, advanced spear fishing and evasive malwares. We also explain the application of machine learning in cyber security, such as in threat detection and prevention, malware detection and classification, and network risk scoring. KeywordsCyber Security; Machine Learning; Malware; Thread Detection and Classification; Network Risk Scoring.","",""
11,"Y. Nishi, Satoshi Masuda, H. Ogawa, Keiji Uetsuki","A Test Architecture for Machine Learning Product",2018,"","","","",169,"2022-07-13 09:37:52","","10.1109/ICSTW.2018.00060","","",,,,,11,2.75,3,4,4,"As machine learning (ML) technology continues to spread by rapid evolution, the system or service using Machine Learning technology, called ML product, makes big impact on our life, society and economy. Meanwhile, Quality Assurance (QA) for ML product is quite more difficult than hardware, non-ML software and service because performance of ML technology is much better than non-ML technology in exchange for the characteristics of ML product, e.g. low explainability. We must keep rapid evolution and reduce quality risk of ML product simultaneously. In this paper, we show a Quality Assurance Framework for Machine Learning product. Scope of QA in this paper is limited to product evaluation. First, a policy of QA for ML Product is proposed. General principles of product evaluation is introduced and applied to ML product evaluation as a part of the policy. They are composed of A-ARAI: Allowability, Achievability, Robustness, Avoidability and Improvability. A strategy of ML Product Evaluation is constructed as another part of the policy. Quality Integrity Level for ML product is also modelled. Second, we propose a test architecture of ML product testing. It consists of test levels and fundamental test types of ML product testing, including snapshot testing, learning testing and confrontation testing. Finally, we defines QA activity levels for ML product.","",""
12,"N. Khoa, M. M. Alamdari, T. Rakotoarivelo, Ali Anaissi, Yang Wang","Structural Health Monitoring Using Machine Learning Techniques and Domain Knowledge Based Features",2018,"","","","",170,"2022-07-13 09:37:52","","10.1007/978-3-319-90403-0_20","","",,,,,12,3.00,2,5,4,"","",""
10,"Yufang Jin, Bin Chen, B. Lampinen, P. Brown","Advancing Agricultural Production With Machine Learning Analytics: Yield Determinants for California’s Almond Orchards",2020,"","","","",171,"2022-07-13 09:37:52","","10.3389/fpls.2020.00290","","",,,,,10,5.00,3,4,2,"Agricultural productivity is subject to various stressors, including abiotic and biotic threats, many of which are exacerbated by a changing climate, thereby affecting long-term sustainability. The productivity of tree crops such as almond orchards, is particularly complex. To understand and mitigate these threats requires a collection of multi-layer large data sets, and advanced analytics is also critical to integrate these highly heterogeneous datasets to generate insights about the key constraints on the yields at tree and field scales. Here we used a machine learning approach to investigate the determinants of almond yield variation in California’s almond orchards, based on a unique 10-year dataset of field measurements of light interception and almond yield along with meteorological data. We found that overall the maximum almond yield was highly dependent on light interception, e.g., with each one percent increase in light interception resulting in an increase of 57.9 lbs/acre in the potential yield. Light interception was highest for mature sites with higher long term mean spring incoming solar radiation (SRAD), and lowest for younger orchards when March maximum temperature was lower than 19°C. However, at any given level of light interception, actual yield often falls significantly below full yield potential, driven mostly by tree age, temperature profiles in June and winter, summer mean daily maximum vapor pressure deficit (VPDmax), and SRAD. Utilizing a full random forest model, 82% (±1%) of yield variation could be explained when using a sixfold cross validation, with a RMSE of 480 ± 9 lbs/acre. When excluding light interception from the predictors, overall orchard characteristics (such as age, location, and tree density) and inclusive meteorological variables could still explain 78% of yield variation. The model analysis also showed that warmer winter conditions often limited mature orchards from reaching maximum yield potential and summer VPDmax beyond 40 hPa significantly limited the yield. Our findings through the machine learning approach improved our understanding of the complex interaction between climate, canopy light interception, and almond nut production, and demonstrated a relatively robust predictability of almond yield. This will ultimately benefit data-driven climate adaptation and orchard nutrient management approaches.","",""
6,"Farzin Piltan, Jong-Myon Kim","Bearing Fault Identification Using Machine Learning and Adaptive Cascade Fault Observer",2020,"","","","",172,"2022-07-13 09:37:52","","10.3390/app10175827","","",,,,,6,3.00,3,2,2,"In this work, a hybrid procedure for bearing fault identification using a machine learning and adaptive cascade observer is explained. To design an adaptive cascade observer, the normal signal approximation is the first step. Therefore, the fuzzy orthonormal regressive (FOR) technique was developed to approximate the acoustic emission (AE) and vibration (non-stationary and nonlinear) bearing signals in normal conditions. After approximating the normal signal of bearing using the FOR technique, the adaptive cascade observer is modeled in four steps. First, the linear observation technique using a FOR proportional-integral (PI) observer (FOR-PIO) is developed. In the second step, to increase the power of uncertaintie rejection (robustness) of the FOR-PIO, the structure procedure is used serially. Next, the fuzzy like observer is selected to increase the accuracy of FOR structure PI observer (FOR-SPIO). Moreover, the adaptive technique is used to develop the reliability of the cascade (fuzzy-structure PI) observer. Additionally to fault identification, the machine-learning algorithm using a support vector machine (SVM) is recommended. The effectiveness of the adaptive cascade observer with the SVM fault identifier was validated by a vibration and AE datasets. Based on the results, the average vibration and AE fault diagnosis using the adaptive cascade observer with the SVM fault identifier are 97.8% and 97.65%, respectively.","",""
80,"Xiaoqin Zhang, Mingyu Fan, Di Wang, Peng Zhou, D. Tao","Top-k Feature Selection Framework Using Robust 0–1 Integer Programming",2020,"","","","",173,"2022-07-13 09:37:52","","10.1109/TNNLS.2020.3009209","","",,,,,80,40.00,16,5,2,"Feature selection (FS), which identifies the relevant features in a data set to facilitate subsequent data analysis, is a fundamental problem in machine learning and has been widely studied in recent years. Most FS methods rank the features in order of their scores based on a specific criterion and then select the <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> top-ranked features, where <inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> is the number of desired features. However, these features are usually not the top-<inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> features and may present a suboptimal choice. To address this issue, we propose a novel FS framework in this article to select the exact top-<inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> features in the unsupervised, semisupervised, and supervised scenarios. The new framework utilizes the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm as the matrix sparsity constraint rather than its relaxations, such as the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{1,2}$ </tex-math></inline-formula>-norm. Since the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm constrained problem is difficult to solve, we transform the discrete <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm-based constraint into an equivalent 0–1 integer constraint and replace the 0–1 integer constraint with two continuous constraints. The obtained top-<inline-formula> <tex-math notation=""LaTeX"">$k$ </tex-math></inline-formula> FS framework with two continuous constraints is theoretically equivalent to the <inline-formula> <tex-math notation=""LaTeX"">$\ell _{0,2}$ </tex-math></inline-formula>-norm constrained problem and can be optimized by the alternating direction method of multipliers (ADMM). Unsupervised and semisupervised FS methods are developed based on the proposed framework, and extensive experiments on real-world data sets are conducted to demonstrate the effectiveness of the proposed FS framework.","",""
5,"A. Barnard, A. Parker, B. Motevalli, G. Opletal","The pure and representative types of disordered platinum nanoparticles from machine learning.",2020,"","","","",174,"2022-07-13 09:37:52","","10.1088/1361-6528/abcc23","","",,,,,5,2.50,1,4,2,"The development of interpretable structure/property relationships is a cornerstone of nanoscience, but can be challenging when the structural diversity and complexity exceeds our ability to characterise it. This is often the case for imperfect, disordered and amorphous nanoparticles, where even the nomenclature can be unspecific. Disordered platinum nanoparticles have exhibited superior performance for some reactions, which makes a systematic way of describing them highly desirable. In this study we have used a diverse set of disorder platinum nanoparticles and machine learning to identify the pure and representative structures based on their similarity in 121 dimensions. We identify two prototypes that are representative of separable classes, and seven archetypes that are the pure structures on the convex hull with which all other possibilities can be described. Together these nine nanoparticles can explain all of the variance in the set, and can be described as either single crystal, twinned, spherical or branched; with or without roughened surfaces. This forms a robust sub-set of platinum nanoparticle upon which to base further work, and provides a theoretical basis for discussing structure/property relationships of platinum nanoparticles that are not geometrically ideal.","",""
2,"Markus Jaeger, Stephan Krügel, D. Marinelli, J. Papenbrock, P. Schwendner","Understanding Machine Learning for Diversified Portfolio Construction by Explainable AI",2020,"","","","",175,"2022-07-13 09:37:52","","10.2139/ssrn.3528616","","",,,,,2,1.00,0,5,2,"In this paper, we construct a pipeline to investigate heuristic diversification strategies in asset allocation. We use machine learning concepts (""explainable AI"") to compare the robustness of different strategies and back out implicit rules for decision making.    In a first step, we augment the asset universe (the empirical dataset) with a range of scenarios generated with a block bootstrap from the empirical dataset.    Second, we backtest the candidate strategies over a long period of time, checking their performance variability. Third, we use XGBoost as a regression model to connect the difference between the measured performances between two strategies to a pool of statistical features of the portfolio universe tailored to the investigated strategy.    Finally, we employ the concept of Shapley values to extract the relationships that the model could identify between the portfolio characteristics and the statistical properties of the asset universe.    We test this pipeline for studying risk-parity strategies with a volatility target, and in particular, comparing the machine learning-driven Hierarchical Risk Parity (HRP) to the classical Equal Risk Contribution (ERC) strategy.    In the augmented dataset built from a multi-asset investment universe of commodities, equities and fixed income futures, we find that HRP better matches the volatility target, and shows better risk-adjusted performances. Finally, we train XGBoost to learn the difference between the realized Calmar ratios of HRP and ERC and extract explanations.    The explanations provide fruitful ex-post indications of the connection between the statistical properties of the universe and the strategy performance in the training set. For example, the model confirms that features addressing the hierarchical properties of the universe are connected to the relative performance of HRP respect to ERC.","",""
2,"Yangxiaoyue Liu, Xiaolin Xia, L. Yao, Wenlong Jing, Chenghu Zhou, Wumeng Huang, Yong Li, Ji Yang","Downscaling Satellite Retrieved Soil Moisture Using Regression Tree‐Based Machine Learning Algorithms Over Southwest France",2020,"","","","",176,"2022-07-13 09:37:52","","10.1029/2020EA001267","","",,,,,2,1.00,0,8,2,"Satellite retrieved soil moisture (SM) shows great potential in hydrological, meteorological, ecological, and agricultural applications, while the coarse resolution limits its utilization in regional scale. The regression tree‐based machine learning algorithms reveal promising capability in SM downscaling. However, it lacks systematic study dedicated to intercomparisons of algorithms to explicitly illuminate their characteristics. In this study, comparisons are made to systematically evaluate performances of classification and regression tree (CART), random forest (RF), gradient boost decision tree (GBDT), and extreme gradient boost (XGB) in Soil Moisture Active Passive (SMAP) SM downscaling in southwest France. The results show that the four algorithms downscaled SM are capable of capturing spatial distribution features of the original SMAP SM. The downscaled regions with favorable accuracy are mostly situated in the dominant Mediterranean climate zone with moderate vegetation coverage and mild topography variation. The best results are obtained by GBDT in grassland with R value of 0.77 and ubRMSE value of 0.04 m3/m3. The RF and XGB also achieve good performances. On the whole, the GBDT approach is robust and reliable, which could downscale SM with superior correlation and smaller bias than the others. Besides, it achieves higher accuracy than the original SMAP in grassland and shrubland. The feature importance index of each explainable variable fluctuates regularly among different seasons and models. This study proves the outstanding performance of GBDT in SMAP SM downscaling and is expected to act as a valuable reference for studies focusing on SM scale conversion algorithms.","",""
2,"P. Krishnamurthy, Animesh Basak Chowdhury, Benjamin Tan, F. Khorrami, R. Karri","Explaining and Interpreting Machine Learning CAD Decisions: An IC Testing Case Study",2020,"","","","",177,"2022-07-13 09:37:52","","10.1145/3380446.3430643","","",,,,,2,1.00,0,5,2,"We provide a methodology to explain and interpret machine learning decisions in Computer-Aided Design (CAD) flows. We demonstrate the efficacy of the methodology to the VLSI testing case. Such a tool will provide designers with insight into the “black box” machine learning models/classifiers through human readable sentences based on normally understood design rules or new design rules. The methodology builds on an intrinsically explainable, rule-based ML framework, called Sentences in Feature Subsets (SiFS), to mine human readable decision rules from empirical data sets. SiFS derives decision rules as compact Boolean logic sentences involving subsets of features in the input data. The approach is applied to test point insertion problem in circuits and compared to the ground truth and traditional design rules.","",""
3,"N. Lau, Michael Hildebrandt, M. Jeon","Ergonomics in AI: Designing and Interacting With Machine Learning and AI",2020,"","","","",178,"2022-07-13 09:37:52","","10.1177/1064804620915238","","",,,,,3,1.50,1,3,2,"Machine learning and artificial intelligence (AI) enable new types of autonomous systems that are changing our personal and professional lives. While there are plenty of stories about machine learning delivering the promise of a better future, such as autonomous vehicles for improving mobility, safety, and fuel efficiency, many examples have indicated great risks, such as bots on social media for spreading false information and manipulating public opinions. As machine learning approaches ubiquity in industrial systems and consumer products, Human Factors must innovate to support users in coping with emerging autonomous capabilities. The rise of machine learning technologies poses serious questions that have been discussed in the panels at the recent Annual Meetings of the Human Factors Ergonomics Society (Lau et al., 2018; Lau et al., 2019). How can we help users understand the autonomous capabilities developed through supervised or unsupervised learning? What kind of interactions could enhance cooperation between human and machine learning algorithms? We must also take advantage of machine learning techniques in advancing our own research and design science. How can we use machine learning in assessing human states and capabilities? How should we help incorporate human sensing into machine learning algorithms? In this special issue, we embrace the broad spectrum of research and design efforts that investigate machine learning for improving usability and safety of intelligent systems and consumer products. Our goal is to clarify the roles of Human Factors in contributing to a humanist perspective that considers the social, political, ethical and cultural factors of implementing AI into daily human–system interactions. At the same time, this special issue can only accommodate five articles, about one third of the submissions, after a rigorous peer-review process. So, what we have hoped to curate for the readers is an intellectually stimulating, short exhibit of our discipline in developing and applying next-generation AI. The first article in this special issue is a commentary by Hancock who envisions work to be eventually shared between self-evolving machines and humans. This vision of work challenges the Human Factors community to prepare for a future that requires designing interactions and user interfaces for machines whose behaviors we cannot fully anticipate and for work that we do not yet know. The second article by Zhang et al. speaks to the challenge of anticipating the consequences of machine learning in designing technology and making policy decisions. The authors use a speech recognition example to illustrate a violation of inclusivity in design. In their second example, they illustrate how a loan policy aimed at supporting a disadvantaged group ultimately harms the group in the long run. These examples raise questions about how Human Factors professionals can engage in a data-driven design process and how to develop “explainable” AI that presents the true behavior of the machine learning model to the user. Anthropomorphism is a much talkedabout concept to help design AI that humans can understand and interact with intuitively. Muller compares how deep neural networks and humans classify images to illustrate how their differences likely require appropriate interactivity to minimize the mismatch between how human and AI think about the intelligence of each other. The article highlights not only the need for interaction design but also the importance of understanding machine learning algorithms. The final two articles present empirical investigations on how ergonomic research can promote the appropriate use of machine learning tools. Wang et al. describe metrics of stability, robustness and sensitivity for aiding users to interpret prediction results of supervised learning algorithms. The authors illustrate how effective visualization of those metrics can improve decision making. Gilbank et al. describe a qualitative study with ten medical professionals using a machine learning–driven toxicity prediction tool that utilizes 10 years of historical data. The study presents the expectations and perspectives of medical professionals on AI, and the user interface design considerations for promoting trust and, ultimately use of the machine learning system. We hope that these five articles contribute to new thoughts and present exciting challenges. At the same time, we recognize that these articles only represent a speck of all the relevant Human Factors research and design issues related to machine learning and artificial intelligence. As we prepared this special issue, we realized that so much progress is needed to formulate design methods for machine learning. So, we agree with Hancock’s conclusion that our road ahead for ergonomics design of machine learning and artificial intelligence “promises to be a bumpy but exciting ride.”","",""
8,"Xiaoxuan Lu, Yushen Long, Han Zou, Chengpu Yu, Lihua Xie","Robust extreme learning machine for regression problems with its application to wifi based indoor positioning system",2014,"","","","",179,"2022-07-13 09:37:52","","10.1109/MLSP.2014.6958903","","",,,,,8,1.00,2,5,8,"We propose two kinds of robust extreme learning machines (RELMs) based on the close-to-mean constraint and the small-residual constraint respectively to solve the problem of noisy measurements in indoor positioning systems (IPSs). We formulate both RELMs as second order cone programming problems. The fact that feature mapping in ELM is known to users is exploited to give the needed information for robust constraints. Real-world indoor localization experimental results show that, the proposed algorithms can not only improve the accuracy and repeatability, but also reduce the deviations and worst case errors of IPSs compared with basic ELM and OPT-ELM based IPSs.","",""
1,"Zheren Ma, E. Davani, Xiaodan Ma, Hanna Lee, I. Arslan, Xiang Zhai, H. Darabi, D. Castineira","Finding a Trend Out of Chaos, A Machine Learning Approach for Well Spacing Optimization",2020,"","","","",180,"2022-07-13 09:37:52","","10.2118/201698-ms","","",,,,,1,0.50,0,8,2,"  Data-driven decisions powered by machine-learning methods are increasing in popularity when it comes to optimizing field development in unconventional reservoirs. However, since well performance is impacted by many factors (e.g., geological characteristics, completion design, well design, etc.), the challenge is uncovering trends from all the noise.  By leveraging basin-level knowledge captured by big data sculpting, integrating private and public data with the use of uncertainty quantification, Augmented AI (a combination of expert-based knowledge and advanced AI frameworks) can provide quick and science-based answers for well spacing and fracking optimization and assess the full potential of an asset in unconventional reservoirs.  Augmented AI is artificial intelligence powered by engineering wisdom. The Augmented AI workflow starts with data sculpting, which includes information retrieval, data cleaning and standardization, and finally a smart, deep and systematic data QC. Feature engineering generates all the relevant parameters going into the machine learning model—over 50 features have been generated for this work and categorized. The final step is to perform model tuning and ensemble, evaluating the model robustness, generating model explanation and uncertainty quantification. Augmented AI adopts an iterative machine learning modeling approach. This approach combines new and innovative engineering and G&G workflows with data-driven models so that a deep understanding of the field behavior can be developed. Loops from feature selection to model tuning are used until good model results are achieved. The loop is automated using Bayesians optimization. All machine learning models have different strengths and weaknesses for prediction. Instead of manually determining which machine learning model to use, this approach uses an adaptive ensemble machine learning approach that is a stacking algorithm that combines multiple regression models via a second level machine learning model. It smartly aggregates opinions from different models with reduced variance and better robustness.  Augmented AI has been applied in unconventional reservoirs with great results. A case study in Midland Basin is presented in this paper. Domain-induced feature engineering was performed to obtain important features for predicting well performance, and initial feature selection was conducted using feature correlation analysis. A trusted and explainable ML model was built and enhanced with uncertainty quantification. After running several sensitivity analyses, Augmented AI optimized the attributes of interest, then vetted the outcome, generating a report and visualizing the results.  In addition, further information about the direct impact of well spacing on EUR was deconvoluted from other parameters using an ML explanation technique for Wolfcamp Formation in Permian Basin and subsequently well spacing optimization was presented for the case study in Midland Basin.  An innovative model was created using Augmented AI to optimize well spacing, leveraging big data sculpting, domain and physics-induced feature engineering, and machine learning. The learning was transferred from the basin model to the specific region of interest. Augmented AI provides efficient and systematic private data organization, an explainable machine learning model, robust production forecast with quantified uncertainty and well spacing and frac parameters optimization.  Augmented AI models are already built for major basins such as Midland and Delaware basins. The learning and knowledge of the model can be transferred to any region in a basin and can be refined using more accurate private data. This allows conclusions to be drawn even with a limited number of wells.","",""
1,"M. Amini, Ahmed Imteaj, J. Mohammadi","Distributed Machine Learning for Resilient Operation of Electric Systems",2020,"","","","",181,"2022-07-13 09:37:52","","10.1109/SEST48500.2020.9203368","","",,,,,1,0.50,0,3,2,"Power system resilience is crucial to ensure secure energy delivery to electricity consumers. Power system outages lead to economical and societal burdens for the society and industries. To mitigate the socio-economical impacts of a power outage, we need to develop efficient algorithms to ensure resilient operation of the power system. In this paper, we first explain the notion of data-driven resilience. Then, we present a pathway of leveraging edge intelligence to improve resilience. To this end, we propose a novel distributed machine learning paradigm. Our proposed structure relies on local Resilience Management Systems (RMS) that serve as intelligent decision-making entities in each area, e.g. an autonomous micro-grid or a smart home can act as RMS. The RMS agents, which are available in different areas, can share their local data (i.e., a microgrid's operational data) with their neighboring RMS to coordinate their decisions in a distributed fashion. This will provide two major advantages: 1) distributed intelligence replaces centralized decision-making leading to robust decision-making and enhanced resilience; 2) since local data are locally shared among all entities within an RMS, if one of the RMS agents fails to communicate with the rest of network, we still can maintain a feasible solution (which is not necessarily optimal). Finally, we presents different scenarios in the simulation results section that showcases the system performance for two buildings under various outage scenarios.","",""
1,"Chih-Yuan Yang, R. Sahita","Towards a Resilient Machine Learning Classifier - a Case Study of Ransomware Detection",2020,"","","","",182,"2022-07-13 09:37:52","","","","",,,,,1,0.50,1,2,2,"The damage caused by crypto-ransomware, due to encryption, is difficult to revert and cause data losses. In this paper, a machine learning (ML) classifier was built to early detect ransomware (called crypto-ransomware) that uses cryptography by program behavior. If a signature-based detection was missed, a behavior-based detector can be the last line of defense to detect and contain the damages. We find that input/output activities of ransomware and the file-content entropy are unique traits to detect crypto-ransomware. A deep-learning (DL) classifier can detect ransomware with a high accuracy and a low false positive rate. We conduct an adversarial research against the models generated. We use simulated ransomware programs to launch a gray-box analysis to probe the weakness of ML classifiers and to improve model robustness. In addition to accuracy and resiliency, trustworthiness is the other key criteria for a quality detector. Making sure that the correct information was used for inference is important for a security application. The Integrated Gradient method was used to explain the deep learning model and also to reveal why false negatives evade the detection. The approaches to build and to evaluate a real-world detector were demonstrated and discussed.","",""
1,"R. Masoudi, S. Mohaghegh, Daniel Yingling, A. Ansari, Hadi B. Amat, N. Mohamad, Ali Sabzabadi, Dipak Mandel","Subsurface Analytics Case Study; Reservoir Simulation and Modeling of Highly Complex Offshore Field in Malaysia, Using Artificial Intelligent and Machine Learning",2020,"","","","",183,"2022-07-13 09:37:52","","10.2118/201693-ms","","",,,,,1,0.50,0,8,2,"  Using commercial numerical reservoir simulators to build a full field reservoir model and simultaneously history match multiple dynamic variables for a highly complex, offshore mature field in Malaysia, had proven to be challenging, manpower intensive, highly expensive, and not very successful. This field includes almost two hundred wells that have been completed in more than 60 different, non-continuous reservoir layers. The field has been producing oil, gas and water for decades. The objective of this article is to demonstrate how Artificial Intelligence (AI) and Machine Learning is used to build a purely data-driven reservoir simulation model that successfully history match all the dynamic variables for all the wells in this field and subsequently used for production forecast. The model has been validated in space and time.  The AI and Machine Learning technology that was used to build the dynamic reservoir simulation and modeling is called spatio-temporal learning. Spatio-temporal learning is a machine-learning algorithm specifically developed for data-driven modeling of the physics of fluid flow through porous media. Spatio-temporal learning is used in the context of Deconvolutional Neural Networks. In this article Spatio-temporal Learning and Deconvolutional Neural Networks will be explained. This new technology is the result of more than 20 years of research and development in the application of AI and Machine Learning in reservoir modeling. This technology develops a coupled reservoir and wellbore model that for this particular oil & gas field in Malaysia uses choke setting, well-head pressure and well-head temperature as input and simultaneously history matches Oil production, GOR, WC, reservoir pressure, and water saturation for more than a hundred wells through a completely automated process.  Once the data-driven reservoir model is developed and history matched, it is blind validated in space and time in order to establish a reliable and robust reservoir model to be used for decision making purposes and opportunity generation to maximise the field value. The concepts and the methodology of history match of multiple wells, individual offshore platforms, and the entire field will be presented in this article along with the results of blind validation and production forecasting. Results of using this model to perform uncertainty quantification will also be presented.  A case study of a highly complex mature field with large number of wells and years of production has been used to be studied and simulated by this data-driven approach. Time, efforts, and resources required for the development of the dynamic reservoir simulation models using AI and Machine Learning is considerably less than time and resources required using the commercial numerical simulators. It is validated that the TDM developed model can make very reasonable prediction of field behavior and production from the existing wells based on modification of operational constraints and can be a prudent complementary tool to conventional numerical simulators for such complex assets.","",""
0,"G. Montavon, W. Samek","Statistics meets Machine Learning 5 Abstracts Explaining the decisions of deep neural networks and beyond",2020,"","","","",184,"2022-07-13 09:37:52","","","","",,,,,0,0.00,0,2,2,"Explaining the decisions of deep neural networks and beyond Grégoire Montavon and Wojciech Samek (joint work with Klaus-Robert Müller, Sebastian Lapuschkin, Alexander Binder, Jacob Kauffmann) Machine learning models have become increasingly complex and this complexity has allowed them to reach high prediction accuracy on challenging datasets. In some cases, improved predictivity has come at the expense of interpretability, in particular, complex models tend to be perceived as black-boxes. A lack of interpretability is problematic, not only because interpretability is desirable in itself (e.g. to extract useful insights from a model or from the modeled data), but also because common measurements of prediction accuracy can become strongly unreliable when certain assumptions about the training data are not met. Real-world datasets are typically not representative of all possible cases and the truly relevant variables may correlate with other irrelevant variables. In such circumstances, one would need to ensure that the machine learning model does not rely on these irrelevant variables. An assessment based purely on test set accuracy would be oblivious to the exact decision strategy and could overestimate the true prediction performance. This phenomenon has been referred to as the ‘Clever Hans’ effect [9]. Only an extension of the dataset with specific test cases, or an inspection of the model, e.g. via interpretability techniques [3, 16, 12], is capable of highlighting the improper decision structure. In this talk, we look at the question of explaining the predictions of deep neural networks, a successful machine learning approach that has been used increasingly in real-world applications. A challenge for getting these explanations is the complexity of the decision function, which makes it hard to apply simple explanation methods developed in the context of linear models, e.g. based on first-order Taylor expansions. In particular, DNN decision functions are highly nonlinear and multiscale, with a gradient that is highly varying or ‘shattered’ [4]. Also, local searches in the input space easily result in ‘adversarial examples’ [13] where the prediction no longer corresponds to the observed pattern in the input. Layer-wise relevance propagation (LRP) [3] is a technique that was proposed to robustly explain the neural network decision in terms of input features. It was shown to work on numerous models in a wide range of applications [14, 5, 15]. LRP departs from the neural network’s function representation to consider instead its graph structure. Specifically, the LRP algorithm performs an iterative redistribution of the neural network output to the lower layers. Redistribution from each layer to the layer below is achieved by means of propagation rules that satisfy a conservation property analogous to Kirchoff’s conservation laws in electrical circuits. The LRP algorithm terminates once the input layer has been reached. The LRP algorithm can be motivated as decomposing a complex problem 6 Oberwolfach Report 4/2020 (analyzing a highly nonlinear function) into a collection of simpler subproblems (treating each neuron individually). Furthermore, it was shown that the LRP algorithm can be interpreted as a collection of Taylor expansions performed at each layer and neuron of the neural network [11]. Specifically, the ‘relevance’ received by a given neuron is approximately the product of the neuron activation and a locally constant term. In turn, the LRP redistribution step can be interpreted as (1) identifying the linear terms of a Taylor expansion of the relevance expressed as a function of activations in the lower layer, and (2) propagating to the lower layer accordingly. A connection can be made between different proposed LRP propagation rules and the choice of reference point at which the Taylor expansion is performed [11, 10]. This Taylor-based view on the LRP algorithm allows in particular to verify that the corresponding reference points are meaningful, for example, that they satisfy domain membership constraints. This interpretation of LRP as a collection of Taylor expansions is referred to as “deep Taylor decomposition” [11]. The LRP algorithm has been successfully applied to various data types and problems, ranging from computer vision and natural language processing tasks such as classification of concepts in images [3], age prediction [8] or categorization of text documents [2], over reinforcement learning tasks such as playing computer games [9], to various medical data analysis tasks, e.g., decoding of fMRI signals [14] or therapy outcome prediction [15]. In these diverse applications, LRP explanations provide additional insights into the decision strategies used by the model, which not only help to better understand the data, including its biases and artifacts [8, 9], but also help to analyze the learning processes and model’s decision strategies [9]. In the second part of the talk, two recent advances that broaden the usefulness of explanation methods are discussed. First, Spectral Relevance Analysis (SpRAy) [9], a dataset-wide analysis of individual explanations that summarizes the overall decision structure of the model into a finite and easily interpretable set of prototypical decision strategies. This analysis allows to systematically investigate complex models on large datasets. It has unveiled in commonly used datasets, artifacts, that tend to systematically induce flaws into the decision structure of ML models trained on them. For example, a website logo was found in some images of the class ‘truck’ of the ImageNet dataset, which the state-of-the-art VGG-16 neural network would then use for its predictions [1]. Another advance brings successful explanation techniques to non-neural network architectures such as kernel-based models. The approach that we term ‘neuralization’ [6] finds for these non-neural network architectures a functionally equivalent neural network so that state-of-the-art explanation techniques such as LRP can be applied. The approach was successfully applied to various unsupervised models, in particular, kernel one-class SVMs [7] and various k-means clustering models [6], thereby shedding light into what input features make a data point anomalous or member of a given cluster. Statistics meets Machine Learning 7 Although significant progress has been made to improve the transparency of ML models such as deep neural networks, numerous challenges still need to be addressed both on the methods and theory side. In particular, there is a need for standardized and unbiased evaluation benchmarks for assessing the quality and usefulness of an explanation. Furthermore, an important future work will be to adopt a more holistic view on the problem of explanation, that considers how to make best use of the user’s interpretation and feedback capabilities, and that also integrates the end goal of the explanation method, for example, achieving better and more informed decisions, or systematically improving and robustifying a machine learning model.","",""
0,"Yufang Jin, Bin Chen, B. Lampinen, P. Brown","Yield Determinants and Prediction for California’s Almond Orchards Based on Machine Learning Analytics",2020,"","","","",185,"2022-07-13 09:37:52","","10.5194/egusphere-egu2020-10687","","",,,,,0,0.00,0,4,2,"Agricultural productivity is subject to various stressors, including abiotic and biotic threats, many of which are exacerbated by a changing climate. The productivity of tree crops, such as almond orchards, is particularly complex. Moreover, the State of California has implemented legislatively mandated nitrogen (N) management strategies of all growers statewide to minimize nitrogen losses to the environment, and almond growers must now apply N in accordance with the estimated yield in early spring. To understand and mitigate these threats requires a collection of multi-layer large data sets, and advanced analytics is also critical to integrate these highly heterogeneous datasets to generate insights about the key constraints on the yields at tree and field scales. Here we used machine learning approaches to predict orchard-level yield and examine the determinants of almond yield variation in California’s almond orchards, based on a unique 10-year dataset of field measurements of light interception, remote sensing metrics, and almond yield, along with meteorological data. We found that overall the maximum almond yield was highly dependent on light interception, e.g., with each one percent increase in light interception resulting in an increase of 57.9 lbs/acre in the potential yield. Light interception was highest for mature sites with higher long term mean spring incoming solar radiation, and lowest for younger orchards and when March maximum temperature was lower than 19 o C. However, at any given level of light interception, actual yield often falls significantly below full yield potential, driven mostly by tree age, temperature profiles in June and winter, and summer maximum vapor pressure deficit (VPDmax). The full random forest model was found to explain 82% (±1%) of yield variation, with a RMSE of 480±9 lbs/acre. When excluding light interception from the predictors, overall orchard characteristics (such as age, location and tree density) and key meteorological variables could still explain 78% of yield variation. The model analysis also showed that warmer winter conditions often limited mature orchards from reaching maximum yield potential and higher summer VPDmax significantly limited the yield. Our findings through the machine learning approach improved our understanding of the complex interaction between climate, canopy light interception, and almond nut production. The demonstrated relatively robust predictability of almond yield, driven by “big data”, also provides quantitative information and guidance to make informed orchard nutrient management decisions, allocate resources, determine almond price targets, and improve market planning.","",""
1,"Richard Y. Zhang, C. Josz, S. Sojoudi","Conic Optimization With Applications to Machine Learning and Energy Systems",2018,"","","","",186,"2022-07-13 09:37:52","","","","",,,,,1,0.25,0,3,4,"Optimization is at the core of control theory and appears in several areas of this field, such as optimal control, distributed control, system identification, robust control, state estimation, model predictive control and dynamic programming. The recent advances in various topics of modern optimization have also been revamping the area of machine learning. Motivated by the crucial role of optimization theory in the design, analysis, control and operation of real-world systems, this tutorial paper offers a detailed overview of some major advances in this area, namely conic optimization and its emerging applications. First, we discuss the importance of conic optimization in different areas. Then, we explain seminal results on the design of hierarchies of convex relaxations for a wide range of nonconvex problems. Finally, we study different numerical algorithms for large-scale conic optimization problems.","",""
217,"Ian J. Goodfellow, Nicolas Papernot, P. Mcdaniel","Cleverhans V0.1: an Adversarial Machine Learning Library",2016,"","","","",187,"2022-07-13 09:37:52","","","","",,,,,217,36.17,72,3,6,"cleverhans is a software library that provides standardized reference implementations of adversarial example construction techniques and adversarial training. The library may be used to develop more robust machine learning models and to provide standardized benchmarks of models’ performance in the adversarial setting. Benchmarks constructed without a standardized implementation of adversarial example construction are not comparable to each other, because a good result may indicate a robust model or it may merely indicate a weak implementation of the adversarial example construction procedure. This technical report is structured as follows. Section 1 provides an overview of adversarial examples in machine learning and of the cleverhans software. Section 2 presents the core functionalities of the library: namely the attacks based on adversarial examples and defenses to improve the robustness of machine learning models to these attacks. Section 3 describes how to report benchmark results using the library. Section 4 describes the versioning system.","",""
18,"M. Alhusseini, Firas Abuzaid, A. Rogers, J. Zaman, T. Baykaner, P. Clopton, Peter D. Bailis, M. Zaharia, Paul J. Wang, W. Rappel, S. Narayan","Machine Learning to Classify Intracardiac Electrical Patterns During Atrial Fibrillation",2020,"","","","",188,"2022-07-13 09:37:52","","10.1161/CIRCEP.119.008160","","",,,,,18,9.00,2,11,2,"Supplemental Digital Content is available in the text. Background: Advances in ablation for atrial fibrillation (AF) continue to be hindered by ambiguities in mapping, even between experts. We hypothesized that convolutional neural networks (CNN) may enable objective analysis of intracardiac activation in AF, which could be applied clinically if CNN classifications could also be explained. Methods: We performed panoramic recording of bi-atrial electrical signals in AF. We used the Hilbert-transform to produce 175 000 image grids in 35 patients, labeled for rotational activation by experts who showed consistency but with variability (kappa [κ]=0.79). In each patient, ablation terminated AF. A CNN was developed and trained on 100 000 AF image grids, validated on 25 000 grids, then tested on a separate 50 000 grids. Results: In the separate test cohort (50 000 grids), CNN reproducibly classified AF image grids into those with/without rotational sites with 95.0% accuracy (CI, 94.8%–95.2%). This accuracy exceeded that of support vector machines, traditional linear discriminant, and k-nearest neighbor statistical analyses. To probe the CNN, we applied gradient-weighted class activation mapping which revealed that the decision logic closely mimicked rules used by experts (C statistic 0.96). Conclusions: CNNs improved the classification of intracardiac AF maps compared with other analyses and agreed with expert evaluation. Novel explainability analyses revealed that the CNN operated using a decision logic similar to rules used by experts, even though these rules were not provided in training. We thus describe a scaleable platform for robust comparisons of complex AF data from multiple systems, which may provide immediate clinical utility to guide ablation. Registration: URL: https://www.clinicaltrials.gov; Unique identifier: NCT02997254.","",""
0,"Yuanyi Zhong, Haoran Tang, Junkun Chen, Jian Peng, Yu-Xiong Wang","Is Self-Supervised Learning More Robust Than Supervised Learning?",2022,"","","","",189,"2022-07-13 09:37:52","","10.48550/arXiv.2206.05259","","",,,,,0,0.00,0,5,1,"Self-supervised contrastive learning is a powerful tool to learn visual representation without labels. Prior work has primarily focused on evaluating the recognition accuracy of various pre-training algorithms, but has overlooked other behavioral aspects. In addition to accuracy, distributional robustness plays a critical role in the reliability of machine learning models. We design and conduct a series of robustness tests to quantify the behavioral differences between contrastive learning and supervised learning to downstream or pretraining data distribution changes. These tests leverage data corruptions at multiple levels, ranging from pixellevel gamma distortion to patch-level shuffling and to dataset-level distribution shift. Our tests unveil intriguing robustness behaviors of contrastive and supervised learning. On the one hand, under downstream corruptions, we generally observe that contrastive learning is surprisingly more robust than supervised learning. On * Equal contribution the other hand, under pre-training corruptions, we find contrastive learning vulnerable to patch shuffling and pixel intensity change, yet less sensitive to dataset-level distribution change. We attempt to explain these results through the role of data augmentation and feature space properties. Our insight has implications in improving the downstream robustness of supervised learning.","",""
0,"G. Schuster","Machine Learning and Wave Equation Inversion ofSkeletonized Data",2018,"","","","",190,"2022-07-13 09:37:52","","10.3997/2214-4609.201801882","","",,,,,0,0.00,0,1,4,"We compare the full waveform inversion (FWI), skeletonized wave equation inversion (SWI), and supervised Machine Learning (ML) algorithms with one another. For velocity inversion the advantage of SWI over FWI is it is more robust and has less of a tendency in getting stuck at local minima. This is because SWI only needs to explain the kinematic information in the seismograms, which is less demanding than FWI’s difficult task of explaining all of the wiggles in every arrival. The disadvantage of SWI is that it provides a tomogram with theoretically less resolution than the ideal FWI tomogram. In this case, the SWI tomogram can be used as an excellent starting model for FWI. SWI is similar to supervised Machine Learning in that both use skeletonized representations of the original data. Simpler input data lead to simpler misfit functions characterized by quicker convergence to useful solutions. I show how a hybrid ML+SWI method and the implicit function theorem can be used to extract almost any skeletal feature in the data and invert it using the wave equation. This assumes that the skeletal data are sensitive to variations in the model parameter of interest.","",""
0,"Claus Huber","Machine Learning for Visual Risk Analysis and Hedge Fund Selection",2018,"","","","",191,"2022-07-13 09:37:52","","10.2139/ssrn.3289979","","",,,,,0,0.00,0,1,4,"One of the main principles to build portfolios of financial assets is to achieve stable long-term performance and avoid large drawdowns. This article describes how a method of Machine Learning, Kohonen’s Self-Organising Maps (SOM), can be applied to visualise risk and to build robust portfolios of hedge fund managers. Essentially, it documents a feasibility study that was conducted to gauge whether Machine Learning can add any value to the investment process of an investor in hedge funds.    We suggest a simple method to exploit the SOM feature of identifying similarities in high-dimensional data: managers are selected from the 4 most remote parts of the SOM, i.e., the units in the lower left, lower right, upper left and upper right corners. Hedge Fund portfolios based on this method achieve more favourable risk/return ratios and lower drawdowns than benchmarks. In discussions with clients it has turned out that the way SOMs work as well as the method to pick managers from remote areas of the SOM can be intuitively explained and understood, which increases acceptance by practitioners.","",""
10,"J. Nicely, B. Duncan, T. Hanisco, G. Wolfe, R. Salawitch, M. Deushi, A. S. Haslerud, P. Jöckel, B. Josse, D. Kinnison, A. Klekociuk, M. Manyin, V. Marécal, O. Morgenstern, L. Murray, G. Myhre, L. Oman, G. Pitari, A. Pozzer, Ilaria Quaglia, L. Revell, E. Rozanov, A. Stenke, K. Stone, S. Strahan, S. Tilmes, H. Tost, D. Westervelt, G. Zeng","A machine learning examination of hydroxyl radical differences among model simulations for CCMI-1",2020,"","","","",192,"2022-07-13 09:37:52","","10.5194/ACP-20-1341-2020","","",,,,,10,5.00,1,29,2,"Abstract. The hydroxyl radical (OH) plays critical roles within the troposphere, such as determining the lifetime of methane ( CH4 ), yet is challenging to model due to its fast cycling and dependence on a multitude of sources and sinks. As a result, the reasons for variations in OH and the resulting methane lifetime ( τ CH 4 ), both between models and in time, are difficult to diagnose. We apply a neural network (NN) approach to address this issue within a group of models that participated in the Chemistry-Climate Model Initiative (CCMI). Analysis of the historical specified dynamics simulations performed for CCMI indicates that the primary drivers of τ CH 4  differences among 10 models are the flux of UV light to the troposphere (indicated by the photolysis frequency JO1D ), the mixing ratio of tropospheric ozone ( O3 ), the abundance of nitrogen oxides ( NO x ≡ NO + NO 2 ), and details of the various chemical mechanisms that drive OH. Water vapour, carbon monoxide (CO), the ratio of NO:NOx , and formaldehyde (HCHO) explain moderate differences in τ CH 4 , while isoprene, methane, the photolysis frequency of NO2 by visible light ( JNO2 ), overhead ozone column, and temperature account for little to no model variation in τ CH 4 . We also apply the NNs to analysis of temporal trends in OH from 1980 to 2015. All models that participated in the specified dynamics historical simulation for CCMI demonstrate a decline in τ CH 4  during the analysed timeframe. The significant contributors to this trend, in order of importance, are tropospheric O3 , JO1D , NOx , and  H2O , with CO also causing substantial interannual variability in OH burden. Finally, the identified trends in τ CH 4 are compared to calculated trends in the tropospheric mean OH concentration from previous work, based on analysis of observations. The comparison reveals a robust result for the effect of rising water vapour on OH and  τ CH 4 , imparting an increasing and decreasing trend of about 0.5 % decade −1 , respectively. The responses due to NOx , ozone column, and temperature are also in reasonably good agreement between the two studies.","",""
0,"Jannik Zgraggen, G. Pizza, Lilach Goren Huber","Uncertainty Informed Anomaly Scores with Deep Learning: Robust Fault Detection with Limited Data",2022,"","","","",193,"2022-07-13 09:37:52","","10.36001/phme.2022.v7i1.3342","","",,,,,0,0.00,0,3,1,"Quantifying the predictive uncertainty of a model is an important ingredient in data-driven decision making. Uncertainty quantification has been gaining interest especially for deep learning models, which are often hard to justify or explain. Various techniques for deep learning based uncertainty estimates have been developed primarily for image classification and segmentation, but also for regression and forecasting tasks. Uncertainty quantification for anomaly detection tasks is still rather limited for image data and has not yet been demonstrated for machine fault detection in PHM applications. In this paper we suggest an approach to derive an uncertaintyinformed anomaly score for regression models trained with normal data only. The score is derived using a deep ensemble of probabilistic neural networks for uncertainty quantification. Using an example of wind-turbine fault detection, we demonstrate the superiority of the uncertainty-informed anomaly score over the conventional score. The advantage is particularly clear in an ”out-of-distribution” scenario, in which the model is trained with limited data which does not represent all normal regimes that are observed during model deployment.","",""
66,"R. Cuocolo, Maria Brunella Cipullo, A. Stanzione, L. Ugga, V. Romeo, L. Radice, A. Brunetti, M. Imbriaco","Machine learning applications in prostate cancer magnetic resonance imaging",2019,"","","","",194,"2022-07-13 09:37:52","","10.1186/s41747-019-0109-2","","",,,,,66,22.00,8,8,3,"","",""
5,"P. P. Chaves, G. Zuquim, K. Ruokolainen, J. V. Doninck, R. Kalliola, Elvira Gómez Rivero, H. Tuomisto","Mapping Floristic Patterns of Trees in Peruvian Amazonia Using Remote Sensing and Machine Learning",2020,"","","","",195,"2022-07-13 09:37:52","","10.3390/rs12091523","","",,,,,5,2.50,1,7,2,"Recognition of the spatial variation in tree species composition is a necessary precondition for wise management and conservation of forests. In the Peruvian Amazonia, this goal is not yet achieved mostly because adequate species inventory data has been lacking. The recently started Peruvian national forest inventory (INFFS) is expected to change the situation. Here, we analyzed genus-level variation, summarized through non-metric multidimensional scaling (NMDS), in a set of 157 INFFS inventory plots in lowland to low mountain rain forests (<2000 m above sea level) using Landsat satellite imagery and climatic, edaphic, and elevation data as predictor variables. Genus-level floristic patterns have earlier been found to be indicative of species-level patterns. In correlation tests, the floristic variation of tree genera was most strongly related to Landsat variables and secondly to climatic variables. We used random forest regression, under varying criteria of feature selection and cross-validation, to predict the floristic composition on the basis of Landsat and environmental data. The best model explained >60% of the variation along NMDS axes 1 and 2 and 40% of the variation along NMDS axis 3. We used this model to predict the three NMDS dimensions at a 450-m resolution over all of the Peruvian Amazonia and classified the pixels into 10 floristic classes using k-means classification. An indicator analysis identified statistically significant indicator genera for 8 out of the 10 classes. The results are congruent with earlier studies, suggesting that the approach is robust and can be applied to other tropical regions, which is useful for reducing research gaps and for identifying suitable areas for conservation.","",""
2,"T. Khan, Kushsairy A., S. Nasim, M. Alam, Z. Shahid, M. Mazliham","Proficiency Assessment of Machine Learning Classifiers: An Implementation for the Prognosis of Breast Tumor and Heart Disease Classification",2020,"","","","",196,"2022-07-13 09:37:52","","10.14569/ijacsa.2020.0111170","","",,,,,2,1.00,0,6,2,"Breast cancer and heart disease can be acknowledged as very dangerous and common disease in many countries including Pakistan. In this paper classifiers comparative study has been performed for the tumor and heart disease classification. Around one lac women are diagnosed annually with this life-threatening disease having no family history of the disease. If it is not treated on time it may grow and spread to the other parts of human body. Mammograms are the X-rays of the breast which can be used for the screening of cancer tumor. Prior identification of breast cancer may increase the chance of survival up to 70 percent. Tumors which causes cancer can be categorized into two types: a) Benign and b) Malignant. Benign tumor can be explained as the tumor which are not attached to neighbor tissues or spread in the other parts of the body. In Malignant tumor, other parts may be affected by it as it can grow and spread in the other parts of the body. To classify the tumor as Malignant or Benign is very complex as the similarities of cancer tumor and tumor caused by the skin inflammation are almost same. The early identification of Malignant is mandatory to protect the patient life. Diversified medical methods based on deep learning and machine learning have been developed to treat the patients as cancer is a very serious and crucial issue in this era. In this research paper machine learning algorithms like logistic regression, K-NN and tree have been applied to the breast cancer data set which has been taken from UCI Machine learning repository. Comparative study of classifiers has been performed to determine the better classifier for the robust prediction of breast tumors. Simulated results proved that using Logistic regression, ninety-one percent accuracy was achieved. The research showed that logistic regression can be applied for the accurate and precise early prediction of breast cancer. Cardiovascular disease is very common throughout the world. It has been noticed that health in cardiac patients that there are so many factors which causes heart disease or heart attack. The factors leading to the heart failure includes varying blood pressure, high sugar, cardiac pain, and heart rate, high cholesterol level (LDL), artery blockage and irregular ECG signals. Many researchers proved that stress in patients can also be the reason for the heart disease. Higher numbers of cardiac surgeries like angioplasty and heart by-pass are performed on annual basis. Actually, people don’t care about their lifestyle and diet and fully ignore the symbols. It can be early predicted and cured if proper testing and medication for heart is done. Sometimes there is a false pain which has the same feeling like angina pain depicting cardiovascular disease. To reduce the false alarm and robustly classify the heart disease, several machine learning approaches have been adopted. In proposed research for the accurate classification of heart disease comparison has been performed among support vector machine (SVM), K-nearest neighbors K-NN and linear discriminant analysis. Simulated results demonstrated that Support vector machine was found to be a better classifier having an accuracy of 80.4%. Keywords—Breast cancer; benign; malignant; logistic regression; cardiovascular disease; heart disease diagnosis; support vector machine; classifiers; k-nearest neighbors","",""
1,"N. Howard, Naima Chouikhi, Ahsan Adeel, Katelyn Dial, Adam Howard, A. Hussain","BrainOS: A Novel Artificial Brain-Alike Automatic Machine Learning Framework",2020,"","","","",197,"2022-07-13 09:37:52","","10.3389/fncom.2020.00016","","",,,,,1,0.50,0,6,2,"Human intelligence is constituted by a multitude of cognitive functions activated either directly or indirectly by external stimuli of various kinds. Computational approaches to the cognitive sciences and to neuroscience are partly premised on the idea that computational simulations of such cognitive functions and brain operations suspected to correspond to them can help to further uncover knowledge about those functions and operations, specifically, how they might work together. These approaches are also partly premised on the idea that empirical neuroscience research, whether following on from such a simulation (as indeed simulation and empirical research are complementary) or otherwise, could help us build better artificially intelligent systems. This is based on the assumption that principles by which the brain seemingly operate, to the extent that it can be understood as computational, should at least be tested as principles for the operation of artificial systems. This paper explores some of the principles of the brain that seem to be responsible for its autonomous, problem-adaptive nature. The brain operating system (BrainOS) explicated here is an introduction to ongoing work aiming to create a robust, integrated model, combining the connectionist paradigm underlying neural networks and the symbolic paradigm underlying much else of AI. BrainOS is an automatic approach that selects the most appropriate model based on the (a) input at hand, (b) prior experience (a history of results of prior problem solving attempts), and (c) world knowledge (represented in the symbolic way and used as a means to explain its approach). It is able to accept diverse and mixed input data types, process histories and objectives, extract knowledge and infer a situational context. BrainOS is designed to be efficient through its ability to not only choose the most suitable learning model but to effectively calibrate it based on the task at hand.","",""
92,"Rohit Punnoose, Pankaj Ajit","Prediction of Employee Turnover in Organizations using Machine Learning Algorithms A case for Extreme Gradient Boosting",2016,"","","","",198,"2022-07-13 09:37:52","","10.14569/IJARAI.2016.050904","","",,,,,92,15.33,46,2,6,"Employee turnover has been identified as a key issue for organizations because of its adverse impact on work place productivity and long term growth strategies. To solve this problem, organizations use machine learning techniques to predict employee turnover. Accurate predictions enable organizations to take action for retention or succession planning of employees. However, the data for this modeling problem comes from HR Information Systems (HRIS); these are typically under-funded compared to the Information Systems of other domains in the organization which are directly related to its priorities. This leads to the prevalence of noise in the data that renders predictive models prone to over-fitting and hence inaccurate. This is the key challenge that is the focus of this paper, and one that has not been addressed historically. The novel contribution of this paper is to explore the application of Extreme Gradient Boosting (XGBoost) technique which is more robust because of its regularization formulation. Data from the HRIS of a global retailer is used to compare XGBoost against six historically used supervised classifiers and demonstrate its significantly higher accuracy for predicting employee turnover. In this paper, the problem of employee turnover and the key machine learning algorithms that have been used to solve it are discussed. The novel contribution of this paper is to explore the application of extreme gradient boosting (XGBoost) as an improvement on these traditional algorithms, specifically in its ability to generalize on noise-ridden data which is prevalent in this domain. This is done by using data from the HRIS of a global retailer and treating the attrition problem as a classification task and modeling it using supervised techniques. The conclusion is reached by contrasting the superior accuracy of the XGBoost classifier against other techniques and explaining the reason for its superior performance. This paper is structured as follows. Section II gives a brief overview of the employee turnover problem, the importance of solving it, and the historical work done in terms of application of machine learning techniques to solve this problem. Section III explores the 7 different supervised techniques, including XGBoost, that this paper compares. Section IV outlines the experimental design in terms of the characteristics of the dataset, pre-processing, cross-validation, and the choice of metrics for accuracy comparison. Section V showcases the results of the study and its subsequent discussion. Section VI concludes the paper by recommending the XGBoost classifier for predicting turnover.","",""
26,"Leila Etaati","Azure Machine Learning Studio",2019,"","","","",199,"2022-07-13 09:37:52","","10.1007/978-1-4842-3658-1_12","","",,,,,26,8.67,26,1,3,"","",""
19,"Finale Doshi-Velez, R. Perlis","Evaluating Machine Learning Articles.",2019,"","","","",200,"2022-07-13 09:37:52","","10.1001/jama.2019.17304","","",,,,,19,6.33,10,2,3,"In this issue of JAMA, Liu and colleagues1 provide a users’ guide to reading clinical machine learning articles. Beyond a synopsis of selected concepts in modern machine learning, the authors elaborate step-by-step guidance for physicians seeking to evaluate this evidence with a critical eye. In an era when readers are bombarded with artificial intelligence in everyday life, from credit card fraud warnings and smartphones that anticipate their needs to life-like videos of people who do not actually exist, the sanity check provided by this article is most welcome. Reassuringly, many of the key elements in reading a machine learning article draw directly on concerns familiar to JAMA readers of users’ guides, and they have changed little in the 3 decades since Nierenberg described an approach to diagnostic testing.2 Common sense and standard statistical principles still apply when it comes to these more complex models. For example, choices about the inputs and outputs of a model, such as what and how patient features are measured and what is to be predicted, are essential in determining the practical value of an algorithm. Are the inputs measured reliably, and do they draw on readily available technology (facts from electronic health records; routine laboratory studies) or emerging technology (new positron emission tomography tracers, single-cell transcriptomics) that may make implementation and dissemination more challenging? Are the outputs clinically actionable? Generations of medical students recall the adage, ”don’t order a test unless it will change management”; certainly this applies to artificial intelligence as well. Tools to detect retinopathy3 or identify tuberculosis or malaria using smartphone images4 may be particularly beneficial in low-resource settings. Choices about cohort selection and data preparation (most notably, handling of missing data) will have important consequences for subsequent analyses; machine learning does not solve problems of bias introduced by missing data. Were models trained only with canonical or clear-cut examples? In clinical practice, data are noisy and not always complete; failure to consider these circumstances may yield models that perform beautifully on cleaned data sets for the purposes of publication but miserably in practice. Radiologists do not struggle to identify cancer in pristine chest radiologic images accompanied by detailed history but poorer-quality images with superimposed pneumonia and little clinical context pose a more realistic challenge. In addition, proper validation is essential, and replication is a crucial piece of the validation process. As Liu et al1 note, in machine learning studies, it remains critical to know whether the model has been validated across new clinical settings. Many of the most important challenges in machine learning are related to various forms of overfitting in which a model explains a training data set perfectly but fails to generalize. Showing a model performs well in another patient cohort in the same health system is good; showing that it performs well in an entirely different setting is far better. Such replication is the beginning, not the end, of a long process for validation and dissemination— one that draws on decades of lessons from work on developing diagnostics. While much of the guidance in the article by Liu et al1 will be familiar, a few key considerations bear particular emphasis in the context of machine learning applications to medicine. For example, the authors note that more complex machine learning systems are often pretrained on one data set (eg, public images on the internet of places and things) and then refit to another task (eg, retinal images). The kinds of bias introduced by such procedures is not well understood. For example, it seems likely that interpreting ophthalmologic images requires additional features beyond those needed to distinguish major categories, such as with images in general. In this case, the trained model may be systematically failing on those elements specific to opthalmology—that is, requiring features not present in general internet images while performing well overall. Such failures may be particularly concerning if they result in the model performing more poorly for specific types of patients. The preceding example raises a larger point: because machine learning methods are myriad, in a state of rapid development, and less familiar to most clinical readers, authors of articles using machine learning must make their underlying assumptions, model properties, optimization strategies, and limitations explicit in the article. The example of transfer learning reusing a previously trained model is just one way in which properties are implicitly introduced; another is how regularization, a form of smoothing, is performed—smoothing different parameters can have different effects on the final behavior and performance the model. The predictions made by an algorithm may or may not be robust to even tiny changes to the input (eg, how differences in an image that are nondiscernible to the human eye may cause an algorithm to change its predictions).5 Because these failure modes may not be expected, it is essential that the authors of articles reporting on machine learning point out what the failure modes of their algorithmic approaches might be. An acknowledgment of limitations should make readers more rather than less Related article page 1806 Opinion","",""
