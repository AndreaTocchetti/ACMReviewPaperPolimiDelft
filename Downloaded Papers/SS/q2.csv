Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
0,"Minh-Hoang Tran, Ngoc Quy Nguyen, H. Pham","A New Hope in the Fight Against Antimicrobial Resistance with Artificial Intelligence",2022,"","","","",1,"2022-07-13 09:19:07","","10.2147/IDR.S362356","","",,,,,0,0.00,0,3,1,"Abstract Recent years have witnessed the rise of artificial intelligence (AI) in antimicrobial resistance (AMR) management, implying a positive signal in the fight against antibiotic-resistant microbes. The impact of AI starts with data collection and preparation for deploying AI-driven systems, which can lay the foundation for some effective infection control strategies. Primary applications of AI include identifying potential antimicrobial molecules, rapidly testing antimicrobial susceptibility, and optimizing antibiotic combinations. Aside from their outstanding effectiveness, these applications also express high potential in narrowing the burden gap of AMR among different settings around the world. Despite these benefits, the interpretability of AI-based systems or models remains vague. Attempts to address this issue had led to two novel explanation techniques, but none have shown enough robustness or comprehensiveness to be widely applied in AI and AMR control. A multidisciplinary collaboration between the medical field and advanced technology is therefore needed to partially manage this situation and improve the AI systems’ performance and their effectiveness against drug-resistant pathogens, in addition to multiple equity actions for mitigating the failure risks of AI due to a global-scale equity gap.","",""
0,"Liu Zhendong, Wenyu Jiang, Yan Zhang, Chongjun Wang","Explanation-based Counterfactual Retraining(XCR): A Calibration Method for Black-box Models",2022,"","","","",2,"2022-07-13 09:19:07","","10.48550/arXiv.2206.11126","","",,,,,0,0.00,0,4,1,"With the rapid development of eXplainable Artificial Intelligence (XAI), a long line of past papers have shown concerns about the Out-of-Distribution (OOD) problem in perturbation-based post-hoc XAI models and explanations are socially misaligned. We explore the limitations of post-hoc explanation methods that use approximators to mimic the behavior of black-box models. Then we propose eXplanationbased Counterfactual Retraining (XCR), which extracts feature importance fastly. XCR applies the explanations generated by the XAI model as counterfactual input to retrain the black-box model to address OOD and social misalignment problems. Evaluation of popular image datasets shows that XCR can even improve model performance when retaining 12.5% of the most crucial features without changing the black-box model structure. Furthermore, the evaluation of the benchmark of corruption datasets shows that the XCR is very helpful for improving model robustness and positively impacts the calibration of OOD problems. Even though not calibrated in the validation set like some OOD calibration methods, the corrupted data metric outperforms existing methods. Our method also beats current OOD calibration methods on the OOD calibration metric if calibration on the validation set is applied.","",""
11,"R. Confalonieri, Tarek R. Besold, Tillman Weyde, Kathleen A. Creel, T. Lombrozo, Shane T. Mueller, Patrick Shafto","What makes a good explanation? Cognitive dimensions of explaining intelligent machines",2019,"","","","",3,"2022-07-13 09:19:07","","","","",,,,,11,3.67,2,7,3,"Explainability is assumed to be a key factor for the adoption of Artificial Intelligence systems in a wide range of contexts (Hoffman, Mueller, & Klein, 2017; Hoffman, Mueller, Klein, & Litman, 2018; Doran, Schulz, & Besold, 2017; Lipton, 2018; Miller, 2017; Lombrozo, 2016). The use of AI components in self-driving cars, medical diagnosis, or insurance and financial services has shown that when decisions are taken or suggested by automated systems it is essential for practical, social, and increasingly legal reasons that an explanation can be provided to users, developers or regulators.1Moreover, the reasons for equipping intelligent systems with explanation capabilities are not limited to user rights and acceptance. Explainability is also needed for designers and developers to enhance system robustness and enable diagnostics to prevent bias, unfairness and discrimination, as well as to increase trust by all users in why and how decisions are made. Against that background, increased efforts are directed towards studying and provisioning explainable intelligent systems, both in industry and academia, sparked by initiatives like the DARPA Explainable Artificial Intelligence Program (DARPA, 2016). In parallel, scientific conferences and workshops dedicated to explainability are now regularly organised, such as the ‘ACM Conference on Fairness, Accountability, and Transparency (ACM FAT)’ (Friedler & Wilson, n.d.) or the ‘Workshop on Explainability in AI’ at the 2017 and 2018 editions of the International Joint Conference on Artificial Intelligence. However, one important question remains hitherto unanswered: What are the criteria for a good explanation?","",""
3,"F. Lécué, B. Abeloos, Jonathan Anctil, Manuel Bergeron, Damien Dalla-Rosa, Simon Corbeil-Letourneau, Florian Martet, Tanguy Pommellet, L. Salvan, Simon Veilleux, M. Ziaeefard","Thales XAI Platform: Adaptable Explanation of Machine Learning Systems - A Knowledge Graphs Perspective",2019,"","","","",4,"2022-07-13 09:19:07","","","","",,,,,3,1.00,0,11,3,"Explanation in Machine Learning systems has been identified to be the main asset to have for large scale deployment of Artificial Intelligence (AI) in critical systems. Explanations could be example-, features-, semantics-based or even counterfactual to potentially action on an AI system; they could be represented in many different ways e.g., textual, graphical, or visual. All representations serve different means, purpose and operators. We built the first-of-its-kind XAI (eXplainable AI) platform for critical systems i.e., Thales XAI Platform which aims at serving explanations through various forms. This paper emphasizes on the semantics-based explanations for Machine Learning systems. 1 Explainable AI in Critical Systems Motivation: The current hype of Artificial Intelligence (AI) mostly refers to the success of Machine Learning (ML) and its sub-domain of deep learning. However industries operating with critical systems are either highly regulated, or require high level of certification and robustness. Therefore, such industry constraints do limit the adoption of non deterministic and ML systems. Answers to the question of explainability will be intrinsically connected to the adoption of AI in industry at scale. Indeed explanation, which could be used for debugging intelligent systems or deciding to follow a recommendation in real-time, will increase acceptance and (business) user trust. Explainable AI (XAI) is now referring to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. Focus: Thales XAI Platform is designed to provide explanation for a ML task (classification, regression, object detection, segmentation). Although Thales XAI Platform does provide different levels of explanation e.g., example-based, featuresbased, counterfactual using textual and visual representations, we emphasis only on the semantics-based explanation through knowledge graphs. ? Copyright c © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Critical Applications: From adapting a plane trajectory, stopping a train, refitting a boat to reconfiguring a satellite, all are examples of critical situations where explanation is a must-to-have to follow an AI system decision. 2 Why Knowledge Graphs for Explainable AI? State-of-the-Art Limitations: Most approaches limits explanation of ML systems to features involved in the data and model, or at best to examples, prototypes or counterfactuals. Explanation should go beyond correlation (features importance) and numerical similarity (local explanation). Opportunity: By expanding and linking initial (training, validation and test) data with entities in knowledge graphs, (i) context is encoded, (ii) connections and relations are exposed, and (iii) inference and causation are natively supported. Knowledge graphs are used for encoding better representation of data, structuring a ML model in a more interpretable way, and adopt a semantic similarity for local (instance-based) and global (model-based) explanation. 3 Thales XAI Platform: A Knowledge Graph Perspective (Semantic) Perspective: The platform is combining ML and reasoning functionalities to expose a human-like rational as explanation when (i) recognizing an object (in a raw image) of any class in a knowledge graph, (ii) predicting a link in a knowledge graph. Thales XAI Platform is using state-of-the-art Semantic Web tools for enriching input, output (class) data with DBpedia (4, 233, 000 resources) and domain-specific knowledge graphs, usually enterprise knowledge graphs. This is a crucial step for contextualizing training, validation, test data. Explainable ML Classifications: Starting from raw images, as unstructured data, but with class labels augmented with a domain knowledge graph, Thales XAI Platform relies on existing neural network architectures to build the most appropriate models. All confidence scores of output classes on any input image are updated based on the semantic description of the output classes. For instance, an input classified as a car will have a higher overall confidence score in case some properties of car in the knowledge graph are retrieved e.g., having wheels, being on a road. In addition the platform is embedding naturally explanation i.e., properties of the objects retrieved in both the raw data and knowledge graph. Explainable Relational Learning: Starting from relational data, structured as graph, and augmented with a domain knowledge graph, Thales XAI Platform relies on existing knowledge graph embeddings frameworks to build the most appropriate models. Explanation of any link prediction is retrieved by identifying representative hotspots in the knowledge graph i.e., connected parts of the graphs that negatively impact prediction accuracy when removed.","",""
3,"Satya M. Muddamsetty, M. N. Jahromi, Andreea-Emilia Ciontos, Laura M. Fenoy, T. Moeslund","Introducing and assessing the explainable AI (XAI)method: SIDU",2021,"","","","",5,"2022-07-13 09:19:07","","","","",,,,,3,3.00,1,5,1,"Explainable Artificial Intelligence (XAI) has in recent years become a well-suited framework to generate human understandable explanations of black box models. In this paper, we present a novel XAI visual explanation algorithm denoted SIDU that can effectively localize entire object regions responsible for prediction in a full extend. We analyze its robustness and effectiveness through various computational and human subject experiments. In particular, we assess the SIDU algorithm using three different types of evaluations (Application, Human and Functionally-Grounded) to demonstrate its superior performance. The robustness of SIDU is further studied in presence of adversarial attack on black box models to better understand its performance.","",""
3,"A. Ran, C. Cheung","Deep Learning-Based Optical Coherence Tomography and Optical Coherence Tomography Angiography Image Analysis: An Updated Summary",2021,"","","","",6,"2022-07-13 09:19:07","","10.1097/APO.0000000000000405","","",,,,,3,3.00,2,2,1,"Supplemental Digital Content is available in the text Abstract Deep learning (DL) is a subset of artificial intelligence based on deep neural networks. It has made remarkable breakthroughs in medical imaging, particularly for image classification and pattern recognition. In ophthalmology, there are rising interests in applying DL methods to analyze optical coherence tomography (OCT) and optical coherence tomography angiography (OCTA) images. Studies showed that OCT and OCTA image evaluation by DL algorithms achieved good performance for disease detection, prognosis prediction, and image quality control, suggesting that the incorporation of DL technology could potentially enhance the accuracy of disease evaluation and the efficiency of clinical workflow. However, substantial issues, such as small training sample size, data preprocessing standardization, model robustness, results explanation, and performance cross-validation, are yet to be tackled before deploying these DL models in real-time clinics. This review summarized recent studies on DL-based image analysis models for OCT and OCTA images and discussed the potential challenges of clinical deployment and future research directions.","",""
2,"V. W. Anelli, Alejandro Bellog'in, T. D. Noia, F. Donini, Vincenzo Paparella, Claudio Pomo","Adherence and Constancy in LIME-RS Explanations for Recommendation (Long paper)",2021,"","","","",7,"2022-07-13 09:19:07","","","","",,,,,2,2.00,0,6,1,"Explainable Recommendation has attracted a lot of attention due to a renewed interest in explainable artificial intelligence. In particular, post-hoc approaches have proved to be the most easily applicable ones to increasingly complex recommendation models, which are then treated as black boxes. The most recent literature has shown that for post-hoc explanations based on local surrogate models, there are problems related to the robustness of the approach itself. This consideration becomes even more relevant in human-related tasks like recommendation. The explanation also has the arduous task of enhancing increasingly relevant aspects of user experience such as transparency or trustworthiness. This paper aims to show how the characteristics of a classical post-hoc model based on surrogates is strongly model-dependent and does not prove to be accountable for the explanations generated.","",""
3,"A. Preece, Daniel Harborne, R. Raghavendra, Richard J. Tomsett, Dave Braines","Provisioning Robust and Interpretable AI/ML-Based Service Bundles",2018,"","","","",8,"2022-07-13 09:19:07","","10.1109/MILCOM.2018.8599838","","",,,,,3,0.75,1,5,4,"Coalition operations environments are characterised by the need to share intelligence, surveillance and reconnaissance services. Increasingly, such services are based on artificial intelligence (AI)and machine learning (ML)technologies. Two key issues in the exploitation of AI/ML services are robustness and interpretability. Employing a diverse portfolio of services can make a system robust to ‘unknown unknowns’. Interpretability - the need for services to offer explanation facilities to engender user trust - can be addressed by a variety of methods to generate either transparent or post hoc explanations according to users' requirements. This paper shows how a service-provisioning framework for coalition operations can be extended to address specific requirements for robustness and interpretability, allowing automatic selection of service bundles for intelligence, surveillance and reconnaissance tasks. The approach is demonstrated in a case study on traffic monitoring featuring a diverse set of AI/ML services based on deep neural networks and heuristic reasoning approaches.","",""
11,"Ramon Quiza, Omar López-Armas, J. Davim","Hybrid Modeling and Optimization of Manufacturing",2012,"","","","",9,"2022-07-13 09:19:07","","10.1007/978-3-642-28085-6","","",,,,,11,1.10,4,3,10,"","",""
4,"Ekhlas Mhawi, H. Daniyal, M. Sulaiman","Advanced Techniques in Harmonic Suppression via Active Power Filter: A Review",2015,"","","","",10,"2022-07-13 09:19:07","","10.11591/IJPEDS.V6.I2.PP185-195","","",,,,,4,0.57,1,3,7,"This paper intends to present the recent development of artificial intelligence (AI) applications in active power filter (APF). As a result of the development in power electronic technology, (APF) continues to attract ample attention. Compared with the traditional reactive LC filter, active power filter is considered to be more effective in compensating harmonic current generated by nonlinear loads.APF, can correct the power quality and improve the reliability and stability on power utility. A brief explanation of some important areas in AI and a comprehensive survey of the literature along the main categories of AI is presented to introduce the readers into the wide-ranging topics that AI encompasses. Plenty of relevant literatures have been selected in the review, mostly emphasized on better accuracy, robustness, efficiency, stability and tracking ability of the system.","",""
0,"Cuncun Wei","Object Recognition Based on Descriptor of Improved Histograms of Second-Order Gradients",2016,"","","","",11,"2022-07-13 09:19:07","","","","",,,,,0,0.00,0,1,6,"Object recognition is a very meaningful work in the research and application of computer vision and artificial intelligence. The main task of object identification is to determine whether there exist objects of interest in images taken, if yes, to give reasonable explanation to objects, namely, to judge what the object is and determine its position. According to the defect of classic local feature descriptors in characterizing the curvature related features, as well as the insufficient of classic second order gradient histogram when calculate first-order gradient and the second-order gradient histogram, in order to improve recognition accuracy and noise robustness, an object recognition method based on an improved second-order gradient histogram descriptor is proposed in this paper. Using the absolute value to retain the first-order gradient of all directions, eliminating the defect of lost some direction when the gradient direction is lost odd, Gaussian function weighted is used to calculate second order gradient histograms, which considered the influence of the neighboring pixels to the center pixel and improved anti-noise performance. Experimental results show that the method in this paper improves the recognition accuracy and noise robustness.","",""
3,"J. Kipps, D. Gajski","The role of learning in logic synthesis",1989,"","","","",12,"2022-07-13 09:19:07","","10.1109/TAI.1989.65328","","",,,,,3,0.09,2,2,33,"A model of logic synthesis that uses technology-specific design rules and extends rule-based search to functional decomposition and technology mapping is proposed. The problem of technology independence is addressed with the addition of a model of learning for automating the generation of design rules. While this model improves design quality by taking advantage of the target technology, it is not robust to technology changes. To improve robustness, the model is augmented with two learning components: one for acquiring rules that make use of physical cells in a technology library and another for acquiring rules that make use of appropriate design styles. These components are related to work in the learning of macro-operators and explanation-based learning.<<ETX>>","",""
1954,"Tim Miller","Explanation in Artificial Intelligence: Insights from the Social Sciences",2017,"","","","",13,"2022-07-13 09:19:07","","10.1016/J.ARTINT.2018.07.007","","",,,,,1954,390.80,1954,1,5,"","",""
68,"Ilia Stepin, J. M. Alonso, Alejandro Catalá, Martin Pereira-Fariña","A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence",2021,"","","","",14,"2022-07-13 09:19:07","","10.1109/ACCESS.2021.3051315","","",,,,,68,68.00,17,4,1,"A number of algorithms in the field of artificial intelligence offer poorly interpretable decisions. To disclose the reasoning behind such algorithms, their output can be explained by means of so-called evidence-based (or factual) explanations. Alternatively, contrastive and counterfactual explanations justify why the output of the algorithms is not any different and how it could be changed, respectively. It is of crucial importance to bridge the gap between theoretical approaches to contrastive and counterfactual explanation and the corresponding computational frameworks. In this work we conduct a systematic literature review which provides readers with a thorough and reproducible analysis of the interdisciplinary research field under study. We first examine theoretical foundations of contrastive and counterfactual accounts of explanation. Then, we report the state-of-the-art computational frameworks for contrastive and counterfactual explanation generation. In addition, we analyze how grounded such frameworks are on the insights from the inspected theoretical approaches. As a result, we highlight a variety of properties of the approaches under study and reveal a number of shortcomings thereof. Moreover, we define a taxonomy regarding both theoretical and practical approaches to contrastive and counterfactual explanation.","",""
52,"Hamon Ronan, Junklewitz Henrik, S. Ignacio","Robustness and Explainability of Artificial Intelligence",2020,"","","","",15,"2022-07-13 09:19:07","","10.2760/57493","","",,,,,52,26.00,17,3,2,"","",""
4,"Tingting Wu, Yunwei Dong, Zhiwei Dong, Aziz Singa, Xiong Chen, Yu Zhang","Testing Artificial Intelligence System Towards Safety and Robustness: State of the Art",2020,"","","","",16,"2022-07-13 09:19:07","","","","",,,,,4,2.00,1,6,2,"With the increasing development of machine learning, conventional embedded systems cannot meet the requirement of current academic researches and industrial applications. Artificial Intelligence System (AIS) based on machine learning has been widely used in various safety-critical systems, such as machine vision, autonomous vehicles, collision avoidance system. Different from conventional embedded systems, AIS generates and updates control strategies through learning algorithms which make the control behaviors nondeterministic and bring about the test oracle problem in AIS testing procedure. There have been various testing approaches for AIS to guarantee the safety and robustness. However, few researches explain how to conduct AIS testing with a complete workflow systematically. This paper provides a comprehensive survey of existing testing techniques to detect the erroneous behaviors of AIS, and sums up the involved key steps and testing components in terms of test coverage criterion, test data generation, testing approach and common dataset. This literature review aims at organizing a standardized workflow and leading to a practicable insight and research trend towards AIS testing.","",""
12,"Nijat Mehdiyev, P. Fettke","Explainable Artificial Intelligence for Process Mining: A General Overview and Application of a Novel Local Explanation Approach for Predictive Process Monitoring",2020,"","","","",17,"2022-07-13 09:19:07","","10.1007/978-3-030-64949-4_1","","",,,,,12,6.00,6,2,2,"","",""
51,"Shubham Sharma, Jette Henderson, Joydeep Ghosh","CERTIFAI: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models",2019,"","","","",18,"2022-07-13 09:19:07","","10.1145/3375627.3375812","","",,,,,51,17.00,17,3,3,"As artificial intelligence plays an increasingly important role in our society, there are ethical and moral obligations for both businesses and researchers to ensure that their machine learning models are designed, deployed, and maintained responsibly. These models need to be rigorously audited for fairness, robustness, transparency, and interpretability. A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming. In this paper, we introduce a unified and model-agnostic approach to address these issues: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models (CERTIFAI). Unlike previous methods in this domain, CERTIFAI is a general tool that can be applied to any black-box model and any type of input data. Given a model and an input instance, CERTIFAI uses a custom genetic algorithm to generate counterfactuals: instances close to the input that change the prediction of the model. We demonstrate how these counterfactuals can be used to examine issues of robustness, interpretability, transparency, and fairness. Additionally, we introduce CERScore, the first black-box model robustness score that performs comparably to methods that have access to model internals.","",""
14,"A. Zaji, H. Bonakdari","Robustness lake water level prediction using the search heuristic-based artificial intelligence methods",2019,"","","","",19,"2022-07-13 09:19:07","","10.1080/09715010.2018.1424568","","",,,,,14,4.67,7,2,3,"Abstract Lakes have a crucial role in the industrial, agricultural, environment, and drinking water fields. Accurate prediction of lake levels is one of the most important parameters in the reservoir management and lakeshore structure designing. The goal of the present study is to examine the robustness of two different Genetic Algorithm-based regression methods namely the Genetic Algorithm Artificial neural network (GAA) and the Genetic Programming (GP) by considering their performance in predicting the non-observed lakes. To do that, data collected from the four-year daily measurements of the Chahnimeh#1 lake in Eastern Iran were used for developing the GAA and GP models and after that, the performance of the considered models are examined to predict the lake water levels of an adjacent lake namely Chahnimeh#4 as the non-observed information. The results showed that both model has the ability to simulate adjacent lakes using the considered lake water levels for the training procedure. In addition, another goal is to develop simple, practical formulation for predicting the lake water level, So that, using the GP method, as the superior model, three different formulations are proposed in order to predict the one, three, and five days ahead lake water level, respectively.","",""
755,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xisheng Fang, Shiqin Zhang, J. Xia, Jun Xia","Artificial Intelligence Distinguishes COVID-19 from Community Acquired Pneumonia on Chest CT",2020,"","","","",20,"2022-07-13 09:19:07","","10.1148/radiol.2020200905","","",,,,,755,377.50,76,18,2,"Background Coronavirus disease has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performances. Materials and Methods In this retrospective and multi-center study, a deep learning model, COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT exams for the detection of COVID-19. Community acquired pneumonia (CAP) and other non-pneumonia CT exams were included to test the robustness of the model. The datasets were collected from 6 hospitals between August 2016 and February 2020. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity and specificity. Results The collected dataset consisted of 4356 chest CT exams from 3,322 patients. The average age is 49±15 years and there were slightly more male patients than female (1838 vs 1484; p-value=0.29). The per-exam sensitivity and specificity for detecting COVID-19 in the independent test set was 114 of 127 (90% [95% CI: 83%, 94%]) and 294 of 307 (96% [95% CI: 93%, 98%]), respectively, with an AUC of 0.96 (p-value<0.001). The per-exam sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175) and 92% (239 of 259), respectively, with an AUC of 0.95 (95% CI: 0.93, 0.97). Conclusions A deep learning model can accurately detect COVID-19 and differentiate it from community acquired pneumonia and other lung diseases.","",""
47,"L. Faes, B. Geerts, Xiaoxuan Liu, L. Morgan, P. Watkinson, P. McCulloch","DECIDE-AI: new reporting guidelines to bridge the development-to-implementation gap in clinical artificial intelligence.",2021,"","","","",21,"2022-07-13 09:19:07","","10.1038/s41591-021-01229-5","","",,,,,47,47.00,8,6,1,"","",""
7,"Ismail Baaj, J. Poli, Wassila Ouerdane","Some Insights Towards a Unified Semantic Representation of Explanation for eXplainable Artificial Intelligence",2019,"","","","",22,"2022-07-13 09:19:07","","10.18653/v1/W19-8404","","",,,,,7,2.33,2,3,3,"Among challenges for eXplainable Artificial Intelligence (XAI) is explanation generation. In this paper we put the stress on this issue by focusing on a semantic representation of the content of an explanation that could be common to any kind of XAI. We investigate knowledge representations, and discuss the benefits of conceptual graph structures for being a basis to represent explanations in AI.","",""
19,"Ruhhee Tabbussum, A. Q. Dar","Performance evaluation of artificial intelligence paradigms—artificial neural networks, fuzzy logic, and adaptive neuro-fuzzy inference system for flood prediction",2021,"","","","",23,"2022-07-13 09:19:07","","10.1007/s11356-021-12410-1","","",,,,,19,19.00,10,2,1,"","",""
1,"D. Mefford","Steps Toward Artificial Intelligence: Rule-Based, Case-Based, and Explanation-Based Models of Politics",2019,"","","","",24,"2022-07-13 09:19:07","","10.4324/9780429033575-4","","",,,,,1,0.33,1,1,3,"","",""
14,"A. Rosenfeld","Better Metrics for Evaluating Explainable Artificial Intelligence",2021,"","","","",25,"2022-07-13 09:19:07","","10.5555/3463952.3463962","","",,,,,14,14.00,14,1,1,"This paper presents objective metrics for how explainable artificial intelligence (XAI) can be quantified. Through an overview of current trends, we show that many explanations are generated post-hoc and independent of the agent’s logical process, which in turn creates explanations with limited meaning as they lack transparency and fidelity. While user studies are a known basis for evaluating XAI, studies that do not consider objective metrics for evaluating XAI may have limited meaning and may suffer from confirmation bias, particularly if they use low fidelity explanations unnecessarily. To avoid this issue, this paper suggests a paradigm shift in evaluating XAI that focuses on metrics that quantify the explanation itself and its appropriateness given the XAI goal. We suggest four such metrics based on performance differences, D, between the explanation’s logic and the agent’s actual performance, the number of rules, R, outputted by the explanation, the number of features, F , used to generate that explanation, and the stability, S, of the explanation. We believe that user studies that focus on these metrics in their evaluations are inherently more valid and should be integrated in future XAI research.","",""
14,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor, Mohiuddin Ahmed","Explainable Artificial Intelligence Approaches: A Survey",2021,"","","","",26,"2022-07-13 09:19:07","","","","",,,,,14,14.00,4,4,1,"The lack of explainability of a decision from an Artificial Intelligence (AI) based “black box” system/model, despite its superiority in many real-world applications, is a key stumbling block for adopting AI in many high stakes applications of different domain or industry. While many popular Explainable Artificial Intelligence (XAI) methods or approaches are available to facilitate a human-friendly explanation of the decision, each has its own merits and demerits, with a plethora of open challenges. We demonstrate popular XAI methods with a mutual case study/task (i.e., credit default prediction), analyze for competitive advantages from multiple perspectives (e.g., local, global), provide meaningful insight on quantifying explainability, and recommend paths towards responsible or human-centered AI using XAI as a medium. Practitioners can use this work as a catalog to understand, compare, and correlate competitive advantages of popular XAI methods. In addition, this survey elicits future research directions towards responsible or human-centric AI systems, which is crucial to adopt AI in high stakes applications.","",""
822,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xi Fang, Shiqin Zhang, J. Xia, Jun Xia","Using Artificial Intelligence to Detect COVID-19 and Community-acquired Pneumonia Based on Pulmonary CT: Evaluation of the Diagnostic Accuracy",2020,"","","","",27,"2022-07-13 09:19:07","","10.1148/RADIOL.2020200905","","",,,,,822,411.00,82,18,2,"Background Coronavirus disease 2019 (COVID-19) has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performance. Materials and Methods In this retrospective and multicenter study, a deep learning model, the COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT scans for the detection of COVID-19. CT scans of community-acquired pneumonia (CAP) and other non-pneumonia abnormalities were included to test the robustness of the model. The datasets were collected from six hospitals between August 2016 and February 2020. Diagnostic performance was assessed with the area under the receiver operating characteristic curve, sensitivity, and specificity. Results The collected dataset consisted of 4352 chest CT scans from 3322 patients. The average patient age (±standard deviation) was 49 years ± 15, and there were slightly more men than women (1838 vs 1484, respectively; P = .29). The per-scan sensitivity and specificity for detecting COVID-19 in the independent test set was 90% (95% confidence interval [CI]: 83%, 94%; 114 of 127 scans) and 96% (95% CI: 93%, 98%; 294 of 307 scans), respectively, with an area under the receiver operating characteristic curve of 0.96 (P < .001). The per-scan sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175 scans) and 92% (239 of 259 scans), respectively, with an area under the receiver operating characteristic curve of 0.95 (95% CI: 0.93, 0.97). Conclusion A deep learning model can accurately detect coronavirus 2019 and differentiate it from community-acquired pneumonia and other lung conditions. © RSNA, 2020 Online supplemental material is available for this article.","",""
10,"Shun Zhang, Muye Li, Mengnan Jian, Yajun Zhao, Feifei Gao","AIRIS: Artificial intelligence enhanced signal processing in reconfigurable intelligent surface communications",2021,"","","","",28,"2022-07-13 09:19:07","","10.23919/JCC.2021.07.013","","",,,,,10,10.00,2,5,1,"Reconfigurable intelligent surface (RIS) is an emerging meta-surface that can provide additional communications links through reflecting the signals, and has been recognized as a strong candidate of 6G mobile communications systems. Meanwhile, it has been recently admitted that implementing artificial intelligence (AI) into RIS communications will extensively benefit the reconfiguration capacity and enhance the robustness to complicated transmission environments. Besides the conventional model-driven approaches, AI can also deal with the existing signal processing problems in a data-driven manner via digging the inherent characteristic from the real data. Hence, AI is particularly suitable for the signal processing problems over RIS networks under unideal scenarios like modeling mismatching, insufficient resource, hardware impairment, as well as dynamical transmissions. As one of the earliest survey papers, we will introduce the merging of AI and RIS, called AIRIS, over various signal processing topics, including environmental sensing, channel acquisition, beam-forming design, and resource scheduling, etc. We will also discuss the challenges of AIRIS and present some interesting future directions.","",""
10,"David A. Broniatowski","Psychological Foundations of Explainability and Interpretability in Artificial Intelligence",2021,"","","","",29,"2022-07-13 09:19:07","","10.6028/NIST.IR.8367","","",,,,,10,10.00,10,1,1,"In this paper, we make the case that interpretability and explainability are distinct requirements for machine learning systems. To make this case, we provide an overview of the literature in experimental psychology pertaining to interpretation (especially of numerical stimuli) and comprehension. We find that interpretation refers to the ability to contextualize a model’s output in a manner that relates it to the system’s designed functional purpose, and the goals, values, and preferences of end users. In contrast, explanation refers to the ability to accurately describe the mechanism, or implementation, that led to an algorithm’s output, often so that the algorithm can be improved in some way. Beyond these definitions, our review shows that humans differ from one another in systematic ways, that affect the extent to which they prefer to make decisions based on detailed explanations versus less precise interpretations. These individual differences, such as personality traits and skills, are associated with their abilities to derive meaningful interpretations from precise explanations of model output. This implies that system output should be tailored to different types of users.","",""
129,"Arun Das, P. Rad","Opportunities and Challenges in Explainable Artificial Intelligence (XAI): A Survey",2020,"","","","",30,"2022-07-13 09:19:07","","","","",,,,,129,64.50,65,2,2,"Nowadays, deep neural networks are widely used in mission critical systems such as healthcare, self-driving vehicles, and military which have direct impact on human lives. However, the black-box nature of deep neural networks challenges its use in mission critical applications, raising ethical and judicial concerns inducing lack of trust. Explainable Artificial Intelligence (XAI) is a field of Artificial Intelligence (AI) that promotes a set of tools, techniques, and algorithms that can generate high-quality interpretable, intuitive, human-understandable explanations of AI decisions. In addition to providing a holistic view of the current XAI landscape in deep learning, this paper provides mathematical summaries of seminal work. We start by proposing a taxonomy and categorizing the XAI techniques based on their scope of explanations, methodology behind the algorithms, and explanation level or usage which helps build trustworthy, interpretable, and self-explanatory deep learning models. We then describe the main principles used in XAI research and present the historical timeline for landmark studies in XAI from 2007 to 2020. After explaining each category of algorithms and approaches in detail, we then evaluate the explanation maps generated by eight XAI algorithms on image data, discuss the limitations of this approach, and provide potential future directions to improve XAI evaluation.","",""
90,"R. Shafin, Lingjia Liu, V. Chandrasekhar, Hao Chen, J. Reed, Jianzhong Zhang","Artificial Intelligence-Enabled Cellular Networks: A Critical Path to Beyond-5G and 6G",2019,"","","","",31,"2022-07-13 09:19:07","","10.1109/MWC.001.1900323","","",,,,,90,30.00,15,6,3,"Mobile network operators (MNOs) are in the process of overlaying their conventional macro cellular networks with shorter range cells such as outdoor pico cells. The resultant increase in network complexity creates substantial overhead in terms of operating expenses, time, and labor for their planning and management. Artificial intelligence (AI) offers the potential for MNOs to operate their networks in a more organic and cost-efficient manner. We argue that deploying AI in fifth generation (5G) and beyond will require surmounting significant technical barriers in terms of robustness, performance, and complexity. We outline future research directions, identify top five challenges, and present a possible roadmap to realize the vision of AI-enabled cellular networks for Beyond- 5G and sixth generation (6G) networks.","",""
79,"V. C. Müller","Ethics of artificial intelligence and robotics",2020,"","","","",32,"2022-07-13 09:19:07","","","","",,,,,79,39.50,79,1,2,"Artificial intelligence (AI) and robotics are digital technologies that will have significant impact on the development of humanity in the near future. They have raised fundamental questions about what we should do with these systems, what the systems themselves should do, what risks they involve, and how we can control these. - After the Introduction to the field (§1), the main themes (§2) of this article are: Ethical issues that arise with AI systems as objects, i.e., tools made and used by humans. This includes issues of privacy (§2.1) and manipulation (§2.2), opacity (§2.3) and bias (§2.4), human-robot interaction (§2.5), employment (§2.6), and the effects of autonomy (§2.7). Then AI systems as subjects, i.e., ethics for the AI systems themselves in machine ethics (§2.8) and artificial moral agency (§2.9). Finally, the problem of a possible future AI superintelligence leading to a “singularity” (§2.10). We close with a remark on the vision of AI (§3). - For each section within these themes, we provide a general explanation of the ethical issues, outline existing positions and arguments, then analyse how these play out with current technologies and finally, what policy consequences may be drawn.","",""
8,"Peikun Zheng, R. Zubatyuk, Wéi Wú, O. Isayev, Pavlo O. Dral","Artificial intelligence-enhanced quantum chemical method with broad applicability",2021,"","","","",33,"2022-07-13 09:19:07","","10.1038/s41467-021-27340-2","","",,,,,8,8.00,2,5,1,"","",""
353,"Erico Tjoa, Cuntai Guan","A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI",2019,"","","","",34,"2022-07-13 09:19:07","","10.1109/TNNLS.2020.3027314","","",,,,,353,117.67,177,2,3,"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.","",""
34,"T. H. Aldhyani, M. Al-Yaari, Hasan Alkahtani, Mashael S. Maashi","Water Quality Prediction Using Artificial Intelligence Algorithms",2020,"","","","",35,"2022-07-13 09:19:07","","10.1155/2020/6659314","","",,,,,34,17.00,9,4,2,"During the last years, water quality has been threatened by various pollutants. Therefore, modeling and predicting water quality have become very important in controlling water pollution. In this work, advanced artificial intelligence (AI) algorithms are developed to predict water quality index (WQI) and water quality classification (WQC). For the WQI prediction, artificial neural network models, namely nonlinear autoregressive neural network (NARNET) and long short-term memory (LSTM) deep learning algorithm, have been developed. In addition, three machine learning algorithms, namely, support vector machine (SVM), K-nearest neighbor (K-NN), and Naive Bayes, have been used for the WQC forecasting. The used dataset has 7 significant parameters, and the developed models were evaluated based on some statistical parameters. The results revealed that the proposed models can accurately predict WQI and classify the water quality according to superior robustness. Prediction results demonstrated that the NARNET model performed slightly better than the LSTM for the prediction of the WQI values and the SVM algorithm has achieved the highest accuracy (97.01%) for the WQC prediction. Furthermore, the NARNET and LSTM models have achieved similar accuracy for the testing phase with a slight difference in the regression coefficient (RNARNET = 96.17% and RLSTM = 94.21%). This kind of promising research can contribute significantly to water management.","",""
5,"Giulia Vilone, L. Longo","Classification of Explainable Artificial Intelligence Methods through Their Output Formats",2021,"","","","",36,"2022-07-13 09:19:07","","10.3390/make3030032","","",,,,,5,5.00,3,2,1,"Machine and deep learning have proven their utility to generate data-driven models with high accuracy and precision. However, their non-linear, complex structures are often difficult to interpret. Consequently, many scholars have developed a plethora of methods to explain their functioning and the logic of their inferences. This systematic review aimed to organise these methods into a hierarchical classification system that builds upon and extends existing taxonomies by adding a significant dimension—the output formats. The reviewed scientific papers were retrieved by conducting an initial search on Google Scholar with the keywords “explainable artificial intelligence”; “explainable machine learning”; and “interpretable machine learning”. A subsequent iterative search was carried out by checking the bibliography of these articles. The addition of the dimension of the explanation format makes the proposed classification system a practical tool for scholars, supporting them to select the most suitable type of explanation format for the problem at hand. Given the wide variety of challenges faced by researchers, the existing XAI methods provide several solutions to meet the requirements that differ considerably between the users, problems and application fields of artificial intelligence (AI). The task of identifying the most appropriate explanation can be daunting, thus the need for a classification system that helps with the selection of methods. This work concludes by critically identifying the limitations of the formats of explanations and by providing recommendations and possible future research directions on how to build a more generally applicable XAI method. Future work should be flexible enough to meet the many requirements posed by the widespread use of AI in several fields, and the new regulations.","",""
5,"Xiaochen Zhang, Dayu Yang","Research on Music Assisted Teaching System Based on Artificial Intelligence Technology",2021,"","","","",37,"2022-07-13 09:19:07","","10.1088/1742-6596/1852/2/022032","","",,,,,5,5.00,3,2,1,"With the advent of the information age, computer technology has been greatly developed, especially the development of Artificial Intelligence(AI). And with the passage of time, AI began to involve various fields, music education is no exception. In this paper, after a detailed understanding of some research results of AI on music assisted instruction system, we mainly analyze the students’ video, audio and other related information, and save it in the database. This paper first introduces the evaluation process by using AI technology. In fact, it is necessary to find out the relationship between the influencing factors and evaluation of music assisted teaching system. Neural network(NN) is actually a model proposed by simulating the way people think in the brain. It has no strict requirements for data distribution. In terms of nonlinear data processing method, robustness and dynamics, it is very suitable to be used as a model for evaluating music assisted instruction system. Then each factor is taken as the input parameter of the NN. According to the evaluation index of music teaching, a special modeling system is designed. With the help of technical personnel, we obtained the sample data of music performance and completed the neural training. The experimental results show that the development of AI technology has broken the original situation of traditional teaching, especially the application of music system and intelligent music software based on AI in music teaching.","",""
25,"P. Phillips, Carina A. Hahn, Peter C. Fontana, David A. Broniatowski, Mark A. Przybocki","Four Principles of Explainable Artificial Intelligence",2020,"","","","",38,"2022-07-13 09:19:07","","10.6028/nist.ir.8312-draft","","",,,,,25,12.50,5,5,2,"We introduce four principles for explainable artificial intelligence (AI) that comprise fundamental properties for explainable AI systems. We propose that explainable AI systems deliver accompanying evidence or reasons for outcomes and processes; provide explanations that are understandable to individual users; provide explanations that correctly reflect the system’s process for generating the output; and that a system only operates under conditions for which it was designed and when it reaches sufficient confidence in its output. We have termed these four principles as explanation, meaningful, explanation accuracy, and knowledge limits, respectively. Through significant stakeholder engagement, these four principles were developed to encompass the multidisciplinary nature of explainable AI, including the fields of computer science, engineering, and psychology. Because one-sizefits-all explanations do not exist, different users will require different types of explanations. We present five categories of explanation and summarize theories of explainable AI. We give an overview of the algorithms in the field that cover the major classes of explainable algorithms. As a baseline comparison, we assess how well explanations provided by people follow our four principles. This assessment provides insights to the challenges of designing explainable AI systems.","",""
42,"M. Nassar, K. Salah, M. H. Rehman, D. Svetinovic","Blockchain for explainable and trustworthy artificial intelligence",2019,"","","","",39,"2022-07-13 09:19:07","","10.1002/widm.1340","","",,,,,42,14.00,11,4,3,"The increasing computational power and proliferation of big data are now empowering Artificial Intelligence (AI) to achieve massive adoption and applicability in many fields. The lack of explanation when it comes to the decisions made by today's AI algorithms is a major drawback in critical decision‐making systems. For example, deep learning does not offer control or reasoning over its internal processes or outputs. More importantly, current black‐box AI implementations are subject to bias and adversarial attacks that may poison the learning or the inference processes. Explainable AI (XAI) is a new trend of AI algorithms that provide explanations of their AI decisions. In this paper, we propose a framework for achieving a more trustworthy and XAI by leveraging features of blockchain, smart contracts, trusted oracles, and decentralized storage. We specify a framework for complex AI systems in which the decision outcomes are reached based on decentralized consensuses of multiple AI and XAI predictors. The paper discusses how our proposed framework can be utilized in key application areas with practical use cases.","",""
7,"J. McDermid, Yan Jia, Zoe Porter, I. Habli","Artificial intelligence explainability: the technical and ethical dimensions",2021,"","","","",40,"2022-07-13 09:19:07","","10.1098/rsta.2020.0363","","",,,,,7,7.00,2,4,1,"In recent years, several new technical methods have been developed to make AI-models more transparent and interpretable. These techniques are often referred to collectively as ‘AI explainability’ or ‘XAI’ methods. This paper presents an overview of XAI methods, and links them to stakeholder purposes for seeking an explanation. Because the underlying stakeholder purposes are broadly ethical in nature, we see this analysis as a contribution towards bringing together the technical and ethical dimensions of XAI. We emphasize that use of XAI methods must be linked to explanations of human decisions made during the development life cycle. Situated within that wider accountability framework, our analysis may offer a helpful starting point for designers, safety engineers, service providers and regulators who need to make practical judgements about which XAI methods to employ or to require. This article is part of the theme issue ‘Towards symbiotic autonomous systems’.","",""
32,"D. Bates, A. Auerbach, Peter F. Schulam, A. Wright, S. Saria","Reporting and Implementing Interventions Involving Machine Learning and Artificial Intelligence",2020,"","","","",41,"2022-07-13 09:19:07","","10.7326/M19-0872","","",,,,,32,16.00,6,5,2,"Increasingly, interventions aimed at improving care are likely to use such technologies as machine learning and artificial intelligence. However, health care has been relatively late to adopt them. This article provides clinical examples in which machine learning and artificial intelligence are already in use in health care and appear to deliver benefit. Three key bottlenecks toward increasing the pace of diffusion and adoption are methodological issues in evaluation of artificial intelligence-based interventions, reporting standards to enable assessment of model performance, and issues that need to be addressed for an institution to adopt these interventions. Methodological best practices will include external validation, ideally at a different site; use of proactive learning algorithms to correct for site-specific biases and increase robustness as algorithms are deployed across multiple sites; addressing subgroup performance; and communicating to providers the uncertainty of predictions. Regarding reporting, especially important issues are the extent to which implementing standardized approaches for introducing clinical decision support has been followed, describing the data sources, reporting on data assumptions, and addressing biases. Although most health care organizations in the United States have adopted electronic health records, they may be ill prepared to adopt machine learning and artificial intelligence. Several steps can enable this: preparing data, developing tools to get suggestions to clinicians in useful ways, and getting clinicians engaged in the process. Open challenges and the role of regulation in this area are briefly discussed. Although these techniques have enormous potential to improve care and personalize recommendations for individuals, the hype regarding them is tremendous. Organizations will need to approach this domain carefully with knowledgeable partners to obtain the hoped-for benefits and avoid failures.","",""
2,"Iam Palatnik de Sousa, Marley Vellasco, E. C. Silva","Explainable Artificial Intelligence for Bias Detection in COVID CT-Scan Classifiers",2021,"","","","",42,"2022-07-13 09:19:07","","10.3390/s21165657","","",,,,,2,2.00,1,3,1,"Problem: An application of Explainable Artificial Intelligence Methods for COVID CT-Scan classifiers is presented. Motivation: It is possible that classifiers are using spurious artifacts in dataset images to achieve high performances, and such explainable techniques can help identify this issue. Aim: For this purpose, several approaches were used in tandem, in order to create a complete overview of the classificatios. Methodology: The techniques used included GradCAM, LIME, RISE, Squaregrid, and direct Gradient approaches (Vanilla, Smooth, Integrated). Main results: Among the deep neural networks architectures evaluated for this image classification task, VGG16 was shown to be most affected by biases towards spurious artifacts, while DenseNet was notably more robust against them. Further impacts: Results further show that small differences in validation accuracies can cause drastic changes in explanation heatmaps for DenseNet architectures, indicating that small changes in validation accuracy may have large impacts on the biases learned by the networks. Notably, it is important to notice that the strong performance metrics achieved by all these networks (Accuracy, F1 score, AUC all in the 80 to 90% range) could give users the erroneous impression that there is no bias. However, the analysis of the explanation heatmaps highlights the bias.","",""
1,"Husain Alansari, Oksana Gerwe, A. Razzaque","Role of Artificial Intelligence During the Covid-19 Era",2021,"","","","",43,"2022-07-13 09:19:07","","10.1007/978-3-030-73057-4_13","","",,,,,1,1.00,0,3,1,"","",""
0,"Jie Wang, Xiangyuan Zheng, Qingdong He","Artificial Intelligence Applied to Extreme Value Prediction of Non-Gaussian Processes with Bandwidth Effect and Non-monotonicity",2021,"","","","",44,"2022-07-13 09:19:07","","10.1109/ICAICA52286.2021.9498204","","",,,,,0,0.00,0,3,1,"Extreme value prediction of a short-term non-Gaussian random process like ocean waves has been a tough issue for decades. In the 1990’s Winterstein proposed a cubic Hermite transformation using skewness and kurtosis, which has been widely applied in many areas for its accuracy and robustness. However, this approach is valid for monotonic transformation and narrow-banded processes. When the bandwidth of a random process is wide, no reasonable methods are available for acquiring the extreme value. This paper therefore applies the artificial neural network and genetic algorithm to do the extreme value prediction, without seeking rigorous mathematical derivations. Not only skewness and kurtosis are used, the spectral moments up to 4th-order reflecting bandwidth effects are also adopted. The results of many random case studies show that the artificial intelligence method is more accurate than the Hermite method in most of situations, especially for non-monotonic transformations. Besides, the artificial intelligence method has a wider application range.","",""
30,"Xinqin Liao, Wei‐ming Song, X. Zhang, Chaoqun Yan, Tianliang Li, Hongliang Ren, Cunzhi Liu, Yongtian Wang, Yuanjin Zheng","A bioinspired analogous nerve towards artificial intelligence",2020,"","","","",45,"2022-07-13 09:19:07","","10.1038/s41467-019-14214-x","","",,,,,30,15.00,3,9,2,"","",""
1,"Michael Tsang, James Enouen, Yan Liu","Interpretable Artificial Intelligence through the Lens of Feature Interaction",2021,"","","","",46,"2022-07-13 09:19:07","","","","",,,,,1,1.00,0,3,1,"Deep learning, alongside other modern machine learning techniques, has become the state of the art solution for a diverse range of real-world tasks. These include a variety of sensitive applications such as healthcare, finance, autonomous driving, criminal justice, and others which all pose significant concerns for fairness, robustness, safety, and trustworthiness. Despite these applications to critical tasks, deep networks are infamously referred to as black-box models because of their total lack of transparency in decision-making. If we are able to gain insight into how a model is coming to its conclusions, we are able to more clearly assess the trustworthiness and validity of its decisions. Consequently, an abundance of ongoing research is attempting to address model interpretability as the key problem to resolving these issues. There are many methods which are currently used to provide explanations of complex model predictions. LIME (Ribeiro et al., 2016) fits a local linear model around a data point, showing which features positively and negatively influence the prediction results. Despite the overall model being nonlinear, the local model gets an interpretable picture of how the model looks at small scales around the data point. Extensions of this method use other interpretable models like small decision tress. Shapley Values and SHAP follows a similar idea to assign a score to each feature, using a gametheoretic formulation which treats each feature as a player causing the final prediction (Lundberg and Lee, 2017). Its more rigorous formulation yields guarantees of its explanations summing up to the prediction score, but practically it usually must be estimated because of its high computational cost. Shuffle-based feature importance permutes the data of each feature to ascertain its importance in the final prediction in comparison to its normal prediction (Fisher et al., 2018). IG uses the fundamental theorem of calculus to provide additive explanations of a prediction (Sundararajan et al., 2017). This method is very popular in computer vision where its computational efficiency and saliency are prized, even though other work has exposed some of its shortcomings in providing an interpretation (Adebayo et al., 2018). Other methods are specifically designed for computer vision like TCAV (Kim et al., 2018) which finds a ‘concept direction’ corresponding to a large sample of concept images from the user. Surprisingly, all of these most popular interpretability methods share the same one limitation. None of these methods consider the shared importance of groups of two or more features; they only look at the effects had by each of the features individually. A feature interaction between two variables broadly describes a situation where both of the features/ variables are simultaneously important for a model’s prediction. In text applications for sentiment, ”not good” is a very simple example of two words strongly interacting with one another to create a negative sentiment. In modern-day applications, neural networks are usually hailed as amazing function approximators exactly because of their incredible ability to automatically uncover these kinds of complex relationships between the variables of the dataset. In many ways, however,","",""
21,"B. Stoel","Use of artificial intelligence in imaging in rheumatology – current status and future perspectives",2020,"","","","",47,"2022-07-13 09:19:07","","10.1136/rmdopen-2019-001063","","",,,,,21,10.50,21,1,2,"After decades of basic research with many setbacks, artificial intelligence (AI) has recently obtained significant breakthroughs, enabling computer programs to outperform human interpretation of medical images in very specific areas. After this shock wave that probably exceeds the impact of the first AI victory of defeating the world chess champion in 1997, some reflection may be appropriate on the consequences for clinical imaging in rheumatology. In this narrative review, a short explanation is given about the various AI techniques, including ‘deep learning’, and how these have been applied to rheumatological imaging, focussing on rheumatoid arthritis and systemic sclerosis as examples. By discussing the principle limitations of AI and deep learning, this review aims to give insight into possible future perspectives of AI applications in rheumatology.","",""
21,"Chuan Zhang, Yeong-Luh Ueng, Christoph Studer, A. Burg","Artificial Intelligence for 5G and Beyond 5G: Implementations, Algorithms, and Optimizations",2020,"","","","",48,"2022-07-13 09:19:07","","10.1109/JETCAS.2020.3000103","","",,,,,21,10.50,5,4,2,"The communication industry is rapidly advancing towards 5G and beyond 5G (B5G) wireless technologies in order to fulfill the ever-growing needs for higher data rates and improved quality-of-service (QoS). Emerging applications require wireless connectivity with tremendously increased data rates, substantially reduced latency, and growing support for a large number of devices. These requirements pose new challenges that can no longer be efficiently addressed by conventional approaches. Artificial intelligence (AI) is considered as one of the most promising solutions to improve the performance and robustness of 5G and B5G systems, fueled by the massive amount of data generated in 5G and B5G networks and the availability of powerful data processing fabrics. As a consequence, a plethora of research on AI-based communication technologies has emerged recently, promising higher data rates and improved QoS with affordable implementation overhead. In this overview paper, we summarize the state-of-the-art of AI-based 5G and B5G techniques on the algorithm, implementation, and optimization levels. We shed light on the advantages and limitations of AI-based solutions, and we provide a summary of emerging techniques and open research problems.","",""
0,"Joe Hays, S. Ramamoorthy, Christian Tetzlaff","Editorial: Robust Artificial Intelligence for Neurorobotics",2021,"","","","",49,"2022-07-13 09:19:07","","10.3389/fnbot.2021.809903","","",,,,,0,0.00,0,3,1,"Neural computing is a powerful paradigm that has revolutionized machine learning. Building from early roots in the study of adaptive behavior and attempts to understand information processing in parallel and distributed neural architectures, modern neural networks have convincingly demonstrated successes in numerous areas—transforming the practice of computer vision, natural language processing, and even computational biology. Applications in robotics bring stringent constraints on size, weight and power constraints (SWaP), which challenge the developers of these technologies in new ways. Indeed, these requirements take us back to the roots of the field of neural computing, forcing us to ask how it could be that the human brain achieves with as little as 12 watts of power what seems to require entire server farms with state of the art computational and numerical methods. Likewise, even lowly insects demonstrate a degree of adaptivity and resilience that still defy easy explanation or computational replication. In this Research Topic, we have compiled the latest research addressing several aspects of these broadly defined challenge questions. As illustrated in Figure 1, the articles are organized into four prevailing themes: Sense, Think, Act, and Tools.","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",50,"2022-07-13 09:19:07","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
14,"Gaolei Li, K. Ota, M. Dong, Jun Wu, Jianhua Li","DeSVig: Decentralized Swift Vigilance Against Adversarial Attacks in Industrial Artificial Intelligence Systems",2020,"","","","",51,"2022-07-13 09:19:07","","10.1109/TII.2019.2951766","","",,,,,14,7.00,3,5,2,"Individually reinforcing the robustness of a single deep learning model only gives limited security guarantees especially when facing adversarial examples. In this article, we propose DeSVig, a decentralized swift vigilance framework to identify adversarial attacks in an industrial artificial intelligence systems (IAISs), which enables IAISs to correct the mistake in a few seconds. The DeSVig is highly decentralized, which improves the effectiveness of recognizing abnormal inputs. We try to overcome the challenges on ultralow latency caused by dynamics in industries using peculiarly designated mobile edge computing and generative adversarial networks. The most important advantage of our work is that it can significantly reduce the failure risks of being deceived by adversarial examples, which is critical for safety-prioritized and delay-sensitive environments. In our experiments, adversarial examples of industrial electronic components are generated by several classical attacking models. Experimental results demonstrate that the DeSVig is more robust, efficient, and scalable than some state-of-art defenses.","",""
13,"Yuanbin Wang, P. Zheng, Tao Peng, Huayong Yang, J. Zou","Smart additive manufacturing: Current artificial intelligence-enabled methods and future perspectives",2020,"","","","",52,"2022-07-13 09:19:07","","10.1007/s11431-020-1581-2","","",,,,,13,6.50,3,5,2,"","",""
11,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor","Towards Quantification of Explainability in Explainable Artificial Intelligence Methods",2019,"","","","",53,"2022-07-13 09:19:07","","","","",,,,,11,3.67,4,3,3,"Artificial Intelligence (AI) has become an integral part of domains such as security, finance, healthcare, medicine, and criminal justice. Explaining the decisions of AI systems in human terms is a key challenge--due to the high complexity of the model, as well as the potential implications on human interests, rights, and lives . While Explainable AI is an emerging field of research, there is no consensus on the definition, quantification, and formalization of explainability. In fact, the quantification of explainability is an open challenge. In our previous work, we incorporated domain knowledge for better explainability, however, we were unable to quantify the extent of explainability. In this work, we (1) briefly analyze the definitions of explainability from the perspective of different disciplines (e.g., psychology, social science), properties of explanation, explanation methods, and human-friendly explanations; and (2) propose and formulate an approach to quantify the extent of explainability. Our experimental result suggests a reasonable and model-agnostic way to quantify explainability","",""
9,"M. Gorris, S. Hoogenboom, M. Wallace, J. V. van Hooft","Artificial intelligence for the management of pancreatic diseases",2020,"","","","",54,"2022-07-13 09:19:07","","10.1111/den.13875","","",,,,,9,4.50,2,4,2,"Novel artificial intelligence techniques are emerging in all fields of healthcare, including gastroenterology. The aim of this review is to give an overview of artificial intelligence applications in the management of pancreatic diseases. We performed a systematic literature search in PubMed and Medline up to May 2020 to identify relevant articles. Our results showed that the development of machine‐learning based applications is rapidly evolving in the management of pancreatic diseases, guiding precision medicine in clinical, endoscopic and radiologic settings. Before implementation into clinical practice, further research should focus on the external validation of novel techniques, clarifying the accuracy and robustness of these models.","",""
7,"R. Y. Goh, L. Lee, H. Seow, Kathiresan Gopal","Hybrid Harmony Search–Artificial Intelligence Models in Credit Scoring",2020,"","","","",55,"2022-07-13 09:19:07","","10.3390/e22090989","","",,,,,7,3.50,2,4,2,"Credit scoring is an important tool used by financial institutions to correctly identify defaulters and non-defaulters. Support Vector Machines (SVM) and Random Forest (RF) are the Artificial Intelligence techniques that have been attracting interest due to their flexibility to account for various data patterns. Both are black-box models which are sensitive to hyperparameter settings. Feature selection can be performed on SVM to enable explanation with the reduced features, whereas feature importance computed by RF can be used for model explanation. The benefits of accuracy and interpretation allow for significant improvement in the area of credit risk and credit scoring. This paper proposes the use of Harmony Search (HS), to form a hybrid HS-SVM to perform feature selection and hyperparameter tuning simultaneously, and a hybrid HS-RF to tune the hyperparameters. A Modified HS (MHS) is also proposed with the main objective to achieve comparable results as the standard HS with a shorter computational time. MHS consists of four main modifications in the standard HS: (i) Elitism selection during memory consideration instead of random selection, (ii) dynamic exploration and exploitation operators in place of the original static operators, (iii) a self-adjusted bandwidth operator, and (iv) inclusion of additional termination criteria to reach faster convergence. Along with parallel computing, MHS effectively reduces the computational time of the proposed hybrid models. The proposed hybrid models are compared with standard statistical models across three different datasets commonly used in credit scoring studies. The computational results show that MHS-RF is most robust in terms of model performance, model explainability and computational time.","",""
6,"S. Hepenstal, David McNeish","Explainable Artificial Intelligence: What Do You Need to Know?",2020,"","","","",56,"2022-07-13 09:19:07","","10.1007/978-3-030-50353-6_20","","",,,,,6,3.00,3,2,2,"","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",57,"2022-07-13 09:19:07","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
423,"Andreas Holzinger, G. Langs, H. Denk, K. Zatloukal, Heimo Müller","Causability and explainability of artificial intelligence in medicine",2019,"","","","",58,"2022-07-13 09:19:07","","10.1002/widm.1312","","",,,,,423,141.00,85,5,3,"Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system","",""
54,"G. Collins, P. Dhiman, Constanza L. Andaur Navarro, Jie Ma, L. Hooft, J. Reitsma, P. Logullo, Andrew Beam, Lily Peng, B. van Calster, M. van Smeden, R. Riley, K. Moons","Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence",2021,"","","","",59,"2022-07-13 09:19:07","","10.1136/bmjopen-2020-048008","","",,,,,54,54.00,5,13,1,"Introduction The Transparent Reporting of a multivariable prediction model of Individual Prognosis Or Diagnosis (TRIPOD) statement and the Prediction model Risk Of Bias ASsessment Tool (PROBAST) were both published to improve the reporting and critical appraisal of prediction model studies for diagnosis and prognosis. This paper describes the processes and methods that will be used to develop an extension to the TRIPOD statement (TRIPOD-artificial intelligence, AI) and the PROBAST (PROBAST-AI) tool for prediction model studies that applied machine learning techniques. Methods and analysis TRIPOD-AI and PROBAST-AI will be developed following published guidance from the EQUATOR Network, and will comprise five stages. Stage 1 will comprise two systematic reviews (across all medical fields and specifically in oncology) to examine the quality of reporting in published machine-learning-based prediction model studies. In stage 2, we will consult a diverse group of key stakeholders using a Delphi process to identify items to be considered for inclusion in TRIPOD-AI and PROBAST-AI. Stage 3 will be virtual consensus meetings to consolidate and prioritise key items to be included in TRIPOD-AI and PROBAST-AI. Stage 4 will involve developing the TRIPOD-AI checklist and the PROBAST-AI tool, and writing the accompanying explanation and elaboration papers. In the final stage, stage 5, we will disseminate TRIPOD-AI and PROBAST-AI via journals, conferences, blogs, websites (including TRIPOD, PROBAST and EQUATOR Network) and social media. TRIPOD-AI will provide researchers working on prediction model studies based on machine learning with a reporting guideline that can help them report key details that readers need to evaluate the study quality and interpret its findings, potentially reducing research waste. We anticipate PROBAST-AI will help researchers, clinicians, systematic reviewers and policymakers critically appraise the design, conduct and analysis of machine learning based prediction model studies, with a robust standardised tool for bias evaluation. Ethics and dissemination Ethical approval has been granted by the Central University Research Ethics Committee, University of Oxford on 10-December-2020 (R73034/RE001). Findings from this study will be disseminated through peer-review publications. PROSPERO registration number CRD42019140361 and CRD42019161764.","",""
46,"M. Ryan, B. Stahl","Artificial intelligence ethics guidelines for developers and users: clarifying their content and normative implications",2020,"","","","",60,"2022-07-13 09:19:07","","10.1108/JICES-12-2019-0138","","",,,,,46,23.00,23,2,2,"The purpose of this paper is clearly illustrate this convergence and the prescriptive recommendations that such documents entail. There is a significant amount of research into the ethical consequences of artificial intelligence (AI). This is reflected by many outputs across academia, policy and the media. Many of these outputs aim to provide guidance to particular stakeholder groups. It has recently been shown that there is a large degree of convergence in terms of the principles upon which these guidance documents are based. Despite this convergence, it is not always clear how these principles are to be translated into practice.,In this paper, the authors move beyond the high-level ethical principles that are common across the AI ethics guidance literature and provide a description of the normative content that is covered by these principles. The outcome is a comprehensive compilation of normative requirements arising from existing guidance documents. This is not only required for a deeper theoretical understanding of AI ethics discussions but also for the creation of practical and implementable guidance for developers and users of AI.,In this paper, the authors therefore provide a detailed explanation of the normative implications of existing AI ethics guidelines but directed towards developers and organisational users of AI. The authors believe that the paper provides the most comprehensive account of ethical requirements in AI currently available, which is of interest not only to the research and policy communities engaged in the topic but also to the user communities that require guidance when developing or deploying AI systems.,The authors believe that they have managed to compile the most comprehensive document collecting existing guidance which can guide practical action but will hopefully also support the consolidation of the guidelines landscape. The authors’ findings should also be of academic interest and inspire philosophical research on the consistency and justification of the various normative statements that can be found in the literature.","",""
39,"R. Confalonieri, Ludovik Çoba, Benedikt Wagner, Tarek R. Besold","A historical perspective of explainable Artificial Intelligence",2020,"","","","",61,"2022-07-13 09:19:07","","10.1002/widm.1391","","",,,,,39,19.50,10,4,2,"Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision‐making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge‐based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural‐symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human‐understandable explainable systems.","",""
10,"N. Rodríguez, G. Pisoni","Accessible Cultural Heritage through Explainable Artificial Intelligence",2020,"","","","",62,"2022-07-13 09:19:07","","10.1145/3386392.3399276","","",,,,,10,5.00,5,2,2,"Ethics Guidelines for Trustworthy AI advocate for AI technology that is, among other things, more inclusive. Explainable AI (XAI) aims at making state of the art opaque models more transparent, and defends AI-based outcomes endorsed with a rationale explanation, i.e., an explanation that has as target the non-technical users. XAI and Responsible AI principles defend the fact that the audience expertise should be included in the evaluation of explainable AI systems. However, AI has not yet reached all public and audiences, some of which may need it the most. One example of domain where accessibility has not much been influenced by the latest AI advances is cultural heritage. We propose including minorities as special user and evaluator of the latest XAI techniques. In order to define catalytic scenarios for collaboration and improved user experience, we pose some challenges and research questions yet to address by the latest AI models likely to be involved in such synergy.","",""
21,"Adrien Bécue, Isabel Praça, J. Gama","Artificial intelligence, cyber-threats and Industry 4.0: challenges and opportunities",2021,"","","","",63,"2022-07-13 09:19:07","","10.1007/S10462-020-09942-2","","",,,,,21,21.00,7,3,1,"","",""
16,"A. Amritphale, Ranojoy Chatterjee, Suvo Chatterjee, N. Amritphale, Ali Rahnavard, G. Awan, B. Omar, G. Fonarow","Predictors of 30-Day Unplanned Readmission After Carotid Artery Stenting Using Artificial Intelligence",2021,"","","","",64,"2022-07-13 09:19:07","","10.1007/s12325-021-01709-7","","",,,,,16,16.00,2,8,1,"","",""
19,"V. Sounderajah, H. Ashrafian, R. Golub, S. Shetty, Jeffrey De Fauw, L. Hooft, K. Moons, G. Collins, D. Moher, P. Bossuyt, A. Darzi, A. Karthikesalingam, A. Denniston, B. Mateen, D. Ting, D. Treanor, Dominic King, F. Greaves, Jonathan Godwin, J. Pearson-Stuttard, L. Harling, M. McInnes, Nader Rifai, Nenad Tomašev, P. Normahani, P. Whiting, R. Aggarwal, S. Vollmer, S. Markar, T. Panch, Xiaoxuan Liu","Developing a reporting guideline for artificial intelligence-centred diagnostic test accuracy studies: the STARD-AI protocol",2021,"","","","",65,"2022-07-13 09:19:07","","10.1136/bmjopen-2020-047709","","",,,,,19,19.00,2,31,1,"Introduction Standards for Reporting of Diagnostic Accuracy Study (STARD) was developed to improve the completeness and transparency of reporting in studies investigating diagnostic test accuracy. However, its current form, STARD 2015 does not address the issues and challenges raised by artificial intelligence (AI)-centred interventions. As such, we propose an AI-specific version of the STARD checklist (STARD-AI), which focuses on the reporting of AI diagnostic test accuracy studies. This paper describes the methods that will be used to develop STARD-AI. Methods and analysis The development of the STARD-AI checklist can be distilled into six stages. (1) A project organisation phase has been undertaken, during which a Project Team and a Steering Committee were established; (2) An item generation process has been completed following a literature review, a patient and public involvement and engagement exercise and an online scoping survey of international experts; (3) A three-round modified Delphi consensus methodology is underway, which will culminate in a teleconference consensus meeting of experts; (4) Thereafter, the Project Team will draft the initial STARD-AI checklist and the accompanying documents; (5) A piloting phase among expert users will be undertaken to identify items which are either unclear or missing. This process, consisting of surveys and semistructured interviews, will contribute towards the explanation and elaboration document and (6) On finalisation of the manuscripts, the group’s efforts turn towards an organised dissemination and implementation strategy to maximise end-user adoption. Ethics and dissemination Ethical approval has been granted by the Joint Research Compliance Office at Imperial College London (reference number: 19IC5679). A dissemination strategy will be aimed towards five groups of stakeholders: (1) academia, (2) policy, (3) guidelines and regulation, (4) industry and (5) public and non-specific stakeholders. We anticipate that dissemination will take place in Q3 of 2021.","",""
15,"S. Ebrahimian, Fatemeh Homayounieh, M. Rockenbach, Preetham Putha, T. Raj, I. Dayan, B. Bizzo, Varun Buch, Dufan Wu, Kyungsang Kim, Quanzheng Li, S. Digumarthy, M. Kalra","Artificial intelligence matches subjective severity assessment of pneumonia for prediction of patient outcome and need for mechanical ventilation: a cohort study",2021,"","","","",66,"2022-07-13 09:19:07","","10.1038/s41598-020-79470-0","","",,,,,15,15.00,2,13,1,"","",""
0,"P. Wu, Yu Zhao, Jianjun Wu, J. Ge, Ling Li, Jiaying Lu, I. Alberts, I. Yakushev, Qian Xu, Yi-Min Sun, Feng-tao Liu, M. Brendel, G. Höglinger, C. Bassetti, Y. Guan, W. Oertel, Wolfgang Weber, A. Rominger, Jian Wang, C. Zuo, Kuangyu Shi","Differential Diagnosis of Parkinsonism with Deep Metabolic Imaging Biomarker – An Artificial Intelligence-Aided Multi-Cohort FDG PET Study",2020,"","","","",67,"2022-07-13 09:19:07","","10.2139/ssrn.3675448","","",,,,,0,0.00,0,21,2,"Background:  The clinical presentation of early idiopathic Parkinson’s disease (IPD) substantially overlaps with that of atypical parkinsonian syndromes such as multiple system atrophy (MSA) and progressive supranuclear palsy (PSP). 18F-fluorodeoxyglucose (FDG) positron emission tomography (PET) has proven useful in assisting in the differential diagnosis of these parkinsonian syndromes. The aim of this study was to develop a metabolic imaging biomarker based on deep learning to support the differential diagnosis of parkinsonism.     Methods:  In addition to 863 non-parkinsonian subjects, a total of 1275 parkinsonian patients were accessed who underwent FDG PET imaging, clinical evaluation, and follow-up, yielding clinically possible, definite, or confirmative diagnoses of IPD, MSA, and PSP. A 3D deep convolutional neural network was developed to extract a deep metabolic imaging (DMI) biomarker. The accuracy of the DMI biomarker was evaluated with a cross-validation and a blind test. The robustness of the DMI biomarker was assessed on patients with longitudinal follow-up. The functional basis of DMI biomarker was further investigated based on the saliency map of the obtained deep neural network.     Findings:  The proposed DMI biomarker achieved an area under the receiver operating characteristic curve of 0·986 for the differential diagnosis of IPD, 0·997 for MSA, and 0·982 for PSP in the cross-validation. In the blind test, the DMI biomarker resulted in sensitivities of 98·1%, 88·5%, and 84·5%, as well as specificities of 90·0%, 99·2%, and 97·8% for IPD, MSA, and PSP respectively. The integration of demographic information and clinical assessments to DMI biomarker resulted in slight improvements. Saliencies for the DMI biomarker were found in several parkinsonism-related regions such as midbrain, putamen, and cerebellum.     Interpretation:  The DMI biomarker based on deep learning shows potential to provide accurate differential diagnosis for parkinsonism on FDG PET. The functional regions behind the DMI biomarker are consistent with known parkinsonian pathology.    Funding Statement: The work was supported by grants from the National Natural Science Foundation of China (No. 81771483, 81671239, 81361120393, 81401135, 81971641, 81902282, 91949118, 81771372), from the Ministry of Science and Technology of China (2016YFC1306504), Shanghai Municipal Science and Technology Major Project (No. 2017SHZDZX01, 2018SHZDZX03) and ZJ Lab, Shanghai Sailing Program (No. 18YF1403100). It was also supported by Swiss National Science Foundation (No. 188350) and Siemens Healthineers.    Declaration of Interests: W.H.O is Hertie Senior Research Professor, supported by the Charitable Hertie Foundation, Frankfurt/Main, Germany. A.R. and K.S. received research support from Siemens Healthineer. Other authors report no financial interests or potential conflicts of interest regarding the subject matter of the manuscript.    Ethics Approval Statement: All procedures performed in accordance with the ethical standards of the institutional and/or national research committee and with the 1964 Helsinki declaration and its later amendments or comparable ethical standards. All the data were from Parkinson's Disease Database and Samples Bank in Huashan Hospital. Ethics permission was obtained from the Institutional Review Board of Huashan Hospital and written consent was obtained from each subject after detailed explanation of the procedures.","",""
195,"A. London","Artificial Intelligence and Black-Box Medical Decisions: Accuracy versus Explainability.",2019,"","","","",68,"2022-07-13 09:19:07","","10.1002/hast.973","","",,,,,195,65.00,195,1,3,"Although decision-making algorithms are not new to medicine, the availability of vast stores of medical data, gains in computing power, and breakthroughs in machine learning are accelerating the pace of their development, expanding the range of questions they can address, and increasing their predictive power. In many cases, however, the most powerful machine learning techniques purchase diagnostic or predictive accuracy at the expense of our ability to access ""the knowledge within the machine."" Without an explanation in terms of reasons or a rationale for particular decisions in individual cases, some commentators regard ceding medical decision-making to black box systems as contravening the profound moral responsibilities of clinicians. I argue, however, that opaque decisions are more common in medicine than critics realize. Moreover, as Aristotle noted over two millennia ago, when our knowledge of causal systems is incomplete and precarious-as it often is in medicine-the ability to explain how results are produced can be less important than the ability to produce such results and empirically verify their accuracy.","",""
183,"David Gunning, D. Aha","DARPA’s Explainable Artificial Intelligence (XAI) Program",2019,"","","","",69,"2022-07-13 09:19:07","","10.1609/AIMAG.V40I2.2850","","",,,,,183,61.00,92,2,3,"Dramatic success in machine learning has led to a new wave of AI applications (for example, transportation, security, medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. The XAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance.","",""
109,"Shilin Qiu, Qihe Liu, Shijie Zhou, Chunjiang Wu","Review of Artificial Intelligence Adversarial Attack and Defense Technologies",2019,"","","","",70,"2022-07-13 09:19:07","","10.3390/APP9050909","","",,,,,109,36.33,27,4,3,"In recent years, artificial intelligence technologies have been widely used in computer vision, natural language processing, automatic driving, and other fields. However, artificial intelligence systems are vulnerable to adversarial attacks, which limit the applications of artificial intelligence (AI) technologies in key security fields. Therefore, improving the robustness of AI systems against adversarial attacks has played an increasingly important role in the further development of AI. This paper aims to comprehensively summarize the latest research progress on adversarial attack and defense technologies in deep learning. According to the target model’s different stages where the adversarial attack occurred, this paper expounds the adversarial attack methods in the training stage and testing stage respectively. Then, we sort out the applications of adversarial attack technologies in computer vision, natural language processing, cyberspace security, and the physical world. Finally, we describe the existing adversarial defense methods respectively in three main categories, i.e., modifying data, modifying models and using auxiliary tools.","",""
17,"Yi-Shan Lin, Wen-Chuan Lee, Z. B. Celik","What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors",2020,"","","","",71,"2022-07-13 09:19:07","","10.1145/3447548.3467213","","",,,,,17,8.50,6,3,2,"EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.","",""
17,"R. Dazeley, P. Vamplew, C. Foale, Charlotte Young, Sunil Aryal, F. Cruz","Levels of explainable artificial intelligence for human-aligned conversational explanations",2021,"","","","",72,"2022-07-13 09:19:07","","10.1016/j.artint.2021.103525","","",,,,,17,17.00,3,6,1,"","",""
15,"J. Janet, Chenru Duan, A. Nandy, Fang Liu, H. Kulik","Navigating Transition-Metal Chemical Space: Artificial Intelligence for First-Principles Design.",2021,"","","","",73,"2022-07-13 09:19:07","","10.1021/acs.accounts.0c00686","","",,,,,15,15.00,3,5,1,"ConspectusThe variability of chemical bonding in open-shell transition-metal complexes not only motivates their study as functional materials and catalysts but also challenges conventional computational modeling tools. Here, tailoring ligand chemistry can alter preferred spin or oxidation states as well as electronic structure properties and reactivity, creating vast regions of chemical space to explore when designing new materials atom by atom. Although first-principles density functional theory (DFT) remains the workhorse of computational chemistry in mechanism deduction and property prediction, it is of limited use here. DFT is both far too computationally costly for widespread exploration of transition-metal chemical space and also prone to inaccuracies that limit its predictive performance for localized d electrons in transition-metal complexes. These challenges starkly contrast with the well-trodden regions of small-organic-molecule chemical space, where the analytical forms of molecular mechanics force fields and semiempirical theories have for decades accelerated the discovery of new molecules, accurate DFT functional performance has been demonstrated, and gold-standard methods from correlated wavefunction theory can predict experimental results to chemical accuracy.The combined promise of transition-metal chemical space exploration and lack of established tools has mandated a distinct approach. In this Account, we outline the path we charted in exploration of transition-metal chemical space starting from the first machine learning (ML) models (i.e., artificial neural network and kernel ridge regression) and representations for the prediction of open-shell transition-metal complex properties. The distinct importance of the immediate coordination environment of the metal center as well as the lack of low-level methods to accurately predict structural properties in this coordination environment first motivated and then benefited from these ML models and representations. Once developed, the recipe for prediction of geometric, spin state, and redox potential properties was straightforwardly extended to a diverse range of other properties, including in catalysis, computational ""feasibility"", and the gas separation properties of periodic metal-organic frameworks. Interpretation of selected features most important for model prediction revealed new ways to encapsulate design rules and confirmed that models were robustly mapping essential structure-property relationships. Encountering the special challenge of ensuring that good model performance could generalize to new discovery targets motivated investigation of how to best carry out model uncertainty quantification. Distance-based approaches, whether in model latent space or in carefully engineered feature space, provided intuitive measures of the domain of applicability. With all of these pieces together, ML can be harnessed as an engine to tackle the large-scale exploration of transition-metal chemical space needed to satisfy multiple objectives using efficient global optimization methods. In practical terms, bringing these artificial intelligence tools to bear on the problems of transition-metal chemical space exploration has resulted in ML-model assessments of large, multimillion compound spaces in minutes and validated new design leads in weeks instead of decades.","",""
86,"H. Felzmann, E. F. Villaronga, C. Lutz, A. Tamó-Larrieux","Transparency you can trust: Transparency requirements for artificial intelligence between legal norms and contextual concerns",2019,"","","","",74,"2022-07-13 09:19:07","","10.1177/2053951719860542","","",,,,,86,28.67,22,4,3,"Transparency is now a fundamental principle for data processing under the General Data Protection Regulation. We explore what this requirement entails for artificial intelligence and automated decision-making systems. We address the topic of transparency in artificial intelligence by integrating legal, social, and ethical aspects. We first investigate the ratio legis of the transparency requirement in the General Data Protection Regulation and its ethical underpinnings, showing its focus on the provision of information and explanation. We then discuss the pitfalls with respect to this requirement by focusing on the significance of contextual and performative factors in the implementation of transparency. We show that human–computer interaction and human-robot interaction literature do not provide clear results with respect to the benefits of transparency for users of artificial intelligence technologies due to the impact of a wide range of contextual factors, including performative aspects. We conclude by integrating the information- and explanation-based approach to transparency with the critical contextual approach, proposing that transparency as required by the General Data Protection Regulation in itself may be insufficient to achieve the positive goals associated with transparency. Instead, we propose to understand transparency relationally, where information provision is conceptualized as communication between technology providers and users, and where assessments of trustworthiness based on contextual factors mediate the value of transparency communications. This relational concept of transparency points to future research directions for the study of transparency in artificial intelligence systems and should be taken into account in policymaking.","",""
87,"Carlos Zednik","Solving the Black Box Problem: A Normative Framework for Explainable Artificial Intelligence",2019,"","","","",75,"2022-07-13 09:19:07","","10.1007/s13347-019-00382-7","","",,,,,87,29.00,87,1,3,"","",""
59,"David Gunning, D. Aha","DARPA’s Explainable Artificial Intelligence Program",2019,"","","","",76,"2022-07-13 09:19:07","","","","",,,,,59,19.67,30,2,3,"n Dramatic success in machine learning has led toanewwaveofAIapplications (for example, transportation, security,medicine, finance, defense) that offer tremendous benefits but cannot explain their decisions and actions to human users. DARPA’s explainable artificial intelligence (XAI) program endeavors to create AI systems whose learned models and decisions can be understood and appropriately trusted by end users. Realizing this goal requires methods for learning more explainable models, designing effective explanation interfaces, and understanding the psychologic requirements for effective explanations. TheXAI developer teams are addressing the first two challenges by creating ML techniques and developing principles, strategies, and human-computer interaction techniques for generating effective explanations. Another XAI team is addressing the third challenge by summarizing, extending, and applying psychologic theories of explanation to help the XAI evaluator define a suitable evaluation framework, which the developer teams will use to test their systems. The XAI teams completed the first of this 4-year program in May 2018. In a series of ongoing evaluations, the developer teams are assessing how well their XAM systems’ explanations improve user understanding, user trust, and user task performance. Advances inmachine learning (ML) techniques promise to produce AI systems that perceive, learn, decide, and act on their own. However, they will be unable to explain their decisions and actions to human users. This lack is especially important for the Department of Defense, whose challenges require developingmore intelligent, autonomous, and symbiotic systems. Explainable AI will be essential if users are to understand, appropriately trust, and effectively manage these artificially intelligent partners. To address this, DARPA launched its explainable artificial intelligence (XAI) program in May 2017. DARPA defines explainable AI as AI systems that can explain their rationale to a human user, characterize their strengths and weaknesses, and convey an understanding of how theywill behave in the future. Naming this program explainable AI (rather than interpretable, comprehensible, or transparent AI, for example) reflects DARPA’s objective to create more human-understandable AI systems through the use of effective explanations. It also reflects the XAI team’s interest in the human psychology of explanation, which draws on the vast body of research and expertise in the social sciences.","",""
62,"Prashan Madumal, Tim Miller, L. Sonenberg, F. Vetere","A Grounded Interaction Protocol for Explainable Artificial Intelligence",2019,"","","","",77,"2022-07-13 09:19:07","","","","",,,,,62,20.67,16,4,3,"Explainable Artificial Intelligence (XAI) systems need to include an explanation model to communicate the internal decisions, behaviours and actions to the interacting humans. Successful explanation involves both cognitive and social processes. In this paper we focus on the challenge of meaningful interaction between an explainer and an explainee and investigate the structural aspects of an interactive explanation to propose an interaction protocol. We follow a bottom-up approach to derive the model by analysing transcripts of different explanation dialogue types with 398 explanation dialogues. We use grounded theory to code and identify key components of an explanation dialogue. We formalize the model using the agent dialogue framework (ADF) as a new dialogue type and then evaluate it in a human-agent interaction study with 101 dialogues from 14 participants. Our results show that the proposed model can closely follow the explanation dialogues of human-agent conversations.","",""
51,"Miriam C. Buiten","Towards Intelligent Regulation of Artificial Intelligence",2019,"","","","",78,"2022-07-13 09:19:07","","10.1017/err.2019.8","","",,,,,51,17.00,51,1,3,"Artificial intelligence (AI) is becoming a part of our daily lives at a fast pace, offering myriad benefits for society. At the same time, there is concern about the unpredictability and uncontrollability of AI. In response, legislators and scholars call for more transparency and explainability of AI. This article considers what it would mean to require transparency of AI. It advocates looking beyond the opaque concept of AI, focusing on the concrete risks and biases of its underlying technology: machine-learning algorithms. The article discusses the biases that algorithms may produce through the input data, the testing of the algorithm and the decision model. Any transparency requirement for algorithms should result in explanations of these biases that are both understandable for the prospective recipients, and technically feasible for producers. Before asking how much transparency the law should require from algorithms, we should therefore consider if the explanation that programmers could offer is useful in specific legal contexts.","",""
74,"Andrés Páez","The Pragmatic Turn in Explainable Artificial Intelligence (XAI)",2019,"","","","",79,"2022-07-13 09:19:07","","10.1007/s11023-019-09502-w","","",,,,,74,24.67,74,1,3,"","",""
31,"T. Ertekin, Qian Sun","Artificial Intelligence Applications in Reservoir Engineering: A Status Check",2019,"","","","",80,"2022-07-13 09:19:07","","10.3390/EN12152897","","",,,,,31,10.33,16,2,3,"This article provides a comprehensive review of the state-of-art in the area of artificial intelligence applications to solve reservoir engineering problems. Research works including proxy model development, artificial-intelligence-assisted history-matching, project design, and optimization, etc. are presented to demonstrate the robustness of the intelligence systems. The successes of the developments prove the advantages of the AI approaches in terms of high computational efficacy and strong learning capabilities. Thus, the implementation of intelligence models enables reservoir engineers to accomplish many challenging and time-intensive works more effectively. However, it is not yet astute to completely replace the conventional reservoir engineering models with intelligent systems, since the defects of the technology cannot be ignored. The trend of research and industrial practices of reservoir engineering area would be establishing a hand-shaking protocol between the conventional modeling and the intelligent systems. Taking advantages of both methods, more robust solutions could be obtained with significantly less computational overheads.","",""
29,"Melanie Mitchell","Artificial Intelligence Hits the Barrier of Meaning",2019,"","","","",81,"2022-07-13 09:19:07","","10.3390/info10020051","","",,,,,29,9.67,29,1,3,"Today’s AI systems sorely lack the essence of human intelligence: Understanding the situations we experience, being able to grasp their meaning. The lack of humanlike understanding in machines is underscored by recent studies demonstrating lack of robustness of state-of-the-art deep-learning systems. Deeper networks and larger datasets alone are not likely to unlock AI’s “barrier of meaning”; instead the field will need to embrace its original roots as an interdisciplinary science of intelligence.","",""
2,"A. Georgieff, Raphaela Hyee","Artificial Intelligence and Employment: New Cross-Country Evidence",2022,"","","","",82,"2022-07-13 09:19:07","","10.3389/frai.2022.832736","","",,,,,2,2.00,1,2,1,"Recent years have seen impressive advances in artificial intelligence (AI) and this has stoked renewed concern about the impact of technological progress on the labor market, including on worker displacement. This paper looks at the possible links between AI and employment in a cross-country context. It adapts the AI occupational impact measure developed by Felten, Raj and Seamans—an indicator measuring the degree to which occupations rely on abilities in which AI has made the most progress—and extends it to 23 OECD countries. Overall, there appears to be no clear relationship between AI exposure and employment growth. However, in occupations where computer use is high, greater exposure to AI is linked to higher employment growth. The paper also finds suggestive evidence of a negative relationship between AI exposure and growth in average hours worked among occupations where computer use is low. One possible explanation is that partial automation by AI increases productivity directly as well as by shifting the task composition of occupations toward higher value-added tasks. This increase in labor productivity and output counteracts the direct displacement effect of automation through AI for workers with good digital skills, who may find it easier to use AI effectively and shift to non-automatable, higher-value added tasks within their occupations. The opposite could be true for workers with poor digital skills, who may not be able to interact efficiently with AI and thus reap all potential benefits of the technology1.","",""
0,"Hui Tang, Xiangtian Yu, Rui Liu, Tao Zeng","Vec2image: an explainable artificial intelligence model for the feature representation and classification of high-dimensional biological data by vector-to-image conversion",2022,"","","","",83,"2022-07-13 09:19:07","","10.1093/bib/bbab584","","",,,,,0,0.00,0,4,1,"Abstract Feature representation and discriminative learning are proven models and technologies in artificial intelligence fields; however, major challenges for machine learning on large biological datasets are learning an effective model with mechanistical explanation on the model determination and prediction. To satisfy such demands, we developed Vec2image, an explainable convolutional neural network framework for characterizing the feature engineering, feature selection and classifier training that is mainly based on the collaboration of principal component coordinate conversion, deep residual neural networks and embedded k-nearest neighbor representation on pseudo images of high-dimensional biological data, where the pseudo images represent feature measurements and feature associations simultaneously. Vec2image has achieved better performance compared with other popular methods and illustrated its efficiency on feature selection in cell marker identification from tissue-specific single-cell datasets. In particular, in a case study on type 2 diabetes (T2D) by multiple human islet scRNA-seq datasets, Vec2image first displayed robust performance on T2D classification model building across different datasets, then a specific Vec2image model was trained to accurately recognize the cell state and efficiently rank feature genes relevant to T2D which uncovered potential T2D cellular pathogenesis; and next the cell activity changes, cell composition imbalances and cell–cell communication dysfunctions were associated to our finding T2D feature genes from both population-shared and individual-specific perspectives. Collectively, Vec2image is a new and efficient explainable artificial intelligence methodology that can be widely applied in human-readable classification and prediction on the basis of pseudo image representation of biological deep sequencing data.","",""
29,"Vanessa Putnam, C. Conati","Exploring the Need for Explainable Artificial Intelligence (XAI) in Intelligent Tutoring Systems (ITS)",2019,"","","","",84,"2022-07-13 09:19:07","","","","",,,,,29,9.67,15,2,3,"This work is the first step towards understanding when and if it is necessary for an Intelligent Tutoring System (ITS) to explain its underlying user modeling techniques to students. We conduct an initial pilot study to explore student attitudes towards incorporating explanations to an ITS, by asking participants for suggestions on the type of explanations, if any, that they would like to see. Our results indicate an overall positive sentiment towards wanting explanation and suggest a few design directions for incorporating explanation into an existing ITS.","",""
0,"Pan Wang, Yangyang Zhong, Zhenan Yao","Modeling and Estimation of CO2 Emissions in China Based on Artificial Intelligence",2022,"","","","",85,"2022-07-13 09:19:07","","10.1155/2022/6822467","","",,,,,0,0.00,0,3,1,"Since China’s reform and opening up, the social economy has achieved rapid development, followed by a sharp increase in carbon dioxide (CO2) emissions. Therefore, at the 75th United Nations General Assembly, China proposed to achieve carbon peaking by 2030 and carbon neutrality by 2060. The research work on advance forecasting of CO2 emissions is essential to achieve the above-mentioned carbon peaking and carbon neutrality goals in China. In order to achieve accurate prediction of CO2 emissions, this study establishes a hybrid intelligent algorithm model suitable for CO2 emissions prediction based on China’s CO2 emissions and related socioeconomic indicator data from 1971 to 2017. The hyperparameters of Least Squares Support Vector Regression (LSSVR) are optimized by the Adaptive Artificial Bee Colony (AABC) algorithm to build a high-performance hybrid intelligence model. The research results show that the hybrid intelligent algorithm model designed in this paper has stronger robustness and accuracy with relative error almost within ±5% in the advance prediction of CO2 emissions. The modeling scheme proposed in this study can not only provide strong support for the Chinese government and industry departments to formulate policies related to the carbon peaking and carbon neutrality goals, but also can be extended to the research of other socioeconomic-related issues.","",""
10,"Zihao Chen, Long Hu, Baoting Zhang, Aiping Lu, Yaofeng Wang, Yuanyuan Yu, Ge Zhang","Artificial Intelligence in Aptamer–Target Binding Prediction",2021,"","","","",86,"2022-07-13 09:19:07","","10.3390/ijms22073605","","",,,,,10,10.00,1,7,1,"Aptamers are short single-stranded DNA, RNA, or synthetic Xeno nucleic acids (XNA) molecules that can interact with corresponding targets with high affinity. Owing to their unique features, including low cost of production, easy chemical modification, high thermal stability, reproducibility, as well as low levels of immunogenicity and toxicity, aptamers can be used as an alternative to antibodies in diagnostics and therapeutics. Systematic evolution of ligands by exponential enrichment (SELEX), an experimental approach for aptamer screening, allows the selection and identification of in vitro aptamers with high affinity and specificity. However, the SELEX process is time consuming and characterization of the representative aptamer candidates from SELEX is rather laborious. Artificial intelligence (AI) could help to rapidly identify the potential aptamer candidates from a vast number of sequences. This review discusses the advancements of AI pipelines/methods, including structure-based and machine/deep learning-based methods, for predicting the binding ability of aptamers to targets. Structure-based methods are the most used in computer-aided drug design. For this part, we review the secondary and tertiary structure prediction methods for aptamers, molecular docking, as well as molecular dynamic simulation methods for aptamer–target binding. We also performed analysis to compare the accuracy of different secondary and tertiary structure prediction methods for aptamers. On the other hand, advanced machine-/deep-learning models have witnessed successes in predicting the binding abilities between targets and ligands in drug discovery and thus potentially offer a robust and accurate approach to predict the binding between aptamers and targets. The research utilizing machine-/deep-learning techniques for prediction of aptamer–target binding is limited currently. Therefore, perspectives for models, algorithms, and implementation strategies of machine/deep learning-based methods are discussed. This review could facilitate the development and application of high-throughput and less laborious in silico methods in aptamer selection and characterization.","",""
10,"T. Penzkofer, A. Padhani, B. Turkbey, M. Haider, H. Huisman, J. Walz, G. Salomon, I. Schoots, J. Richenberg, G. Villeirs, V. Panebianco, O. Rouvière, V. Løgager, J. Barentsz","ESUR/ESUI position paper: developing artificial intelligence for precision diagnosis of prostate cancer using magnetic resonance imaging",2021,"","","","",87,"2022-07-13 09:19:07","","10.1007/s00330-021-08021-6","","",,,,,10,10.00,1,14,1,"","",""
0,"D. Gilboa, L. Bori, M. Shapiro, A. Pellicer, R. Maor, A. Delgado, D. Seidman, M. Meseguer","P-277 An artificial intelligence (AI) deselection model for top-quality blastocysts: algorithmic analysis of morphokinetic features for aneuploidy may increase implantation rates",2022,"","","","",88,"2022-07-13 09:19:07","","10.1093/humrep/deac107.266","","",,,,,0,0.00,0,8,1,"      Can an AI deselection model identify distinct morphokinetic patterns in top-quality blastocyst with unknown ploidy that fail to implant?        An AI based deselection model was able to predict implantation failure based on morphokinetic features previously found to associate with aneuploidy.        Aneuploidy is the most common explanation for implantation failure of high-quality blastocysts. Yet, high-quality blastocysts with unknown ploidy that fail to implant are often morphologically indistinguishable from blastocysts that succeed to implant. Our previously published results (ESHRE 2021) demonstrated that aneuploid blastocysts were more likely to reach development events (t2-t8) later, and that the timing between each event was statistically longer (p < 0.001), when compared to euploid embryos. Given that delayed morphokinetic rates are tightly linked to ploidy, we investigated whether similar known morphokinetic features were associated with implantation failure in top-graded embryos.        Time-lapse sequences of 3,259 top-quality blastocysts from fresh single embryo transfer cycles with known implantation outcomes were analyzed using an AI-based algorithm. The algorithm utilized convolutional neural network extracted temporal features based on multiple morphokinetic parameters known to associate with ploidy.        time-lapse sequences and morphokinetic events were algorithmically analyzed to measure the rate of mitotic division events and compare the number of embryos in each category (implanted/nonimplanted) that reached each developmental event at least one standard deviation (SD) later than the mean for implanted embryos.        Results showed statistical differences in the following morphokinetic features between the two categories: t2, t3, t4, and t3-t4 (p < 0.05). Implanted top-graded blastocysts were likely to reach t2, t3, and t4 after 25.23 ± 3.8 SD, 36.06 ± 3.4 SD, and 37.14 hours ±3.6 SD, respectively. The time gap between t3 and t4 was found to be 12.25 hours ± 5.31 SD. Given this, we followed the methodology described above to propose cutoff values (in hours) that differentiated between non-implanted and implanted top-graded blastocysts based on their morphokinetic profiles. Implantation failure was found to be associated with the likelihood of reaching t2 after 28.61 hours (OR = 2.36, CI 0.96-5.77), t3 after 39.46 (OR = 3.48, CI 1.62-7.47), and t4 after 40.79 hours (OR = 2.23, CI 1.09- 4.53). A time gap between t3 and t4 of more than 17.56 hours was also associated with implantation failure (OR = 2.48, CI 1.12-5.48), indicating perturbed mitotic activity. The cutoff values proposed here were incorporated into the algorithm for optimized deselection of morphologically similar top-quality blastocysts with delayed morphokinetic profiles.        This study needs to be validated on a larger, multi-centric dataset that takes into account more morphokinetic features associated with ploidy in order to increase the robustness of our algorithm.        For the first time, our algorithmic model proposed here demonstrates the utility of an AI tool to deselect top-graded blastocysts that would otherwise be selected for transfer based on conventional morphologic assessment alone.        Not Applicable ","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",89,"2022-07-13 09:19:07","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",90,"2022-07-13 09:19:07","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
63,"M. VerMilyea, J. Hall, S. Diakiw, A. Johnston, T. Nguyen, D. Perugini, A. Miller, A. Picou, A. P. Murphy, M. Perugini","Development of an artificial intelligence-based assessment model for prediction of embryo viability using static images captured by optical light microscopy during IVF",2020,"","","","",91,"2022-07-13 09:19:07","","10.1093/humrep/deaa013","","",,,,,63,31.50,6,10,2,"Abstract STUDY QUESTION Can an artificial intelligence (AI)-based model predict human embryo viability using images captured by optical light microscopy? SUMMARY ANSWER We have combined computer vision image processing methods and deep learning techniques to create the non-invasive Life Whisperer AI model for robust prediction of embryo viability, as measured by clinical pregnancy outcome, using single static images of Day 5 blastocysts obtained from standard optical light microscope systems. WHAT IS KNOWN ALREADY Embryo selection following IVF is a critical factor in determining the success of ensuing pregnancy. Traditional morphokinetic grading by trained embryologists can be subjective and variable, and other complementary techniques, such as time-lapse imaging, require costly equipment and have not reliably demonstrated predictive ability for the endpoint of clinical pregnancy. AI methods are being investigated as a promising means for improving embryo selection and predicting implantation and pregnancy outcomes. STUDY DESIGN, SIZE, DURATION These studies involved analysis of retrospectively collected data including standard optical light microscope images and clinical outcomes of 8886 embryos from 11 different IVF clinics, across three different countries, between 2011 and 2018. PARTICIPANTS/MATERIALS, SETTING, METHODS The AI-based model was trained using static two-dimensional optical light microscope images with known clinical pregnancy outcome as measured by fetal heartbeat to provide a confidence score for prediction of pregnancy. Predictive accuracy was determined by evaluating sensitivity, specificity and overall weighted accuracy, and was visualized using histograms of the distributions of predictions. Comparison to embryologists’ predictive accuracy was performed using a binary classification approach and a 5-band ranking comparison. MAIN RESULTS AND THE ROLE OF CHANCE The Life Whisperer AI model showed a sensitivity of 70.1% for viable embryos while maintaining a specificity of 60.5% for non-viable embryos across three independent blind test sets from different clinics. The weighted overall accuracy in each blind test set was >63%, with a combined accuracy of 64.3% across both viable and non-viable embryos, demonstrating model robustness and generalizability beyond the result expected from chance. Distributions of predictions showed clear separation of correctly and incorrectly classified embryos. Binary comparison of viable/non-viable embryo classification demonstrated an improvement of 24.7% over embryologists’ accuracy (P = 0.047, n = 2, Student’s t test), and 5-band ranking comparison demonstrated an improvement of 42.0% over embryologists (P = 0.028, n = 2, Student’s t test). LIMITATIONS, REASONS FOR CAUTION The AI model developed here is limited to analysis of Day 5 embryos; therefore, further evaluation or modification of the model is needed to incorporate information from different time points. The endpoint described is clinical pregnancy as measured by fetal heartbeat, and this does not indicate the probability of live birth. The current investigation was performed with retrospectively collected data, and hence it will be of importance to collect data prospectively to assess real-world use of the AI model. WIDER IMPLICATIONS OF THE FINDINGS These studies demonstrated an improved predictive ability for evaluation of embryo viability when compared with embryologists’ traditional morphokinetic grading methods. The superior accuracy of the Life Whisperer AI model could lead to improved pregnancy success rates in IVF when used in a clinical setting. It could also potentially assist in standardization of embryo selection methods across multiple clinical environments, while eliminating the need for complex time-lapse imaging equipment. Finally, the cloud-based software application used to apply the Life Whisperer AI model in clinical practice makes it broadly applicable and globally scalable to IVF clinics worldwide. STUDY FUNDING/COMPETING INTEREST(S) Life Whisperer Diagnostics, Pty Ltd is a wholly owned subsidiary of the parent company, Presagen Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation and Startup Fund (RCSF). ‘In kind’ support and embryology expertise to guide algorithm development were provided by Ovation Fertility. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. Presagen has filed a provisional patent for the technology described in this manuscript (52985P pending). A.P.M. owns stock in Life Whisperer, and S.M.D., A.J., T.N. and A.P.M. are employees of Life Whisperer.","",""
9,"J. Senoner, Torbjørn H. Netland, S. Feuerriegel","Using Explainable Artificial Intelligence to Improve Process Quality: Evidence from Semiconductor Manufacturing",2021,"","","","",92,"2022-07-13 09:19:07","","10.1287/mnsc.2021.4190","","",,,,,9,9.00,3,3,1,"We develop a data-driven decision model to improve process quality in manufacturing. A challenge for traditional methods in quality management is to handle high-dimensional and nonlinear manufacturing data. We address this challenge by adapting explainable artificial intelligence to the context of quality management. Specifically, we propose the use of nonlinear modeling with Shapley additive explanations to infer how a set of production parameters and the process quality of a manufacturing system are related. Thereby, we contribute a measure of process importance based on which manufacturers can prioritize processes for quality improvement. Grounded in quality management theory, our decision model selects improvement actions that target the sources of quality variation. The decision model is validated in a real-world application at a leading manufacturer of high-power semiconductors. Seeking to improve production yield, we apply our decision model to select improvement actions for a transistor chip product. We then conduct a field experiment to confirm the effectiveness of the improvement actions. Compared with the average yield in our sample, the experiment returns a reduction in yield loss of 21.7%. Furthermore, we report on results from a postexperimental rollout of the decision model, which also resulted in significant yield improvements. We demonstrate the operational value of explainable artificial intelligence by showing that critical drivers of process quality can go undiscovered by the use of traditional methods. This paper was accepted by Charles Corbett, operations management.","",""
9,"B. N. Manjunatha Reddy, S. K. Pramada, T. Roshni","Monthly surface runoff prediction using artificial intelligence: A study from a tropical climate river basin",2021,"","","","",93,"2022-07-13 09:19:07","","10.1007/s12040-020-01508-8","","",,,,,9,9.00,3,3,1,"","",""
9,"Junfeng Peng, Kaiqiang Zou, Mi Zhou, Yi Teng, Xiongyong Zhu, Feifei Zhang, Jun Xu","An Explainable Artificial Intelligence Framework for the Deterioration Risk Prediction of Hepatitis Patients",2021,"","","","",94,"2022-07-13 09:19:07","","10.1007/s10916-021-01736-5","","",,,,,9,9.00,1,7,1,"","",""
37,"T. Babina, A. Fedyk, A. He, James Hodson","Artificial Intelligence, Firm Growth, and Industry Concentration",2020,"","","","",95,"2022-07-13 09:19:07","","10.2139/ssrn.3651052","","",,,,,37,18.50,9,4,2,"Which firms invest in artificial intelligence (AI) technologies, and how do these investments affect individual firms and industries? We provide a comprehensive picture of the use of AI technologies and their impact among US firms over the last decade, using a unique combination of job postings and individual-level employment profiles. We introduce a novel measure of investments in AI technologies based on human capital and document that larger firms with higher sales, markups, and cash holdings tend to invest more in AI. Firms that invest in AI experience faster growth in both sales and employment, which translates into analogous growth at the industry level. The positive effects are concentrated among the ex ante largest firms, leading to a positive correlation between AI investments and an increase in industry concentration. However, the increase in concentration is not accompanied by either increased markups or increased productivity. Instead, firms tend to expand into new product and geographic markets. Our results are robust to instrumenting firm-level AI investments with foreign industry-level AI investments and with local variation in industry-level AI investments, and to controlling for investments in general information technology and robotics. We also document consistent patterns across measures of AI using firms' demand for AI talent (job postings) and actual AI talent (resumes). Overall, our findings support the view that new technologies, such as AI, increase the scale of the most productive firms and contribute to the rise of superstar firms.","",""
3,"Saad Mahamood","Explainable Artificial Intelligence and its potential within Industry",2019,"","","","",96,"2022-07-13 09:19:07","","10.18653/v1/W19-8401","","",,,,,3,1.00,3,1,3,"The age of Big Data has enabled the creation of artificial intelligence solutions that has allowed systems to better respond to their users requests and needs. Applications such as recommender systems, automated content generation systems, etc. are increasingly leveraging such large amounts of data to make better informed decisions about how to tailor their output appropriately. However, the opaqueness of these AI systems in how they derive their decisions or outputs has led to an increasing call for transparency with increasing concerns for the potential of bias to occur in areas such as finance and criminal law. The culmination of these calls have lead to tentative legislative steps. For example, the ""Right to explanation"" as part of the recently enacted European Union’s General Data Protection Regulation. Natural Language Generation (NLG) has been used in successfully in many data-to-text applications allowing users to gain insights from their data sets. Whilst NLG technology has a strong role to play in generating explanations for AI models there still remains inherit challenges in developing and deploying text generation systems within a commercial context. In this talk I will explore the role and potential that Natural Language Explainable AI can have within trivago and the wider industry. trivago is a leading accommodation meta-search engine that enables users find the right hotel or apartment at the right price. In particular, this talk will describe the work we have done to apply natural language solutions within trivago and the challenges of applying AI solutions from a commercial perspective. Finally, this talk will also explore the potential applications of where explainable AI approaches could be used within trivago.","",""
3,"James A. Crowder, John Carbone, Shelli Friess","Abductive Artificial Intelligence Learning Models",2019,"","","","",97,"2022-07-13 09:19:07","","10.1007/978-3-030-17081-3_5","","",,,,,3,1.00,1,3,3,"","",""
6,"R. Lemos, M. Grzes","Self-Adaptive Artificial Intelligence",2019,"","","","",98,"2022-07-13 09:19:07","","10.1109/SEAMS.2019.00028","","",,,,,6,2.00,3,2,3,"Machine learning tools, like deep neural networks, are perceived to be black boxes. That is, the only way of changing their internal data models is to retrain these models using different inputs. This is ineffective in dynamic systems that are prone to changes, like concept drift. A new promising solution is transparent artificial intelligence, based on the notions of interpretation and explanation, whose objective is to correlate the internal data models with predictions. The research question being addressed is whether we can have a self-adaptive machine learning system that is able to interpret and explain its data model in order for it to be controlled. In this position paper, we present our initial thoughts whether this can be achieved.","",""
8,"Linbo Liu, Mingcheng Bi, Yunhua Wang, Junfeng Liu, Xiwen Jiang, Zhongbin Xu, Xingcai Zhang","Artificial intelligence-powered microfluidics for nanomedicine and materials synthesis.",2021,"","","","",99,"2022-07-13 09:19:07","","10.1039/d1nr06195j","","",,,,,8,8.00,1,7,1,"Artificial intelligence (AI) is an emerging technology with great potential, and its robust calculation and analysis capabilities are unmatched by traditional calculation tools. With the promotion of deep learning and open-source platforms, the threshold of AI has also become lower. Combining artificial intelligence with traditional fields to create new fields of high research and application value has become a trend. AI has been involved in many disciplines, such as medicine, materials, energy, and economics. The development of AI requires the support of many kinds of data, and microfluidic systems can often mine object data on a large scale to support AI. Due to the excellent synergy between the two technologies, excellent research results have emerged in many fields. In this review, we briefly review AI and microfluidics and introduce some applications of their combination, mainly in nanomedicine and material synthesis. Finally, we discuss the development trend of the combination of the two technologies.","",""
103,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dentistry: Chances and Challenges",2020,"","","","",100,"2022-07-13 09:19:07","","10.1177/0022034520915714","","",,,,,103,51.50,34,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
103,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dentistry: Chances and Challenges",2020,"","","","",101,"2022-07-13 09:19:07","","10.1177/0022034520915714","","",,,,,103,51.50,34,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
6,"Atoosa Kasirzadeh","Reasons, Values, Stakeholders: A Philosophical Framework for Explainable Artificial Intelligence",2021,"","","","",102,"2022-07-13 09:19:07","","10.1145/3442188.3445866","","",,,,,6,6.00,6,1,1,"The societal and ethical implications of the use of opaque artificial intelligence systems in consequential decisions, such as welfare allocation and criminal justice, have generated a lively debate among multiple stakeholders, including computer scientists, ethicists, social scientists, policy makers, and end users. However, the lack of a common language or a multi-dimensional framework to appropriately bridge the technical, epistemic, and normative aspects of this debate nearly prevents the discussion from being as productive as it could be. Drawing on the philosophical literature on the nature and value of explanations, this paper offers a multi-faceted framework that brings more conceptual precision to the present debate by identifying the types of explanations that are most pertinent to artificial intelligence predictions, recognizing the relevance and importance of the social and ethical values for the evaluation of these explanations, and demonstrating the importance of these explanations for incorporating a diversified approach to improving the design of truthful algorithmic ecosystems. The proposed philosophical framework thus lays the groundwork for establishing a pertinent connection between the technical and ethical aspects of artificial intelligence systems.","",""
7,"Zhan Zhang, Daniel Citardi, Dakuo Wang, Y. Genc, J. Shan, Xiangmin Fan","Patients’ perceptions of using artificial intelligence (AI)-based technology to comprehend radiology imaging data",2021,"","","","",103,"2022-07-13 09:19:07","","10.1177/14604582211011215","","",,,,,7,7.00,1,6,1,"Results of radiology imaging studies are not typically comprehensible to patients. With the advances in artificial intelligence (AI) technology in recent years, it is expected that AI technology can aid patients’ understanding of radiology imaging data. The aim of this study is to understand patients’ perceptions and acceptance of using AI technology to interpret their radiology reports. We conducted semi-structured interviews with 13 participants to elicit reflections pertaining to the use of AI technology in radiology report interpretation. A thematic analysis approach was employed to analyze the interview data. Participants have a generally positive attitude toward using AI-based systems to comprehend their radiology reports. AI is perceived to be particularly useful in seeking actionable information, confirming the doctor’s opinions, and preparing for the consultation. However, we also found various concerns related to the use of AI in this context, such as cyber-security, accuracy, and lack of empathy. Our results highlight the necessity of providing AI explanations to promote people’s trust and acceptance of AI. Designers of patient-centered AI systems should employ user-centered design approaches to address patients’ concerns. Such systems should also be designed to promote trust and deliver concerning health results in an empathetic manner to optimize the user experience.","",""
0,"S. Sadeghi, M. Amiri, Farzaneh Mansoori Mooseloo","Artificial Intelligence and Its Application in Optimization under Uncertainty",2021,"","","","",104,"2022-07-13 09:19:07","","10.5772/intechopen.98628","","",,,,,0,0.00,0,3,1,"Nowadays, the increase in data acquisition and availability and complexity around optimization make it imperative to jointly use artificial intelligence (AI) and optimization for devising data-driven and intelligent decision support systems (DSS). A DSS can be successful if large amounts of interactive data proceed fast and robustly and extract useful information and knowledge to help decision-making. In this context, the data-driven approach has gained prominence due to its provision of insights for decision-making and easy implementation. The data-driven approach can discover various database patterns without relying on prior knowledge while also handling flexible objectives and multiple scenarios. This chapter reviews recent advances in data-driven optimization, highlighting the promise of data-driven optimization that integrates mathematical programming and machine learning (ML) for decision-making under uncertainty and identifies potential research opportunities. This chapter provides guidelines and implications for researchers, managers, and practitioners in operations research who want to advance their decision-making capabilities under uncertainty concerning data-driven optimization. Then, a comprehensive review and classification of the relevant publications on the data-driven stochastic program, data-driven robust optimization, and data-driven chance-constrained are presented. This chapter also identifies fertile avenues for future research that focus on deep-data-driven optimization, deep data-driven models, as well as online learning-based data-driven optimization. Perspectives on reinforcement learning (RL)-based data-driven optimization and deep RL for solving NP-hard problems are discussed. We investigate the application of data-driven optimization in different case studies to demonstrate improvements in operational performance over conventional optimization methodology. Finally, some managerial implications and some future directions are provided.","",""
50,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello","Artificial Intelligence, Algorithmic Pricing, and Collusion",2020,"","","","",105,"2022-07-13 09:19:07","","10.1257/AER.20190623","","",,,,,50,25.00,13,4,2,"Increasingly, pricing algorithms are supplanting human decision making in real marketplaces. To inform the competition policy debate on the possible consequences of this development, we experiment with pricing algorithms powered by Artificial Intelligence (AI) in controlled environments (computer simulations), studying the interaction among a number of Q-learning algorithms in a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. In this setting the algorithms consistently learn to charge supra-competitive prices, without communicating with one another. The high prices are sustained by classical collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.","",""
42,"Christian Meske, Enrico Bunde, Johannes Schneider, Martin Gersch","Explainable Artificial Intelligence: Objectives, Stakeholders, and Future Research Opportunities",2020,"","","","",106,"2022-07-13 09:19:07","","10.1080/10580530.2020.1849465","","",,,,,42,21.00,11,4,2,"ABSTRACT Artificial Intelligence (AI) has diffused into many areas of our private and professional life. In this research note, we describe exemplary risks of black-box AI, the consequent need for explainability, and previous research on Explainable AI (XAI) in information systems research. Moreover, we discuss the origin of the term XAI, generalized XAI objectives, and stakeholder groups, as well as quality criteria of personalized explanations. We conclude with an outlook to future research on XAI.","",""
45,"Avishek Choudhury, Onur Asan","Role of Artificial Intelligence in Patient Safety Outcomes: Systematic Literature Review",2020,"","","","",107,"2022-07-13 09:19:07","","10.2196/18599","","",,,,,45,22.50,23,2,2,"Background Artificial intelligence (AI) provides opportunities to identify the health risks of patients and thus influence patient safety outcomes. Objective The purpose of this systematic literature review was to identify and analyze quantitative studies utilizing or integrating AI to address and report clinical-level patient safety outcomes. Methods We restricted our search to the PubMed, PubMed Central, and Web of Science databases to retrieve research articles published in English between January 2009 and August 2019. We focused on quantitative studies that reported positive, negative, or intermediate changes in patient safety outcomes using AI apps, specifically those based on machine-learning algorithms and natural language processing. Quantitative studies reporting only AI performance but not its influence on patient safety outcomes were excluded from further review. Results We identified 53 eligible studies, which were summarized concerning their patient safety subcategories, the most frequently used AI, and reported performance metrics. Recognized safety subcategories were clinical alarms (n=9; mainly based on decision tree models), clinical reports (n=21; based on support vector machine models), and drug safety (n=23; mainly based on decision tree models). Analysis of these 53 studies also identified two essential findings: (1) the lack of a standardized benchmark and (2) heterogeneity in AI reporting. Conclusions This systematic review indicates that AI-enabled decision support systems, when implemented correctly, can aid in enhancing patient safety by improving error detection, patient stratification, and drug management. Future work is still needed for robust validation of these systems in prospective and real-world clinical environments to understand how well AI can predict safety outcomes in health care settings.","",""
38,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases.",2020,"","","","",108,"2022-07-13 09:19:07","","10.1038/s41746-020-0229-3","","",,,,,38,19.00,6,6,2,"","",""
43,"M. González-Rivero, Oscar Beijbom, A. Rodriguez-Ramirez, D. Bryant, A. Ganase, Y. González-Marrero, A. Herrera-Reveles, E. Kennedy, Catherine J. S. Kim, S. Lopez-Marcano, Kathryn Markey, B. Neal, K. Osborne, C. Reyes-Nivia, E. Sampayo, Kristin Stolberg, Abbie Taylor, J. Vercelloni, Mathew Wyatt, O. Hoegh‐Guldberg","Monitoring of Coral Reefs Using Artificial Intelligence: A Feasible and Cost-Effective Approach",2020,"","","","",109,"2022-07-13 09:19:07","","10.3390/rs12030489","","",,,,,43,21.50,4,20,2,"Ecosystem monitoring is central to effective management, where rapid reporting is essential to provide timely advice. While digital imagery has greatly improved the speed of underwater data collection for monitoring benthic communities, image analysis remains a bottleneck in reporting observations. In recent years, a rapid evolution of artificial intelligence in image recognition has been evident in its broad applications in modern society, offering new opportunities for increasing the capabilities of coral reef monitoring. Here, we evaluated the performance of Deep Learning Convolutional Neural Networks for automated image analysis, using a global coral reef monitoring dataset. The study demonstrates the advantages of automated image analysis for coral reef monitoring in terms of error and repeatability of benthic abundance estimations, as well as cost and benefit. We found unbiased and high agreement between expert and automated observations (97%). Repeated surveys and comparisons against existing monitoring programs also show that automated estimation of benthic composition is equally robust in detecting change and ensuring the continuity of existing monitoring data. Using this automated approach, data analysis and reporting can be accelerated by at least 200x and at a fraction of the cost (1%). Combining commonly used underwater imagery in monitoring with automated image annotation can dramatically improve how we measure and monitor coral reefs worldwide, particularly in terms of allocating limited resources, rapid reporting and data integration within and across management areas.","",""
40,"Philipp Schmidt, F. Biessmann, Timm Teubner","Transparency and trust in artificial intelligence systems",2020,"","","","",110,"2022-07-13 09:19:07","","10.1080/12460125.2020.1819094","","",,,,,40,20.00,13,3,2,"ABSTRACT Assistive technology featuring artificial intelligence (AI) to support human decision-making has become ubiquitous. Assistive AI achieves accuracy comparable to or even surpassing that of human experts. However, often the adoption of assistive AI systems is limited by a lack of trust of humans into an AI’s prediction. This is why the AI research community has been focusing on rendering AI decisions more transparent by providing explanations of an AIs decision. To what extent these explanations really help to foster trust into an AI system remains an open question. In this paper, we report the results of a behavioural experiment in which subjects were able to draw on the support of an ML-based decision support tool for text classification. We experimentally varied the information subjects received and show that transparency can actually have a negative impact on trust. We discuss implications for decision makers employing assistive AI technology.","",""
37,"Z. Yaseen, Z. H. Ali, Sinan Q. Salih, N. Al‐Ansari","Prediction of Risk Delay in Construction Projects Using a Hybrid Artificial Intelligence Model",2020,"","","","",111,"2022-07-13 09:19:07","","10.3390/su12041514","","",,,,,37,18.50,9,4,2,"Project delays are the major problems tackled by the construction sector owing to the associated complexity and uncertainty in the construction activities. Artificial Intelligence (AI) models have evidenced their capacity to solve dynamic, uncertain and complex tasks. The aim of this current study is to develop a hybrid artificial intelligence model called integrative Random Forest classifier with Genetic Algorithm optimization (RF-GA) for delay problem prediction. At first, related sources and factors of delay problems are identified. A questionnaire is adopted to quantify the impact of delay sources on project performance. The developed hybrid model is trained using the collected data of the previous construction projects. The proposed RF-GA is validated against the classical version of an RF model using statistical performance measure indices. The achieved results of the developed hybrid RF-GA model revealed a good resultant performance in terms of accuracy, kappa and classification error. Based on the measured accuracy, kappa and classification error, RF-GA attained 91.67%, 87% and 8.33%, respectively. Overall, the proposed methodology indicated a robust and reliable technique for project delay prediction that is contributing to the construction project management monitoring and sustainability.","",""
31,"I. Habli, T. Lawton, Zoe Porter","Artificial intelligence in health care: accountability and safety",2020,"","","","",112,"2022-07-13 09:19:07","","10.2471/BLT.19.237487","","",,,,,31,15.50,10,3,2,"Abstract The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed.","",""
34,"Shashank Vaid, Aaron McAdie, Ran Kremer, V. Khanduja, M. Bhandari","Risk of a second wave of Covid-19 infections: using artificial intelligence to investigate stringency of physical distancing policies in North America",2020,"","","","",113,"2022-07-13 09:19:07","","10.1007/s00264-020-04653-3","","",,,,,34,17.00,7,5,2,"","",""
4,"Claire Woodcock, B. Mittelstadt, Dan Busbridge, Grant Blank","The Impact of Explanations on Layperson Trust in Artificial Intelligence–Driven Symptom Checker Apps: Experimental Study",2021,"","","","",114,"2022-07-13 09:19:07","","10.2196/29386","","",,,,,4,4.00,1,4,1,"Background Artificial intelligence (AI)–driven symptom checkers are available to millions of users globally and are advocated as a tool to deliver health care more efficiently. To achieve the promoted benefits of a symptom checker, laypeople must trust and subsequently follow its instructions. In AI, explanations are seen as a tool to communicate the rationale behind black-box decisions to encourage trust and adoption. However, the effectiveness of the types of explanations used in AI-driven symptom checkers has not yet been studied. Explanations can follow many forms, including why-explanations and how-explanations. Social theories suggest that why-explanations are better at communicating knowledge and cultivating trust among laypeople. Objective The aim of this study is to ascertain whether explanations provided by a symptom checker affect explanatory trust among laypeople and whether this trust is impacted by their existing knowledge of disease. Methods A cross-sectional survey of 750 healthy participants was conducted. The participants were shown a video of a chatbot simulation that resulted in the diagnosis of either a migraine or temporal arteritis, chosen for their differing levels of epidemiological prevalence. These diagnoses were accompanied by one of four types of explanations. Each explanation type was selected either because of its current use in symptom checkers or because it was informed by theories of contrastive explanation. Exploratory factor analysis of participants’ responses followed by comparison-of-means tests were used to evaluate group differences in trust. Results Depending on the treatment group, two or three variables were generated, reflecting the prior knowledge and subsequent mental model that the participants held. When varying explanation type by disease, migraine was found to be nonsignificant (P=.65) and temporal arteritis, marginally significant (P=.09). Varying disease by explanation type resulted in statistical significance for input influence (P=.001), social proof (P=.049), and no explanation (P=.006), with counterfactual explanation (P=.053). The results suggest that trust in explanations is significantly affected by the disease being explained. When laypeople have existing knowledge of a disease, explanations have little impact on trust. Where the need for information is greater, different explanation types engender significantly different levels of trust. These results indicate that to be successful, symptom checkers need to tailor explanations to each user’s specific question and discount the diseases that they may also be aware of. Conclusions System builders developing explanations for symptom-checking apps should consider the recipient’s knowledge of a disease and tailor explanations to each user’s specific need. Effort should be placed on generating explanations that are personalized to each user of a symptom checker to fully discount the diseases that they may be aware of and to close their information gap.","",""
4,"Lukasz Górski, Shashishekar Ramakrishna","Explainable artificial intelligence, lawyer's perspective",2021,"","","","",115,"2022-07-13 09:19:07","","10.1145/3462757.3466145","","",,,,,4,4.00,2,2,1,"Explainable artificial intelligence (XAI) is a research direction that was already put under scrutiny, in particular in the AI&Law community. Whilst there were notable developments in the area of (general, not necessarily legal) XAI, user experience studies regarding such methods, as well as more general studies pertaining to the concept of explainability among the users are still lagging behind. This paper firstly, assesses the performance of different explainability methods (Grad-CAM, LIME, SHAP), in explaining the predictions for a legal text classification problem; those explanations were then judged by legal professionals according to their accuracy. Secondly, the same respondents were asked to give their opinion on the desired qualities of (explainable) artificial intelligence (AI) legal decision system and to present their general understanding of the term XAI. This part was treated as a pilot study for a more pronounced one regarding the lawyer's position on AI, and XAI in particular.","",""
2,"Xiang Bai, Hanchen Wang, Liya Ma, Yongchao Xu, Jiefeng Gan, Ziwei Fan, Fan Yang, Ke Ma, Jiehua Yang, S. Bai, Chang Shu, X. Zou, Renhao Huang, Changzheng Zhang, Xiaowu Liu, Dandan Tu, Chuou Xu, Wenqing Zhang, X. Wang, Anguo Chen, Yu Zeng, Dehua Yang, Ming-Wei Wang, N. Holalkere, N. Halin, I. Kamel, Jia Wu, Xue-Hua Peng, Xiang Wang, Jianbo Shao, P. Mongkolwat, Jianjun Zhang, Weiyang Liu, Michael Roberts, Z. Teng, L. Beer, Lorena E. Sanchez, E. Sala, D. Rubin, Adrian Weller, Joan Lasenby, Chuangsheng Zheng, Jianming Wang, Zhen Li, C. Schonlieb, Tian Xia","Advancing COVID-19 Diagnosis with Privacy-Preserving Collaboration in Artificial Intelligence",2021,"","","","",116,"2022-07-13 09:19:07","","10.1038/s42256-021-00421-z","","",,,,,2,2.00,0,46,1,"","",""
442,"M. Ridley","Explainable Artificial Intelligence (XAI)",2022,"","","","",117,"2022-07-13 09:19:07","","10.6017/ital.v41i2.14683","","",,,,,442,442.00,442,1,1,"The field of explainable artificial intelligence (XAI) advances techniques, processes, and strategies that provide explanations for the predictions, recommendations, and decisions of opaque and complex machine learning systems. Increasingly academic libraries are providing library users with systems, services, and collections created and delivered by machine learning. Academic libraries should adopt XAI as a tool set to verify and validate these resources, and advocate for public policy regarding XAI that serves libraries, the academy, and the public interest.","",""
29,"Grayson W. Armstrong, A. Lorch","A(eye): A Review of Current Applications of Artificial Intelligence and Machine Learning in Ophthalmology",2019,"","","","",118,"2022-07-13 09:19:07","","10.1097/IIO.0000000000000298","","",,,,,29,9.67,15,2,3,"Artificial intelligence (AI) is a branch of computer science that aims to enable computers to perform human-like tasks. Although AI is a broad discipline, machine learning is a specific branch of AI that uses computer algorithms capable of “learning” through the simulation of human intelligence. Machine learning algorithms have been applied to the medical field since the 1970s,1 and since that time have proven useful in computerassisted diagnosis, screening, and prognostication of disease.2–7 Ophthalmology is uniquely capable of capitalizing on the promise of AI. Ophthalmologists, during routine clinical encounters, generate robust data sources capable of supporting machine learning algorithms including multimodal ophthalmic images and quantifiable metrics such as visual acuity (VA), intraocular pressure, and cup to disk ratio. To date, AI techniques have been applied to ophthalmology to screen for and diagnose diseases, such as diabetic retinopathy (DR), age-related macular degeneration (AMD), macular edema (ME), glaucoma, keratoconus, postlaserassisted in situ keratomileusis corneal ectasia, retinopathy of prematurity (ROP), and cataracts, as well as predict the prognosis of various ophthalmic diseases. Advances in ophthalmology-specific AI stand to increase patient access to clinical screening and diagnosis as well as decrease health care costs, especially when applied to high-risk populations, low-resource communities, or when combined with telemedicine initiatives. This review provides an introduction to AI andmachine learning, as well as an overview of current applications in the field of ophthalmology.","",""
29,"Brandon Malone, Boris Simovski, Clément Moliné, Jun Cheng, Marius Gheorghe, Hugues Fontenelle, Ioannis Vardaxis, Simen Tennøe, Jenny-Ann Malmberg, R. Stratford, T. Clancy","Artificial intelligence predicts the immunogenic landscape of SARS-CoV-2 leading to universal blueprints for vaccine designs",2020,"","","","",119,"2022-07-13 09:19:07","","10.1038/s41598-020-78758-5","","",,,,,29,14.50,3,11,2,"","",""
0,"N. Vinson, Heather Molyneaux, Joel D. Martin","Explanations in Artificial Intelligence Decision Making",2019,"","","","",120,"2022-07-13 09:19:07","","10.4018/978-1-5225-9069-9.CH006","","",,,,,0,0.00,0,3,3,"The opacity of AI systems' decision making has led to calls to modify these systems so they can provide explanations for their decisions. This chapter contains a discussion of what these explanations should address and what their nature should be to meet the concerns that have been raised and to prove satisfactory to users. More specifically, the chapter briefly reviews the typical forms of AI decision-making that are currently used to make real-world decisions affecting people's lives. Based on concerns about AI decision making expressed in the literature and the media, the chapter follows with principles that the systems should respect and corresponding requirements for explanations to respect those principles. A mapping between those explanation requirements and the types of explanations generated by AI decision making systems reveals the strengths and shortcomings of the explanations generated by those systems.","",""
20,"L. McCoy, Sujay Nagaraj, F. Morgado, V. Harish, Sunit Das, L. Celi","What do medical students actually need to know about artificial intelligence?",2020,"","","","",121,"2022-07-13 09:19:07","","10.1038/s41746-020-0294-7","","",,,,,20,10.00,3,6,2,"","",""
0,"Julio Torres-Tello, Seok-Bum Ko","Interpretability of artificial intelligence models that use data fusion to predict yield in aeroponics",2021,"","","","",122,"2022-07-13 09:19:07","","10.1007/s12652-021-03470-9","","",,,,,0,0.00,0,2,1,"","",""
0,"T. Piotrowski, J. Kazmierska, M. Mocydlarz-Adamcewicz, A. Ryczkowski","Ethical issues on artificial intelligence in radiology: how is it reported in research articles? The current state and future directions",2021,"","","","",123,"2022-07-13 09:19:07","","10.20883/medical.e513","","",,,,,0,0.00,0,4,1,"Background. This paper evaluates the status of reporting information related to the usage and ethical issues of artificial intelligence (AI) procedures in clinical trial (CT) papers focussed on radiology issues as well as other (non-trial) original radiology articles (OA). Material and Methods. The evaluation was performed by three independent observers who were, respectively physicist, physician and computer scientist. The analysis was performed for two groups of publications, i.e., for CT and OA. Each group included 30 papers published from 2018 to 2020, published before guidelines proposed by Liu et al. (Nat Med. 2020; 26:1364-1374). The set of items used to catalogue and to verify the ethical status of the AI reporting was developed using the above-mentioned guidelines. Results. Most of the reviewed studies, clearly stated their use of AI methods and more importantly, almost all tried to address relevant clinical questions. Although in most of the studies, patient inclusion and exclusion criteria were presented, the widespread lack of rigorous descriptions of the study design apart from a detailed explanation of the AI approach itself is noticeable. Few of the chosen studies provided information about anonymization of data and the process of secure data sharing. Only a few studies explore the patterns of incorrect predictions by the proposed AI tools and their possible reasons. Conclusion. Results of review support idea of implementation of uniform guidelines for designing and reporting studies with use of AI tools. Such guidelines help to design robust, transparent and reproducible tools for use in real life.","",""
0,"Abdulraqeb Alhammadi, Ayman A. El-Saleh, Ibraheem Shayea","MOS Prediction for Mobile Broadband Networks Using Bayesian Artificial Intelligence",2021,"","","","",124,"2022-07-13 09:19:07","","10.1109/ICAICST53116.2021.9497834","","",,,,,0,0.00,0,3,1,"Mobile broadband (MBB) networks are growing fast with supporting high-speed internet access. Fifth-generation networks promise an enhanced MBB that offers a high-speed data rate and video streaming with ultra-low latency. Thus, monitoring the level quality of these services supported by network providers becomes essential. Mobile network operators continuously optimize their network performance to provide a better quality of service and quality of experience. Moreover, artificial intelligence has been used considerably in optimizations to efficiently meet the requirements of future mobile networks. In this paper, we propose a Bayesian network model to predict the minimum opinion score (MOS), which contributes to evaluating the network performance of video streaming services. The proposed model depends on several input data, namely, bite rate, stalling load, and round-trip time. The predicted MOS depends on prior probability distributions to generate posterior probabilities. The predicted MOS depends on these input data. Results demonstrate that the proposed model achieves a high prediction accuracy of 86%, with a mean square error of 0.34. The proposed model also has a robust performance design through various testing methods.","",""
0,"A. Auerbach, S. Fihn","Discovery, Learning, and Experimentation With Artificial Intelligence-Based Tools at the Point of Care-Perils and Opportunity.",2021,"","","","",125,"2022-07-13 09:19:07","","10.1001/jamanetworkopen.2021.1474","","",,,,,0,0.00,0,2,1,"Our ability to gain insights about diseases, treatments, and how we practice (or how we should practice) medicine using data is growing at an accelerating rate and is a key precondition for health systems and the accurate delivery of precision medicine.1-3 The use of real-world data to direct decisions during the conduct of clinical care is viewed as a critical advance. However, it is unclear whether or how new approaches using advanced analytics, such as machine learning or artificial intelligence (AI), are exempt from well-known and long-standing challenges affecting clinical decision support systems (CDSSs) more generally. Vasey and colleagues4 conducted a systematic review of studies that compared clinicians’ diagnostic performance in interpreting imaging with and without the assistance of CDSS interventions based on machine learning.5 Of the 37 studies evaluated, most pertained to diagnosis of cancer or lung pathology and more than half assessed commercial CDSS technology. The authors found no robust evidence that clinicians made better diagnostic decisions when provided with support from machine-learning algorithms, suggesting that AI-based CDSSs are far from achieving their potential. This finding is similar for essentially all forms of AI-based decision support systems that have been evaluated to date. One explanation is that data amassed as part of providing usual care are, by their nature, messy and complex.1 Observational data are invariably deeply confounded in ways that will produce misleading associations because of selection bias, confounding by indication, immortal time bias, and observation-time bias. Additionally, data in electronic health records are entered by busy people with varying priorities on the information that they enter into the records and where, when, and how they enter that information. In clinical care, lack of documentation of a diagnosis, problem, finding, communication, or even an action does not constitute evidence that it did not exist or occur. The computer only knows what it is told—missing data and the meaning of gaps are informative but often are unknowable and may lead to further imprecision in AI-based CDSSs. A CDSS based on algorithms that are based on confounded or incomplete data will be confounded and provide inaccurate guidance. Even with big numbers and advanced modeling algorithms, predicting rare events is a fundamentally tough business, particularly when trying to predict a patient’s outcome or risks. All too often, investigators and those building AI-based CDSSs tout use of data from massive numbers of patients even as the actual event rate is low (eg, only 1 or 2%), which means that even when the area under the curve is as high as 0.9, the positive predictive value may not exceed 0.1, and the resulting false alarms will greatly outnumber true signals.6 Moreover, there is no agreement on the best metrics to categorize the performance of predictive algorithms. Metrics have included area under the receiver operating curve, confusion matrix, accuracy, positive predictive value/precision, recall/ sensitivity, precision-recall curve, specificity, negative predictive value, F1 score, number needed to screen, likelihood ratio, net reclassification index, stratification tables, and calibration curves. In addition, model performance inevitably degrades over time (because of population drift or more commonly, alterations in the underlying data structure) and when applied to data emanating from a different system or setting. Many experts now advocate not only that models be tested on multiple independent data sets, but that testing be conducted by independent investigators.6 + Related article","",""
0,"Xiaohong W. Gao, B. Braden","Artificial intelligence in endoscopy: The challenges and future directions",2021,"","","","",126,"2022-07-13 09:19:07","","10.37126/aige.v2.i4.117","","",,,,,0,0.00,0,2,1,"Artificial intelligence based approaches, in particular deep learning, have achieved state-of-the-art performance in medical fields with increasing number of software systems being approved by both Europe and United States. This paper reviews their applications to early detection of oesophageal cancers with a focus on their advantages and pitfalls. The paper concludes with future recommendations towards the development of a real-time, clinical implementable, interpretable and robust diagnosis support systems.","",""
0,"Mir Riyanul Islam, Mobyen Uddin Ahmed, S. Begum","Local and Global Interpretability Using Mutual Information in Explainable Artificial Intelligence",2021,"","","","",127,"2022-07-13 09:19:07","","10.1109/ISCMI53840.2021.9654898","","",,,,,0,0.00,0,3,1,"Numerous studies have exploited the potential of Artificial Intelligence (AI) and Machine Learning (ML) models to develop intelligent systems in diverse domains for complex tasks, such as analysing data, extracting features, prediction, recommendation etc. However, presently these systems embrace acceptability issues from the end-users. The models deployed at the back of the systems mostly analyse the correlations or dependencies between the input and output to uncover the important characteristics of the input features, but they lack explainability and interpretability that causing the acceptability issues of intelligent systems and raising the research domain of eXplainable Artificial Intelligence (XAI). In this study, to overcome these shortcomings, a hybrid XAI approach is developed to explain an AI/ML model’s inference mechanism as well as the final outcome. The overall approach comprises of 1) a convolutional encoder that extracts deep features from the data and computes their relevancy with features extracted using domain knowledge, 2) a model for classifying data points using the features from autoencoder, and 3) a process of explaining the model’s working procedure and decisions using mutual information to provide global and local interpretability. To demonstrate and validate the proposed approach, experimentation was performed using an electroencephalography dataset from road safety to classify drivers’ in-vehicle mental workload. The outcome of the experiment was found to be promising that produced a Support Vector Machine classifier for mental workload with approximately 89% performance accuracy. Moreover, the proposed approach can also provide an explanation for the classifier model’s behaviour and decisions with the combined illustration of Shapely values and mutual information.","",""
0,"R. Sheh","Explainable Artificial Intelligence Requirements for Safe, Intelligent Robots",2021,"","","","",128,"2022-07-13 09:19:07","","10.1109/ISR50024.2021.9419498","","",,,,,0,0.00,0,1,1,"While requirements for robot performance to perform a task are generally well understood, the requirements around the explanatory capabilities of these systems are often at best an afterthought. This results in a dangerous situation where neither users nor experts can predict situations where the robot will or will not work, nor understand what causes failures and unexpected behaviour. In this paper, we discuss and survey the field of Explainable Artificial Intelligence, as it relates to the generation of requirements for the development of safe, intelligent robots. We then present a categorisation of explanatory capabilities and requirements that aims to help users and developers alike to ensure an appropriate match between the types of explanations that a given application requires, and the capabilities of various underlying AI techniques.","",""
28,"H. Alami, L. Rivard, P. Lehoux, S. Hoffman, Stephanie B. M. Cadeddu, Mathilde Savoldelli, M. A. Samri, M. A. Ag Ahmed, R. Fleet, J. Fortin","Artificial intelligence in health care: laying the Foundation for Responsible, sustainable, and inclusive innovation in low- and middle-income countries",2020,"","","","",129,"2022-07-13 09:19:07","","10.1186/s12992-020-00584-1","","",,,,,28,14.00,3,10,2,"","",""
24,"P. Iftikhar, Marcela Kuijpers, Azadeh Khayyat, Aqsa Iftikhar, Maribel DeGouvia De Sa","Artificial Intelligence: A New Paradigm in Obstetrics and Gynecology Research and Clinical Practice",2020,"","","","",130,"2022-07-13 09:19:07","","10.7759/cureus.7124","","",,,,,24,12.00,5,5,2,"Artificial intelligence (AI) is growing exponentially in various fields, including medicine. This paper reviews the pertinent aspects of AI in obstetrics and gynecology (OB/GYN) and how these can be applied to improve patient outcomes and reduce the healthcare costs and workload for clinicians. Herein, we will address current AI uses in OB/GYN, and the use of AI as a tool to interpret fetal heart rate (FHR) and cardiotocography (CTG) to aid in the detection of preterm labor, pregnancy complications, and review discrepancies in its interpretation between clinicians to reduce maternal and infant morbidity and mortality. AI systems can be used as tools to create algorithms identifying asymptomatic women with short cervical length who are at risk of preterm birth. Additionally, the benefits of using the vast data capacity of AI storage can assist in determining the risk factors for preterm labor using multiomics and extensive genomic data. In the field of gynecological surgery, the use of augmented reality helps surgeons detect vital structures, thus decreasing complications, reducing operative time, and helping surgeons in training to practice in a realistic setting. Using three-dimensional (3D) printers can provide materials that mimic real tissues and also helps trainees to practice on a realistic model. Furthermore, 3D imaging allows better depth perception than its two-dimensional (2D) counterpart, allowing the surgeon to create preoperative plans according to tissue depth and dimensions. Although AI has some limitations, this new technology can improve the prognosis and management of patients, reduce healthcare costs, and help OB/GYN practitioners to reduce their workload and increase their efficiency and accuracy by incorporating AI systems into their daily practice. AI has the potential to guide practitioners in decision-making, reaching a diagnosis, and improving case management. It can reduce healthcare costs by decreasing medical errors and providing more dependable predictions. AI systems can accurately provide information on the large array of patients in clinical settings, although more robust data is required.","",""
27,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases",2020,"","","","",131,"2022-07-13 09:19:07","","10.1038/s41746-020-0229-3","","",,,,,27,13.50,5,6,2,"","",""
23,"M. Rohaim, E. Clayton, I. Sahin, J. Vilela, M. Khalifa, M. Al-Natour, M. Bayoumi, A. Poirier, M. Branavan, M. Tharmakulasingam, N. S. Chaudhry, R. Sodi, A. Brown, P. Burkhart, W. Hacking, J. Botham, J. Boyce, H. Wilkinson, Craig Williams, Jayde Whittingham-Dowd, E. Shaw, Matt D. Hodges, L. Butler, M. Bates, R. L. La Ragione, W. Balachandran, A. Fernando, M. Munir","Artificial Intelligence-Assisted Loop Mediated Isothermal Amplification (AI-LAMP) for Rapid Detection of SARS-CoV-2",2020,"","","","",132,"2022-07-13 09:19:07","","10.3390/v12090972","","",,,,,23,11.50,2,28,2,"Until vaccines and effective therapeutics become available, the practical solution to transit safely out of the current coronavirus disease 19 (CoVID-19) lockdown may include the implementation of an effective testing, tracing and tracking system. However, this requires a reliable and clinically validated diagnostic platform for the sensitive and specific identification of SARS-CoV-2. Here, we report on the development of a de novo, high-resolution and comparative genomics guided reverse-transcribed loop-mediated isothermal amplification (LAMP) assay. To further enhance the assay performance and to remove any subjectivity associated with operator interpretation of results, we engineered a novel hand-held smart diagnostic device. The robust diagnostic device was further furnished with automated image acquisition and processing algorithms and the collated data was processed through artificial intelligence (AI) pipelines to further reduce the assay run time and the subjectivity of the colorimetric LAMP detection. This advanced AI algorithm-implemented LAMP (ai-LAMP) assay, targeting the RNA-dependent RNA polymerase gene, showed high analytical sensitivity and specificity for SARS-CoV-2. A total of ~200 coronavirus disease (CoVID-19)-suspected NHS patient samples were tested using the platform and it was shown to be reliable, highly specific and significantly more sensitive than the current gold standard qRT-PCR. Therefore, this system could provide an efficient and cost-effective platform to detect SARS-CoV-2 in resource-limited laboratories.","",""
25,"D. Schiff","Out of the laboratory and into the classroom: the future of artificial intelligence in education",2020,"","","","",133,"2022-07-13 09:19:07","","10.1007/s00146-020-01033-8","","",,,,,25,12.50,25,1,2,"","",""
75,"Qing Sun, Min Zhang, A. Mujumdar","Recent developments of artificial intelligence in drying of fresh food: A review",2019,"","","","",134,"2022-07-13 09:19:07","","10.1080/10408398.2018.1446900","","",,,,,75,25.00,25,3,3,"ABSTRACT Intellectualization is an important direction of drying development and artificial intelligence (AI) technologies have been widely used to solve problems of nonlinear function approximation, pattern detection, data interpretation, optimization, simulation, diagnosis, control, data sorting, clustering, and noise reduction in different food drying technologies due to the advantages of self-learning ability, adaptive ability, strong fault tolerance and high degree robustness to map the nonlinear structures of arbitrarily complex and dynamic phenomena. This article presents a comprehensive review on intelligent drying technologies and their applications. The paper starts with the introduction of basic theoretical knowledge of ANN, fuzzy logic and expert system. Then, we summarize the AI application of modeling, predicting, and optimization of heat and mass transfer, thermodynamic performance parameters, and quality indicators as well as physiochemical properties of dried products in artificial biomimetic technology (electronic nose, computer vision) and different conventional drying technologies. Furthermore, opportunities and limitations of AI technique in drying are also outlined to provide more ideas for researchers in this area.","",""
62,"Feisheng Zhong, Jing Xing, Xutong Li, Xiaohong Liu, Zunyun Fu, Zhaoping Xiong, Dong Lu, Xiaolong Wu, Jihui Zhao, Xiaoqin Tan, Fei Li, Xiaomin Luo, Zhaojun Li, Kaixian Chen, M. Zheng, Hualiang Jiang","Artificial intelligence in drug design",2018,"","","","",135,"2022-07-13 09:19:07","","10.1007/s11427-018-9342-2","","",,,,,62,15.50,6,16,4,"","",""
96,"Eduardo H. B. Maia, L. Assis, Tiago Alves de Oliveira, Alisson Marques da Silva, A. Taranto","Structure-Based Virtual Screening: From Classical to Artificial Intelligence",2020,"","","","",136,"2022-07-13 09:19:07","","10.3389/fchem.2020.00343","","",,,,,96,48.00,19,5,2,"The drug development process is a major challenge in the pharmaceutical industry since it takes a substantial amount of time and money to move through all the phases of developing of a new drug. One extensively used method to minimize the cost and time for the drug development process is computer-aided drug design (CADD). CADD allows better focusing on experiments, which can reduce the time and cost involved in researching new drugs. In this context, structure-based virtual screening (SBVS) is robust and useful and is one of the most promising in silico techniques for drug design. SBVS attempts to predict the best interaction mode between two molecules to form a stable complex, and it uses scoring functions to estimate the force of non-covalent interactions between a ligand and molecular target. Thus, scoring functions are the main reason for the success or failure of SBVS software. Many software programs are used to perform SBVS, and since they use different algorithms, it is possible to obtain different results from different software using the same input. In the last decade, a new technique of SBVS called consensus virtual screening (CVS) has been used in some studies to increase the accuracy of SBVS and to reduce the false positives obtained in these experiments. An indispensable condition to be able to utilize SBVS is the availability of a 3D structure of the target protein. Some virtual databases, such as the Protein Data Bank, have been created to store the 3D structures of molecules. However, sometimes it is not possible to experimentally obtain the 3D structure. In this situation, the homology modeling methodology allows the prediction of the 3D structure of a protein from its amino acid sequence. This review presents an overview of the challenges involved in the use of CADD to perform SBVS, the areas where CADD tools support SBVS, a comparison between the most commonly used tools, and the techniques currently used in an attempt to reduce the time and cost in the drug development process. Finally, the final considerations demonstrate the importance of using SBVS in the drug development process.","",""
4,"Shivam Mehta, Y. Suhail, J. Nelson, M. Upadhyay","Artificial Intelligence for radiographic image analysis",2021,"","","","",137,"2022-07-13 09:19:07","","10.1053/J.SODO.2021.05.007","","",,,,,4,4.00,1,4,1,"Abstract Automated identification of landmarks on lateral cephalogram and cone-beam computed tomography (CBCT) scans can save time for the clinicians and act as a second set of eyes for analysis of radiographic images in diagnosis and treatment planning. Several machine-learning techniques have been utilized for this purpose with varying accuracies. However, high degree of variability in the clinical presentation of orthodontic patients, limitations of the algorithms, lack of labelled data, high compute power, etc. are some drawbacks that have limited robust clinical application of such techniques. In recent years, artificial neural networks like deep learning and more specifically deep neural networks are making significant inroads in the true adoption of this technology. YOLOv3 and Single Shot Multibox Detector are some of the deep learning algorithms that have shown promising results. This paper is a theoretical review of the evolution of these technologies and the current state of the art in orthodontic image analysis.","",""
19,"Sandip K. Patel, Bhawana George, Vineeta Rai","Artificial Intelligence to Decode Cancer Mechanism: Beyond Patient Stratification for Precision Oncology",2020,"","","","",138,"2022-07-13 09:19:07","","10.3389/fphar.2020.01177","","",,,,,19,9.50,6,3,2,"The multitude of multi-omics data generated cost-effectively using advanced high-throughput technologies has imposed challenging domain for research in Artificial Intelligence (AI). Data curation poses a significant challenge as different parameters, instruments, and sample preparations approaches are employed for generating these big data sets. AI could reduce the fuzziness and randomness in data handling and build a platform for the data ecosystem, and thus serve as the primary choice for data mining and big data analysis to make informed decisions. However, AI implication remains intricate for researchers/clinicians lacking specific training in computational tools and informatics. Cancer is a major cause of death worldwide, accounting for an estimated 9.6 million deaths in 2018. Certain cancers, such as pancreatic and gastric cancers, are detected only after they have reached their advanced stages with frequent relapses. Cancer is one of the most complex diseases affecting a range of organs with diverse disease progression mechanisms and the effectors ranging from gene-epigenetics to a wide array of metabolites. Hence a comprehensive study, including genomics, epi-genomics, transcriptomics, proteomics, and metabolomics, along with the medical/mass-spectrometry imaging, patient clinical history, treatments provided, genetics, and disease endemicity, is essential. Cancer Moonshot℠ Research Initiatives by NIH National Cancer Institute aims to collect as much information as possible from different regions of the world and make a cancer data repository. AI could play an immense role in (a) analysis of complex and heterogeneous data sets (multi-omics and/or inter-omics), (b) data integration to provide a holistic disease molecular mechanism, (c) identification of diagnostic and prognostic markers, and (d) monitor patient’s response to drugs/treatments and recovery. AI enables precision disease management well beyond the prevalent disease stratification patterns, such as differential expression and supervised classification. This review highlights critical advances and challenges in omics data analysis, dealing with data variability from lab-to-lab, and data integration. We also describe methods used in data mining and AI methods to obtain robust results for precision medicine from “big” data. In the future, AI could be expanded to achieve ground-breaking progress in disease management.","",""
19,"E. I. Fernandez, André Satoshi Ferreira, M. Cecílio, D. S. Chéles, Rebeca Colauto Milanezi de Souza, M. Nogueira, J. C. Rocha","Artificial intelligence in the IVF laboratory: overview through the application of different types of algorithms for the classification of reproductive data",2020,"","","","",139,"2022-07-13 09:19:07","","10.1007/s10815-020-01881-9","","",,,,,19,9.50,3,7,2,"","",""
44,"Chiara Longoni, Luca Cian","Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect",2020,"","","","",140,"2022-07-13 09:19:07","","10.1177/0022242920957347","","",,,,,44,22.00,22,2,2,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).","",""
18,"Ahmed Gowida, Salaheldin Elkatatny, Saad F. K. Al-Afnan, A. Abdulraheem","New Computational Artificial Intelligence Models for Generating Synthetic Formation Bulk Density Logs While Drilling",2020,"","","","",141,"2022-07-13 09:19:07","","10.3390/su12020686","","",,,,,18,9.00,5,4,2,"Synthetic well log generation using artificial intelligence tools is a robust solution for situations in which logging data are not available or are partially lost. Formation bulk density (RHOB) logging data greatly assist in identifying downhole formations. These data are measured in the field while drilling by using a density log tool in the form of either a logging while drilling (LWD) technique or (more often) by wireline logging after the formations are drilled. This is due to operational limitations during the drilling process. Therefore, the objective of this study was to develop a predictive tool for estimating RHOB while drilling using an adaptive network-based fuzzy interference system (ANFIS), functional network (FN), and support vector machine (SVM). The proposed model uses the mechanical drilling constraints as feeding input parameters, and the conventional RHOB log data as an output parameter. These mechanical drilling parameters are usually measured while drilling, and their responses vary with different formations. A dataset of 2400 actual datapoints, obtained from a horizontal well in the Middle East, were used to build the proposed models. The obtained dataset was divided into a 70/30 ratio for model training and testing, respectively. The optimized ANFIS-based model outperformed the FN- and SVM-based models with a correlation coefficient (R) of 0.93, and average absolute percentage error (AAPE) of 0.81% between the predicted and measured RHOB values. These results demonstrate the reliability of the developed ANFIS model for predicting RHOB while drilling, based on the mechanical drilling parameters. Subsequently, the ANFIS-based model was validated using unseen data from another well within the same field. The validation process yielded an AAPE of 0.97% between the predicted and actual RHOB values, which confirmed the robustness of the developed model as an effective predictive tool for RHOB.","",""
16,"J. Lobera, Carlos J. Fernández Rodríguez, C. Torres-Albero","Privacy, Values and Machines: Predicting Opposition to Artificial Intelligence",2020,"","","","",142,"2022-07-13 09:19:07","","10.1080/10510974.2020.1736114","","",,,,,16,8.00,5,3,2,"ABSTRACT In this study we identify, for the first time, social determinants of opposition to artificial intelligence, based on the assessment of its benefits and risks. Using a national survey in Spain (n = 5200) and linear regression models, we show that common explanations regarding opposition to artificial intelligence, such as competition and relative vulnerability theories, are not confirmed or have limited explanatory power. Stronger effects are shown by social values and general attitudes to science. Those expressing egalitarian values and privacy concerns, as well as those less predisposed to innovation in a general sense, are more prone to oppose both technological applications. Lastly, we found evidence that, as in other complex technological applications, a new cognitive shortcut is produced. In this case, we found a strong correlation (0.652, p < .001) between public attitudes toward robotization in the workplace and toward artificial intelligence. We discuss the implications of this new cognitive schema, the “intelligent machine”, as a new threatening or beneficial element.","",""
1,"Andrea Torcianti, S. Matzka","Explainable Artificial Intelligence for Predictive Maintenance Applications using a Local Surrogate Model",2021,"","","","",143,"2022-07-13 09:19:07","","10.1109/AI4I51902.2021.00029","","",,,,,1,1.00,1,2,1,"This paper provides an explanatory interface using Local Interpretable Model-agnostic Explanations (LIME) for a predictive maintenance dataset. The explanations are evaluated and the explanatory quality of the model is compared to two previous explainable models for the same dataset.","",""
15,"L. Musikanski, B. Rakova, J. Bradbury, R. Phillips, M. Manson","Artificial Intelligence and Community Well-being: A Proposal for an Emerging Area of Research",2020,"","","","",144,"2022-07-13 09:19:07","","10.1007/s42413-019-00054-6","","",,,,,15,7.50,3,5,2,"","",""
30,"L. Longo, R. Goebel, F. Lécué, Peter Kieseberg, Andreas Holzinger","Explainable Artificial Intelligence: Concepts, Applications, Research Challenges and Visions",2020,"","","","",145,"2022-07-13 09:19:07","","10.1007/978-3-030-57321-8_1","","",,,,,30,15.00,6,5,2,"","",""
0,"E. Cambouropoulos, Maximos A. Kaliakatsos-Papakostas","Cognitive Musicology and Artificial Intelligence: Harmonic Analysis, Learning, and Generation",2021,"","","","",146,"2022-07-13 09:19:07","","10.1007/978-3-030-72116-9_10","","",,,,,0,0.00,0,2,1,"","",""
0,"Keeley A. Crockett, Edwin Colyer, A. Latham","The Ethical Landscape of Data and Artificial Intelligence: Citizen Perspectives",2021,"","","","",147,"2022-07-13 09:19:07","","10.1109/SSCI50451.2021.9660153","","",,,,,0,0.00,0,3,1,"Globally, there is growing acknowledgement that those involved in the development and deployment of AI products and services should act responsibly and conduct their work within robust ethical frameworks. Many of the ethical guidelines now published highlight a requirement for citizens to have greater voice and involvement in this process and to hold actors to account regarding compliance and the impacts of their AI innovations. For citizens to participate in co-creation activities they need to be representative of the diverse communities of society and have an appropriate level of understanding of basic AI concepts. This paper presents the preliminary results of a longitudinal survey designed to capture citizen perspectives of the ethical landscape of data and AI. Forty participants were asked to participate in a survey and results were analyzed based on gender, age range and educational attainment. Results have shown that participant perception of AI, trust, bias and fairness is different but related to specific AI applications, and the context in which is applied. Citizens also are also very receptive to undertaking free courses/workshops on a wide range of AI concepts, ranging from family workshops to work-based training.","",""
0,"Lucas Mendes Lima, Victor Calebe Cavalcante, Mariana Guimarães de Sousa, Cláudio Afonso Fleury, D. Oliveira, Eduardo Noronha de Andrade Freitas","Artificial Intelligence in Support of Welfare Monitoring of Dairy Cattle: A Systematic Literature Review",2021,"","","","",148,"2022-07-13 09:19:07","","10.1109/CSCI54926.2021.00324","","",,,,,0,0.00,0,6,1,"Context: Although agribusiness corresponded to more than 20% of Brazil’s Gross Domestic Product (GDP), most livestock is under manual control and manual monitoring. Additionally, alternative technologies are either uncomfortable and stressful, or expensive. Now, despite the great scientific advances in the area, there is still a pressing need for an automated robust, inexpensive and (sub)optimal technology to monitor animal behavior in a cost-effective, contact-less and stress-free fashion. Overall, this niche can leverage the benefits of Deep Learning schemes.Objective: This review aims to provide a systematic overview of most current projects in the area of comfort monitoring dairy cattle, as well as their corresponding image recognition-based techniques and technologies.Methods: First, a systematic review planning was carried out, and objectives, research questions, search strings, among others, were defined. Subsequently,a broad survey was conducted to extract, analyze and compile the data, to generate a easy-to-read visual source of information (tables and graphics).Results: Information was extracted from the reviewed papers. Among this data collected from the papers are techniques utilized, target behaviors, cow bodyparts identified in visual computational, besides their paper source font, the publication date, and localization. For example, the papers present are mostly recent. China has had a larger number of relevant papers in the area. The back was the body region most analyzed by the papers and the behaviors most analyzed were body condition score, lameness, cow’s body position and feeding/drinking behavior. Among the methods used is RCNN Inception V3 with the best accuracy for cow’s back region.Conclusion: The aim of this work is to present some of the papers that are being carried out in the area of dairy cow behavior monitoring, using techniques of Artifical Intelligence. It is expected that the information collected and presented in the present systematic review paper contribute to the future researches and projects of the area and the application of new techniques.","",""
14,"Claudia Gonzalez Viejo, S. Fuentes","Beer Aroma and Quality Traits Assessment Using Artificial Intelligence",2020,"","","","",149,"2022-07-13 09:19:07","","10.3390/fermentation6020056","","",,,,,14,7.00,7,2,2,"Increasing beer quality demands from consumers have put pressure on brewers to target specific steps within the beer-making process to modify beer styles and quality traits. However, this demands more robust methodologies to assess the final aroma profiles and physicochemical characteristics of beers. This research shows the construction of artificial intelligence (AI) models based on aroma profiles, chemometrics, and chemical fingerprinting using near-infrared spectroscopy (NIR) obtained from 20 commercial beers used as targets. Results showed that machine learning models obtained using NIR from beers as inputs were accurate and robust in the prediction of six important aromas for beer (Model 1; R = 0.91; b = 0.87) and chemometrics (Model 2; R = 0.93; b = 0.90). Additionally, two more accurate models were obtained from robotics (RoboBEER) to obtain the same aroma profiles (Model 3; R = 0.99; b = 1.00) and chemometrics (Model 4; R = 0.98; b = 1.00). Low-cost robotics and sensors coupled with computer vision and machine learning modeling could help brewers in the decision-making process to target specific consumer preferences and to secure higher consumer demands.","",""
14,"M. Yazdani-Asrami, Mehran Taghipour-Gorjikolaie, Wenjuan Song, Min Zhang, W. Yuan","Prediction of Nonsinusoidal AC Loss of Superconducting Tapes Using Artificial Intelligence-Based Models",2020,"","","","",150,"2022-07-13 09:19:07","","10.1109/ACCESS.2020.3037685","","",,,,,14,7.00,3,5,2,"Current is no longer sinusoidal in modern electric networks because of widespread use of power electronic-based equipments and nonlinear loads. Usually AC loss is calculated for pure sinusoidal current, while it is no longer accurate when current is nonsinusoidal. On the other hand, efficiency of cooling system in large scale power devices is dependent on accurate estimation and prediction of the heat load caused by AC loss in design stage. Therefore, estimation of nonsinusoidal AC loss of high temperature superconducting (HTS) material would be of great interest for designers of large-scale superconducting devices. In this paper, at first nonsinusoidal AC loss of a typical HTS tape was calculated under distorted currents using H-formulation finite element method. Then, a range of artificial intelligence (AI) models were implemented to predict AC loss of a typical HTS tape. In order to find the best and more adaptive AI model for nonsinusoidal AC loss prediction, different regression models are evaluated using Support Vector Machine regression model, Generalized Linear regression model, Decision Tree regression model, Feed Forward Neural Network based model, Adaptive Neuro Fuzzy Inference System based model, and Radial Basis Function Neural Network (RBFNN) based model. In order to evaluate robustness of developed models cross-validation technique is implemented on experimental data. To compare the performance of different AI models, four prediction measures were used: Theil’s U coefficients (U_Accuracy and U_Quality), Root Mean Square Error (RMSE) and Regression value (R-value). Obtained results show that best performance belongs to RBFNN based model and then ANFIS based model. U coefficients and RMSE values are obtained less than 0.005 and R-Value is become close to one by using RBFNN based model for testing data, which indicates high accuracy prediction performance.","",""
14,"A. Burlacu, Adrian Iftene, Daniel Jugrin, I. Popa, Paula Madalina Lupu, C. Vlad, A. Covic","Using Artificial Intelligence Resources in Dialysis and Kidney Transplant Patients: A Literature Review",2020,"","","","",151,"2022-07-13 09:19:07","","10.1155/2020/9867872","","",,,,,14,7.00,2,7,2,"Background The purpose of this review is to depict current research and impact of artificial intelligence/machine learning (AI/ML) algorithms on dialysis and kidney transplantation. Published studies were presented from two points of view: What medical aspects were covered? What AI/ML algorithms have been used? Methods We searched four electronic databases or studies that used AI/ML in hemodialysis (HD), peritoneal dialysis (PD), and kidney transplantation (KT). Sixty-nine studies were split into three categories: AI/ML and HD, PD, and KT, respectively. We identified 43 trials in the first group, 8 in the second, and 18 in the third. Then, studies were classified according to the type of algorithm. Results AI and HD trials covered: (a) dialysis service management, (b) dialysis procedure, (c) anemia management, (d) hormonal/dietary issues, and (e) arteriovenous fistula assessment. PD studies were divided into (a) peritoneal technique issues, (b) infections, and (c) cardiovascular event prediction. AI in transplantation studies were allocated into (a) management systems (ML used as pretransplant organ-matching tools), (b) predicting graft rejection, (c) tacrolimus therapy modulation, and (d) dietary issues. Conclusions Although guidelines are reluctant to recommend AI implementation in daily practice, there is plenty of evidence that AI/ML algorithms can predict better than nephrologists: volumes, Kt/V, and hypotension or cardiovascular events during dialysis. Altogether, these trials report a robust impact of AI/ML on quality of life and survival in G5D/T patients. In the coming years, one would probably witness the emergence of AI/ML devices that facilitate the management of dialysis patients, thus increasing the quality of life and survival.","",""
14,"E. Kotter, E. Ranschaert","Challenges and solutions for introducing artificial intelligence (AI) in daily clinical workflow",2020,"","","","",152,"2022-07-13 09:19:07","","10.1007/s00330-020-07148-2","","",,,,,14,7.00,7,2,2,"","",""
27,"Omar Alshorman, Muhammad Irfan, N. Saad, D. Zhen, Noman Haider, A. Głowacz, Ahmad M. Alshorman","A Review of Artificial Intelligence Methods for Condition Monitoring and Fault Diagnosis of Rolling Element Bearings for Induction Motor",2020,"","","","",153,"2022-07-13 09:19:07","","10.1155/2020/8843759","","",,,,,27,13.50,4,7,2,"The fault detection and diagnosis (FDD) along with condition monitoring (CM) and of rotating machinery (RM) have critical importance for early diagnosis to prevent severe damage of infrastructure in industrial environments. Importantly, valuable industrial equipment needs continuous monitoring to enhance the safety, reliability, and availability and to decrease the cost of maintenance of modern industrial systems and applications. However, induction motor (IM) has been extensively used in several industrial processes because it is cheap, reliable, and robust. Rolling bearings are considered to be the main component of IM. Undoubtedly, any failure of this basic component can lead to a serious breakdown of IM and for whole industrial system. Thus, many current methods based on different techniques are employed as a fault prognosis and diagnosis of rolling elements bearing of IM. Moreover, these techniques include signal/image processing, intelligent diagnostics, data fusion, data mining, and expert systems for time and frequency as well as time-frequency domains. Artificial intelligence (AI) techniques have proven their significance in every field of digital technology. Industrial machines, automation, and processes are the net frontiers of AI adaptation. There are quite developed literatures that have been approaching the issues using signals and data processing techniques. However, the key contribution of this work is to present an extensive review of CM and FDD of the IM, especially for rolling elements bearings, based on artificial intelligent (AI) methods. This study highlights the advantages and performance limitations of each method. Finally, challenges and future trends are also highlighted.","",""
13,"P. Slomka, R. Miller, I. Išgum, D. Dey","Application and Translation of Artificial Intelligence to Cardiovascular Imaging in Nuclear Medicine and Noncontrast CT.",2020,"","","","",154,"2022-07-13 09:19:07","","10.1053/j.semnuclmed.2020.03.004","","",,,,,13,6.50,3,4,2,"Myocardial perfusion imaging with single photon emission computed tomography or positron emission tomography is commonly used for diagnosis and risk stratification in patients with known or suspected coronary artery disease. Current scanners often incorporate computed tomography for attenuation correction, resulting in a wealth of clinical and imaging information associated with a typical study. Novel highly efficient artificial intelligence (AI) tools have emerged, revolutionizing image analysis with direct and accurate extraction of information from cardiovascular images. These methods have accuracy similar or better to expert interpretation, without the need for timely manual adjustments or measurements. Additionally, artificial intelligence-based algorithms have been developed to integrate the large volume of clinical and imaging information to improve disease diagnosis and risk estimation. Lastly, explainable AI techniques are being developed, overcoming the traditional perception of AI as a ""black box"" by presenting the rationale for the computed decision or recommendation through attention maps and individualized explanations of risk estimates. In this review we focus on these applications of the latest AI tools in nuclear cardiology and non-contrast cardiac CT.","",""
11,"K. Mudgal, Neelanjan Das","The ethical adoption of artificial intelligence in radiology",2019,"","","","",155,"2022-07-13 09:19:07","","10.1259/bjro.20190020","","",,,,,11,3.67,6,2,3,"Artificial intelligence (AI) is rapidly transforming healthcare—with radiology at the pioneering forefront. To be trustfully adopted, AI needs to be lawful, ethical and robust. This article covers the different aspects of a safe and sustainable deployment of AI in radiology during: training, integration and regulation. For training, data must be appropriately valued, and deals with AI companies must be centralized. Companies must clearly define anonymization and consent, and patients must be well-informed about their data usage. Data fed into algorithms must be made AI-ready by refining, purification, digitization and centralization. Finally, data must represent various demographics. AI needs to be safely integrated with radiologists-in-the-loop: guiding forming concepts of AI solutions and supervising training and feedback. To be well-regulated, AI systems must be approved by a health authority and agreements must be made upon liability for errors, roles of supervised and unsupervised AI and fair workforce distribution (between AI and radiologists), with a renewal of policy at regular intervals. Any errors made must have a root-cause analysis, with outcomes fedback to companies to close the loop—thus enabling a dynamic best prediction system. In the distant future, AI may act autonomously with little human supervision. Ethical training and integration can ensure a ""transparent"" technology that will allow insight: helping us reflect on our current understanding of imaging interpretation and fill knowledge gaps, eventually moulding radiological practice. This article proposes recommendations for ethical practise that can guide a nationalized framework to build a sustainable and transparent system.","",""
11,"K. Hayward, M. Maas","Artificial intelligence and crime: A primer for criminologists",2020,"","","","",156,"2022-07-13 09:19:07","","10.1177/1741659020917434","","",,,,,11,5.50,6,2,2,"This article introduces the concept of Artificial Intelligence (AI) to a criminological audience. After a general review of the phenomenon (including brief explanations of important cognate fields such as ‘machine learning’, ‘deep learning’, and ‘reinforcement learning’), the paper then turns to the potential application of AI by criminals, including what we term here ‘crimes with AI’, ‘crimes against AI’, and ‘crimes by AI’. In these sections, our aim is to highlight AI’s potential as a criminogenic phenomenon, both in terms of scaling up existing crimes and facilitating new digital transgressions. In the third part of the article, we turn our attention to the main ways the AI paradigm is transforming policing, surveillance, and criminal justice practices via diffuse monitoring modalities based on prediction and prevention. Throughout the paper, we deploy an array of programmatic examples which, collectively, we hope will serve as a useful AI primer for criminologists interested in the ‘tech-crime nexus’.","",""
11,"S. Goto, K. Mahara, L. Beussink-Nelson, H. Ikura, Y. Katsumata, J. Endo, H. Gaggin, S. J. Shah, Y. Itabashi, C. Macrae, R. Deo","Artificial Intelligence-Enabled, Fully Automated Detection of Cardiac Amyloidosis Using Electrocardiograms and Echocardiograms.",2020,"","","","",157,"2022-07-13 09:19:07","","10.1101/2020.07.02.20141028","","",,,,,11,5.50,1,11,2,"Although individually uncommon, rare diseases collectively affect over 350 million patients worldwide and are increasingly the target of therapeutic development efforts. Unfortunately, the pursuit and use of such therapies have been hindered by a common challenge: patients with specific rare diseases are difficult to identify, especially if the conditions resemble more prevalent disorders. Cardiac amyloidosis is one such rare disease, which is characterized by deposition of misfolded proteins within the heart muscle resulting in heart failure and death. In recent years, specific therapies have emerged for cardiac amyloidosis and several more are under investigation, but because cardiac amyloidosis is mistaken for common forms of heart failure, it is typically diagnosed late in its course. As a possible solution, artificial intelligence methods could enable automated detection of rare diseases, but model performance must address low disease prevalence. Here we present an automated multi-modality pipeline for cardiac amyloidosis detection using two neural-network models; one using electrocardiograms (ECG) and the second using echocardiographic videos as input. These models were trained and validated on 3 and 5 academic medical centers (AMC), respectively, in the United States and Japan. Both models had excellent accuracy for detecting cardiac amyloidosis with C-statistics of 0.85-0.92 and 0.91-1.00 for the ECG and echocardiography models, respectively, with the latter outperforming expert diagnosis. Simulating deployment on 13,906 and 7775 patients with ECG-echocardiography paired data for AMC2 and AMC3 indicated a positive predictive value (PPV) for the ECG model of 4% and 3% at 61% and 54% recall, respectively. Pre-screening with ECG enhanced the echocardiography model performance from PPV 23% and 20% to PPV 58% and 53% at 64% recall, respectively for AMC2 and AMC3. In conclusion, we have developed a robust pipeline to augment detection of cardiac amyloidosis, which should serve as a generalizable strategy for other rare and intermediate frequency cardiac diseases with established or emerging therapies.","",""
18,"Prashan Madumal, Tim Miller, F. Vetere, L. Sonenberg","Towards a Grounded Dialog Model for Explainable Artificial Intelligence",2018,"","","","",158,"2022-07-13 09:19:07","","","","",,,,,18,4.50,5,4,4,"To generate trust with their users, Explainable Artificial Intelligence (XAI) systems need to include an explanation model that can communicate the internal decisions, behaviours and actions to the interacting humans. Successful explanation involves both cognitive and social processes. In this paper we focus on the challenge of meaningful interaction between an explainer and an explainee and investigate the structural aspects of an explanation in order to propose a human explanation dialog model. We follow a bottom-up approach to derive the model by analysing transcripts of 398 different explanation dialog types. We use grounded theory to code and identify key components of which an explanation dialog consists. We carry out further analysis to identify the relationships between components and sequences and cycles that occur in a dialog. We present a generalized state model obtained by the analysis and compare it with an existing conceptual dialog model of explanation.","",""
7,"S. Harmon, Palak G Patel, T. Sanford, Isabelle Caven, Rachael Iseman, T. Vidotto, C. Picanço, J. Squire, Samira Masoudi, Sherif Mehralivand, P. Choyke, D. Berman, B. Turkbey, T. Jamaspishvili","High throughput assessment of biomarkers in tissue microarrays using artificial intelligence: PTEN loss as a proof-of-principle in multi-center prostate cancer cohorts",2020,"","","","",159,"2022-07-13 09:19:07","","10.1038/s41379-020-00674-w","","",,,,,7,3.50,1,14,2,"","",""
8,"Jun Zhu, Hang Su, Bo Zhang","Toward the third generation of artificial intelligence",2020,"","","","",160,"2022-07-13 09:19:07","","10.1360/ssi-2020-0204","","",,,,,8,4.00,3,3,2,"There have been two competing paradigms of artificial intelligence (AI) development since 1956, i.e., symbolism and connectionism (or subsymbolism). Both started at the same time, but symbolism had dominated AI development until the end of the 1980s. Connectionism began to develop in the 1990s and reached its climax at the beginning of this century, and it is likely to displace symbolism. Today, it seems that the two paradigms only simulate the human mind (or brain) in different ways and have their own advantages. True human intelligence cannot be achieved by relying on only one paradigm. Both are necessary to establish a new, explainable, and robust AI theory and method and develop safe, trustworthy, reliable, and extensible AI technology. To this end, it is imperative to combine the two paradigms, and the present article will illustrate this idea. For the sake of description, symbolism, connectionism, and the newly developed paradigm are termed as first-, second-, and third-generation AIs.","",""
7,"Ashley Kras, L. Celi, John B. Miller","Accelerating ophthalmic artificial intelligence research: the role of an open access data repository.",2020,"","","","",161,"2022-07-13 09:19:07","","10.1097/ICU.0000000000000678","","",,,,,7,3.50,2,3,2,"PURPOSE OF REVIEW Artificial intelligence has already provided multiple clinically relevant applications in ophthalmology. Yet, the explosion of nonstandardized reporting of high-performing algorithms are rendered useless without robust and streamlined implementation guidelines. The development of protocols and checklists will accelerate the translation of research publications to impact on patient care.   RECENT FINDINGS Beyond technological scepticism, we lack uniformity in analysing algorithmic performance generalizability, and benchmarking impacts across clinical settings. No regulatory guardrails have been set to minimize bias or optimize interpretability; no consensus clinical acceptability thresholds or systematized postdeployment monitoring has been set. Moreover, stakeholders with misaligned incentives deepen the landscape complexity especially when it comes to the requisite data integration and harmonization to advance the field. Therefore, despite increasing algorithmic accuracy and commoditization, the infamous 'implementation gap' persists. Open clinical data repositories have been shown to rapidly accelerate research, minimize redundancies and disseminate the expertise and knowledge required to overcome existing barriers. Drawing upon the longstanding success of existing governance frameworks and robust data use and sharing agreements, the ophthalmic community has tremendous opportunity in ushering artificial intelligence into medicine. By collaboratively building a powerful resource of open, anonymized multimodal ophthalmic data, the next generation of clinicians can advance data-driven eye care in unprecedented ways.   SUMMARY This piece demonstrates that with readily accessible data, immense progress can be achieved clinically and methodologically to realize artificial intelligence's impact on clinical care. Exponentially progressive network effects can be seen by consolidating, curating and distributing data amongst both clinicians and data scientists.","",""
8,"I. Wiafe, F. N. Koranteng, Emmanuel Nyarko Obeng, Nana Assyne, Abigail Wiafe, S. Gulliver","Artificial Intelligence for Cybersecurity: A Systematic Mapping of Literature",2020,"","","","",162,"2022-07-13 09:19:07","","10.1109/ACCESS.2020.3013145","","",,,,,8,4.00,1,6,2,"Due to the ever-increasing complexities in cybercrimes, there is the need for cybersecurity methods to be more robust and intelligent. This will make defense mechanisms to be capable of making real-time decisions that can effectively respond to sophisticated attacks. To support this, both researchers and practitioners need to be familiar with current methods of ensuring cybersecurity (CyberSec). In particular, the use of artificial intelligence for combating cybercrimes. However, there is lack of summaries on artificial intelligent methods for combating cybercrimes. To address this knowledge gap, this study sampled 131 articles from two main scholarly databases (ACM digital library and IEEE Xplore). Using a systematic mapping, the articles were analyzed using quantitative and qualitative methods. It was observed that artificial intelligent methods have made remarkable contributions to combating cybercrimes with significant improvement in intrusion detection systems. It was also observed that there is a reduction in computational complexity, model training times and false alarms. However, there is a significant skewness within the domain. Most studies have focused on intrusion detection and prevention systems, and the most dominant technique used was support vector machines. The findings also revealed that majority of the studies were published in two journal outlets. It is therefore suggested that to enhance research in artificial intelligence for CyberSec, researchers need to adopt newer techniques and also publish in other related outlets.","",""
7,"Nariman Ammar, Arash Shaban-Nejad","Explainable Artificial Intelligence Recommendation System by Leveraging the Semantics of Adverse Childhood Experiences: Proof-of-Concept Prototype Development",2020,"","","","",163,"2022-07-13 09:19:07","","10.2196/18752","","",,,,,7,3.50,4,2,2,"Background The study of adverse childhood experiences and their consequences has emerged over the past 20 years. Although the conclusions from these studies are available, the same is not true of the data. Accordingly, it is a complex problem to build a training set and develop machine-learning models from these studies. Classic machine learning and artificial intelligence techniques cannot provide a full scientific understanding of the inner workings of the underlying models. This raises credibility issues due to the lack of transparency and generalizability. Explainable artificial intelligence is an emerging approach for promoting credibility, accountability, and trust in mission-critical areas such as medicine by combining machine-learning approaches with explanatory techniques that explicitly show what the decision criteria are and why (or how) they have been established. Hence, thinking about how machine learning could benefit from knowledge graphs that combine “common sense” knowledge as well as semantic reasoning and causality models is a potential solution to this problem. Objective In this study, we aimed to leverage explainable artificial intelligence, and propose a proof-of-concept prototype for a knowledge-driven evidence-based recommendation system to improve mental health surveillance. Methods We used concepts from an ontology that we have developed to build and train a question-answering agent using the Google DialogFlow engine. In addition to the question-answering agent, the initial prototype includes knowledge graph generation and recommendation components that leverage third-party graph technology. Results To showcase the framework functionalities, we here present a prototype design and demonstrate the main features through four use case scenarios motivated by an initiative currently implemented at a children’s hospital in Memphis, Tennessee. Ongoing development of the prototype requires implementing an optimization algorithm of the recommendations, incorporating a privacy layer through a personal health library, and conducting a clinical trial to assess both usability and usefulness of the implementation. Conclusions This semantic-driven explainable artificial intelligence prototype can enhance health care practitioners’ ability to provide explanations for the decisions they make.","",""
6,"Oscar Serradilla, E. Zugasti, C. Cernuda, Andoitz Aranburu, Julian Ramirez de Okariz, Urko Zurutuza","Interpreting Remaining Useful Life estimations combining Explainable Artificial Intelligence and domain knowledge in industrial machinery",2020,"","","","",164,"2022-07-13 09:19:07","","10.1109/FUZZ48607.2020.9177537","","",,,,,6,3.00,1,6,2,"This paper presents the implementation and explanations of a remaining life estimator model based on machine learning, applied to industrial data. Concretely, the model has been applied to a bushings testbed, where fatigue life tests are performed to find more suitable bushing characteristics. Different regressors have been compared Environmental and Operational Condition and setting variables as input data to prognosticate the remaining life on each observation during fatigue tests, where final model is a Random Forest was chosen given its accuracy and explainability potential. The model creation, optimisation and interpretation has been guided combining eXplainable Artificial Intelligence with domain knowledge.Precisely, ELI5 and LIME explainable techniques have been used to perform local and global explanations. These were used to understand the relevance of predictor variables in individual and overall remaining life estimations. The achieved results have been process knowledge gain and expert knowledge validation, assertion of huge potential of data-driven models in industrial processes and highlight the need of collaboration between expert knowledge technicians and eXplainable Artificial Intelligence techniques to understand advanced machine learning models.","",""
2,"Dr. Uma Devi, Maria Tresita, V. Paul","Artificial Intelligence: Pertinence in Supply Chain and Logistics Management",2020,"","","","",165,"2022-07-13 09:19:07","","","","",,,,,2,1.00,1,3,2,"-Artificial Intelligence (AI) is the revolutionary invention of human intelligence. Artificial Intelligence is nothing but the duplication of human in which machines are programmed to rationally think and behave like humans developed for very many purposes including business decision making, problem-solving, business data analysis and interpretation and information management. The application of AI in business endeavours decides the competitive advantage, market leadership, robust operating efficiency of corporates and other business houses. Exploiting the application of AI in the manufacturing and distribution process enables the organisations to reach the pinnacle in their business graph. Businesses are operating in the international market which is highly multifaceted and challenging to serve the world as a sole market for their products, services and their products and without the integration of technology into their business processes, they cannot assure the sustainable growth. The management of the process of transforming the raw materials into the final product is called Supply Chain Management (SCM) and the effective movement and storage of goods, services and information are called Logistics Management (LM). This article analyses the applications of Artificial Intelligence in Supply Chain and Logistics Management (SC&LM) Keywords--Artificial Intelligence, Supply Chain Management, Logistics Management, Supply Chain Profitability","",""
5,"Cathy O'Neil, H. Gunn","Near-Term Artificial Intelligence and the Ethical Matrix",2020,"","","","",166,"2022-07-13 09:19:07","","10.1093/oso/9780190905033.003.0009","","",,,,,5,2.50,3,2,2,"This chapter takes up the issue of near-term artificial intelligence, or the algorithms that are already in place in a variety of public and private sectors, guiding decisions from advertising and to credit ratings to sentencing in the justice system. There is a pressing need to recognize and evaluate the ways that structural racism, sexism, classism, and ableism may be embedded in and amplified by these systems. The chapter proposes a framework for ethical analysis that can be used to facilitate more robust ethical reflection in AI development and implementation. It presents an ethical matrix that incorporates the language of data science as a tool that data scientists can build themselves in order to integrate ethical analysis into the design process, addressing the need for immediate analysis and accountability over the design and deployment of near-term AI.","",""
81,"Thomas G. Dietterich","Steps Toward Robust Artificial Intelligence",2017,"","","","",167,"2022-07-13 09:19:07","","10.1609/aimag.v38i3.2756","","",,,,,81,16.20,81,1,5,"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world","",""
80,"Tim Miller","Contrastive explanation: a structural-model approach",2018,"","","","",168,"2022-07-13 09:19:07","","10.1017/S0269888921000102","","",,,,,80,20.00,80,1,4,"Abstract This paper presents a model of contrastive explanation using structural casual models. The topic of causal explanation in artificial intelligence has gathered interest in recent years as researchers and practitioners aim to increase trust and understanding of intelligent decision-making. While different sub-fields of artificial intelligence have looked into this problem with a sub-field-specific view, there are few models that aim to capture explanation more generally. One general model is based on structural causal models. It defines an explanation as a fact that, if found to be true, would constitute an actual cause of a specific event. However, research in philosophy and social sciences shows that explanations are contrastive: that is, when people ask for an explanation of an event—the fact—they (sometimes implicitly) are asking for an explanation relative to some contrast case; that is, ‘Why P rather than Q?’. In this paper, we extend the structural causal model approach to define two complementary notions of contrastive explanation, and demonstrate them on two classical problems in artificial intelligence: classification and planning. We believe that this model can help researchers in subfields of artificial intelligence to better understand contrastive explanation.","",""
1,"Antti Nurminen, A. Malhi","Green Thumb Engineering: Artificial intelligence for managing IoT enabled houseplants",2020,"","","","",169,"2022-07-13 09:19:07","","10.1109/GCAIoT51063.2020.9345850","","",,,,,1,0.50,1,2,2,"Many hobbies and household activities can be carried out with little effort, and without deeper understanding of the phenomenon at hand. Managing houseplants and herbs is such a hobby; one can simply provide water for plants and hope they would prosper. However, some people seem to have a better luck in maintaining their plants, while many fail and claim they do not possess a “green thumb”. Fortunately, this does not require magic, but engineering. We use innovative Internet of Things and artificial intelligence technologies to monitor and analyze moisture, humidity, luminosity and temperature levels to assist end users for optimization of environmental conditions for their houseplants, simultaneously increasing their understanding of plant life. For plant health monitoring, we construct a system yielding the normalized difference vegetation index, supported by visual validation by users. We run the system for a selected plant, basil, in varying environmental conditions to cater for typical home conditions in pursue of bootstrapping our artificial intelligence with the acquired data. For end users, we implement a web based user interface which provides both instructions and explanations.","",""
0,"Katanosh Morovat, B. Panda","A Survey of Artificial Intelligence in Cybersecurity",2020,"","","","",170,"2022-07-13 09:19:07","","10.1109/CSCI51800.2020.00026","","",,,,,0,0.00,0,2,2,"During the last decades, not only the number of cyberattacks have increased significantly, they have also become more sophisticated. Hence designing a cyber-resilient approach is of paramount importance. Traditional security methods are not adequate to prevent data breaches in case of cyberattacks. Cybercriminals have learned how to use new techniques and robust tools to hack, attack, and breach data. Fortunately, Artificial Intelligence (AI) technologies have been introduced into cyberspace to construct smart models for defending systems from attacks. Since AI technologies can rapidly evolve to address complex situations, they can be used as fundamental tools in the field of cybersecurity. Al-based techniques can provide efficient and powerful cyber defense tools to recognize malware attacks, network intrusions, phishing and spam emails, and data breaches, to name a few, and to alert security incidents when they occur. In this paper, we review the impact of AI in cybersecurity and summarize existing research in terms of benefits of AI in cybersecurity.","",""
496,"Christopher J. Kelly, A. Karthikesalingam, Mustafa Suleyman, Greg Corrado, Dominic King","Key challenges for delivering clinical impact with artificial intelligence",2019,"","","","",171,"2022-07-13 09:19:07","","10.1186/s12916-019-1426-2","","",,,,,496,165.33,99,5,3,"","",""
14,"G. Coskuner, Majeed S Jassim, M. Zontul, Seda Karateke","Application of artificial intelligence neural network modeling to predict the generation of domestic, commercial and construction wastes",2020,"","","","",172,"2022-07-13 09:19:07","","10.1177/0734242X20935181","","",,,,,14,7.00,4,4,2,"Reliable prediction of municipal solid waste (MSW) generation rates is a significant element of planning and implementation of sustainable solid waste management strategies. In this study, the multi-layer perceptron artificial neural network (MLP-ANN) is applied to verify the prediction of annual generation rates of domestic, commercial and construction and demolition (C&D) wastes from the year 1997 to 2016 in Askar Landfill site in the Kingdom of Bahrain. The proposed robust predictive models incorporated selected explanatory variables to reflect the influence of social, demographical, economic, geographical and touristic factors upon waste generation rates (WGRs). The Mean Squared Error (MSE) and coefficient of determination (R2) are used as performance indicators to evaluate effectiveness of the developed models. MLP-ANN models exhibited strong accuracy in predictions with high R2 and low MSE values. The R2 values for domestic, commercial and C&D wastes are 0.95, 0.99 and 0.91, respectively. Our results show that the developed MLP-ANN models are effective for the prediction of WGRs from different sources and could be considered as a cost-effective approach for planning integrated MSW management systems.","",""
66,"Keping Yu, Zhiwei Guo, Yulian Shen, Wei Wang, Jerry Chun‐wei Lin, Takuro Sato","Secure Artificial Intelligence of Things for Implicit Group Recommendations",2021,"","","","",173,"2022-07-13 09:19:07","","10.1109/JIOT.2021.3079574","","",,,,,66,66.00,11,6,1,"The emergence of Artificial Intelligence of Things (AIoT) has provided novel insights for many social computing applications, such as group recommender systems. As the distances between people have been greatly shortened, there has been more general demand for the provision of personalized services aimed at groups instead of individuals. The existing methods for capturing group-level preference features from individuals have mostly been established via aggregation and face two challenges: 1) secure data management workflows are absent and 2) implicit preference feedback is ignored. To tackle these current difficulties, this article proposes secure AIoT for implicit group recommendations (SAIoT-GRs). For the hardware module, a secure Internet of Things structure is developed as the bottom support platform. For the software module, a collaborative Bayesian network model and noncooperative game are introduced as algorithms. This secure AIoT architecture is able to maximize the advantages of the two modules. In addition, a large number of experiments are carried out to evaluate the performance of SAIoT-GR in terms of efficiency and robustness.","",""
199,"Dong Wook Kim, H. Jang, K. Kim, Youngbin Shin, S. Park","Design Characteristics of Studies Reporting the Performance of Artificial Intelligence Algorithms for Diagnostic Analysis of Medical Images: Results from Recently Published Papers",2019,"","","","",174,"2022-07-13 09:19:07","","10.3348/kjr.2019.0025","","",,,,,199,66.33,40,5,3,"Objective To evaluate the design characteristics of studies that evaluated the performance of artificial intelligence (AI) algorithms for the diagnostic analysis of medical images. Materials and Methods PubMed MEDLINE and Embase databases were searched to identify original research articles published between January 1, 2018 and August 17, 2018 that investigated the performance of AI algorithms that analyze medical images to provide diagnostic decisions. Eligible articles were evaluated to determine 1) whether the study used external validation rather than internal validation, and in case of external validation, whether the data for validation were collected, 2) with diagnostic cohort design instead of diagnostic case-control design, 3) from multiple institutions, and 4) in a prospective manner. These are fundamental methodologic features recommended for clinical validation of AI performance in real-world practice. The studies that fulfilled the above criteria were identified. We classified the publishing journals into medical vs. non-medical journal groups. Then, the results were compared between medical and non-medical journals. Results Of 516 eligible published studies, only 6% (31 studies) performed external validation. None of the 31 studies adopted all three design features: diagnostic cohort design, the inclusion of multiple institutions, and prospective data collection for external validation. No significant difference was found between medical and non-medical journals. Conclusion Nearly all of the studies published in the study period that evaluated the performance of AI algorithms for diagnostic analysis of medical images were designed as proof-of-concept technical feasibility studies and did not have the design features that are recommended for robust validation of the real-world clinical performance of AI algorithms.","",""
6,"Tanya Tiwari, Tanuj Tiwari, Sanjay Tiwari","How Artificial Intelligence, Machine Learning and Deep Learning are Radically Different?",2018,"","","","",175,"2022-07-13 09:19:07","","10.23956/IJARCSSE.V8I2.569","","",,,,,6,1.50,2,3,4,"There is a lot of confusion these days about Artificial Intelligence (AI), Machine Learning (ML) and Deep Learning (DL). A computer system able to perform tasks that normally require human intelligence, such as visual perception, speech recognition, decision-making, and translation between languages. Artificial Intelligence has made it possible. Deep learning is a subset of machine learning, and machine learning is a subset of AI, which is an umbrella term for any computer program that does something smart. In other words, all machine learning is AI, but not all AI is machine learning, and so forth. Machine Learning represents a key evolution in the fields of computer science, data analysis, software engineering, and artificial intelligence. Machine learning (ML)is a vibrant field of research, with a range of exciting areas for further development across different methods and applications. These areas include algorithmic interpretability, robustness, privacy, fairness, inference of causality, human-machine interaction, and security. The goal of ML is never to make “perfect” guesses, because ML deals in domains where there is no such thing. The goal is to make guesses that are good enough to be useful. Deep learning is a particular kind of machine learning that achieves great power and flexibility by learning to represent the world as nested hierarchy of concepts, with each concept defined in relation to simpler concepts, and more abstract representations computed in terms of less abstract ones. This paper gives an overview of artificial intelligence, machine learning & deep learning techniques and compare these techniques.","",""
11,"Tae Wan Kim","Explainable artificial intelligence (XAI), the goodness criteria and the grasp-ability test",2018,"","","","",176,"2022-07-13 09:19:07","","","","",,,,,11,2.75,11,1,4,"This paper introduces the ""grasp-ability test"" as a ""goodness"" criteria by which to compare which explanation is more or less meaningful than others for users to understand the automated algorithmic data processing.","",""
11,"Aditya Kuppa, N. Le-Khac","Black Box Attacks on Explainable Artificial Intelligence(XAI) methods in Cyber Security",2020,"","","","",177,"2022-07-13 09:19:07","","10.1109/IJCNN48605.2020.9206780","","",,,,,11,5.50,6,2,2,"Cybersecurity community is slowly leveraging Machine Learning (ML) to combat ever evolving threats. One of the biggest drivers for successful adoption of these models is how well domain experts and users are able to understand and trust their functionality. As these black-box models are being employed to make important predictions, the demand for transparency and explainability is increasing from the stakeholders.Explanations supporting the output of ML models are crucial in cyber security, where experts require far more information from the model than a simple binary output for their analysis. Recent approaches in the literature have focused on three different areas: (a) creating and improving explainability methods which help users better understand the internal workings of ML models and their outputs; (b) attacks on interpreters in white box setting; (c) defining the exact properties and metrics of the explanations generated by models. However, they have not covered, the security properties and threat models relevant to cybersecurity domain, and attacks on explainable models in black box settings.In this paper, we bridge this gap by proposing a taxonomy for Explainable Artificial Intelligence (XAI) methods, covering various security properties and threat models relevant to cyber security domain. We design a novel black box attack for analyzing the consistency, correctness and confidence security properties of gradient based XAI methods. We validate our proposed system on 3 security-relevant data-sets and models, and demonstrate that the method achieves attacker’s goal of misleading both the classifier and explanation report and, only explainability method without affecting the classifier output. Our evaluation of the proposed approach shows promising results and can help in designing secure and robust XAI methods.","",""
10,"M. Alomar, M. Hameed, N. Al‐Ansari, M. Alsaadi","Data-Driven Model for the Prediction of Total Dissolved Gas: Robust Artificial Intelligence Approach",2020,"","","","",178,"2022-07-13 09:19:07","","10.1155/2020/6618842","","",,,,,10,5.00,3,4,2,"Saturated total dissolved gas (TDG) is recently considered as a serious issue in the environmental engineering field since it stands behind the reasons for increasing the mortality rates of fish and aquatic organisms. The accurate and more reliable prediction of TDG has a very significant role in preserving the diversity of aquatic organisms and reducing the phenomenon of fish deaths. Herein, two machine learning approaches called support vector regression (SVR) and extreme learning machine (ELM) have been applied to predict the saturated TDG% at USGS 14150000 and USGS 14181500 stations which are located in the USA. For the USGS 14150000 station, the recorded samples from 13 October 2016 to 14 March 2019 (75%) were used for training set, and the rest from 15 March 2019 to 13 October 2019 (25%) were used for testing requirements. Similarly, for USGS 14181500 station, the hourly data samples which covered the period from 9 June 2017 till 11 March 2019 were used for calibrating the models and from 12 March 2019 until 9 October 2019 were used for testing the predictive models. Eight input combinations based on different parameters have been established as well as nine statistical performance measures have been used for evaluating the accuracy of adopted models, for instance, not limited, correlation of determination (        R      2        ), mean absolute relative error (MAE), and uncertainty at 95% (        U      95        ). The obtained results of the study for both stations revealed that the ELM managed efficiently to estimate the TDG in comparison to SVR technique. For USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.986 (0.986), MAE of 0.316 (0.441), and         U      95        of 3.592 (3.869). Lastly, for USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.991 (0.991), MAE of 0.338 (0.396), and         U      95        of 0.832 (0.837). In addition, ELM’s training process computational time is stated to be much shorter than that of SVM. The results also showed that the temperature parameter was the most significant variable that influenced TDG relative to the other parameters. Overall, the proposed model (ELM) proved to be an appropriate and efficient computer-assisted technology for saturated TDG modeling that will contribute to the basic knowledge of environmental considerations.","",""
2,"Takahiro Sasaki, Hiroo","A Study on the Use of Artificial Intelligence within Government Pension Investment Fund’s Investment Management Practices (Summary Report)",2018,"","","","",179,"2022-07-13 09:19:07","","","","",,,,,2,0.50,1,2,4,"The proper selection and monitoring of fund managers is one of the most important tasks for Government Pension Investment Fund (GPIF). Its current approach, which depends on the track records and qualitative explanation of candidates and commissioned fund managers, can be significantly improved with the use of Artificial Intelligence (AI). This would lead to better management of GPIF’s assets, which amount to over 150 trillion yen. GPIF expressed their interest in using AI technologies to improve overall business practices, especially on their core competences. Thus, this commissioned study focused on the investigation of: (1) The possibility and implications of applying AI technologies to the long-term management of pension assets, and (2) The impact of AI technology on the business models of asset management companies, especially if GPIF establishes its AI capability to select and monitor fund managers. One of GPIF’s core activities is the development and maintenance of a “manager structure”. “Manager structure” refers to the structure of managing organizations (managers), and the associated allocations and reallocations of assets to be managed. Within the process of developing and maintaining the manager structure, it is necessary to make various decisions, such as defining the characteristics of each manager, defining their management behavior with respect to various aspects of the economic background, and determining whether their behavior is consistent with the policy declared to GPIF. The fundamental questions for this study are, “Is there any chance AI could be used within manager structure development and maintenance processes?”, and “In which part of the process can we use AI most effectively?” A joint team formed between GPIF and Sony CSL have gone through GPIF’s manager structure development and maintenance processes in-depth, and agreed to develop a proof-of-concept prototype system to test the principle of using deep learning to detect the investment style of managers from trading behavior data (trading items, timing, volume, unrealized gain and loss, etc) collected daily by GPIF. The system is composed of a series of “detector arrays”, each reacting to the specific investment style of each manager. A detector array is a set of neural networks that are trained beforehand with the data generated by virtual fund managers, that each faithfully execute one of the typical investment styles. The system provides an N-dimension vector representing a mixture of investment styles of the manager at given point in time. A blind test of style detection using actual trading behavior data for 16 fund managers demonstrated that the system can properly detect the styles and drifts of each fund manager. In addition, the system’s visualization capabilities were proven to be effective in identifying the spontaneous convergence of trading behaviors where most funding managers happen to trade similar items. Results from the proof-of-concept pilot system indicate that, with the introduction of such a system, GPIF should be able to conduct more prudent and data-driven selection and monitoring of fund managers. In addition, this may foster more constructive and in-depth dialog between GPIF and fund managers, which will improve the robustness and performance of investment practices at GPIF in the long run.","",""
132,"Y. Yang, C. S. Bang","Application of artificial intelligence in gastroenterology",2019,"","","","",180,"2022-07-13 09:19:07","","10.3748/wjg.v25.i14.1666","","",,,,,132,44.00,66,2,3,"Artificial intelligence (AI) using deep-learning (DL) has emerged as a breakthrough computer technology. By the era of big data, the accumulation of an enormous number of digital images and medical records drove the need for the utilization of AI to efficiently deal with these data, which have become fundamental resources for a machine to learn by itself. Among several DL models, the convolutional neural network showed outstanding performance in image analysis. In the field of gastroenterology, physicians handle large amounts of clinical data and various kinds of image devices such as endoscopy and ultrasound. AI has been applied in gastroenterology in terms of diagnosis, prognosis, and image analysis. However, potential inherent selection bias cannot be excluded in the form of retrospective study. Because overfitting and spectrum bias (class imbalance) have the possibility of overestimating the accuracy, external validation using unused datasets for model development, collected in a way that minimizes the spectrum bias, is mandatory. For robust verification, prospective studies with adequate inclusion/exclusion criteria, which represent the target populations, are needed. DL has its own lack of interpretability. Because interpretability is important in that it can provide safety measures, help to detect bias, and create social acceptance, further investigations should be performed.","",""
99,"R. Colling, Helen Pitman, K. Oien, N. Rajpoot, P. Macklin, D. Snead, Tony Sackville, C. Verrill","Artificial intelligence in digital pathology: a roadmap to routine use in clinical practice",2019,"","","","",181,"2022-07-13 09:19:07","","10.1002/path.5310","","",,,,,99,33.00,12,8,3,"The use of artificial intelligence will transform clinical practice over the next decade and the early impact of this will likely be the integration of image analysis and machine learning into routine histopathology. In the UK and around the world, a digital revolution is transforming the reporting practice of diagnostic histopathology and this has sparked a proliferation of image analysis software tools. While this is an exciting development that could discover novel predictive clinical information and potentially address international pathology workforce shortages, there is a clear need for a robust and evidence‐based framework in which to develop these new tools in a collaborative manner that meets regulatory approval. With these issues in mind, the NCRI Cellular Molecular Pathology (CM‐Path) initiative and the British In Vitro Diagnostics Association (BIVDA) have set out a roadmap to help academia, industry, and clinicians develop new software tools to the point of approved clinical use. © 2019 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.","",""
73,"Valentina Bellemo, Gilbert Lim, T. Rim, G. Tan, C. Cheung, S. Sadda, M. He, A. Tufail, M. Lee, W. Hsu, D. Ting","Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application",2019,"","","","",182,"2022-07-13 09:19:07","","10.1007/s11892-019-1189-3","","",,,,,73,24.33,7,11,3,"","",""
85,"A. Grzybowski, Piotr Brona, Gilbert Lim, P. Ruamviboonsuk, G. Tan, M. Abràmoff, D. Ting","Artificial intelligence for diabetic retinopathy screening: a review",2019,"","","","",183,"2022-07-13 09:19:07","","10.1038/s41433-019-0566-0","","",,,,,85,28.33,12,7,3,"","",""
67,"Yonghui Shang, Hoang Nguyen, X. Bui, Quang-Hieu Tran, H. Moayedi","A Novel Artificial Intelligence Approach to Predict Blast-Induced Ground Vibration in Open-Pit Mines Based on the Firefly Algorithm and Artificial Neural Network",2019,"","","","",184,"2022-07-13 09:19:07","","10.1007/s11053-019-09503-7","","",,,,,67,22.33,13,5,3,"","",""
51,"Xiaohang Wu, Yelin Huang, Zhenzhen Liu, Weiyi Lai, Erping Long, Kai Zhang, Jiewei Jiang, Duoru Lin, Kexin Chen, Tongyong Yu, Dongxuan Wu, Cong Li, Yanyi Chen, Minjie Zou, Chuan Chen, Yi Zhu, Chong Guo, Xiayin Zhang, Ruixin Wang, Yahan Yang, Yifan Xiang, Lijian Chen, Congxin Liu, J. Xiong, Z. Ge, Ding-ding Wang, Guihua Xu, Shao-lin Du, Chi Xiao, Jianghao Wu, Ke Zhu, Dan-yao Nie, Fan Xu, Jian Lv, Weirong Chen, Yizhi Liu, Haotian Lin","Universal artificial intelligence platform for collaborative management of cataracts",2019,"","","","",185,"2022-07-13 09:19:07","","10.1136/bjophthalmol-2019-314729","","",,,,,51,17.00,5,37,3,"Purpose To establish and validate a universal artificial intelligence (AI) platform for collaborative management of cataracts involving multilevel clinical scenarios and explored an AI-based medical referral pattern to improve collaborative efficiency and resource coverage. Methods The training and validation datasets were derived from the Chinese Medical Alliance for Artificial Intelligence, covering multilevel healthcare facilities and capture modes. The datasets were labelled using a three-step strategy: (1) capture mode recognition; (2) cataract diagnosis as a normal lens, cataract or a postoperative eye and (3) detection of referable cataracts with respect to aetiology and severity. Moreover, we integrated the cataract AI agent with a real-world multilevel referral pattern involving self-monitoring at home, primary healthcare and specialised hospital services. Results The universal AI platform and multilevel collaborative pattern showed robust diagnostic performance in three-step tasks: (1) capture mode recognition (area under the curve (AUC) 99.28%–99.71%), (2) cataract diagnosis (normal lens, cataract or postoperative eye with AUCs of 99.82%, 99.96% and 99.93% for mydriatic-slit lamp mode and AUCs >99% for other capture modes) and (3) detection of referable cataracts (AUCs >91% in all tests). In the real-world tertiary referral pattern, the agent suggested 30.3% of people be ‘referred’, substantially increasing the ophthalmologist-to-population service ratio by 10.2-fold compared with the traditional pattern. Conclusions The universal AI platform and multilevel collaborative pattern showed robust diagnostic performance and effective service for cataracts. The context of our AI-based medical referral pattern will be extended to other common disease conditions and resource-intensive situations.","",""
51,"Lu Minh Le, H. Ly, B. Pham, Vuong Minh Le, T. Pham, Duy-Hung Nguyen, Xuan-Tuan Tran, Tien-Thinh Le","Hybrid Artificial Intelligence Approaches for Predicting Buckling Damage of Steel Columns Under Axial Compression",2019,"","","","",186,"2022-07-13 09:19:07","","10.3390/ma12101670","","",,,,,51,17.00,6,8,3,"This study aims to investigate the prediction of critical buckling load of steel columns using two hybrid Artificial Intelligence (AI) models such as Adaptive Neuro-Fuzzy Inference System optimized by Genetic Algorithm (ANFIS-GA) and Adaptive Neuro-Fuzzy Inference System optimized by Particle Swarm Optimization (ANFIS-PSO). For this purpose, a total number of 57 experimental buckling tests of novel high strength steel Y-section columns were collected from the available literature to generate the dataset for training and validating the two proposed AI models. Quality assessment criteria such as coefficient of determination (R2), Mean Absolute Error (MAE) and Root Mean Squared Error (RMSE) were used to validate and evaluate the performance of the prediction models. Results showed that both ANFIS-GA and ANFIS-PSO had a strong ability in predicting the buckling load of steel columns, but ANFIS-PSO (R2 = 0.929, RMSE = 60.522 and MAE = 44.044) was slightly better than ANFIS-GA (R2 = 0.916, RMSE = 65.371 and MAE = 48.588). The two models were also robust even with the presence of input variability, as investigated via Monte Carlo simulations. This study showed that the hybrid AI techniques could help constructing an efficient numerical tool for buckling analysis.","",""
47,"Chengjie Zheng, T. V. Johnson, Aakriti Garg, Michael V. Boland","Artificial intelligence in glaucoma",2019,"","","","",187,"2022-07-13 09:19:07","","10.1097/ICU.0000000000000552","","",,,,,47,15.67,12,4,3,"Purpose of review The use of computers has become increasingly relevant to medical decision-making, and artificial intelligence methods have recently demonstrated significant advances in medicine. We therefore provide an overview of current artificial intelligence methods and their applications, to help the practicing ophthalmologist understand their potential impact on glaucoma care. Recent findings Techniques used in artificial intelligence can successfully analyze and categorize data from visual fields, optic nerve structure [e.g., optical coherence tomography (OCT) and fundus photography], ocular biomechanical properties, and a combination thereof to identify disease severity, determine disease progression, and/or recommend referral for specialized care. Algorithms have become increasingly complex in recent years, utilizing both supervised and unsupervised methods of artificial intelligence. Impressive performance of these algorithms on previously unseen data has been reported, often outperforming standard global indices and expert observers. However, there remains no clearly defined gold standard for determining the presence and severity of glaucoma, which undermines the training of these algorithms. To improve upon existing methodologies, future work must employ more robust definitions of disease, optimize data inputs for artificial intelligence analysis, and improve methods of extracting knowledge from learned results. Summary Artificial intelligence has the potential to revolutionize the screening, diagnosis, and classification of glaucoma, both through the automated processing of large data sets, and by earlier detection of new disease patterns. In addition, artificial intelligence holds promise for fundamentally changing research aimed at understanding the development, progression, and treatment of glaucoma, by identifying novel risk factors and by evaluating the importance of existing ones.","",""
3,"Dercilio Junior Verly Lopes, G. S. Bobadilha, Karl Michael Grebner","A fast and robust artificial intelligence technique for wood knot detection",2020,"","","","",188,"2022-07-13 09:19:07","","10.15376/BIORES.15.4.9351-9361","","",,,,,3,1.50,1,3,2,"This study reports the feasibility of using deep convolutional neural networks (CNN), for automatically detecting knots on the surface of wood with high speed and accuracy. A limited dataset of 921 images were photographed in different contexts and divided into 80:20 ratio for training and validation, respectively. The “You only look once” (YoloV3) CNN-based architecture was adopted for training the neural network. The Adam gradient descent optimizer algorithm was used to iteratively minimize the generalized intersection-over-union loss function. Knots on the surface of wood were manually annotated. Images and annotations were analyzed by a stack of convolutional and fully connected layers with skipped connections. After training, model checkpoint was created and inferences on the validation set were made. The quality of results was assessed by several metrics: precision, recall, F1-score, average precision, and precision x recall curve. Results indicated that YoloV3 provided knot detection time of approximately 0.0102 s per knot with a relatively low false positive and false negative ratios. Precision, recall, f1-score metrics reached 0.77, 0.79, and 0.78, respectively. The average precision was 80%. With an adequate number of images, it is possible to improve this tool for use within sawmills in the forms of both workstation and mobile device applications.","",""
39,"Ashley S. Deeks","The Judicial Demand for Explainable Artificial Intelligence",2019,"","","","",189,"2022-07-13 09:19:07","","","","",,,,,39,13.00,39,1,3,"A recurrent concern about machine learning algorithms is that they operate as “black boxes,” making it difficult to identify how and why the algorithms reach particular decisions, recommendations, or predictions. Yet judges will confront machine learning algorithms with increasing frequency, including in criminal, administrative, and tort cases. This Essay argues that judges should demand explanations for these algorithmic outcomes. One way to address the “black box” problem is to design systems that explain how the algorithms reach their conclusions or predictions. If and as judges demand these explanations, they will play a seminal role in shaping the nature and form of “explainable artificial intelligence” (or “xAI”). Using the tools of the common law, courts can develop what xAI should mean in different legal contexts.    There are advantages to having courts to play this role: Judicial reasoning that builds from the bottom up, using case-by-case consideration of the facts to produce nuanced decisions, is a pragmatic way to develop rules for xAI. Further, courts are likely to stimulate the production of different forms of xAI that are responsive to distinct legal settings and audiences. More generally, we should favor the greater involvement of public actors in shaping xAI, which to date has largely been left in private hands.","",""
41,"C. Macrae","Governing the safety of artificial intelligence in healthcare",2019,"","","","",190,"2022-07-13 09:19:07","","10.1136/bmjqs-2019-009484","","",,,,,41,13.67,41,1,3,"Artificial intelligence (AI) has enormous potential to improve the safety of healthcare, from increasing diagnostic accuracy,1 to optimising treatment planning,2 to forecasting outcomes of care.3 However, integrating AI technologies into the delivery of healthcare is likely to introduce a range of new risks and amplify existing ones. For instance, failures in widely used software have the potential to quickly affect large numbers of patients4; hidden assumptions in underlying data and models can lead to AI systems delivering dangerous recommendations that are insensitive to local care processes,5 6 and opaque AI techniques such as deep learning can make explaining and learning from failure extremely difficult.7 8 To maximise the benefits of AI in healthcare and to build trust among patients and practitioners, it will therefore be essential to robustly govern the risks that AI poses to patient safety.  In a recent review in this journal, Challen and colleagues present an important and timely analysis of some of the key technological risks associated with the application of machine learning in clinical settings.9 Machine learning is a subfield of AI that focuses on the development of algorithms that are automatically derived and optimised through exposure to large quantities of exemplar ‘training’ data.10 The outputs of machine learning algorithms are essentially classifications of patterns that provide some sort of prediction—for instance, predicting whether an image shows a malignant melanoma or a benign mole.11 Some of the basic techniques of machine learning have existed for half a century or more, but progress in the field has accelerated rapidly due to advances in the development of ‘deep’ artificial neural networks12 combined with huge increases in computational power and the availability of enormous quantities of data. These techniques have underpinned recent public demonstrations of AI systems …","",""
37,"C. Kulikowski","Beginnings of Artificial Intelligence in Medicine (AIM): Computational Artifice Assisting Scientific Inquiry and Clinical Art – with Reflections on Present AIM Challenges",2019,"","","","",191,"2022-07-13 09:19:07","","10.1055/s-0039-1677895","","",,,,,37,12.33,37,1,3,"Summary Background : The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970’s led to a major change in the paradigm that affected all of artificial intelligence (AI) research. Since then, AI has evolved, surviving several “winters”, as it has oscillated between relying on expensive and hard-to-validate knowledge-based approaches, and the alternative of using machine learning methods for inferring classification rules from labelled datasets. In the past couple of decades, we are seeing a gradual but progressive intertwining of the two. Objectives : To give an overview of early directions in AI in medicine and threads of some subsequent developments motivated by the very different goals of scientific inquiry for biomedical research, and for computational modeling of clinical reasoning and more general healthcare problem solving from the perspective of today’s “AI-Deep Learning Boom”. To show how, from the beginning, AI was central to Biomedical and Health Informatics (BMHI), as a field investigating how to understand intelligent thinking in dealing professionally with the practice for healthcare, developing mathematical models, technology, and software tools to aid human experts in biomedicine, despite many previous bouts of “exuberant optimism” about the methodologies deployed. Methods : An overview and commentary on some of the early research and publications in AI in biomedicine, emphasizing the different approaches to the modeling of problems involved in clinical practice in contrast to those of biomedical science. A concluding reflection of a few current challenges and pitfalls of AI in some biomedical applications. Conclusion : While biomedical knowledge-based systems played a critical role in influencing AI in its early days, 50 years later they have taken a back seat behind “Deep Learning” which promises to discover knowledge structures for inference and prediction, both in science and for clinical decision-support. Early work on AI for medical consultation turned out to be more useful for explanation and teaching than for clinical practice, as had been originally intended. Today, despite the many reported successes of deep learning, fundamental scientific challenges arise in drawing on models of brain science, cognition, and language, if AI is to augment and complement rather than replace human judgment and expertise in biomedicine while also incorporating these advances for translational medicine. Understanding clinical phenotypes and how they relate to precision and personalization of care requires not only scientific inquiry, but also humanistic models of treatment that respond to patient and practitioner narrative exchanges, since it is the stories and insights of human experts which encourage what Norbert Weiner termed the ethical “human use of human beings”, so central to adherence to the Hippocratic Oath","",""
32,"Matt Taddy","The Technological Elements of Artificial Intelligence",2018,"","","","",192,"2022-07-13 09:19:07","","10.3386/W24301","","",,,,,32,8.00,32,1,4,"We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together – each targeting a straightforward prediction task – to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.","",""
32,"Jun-Ho Huh, Yeong-Seok Seo","Understanding Edge Computing: Engineering Evolution With Artificial Intelligence",2019,"","","","",193,"2022-07-13 09:19:07","","10.1109/ACCESS.2019.2945338","","",,,,,32,10.67,16,2,3,"The key to the explosion of the Internet of Things and the ability to collect, analyze, and provide big data in the cloud is edge computing, which is a new computing paradigm in which data is processed from edges. Edge Computing has been attracting attention as one of the top 10 strategic technology trends in the past two years and has innovative potential. It provides shorter response times, lower bandwidth costs, and more robust data safety and privacy protection than cloud computing. In particular, artificial intelligence technologies are rapidly incorporating edge computing. In this paper, we introduce the concepts, backgrounds, and pros and cons of edge computing, explain how it operates and its structure hierarchically with artificial intelligence concepts, list examples of its applications in various fields, and finally suggest some improvements and discuss the challenges of its application in three representative technological fields. We intend to clarify various analyses and opinions regarding edge computing and artificial intelligence.","",""
27,"Jing Zhang, Chao-Kai Wen, Shi Jin, Geoffrey Y. Li","Artificial Intelligence-Aided Receiver for a CP-Free OFDM System: Design, Simulation, and Experimental Test",2019,"","","","",194,"2022-07-13 09:19:07","","10.1109/ACCESS.2019.2914928","","",,,,,27,9.00,7,4,3,"Orthogonal frequency division multiplexing (OFDM), usually with sufficient cyclic prefix (CP), has been widely applied in various communication systems. The CP in OFDM consumes additional resource and reduces spectrum and energy efficiency. However, channel estimation and signal detection are very challenging for CP-free OFDM systems. In this paper, we propose a novel artificial intelligence (AI)-aided receiver (AI receiver) for a CP-free OFDM system. The AI receiver includes a channel estimation neural network (CE-NET) and a signal detection neural network based on orthogonal approximate message passing (OAMP), called OAMP-NET. The CE-NET is initialized by the least-square channel estimation algorithm and refined by a linear minimum mean-squared error neural network. The OAMP-NET is established by unfolding the iterative OAMP algorithm and adding several trainable parameters to improve the detection performance. We first investigate their performance under different channel models through extensive simulation and then establish a real transmission system using a 5G rapid prototyping system for an over-the-air (OTA) test. Based on our study, the AI receiver can estimate time-varying channels with a single training phase. It also has great robustness to various imperfections and has better performance than those competitive algorithms, especially for high-order modulation. The OTA test further verifies its feasibility to real environments and indicates its potential for future communications systems.","",""
8,"Imran Ahmed, Gwanggil Jeon, F. Piccialli","From Artificial Intelligence to Explainable Artificial Intelligence in Industry 4.0: A Survey on What, How, and Where",2022,"","","","",195,"2022-07-13 09:19:07","","10.1109/tii.2022.3146552","","",,,,,8,8.00,3,3,1,"Nowadays, Industry 4.0 can be considered a reality, a paradigm integrating modern technologies and innovations. Artificial intelligence (AI) can be considered the leading component of the industrial transformation enabling intelligent machines to execute tasks autonomously such as self-monitoring, interpretation, diagnosis, and analysis. AI-based methodologies (especially machine learning and deep learning support manufacturers and industries in predicting their maintenance needs and reducing downtime. Explainable artificial intelligence (XAI) studies and designs approaches, algorithms and tools producing human-understandable explanations of AI-based systems information and decisions. This article presents a comprehensive survey of AI and XAI-based methods adopted in the Industry 4.0 scenario. First, we briefly discuss different technologies enabling Industry 4.0. Then, we present an in-depth investigation of the main methods used in the literature: we also provide the details of what, how, why, and where these methods have been applied for Industry 4.0. Furthermore, we illustrate the opportunities and challenges that elicit future research directions toward responsible or human-centric AI and XAI systems, essential for adopting high-stakes industry applications.","",""
7,"Mir Riyanul Islam, Mobyen Uddin Ahmed, Shaibal Barua, S. Begum","A Systematic Review of Explainable Artificial Intelligence in Terms of Different Application Domains and Tasks",2022,"","","","",196,"2022-07-13 09:19:07","","10.3390/app12031353","","",,,,,7,7.00,2,4,1,"Artificial intelligence (AI) and machine learning (ML) have recently been radically improved and are now being employed in almost every application domain to develop automated or semi-automated systems. To facilitate greater human acceptability of these systems, explainable artificial intelligence (XAI) has experienced significant growth over the last couple of years with the development of highly accurate models but with a paucity of explainability and interpretability. The literature shows evidence from numerous studies on the philosophy and methodologies of XAI. Nonetheless, there is an evident scarcity of secondary studies in connection with the application domains and tasks, let alone review studies following prescribed guidelines, that can enable researchers’ understanding of the current trends in XAI, which could lead to future research for domain- and application-specific method development. Therefore, this paper presents a systematic literature review (SLR) on the recent developments of XAI methods and evaluation metrics concerning different application domains and tasks. This study considers 137 articles published in recent years and identified through the prominent bibliographic databases. This systematic synthesis of research articles resulted in several analytical findings: XAI methods are mostly developed for safety-critical domains worldwide, deep learning and ensemble models are being exploited more than other types of AI/ML models, visual explanations are more acceptable to end-users and robust evaluation metrics are being developed to assess the quality of explanations. Research studies have been performed on the addition of explanations to widely used AI/ML models for expert users. However, more attention is required to generate explanations for general users from sensitive domains such as finance and the judicial system.","",""
21,"D. Ting, M. Ang, J. Mehta, D. Ting","Artificial intelligence-assisted telemedicine platform for cataract screening and management: a potential model of care for global eye health",2019,"","","","",197,"2022-07-13 09:19:07","","10.1136/bjophthalmol-2019-315025","","",,,,,21,7.00,5,4,3,"Artificial intelligence (AI) is the fourth industrial revolution.1 Deep learning is a robust machine learning technique that uses convolutional neural network to perform multilevel data abstraction without the need for manual feature engineering.2 In ophthalmology, many studies showed comparable, if not better, diagnostic performance in using AI to screen, diagnose, predict and monitor various eye conditions on fundus photographs and optical coherence tomography,3 4 including diabetic retinopathy (DR),5 age-related macular degeneration,6 glaucoma,7 retinopathy of prematurity (ROP).8   To date, many countries have reported well-established telemedicine programme to screen for DR and ROP,9–12 but limited for cataracts. Cataract is the leading cause of reversible blindness, affecting approximately 12.6 million (3.4–28.7 million) worldwide.13 14 The prevalence of cataract-related visual impairment also varies between high-income and low-income countries, with the latter having poorer access to tertiary care.13 In this issue, Wu et al 15 reported an AI-integrated telemedicine platform to screen and refer patients with cataract. This article consists of two parts: (1) the first part focusing on the AI system in detection of three tasks (capture mode, cataract diagnosis and referable cataract) and (2) the second part describing how these AI algorithms could be integrated in the telemedicine platform for real-world operational use. In this study, the referable cases were defined as: (1) grade 3 and grade 4 nuclear sclerotic …","",""
23,"B. Chin-Yee, Ross E. G. Upshur","Three Problems with Big Data and Artificial Intelligence in Medicine",2019,"","","","",198,"2022-07-13 09:19:07","","10.1353/pbm.2019.0012","","",,,,,23,7.67,12,2,3,"ABSTRACT:The rise of big data and artificial intelligence (AI) in health care has engendered considerable excitement, claiming to improve approaches to diagnosis, prognosis, and treatment. Amidst the enthusiasm, the philosophical assumptions that underlie the big data and AI movement in medicine are rarely examined. This essay outlines three philosophical challenges faced by this movement: (1) the epistemological-ontological problem arising from the theory-ladenness of big data and measurement; (2) the epistemological-logical problem resulting from the inherent limitations of algorithms and attendant issues of reliability and interpretability; and (3) the phenomenological problem concerning the irreducibility of human experience to quantitative data. These philosophical issues demonstrate several important challenges for these technologies that must be considered prior to their integration into clinical care. Our article aims to initiate a critical dialogue on the impact of big data and AI in health care in order to allow for more robust evaluation of these technologies and to aid in the development of approaches to clinical care that better serve clinicians and their patients.","",""
22,"Rushikesh S. Joshi, Alexander F. Haddad, Darryl Lau, C. Ames","Artificial Intelligence for Adult Spinal Deformity",2019,"","","","",199,"2022-07-13 09:19:07","","10.14245/ns.1938414.207","","",,,,,22,7.33,6,4,3,"Adult spinal deformity (ASD) is a complex disease that significantly affects the lives of many patients. Surgical correction has proven to be effective in achieving improvement of spinopelvic parameters as well as improving quality of life (QoL) for these patients. However, given the relatively high complication risk associated with ASD correction, it is of paramount importance to develop robust prognostic tools for predicting risk profile and outcomes. Historically, statistical models such as linear and logistic regression models were used to identify preoperative factors associated with postoperative outcomes. While these tools were useful for looking at simple associations, they represent generalizations across large populations, with little applicability to individual patients. More recently, predictive analytics utilizing artificial intelligence (AI) through machine learning for comprehensive processing of large amounts of data have become available for surgeons to implement. The use of these computational techniques has given surgeons the ability to leverage far more accurate and individualized predictive tools to better inform individual patients regarding predicted outcomes after ASD correction surgery. Applications range from predicting QoL measures to predicting the risk of major complications, hospital readmission, and reoperation rates. In addition, AI has been used to create a novel classification system for ASD patients, which will help surgeons identify distinct patient subpopulations with unique risk-benefit profiles. Overall, these tools will help surgeons tailor their clinical practice to address patients’ individual needs and create an opportunity for personalized medicine within spine surgery.","",""
28,"Francesco Bodria, F. Giannotti, Riccardo Guidotti, Francesca Naretto, D. Pedreschi, S. Rinzivillo","Benchmarking and Survey of Explanation Methods for Black Box Models",2021,"","","","",200,"2022-07-13 09:19:07","","","","",,,,,28,28.00,5,6,1,"The widespread adoption of black-box models in Artificial Intelligence has enhanced the need for explanation methods to reveal how these obscure models reach specific decisions. Retrieving explanations is fundamental to unveil possible biases and to resolve practical or ethical issues. Nowadays, the literature is full of methods with different explanations. We provide a categorization of explanation methods based on the type of explanation returned. We present the most recent and widely used explainers, and we show a visual comparison among explanations and a quantitative benchmarking.","",""
