Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
11,"Markus Jaeger, Stephan Krügel, D. Marinelli, J. Papenbrock, P. Schwendner","Interpretable Machine Learning for Diversified Portfolio Construction",2020,"","","","",1,"2022-07-13 09:23:23","","10.2139/ssrn.3730144","","",,,,,11,5.50,2,5,2,"In this article, the authors construct a pipeline to benchmark hierarchical risk parity (HRP) relative to equal risk contribution (ERC) as examples of diversification strategies allocating to liquid multi-asset futures markets with dynamic leverage (volatility target). The authors use interpretable machine learning concepts (explainable AI) to compare the robustness of the strategies and to back out implicit rules for decision-making. The empirical dataset consists of 17 equity index, government bond, and commodity futures markets across 20 years. The two strategies are back tested for the empirical dataset and for about 100,000 bootstrapped datasets. XGBoost is used to regress the Calmar ratio spread between the two strategies against features of the bootstrapped datasets. Compared to ERC, HRP shows higher Calmar ratios and better matches the volatility target. Using Shapley values, the Calmar ratio spread can be attributed especially to univariate drawdown measures of the asset classes. TOPICS: Quantitative methods, statistical methods, big data/machine learning, portfolio construction, performance measurement Key Findings ▪ The authors introduce a procedure to benchmark rule-based investment strategies and to explain the differences in path-dependent risk-adjusted performance measures using interpretable machine learning. ▪ They apply the procedure to the Calmar ratio spread between hierarchical risk parity (HRP) and equal risk contribution (ERC) allocations of a multi-asset futures portfolio and find HRP to have superior risk-adjusted performance. ▪ The authors regress the Calmar ratio spread against statistical features of bootstrapped futures return datasets using XGBoost and apply the SHAP framework by Lundberg and Lee (2017) to discuss the local and global feature importance.","",""
0,"Katrin Sophie Bohnsack, M. Kaden, Julia Abel, S. Saralajew, T. Villmann","The Resolved Mutual Information Function as a Structural Fingerprint of Biomolecular Sequences for Interpretable Machine Learning Classifiers",2021,"","","","",2,"2022-07-13 09:23:23","","10.3390/e23101357","","",,,,,0,0.00,0,5,1,"In the present article we propose the application of variants of the mutual information function as characteristic fingerprints of biomolecular sequences for classification analysis. In particular, we consider the resolved mutual information functions based on Shannon-, Rényi-, and Tsallis-entropy. In combination with interpretable machine learning classifier models based on generalized learning vector quantization, a powerful methodology for sequence classification is achieved which allows substantial knowledge extraction in addition to the high classification ability due to the model-inherent robustness. Any potential (slightly) inferior performance of the used classifier is compensated by the additional knowledge provided by interpretable models. This knowledge may assist the user in the analysis and understanding of the used data and considered task. After theoretical justification of the concepts, we demonstrate the approach for various example data sets covering different areas in biomolecular sequence analysis.","",""
0,"Mohammadreza Amirian, Lukas Tuggener, R. Chavarriaga, Y. Satyawan, F. Schilling, F. Schwenker, Thilo Stadelmann","Two to trust : AutoML for safe modelling and interpretable deep learning for robustness",2020,"","","","",3,"2022-07-13 09:23:23","","10.21256/ZHAW-20217","","",,,,,0,0.00,0,7,2,"With great power comes great responsibility. The success of machine learning, especially deep learning, in research and practice has attracted a great deal of interest, which in turn necessitates increased trust. Sources of mistrust include matters of model genesis (“Is this really the appropriate model?”) and interpretability (“Why did the model come to this conclusion?”, “Is the model safe from being easily fooled by adversaries?”). In this paper, two partners for the trustworthiness tango are presented: recent advances and ideas, as well as practical applications in industry in (a) Automated machine learning (AutoML), a powerful tool to optimize deep neural network architectures and finetune hyperparameters, which promises to build models in a safer and more comprehensive way; (b) Interpretability of neural network outputs, which addresses the vital question regarding the reasoning behind model predictions and provides insights to improve robustness against adversarial attacks.","",""
2,"J. Sarkar, Cory Peterson","Operational Workload Impact on Robust Solid-State Storage Analyzed with Interpretable Machine Learning",2019,"","","","",4,"2022-07-13 09:23:23","","10.1109/IRPS.2019.8720510","","",,,,,2,0.67,1,2,3,"Solid-state storage technology is finding increasing adoption in enterprise and data center environments due to their high reliability and reducing cost. With high performance solid-state storage devices (SSDs) internally designed as distributed resilient systems, their operational behavior under materially different workloads is described in this research. Application of interpretable machine learning on internal parametric data of SSDs enables insights on workloads' interaction with the resilient system design. After prior research demonstrated significantly different accelerated workload stress, the analysis on resilience of the SSDs under random vs. pseudo-sequential workloads emphasize the efficacy and importance of their distributed resilience schemes. As such, these results provide causational insights on the mechanism of differential stress of the workloads impacting the resilience design principles. Moreover, the results elucidate guidelines strongly relevant from design robustness perspective for research on novel SSD architectures such as the proposed Open Channel SSD, towards deployment in hyperscale and virtualization environments.","",""
0,"Sasmitha Dasanayaka, S. Silva, V. Shantha, D. Meedeniya, Thanuja D. Ambegoda","Interpretable Machine Learning for Brain Tumor Analysis Using MRI",2022,"","","","",5,"2022-07-13 09:23:23","","10.1109/ICARC54489.2022.9754131","","",,,,,0,0.00,0,5,1,"A brain tumor is a potentially fatal growth of cells in the central nervous system that can be categorized as benign or malignant. Advancements in deep learning in the recent past and the availability of high computational power have been influencing the automation of diagnosing brain tumors. DenseNet and U-Net are considered state of the art deep learning models for classification and segmentation of MRIs respectively. Despite the progress of deep learning in diagnosing using medical images, generic convolutional neural networks are still not fully adopted in clinical settings as they lack robustness and reliability. Moreover, such black-box models don’t offer a human interpretable justification as to why certain classification decisions are made, which makes them less preferable for medical diagnostics. Brain tumor segmentation and classification using deep learning techniques has been a popular research area in the last few decades but still, there are only a few models that are interpretable. In this paper, we have proposed an interpretable deep learning model which is more human understandable than existing black-box models, designed based on U-Net and DenseNet to segment and classify brain tumors using MRI. In our proposed model, we generate a heat map highlighting the contribution of each region of the input to the classification output and have validated the system using the MICCAI 2020 Brain Tumor dataset.","",""
0,"George J. Siedel, S. Vock, A. Morozov, Stefan Voss","Utilizing Class Separation Distance for the Evaluation of Corruption Robustness of Machine Learning Classifiers",2022,"","","","",6,"2022-07-13 09:23:23","","10.48550/arXiv.2206.13405","","",,,,,0,0.00,0,4,1,"Robustness is a fundamental pillar of Machine Learning (ML) classifiers, substantially determining their reliability. Methods for assessing classifier robustness are therefore essential. In this work, we address the challenge of evaluating corruption robustness in a way that allows comparability and interpretability on a given dataset. We propose a test data augmentation method that uses a robustness distance 𝜖 derived from the datasets minimal class separation distance. The resulting MSCR (mean statistical corruption robustness) metric allows a dataset-specific comparison of different classifiers with respect to their corruption robustness. The MSCR value is interpretable, as it represents the classifiers avoidable loss of accuracy due to statistical corruptions. On 2D and image data, we show that the metric reflects different levels of classifier robustness. Furthermore, we observe unexpected optima in classifiers robust accuracy through training and testing classifiers with different levels of noise. While researchers have frequently reported on a significant tradeoff on accuracy when training robust models, we strengthen the view that a tradeoff between accuracy and corruption robustness is not inherent. Our results indicate that robustness training through simple data augmentation can already slightly improve accuracy.","",""
0,"Takaki Yamamoto, Katie Cockburn, V. Greco, Kyogo Kawaguchi","Probing the rules of cell coordination in live tissues by interpretable machine learning based on graph neural networks",2022,"","","","",7,"2022-07-13 09:23:23","","10.1101/2021.06.23.449559","","",,,,,0,0.00,0,4,1,"Robustness in developing and homeostatic tissues is supported by various types of spatiotemporal cell-to-cell interactions. Although live imaging and cell tracking are powerful in providing direct evidence of cell coordination rules, extracting and comparing these rules across many tissues with potentially different length and timescales of coordination requires a versatile framework of analysis. Here we demonstrate that graph neural network (GNN) models are suited for this purpose, by showing how they can be applied to predict cell fate in tissues and utilized to infer the cell interactions governing the multicellular dynamics. Analyzing the live mammalian epidermis data, where spatiotemporal graphs constructed from cell tracks and cell contacts are given as inputs, GNN discovers distinct neighbor cell fate coordination rules that depend on the region of the body. This approach demonstrates how the GNN framework is powerful in inferring general cell interaction rules from live data without prior knowledge of the signaling involved.","",""
4,"Rahul Singh","A Finite Sample Theorem for Longitudinal Causal Inference with Machine Learning: Long Term, Dynamic, and Mediated Effects",2021,"","","","",8,"2022-07-13 09:23:23","","","","",,,,,4,4.00,4,1,1,"I construct and justify confidence intervals for longitudinal causal parameters estimated with machine learning. Longitudinal parameters include long term, dynamic, and mediated effects. I provide a nonasymptotic theorem for any longitudinal causal parameter estimated with any machine learning algorithm that satisfies a few simple, interpretable conditions. The main result encompasses local parameters defined for specific demographics as well as proximal parameters defined in the presence of unobserved confounding. Formally, I prove consistency, Gaussian approximation, and semiparametric efficiency. The rate of convergence is n for global parameters, and it degrades gracefully for local parameters. I articulate a simple set of conditions to translate mean square rates into statistical inference. A key feature of the main result is a new multiple robustness to ill posedness for proximal causal inference in longitudinal settings.","",""
9,"Chun-Hao Chang, R. Caruana, A. Goldenberg","NODE-GAM: Neural Generalized Additive Model for Interpretable Deep Learning",2021,"","","","",9,"2022-07-13 09:23:23","","","","",,,,,9,9.00,3,3,1,"Deployment of machine learning models in real high-risk settings (e.g. healthcare) often depends not only on model’s accuracy but also on its fairness, robustness and interpretability. Generalized Additive Models (GAMs) have a long history of use in these high-risk domains, but lack desirable features of deep learning such as differentiability and scalability. In this work, we propose a neural GAM (NODE-GAM) and neural GAM (NODE-GAM) that scale well to large datasets, while remaining interpretable and accurate. We show that our proposed models have comparable accuracy to other non-interpretable models, and outperform other GAMs on large datasets. We also show that our models are more accurate in self-supervised learning setting when access to labeled data is limited.","",""
2,"J. de Nijs, T. J. Burger, Ronald J. Janssen, S. M. Kia, Daniel P J van Opstal, M. D. de Koning, L. de Haan, Behrooz Z. Agna A. Nico J. Richard Lieuwe Philippe Jurjen  Alizadeh Bartels-Velthuis van Beveren Bruggeman de, B. Alizadeh, A. Bartels-Velthuis, N. V. van Beveren, R. Bruggeman, P. Delespaul, J. Luykx, I. Myin-Germeys, R. Kahn, F. Schirmbeck, C. Simons, T. van Amelsvoort, J. van os, R. van Winkel, W. Cahn, H. Schnack","Individualized prediction of three- and six-year outcomes of psychosis in a longitudinal multicenter study: a machine learning approach",2021,"","","","",10,"2022-07-13 09:23:23","","10.1038/s41537-021-00162-3","","",,,,,2,2.00,0,23,1,"","",""
1,"Khansa Rasheed, A. Qayyum, M. Ghaly, A. Al-Fuqaha, A. Razi, Junaid Qadir","Explainable, Trustworthy, and Ethical Machine Learning for Healthcare: A Survey",2021,"","","","",11,"2022-07-13 09:23:23","","10.36227/TECHRXIV.14376179.V1","","",,,,,1,1.00,0,6,1,"With the advent of machine learning (ML) applications in daily life, the questions about liability, trust, and interpretability of their outputs are raising, especially for healthcare applications. The black-box nature of ML models is a roadblock for clinical utilization. Therefore, to gain the trust of clinicians and patients, researchers need to provide explanations of how and why the model is making a specific decision. With the promise of enhancing the trust and transparency of black-box models, researchers are in the phase of maturing the field of eXplainable ML (XML). In this paper, we provide a comprehensive review of explainable and interpretable ML techniques implemented for providing the reasons behind their decisions for various healthcare applications. Along with highlighting various security, safety, and robustness challenges that hinder the trustworthiness of ML we also discussed the ethical issues of healthcare ML and describe how explainable and trustworthy ML can resolve these ethical problems. Finally, we elaborate on the limitations of existing approaches and highlight various open research problems that require further development.","",""
3,"Jivitesh Sharma, Rohan Kumar Yadav, Ole-Christoffer Granmo, Lei Jiao","Human Interpretable AI: Enhancing Tsetlin Machine Stochasticity with Drop Clause",2021,"","","","",12,"2022-07-13 09:23:23","","","","",,,,,3,3.00,1,4,1,"In this article, we introduce a novel variant of the Tsetlin machine (TM) that randomly drops clauses, the key learning elements of a TM. In effect, TM with drop clause ignores a random selection of the clauses in each epoch, selected according to a predefined probability. In this way, additional stochasticity is introduced in the learning phase of TM. Along with producing more distinct and well-structured patterns that improve the performance, we also show that dropping clauses increases learning robustness. To explore the effects clause dropping has on accuracy, training time, and interpretability, we conduct extensive experiments on various benchmark datasets in natural language processing (NLP) (IMDb and SST2) as well as computer vision (MNIST and CIFAR10). In brief, we observe from +2% to +4% increase in accuracy and 2× to 4× faster learning. We further employ the Convolutional TM to document interpretable results on the CIFAR10 dataset. To the best of our knowledge, this is the first time an interpretable machine learning algorithm has been used to produce pixel-level human-interpretable results on CIFAR10. Also, unlike previous interpretable methods that focus on attention visualisation or gradient interpretability, we show that the TM is a more general interpretable method. That is, by producing rule-based propositional logic expressions that are human-interpretable, the TM can explain how it classifies a particular instance at the pixel level for computer vision and at the word level for NLP.","",""
1,"Jivitesh Sharma, Rohan Kumar Yadav, Ole-Christoffer Granmo, Lei Jiao","Drop Clause: Enhancing Performance, Interpretability and Robustness of the Tsetlin Machine",2021,"","","","",13,"2022-07-13 09:23:23","","","","",,,,,1,1.00,0,4,1,"In this article, we introduce a novel variant of the Tsetlin machine (TM) that randomly drops clauses, the key learning elements of a TM. In effect, TM with drop clause ignores a random selection of the clauses in each epoch, selected according to a predefined probability. In this way, additional stochasticity is introduced in the learning phase of TM. To explore the effects drop clause has on accuracy, training time, interpretability and robustness, we conduct extensive experiments on nine benchmark datasets in natural language processing (NLP) (IMDb, R8, R52, MR and TREC) and image classification (MNIST, Fashion MNIST, CIFAR-10 and CIFAR100). Our proposed model outperforms baseline machine learning algorithms by a wide margin and achieves competitive performance in comparison with recent deep learning model such as BERT and AlexNET-DFA. In brief, we observe up to +10% increase in accuracy and 2× to 4× faster learning compared with standard TM. We further employ the Convolutional TM to document interpretable results on the CIFAR datasets, visualizing how the heatmaps produced by the TM become more interpretable with drop clause. We also evaluate how drop clause affects learning robustness by introducing corruptions and alterations in the image/language test data. Our results show that drop clause makes TM more robust towards such changes1.","",""
4,"Ninghao Liu, Mengnan Du, Xia Hu","Adversarial Machine Learning: An Interpretation Perspective",2020,"","","","",14,"2022-07-13 09:23:23","","","","",,,,,4,2.00,1,3,2,"Recent years have witnessed the significant advances of machine learning in a wide spectrum of applications. However, machine learning models, especially deep neural networks, have been recently found to be vulnerable to carefully-crafted input called adversarial samples. The difference between normal and adversarial samples is almost imperceptible to human. Many work have been proposed to study adversarial attack and defense in different scenarios. An intriguing and crucial aspect among those work is to understand the essential cause of model vulnerability, which requires in-depth exploration of another concept in machine learning models, i.e., interpretability. Interpretable machine learning tries to extract human-understandable terms for the working mechanism of models, which also receives a lot of attention from both academia and industry. Recently, an increasing number of work start to incorporate interpretation into the exploration of adversarial robustness. Furthermore, we observe that many previous work of adversarial attacking, although did not mention it explicitly, can be regarded as natural extension of interpretation. In this paper, we review recent work on adversarial attack and defense, particularly, from the perspective of machine learning interpretation. We categorize interpretation into two types, according to whether it focuses on raw features or model components. For each type of interpretation, we elaborate on how it could be used in attacks, or defense against adversaries. After that, we briefly illustrate other possible correlations between the two domains. Finally, we discuss the challenges and future directions along tackling adversary issues with interpretation.","",""
327,"Nicolas Papernot, P. Mcdaniel","Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning",2018,"","","","",15,"2022-07-13 09:23:23","","","","",,,,,327,81.75,164,2,4,"Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.","",""
18,"C. Rudin, David Edwin Carlson","The Secrets of Machine Learning: Ten Things You Wish You Had Known Earlier to be More Effective at Data Analysis",2019,"","","","",16,"2022-07-13 09:23:23","","10.1287/educ.2019.0200","","",,,,,18,6.00,9,2,3,"Despite the widespread usage of machine learning throughout organizations, there are some key principles that are commonly missed. In particular: 1) There are at least four main families for supervised learning: logical modeling methods, linear combination methods, case-based reasoning methods, and iterative summarization methods. 2) For many application domains, almost all machine learning methods perform similarly (with some caveats). Deep learning methods, which are the leading technique for computer vision problems, do not maintain an edge over other methods for most problems (and there are reasons why). 3) Neural networks are hard to train and weird stuff often happens when you try to train them. 4) If you don't use an interpretable model, you can make bad mistakes. 5) Explanations can be misleading and you can't trust them. 6) You can pretty much always find an accurate-yet-interpretable model, even for deep neural networks. 7) Special properties such as decision making or robustness must be built in, they don't happen on their own. 8) Causal inference is different than prediction (correlation is not causation). 9) There is a method to the madness of deep neural architectures, but not always. 10) It is a myth that artificial intelligence can do anything.","",""
6,"V. Chernozhukov, W. Newey, Rahul Singh","A Simple and General Debiased Machine Learning Theorem with Finite Sample Guarantees",2021,"","","","",17,"2022-07-13 09:23:23","","10.1093/biomet/asac033","","",,,,,6,6.00,2,3,1,"  Debiased machine learning is a meta algorithm based on bias correction and sample splitting to calculate confidence intervals for functionals, i.e., scalar summaries, of machine learning algorithms. For example, an analyst may desire the confidence interval for a treatment effect estimated with a neural network. We provide a non-asymptotic debiased machine learning theorem that encompasses any global or local functional of any machine learning algorithm that satisfies a few simple, interpretable conditions. Formally, we prove consistency, Gaussian approximation, and semiparametric efficiency by finite sample arguments. The rate of convergence is n−1/2 for global functionals, and it degrades gracefully for local functionals. Our results culminate in a simple set of conditions that an analyst can use to translate modern learning theory rates into traditional statistical inference. The conditions reveal a general double robustness property for ill-posed inverse problems.","",""
0,"R. Castellanos, G. Y. C. Maceda, I. D. L. Fuente, B. R. Noack, A. Ianiro, S. Discetti","Machine-learning flow control with few sensor feedback and measurement noise",2022,"","","","",18,"2022-07-13 09:23:23","","10.1063/5.0087208","","",,,,,0,0.00,0,6,1,"A comparative assessment of machine-learning (ML) methods for active flow control is performed. The chosen benchmark problem is the drag reduction of a two-dimensional Kármán vortex street past a circular cylinder at a low Reynolds number ( Re =  100). The flow is manipulated with two blowing/suction actuators on the upper and lower side of a cylinder. The feedback employs several velocity sensors. Two probe configurations are evaluated: 5 and 11 velocity probes located at different points around the cylinder and in the wake. The control laws are optimized with Deep Reinforcement Learning (DRL) and Linear Genetic Programming Control (LGPC). By interacting with the unsteady wake, both methods successfully stabilize the vortex alley and effectively reduce drag while using small mass flow rates for the actuation. DRL has shown higher robustness with respect to different initial conditions and to noise contamination of the sensor data; on the other hand, LGPC is able to identify compact and interpretable control laws, which only use a subset of sensors, thus allowing for the reduction of the system complexity with reasonably good results. Our study points at directions of future machine-learning control combining desirable features of different approaches.","",""
0,"G. Bomarito, P. Leser, N.C.M Strauss, K. Garbrecht, J. Hochhalter","Automated Learning of Interpretable Models with Quantified Uncertainty",2022,"","","","",19,"2022-07-13 09:23:23","","10.48550/arXiv.2205.01626","","",,,,,0,0.00,0,5,1,"Interpretability and uncertainty quantiﬁcation in machine learning can provide justiﬁcation for decisions, promote scientiﬁc discovery and lead to a better understanding of model behavior. Symbolic regression provides inherently interpretable machine learning, but relatively little work has focused on the use of symbolic regression on noisy data and the accompanying necessity to quantify uncertainty. A new Bayesian framework for genetic-programming-based symbolic regression (GPSR) is introduced that uses model evidence (i.e., marginal likelihood) to formulate replacement probability during the selection phase of evolution. Model parameter uncertainty is automatically quantiﬁed, enabling probabilistic predictions with each equation produced by the GPSR algorithm. Model evidence is also quantiﬁed in this process, and its use is shown to increase interpretability, improve robustness to noise, and reduce overﬁtting when compared to a conventional GPSR implementation on both numerical and physical experiments.","",""
3,"Barbara Rychalska, Dominika Basaj, P. Biecek","Are you tough enough? Framework for Robustness Validation of Machine Comprehension Systems",2018,"","","","",20,"2022-07-13 09:23:23","","","","",,,,,3,0.75,1,3,4,"Deep Learning NLP domain lacks procedures for the analysis of model robustness. In this paper we propose a framework which validates robustness of any Question Answering model through model explainers. We propose that a robust model should transgress the initial notion of semantic similarity induced by word embeddings to learn a more human-like understanding of meaning. We test this property by manipulating questions in two ways: swapping important question word for 1) its semantically correct synonym and 2) for word vector that is close in embedding space. We estimate importance of words in asked questions with Locally Interpretable Model Agnostic Explanations method (LIME). With these two steps we compare state-of-the-art Q&A models. We show that although accuracy of state-of-the-art models is high, they are very fragile to changes in the input. Moreover, we propose 2 adversarial training scenarios which raise model sensitivity to true synonyms by up to 7% accuracy measure. Our findings help to understand which models are more stable and how they can be improved. In addition, we have created and published a new dataset that may be used for validation of robustness of a Q&A model.","",""
2,"K. Yan, Adam P. Harrison","Interpretable Medical Image Classification with Self-Supervised Anatomical Embedding and Prior Knowledge",2021,"","","","",21,"2022-07-13 09:23:23","","","","",,,,,2,2.00,1,2,1,"In medical image analysis tasks, it is important to make machine learning models focus on correct anatomical locations, so as to improve interpretability and robustness of the model. We adopt a latest algorithm called self-supervised anatomical embedding (SAM) to locate point of interest (POI) on computed tomography (CT) scans. SAM can detect arbitrary POI with only one labeled sample needed. Then, we can extract targeted features from the POIs to train a simple prediction model guided by clinical prior knowledge. This approach mimics the practice of human radiologists, thus is interpretable, controllable, and robust. We illustrate our approach on the application of CT contrast phase classification and it outperforms an existing deep learning based method trained on the whole image.","",""
1,"Michael Tsang, James Enouen, Yan Liu","Interpretable Artificial Intelligence through the Lens of Feature Interaction",2021,"","","","",22,"2022-07-13 09:23:23","","","","",,,,,1,1.00,0,3,1,"Deep learning, alongside other modern machine learning techniques, has become the state of the art solution for a diverse range of real-world tasks. These include a variety of sensitive applications such as healthcare, finance, autonomous driving, criminal justice, and others which all pose significant concerns for fairness, robustness, safety, and trustworthiness. Despite these applications to critical tasks, deep networks are infamously referred to as black-box models because of their total lack of transparency in decision-making. If we are able to gain insight into how a model is coming to its conclusions, we are able to more clearly assess the trustworthiness and validity of its decisions. Consequently, an abundance of ongoing research is attempting to address model interpretability as the key problem to resolving these issues. There are many methods which are currently used to provide explanations of complex model predictions. LIME (Ribeiro et al., 2016) fits a local linear model around a data point, showing which features positively and negatively influence the prediction results. Despite the overall model being nonlinear, the local model gets an interpretable picture of how the model looks at small scales around the data point. Extensions of this method use other interpretable models like small decision tress. Shapley Values and SHAP follows a similar idea to assign a score to each feature, using a gametheoretic formulation which treats each feature as a player causing the final prediction (Lundberg and Lee, 2017). Its more rigorous formulation yields guarantees of its explanations summing up to the prediction score, but practically it usually must be estimated because of its high computational cost. Shuffle-based feature importance permutes the data of each feature to ascertain its importance in the final prediction in comparison to its normal prediction (Fisher et al., 2018). IG uses the fundamental theorem of calculus to provide additive explanations of a prediction (Sundararajan et al., 2017). This method is very popular in computer vision where its computational efficiency and saliency are prized, even though other work has exposed some of its shortcomings in providing an interpretation (Adebayo et al., 2018). Other methods are specifically designed for computer vision like TCAV (Kim et al., 2018) which finds a ‘concept direction’ corresponding to a large sample of concept images from the user. Surprisingly, all of these most popular interpretability methods share the same one limitation. None of these methods consider the shared importance of groups of two or more features; they only look at the effects had by each of the features individually. A feature interaction between two variables broadly describes a situation where both of the features/ variables are simultaneously important for a model’s prediction. In text applications for sentiment, ”not good” is a very simple example of two words strongly interacting with one another to create a negative sentiment. In modern-day applications, neural networks are usually hailed as amazing function approximators exactly because of their incredible ability to automatically uncover these kinds of complex relationships between the variables of the dataset. In many ways, however,","",""
0,"T. Grassi, F. Nauman, J. P. Ramsey, S. Bovino, G. Picogna, B. Ercolano","Reducing the complexity of chemical networks via interpretable autoencoders",2021,"","","","",23,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,6,1,"In many astrophysical applications, the cost of solving a chemical network represented by a system of ordinary differential equations (ODEs) grows significantly with the size of the network, and can often represent a significant computational bottleneck, particularly in coupled chemo-dynamical models. Although standard numerical techniques and complex solutions tailored to thermochemistry can somewhat reduce the cost, more recently, machine learning algorithms have begun to attack this challenge via data-driven dimensional reduction techniques. In this work, we present a new class of methods that take advantage of machine learning techniques to reduce complex data sets (autoencoders), the optimization of multi-parameter systems (standard backpropagation), and the robustness of well-established ODE solvers to to explicitly incorporate time-dependence. This new method allows us to find a compressed and simplified version of a large chemical network in a semi-automated fashion that can be solved with a standard ODE solver, while also enabling interpretability of the compressed, latent network. As a proof of concept, we tested the method on an astrophysicallyrelevant chemical network with 29 species and 224 reactions, obtaining a reduced but representative network with only 5 species and 12 reactions, and a ×65 speed-up.","",""
9,"Darius Afchar, Romain Hennequin","Making Neural Networks Interpretable with Attribution: Application to Implicit Signals Prediction",2020,"","","","",24,"2022-07-13 09:23:23","","10.1145/3383313.3412253","","",,,,,9,4.50,5,2,2,"Explaining recommendations enables users to understand whether recommended items are relevant to their needs and has been shown to increase their trust in the system. More generally, if designing explainable machine learning models is key to check the sanity and robustness of a decision process and improve their efficiency, it however remains a challenge for complex architectures, especially deep neural networks that are often deemed ”black-box”. In this paper, we propose a novel formulation of interpretable deep neural networks for the attribution task. Differently to popular post-hoc methods, our approach is interpretable by design. Using masked weights, hidden features can be deeply attributed, split into several input-restricted sub-networks and trained as a boosted mixture of experts. Experimental results on synthetic data and real-world recommendation tasks demonstrate that our method enables to build models achieving close predictive performances to their non-interpretable counterparts, while providing informative attribution interpretations.","",""
5,"J. Günther, Elias Reichensdorfer, P. Pilarski, K. Diepold","Interpretable PID parameter tuning for control engineering using general dynamic neural networks: An extensive comparison",2019,"","","","",25,"2022-07-13 09:23:23","","10.1371/journal.pone.0243320","","",,,,,5,1.67,1,4,3,"Modern automation systems largely rely on closed loop control, wherein a controller interacts with a controlled process via actions, based on observations. These systems are increasingly complex, yet most deployed controllers are linear Proportional-Integral-Derivative (PID) controllers. PID controllers perform well on linear and near-linear systems but their simplicity is at odds with the robustness required to reliably control complex processes. Modern machine learning techniques offer a way to extend PID controllers beyond their linear control capabilities by using neural networks. However, such an extension comes at the cost of losing stability guarantees and controller interpretability. In this paper, we examine the utility of extending PID controllers with recurrent neural networks—–namely, General Dynamic Neural Networks (GDNN); we show that GDNN (neural) PID controllers perform well on a range of complex control systems and highlight how they can be a scalable and interpretable option for modern control systems. To do so, we provide an extensive study using four benchmark systems that represent the most common control engineering benchmarks. All control environments are evaluated with and without noise as well as with and without disturbances. The neural PID controller performs better than standard PID control in 15 of 16 tasks and better than model-based control in 13 of 16 tasks. As a second contribution, we address the lack of interpretability that prevents neural networks from being used in real-world control processes. We use bounded-input bounded-output stability analysis to evaluate the parameters suggested by the neural network, making them understandable for engineers. This combination of rigorous evaluation paired with better interpretability is an important step towards the acceptance of neural-network-based control approaches for real-world systems. It is furthermore an important step towards interpretable and safely applied artificial intelligence.","",""
1,"Samir Rachid Zaim, C. Kenost, J. Berghout, Wesley Chiu, Liam Wilson, H. Zhang, Y. Lussier","binomialRF: interpretable combinatoric efficiency of random forests to identify biomarker interactions",2020,"","","","",26,"2022-07-13 09:23:23","","10.1186/s12859-020-03718-9","","",,,,,1,0.50,0,7,2,"","",""
35,"Gunjan Verma, A. Swami","Error Correcting Output Codes Improve Probability Estimation and Adversarial Robustness of Deep Neural Networks",2019,"","","","",27,"2022-07-13 09:23:23","","","","",,,,,35,11.67,18,2,3,"Modern machine learning systems are susceptible to adversarial examples; inputs which clearly preserve the characteristic semantics of a given class, but whose classification is (usually confidently) incorrect. Existing approaches to adversarial defense generally rely on modifying the input, e.g. quantization, or the learned model parameters, e.g. via adversarial training. However, recent research has shown that most such approaches succumb to adversarial examples when different norms or more sophisticated adaptive attacks are considered. In this paper, we propose a fundamentally different approach which instead changes the way the output is represented and decoded. This simple approach achieves state-of-the-art robustness to adversarial examples for L 2 and L ∞ based adversarial perturbations on MNIST and CIFAR10. In addition, even under strong white-box attacks, we find that our model often assigns adversarial examples a low probability; those with high probability are usually interpretable, i.e. perturbed towards the perceptual boundary between the original and adversarial class. Our approach has several advantages: it yields more meaningful probability estimates, is extremely fast during training and testing, requires essentially no architectural changes to existing discriminative learning pipelines, is wholly complementary to other defense approaches including adversarial training, and does not sacrifice benign test set performance","",""
0,"Bruce Lee, Thomas T. C. K. Zhang, Hamed Hassani, N. Matni","Performance-Robustness Tradeoffs in Adversarially Robust Linear-Quadratic Control",2022,"","","","",28,"2022-07-13 09:23:23","","10.48550/arXiv.2203.10763","","",,,,,0,0.00,0,4,1,"While H∞ methods can introduce robustness against worst-case perturbations, their nominal performance under conventional stochastic disturbances is often drastically reduced. Though this fundamental tradeoff between nominal performance and robustness is known to exist, it is not well-characterized in quantitative terms. Toward addressing this issue, we borrow from the increasingly ubiquitous notion of adversarial training from machine learning to construct a class of controllers which are optimized for disturbances consisting of mixed stochastic and worst-case components. We find that this problem admits a stationary optimal controller that has a simple analytic form closely related to suboptimal H∞ solutions. We then provide a quantitative performance-robustness tradeoff analysis, in which systemtheoretic properties such as controllability and stability explicitly manifest in an interpretable manner. This provides practitioners with general guidance for determining how much robustness to incorporate based on a priori system knowledge. We empirically validate our results by comparing the performance of our controller against standard baselines, and plotting tradeoff curves.","",""
7,"I. L. Ruiz, M. A. Gómez-Nieto","Building of Robust and Interpretable QSAR Classification Models by Means of the Rivality Index",2019,"","","","",29,"2022-07-13 09:23:23","","10.1021/acs.jcim.9b00264","","",,,,,7,2.33,4,2,3,"An unambiguous algorithm, added to the study of the applicability domain and appropriate measures of the goodness of fit and robustness, represent the key characteristics that should be ideally fulfilled for a QSAR model to be considered for regulatory purposes. In this paper, we propose a new algorithm (RINH) based on the rivality index for the construction of QSAR classification models. This index is capable of predicting the activity of the data set molecules by means of a measurement of the rivality between their nearest neighbors belonging to different classes, contributing with a robust measurement of the reliability of the predictions. In order to demonstrate the goodness of the proposed algorithm we have selected four independent and orthogonally different benchmark data sets (balanced/unbalanced and high/low modelable) and we have compared the results with those obtained using 12 different machine learning algorithms. These results have been validated using 20 data sets of different balancing and sizes, corroborating that the proposed algorithm is able to generate highly accurate classification models and contribute with valuable measurements of the reliability of the predictions and the applicability domain of the built models.","",""
2,"Samir Rachid Zaim, C. Kenost, J. Berghout, Wesley Chiu, Liam Wilson, H. Zhang, Y. Lussier","binomialRF: interpretable combinatoric efficiency of random forests to identify biomarker interactions",2019,"","","","",30,"2022-07-13 09:23:23","","10.1186/s12859-020-03718-9","","",,,,,2,0.67,0,7,3,"","",""
2,"F. Hu, Jiaxin Jiang, P. Yin","Interpretable Prediction of Protein-Ligand Interaction by Convolutional Neural Network",2019,"","","","",31,"2022-07-13 09:23:23","","10.1109/BIBM47256.2019.8982989","","",,,,,2,0.67,1,3,3,"Evaluation of protein-ligand interaction is a crucial step in the process of drug discovery. Recently, several methods based on deep learning have gained impressive binary classification performance on protein-ligand binding prediction. However, lack of three-dimensional complex data still limits the accuracy and robustness of evaluation of protein-ligand binding affinity, as well as the prediction of their binding sites. In this paper, we propose a novel convolutional neural network based method for estimating the binding affinity between protein and ligand using only 1D sequence data. Even with the same amount of sample size, this model outperforms other structure-dependent traditional and machine learning based methods in terms of both binary classification and regression task. Furthermore, we use this model to identify the key amino acid residues of protein that are vital for binding interaction, which provides biological interpretation.","",""
2,"Hossein Tavakoli, Jahan B. Ghasemi","An improved ensemble learning machine for biological activity prediction of tyrosine kinase inhibitors",2015,"","","","",32,"2022-07-13 09:23:23","","10.1002/cem.2698","","",,,,,2,0.29,1,2,7,"Boosting is one of the most important strategies in ensemble learning because of its ability to improve the stability and performance of weak learners. It is nonparametric, multivariate, fast and interpretable but is not robust against outliers. To enhance its prediction accuracy as well as immunize it against outliers, a modified version of a boosting algorithm (AdaBoost R2) was developed and called AdaBoost R3. In the sampling step, extremum samples were added to the boosting set. In the robustness step, a modified Huber loss function was applied to overcome the outlier problem. In the output step, a deterministic threshold was used to guarantee that bad predictions do not participate in the final output. The performance of the modified algorithm was investigated with two anticancer data sets of tyrosine kinase inhibitors, and the mechanism of inhibition was studied using the relative weighted variable importance procedure. Investigating the effect of base learner's strength reveals that boosting is only successful using the classification and regression tree method (a weak to moderate learner) and does not have a significant effect using the radial basis functions partial least square method (a strong base learners). Copyright © 2015 John Wiley & Sons, Ltd.","",""
0,"Maximilian-Peter Radtke, Jürgen Bock","Combining Knowledge and Deep Learning for Prognostics and Health Management",2022,"","","","",33,"2022-07-13 09:23:23","","10.36001/phme.2022.v7i1.3302","","",,,,,0,0.00,0,2,1,"In the recent past deep learning approaches have achieved remarkable results in the area of Prognostics and Health Management (PHM). These algorithms rely on large amounts of data, which is often not available, and produce outputs, which are hard to interpret. Before the broad success of deep learning machine faults were often classified using domain expert knowledge based on experience and physical models. In comparison, these approaches only require small amounts of data and produce highly interpretable results. On the downside, however, they struggle to predict unexpected patterns hidden in data. This research aims to combine knowledge and deep learning to increase accuracy, robustness and interpretability of current models.","",""
4,"Wei Zhang, Q. Chen, Yunfang Chen","Deep Learning Based Robust Text Classification Method via Virtual Adversarial Training",2020,"","","","",34,"2022-07-13 09:23:23","","10.1109/ACCESS.2020.2981616","","",,,,,4,2.00,1,3,2,"The existing methods of generating adversarial texts usually change the original meanings of texts significantly and even generate the unreadable texts. These less readable adversarial texts can misclassify the machine classifier successfully, but they cannot deceive the human observers very well. In this paper, we propose a novel method that generates readable adversarial texts with some perturbations that can also confuse human observers successfully. Based on the continuous bag-of-words (CBOW) model, the proposed method looks for the appropriate perturbations to generate the adversarial texts through controlling the perturbation direction vectors. Meanwhile, we apply adversarial training to regularize the classification model and extend it to semi-supervised tasks with virtual adversarial training. Experiments are conducted to show that the generated adversaries are interpretable and confused to humans and the virtual adversarial training effectively improves the robustness of the model.","",""
3,"A. Preece, Daniel Harborne, R. Raghavendra, Richard J. Tomsett, Dave Braines","Provisioning Robust and Interpretable AI/ML-Based Service Bundles",2018,"","","","",35,"2022-07-13 09:23:23","","10.1109/MILCOM.2018.8599838","","",,,,,3,0.75,1,5,4,"Coalition operations environments are characterised by the need to share intelligence, surveillance and reconnaissance services. Increasingly, such services are based on artificial intelligence (AI)and machine learning (ML)technologies. Two key issues in the exploitation of AI/ML services are robustness and interpretability. Employing a diverse portfolio of services can make a system robust to ‘unknown unknowns’. Interpretability - the need for services to offer explanation facilities to engender user trust - can be addressed by a variety of methods to generate either transparent or post hoc explanations according to users' requirements. This paper shows how a service-provisioning framework for coalition operations can be extended to address specific requirements for robustness and interpretability, allowing automatic selection of service bundles for intelligence, surveillance and reconnaissance tasks. The approach is demonstrated in a case study on traffic monitoring featuring a diverse set of AI/ML services based on deep neural networks and heuristic reasoning approaches.","",""
8,"Henry Kenlay, D. Thanou, Xiaowen Dong","On The Stability of Graph Convolutional Neural Networks Under Edge Rewiring",2020,"","","","",36,"2022-07-13 09:23:23","","10.1109/ICASSP39728.2021.9413474","","",,,,,8,4.00,3,3,2,"Graph neural networks are experiencing a surge of popularity within the machine learning community due to their ability to adapt to nonEuclidean domains and instil inductive biases. Despite this, their stability, i.e., their robustness to small perturbations in the input, is not yet well understood. Although there exists some results showing the stability of graph neural networks, most take the form of an upper bound on the magnitude of change due to a perturbation in the graph topology. However, the change in the graph topology captured in existing bounds tend not to be expressed in terms of structural properties, limiting our understanding of the model robustness properties. In this work, we develop an interpretable upper bound elucidating that graph neural networks are stable to rewiring between high degree nodes. This bound and further research in bounds of similar type provide further understanding of the stability properties of graph neural networks.","",""
5,"L. Manduchi, Kieran Chin-Cheong, H. Michel, S. Wellmann, Julia E. Vogt","Deep Conditional Gaussian Mixture Model for Constrained Clustering",2021,"","","","",37,"2022-07-13 09:23:23","","","","",,,,,5,5.00,1,5,1,"Constrained clustering has gained signiﬁcant attention in the ﬁeld of machine learning as it can leverage prior information on a growing amount of only partially labeled data. Following recent advances in deep generative models, we propose a novel framework for constrained clustering that is intuitive, interpretable, and can be trained efﬁciently in the framework of stochastic gradient variational inference. By explicitly integrating domain knowledge in the form of probabilistic relations, our proposed model (DC-GMM) uncovers the underlying distribution of data conditioned on prior clustering preferences, expressed as pairwise constraints . These constraints guide the clustering process towards a desirable partition of the data by indicating which samples should or should not belong to the same cluster. We provide extensive experiments to demonstrate that DC-GMM shows superior clustering performances and robustness compared to state-of-the-art deep constrained clustering methods on a wide range of data sets. We further demonstrate the usefulness of our approach on two challenging real-world applications.","",""
19,"Mohsen Hajiloo, H. Rabiee, Mahdi Anooshahpour","Fuzzy support vector machine: an efficient rule-based classification technique for microarrays",2013,"","","","",38,"2022-07-13 09:23:23","","10.1186/1471-2105-14-S13-S4","","",,,,,19,2.11,6,3,9,"","",""
3,"M. Gherardi","Solvable Model for the Linear Separability of Structured Data",2021,"","","","",39,"2022-07-13 09:23:23","","10.3390/e23030305","","",,,,,3,3.00,3,1,1,"Linear separability, a core concept in supervised machine learning, refers to whether the labels of a data set can be captured by the simplest possible machine: a linear classifier. In order to quantify linear separability beyond this single bit of information, one needs models of data structure parameterized by interpretable quantities, and tractable analytically. Here, I address one class of models with these properties, and show how a combinatorial method allows for the computation, in a mean field approximation, of two useful descriptors of linear separability, one of which is closely related to the popular concept of storage capacity. I motivate the need for multiple metrics by quantifying linear separability in a simple synthetic data set with controlled correlations between the points and their labels, as well as in the benchmark data set MNIST, where the capacity alone paints an incomplete picture. The analytical results indicate a high degree of “universality”, or robustness with respect to the microscopic parameters controlling data structure.","",""
1,"M. Wischow, Guillermo Gallego, I. Ernst, Anko Borner","Camera Condition Monitoring and Readjustment by means of Noise and Blur",2021,"","","","",40,"2022-07-13 09:23:23","","","","",,,,,1,1.00,0,4,1,"Autonomous vehicles and robots require increasingly more robustness and reliability to meet the demands of modern tasks. These requirements specially apply to cameras because they are the predominant sensors to acquire information about the environment and support actions. A camera must maintain proper functionality and take automatic countermeasures if necessary. However, there is little work that examines the practical use of a general condition monitoring approach for cameras and designs countermeasures in the context of an envisaged high-level application. We propose a generic and interpretable self-health-maintenance framework for cameras based on dataand physically-grounded models. To this end, we determine two reliable, real-time capable estimators for typical image effects of a camera in poor condition (defocus blur, motion blur, different noise phenomena and most common combinations) by comparing traditional and retrained machine learning-based approaches in extensive experiments. Furthermore, we demonstrate how one can adjust the camera parameters (e.g., exposure time and ISO gain) to achieve optimal whole-system capability based on experimental (non-linear and non-monotonic) input-output performance curves, using object detection, motion blur and sensor noise as examples. Our framework not only provides a practical ready-to-use solution to evaluate and maintain the health of cameras, but can also serve as a basis for extensions to tackle more sophisticated problems that combine additional data sources (e.g., sensor or environment parameters) empirically in order to attain fully reliable and robust machines.","",""
0,"Evan M. Yu, Alan Q. Wang, Adrian V. Dalca, M. Sabuncu","KeypointMorph: Robust Multi-modal A ne Registration via Unsupervised Keypoint Detection",2021,"","","","",41,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,4,1,"Registration is a fundamental task in medical imaging, and recent machine learning methods have become the state-of-the-art. However, these approaches are often not interpretable, lack robustness to large misalignments, and do not incorporate symmetries of the problem. In this work, we propose KeypointMorph, an unsupervised end-to-end learningbased image registration framework that relies on automatically detecting corresponding keypoints. Our core insight is straightforward: matching keypoints between images can be used to obtain the optimal transformation via a di↵erentiable closed-form expression. We use this observation to drive the unsupervised learning of anatomically-consistent keypoints from images. This not only leads to substantially more robust registration but also yields better interpretability, since the keypoints reveal which parts of the image are driving the final alignment. Moreover, KeypointMorph can be designed to be equivariant under image translations and/or symmetric with respect to the input image ordering. We demonstrate the proposed framework in solving 3D a ne registration of multi-modal brain MRI scans. Remarkably, we show that this strategy leads to consistent keypoints, even across modalities. We demonstrate registration accuracy that surpasses current state-of-the-art methods, especially in the context of large displacements. Our code is available at URL1","",""
0,"Alexander Hvatov, Mikhail Maslyaev, Iana S. Polonskaya, M. Sarafanov, Mark Merezhnikov, Nikolay O. Nikitin","Model-agnostic multi-objective approach for the evolutionary discovery of mathematical models",2021,"","","","",42,"2022-07-13 09:23:23","","10.1007/978-3-030-91885-9_6","","",,,,,0,0.00,0,6,1,"","",""
0,"A. Dave, Hao Wang, R. Ponciroli, Richard B. Vilim","Numerical Demonstration of Multiple Actuator Constraint Enforcement Algorithm for a Molten Salt Loop",2022,"","","","",43,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,4,1,"To advance the paradigm of autonomous operation for nuclear power plants, a data-driven machine learning approach to control is sought. Autonomous operation for next-generation reactor designs is anticipated to bolster safety and improve economics. However, any algorithms that are utilized need to be interpretable, adaptable, and robust. Interpretable means that one can inspect and understand the underlying relationships between inputs and outputs of the algorithm. Adaptable refers to the capability of accommodating temporal changes in the underlying system dynamics (e.g., due to reactivity swings or fouling). Robustness refers to stability over long horizons under the presence of sensor or process noise. In this work, we focus on the specific problem of optimal control during autonomous operation.","",""
6,"S. Hegselmann, T. Volkert, Hendrik Ohlenburg, A. Gottschalk, M. Dugas, C. Ertmer","An Evaluation of the Doctor-Interpretability of Generalized Additive Models with Interactions",2020,"","","","",44,"2022-07-13 09:23:23","","","","",,,,,6,3.00,1,6,2,"Applying machine learning in healthcare can be problematic because predictions might be biased, can lack robustness, and are prone to overly rely on correlations. Interpretable machine learning can mitigate these issues by visualizing gaps in problem formalization and putting the responsibility to meet additional desiderata of machine learning systems on human practitioners. Generalized additive models with interactions are transparent, with modular oneand two-dimensional risk functions that can be reviewed and, if necessary, removed. The key objective of this study is to determine whether these models can be interpreted by doctors to safely deploy them in a clinical setting. To this end, we simulated the review process of eight risk functions trained on a clinical task with twelve clinicians and collected information about objective and subjective factors of interpretability. The ratio of correct answers for dichotomous statements covering important properties of risk functions was 0.83±0.02 (n = 360) and the median of the participants’ certainty to correctly understand them was Certain (n = 96) on a seven-level Likert scale (one = Very Uncertain to seven = Very Certain). These results suggest that doctors can correctly interpret risk functions of generalized additive models with interactions and also feel confident to do so. However, the evaluation also identified several interpretability issues and it showed that interpretability of generalized additive models depends on the complexity of risk functions.","",""
16,"Evan Campbell, A. Phinyomark, E. Scheme","Feature Extraction and Selection for Pain Recognition Using Peripheral Physiological Signals",2019,"","","","",45,"2022-07-13 09:23:23","","10.3389/fnins.2019.00437","","",,,,,16,5.33,5,3,3,"In pattern recognition, the selection of appropriate features is paramount to both the performance and the robustness of the system. Over-reliance on machine learning-based feature selection methods can, therefore, be problematic; especially when conducted using small snapshots of data. The results of these methods, if adopted without proper interpretation, can lead to sub-optimal system design or worse, the abandonment of otherwise viable and important features. In this work, a deep exploration of pain-based emotion classification was conducted to better understand differences in the results of the related literature. In total, 155 different time domain and frequency domain features were explored, derived from electromyogram (EMG), skin conductance levels (SCL), and electrocardiogram (ECG) readings taken from the 85 subjects in response to heat-induced pain. To address the inconsistency in the optimal feature sets found in related works, an exhaustive and interpretable feature selection protocol was followed to obtain a generalizable feature set. Associations between features were then visualized using a topologically-informed chart, called Mapper, of this physiological feature space, including synthesis and comparison of results from previous literature. This topological feature chart was able to identify key sources of information that led to the formation of five main functional feature groups: signal amplitude and power, frequency information, nonlinear complexity, unique, and connecting. These functional groupings were used to extract further insight into observable autonomic responses to pain through a complementary statistical interaction analysis. From this chart, it was observed that EMG and SCL derived features could functionally replace those obtained from ECG. These insights motivate future work on novel sensing modalities, feature design, deep learning approaches, and dimensionality reduction techniques.","",""
0,"Julien Damond","Presentation and study of robustness for several methods to classify individuals based on their gene expressions",2011,"","","","",46,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,1,11,"Several studies have shown that it is possible to detect cancer tissues based on gene expressions using methods of machine learning. The main problem with classifying gene expression data is to obtain accurate rules that are easy to interpret and provide indications for follow up studies. Indeed high accuracy is hard to achieve due to the small number of observations and the large amount of genes in the human genome. Some methods of machine learning are based on an important quantity of genes, which lead to decision rules that are usually difficult to interpret. These methods were tested on different samples and their results were compared. Most of them provided good results with a high accuracy. Among these methods for gene classification one distanced itself from the others by producing transparents results which were readily interpretable and were very useful for follow up studies. It highlighted pair of genes that were the most efficient to classify individuals with respect to their gene expressions. This is the so called Top Scoring Pair (TSP) classifier. This method achieves prediction rates that are as high as those of the other methods. In contrast to other classifiers which use considerably more genes and more complicated procedures, the TSP has an easy and quick implementation and involves very few genes, namely only two. This provides very easy rules that are accurate and transparent. Finally, the TSP is paramter-free, which avoids overfitting and inflation of the estimation of the prediction rate.","",""
8,"Zitai Chen, Chuan Chen, Zong Zhang, Zibin Zheng, Q. Zou","Variational Graph Embedding and Clustering with Laplacian Eigenmaps",2019,"","","","",47,"2022-07-13 09:23:23","","10.24963/ijcai.2019/297","","",,,,,8,2.67,2,5,3,"As a fundamental machine learning problem, graph clustering has facilitated various real-world applications, and tremendous efforts had been devoted to it in the past few decades. However, most of the existing methods like spectral clustering suffer from the sparsity, scalability, robustness and handling high dimensional raw information in clustering. To address this issue, we propose a deep probabilistic model, called Variational Graph Embedding and Clustering with Laplacian Eigenmaps (VGECLE), which learns node embeddings and assigns node clusters simultaneously. It represents each node as a Gaussian distribution to disentangle the true embedding position and the uncertainty from the graph. With a Mixture of Gaussian (MoG) prior, VGECLE is capable of learning an interpretable clustering by the variational inference and generative process. In order to learn the pairwise relationships better, we propose a Teacher-Student mechanism encouraging node to learn a better Gaussian from its instant neighbors in the stochastic gradient descent (SGD) training fashion. By optimizing the graph embedding and the graph clustering problem as a whole, our model can fully take the advantages in their correlation. To our best knowledge, we are the first to tackle graph clustering in a deep probabilistic viewpoint. We perform extensive experiments on both synthetic and real-world networks to corroborate the effectiveness and efficiency of the proposed framework.","",""
1,"Jing Ya, Tingwen Liu, Panpan Zhang, Jinqiao Shi, Li Guo, Zhaojun Gu","NeuralAS: Deep Word-Based Spoofed URLs Detection Against Strong Similar Samples",2019,"","","","",48,"2022-07-13 09:23:23","","10.1109/IJCNN.2019.8852416","","",,,,,1,0.33,0,6,3,"Spoofed URLs are associated with various cyber crimes such as phishing and ransomware etc. Most existing detection approaches design a set of hand-crafted features and feed them to machine learning classifiers. However, designing such features is a time consuming and labor intensive process. This paper proposes an approach named NeuralAS (Neural Anti-Spoofing) by segmenting URLs into word sequences and detecting spoofed URLs with recurrent neural networks. As a result, NeuralAS can perform detection with high-abstract and poor-interpretable features learned automatically, and achieve accurate detection with contextual information in sequences. We also propose a novel method to construct indistinguishable data sets of strong similar samples, which can be used to evaluate the robustness of different approaches. Extensive experimental results show that NeuralAS works well on spoofed URLs detection, and has a significant effectiveness and robustness even on strong similar data sets.","",""
13,"S. Saralajew, Lars Holdijk, Maike Rees, T. Villmann","Prototype-based Neural Network Layers: Incorporating Vector Quantization",2018,"","","","",49,"2022-07-13 09:23:23","","","","",,,,,13,3.25,3,4,4,"Neural networks currently dominate the machine learning community and they do so for good reasons. Their accuracy on complex tasks such as image classification is unrivaled at the moment and with recent improvements they are reasonably easy to train. Nevertheless, neural networks are lacking robustness and interpretability. Prototype-based vector quantization methods on the other hand are known for being robust and interpretable. For this reason, we propose techniques and strategies to merge both approaches. This contribution will particularly highlight the similarities between them and outline how to construct a prototype-based classification layer for multilayer networks. Additionally, we provide an alternative, prototype-based, approach to the classical convolution operation. Numerical results are not part of this report, instead the focus lays on establishing a strong theoretical framework. By publishing our framework and the respective theoretical considerations and justifications before finalizing our numerical experiments we hope to jump-start the incorporation of prototype-based learning in neural networks and vice versa.","",""
0,"Samuel Madden","AutoFE : Efficient and Robust Automated Feature Engineering by Hyunjoon Song",2018,"","","","",50,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,1,4,"Feature engineering is the key to building highly successful machine learning models. We present AutoFE, a system designed to automate feature engineering. AutoFE generates a large set of new interpretable features by combining information in the original features. Given an augmented dataset, it discovers a set of features that significantly improves the performance of any traditional classification using an evolutionary algorithm. We demonstrate the effectiveness and robustness of our approach by conducting an extensive evaluation on 8 datasets and 5 different classification algorithms. We show that AutoFE can achieve an average improvement in predictive performance of 25.24% for all classification algorithms over their baseline performance obtained with the original features. Thesis Supervisor: Samuel Madden Title: Professor of Electrical Engineering and Computer Science","",""
9,"A. W. Example","Belief Propagation in Fuzzy Bayesian Networks",2008,"","","","",51,"2022-07-13 09:23:23","","","","",,,,,9,0.64,9,1,14,"Fuzzy Bayesian networks (FBN) are a graphical machine learning model representation with variables which are simultaneously fuzzy and uncertain[2]. Bayesian networks (BN) are commonly used in machine learning. This is due to their statistical rationality, capacity for rigorous causal inference, and robustness in the face of noisy, partially missing and realistic data. They are also more easily human-interpretable than other machine learning representations such as neural networks, and experts can specify prior knowledge in a principled manner to guide the machine learning search. A wide range of search algorithms have been developed for structural and parameter inference, including structural EM [3] and MCMC. Classically, Bayesian networks use continuous (Gaussian) or multinomial variables. Similarly, a fuzzy model has a wide range of advantages. Fuzzy models are also robust in the face of noise-corrupted data. The use of linguistic terms aids human comprehension of the learnt model, and they are particularly useful when the data is insufficient to formulate a precise model. The need to specify membership functions also forces the designer to consider the semantic interpretation of the model parameterisation and construction more explicitly. For these reasons, FBN (which combine these advantages) may be useful. Theoretical analysis in current research[1] indicates that fuzzy variables can be more expressive than multinomial or continuous variables. FBN may also be used as part of an integrated sequence of machine learning techniques that include reversible dimensionality reduction techniques such as fuzzy cover clustering algorithms. This may allow larger problems to be addressed with FBN than with classic BN.","",""
1981,"C. Rudin","Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead",2018,"","","","",52,"2022-07-13 09:23:23","","10.1038/S42256-019-0048-X","","",,,,,1981,495.25,1981,1,4,"","",""
1,"A. Saadallah, K. Morik","Active Sampling for Learning Interpretable Surrogate Machine Learning Models",2020,"","","","",53,"2022-07-13 09:23:23","","10.1109/DSAA49011.2020.00039","","",,,,,1,0.50,1,2,2,"The use of machine learning methods to inform consequential decisions is increasingly expanding across many fields. As a result, the ability to interpret these models has become to a greater extent crucial to increase the related-technologies acceptance level and reliability. In this paper, we propose an active sampling approach for learning accurately interpretable surrogate machine learning model to better approximate black-box models for supervised learning problems. Hence, the surrogate model is used to learn the black-box model and reflect its properties. Active sampling is used as an informed sampling method to adaptively and iteratively build an optimized training set based on the predictions of the black-box model to enhance the accuracy of the surrogate model. Subsequently, the surrogate model is used to interpret and debug the black-box model. The developed method is flexible and can be used to approximate any family of black-box models using any type of interpretable machine learning models, as it only requires the ability to compute their outputs. It is also applicable to both regression and classification tasks. In this work, we bring focus to decision tree due to their proven high interpretability. An experimental evaluation of the method on several real-world data sets is presented to show its flexibility and its robustness compared to traditional approaches for learning surrogate models.","",""
1973,"Finale Doshi-Velez, Been Kim","Towards A Rigorous Science of Interpretable Machine Learning",2017,"","","","",54,"2022-07-13 09:23:23","","","","",,,,,1973,394.60,987,2,5,"As machine learning systems become ubiquitous, there has been a surge of interest in interpretable machine learning: systems that provide explanation for their outputs. These explanations are often used to qualitatively assess other criteria such as safety or non-discrimination. However, despite the interest in interpretability, there is very little consensus on what interpretable machine learning is and how it should be measured. In this position paper, we first define interpretability and describe when interpretability is needed (and when it is not). Next, we suggest a taxonomy for rigorous evaluation and expose open questions towards a more rigorous science of interpretable machine learning.","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",55,"2022-07-13 09:23:23","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
8,"Lakshya Singhal, Yash Garg, Philip Yang, A. Tabaie, A. Wong, Akram Mohammed, Lokesh Chinthala, D. Kadaria, A. Sodhi, A. Holder, A. Esper, J. Blum, R. Davis, G. Clifford, G. Martin, R. Kamaleswaran","eARDS: A multi-center validation of an interpretable machine learning algorithm of early onset Acute Respiratory Distress Syndrome (ARDS) among critically ill adults with COVID-19",2021,"","","","",56,"2022-07-13 09:23:23","","10.1371/journal.pone.0257056","","",,,,,8,8.00,1,16,1,"We present an interpretable machine learning algorithm called ‘eARDS’ for predicting ARDS in an ICU population comprising COVID-19 patients, up to 12-hours before satisfying the Berlin clinical criteria. The analysis was conducted on data collected from the Intensive care units (ICU) at Emory Healthcare, Atlanta, GA and University of Tennessee Health Science Center, Memphis, TN and the Cerner® Health Facts Deidentified Database, a multi-site COVID-19 EMR database. The participants in the analysis consisted of adults over 18 years of age. Clinical data from 35,804 patients who developed ARDS and controls were used to generate predictive models that identify risk for ARDS onset up to 12-hours before satisfying the Berlin criteria. We identified salient features from the electronic medical record that predicted respiratory failure among this population. The machine learning algorithm which provided the best performance exhibited AUROC of 0.89 (95% CI = 0.88–0.90), sensitivity of 0.77 (95% CI = 0.75–0.78), specificity 0.85 (95% CI = 085–0.86). Validation performance across two separate health systems (comprising 899 COVID-19 patients) exhibited AUROC of 0.82 (0.81–0.83) and 0.89 (0.87, 0.90). Important features for prediction of ARDS included minimum oxygen saturation (SpO2), standard deviation of the systolic blood pressure (SBP), O2 flow, and maximum respiratory rate over an observational window of 16-hours. Analyzing the performance of the model across various cohorts indicates that the model performed best among a younger age group (18–40) (AUROC = 0.93 [0.92–0.94]), compared to an older age group (80+) (AUROC = 0.81 [0.81–0.82]). The model performance was comparable on both male and female groups, but performed significantly better on the severe ARDS group compared to the mild and moderate groups. The eARDS system demonstrated robust performance for predicting COVID19 patients who developed ARDS at least 12-hours before the Berlin clinical criteria, across two independent health systems.","",""
3,"Tao Zhong, Zian Zhuang, Xiao-fei Dong, Ka-hing Wong, W. Wong, Jian Wang, D. He, Shengyuan Liu","Predicting Antituberculosis Drug–Induced Liver Injury Using an Interpretable Machine Learning Method: Model Development and Validation Study",2021,"","","","",57,"2022-07-13 09:23:23","","10.2196/29226","","",,,,,3,3.00,0,8,1,"Background Tuberculosis (TB) is a pandemic, being one of the top 10 causes of death and the main cause of death from a single source of infection. Drug-induced liver injury (DILI) is the most common and serious side effect during the treatment of TB. Objective We aim to predict the status of liver injury in patients with TB at the clinical treatment stage. Methods We designed an interpretable prediction model based on the XGBoost algorithm and identified the most robust and meaningful predictors of the risk of TB-DILI on the basis of clinical data extracted from the Hospital Information System of Shenzhen Nanshan Center for Chronic Disease Control from 2014 to 2019. Results In total, 757 patients were included, and 287 (38%) had developed TB-DILI. Based on values of relative importance and area under the receiver operating characteristic curve, machine learning tools selected patients’ most recent alanine transaminase levels, average rate of change of patients’ last 2 measures of alanine transaminase levels, cumulative dose of pyrazinamide, and cumulative dose of ethambutol as the best predictors for assessing the risk of TB-DILI. In the validation data set, the model had a precision of 90%, recall of 74%, classification accuracy of 76%, and balanced error rate of 77% in predicting cases of TB-DILI. The area under the receiver operating characteristic curve score upon 10-fold cross-validation was 0.912 (95% CI 0.890-0.935). In addition, the model provided warnings of high risk for patients in advance of DILI onset for a median of 15 (IQR 7.3-27.5) days. Conclusions Our model shows high accuracy and interpretability in predicting cases of TB-DILI, which can provide useful information to clinicians to adjust the medication regimen and avoid more serious liver injury in patients.","",""
1,"Ana Kostovska, Matej Petković, Tomaz Stepisnik, L. Lucas, T. Finn, José Antonio Martinez Heras, P. Panov, S. Džeroski, A. Donati, N. Simidjievski, D. Kocev","GalaxAI: Machine learning toolbox for interpretable analysis of spacecraft telemetry data",2021,"","","","",58,"2022-07-13 09:23:23","","10.1109/smc-it51442.2021.00013","","",,,,,1,1.00,0,11,1,"We present GalaxAI - a versatile machine learning toolbox for efficient and interpretable end-to-end analysis of spacecraft telemetry data. GalaxAI employs various machine learning algorithms for multivariate time series analyses, classification, regression and structured output prediction, capable of handling high-throughput heterogeneous data. These methods allow for the construction of robust and accurate predictive models, that are in turn applied to different tasks of spacecraft monitoring and operations planning. More importantly, besides the accurate building of models, GalaxAI implements a visualisation layer, providing mission specialists and operators with a full, detailed and interpretable view of the data analysis process. We show the utility and versatility of GalaxAI on two use-cases concerning two different spacecraft: i) analysis and planning of Mars Express thermal power consumption and ii) predicting of INTEGRAL’s crossings through Van Allen belts.","",""
2,"Artur Movsessian, D. Cava, D. Tcherniak","Interpretable machine learning in damage detection using Shapley Additive Explanations",2021,"","","","",59,"2022-07-13 09:23:23","","10.31224/osf.io/96yf5","","",,,,,2,2.00,1,3,1,"In recent years, Machine Learning (ML) techniques have gained popularity in Structural Health Monitoring (SHM). These have been particularly used for damage detection in a wide range of engineering applications such as wind turbine blades. The outcomes of previous research studies in this area have demonstrated the capabilities of ML for robust damage detection. However, the primary challenge facing ML in SHM is the lack of interpretability of the prediction models hindering the broader implementation of these techniques. For this purpose, this study integrates the novel Shapley Additive exPlanations (SHAP) method into a ML-based damage detection process as a tool for introducing interpretability and, thus, build evidence for reliable decision-making in SHM applications. The SHAP method is based on coalitional game theory and adds global and local interpretability to ML-based models by computing the marginal contribution of each feature. The contribution is used to understand the nature of damage indices (DIs). The applicability of the SHAP method is first demonstrated on a simple lumped mass-spring-damper system with simulated temperature variabilities. Later, the SHAP method has been evaluated on data from an in-operation V27 wind turbine with artificially introduced damage in one of its blades. The results show the relationship between the environmental and operational variabilities (EOVs) and their direct influence on the damage indices. This ultimately helps to understand the difference between false positives caused by EOVs and true positives resulting from damage in the structure.","",""
26,"W. Gou, Chu-wen Ling, Yan He, Zengliang Jiang, Yuanqing Fu, Fengzhe Xu, Z. Miao, Ting-yu Sun, Jie-sheng Lin, Hui-lian Zhu, Hongwei Zhou, Yu-ming Chen, Ju-Sheng Zheng","Interpretable Machine Learning Framework Reveals Robust Gut Microbiome Features Associated With Type 2 Diabetes",2020,"","","","",60,"2022-07-13 09:23:23","","10.2337/dc20-1536","","",,,,,26,13.00,3,13,2,"OBJECTIVE To identify the core gut microbial features associated with type 2 diabetes risk and potential demographic, adiposity, and dietary factors associated with these features. RESEARCH DESIGN AND METHODS We used an interpretable machine learning framework to identify the type 2 diabetes–related gut microbiome features in the cross-sectional analyses of three Chinese cohorts: one discovery cohort (n = 1,832, 270 cases of type 2 diabetes) and two validation cohorts (cohort 1: n = 203, 48 cases; cohort 2: n = 7,009, 608 cases). We constructed a microbiome risk score (MRS) with the identified features. We examined the prospective association of the MRS with glucose increment in 249 participants without type 2 diabetes and assessed the correlation between the MRS and host blood metabolites (n = 1,016). We transferred human fecal samples with different MRS levels to germ-free mice to confirm the MRS–type 2 diabetes relationship. We then examined the prospective association of demographic, adiposity, and dietary factors with the MRS (n = 1,832). RESULTS The MRS (including 14 microbial features) consistently associated with type 2 diabetes, with risk ratio for per 1-unit change in MRS 1.28 (95% CI 1.23–1.33), 1.23 (1.13–1.34), and 1.12 (1.06–1.18) across three cohorts. The MRS was positively associated with future glucose increment (P < 0.05) and was correlated with a variety of gut microbiota–derived blood metabolites. Animal study further confirmed the MRS–type 2 diabetes relationship. Body fat distribution was found to be a key factor modulating the gut microbiome–type 2 diabetes relationship. CONCLUSIONS Our results reveal a core set of gut microbiome features associated with type 2 diabetes risk and future glucose increment.","",""
0,"H. Tomas Rube, Chaitanya Rastogi, Siqian Feng, J. Kribelbauer, Allyson Li, Basheer Becerra, Lucas A. N. Melo, Bach-Viet Do, Xiaoting Li, Hammaad Adam, Neel H. Shah, R. Mann, H. Bussemaker","Probing molecular specificity with deep sequencing and biophysically interpretable machine learning",2021,"","","","",61,"2022-07-13 09:23:23","","10.1101/2021.06.30.450414","","",,,,,0,0.00,0,13,1,"Quantifying sequence-specific protein-ligand interactions is critical for understanding and exploiting numerous cellular processes, including gene regulation and signal transduction. Next-generation sequencing (NGS) based assays are increasingly being used to profile these interactions with high-throughput. However, these assays do not provide the biophysical parameters that have long been used to uncover the quantitative rules underlying sequence recognition. We developed a highly flexible machine learning framework, called ProBound, to define sequence recognition in terms of biophysical parameters based on NGS data. ProBound quantifies transcription factor (TF) behavior with models that accurately predict binding affinity over a range exceeding that of previous resources, captures the impact of DNA modifications and conformational flexibility of multi-TF complexes, and infers specificity directly from in vivo data such as ChIP-seq without peak calling. When coupled with a new assay called Kd-seq, it determines the absolute affinity of protein-ligand interactions. It can also profile the kinetics of kinase-substrate interactions. By constructing a biophysically robust foundation for profiling sequence recognition, ProBound opens up new avenues for decoding biological networks and rationally engineering protein-ligand interactions.","",""
0,"James Dean, M. Scheffler, Thomas A. R. Purcell, S. Barabash, Rahul Bhowmik, T. Bazhirov","Interpretable Machine Learning for Materials Design",2021,"","","","",62,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,6,1,"Fueled by the widespread adoption of Machine Learning and the high-throughput screening of materials, the data-centric approach to materials design has asserted itself as a robust and powerful tool for the in-silico prediction of materials properties. When training models to predict material properties, researchers often face a difficult choice between a model’s interpretability or its performance. We study this trade-off by leveraging four different state-of-the-art Machine Learning techniques: XGBoost, SISSO, Roost, and TPOT for the prediction of structural and electronic properties of perovskites and 2D materials. We then assess the future outlook of the continued integration of Machine Learning into materials discovery, and identify key problems that will continue to challenge researchers as the size of the literature’s datasets and complexity of models increases. Finally, we offer several possible solutions to these challenges with a focus on retaining interpretability, and share our thoughts on magnifying the impact of Machine Learning on materials design.","",""
10,"Krupal P. Jethava, Jonathan A Fine, Yingqi Chen, Ahad Hossain, G. Chopra","Accelerated Reactivity Mechanism and Interpretable Machine Learning Model of N-Sulfonylimines toward Fast Multicomponent Reactions.",2020,"","","","",63,"2022-07-13 09:23:23","","10.26434/chemrxiv.12116163.v1","","",,,,,10,5.00,2,5,2,"We introduce chemical reactivity flowcharts to help chemists interpret reaction outcomes using statistically robust machine learning models trained on a small number of reactions. We developed fast N-sulfonylimine multicomponent reactions for understanding reactivity and to generate training data. Accelerated reactivity mechanisms were investigated using density functional theory. Intuitive chemical features learned by the model accurately predicted heterogeneous reactivity of N-sulfonylimine with different carboxylic acids. Validation of the predictions shows that reaction outcome interpretation is useful for human chemists.","",""
8,"C. Rea, K. Montes, A. Pau, R. Granetz, O. Sauter","Progress Toward Interpretable Machine Learning–Based Disruption Predictors Across Tokamaks",2020,"","","","",64,"2022-07-13 09:23:23","","10.1080/15361055.2020.1798589","","",,,,,8,4.00,2,5,2,"Abstract In this paper we lay the groundwork for a robust cross-device comparison of data-driven disruption prediction algorithms on DIII-D and JET tokamaks. In order to consistently carry on a comparative analysis, we define physics-based indicators of disruption precursors based on temperature, density, and radiation profiles that are currently not used in many other machine learning predictors for DIII-D data. These profile-based indicators are shown to well-describe impurity accumulation events in both DIII-D and JET discharges that eventually disrupt. The univariate analysis of the features used as input signals in the data-driven algorithms applied on the data of both tokamaks statistically highlights the differences in the dominant disruption precursors. JET with its ITER-like wall is more prone to impurity accumulation events, while DIII-D is more subject to edge-cooling mechanisms that destabilize dangerous magnetohydrodynamic modes. Even though the analyzed data sets are characterized by such intrinsic differences, we show through a few examples that the inclusion of physics-based disruption markers in data-driven algorithms is a promising path toward the realization of a uniform framework to predict and interpret disruptive scenarios across different tokamaks. As long as the destabilizing precursors are diagnosed in a device-independent way, the knowledge that data-driven algorithms learn on one device can be re-used to explain a disruptive behavior on another device.","",""
5,"M. Kaden, Katrin Sophie Bohnsack, Mirko Weber, Mateusz Kudla, Kaja Gutowska, J. Blazewicz, T. Villmann","Analysis of SARS-CoV-2 RNA-Sequences by Interpretable Machine Learning Models",2020,"","","","",65,"2022-07-13 09:23:23","","10.1101/2020.05.15.097741","","",,,,,5,2.50,1,7,2,"We present an approach to investigate SARS-CoV-2 virus sequences based on alignment-free methods for RNA sequence comparison. In particular, we verify a given clustering result for the GISAID data set, which was obtained analyzing the molecular differences in coronavirus populations by phylogenetic trees. For this purpose, we use alignment-free dissimilarity measures for sequences and combine them with learning vector quantization classifiers for virus type discriminant analysis and classification. Those vector quantizers belong to the class of interpretable machine learning methods, which, on the one hand side provide additional knowledge about the classification decisions like discriminant feature correlations, and on the other hand can be equipped with a reject option. This option gives the model the property of self controlled evidence if applied to new data, i.e. the models refuses to make a classification decision, if the model evidence for the presented data is not given. After training such a classifier for the GISAID data set, we apply the obtained classifier model to another but unlabeled SARS-CoV-2 virus data set. On the one hand side, this allows us to assign new sequences to already known virus types and, on the other hand, the rejected sequences allow speculations about new virus types with respect to nucleotide base mutations in the viral sequences. Author summary The currently emerging global disease COVID-19 caused by novel SARS-CoV-2 viruses requires all scientific effort to investigate the development of the viral epidemy, the properties of the virus and its types. Investigations of the virus sequence are of special interest. Frequently, those are based on mathematical/statistical analysis. However, machine learning methods represent a promising alternative, if one focuses on interpretable models, i.e. those that do not act as black-boxes. Doing so, we apply variants of Learning Vector Quantizers to analyze the SARS-CoV-2 sequences. We encoded the sequences and compared them in their numerical representations to avoid the computationally costly comparison based on sequence alignments. Our resulting model is interpretable, robust, efficient, and has a self-controlling mechanism regarding the applicability to data. This framework was applied to two data sets concerning SARS-CoV-2. We were able to verify previously published virus type findings for one of the data sets by training our model to accurately identify the virus type of sequences. For sequences without virus type information (second data set), our trained model can predict them. Thereby, we observe a new scattered spreading of the sequences in the data space which probably is caused by mutations in the viral sequences.","",""
3,"Numair Sani, Jaron J. R. Lee, Razieh Nabi, I. Shpitser","A Semiparametric Approach to Interpretable Machine Learning",2020,"","","","",66,"2022-07-13 09:23:23","","","","",,,,,3,1.50,1,4,2,"Black box models in machine learning have demonstrated excellent predictive performance in complex problems and high-dimensional settings. However, their lack of transparency and interpretability restrict the applicability of such models in critical decision-making processes. In order to combat this shortcoming, we propose a novel approach to trading off interpretability and performance in prediction models using ideas from semiparametric statistics, allowing us to combine the interpretability of parametric regression models with performance of nonparametric methods. We achieve this by utilizing a two-piece model: the first piece is interpretable and parametric, to which a second, uninterpretable residual piece is added. The performance of the overall model is optimized using methods from the sufficient dimension reduction literature. Influence function based estimators are derived and shown to be doubly robust. This allows for use of approaches such as double Machine Learning in estimating our model parameters. We illustrate the utility of our approach via simulation studies and a data application based on predicting the length of stay in the intensive care unit among surgery patients.","",""
6,"D. Raimondi, J. Simm, A. Arany, P. Fariselli, I. Cleynen, Y. Moreau","An interpretable low-complexity machine learning framework for robust exome-based in-silico diagnosis of Crohn’s disease patients",2020,"","","","",67,"2022-07-13 09:23:23","","10.1093/nargab/lqaa011","","",,,,,6,3.00,1,6,2,"Abstract Whole exome sequencing (WES) data are allowing researchers to pinpoint the causes of many Mendelian disorders. In time, sequencing data will be crucial to solve the genome interpretation puzzle, which aims at uncovering the genotype-to-phenotype relationship, but for the moment many conceptual and technical problems need to be addressed. In particular, very few attempts at the in-silico diagnosis of oligo-to-polygenic disorders have been made so far, due to the complexity of the challenge, the relative scarcity of the data and issues such as batch effects and data heterogeneity, which are confounder factors for machine learning (ML) methods. Here, we propose a method for the exome-based in-silico diagnosis of Crohn’s disease (CD) patients which addresses many of the current methodological issues. First, we devise a rational ML-friendly feature representation for WES data based on the gene mutational burden concept, which is suitable for small sample sizes datasets. Second, we propose a Neural Network (NN) with parameter tying and heavy regularization, in order to limit its complexity and thus the risk of over-fitting. We trained and tested our NN on 3 CD case-controls datasets, comparing the performance with the participants of previous CAGI challenges. We show that, notwithstanding the limited NN complexity, it outperforms the previous approaches. Moreover, we interpret the NN predictions by analyzing the learned patterns at the variant and gene level and investigating the decision process leading to each prediction.","",""
2,"Axel X. Montout, R. Bhamber, Debbie S. Lange, Doreen Z. Ndlovu, E. Morgan, C. Ioannou, T. Terrill, J. A. van Wyk, T. Burghardt, A. Dowsey","Accurate and interpretable prediction of poor health in small ruminants with accelerometers and machine learning",2020,"","","","",68,"2022-07-13 09:23:23","","10.1101/2020.08.03.234203","","",,,,,2,1.00,0,10,2,"Accurate assessment of the health status of individual animals is a key step in timely and targeted treatment of infections, which is critical in the fight against anthelmintic and antimicrobial resistance. The FAMACHA scoring system has been used successfully to detect levels of anaemia caused by infection with the parasitic nematode Haemonchus contortus in small ruminants and is an effective way to identify individuals in need of treatment. However, assessing FAMACHA is labour-intensive and costly as individuals must be manually examined at frequent intervals over the Haemonchus season. Here, we show that accelerometers can measure individual activity in extensively grazing small ruminants subject to natural Haemonchus contortus worm infection in southern Africa over long time-scales, and when combined with machine learning, can predict the smallest pre-clinical increases in FAMACHA score as well as those individuals that respond to treatment, all with high precision (>95%). We demonstrate that these classifiers remain robust over time, and remarkably, generalise without retraining across goats and sheep in different regions and types of farming enterprise. Interpretation of the trained classifiers reveal that as the effect of haemonchosis increases, both sheep and goats exhibit a similar reduction in the fine-grained variation of their activity levels. Our study thus reveals common behavioural patterns across small ruminant species, which low-cost biologgers can exploit to detect subtle changes in animal health and enable timely and targeted intervention. This has real potential to improve economic outcomes and animal welfare as well as limit the use of anthelmintic drugs and hence diminish pressures on anthelmintic resistance under conditions of both commercial and resource-poor communal farming. Significance Statement Increasing availability make biologgers and machine learning viable solutions to current challenges in global livestock farming. We demonstrate a pipeline that accurately predicts the earliest signs of parasitic disease in small ruminants. With Haemonchus contortus nematode infection in sheep and goats as the exemplar, we illustrate that the predictive model generalises across time and even species without retraining. We show that prediction is driven by a reduction in the variation of activity levels in animals with poor health. Our findings suggest that health of individual livestock can be monitored remotely, reducing labour costs, improving animal welfare, and allowing for targeted selective treatment under contrasting farming conditions. This will decrease animal loss, maximise economic outcomes, and reduce pressures on drug resistance.","",""
1,"W. Gou, Chu-wen Ling, Yan He, Zengliang Jiang, Yuanqing Fu, Fengzhe Xu, Z. Miao, Ting-yu Sun, Jie-sheng Lin, Hui-lian Zhu, Hongwei Zhou, Yu-ming Chen, Ju-Sheng Zheng","Interpretable machine learning framework reveals novel gut microbiome features in predicting type 2 diabetes",2020,"","","","",69,"2022-07-13 09:23:23","","10.1101/2020.04.05.024984","","",,,,,1,0.50,0,13,2,"Gut microbiome targets for type 2 diabetes (T2D) prevention among human cohorts have been controversial. Using an interpretable machine learning-based analytic framework, we identified robust human gut microbiome features, with their optimal threshold, in predicting T2D. Based on the results, we constructed a microbiome risk score (MRS), which was consistently associated with T2D across 3 independent Chinese cohorts involving 9111 participants (926 T2D cases). The MRS could also predict future glucose increment, and was correlated with a variety of gut microbiota-derived blood metabolites. Faecal microbiota transplantation from humans to germ-free mice demonstrated a causal role of the identified combination of microbes in the T2D development. We further identified adiposity and dietary factors which could prospectively modulate the MRS, and found that body fat distribution may be the key factor modulating the gut microbiome-T2D relationship. Taken together, we proposed a new analytical framework for the investigation of microbiome-disease relationship. The identified microbiota may serve as potential drug targets for T2D in future.","",""
506,"W. James Murdoch, Chandan Singh, Karl Kumbier, R. Abbasi-Asl, Bin Yu","Definitions, methods, and applications in interpretable machine learning",2019,"","","","",70,"2022-07-13 09:23:23","","10.1073/pnas.1900654116","","",,,,,506,168.67,101,5,3,"Significance The recent surge in interpretability research has led to confusion on numerous fronts. In particular, it is unclear what it means to be interpretable and how to select, evaluate, or even discuss methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the underappreciated role played by human audiences. Within this framework, methods are organized into 2 classes: model based and post hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce 3 desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work. Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. However, this increased focus has led to considerable confusion about the notion of interpretability. In particular, it is unclear how the wide array of proposed interpretation methods are related and what common concepts can be used to evaluate them. We aim to address these concerns by defining interpretability in the context of machine learning and introducing the predictive, descriptive, relevant (PDR) framework for discussing interpretations. The PDR framework provides 3 overarching desiderata for evaluation: predictive accuracy, descriptive accuracy, and relevancy, with relevancy judged relative to a human audience. Moreover, to help manage the deluge of interpretation methods, we introduce a categorization of existing techniques into model-based and post hoc categories, with subgroups including sparsity, modularity, and simulatability. To demonstrate how practitioners can use the PDR framework to evaluate and understand interpretations, we provide numerous real-world examples. These examples highlight the often underappreciated role played by human audiences in discussions of interpretability. Finally, based on our framework, we discuss limitations of existing methods and directions for future work. We hope that this work will provide a common vocabulary that will make it easier for both practitioners and researchers to discuss and choose from the full range of interpretation methods.","",""
0,"Gabriel D. Patrón, D. León, Edwin Lopez, G. Hernández","An Interpretable Automated Machine Learning Credit Risk Model",2020,"","","","",71,"2022-07-13 09:23:23","","10.1007/978-3-030-61834-6_2","","",,,,,0,0.00,0,4,2,"","",""
1,"Xiaoli Liu, P. Hu, Z. Mao, Po-Chih Kuo, Peiyao Li, Chao Liu, Jie Hu, Deyu Li, Desen Cao, R. Mark, L. Celi, Zhengbo Zhang, F. Zhou","Interpretable Machine Learning Model for Early Prediction of Mortality in Elderly Patients with Multiple Organ Dysfunction Syndrome (MODS): a Multicenter Retrospective Study and Cross Validation",2020,"","","","",72,"2022-07-13 09:23:23","","","","",,,,,1,0.50,0,13,2,"Background: Elderly patients with MODS have high risk of death and poor prognosis. The performance of current scoring systems assessing the severity of MODS and its mortality remains unsatisfactory. This study aims to develop an interpretable and generalizable model for early mortality prediction in elderly patients with MODS. Methods: The MIMIC-III, eICU-CRD and PLAGH-S databases were employed for model generation and evaluation. We used the eXtreme Gradient Boosting model with the SHapley Additive exPlanations method to conduct early and interpretable predictions of patients' hospital outcome. Three types of data source combinations and five typical evaluation indexes were adopted to develop a generalizable model. Findings: The interpretable model, with optimal performance developed by using MIMIC-III and eICU-CRD datasets, was separately validated in MIMIC-III, eICU-CRD and PLAGH-S datasets (no overlapping with training set). The performances of the model in predicting hospital mortality as validated by the three datasets were: AUC of 0.858, sensitivity of 0.834 and specificity of 0.705; AUC of 0.849, sensitivity of 0.763 and specificity of 0.784; and AUC of 0.838, sensitivity of 0.882 and specificity of 0.691, respectively. Comparisons of AUC between this model and baseline models with MIMIC-III dataset validation showed superior performances of this model; In addition, comparisons in AUC between this model and commonly used clinical scores showed significantly better performance of this model. Interpretation: The interpretable machine learning model developed in this study using fused datasets with large sample sizes was robust and generalizable. This model outperformed the baseline models and several clinical scores for early prediction of mortality in elderly ICU patients. The interpretative nature of this model provided clinicians with the ranking of mortality risk features.","",""
200,"W. James Murdoch, Chandan Singh, Karl Kumbier, R. Abbasi-Asl, Bin Yu","Interpretable machine learning: definitions, methods, and applications",2019,"","","","",73,"2022-07-13 09:23:23","","10.1073/pnas.1900654116","","",,,,,200,66.67,40,5,3,"M learning (ML) has recently received considerable attention for its ability to accurately predict a wide variety of complex phenomena. However, there is a growing realization that, in addition to predictions, ML models are capable of producing knowledge about domain relationships contained in data, often referred to as interpretations. These interpretations have found uses both in their own right, e.g. medicine (1), policy-making (2), and science (3, 4), as well as in auditing the predictions themselves in response to issues such as regulatory pressure (5) and fairness (6). In the absence of a well-formed definition of interpretability, a broad range of methods with a correspondingly broad range of outputs (e.g. visualizations, natural language, mathematical equations) have been labeled as interpretation. This has led to considerable confusion about the notion of interpretability. In particular, it is unclear what it means to interpret something, what common threads exist among disparate methods, and how to select an interpretation method for a particular problem/audience. In this paper, we attempt to address these concerns. To do so, we first define interpretability in the context of machine learning and place it within a generic data science life cycle. This allows us to distinguish between two main classes of interpretation methods: model-based∗ and post hoc. We then introduce the Predictive, Descriptive, Relevant (PDR) framework, consisting of three desiderata for evaluating and constructing interpretations: predictive accuracy, descriptive","",""
0,"Mimansa Jaiswal","Interpreting Multimodal Machine Learning Models Trained for Emotion Recognition to Address Robustness and Privacy Concerns",2020,"","","","",74,"2022-07-13 09:23:23","","10.1609/aaai.v34i10.7130","","",,,,,0,0.00,0,1,2,"Many mobile applications and virtual conversational agents now aim to recognize and adapt to emotions. These predicted emotions are used in variety of downstream applications: (a) generating more human like dialogues, (b) predicting mental health issues, and (c) hate speech detection and intervention. To enable this, data are transmitted from users' devices and stored on central servers. These data are then processed further, either annotated or used as inputs for training a model for a specific task. Yet, these data contain sensitive information that could be used by mobile applications without user's consent or, maliciously, by an eavesdropping adversary. My work focuses on two major issues that are faced while training emotion recognition algorithms: (a) privacy of the generated representations and, (b) explaining and ensuring that the predictions are robust to various situations. Tackling these issues would lead to emotion based algorithms that are deployable and helpful at a larger scale, thus enabling more human like experience when interacting with AI.","",""
11,"Evan Greene, Greg Finak, Leonard D'Amico, N. Bhardwaj, C. Church, C. Morishima, N. Ramchurren, J. Taube, P. Nghiem, M. Cheever, S. Fling, R. Gottardo","New interpretable machine learning method for single-cell data reveals correlates of clinical response to cancer immunotherapy",2019,"","","","",75,"2022-07-13 09:23:23","","10.1101/702118","","",,,,,11,3.67,1,12,3,"High-dimensional single-cell cytometry is routinely used to characterize patient responses to cancer immunotherapy and other treatments. This has produced a wealth of datasets ripe for exploration but whose biological and technical heterogeneity make them difficult to analyze with current tools. We introduce a new interpretable machine learning method for single-cell mass and flow cytometry studies, FAUST, that robustly performs unbiased cell population discovery and annotation. FAUST processes data on a per-sample basis and returns biologically interpretable cell phenotypes that can be compared across studies, making it well-suited for the analysis and integration of complex datasets. We demonstrate how FAUST can be used for candidate biomarker discovery and validation by applying it to a flow cytometry dataset from a Merkel cell carcinoma anti-PD-1 trial and discover new CD4+ and CD8+ effector-memory T cell correlates of outcome co-expressing PD-1, HLA-DR, and CD28. We then use FAUST to validate these correlates in an independent CyTOF dataset from a published metastatic melanoma trial. Importantly, existing state-of-the-art computational discovery approaches as well as prior manual analysis did not detect these or any other statistically significant T cell sub-populations associated with anti-PD-1 treatment in either data set. We further validate our methodology by using FAUST to replicate the discovery of a previously reported myeloid correlate in a different published melanoma trial, and validate the correlate by identifying itde novoin two additional independent trials. FAUST’s phenotypic annotations can be used to perform cross-study data integration in the presence of heterogeneous data and diverse immunophenotyping staining panels, enabling hypothesis-driven inference about cell sub-population abundance through a multivariate modeling framework we callPhenotypic andFunctionalDifferentialAbundance (PFDA). We demonstrate this approach on data from myeloid and T cell panels across multiple trials. Together, these results establish FAUST as a powerful and versatile new approach for unbiased discovery in single-cell cytometry.","",""
11,"Tamer Karatekin, S. Sancak, G. Celik, S. Topçuoğlu, G. Karatekin, Pınar Kırcı, A. Okatan","Interpretable Machine Learning in Healthcare through Generalized Additive Model with Pairwise Interactions (GA2M): Predicting Severe Retinopathy of Prematurity",2019,"","","","",76,"2022-07-13 09:23:23","","10.1109/Deep-ML.2019.00020","","",,,,,11,3.67,2,7,3,"We have investigated the risk factors that lead to severe retinopathy of prematurity using statistical analysis and logistic regression as a form of generalized additive model (GAM) with pairwise interaction terms (GA2M). In this process, we discuss the trade-off between accuracy and interpretability of these machine learning techniques on clinical data. We also confirm the intuition of expert neonatologists on a few risk factors, such as gender, that were previously deemed as clinically not significant in RoP prediction.","",""
19,"Bradley C. Boehmke, Brandon M. Greenwell","Interpretable Machine Learning",2019,"","","","",77,"2022-07-13 09:23:23","","10.1201/9780367816377-16","","",,,,,19,6.33,10,2,3,"","",""
191,"M. Ahmad, A. Teredesai, C. Eckert","Interpretable Machine Learning in Healthcare",2018,"","","","",78,"2022-07-13 09:23:23","","10.1145/3233547.3233667","","",,,,,191,47.75,64,3,4,"This tutorial extensively covers the definitions, nuances, challenges, and requirements for the design of interpretable and explainable machine learning models and systems in healthcare. We discuss many uses in which interpretable machine learning models are needed in healthcare and how they should be deployed. Additionally, we explore the landscape of recent advances to address the challenges model interpretability in healthcare and also describe how one would go about choosing the right interpretable machine learnig algorithm for a given problem in healthcare.","",""
199,"Christoph Molnar, Giuseppe Casalicchio, B. Bischl","iml: An R package for Interpretable Machine Learning",2018,"","","","",79,"2022-07-13 09:23:23","","10.21105/joss.00786","","",,,,,199,49.75,66,3,4,"Complex, non-parametric models, which are typically used in machine learning, have proven to be successful in many prediction tasks. But these models usually operate as black boxes: While they are good at predicting, they are often not interpretable. Many inherently interpretable models have been suggested, which come at the cost of losing predictive power. Another option is to apply interpretability methods to a black box model after model training. Given the velocity of research on new machine learning models, it is preferable to have model-agnostic tools which can be applied to a random forest as well as to a neural network. Tools for model-agnostic interpretability methods should improve the adoption of machine learning.","",""
5,"Gideon A. Lyngdoh, Mohd Zaki, N. Krishnan, Sumanta","Prediction of Concrete Strengths Enabled by Missing Data Imputation and Interpretable Machine Learning",2022,"","","","",80,"2022-07-13 09:23:23","","","","",,,,,5,5.00,1,4,1,"Machine learning (ML)-based prediction of non-linear composition-strength relationship in concretes requires a large, complete, and consistent dataset. However, the availability of such datasets is limited as the datasets often suffer from incompleteness because of missing data corresponding to different input features, which makes the development of robust ML-based predictive models challenging. Besides, as the degree of complexity in these ML models increases, the interpretation of the results becomes challenging. These interpretations of results are critical towards the development of efficient materials design strategies for enhanced materials performance. To address these challenges, this paper implements different data imputation approaches for enhanced dataset completeness. The imputed dataset is leveraged to predict the compressive and tensile strength of concrete using various hyperparameteroptimized ML approaches. Among all the approaches, Extreme Gradient Boosted Decision Trees (XGBoost) showed the highest prediction efficacy when the dataset is imputed using k-nearest neighbors (kNN) with a 10-neighbor configuration. To interpret the predicted results, SHapley Additive exPlanations (SHAP) is employed. Overall, by implementing efficient combinations of data imputation approach, machine learning, and data interpretation, this paper develops an efficient approach to evaluate the compositionstrength relationship in concrete. This work, in turn, can be used as a starting point toward the design and development of various performance-enhanced and sustainable concretes.","",""
0,"Yilin Ning, Siqi Li, M. Ong, F. Xie, B. Chakraborty, D. Ting, Nan Liu","A novel interpretable machine learning system to generate clinical risk scores: An application for predicting early mortality or unplanned readmission in a retrospective cohort study",2022,"","","","",81,"2022-07-13 09:23:23","","10.1371/journal.pdig.0000062","","",,,,,0,0.00,0,7,1,"Risk scores are widely used for clinical decision making and commonly generated from logistic regression models. Machine-learning-based methods may work well for identifying important predictors to create parsimonious scores, but such ‘black box’ variable selection limits interpretability, and variable importance evaluated from a single model can be biased. We propose a robust and interpretable variable selection approach using the recently developed Shapley variable importance cloud (ShapleyVIC) that accounts for variability in variable importance across models. Our approach evaluates and visualizes overall variable contributions for in-depth inference and transparent variable selection, and filters out non-significant contributors to simplify model building steps. We derive an ensemble variable ranking from variable contributions across models, which is easily integrated with an automated and modularized risk score generator, AutoScore, for convenient implementation. In a study of early death or unplanned readmission after hospital discharge, ShapleyVIC selected 6 variables from 41 candidates to create a well-performing risk score, which had similar performance to a 16-variable model from machine-learning-based ranking. Our work contributes to the recent emphasis on interpretability of prediction models for high-stakes decision making, providing a disciplined solution to detailed assessment of variable importance and transparent development of parsimonious clinical risk scores.","",""
0,"Joseph Giorgio, W. Jagust, S. Baker, S. Landau, P. Tiňo, Z. Kourtzi","A robust and interpretable machine learning approach using multimodal biological data to predict future pathological tau accumulation",2022,"","","","",82,"2022-07-13 09:23:23","","10.1038/s41467-022-28795-7","","",,,,,0,0.00,0,6,1,"","",""
0,"Gang Yu, Jiawang Tao, Jie Wang","Odysseia: Genetic Regulatory Feature Analysis with Interpretable Classification Machine Learning Models",2022,"","","","",83,"2022-07-13 09:23:23","","10.1101/2022.02.17.480852","","",,,,,0,0.00,0,3,1,"With rapid progress of robust single-cell transcriptome sequencing since last decade, numerous complex mechanisms underlying cell development has been revealed. Single-cell RNA sequencing (scRNA-seq) analysis is widely accepted as the main approach to define cell stages and phenotypes. As conversion of somatic cells into induced pluripotency cells succeeded, identification key genetic factors(GFs) with scRNA-seq for cell reprogramming in biological research and regenerative medicine fields gained increasing attention. Herein, we describe Odysseia, an interpretable machine learning classifier based single-cell gene expression profile(scGEP) analysis system, that assesses importances of genetic regulatory features in differentiating cell states(CSs). Furthermore, extracted factors, when combining with regulatory network analysis, can help to find key GFs in classifying CSs and possibly inducing CS conversions. Analyzed three published scRNA-seq datasets used to study divergent cell types, Odysseia correctly extracted GFs acclaimed to be capable of inducing CS conversions. Overall, Odysseia provides an automated alternative to obtain guidance information while explicating mechanism to engineer cellular phenotypes.","",""
0,"L. Tideman","Interpretable Machine Learning for Biomarker Discovery in Imaging Mass Spectrometry Data",2019,"","","","",84,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,1,3,"Imaging mass spectrometry (IMS) is a multiplexed chemical imaging technique that enables the spatially targeted molecular mapping of biological samples at cellular resolutions. Within a single experiment, IMS can measure the spatial distribution and relative concentration of thousands of distinct molecular species across the surface of a tissue sample. The large size and high-dimensionality of IMS datasets, which can consist of hundreds of thousands of pixels and hundreds to thousands of molecular ions tracked per pixel, have made computational approaches necessary for effective analysis. This thesis focuses primarily on biomarker discovery in IMS data using supervised machine learning algorithms. Biomarker discovery is the identification of molecular markers that enable the recognition of a specific biological state, for example recognizing diseased tissue from healthy tissue. Biomarkers are increasingly used in biology and medicine for diagnostic and prognostic purposes, as well as for driving the development of new drugs and therapies. Traditionally, the focus has been on maximizing the predictive performance of supervised machine learning models, without necessarily examining the models' internal decision-making processes. Yet, in order to generate insight into the underlying chemical mechanism of disease or drug action, we must go beyond the scope of just prediction and learn how these empirically trained models make their decisions and who are the primary chemical drivers of this prediction process. Machine learning model interpretability is the ability to explain a model's predictions, and can practically be translated into the ability to explicitly report the relative predictive importance of each of the dataset's features. When analyzing IMS data, interpretability is crucial for understanding how the spatial distribution and relative concentration of certain molecular features relate to the labeling of pixels into different physiological classes. The key to our data-driven approach to biomarker discovery in IMS data is to establish (in relation to a specific biomedical recognition task) a means of ranking the molecular features of supervised machine learning models according to their respective predictive importance scores. Ensuring model interpretability and feature ranking in supervised machine learning allows empirical model building to be used as a filtering mechanism to rapidly determine, among thousands of features, those features that exert a large amount of relevance to a specific class determination. With regards to biology, the top-ranking features can help empirically highlight important molecular drivers in the biological process under examination, and can help generate new hypotheses. In terms of translational medicine, such top-ranking features can yield a shortlist of candidate biomarkers worthy of further clinical investigation. Three different classifiers, namely logistic regression, random forests, and support vector machines, are implemented and their performance is compared in terms of accuracy, precision, recall, scale invariance, sensitivity to noise, and computational efficiency. Subsequently, several approaches to explaining these classifiers' predictions are implemented and investigated: model-specific interpretability methods are tied to intrinsically interpretable classifiers, such as generalized linear models and decision trees, whereas model-agnostic interpretability methods can also explain the predictions of black-box models, such as support vector machines with nonlinear kernels or deep neural networks. In addition to three model-specific methods, we present two post-hoc model-agnostic interpretability methods: permutation importance and Shapley importance. Our implementation of Shapley importance, based on Shapley values from cooperative game theory, is novel. Having observed a variability between the rankings of different interpretability methods, we investigate improving the inter-method reliability of feature rankings by decorrelating the features prior to training the classifiers. We also propose a robust ensemble approach to interpretability that aggregates the importance scores attributed to each feature by different model-specific interpretability methods. We demonstrate our methodology on two biomedical case studies: one MALDI-FTICR IMS dataset taken from the coronal section of a rat brain, and one MALDI-TOF IMS dataset taken from the sagittal section of a mouse-pup.","",""
0,"J. Sarkar, Cory Peterson","Enabling Prognostics of Robust Design with Interpretable Machine Learning",2019,"","","","",85,"2022-07-13 09:23:23","","10.1109/IEDM19573.2019.8993481","","",,,,,0,0.00,0,2,3,"Design of robust systems needs to fully account for reliability physics, operational stresses and interactions thereof - while accommodating range of stresses from qualification to field. This research demonstrates the method of empirically analyzing system-internal parametric data of Solid-State Storage devices (SSD) with Machine Learning (ML). ML is shown to be a necessary, effective and novel means of proactively assessing and interpreting prognostics of the resilient system design. The methodologies and results also bear strong relevance to assessment of current and future designs for evolving usage models and new application areas.","",""
0,"Yuan Wang, Liping Yang, Jun Wu, Zisheng Song, Li-nan Shi","Mining Campus Big Data: Prediction of Career Choice Using Interpretable Machine Learning Method",2022,"","","","",86,"2022-07-13 09:23:23","","10.3390/math10081289","","",,,,,0,0.00,0,5,1,"The issue of students’ career choice is the common concern of students themselves, parents, and educators. However, students’ behavioral data have not been thoroughly studied for understanding their career choice. In this study, we used eXtreme Gradient Boosting (XGBoost), a machine learning (ML) technique, to predict the career choice of college students using a real-world dataset collected in a specific college. Specifically, the data include information on the education and career choice of 18,000 graduates during their college years. In addition, SHAP (Shapley Additive exPlanation) was employed to interpret the results and analyze the importance of individual features. The results show that XGBoost can predict students’ career choice robustly with a precision, recall rate, and an F1 value of 89.1%, 85.4%, and 0.872, respectively. Furthermore, the interaction of features among four different choices of students (i.e., choose to study in China, choose to work, difficulty in finding a job, and choose to study aboard) were also explored. Several educational features, especially differences in grade point average (GPA) during their college studying, are found to have relatively larger impact on the final choice of career. These results can be of help in the planning, design, and implementation of higher educational institutions’ (HEIs) events.","",""
45,"H. Escalante, S. Escalera, I. Guyon, Xavier Baró, Yağmur Güçlütürk, Umut Güçlü, M. V. Gerven","Explainable and Interpretable Models in Computer Vision and Machine Learning",2018,"","","","",87,"2022-07-13 09:23:23","","10.1007/978-3-319-98131-4","","",,,,,45,11.25,6,7,4,"","",""
0,"R. Shokri","Trusting Machine Learning: Privacy, Robustness, and Transparency Challenges",2019,"","","","",88,"2022-07-13 09:23:23","","10.1145/3335203.3335728","","",,,,,0,0.00,0,1,3,"Machine learning algorithms have shown an unprecedented predictive power for many complex learning tasks. As they are increasingly being deployed in large scale critical applications for processing various types of data, new questions related to their trustworthiness would arise. Can machine learning algorithms be trusted to have access to individuals’ sensitive data? Can they be robust against noisy or adversarially perturbed data? Can we reliably interpret their learning process, and explain their predictions? In this talk, I will go over the challenges of building trustworthy machine learning algorithms in centralized and distributed (federated) settings, and will discuss the inter-relation between privacy, robustness, and interpretability.","",""
0,"Jianbo Chen","Towards Interpretability and Robustness of Machine Learning Models",2019,"","","","",89,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,1,3,"Author(s): Chen, Jianbo | Advisor(s): Jordan, Michael I; Wainwright, Martin J | Abstract: Modern machine learning models can be difficult to probe and understand after they have been trained. This is a major problem for the field, with consequences for trustworthiness, diagnostics, debugging, robustness, and a range of other engineering and human interaction issues surrounding the deployment of a model. Another problem of modern machine learning models is their vulnerability to small adversarial perturbations to the input, which incurs a security risk when they are applied to critical areas.In this thesis, we develop systematic and efficient tools for interpreting machine learning models and evaluating their adversarial robustness. Part I focuses on model interpretation. We derive an efficient feature scoring method by exploiting the graph structure in data. We also develop a learning-based method under an information-based framework. As an attempt to leverage prior knowledge about what constitutes a satisfying interpretation in a given domain, we propose a systematic approach to exploiting syntactic constituency structure by leveraging a parse tree for interpretation of models in the setting of linguistic data. Part II focuses on the evaluation of adversarial robustness. We first propose a probabilistic framework for generating adversarial examples on discrete data, and develop two algorithms to implement it. We also introduce a novel attack method in the setting where the attacker has access to model decisions alone. We investigate the robustness of various machine learning models and existing defense mechanisms under the proposed attack method. In Part III, we build a connection between the two fields by developing a method for detecting adversarial examples via tools in model interpretation.","",""
0,"Daniel Grahn, Melonie Richey","The prediction management framework: ethical, governable, and interpretable deployment of artificial intelligence/machine learning systems",2022,"","","","",90,"2022-07-13 09:23:23","","10.1117/12.2617772","","",,,,,0,0.00,0,2,1,"As defense organizations integrate artificial intelligence (AI) into evermore critical operations, especially those near the tactical edge with real-time decision making, the necessity of a standardized, robust framework for deployment and management of AI systems is increasing. In this paper, we propose a Prediction Management Framework (PMF) that aligns with the Department of Defense’s Ethical Principles for AI for ethical, governable, and interpretable deployments. We explore different requirements for the framework with inspiration drawn from various regulatory, safety, and communication standards. In support of these requirements, we offer recommendations and implementation guidance to provide comprehensive visibility into the system.","",""
26,"M. Hasan, Md. Ashad Alam, W. Shoombuatong, H. Deng, Balachandran Manavalan, H. Kurata","NeuroPred-FRL: an interpretable prediction model for identifying neuropeptide using feature representation learning.",2021,"","","","",91,"2022-07-13 09:23:23","","10.1093/bib/bbab167","","",,,,,26,26.00,4,6,1,"Neuropeptides (NPs) are the most versatile neurotransmitters in the immune systems that regulate various central anxious hormones. An efficient and effective bioinformatics tool for rapid and accurate large-scale identification of NPs is critical in immunoinformatics, which is indispensable for basic research and drug development. Although a few NP prediction tools have been developed, it is mandatory to improve their NPs' prediction performances. In this study, we have developed a machine learning-based meta-predictor called NeuroPred-FRL by employing the feature representation learning approach. First, we generated 66 optimal baseline models by employing 11 different encodings, six different classifiers and a two-step feature selection approach. The predicted probability scores of NPs based on the 66 baseline models were combined to be deemed as the input feature vector. Second, in order to enhance the feature representation ability, we applied the two-step feature selection approach to optimize the 66-D probability feature vector and then inputted the optimal one into a random forest classifier for the final meta-model (NeuroPred-FRL) construction. Benchmarking experiments based on both cross-validation and independent tests indicate that the NeuroPred-FRL achieves a superior prediction performance of NPs compared with the other state-of-the-art predictors. We believe that the proposed NeuroPred-FRL can serve as a powerful tool for large-scale identification of NPs, facilitating the characterization of their functional mechanisms and expediting their applications in clinical therapy. Moreover, we interpreted some model mechanisms of NeuroPred-FRL by leveraging the robust SHapley Additive exPlanation algorithm.","",""
13,"Xinlei Mi, Baiming Zou, F. Zou, J. Hu","Permutation-based identification of important biomarkers for complex diseases via machine learning models",2021,"","","","",92,"2022-07-13 09:23:23","","10.1038/s41467-021-22756-2","","",,,,,13,13.00,3,4,1,"","",""
14,"Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, D. Dou","Interpretable Deep Learning: Interpretations, Interpretability, Trustworthiness, and Beyond",2021,"","","","",93,"2022-07-13 09:23:23","","","","",,,,,14,14.00,2,8,1,"Deep neural networks have been well-known for their superb performance in handling various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal the ways that deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we introduce and clarify two basic concepts— interpretations and interpretability—that people usually get confused. First of all, to address the research efforts in interpretations, we elaborate the design of several recent interpretation algorithms, from different perspectives, through proposing a new taxonomy. Then, to understand the results of interpretation, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the existing work in evaluating models’ interpretability using “trustworthy” interpretation algorithms. Finally, we review and discuss the connections between deep models’ interpretations and other factors, such as adversarial robustness and data augmentations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.","",""
5,"A. Pierrefeu, Tommy Löfstedt, C. Laidi, F. Hadj-Selem, M. Leboyer, P. Ciuciu, J. Houenou, E. Duchesnay","Interpretable and stable prediction of schizophrenia on a large multisite dataset using machine learning with structured sparsity",2018,"","","","",94,"2022-07-13 09:23:23","","10.1109/PRNI.2018.8423946","","",,,,,5,1.25,1,8,4,"The use of machine-learning (ML) in neuroimaging offers new perspectives in early diagnosis and prognosis of brain diseases. Indeed, ML algorithms can jointly examine all brain features to capture complex relationships in the data in order to make inferences at a single-subject level. To deal with such high dimensional input and the associated risk of overfitting on the training data, a proper regularization (or feature selection) is required. Standard ℓ2-regularized predictors, such as Support Vector Machine, provide dense patterns of predictors. However, in the context of predictive disease signature discovery, it is now essential to understand the brain pattern that underpins the prediction. Despite ℓ1-regularized (sparse) has often been advocated as leading to more interpretable models, they generally lead to scattered and unstable patterns. We hypothesize that the integration of prior knowledge regarding the structure of the input images should improve the relevance and the stability of the predictive signature. Such structured sparsity can be obtained by combining together ℓ1 (possibly ℓ2) and Total variation (TV) penalties. We demonstrated the relevance of using ML with structured sparsity on a large multisite dataset of schizophrenia patients and controls. Using 3D maps of grey matter density, we obtained promising inter-site prediction performances. More importantly, we have uncovered a predictive signature of schizophrenia that is clinically interpretable and stable across resampling. This suggests that structured sparsity provides a major breakthrough over ‘off-the-shelf’ algorithms to perform a robust selection of important brain regions in the context of biomarkers discovery.","",""
12,"E. M. Mortani Barbosa, B. Georgescu, S. Chaganti, G. Alemañ, Jordi Broncano Cabrero, G. Chabin, T. Flohr, P. Grenier, Sasa Grbic, Nakul Gupta, F. Mellot, S. Nicolaou, Thomas J. Re, P. Sanelli, A. Sauter, Y. Yoo, Valentin Ziebandt, D. Comaniciu","Machine learning automatically detects COVID-19 using chest CTs in a large multicenter cohort",2020,"","","","",95,"2022-07-13 09:23:23","","10.1007/s00330-021-07937-3","","",,,,,12,6.00,1,18,2,"","",""
48,"Clemens Stachl, F. Pargent, S. Hilbert, Gabriella M. Harari, Ramona Schoedel, Sumer S. Vaid, S. Gosling, M. Bühner","Personality Research and Assessment in the Era of Machine Learning",2019,"","","","",96,"2022-07-13 09:23:23","","10.1002/per.2257","","",,,,,48,16.00,6,8,3,"The increasing availability of high–dimensional, fine–grained data about human behaviour, gathered from mobile sensing studies and in the form of digital footprints, is poised to drastically alter the way personality psychologists perform research and undertake personality assessment. These new kinds and quantities of data raise important questions about how to analyse the data and interpret the results appropriately. Machine learning models are well suited to these kinds of data, allowing researchers to model highly complex relationships and to evaluate the generalizability and robustness of their results using resampling methods. The correct usage of machine learning models requires specialized methodological training that considers issues specific to this type of modelling. Here, we first provide a brief overview of past studies using machine learning in personality psychology. Second, we illustrate the main challenges that researchers face when building, interpreting, and validating machine learning models. Third, we discuss the evaluation of personality scales, derived using machine learning methods. Fourth, we highlight some key issues that arise from the use of latent variables in the modelling process. We conclude with an outlook on the future role of machine learning models in personality research and assessment.","",""
4,"Alexandra Renouard, A. Maggi, M. Grunberg, C. Doubre, C. Hibert","Toward False Event Detection and Quarry Blast versus Earthquake Discrimination in an Operational Setting Using Semiautomated Machine Learning",2021,"","","","",97,"2022-07-13 09:23:23","","10.1785/0220200305","","",,,,,4,4.00,1,5,1,"  Small-magnitude earthquakes shed light on the spatial and magnitude distribution of natural seismicity, as well as its rate and occurrence, especially in stable continental regions where natural seismicity remains difficult to explain under slow strain-rate conditions. However, capturing them in catalogs is strongly hindered by signal-to-noise ratio issues, resulting in high rates of false and man-made events also being detected. Accurate and robust discrimination of these events is critical for optimally detecting small earthquakes. This requires uncovering recurrent salient features that can rapidly distinguish first false events from real events, then earthquakes from man-made events (mainly quarry blasts), despite high signal variability and noise content. In this study, we combined the complementary strengths of human and interpretable rule-based machine-learning algorithms for solving this classification problem. We used human expert knowledge to co-create two reliable machine-learning classifiers through human-assisted selection of classification features and review of events with uncertain classifier predictions. The two classifiers are integrated into the SeisComP3 operational monitoring system. The first one discards false events from the set of events obtained with a low short-term average/long-term average threshold; the second one labels the remaining events as either earthquakes or quarry blasts. When run in an operational setting, the first classifier correctly detected more than 99% of false events and just over 93% of earthquakes; the second classifier correctly labeled 95% of quarry blasts and 96% of earthquakes. After a manual review of the second classifier low-confidence outputs, the final catalog contained fewer than 2% of misclassified events. These results confirm that machine learning strengthens the quality of earthquake catalogs and that the performance of machine-learning classifiers can be improved through human expertise. Our study promotes a broader implication of hybrid intelligence monitoring within seismological observatories.","",""
242,"Pantelis Linardatos, Vasilis Papastefanopoulos, S. Kotsiantis","Explainable AI: A Review of Machine Learning Interpretability Methods",2020,"","","","",98,"2022-07-13 09:23:23","","10.3390/e23010018","","",,,,,242,121.00,81,3,2,"Recent advances in artificial intelligence (AI) have led to its widespread industrial adoption, with machine learning systems demonstrating superhuman performance in a significant number of tasks. However, this surge in performance, has often been achieved through increased model complexity, turning such systems into “black box” approaches and causing uncertainty regarding the way they operate and, ultimately, the way that they come to decisions. This ambiguity has made it problematic for machine learning systems to be adopted in sensitive yet critical domains, where their value could be immense, such as healthcare. As a result, scientific interest in the field of Explainable Artificial Intelligence (XAI), a field that is concerned with the development of new methods that explain and interpret machine learning models, has been tremendously reignited over recent years. This study focuses on machine learning interpretability methods; more specifically, a literature review and taxonomy of these methods are presented, as well as links to their programming implementations, in the hope that this survey would serve as a reference point for both theorists and practitioners.","",""
3,"F. Biessmann, D. Refiano","Quality Metrics for Transparent Machine Learning With and Without Humans In the Loop Are Not Correlated",2021,"","","","",99,"2022-07-13 09:23:23","","","","",,,,,3,3.00,2,2,1,"The field explainable artificial intelligence (XAI) has brought about an arsenal of methods to render Machine Learning (ML) predictions more interpretable. But how useful explanations provided by transparent ML methods are for humans remains difficult to assess. Here we investigate the quality of interpretable computer vision algorithms using techniques from psychophysics. In crowdsourced annotation tasks we study the impact of different interpretability approaches on annotation accuracy and task time. We compare these quality metrics with classical XAI, automated quality metrics. Our results demonstrate that psychophysical experiments allow for robust quality assessment of transparency in machine learning. Interestingly the quality metrics computed without humans in the loop did not provide a consistent ranking of interpretability methods nor were they representative for how useful an explanation was for humans. These findings highlight the potential of methods from classical psychophysics for modern machine learning applications. We hope that our results provide convincing arguments for evaluating interpretability in its natural habitat, human-ML interaction, if the goal is to obtain an authentic assessment of interpretability.","",""
2,"S. Newman, R. Furbank","Explainable machine learning models of major crop traits from satellite-monitored continent-wide field trial data",2021,"","","","",100,"2022-07-13 09:23:23","","10.1101/2021.03.08.434495","","",,,,,2,2.00,1,2,1,"Four species of grass generate half of all human-consumed calories1. However, abundant biological data on species that produce our food remains largely inaccessible, imposing direct barriers to understanding crop yield and fitness traits. Here, we assemble and analyse a continent-wide database of field experiments spanning ten years and hundreds of thousands of machine-phenotyped populations of ten major crop species. Training an ensemble of machine learning models, using thousands of variables capturing weather, ground-sensor, soil, chemical and fertiliser dosage, management, and satellite data, produces robust cross-continent yield models exceeding R2 = 0.8 prediction accuracy. In contrast to ‘black box’ analytics, detailed interrogation of these models reveals fundamental drivers of crop behaviour and complex interactions predicting yield and agronomic traits. These results demonstrate the capacity of machine learning models to build unified, interpretable, and explainable models of crop behaviour, and highlight the powerful role of data in the future of food.","",""
3,"A. Heinlein, A. Klawonn, M. Lanser, J. Weber","Combining Machine Learning and Adaptive Coarse Spaces---A Hybrid Approach for Robust FETI-DP Methods in Three Dimensions",2020,"","","","",101,"2022-07-13 09:23:23","","10.1137/20m1344913","","",,,,,3,1.50,1,4,2,"The hybrid ML-FETI-DP algorithm combines the advantages of adaptive coarse spaces in domain decomposition methods and certain supervised machine learning techniques. Adaptive coarse spaces ensure robustness of highly scalable domain decomposition solvers, even for highly heterogeneous coefficient distributions with arbitrary coefficient jumps. However, their construction requires the setup and solution of local generalized eigenvalue problems, which is typically computationally expensive. The idea of ML-FETI-DP is to interpret the coefficient distribution as image data and predict whether an eigenvalue problem has to be solved or can be neglected while still maintaining robustness of the adaptive FETI-DP method. For this purpose, neural networks are used as image classifiers. In the present work, the ML-FETI-DP algorithm is extended to three dimensions, which requires both a complex data preprocessing procedure to construct consistent input data for the neural network as well as a representative training and validation data set to ensure generalization properties of the machine learning model. Numerical experiments for stationary diffusion and linear elasticity problems with realistic coefficient distributions show that a large number of eigenvalue problems can be saved; in the best case of the numerical results presented here, 97% of the eigenvalue problems can be avoided to be set up and solved.","",""
1,"Tim G. J. Rudner, H. Toner","Key Concepts in AI Safety: Interpretability in Machine Learning",2021,"","","","",102,"2022-07-13 09:23:23","","10.51593/20190042","","",,,,,1,1.00,1,2,1,"This paper is the third installment in a series on “AI safety,” an area of machine learning research that aims to identify causes of unintended behavior in machine learning systems and develop tools to ensure these systems work safely and reliably. The first paper in the series, “Key Concepts in AI Safety: An Overview,” described three categories of AI safety issues: problems of robustness, assurance, and specification. This paper introduces interpretability as a means to enable assurance in modern machine learning systems.","",""
1,"Yusuke Kawamoto","An Epistemic Approach to the Formal Specification of Statistical Machine Learning",2020,"","","","",103,"2022-07-13 09:23:23","","10.1007/S10270-020-00825-2","","",,,,,1,0.50,1,1,2,"","",""
0,"Kaiyu Yang","1 Machine Learning for Reasoning",2021,"","","","",104,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,1,1,"Reasoning is a core component of human intelligence that machines still struggle with. I do research in the field of artificial intelligence, with the long-term goal of building machines that reason precisely, systematically, in ways that are interpretable and robust to ambiguity in real-world environments. My research advances towards this goal by attempting to combine the complementary strengths of machine learning and symbolic reasoning. My graduate research has focused on developing machine learning models that represent reasoning via symbolic proofs. They show the promise of new learning paradigms that I envision to be more robust, interpretable, and trustworthy for deployment in real-world high-stake applications. Symbolic reasoning is precise and generalizes systematically to unseen scenarios. But it has been restricted to domains amenable to rigid formalization. In contrast, machine learning has the flexibility to handle noisy and ambiguous domains that are hard to formalize. But predominant machine learning models, such as deep neural networks, are notoriously uninterpretable, data-hungry, and incapable of generalizing outside the training data distribution. Integrating the strengths of both approaches is essential for building flexible reasoning machines with precise and systematic generalization. However, due to the discrete nature of symbolic reasoning, such integration may require a radical departure from the predominant paradigm of gradient-based learning. And my research tries to answer what that alternative form of learning might look like.","",""
0,"Ying Li, Qinglai Yu, Ming Xie, Zhenduo Zhang, Zhanjun Ma, Kai Cao","Identifying Oil Spill Types Based on Remotely Sensed Reflectance Spectra and Multiple Machine Learning Algorithms",2021,"","","","",105,"2022-07-13 09:23:23","","10.1109/JSTARS.2021.3109951","","",,,,,0,0.00,0,6,1,"An accurate identification of oil spill types is the basis of determining the source of leakage, evaluating the potential damage, and deciding a plan of responses for an oil spill event. Despite sufficient studies that interpreted and analyzed hyperspectral data of oil spills, these studies that identify or classify oil spill types is rather limited. Aiming at identifying different types of oil spills, this article analyses the reflectance spectra obtained from high-resolution hyperspectral sensors using multiple machine learning methods. Four types of machine learning models are applied in this article: random forest; support vector machine (SVM); and deep neural network (DNN); and DNN with differential pooling (DP-DNN). The training and testing data are collected by field experiments under different environmental condition in order to verify the robustness of the machine learning models. The characteristics of reflectance is briefly described, and the results conform with results from previous studies. The performances of the machine learning models are evaluated and compared in terms of both accuracy of prediction and computational complexity. The results indicate that the two DNN models are able to achieve the most accurate prediction among the four machine learning models at the cost of more computation. The SVM model, or the proposed DP-DNN model may be a favorable choice when training time is limited.","",""
0,"Aurélien Olivier, C. Hoffmann, A. Mansour, L. Bressollette, Benoit Clement","Survey on machine learning applied to medical image analysis",2021,"","","","",106,"2022-07-13 09:23:23","","10.1109/CISP-BMEI53629.2021.9624442","","",,,,,0,0.00,0,5,1,"This paper presents a selective survey on recent advances in machine learning applied to medical imaging. It aims to highlight both innovations that increase the performance of the models and methods that ensure certainty, interpretability and robustness of the trained models. The paper focuses particularly on new concepts such as attention modules that allow to gather specific features considering global context. Its second main focus is given to domain adaptation methods to enhance model robustness to distribution shifts. Finally, we discuss uncertainty estimation and interpretability methods to evaluate confidence in a trained model.","",""
0,"Hiroto Mizutani, Masateu Tsunoda, K. Nakasai","How to Enlighten Novice Users on Behavior of Machine Learning Models?",2021,"","","","",107,"2022-07-13 09:23:23","","10.1109/SNPD51163.2021.9704891","","",,,,,0,0.00,0,3,1,"Background: Machine learning models are sometimes embedded in software to implement the required functions. As a result, non-experts in machine learning are becoming familiar with the models. However, the interpretability of the built models is often low in machine learning, such as deep learning, and the recognition process of such models is very different from that of humans. Therefore, it is not easy for novice users, such as end-users and beginners, to anticipate the behavior of models that they will use or build. Aim: We assist novice users to realize an aspect of the behavior of machine learning models relating to robustness intuitively. Method: We formalized and evaluated quiz-based analysis, which is often applied by practitioners to test the robustness of machine learning models arbitrarily. To generate test cases of the models, the analysis converts images towards the boundary of classification for both machine learning and humans. It can be regarded as a type of boundary value analysis of software development. Results: In the experiment, we evaluated whether the analysis quantitatively clarified the aspects of the models. The analysis clarified the robustness of the model for image conversion and misclassification quantitatively. Conclusion: The analysis is expected to enlighten novice users on the behavior of machine learning models. This may promote behavioral changes in the evaluation of models for novice users.","",""
25,"Nastaran Meftahi, M. Klymenko, A. Christofferson, U. Bach, D. Winkler, S. Russo","Machine learning property prediction for organic photovoltaic devices",2020,"","","","",108,"2022-07-13 09:23:23","","10.1038/s41524-020-00429-w","","",,,,,25,12.50,4,6,2,"","",""
25,"F. Noé","Machine Learning for Molecular Dynamics on Long Timescales",2018,"","","","",109,"2022-07-13 09:23:23","","10.1007/978-3-030-40245-7_16","","",,,,,25,6.25,25,1,4,"","",""
2,"Weishen Pan, Changshui Zhang","The Definitions of Interpretability and Learning of Interpretable Models",2021,"","","","",110,"2022-07-13 09:23:23","","","","",,,,,2,2.00,1,2,1,"As machine learning algorithms getting adopted in an ever-increasing number of applications, interpretation has emerged as a crucial desideratum. In this paper, we propose a mathematical definition for the humaninterpretable model. In particular, we define interpretability between two information process systems. If a prediction model is interpretable by a human recognition system based on the above interpretability definition, the prediction model is defined as a completely human-interpretable model. We further design a practical framework to train a completely human-interpretable model by user interactions. Experiments on image datasets show the advantages of our proposed model in two aspects: 1) The completely human-interpretable model can provide an entire decisionmaking process that is human-understandable; 2) The completely humaninterpretable model is more robust against adversarial attacks.","",""
1,"David Roschewitz, Mary-Anne Hartley, Luca Corinzia, Martin Jaggi","IFedAvg: Interpretable Data-Interoperability for Federated Learning",2021,"","","","",111,"2022-07-13 09:23:23","","","","",,,,,1,1.00,0,4,1,"Recently, the ever-growing demand for privacy-oriented machine learning has motivated researchers to develop federated and decentralized learning techniques, allowing individual clients to train models collaboratively without disclosing their private datasets. However, widespread adoption has been limited in domains relying on high levels of user trust, where assessment of data compatibility is essential. In this work, we define and address low interoperability induced by underlying client data inconsistencies in federated learning for tabular data. The proposed method, iFedAvg, builds on federated averaging adding local element-wise affine layers to allow for a personalized and granular understanding of the collaborative learning process. Thus, enabling the detection of outlier datasets in the federation and also learning the compensation for local data distribution shifts without sharing any original data. We evaluate iFedAvg using several public benchmarks and a previously unstudied collection of real-world datasets from the 2014 2016 West African Ebola epidemic, jointly forming the largest such dataset in the world. In all evaluations, iFedAvg achieves competitive average performance with negligible overhead. It additionally shows substantial improvement on outlier clients, highlighting increased robustness to individual dataset shifts. Most importantly, our method provides valuable client-specific insights at a fine-grained level to guide interoperable federated learning.","",""
1,"Ali Foroughi pour, Brian S. White, Jonghanne Park, T. Sheridan, Jeffrey H. Chuang","Deep learning features encode interpretable morphologies within histological images",2021,"","","","",112,"2022-07-13 09:23:23","","10.1101/2021.08.16.456518","","",,,,,1,1.00,0,5,1,"Convolutional neural networks (CNNs) are revolutionizing digital pathology by enabling machine learning-based classification of a variety of phenotypes from hematoxylin and eosin (H&E) whole slide images (WSIs), but the interpretation of CNNs remains difficult. Most studies have considered interpretability in a post hoc fashion, e.g. by presenting example regions with strongly predicted class labels. However, such an approach does not explain the biological features that contribute to correct predictions. To address this problem, here we investigate the interpretability of H&E-derived CNN features (the feature weights in the final layer of a transfer-learning-based architecture), which we show can be construed as abstract morphological genes (“mones”) with strong independent associations to biological phenotypes. We observe that many mones are specific to individual cancer types, while others are found in multiple cancers especially from related tissue types. We also observe that mone-mone correlations are strong and robustly preserved across related cancers. Importantly, linear mone-based classifiers can very accurately separate 38 distinct classes (19 tumor types and their adjacent normals, AUC=97.1% ± 2.8% for each class prediction), and linear classifiers are also highly effective for universal tumor detection (AUC=99.2% ± 0.12%). This linearity provides evidence that individual mones or correlated mone clusters may be associated with interpretable histopathological features or other patient characteristics. In particular, the statistical similarity of mones to gene expression values allows integrative mone analysis via expression-based bioinformatics approaches. We observe strong correlations between individual mones and individual gene expression values, notably mones associated with collagen gene expression in ovarian cancer. Mone-expression comparisons also indicate that immunoglobulin expression can be identified using mones in colon adenocarcinoma and that immune activity can be identified across multiple cancer types, and we verify these findings by expert histopathological review. Our work demonstrates that mones provide a morphological H&E decomposition that can be effectively associated with diverse phenotypes, analogous to the interpretability of transcription via gene expression values.","",""
6,"Olivier Deiss, S. Biswal, Jing Jin, Haoqi Sun, M. Westover, Jimeng Sun","HAMLET: Interpretable Human And Machine co-LEarning Technique",2018,"","","","",113,"2022-07-13 09:23:23","","","","",,,,,6,1.50,1,6,4,"Efficient label acquisition processes are key to obtaining robust classifiers. However, data labeling is often challenging and subject to high levels of label noise. This can arise even when classification targets are well defined, if instances to be labeled are more difficult than the prototypes used to define the class, leading to disagreements among the expert community. Here, we enable efficient training of deep neural networks. From low-confidence labels, we iteratively improve their quality by simultaneous learning of machines and experts. We call it Human And Machine co-LEarning Technique (HAMLET). Throughout the process, experts become more consistent, while the algorithm provides them with explainable feedback for confirmation. HAMLET uses a neural embedding function and a memory module filled with diverse reference embeddings from different classes. Its output includes classification labels and highly relevant reference embeddings as explanation. We took the study of brain monitoring at intensive care unit (ICU) as an application of HAMLET on continuous electroencephalography (cEEG) data. Although cEEG monitoring yields large volumes of data, labeling costs and difficulty make it hard to build a classifier. Additionally, while experts agree on the labels of clear-cut examples of cEEG patterns, labeling many real-world cEEG data can be extremely challenging. Thus, a large minority of sequences might be mislabeled. HAMLET has shown significant performance gain against deep learning and other baselines, increasing accuracy from 7.03% to 68.75% on challenging inputs. Besides improved performance, clinical experts confirmed the interpretability of those reference embeddings in helping explaining the classification results by HAMLET.","",""
0,"Jian Jiang","MIIDL: a Python package for microbial biomarkers identification powered by interpretable deep learning",2021,"","","","",114,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,1,1,"Summary: Detecting microbial biomarkers used to predict disease phenotypes and clinical outcomes is crucial for disease early-stage screening and diagnosis. Most methods for biomarker identification are linear-based, which is very limited as biological processes are rarely fully linear. The introduction of machine learning to this field tends to bring a promising solution. However, identifying microbial biomarkers in an interpretable, datadriven and robust manner remains challenging. We present MIIDL, a Python package for the identification of microbial biomarkers based on interpretable deep learning. MIIDL innovatively applies convolutional neural networks, a variety of interpretability algorithms and plenty of pre-processing methods to provide a one-stop and robust pipeline for microbial biomarkers identification from high-dimensional and sparse data sets. Availability: Source code is available on GitHub (https://github.com/chunribu/miidl/) under the MIT license. MIIDL is operating system independent and can be installed directly via pip or conda. Contact: chunribu@mail.sdu.edu.cn","",""
0,"Yu-Chung Peng, N. S. D'Souza, Brian Bush, Charles Brown, A. Venkataraman","Predicting Acute Kidney Injury via Interpretable Ensemble Learning and Attention Weighted Convoutional-Recurrent Neural Networks",2021,"","","","",115,"2022-07-13 09:23:23","","10.1109/CISS50987.2021.9400242","","",,,,,0,0.00,0,5,1,"Acute Kidney Injury (AKI) is one of the most frequent postoperative complications and is associated with both short- and long-term mortality. Improved prediction of AKI is crucial and may help clinicians prevent and mitigate its adverse effects. In this paper, we explore the use of machine learning methods to predict postoperative AKI. Our analysis centers on the ensemble-based random forest (RF) classifier, which operates on static clinical variables, and a novel deep learning architecture that incorporates intraoperative time series data along with the static variables. The architecture uses a dual-attention mechanism to select both features and time intervals relevant for AKI prediction. We evaluate our models on the publicly available VitalDB database of 3,640 patients who underwent non-cardiac surgery. The RF outperformed existing machine learning classifiers in the AKI literature (AUROC: 0.86, AUPRC: 0.54). In addition, the RF identified a robust set of preoperative variables that can be screened in a simple blood test. While the deep learning model achieved slightly lower performance (AUROC: 0.84, AUPRC: 0.44), the attention weights provide important intraoperative information, which can be monitored by clinicians during surgery. Taken together, our results highlight the promise of machine learning for AKI prediction and take the first steps towards developing clinically translatable models.","",""
17,"Frank Male, J. Jensen, L. Lake","Comparison of permeability predictions on cemented sandstones with physics-based and machine learning approaches",2020,"","","","",116,"2022-07-13 09:23:23","","10.31223/osf.io/3w6jx","","",,,,,17,8.50,6,3,2,"Abstract Permeability prediction has been an important problem since the time of Darcy. Most approaches to solve this problem have used either idealized physical models or empirical relations. In recent years, machine learning (ML) has led to more accurate and robust, but less interpretable empirical models. Using 211 core samples collected from 12 wells in the Garn Sandstone from the North Sea, this study compared idealized physical models based on the Carman-Kozeny equation to interpretable ML models. We found that ML models trained on estimates of physical properties are more accurate than physical models. Also, the results show evidence of a threshold of about 10% volume fraction, above which pore-filling cement strongly affects permeability.","",""
3,"A. Creagh, F. Lipsmeier, M. Lindemann, M. Vos","Interpretable deep learning for the remote characterisation of ambulation in multiple sclerosis using smartphones",2021,"","","","",117,"2022-07-13 09:23:23","","10.1038/s41598-021-92776-x","","",,,,,3,3.00,1,4,1,"","",""
10,"T. Botari, Frederik Hvilshøj, Rafael Izbicki, A. Carvalho","MeLIME: Meaningful Local Explanation for Machine Learning Models",2020,"","","","",118,"2022-07-13 09:23:23","","","","",,,,,10,5.00,3,4,2,"Most state-of-the-art machine learning algorithms induce black-box models, preventing their application in many sensitive domains. Hence, many methodologies for explaining machine learning models have been proposed to address this problem. In this work, we introduce strategies to improve local explanations taking into account the distribution of the data used to train the black-box models. We show that our approach, MeLIME, produces more meaningful explanations compared to other techniques over different ML models, operating on various types of data. MeLIME generalizes the LIME method, allowing more flexible perturbation sampling and the use of different local interpretable models. Additionally, we introduce modifications to standard training algorithms of local interpretable models fostering more robust explanations, even allowing the production of counterfactual examples. To show the strengths of the proposed approach, we include experiments on tabular data, images, and text; all showing improved explanations. In particular, MeLIME generated more meaningful explanations on the MNIST dataset than methods such as GuidedBackprop, SmoothGrad, and Layer-wise Relevance Propagation. MeLIME is available on this https URL.","",""
15,"Zihao Wang, Yang Su, Saimeng Jin, W. Shen, Jingzheng Ren, Xiang-ping Zhang, J. Clark","A novel unambiguous strategy of molecular feature extraction in machine learning assisted predictive models for environmental properties",2020,"","","","",119,"2022-07-13 09:23:23","","10.1039/d0gc01122c","","",,,,,15,7.50,2,7,2,"Environmental properties of compounds provide significant information in treating organic pollutants, which drives the chemical process and environmental science toward eco-friendly technology. Traditional group contribution methods play an important role in property estimations, whereas various disadvantages emerge in their applications, such as scattered predicted values for certain groups of compounds. In order to address such issues, an extraction strategy for molecular features is proposed in this research, which is characterized by interpretability and discriminating power with regard to isomers. Based on the Henry's law constant data of organic compounds in water, we developed a hybrid predictive model that integrates the proposed strategy in conjunction with a neural network framework. The structure of the predictive model is optimized using cross-validation and grid search to improve its robustness. Moreover, the predictive model is improved by introducing the plane of best fit descriptor as input and adopting k-means clustering in sampling. In contrast with reported models in the literature, the developed predictive model demonstrates improved generality, higher accuracy, and fewer molecular features used in its development.","",""
404,"D. V. Carvalho, E. M. Pereira, Jaime S. Cardoso","Machine Learning Interpretability: A Survey on Methods and Metrics",2019,"","","","",120,"2022-07-13 09:23:23","","10.3390/ELECTRONICS8080832","","",,,,,404,134.67,135,3,3,"Machine learning systems are becoming increasingly ubiquitous. These systems’s adoption has been expanding, accelerating the shift towards a more algorithmic society, meaning that algorithmically informed decisions have greater potential for significant social impact. However, most of these accurate decision support systems remain complex black boxes, meaning their internal logic and inner workings are hidden to the user and even experts cannot fully understand the rationale behind their predictions. Moreover, new regulations and highly regulated domains have made the audit and verifiability of decisions mandatory, increasing the demand for the ability to question, understand, and trust machine learning systems, for which interpretability is indispensable. The research community has recognized this interpretability problem and focused on developing both interpretable models and explanation methods over the past few years. However, the emergence of these methods shows there is no consensus on how to assess the explanation quality. Which are the most suitable metrics to assess the quality of an explanation? The aim of this article is to provide a review of the current state of the research field on machine learning interpretability while focusing on the societal impact and on the developed methods and metrics. Furthermore, a complete literature review is presented in order to identify future directions of work on this field.","",""
5,"A. Barnard, A. Parker, B. Motevalli, G. Opletal","The pure and representative types of disordered platinum nanoparticles from machine learning.",2020,"","","","",121,"2022-07-13 09:23:23","","10.1088/1361-6528/abcc23","","",,,,,5,2.50,1,4,2,"The development of interpretable structure/property relationships is a cornerstone of nanoscience, but can be challenging when the structural diversity and complexity exceeds our ability to characterise it. This is often the case for imperfect, disordered and amorphous nanoparticles, where even the nomenclature can be unspecific. Disordered platinum nanoparticles have exhibited superior performance for some reactions, which makes a systematic way of describing them highly desirable. In this study we have used a diverse set of disorder platinum nanoparticles and machine learning to identify the pure and representative structures based on their similarity in 121 dimensions. We identify two prototypes that are representative of separable classes, and seven archetypes that are the pure structures on the convex hull with which all other possibilities can be described. Together these nine nanoparticles can explain all of the variance in the set, and can be described as either single crystal, twinned, spherical or branched; with or without roughened surfaces. This forms a robust sub-set of platinum nanoparticle upon which to base further work, and provides a theoretical basis for discussing structure/property relationships of platinum nanoparticles that are not geometrically ideal.","",""
4,"Hala Abdelkader","Towards Robust Production Machine Learning Systems: Managing Dataset Shift",2020,"","","","",122,"2022-07-13 09:23:23","","10.1145/3324884.3415281","","",,,,,4,2.00,4,1,2,"The advances in machine learning (ML) have stimulated the integration of their capabilities into software systems. However, there is a tangible gap between software engineering and machine learning practices, that is delaying the progress of intelligent services development. Software organisations are devoting effort to adjust the software engineering processes and practices to facilitate the integration of machine learning models. Machine learning researchers as well are focusing on improving the interpretability of machine learning models to support overall system robustness. Our research focuses on bridging this gap through a methodology that evaluates the robustness of machine learning-enabled software engineering systems. In particular, this methodology will automate the evaluation of the robustness properties of software systems against dataset shift problems in ML. It will also feature a notification mechanism that facilitates the debugging of ML components.","",""
4,"N. Alaya, S. Yahia, M. Lamolle","Predicting the Empirical Robustness of the Ontology Reasoners based on Machine Learning Techniques",2015,"","","","",123,"2022-07-13 09:23:23","","10.5220/0005599800610073","","",,,,,4,0.57,1,3,7,"Reasoning with ontologies is one of the core tasks of research in Description Logics. A variety of reasoners    with highly optimized algorithms have been developed to allow inference tasks on expressive ontology    languages such as OWL (DL). However, unexpected behaviours of reasoner engines is often observed in practice.    Both reasoner time efficiency and result correctness would vary across input ontologies, which is hardly    predictable even for experienced reasoner designers. Seeking for better understanding of reasoner empirical    behaviours, we propose to use supervised machine learning techniques to automatically predict reasoner robustness    from its previous running. For this purpose, we introduced a set of comprehensive ontology features.    We conducted huge body of experiments for 6 well known reasoners and using over 1000 ontologies from the    OREâ2014 corpus. Our learning results show that we could build highly accuracy reasoner robustness predictive    models. Moreover, by interpreting these models, it would be possible to gain insights about particular    ontology features likely to be reasoner robustness degrading factors.","",""
0,"Taru Jain","Adversarial Machine Learning for Self Harm Disclosure Analysis (Workshop Paper)",2020,"","","","",124,"2022-07-13 09:23:23","","10.1109/BigMM50055.2020.00070","","",,,,,0,0.00,0,1,2,"Adversarial Machine Learning has been gaining attention from the NLP community due to low interpretability and low robustness of the current state-of-the-art systems. In this work, we study the effect of various adversarial attacks for detection of suicidal intent in social media setting. Suicide Ideation is a sensitive issue and is a leading cause of death. We show how various models are rendered useless after attacks and perform adversarial training using the most ideal attacks to improve their robustness. We also conduct several experiments with the attacks to study their effect and propose an approach for adversarial training using Generative Adversarial Networks.","",""
0,"T. Gärtner","Interactive Machine Learning with Structured Data",2020,"","","","",125,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,1,2,"In this talk I’ll give an overview of our contributions to what I call interactive machine learning. Often, interaction in Computer Science is interpreted as the interaction of humans with the computer but I intend a broader meaning of the interaction of machine learning algorithms with the real world, including but not restricted to humans. Interactions with humans span a broad range where they can be intentional and guided by the human or they can be guided by the computer such that the human is oblivious of the fact that he is being guided. Another example of an interaction with the real world is the use of machine learning algorithms in cyclic discovery processes such as drug design. Important properties of interactive machine learning algorithms include efficiency, effectiveness, responsiveness, and robustness. In the talk I will show how these can be achieved in a variety of interactive contexts. Copyright © 2020 by the paper’s authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0).","",""
3,"Barbora Bučková, M. Brunovsky, M. Bareš, J. Hlinka","Predicting Sex From EEG: Validity and Generalizability of Deep-Learning-Based Interpretable Classifier",2020,"","","","",126,"2022-07-13 09:23:23","","10.3389/fnins.2020.589303","","",,,,,3,1.50,1,4,2,"Explainable artificial intelligence holds a great promise for neuroscience and plays an important role in the hypothesis generation process. We follow-up a recent machine learning-oriented study that constructed a deep convolutional neural network to automatically identify biological sex from EEG recordings in healthy individuals and highlighted the discriminative role of beta-band power. If generalizing, this finding would be relevant not only theoretically by pointing to some specific neurobiological sexual dimorphisms, but potentially also as a relevant confound in quantitative EEG diagnostic practice. To put this finding to test, we assess whether the automatic identification of biological sex generalizes to another dataset, particularly in the presence of a psychiatric disease, by testing the hypothesis of higher beta power in women compared to men on 134 patients suffering from Major Depressive Disorder. Moreover, we construct ROC curves and compare the performance of the classifiers in determining sex both before and after the antidepressant treatment. We replicate the observation of a significant difference in beta-band power between men and women, providing classification accuracy of nearly 77%. The difference was consistent across the majority of electrodes, however multivariate classification models did not generally improve the performance. Similar results were observed also after the antidepressant treatment (classification accuracy above 70%), further supporting the robustness of the initial finding.","",""
270,"A. Vellido, J. Martín-Guerrero, P. Lisboa","Making machine learning models interpretable",2012,"","","","",127,"2022-07-13 09:23:23","","","","",,,,,270,27.00,90,3,10,"Data of different levels of complexity and of ever growing diversity of characteristics are the raw materials that machine learning practitioners try to model using their wide palette of methods and tools. The obtained models are meant to be a synthetic representation of the available, observed data that captures some of their intrinsic regularities or patterns. Therefore, the use of machine learning techniques for data analysis can be understood as a problem of pattern recognition or, more informally, of knowledge discovery and data mining. There exists a gap, though, between data modeling and knowledge extraction. Models, de- pending on the machine learning techniques employed, can be described in diverse ways but, in order to consider that some knowledge has been achieved from their description, we must take into account the human cog- nitive factor that any knowledge extraction process entails. These models as such can be rendered powerless unless they can be interpreted ,a nd the process of human interpretation follows rules that go well beyond techni- cal prowess. For this reason, interpretability is a paramount quality that machine learning methods should aim to achieve if they are to be applied in practice. This paper is a brief introduction to the special session on interpretable models in machine learning, organized as part of the 20 th European Symposium on Artificial Neural Networks, Computational In- telligence and Machine Learning. It includes a discussion on the several works accepted for the session, with an overview of the context of wider research on interpretability of machine learning models.","",""
162,"Harsha Nori, Samuel Jenkins, Paul Koch, R. Caruana","InterpretML: A Unified Framework for Machine Learning Interpretability",2019,"","","","",128,"2022-07-13 09:23:23","","","","",,,,,162,54.00,41,4,3,"InterpretML is an open-source Python package which exposes machine learning interpretability algorithms to practitioners and researchers. InterpretML exposes two types of interpretability - glassbox models, which are machine learning models designed for interpretability (ex: linear models, rule lists, generalized additive models), and blackbox explainability techniques for explaining existing systems (ex: Partial Dependence, LIME). The package enables practitioners to easily compare interpretability algorithms by exposing multiple methods under a unified API, and by having a built-in, extensible visualization platform. InterpretML also includes the first implementation of the Explainable Boosting Machine, a powerful, interpretable, glassbox model that can be as accurate as many blackbox models. The MIT licensed source code can be downloaded from github.com/microsoft/interpret.","",""
3,"Nathan G. Drenkow, Numair Sani, I. Shpitser, M. Unberath","Robustness in Deep Learning for Computer Vision: Mind the gap?",2021,"","","","",129,"2022-07-13 09:23:23","","","","",,,,,3,3.00,1,4,1,"Deep neural networks for computer vision tasks are deployed in increasingly safety-critical and socially-impactful applications, motivating the need to close the gap in model performance under varied, naturally occurring imaging conditions. Robustness, ambiguously used in multiple contexts including adversarial machine learning, here then refers to preserving model performance under naturally-induced image corruptions or alterations. We perform a systematic review to identify, analyze, and summarize current definitions and progress towards non-adversarial robustness in deep learning for computer vision. We find that this area of research has received disproportionately little attention relative to adversarial machine learning, yet a significant robustness gap exists that often manifests in performance degradation similar in magnitude to adversarial conditions. To provide a more transparent definition of robustness across contexts, we introduce a structural causal model of the data generating process and interpret non-adversarial robustness as pertaining to a model’s behavior on corrupted images which correspond to low-probability samples from the unaltered data distribution. We then identify key architecture-, data augmentation-, and optimization tactics for improving neural network robustness. This causal view of robustness reveals that common practices in the current literature, both in regards to robustness tactics and evaluations, correspond to causal concepts, such as soft interventions resulting in a counterfactually-altered distribution of imaging conditions. Through our findings and analysis, we offer perspectives on how future research may mind this evident and significant non-adversarial robustness gap.","",""
14,"M. Levine, A. Stuart","A Framework for Machine Learning of Model Error in Dynamical Systems",2021,"","","","",130,"2022-07-13 09:23:23","","","","",,,,,14,14.00,7,2,1,"The development of data-informed predictive models for dynamical systems is of widespread interest in many disciplines. We present a unifying framework for blending mechanistic and machine-learning approaches to identify dynamical systems from noisily and partially observed data. We compare pure data-driven learning with hybrid models which incorporate imperfect domain knowledge, referring to the discrepancy between an assumed truth model and the imperfect mechanistic model as model error. Our formulation is agnostic to the chosen machine learning model, is presented in both continuousand discrete-time settings, and is compatible both with model errors that exhibit substantial memory and errors that are memoryless. First, we study memoryless linear (w.r.t. parametric-dependence) model error from a learning theory perspective, defining excess risk and generalization error. For ergodic continuous-time systems, we prove that both excess risk and generalization error are bounded above by terms that diminish with the square-root of T , the time-interval over which training data is specified. Secondly, we study scenarios that benefit from modeling with memory, proving universal approximation theorems for two classes of continuous-time recurrent neural networks (RNNs): both can learn memory-dependent model error, assuming that it is governed by a finite-dimensional hidden variable and that, together, the observed and hidden variables form a continuous-time Markovian system. In addition, we connect one class of RNNs to reservoir computing, thereby relating learning of memory-dependent error to recent work on supervised learning between Banach spaces using random features. Numerical results are presented (Lorenz ’63, Lorenz ’96 Multiscale systems) to compare purely data-driven and hybrid approaches, finding hybrid methods less data-hungry and more parametrically efficient. We also find that, while a continuous-time framing allows for robustness to irregular sampling and desirable domain-interpretability, a discrete-time framing can provide similar or better predictive performance, especially when data are undersampled and the vector field defining the true dynamics cannot be identified. Finally, we demonstrate numerically how data assimilation can be leveraged to learn hidden dynamics from noisy, partially-observed data, and illustrate challenges in representing memory by this approach, and in the training of such models. Received by the editors July 13, 2021. 2020 Mathematics Subject Classification. Primary 68T30, 37A30, 37M10; Secondary 37M25,","",""
37,"Radwa El Shawi, Youssef Mohamed, M. Al-mallah, S. Sakr","Interpretability in HealthCare A Comparative Study of Local Machine Learning Interpretability Techniques",2019,"","","","",131,"2022-07-13 09:23:23","","10.1109/CBMS.2019.00065","","",,,,,37,12.33,9,4,3,"Although complex machine learning models (e.g., Random Forest, Neural Networks) are commonly outperforming the traditional simple interpretable models (e.g., Linear Regression, Decision Tree), in the healthcare domain, clinicians find it hard to understand and trust these complex models due to the lack of intuition and explanation of their predictions. With the new General Data Protection Regulation (GDPR), the importance for plausibility and verifiability of the predictions made by machine learning models has become essential. To tackle this challenge, recently, several machine learning interpretability techniques have been developed and introduced. In general, the main aim of these interpretability techniques is to shed light and provide insights into the predictions process of the machine learning models and explain how the model predictions have resulted. However, in practice, assessing the quality of the explanations provided by the various interpretability techniques is still questionable. In this paper, we present a comprehensive experimental evaluation of three recent and popular local model agnostic interpretability techniques, namely, LIME, SHAP and Anchors on different types of real-world healthcare data. Our experimental evaluation covers different aspects for its comparison including identity, stability, separability, similarity, execution time and bias detection. The results of our experiments show that LIME achieves the lowest performance for the identity metric and the highest performance for the separability metric across all datasets included in this study. On average, SHAP has the smallest average time to output explanation across all datasets included in this study. For detecting the bias, SHAP enables the participants to better detect the bias.","",""
21,"Nidan Qiao","A systematic review on machine learning in sellar region diseases: quality and reporting items",2019,"","","","",132,"2022-07-13 09:23:23","","10.1530/EC-19-0156","","",,,,,21,7.00,21,1,3,"Introduction Machine learning methods in sellar region diseases present a particular challenge because of the complexity and the necessity for reproducibility. This systematic review aims to compile the current literature on sellar region diseases that utilized machine learning methods and to propose a quality assessment tool and reporting checklist for future studies. Methods PubMed and Web of Science were searched to identify relevant studies. The quality assessment included five categories: unmet needs, reproducibility, robustness, generalizability and clinical significance. Results Seventeen studies were included with the diagnosis of general pituitary neoplasms, acromegaly, Cushing’s disease, craniopharyngioma and growth hormone deficiency. 87.5% of the studies arbitrarily chose one or two machine learning models. One study chose ensemble models, and one study compared several models. 43.8% of studies did not provide the platform for model training, and roughly half did not offer parameters or hyperparameters. 62.5% of the studies provided a valid method to avoid over-fitting, but only five reported variations in the validation statistics. Only one study validated the algorithm in a different external database. Four studies reported how to interpret the predictors, and most studies (68.8%) suggested possible clinical applications of the developed algorithm. The workflow of a machine-learning study and the recommended reporting items were also provided based on the results. Conclusions Machine learning methods were used to predict diagnosis and posttreatment outcomes in sellar region diseases. Though most studies had substantial unmet need and proposed possible clinical application, replicability, robustness and generalizability were major limits in current studies.","",""
15,"Bingqiang Liu, Ling Han, Xiangrong Liu, Jichang Wu, Q. Ma","Computational Prediction of Sigma-54 Promoters in Bacterial Genomes by Integrating Motif Finding and Machine Learning Strategies",2019,"","","","",133,"2022-07-13 09:23:23","","10.1109/TCBB.2018.2816032","","",,,,,15,5.00,3,5,3,"Sigma factor, as a unit of RNA polymerase holoenzyme, is a critical factor in the process of gene transcriptional regulation. It recognizes the specific DNA sites and brings the core enzyme of RNA polymerase to the upstream regions of target genes. Therefore, the prediction of the promoters for a particular sigma factor is essential for interpreting functional genomic data and observation. This paper develops a new method to predict sigma-54 promoters in bacterial genomes. The new method organically integrates motif finding and machine learning strategies to capture the intrinsic features of sigma-54 promoters. The experiments on E. coli benchmark test set show that our method has good capability to distinguish sigma-54 promoters from surrounding or randomly selected DNA sequences. The applications of the other three bacterial genomes indicate the potential robustness and applicative power of our method on a large number of bacterial genomes. The source code of our method can be freely downloaded at https://github.com/maqin2001/PromotePredictor.","",""
14,"A. A. Alshaikh, A. Magana-Mora, S. Gharbi, A. Al-yami","Machine Learning for Detecting Stuck Pipe Incidents: Data Analytics and Models Evaluation",2019,"","","","",134,"2022-07-13 09:23:23","","10.2523/IPTC-19394-MS","","",,,,,14,4.67,4,4,3,"  The earlier a stuck pipe incident is predicted and mitigated, the higher the chance of success in freeing the pipe or avoiding severe sticking in the first place. Time is crucial in such cases as an improper reaction to a stuck pipe incident can easily make it worse. In this work, practical machine learning, classification models were developed using real-time drilling data to automatically detect stuck pipe incidents during drilling operations and communicate the observations and alerts, sufficiently ahead of time, to the rig crew for avoidance or remediation actions to be taken.  The models use machine learning algorithms that feed on identified key drilling parameters to detect stuck pipe anomalies. The parameters used in building the system were selected based on published literature and historical data and reports of stuck pipe incidents and were analyzed and ranked to identify the ones of key influence on the accuracy of stuck pipe detection via a nonlinear relationship. The model exceptionally uses the robustness of data-based analysis along with the physics-based analysis.  The model has shown effective detection of the signs observed by experts ahead of time and has helped with providing enhanced stuck pipe detection and risk assessment. Validating and testing the model on several cases showed promising results as anomalies on simple and complex parameters were detected before or near the actual time stuck pipe incidents were reported from the rig crew. This facilitated better understanding of the underlying physics principles and provided awareness of stuck pipe occurrence.  The model improved monitoring and interpreting the drilling data streams. Beside such pipe signs, the model helped with detecting signs of other impeding problems in the downhole conditions of the wellbore, the drilling equipment, and the sensors. The model is designed to be implemented in the real-time drilling data portal to provide an alarm system for all oil and gas rigs based on the observed abnormalities. The alarm is to be populated on the real-time environment and communicated to the rig crew in a timely manner to ensure optimal results, giving them sufficient time ahead to prevent or remediate a potential stuck pipe incident.","",""
0,"Pongpisit Thanasutives, Takeshi Morita, M. Numao, Ken-ichi Fukui","Noise-aware Physics-informed Machine Learning for Robust PDE Discovery",2022,"","","","",135,"2022-07-13 09:23:23","","10.48550/arXiv.2206.12901","","",,,,,0,0.00,0,4,1,"—This work is concerned with discovering the gov- erning partial differential equation (PDE) of a physical system. Existing methods have demonstrated the PDE identiﬁcation from ﬁnite observations but failed to maintain satisfying performance against noisy data, partly owing to suboptimal estimated deriva- tives and found PDE coefﬁcients. We address the issues by introducing a noise-aware physics-informed machine learning (nPIML) framework to discover the governing PDE from data following arbitrary distributions. Our proposals are twofold. First, we propose a couple of neural networks, namely solver and preselector, which yield an interpretable neural representation of the hidden physical constraint. After they are jointly trained, the solver network approximates potential candidates, e.g., partial derivatives, which are then fed to the sparse regression algorithm that initially unveils the most likely parsimonious PDE, decided according to the information criterion. Second, we propose the denoising physics-informed neural networks (dPINNs), based on Discrete Fourier Transform (DFT), to deliver a set of the optimal ﬁnetuned PDE coefﬁcients respecting the noise-reduced variables. The denoising PINNs’ structures are compartmentalized into forefront projection networks and a PINN, by which the formerly learned solver initializes. Our extensive experiments on ﬁve canonical PDEs afﬁrm that the proposed framework presents a robust and interpretable approach for PDE discovery, applicable to a wide range of systems, possibly complicated by noise. and model while suppressing paved towards the applications of interpretable artiﬁcial intelligence (AI) to enhance the understandability of physical sciences.","",""
0,"Jing-Jing Liu, Jian-chao Liu","Permeability Predictions for Tight Sandstone Reservoir Using Explainable Machine Learning and Particle Swarm Optimization",2022,"","","","",136,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,2,1,"High-precision permeability prediction is of great significance to tight sandstone reservoirs. However, while considerable progress has recently been made in the machine learning based prediction of reservoir permeability, the generalization of this approach is limited by weak interpretability. Hence, an interpretable XGBoost model is proposed herein based on particle swarm optimization to predict the permeability of tight sandstone reservoirs with higher accuracy and robust interpretability. The porosity and permeability of 202 core plugs and 6 logging curves (namely, the gamma-ray (GR) curve, the acoustic curve (AC), the spontaneous potential (SP) curve, the caliper (CAL) curve, the deep lateral resistivity (RILD) curve, and eight lateral resistivity (RFOC) curve) are extracted along with three derived variables (i.e., the shale content, the AC slope, and the GR slope) as data sets. Based on the data preprocessing, global and local interpretations are performed according to the Shapley additive explanations (SHAP) analysis, and the redundant features in the data set are screened to identify the porosity, AC, CAL, and GR slope as the four most important features. The particle swarm optimization algorithm is then used to optimize the hyperparameters of the XGBoost model. The prediction results of the PSO-XGBoost model indicate a superior performance compared with that of the benchmark XGBoost model. In addition, the reliable application of the interpretable PSO-XGBoost model in the prediction of tight sandstone reservoir permeability is examined by comparing the results with those of two traditional mathematical regression models, five machine learning models, and three deep learning models. Thus, the interpretable PSO-XGBoost model is shown to have more advantages in permeability prediction along with the lowest root mean square error, thereby confirming the effectiveness and practicability of this method.","",""
0,"Shujie Cheng, Lei Cheng, Shujing Qin, Lu Zhang, Pan Liu, Liu Liu, Zhicheng Xu, Qilin Wang","Improved Understanding of How Catchment Properties Control Hydrological Partitioning Through Machine Learning",2022,"","","","",137,"2022-07-13 09:23:23","","10.1029/2021WR031412","","",,,,,0,0.00,0,8,1,"Long‐term hydrological partitioning of catchments can be well described by the Budyko framework with a parameter (e.g., Fu's equations with parameter ω). The Budyko framework considers aridity index as the dominant control on hydrological partitioning, while the parameter represents integrated influences of catchment properties. Our understanding regarding the controls of catchment properties on the parameter is still limited. In this study, two machine learning methods, that is, boosted regression tree (BRT) and CUBIST, were used to model ω. Interpretable machine learning methods were adopted for better physical understanding including feature importance, accumulated local effects (ALE), and local interpretable model‐agnostic explanations. Among the 15 properties of 443 Australian catchments, analysis of feature importance showed that root zone storage capacity (SR), vapor pressure, vegetation coverage (M), precipitation depth, climate seasonality and asynchrony index (SAI), and water use efficiency (WUE) were the six primary control factors on ω. ALE showed that ω varied nonlinearly with all factors, and varied non‐monotonically with M, SAI, and WUE. LIME showed that the importance of the six dominant factors on ω varied between regions. CUBIST was further used to build regionally varying relationships between ω and the primary factors. Continental scale ω and evapotranspiration were further mapped across Australia based on the most robust BRT‐trained parameterization scheme with a resolution of 0.05°. Instead of using the machine learning method as a black box, we employed interpretability approaches to identify the controls. Our findings not only improved the capability of the Budyko method for hydrological partitioning across Australia, but also demonstrated that the controls of catchment properties on hydrological partitioning vary in different regions.","",""
1,"S. Javed, Dinkar Juyal, Zahil Shanis, S. Chakraborty, Harsha Pokkalla, A. Prakash","Rethinking Machine Learning Model Evaluation in Pathology",2022,"","","","",138,"2022-07-13 09:23:23","","10.48550/arXiv.2204.05205","","",,,,,1,1.00,0,6,1,"Machine Learning has been applied to pathology images in research and clinical practice with promising outcomes. However, standard ML models often lack the rigorous evaluation required for clinical decisions. Machine learning techniques for natural images are ill-equipped to deal with pathology images that are signiﬁcantly large and noisy, require expensive labeling, are hard to interpret, and are susceptible to spurious correlations. We propose a set of practical guidelines for ML evaluation in pathology that address the above concerns. The paper includes measures for setting up the evaluation framework, effectively dealing with variability in labels, and a rec-ommended suite of tests to address issues related to domain shift, robustness, and confounding variables. We hope that the proposed framework will bridge the gap between ML researchers and domain experts, leading to wider adoption of ML techniques in pathology and improving patient outcomes.","",""
1,"Hamed Zamani, Fernando Diaz, M. Dehghani, Donald Metzler, Michael Bendersky","Retrieval-Enhanced Machine Learning",2022,"","","","",139,"2022-07-13 09:23:23","","10.1145/3477495.3531722","","",,,,,1,1.00,0,5,1,"Although information access systems have long supportedpeople in accomplishing a wide range of tasks, we propose broadening the scope of users of information access systems to include task-driven machines, such as machine learning models. In this way, the core principles of indexing, representation, retrieval, and ranking can be applied and extended to substantially improve model generalization, scalability, robustness, and interpretability. We describe a generic retrieval-enhanced machine learning (REML) framework, which includes a number of existing models as special cases. REML challenges information retrieval conventions, presenting opportunities for novel advances in core areas, including optimization. The REML research agenda lays a foundation for a new style of information access research and paves a path towards advancing machine learning and artificial intelligence.","",""
1,"Aida Rahmattalabi, Alice Xiang","Promises and Challenges of Causality for Ethical Machine Learning",2022,"","","","",140,"2022-07-13 09:23:23","","","","",,,,,1,1.00,1,2,1,"In recent years, there has been increasing interest in causal reasoning for designing fair decision-making systems due to its compatibility with legal frameworks, interpretability for human stakeholders, and robustness to spurious correlations inherent in observational data, among other factors. The recent attention to causal fairness, however, has been accompanied with great skepticism due to practical and epistemological challenges with applying current causal fairness approaches in the literature. Motivated by the long-standing empirical work on causality in econometrics, social sciences, and biomedical sciences, in this paper we lay out the conditions for appropriate application of causal fairness under the""potential outcomes framework.""We highlight key aspects of causal inference that are often ignored in the causal fairness literature. In particular, we discuss the importance of specifying the nature and timing of interventions on social categories such as race or gender. Precisely, instead of postulating an intervention on immutable attributes, we propose a shift in focus to their perceptions and discuss the implications for fairness evaluation. We argue that such conceptualization of the intervention is key in evaluating the validity of causal assumptions and conducting sound causal analysis including avoiding post-treatment bias. Subsequently, we illustrate how causality can address the limitations of existing fairness metrics, including those that depend upon statistical correlations. Specifically, we introduce causal variants of common statistical notions of fairness, and we make a novel observation that under the causal framework there is no fundamental disagreement between different notions of fairness. Finally, we conduct extensive experiments where we demonstrate our approach for evaluating and mitigating unfairness, specially when post-treatment variables are present.","",""
12,"Frank Male, I. Duncan","Lessons for machine learning from the analysis of porosity-permeability transforms for carbonate reservoirs",2019,"","","","",141,"2022-07-13 09:23:23","","10.31223/osf.io/fwndb","","",,,,,12,4.00,6,2,3,"Abstract Prediction of permeability is one of the most difficult aspects of reservoir characterization because permeability cannot be directly measured by current well logging technology. This is particularly challenging for carbonate rocks. Machine learning (ML) and robust multivariate methods have been developed that have been used in many fields of study to make accurate estimators for variables of interest from both large and small datasets. ML has been criticized for utilizing approaches that are typically not interpretable. That is, it is not clear how the answers are arrived at and what aspects of input data may be resulting in inaccurate results. The current study uses a number of the mathematical algorithms that operate inside ML modules. It applies them to developing porosity-permeability transforms, with or without rock types, to two well-characterized data sets for carbonate reservoirs. One data set is from Jerry Lucia's 1995 study of carbonate rock types, and the other is from a study of the Seminole, West Texas, San Andres Unit. This study of statistical analysis of porosity-permeability transforms includes: transforming the data to normal distributions; performing cross-validation blind testing; and detecting heteroscedasticity by creating plots of residuals. Heteroscedastic data (populations with variable variance) may have an adverse impact on ML algorithms such as Random Forests (RF). We find that including lithofacies information does not greatly improve porosity-permeability transforms. We also propose a number of strategies to make ML analyses of reservoir (and other geosciences) data sets more robust and accurate.","",""
13,"Thibault Laugel, Marie-Jeanne Lesot, C. Marsala, X. Renard, Marcin Detyniecki","Unjustified Classification Regions and Counterfactual Explanations in Machine Learning",2019,"","","","",142,"2022-07-13 09:23:23","","10.1007/978-3-030-46147-8_3","","",,,,,13,4.33,3,5,3,"","",""
11,"Tianfang Xu, F. Liang","Machine learning for hydrologic sciences: An introductory overview",2021,"","","","",143,"2022-07-13 09:23:23","","10.1002/wat2.1533","","",,,,,11,11.00,6,2,1,"The hydrologic community has experienced a surge in interest in machine learning in recent years. This interest is primarily driven by rapidly growing hydrologic data repositories, as well as success of machine learning in various academic and commercial applications, now possible due to increasing accessibility to enabling hardware and software. This overview is intended for readers new to the field of machine learning. It provides a non‐technical introduction, placed within a historical context, to commonly used machine learning algorithms and deep learning architectures. Applications in hydrologic sciences are summarized next, with a focus on recent studies. They include the detection of patterns and events such as land use change, approximation of hydrologic variables and processes such as rainfall‐runoff modeling, and mining relationships among variables for identifying controlling factors. The use of machine learning is also discussed in the context of integrated with process‐based modeling for parameterization, surrogate modeling, and bias correction. Finally, the article highlights challenges of extrapolating robustness, physical interpretability, and small sample size in hydrologic applications.","",""
9,"T. Botari, Rafael Izbicki, A. Carvalho","Local Interpretation Methods to Machine Learning Using the Domain of the Feature Space",2019,"","","","",144,"2022-07-13 09:23:23","","10.1007/978-3-030-43823-4_21","","",,,,,9,3.00,3,3,3,"","",""
0,"Willa Potosnak","Robust Rule Learning for Reliable and Interpretable Insight into Expertise Transfer Opportunities",2022,"","","","",145,"2022-07-13 09:23:23","","10.1609/aaai.v36i11.21704","","",,,,,0,0.00,0,1,1,"Intensive care in hospitals is distributed to different units that care for patient populations reflecting specific comorbidities, treatments, and outcomes. Unit expertise can be shared to potentially improve the quality of methods and outcomes for patients across units. We propose an algorithmic rule pruning approach for use in building short lists of human-interpretable rules that reliably identify patient beneficiaries of expertise transfers in the form of machine learning risk models. Our experimental results, obtained with two intensive care monitoring datasets, demonstrate the potential utility of the proposed method in practice.","",""
1,"Johannes Haug, Klaus Broelemann, Gjergji Kasneci","Dynamic Model Tree for Interpretable Data Stream Learning",2022,"","","","",146,"2022-07-13 09:23:23","","10.48550/arXiv.2203.16181","","",,,,,1,1.00,0,3,1,"—Data streams are ubiquitous in modern business and society. In practice, data streams may evolve over time and cannot be stored indeﬁnitely. Effective and transparent machine learning on data streams is thus often challenging. Hoeffding Trees have emerged as a state-of-the art for online predictive modelling. They are easy to train and provide meaningful convergence guarantees under a stationary process. Yet, at the same time, Hoeffding Trees often require heuristic and costly extensions to adjust to distributional change, which may considerably impair their interpretability. In this work, we revisit Model Trees for machine learning in evolving data streams. Model Trees are able to maintain more ﬂexible and locally robust representations of the active data concept, making them a natural ﬁt for data stream applications. Our novel framework, called Dynamic Model Tree, satisﬁes desirable consistency and minimality properties. In experiments with synthetic and real-world tabular streaming data sets, we show that the proposed framework can drastically reduce the number of splits required by existing incremental decision trees. At the same time, our framework often outperforms state- of-the-art models in terms of predictive quality – especially when concept drift is involved. Dynamic Model Trees are thus a powerful online learning framework that contributes to more lightweight and interpretable machine learning in data streams.","",""
3,"Kyemyung Park, T. Prüstel, Yong Lu, J. Tsang","Machine learning of stochastic gene network phenotypes",2019,"","","","",147,"2022-07-13 09:23:23","","10.1101/825943","","",,,,,3,1.00,1,4,3,"A recurrent challenge in biology is the development of predictive quantitative models because most molecular and cellular parameters have unknown values and realistic models are analytically intractable. While the dynamics of the system can be analyzed via computer simulations, substantial computational resources are often required given uncertain parameter values resulting in large numbers of parameter combinations, especially when realistic biological features are included. Simulation alone also often does not yield the kinds of intuitive insights from analytical solutions. Here we introduce a general framework combining stochastic/mechanistic simulation of reaction systems and machine learning of the simulation data to generate computationally efficient predictive models and interpretable parameter-phenotype maps. We applied our approach to investigate stochastic gene expression propagation in biological networks, which is a contemporary challenge in the quantitative modeling of single-cell heterogeneity. We found that accurate, predictive machine-learning models of stochastic simulation results can be constructed. Even in the simplest networks existing analytical schemes generated significantly less accurate predictions than our approach, which revealed interesting insights when applied to more complex circuits, including the extensive tunability of information propagation enabled by feedforward circuits and how even single negative feedbacks can utilize stochastic fluctuations to generate robust oscillations. Our approach is applicable beyond biology and opens up a new avenue for exploring complex dynamical systems.","",""
4,"T. Schmid","Deconstructing the Final Frontier of Artificial Intelligence: Five Theses for a Constructivist Machine Learning",2019,"","","","",148,"2022-07-13 09:23:23","","","","",,,,,4,1.33,4,1,3,"Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subirón 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world – and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (Méhaut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as “hypothetical constructs” (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n","",""
1,"William Briguglio, Sherif Saad","Interpreting Machine Learning Malware Detectors Which Leverage N-gram Analysis",2019,"","","","",149,"2022-07-13 09:23:23","","10.1007/978-3-030-45371-8_6","","",,,,,1,0.33,1,2,3,"","",""
3,"F. Lécué, B. Abeloos, Jonathan Anctil, Manuel Bergeron, Damien Dalla-Rosa, Simon Corbeil-Letourneau, Florian Martet, Tanguy Pommellet, L. Salvan, Simon Veilleux, M. Ziaeefard","Thales XAI Platform: Adaptable Explanation of Machine Learning Systems - A Knowledge Graphs Perspective",2019,"","","","",150,"2022-07-13 09:23:23","","","","",,,,,3,1.00,0,11,3,"Explanation in Machine Learning systems has been identified to be the main asset to have for large scale deployment of Artificial Intelligence (AI) in critical systems. Explanations could be example-, features-, semantics-based or even counterfactual to potentially action on an AI system; they could be represented in many different ways e.g., textual, graphical, or visual. All representations serve different means, purpose and operators. We built the first-of-its-kind XAI (eXplainable AI) platform for critical systems i.e., Thales XAI Platform which aims at serving explanations through various forms. This paper emphasizes on the semantics-based explanations for Machine Learning systems. 1 Explainable AI in Critical Systems Motivation: The current hype of Artificial Intelligence (AI) mostly refers to the success of Machine Learning (ML) and its sub-domain of deep learning. However industries operating with critical systems are either highly regulated, or require high level of certification and robustness. Therefore, such industry constraints do limit the adoption of non deterministic and ML systems. Answers to the question of explainability will be intrinsically connected to the adoption of AI in industry at scale. Indeed explanation, which could be used for debugging intelligent systems or deciding to follow a recommendation in real-time, will increase acceptance and (business) user trust. Explainable AI (XAI) is now referring to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. Focus: Thales XAI Platform is designed to provide explanation for a ML task (classification, regression, object detection, segmentation). Although Thales XAI Platform does provide different levels of explanation e.g., example-based, featuresbased, counterfactual using textual and visual representations, we emphasis only on the semantics-based explanation through knowledge graphs. ? Copyright c © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Critical Applications: From adapting a plane trajectory, stopping a train, refitting a boat to reconfiguring a satellite, all are examples of critical situations where explanation is a must-to-have to follow an AI system decision. 2 Why Knowledge Graphs for Explainable AI? State-of-the-Art Limitations: Most approaches limits explanation of ML systems to features involved in the data and model, or at best to examples, prototypes or counterfactuals. Explanation should go beyond correlation (features importance) and numerical similarity (local explanation). Opportunity: By expanding and linking initial (training, validation and test) data with entities in knowledge graphs, (i) context is encoded, (ii) connections and relations are exposed, and (iii) inference and causation are natively supported. Knowledge graphs are used for encoding better representation of data, structuring a ML model in a more interpretable way, and adopt a semantic similarity for local (instance-based) and global (model-based) explanation. 3 Thales XAI Platform: A Knowledge Graph Perspective (Semantic) Perspective: The platform is combining ML and reasoning functionalities to expose a human-like rational as explanation when (i) recognizing an object (in a raw image) of any class in a knowledge graph, (ii) predicting a link in a knowledge graph. Thales XAI Platform is using state-of-the-art Semantic Web tools for enriching input, output (class) data with DBpedia (4, 233, 000 resources) and domain-specific knowledge graphs, usually enterprise knowledge graphs. This is a crucial step for contextualizing training, validation, test data. Explainable ML Classifications: Starting from raw images, as unstructured data, but with class labels augmented with a domain knowledge graph, Thales XAI Platform relies on existing neural network architectures to build the most appropriate models. All confidence scores of output classes on any input image are updated based on the semantic description of the output classes. For instance, an input classified as a car will have a higher overall confidence score in case some properties of car in the knowledge graph are retrieved e.g., having wheels, being on a road. In addition the platform is embedding naturally explanation i.e., properties of the objects retrieved in both the raw data and knowledge graph. Explainable Relational Learning: Starting from relational data, structured as graph, and augmented with a domain knowledge graph, Thales XAI Platform relies on existing knowledge graph embeddings frameworks to build the most appropriate models. Explanation of any link prediction is retrieved by identifying representative hotspots in the knowledge graph i.e., connected parts of the graphs that negatively impact prediction accuracy when removed.","",""
0,"Karsten Schwalbe, Alexander Groh, Frank Hertwig, U. Scheunert","Data fusion strategy to improve the realiability of machine learning based classifications",2019,"","","","",151,"2022-07-13 09:23:23","","10.1109/SDF.2019.8916636","","",,,,,0,0.00,0,4,3,"Automatic object recognition plays a major role in many industrial applications. This task is mostly performed by using optical sensors and image processing methods. Degeneration processes, such as surface wear, however, can pose quite some challenges when it comes to high-quality optical recognition. In this article we present our solution to optical character recognition of strongly degenerated numbers, characterized by a varying embossing depth and texture intensity, imprinted on metal surfaces. Under these conditions Machine Learning (ML) based recognition models seem to perform better than conventional ones. Typically, ML models have a black box character in the sense that the algorithm steps have no direct interpretable meaning and are kind of arbitrary. Consequently, the results of such models are difficult to interpret with respect to their trustworthiness. In order to receive more reliable recognition results, we have developed a rule-based fusion strategy that combines the output of several different AI models. This approach not only leads to a higher rate of correctly recognized objects, it also indicates when the recognition result is uncertain. As a result, our method increases the process safety and makes object recognition in industrial applications more flexible and robust.","",""
37,"","Towards trustable machine learning",2018,"","","","",152,"2022-07-13 09:23:23","","10.1038/s41551-018-0315-x","","",,,,,37,9.25,0,0,4,"","",""
8,"Jo-Hsuan Wu, T. Y. A. Liu, W. Hsu, J. Ho, Chien-Chang Lee","Performance and Limitation of Machine Learning Algorithms for Diabetic Retinopathy Screening: Meta-analysis",2021,"","","","",153,"2022-07-13 09:23:23","","10.2196/23863","","",,,,,8,8.00,2,5,1,"Background Diabetic retinopathy (DR), whose standard diagnosis is performed by human experts, has high prevalence and requires a more efficient screening method. Although machine learning (ML)–based automated DR diagnosis has gained attention due to recent approval of IDx-DR, performance of this tool has not been examined systematically, and the best ML technique for use in a real-world setting has not been discussed. Objective The aim of this study was to systematically examine the overall diagnostic accuracy of ML in diagnosing DR of different categories based on color fundus photographs and to determine the state-of-the-art ML approach. Methods Published studies in PubMed and EMBASE were searched from inception to June 2020. Studies were screened for relevant outcomes, publication types, and data sufficiency, and a total of 60 out of 2128 (2.82%) studies were retrieved after study selection. Extraction of data was performed by 2 authors according to PRISMA (Preferred Reporting Items for Systematic Reviews and Meta-Analyses), and the quality assessment was performed according to the Quality Assessment of Diagnostic Accuracy Studies 2 (QUADAS-2). Meta-analysis of diagnostic accuracy was pooled using a bivariate random effects model. The main outcomes included diagnostic accuracy, sensitivity, and specificity of ML in diagnosing DR based on color fundus photographs, as well as the performances of different major types of ML algorithms. Results The primary meta-analysis included 60 color fundus photograph studies (445,175 interpretations). Overall, ML demonstrated high accuracy in diagnosing DR of various categories, with a pooled area under the receiver operating characteristic (AUROC) ranging from 0.97 (95% CI 0.96-0.99) to 0.99 (95% CI 0.98-1.00). The performance of ML in detecting more-than-mild DR was robust (sensitivity 0.95; AUROC 0.97), and by subgroup analyses, we observed that robust performance of ML was not limited to benchmark data sets (sensitivity 0.92; AUROC 0.96) but could be generalized to images collected in clinical practice (sensitivity 0.97; AUROC 0.97). Neural network was the most widely used method, and the subgroup analysis revealed a pooled AUROC of 0.98 (95% CI 0.96-0.99) for studies that used neural networks to diagnose more-than-mild DR. Conclusions This meta-analysis demonstrated high diagnostic accuracy of ML algorithms in detecting DR on color fundus photographs, suggesting that state-of-the-art, ML-based DR screening algorithms are likely ready for clinical applications. However, a significant portion of the earlier published studies had methodology flaws, such as the lack of external validation and presence of spectrum bias. The results of these studies should be interpreted with caution.","",""
6,"S. Kalinin, K. Roccapriore, S. Cho, D. Milliron, R. Vasudevan, M. Ziatdinov, J. Hachtel","Separating Physically Distinct Mechanisms in Complex Infrared Plasmonic Nanostructures via Machine Learning Enhanced Electron Energy Loss Spectroscopy",2020,"","","","",154,"2022-07-13 09:23:23","","10.1002/adom.202001808","","",,,,,6,3.00,1,7,2,"Electron energy loss spectroscopy (EELS) enables direct exploration of plasmonic phenomena at the nanometer level. To isolate individual plasmon modes, linear unmixing methods can be used to separate different physical mechanisms, but in larger and more complex systems the interpretability of the components becomes uncertain. Here, infrared plasmonic resonances in self‐assembled heterogeneous monolayer films of doped‐semiconductor nanoparticles are examined beyond linear unmixing techniques, and both supervised and unsupervised machine‐learning‐based analyses of hyperspectral EELS datasets are demonstrated. In the supervised approach, a human operator labels a small number of pixels in the hyperspectral dataset corresponding to features of interest which are then propagated across the entire dataset. In the unsupervised approach, non‐linear autoencoders are used to create a highly‐reduced latent‐space representation of the dataset, within which insight into the relevant physics can be gleaned from straightforward distance metrics that do not depend on operator input and bias. The advantage of these approaches is that the labeling separates physical mechanisms without altering the data, enabling robust analyses of the influence of heterogeneities in mesoscale complex systems.","",""
7,"E. Rozos, P. Dimitriadis, V. Bellos","Machine Learning in Assessing the Performance of Hydrological Models",2021,"","","","",155,"2022-07-13 09:23:23","","10.3390/hydrology9010005","","",,,,,7,7.00,2,3,1,"Machine learning has been employed successfully as a tool virtually in every scientific and technological field. In hydrology, machine learning models first appeared as simple feed-forward networks that were used for short-term forecasting, and have evolved into complex models that can take into account even the static features of catchments, imitating the hydrological experience. Recent studies have found machine learning models to be robust and efficient, frequently outperforming the standard hydrological models (both conceptual and physically based). However, and despite some recent efforts, the results of the machine learning models require significant effort to interpret and derive inferences. Furthermore, all successful applications of machine learning in hydrology are based on networks of fairly complex topology that require significant computational power and CPU time to train. For these reasons, the value of the standard hydrological models remains indisputable. In this study, we suggest employing machine learning models not as a substitute for hydrological models, but as an independent tool to assess their performance. We argue that this approach can help to unveil the anomalies in catchment data that do not fit in the employed hydrological model structure or configuration, and to deal with them without compromising the understanding of the underlying physical processes.","",""
0,"Ali M’Rabeth","Model Risk in the age of Artificial Intelligence and Machine Learning What are the impacts on Model Risk?",2019,"","","","",156,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,1,3,"An increasing reliance on Artificial Intelligence for decision making is driving financial institutions, regulators, and supervisors towards a clarification of sources and control of risks. These risks were either already present (but marginal) or even non-existent in the usual model risk management framework. In a context where the use of machine learning is becoming massive and industrialized across banks and insurance companies, problematics such as interpretability and dynamic monitoring, robustness, ethics, bias and fairness require a specific attention.","",""
1111,"T. Baltrušaitis, Chaitanya Ahuja, Louis-Philippe Morency","Multimodal Machine Learning: A Survey and Taxonomy",2017,"","","","",157,"2022-07-13 09:23:23","","10.1109/TPAMI.2018.2798607","","",,,,,1111,222.20,370,3,5,"Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.","",""
1,"S. Perrier, A. Delpeint","Characterization of Hydraulic Fracture Barriers in Shale Play Through Core-Log Integration: Practical Integration of Machine Learning and Geological Domain Expertise",2019,"","","","",158,"2022-07-13 09:23:23","","10.2118/197307-ms","","",,,,,1,0.33,1,2,3,"      Unconventional shale development requires continuous optimization to improve the Stimulated Rock Volume (SRV) created by hydraulic fracturing, which in turn maximizes the hydrocarbon recovery of wells. Whenever shale formations exhibit a geological heterogeneity, the distribution and magnitude of the associated geomechanical heterogeneity can significantly impact fracture propagation and result in fracture barriers or baffles that negatively impact the SRV. It is essential to adapt well targeting, hydraulic fracture design and well spacing to these heterogeneities to optimize the SRV. In this case study, such mechanically heterogeneous beds within the reservoir (resulting from geologic variability) were identified through core analysis and measurements. These heterogeneities did not have a clear interpretable log signature so it was difficult to locate, map, and assess their distribution across the play using well logs prior to applying the methods described in this paper.        The method discussed in this paper consists of designing a machine learning predictive model that after training on 9 cored wells, was able to predict the distribution and thickness of the geomechanical heterogeneities across the play using roughly 100 vertical wells with triple combo logs.  Beyond the classic methodology of machine learning, today considered a conventional technology, this paper presents the key steps of data processing that significantly improved prediction accuracy, and focuses on explaining why most of those steps are likely to be useful for a variety of analogous geological machine learning workflows. The workflow included: 1- an original transformation of the raw logs into engineered features based on a proper understanding of the impact of the heterogeneities on the behavior of each log; 2- a decomposition of the classification model into multiple stages, to integrate geological expertise and boost some critical algorithmic elements (in particular through class imbalance correction and bias-variance optimization); 3- an advanced management of cross-validation and exploitation of genetic searching, to optimize model robustness with a relatively small input dataset.        Excellent prediction accuracy based on cross-validation was confirmed by a remarkable geological/geographical consistency of the results, once prediction results were converted into maps. The continuity of deposits and orientations of the sediment supply were in line with known basin paleogeography.  This paper defines a comprehensive approach of machine learning applied to electrofacies, and beyond the direct results of the study, it highlights how data science methods benefit from in-depth integration of geological interpretation.  As mentioned earlier, this case is also a great demonstration of the capacity of Machine Learning to identify weak signals within the data, in a case where human interpretation is limited. ","",""
5,"Francesco Regazzoni, D. Chapelle, P. Moireau","Combining data assimilation and machine learning to build data‐driven models for unknown long time dynamics—Applications in cardiovascular modeling",2021,"","","","",159,"2022-07-13 09:23:23","","10.1002/cnm.3471","","",,,,,5,5.00,2,3,1,"We propose a method to discover differential equations describing the long‐term dynamics of phenomena featuring a multiscale behavior in time, starting from measurements taken at the fast‐scale. Our methodology is based on a synergetic combination of data assimilation (DA), used to estimate the parameters associated with the known fast‐scale dynamics, and machine learning (ML), used to infer the laws underlying the slow‐scale dynamics. Specifically, by exploiting the scale separation between the fast and the slow dynamics, we propose a decoupling of time scales that allows to drastically lower the computational burden. Then, we propose a ML algorithm that learns a parametric mathematical model from a collection of time series coming from the phenomenon to be modeled. Moreover, we study the interpretability of the data‐driven models obtained within the black‐box learning framework proposed in this paper. In particular, we show that every model can be rewritten in infinitely many different equivalent ways, thus making intrinsically ill‐posed the problem of learning a parametric differential equation starting from time series. Hence, we propose a strategy that allows to select a unique representative model in each equivalence class, thus enhancing the interpretability of the results. We demonstrate the effectiveness and noise‐robustness of the proposed methods through several test cases, in which we reconstruct several differential models starting from time series generated through the models themselves. Finally, we show the results obtained for a test case in the cardiovascular modeling context, which sheds light on a promising field of application of the proposed methods.","",""
4,"Kengo Ito, Xiangru Xu, J. Kikuchi","Improved Prediction of Carbonless NMR Spectra by the Machine Learning of Theoretical and Fragment Descriptors for Environmental Mixture Analysis.",2021,"","","","",160,"2022-07-13 09:23:23","","10.1021/acs.analchem.1c00756","","",,,,,4,4.00,1,3,1,"As the first multidimensional NMR approach, 2D J-resolved (2DJ) spectroscopy is distinguished by signal resolution and detection sensitivity with remarkable advantages for the exhaustive evaluation of complex mixtures and environmental samples due to its carbonless feature without the requirement of 13C connectivity. Generally, the 2DJ signal assignment of metabolic mixtures is problematic in spite of references to experimental NMR databases, owing to the existence of metabolic ""dark matter."" In this study, a new method to predict 2DJ spectra was developed with a combination of quantum mechanical (QM) computation and machine learning (ML). The predictive accuracy of J-coupling constants was evaluated using validated data. The root-mean-square deviation (RMSD) for QM computation was 3.52 Hz, while the RMSD for QM + ML was 1.21 Hz, indicating a substantial increase in predictive accuracy. The proposed model was applied to predict the 2DJ spectra of 60 standard substances and 55 components of seawater. Furthermore, two practical environmental samples were used to evaluate the robustness of the constructed predictive model. A J-coupling tree and J-split spectra produced from QM + ML of aliphatic moieties had good consistency with the experimental data, as compared with the theoretical data produced by QM computation. The predicted J-coupling tree for the J-coupling multiplet analysis of freely rotating bonds in the complex mixture, which is traditionally difficult, was interpretable. In addition, in silico identification of the J-split 1H NMR signals, which was independent of experimental databases, aided in the discovery of new components in a mixture.","",""
4,"A. Serban, Joost Visser","An Empirical Study of Software Architecture for Machine Learning",2021,"","","","",161,"2022-07-13 09:23:23","","","","",,,,,4,4.00,2,2,1,"Specific developmental and operational characteristics of machine learning (ML) components, as well as their inherent uncertainty, demand robust engineering principles are used to ensure their quality. We aim to determine how software systems can be (re-) architected to enable robust integration of ML components. Towards this goal, we conducted a mixed-methods empirical study consisting of (i) a systematic literature review to identify the challenges and their solutions in software architecture for ML, (ii) semi-structured interviews with practitioners to qualitatively complement the initial findings, and (iii) a survey to quantitatively validate the challenges and their solutions. In total, we compiled and validated twenty challenges and solutions for (re-) architecting systems with ML components. Our results indicate, for example, that traditional software architecture challenges (e.g., component coupling) also play an important role when using ML components; along new ML specific challenges (e.g., the need for continuous retraining). Moreover, the results indicate that ML heightened decision drivers, such as privacy, play a marginal role compared to traditional decision drivers, such as scalability or interoperability. Using the survey, we were able to establish a link between architectural solutions and software quality attributes; which enabled us to provide twenty architectural tactics used for satisfying individual quality requirements of systems with ML components. Altogether, the results can be interpreted as an empirical framework that supports the process of (re-) architecting software systems with ML components.","",""
4,"A. Mei, I. Milosavljevic, A. L. Simpson, Valerie A. Smetanka, Colin P. Feeney, Shay M. Seguin, S. D. Ha, W. Ha, M. Reed","Optimization of quantum-dot qubit fabrication via machine learning",2020,"","","","",162,"2022-07-13 09:23:23","","10.1063/5.0040967","","",,,,,4,2.00,0,9,2,"Precise nanofabrication represents a critical challenge to developing semiconductor quantum-dot qubits for practical quantum computation. Here, we design and train a convolutional neural network to interpret in-line scanning electron micrographs and quantify qualitative features affecting device functionality. The high-throughput strategy is exemplified by optimizing a model lithographic process within a five-dimensional design space and by demonstrating a new approach to address lithographic proximity effects. The present results emphasize the benefits of machine learning for developing robust processes, shortening development cycles, and enforcing quality control during qubit fabrication.","",""
3,"Oliver Aasmets, Kreete Lüll, Jennifer M. Lang, Calvin Pan, J. Kuusisto, K. Fischer, M. Laakso, A. Lusis, E. Org","Machine Learning Reveals Time-Varying Microbial Predictors with Complex Effects on Glucose Regulation",2020,"","","","",163,"2022-07-13 09:23:23","","10.1128/mSystems.01191-20","","",,,,,3,1.50,0,9,2,"Recent studies have shown a clear link between gut microbiota and type 2 diabetes. However, current results are based on cross-sectional studies that aim to determine the microbial dysbiosis when the disease is already prevalent. ABSTRACT The incidence of type 2 diabetes (T2D) has been increasing globally, and a growing body of evidence links type 2 diabetes with altered microbiota composition. Type 2 diabetes is preceded by a long prediabetic state characterized by changes in various metabolic parameters. We tested whether the gut microbiome could have predictive potential for T2D development during the healthy and prediabetic disease stages. We used prospective data of 608 well-phenotyped Finnish men collected from the population-based Metabolic Syndrome in Men (METSIM) study to build machine learning models for predicting continuous glucose and insulin measures in a shorter (1.5 year) and longer (4 year) period. Our results show that the inclusion of the gut microbiome improves prediction accuracy for modeling T2D-associated parameters such as glycosylated hemoglobin and insulin measures. We identified novel microbial biomarkers and described their effects on the predictions using interpretable machine learning techniques, which revealed complex linear and nonlinear associations. Additionally, the modeling strategy carried out allowed us to compare the stability of model performance and biomarker selection, also revealing differences in short-term and long-term predictions. The identified microbiome biomarkers provide a predictive measure for various metabolic traits related to T2D, thus providing an additional parameter for personal risk assessment. Our work also highlights the need for robust modeling strategies and the value of interpretable machine learning. IMPORTANCE Recent studies have shown a clear link between gut microbiota and type 2 diabetes. However, current results are based on cross-sectional studies that aim to determine the microbial dysbiosis when the disease is already prevalent. In order to consider the microbiome as a factor in disease risk assessment, prospective studies are needed. Our study is the first study that assesses the gut microbiome as a predictive measure for several type 2 diabetes-associated parameters in a longitudinal study setting. Our results revealed a number of novel microbial biomarkers that can improve the prediction accuracy for continuous insulin measures and glycosylated hemoglobin levels. These results make the prospect of using the microbiome in personalized medicine promising.","",""
3,"A. Ounajim, M. Billot, L. Goudman, P. Louis, Y. Slaoui, M. Roulaud, B. Bouche, P. Page, B. Lorgeoux, Sandrine Baron, Nihel Adjali, K. Nivole, Nicolas Naiditch, Chantal Wood, Raphaël Rigoard, R. David, M. Moens, P. Rigoard","Machine Learning Algorithms Provide Greater Prediction of Response to SCS Than Lead Screening Trial: A Predictive AI-Based Multicenter Study",2021,"","","","",164,"2022-07-13 09:23:23","","10.3390/jcm10204764","","",,,,,3,3.00,0,18,1,"Persistent pain after spinal surgery can be successfully addressed by spinal cord stimulation (SCS). International guidelines strongly recommend that a lead trial be performed before any permanent implantation. Recent clinical data highlight some major limitations of this approach. First, it appears that patient outco mes, with or without lead trial, are similar. In contrast, during trialing, infection rate drops drastically within time and can compromise the therapy. Using composite pain assessment experience and previous research, we hypothesized that machine learning models could be robust screening tools and reliable predictors of long-term SCS efficacy. We developed several algorithms including logistic regression, regularized logistic regression (RLR), naive Bayes classifier, artificial neural networks, random forest and gradient-boosted trees to test this hypothesis and to perform internal and external validations, the objective being to confront model predictions with lead trial results using a 1-year composite outcome from 103 patients. While almost all models have demonstrated superiority on lead trialing, the RLR model appears to represent the best compromise between complexity and interpretability in the prediction of SCS efficacy. These results underscore the need to use AI-based predictive medicine, as a synergistic mathematical approach, aimed at helping implanters to optimize their clinical choices on daily practice.","",""
3,"R. Barker, S. Barker, M. Cracknell, Elizabeth D. Stock, G. Holmes","Quantitative Mineral Mapping of Drill Core Surfaces II: Long-Wave Infrared Mineral Characterization Using μXRF and Machine Learning",2021,"","","","",165,"2022-07-13 09:23:23","","10.5382/econgeo.4804","","",,,,,3,3.00,1,5,1,"Long-wave infrared (LWIR) spectra can be interpreted using a Random Forest machine learning approach to predict mineral species and abundances. In this study, hydrothermally altered carbonate rock core samples from the Fourmile Carlin-type Au discovery, Nevada, were analyzed by LWIR and micro-X-ray fluorescence (μXRF). Linear programming-derived mineral abundances from quantified μXRF data were used as training data to construct a series of Random Forest regression models. The LWIR Random Forest models produced mineral proportion estimates with root mean square errors of 1.17 to 6.75% (model predictions) and 1.06 to 6.19% (compared to quantitative X-ray diffraction data) for calcite, dolomite, kaolinite, white mica, phlogopite, K-feldspar, and quartz. These results are comparable to the error of proportion estimates from linear spectral deconvolution (±7–15%), a commonly used spectral unmixing technique. Having a mineralogical and chemical training data set makes it possible to identify and quantify mineralogy and provides a more robust and meaningful LWIR spectral interpretation than current methods of utilizing a spectral library or spectral end-member extraction. Using the method presented here, LWIR spectroscopy can be used to overcome the limitations inherent with the use of short-wave infrared (SWIR) in fine-grained, low reflectance rocks. This new approach can be applied to any deposit type, improving the accuracy and speed of infrared data interpretation.","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",166,"2022-07-13 09:23:23","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
1,"R. Pettit, Robert Fullem, Chao Cheng, Chris Amos","Artificial intelligence, machine learning, and deep learning for clinical outcome prediction",2021,"","","","",167,"2022-07-13 09:23:23","","10.1042/ETLS20210246","","",,,,,1,1.00,0,4,1,"AI is a broad concept, grouping initiatives that use a computer to perform tasks that would usually require a human to complete. AI methods are well suited to predict clinical outcomes. In practice, AI methods can be thought of as functions that learn the outcomes accompanying standardized input data to produce accurate outcome predictions when trialed with new data. Current methods for cleaning, creating, accessing, extracting, augmenting, and representing data for training AI clinical prediction models are well defined. The use of AI to predict clinical outcomes is a dynamic and rapidly evolving arena, with new methods and applications emerging. Extraction or accession of electronic health care records and combining these with patient genetic data is an area of present attention, with tremendous potential for future growth. Machine learning approaches, including decision tree methods of Random Forest and XGBoost, and deep learning techniques including deep multi-layer and recurrent neural networks, afford unique capabilities to accurately create predictions from high dimensional, multimodal data. Furthermore, AI methods are increasing our ability to accurately predict clinical outcomes that previously were difficult to model, including time-dependent and multi-class outcomes. Barriers to robust AI-based clinical outcome model deployment include changing AI product development interfaces, the specificity of regulation requirements, and limitations in ensuring model interpretability, generalizability, and adaptability over time.","",""
2,"D. Devakumar, Goutham Sunny, B. Sasidharan, S. Bowen, Ambily Nadaraj, L. Jeyseelan, Manu Mathew, A. Irodi, R. Isiah, S. Pavamani, S. John, H. T. T Thomas","Framework for Machine Learning of CT and PET Radiomics to Predict Local Failure after Radiotherapy in Locally Advanced Head and Neck Cancers",2021,"","","","",168,"2022-07-13 09:23:23","","10.4103/jmp.JMP_6_21","","",,,,,2,2.00,0,12,1,"Context: Cancer Radiomics is an emerging field in medical imaging and refers to the process of converting routine radiological images that are typically qualitatively interpreted to quantifiable descriptions of the tumor phenotypes and when combined with statistical analytics can improve the accuracy of clinical outcome prediction models. However, to understand the radiomic features and their correlation to molecular changes in the tumor, first, there is a need for the development of robust image analysis methods, software tools and statistical prediction models which is often limited in low- and middle-income countries (LMIC). Aims: The aim is to build a framework for machine learning of radiomic features of planning computed tomography (CT) and positron emission tomography (PET) using open source radiomics and data analytics platforms to make it widely accessible to clinical groups. The framework is tested in a small cohort to predict local disease failure following radiation treatment for head-and-neck cancer (HNC). The predictors were also compared with the existing Aerts HNC radiomics signature. Settings and Design: Retrospective analysis of patients with locally advanced HNC between 2017 and 2018 and 31 patients with both pre- and post-radiation CT and evaluation PET were selected. Subjects and Methods: Tumor volumes were delineated on baseline PET using the semi-automatic adaptive-threshold algorithm and propagated to CT; PyRadiomics features (total of 110 under shape/intensity/texture classes) were extracted. Two feature-selection methods were tested for model stability. Models were built based on least absolute shrinkage and selection operator-logistic and Ridge regression of the top pretreatment radiomic features and compared to Aerts' HNC-signature. Average model performance across all internal validation test folds was summarized by the area under the receiver operator curve (ROC). Results: Both feature selection methods selected CT features MCC (GLCM), SumEntropy (GLCM) and Sphericity (Shape) that could predict the binary failure status in the cross-validated group and achieved an AUC >0.7. However, models using Aerts' signature features (Energy, Compactness, GLRLM-GrayLevelNonUniformity and GrayLevelNonUniformity-HLH wavelet) could not achieve a clear separation between outcomes (AUC = 0.51–0.54). Conclusions: Radiomics pipeline included open-source workflows which makes it adoptable in LMIC countries. Additional independent validation of data is crucial for the implementation of radiomic models for clinical risk stratification.","",""
1,"P. Benner, A. Klawonn, M. Stoll","Topical Issue Scientific Machine Learning (1/2)",2021,"","","","",169,"2022-07-13 09:23:23","","10.1002/gamm.202100005","","",,,,,1,1.00,0,3,1,"Scientific Machine Learning is a rapidly evolving field of research that combines and further develops techniques of scientific computing and machine learning. Special emphasis is given to the scientific (physical, chemical, biological, etc.) interpretability of models learned from data and their usefulness for robust predictions. On the other hand, this young field also investigates the utilization of Machine Learning methods for improving numerical algorithms in Scientific Computing. The name Scientific Machine Learning has been coined at a Basic Research Needs Workshop of the US Department of Energy (DOE) in January, 2018. It resulted in a report [2] published in February, 2019; see also [1] for a short brochure on this topic. The present special issue of the GAMM Mitteilungen, which is the first of a two-part series, contains contributions on the topic of Scientific Machine Learning in the context of complex applications across the sciences and engineering. Research in this new exciting field needs to address challenges such as complex physics, uncertain parameters, and possibly limited data through the development of new methods that combine algorithms from computational science and engineering and from numerical analysis with state of the art techniques from machine learning. At the GAMM Annual Meeting 2019, the activity group Computational and Mathematical Methods in Data Science (CoMinDS) has been established. Meanwhile, it has become a meeting place for researchers interested in all aspects of data science. All three editors of this special issue are founding members of this activity group. Because of the rapid development both in the theoretical foundations and the applicability of Scientific Machine Learning techniques, it is time to highlight developments within the field in the hope that it will become an essential domain within the GAMM and topical issues like this will have a frequent occurrence within this journal. We are happy that eight teams of authors have accepted our invitation to report on recent research highlights in Scientific Machine Learning, and to point out the relevant literature as well as software. The four papers in this first part of the special issue are: • Stoll, Benner: Machine Learning for Material Characterization with an Application for Predicting Mechanical Properties. This work explores the use of machine learning techniques for material property prediction. Given the abundance of data available in industrial applications, machine learning methods can help finding patterns in the data and the authors focus on the case of the small punch test and tensile data for illustration purposes. • Beck, Kurz: A Perspective on Machine Modelling Learning Methods in Turbulence. Turbulence modelling remains a humongous challenge in the simulation and analysis of complex flows. The authors review the use of data-driven techniques to open up new ways for studying turbulence and focus on the challenges and opportunities that machine learning brings to this field. • Heinlein, Klawonn, Lanser, Weber: Combining Machine Learning and Domain Decomposition Methods for the Solution of Partial Differential Equations – A Review. Domain decomposition (DD) has been a workhorse of solving complex simulation tasks. The authors review the combination of machine learning approaches with state-of-the-art DD-schemes. Their focus is on the use of ML techniques to improve the computational effort of adaptive domain decomposition schemes and the use of novel ML methods for the discretization and solution of subdomain problems. • Budd, van Gennip, Latz: Classification and image processing with a semi-discrete scheme for fidelity forced Allen–Cahn on graphs. Learning based on graphs provides exciting possibilities for discovering and using additional structure in data. In this work, the authors illustrate the use of a PDE-based learning technique relying on the graph Allen-Cahn equation for the segmentation of images. The authors illustrate that computational and mathematical advances can lead to efficiency and accuracy gains. Peter Benner1,2 Axel Klawonn3,4 Martin Stoll5","",""
2,"Nick Fountain-Jones, Megan L. Smith, F. Austerlitz","Machine learning in molecular ecology",2021,"","","","",170,"2022-07-13 09:23:23","","10.1111/1755-0998.13532","","",,,,,2,2.00,1,3,1,"Advances in nextgeneration sequencing (NGS) platforms are allowing researchers to routinely collate large genomewide data sets to address a variety of ecological questions. However, with this big data comes big analytical challenges that are increasingly addressed using machine learning (for a review, see Schrider & Kern, 2018). Machine learning is a subfield of artificial intelligence and represents a conglomeration of methods where predictive accuracy is the primary goal (e.g., Belcaid & Toonen, 2015; Breiman, 2001; Elith et al., 2008; Lucas, 2020). Machine learning assumes that the datagenerating process is unknown and complex and finds the dominant patterns by learning the relationships between inputs and responses (Elith et al., 2008). Broadly, machine learning differs from other statistical approaches in two important ways. The first is that predictive performance drives model formulation rather than model selection or expert opinion, and the second is there is less emphasis on model selection ( Breiman, 2001; Lucas, 2020). For these reasons, machine learning has the reputation for being less interpretable and difficult to apply rigorously (Elith et al., 2008; Lucas, 2020; Molnar, 2018). However, in parallel with the revolution of sequencing techniques, there has also been a revolution in data science in terms of predictive performance and techniques to interpret machine learning models (FountainJones et al., 2019; Lucas, 2020; Molnar, 2018). There are now streamlined R and Python packages that make the robust use of algorithms from support vector machines (SVMs) to neural networks readily achievable (e.g., Abadi et al., 2015; Kuhn & Wickham, 2020, see Text Box 1 for some important machine learning terminology). Moreover, other statistical paradigms such as approximate Bayesian computation (ABC) are being applied sidebyside or within machine learning frameworks to enhance the utility of these approaches (e.g., Carlson, 2020; Raynal et al., 2019). The ability of machine learning algorithms to build powerful predictive models that capture complex nonlinear responses with minimal statistical assumptions has been harnessed by most molecular ecology subdisciplines for decades. For example, machine learning models were developed before the turn of the millennium to classify normal or cancerous tissue based on transcription profiles (Furey et al., 2000). Not long after gradient boosting models (GBMs) were developed (e.g., Hastie et al., 2009), researchers were applying the approach to classify population genetics models based on a suite of summary statistics such as Tajima's θπ (Lin et al., 2011). In addition, extensions of the popular random forest algorithm have been utilized in ecological genetics to untangle the drivers of climate adaptation (Fitzpatrick & Keller, 2015). Generally, however, advances in computer science and machine learning are slow to filter down to ecologists (Belcaid & Toonen, 2015; Elith & Hastie, 2008; FountainJones et al., 2019), partly through unfamiliarity with these types of approaches but also because of the rapid rate of advance in the data science field. This Special Issue aims to help expand the use of machine learning approaches and to help bring advances in data science to the toolkits of molecular ecologists. This issue comprises 17 papers grouped into four sections covering a diverse variety of molecular ecology subdisciplines. The first section covers how machine learning can be applied to make inferences about population demography. We further group these papers algorithmically with four papers utilizing random forest architecture and the remaining four using neural networks. The second section highlights how machine learning can detect signatures of selection across loci. The third section highlights how these methods can be applied to untangle the complex ecological drivers of genomic change (‘ecological genomics’) and species community dynamics. The last section explores how advances in machine learning can provide insights into species limits and contribute to biodiversity monitoring.","",""
2,"N. Frolov, Muhammad Salman Kabir, V. Maksimenko, A. Hramov","Machine learning evaluates changes in functional connectivity under a prolonged cognitive load.",2021,"","","","",171,"2022-07-13 09:23:23","","10.1063/5.0070493","","",,,,,2,2.00,1,4,1,"One must be aware of the black-box problem by applying machine learning models to analyze high-dimensional neuroimaging data. It is due to a lack of understanding of the internal algorithms or the input features upon which most models make decisions despite outstanding performance in classification, pattern recognition, and prediction. Here, we approach the fundamentally high-dimensional problem of classifying cognitive brain states based on functional connectivity by selecting and interpreting the most relevant input features. Specifically, we consider the alterations in the cortical synchrony under a prolonged cognitive load. Our study highlights the advances of this machine learning method in building a robust classification model and percept-related prestimulus connectivity changes over the conventional trial-averaged statistical analysis.","",""
1,"Nicola Loi, C. Borile, Daniele Ucci","Towards an Automated Pipeline for Detecting and Classifying Malware through Machine Learning",2021,"","","","",172,"2022-07-13 09:23:23","","","","",,,,,1,1.00,0,3,1,"The constant growth in the number of malware software or code fragment potentially harmful for computers and information networks and the use of sophisticated evasion and obfuscation techniques have seriously hindered classic signature-based approaches. On the other hand, malware detection systems based on machine learning techniques started offering a promising alternative to standard approaches, drastically reducing analysis time and turning out to be more robust against evasion and obfuscation techniques. In this paper, we propose a malware taxonomic classification pipeline able to classify Windows Portable Executable files (PEs). Given an input PE sample, it is first classified as either malicious or benign. If malicious, the pipeline further analyzes it in order to establish its threat type, family, and behavior(s). We tested the proposed pipeline on the open source dataset EMBER, containing approximately 1 million PE samples, analyzed through static analysis. Obtained malware detection results are comparable to other academic works in the current state of art and, in addition, we provide an in-depth classification of malicious samples. Models used in the pipeline provides interpretable results which can help security analysts in better understanding decisions taken by the automated pipeline.","",""
0,"Tochukwu Idika, Ismail Akturk","Attack-Centric Approach for Evaluating Transferability of Adversarial Samples in Machine Learning Models",2021,"","","","",173,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,2,1,"Transferability of adversarial samples became a serious concern due to their impact on the reliability of machine learning system deployments, as they find their way into many critical applications. Knowing factors that influence transferability of adversarial samples can assist experts to make informed decisions on how to build robust and reliable machine learning systems. The goal of this study is to provide insights on the mechanisms behind the transferability of adversarial samples through an attack-centric approach. This attack-centric perspective interprets how adversarial samples would transfer by assessing the impact of machine learning attacks (that generated them) on a given input dataset. To achieve this goal, we generated adversarial samples using attacker models and transferred these samples to victim models. We analyzed the behavior of adversarial samples on victim models and outlined four factors that can influence the transferability of adversarial samples. Although these factors are not necessarily exhaustive, they provide useful insights to researchers and practitioners of machine learning systems.","",""
0,"A. Serban, Joost Visser","Adapting Software Architectures to Machine Learning Challenges",2021,"","","","",174,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,2,1,"Unique developmental and operational characteristics of machine learning (ML) components as well as their inherent uncertainty demand robust engineering principles are used to ensure their quality. We aim to determine how software systems can be (re-) architected to enable robust integration of ML components. Towards this goal, we conducted a mixed-methods empirical study consisting of (i) a systematic literature review to identify the challenges and their solutions in software architecture for ML, (ii) semi-structured interviews with practitioners to qualitatively complement the initial findings and (iii) a survey to quantitatively validate the challenges and their solutions. We compiled and validated twenty challenges and solutions for (re-) architecting systems with ML components. Our results indicate, for example, that traditional software architecture challenges (e.g., component coupling) also play an important role when using ML components; along with new ML specific challenges (e.g., the need for continuous retraining). Moreover, the results indicate that ML heightened decision drivers, such as privacy, play a marginal role compared to traditional decision drivers, such as scalability. Using the survey we were able to establish a link between architectural solutions and software quality attributes, which enabled us to provide twenty architectural tactics used to satisfy individual quality requirements of systems with ML components. Altogether, the results of the study can be interpreted as an empirical framework that supports the process of (re-) architecting software systems with ML components.","",""
0,"M. Ntampaka, Matthew Ho, B. Nord","Building Trustworthy Machine Learning Models for Astronomy",2021,"","","","",175,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,3,1,". Astronomy is entering an era of data-driven discovery, due in part to modern machine learning (ML) techniques enabling powerful new ways to interpret observations. This shift in our scientiﬁc approach requires us to consider whether we can trust the black box. Here, we overview methods for an often-overlooked step in the development of ML models: building community trust in the algorithms. Trust is an essential ingredient not just for creating more robust data analysis techniques, but also for building conﬁdence within the astronomy community to embrace machine learning methods and results.","",""
0,"S. Pande, Bineet Kumar Jha","Character Recognition System for Devanagari Script Using Machine Learning Approach",2021,"","","","",176,"2022-07-13 09:23:23","","10.1109/ICCMC51019.2021.9418028","","",,,,,0,0.00,0,2,1,"It is a very difficult task to manually process the handwritten documents due to varieties of handwritten scripts and lack of associated language dictionary to interpret documents. Most of the large companies as well as small-scale industries want to automate the process of script recognition. The big challenge is to make machines recognize the hand-printed scripts. Humans can recognize handwritten or hand-printed words after gaining knowledge of a specific language. In the same way, machines should be trained to recognize the handwritten scripts. This process of transferring human knowledge to computers should be automated. The proposed research work attempts to automate the character recognition system for Devanagari script using various machine learning classifiers like Decision Tree classifier, Nearest Centroid classifier, K Nearest Neighbors classifier, Extra Trees classifiers and Random Forest classifier. The performance of all the classifiers is evaluated using accuracy parameter as success criteria. The Extra Trees classifiers and Random Forest classifier is proved to better than other classifiers with 78% and 77% of accuracy respectively. The robustness to picture quality, writing style, font size is the novelty of the OCR system which makes it ideal to use.","",""
0,"V. Yasaswini, P. Reeshika, K. Roy, N. Kalpana, Rajesh Yamparala","Drowsiness Detection Using Machine Learning Algorithms",2021,"","","","",177,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,5,1,": The abstract presents a literature review of driver drowsiness detection based on behavioural measures using machine learning techniques. Faces contain information that can be used to interpret levels of drowsiness. There are many facial features that can be extracted from the face to infer the level of drowsiness. However, the development of a drowsiness detection system that yields reliable and accurate results is a challenging task as it requires accurate and robust algorithms. A wide range of techniques has been examined to detect driver drowsiness in the past. As a result, machine learning techniques which include convolution neural networks in the context of drowsiness detection. Here convolution neural networks performed better than any other techniques.","",""
0,"B. Kaiser, J. Saenz, M. Sonnewald, D. Livescu","Objective discovery of dominant dynamical processes with machine learning",2021,"","","","",178,"2022-07-13 09:23:23","","10.21203/rs.3.rs-745356/v1","","",,,,,0,0.00,0,4,1,"  Significant advances in the understanding and modeling of dynamical systems has been enabled by the identification of processes that locally and approximately dominate system behavior, or dynamical regimes. The conventional regime identification method involves tedious and ad hoc parsing of data to judiciously obtain scales to ascertain which governing equation terms are dominant in each regime. Surprisingly, no objective and universally applicable criterion exists to robustly identify dynamical regimes in an unbiased manner, neither for conventional nor for machine learning-based methods of analysis. Here, we formally define dynamical regime identification as an optimization problem by using a verification criterion, and we show that an unsupervised learning framework can automatically and credibly identify regimes. This eliminates reliance upon ad hoc conventional analyses, with vast potential to accelerate discovery. Our verification criterion also enables unbiased comparison of regimes identified by different methods. In addition to diagnostic applications, the verification criterion and learning framework are immediately useful for data-driven dynamical process modeling, and are relevant to researchers interested in the development of inherently interpretable methods for scientific machine learning. Automation of this kind of approximate mechanistic analysis is necessary for scientists to gain new dynamical insights from increasingly large data streams.","",""
0,"P. Rasouli, Ingrid Chieh Yu","Explainable Debugger for Black-box Machine Learning Models",2021,"","","","",179,"2022-07-13 09:23:23","","10.1109/IJCNN52387.2021.9533944","","",,,,,0,0.00,0,2,1,"The research around developing methods for debugging and refining Machine Learning (ML) models is still in its infancy. We believe employing tailored tools in the development process can help developers in creating more trustworthy and reliable models. This is particularly essential for creating black-box models such as deep neural networks and random forests, as their opaque decision-making and complex structure prevent detailed investigations. Although many explanation techniques provide interpretability in terms of predictive features for a mispredicted instance, it would be beneficial for a developer to find a partition of the training data that significantly influences the anomaly. Such responsible partitions can be subjected to data visualization and data engineering in the development phase to improve the model's accuracy. In this paper, we propose a systematic debugging framework for the development of ML models that guides the data engineering process using the model's decision boundary. Our approach finds the influential neighborhood of anomalous data points using observation-level feature importance and explains them via a novel quasi-global explanation technique. It is also equipped with a robust global explanation approach to reveal general trends and expose potential biases in the neighborhoods. We demonstrate the efficacy of the devised framework through several experiments on standard data sets and black-box models and propose various guidelines on how the framework's components can be practically useful from a developer's perspective.","",""
0,"W. James Murdoch","Interpretable deep learning for natural language processing",2019,"","","","",180,"2022-07-13 09:23:23","","","","",,,,,0,0.00,0,1,3,"Author(s): Murdoch, William James | Advisor(s): Yu, Bin | Abstract: Machine-learning models have demonstrated great success in learning complex patterns that enable them to make predictions about unobserved data. In addition to using models for prediction, the ability to interpret what a model has learned is receiving an increasing amount of attention. These interpretations have found a number of uses, ranging from providing scientific insight to auditing the predictions themselves to ensure fairness with respect to protected categories like race or gender. However, there is still considerable confusion about the notion of interpretability. In particular, it is currently unclear both what it means to interpret a ML model, and how to actually do so.In the first part of this thesis, we address the foundational question of what it means to interpret a ML model. In particular, it is currently unclear what it means to be interpretable, and how to select, evaluate, or even discuss, methods for producing interpretations of machine-learning models. We aim to clarify these concerns by defining interpretable machine learning and constructing a unifying framework for existing methods which highlights the under-appreciated role played by human audiences. Within this framework, methods are organized into two classes: model-based and post-hoc. To provide guidance in selecting and evaluating interpretation methods, we introduce three desiderata: predictive accuracy, descriptive accuracy, and relevancy. Using our framework, we review existing work, grounded in real-world studies which exemplify our desiderata, and suggest directions for future work.The second through fourth parts introduce a succession of methods for interpreting predictions made by neural networks. The second part focuses on Long Short Term Memory networks (LSTMs) trained on question-answering and sentiment analysis, two popular tasks in natural language processing. By decomposing the LSTM's update equations, we introduce a novel method for computing feature importance scores of specific inputs for determining the output of an LSTM. In order to verify the output of our method, we use the introduced scores to search for consistently important patterns of words learned by state ofthe art LSTMs on sentiment analysis and question answering. This representation is then quantitatively validated by using the extracted phrases to construct a simple, rule-based classifier which approximates the output of the LSTM.While feature importance scores are helpful in understanding a model's predictions, they ignore the complex interactions between variables typically learned by neural networks. To this end, the third part introduces contextual decomposition (CD), an interpretation algorithm for analysing individual predictions made by standard LSTMs, without any changes to the underlying model. By decomposing the output of a LSTM, CD captures the contributions of combinations of words or variables to the final prediction of an LSTM. On the task of sentiment analysis with the Yelp and Stanford Sentiment Treebank (SST) data sets, we show that CD is able to reliably identify words and phrases of contrasting sentiment, and how they are combined to yield the LSTM's final prediction. Using the phrase-level labels in SST, we also demonstrate that CD is able to successfully extract positive and negative negations from an LSTM, something which has not previously been done.When considering interactions between variables, the number of interactions quickly becomes too large for manual inspection, leading to the question of how to automatically select and display an informative subset. In the fourth part, we introduce the use of hierarchical interpretations to explain DNN predictions through our proposed method: agglomerative contextual decomposition (ACD). Given a prediction from a trained DNN, ACD produces a hierarchical clustering of the input features, along with the contribution of each cluster to the final prediction. This hierarchy is optimized to identify clusters of features that the DNN learned are predictive. We introduce ACD using examples from Stanford Sentiment Treebank and ImageNet, in order to diagnose incorrect predictions, identify dataset bias, and extract polarizing phrases of varying lengths. Through human experiments, we demonstrate that ACD enables users both to identify the more accurate of two DNNs and to better trust a DNN's outputs. We also find that ACD's hierarchy is largely robust to adversarial perturbations, implying that it captures fundamental aspects of the input and ignores spurious noise.","",""
21,"Christopher Culley, S. Vijayakumar, Guido Zampieri, C. Angione","A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth",2020,"","","","",181,"2022-07-13 09:23:23","","10.1073/pnas.2002959117","","",,,,,21,10.50,5,4,2,"Significance Linking genotype and phenotype is a fundamental problem in biology, key to several biomedical and biotechnological applications. Cell growth is a central phenotypic trait, resulting from interactions between environment, gene regulation, and metabolism, yet its functional bases are still not completely understood. We propose and test a machine-learning approach that integrates large-scale gene expression profiles and mechanistic metabolic models, for characterizing cell growth and understanding its driving mechanisms in Saccharomyces cerevisiae. At its core, a custom-built multimodal learning method merges experimentally generated and model-generated data. We show that our approach can leverage the advantages of both machine learning and metabolic modeling, revealing unknown interactions between biological domains, incorporating mechanistic knowledge, and therefore overcoming black-box limitations of conventional data-driven approaches. Metabolic modeling and machine learning are key components in the emerging next generation of systems and synthetic biology tools, targeting the genotype–phenotype–environment relationship. Rather than being used in isolation, it is becoming clear that their value is maximized when they are combined. However, the potential of integrating these two frameworks for omic data augmentation and integration is largely unexplored. We propose, rigorously assess, and compare machine-learning–based data integration techniques, combining gene expression profiles with computationally generated metabolic flux data to predict yeast cell growth. To this end, we create strain-specific metabolic models for 1,143 Saccharomyces cerevisiae mutants and we test 27 machine-learning methods, incorporating state-of-the-art feature selection and multiview learning approaches. We propose a multiview neural network using fluxomic and transcriptomic data, showing that the former increases the predictive accuracy of the latter and reveals functional patterns that are not directly deducible from gene expression alone. We test the proposed neural network on a further 86 strains generated in a different experiment, therefore verifying its robustness to an additional independent dataset. Finally, we show that introducing mechanistic flux features improves the predictions also for knockout strains whose genes were not modeled in the metabolic reconstruction. Our results thus demonstrate that fusing experimental cues with in silico models, based on known biochemistry, can contribute with disjoint information toward biologically informed and interpretable machine learning. Overall, this study provides tools for understanding and manipulating complex phenotypes, increasing both the prediction accuracy and the extent of discernible mechanistic biological insights.","",""
19,"Alejandro Lopez-Rincon, Lucero Mendoza-Maldonado, M. Martínez-Archundia, A. Schönhuth, A. Kraneveld, J. Garssen, A. Tonda","Machine Learning-Based Ensemble Recursive Feature Selection of Circulating miRNAs for Cancer Tumor Classification",2020,"","","","",182,"2022-07-13 09:23:23","","10.3390/cancers12071785","","",,,,,19,9.50,3,7,2,"Circulating microRNAs (miRNA) are small noncoding RNA molecules that can be detected in bodily fluids without the need for major invasive procedures on patients. miRNAs have shown great promise as biomarkers for tumors to both assess their presence and to predict their type and subtype. Recently, thanks to the availability of miRNAs datasets, machine learning techniques have been successfully applied to tumor classification. The results, however, are difficult to assess and interpret by medical experts because the algorithms exploit information from thousands of miRNAs. In this work, we propose a novel technique that aims at reducing the necessary information to the smallest possible set of circulating miRNAs. The dimensionality reduction achieved reflects a very important first step in a potential, clinically actionable, circulating miRNA-based precision medicine pipeline. While it is currently under discussion whether this first step can be taken, we demonstrate here that it is possible to perform classification tasks by exploiting a recursive feature elimination procedure that integrates a heterogeneous ensemble of high-quality, state-of-the-art classifiers on circulating miRNAs. Heterogeneous ensembles can compensate inherent biases of classifiers by using different classification algorithms. Selecting features then further eliminates biases emerging from using data from different studies or batches, yielding more robust and reliable outcomes. The proposed approach is first tested on a tumor classification problem in order to separate 10 different types of cancer, with samples collected over 10 different clinical trials, and later is assessed on a cancer subtype classification task, with the aim to distinguish triple negative breast cancer from other subtypes of breast cancer. Overall, the presented methodology proves to be effective and compares favorably to other state-of-the-art feature selection methods.","",""
19,"P. Choudhury, Ryan T. Allen, Michael G. Endres","Machine Learning for Pattern Discovery in Management Research",2020,"","","","",183,"2022-07-13 09:23:23","","10.2139/ssrn.3518780","","",,,,,19,9.50,6,3,2,"Supervised machine learning (ML) methods are a powerful toolkit for discovering robust patterns in quantitative data. The patterns identified by ML could be used for exploratory inductive or abductive research, or for post-hoc analysis of regression results to detect patterns that may have gone unnoticed. However, ML models should not be treated as the result of a deductive causal test. To demonstrate the application of ML for pattern discovery, we implement ML algorithms to study employee turnover at a large technology company. We interpret the relationships between variables using partial dependence plots, which uncover surprising nonlinear and interdependent patterns between variables that may have gone unnoticed using traditional methods. To guide readers evaluating ML for pattern discovery, we provide guidance for evaluating model performance, highlight human decisions in the process, and warn of common misinterpretation pitfalls. An online appendix provides code and data to implement the algorithms demonstrated in the paper.","",""
5,"M. Ankenbrand, Liliia Shainberg, M. Hock, D. Lohr, L. Schreiber","Sensitivity analysis for interpretation of machine learning based segmentation models in cardiac MRI",2020,"","","","",184,"2022-07-13 09:23:23","","10.1186/s12880-021-00551-1","","",,,,,5,2.50,1,5,2,"","",""
4,"M. Valetich, C. Le Losq, R. Arculus, S. Umino, J. Mavrogenes","Compositions and Classification of Fractionated Boninite Series Melts from the Izu–Bonin–Mariana Arc: A Machine Learning Approach",2021,"","","","",185,"2022-07-13 09:23:23","","10.1093/PETROLOGY/EGAB013","","",,,,,4,4.00,1,5,1,"  Much of the boninite magmatism in the Izu–Bonin–Mariana arc is preserved as evolved boninite series compositions wherein extensive fractional crystallization of pyroxene and spinel have obscured the diagnostic geochemical indicators of boninite parentage, such as high Mg and low Ti at intermediate silica contents. As a result, the usual geochemical discriminants used for the classification of the broad range of parental boninites are inapplicable to such highly fractionated melts. These issues are compounded by the mixing of demonstrably different whole-rock and glass analyses in classification schemes and petrological interpretations based thereon. Whole-rock compositions are compromised by entrainment of variable proportions of crystalline phases resulting in inconsistent differences from corresponding in situ glass analyses, which arguably better reflect prior melt compositions. To circumvent such issues, we herein present a robust method for the classification of highly fractionated boninite series glasses. This new classification leverages the analysis of trace elements, which are much more sensitive to evolutionary processes than major elements, and benefits from the use of unsupervised machine learning as a classification tool. The results show that the most fractionated boninite series melts preserve geochemical indicators of their parentage, and highlight the pitfalls of interpreting whole-rock and glass analyses interchangeably.","",""
17,"Yong'ai Li, Junming Jian, P. Pickhardt, F. Ma, W. Xia, Hai-ming Li, Rui Zhang, S. Zhao, S. Cai, Xingyu Zhao, Jiayi Zhang, Guofu Zhang, Jingxuan Jiang, Yan Zhang, Keying Wang, G. Lin, F. Feng, Jingjing Lu, Lin Deng, Xiaodong Wu, J. Qiang, Xin Gao","MRI-Based Machine Learning for Differentiating Borderline From Malignant Epithelial Ovarian Tumors: A Multicenter Study.",2020,"","","","",186,"2022-07-13 09:23:23","","10.1002/jmri.27084","","",,,,,17,8.50,2,22,2,"BACKGROUND Preoperative differentiation of borderline from malignant epithelial ovarian tumors (BEOT from MEOT) can impact surgical management. MRI has improved this assessment but subjective interpretation by radiologists may lead to inconsistent results.   PURPOSE To develop and validate an objective MRI-based machine-learning (ML) assessment model for differentiating BEOT from MEOT, and compare the performance against radiologists' interpretation.   STUDY TYPE Retrospective study of eight clinical centers.   POPULATION In all, 501 women with histopathologically-confirmed BEOT (n = 165) or MEOT (n = 336) from 2010 to 2018 were enrolled. Three cohorts were constructed: a training cohort (n = 250), an internal validation cohort (n = 92), and an external validation cohort (n = 159).   FIELD STRENGTH/SEQUENCE Preoperative MRI within 2 weeks of surgery. Single- and multiparameter (MP) machine-learning assessment models were built utilizing the following four MRI sequences: T2 -weighted imaging (T2 WI), fat saturation (FS), diffusion-weighted imaging (DWI), apparent diffusion coefficient (ADC), and contrast-enhanced (CE)-T1 WI.   ASSESSMENT Diagnostic performance of the models was assessed for both whole tumor (WT) and solid tumor (ST) components. Assessment of the performance of the model in discriminating BEOT vs. early-stage MEOT was made. Six radiologists of varying experience also interpreted the MR images.   STATISTICAL TESTS Mann-Whitney U-test: significance of the clinical characteristics; chi-square test: difference of label; DeLong test: difference of receiver operating characteristic (ROC).   RESULTS The MP-ST model performed better than the MP-WT model for both the internal validation cohort (area under the curve [AUC] = 0.932 vs. 0.917) and external validation cohort (AUC = 0.902 vs. 0.767). The model showed capability in discriminating BEOT vs. early-stage MEOT, with AUCs of 0.909 and 0.920, respectively. Radiologist performance was considerably poorer than both the internal (mean AUC = 0.792; range, 0.679-0.924) and external (mean AUC = 0.797; range, 0.744-0.867) validation cohorts.   DATA CONCLUSION Performance of the MRI-based ML model was robust and superior to subjective assessment of radiologists. If our approach can be implemented in clinical practice, improved preoperative prediction could potentially lead to preserved ovarian function and fertility for some women.   LEVEL OF EVIDENCE Level 4.   TECHNICAL EFFICACY Stage 2.","",""
1,"T. Thung, Murray E. White, Wei Dai, J. Wilksch, R. Bamert, A. Rocker, C. Stubenrauch, Daniel Williams, Cheng Huang, Ralf Schittelhelm, J. Barr, E. Jameson, S. McGowan, Yanju Zhang, Jiawei Wang, R. Dunstan, T. Lithgow","The component parts of bacteriophage virions accurately defined by a machine-learning approach built on evolutionary features",2021,"","","","",187,"2022-07-13 09:23:23","","10.1101/2021.02.28.433281","","",,,,,1,1.00,0,17,1,"Antimicrobial resistance (AMR) continues to evolve as a major threat to human health and new strategies are required for the treatment of AMR infections. Bacteriophages (phages) that kill bacterial pathogens are being identified for use in phage therapies, with the intention to apply these bactericidal viruses directly into the infection sites in bespoke phage cocktails. Despite the great unsampled phage diversity for this purpose, an issue hampering the roll out of phage therapy is the poor quality annotation of many of the phage genomes, particularly for those from infrequently sampled environmental sources. We developed a computational tool called STEP3 to use the “evolutionary features” that can be recognized in genome sequences of diverse phages. These features, when integrated into an ensemble framework, achieved a stable and robust prediction performance when benchmarked against other prediction tools using phages from diverse sources. Validation of the prediction accuracy of STEP3 was conducted with high-resolution mass spectrometry analysis of two novel phages, isolated from a watercourse in the Southern Hemisphere. STEP3 provides a robust computational approach to distinguish specific and universal features in phages to improve the quality of phage cocktails, and is available for use at http://step3.erc.monash.edu/. IMPORTANCE In response to the global problem of antimicrobial resistance there are moves to use bacteriophages (phages) as therapeutic agents. Selecting which phages will be effective therapeutics relies on interpreting features contributing to shelf-life and applicability to diagnosed infections. However, the protein components of the phage virions that dictate these properties vary so much in sequence that best estimates suggest failure to recognize up to 90% of them. We have utilised this diversity in evolutionary features as an advantage, to apply machine learning for prediction accuracy for diverse components in phage virions. We benchmark this new tool showing the accurate recognition and evaluation of phage components parts using genome sequence data of phages from under-sampled environments, where the richest diversity of phage still lies.","",""
2,"T. Thung, Murray E. White, Wei Dai, J. Wilksch, R. Bamert, A. Rocker, C. Stubenrauch, Daniel Williams, Cheng Huang, Ralf Schittelhelm, J. Barr, E. Jameson, S. McGowan, Yanju Zhang, Jiawei Wang, R. Dunstan, T. Lithgow","Component Parts of Bacteriophage Virions Accurately Defined by a Machine-Learning Approach Built on Evolutionary Features",2021,"","","","",188,"2022-07-13 09:23:23","","10.1128/mSystems.00242-21","","",,,,,2,2.00,0,17,1,"In response to the global problem of antimicrobial resistance, there are moves to use bacteriophages (phages) as therapeutic agents. Selecting which phages will be effective therapeutics relies on interpreting features contributing to shelf-life and applicability to diagnosed infections. ABSTRACT Antimicrobial resistance (AMR) continues to evolve as a major threat to human health, and new strategies are required for the treatment of AMR infections. Bacteriophages (phages) that kill bacterial pathogens are being identified for use in phage therapies, with the intention to apply these bactericidal viruses directly into the infection sites in bespoke phage cocktails. Despite the great unsampled phage diversity for this purpose, an issue hampering the roll out of phage therapy is the poor quality annotation of many of the phage genomes, particularly for those from infrequently sampled environmental sources. We developed a computational tool called STEP3 to use the “evolutionary features” that can be recognized in genome sequences of diverse phages. These features, when integrated into an ensemble framework, achieved a stable and robust prediction performance when benchmarked against other prediction tools using phages from diverse sources. Validation of the prediction accuracy of STEP3 was conducted with high-resolution mass spectrometry analysis of two novel phages, isolated from a watercourse in the Southern Hemisphere. STEP3 provides a robust computational approach to distinguish specific and universal features in phages to improve the quality of phage cocktails and is available for use at http://step3.erc.monash.edu/. IMPORTANCE In response to the global problem of antimicrobial resistance, there are moves to use bacteriophages (phages) as therapeutic agents. Selecting which phages will be effective therapeutics relies on interpreting features contributing to shelf-life and applicability to diagnosed infections. However, the protein components of the phage virions that dictate these properties vary so much in sequence that best estimates suggest failure to recognize up to 90% of them. We have utilized this diversity in evolutionary features as an advantage, to apply machine learning for prediction accuracy for diverse components in phage virions. We benchmark this new tool showing the accurate recognition and evaluation of phage component parts using genome sequence data of phages from undersampled environments, where the richest diversity of phage still lies.","",""
2,"I. M. Lei, Chen Jiang, Chon Lok Lei, S. D. de Rijk, Y. C. Tam, C. Swords, M. Sutcliffe, G. Malliaras, M. Bance, Yan Yan Shery Huang","3D printed biomimetic cochleae and machine learning co-modelling provides clinical informatics for cochlear implant patients",2021,"","","","",189,"2022-07-13 09:23:23","","10.1038/s41467-021-26491-6","","",,,,,2,2.00,0,10,1,"","",""
1,"T. Welchowski, K. Maloney, R. Mitchell, M. Schmid","Techniques to Improve Ecological Interpretability of Black-Box Machine Learning Models",2021,"","","","",190,"2022-07-13 09:23:23","","10.1007/s13253-021-00479-7","","",,,,,1,1.00,0,4,1,"","",""
15,"Niraj Thapa, Zhipeng Liu, KC DukkaB., B. Gokaraju, Kaushik Roy","Comparison of Machine Learning and Deep Learning Models for Network Intrusion Detection Systems",2020,"","","","",191,"2022-07-13 09:23:23","","10.3390/FI12100167","","",,,,,15,7.50,3,5,2,"The development of robust anomaly-based network detection systems, which are preferred over static signal-based network intrusion, is vital for cybersecurity. The development of a flexible and dynamic security system is required to tackle the new attacks. Current intrusion detection systems (IDSs) suffer to attain both the high detection rate and low false alarm rate. To address this issue, in this paper, we propose an IDS using different machine learning (ML) and deep learning (DL) models. This paper presents a comparative analysis of different ML models and DL models on Coburg intrusion detection datasets (CIDDSs). First, we compare different ML- and DL-based models on the CIDDS dataset. Second, we propose an ensemble model that combines the best ML and DL models to achieve high-performance metrics. Finally, we benchmarked our best models with the CIC-IDS2017 dataset and compared them with state-of-the-art models. While the popular IDS datasets like KDD99 and NSL-KDD fail to represent the recent attacks and suffer from network biases, CIDDS, used in this research, encompasses labeled flow-based data in a simulated office environment with both updated attacks and normal usage. Furthermore, both accuracy and interpretability must be considered while implementing AI models. Both ML and DL models achieved an accuracy of 99% on the CIDDS dataset with a high detection rate, low false alarm rate, and relatively low training costs. Feature importance was also studied using the Classification and regression tree (CART) model. Our models performed well in 10-fold cross-validation and independent testing. CART and convolutional neural network (CNN) with embedding achieved slightly better performance on the CIC-IDS2017 dataset compared to previous models. Together, these results suggest that both ML and DL methods are robust and complementary techniques as an effective network intrusion detection system.","",""
0,"K. Tokarska, S. Sippel, R. Knutti","Towards data-driven estimates of the transient climate response to cumulative CO2 emissions using interpretable statistical learning methods",2021,"","","","",192,"2022-07-13 09:23:23","","10.5194/egusphere-egu21-2451","","",,,,,0,0.00,0,3,1,"<div> <div> <p>CO<sub>2</sub>-induced warming is approximately proportional to the total amount of CO<sub>2</sub> emitted. This emergent property of the climate system, known as the Transient Climate Response to cumulative CO<sub>2</sub> Emissions (TCRE), gave rise to the concept of a remaining carbon budget that specifies a cap on global CO<sub>2</sub> emissions in line with reaching a given temperature target, such as those in the Paris Agreement (e.g., Matthews et al. 2020). However, estimating the policy-relevant TCRE metric directly from the observation-based data products remains challenging due to non-CO<sub>2</sub> forcing and land-use change emissions present in the real-world climate conditions.</p> <p>Here, we present preliminary results for applying and comparing different statistical learning methods to determine TCRE (and later, remaining carbon budgets) from: (i) climate models&#8217; output and (ii) the observational data products. First, we make use of a &#8216;perfect-model&#8217; setting, i.e. using output from physics-based climate models (CMIP5 and CMIP6) under historical forcing (treated as pseudo-observations). This output is used to train different statistical-learning models, and to make predictions of TCRE (which are known from climate model simulations under CO<sub>2</sub>-only forcing, per experimental design). Next, we use such trained statistical learning models to make TCRE predictions directly from the observation-based data products.</p> <p>We also explore interpretability of the applied techniques, to determine where the statistical models are learning from, what the regions of importance are, and the key input features and weights. Explainable AI methods (e.g., McGovern et al. 2019; Molnar 2019; Samek et al. 2019) present a promising way forward in linking data-driven statistical and machine learning methods with traditional physical climate sciences, while leveraging from the large amount of data from the observational data products to provide more robust estimates of, often policy relevant, climate metrics.</p> <p>References:</p> <p>Matthews et al. (2020). Opportunities and challenges in using carbon budgets to guide climate policy. Nature Geoscience, 13, 769&#8211;779. https://doi.org/10.1038/s41561-020-00663-3</p> <p>McGovern et al. (2019). Making the Black Box More Transparent: Understanding the Physical Implications of Machine Learning, B. Am. Meteorol. Soc., 100, 2175&#8211;2199, https://doi.org/10.1175/BAMS-D-18-0195.1</p> <p>Molnar, C. (2019) Interpretable Machine Learning -A Guide for Making Black Box Models Explainable. https://christophm.github.io/interpretable-ml-book/</p> <p>Samek, W. et al. (2019) Explainable AI: Interpreting, explaining and visualizing deep learning. https://doi.org/10.1007/978-3-030-28954-6</p></div></div>","",""
12,"C. Fisher, H. J. Hoeijmakers, D. Kitzmann, Pablo M'arquez-Neila, S. Grimm, R. Sznitman, K. Heng","Interpreting High-resolution Spectroscopy of Exoplanets using Cross-correlations and Supervised Machine Learning",2019,"","","","",193,"2022-07-13 09:23:23","","10.3847/1538-3881/ab7a92","","",,,,,12,4.00,2,7,3,"We present a new method for performing atmospheric retrieval on ground-based, high-resolution data of exoplanets. Our method combines cross-correlation functions with a random forest, a supervised machine learning technique, to overcome challenges associated with high-resolution data. A series of cross-correlation functions are concatenated to give a ""CCF-sequence"" for each model atmosphere, which reduces the dimensionality by a factor of ~100. The random forest, trained on our grid of ~65,000 models, provides a likelihood-free method of retrieval. The pre-computed grid spans 31 values of both temperature and metallicity, and incorporates a realistic noise model. We apply our method to HARPS-N observations of the ultra-hot Jupiter KELT-9b, and obtain a metallicity consistent with solar (logM = $-0.2\pm0.2$). Our retrieved transit chord temperature (T = $6000^{+0}_{-200}$K) is unreliable as the ion cross-correlations lie outside of the training set, which we interpret as being indicative of missing physics in our atmospheric model. We compare our method to traditional nested-sampling, as well as other machine learning techniques, such as Bayesian neural networks. We demonstrate that the likelihood-free aspect of the random forest makes it more robust than nested-sampling to different error distributions, and that the Bayesian neural network we tested is unable to reproduce complex posteriors. We also address the claim in Cobb et al. (2019) that our random forest retrieval technique can be over-confident but incorrect. We show that this is an artefact of the training set, rather than the machine learning method, and that the posteriors agree with those obtained using nested-sampling.","",""
14,"Steve Agajanian, O. Odeyemi, N. Bischoff, S. Ratra, Gennady M Verkhivker","Machine Learning Classification and Structure-Functional Analysis of Cancer Mutations Reveal Unique Dynamic and Network Signatures of Driver Sites in Oncogenes and Tumor Suppressor Genes",2018,"","","","",194,"2022-07-13 09:23:23","","10.1021/acs.jcim.8b00414","","",,,,,14,3.50,3,5,4,"In this study, we developed two cancer-specific machine learning classifiers for prediction of driver mutations in cancer-associated genes that were validated on canonical data sets of functionally validated mutations and applied to a large cancer genomics data set. By examining sequence, structure, and ensemble-based integrated features, we have shown that evolutionary conservation scores play a critical role in classification of cancer drivers and provide the strongest signal in the machine learning prediction. Through extensive comparative analysis with structure-functional experiments and multicenter mutational calling data from Pan Cancer Atlas studies, we have demonstrated the robustness of our models and addressed the validity of computational predictions. To address the interpretability of cancer-specific classification models and obtain novel insights about molecular signatures of driver mutations, we have complemented machine learning predictions with structure-functional analysis of cancer driver mutations in several important oncogenes and tumor suppressor genes. By examining structural and dynamic signatures of known mutational hotspots and the predicted driver mutations, we have shown that the greater flexibility of specific functional regions targeted by driver mutations in oncogenes may facilitate activating conformational changes, while loss-of-function driver mutations in tumor suppressor genes can preferentially target structurally rigid positions that mediate allosteric communications in residue interaction networks and modulate protein binding interfaces. By revealing molecular signatures of cancer driver mutations, our results highlighted limitations of the binary driver/passenger classification, suggesting that functionally relevant cancer mutations may span a continuum spectrum of driverlike effects. Based on this analysis, we propose for experimental testing a group of novel potential driver mutations that can act by altering structure, global dynamics, and allosteric interaction networks in important cancer genes.","",""
10,"T. Mizoguchi, S. Kiyohara","Machine learning approaches for ELNES/XANES.",2020,"","","","",195,"2022-07-13 09:23:23","","10.1093/jmicro/dfz109","","",,,,,10,5.00,5,2,2,"Materials characterization is indispensable for materials development. In particular, spectroscopy provides atomic configuration, chemical bonding and vibrational information, which are crucial for understanding the mechanism underlying the functions of a material. Despite its importance, the interpretation of spectra using human-driven methods, such as manual comparison of experimental spectra with reference/simulated spectra, is becoming difficult owing to the rapid increase in experimental spectral data. To overcome the limitations of such methods, we develop new data-driven approaches based on machine learning. Specifically, we use hierarchical clustering, a decision tree and a feedforward neural network to investigate the electron energy loss near edge structures (ELNES) spectrum, which is identical to the X-ray absorption near edge structure (XANES) spectrum. Hierarchical clustering and the decision tree are used to interpret and predict ELNES/XANES, while the feedforward neural network is used to obtain hidden information about the material structure and properties from the spectra. Further, we construct a prediction model that is robust against noise by data augmentation. Finally, we apply our method to noisy spectra and predict six properties accurately. In summary, the proposed approaches can pave the way for fast and accurate spectrum interpretation/prediction as well as local measurement of material functions.","",""
297,"Andrius Vabalas, E. Gowen, E. Poliakoff, A. Casson","Machine learning algorithm validation with a limited sample size",2019,"","","","",196,"2022-07-13 09:23:23","","10.1371/journal.pone.0224365","","",,,,,297,99.00,74,4,3,"Advances in neuroimaging, genomic, motion tracking, eye-tracking and many other technology-based data collection methods have led to a torrent of high dimensional datasets, which commonly have a small number of samples because of the intrinsic high cost of data collection involving human participants. High dimensional data with a small number of samples is of critical importance for identifying biomarkers and conducting feasibility and pilot work, however it can lead to biased machine learning (ML) performance estimates. Our review of studies which have applied ML to predict autistic from non-autistic individuals showed that small sample size is associated with higher reported classification accuracy. Thus, we have investigated whether this bias could be caused by the use of validation methods which do not sufficiently control overfitting. Our simulations show that K-fold Cross-Validation (CV) produces strongly biased performance estimates with small sample sizes, and the bias is still evident with sample size of 1000. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. We also show that feature selection if performed on pooled training and testing data is contributing to bias considerably more than parameter tuning. In addition, the contribution to bias by data dimensionality, hyper-parameter space and number of CV folds was explored, and validation methods were compared with discriminable data. The results suggest how to design robust testing methodologies when working with small datasets and how to interpret the results of other studies based on what validation method was used.","",""
439,"Marco Tulio Ribeiro, Sameer Singh, Carlos Guestrin","Model-Agnostic Interpretability of Machine Learning",2016,"","","","",197,"2022-07-13 09:23:23","","","","",,,,,439,73.17,146,3,6,"Understanding why machine learning models behave the way they do empowers both system designers and end-users in many ways: in model selection, feature engineering, in order to trust and act upon the predictions, and in more intuitive user interfaces. Thus, interpretability has become a vital concern in machine learning, and work in the area of interpretable models has found renewed interest. In some applications, such models are as accurate as non-interpretable ones, and thus are preferred for their transparency. Even when they are not accurate, they may still be preferred when interpretability is of paramount importance. However, restricting machine learning to interpretable models is often a severe limitation. In this paper we argue for explaining machine learning predictions using model-agnostic approaches. By treating the machine learning models as black-box functions, these approaches provide crucial flexibility in the choice of models, explanations, and representations, improving debugging, comparison, and interfaces for a variety of users and models. We also outline the main challenges for such methods, and review a recently-introduced model-agnostic explanation approach (LIME) that addresses these challenges.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",198,"2022-07-13 09:23:23","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
5,"M. Paggi","An Analysis of the Italian Lockdown in Retrospective Using Particle Swarm Optimization in Machine Learning Applied to an Epidemiological Model",2020,"","","","",199,"2022-07-13 09:23:23","","10.3390/physics2030020","","",,,,,5,2.50,5,1,2,"A critical analysis of the open data provided by the Italian Civil Protection Centre during phase 1 of Covid-19 epidemic—the so-called Italian lockdown—is herein proposed in relation to four of the most affected Italian regions, namely Lombardy, Reggio Emilia, Valle d’Aosta, and Veneto. A possible bias in the data induced by the extent in the use of medical swabs is found in relation to Valle d’Aosta and Veneto. Observed data are then interpreted using a Susceptible-Infectious-Recovered (SIR) epidemiological model enhanced with asymptomatic (infected and recovered) compartments, including lockdown effects through time-dependent model parameters. The initial number of susceptible individuals for each region is also considered as a parameter to be identified. The issue of parameters identification is herein addressed by a robust machine learning approach based on particle swarm optimization. Model predictions provide relevant information for policymakers in terms of the effect of lockdown measures in the different regions. The number of susceptible individuals involved in the epidemic, important for a safe release of lockdown during the next phases, is predicted to be around 10% of the population for Lombardy, 16% for Reggio Emilia, 18% for Veneto, and 40% for Valle d’Aosta.","",""
5,"T. Kootstra, J. Teuwen, J. Goudsmit, T. Nijboer, Michael D. Dodd, S. van der Stigchel","Machine learning-based classification of viewing behavior using a wide range of statistical oculomotor features",2020,"","","","",200,"2022-07-13 09:23:23","","10.1167/jov.20.9.1","","",,,,,5,2.50,1,6,2,"Since the seminal work of Yarbus, multiple studies have demonstrated the influence of task-set on oculomotor behavior and the current cognitive state. In more recent years, this field of research has expanded by evaluating the costs of abruptly switching between such different tasks. At the same time, the field of classifying oculomotor behavior has been moving toward more advanced, data-driven methods of decoding data. For the current study, we used a large dataset compiled over multiple experiments and implemented separate state-of-the-art machine learning methods for decoding both cognitive state and task-switching. We found that, by extracting a wide range of oculomotor features, we were able to implement robust classifier models for decoding both cognitive state and task-switching. Our decoding performance highlights the feasibility of this approach, even invariant of image statistics. Additionally, we present a feature ranking for both models, indicating the relative magnitude of different oculomotor features for both classifiers. These rankings indicate a separate set of important predictors for decoding each task, respectively. Finally, we discuss the implications of the current approach related to interpreting the decoding results.","",""
