Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
1,"Fang Zhang, Xiaochen Wang, J. Han, Jie Tang, Shiyin Wang","Fast Top-k Area Topics Extraction with Knowledge Base",2017,"","","","",1,"2022-07-13 09:22:00","","10.1109/DSC.2018.00016","","",,,,,1,0.20,0,5,5,"What are the most representative research topics in Artificial Intelligence (AI)? We formulate the problem as extracting top-k topics that can best represent a given area with the help of knowledge base. We theoretically prove that the problem is NP-hard and propose an optimization model, FastKATE, to address this problem by combining both explicit and latent representations for each topic. We leverage a large-scale knowledge base (Wikipedia) to generate topic embeddings using neural networks and use this kind of representations to help capture the representativeness of topics for given areas. We develop a fast heuristic algorithm to efficiently solve the problem with a provable error bound. We evaluate the proposed model on three real-world datasets. Experimental results demonstrate our model's effectiveness, robustness, real-timeness (return results in <1s), and its superiority over several alternative methods.","",""
0,"Fang Zhang, Xiaochen Wang, J. Han, Jie Tang, Shiyin Wang","Fast Top-$\boldsymbol{k}$ Area Topics Extraction with Knowledge Base",2017,"","","","",2,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,5,5,"What are the most popular research topics in Artificial Intelligence (AI)? We formulate the problem as extracting top-$k$ topics that can best represent a given area with the help of knowledge base. We theoretically prove that the problem is NP-hard and propose an optimization model, FastKATE, to address this problem by combining both explicit and latent representations for each topic. We leverage a large-scale knowledge base (Wikipedia) to generate topic embeddings using neural networks and use this kind of representations to help capture the representativeness of topics for given areas. We develop a fast heuristic algorithm to efficiently solve the problem with a provable error bound. We evaluate the proposed model on three real-world datasets. Experimental results demonstrate our model's effectiveness, robustness, real-timeness (return results in $<1$s), and its superiority over several alternative methods.","",""
110,"C. Kulikowski","Artificial intelligence methods and systems for medical consultation",1980,"","","","",3,"2022-07-13 09:22:00","","10.1109/TPAMI.1980.6592368","","",,,,,110,2.62,110,1,42,"The major AI problems that arise in designing a consultation program involve choices of knowledge representations, diagnostic interpretation strategies, and treatment planning strategies. The need to justify decisions and update the knowledge base in the light of new research findings places a premium on the modularity of a representation and the ease with which its reasoning procedures can be explained. In both diagnosis and treatment decisions, the relative advantages and disadvantages of different schemes for quantifying the uncertainty of inferences raises difficult issues of a formal logical nature, as well as many specific practical problems of system design. An important insight that has resulted from the design of several artificial intelligence systems is that robustness of performance in the presence of many uncertainty relationships can be achieved by eliciting from the expert a segmentation of knowledge that will also provide a rich network of deterministic relationships to interweave the space of hypotheses.","",""
1,"S. Sugiyama","Self evolving dynamic knowledge base",1998,"","","","",4,"2022-07-13 09:22:00","","10.1109/ICSMC.1998.728187","","",,,,,1,0.04,1,1,24,"There have been a lot of studies on artificial intelligence. But still we have a lot of problems left, like analysis of the environment for solving problems, low robustness, lack of information on a target, less matured computer languages, and so forth. Which means that a lot of human interactions are still necessary in order to get proper answers for given problems. Studies on how to reduce the human interactions are performed by using backpropagation neural networks with the idea of dynamic knowledge base which has an ability to evolve itself in order to get the most appropriate knowledge base and performance mechanisms for solving problems given. And as a result, it has been proved that it is possible to have a self evolving dynamic knowledge base which reduces a lot of human interactions which used to need.","",""
0,"Dr. Roberto Passailaigue Baquerizo, Vivian Estrada Sentí, Dr. Juan P. Febles Rodríguez","Evaluation of the socialization of knowledge and collaboration in educational management",2017,"","","","",5,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,3,5,"1Ph.D, Canciller Universidad Tecnológica ECOTEC, Ecuador 2 Ph.D, Universidad de las Ciencias Informáticas, Metodóloga de posgrado, La Habana, Cuba 3 Ph.D, Universidad de las Ciencias Informáticas, Metodólogo de posgrado, La Habana, Cuba ---------------------------------------------------------------------***--------------------------------------------------------------------Abstract The authors, using qualitative research methods, the Case-Based Reasoning as a paradigm of Artificial Intelligence and based on various data and opinions obtained from research on educational networks developed previously achieved knowledge representation for the status of socialization and collaboration in the process of educational management of selected educational institutions. The base case was tested with various situations and the results were 100% correct (in the test cases were used whose answers were known). The result intersects technological aspects associated with conclusions and recommendations to educational management, which were timely validated in previous processes and ensures the robustness of the proposal that article is performed.","",""
53,"R. Alcalá, J. Alcalá-Fdez, María José Gacto, F. Herrera","Improving fuzzy logic controllers obtained by experts: a case study in HVAC systems",2009,"","","","",6,"2022-07-13 09:22:00","","10.1007/s10489-007-0107-6","","",,,,,53,4.08,13,4,13,"","",""
0,"Stefan Zwicklbauer","Robust Entity Linking in Heterogeneous Domains",2017,"","","","",7,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,1,5,"Entity Linking is the task of mapping terms in arbitrary documents to entities in a knowledge base by identifying the correct semantic meaning. It is applied in the extraction of structured data in RDF (Resource Description Framework) from textual documents, but equally so in facilitating artificial intelligence applications, such as Semantic Search, Reasoning and Question and Answering. Most existing Entity Linking systems were optimized for specific domains (e.g., general domain, biomedical domain), knowledge base types (e.g., DBpedia, Wikipedia), or document structures (e.g., tables) and types (e.g., news articles, tweets). This led to very specialized systems that lack robustness and are only applicable for very specific tasks. In this regard, this work focuses on the research and development of a robust Entity Linking system in terms of domains, knowledge base types, and document structures and types.    To create a robust Entity Linking system, we first analyze the following three crucial components of an Entity Linking algorithm in terms of robustness criteria: (i) the underlying knowledge base, (ii) the entity relatedness measure, and (iii) the textual context matching technique. Based on the analyzed components, our scientific contributions are three-fold. First, we show that a federated approach leveraging knowledge from various knowledge base types can significantly improve robustness in Entity Linking systems. Second, we propose a new state-of-the-art, robust entity relatedness measure for topical coherence computation based on semantic entity embeddings. Third, we present the neural-network-based approach Doc2Vec as a textual context matching technique for robust Entity Linking.    Based on our previous findings and outcomes, our main contribution in this work is DoSeR (Disambiguation of Semantic Resources). DoSeR is a robust, knowledge-base-agnostic Entity Linking framework that extracts relevant entity information from multiple knowledge bases in a fully automatic way. The integrated algorithm represents a collective, graph-based approach that utilizes semantic entity and document embeddings for entity relatedness and textual context matching computation. Our evaluation shows, that DoSeR achieves state-of-the-art results over a wide range of different document structures (e.g., tables), document types (e.g., news documents) and domains (e.g., general domain, biomedical domain). In this context, DoSeR outperforms all other (publicly available) Entity Linking algorithms on most data sets.","",""
1,"M. Hagiwara, Y. Anzai","Connectionist Model Data Base System with a Template for Association",1992,"","","","",8,"2022-07-13 09:22:00","","10.1541/IEEJEISS1987.112.3_165","","",,,,,1,0.03,1,2,30,"Combination of Artificial Intelligence (AI) and Connectionist model is very effective to construct an intelligent information processing system. There are several studies based on such a concept. An example, a knowledge base system based on connectionist model has the following features. (1) Robustness : even when there are some errors in a user's question or in a knowledge base, a near right answer can be obtained. (2) Context dependency : retrieval with prediction is possible. (3) Easy parallel retrieval for multiple concepts. (4) Easy maintenance of knowledge base : facts are expressed by symbols. In spite of the above advantages, the conventional system has a great shortcoming : it can only make inference of facts. This paper proposes a connectionist model data base system with a template for association. The proposed system has two features : inference is possible when data is insufficient, and new knowl edge can be generated by inference. The proposed system uses a template to create a network for associative reasoning, and it can be done by interactive activation and competition process. Computer simulation result indicates the effectiveness of the proposed system.","",""
3,"M. Geetha, J. Jerome","Fuzzy expert system based sensor and actuator fault diagnosis for continuous stirred tank reactor",2013,"","","","",9,"2022-07-13 09:22:00","","10.1109/IFUZZY.2013.6825445","","",,,,,3,0.33,2,2,9,"An approach on the degree in artificial intelligence applications to model-based diagnosis for dynamic nonlinear CSTR processes is proposed. Stress is placed on residual generation and residual evaluation employing fuzzy logic. Especially for residual generation, a novel Extended Kalman Filter concept, on knowledge Estimator, is introduced. An intelligent fuzzy approach for residual generation and evaluation is also defined. A fuzzy-reasoning approach is proposed to guarantee robustness to the inherent doubtfulness in the identified trends and to provide compact mapping. A two-staged strategy is employed: (i) identifying the most likely fault candidates based on a resemblance measure between the observed trends and the event-signatures in the cognition-base and, (ii) estimation of the fault magnitude. The fuzzy-cognition-base consists of a set of physically explainable if/then rules providing physical insight into the process. This technique provides multivariate differencing and is transparent for fault detection.","",""
0,"Guo Chao","Discussion of the Software Testing Programs of Foresty Expert System",2007,"","","","",10,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,1,15,"In view of the designing features of China's forestry expert system and the problems that exist in its research,this paper discusses several issues about the standardized software testing system.It makes an analytical study of the application of software testing technology,and of the important role this technology plays in designing the forestry expert system from the database testing,knowledge base testing,artificial intelligence field testing,maintaining,Robustness testing and other aspects,putting forward a new software testing scheme such as the software of forestry expert system.","",""
6,"D. Patterson, M. Galushka, N. Rooney","Characterisation of a Novel Indexing Technique for Case-Based Reasoning",2005,"","","","",11,"2022-07-13 09:22:00","","10.1007/s10462-004-7188-y","","",,,,,6,0.35,2,3,17,"","",""
0,"Beilei Wang, Jie Jing, Xiaochun Huang, Cheng Hua, Qin Qin, Y. Jia, Zhiyong Wang, Lei Jiang, Bai Gao, Les J. Wu, Xianfei Zeng, Fubo Wang, Chuanbin Mao, Shanrong Liu","Establishment of a Knowledge‐and‐Data‐Driven Artificial Intelligence System with Robustness and Interpretability in Laboratory Medicine",2022,"","","","",12,"2022-07-13 09:22:00","","10.1002/aisy.202100204","","",,,,,0,0.00,0,14,1,"Laboratory medicine plays an important role in clinical diagnosis. However, no laboratory‐based artificial intelligence (AI) diagnostic system has been applied in current clinical practice due to the lack of robustness and interpretability. Although many attempts have been made, it is still difficult for doctors to adopt the existing machine learning (ML) patterns in interpreting laboratory (lab) big data. Here, a knowledge‐and‐data‐driven laboratory diagnostic system is developed, termed AI‐based Lab tEst tO diagNosis (AI LEON), by integrating an innovative knowledge graph analysis framework and “mixed XGboost and Genetic Algorithm (MiXG)” technique to simulate the doctor's laboratory‐based diagnosis. To establish AI LEON, we included 89 116 949 laboratory data and 10 423 581 diagnosis data points from 730 113 participants. Among them, 686 626 participants were recruited for training and validating purposes with the remaining for testing purposes. AI LEON automatically identified and analyzed 2071 lab indexes, resulting in multiple disease recommendations that involved 441 common diseases in ten organ systems. AI LEON exhibited outstanding transparency and interpretability in three universal clinical application scenarios and outperformed human physicians in interpreting lab reports. AI LEON is an advanced intelligent system that enables a comprehensive interpretation of lab big data, which substantially improves the clinical diagnosis.","",""
11,"A. Massaro, A. Calicchio, Vincenzo Maritati, A. Galiano, Vitangelo Birardi, L. Pellicani, Maria Gutierrez Millan, Barbara Dalla Tezza, Mauro Bianchi, Guido Vertua, Antonello Puggioni","A Case Study of Innovation of an Information Communication System and Upgrade of the Knowledge Base in Industry by ESB, Artificial Intelligence, and Big Data System Integration",2018,"","","","",13,"2022-07-13 09:22:00","","10.5121/IJAIA.2018.9503","","",,,,,11,2.75,1,11,4,"In this paper, a case study is analyzed. This case study is about an upgrade of an industry communication system developed by following Frascati research guidelines. The knowledge Base (KB) of the industry is gained by means of different tools that are able to provide data and information having different formats and structures into an unique bus system connected to a Big Data. The initial part of the research is focused on the implementation of strategic tools, which can able to upgrade the KB. The second part of the proposed study is related to the implementation of innovative algorithms based on a KNIME (Konstanz Information Miner) Gradient Boosted Trees workflow processing data of the communication system which travel into an Enterprise Service Bus (ESB) infrastructure. The goal of the paper is to prove that all the new KB collected into a Cassandra big data system could be processed through the ESB by predictive algorithms solving possible conflicts between hardware and software. The conflicts are due to the integration of different database technologies and data structures. In order to check the outputs of the Gradient Boosted Trees algorithm an experimental dataset suitable for machine learning testing has been tested. The test has been performed on a prototype network system modeling a part of the whole communication system. The paper shows how to validate industrial research by following a complete design and development of a whole communication system network improving business intelligence (BI).","",""
0,"D. Lange","Robustness of artificial intelligence in the face of novelty",2022,"","","","",14,"2022-07-13 09:22:00","","10.1117/12.2622912","","",,,,,0,0.00,0,1,1,"A critical factor in utilizing agents with Artificial Intelligence (AI) is their robustness to novelty. AI agents include models that are either engineered or trained. Engineered models include knowledge of those aspects of the environment that are known and considered important by the engineers. Learned models form embeddings of aspects of the environment based on connections made through the training data. In operation, however, a rich environment is likely to present challenges not seen in training sets or accounted for in engineered models. Worse still, adversarial environments are subject to change by opponents. A program at the Defense Advanced Research Project Agency (DARPA) seeks to develop the science necessary to develop and evaluate agents that are robust to novelty. This capability will be required, before AI has the role envisioned within mission critical environments.","",""
1,"K. Panetta, Landry Kezebou, Victor Oludare, J. Intriligator, S. Agaian","Artificial Intelligence for Text-Based Vehicle Search, Recognition, and Continuous Localization in Traffic Videos",2021,"","","","",15,"2022-07-13 09:22:00","","10.3390/ai2040041","","",,,,,1,1.00,0,5,1,"The concept of searching and localizing vehicles from live traffic videos based on descriptive textual input has yet to be explored in the scholarly literature. Endowing Intelligent Transportation Systems (ITS) with such a capability could help solve crimes on roadways. One major impediment to the advancement of fine-grain vehicle recognition models is the lack of video testbench datasets with annotated ground truth data. Additionally, to the best of our knowledge, no metrics currently exist for evaluating the robustness and performance efficiency of a vehicle recognition model on live videos and even less so for vehicle search and localization models. In this paper, we address these challenges by proposing V-Localize, a novel artificial intelligence framework for vehicle search and continuous localization captured from live traffic videos based on input textual descriptions. An efficient hashgraph algorithm is introduced to compute valid target information from textual input. This work further introduces two novel datasets to advance AI research in these challenging areas. These datasets include (a) the most diverse and large-scale Vehicle Color Recognition (VCoR) dataset with 15 color classes—twice as many as the number of color classes in the largest existing such dataset—to facilitate finer-grain recognition with color information; and (b) a Vehicle Recognition in Video (VRiV) dataset, a first of its kind video testbench dataset for evaluating the performance of vehicle recognition models in live videos rather than still image data. The VRiV dataset will open new avenues for AI researchers to investigate innovative approaches that were previously intractable due to the lack of annotated traffic vehicle recognition video testbench dataset. Finally, to address the gap in the field, five novel metrics are introduced in this paper for adequately accessing the performance of vehicle recognition models in live videos. Ultimately, the proposed metrics could also prove intuitively effective at quantitative model evaluation in other video recognition applications. T One major advantage of the proposed vehicle search and continuous localization framework is that it could be integrated in ITS software solution to aid law enforcement, especially in critical cases such as of amber alerts or hit-and-run incidents.","",""
25,"Abbas Abbaszadeh Shahri, S. Larsson, Crister Renkel","Artificial intelligence models to generate visualized bedrock level: a case study in Sweden",2020,"","","","",16,"2022-07-13 09:22:00","","10.1007/s40808-020-00767-0","","",,,,,25,12.50,8,3,2,"","",""
0,"Poona Bahrebar, Leon Denis, Maxim Bonnaerens, Kristof Coddens, J. Dambre, W. Favoreel, I. Khvastunov, A. Munteanu, Hung Nguyen-Duc, S. Schulte, D. Stroobandt, Ramses Valvekens, N. V. D. Broeck, Geert Verbruggen","cREAtIve: reconfigurable embedded artificial intelligence",2021,"","","","",17,"2022-07-13 09:22:00","","10.1145/3457388.3458857","","",,,,,0,0.00,0,14,1,"cREAtIve targets the development of novel highly-adaptable embedded deep learning solutions for automotive and traffic monitoring applications, including position sensor processing, scene interpretation based on LiDAR, and object detection and classification in thermal images for traffic camera systems. These applications share the need for deep learning solutions tailored for deployment on embedded devices with limited resources and featuring high adaptability and robustness to changing environmental conditions. cREAtIve develops knowledge, tools and methods that enable hardware-efficient, adaptable, and robust deep learning.","",""
1,"Pingping Sun, Lingang Gu","Fuzzy knowledge graph system for artificial intelligence-based smart education",2021,"","","","",18,"2022-07-13 09:22:00","","10.3233/JIFS-189332","","",,,,,1,1.00,1,2,1,"Fuzzy knowledge graph system is a semantic network that reveals the relationships between entities, and a tool or methodology that can formally describe things in the real world and their relationships. Smart education is an educational concept or model that uses advanced information technology to build a smart environment, integrates theory and practice to build an educational framework for information age, and provides paths to practice it. Artificial intelligence (AI) is a comprehensive discipline developed by the interpenetration of computer science, cybernetics, information theory, linguistics, neurophysiology and other disciplines, which is a direction for the development of information technology in the future. On the basis of summarizing and analyzing of previous research works, this paper expounded the research status and significance of AI technology, elaborated the development background, current status and future challenges of the construction and application of fuzzy knowledge graph system for smart education, introduced the methods and principles of data acquisition methods and digitalized apprenticeship, realized the process design, information extraction, entity recognition and relationship mining of smart education, constructed a systematic framework for fuzzy knowledge graph, and analyzed the high-quality resources sharing and personalized service of AI-assisted smart education, discussed automatic knowledge acquisition and fusion of fuzzy knowledge graph, performed co-occurrence relationship analysis, and finally conducted application case analysis. The results show that the smart education knowledge graph for AI-assisted smart education can integrate teaching experience and domain knowledge of discipline experts, enhance explainable and robust machine intelligence for AI-assisted smart education, and provide data-driven and knowledge-driven information processing methods; it can also discover the analysis hotspots and main content of research objects through clustering of high-frequency topic words, reveal the corresponding research structure in depth, and then systematically explore its research dimensions, subject background and theoretical basis.","",""
0,"C. Hamel, Mona Hersi, S. Kelly, A. Tricco, S. Straus, G. Wells, B. Pham, B. Hutton","Guidance for using artificial intelligence for title and abstract screening while conducting knowledge syntheses",2021,"","","","",19,"2022-07-13 09:22:00","","10.1186/s12874-021-01451-2","","",,,,,0,0.00,0,8,1,"","",""
0,"A. Stanciu, A. M. Țîțu, Dorin Vasile Deac-Şuteu","Driving Digital Transformation Of Knowledge-Based Organizations Through Artificial Intelligence Enabled Data Centric, Consumption Based, As-A-Service Models",2021,"","","","",20,"2022-07-13 09:22:00","","10.1109/ECAI52376.2021.9515172","","",,,,,0,0.00,0,3,1,"A recent shift in the way people live, work and develop knowledge demands unlocking insights from personal, organizational, and market data. Starting with ingestion, processing, visualization, interactions through insights, cross-team efforts, and decision-making processes, artificial intelligence-based IT support systems are no longer an option in day-to-day activities but an imperative for reaching today’s organizations’ innovation, efficiency, and effectiveness goals. A consistent automation framework for the needed infrastructure, data, IT assets, and life cycle management is highly related to sustainability, security, compatibility, compliance, and legal regulations. The biggest challenge remains the continuous progress in both infrastructure and software advancements. This paper addresses the challenges of modern businesses’ digitalization, states, and demonstrates the solution through flexible, consumption-based information technology services. The purpose is to accelerate the transition from capital expenditures to operating expenses, thereby accelerating innovation and enhancing agility, security, compliance, flexibility, and cost control. The ultimate goal is to use IT as a platform, shifting from an operations realm to an innovation driver, thus creating new revenue streams.","",""
21,"D. Ali, S. Frimpong","Artificial intelligence, machine learning and process automation: existing knowledge frontier and way forward for mining sector",2020,"","","","",21,"2022-07-13 09:22:00","","10.1007/s10462-020-09841-6","","",,,,,21,10.50,11,2,2,"","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",22,"2022-07-13 09:22:00","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
5,"David Abele, Sara D’Onofrio","Artificial Intelligence – The Big Picture",2020,"","","","",23,"2022-07-13 09:22:00","","10.1007/978-3-658-27941-7_2","","",,,,,5,2.50,3,2,2,"","",""
39,"R. Confalonieri, Ludovik Çoba, Benedikt Wagner, Tarek R. Besold","A historical perspective of explainable Artificial Intelligence",2020,"","","","",24,"2022-07-13 09:22:00","","10.1002/widm.1391","","",,,,,39,19.50,10,4,2,"Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision‐making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge‐based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural‐symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human‐understandable explainable systems.","",""
13,"Francesca Iandolo, F. Loia, Irene Fulco, Chiara Nespoli, F. Caputo","Combining Big Data and Artificial Intelligence for Managing Collective Knowledge in Unpredictable Environment—Insights from the Chinese Case in Facing COVID-19",2020,"","","","",25,"2022-07-13 09:22:00","","10.1007/s13132-020-00703-8","","",,,,,13,6.50,3,5,2,"","",""
2,"Yuan Huang, Z. Cheng, Qianyu Zhou, Yuxing Xiang, Ruixiao Zhao","Data Mining Algorithm for Cloud Network Information Based on Artificial Intelligence Decision Mechanism",2020,"","","","",26,"2022-07-13 09:22:00","","10.1109/ACCESS.2020.2981632","","",,,,,2,1.00,0,5,2,"Due to the rapid development of information technology and network technology, there is a lot of data, but the phenomenon of lack of knowledge is becoming more and more serious. Data mining technology has developed vigorously in this environment, and it has shown more and more vitality. Based on Spark programming model, this paper designs the parallel extension of fuzzy c-means. In order to enhance the performance of fuzzy c-means parallel expansion, the improvement strategy of k-means during the initialization phase is borrowed, and k-means// is extended to fuzzy c-means to obtain better clustering performance. Combined with Spark’s programming model, this paper can obtain extended parallel fuzzy c-means algorithm. Several experiments on the data set of the algorithm proposed in this paper have shown good scalability and parallelism, effectively expanding fuzzy c-means clustering to distributed applications, greatly increasing the scale of the data processed by the algorithm. This improves the robustness of the algorithm and the adaptability of the algorithm to the shape and structure of the data, so that the parallel and scalable clustering algorithm can more effectively perform cluster analysis on big data. Three algorithms were simulated on MATLAB platform. We use simple data sets and complex two-dimensional data sets, and compare with the traditional fuzzy c-means algorithm and fuzzy c-means algorithm based on fuzzy entropy. Experiments show that the scalable parallel fuzzy c-means algorithm not only greatly improves the anti-noise performance, but also improves the convergence speed, and it can automatically determine the optimal number of clusters.","",""
8,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor, A. Siraj, Mike Rogers","Domain Knowledge Aided Explainable Artificial Intelligence for Intrusion Detection and Response",2019,"","","","",27,"2022-07-13 09:22:00","","","","",,,,,8,2.67,2,5,3,"Artificial Intelligence (AI) has become an integral part of modern-day security solutions for its ability to learn very complex functions and handling ""Big Data"". However, the lack of explainability and interpretability of successful AI models is a key stumbling block when trust in a model's prediction is critical. This leads to human intervention, which in turn results in a delayed response or decision. While there have been major advancements in the speed and performance of AI-based intrusion detection systems, the response is still at human speed when it comes to explaining and interpreting a specific prediction or decision. In this work, we infuse popular domain knowledge (i.e., CIA principles) in our model for better explainability and validate the approach on a network intrusion detection test case. Our experimental results suggest that the infusion of domain knowledge provides better explainability as well as a faster decision or response. In addition, the infused domain knowledge generalizes the model to work well with unknown attacks, as well as opens the path to adapt to a large stream of network traffic from numerous IoT devices.","",""
88,"Jeannette Paschen, Jan H. Kietzmann, T. Kietzmann","Artificial intelligence (AI) and its implications for market knowledge in B2B marketing",2019,"","","","",28,"2022-07-13 09:22:00","","10.1108/JBIM-10-2018-0295","","",,,,,88,29.33,29,3,3," Purpose The purpose of this paper is to explain the technological phenomenon artificial intelligence (AI) and how it can contribute to knowledge-based marketing in B2B. Specifically, this paper describes the foundational building blocks of any artificial intelligence system and their interrelationships. This paper also discusses the implications of the different building blocks with respect to market knowledge in B2B marketing and outlines avenues for future research.   Design/methodology/approach The paper is conceptual and proposes a framework to explicate the phenomenon AI and its building blocks. It further provides a structured discussion of how AI can contribute to different types of market knowledge critical for B2B marketing: customer knowledge, user knowledge and external market knowledge.   Findings The paper explains AI from an input–processes–output lens and explicates the six foundational building blocks of any AI system. It also discussed how the combination of the building blocks transforms data into information and knowledge.   Practical implications Aimed at general marketing executives, rather than AI specialists, this paper explains the phenomenon artificial intelligence, how it works and its relevance for the knowledge-based marketing in B2B firms. The paper highlights illustrative use cases to show how AI can impact B2B marketing functions.   Originality/value The study conceptualizes the technological phenomenon artificial intelligence from a knowledge management perspective and contributes to the literature on knowledge management in the era of big data. It addresses calls for more scholarly research on AI and B2B marketing. ","",""
0,"V. Naruka, A. Arjomandi Rad, H. Subbiah Ponniah, Jeevan Francis, R. Vardanyan, P. Tasoudis, D. Magouliotis, G. Lazopoulos, M. Salmasi, T. Athanasiou","Machine learning and artificial intelligence in cardiac transplantation: A systematic review.",2022,"","","","",29,"2022-07-13 09:22:00","","10.1111/aor.14334","","",,,,,0,0.00,0,10,1,"BACKGROUND This review aims to systematically evaluate the currently available evidence investigating the use of artificial intelligence (AI) and machine learning (ML) in the field of cardiac transplantation. Furthermore, based on the challenges identified we aim to provide a series of recommendations and a knowledge base for future research in the field of ML and heart transplantation.   METHODS A systematic database search was conducted of original articles that explored the use of ML and/or AI in heart transplantation in EMBASE, MEDLINE, Cochrane database, and Google Scholar, from inception to November 2021.   RESULTS Our search yielded 237 articles, of which 13 studies were included in this review, featuring 463 850 patients. Three main areas of application were identified: (1) ML for predictive modeling of heart transplantation mortality outcomes; (2) ML in graft failure outcomes; (3) ML to aid imaging in heart transplantation. The results of the included studies suggest that AI and ML are more accurate in predicting graft failure and mortality than traditional scoring systems and conventional regression analysis. Major predictors of graft failure and mortality identified in ML models were: length of hospital stay, immunosuppressive regimen, recipient's age, congenital heart disease, and organ ischemia time. Other potential benefits include analyzing initial lab investigations and imaging, assisting a patient with medication adherence, and creating positive behavioral changes to minimize further cardiovascular risk.   CONCLUSION ML demonstrated promising applications for improving heart transplantation outcomes and patient-centered care, nevertheless, there remain important limitations relating to implementing AI into everyday surgical practices.","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",30,"2022-07-13 09:22:00","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",31,"2022-07-13 09:22:00","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
0,"Iván Manuel De la Vega Hernández, Angel Serrano Urdaneta, E. Carayannis","Global bibliometric mapping of the frontier of knowledge in the field of artificial intelligence for the period 1990–2019",2022,"","","","",32,"2022-07-13 09:22:00","","10.1007/s10462-022-10206-4","","",,,,,0,0.00,0,3,1,"","",""
0,"Gang Li, Tongzhou Zhao","Approach of Intelligence Question-Answering System Based on Physical Fitness Knowledge Graph",2021,"","","","",33,"2022-07-13 09:22:00","","10.1109/RCAE53607.2021.9638824","","",,,,,0,0.00,0,2,1,"Artificial intelligence’s penetrating sports is a new development trend of modern sports. Physical Intelligence Question-Answering System (QAS) is a typical application of artificial intelligence in sports, which can quickly respond the physic fitness questions raised by people. This paper aims the construction method of physical fitness QAS based on knowledge graph. Firstly, the physical fitness knowledge graph is constructed based on the crawling data and expert knowledge. Secondly, several physical knowledge question templates are constructed. Thirdly, the Bayesian classifier is used to classify the questions and the Bidirectional Long Short-Term Memory (BiLSTM) combined with Conditional Random Fields (CRF) method is applied to extract contents from the input questions. Then a matching algorithm based on bidirectional slicing string and a statistical method are performed to implement the fuzzy query to enhance the accuracy and robustness of the QAS. The experiments show that the accuracy of physical fitness QAS can reach 93.5% when the question sentences are matched with the query templates, and 86.5% when not matched.","",""
7,"Redmond R. Shamshiri, Ibrahim A. Hameed, Kelly R. Thorp, Siva K. Balasundram, S. Shafian, Mohammad Fatemieh, M. Sultan, B. Mahns, S. Samiei","Greenhouse Automation Using Wireless Sensors and IoT Instruments Integrated with Artificial Intelligence",2021,"","","","",34,"2022-07-13 09:22:00","","10.5772/INTECHOPEN.97714","","",,,,,7,7.00,1,9,1,"Automation of greenhouse environment using simple timer-based actuators or by means of conventional control algorithms that require feedbacks from offline sensors for switching devices are not efficient solutions in large-scale modern greenhouses. Wireless instruments that are integrated with artificial intelligence (AI) algorithms and knowledge-based decision support systems have attracted growers’ attention due to their implementation flexibility, contribution to energy reduction, and yield predictability. Sustainable production of fruits and vegetables under greenhouse environments with reduced energy inputs entails proper integration of the existing climate control systems with IoT automation in order to incorporate real-time data transfer from multiple sensors into AI algorithms and crop growth models using cloud-based streaming systems. This chapter provides an overview of such an automation workflow in greenhouse environments by means of distributed wireless nodes that are custom-designed based on the powerful dual-core 32-bit microcontroller with LoRa modulation at 868 MHz. Sample results from commercial and research greenhouse experiments with the IoT hardware and software have been provided to show connection stability, robustness, and reliability. The presented setup allows deployment of AI on embedded hardware units such as CPUs and GPUs, or on cloud-based streaming systems that collect precise measurements from multiple sensors in different locations inside greenhouse environments.","",""
0,"S. Sadeghi, M. Amiri, Farzaneh Mansoori Mooseloo","Artificial Intelligence and Its Application in Optimization under Uncertainty",2021,"","","","",35,"2022-07-13 09:22:00","","10.5772/intechopen.98628","","",,,,,0,0.00,0,3,1,"Nowadays, the increase in data acquisition and availability and complexity around optimization make it imperative to jointly use artificial intelligence (AI) and optimization for devising data-driven and intelligent decision support systems (DSS). A DSS can be successful if large amounts of interactive data proceed fast and robustly and extract useful information and knowledge to help decision-making. In this context, the data-driven approach has gained prominence due to its provision of insights for decision-making and easy implementation. The data-driven approach can discover various database patterns without relying on prior knowledge while also handling flexible objectives and multiple scenarios. This chapter reviews recent advances in data-driven optimization, highlighting the promise of data-driven optimization that integrates mathematical programming and machine learning (ML) for decision-making under uncertainty and identifies potential research opportunities. This chapter provides guidelines and implications for researchers, managers, and practitioners in operations research who want to advance their decision-making capabilities under uncertainty concerning data-driven optimization. Then, a comprehensive review and classification of the relevant publications on the data-driven stochastic program, data-driven robust optimization, and data-driven chance-constrained are presented. This chapter also identifies fertile avenues for future research that focus on deep-data-driven optimization, deep data-driven models, as well as online learning-based data-driven optimization. Perspectives on reinforcement learning (RL)-based data-driven optimization and deep RL for solving NP-hard problems are discussed. We investigate the application of data-driven optimization in different case studies to demonstrate improvements in operational performance over conventional optimization methodology. Finally, some managerial implications and some future directions are provided.","",""
7,"A. Massaro, Palo Lisco, A. Lombardi, A. Galiano, Nicola Savino","A Case Study of Research Improvements in an Service Industry Upgrading the Knowledge Base of the Information System and the Process Management: Data Flow Automation, Association Rules and Data Mining",2019,"","","","",36,"2022-07-13 09:22:00","","10.5121/IJAIA.2019.10103","","",,,,,7,2.33,1,5,3,"In this paper is analyzed a case study of an upgrade of an industry communication system developed by following ‘Frascati’ research guidelines. The goal of the proposed model is to enhance the industry knowledge Base –KB- by acting directly on information communication system improvements and data system integration, enabling automated process and data processing. The paper follow all the steps performed during the project development: the preliminary data infrastructure design, the information infrastructure improvements, and data processing. Data processing is performed by a calculus engine embedding data mining association rules and Artificial Neural Network –ANN- predictive algorithms thus improving the research. The calculus engine has been implemented by a multiple variables model where the contract data are preliminary processed in order to define functions classifying the operation processes and activating automatically the service process management. The business intelligence –BI- operations are performed mainly by the calculus engine optimizing industry performances. The goal of the paper is to show how research and development –R&D- can be applied by gaining and optimizing the knowledge and processes of an Italian industry working in car services. The project has been developed with the collaboration of the industry ACI Global working in roadside assistance services. By means of a research project resources, the information technology –IT- infrastructure has been improved by new solutions of the communication system and of the data transfer. The proposed case of study provides a model and a guideline to follow in order to apply research in industry acting directly on data and information network.","",""
31,"I. Habli, T. Lawton, Zoe Porter","Artificial intelligence in health care: accountability and safety",2020,"","","","",37,"2022-07-13 09:22:00","","10.2471/BLT.19.237487","","",,,,,31,15.50,10,3,2,"Abstract The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed.","",""
4,"Shubham Yadav, S. Ganesh, Debanjan Das, U. Venkanna, R. Mahapatra, A. Shrivastava, Prantar Chakrabarti, A. Talukder","Suśruta: Artificial Intelligence and Bayesian Knowledge Network in Health Care - Smartphone Apps for Diagnosis and Differentiation of Anemias with Higher Accuracy at Resource Constrained Point-of-Care Settings",2019,"","","","",38,"2022-07-13 09:22:00","","10.1007/978-3-030-37188-3_10","","",,,,,4,1.33,1,8,3,"","",""
1,"Rahul Sadashiv Kharat, T. Devi","Artificial Intelligence in Environmental Management",2021,"","","","",39,"2022-07-13 09:22:00","","10.1201/9781003175865-3","","",,,,,1,1.00,1,2,1,"Artificial intelligence is the way of integrating technology and nature, to refine the understanding of living with nature as everything we find synthetic, artificial, l is not their own, but replicated from nature. Artificial intelligence is nothing but the inferences drawn from the real time situation by subjecting the fact related to them by a natural process as neural networks, genetic algorithm and several different optimization techniques, that the strategy, approach becomes specific to the issue solved by these training programs. Life is secured to none, it is predefined to none, it cannot be scheduled for the illusions of life, as time, space, knowledge, wish, and power but for the purpose, understanding that evolution, changes are unavoidable and which happens through openness to accept facts, analyze the need to get into the changes for progression. The nature as the supreme force has every way for simple living which we ignore since we think we are superior to nature. All the nature based artificial intelligence algorithms are now available as black box model and depending on the nature , trend and data available , any problem can be solved easily. We can simulate exactly similar situation if we understand the data and their occurrence in line with the nature. The less the volume of data, more accurate will be the prediction as the same do include outliers and represent immediate real time situations. The volume of data is not an issue, when we fix the underlying element as individual specific. When this is difficult in conventional statistical methods, considering the data with equal weights and individual specific is very much possible with artificial intelligence. It is possible to relate two independent variables for their influence","",""
0,"Qi Deng","Artificial Intelligence BlockCloud (AIBC) Technical Whitepaper",2018,"","","","",40,"2022-07-13 09:22:00","","10.2139/ssrn.3464239","","",,,,,0,0.00,0,1,4,"The AIBC is an Artificial Intelligence and blockchain technology based large-scale decentralized ecosystem that allows system-wide low-cost sharing of computing and storage resources. The AIBC consists of four layers: a fundamental layer, a resource layer, an application layer, and an ecosystem layer. The AIBC implements a two-consensus scheme to enforce upper-layer economic policies and achieve fundamental layer performance and robustness: the DPoEV incentive consensus on the application and resource layers, and the DABFT distributed consensus on the fundamental layer. The DABFT uses deep learning techniques to predict and select the most suitable BFT algorithm in order to achieve the best balance of performance, robustness, and security. The DPoEV uses the knowledge map algorithm to accurately assess the economic value of digital assets.","",""
0,"Canan Tiftik","Investigation of Human Resources Dimension in Management and Organization Structure of the Effects of Artificial Intelligence",2021,"","","","",41,"2022-07-13 09:22:00","","10.21733/IBAD.833256","","",,,,,0,0.00,0,1,1,"In the competitive time, there has been a great deal of progress in the industry. It is one of the most serious obstacles to the industry in many industries that adopt contemporary technologies to manage continuous development and faster than ordinary jobs. Many of the scientists and researchers recommend using AI tools and digital technologies for industries. Machine language and artificial intelligence are used by many organizations in the human resources unit, where it undertakes an integrated task in recruiting, performance analysis, personnel selection, data collection for employees, providing real-time information and obtaining the right information. Artificial intelligence-based Human Resources (HR) applications have a solid potential to increase employee productivity and support HR experts to become knowledge and trained consultants that increase the success of the employee. HR applications authorized by artificial intelligence have the ability to analyze, predict, diagnose and seek and find more robust and capable resources.","",""
1,"H. López-Fernández","Application of data mining and artificial intelligence techniques to mass spectrometry data for knowledge discovery",2016,"","","","",42,"2022-07-13 09:22:00","","","","",,,,,1,0.17,1,1,6,"Mass spectrometry using matrix assisted laser desorption ionization coupled to time of flight analyzers (MALDI-TOF MS) has become popular during the last decade due to its high speed, sensitivity and robustness for detecting proteins and peptides. This allows quickly analyzing large sets of samples are in one single batch and doing high-throughput proteomics. In this scenario, bioinformatics methods and computational tools play a key role in MALDI-TOF data analysis, as they are able handle the large amounts of raw data generated in order to extract new knowledge and useful conclusions. A typical MALDI-TOF MS data analysis workflow has three main stages: data acquisition, preprocessing and analysis. Although the most popular use of this technology is to identify proteins through their peptides, analyses that make use of artificial intelligence (AI), machine learning (ML), and statistical methods can be also carried out in order to perform biomarker discovery, automatic diagnosis, and knowledge discovery. In this research work, this workflow is deeply explored and new solutions based on the application of AI, ML, and statistical methods are proposed. In addition, an integrated software platform that supports the full MALDI-TOF MS data analysis workflow that facilitate the work of proteomics researchers without advanced bioinformatics skills has been developed and released to the scientific community.","",""
0,"H. L. Fernández","Application of data mining and artificial intelligence techniques to mass spectrometry data for knowledge discovery",2016,"","","","",43,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,1,6,"Mass spectrometry using matrix assisted laser desorption ionization coupled to time of flight analyzers (MALDI-TOF MS) has become popular during the last decade due to its high speed, sensitivity and robustness for detecting proteins and peptides. This allows quickly analyzing large sets of samples are in one single batch and doing high-throughput proteomics. In this scenario, bioinformatics methods and computational tools play a key role in MALDI-TOF data analysis, as they are able handle the large amounts of raw data generated in order to extract new knowledge and useful conclusions. A typical MALDI-TOF MS data analysis workflow has three main stages: data acquisition, preprocessing and analysis. Although the most popular use of this technology is to identify proteins through their peptides, analyses that make use of artificial intelligence (AI), machine learning (ML), and statistical methods can be also carried out in order to perform biomarker discovery, automatic diagnosis, and knowledge discovery. In this research work, this workflow is deeply explored and new solutions based on the application of AI, ML, and statistical methods are proposed. In addition, an integrated software platform that supports the full MALDI-TOF MS data analysis workflow that facilitate the work of proteomics researchers without advanced bioinformatics skills has been developed and released to the scientific community.","",""
0,"","Neural Network Training Using Genetic Algorithms Series In Machine Perception And Artificial Intelligence",2021,"","","","",44,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,0,1,"Knowledge-Based Intelligent Information and Engineering Systems 2Nature-inspired Methods in Chemometrics: Genetic Algorithms and Artificial Neural NetworksParallel Implementations of Backpropagation Neural Networks on TransputersEvolutionary Algorithms and Neural NetworksTraining Neural Networks Using Hybrids with Genetic AlgorithmsNeural Network Training Using Genetic AlgorithmsGene Expression ProgrammingTraining a Neural Network with a Genetic AlgorithmMethods and Applications of Artificial IntelligenceClassification and Learning Using Genetic AlgorithmsPractical Computer Vision Applications Using Deep Learning with CNNsIntelligent Hybrid SystemsNeurogenetic LearningAdvances in Neural Networks ISNN 2007Hybrid Intelligent SystemsEncyclopedia of Computer Science and TechnologyMachine LearningUsing a Genetic Algorithm in Training an Artificial Neural Network to Implement the XOR FunctionGenetic and Evolutionary Computation — GECCO 2004Handbook of Fuzzy ComputationNeural Network Data Analysis Using SimulnetTMArtificial Neural Nets and Genetic AlgorithmsApplied Soft Computing Technologies: The Challenge of ComplexityGenetic Algorithm for Artificial Neural Network Training for the Purpose of Automated Part RecognitionNEURAL NETWORKS, FUZZY LOGIC AND GENETIC ALGORITHMAutomatic Generation of Neural Network Architecture Using Evolutionary ComputationPGANETThe Sixth International Symposium on Neural Networks (ISNN 2009)Evolutionary Machine Learning TechniquesModeling Decisions for Artificial IntelligenceEmpirical Studies on the Utility of Genetic Algorithms for Training and Designing of Neural NetworksTraining Neural Networks Using Genetic AlgorithmsTraining feedforward neural networks using genetic algorithmsMetaheuristic Procedures for Training Neural NetworksArtificial Neural Nets and Genetic AlgorithmsNature-Inspired Computing: Concepts, Methodologies, Tools, and ApplicationsApplications of Evolutionary ComputingArtificial Intelligence and CreativityArtificial Neural Nets and Genetic AlgorithmsDeep Learning Using Genetic Algorithms Creativity is one of the least understood aspects of intelligence and is often seen as `intuitive' and not susceptible to rational enquiry. Recently, however, there has been a resurgence of interest in the area, principally in artificial intelligence and cognitive science, but also in psychology, philosophy, computer science, logic, mathematics, sociology, and architecture and design. This volume brings this work together and provides an overview of this rapidly developing field. It addresses a range of issues. Can computers be creative? Can they help us to understand human creativity? How can artificial intelligence (AI) enhance human creativity? How, in particular, can it contribute to the `sciences of the artificial', such as design? Does the new wave of AI (connectionism, geneticism and artificial life) offer more promise in these areas than classical, symbol-handling AI? What would the implications be for AI and cognitive science if computers could not be creative? These issues are explored in five interrelated parts, each of which is introducted and explained by a leading figure in the field. Prologue (Margaret Boden) Part I: Foundational Issues (Terry Dartnall) Part II: Creativity and Cognition (Graeme S. Halford and Robert Levinson) Part III: Creativity and Connectionism (Chris Thornton) Part IV: Creativity and Design (John Gero) Part V: Human Creativity Enhancement (Ernest Edmonds) Epilogue (Douglas Hofstadter) For researchers in AI, cognitive science, computer science, philosophy, psychology, mathematics, logic, sociology, and architecture and design; and anyone interested in the rapidly growing field of artificial intelligence and creativity.From the contents: Neural networks – theory and applications: NNs (= neural networks) classifier on continuous data domains– quantum associative memory – a new class of neuron-like discrete filters to image processing – modular NNs for improving generalisation properties – presynaptic inhibition modelling for image processing application – NN recognition system for a curvature primal sketch – NN based nonlinear temporalspatial noise rejection system – relaxation rate for improving Hopfield network – Oja's NN and influence of the learning gain on its dynamics Genetic algorithms – theory and applications: transposition: a biological-inspired mechanism to use with GAs (= genetic algorithms) – GA for decision tree induction – optimising decision classifications using GAs – scheduling tasks with intertask communication onto multiprocessors by GAs – design of robust networks with GA – effect of degenerate coding on GAs – multiple traffic signal control using a GA – evolving musical harmonisation – niched-penalty approach for constraint handling in GAs – GA with dynamic population size – GA with dynamic niche clustering for multimodal function optimisation Soft computing and uncertainty: self-adaptation of evolutionary constructed decision trees by information spreading – evolutionary programming of near optimal NNsArtificial neural networks and genetic algorithms both are areas of research","",""
0,"IM Ing. DI Dragos-Cristian Vasilescu, Michael Filzmoser, PhD, Diana Löffler, Andreas Theodorou, Johannes Grössl, Jan G. Michel, Vanessa Schäffner, Stefan Reining, Julia Alessandra Harzheim, Rebecca Davnall, Kilian Karger, Leonie Seng, Lukas Brand, A. Wykowska, D. Neumann, Walther Ch. Zimmerli, Scarlet Schaffrath, David J. Gunkel, Carmen Krämer, Astrid Marieke Rosenthal-von der Pütten, Benedikt Paul Göcke, Andreas Bischof, Arne Maibaum, Gábor L. Ambrus, Tobias Müller, B. P. Göcke","Artificial Intelligence",2019,"","","","",45,"2022-07-13 09:22:00","","10.1017/9781108555814.018","","",,,,,0,0.00,0,26,3,"The Collaborative Specialization in Artificial Intelligence (AI) provides thesis-based Master's students in Computer Science, Engineering, Mathematics and Statistics, and Bioinformatics with a diverse and comprehensive knowledge base in AI. Students wishing to undertake graduate studies at the Master's level with emphasis on artificial intelligence will be admitted by a participating department and will register in both the participating department and in the collaborative specialization.","",""
19,"Simone Castagno, Mohamed Khalifa","Perceptions of Artificial Intelligence Among Healthcare Staff: A Qualitative Survey Study",2020,"","","","",46,"2022-07-13 09:22:00","","10.3389/frai.2020.578983","","",,,,,19,9.50,10,2,2,"Objectives: The medical community is in agreement that artificial intelligence (AI) will have a radical impact on patient care in the near future. The purpose of this study is to assess the awareness of AI technologies among health professionals and to investigate their perceptions toward AI applications in medicine. Design: A web-based Google Forms survey was distributed via the Royal Free London NHS Foundation Trust e-newsletter. Setting: Only staff working at the NHS Foundation Trust received an invitation to complete the online questionnaire. Participants: 98 healthcare professionals out of 7,538 (response rate 1.3%; CI 95%; margin of error 9.64%) completed the survey, including medical doctors, nurses, therapists, managers, and others. Primary outcome: To investigate the prior knowledge of health professionals on the subject of AI as well as their attitudes and worries about its current and future applications. Results: 64% of respondents reported never coming across applications of AI in their work and 87% did not know the difference between machine learning and deep learning, although 50% knew at least one of the two terms. Furthermore, only 5% stated using speech recognition or transcription applications on a daily basis, while 63% never utilize them. 80% of participants believed there may be serious privacy issues associated with the use of AI and 40% considered AI to be potentially even more dangerous than nuclear weapons. However, 79% also believed AI could be useful or extremely useful in their field of work and only 10% were worried AI will replace them at their job. Conclusions: Despite agreeing on the usefulness of AI in the medical field, most health professionals lack a full understanding of the principles of AI and are worried about potential consequences of its widespread use in clinical practice. The cooperation of healthcare workers is crucial for the integration of AI into clinical practice and without it the NHS may miss out on an exceptionally rewarding opportunity. This highlights the need for better education and clear regulatory frameworks.","",""
0,"Chengbing Tan, Qun Chen","Application of an artificial intelligence algorithm model of memory retrieval and roaming in sorting Chinese medicinal materials",2021,"","","","",47,"2022-07-13 09:22:00","","10.3233/jcm-215477","","",,,,,0,0.00,0,2,1,"In order to capture autobiographical memory, inspired by the development of human intelligence, a computational AM model for autobiographical memory is proposed in this paper, which is a three-layer network structure, in which the bottom layer encodes the event-specific knowledge comprising 5W1H, and provides retrieval clues to the middle layer, encodes the related events, and the top layer encodes the event set. According to the bottom-up memory search process, the corresponding events and event sets can be identified in the middle layer and the top layer respectively; At the same time, AM model can simulate human memory roaming through the process of rule-based memory retrieval. The computational AM model proposed in this paper not only has robust and flexible memory retrieval, but also has better response performance to noisy memory retrieval cues than the commonly used memory retrieval model based on keyword query method, and can also imitate the roaming phenomenon in memory.","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",48,"2022-07-13 09:22:00","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
9,"Kuansong Wang, Gang Yu, Chao Xu, Xiang-He Meng, Jian-hua Zhou, C. Zheng, Z. Deng, L. Shang, Ruijie Liu, S. Su, Xunjian Zhou, Qingling Li, Juanni Li, Jing Wang, K. Ma, J. Qi, Zhenmin Hu, P. Tang, Jeffrey Deng, X. Qiu, Bo Li, W. Shen, R. Quan, Juntao Yang, Lin Huang, Yao Xiao, Zhichun Yang, Zhongming Li, Shengchun Wang, Hongzheng Ren, C. Liang, Wei Guo, Yanchun Li, Heng Xiao, Yong-hong Gu, J. Yun, Dan Huang, Zhigang Song, Xiangshan Fan, Ling Chen, Xiaochu Yan, Zhi Li, Zhongjun Huang, Jufang Huang, Joseph Luttrell, Chaoyang Zhang, Weihua Zhou, Kun Zhang, C. Yi, Hui Shen","Accurate diagnosis of colorectal cancer based on histopathology images using artificial intelligence",2020,"","","","",49,"2022-07-13 09:22:00","","10.1186/s12916-021-01942-5","","",,,,,9,4.50,1,50,2,"","",""
2,"Qiwei-Kong, Jing He, Peizhuang Wang","Factor space:a new idea for artificial intelligence based on causal reasoning",2020,"","","","",50,"2022-07-13 09:22:00","","10.1109/WIIAT50758.2020.00089","","",,,,,2,1.00,1,3,2,"In the rapid development of artificial intelligence in the last several years, machine learning is the mainstream method to realize artificial intelligence. What people usually call machine learning can be equivalent to statistical learning, which requires big data and powerful computing power; This is a machine learning trend driven by data, using algorithms to get a model with clear parameters, ignoring causal reasoning and focusing on statistical data; The machine learning method lacking logical causal reasoning will greatly hinder the advancement of artificial intelligence; How knowledge-driven causal reasoning provides new ideas for artificial intelligence is a question worth thinking about for scholars of artificial intelligence. Factor space theory that emphasizes causal reasoning will provide a new perspective and thinking for the development of artificial intelligence.","",""
1,"A. Zarzeczny, P. Babyn, S. Adams, Justin Longo","Artificial intelligence-based imaging analytics and lung cancer diagnostics: Considerations for health system leaders",2020,"","","","",51,"2022-07-13 09:22:00","","10.1177/0840470420975062","","",,,,,1,0.50,0,4,2,"Lung cancer is a leading cause of cancer death in Canada, and accurate, early diagnosis are critical to improving clinical outcomes. Artificial Intelligence (AI)-based imaging analytics are a promising healthcare innovation that aim to improve the accuracy and efficiency of lung cancer diagnosis. Maximizing their clinical potential while mitigating their risks and limitations will require focused leadership informed by interdisciplinary expertise and system-wide insight. We convened a knowledge exchange workshop with diverse Saskatchewan health system leaders and stakeholders to explore issues surrounding the use of AI in diagnostic imaging for lung cancer, including implementation opportunities, challenges, and priorities. This technology is anticipated to improve patient outcomes, reduce unnecessary healthcare spending, and increase knowledge. However, health system leaders must also address the needs for robust data, financial investment, effective communication and collaboration between healthcare sectors, privacy and data protections, and continued interdisciplinary research to achieve this technology’s potential benefits.","",""
0,"Wei Yan","IEEE Transactions on Artificial Intelligence",2020,"","","","",52,"2022-07-13 09:22:00","","10.1109/tfuzz.2020.2987029","","",,,,,0,0.00,0,1,2,"The IEEE Transactions on Artificial Intelligence (TAI) is a multidisciplinary journal publishing papers on theories and methodologies of Artificial Intelligence. Applications of Artificial Intelligence are also considered. Topics covered by IEEE TAI include, but not limited to, Agent-based Systems, Augmented Intelligence, Autonomic Computing, Constraint Systems, Explainable AI, Knowledge-Based Systems, Learning Theories, Planning, Reasoning, Search, Natural Language Processing, and Applications. Technical papers addressing contemporary topics in AI such as Ethics and Social Implications are welcomed.","",""
22,"L. Valiant","Knowledge Infusion: In Pursuit of Robustness in Artificial Intelligence",2008,"","","","",53,"2022-07-13 09:22:00","","10.4230/LIPIcs.FSTTCS.2008.1770","","",,,,,22,1.57,22,1,14,"Endowing computers with the ability to apply commonsense knowledge with human- level performance is a primary challenge for computer science, comparable in importance to past great challenges in other fields of science such as the sequencing of the human genome. The right approach to this problem is still under debate. Here we shall discuss and attempt to justify one ap- proach, that of knowledge infusion. This approach is based on the view that the fundamental objective that needs to be achieved is robustness in the following sense: a framework is needed in which a computer system can represent pieces of knowledge about the world, each piece having some un- certainty, and the interactions among the pieces having even more uncertainty, such that the system can nevertheless reason from these pieces so that the uncertainties in its conclusions are at least controlled. In knowledge infusion rules are learned from the world in a principled way so that sub- sequent reasoning using these rules will also be principled, and subject only to errors that can be bounded in terms of the inverse of the effort invested in the learning process.","",""
1,"O. Jenkins, D. Lopresti, M. Mitchell","Next Wave Artificial Intelligence: Robust, Explainable, Adaptable, Ethical, and Accountable",2020,"","","","",54,"2022-07-13 09:22:00","","","","",,,,,1,0.50,0,3,2,"The history of AI has included several ""waves"" of ideas. The first wave, from the mid-1950s to the 1980s, focused on logic and symbolic hand-encoded representations of knowledge, the foundations of so-called ""expert systems"". The second wave, starting in the 1990s, focused on statistics and machine learning, in which, instead of hand-programming rules for behavior, programmers constructed ""statistical learning algorithms"" that could be trained on large datasets. In the most recent wave research in AI has largely focused on deep (i.e., many-layered) neural networks, which are loosely inspired by the brain and trained by ""deep learning"" methods. However, while deep neural networks have led to many successes and new capabilities in computer vision, speech recognition, language processing, game-playing, and robotics, their potential for broad application remains limited by several factors.  A concerning limitation is that even the most successful of today's AI systems suffer from brittleness-they can fail in unexpected ways when faced with situations that differ sufficiently from ones they have been trained on. This lack of robustness also appears in the vulnerability of AI systems to adversarial attacks, in which an adversary can subtly manipulate data in a way to guarantee a specific wrong answer or action from an AI system. AI systems also can absorb biases-based on gender, race, or other factors-from their training data and further magnify these biases in their subsequent decision-making. Taken together, these various limitations have prevented AI systems such as automatic medical diagnosis or autonomous vehicles from being sufficiently trustworthy for wide deployment. The massive proliferation of AI across society will require radically new ideas to yield technology that will not sacrifice our productivity, our quality of life, or our values.","",""
0,"J. Blay, Jurgi Camblong, F. Sigaux","Artificial Intelligence Applied to Oncology",2020,"","","","",55,"2022-07-13 09:22:00","","10.1007/978-3-030-32161-1_24","","",,,,,0,0.00,0,3,2,"","",""
1,"F. Voskens, J. Abbing, A. T. Ruys, J. Ruurda, I. Broeders","A nationwide survey on the perceptions of general surgeons on artificial intelligence",2022,"","","","",56,"2022-07-13 09:22:00","","10.20517/ais.2021.10","","",,,,,1,1.00,0,5,1,"Aim: Artificial intelligence (AI) has the potential to improve perioperative diagnosis and decision making. Despite promising study results, the majority of AI platforms in surgery currently remain in the research setting. Understanding the current knowledge and general attitude of surgeons toward AI applications in their surgical practice is essential and can contribute to the future development and uptake of AI in surgery. Methods: In March 2021, a web-based survey was conducted among members of the Dutch Association of Surgery. The survey measured opinions on the existing knowledge, expectations, and concerns on AI among surgical residents and surgeons. Results: A total of 313 respondents completed the survey. Overall, 85% of the respondents agreed that AI could be of value in the surgical field and 61% expected AI to improve their diagnostic ability. The outpatient clinic (35.8%) and operating room (39.6%) were stated as area of interest for the use of AI. Statistically, surgeons working in an academic hospital were more likely to be aware of the possibilities of AI (P = 0.01). The surgeons in this survey were not worried about job replacement, however they raised the greatest concerns on accountability issues (50.5%), loss of autonomy (46.6%), and risk of bias (43.5%). Conclusion: This survey demonstrates that the majority of the surgeons show a positive and open attitude towards AI. Although various ethical issues and concerns arise, the expectations regarding the implementation of future surgical AI applications are high.","",""
0,"Chris Yang","Explainable Artificial Intelligence for Predictive Modeling in Healthcare",2022,"","","","",57,"2022-07-13 09:22:00","","10.1007/s41666-022-00114-1","","",,,,,0,0.00,0,1,1,"","",""
0,"Bukhoree Sahoh, Kanjana Haruehansapong, Mallika Kliangkhlao","Causal Artificial Intelligence for High-Stakes Decisions: The Design and Development of a Causal Machine Learning Model",2022,"","","","",58,"2022-07-13 09:22:00","","10.1109/access.2022.3155118","","",,,,,0,0.00,0,3,1,"A high-stakes decision requires deep thought to understand the complex factors that stop a situation from becoming worse. Such decisions are carried out under high pressure, with a lack of information, and in limited time. This research applies Causal Artificial Intelligence to high-stakes decisions, aiming to encode causal assumptions based on human-like intelligence, and thereby produce interpretable and argumentative knowledge. We develop a Causal Bayesian Networks model based on causal science using $d$ -separation and do-operations to discover the causal graph aligned with cognitive understanding. Causal odd ratios are used to measure the causal assumptions integrated with the real-world data to prove the proposed causal model compatibility. Causal effect relationships in the model are verified based on causal P-values and causal confident intervals and approved less than 1% by random chance. It shows that the causal model can encode cognitive understanding as precise, robust relationships. The concept of model design allows software agents to imitate human intelligence by inferring potential knowledge and be employed in high-stakes decision applications.","",""
4,"Qi Deng","Blockchain Economical Models, Delegated Proof of Economic Value and Delegated Adaptive Byzantine Fault Tolerance and their implementation in Artificial Intelligence BlockCloud",2019,"","","","",59,"2022-07-13 09:22:00","","10.3390/jrfm12040177","","",,,,,4,1.33,4,1,3,"The Artificial Intelligence BlockCloud (AIBC) is an artificial intelligence and blockchain technology based large-scale decentralized ecosystem that allows system-wide low-cost sharing of computing and storage resources. The AIBC consists of four layers: a fundamental layer, a resource layer, an application layer, and an ecosystem layer (the latter three are the collective “upper-layers”). The AIBC layers have distinguished responsibilities and thus performance and robustness requirements. The upper layers need to follow a set of economic policies strictly and run on a deterministic and robust protocol. While the fundamental layer needs to follow a protocol with high throughput without sacrificing robustness. As such, the AIBC implements a two-consensus scheme to enforce economic policies and achieve performance and robustness: Delegated Proof of Economic Value (DPoEV) incentive consensus on the upper layers, and Delegated Adaptive Byzantine Fault Tolerance (DABFT) distributed consensus on the fundamental layer. The DPoEV uses the knowledge map algorithm to accurately assess the economic value of digital assets. The DABFT uses deep learning techniques to predict and select the most suitable BFT algorithm in order to enforce the DPoEV, as well as to achieve the best balance of performance, robustness, and security. The DPoEV-DABFT dual-consensus architecture, by design, makes the AIBC attack-proof against risks such as double-spending, short-range and 51% attacks; it has a built-in dynamic sharding feature that allows scalability and eliminates the single-shard takeover. Our contribution is four-fold: that we develop a set of innovative economic models governing the monetary, trading and supply-demand policies in the AIBC; that we establish an upper-layer DPoEV incentive consensus algorithm that implements the economic policies; that we provide a fundamental layer DABFT distributed consensus algorithm that executes the DPoEV with adaptability; and that we prove the economic models can be effectively enforced by AIBC’s DPoEV-DABFT dual-consensus architecture.","",""
10,"S. Mukhopadhyay, Sumarga Kumar Sah Tyagi, N. Suryadevara, V. Piuri, F. Scotti, S. Zeadally","Artificial Intelligence-Based Sensors for Next Generation IoT Applications: A Review",2021,"","","","",60,"2022-07-13 09:22:00","","10.1109/JSEN.2021.3055618","","",,,,,10,10.00,2,6,1,"Sensors play a vital role in our daily lives and are an essential component for Internet of Things (IoT) based systems as they enable the IoT to collect data to take smart and intelligent decisions. Recent advances in IoT systems, applications, and technologies, including industrial Cyber-Physical Systems (CPSs), are being supported by a wide range of different types of sensors based on artificial intelligence (AI). These smart AI-based sensors are typically characterized by onboard intelligence and have the ability to communicate collaboratively or through the Internet. To achieve the high level of automation required in today’s smart IoT applications, sensors incorporated into nodes must be efficient, intelligent, context-aware, reliable, accurate, and connected. Such sensors must also be robust, safety- and privacy-aware for users interacting with them. Sensors leveraging advanced AI technologies, new capabilities have recently emerged which have the potential to detect, identify, and avoid performance degradation and discover new patterns. Along with knowledge from complex sensor datasets, they can promote product innovation, improve operation level, and open up novel business models. We review sensors, smart data processing, communication protocol, and artificial intelligence which will enable the deployment of AI-based sensors for next-generation IoT applications.","",""
27,"N. S. Saravana Kumar","IMPLEMENTATION OF ARTIFICIAL INTELLIGENCE IN IMPARTING EDUCATION AND EVALUATING STUDENT PERFORMANCE",2019,"","","","",61,"2022-07-13 09:22:00","","10.36548/jaicn.2019.1.001","","",,,,,27,9.00,27,1,3,"Simulation of human intelligence process is made possible with the help of artificial intelligence. The learning, reasoning and self-correction properties are made possible in computer systems. Along with AI, other technologies are combined effectively in order to create remarkable applications. We apply the changing role of AI and its techniques in new educational paradigms to create a personalised teaching-learning environment. Features like recognition, pattern matching, decision making, reasoning, problem solving and so on are applied along with knowledge based system and supervised machine learning for a complete learning and assessment process.","",""
26779,"Stuart J. Russell, Peter Norvig","Artificial Intelligence: A Modern Approach",1995,"","","","",62,"2022-07-13 09:22:00","","10.5860/choice.33-1577","","",,,,,26779,991.81,13390,2,27,"The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.","",""
3,"H. Mohammed, S. Ismail","Proposition of new computer artificial intelligence models for shear strength prediction of reinforced concrete beams",2021,"","","","",63,"2022-07-13 09:22:00","","10.1007/S00366-021-01400-Z","","",,,,,3,3.00,2,2,1,"","",""
3,"T. Kaur, Anirudra Diwakar, Kirandeep, Pranav Mirpuri, M. Tripathi, P. Chandra, T. Gandhi","Artificial Intelligence in Epilepsy",2021,"","","","",64,"2022-07-13 09:22:00","","10.4103/0028-3886.317233","","",,,,,3,3.00,0,7,1,"Background: The study of seizure patterns in electroencephalography (EEG) requires several years of intensive training. In addition, inadequate training and human error may lead to misinterpretation and incorrect diagnosis. Artificial intelligence (AI)-based automated seizure detection systems hold an exciting potential to create paradigms for proper diagnosis and interpretation. AI holds the promise to transform healthcare into a system where machines and humans can work together to provide an accurate, timely diagnosis, and treatment to the patients. Objective: This article presents a brief overview of research on the use of AI systems for pattern recognition in EEG for clinical diagnosis. Material and Methods: The article begins with the need for understanding nonstationary signals such as EEG and simplifying their complexity for accurate pattern recognition in medical diagnosis. It also explains the core concepts of AI, machine learning (ML), and deep learning (DL) methods. Results and Conclusions: In this present context of epilepsy diagnosis, AI may work in two ways; first by creating visual representations (e.g., color-coded paradigms), which allow persons with limited training to make a diagnosis. The second is by directly explaining a complete automated analysis, which of course requires more complex paradigms than the previous one. We also clarify that AI is not about replacing doctors and strongly emphasize the need for domain knowledge in building robust AI models that can work in real-time scenarios rendering good detection accuracy in a minimum amount of time.","",""
2,"N. Shehab, A. Hamdan","Artificial Intelligence and Women Empowerment in Bahrain",2021,"","","","",65,"2022-07-13 09:22:00","","10.1007/978-3-030-72080-3_6","","",,,,,2,2.00,1,2,1,"","",""
41,"Guangnan Zhang, Z. H. Ali, M. Aldlemy, Mohamed H. Mussa, Sinan Q. Salih, M. Hameed, Z. Al-khafaji, Z. Yaseen","Reinforced concrete deep beam shear strength capacity modelling using an integrative bio-inspired algorithm with an artificial intelligence model",2020,"","","","",66,"2022-07-13 09:22:00","","10.1007/s00366-020-01137-1","","",,,,,41,20.50,5,8,2,"","",""
4,"T. Schmid","Deconstructing the Final Frontier of Artificial Intelligence: Five Theses for a Constructivist Machine Learning",2019,"","","","",67,"2022-07-13 09:22:00","","","","",,,,,4,1.33,4,1,3,"Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subirón 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world – and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (Méhaut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as “hypothetical constructs” (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n","",""
6,"Yaron Einhorn, M. Einhorn, Adaia Kamshov, Oron Lev, A. Trabelsi, N. Paz-Yaacov, S. Gross","Gene-specific artificial intelligence-based variant classification engine: results of a time-capsule experiment",2019,"","","","",68,"2022-07-13 09:22:00","","10.21203/rs.2.11834/v1","","",,,,,6,2.00,1,7,3,"  Background: Interpretation of genetic variation remains an impediment to cost-effective application of genomics to medicine. An advanced artificial intelligence (AI)-based Variant Classification Engine (aiVCE), rooted in ACMG/AMP guidelines, employs data-driven methods to expedite gene-specific classification (franklin.genoox.com). In this blinded study, the aiVCE’s overall and rule-level performances were evaluated using ClinVar (v. 2018-10) variants with creation dates after 5/01/2017. By removing any prior knowledge of these variants from the aiVCE training data, they were treated as novel variants. Using a ‘Full’ dataset (75,801 variants with ≥1 star) and an ‘Increased-Certainty’ dataset (3,993 variants with ≥2 stars), the aiVCE classified variants as pathogenic (P), likely-pathogenic (LP), uncertain significance (VUS), likely-benign (LB), or benign (B). VUS with sufficient supporting data were subclassified as VUS-leaning benign or VUS-leaning pathogenic. aiVCE results were evaluated to determine concordance with final ClinVar classification and rule-level determinations. Results: The aiVCE demonstrated >97% concordance among Increased-Certainty variants. Concordance was >95% across variant effects (e.g., missense, null, splice region), and was >93.5% for the Full dataset. When assessing the aiVCE’s application of specific ACMG rules, significant differences were observed between ClinVar P/LP and B/LB variants rule-met proportions (all P<0.00001), thus supporting gene-specific rule selections. Evaluation of discordance between the aiVCE and ClinVar uncovered evidences that might have been unavailable to submitting laboratories, highlighting AI utility in variant classification. Conclusions: The aiVCE exhibited robust performance, despite lacking past evidence, in determining whether variants would be categorized as P/LP. Applying latest computational advances to existing guidelines may assist scientists and clinicians interpret variants with limited clinical information and greatly reduce analytical bottlenecks.","",""
2,"Chun-Hsi Huang, Xiangrong Wang","Financial Innovation Based on Artificial Intelligence Technologies",2019,"","","","",69,"2022-07-13 09:22:00","","10.1145/3349341.3349504","","",,,,,2,0.67,1,2,3,"Nowadays, the degree of the heated topic of artificial intelligence in the world reaches a new height. Due to the breakthrough of deep learning algorithm based on neural network, the level of artificial intelligence technologies has been enhanced significantly. The global financial industry is quietly changing under the catalysis of artificial intelligence. The frontier artificial intelligence technologies, such as the technology of expert system, machine learning and knowledge discovery in database are combed to explore the financial applications of artificial intelligence. Based on these key technologies, this paper proposed three applications of artificial intelligence in the financial field, including intelligent investment adviser, transaction forecast and financial regulation, discusses the key technologies of artificial intelligence and financial innovation products based on these technologies, such as the functions of the transaction prediction system based on artificial intelligence technologies include forecast analysis, index statistics, stock analysis and information retrieval, etc. The structures of the systems are drawn and the design principles are provided. Finally, to guard the safety of the applications of artificial intelligence, the paper gives the suggestions of enhancing identity authentication, introducing monitoring measures and limiting autonomy degree.","",""
0,"Xiaohong W. Gao, B. Braden","Artificial intelligence in endoscopy: The challenges and future directions",2021,"","","","",70,"2022-07-13 09:22:00","","10.37126/aige.v2.i4.117","","",,,,,0,0.00,0,2,1,"Artificial intelligence based approaches, in particular deep learning, have achieved state-of-the-art performance in medical fields with increasing number of software systems being approved by both Europe and United States. This paper reviews their applications to early detection of oesophageal cancers with a focus on their advantages and pitfalls. The paper concludes with future recommendations towards the development of a real-time, clinical implementable, interpretable and robust diagnosis support systems.","",""
1,"Sonal Modak, D. Sehgal, J. Valadi","Applications of Artificial Intelligence and Machine Learning in Viral Biology",2019,"","","","",71,"2022-07-13 09:22:00","","10.1007/978-3-030-29022-1_1","","",,,,,1,0.33,0,3,3,"","",""
0,"Suk Lee, E. Ju, Suk Woo Choi, Hyung-ju Lee, Jang Bo Shim, Kyung Hwan Chang, Kwang Hyeon Kim, Chul Yong Kim","Prediction of Cancer Patient Outcomes Based on Artificial Intelligence",2018,"","","","",72,"2022-07-13 09:22:00","","10.5772/INTECHOPEN.81872","","",,,,,0,0.00,0,8,4,"Knowledge-based outcome predictions are common before radiotherapy. Because there are various treatment techniques, numerous factors must be considered in predicting cancer patient outcomes. As expectations surrounding personalized radiotherapy using complex data have increased, studies on outcome predictions using artificial intelligence have also increased. Representative artificial intelligence techniques used to predict the outcomes of cancer patients in the field of radiation oncology include collecting and processing big data, text mining of clinical literature, and machine learning for implementing prediction models. Here, methods of data preparation and model construction to predict rates of survival and toxicity using artificial intelligence are described.","",""
0,"C. Carpenter","Augmented Artificial Intelligence Improves Data Analytics in Heavy-Oil Reservoirs",2019,"","","","",73,"2022-07-13 09:22:00","","10.2118/0519-0068-JPT","","",,,,,0,0.00,0,1,3,"This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 193650, “Augmented-Artificial-Intelligence Solutions for Heavy-Oil Reservoirs: Innovative Work Flows That Build From Smart Analytics, Machine Learning, and Expert-Based Systems,” by David Castineira, Xiang Zhai, and Hamed Darabi, Quantum Reservoir Impact Group, prepared for the 2018 SPE International Heavy Oil Conference and Exhibition, Kuwait City, Kuwait, 10–12 December. The paper has not been peer reviewed.  Recently, many heavy-oil fields have seen exponentially higher volumes of data made available as a result of omnipresent connectivity. Existing data platforms have focused traditionally on solving the problem of data storage and access. The more-complex problem of true knowledge discovery and systematic value creation from the massive amount of data is less frequently addressed. The authors of this paper propose a novel work flow for the problem of building intelligent data analytics in heavy-oil fields.  Introduction  Optimal reservoir management for heavy-oil reservoirs requires systematic solutions that combine both engineering ability and advanced analytics. The authors believe that this requirement is addressed by what they call augmented artificial intelligence (AAI), a process inspired by the intelligence-amplification concept in which machine learning and human expertise are combined to improve solutions derived by systems that learn without any type of input from engineers or geoscientists. Practical deployment of AAI will involve automated work flows that use solid technical expertise and proven processes to transform field data into more-effective reservoir-management solutions.  Even with rapid data-preprocessing solutions in place, developing an optimal reservoir-management framework for heavy-oil assets is inherently complex. Identifying key recovery obstacles (KROs) and field-development plans (FDPs) typically takes many months, involving a large team of experts and the construction of sophisticated full-field simulation models. The recommendation is that automated work flows and AAI solutions are combined to identify those KROs rapidly and prepare robust FDPs that increase production and optimize current operations.  Perhaps the less-intuitive step in developing systematic solutions for heavy-oil fields is the process of developing a quantitative reservoir diagnostic framework. This process must build from big-data analytics platforms and an array of analytical, numerical, and empirical models combined to deliver a catalog of KROs affecting field performance. To this end, the entire historical set of well, field, and reservoir data must be processed and input into this diagnostics platform. Once the KROs are understood, the next step is to translate the diagnostics into detailed action plans in the field that can generate production, reserves, or capital-efficiency improvements.  This paper aims to offer an alternative approach to traditional work flows that identify recovery obstacles and development opportunities in heavy-oil fields by labor-intensive solutions. In contrast, the authors propose a systematic framework that provides three key advantages:  Execution time is fast, and an initial opportunity inventory can be generated.  The user can choose from multiple algorithms and methods to customize the technology to unique field/reservoir complexities.  The core algorithms are data-driven, integrate multidisciplinary data sets, and leave little room for the biases of the user, which allows for a consistent and repeatable analysis.","",""
0,"Yaxin Peng, S. Du, T. Zeng","Preface: Special Issue on Optimization Models and Algorithms in Artificial Intelligence",2019,"","","","",74,"2022-07-13 09:22:00","","10.1007/s40305-019-00278-5","","",,,,,0,0.00,0,3,3,"","",""
8,"M. Peters, P. Jandrić","Artificial Intelligence, Human Evolution, and the Speed of Learning",2019,"","","","",75,"2022-07-13 09:22:00","","10.1007/978-981-13-8161-4_12","","",,,,,8,2.67,4,2,3,"","",""
1,"James A. Crowder, John Carbone, Shelli Friess","Ontology-Based Knowledge Management for Artificial Intelligent Systems",2019,"","","","",76,"2022-07-13 09:22:00","","10.1007/978-3-030-17081-3_9","","",,,,,1,0.33,0,3,3,"","",""
2,"M. Shahid, G. Abbas, Mohammad Rashid Hussain, M. Asad, U. Farooq, J. Gu, V. Balas, M. Uzair, A. Awan, T. Yazdan","Artificial Intelligence-Based Controller for DC-DC Flyback Converter",2019,"","","","",77,"2022-07-13 09:22:00","","10.3390/app9235108","","",,,,,2,0.67,0,10,3,"This paper presents an intelligent voltage controller designed on the basis of an adaptive neuro-fuzzy inference system (ANFIS) for a flyback converter (FC) working in continuous conduction mode (CCM). The union of fuzzy logic (FL) and adaptive neural networks (ANN) makes ANFIS more robust against model parameters’ uncertainties and perturbations in input voltage or load current. ANFIS inherits the advantages of structured knowledge representation from FL and learning capability from NN. Comparative analysis showed that the ANFIS controller offers not only the superior transient response characteristics, but also excellent steady-state characteristics compared to those of the FL controller (FLC) and proportional–integral–derivative (PID) controllers, thus validating its superiority over these traditional controllers. For this purpose, MATLAB/Simulink environment-based simulation results are presented for validation of the proposed converter compensated system under all operating conditions.","",""
1,"L. Bori, M. Valera, D. Gilboa, R. Maor, I. Kottel, J. Remohi, D. Seidman, M. Meseguer","O-084 Computer vision can distinguish between euploid and aneuploid embryos. A novel artificial intelligence (AI) approach to measure cell division activity associated with chromosomal status",2021,"","","","",78,"2022-07-13 09:22:00","","10.1093/humrep/deab125.014","","",,,,,1,1.00,0,8,1,"      Can we distinguish between top-grade euploid and aneuploid embryos by AI measurement of cell edges in time-lapse videos?        Aneuploid embryos can be distinguished from euploid embryos by AI determination of a longer time to blastulation and higher cell activity.        Continuous monitoring of the embryo development has brought out morphokinetic parameters that are used to predict pre-implantation genetic testing (PGT) results. Previous publications showed that euploid embryos reach blastulation earlier than non-euploid embryos. However, time-lapse data are currently under-utilized in making predictions about embryo chromosomal content. AI and computer vision could take advantage of the massive amount of data embedded in the images of embryo development. This is the first attempt to distinguish between euploid and aneuploid embryos by computer vision in an objective and indirect way based on the measurement of cell edges as a proxy for cell activity.        We performed a retrospective analysis of 1,314 time-lapse videos from embryos cultured to the blastocyst stage with PGT results. This single-center study involved two phases; a comparison of the start time of blastulation between euploid (n = 544) and aneuploid embryos (n = 797). In phase two, we designed a novel methodology to examine whether precise measurement of cell edges over time could reflect cell activity differences in blastulation.        We assumed that the delay in blastulation is reflected by higher cell activity that could be determined accurately for the first time using computer vision and machine learning to measure the length of the edges (from t2 to t8). We compared computer vision based measurements of cell edges, reflecting cell number and size, in videos of 231 top-grade euploid (n = 111) and aneuploid (n = 120) embryos.        The mean and standard deviation of blastulation start time was 100.1±6.8 h for euploid embryos and 101.8±8.2 h for aneuploid embryos (p < 0.001). Regarding the measurement of cell activity, a computer vision algorithm identified the edges and provided a certainty score for each edge, higher when the algorithm is more certain that this is a cell edge (as opposed to noise in the images). A threshold was set to distinguish cell edges from noise using this score. The following results for top-grade embryos are shown as the sum of the edge lengths (µm) average of 160 pictures per embryo (frames between t2 and t8). The total length of the cell edges increased from two cells (420±85 µm) to eight cells (861±237 µm), in line with the mitosis events. Both the average total edge measured (450±162 µm for euploid embryos and 489±215 µm for aneuploid embryos, p < 0.01) and the average total of the difference between consecutive frames (135±47 µm for euploid embryos and 153±64 µm for aneuploid embryos, p < 0.01) were higher for aneuploid embryos than for euploid embryos. A regression model to differentiate between the two classes achieved 73% sensitivity and 73% specificity on this dataset.        The main limitation of this study is the difficulty to correlate our findings to other measure of cell activity. A more robust AI function (using not only cell edges lengths) would be required for future analysis to measure the cell activity in cell division up to the blastocyst stage.        Our results show for the first time that an AI based system can precisely measure microscopic cell edges in the dividing embryo. Using this novel method, we could distinguish between euploid and aneuploid embryos. This non-invasive method could further enhance our knowledge of the developing embryo.        Not Applicable ","",""
1,"Latifa Mrisho, N. Mbilinyi, Mathias Ndalahwa, Amanda Ramcharan, Annalyse Kehs, Peter McCloskey, H. Murithi, David P. Hughes, J. Legg","Evaluating the accuracy of a smartphone-based artificial intelligence system, PlantVillage Nuru, in diagnosing of the viral diseases of cassava",2020,"","","","",79,"2022-07-13 09:22:00","","10.1101/2020.01.26.919449","","",,,,,1,0.50,0,9,2,"Premise of the study Nuru is an artificial intelligence system for diagnosis of plant diseases and pests developed as a public good by PlantVillage (Penn State University), FAO, IITA and CIMMYT. It provides a simple, inexpensive and robust means of conducting in-field diagnosis without requiring internet connection and provides real-time results and advice. The present work evaluates the effectiveness of Nuru as an in-field diagnostic tool by comparing the diagnosis capability of Nuru to that of cassava experts (researchers trained on cassava pests and diseases), agricultural extension agents and farmers. Methods The diagnosis capability of Nuru and that of the assessed individuals was determined by inspecting cassava plants in-field and by using the cassava symptom recognition assessment tool (CaSRAT) to score images of cassava leaves. Results Nuru’s accuracy for symptom recognition when using six leaves (74 - 88%, depending on the condition) was similar to that of experts, 1.5-times higher than agricultural extension agents and two-times higher than farmers. Discussion These findings suggests that Nuru can be an effective tool for in-field diagnosis of cassava diseases and has a potential of being a quick and cost-effective means of disseminating knowledge from researchers to agricultural extension agents and farmers.","",""
0,"A. Paic","Policies for Artificial Intelligence in Science and Innovation",2020,"","","","",80,"2022-07-13 09:22:00","","10.22323/1.372.0045","","",,,,,0,0.00,0,1,2,"This contribution synthesizes the discussions of the special session on policies for Artificial Intelligence in Science and Innovation, organized by the OECD’s Directorate for Science, Technology and Innovation. The session was opened by Dr Judith Arrieta, Minister of the Foreign Service at the Chief of Staff’s Office of the Secretary of Foreign Affairs of Mexico, and the two panels included speakers from governments, industry and civil society from European countries, USA,Canada China and Australia. Participants discussed the disruptive nature of AI and the formidable challenges it poses. Most of the discussion focused under the umbrella title of ethics, but they span very different issues of human-centered values, fairness, transparency, explainability, and many more. Other challenges include employment, education, SME policy, enabling environment, access to data and computing technology. Responses by governments were also discussed with a particular focus on national strategies, whose main pillars are oriented toward knowledge creation through AI research, knowledge diffusion through linkages to the private sector, development of human capital which will underpin the development of the sector, and a strong values, ethical and regulatory framework to create the conditions for the development of trustworthy AI.  In a world of finite resources, discussants concluded that one cannot apply very stringent requirements to all AI decisions, and there is clearly a need to require more transparency, explainability and robustness from systems which have the greatest impact on human lives. Therefore an approach based on algorithmic impact assessment seems reasonable. Such an approach needs to be further developed and standardized.","",""
10,"M. Alomar, M. Hameed, N. Al‐Ansari, M. Alsaadi","Data-Driven Model for the Prediction of Total Dissolved Gas: Robust Artificial Intelligence Approach",2020,"","","","",81,"2022-07-13 09:22:00","","10.1155/2020/6618842","","",,,,,10,5.00,3,4,2,"Saturated total dissolved gas (TDG) is recently considered as a serious issue in the environmental engineering field since it stands behind the reasons for increasing the mortality rates of fish and aquatic organisms. The accurate and more reliable prediction of TDG has a very significant role in preserving the diversity of aquatic organisms and reducing the phenomenon of fish deaths. Herein, two machine learning approaches called support vector regression (SVR) and extreme learning machine (ELM) have been applied to predict the saturated TDG% at USGS 14150000 and USGS 14181500 stations which are located in the USA. For the USGS 14150000 station, the recorded samples from 13 October 2016 to 14 March 2019 (75%) were used for training set, and the rest from 15 March 2019 to 13 October 2019 (25%) were used for testing requirements. Similarly, for USGS 14181500 station, the hourly data samples which covered the period from 9 June 2017 till 11 March 2019 were used for calibrating the models and from 12 March 2019 until 9 October 2019 were used for testing the predictive models. Eight input combinations based on different parameters have been established as well as nine statistical performance measures have been used for evaluating the accuracy of adopted models, for instance, not limited, correlation of determination (        R      2        ), mean absolute relative error (MAE), and uncertainty at 95% (        U      95        ). The obtained results of the study for both stations revealed that the ELM managed efficiently to estimate the TDG in comparison to SVR technique. For USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.986 (0.986), MAE of 0.316 (0.441), and         U      95        of 3.592 (3.869). Lastly, for USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.991 (0.991), MAE of 0.338 (0.396), and         U      95        of 0.832 (0.837). In addition, ELM’s training process computational time is stated to be much shorter than that of SVM. The results also showed that the temperature parameter was the most significant variable that influenced TDG relative to the other parameters. Overall, the proposed model (ELM) proved to be an appropriate and efficient computer-assisted technology for saturated TDG modeling that will contribute to the basic knowledge of environmental considerations.","",""
128,"D. Bonderman","Artificial intelligence in cardiology",2017,"","","","",82,"2022-07-13 09:22:00","","10.1007/s00508-017-1275-y","","",,,,,128,25.60,128,1,5,"","",""
19,"P. Svenmarck, L. Luotsinen, Mattias Nilsson, J. Schubert","Possibilities and Challenges for Artificial Intelligence in Military Applications",2018,"","","","",83,"2022-07-13 09:22:00","","","","",,,,,19,4.75,5,4,4,"Recent developments in artificial intelligence (AI) have resulted in a breakthrough for many classical AIapplications, such as computer vision, natural language processing, robotics, and data mining. Therefore, there are many efforts to exploit these developments for military applications, such as surveillance, reconnaissance, threat evaluation, underwater mine warfare, cyber security, intelligence analysis, command and control, and education and training. However, despite the possibilities for AI in military applications, there are many challenges to consider. For instance, 1) high risks means that military AI-systems need to be transparent to gain decision maker trust and to facilitate risk analysis; this is a challenge since many AItechniques are black boxes that lack sufficient transparency, 2) military AI-systems need to be robust and reliable; this is a challenge since it has been shown that AI-techniques may be vulnerable to imperceptible manipulations of input data even without any knowledge about the AI-technique that is used, and 3) many AItechniques are based on machine learning that requires large amounts of training data; this is challenge since there is often a lack of sufficient data in military applications. This paper present results from ongoing projects to identity possibilities for AI in military applications, as well as how to address these challenges.","",""
4,"Morteza Saberi, A. Azadeh, Z. Saberi, P. Pazhoheshfar","A knowledge management system based on artificial intelligence (AI) methods: A flexible fuzzy regression-analysis of variance algorithm for natural gas consumption estimation",2012,"","","","",84,"2022-07-13 09:22:00","","10.1109/InfRKM.2012.6205023","","",,,,,4,0.40,1,4,10,"A knowledge management (KM) system has a different schema that one of them has been studied in the present study. One of version of KM is based on artificial intelligence (AI) methods. KM based on AI has been investigated in the present work based on natural gas consumption estimation domain. Developing accurate and flexible model to natural gas consumption estimation is a strategic step in policy and decision-making process in energy sector. This paper provides a stage algorithm during that it gains optimal fuzzy regression model for studding natural gas consumption in sixteen countries according to data in years 1989 till 2007. Different countries have selected from Africa, America, Asia, Europe and Middle East based on high, middle and low GDP index and for every one of them nine fuzzy regression models have executed and the results and error of each model have calculated. Preprocess has been done on the initial data to gain better results which the min-max method has been used for this purpose. Two criterions have been used to determining suitable and appropriate fuzzy regression model in each country. Firstly, fuzzy regression models with MAPE value below 10 is deleted from the assessment and remained fuzzy regression models are compared with ANOVA. Upon logic that given algorithm sketches for some countries none of used models are proper while for other countries optimal model is gained in first or second filter of algorithm. To show the applicability and superiority of the proposed flexible Fuzzy regression model the data for oil consumption in Japan, Thailand, Bangladesh from Asia and Norway, Italy, Bulgaria from Europe and Qatar, Iran, Iraq from Middle East and The united state, Mexico, Bolivia from North America and Libya, Tunisia, Nigeria from Africa during 1989 to 2007 are used.","",""
30,"S. Elkatatny, Zeeshan Tariq, M. Mahmoud, I. Mohamed, A. Abdulraheem","Development of New Mathematical Model for Compressional and Shear Sonic Times from Wireline Log Data Using Artificial Intelligence Neural Networks (White Box)",2018,"","","","",85,"2022-07-13 09:22:00","","10.1007/S13369-018-3094-5","","",,,,,30,7.50,6,5,4,"","",""
0,"Narmin Ghaffari Laleh, D. Truhn, Gregory Patrick Veldhuizen, Tianyu Han, Marko van Treeck, R. D. Bülow, R. Langer, B. Dislich, P. Boor, V. Schulz, J. Kather","Adversarial attacks and adversarial robustness in computational pathology",2022,"","","","",86,"2022-07-13 09:22:00","","10.1101/2022.03.15.484515","","",,,,,0,0.00,0,11,1,"Artificial Intelligence (AI) can support diagnostic workflows in oncology by aiding diagnosis and providing biomarkers. AI applications are therefore expected to evolve from academic prototypes to commercial products in the coming years. However, AI applications are vulnerable to adversarial attacks, such as malicious interference with test data aiming to cause misclassifications. Therefore, it is essential for the use of AI-based diagnostic devices to secure them against such attacks before widespread use. Unfortunately, no resistant systems exist in computational pathology so far. To address this problem, we investigate the susceptibility of convolutional neural networks (CNNs) to multiple types of white- and black-box attacks. We demonstrate that both attacks can easily confuse CNNs in clinically relevant pathology tasks and impair classification performance. Classical adversarially robust training and dual batch normalization (DBN) are possible mitigation strategies but require precise knowledge of the type of attack used in the inference. We demonstrate that vision transformers (ViTs) perform equally well compared to CNNs at baseline and are orders of magnitude more robust to different types of white-box and black-box attacks. At a mechanistic level, we show that this is associated with a more robust latent representation of clinically relevant categories in ViTs compared to CNNs. Our results are in line with previous theoretical studies. We show that ViTs are robust learners in computational pathology. This implies that large-scale rollout of AI models in computational pathology should rely on ViTs rather than CNN-based classifiers to provide inherent protection against adversaries.","",""
3,"E.V. Blagodarny, A.A Vedyakhin, A.M. Raygorodsky","Development of Educational Projects on the Basis of Technological Platforms with Artificial Intelligence: The Experience of MIPT on the Use of High Vox-Platform",2018,"","","","",87,"2022-07-13 09:22:00","","10.1109/IC-AIAI.2018.8674452","","",,,,,3,0.75,1,3,4,"The MIPT School of Applied Mathematics and Computer Science conducts research on artificial intelligence and develops education in this field in Russia. Modern science and technology are developing so quickly that a person needs to constantly learn and acquire new skills. Therefore, MIPT develops educational courses at the school, academic and corporate levels. Employers and scientific laboratories are more interested in the practical skills of employees in AI, than just theoretical knowledge. For this reason, the MIPT School of Applied Mathematics and Computer Science conducts and develop new practice-oriented educational courses. Moreover, the Laboratory of Innovation at MIPT creates the HighVox platform, which will allow MIPT students to gain experience in solving real problems from Russian companies during their studies. The platform creates the digital trace of each student: competences, scientific interests, courses taken, completed projects, soft skills, etc. Based on the digital trace of each participant, the platform automatically creates recommendations for projects and teams most suitable for the student. In the future, HighVox will become a place where technical specialists search for work, get an education (lifelong learning), communicate with colleagues on specialized topics and offer their ideas for startups. As part of the creation of this platform, the laboratory of innovation conducts research in two directions: a model of human competence and the formation of effective teams based on hard & soft skills using artificial intelligence.","",""
0,"","ACTIVITY REPORT Project-Team Models and Algorithms for Artiﬁcial Intelligence",2022,"","","","",88,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,0,1,"The expectation-maximization (EM) algorithm is a powerful computational technique for maximum likelihood estimation in incomplete data models. When the expectation step cannot be performed in closed form, a stochastic approximation of EM (SAEM) can be used. The convergence of the SAEM toward critical points of the observed likelihood has been proved and its numerical efﬁciency has been demonstrated. However, sampling from the posterior distribution may be intractable or have a high computational cost. Moreover, despite appealing features, the limit position of this algorithm can strongly depend on its starting one. To cope with this two issues, we propose in [11] new stochastic approximation version of the EM in which we do not sample from the exact distribution in the expectation phase of the procedure. We ﬁrst prove the convergence of this algorithm toward critical points of the observed likelihood. Then, we propose an instantiation of this general procedure to favor convergence toward global maxima. Experiments on synthetic and real data highlight the performance of this algorithm in comparison to the SAEM and the EM when feasible. of subject-speciﬁc weights characterizing partial membership across clusters. With this ﬂexibility come challenges in uniquely identifying, estimating, and interpreting the parameters. In [40], we propose a new class of Dimension-Grouped MMMs (Gro-M 3 s) for multivariate categorical data, which improve parsimony and interpretability. In Gro-M 3 s, observed variables are partitioned into groups such that the latent membership is constant for variables within a group but can differ across groups. Traditional latent class models are obtained when all variables are in one group, while traditional MMMs are obtained when each variable is in its own group. The new model corresponds to a novel decomposition of probability tensors. Theoretically, we derive transparent identiﬁability conditions for both the unknown grouping structure and model parameters in general settings. Methodologically, we propose a Bayesian approach for Dirichlet Gro-M3 s to inferring the variable grouping structure and estimating model parameters. Simulation results demonstrate good computational performance and empirically conﬁrm the identiﬁability results. We illustrate the new methodology through an application to a functional disability dataset. from this natural partition. In a Bayesian context, this is achieved by considering the Dirichlet cluster proportion prior parameter α as a regularisation term controlling the granularity of the clustering. This second step allows the exploration of the clustering at coarser scales and the ordering of the clusters an important output for the visual representations of the clustering results. The clustering results obtained with the proposed approach, on simulated as well as real settings, are compared with existing strategies and are shown to be particularly relevant. This work is implemented in the R package greed and Figure 2 illustrates the main idea of the method. In this applied work [19], we use the Fisher-EM algorithm for clustering for the unsupervised classiﬁcation of 702, 248 spectra of galaxies and quasars with resdshifts smaller than 0.25 that were retrieved from the Sloan Digital Sky Survey (SDSS) database, release 7. The spectra were ﬁrst corrected for the redshift, then wavelet-ﬁltered to reduce the noise, and ﬁnally binned to obtain about 1437 wavelengths per spectrum. Fisher-EM, an unsupervised clustering discriminative latent mixture model algorithm, was applied on these corrected spectra, considering the full set as well as several subsets of 100,000 and 300,000 spectra. The optimum number of classes given by a penalized likelihood criterion is 86 classes, the 37 most populated ones gathering 99% of the sample. These classes are established from a subset of 302144 spectra. Using several cross-validation techniques we ﬁnd that this classiﬁcation is in agreement with the results obtained on the other subsets with an average misclassiﬁcation error of about 15%. The large number of very small classes tends to increase this error rate. This is the ﬁrst time that an automatic, objective and robust unsupervised classiﬁcation is established on such a large amount of spectra of galaxies. The mean spectra of the classes can be used as templates for a large majority of galaxies in our Universe. Figure 7 illustrates the obtained results. Recurrent Neural Networks, Deep linguistic patterns the of a of of to the is this linguistic that becomes valuable for our descriptive approach through deep as it allows us to observe complex lexico-grammatical structures, that potentially associate several levels of text representation in the same structure. The convolutional model used until now must therefore be adapted to integrate this additional information in order to obtain an even ﬁner description of the textual salience of a corpus. the relevant features used by the CNN to perform the classiﬁcation task. We empirically demonstrate the efﬁciency of our approach on corpora from two different languages: English and French. On all datasets, wTDS automatically encodes complex linguistic objects based on co-occurrences and possibly on grammatical and syntax analysis. relationships between the concepts in the metadata by analyzing the contrast between the concepts similarities in the Joconde’s semantic model and other vocabularies and we tried to improve the model prediction scores based on the semantic relations. Our results show that cross-fertilization between symbolic AI and machine learning can indeed provide the tools to address the challenges of the museum curators work describing the artwork pieces and searching for the relevant images. that combines a geometric approach for decision rules with existing post hoc solutions for machine learning models to generate an intuitive feature ranking tailored to the end user. We show that established model-agnostic approaches produce poor results in this framework. Figure 13 illustrates this work. Algorithms involving Gaussian processes or determinantal point processes typically require computing the determinant of a kernel matrix. Frequently, the latter is computed from the Cholesky decomposition, an algorithm of cubic complexity in the size of the matrix. We show that, under mild assumptions, it is possible to estimate the determinant from only a sub-matrix, with probabilistic guarantee on the relative error. In [37], we present an augmentation of the Cholesky decomposition that stops under certain conditions before processing the whole matrix. Experiments demonstrate that this can save a considerable amount of time while having an overhead of less than 5% when not stopping early. More generally, we present a probabilistic stopping strategy for the approximation of a sum of known length where addends are revealed sequentially. We do not assume independence between addends, only that they are bounded from below and decrease in conditional expectation. of there is a signiﬁcant from combining and audio data in detecting active speakers. either of the modalities can potentially mislead audiovisual fusion by inducing unreliable or deceptive information. outlines active speaker detection as a multi-objective learning problem to leverage best of each modalities using a novel self-attention, uncertainty-based multimodal fusion scheme. Results obtained show that the proposed multi-objective learning architecture outperforms traditional approaches in improving both mAP and AUC scores. We further demonstrate that our fusion strategy surpasses, in active speaker detection, other modality fusion methods reported in various disciplines. We ﬁnally show that the proposed method signiﬁcantly improves the state-of-the-art on the AVA-ActiveSpeaker dataset. This paper explores the problem of summarizing professional soccer matches as automatically as possible using both the event-stream data collected from the ﬁeld and the content broadcasted on TV. We have designed an architecture, introducing ﬁrst (1) a Multiple Instance Learning method that takes into account the sequential dependency among events and then (2) a hierarchical multimodal attention layer that grasps the importance of each event in an action [31]. We evaluate our approach on matches from two professional European soccer leagues, showing its capability to identify the best actions for automatic summarization by comparing with real summaries made by human operators. Figure 18 illustrates the general schema of the approach. We a coherent framework for studying longitudinal manifold-valued data. We introduce a Bayesian mixed-effects model which allows estimating both a group-representative piecewise-geodesic creating clusters of similar sentences. The ideal practice is to obtain a cluster with only positive blocks and another with only negative ones. Comparing to the supervised approach (Bag of words + Logistic Regression Classiﬁer) with its f1-score as 0.8234 and f2-score as 0.8316, we found that both S-Bert [58] (with a f1-score of 0.6250 and f2-score of 0.6192) and BioBert [57] (f1-score as 0.7004 and f2 as 0.6955) can achieves relatively good results and latter even outperformed the former due to its domain speciﬁc knowledge. around 13 billion euros per year to European citizens [52]. In the ﬁeld of healthcare insurance, in France the compulsory scheme detected over 261.2 million euros of fraudulent services in 2018, mainly due to healthcare professionals and healthcare establishments [50]. In the United States, according to the FBI, medicare fraud costs insurance companies between 21 billion and 71 billion US dollars per year [55]. In a context where reducing management costs is a real issue for healthcare insurers, the ﬁght against fraud is a real expectation of the customers of professionals in the sector so that everyone receives a fair return for their contributions. This stud","",""
7,"G. Mazzini","A System of Governance for Artificial Intelligence through the Lens of Emerging Intersections between AI and EU Law",2019,"","","","",89,"2022-07-13 09:22:00","","","","",,,,,7,2.33,7,1,3,"The work provides an overview and a comment of the Communication on Artificial Intelligence (AI) adopted by the European Commission in April 2018. By offering a bird’s-eye view of those law and policy areas potentially relevant for or affected by AI, the AI Communication sets the stage for understanding how pervasively and extensively AI is likely to be mainstreamed in our economies and societies. Whether it is about safety of products, liability, consumer protection, personal data protection or the foundational values, principles and rights on which the European project is based on, AI is very rapidly cutting across domains. The work identifies and investigates some of the many intersections between AI and EU law. Two main “disrupting” trends emerge.    According to the first trend, AI seems to exercise some pressure on existing regulatory frameworks, such as in the areas of product safety, liability and consumer protection.    As regards product safety, the main concerns seem to revolve around the unpredictability risk of AI. While certain factual characteristics (possibly limitations) of AI as it functions today cannot and should not be denied, the policy debate on AI safety should focus on what potential risks brought about by AI (or rather by specific AI applications) can be considered as socially acceptable when weighed against potential benefits. Even though the challenges posed by AI may generate some pressure on the existing EU product safety frameworks, it seems that EU safety law as a broader normative field has at its disposal a varied set of regulatory tools and approaches that can be relevant sources of inspiration and reference for a discussion on the safety of AI-powered products.    In the field of product liability, although one should note that the Product Liability Directive (PLD) is not necessarily the only tool that can be invoked by victims in case of risks and damages linked to AI-powered products, there seem to be elements suggesting that AI (in general or with regard to certain of its product specific applications) may put under stress the continued suitability of the technology neutral design of the PLD - or at least some provisions thereof - to the extent that the PLD is expected to apply, in its current form, to both “smart” and “non-smart” products.    The protection of consumers in the context of profiling and targeting practices in the business-to-consumers transactions is an area where the General Data Protection Regulation (GDPR) is particularly relevant. To the extent GDPR rules effectively enhance the data subjects’ empowerment vis-a-vis traders and/or curtail the ability of traders to engage in manipulative and unfair practices, then there may be less need for a fine-tuning of dedicated consumer law instruments (such as the Unfair Commercial Practices Directive, the Consumer Rights Directive and the Directive on Unfair Terms in Consumer Contracts) in order to take account of the specificities of commercial transactions mediated by sophisticated algorithms. At the same time, consumer protection could be an interesting testing ground for the potential of AI to empower consumers and civil society in general: the very same tools, techniques and methods used by companies to pursue their commercial interests could also serve the purpose to re-balance the traditional asymmetry of information, power and knowledge impacting negatively on consumers.    Contrary to what happens in the legal domains mentioned above, a different “disrupting” trend emerges in the field of the protection of personal data. Here, the several intersections between AI and the GDPR can essentially be framed in terms of the law disrupting certain technological uses and applications of AI. Due to the fact that AI uses and applications in the context of commercial transactions, and, more generally, of the algorithm-mediated economic, social and political life of individuals extensively rely on and process personal data, the GDPR emerges as a key piece of legislation in the space. While the data protection authorities and the courts will certainly specify and fine-tune its principles and provisions as appropriate, the GDPR presents itself as a robust framework poised to capture and effectively curb at least those uses and applications of AI that appear most egregious and intolerable in light of the degree of legal protection for individual rights and freedoms that is currently expected by citizens in our European society.    The work argues that, even if each legal or policy area where AI surfaces is confronted with distinct normative questions that may not necessarily be relevant for other areas, a connecting tissue is needed. This should take the form of a system of AI governance or cabine de regie which should combine - on an ongoing basis - up-to-date scientific and technical knowledge, internal legal and policy expertise specific to each sector and the authority to impart policy direction and to arbitrate, across the board, between the societal opportunities and the societal concerns that underlie the composite interaction between AI and the law.","",""
1,"D. Handelman, Corban G. Rivera, R. St. Amant, Emma Holmes, Andrew R. Badger, Bryanna Y. Yeh","Adaptive human-robot teaming through integrated symbolic and subsymbolic artificial intelligence: preliminary results",2022,"","","","",90,"2022-07-13 09:22:00","","10.1117/12.2618686","","",,,,,1,1.00,0,6,1,"As the autonomy of intelligent systems continues to increase, the ability of humans to maintain control over machine behavior, work effectively in concert with them, and trust them, becomes paramount. Ideally, a machine’s plan of action would be accessible to and understandable by human team members, and machine behavior would be modifiable in real time, in the field, to accommodate unanticipated situations. The ability of machines to adapt to new situations quickly and reliably based on both human input and autonomous learning has the potential to enhance numerous human-machine teaming scenarios. Our research focuses on the question, “Can robots become competent and adaptive teammates by emulating human skill acquisition strategies?” In this paper we describe the Robotic Skill Acquisition (RSA) cognitive architecture and show preliminary results of teaming experiments involving a human wearing an augmented reality headset and a quadruped robot performing tasks related to reconnaissance. The goal is to combine instruction and discovery by integrating declarative symbolic AI and reflexive neural network learning to produce robust, explainable and trusted robot behavior, adjustable autonomy, and adaptive human-robot teaming. Humans and robots start with a playbook of modifiable hierarchical task descriptions that encode explicit task knowledge. Neural network based feedback error learning enables human-directed behavior shaping, and reinforcement learning enables discovery of novel subtask control strategies. It is anticipated that modifications to and transitions between symbolic and subsymbolic processing will enable highly adaptive behavior in support of enhanced situational awareness and operational effectiveness of human-robot teams.","",""
427,"D. Ting, L. Pasquale, L. Peng, J. P. Campbell, Aaron Y. Lee, R. Raman, G. Tan, L. Schmetterer, P. Keane, T. Wong","Artificial intelligence and deep learning in ophthalmology",2018,"","","","",91,"2022-07-13 09:22:00","","10.1136/bjophthalmol-2018-313173","","",,,,,427,106.75,43,10,4,"Artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest in recent years. DL has been widely adopted in image recognition, speech recognition and natural language processing, but is only beginning to impact on healthcare. In ophthalmology, DL has been applied to fundus photographs, optical coherence tomography and visual fields, achieving robust classification performance in the detection of diabetic retinopathy and retinopathy of prematurity, the glaucoma-like disc, macular oedema and age-related macular degeneration. DL in ocular imaging may be used in conjunction with telemedicine as a possible solution to screen, diagnose and monitor major eye diseases for patients in primary care and community settings. Nonetheless, there are also potential challenges with DL application in ophthalmology, including clinical and technical challenges, explainability of the algorithm results, medicolegal issues, and physician and patient acceptance of the AI ‘black-box’ algorithms. DL could potentially revolutionise how ophthalmology is practised in the future. This review provides a summary of the state-of-the-art DL systems described for ophthalmic applications, potential challenges in clinical deployment and the path forward.","",""
1,"Massoud Sokouti, B. Sokouti","Applying the Science of Systematic Review and Meta-Analysis to Retrospective Artificial Intelligence Based Studies: The importance of performance evaluation",2019,"","","","",92,"2022-07-13 09:22:00","","","","",,,,,1,0.33,1,2,3,"The rationale behind the meta-analysis goes back to the 17th century studies of astronomy which then Karl Pearson performed a study based on meta-analysis using the data for typhoid inoculation in 1904. After, William Cochran applied this type of analysis to medical researches by taking the advantage of multiple previous studies. For more information and details on the history, the readers are referred to. To emerge the important role of systematic and metaanalysis studies even in the area of artificial intelligence systems, it is an anticipated that more reliable results can be driven from previous research studies alongside a simple review of such studies from which most of them may be ignored or not included as a matter of their nonsystematical type of reviews. The meta-analysis technique uses various types of statistics tools and methodologies to commonly derive a predictive diagnostic or non-diagnostic performance result of their compared corresponding approaches on the target defined disorders using information included in different datasets of previous studies. Although, a meta-analysis study can be regarded as a review of previous studies, however, it thoroughly targets not only the achieving results of those studies but also determine the in-common and non-commonpatterns of those researches as well as biases of the performance results whether they have been inserted intentionally or unintentionally. The importance of meta-analysis has been vastly discussed in medical sciences and therefore, been conducted rigorously through various studies, mostly on clinical trial ones. However, this technique is one of those less valued tools imported in to biomedical engineering studies and hence, their related algorithms mostly on the performance of artificial intelligence approaches. One of those studies to mention is the one performed on classification algorithms for pattern recognition by So Young Sohn in 1999 based on some in-house implemented statistics tools without considering the meta-analysis software. Moreover, in 2015,Horn et al have conducted a systematic review on functional brain imaging studies on assessing the familiarity of artificial neural networks and discussed their pros and cons in terms of their experimental conflicting results based on a meta-analysis on 68 publishedarticles. In another recent study, the role of real-time biomedical systems has been evaluated by a meta-analysis approach on 134 real-times papers in terms of computational complexity, delay and speed up considering various types of algorithms and hardware implementation. Recently, two types of systematic review and analysis have been performed which shows the potential non-mature trends of this approach in artificial intelligence based researches.In the first one the authors studied the performance of different machine learning algorithms for heart disease diagnosis; however, the metaanalysis part was not performed due to the existence of heterogeneity in the final included studies through the PRISMA (Preferred reporting items for systematic reviews and meta-analyses) checklist. And in the second one, the performance of several DNA based encryption algorithms based according to the results obtained from previous publications has been proposed where, it has been found out that there were no improvements in the proposed algorithms and it has been suggested that a dataset of images should be available in order to test and evaluate the performance of methodologies. However, the methodologies should also be available for public use. Moreover, the analyses section can be carried out through a simple statistical student’s t test analysisor the metaanalysis procedure using available tools such as MetaDisc, MIX, and Meta-Analyst. While comparing the two environments (i.e., clinical and computational), there are in-common units for decision making in diagnosing symptoms which are human (brain system and some data) and computer (artificial intelligence systems and some data). This outstanding feature and the abovementioned examples makes the meta-analysis studies applicable to the researches performed based on artificial intelligence systems, too. This will open a new view on interactions between the results obtained from previous studies while considering their special algorithms, different datasets, and possible biases. One more thing to emphasize for the future research studies is on publicizing the datasets and the implemented algorithms in terms of web servers, Java, C++ and Matlab libraries or R packages to make the results re-generable using new datasets which make them more comparable with new designed methodologies to ease the metaanalysis robust studies. As, it is also clear, most of the webservers and datasets in the medical parts coupled with data derived from bioscience knowledge are publicly.","",""
12,"S. Craw, A. Aamodt","Case Based Reasoning as a Model for Cognitive Artificial Intelligence",2018,"","","","",93,"2022-07-13 09:22:00","","10.1007/978-3-030-01081-2_5","","",,,,,12,3.00,6,2,4,"","",""
0,"M. Muzammul","Education System re-engineering with AI (artificial intelligence) for Quality Im-provements with proposed model",2019,"","","","",94,"2022-07-13 09:22:00","","10.14201/adcaij2019825160","","",,,,,0,0.00,0,1,3,"Re-engineering (RE) of existing educational institutions (EI) with adoption of latest technology trends (LTT) in form of artificial intelligence (AI) can be great effective in term of quality systems. Increase in student’s strength in class and terrorist attacks on EI urged us to introduce such approach that can assure education quality. Class monitoring with heavy strength always remain major issue for teacher during lecture delivery. In this paper, we implemented reengineering using artificial intelligence based two theories of 1) Multi-face recognition (MFR) system 2) Facial expression recognition (FER) system. Both of these theories supported by intelligent techniques as principal component analysis (PCA), discrete wavelet transform (DWT) and k-nearest neighbor (KNN). After implementation of these intelligent techniques student’s attentiveness will increase. Our developed system can detect expressions like happiness, repulsion, fear, anger, and confusion. Student’s attentiveness score will be displayed on screen. Teacher can interpret on the basis of attentiveness %age. System decision making can be helpful for class continuity or short break. This system is also an application of an expert system (ES) and knowledge base system (KBS) for educational quality assurance. A similar monitoring system was imposed in china with Hikvision Digital Technology. Predations results proved monitoring can be best way for education quality.","",""
15,"Yun-he Pan","Special issue on artificial intelligence 2.0",2017,"","","","",95,"2022-07-13 09:22:00","","10.1631/FITEE.1710000","","",,,,,15,3.00,15,1,5,"With the ever-growing popularization of the Internet, universal existence of sensors, emergence of big data, development of e-commerce, rise of the information community, and interconnection and fusion of data and knowledge in human society, physical space, and cyberspace, the information environment surrounding artificial intelligence (AI) development has changed profoundly, leading to a new evolutionary stage: AI 2.0. The emergence of new technologies also promotes AI to a new stage (Pan, 2016). The next-generation AI, namely AI 2.0, is a more explainable, robust, open, and general AI with the following attractive merits: It effectively integrates data-driven machine learning approaches (bottom-up) with knowledge-guided methods (top-down). In addition, it can employ data with different modalities (e.g., visual, auditory, and natural language processing) to perform cross-media learning and inference. Furthermore, there will be a step from the pursuit of an intelligent machine to the hybridaugmented intelligence (i.e., high-level man-machine collaboration and fusion). AI 2.0 will also promote crowd-based intelligence and autonomous-intelligent systems. In the next decades, AI2.0 will probably achieve remarkable progress in aforementioned trends, and therefore significantly change our cities, products, services, economics, environments, even how we advance our society. This special issue aims at reporting recent re-thinking of AI 2.0 from aforementioned aspects as well as practical methodologies, efficient implementations, and applications of AI 2.0. The papers in this special issue can be categorized into two groups. The first group consists of six review papers and the second group five research papers. In the first group, Zhuang et al. (2017) reviewed recent emerging theoretical and technological advances of AI in big data settings. The authors concluded that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI. Li W et al. (2017) described the concepts of crowd intelligence, and explained its relationship to the existing related concepts, e.g., crowdsourcing and human computation. In addition, the authors introduced four categories of representative crowd intelligence platforms. Peng et al. (2017) presented approaches, advances, and future directions in cross-media analysis and reasoning. This paper covers cross-media representation, mining, reasoning, and cross-media knowledge evolution. Tian et al. (2017) reviewed the state-of-the-art research of the perception in terms of visual perception, auditory perception, and speech perception. It also covered perceptual information processing and learning engines. Zhang et al. (2017) introduced the trends in the development of intelligent unmanned autonomous systems. It covered unmanned vehicles, unmanned aerial vehicles, service robots, space robots, marine robots, and unmanned Editorial: Frontiers of Information Technology & Electronic Engineering www.zju.edu.cn/jzus; engineering.cae.cn; www.springerlink.com ISSN 2095-9184 (print); ISSN 2095-9230 (online) E-mail: jzus@zju.edu.cn","",""
9,"G. D. Gasperis, I. Chiari, N. Florio","AIML Knowledge Base Construction from Text Corpora",2013,"","","","",96,"2022-07-13 09:22:00","","10.1007/978-3-642-29694-9_12","","",,,,,9,1.00,3,3,9,"","",""
53,"S. Sikchi, S. Sikchi, M. S. Ali","Artificial Intelligence in Medical Diagnosis",2012,"","","","",97,"2022-07-13 09:22:00","","","","",,,,,53,5.30,18,3,10,"The logical thinking of medical practitioner involves a lot of subjective decision making and its complexity makes traditional quantitative approaches of analysis inappropriate. The computer based diagnostic tools and knowledge base certainly helps for early diagnosis of diseases. The intelligent decision making systems can appropriately handle both the uncertainty and imprecision. This paper discusses about the application potential of artificial intelligence in medical diagnosis. The fuzzy expert system has been presented specific to liver disease diagnosis.","",""
20,"FarzinPiltan, MarziehKamgari, SaeedZare, FatemehShahryarZadeh, M. Mansoorzadeh","Design Novel Model Reference Artificial Intelligence Based Methodology to Optimized Fuel Ratio in IC Engine",2013,"","","","",98,"2022-07-13 09:22:00","","10.5815/IJIEEB.2013.02.07","","",,,,,20,2.22,4,5,9,"In this research, model reference fuzzy based control is presented as robust controls for IC engine. The objective of the study is to design controls for IC engines without the knowledge of the boundary of uncertainties and dynamic information by using fuzzy model reference PD p lus mass of air while improve the robustness of the PD p lus mass of air control. A PD plus mass of air provides for eliminate the mass of air and ultimate accuracy in the presence of the bounded disturbance/uncertainties, although this methods also causes some oscillation. The fuzzy PD plus mass of air is proposed as a solution to the problems crated by unstability. Th is method has a good performance in presence of uncertainty.","",""
7,"Arwin Datumaya Wahyudi Sumari, A. S. Ahmad, Cognitive Artificial","The application of cognitive artificial intelligence within C4ISR framework for national resilience",2017,"","","","",99,"2022-07-13 09:22:00","","10.1109/ACDTJ.2017.8259600","","",,,,,7,1.40,2,3,5,"Cognitive Artificial Intelligence (CAI) is a new perspective in Artificial Intelligence (AI) which is aimed to emulate how human brain works in generating knowledge. Human becomes intelligent because of knowledge which grows over time in his brain. With comprehensive knowledge, he can understand the world (environment) and is able to make decision and or action on it. On the other hand, strategic decision which impacts to the continuance of having a nation and having state is a critical and crucial matter, and it should be done in precise and quick manner especially in the case of contingency and faced to mutiple-data multiple-decision-alternative problems. The most precise decision has to be based on the knowledge from extracted comprehensive information. In this paper we show you the application of CAI for National Security with Knowledge-Growing System (KGS) as the engine of decision making system. We apply the CAI to a framework called Cognitive Command, Control, Communications, Computers, Intelligence, Surveillance and Reconnaissance (C4ISR) with examples taken from a simulated of real-life case in the Defense-Security domain.","",""
92,"A. Annoni, P. Benczúr, P. Bertoldi, Blagoj Delipetrev, Giuditta De Prato, C. Feijóo, Enrique Fernández-Macías, E. Gutiérrez, M. Portela, H. Junklewitz, M. L. Cobo, B. Martens, Susana Nascimento, S. Nativi, Alexandre Pólvora, Jose Ignacio Sanchez Martin, Songuel Tolan, I. Tuomi, Lucia Vesnić Alujević","Artificial Intelligence: A European Perspective",2018,"","","","",100,"2022-07-13 09:22:00","","10.2760/11251","","",,,,,92,23.00,9,19,4,"We are only at the beginning of a rapid period of transformation of our economy and society due to the convergence of many digital technologies. Artificial Intelligence (AI) is central to this change and offers major opportunities to improve our lives. The recent developments in AI are the result of increased processing power, improvements in algorithms and the exponential growth in the volume and variety of digital data. Many applications of AI have started entering into our every-day lives, from machine translations, to image recognition, and music generation, and are increasingly deployed in industry, government, and commerce. Connected and autonomous vehicles, and AI-supported medical diagnostics are areas of application that will soon be commonplace. There is strong global competition on AI among the US, China, and Europe. The US leads for now but China is catching up fast and aims to lead by 2030. For the EU, it is not so much a question of winning or losing a race but of finding the way of embracing the opportunities offered by AI in a way that is human-centred, ethical, secure, and true to our core values. The EU Member States and the European Commission are developing coordinated national and European strategies, recognising that only together we can succeed. We can build on our areas of strength including excellent research, leadership in some industrial sectors like automotive and robotics, a solid legal and regulatory framework, and very rich cultural diversity also at regional and sub-regional levels. It is generally recognised that AI can flourish only if supported by a robust computing infrastructure and good quality data: â€¢ With respect to computing, we identified a window of opportunity for Europe to invest in the emerging new paradigm of computing distributed towards the edges of the network, in addition to centralised facilities. This will support also the future deployment of 5G and the Internet of Things. â€¢ With respect to data, we argue in favour of learning from successful Internet companies, opening access to data and developing interactivity with the users rather than just broadcasting data. In this way, we can develop ecosystems of public administrations, firms, and civil society enriching the data to make it fit for AI applications responding to European needs. We should embrace the opportunities afforded by AI but not uncritically. The black box characteristics of most leading AI techniques make them opaque even to specialists. AI systems are currently limited to narrow and well-defined tasks, and their technologies inherit imperfections from their human creators, such as the well-recognised bias effect present in data. We should challenge the shortcomings of AI and work towards strong evaluation strategies, transparent and reliable systems, and good human-AI interactions. Ethical and secure-by-design algorithms are crucial to build trust in this disruptive technology, but we also need a broader engagement of civil society on the values to be embedded in AI and the directions for future development. This social engagement should be part of the effort to strengthen our resilience at all levels from local, to national and European, across institutions, industry and civil society. Developing local ecosystems of skills, computing, data, and applications can foster the engagement of local communities, respond to their needs, harness local creativity and knowledge, and build a human-centred, diverse, and socially driven AI. We still know very little about how AI will impact the way we think, make decisions, relate to each other, and how it will affect our jobs. This uncertainty can be a source of concern but is also a sign of opportunity. The future is not yet written. We can shape it based on our collective vision of what future we would like to have. But we need to act together and act fast.","",""
0,"Ahmed Reda Ali, M. Jaya, E. A. Jones","Machine Learning Strategies for Accurate Log Prediction in Reservoir Characterization: Self-Calibrating Versus Domain-Knowledge",2021,"","","","",101,"2022-07-13 09:22:00","","10.2118/205602-ms","","",,,,,0,0.00,0,3,1,"  Petrophysical evaluation is a crucial task for reservoir characterization but it is often complicated, time-consuming and associated with uncertainties. Moreover, this job is subjective and ambiguous depending on the petrophysicist's experience. Utilizing the flourishing Artificial Intelligence (AI)/Machine Learning (ML) is a way to build an automating process with minimal human intervention, improving consistency and efficiency of well log prediction and interpretation. Nowadays, the argument is whether AI-ML should base on a statistically self-calibrating or knowledge-based prediction framework! In this study, we develop a petrophysically knowledge-based AI-ML workflow that upscale sparsely-sampled core porosity and permeability into continuous curves along the entire well interval.  AI-ML focuses on making predictions from analyzing data by learning and identifying patterns. The accuracy of the self-calibrating statistical models is heavily dependent on the volume of training data. The proposed AI-ML workflow uses raw well logs (gamma-ray, neutron and density) to predict porosity and permeability over the well interval using sparsely core data. The challenge in building the AI-ML model is the number of data points used for training showed an imbalance in the relative sampling of plugs, i.e. the number of core data (used as target variable) is less than 10%. Ensemble learning and stacking ML approaches are used to obtain maximum predictive performance of self-calibrating learning strategy.  Alternatively, a new petrophysical workflow is established to debrief the domain experience in the feature selection that is used as an important weight in the regression problem. This helps ML model to learn more accurately by discovering hidden relationships between independent and target variables. This workflow is the inference engine of the AI-ML model to extract relevant domain-knowledge within the system that leads to more accurate predictions.  The proposed knowledge-driven ML strategy achieved a prediction accuracy of R2 score = 87% (Correlation Coefficient (CC) of 96%). This is a significant improvement by R2 = 57% (CC = 62%) compared to the best performing self-calibrating ML models. The predicted properties are upscaled automatically to predict uncored intervals, improving data coverage and property population in reservoir models leading to the improvement of the model robustness. The high prediction accuracy demonstrates the potential of knowledge-driven AI-ML strategy in predicting rock properties under data sparsity and limitations and saving significant cost and time.  This paper describes an AI-ML workflow that predicts high-resolution continuous porosity and permeability logs from imbalanced and sparse core plug data. The method successfully incorporates new type petrophysical facies weight as a feature augmentation engine for ML domain-knowledge framework. The workflow consisted of petrophysical treatment of raw data includes log quality control, preconditioning, processing, features augmentation and labelling, followed by feature selection to impersonate domain experience.","",""
16,"S. Ulyanov","Quantum Fast Algorithm Computational Intelligence PT I: SW / HW Smart Toolkit",2019,"","","","",102,"2022-07-13 09:22:00","","10.30564/AIA.V1I1.619","","",,,,,16,5.33,16,1,3,"A new approach to a circuit implementation design of quantum algorithm gates for quantum massive parallel fast computing implementation is presented. The main attention is focused on the development of design method of fast quantum algorithm operators as superposition, entanglement and interference which are in general time-consuming operations due to the number of products that have to be performed. SW & HW support sophisticated smart toolkit of supercomputing accelerator of quantum algorithm simulation is described. The method for performing Grover’s interference without product operations as Benchmark introduced. The background of developed information technology is the ""Quantum / Soft Computing Optimizer"" (QSCOptKBTM) software based on soft and quantum computational intelligence toolkit. Quantum genetic and quantum fuzzy inference algorithm gate design considered. The quantum information technology of imperfect knowledge base self-organization design of fuzzy robust controllers for the guaranteed achievement of intelligent autonomous robot the control goal in unpredicted control situations is described.","",""
20,"A. Okon, S. Adewole, Emmanuel M. Uguma","Artificial neural network model for reservoir petrophysical properties: porosity, permeability and water saturation prediction",2020,"","","","",103,"2022-07-13 09:22:00","","10.1007/s40808-020-01012-4","","",,,,,20,10.00,7,3,2,"","",""
118,"Pengzhen Lu, Shengyong Chen, Yujun Zheng","Artificial Intelligence in Civil Engineering",2012,"","","","",104,"2022-07-13 09:22:00","","10.1155/2012/145974","","",,,,,118,11.80,39,3,10,"Artificial intelligence is a branch of computer science, involved in the research, design, and application of intelligent computer. Traditional methods for modeling and optimizing complex structure systems require huge amounts of computing resources, and artificial-intelligence-based solutions can often provide valuable alternatives for efficiently solving problems in the civil engineering. This paper summarizes recently developed methods and theories in the developing direction for applications of artificial intelligence in civil engineering, including evolutionary computation, neural networks, fuzzy systems, expert system, reasoning, classification, and learning, as well as others like chaos theory, cuckoo search, firefly algorithm, knowledge-based engineering, and simulated annealing. The main research trends are also pointed out in the end. The paper provides an overview of the advances of artificial intelligence applied in civil engineering.","",""
8,"P. Mittal, Y. Singh","Development of Intelligent Transportation System for Improving Average Moving and Waiting time with Artificial Intelligence",2016,"","","","",105,"2022-07-13 09:22:00","","10.17485/IJST/2016/V9I3/84156","","",,,,,8,1.33,4,2,6,"Background: Real time traffic control is an important tool of Intelligent Transportation System (ITS). The development of system for controlling the urban traffic dynamically provides not only the safety for traffic, but also saves the time, money and provides polluted free environment. This paper describes the development of dynamic and robust traffic management system based on fuzzy logic approach. Method: Knowledge based system have been extensively adopted as approach for real time decision making system. Findings: As the conventional dynamic controllers were used sensors which are having certain limitations, so these limitations can be overcome by vision sensors i.e. camera. Also image and vision computing plays an important role in monitoring and measuring the traffic density on road. Problems were identified with the current traffic control system at the intersection on road and this necessitated the design and implementation of a new system to solve the congestion problems. Improvements: The performance of the proposed framework is evaluated with LabVIEW and MATLAB test bed. The results of extensive simulations using the proposed approach indicate that the system improves the average moving time and decrease the average waiting time than the controllers with conventional sensors.","",""
4,"Donglin Jiang, Chen Shan, Zhihui Zhang","Federated Learning Algorithm Based on Knowledge Distillation",2020,"","","","",106,"2022-07-13 09:22:00","","10.1109/ICAICE51518.2020.00038","","",,,,,4,2.00,1,3,2,"Federated learning is a new scheme of distributed machine learning, which enables a large number of edge computing devices to jointly learn a shared model without private data sharing. Federated learning allows nodes to synchronize only the locally trained models instead of their own private data, which provides a guarantee for privacy and security. However, due to the challenges of heterogeneity in federated learning, which are: (1) heterogeneous model architecture among devices; (2) statistical heterogeneity in real federated dataset, which do not obey independent-identical-distribution, resulting in poor performance of traditional federated learning algorithms. To solve the problems above, this paper proposes FedDistill, a new distributed training method based on knowledge distillation. By introducing personalized model on each device, the personalized model aims to improve the local performance even in a situation that global model fails to adapt to the local dataset, thereby improving the ability and robustness of the global model. The improvement of the performance of local device benefits from the effect of knowledge distillation, which can guide the improvement of global model by knowledge transfer between heterogeneous networks. Experiments show that FedDistill can significantly improve the accuracy of classification tasks and meet the needs of heterogeneous users.","",""
7,"Han Xiao, Yidong Chen, X. Shi","Knowledge Graph Embedding Based on Multi-View Clustering Framework",2019,"","","","",107,"2022-07-13 09:22:00","","10.1109/TKDE.2019.2931548","","",,,,,7,2.33,2,3,3,"Knowledge representation is one of the critical problems in knowledge engineering and artificial intelligence, while knowledge embedding as a knowledge representation methodology indicates entities and relations in knowledge graph as low-dimensional, continuous vectors. In this way, knowledge graph is compatible with numerical machine learning models. Major knowledge embedding methods employ geometric translation to design score function, which is weak-semantic for natural language processing. To overcome this disadvantage, in this paper, we propose our model based on multi-view clustering framework, which could generate semantic representations of knowledge elements (i.e., entities/relations). With our semantic model, we also present an empowered solution to entity retrieval with entity description. Extensive experiments show that our model achieves substantial improvements against baselines on the task of knowledge graph completion, triple classification, entity classification, and entity retrieval.","",""
0,"Khadijeh Karamzadeh, H. Moharrami","Survey of robust artificial intelligence classifier proper for various digital data",2015,"","","","",108,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,2,7,"Artificial intelligence or machine intelligence should be considered as the vast domain of junction of many knowledge, sciences and old and new technics. Today, classification of documents is adopted extensively in information recovery for organizing documents. In the method of document supervised classification some correct information about documents that previously have been classified are available for us and based on these information we classify these documents. Thus, we will examine methods such as: expert systems, artificial neural network, Genetic algorithm and fuzzy logics and so on. In this project we examine documents thematically and then using existing algorithms we predict a theme for a new document.","",""
21,"Tiago Oliveira, P. Novais, J. Neves","Development and implementation of clinical guidelines: An artificial intelligence perspective",2014,"","","","",109,"2022-07-13 09:22:00","","10.1007/s10462-013-9402-2","","",,,,,21,2.63,7,3,8,"","",""
2,"J. R. Silva, J. Silva, T. Vaquero","Formal Knowledge Engineering for Planning: Pre and Post-Design Analysis",2020,"","","","",110,"2022-07-13 09:22:00","","10.1007/978-3-030-38561-3_3","","",,,,,2,1.00,1,3,2,"","",""
2,"T. Schmid","Using Learning Algorithms to Create, Exploit and Maintain Knowledge Bases: Principles of Constructivist Machine Learning",2020,"","","","",111,"2022-07-13 09:22:00","","","","",,,,,2,1.00,2,1,2,"Recently, interest has grown in connecting modern machine learning approaches with traditional expert systems. This can mean, e.g, to identify patterns with neural networks and integrate them with knowledge graphs. While such combined systems offer a variety of advantages, few domainindependent approaches are known to make a hybrid artificial intelligence applicable without human interaction. To this end, we present the implementation of a constructivist machine learning framework (conML). This novel paradigm uses machine learning to manage a knowledge base and thereby allows for both raw data-based and symbolic information processing on the same internal knowledge representation. Based on axioms for a constructivist machine learning, we describe which operations are required to create, exploit and maintain a knowledge base and how these operations may be implemented with machine learning techniques. The major practical obstacle in this approach is to implement an automated deconstruction process that avoids ambiguity, handles continuous learning and allows knowledge abstraction. As we demonstrate, however, these obstacles can be overcome and constructivist machine learning can be put into practice. Combining machine learning and knowledge engineering is currently considered a potential game changing advancement in artificial intelligence. Neural networks and other machine learning techniques have proven strength in adapting to highly complex patterns and relationships, but are unable to represent existing knowledge explicitly and in an abstract fashion as expert systems can. Expert systems, on the other hand, operate on human-understandable knowledge representations but are highly domain-specific and, moreover, unable to process real-world data directly as machine learning can. Therefore, it is expected that joining both fields will produce a hybrid artificial intelligence that is “explainable, compliant and grounded in domain knowledge” (Martin et al. 2019). Such systems may, e.g., be able to identify patterns with neural networks and integrate them with knowledge graphs (Subasic, Yin, and Lin 2019). Copyright 2020 held by the author(s). In A. Martin, K. Hinkelmann, H.-G. Fill, A. Gerber, D. Lenat, R. Stolle, F. van Harmelen (Eds.), Proceedings of the AAAI 2020 Spring Symposium on Combining Machine Learning and Knowledge Engineering in Practice (AAAI-MAKE 2020). Stanford University, Palo Alto, California, USA, March 23-25, 2020. In fact, the idea of a hybrid artificial intelligence has been discussed for more than 30 years (Gallant 1988; Hendler 1989; Skeirik 1990; Levey 1991; Morik et al. 1993). So far, however, most research in this field focuses on specific knowledge or application domains like medical diagnosis (Hudson, Cohen, and Anderson 1991; Karabatak and Ince 2009; Herrmann 1995). This is to a large extent due to the fact that knowledge bases are typically created manually, which is a highly time-consuming task that requires detailled knowledge of the domain (Kidd 2012). No less timeconsuming are exploitation and maintenance of knowledge bases, which are typical follow-up phases within the life cycle of a knowledge base. While some progress has been made in employing algorithms for these tasks, several major challenges for an automated management of knowledge bases are still considered unresolved (Martinez-Gil 2015). Considering recent performance advancements in machine learning, manually managed knowledge bases obviously constitute a serious bottleneck in creating efficient hybrid systems. For truely automated systems, however, an implementable semantic interface between inductive machine learning and deductive expert systems is required. To this end, we have introduced a constructivist machine learning paradigm (Schmid 2019) based on the concept of learnable models and their storage in a knowledge base. While machine learning is currently dominated by neuro-inspired approaches, constructivist theories root in educational research (Fox 2001) and, so far, few actual implementations have been proposed for a constructivist machine learning (Drescher 1989; Quartz 1993). Central challenge for putting this into practice is the implementation of an automated deconstruction process, which to the best of our knowledge has only once been addressed successfully (Schmid 2018). Based on this paradigm, we designed a prototype for a constructivist machine learning that employs a meta databased knowledge base. Here, we present the underlying operationalizations and concepts required to put constructivist machine learning into practice. The rest of the paper is organized as follows: In section I, we lay out guidelines for automated knowledge base management. In section II, we define Stachowiak-like models as building blocks for knowledge representations. In section III, we introduce principles for constructivist machine learning processes. In section IV, we summarize our approach and point out future goals. Data Set or Stream Representation Knowledge Base select learn integrate","",""
2,"Matthew Sills, P. Ranade, Sudip Mittal","Cybersecurity Threat Intelligence Augmentation and Embedding Improvement - A Healthcare Usecase",2020,"","","","",112,"2022-07-13 09:22:00","","10.1109/ISI49825.2020.9280482","","",,,,,2,1.00,1,3,2,"The implementation of Internet of Things (IoT) devices in medical environments, has introduced a growing list of security vulnerabilities and threats. The lack of an extensible big data resource that captures medical device vulnerabilities limits the use of Artificial Intelligence (AI) based cyber defense systems in capturing, detecting, and preventing known and future attacks. We describe a system that generates a repository of Cyber Threat Intelligence (CTI) about various medical devices and their known vulnerabilities from sources such as manufacturer and ICS-CERT vulnerability alerts. We augment the intelligence repository with data sources such as Wikidata and public medical databases. The combined resources are integrated with threat intelligence in our Cybersecurity Knowledge Graph (CKG) from previous research. The augmented graph embeddings are useful in querying relevant information and can help in various AI assisted cybersecurity tasks. Given the integration of multiple resources, we found the augmented CKG produced higher quality graph representations. The augmented CKG produced a 31% increase in the Mean Average Precision (MAP) value, computed over an information retrieval task.","",""
2265,"M. Negnevitsky","Artificial Intelligence: A Guide to Intelligent Systems",2001,"","","","",113,"2022-07-13 09:22:00","","","","",,,,,2265,107.86,2265,1,21,"From the Publisher:  Virtually all the literature on artificial intelligence is expressed in the jargon of commuter science, crowded with complex matrix algebra and differential equations. Unlike many other books on computer intelligence, this one demonstrates that most ideas behind intelligent systems are simple and straightforward. The book has evolved from lectures given to students with little knowledge of calculus, and the reader needs no prerequisites associated with knowledge of any programming language. The methods used in the book have been extensively tested through several courses given by the author.    The book provides an introduction to the field of computer intelligence, covering    rule-based expert systems,  fuzzy expert systems,  frame-based expert systems,  artificail neural networks,  evolutionary computation,  hybrid intelligent systems,  knowledge engineering,  data mining.      In a university setting the book can be used as an introductory course within computer science, information systems or engineering departments. The book is also suitable as a self-study guide for non-computer science professionals, giving access to the state of the art in knowledge-based systems and computational intelligence. Everyone who faces challenging problems and cannot solve them using traditional approaches can benefit","",""
1,"Chen Qiu, Guangyou Zhou, Z. Cai, Anders Søgaard","A Global–Local Attentive Relation Detection Model for Knowledge-Based Question Answering",2021,"","","","",114,"2022-07-13 09:22:00","","10.1109/TAI.2021.3068697","","",,,,,1,1.00,0,4,1,"Knowledge-based question answering (KBQA) is an essential but challenging task for artificial intelligence and natural language processing. A key challenge pertains to the design of effective algorithms for relation detection. Conventional methods model questions and candidate relations separately through the knowledge bases (KBs) without considering the rich word-level interactions between them. This approach may result in local optimal results. This article presents a global–local attentive relation detection model (GLAR) that utilizes the local module to learn the features of word-level interactions and employs the global module to acquire nonlinear relationships between questions and their candidate relations located in KBs. This article also reports on the application of an end-to-end retrieval-based KBQA system incorporating the proposed relation detection model. Experimental results obtained on two datasets demonstrated GLAR's remarkable performance in the relation detection task. Furthermore, the functioning of end-to-end KBQA systems was significantly improved through the relation detection model, whose results on both datasets outperformed even state-of-the-art methods. Impact Statement—Knowledge-based question answering (KBQA) aims at answering user questions posed over the knowledge bases (KBs). KBQA helps users access knowledge in the KBs more easily, and it works on two subtasks: entity mention detection and relation detection. While existing relation detection algorithms perform well on the global representation of questions and relations sequences, they ignore some local semantic information on interaction cases between them. The technology proposed in this article takes both global and local interactions into account. With superior improvement on two relation detection tasks and two KBQA end tasks, the technology provides more precise answers. It could be used in more applications, including intelligent customer service, intelligent finance, and others.","",""
31,"Francesco Calimeri, Michael Fink, Stefano Germano, G. Ianni, Christoph Redl, Anton Wimmer","Angry-HEX: An Artificial Player for Angry Birds Based on Declarative Knowledge Bases",2016,"","","","",115,"2022-07-13 09:22:00","","10.1109/TCIAIG.2015.2509600","","",,,,,31,5.17,5,6,6,"This paper presents the Angry-HEX artificial intelligent agent that participated in the 2013 and 2014 Angry Birds Artificial Intelligence Competitions. The agent has been developed in the context of a joint project between the University of Calabria (UniCal) and the Vienna University of Technology (TU Vienna). The specific issues that arise when introducing artificial intelligence in a physics-based game are dealt with a combination of traditional imperative programming and declarative programming, used for modeling discrete knowledge about the game and the current situation. In particular, we make use of HEX programs, which are an extension of answer set programming (ASP) programs toward integration of external computation sources, such as 2-D physics simulation tools.","",""
2,"L. Iliadis","Special issue of the 8th AIAI 2012 (Artificial Intelligence Applications and Innovations) international conference",2014,"","","","",116,"2022-07-13 09:22:00","","10.1007/s10462-013-9421-z","","",,,,,2,0.25,2,1,8,"","",""
1,"Divya, A. Jain, Gagandeep Singh","Classification of Big Data Through Artificial Intelligence",2015,"","","","",117,"2022-07-13 09:22:00","","","","",,,,,1,0.14,0,3,7,"By technology innovations, there has been a large increase within the utilization of Bigdata knowledge, joined of the foremost most well-liked styles of media thanks to its content richness, for several vital applications. To sustain Associate in Nursing current ascension of knowledge Bigdata, there's Associate in Nursing rising demand for a complicated content-based knowledge classification system. Thanks to the chop-chop increasing massive knowledge, abundant analysis effort has been dedicated to develop classification primarily based massive knowledge retrieval ways which may efficiently retrieve knowledge of interest. Considering the restricted man-power, it's abundant expected to develop retrieval ways that use options mechanically extracted from massive knowledge. Through Architecture-Algorithm co-design for Bigdata processing Applications, a scalable. Manycore processor consists of classification of heterogeneous cores with stream process capabilities, and zero-overhead inter-process communication through computer science with a hardware-software mechanism has been designed. This is often designed for achieving superior and low-power consumption, particularly thus on cut back access needed for Bigdata processing Applications. Keywords— classification , Bigdata , PBO(pollination based optimization ) , BBO(biogeography based optimization ) , Apriori. Divya et al, International Journal of Computer Science and Mobile Computing, Vol.4 Issue.8, August2015, pg. 17-25 © 2015, IJCSMC All Rights Reserved 18 INTRODUCTION Big data technologies are important in providing more accurate analysis, which may lead to more concrete decision-making resulting in greater operational efficiencies, cost reductions, and reduced risks for the business. To harness the power of big data, you would require an infrastructure that can manage and process huge volumes of structured and unstructured data in real time and can protect data privacy and security. There are various technologies in the market from different vendors including Amazon, IBM, Microsoft, etc., to handle big data. While looking into the technologies that handle big data, we examine the following two classes of technology: A. Operational Big Data This includes systems like Mongo DB that provide operational capabilities for real-time, interactive workloads where data is primarily captured and stored. NoSQL Big Data systems are designed to take advantage of new cloud computing architectures that have emerged over the past decade to allow massive computations to be run inexpensively and efficiently. This makes operational big data workloads much easier to manage, cheaper, and faster to implement. Some NoSQL systems can provide insights into patterns and trends based on realtime data with minimal coding and without the need for data scientists and additional infrastructure. B. Analytical Big Data This includes systems like Massively Parallel Processing (MPP) database systems and MapReduce that provide analytical capabilities for retrospective and complex analysis that may touch most or all of the data. MapReduce provides a new method of analyzing data that is complementary to the capabilities provided by SQL, and a system based on MapReduce that can be scaled up from single servers to thousands of high and low end machines. These two classes of technology are complementary and frequently deployed together. BENEFITS OF BIG DATA Big data is really critical to our life and its emerging as one of the most important technologies in modern world. Follow are just few benefits which are very much known to all of us: USING THE INFORMATION KEPT IN THE SOCIAL NETWORK LIKE FACEBOOK, THE MARKETING AGENCIES ARE LEARNING ABOUT THE RESPONSE FOR THEIR CAMPAIGNS, PROMOTIONS, AND OTHER ADVERTISING MEDIUMS. USING THE INFORMATION IN THE SOCIAL MEDIA LIKE PREFERENCES AND PRODUCT PERCEPTION OF THEIR CONSUMERS, PRODUCT COMPANIES AND RETAIL ORGANIZATIONS ARE PLANNING THEIR PRODUCTION. USING THE DATA REGARDING THE PREVIOUS MEDICAL HISTORY OF PATIENTS, HOSPITALS ARE PROVIDING BETTER AND QUICK SERVICE. REVIEW Behrouz et. al.[15] A combination of multiple classifiers leads to a significant improvement in classification performance. Furthermore, by learning an appropriate weighting of the features used via a genetic algorithm (GA), we further improve prediction accuracy. The GA is demonstrated to successfully improve the accuracy of combined classifier performance, about Divya et al, International Journal of Computer Science and Mobile Computing, Vol.4 Issue.8, August2015, pg. 17-25 © 2015, IJCSMC All Rights Reserved 19 10 To 12% when comparing to non-GA classifier. This method may be of considerable usefulness in identifying students at risk early, especially in very large classes, and allow the instructor to provide appropriate advising in a timely manner. Riccardo et al. [14] proposed cognitive, and behavioural aspects of distance students. Course Vis is presented in the paper, and several examples of pictorial representations generated by the tool. Luo et. al. [21] Efficient meaning for sampling of data, reduction of data also needed to develop. Newly develop mining technique and searching algorithms that are suitable for extracting more different or complex relationship between fields. Youssef M.ESSA et. al. [25] The proposed framework is developed by using mobile agent and MapReduce paradigm under Java Agent Development Framework (JADE). JADE is a promising middleware based on the agent paradigm because it supports generic services such as communication support, resource discovery, content delivery, data encoding and agents mobility. Indeed, there are seven reasons for using mobile agents as follows: (1) Reduce the network load, (2) Overcome network latency, (3) Encapsulate protocols, (4) Execute asynchronously and autonomously, (5) Adapt dynamically, (6) Naturally heterogeneous and robust, and","",""
5985,"David Silver, Julian Schrittwieser, K. Simonyan, Ioannis Antonoglou, Aja Huang, A. Guez, T. Hubert, Lucas baker, Matthew Lai, A. Bolton, Yutian Chen, T. Lillicrap, Fan Hui, L. Sifre, George van den Driessche, T. Graepel, D. Hassabis","Mastering the game of Go without human knowledge",2017,"","","","",118,"2022-07-13 09:22:00","","10.1038/nature24270","","",,,,,5985,1197.00,599,17,5,"","",""
5,"Runchi Zhang, Zhiyi Qiu","Optimizing hyper-parameters of neural networks with swarm intelligence: A novel framework for credit scoring",2020,"","","","",119,"2022-07-13 09:22:00","","10.1371/journal.pone.0234254","","",,,,,5,2.50,3,2,2,"Neural networks are widely used in automatic credit scoring systems with high accuracy and outstanding efficiency. However, in the absence of prior knowledge, it is difficult to determine the set of hyper-parameters, which makes its application limited in practice. This paper presents a novel framework of credit-scoring model based on neural networks trained by the optimal swarm intelligence (SI) algorithm. This framework incorporates three procedures. Step 1, pre-processing, including imputation, normalization, and re-ordering of the samples. Step 2, training, where SI algorithms optimize hyper-parameters of back-propagation artificial neural networks (BP-ANN) with the area under curve (AUC) as the evaluation function. Step 3, test, applying the optimized model in Step 2 to predict new samples. The results show that the framework proposed in this paper searches the hyper-parameter space efficiently and finds the optimal set of hyper parameters with appropriate time complexity, which enhances the fitting and generalization ability of BP-ANN. Compared with existing credit-scoring models, the model in this paper predicts with a higher accuracy. Additionally, the model enjoys a greater robustness, for the difference of performance between training and testing phases.","",""
19,"T. Anandharajan, G. Hariharan, K. K. Vignajeth, R. Jijendiran, Kushmita","Weather Monitoring Using Artificial Intelligence",2016,"","","","",120,"2022-07-13 09:22:00","","10.1109/CINE.2016.26","","",,,,,19,3.17,4,5,6,"Weather forecasting is rather a statistical measure than a binary decision. We intend to develop an intelligent weather predicting module since this has become a necessary tool. This tool considers measures such as maximum temperature, minimum temperature and rainfall for a sampled period of days and are analyzed. An intelligent prediction based on the available data is accomplished using machine learning techniques. The analysis and prediction is based on linear regression which predicts the next day's weather with good accuracy. An accuracy of more than 90% is obtained, based on the data set. Recent studies have reflected that machine learning techniques achieved better performance than traditional statistical methods. Machine learning, a branch of artificial intelligence has been proved to be a robust method in predicting and analyzing a given data set. The module plays a vital role in agricultural, industrial and logistical fields where the weather forecast is an important criterion.","",""
22,"Xi Lin, Jun Wu, A. Bashir, Jianhua Li, Wu Yang, Jalil Piran","Blockchain-Based Incentive Energy-Knowledge Trading in IoT: Joint Power Transfer and AI Design",2020,"","","","",121,"2022-07-13 09:22:00","","10.1109/jiot.2020.3024246","","",,,,,22,11.00,4,6,2,"Recently, edge artificial intelligence techniques (e.g., federated edge learning) are emerged to unleash the potential of big data from Internet of Things (IoT). By learning knowledge on local devices, data privacy-preserving and quality of service (QoS) are guaranteed. Nevertheless, the dilemma between the limited on-device battery capacities and the high energy demands in learning is not resolved. When the on-device battery is exhausted, the edge learning process will have to be interrupted. In this paper, we propose a novel Wirelessly Powered Edge intelliGence (WPEG) framework, which aims to achieve a stable, robust, and sustainable edge intelligence by energy harvesting (EH) methods. Firstly, we build a permissioned edge blockchain to secure the peer-to-peer (P2P) energy and knowledge sharing in our framework. To maximize edge intelligence efficiency, we then investigate the wirelessly-powered multi-agent edge learning model and design the optimal edge learning strategy. Moreover, by constructing a two-stage Stackelberg game, the underlying energy-knowledge trading incentive mechanisms are also proposed with the optimal economic incentives and power transmission strategies. Finally, simulation results show that our incentive strategies could optimize the utilities of both parties compared with classic schemes, and our optimal learning design could realize the optimal learning efficiency.","",""
1,"N. Howard, Naima Chouikhi, Ahsan Adeel, Katelyn Dial, Adam Howard, A. Hussain","BrainOS: A Novel Artificial Brain-Alike Automatic Machine Learning Framework",2020,"","","","",122,"2022-07-13 09:22:00","","10.3389/fncom.2020.00016","","",,,,,1,0.50,0,6,2,"Human intelligence is constituted by a multitude of cognitive functions activated either directly or indirectly by external stimuli of various kinds. Computational approaches to the cognitive sciences and to neuroscience are partly premised on the idea that computational simulations of such cognitive functions and brain operations suspected to correspond to them can help to further uncover knowledge about those functions and operations, specifically, how they might work together. These approaches are also partly premised on the idea that empirical neuroscience research, whether following on from such a simulation (as indeed simulation and empirical research are complementary) or otherwise, could help us build better artificially intelligent systems. This is based on the assumption that principles by which the brain seemingly operate, to the extent that it can be understood as computational, should at least be tested as principles for the operation of artificial systems. This paper explores some of the principles of the brain that seem to be responsible for its autonomous, problem-adaptive nature. The brain operating system (BrainOS) explicated here is an introduction to ongoing work aiming to create a robust, integrated model, combining the connectionist paradigm underlying neural networks and the symbolic paradigm underlying much else of AI. BrainOS is an automatic approach that selects the most appropriate model based on the (a) input at hand, (b) prior experience (a history of results of prior problem solving attempts), and (c) world knowledge (represented in the symbolic way and used as a means to explain its approach). It is able to accept diverse and mixed input data types, process histories and objectives, extract knowledge and infer a situational context. BrainOS is designed to be efficient through its ability to not only choose the most suitable learning model but to effectively calibrate it based on the task at hand.","",""
0,"A. Sarkar","QKSA: Quantum Knowledge Seeking Agent",2021,"","","","",123,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,1,1,"In this article we present the motivation and the core thesis towards the implementation of a Quantum Knowledge Seeking Agent (QKSA). QKSA is a general reinforcement learning agent that can be used to model classical and quantum dynamics. It merges ideas from universal artificial general intelligence, constructor theory and genetic programming to build a robust and general framework for testing the capabilities of the agent in a variety of environments. It takes the artificial life (or, animat) path to artificial general intelligence where a population of intelligent agents are instantiated to explore valid ways of modelling the perceptions. The multiplicity and survivability of the agents are defined by the fitness, with respect to the explainability and predictability, of a resource-bounded computational model of the environment. This general learning approach is then employed to model the physics of an environment based on subjective observer states of the agents. A specific case of quantum process tomography as a general modelling principle is presented. The various background ideas and a baseline formalism are discussed in this article which sets the groundwork for the implementations of the QKSA that are currently in active development.","",""
0,"A. Sarkar","J ul 2 02 1 QKSA : Quantum Knowledge Seeking Agent motivation , core thesis and baseline framework",2021,"","","","",124,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,1,1,"In this article we present the motivation and the core thesis towards the implementation of a Quantum Knowledge Seeking Agent (QKSA). QKSA is a general reinforcement learning agent that can be used to model classical and quantum dynamics. It merges ideas from universal artificial general intelligence, constructor theory and genetic programming to build a robust and general framework for testing the capabilities of the agent in a variety of environments. It takes the artificial life (or, animat) path to artificial general intelligence where a population of intelligent agents are instantiated to explore valid ways of modeling the perceptions. The multiplicity and survivability of the agents are defined by the fitness, with respect to the explainability and predictability, of a resource-bounded computational model of the environment. This general learning approach is then employed to model the physics of an environment based on subjective observer states of the agents. A specific case of quantum process tomography as a general modeling principle is presented. The various background ideas and a baseline formalism is discussed in this article which sets the groundwork for the implementations of the QKSA that are currently in active development. Section 2 presents a historic overview of the motivation behind this research In Section 3 we survey some general reinforcement learning models and bio-inspired computing techniques that forms a baseline for the design of the QKSA. Section 4 presents an overview of field of quantum artificial agents and the observer based operational theory that the QKSA aims to learn. In Section 5 and 6 we presents the salient features and a formal definition of our model. In Section 7 we present the task of quantum process tomography (QPT) as a general task to test our framework. In Section 8 we conclude the discussion with suggestive future directions for implementing the QKSA.","",""
1,"Kiet Van Nguyen, Phong Nguyen-Thuan Do, Nhat Duy Nguyen, T. Huynh, A. Nguyen, N. Nguyen","XLMRQA: Open-Domain Question Answering on Vietnamese Wikipedia-based Textual Knowledge Source",2022,"","","","",125,"2022-07-13 09:22:00","","10.48550/arXiv.2204.07002","","",,,,,1,1.00,0,6,1,". Question answering (QA) is a natural language understanding task within the fields of information retrieval and information extraction that has attracted much attention from the computational linguistics and artificial intelligence research community in recent years because of the strong development of machine reading comprehension-based models. A reader-based QA system is a high-level search engine that can find correct answers to queries or questions in open-domain or domain-specific texts using machine reading comprehension (MRC) techniques. The majority of advancements in data resources and machine-learning approaches in the MRC and QA systems, on the other hand, especially in two resource-rich languages such as English and Chinese. A low-resource language like Vietnamese has witnessed a scarcity of research on QA systems. This paper presents XLMRQA, the first Vietnamese QA system using a supervised transformer-based reader on the Wikipedia-based textual knowledge source (using the UIT-ViQuAD corpus), outperforming the two robust QA systems using deep neural network models: DrQA and BERTserini with 24.46% and 6.28%, respectively. From the results obtained on the three systems, we analyze the influence of question types on the performance of the QA systems.","",""
4,"Glaucia C. Pereira","Genomics and Artificial Intelligence Working Together in Drug Discovery and Repositioning: The Advent of Adaptive Pharmacogenomics in Glioblastoma and Chronic Arterial Inflammation Therapies",2017,"","","","",126,"2022-07-13 09:22:00","","10.1007/978-3-319-53880-8_11","","",,,,,4,0.80,4,1,5,"","",""
2,"Sankalp Khanna, A. Sattar, David Hansen","Advances in artificial intelligence research in health.",2012,"","","","",127,"2022-07-13 09:22:00","","10.4066/AMJ.2012.1352","","",,,,,2,0.20,1,3,10,"The business of health delivery is complex. Employing over 850,000 people, and delivering services to 21.3 million residents, the Australian health care system is currently strained to the maximum in dealing with increasing demand for services and an acute shortage of skilled professionals. The National e–Health Strategy drives a nationwide research agenda to provide the infrastructure and tools required to support the planning, management and delivery of health care services.    Deriving principles from the disciplines of computer science, mathematics, philosophy and physiology, and consisting of different fields, from machine vision to expert systems, the field of Artificial Intelligence (AI) deals with the creation of ""machines that can think"". Focused on traits of reasoning, knowledge representation, planning, learning, communication, perception and social intelligence, AI has been widely applied to augment the state of the art in Health Informatics.    This special issue reports on the latest developments in the field of AI motivated research in the health domain. The special issue arose from the inaugural Australian Workshop on Artificial Intelligence in Health (AIH 2011). It was held in conjunction with the 24th Australasian Joint Conference On Artificial Intelligence (AI2011), in Perth, Australia, in December 2011. The AIH 2011 workshop was a first of its kind, national initiative that aimed to bring together scholars and practitioners in the field of AI–driven health informatics to present and discuss their research, share their knowledge and experiences, define key research challenges and explore possible collaborations to advance e–Health development nationally and internationally. Therefore, the affiliation of AIH 2011 with AI2011 was both timely and mutually beneficial for the communities involved in these events.    AIH 2011 received 16 full paper submissions and each paper was reviewed by three program committee members. Six papers were accepted as full papers and five as short papers accompanied with posters.    The workshop brought together researchers from a variety of disciplines across various parts of the country and provided an excellent forum for discussion and exchange of ideas. In addition to presentation of papers, the workshop featured two keynote addresses, a poster session during lunch, and a panel discussion.    The first keynote address, “Using Artificial Intelligence to transform the management of Chronic Disease”, was delivered by Professor Michael Georgeff. In addition to presenting ongoing research efforts to apply AI techniques to better manage chronic disease, Professor Georgeff discussed the greatest innovations in healthcare from a medical practitioner’s viewpoint and focussed on how AI could be leveraged to transform healthcare.    The second keynote address, “Knowledge Acquisition Issues in Interpreting Laboratory Data”, was presented by Professor Paul Compton and posed a number of questions related to large–scale knowledge acquisition for decision– support systems in medicine, drawing lessons from experience in working with knowledge acquisition tools that support over 300 million diagnostics laboratory reports.    The workshop concluded with a panel discussion in which Professor Abdul Sattar and Professor Yogi Kanagasingam joined the keynote speakers to address the topic “AI for eHealth: 2012 and Beyond”. The discussion that ensued was very energetic and actively engaged audience interaction. Topics discussed ranged from the emerging contribution of AI over the past five decades, current issues with the marketing and uptake of AI–based solutions in mainstream healthcare, and the challenges for AI–based research and application in years to come. The panel and audience also acknowledged the key role a workshop like this would play in driving collaborative research efforts between AI and health informatics research communities.    All accepted papers were also invited to revise and submit their manuscripts for inclusion in this special issue of the Australasian Medical Journal (AMJ). Seven papers and a letter to the editor have been accepted for publication in the journal.    The first paper, by Bevan Koopman, Peter Bruza, Laurianne Sitbon and Michael Lawley, titled “Towards Semantic Search and Inference in Electronic Medical Records”, presents concept–based information retrieval for searching electronic medical records and demonstrates that the approach outperforms keyword–based search, working especially well for queries where the latter performs poorly. This paper was awarded the best paper prize at the workshop.    The second paper, by Kinzang Chhogyal, Abhaya Nayak, Rolf Schwitter and Abdul Sattar, titled “A Causal Model for Fluctuating Sugar Levels in Diabetes Patients”, investigates the use of fixed distance based belief revision to improve causal models. A simple scenario for fluctuating blood sugar levels in a diabetes patient is used to demonstrate the efficacy of this approach.    The third paper, by Alexander Krumpholz, David Hawking, Richard Jones, Tom Gedeon and Hugh Greville, titled “Automated Medical Literature Retrieval”, describes a system for the retrieval of relevant medical publications using queries generated automatically from data present in an electronic patient record. Integrated into an electronic record system, such a system would proactively support medical practitioners in the delivery of care.    The fourth paper, by Abeed Sarker, Diego Molla and Cecile Paris, titled “Extractive Summarisation of Medical Documents”, proposes a query focused approach for automatically summarising medical documents and helping medical practitioners find relevant information.    The fifth paper, by Diego Molla and Maria Elena Santiago– Martinez, titled “A Corpus for Evidence Based Summarisation”, presents a corpus of clinical questions and answers designed for training and testing automated text summarisers to support evidence–based medicine.    The sixth paper, by Di Xiao, Janardhan Vignarajan, Jane Lock, Shaun Frost, Mei–Ling Tay–Kearney and Yogi Kanagasingam, titled “Retinal Image Registration and Comparison for Clinical Decision Support”, proposes a set of accurate and robust retinal image registration solutions for longitudinal retinal image alignment and comparison    The seventh paper, by Amol Wagholikar, Maggie Fung and Colleen Nelson, titled “Improving Self–Care of Patients with Chronic Disease using Online Personal Health Record”, employs a case–based reasoning approach to self care for advanced prostate cancer patients in an online patient health record environment.    A letter to the editor, by Anthony Nguyen, Yue Kimi Sun, Laurianne Sitbon and Shlomo Geva, titled “Representation Of Assertions In Clinical Free Text Using SNOMED CT,” explores the use of the SNOMED CT terminology for representing medical concepts and their assertions in clinical free text. The authors demonstrate that that populating assertions as attribute values using SNOMED CT allows over 93% of assertions in their experimental dataset to be represented.    We hope that the breadth and diversity of the papers presented at the workshop and published in this special issue will foster further collaboration and AI driven research in health.    This workshop would not have been possible without the contributions of numerous fine people. First, we are greatly indebted to Professor Aditya Ghose, Professor Anthony Maeder, Professor Wayne Wobcke, Professor Mehmet Orgun, and Dr Yogesan (Yogi) Kanagasingam for their guidance and support. We would also like to thank the organising committee of the 24th Australasian Joint Conference on AI, the Institute of Integrated and Intelligent Systems, Griffith University for supporting the workshop, and the CSIRO Australian e–Health Research Centre for their support and sponsorship of travel scholarships and the best paper prize. Thanks are also due to Professor Moyez Jiwa and the AMJ for supporting the workshop and inviting accepted papers for inclusion into this special issue. Finally, we are indebted to the authors who responded to the invitation to submit their papers, and the reviewers who generously donated their time and expertise and provided very comprehensive reviews of the submitted papers.","",""
2,"D. Grejner-Brzezinska, C. Toth, J. N. Markiel, S. Moafipoor, K. Czarnecka","Integration of Image-Based and Artificial Intelligence Algorithms: A Novel Approach to Personal Navigation",2012,"","","","",128,"2022-07-13 09:22:00","","10.1007/978-3-642-20338-1_120","","",,,,,2,0.20,0,5,10,"","",""
1,"N. Zakaria, Rohayanti Hassan, M. R. Othman, Z. Zakaria, S. Kasim","A Review on Classification of the Urban Poverty Using the Artificial Intelligence Method",2017,"","","","",129,"2022-07-13 09:22:00","","10.18488/JOURNAL.2.2017.711.450.458","","",,,,,1,0.20,0,5,5,"Poverty and how it has been assessed and measured is a frequently discussed topic by policy makers and social developers. The identification process in poverty measurement is indeed essential towards acknowledging the poor in the population; hence this needs to be clarified. Malaysia measures poverty by means of poverty line, indicating the unidimensional and inflexible distribution of poor and non-poor especially in urban areas. Many researchers have used fuzzy logic to solve the problem of rigid poor/non-poor dichotomy. This current trend has been able to augment the gap between the rigid and inflexible classification of poor and non-poor. However, there are still several shortcomings that need attention. For instance, the classification of the poor in fuzzy logic that is based on the average income of households still does not cover on the different range of disadvantage on non-monetary items. Based on these trends, ANFIS is proposed to resolve on the highlighted issues. The winning features of ANFIS, which include on simplicity in implementation, understandable explanation facilities through fuzzy rules, and ease of incorporation of both linguistic and numeric knowledge for problem solving may help in producing better result in classification of the urban poor. Essentially, the neural network is proposed to complement the fuzzy system, hence overcoming the limitations of both fuzzy systems and neural networks. As such, ANFIS method is used in this study to better classify on the poor and non-poor compared to fuzzy rule-based system which is lacking in prediction error rate due to too many variables used. However, this method deteriorates from misclassified poverty indicators; hence this study proposed on ensemble ANFIS to produce more accurate and robust classification results. An ensemble model is usually employed to address the problems of over-fitting, high dimensionality or missing features in the training data. Generally, combining multiple classification models increases predictive performance compared to the use of an individual model alone. Therefore, based on these current trends, this study is aimed to do a review on classification of the urban poverty using the artificial intelligence method.","",""
15,"Wei-Tsong Wang, Su-Ying Wu","Knowledge management based on information technology in response to COVID-19 crisis",2020,"","","","",130,"2022-07-13 09:22:00","","10.1080/14778238.2020.1860665","","",,,,,15,7.50,8,2,2,"ABSTRACT COVID-19’s rapid spread has caused a global pandemic. Consequently, it is imperative that healthcare organisations conduct crisis management (CM) to cope with this calamity. This study presents a set of operational guidelines for healthcare organisations to launch effective countermeasures against such crises by means of effective knowledge management (KM) practices. Additionally, information-technology (IT) applications can significantly improve organisations’ CM and KM capabilities by enhancing organisational responsiveness and flexibility. This study thus aims to articulate how the use of innovative IT-enabled mechanisms (e.g., non-contact monitoring devices, intelligent robots, and telemedicine) can reduce the risk of exposure and leverage an artificial intelligence-based epidemic intelligence dashboard to support appropriate decision-making by taking the operation of healthcare organisations in Taiwan during COVID-19 crisis as an example. The research results demonstrate the effectiveness of the employment of IT-enabled KM practices in CM settings in terms of preventing or minimising undesirable crisis consequences.","",""
2,"M. S. Yakoot, A. Ragab, O. Mahmoud","Multi-Class Taxonomy of Well Integrity Anomalies Applying Inductive Learning Algorithms: Analytical Approach for Artificial-Lift Wells",2021,"","","","",131,"2022-07-13 09:22:00","","10.2118/206129-ms","","",,,,,2,2.00,1,3,1,"  Well integrity has become a crucial field with increased focus and being published intensively in industry researches. It is important to maintain the integrity of the individual well to ensure that wells operate as expected for their designated life (or higher) with all risks kept as low as reasonably practicable, or as specified. Machine learning (ML) and artificial intelligence (AI) models are used intensively in oil and gas industry nowadays. ML concept is based on powerful algorithms and robust database. Developing an efficient classification model for well integrity (WI) anomalies is now feasible because of having enormous number of well failures and well barrier integrity tests, and analyses in the database.  Circa 9000 dataset points were collected from WI tests performed for 800 wells in Gulf of Suez, Egypt for almost 10 years. Moreover, those data have been quality-controlled and quality-assured by experienced engineers. The data contain different forms of WI failures. The contributing parameter set includes a total of 23 barrier elements.  Data were structured and fed into 11 different ML algorithms to build an automated systematic tool for calculating imposed risk category of any well. Comparison analysis for the deployed models was performed to infer the best predictive model that can be relied on. 11 models include both supervised and ensemble learning algorithms such as random forest, support vector machine (SVM), decision tree and scalable boosting techniques. Out of 11 models, the results showed that extreme gradient boosting (XGB), categorical boosting (CatBoost), and decision tree are the most reliable algorithms. Moreover, novel evaluation metrics for confusion matrix of each model have been introduced to overcome the problem of existing metrics which don't consider domain knowledge during model evaluation.  The innovated model will help to utilize company resources efficiently and dedicate personnel efforts to wells with the high-risk. As a result, progressive improvements on business, safety, environment, and performance of the business. This paper would be a milestone in the design and creation of the Well Integrity Database Management Program through the combination of integrity and ML.","",""
82,"Zhongzhi Shi","Advanced Artificial Intelligence",2011,"","","","",132,"2022-07-13 09:22:00","","10.1142/7547","","",,,,,82,7.45,82,1,11,"Logic Foundation of Artificial Intelligence Constraint Reasoning Qualitative Reasoning Case-Based Reasoning Probabilistic Reasoning Inductive Learning Support Vector Machine Explanation-Based Learning Reinforcement Learning Rough Set Association Rules Knowledge Discovery Distributed Intelligence Evolutionary Computation.","",""
67,"B. Chandrasekaran, Ashok K. Goel","From numbers to symbols to knowledge structures: artificial intelligence perspectives on the classification task",1988,"","","","",133,"2022-07-13 09:22:00","","10.1109/21.7491","","",,,,,67,1.97,34,2,34,"The general information-processing task of classification is considered and reviewed from the perspectives of the knowledge-based-reasoning, pattern-recognition, and connectionist paradigms in artificial intelligence, paying special attention to knowledge-based classificatory problem solving. The authors trace the evolution of the mechanisms for classification as the computational complexity of the problem increases, from numerical parameter-setting schemes, through those using intermediate abstractions and then relations between symbols, and finally to complex symbolic structures that explicitly incorporate domain knowledge. >","",""
326,"Randall Davis, D. Lenat","Knowledge-based systems in artificial intelligence",1981,"","","","",134,"2022-07-13 09:22:00","","10.1016/s0736-5853(86)80076-4","","",,,,,326,7.95,163,2,41,"","",""
11,"Gaolei Li, M. Dong, L. Yang, K. Ota, Jun Wu, Jianhua Li","Preserving Edge Knowledge Sharing Among IoT Services: A Blockchain-Based Approach",2020,"","","","",135,"2022-07-13 09:22:00","","10.1109/TETCI.2019.2952587","","",,,,,11,5.50,2,6,2,"Edge computational intelligence, integrating artificial intelligence (AI) and edge computing into Internet of Things (IoT), will generate many scattered knowledge. To enable auditable and delay-sensitive IoT services, these knowledge will be shared among decentralized intelligent network edges (DINEs), end users, and supervisors frequently. Blockchain has a promising ability to provide a traceable, privacy-preserving and tamper-resistant ledger for sharing edge knowledge. However, due to the complicated environments of network edges, knowledge sharing among DINEs still faces many challenges. Firstly, the resource limitation and mobility of DINEs impede the applicability of existing consensus tricks (e.g., Poof of Work, Proof of Stake, and Paxos) of blockchain. Secondly, the adversaries may eavesdrop the content of edge knowledge or entice the blockchain to forks using some attacking models (like man-in-the-middle attack, denial of services, etc.). In this article, an user-centric blockchain (UCB) framework is proposed for preserving edge knowledge sharing in IoT. Significant superiorities of UCB benefit from the proof of popularity (PoP) consensus mechanism, which is more energy-efficient and fast. Security analysis and experiments based on Raspberry Pi 3 Model B demonstrate its feasibility with low block generating delay and complexity.","",""
150,"C. Yan, Liang Li, Chunjie Zhang, Bingtao Liu, Yongdong Zhang, Qionghai Dai","Cross-Modality Bridging and Knowledge Transferring for Image Understanding",2019,"","","","",136,"2022-07-13 09:22:00","","10.1109/TMM.2019.2903448","","",,,,,150,50.00,25,6,3,"The understanding of web images has been a hot research topic in both artificial intelligence and multimedia content analysis domains. The web images are composed of various complex foregrounds and backgrounds, which makes the design of an accurate and robust learning algorithm a challenging task. To solve the above significant problem, first, we learn a cross-modality bridging dictionary for the deep and complete understanding of a vast quantity of web images. The proposed algorithm leverages the visual features into the semantic concept probability distribution, which can construct a global semantic description for images while preserving the local geometric structure. To discover and model the occurrence patterns between intra- and inter-categories, multi-task learning is introduced for formulating the objective formulation with Capped-$\ell _{1}$ penalty, which can obtain the optimal solution with a higher probability and outperform the traditional convex function-based methods. Second, we propose a knowledge-based concept transferring algorithm to discover the underlying relations of different categories. This distribution probability transferring among categories can bring the more robust global feature representation, and enable the image semantic representation to generalize better as the scenario becomes larger. Experimental comparisons and performance discussion with classical methods on the ImageNet, Caltech-256, SUN397, and Scene15 datasets show the effectiveness of our proposed method at three traditional image understanding tasks.","",""
9,"R. Das, Ameya Godbole, S. Dhuliawala, M. Zaheer, A. McCallum","A Simple Approach to Case-Based Reasoning in Knowledge Bases",2020,"","","","",137,"2022-07-13 09:22:00","","10.24432/C52S3K","","",,,,,9,4.50,2,5,2,"We present a surprisingly simple yet accurate approach to reasoning in knowledge graphs (KGs) that requires \emph{no training}, and is reminiscent of case-based reasoning in classical artificial intelligence (AI).  Consider the task of finding a target entity given a source entity and a binary relation.  Our approach finds multiple \textit{graph path patterns} that connect similar source entities through the given relation, and looks for pattern matches starting from the query source.  Using our method, we obtain new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122.  We also demonstrate that our model is robust in low data settings, outperforming recently proposed meta-learning approaches.","",""
242,"A. Kitchen","Knowledge based systems in artificial intelligence",1985,"","","","",138,"2022-07-13 09:22:00","","10.1109/PROC.1985.13127","","",,,,,242,6.54,242,1,37,"","",""
0,"J. Fuchs","4th Workshop on Artificial Intelligence and Knowledge Based Systems for Space",1993,"","","","",139,"2022-07-13 09:22:00","","10.1017/S0269888900000217","","",,,,,0,0.00,0,1,29,"The Workshop on Artificial Intelligence and Knowledge-Based Systems for Space is a bi-annual event organized by the European Space Agency (ESA) at its Technology Centre (ESTEC) in Noordwijk, The Netherlands. It reflects the interest of the Agency in advanced technologies, including software. The 4th Workshop was held on May 17th-19th 1993, and was attended by over 100 industrial and research participants. The Workshop topics focussed on some interests of ESA, in particular: AI techniques in operational or quasi-operational applications; methodology, particularly Verification and Validation of KBSs; and Model-Based Reasoning, Knowledge Reuse and Planning/Scheduling.","",""
92,"Bo Göranzon, I. Josefson","Knowledge, Skill and Artificial Intelligence",1988,"","","","",140,"2022-07-13 09:22:00","","10.1007/978-1-4471-1632-5","","",,,,,92,2.71,46,2,34,"","",""
24,"Sebastian Bader, P. Hitzler, Steffen Hölldobler","The Integration of Connectionism and First-Order Knowledge Representation and Reasoning as a Challenge for Artificial Intelligence",2004,"","","","",141,"2022-07-13 09:22:00","","","","",,,,,24,1.33,8,3,18,"Intelligent systems based on first-order logic on the one hand, and on artificial neural networks (also called connectionist systems) on the other, differ substantially. It would be very desirable to combine the robust neural networking machinery with symbolic knowledge representation and reasoning paradigms like logic programming in such a way that the strengths of either paradigm will be retained. Current state-of-the-art research, however, fails by far to achieve this ultimate goal. As one of the main obstacles to be overcome we perceive the question how symbolic knowledge can be encoded by means of connectionist systems: Satisfactory answers to this will naturally lead the way to knowledge extraction algorithms and to integrated neural-symbolic systems.","",""
12,"Xinxin Jiang, Shirui Pan, Guodong Long, Fei Xiong, Jing Jiang, Chengqi Zhang","Cost-Sensitive Parallel Learning Framework for Insurance Intelligence Operation",2019,"","","","",142,"2022-07-13 09:22:00","","10.1109/TIE.2018.2873526","","",,,,,12,4.00,2,6,3,"Recent advancements in artificial intelligence are providing the insurance industry with new opportunities to create tailored solutions and services based on newfound knowledge of consumers, and the execution of enhanced operations and business functions. However, insurance data are heterogeneous, and imbalanced class distribution with low frequency and high dimensions, which presents four major challenges to machine learning in real-world business. Traditional machine learning algorithms can typically apply to standard data sets, which are normally homogeneous and balanced. In this paper, we focus on an efficient cost-sensitive parallel learning framework (CPLF) to enhance insurance operations with a deep learning approach that does not require preprocessing. Our approach comprises a novel, unified, end-to-end cost-sensitive parallel neural network that learns real-world heterogeneous data. A specifically designed cost-sensitive matrix then automatically generates a robust model for learning minority classifications, and the parameters of both the cost-sensitive matrix and the hybrid neural network are alternately but jointly optimized during training. We also study the CPLF-based architecture for a real-world insurance intelligence operation system, and demonstrate fraud detection and policy renewal experiments on this system. The results of comparative experiments on real-world insurance data sets reflecting actual business cases demonstrate the effectiveness of our design.","",""
101,"J. Mira, José Ramón Álvarez-Sánchez","Artificial Intelligence and Knowledge Engineering Applications: A Bioinspired Approach: First International Work-Conference on the Interplay Between Natural and Artificial Computation, IWINAC 2005, Las Palmas, Canary Islands, Spain, June 15-18, 2005, Proceedings, Part II",2005,"","","","",143,"2022-07-13 09:22:00","","10.1007/b137296","","",,,,,101,5.94,51,2,17,"","",""
0,"V. Lemaire, J. Lamirel, Pascal Cuxac","AIL Active and Incremental Learning August 27 , 2012 , Montpellier , France ECAI 2012 – 20 TH European Conference on Artificial Intelligence",2012,"","","","",144,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,3,10,"Most machine learning techniques assume, either explicitly or implicitly, that the data-generating process is stationary. This assumption guarantees that the model learnt during the initial training phase remains valid over time and that its performance is in line with our expectations. Unfortunately, this assumption does not truly hold in the real world representing, in many cases, a simplistic approximation of the reality. The talk will describe the Just-In-Time (JIT) approach that is a flexible tool implementing the detection/adaptation paradigm to cope with evolving processes. Solutions following this approach improve the knowledge about the model in stationary conditions by exploiting additional information coming from the field during the operational life. Differently, in nonstationary conditions, as soon as a change in the data-generating process is detected, the learnt model is discarded and a suitable one activated to keep the performance. As a valuable and challenging application of the proposed approach, JIT classifiers for concept drift will be detailed and discussed. Incremental Decision Tree based on order statistics Christophe Salperwyck1 and Vincent Lemaire2 Abstract. New application domains generate data which are not persistent anymore but volatile: network management, web profile modeling... These data arrive quickly, massively and are visible just once. Thus they necessarily have to be learnt according to their arrival orders. For classification problems online decision trees are known to perform well and are widely used on streaming data. In this paper, we propose a new decision tree method based on order statistics. The construction of an online tree usually needs summaries in the leaves. Our solution uses bounded error quantiles summaries. A robust and performing discretization or grouping method uses these summaries to provide, at the same time, a criterion to find the best split and better density estimations. This estimation is then used to build a naı̈ve Bayes classifier in the leaves to improve the prediction in the early learning stage. New application domains generate data which are not persistent anymore but volatile: network management, web profile modeling... These data arrive quickly, massively and are visible just once. Thus they necessarily have to be learnt according to their arrival orders. For classification problems online decision trees are known to perform well and are widely used on streaming data. In this paper, we propose a new decision tree method based on order statistics. The construction of an online tree usually needs summaries in the leaves. Our solution uses bounded error quantiles summaries. A robust and performing discretization or grouping method uses these summaries to provide, at the same time, a criterion to find the best split and better density estimations. This estimation is then used to build a naı̈ve Bayes classifier in the leaves to improve the prediction in the early learning stage.","",""
7,"Qianqian Song, Jing Su, Wei Zhang","scGCN: a Graph Convolutional Networks Algorithm for Knowledge Transfer in Single Cell Omics",2020,"","","","",145,"2022-07-13 09:22:00","","10.1101/2020.09.13.295535","","",,,,,7,3.50,2,3,2,"Single-cell omics represent the fastest-growing genomics data type in the literature and the public genomics repositories. Leveraging the growing repository of labeled datasets and transferring labels from existing datasets to newly generated datasets will empower the exploration of the single-cell omics. The current label transfer methods have limited performance, largely due to the intrinsic heterogeneity and extrinsic differences between datasets. Here, we present a robust graph-based artificial intelligence model, single-cell Graph Convolutional Network (scGCN), to achieve effective knowledge transfer across disparate datasets. Benchmarked with other label transfer methods on totally 30 single cell omics datasets, scGCN has consistently demonstrated superior accuracy on leveraging cells from different tissues, platforms, and species, as well as cells profiled at different molecular layers. scGCN is implemented as an integrated workflow as a python software, which is available at https://github.com/QSong-github/scGCN.","",""
4,"M. Gates, Mukesh Ambani","Non-Parametric Reasoning on Knowledge Bases",2020,"","","","",146,"2022-07-13 09:22:00","","","","",,,,,4,2.00,2,2,2,"We present a surprisingly simple yet accurate approach to reasoning in knowledge graphs (KGs) that requires no training, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). Consider the task of finding a target entity given a source entity and a binary relation. Our approach finds multiple graph path patterns that connect similar source entities through the given relation, and looks for pattern matches starting from the query source. Using our method, we obtain new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122. We also demonstrate that our model is robust in low data settings, outperforming recently proposed meta-learning approaches.","",""
6,"Yuchuan Fu, Changle Li, F. Yu, T. Luan, Yao Zhang","An Autonomous Lane-Changing System With Knowledge Accumulation and Transfer Assisted by Vehicular Blockchain",2020,"","","","",147,"2022-07-13 09:22:00","","10.1109/JIOT.2020.2994975","","",,,,,6,3.00,1,5,2,"Inappropriate lane following and changing behaviors of connected and autonomous vehicles (CAVs) can result in accidents, such as rear-end collision and side collision. To remedy that, the use of deep reinforcement learning (DRL) for autonomous driving decisions is currently a widely used promising solution. In this case, the accuracy and effectiveness of such a machine learning (ML) model is quite essential for this artificial intelligence (AI)-enabled CAVs. This article proposes a blockchain-based collective learning (BCL) framework for autonomous lane-changing systems. Four key issues, namely, learning efficiency, data security, users’ privacy, as well as communication burden, are addressed by applying collective learning, vehicular blockchain, and knowledge transfer. First, we model the lane-changing problem as a DRL process and learn the autonomous lane-changing strategy through the deep deterministic policy gradient (DDPG) algorithm. Second, a single CAV involves a limited number of driving scenarios, and the independent learning method has the problem of inefficiency. Therefore, we propose a collective learning framework to utilize the “collective intelligence” shared by CAVs. Third, a vehicular blockchain is then applied to ensure the security and privacy of the user and data. In addition, the introduction of the blockchain can incentivize more users to participate in collective learning. Finally, in order to accelerate the learning process and achieve higher level performance while further reducing the communication burden, we use the corresponding knowledge extracted from the ML model such as human learning, as privileged information for sharing instead of directly sharing local ML models. Extensive simulation results validate the effectiveness and efficiency of our proposal in terms of learning efficiency, driving safety, as well as system security and robustness.","",""
4,"Hongyu Li, D. Meng, Hong Wang, Xiaolin Li","Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework",2020,"","","","",148,"2022-07-13 09:22:00","","10.1109/ICBK50248.2020.00022","","",,,,,4,2.00,1,4,2,"With strict protections and regulations of data privacy and security, conventional machine learning based on centralized datasets is confronted with significant challenges, making artificial intelligence (AI) impractical in many mission-critical and data-sensitive scenarios, such as finance, government, and health. In the meantime, tremendous datasets are scattered in isolated silos in various industries, organizations, different units of an organization, or different branches of an international organization. These valuable data resources are well underused. To advance AI theories and applications, we propose a comprehensive framework (called Knowledge Federation- KF) to address these challenges by enabling AI while preserving data privacy and ownership. Beyond the concepts of federated learning and secure multi-party computation, KF consists of four levels of federation: (1) information level, low-level statistics and computation of data, meeting the requirements of simple queries, searching and simplistic operators; (2) model level, supporting training, learning, and inference; (3) cognition level, enabling abstract feature representation at various levels of abstractions and contexts; (4) knowledge level, fusing knowledge discovery, representation, and reasoning. We further clarify the relationship and differentiation between knowledge federation and other related research areas. We have developed a reference implementation of KF, called iBond Platform, to offer a production-quality KF platform to enable industrial applications in finance, insurance, marketing, and government. The iBond platform will also help establish the KF community and a comprehensive ecosystem and usher in a novel paradigm shift towards secure, privacy-preserving and responsible AI. As far as we know, knowledge federation is the first hierarchical and unified framework for secure multi-party computing (statistics, queries, searching, and low-level operations) and learning (training, representation, discovery, inference, and reasoning).","",""
2,"Joshua Ho, Chien-Min Wang","Explainable and Adaptable Augmentation in Knowledge Attention Network for Multi-Agent Deep Reinforcement Learning Systems",2020,"","","","",149,"2022-07-13 09:22:00","","10.1109/AIKE48582.2020.00031","","",,,,,2,1.00,1,2,2,"The scale of modem Artificial Intelligence systems has been growing and entering more research territories by incorporating Deep Learning (DL) and Deep Reinforcement Learning (DRL) methods. More specifically, multi-agent DRL methods have been widely applied to address the problems of high-dimensional computation, which interpret the conditions that real-world systems mainly encounter and the issues that require resolving. However, the current approaches of DL and DRL are often challenged for their untransparent and time-consuming modeling processes in their attempt to achieve a practical and applicable inference based on human-level perspective and acceptance. This paper presents an explainable and adaptable augmented knowledge attention network for multi-agent DRL systems, which uses game theory simulation to tackle the problem of non-stationarity at the beginning, while improving the learning exploration built upon the strategic ontology to achieve the learning convergence more efficiently for autonomous agents. We anticipate that our approach will facilitate future research studies and potential research inspections of emerging multi-agent DRL systems for increasingly complex and autonomous environments.","",""
2,"M. Mistro, Y. Sheng, Y. Ge, C. Kelsey, J. Palta, Jing Cai, Qiuwen Wu, F. Yin, Q. Wu","Knowledge Models as Teaching Aid for Training Intensity Modulated Radiation Therapy Planning: A Lung Cancer Case Study",2020,"","","","",150,"2022-07-13 09:22:00","","10.3389/frai.2020.00066","","",,,,,2,1.00,0,9,2,"Purpose: Artificial intelligence (AI) employs knowledge models that often behave as a black-box to the majority of users and are not designed to improve the skill level of users. In this study, we aim to demonstrate the feasibility that AI can serve as an effective teaching aid to train individuals to develop optimal intensity modulated radiation therapy (IMRT) plans. Methods and Materials: The training program is composed of a host of training cases and a tutoring system that consists of a front-end visualization module powered by knowledge models and a scoring system. The current tutoring system includes a beam angle prediction model and a dose-volume histogram (DVH) prediction model. The scoring system consists of physician chosen criteria for clinical plan evaluation as well as specially designed criteria for learning guidance. The training program includes six lung/mediastinum IMRT patients: one benchmark case and five training cases. A plan for the benchmark case is completed by each trainee entirely independently pre- and post-training. Five training cases cover a wide spectrum of complexity from easy (2), intermediate (1) to hard (2). Five trainees completed the training program with the help of one trainer. Plans designed by the trainees were evaluated by both the scoring system and a radiation oncologist to quantify planning quality. Results: For the benchmark case, trainees scored an average of 21.6% of the total max points pre-training and improved to an average of 51.8% post-training. In comparison, the benchmark case's clinical plans score an average of 54.1% of the total max points. Two of the five trainees' post-training plans on the benchmark case were rated as comparable to the clinically delivered plans by the physician and all five were noticeably improved by the physician's standards. The total training time for each trainee ranged between 9 and 12 h. Conclusion: This first attempt at a knowledge model based training program brought unexperienced planners to a level close to experienced planners in fewer than 2 days. The proposed tutoring system can serve as an important component in an AI ecosystem that will enable clinical practitioners to effectively and confidently use KBP.","",""
0,"X. Jin, Jianmin Jiang, G. Min","Managing computer files via artificial intelligence approaches",2009,"","","","",151,"2022-07-13 09:22:00","","10.1007/s10462-009-9129-2","","",,,,,0,0.00,0,3,13,"","",""
0,"Gregory A. Luhan","Scaling Intelligence",2021,"","","","",152,"2022-07-13 09:22:00","","10.1080/24751448.2021.1967048","","",,,,,0,0.00,0,1,1,"T A D 5 : 2 Intelligence without action is inert. As the solicited contributions to this volume demonstrate, actionable intelligence relies on a common ground from which architecture and allied disciplines can leverage depths and breadths of knowledge to mobilize new technologies. The Op/Positions essays examine preexisting local knowledge in historical places, enhance discovery through systems-based workflows, and foster the transformational shift from invisible smartness to holistic, design trade-offs that produce more humane and cooperative cities. As Jyoti Hosagrahar notes, place-intelligence provides current generations with a scalable and reflective framework that values the past, promotes deeper foundations, and connects resilient community design and well-being to informed decision-making. Similarly, Azam Khan posits a systems-based approach for leveraging existing knowledge to solve increasingly complex problems holistically. The emergent metaheuristic tools expand architectural design ability, enhance discovery, and yield more energy-efficient and less wasteful buildings. Norbert Streitz advocates for resetting priorities at an urban scale and generating principles that simultaneously privilege the individual and the collective. The resulting types of affordances and ethical alignments could balance data harvesting with people’s need for interactive, communicative, and cooperative spaces and places. The Research Methodology contributions critically examine a site’s latent potential and propose challenging new ways for testing and improving the lived condition at all scales. Whether at the intimate scale of one human-robot interaction or applied to industry-level protocols or full-scale testing scenarios, real-world applied research design necessitates collecting and analyzing large data sets. Jim Tørresen examines predictive intelligent system design, comprising ethical sensor data collection, robot interaction, and human-centric artificial intelligence to anticipate and respond to elderly care needs. Integrating artificial intelligence and problem-solving best practices can interactively adapt to a user’s needs and draw upon years of industry-based construction knowledge. Lukas Kirner, Elisa Lublasser, and Sigrid Brell-Cokcan developed enhanced methods for elevating existing construction industry processes through interdisciplinary collaboration, robot-assisted interaction, laboratory experimentation, factoryto-field investigation, and full-scale testing. The jump from laboratory experiments to full-scale prototyping requires the refinement of previous data exchanges and information flows to produce generalizable results. Maintaining quantitative and qualitative data research design, controlled trials, and procedural rigor requires close monitoring and comparison of real-time data collections and digital simulations. In their Details+ contribution, Jonathan Heppner and Thomas Robinson deployed intelligent testing on an innovative post-tensioned, gravity-resistant, and lateral force-resistant rocking wall system at full scale and detail level. The lab-tested results generated valuable insights into damage-resistant construction methods, informed broader building practices, and demonstrated that their previously unproven assembly could prevent massive failure and save lives. Increasing the use of enhanced digital/computational methods brings renewed attention to gaining greater control over the software and tools used to generate and validate design decisions at all scales. Re/Views addresses these issues, examines interoperable software platforms, compares gaming engines, integrates sensors, and surveys current, emerging, and projected use of autonomous robots across the AEC industry. Karen Kensek presents strategies for improving workflows and overcoming software limitations through customizable add-in solutions for existing Building Information Modeling processes. Enhanced parametric interoperability and data functionality, streamlined procedures, and verified code-compliance bolster deliberative intelligence. Christopher Morse compares game engines that combine visualization, communication, and design in robust, adaptable, flexible, real-time, and interactive environments. Immersive, customizable, connective, and cloud-based integration inform architectural research and professional practice. Peter Kerr investigates scalable interactions with technology, examining affordances and benefits of sensor nodes connected via intelligent Building Management Systems (iBMS). Alvise Simondetti, Nicholas Bachand, Aifric Delahunty, James Griffith, and Julius Sustarevas examine the unfolding paradigm shift toward autonomous robotics, artificial intelligence, and machine learning as architecture moves beyond task-specific operations to inform scalable and sustainable design that augment and complement human capabilities. As shown by these authors, in its performative function, and when viewed through the prismatic lenses of technology, architecture, and design, scaling intelligence successfully narrows the gap between empirical observation, applied research, and professional practice. Scaling Intelligence","",""
23,"J. Prentzas","Artificial Intelligence Methods in Early Childhood Education",2013,"","","","",153,"2022-07-13 09:22:00","","10.1007/978-3-642-29694-9_8","","",,,,,23,2.56,23,1,9,"","",""
3,"F. Lécué, B. Abeloos, Jonathan Anctil, Manuel Bergeron, Damien Dalla-Rosa, Simon Corbeil-Letourneau, Florian Martet, Tanguy Pommellet, L. Salvan, Simon Veilleux, M. Ziaeefard","Thales XAI Platform: Adaptable Explanation of Machine Learning Systems - A Knowledge Graphs Perspective",2019,"","","","",154,"2022-07-13 09:22:00","","","","",,,,,3,1.00,0,11,3,"Explanation in Machine Learning systems has been identified to be the main asset to have for large scale deployment of Artificial Intelligence (AI) in critical systems. Explanations could be example-, features-, semantics-based or even counterfactual to potentially action on an AI system; they could be represented in many different ways e.g., textual, graphical, or visual. All representations serve different means, purpose and operators. We built the first-of-its-kind XAI (eXplainable AI) platform for critical systems i.e., Thales XAI Platform which aims at serving explanations through various forms. This paper emphasizes on the semantics-based explanations for Machine Learning systems. 1 Explainable AI in Critical Systems Motivation: The current hype of Artificial Intelligence (AI) mostly refers to the success of Machine Learning (ML) and its sub-domain of deep learning. However industries operating with critical systems are either highly regulated, or require high level of certification and robustness. Therefore, such industry constraints do limit the adoption of non deterministic and ML systems. Answers to the question of explainability will be intrinsically connected to the adoption of AI in industry at scale. Indeed explanation, which could be used for debugging intelligent systems or deciding to follow a recommendation in real-time, will increase acceptance and (business) user trust. Explainable AI (XAI) is now referring to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. Focus: Thales XAI Platform is designed to provide explanation for a ML task (classification, regression, object detection, segmentation). Although Thales XAI Platform does provide different levels of explanation e.g., example-based, featuresbased, counterfactual using textual and visual representations, we emphasis only on the semantics-based explanation through knowledge graphs. ? Copyright c © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Critical Applications: From adapting a plane trajectory, stopping a train, refitting a boat to reconfiguring a satellite, all are examples of critical situations where explanation is a must-to-have to follow an AI system decision. 2 Why Knowledge Graphs for Explainable AI? State-of-the-Art Limitations: Most approaches limits explanation of ML systems to features involved in the data and model, or at best to examples, prototypes or counterfactuals. Explanation should go beyond correlation (features importance) and numerical similarity (local explanation). Opportunity: By expanding and linking initial (training, validation and test) data with entities in knowledge graphs, (i) context is encoded, (ii) connections and relations are exposed, and (iii) inference and causation are natively supported. Knowledge graphs are used for encoding better representation of data, structuring a ML model in a more interpretable way, and adopt a semantic similarity for local (instance-based) and global (model-based) explanation. 3 Thales XAI Platform: A Knowledge Graph Perspective (Semantic) Perspective: The platform is combining ML and reasoning functionalities to expose a human-like rational as explanation when (i) recognizing an object (in a raw image) of any class in a knowledge graph, (ii) predicting a link in a knowledge graph. Thales XAI Platform is using state-of-the-art Semantic Web tools for enriching input, output (class) data with DBpedia (4, 233, 000 resources) and domain-specific knowledge graphs, usually enterprise knowledge graphs. This is a crucial step for contextualizing training, validation, test data. Explainable ML Classifications: Starting from raw images, as unstructured data, but with class labels augmented with a domain knowledge graph, Thales XAI Platform relies on existing neural network architectures to build the most appropriate models. All confidence scores of output classes on any input image are updated based on the semantic description of the output classes. For instance, an input classified as a car will have a higher overall confidence score in case some properties of car in the knowledge graph are retrieved e.g., having wheels, being on a road. In addition the platform is embedding naturally explanation i.e., properties of the objects retrieved in both the raw data and knowledge graph. Explainable Relational Learning: Starting from relational data, structured as graph, and augmented with a domain knowledge graph, Thales XAI Platform relies on existing knowledge graph embeddings frameworks to build the most appropriate models. Explanation of any link prediction is retrieved by identifying representative hotspots in the knowledge graph i.e., connected parts of the graphs that negatively impact prediction accuracy when removed.","",""
11,"Du Zhang","Quantifying Knowledge Base Inconsistency via Fixpoint Semantics",2007,"","","","",155,"2022-07-13 09:22:00","","10.1007/978-3-540-87563-5_9","","",,,,,11,0.73,11,1,15,"","",""
263,"An He, K. Bae, T. Newman, J. Gaeddert, Kyouwoong Kim, R. Menon, L. Morales-Tirado, James J. Neel, Youping Zhao, J. Reed, W. Tranter","A Survey of Artificial Intelligence for Cognitive Radios",2010,"","","","",156,"2022-07-13 09:22:00","","10.1109/TVT.2010.2043968","","",,,,,263,21.92,26,11,12,"Cognitive radio (CR) is an enabling technology for numerous new capabilities such as dynamic spectrum access, spectrum markets, and self-organizing networks. To realize this diverse set of applications, CR researchers leverage a variety of artificial intelligence (AI) techniques. To help researchers better understand the practical implications of AI to their CR designs, this paper reviews several CR implementations that used the following AI techniques: artificial neural networks (ANNs), metaheuristic algorithms, hidden Markov models (HMMs), rule-based systems, ontology-based systems (OBSs), and case-based systems (CBSs). Factors that influence the choice of AI techniques, such as responsiveness, complexity, security, robustness, and stability, are discussed. To provide readers with a more concrete understanding, these factors are illustrated in an extended discussion of two CR designs.","",""
8,"R. Seidlová, J. Poživil, Jaromír Seidl","Marketing and business intelligence with help of ant colony algorithm",2019,"","","","",157,"2022-07-13 09:22:00","","10.1080/0965254X.2018.1430058","","",,,,,8,2.67,3,3,3,"Abstract Recently, there is increasing need of banks for targeting and acquiring new customers, for fraud detection in real time and for segmentation products through analysis of the customers. Doing it, they can serve their customers better, and can increase the effectiveness of the company. For this purpose, various data mining methods are used which enable extraction of interesting, nontrivial, implicit, previously unknown, and potentially useful patterns or knowledge from huge amounts of data. Traditional data mining methods include classification rule tasks, for their solution there are a number of methods. Among them can be mentioned, for example, Random forest algorithm or C4.5 algorithm. However, accuracy of these methods significantly reduces in the event that some data in databases is missing. These methods are always not optimal for very large databases. The aim of our work is to verify a possible solution of these problems by using the algorithm based on artificial ant colonies. This algorithm was successful in other areas. Therefore, we tested its applicability and accuracy in marketing and business intelligence and compared it with so far used methods. The experimental results showed that the presented algorithm is very effective, robust, and suitable for processing of very large files. It was also found that this algorithm overcomes the previously used algorithms in accuracy. Algorithm is easily implementable on different platforms and can be recommended for using in banking and business intelligence.","",""
0,"P. Grenier, I. Álvarez, Jean-Marie Roger, V. Steinmetz","ARTIFICIAL INTELLIGENCE IN WINE-MAKING L ’ INTELLIGENCE ARTIFICIELLE EN ŒNOLOGIE",2008,"","","","",158,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,4,14,"In this paper, some terms of Artificial Intelligence are defined. Some present and potential applications of knowledge based systems are presented in the field of wine-making. Areas of concern were: multi sensor fusion, prediction by model cooperation, and diagnosis. Artificial intelligence techniques can indeed be applied for aiding the wine-maker in his choices. They facilitate the combination between experience and recent progress in technology. When associated with statistical processing, they allow knowledge sources to be used more effectively. Beyond wine-making, the prospects of artificial intelligence are promising for research and food industry, especially for improving the robustness of measurement systems (multi-sensors, sensors interpreted or validated by models), and for process diagnosis (risk prediction, action proposal). Résumé : Certains termes d’Intelligence Artificielle (IA) sont définis dans cette publication. Quelques applications en cours ou potentielles de systèmes fondés sur la connaissance sont présentés dans le domaine de la vinification. Les domaines d’étude sont : la fusion multi-capteurs, la prédiction par coopération de modèles, et le diagnostic. Les techniques IA peuvent aider le vinificateur dans ses choix par une symbiose entre expérience et progrès technologiques. En association avec des traitements statistiques, ces techniques permettent une utilisation plus efficace des sources de connaissances. Au-delà de la vinification, les perpectives de l’intelligence artificielle sont prometteuses en industrie alimentaire, en particulier pour améliorer la robustesse d’un système de mesure (fustion de capteurs, validation d’un capteur par un modèle) ou pour élaborer un diagnostic sur l’évolution d’un procédé (prédiction de risques, proposition d’intervention).","",""
533,"R. Akerkar, P. Sajja","Knowledge Based Systems",2009,"","","","",159,"2022-07-13 09:22:00","","10.1007/978-3-319-17885-1_100645","","",,,,,533,41.00,267,2,13,"","",""
199,"R. Luckin, Wayne Holmes","Intelligence Unleashed: An argument for AI in Education",2016,"","","","",160,"2022-07-13 09:22:00","","","","",,,,,199,33.17,100,2,6,"This paper on artificial intelligence in education (AIEd) has two aims. The first: to explain to a non-specialist, interested, reader what AIEd is: its goals, how it is built, and how it works. The second: to set out the argument for what AIEd can offer teaching and learning, both now and in the future, with an eye towards improving learning and life outcomes for all. Computer systems that are artificially intelligent interact with the world using capabilities (such as speech recognition) and intelligent behaviours (such as using available information to take the most sensible actions toward a stated goal) that we would think of as essentially human. At the heart of artificial intelligence in education is the scientific goal to make knowledge, which is often left implicit, computationally precise and explicit. In other words, in addition to being the engine behind much ‘smart’ ed tech, AIEd is also designed to be a powerful tool to open up what is sometimes called the ‘black box of learning,’ giving us more fine-grained understandings of how learning actually happens. Although some might find the concept of AIEd alienating, the algorithms and models that underpin ed tech powered by AIEd form the basis of an essentially human endeavor. Using AIEd, teachers will be able to offer learners educational experiences that are more personalised, flexible, inclusive and engaging. Crucially, we do not see a future in which AIEd replaces teachers. What we do see is a future in which the extraordinary expertise of teachers is better leveraged and augmented through the thoughtful deployment of well designed AIEd. We have available, right now, AIEd tools that could support student learning at a scale previously unimaginable by providing one-on-one tutoring to every student, in every subject. Existing technologies also have the capacity to provide intelligent support to learners working in a group, and to create authentic virtual learning environments where students have the right support, at the right time, to tackle real-life problems and puzzles. In the near future, we expect that teaching and learning will increasingly be supported by the thoughtful application of AIEd tools. For example, by lifelong learning companions powered by AI that can accompany and support individual learners throughout their studies - in and beyond school - and new forms of assessment that measure learning while it is taking place, shaping the learning experience in real time. If we are ultimately successful, we predict that AIEd will help us address some of the most intractable problems in education, including achievement gaps and teacher retention. AIEd will also help us respond to the most significant social challenge that AI has already brought - the steady replacement of jobs and occupations with clever algorithms and robots. It is our view that this provides a new innovation imperative in education, which can be expressed simply: as humans live and work alongside increasingly smart machines, our education systems will need to achieve at levels that none have managed to date. True progress will require the development of an AIEd infrastructure. This will not, however, be a single monolithic AIEd system. Instead, it will resemble the marketplace that has developed for smartphone apps: hundreds and then thousands of individual AIEd components, developed in collaboration with educators, conformed to uniform international data standards, and shared with researchers and developers worldwide. These standards will also enable system-level data collation and analysis that will help us to learn much more about learning itself – and how to improve it. Moving forward, we will need to pay close attention to three powerful forces as we map the future of artificial intelligence in education, namely pedagogy, technology, and system change. Paying attention to the pedagogy will mean that the design of new edtech should always start with what we know about learning. It also means that the system for funding this work must be simultaneously opened up and refocused, moving away from isolated pockets of R&D and toward collaborative enterprises that prioritise areas known to make a real difference to teaching and learning. Paying attention to the technology will mean creating smarter demand for commercial grade AIEd products that work. It also means the development of a robust, component-based AIEd infrastructure, similar to the smartphone app marketplace, where researchers and developers can access standardised components that have been developed in collaboration with educators. Paying attention to system change will mean involving teachers, students, and parents in co-designing new tools, so that AIEd will appropriately address the inherent “messiness” of real classroom, university, and workplace learning environments. It also means the development of data standards that promote the safe and ethical use of data. Said succinctly, we need intelligent technologies that embody what we know about great teaching and learning, embodied in enticing consumer grade products, which are then used effectively in real-life settings that combine the best of human and machine. We do not underestimate the new-thinking, inevitable wrong-turns, and effort required to realise these recommendations. However, if we are to properly unleash the intelligence of AIEd, we must do things differently - via new collaborations, sensible funding, and (always) a keen eye on the pedagogy. The potential prize is too great to act otherwise.","",""
0,"Khalid Rabeyee","Computing intelligence technique and multiresolution data processing for condition monitoring",2019,"","","","",161,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,1,3,"Condition monitoring (CM) of rotary machines has gained increasing importance and extensive research in recent years. Due to the rapid growth of data volume, automated data processing is necessary in order to deal with massive data efficiently to produce timely and accurate diagnostic results. Artificial intelligence (AI) and adaptive data processing approaches can be promising solutions to the challenge of large data volume. Unfortunately, the majority of AI-based techniques in CM have been developed for only the post-processing (classification) stage, whereas the critical tasks including feature extraction and selection are still manually processed, which often require considerable time and efforts but also yield a performance depending on prior knowledge and diagnostic expertise.    To achieve an automatic data processing, the research of this PhD project provides an integrated framework with two main approaches. Firstly, it focuses on extending AI techniques in all phases, including feature extraction by applying Componential Coding Neural Network (CCNN) which has been found to have unique properties of being trained through unsupervised learning, capable of dealing with raw datasets, translation invariance and high computational efficiency. These advantages of CCNN make it particularly suitable for automated analyzing of the vibration data arisen from typical machine components such as the rolling element bearings which exhibit periodic phenomena with high non-stationary and strong noise contamination. Then, once an anomaly is detected, a further analysis technique to identify the fault is proposed using a multiresolution data analysis approach based on Double-Density Discrete Wavelet Transform (DD-DWT) which was grounded on over-sampled filter banks with smooth tight frames. This makes it nearly shift-invariant which is important for extracting non-stationary periodical peaks. Also, in order to denoise and enhance the diagnostic features, a novel level-dependant adaptive thresholding method based on harmonic to signal ratio (HSR) is developed and implemented on the selected wavelet coefficients. This method has been developed to be a semi-automated (adaptive) approach to facilitate the process of fault diagnosis. The developed framework has been evaluated using both simulated and measured datasets from typical healthy and defective tapered roller bearings which are critical parts of all rotating machines. The results have demonstrated that the CCNN is a robust technique for early fault detection, and also showed that adaptive DD-DWT is a robust technique for diagnosing the faults induced to test bearings. The developed framework has achieved multi-objectives of high detection sensitivity, reliable diagnosis and minimized computing complexity.","",""
11,"L. Lai, Chao-Chin Wu, Nien-Lin Hsueh, Liang-Tsung Huang, Shiow-Fen Hwang","An Artificial Intelligence Approach to Course Timetabling",2006,"","","","",162,"2022-07-13 09:22:00","","10.1142/S0218213008003868","","",,,,,11,0.69,2,5,16,"Course timetabling is a complex problem and cannot be dealt with using only a few general principles. Each actor (i.e. the administrator, the chairman, the instructor and the student) has his own objective, and these objectives are usually conflicting. The complicated relationships between time periods, classes, classrooms, and instructors make it difficult to attain a feasible solution. In this article, we propose an artificial intelligence approach that integrates expert systems and constraint programming to implement a course timetabling system. Expert systems are utilized to incorporate knowledge into the timetabling system and to provide the reasoning capability for knowledge deduction. The separation of the knowledge base, facts and the inference engine in expert systems provides greater flexibility to support changes. The constraint hierarchy is utilized to capture hard and soft constraints and to reason about constraints using constraint satisfaction and relaxation techniques. Moreover, object-oriented software engineering is applied to improve the development and maintenance of the course timetabling system. A course timetabling system in the Department of Computer Science and Information Engineering at National Changhua University of Education (NCUE) is used as an illustrate example for the proposed approach","",""
454,"P. Harmon, D. King","Expert systems: artificial intelligence in business",1985,"","","","",163,"2022-07-13 09:22:00","","10.2307/3105218","","",,,,,454,12.27,227,2,37,"Case Study: MYCIN: Varieties of Problem Solving Strategies The Anatomy of a Knowledge Base Anatomy of An Inference Engine MYCIN Reconsidered Languages and Tools for Knowledge Systems A Sampler of Knowledge Systems and Their Architectures How Knowledge Systems are Developed Near Futures: Knowledge Engineering in the Next Five Years Large Scale Knowledge Systems Near Futures: Intelligent Job Aids Not So Near Futures: Research Topics Likely to Bear Fruit in 5 Years or More Not So Near Futures: Intelligent Tutoring Systems Not So Near Futures: Planning and Preparing for the Knowledge Systems Revolution Appendixes","",""
624,"Wm To, T. Tryfonas, D. Farthing","Frontiers in Artificial Intelligence and Applications",2009,"","","","",164,"2022-07-13 09:22:00","","","","",,,,,624,48.00,208,3,13,"We trace the roots of ontology-drive information systems (ODIS) back to early work in artificial intelligence and software engineering. We examine the lofty goals of the Knowledge-Based Software Assistant project from the 80s, and pose some questions. Why didn't it work? What do we have today instead? What is on the horizon? We examine two critical ideas in software engineering: raising the level of abstraction, and the use of formal methods. We examine several other key technologies and show how they paved the way for today's ODIS. We identify two companies with surprising capabilities that are on the bleeding edge of today's ODIS, and are pointing the way to a bright future. In that future, application development will be opened up to the masses, who will require no computer science background. People will create models in visual environments and the models will be the applications, self-documenting and executing as they are being built. Neither humans nor computers will be writing application code. Most functionality will be created by reusing and combining pre-coded functionality. All application software will be ontology-driven.","",""
7,"Tatiana Tambouratzis, J. Giannatsis, A. Kyriazis, Panayiotis Siotropos","Applying the Computational Intelligence Paradigm to Nuclear Power Plant Operation",2020,"","","","",165,"2022-07-13 09:22:00","","10.4018/ijeoe.2020010102","","",,,,,7,3.50,2,4,2,"In the guise of artificial neural networks (ANNs), genetic/evolutionary computation algorithms (GAs/ECAs), fuzzy logic (FL) inference systems (FLIS) and their variants as well as combinations, the computational intelligence (CI) paradigm has been applied to nuclear energy (NE) since the late 1980s as a set of efficient and accurate, non-parametric, robust-to-noise as well as to-missing-information, non-invasive on-line tools for monitoring, predicting and overall controlling nuclear (power) plant (N(P)P) operation. Since then, the resulting CI-based implementations have afforded increasingly reliable as well as robust performance, demonstrating their potential as either stand-alone tools, or - whenever more advantageous - combined with each other as well as with traditional signal processing techniques. The present review is focused upon the application of CI methodologies to the - generally acknowledged as - key-issues of N(P)P operation, namely: control, diagnostics and fault detection, monitoring, N(P)P operations, proliferation and resistance applications, sensor and component reliability, spectroscopy, fusion supporting operations, as these have been reported in the relevant primary literature for the period 1990-2015. At one end, 1990 constitutes the beginning of the actual implementation of innovative, and – at the same time – robust as well as practical, directly implementable in H/W, CI-based solutions/tools which have proved to be significantly superior to the traditional as well as the artificial-intelligence-(AI)derived methodologies in terms of operation efficiency as well as robustness-to-noise and/or otherwise distorted/missing information. At the other end, 2015 marks a paradigm shift in terms of the emergent (and, swiftly, ubiquitous) use of deep neural networks (DNNs) over existing ANN architectures and FL problem representations, thus dovetailing the increasing requirements of the era of complex - as well as Big - Data and forever changing the means of ANN/neuro-fuzzy construction and application/performance. By exposing the prevalent CI-based tools for each key-issue of N(P)P operation, overall as well as over time for the given 1990-2015 period, the applicability and optimal use of CI tools to NE problems is revealed, thus providing the necessary know-how concerning crucial decisions that need to be made for the increasingly efficient as well as safe exploitation of NE.","",""
74,"Ning Wu, Elisabete A. Silva","Artificial Intelligence Solutions for Urban Land Dynamics: A Review",2010,"","","","",166,"2022-07-13 09:22:00","","10.1177/0885412210361571","","",,,,,74,6.17,37,2,12,"Artificial intelligence (AI) systems are widely accepted as a technology offering an alternative way to tackle complex and dynamic problems in urban studies. The goal of this article is a review of current literature in the field of planning and AI. The aim of this review is to increase the understanding of how AI approaches urban and land dynamics modeling processes and how, as a result, researchers can structure that knowledge and choose the correct approaches to embed in their models. For this purpose, the authors review the applications of AI techniques in urban land dynamics domain as well as the emerging challenges they face. The authors discuss hybrid AI systems as a need resulting from the trend in planning policy to develop more holistic approaches. The authors conclude that, although challenges exist, AI-based approaches offer promising solutions for urban and land dynamics.","",""
247,"J. Minker","Logic-Based Artificial Intelligence",2000,"","","","",167,"2022-07-13 09:22:00","","10.1007/978-1-4615-1567-8","","",,,,,247,11.23,247,1,22,"","",""
228,"G. Brewka","Artificial intelligence - a modern approach by Stuart Russell and Peter Norvig, Prentice Hall. Series in Artificial Intelligence, Englewood Cliffs, NJ",1996,"","","","",168,"2022-07-13 09:22:00","","10.1017/S0269888900007724","","",,,,,228,8.77,228,1,26,"Benferhat, S, Dubois D and Prade, H, 1992. ""Representing default rules in possibilistic logic"" In: Proc. of the 3rd Inter. Conf. on Principles of knowledge Representation and Reasoning (KR'92), 673-684, Cambridge, MA, October 26-29. De Finetti, B, 1936. ""La logique de la probabilite"" Actes du Congres Inter, de Philosophic Scientifique, Paris. (Hermann et Cie Editions, 1936, IV1-IV9). Driankov, D, Hellendoorn, H and Reinfrank, M, 1995. An Introduction to Fuzzy Control, Springer-Verlag. Dubois, D and Prade, H, 1988. ""An introduction to possibilistic and fuzzy logics"" In: Non-Standard Logics for Automated Reasoning (P Smets, A Mamdani, D Dubois and H Prade, editors), 287-315, Academic Press. Dubois, D and Prade, H, 1994. ""Can we enforce full compositionality in uncertainty calculi?"" In: Proc. 12th US National Conf. On Artificial Intelligence (AAAI94), 149-154, Seattle, WA. Elkan, C, 1994. ""The paradoxical success of fuzzy logic"" IEEE Expert August, 3-8. Lehmann, D and Magidor. M, 1992. ""What does a conditional knowledge base entail?"" Artificial Intelligence 55 (1) 1-60. Maung, 1,1995. ""Two characterizations of a minimum-information principle in possibilistic reasoning"" Int. J. of Approximate Reasoning 12 133-156. Pearl, J, 1990. ""System Z: A natural ordering of defaults with tractable applications to default reasoning"" Proc. of the 2nd Conf. on Theoretical Aspects of Reasoning about Knowledge (TARK'90) 121-135, San Francisco, CA, Morgan Karfman. Shoham, Y, 1988. Reasoning about Change MIT Press. Smets, P, 1988. ""Belief functions"" In: Non-Standard Logics for Automated Reasoning (P Smets, A Mamdani, D Dubois and H Prade, editors), 253-286, Academic Press. Smets, P, 1990a. ""The combination of evidence in the transferable belief model"" IEEE Trans, on Pattern Anal. Mach. Intell. 12 447-458. Smets, P, 1990b. ""Constructing the pignistic probability function in a context of uncertainty"" Un certainty in Artificial Intelligence 5 (M Henrion et al., editors), 29-40, North-Holland. Smets, P, 1995. ""Quantifying beliefs by belief functions: An axiomatic justification"" In: Procoj the 13th Inter. Joint Conf. on Artificial Intelligence (IJACT93), 598-603, Chambey, France, August 28-September 3. Smets, P and Kennes, R, 1994. ""The transferable belief model"" Artificial Intelligence 66 191-234.","",""
342,"Anthony Kulis","Bio-Inspired Artificial Intelligence: Theories, Methods, and Technologies",2009,"","","","",169,"2022-07-13 09:22:00","","10.12694/SCPE.V10I4.623","","",,,,,342,26.31,342,1,13,"Bio-Inspired Artificial Intelligence: Theories, Methods, and Technologies by D. Floreano and C. Mattiussi This is a book that bridges biological systems and computer science. For digital-based researchers, having this book which details the biological components of natural life and seamlessly integrates that knowledge into our digital realm is an essential asset. Each chapter is systematically introduces the reader to a biological system while easing them into the its computational counterpart. There are seven chapters covering evolution, cellular, neural, developmental, immune, behavioral, and collective systems. Chapter 1 introduces the fundamental concept of computational evolution as related to biological systems. This chapter starts with the basic concepts of evolutionary theory and progresses, covering everything from fitness functions to analog circuits. The following chapter presents the next logical step upwards in biology, cellular structures and systems. Again introducing the basics of life and progressing towards cellular automata.  Chapter 3 covers Neural Networks by introducing the Biological Nervous System, then the Artificial Neural Network. The core concepts to Neural Networks are detailed in a systematic and common-sense manner, introducing unsupervised learning, supervised learning, and reinforce learning, then progressing onto neural hardware and hybrid systems. In Chapter 4, the authors detail developmental systems, explaining how nature utilizes the cellular structures to how engineers can mimic nature. This theme of progression from biological introduction to digital computation is reproduced as a single voice through out each chapter. The fundamentals of Bio-Inspired Artificial Intelligence are well demonstrated, allowing for a novice researcher in this area to develop the necessary skills and have a firm grasp on this topic. Once the reader has a solid grasp of the building blocks of life, the authors present chapters related to larger systems. Of particular interest to my research is the chapter on Immune Systems. This chapter provides a fundamental understanding of the Human Immune System, detailing the finer points of immunological cellular structures, while introducing a slightly more than generalized immune response concept. After a lengthy introduction of human immunology, we are introduced to the core of Artificial Immune Systems, the Negative Selection Algorithm and Clonal Selection Algorithm. Each one of these algorithms is covered enough so that the reader is capable of understanding each respective algorithms strengths and limitations. For new researchers to Artificial Immune Systems, days of reading journal articles is summarized in these sections, allowing for intelligent and efficient decision making in choosing your next step of research. Chapter 6 and 7 provides the audience with behavior systems and collective systems, respectively. The behavioral systems covered in this book relate to aspects of AI, robots, and some machine learning. Once behavior is understood, collective and cooperative systems are covered. Optimization techniques of particle swarms, ant colonies, and topics derived for robotics are detailed and well explained. While this is not a textbook, is does cover the fundamental concepts required to research Bio-Inspired Artificial Intelligence. For myself, the quality of this book can simply be noted by the publishers, MIT Press. Many of the best books I have encountered in my studies have been published by MIT, and here is another. Floreano and Mattiussi have not let me down in their quality, albeit I do have some complaints. First, while the topics cover a solid breadth, the depth on detailing the computation side is limited. I would like to have seen either more depth in each chapter or a broader look at each chapters algorithms, but the book falls somewhere in the middle. My current research involves Danger Signals and their relationship to preventing Epidemic Attacks, so I would have like to seen more detail about Polly Matzinger's Danger Theory rather than one short paragraph saying that it is not universally accepted. While Immunologists may debate Danger Theory, novel algorithms have been developed off of the concept of Danger Theory and deserve a place in this book. Yet to counter my own argument, the authors do finish off each chapter with a Suggested Readings section outlining a series of excellent supplement papers to the chapters topics that would eventually lead the reader to these novel topics. Overall, if you are interested in this field, buy this book. You can find it online at MIT Press for a discounted price. This book will make an excellent addition to any computer researchers library. Anthony Kulis, Department of Computer Science, Southern Illinois University","",""
0,"Y. D. Valle, N. Hampton","APPLICATION OF ARTIFICIAL INTELLIGENCE TO THE PROBLEM OF SELECTING THE APPROPRIATE DIAGNOSTIC FOR CABLE SYSTEMS",2011,"","","","",170,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,2,11,"Cable System Management requires an assessment of the health of the cables system. It is increasingly common for the assessment of aged cable systems to be made through the application of diagnostics measurements. There are a plethora of these techniques and embodiments; such that even an informed user has great difficulty making a rational choice on the most appropriate technique. To aid this decision making a Knowledge Based System has been developed that takes the knowledge of many diverse experts and delivers a robust framework by which rational, reproducible and transparent choices may be made. This paper discusses the development of the system and provides a number of illustrative case studies.","",""
0,"D. L. Hall","A comparative analysis of guided vs. query-based intelligent tutoring systems (its) using a class - entity - relationship - attribute (cera) knowledge base",1987,"","","","",171,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,1,35,"One of the greatest problems facing researchers in the subfield of Artificial Intelligence known as Intelligent Tutoring Systems (ITS) is the selection of a knowledge base designs that will facilitate the modification of the knowledge base. The Class-Entity-Relationship-Attribute (CERA), proposed by R. P. Brazile, holds certain promise as a more generic knowledge base design framework upon which can be built robust and efficient ITS.  This study has a twofold purpose. The first is to demonstrate that a CERA knowledge base can be constructed for an ITS on a subset of the domain of Cretaceous paleontology and function as the ""expert module"" of the ITS. The second is to test the validity of the ideas that students guided through a lesson learn more factual knowledge, while those who explore the knowledge base that underlies the lesson through query at their own pace will be able to formulate their own integrative knowledge from the knowledge gained in their explorations and spend more time on the system.  This study concludes that a CERA-based system can be constructed as an effective teaching tool. However, while an ITS-treatment provides for statistically significant gains in achievement test scores, the type of treatment seems not to matter as much as time spent on task. This would seem to indicate that a query-based system which allows the user to progress at their own pace would be a better type of system for the presentation of material due to the greater amount of on-line computer time exhibited by the users.","",""
12,"K. Hinkelmann, Sajjad Ahmed, F. Corradini","Combining Machine Learning with Knowledge Engineering to detect Fake News in Social Networks - A Survey",2022,"","","","",172,"2022-07-13 09:22:00","","","","",,,,,12,12.00,4,3,1,"Due to extensive spread of fake news on social and news media it became an emerging research topic now a days that gained attention. In the news media and social media the information is spread highspeed but without accuracy and hence detection mechanism should be able to predict news fast enough to tackle the dissemination of fake news. It has the potential for negative impacts on individuals and society. Therefore, detecting fake news on social media is important and also a technically challenging problem these days. We knew that Machine learning is helpful for building Artificial intelligence systems based on tacit knowledge because it can help us to solve complex problems due to real word data. On the other side we knew that Knowledge engineering is helpful for representing expert‟s knowledge which people aware of that knowledge. Due to this we proposed that integration of Machine learning and knowledge engineering can be helpful in detection of fake news. In this paper we present what is fake news, importance of fake news, overall impact of fake news on different areas, different ways to detect fake news on social media, existing detections algorithms that can help us to overcome the issue, similar application areas and at the end we proposed combination of data driven and engineered knowledge to combat fake news. We studied and compared three different modules text classifiers, stance detection applications & fact checking existing techniques that can help to detect fake news. Furthermore, we investigated the impact of fake news on society. Experimental evaluation of publically available datasets and our proposed fake news detection combination can serve better in detection of fake news.","",""
3,"D. Schutzer","Applications of Artificial Intelligence to Military Communications",1983,"","","","",173,"2022-07-13 09:22:00","","10.1109/MILCOM.1983.4794808","","",,,,,3,0.08,3,1,39,"This paper explores the field of artificial intelligence with respect to its application to military communication design problems. In particular it is shown how natural processing languages and knowledge-based system technologies can be used to reduce the required communications capacity and to improve a communication systems robustness and tolerance of errors by trading-off computation for communication. These technologies are also shown to improve the security of a military communications system operation. Other applications of artificial technology include the use of expert system technology to the operation, control and maintenance, and training areas.","",""
1,"P. Grenier, I. Álvarez, Jean-Marie Roger, V. Steinmetz, P. Barré, J. Sablayrolles","ARTIFICIAL INTELLIGENCE IN WINE-MAKING",2000,"","","","",174,"2022-07-13 09:22:00","","10.20870/OENO-ONE.2000.34.2.1007","","",,,,,1,0.05,0,6,22,"In this paper, some terms of Artificial Intelligence are defined. Some present and potential applications of knowledge based systems are presented in the field of wine-making. Areas of concern were: multi sensor fusion, prediction by model cooperation, and diagnosis. Artificial intelligence techniques can indeed be applied for aiding the wine-maker in his choices. They facilitate the combination between experience and recent progress in technology. When associated with statistical processing, they allow knowledge sources to be used more effectively. Beyond wine-making, the prospects of artificial intelligence are promising for research and food industry, especially for improving the robustness of measurement systems (multi-sensors, sensors interpreted or validated by models), and for process diagnosis (risk prediction, action proposal).","",""
5,"M. Selfridge, D. J. Dickerson, S. F. Biggs","Cognitive Expert Systems and Machine Learning: Artificial Intelligence Research at the University of Connecticut",1987,"","","","",175,"2022-07-13 09:22:00","","10.1609/AIMAG.V8I1.577","","",,,,,5,0.14,2,3,35,"In order for next-generation expert systems to demonstrate the performance, robustness, flexibility, and learning ability of human experts, they will have to be based on cognitive models of expert human reasoning and learning. We call such next-generation systems cognitive expert systems. Research at the Artificial Intelligence Laboratory at the University of Connecticut is directed toward understanding the principles underlying cognitive expert systems and developing computer programs embodying those principles. The Causal Model Acquisition System (CMACS) learns causal models of physical mechanisms by understanding real-world natural language explanations of those mechanisms. The going Concern Expert ( GCX) uses business and environmental knowledge to assess whether a company will remain in business for at least the following year. The Business Information System (BIS) acquires business and environmental knowledge from in-depth reading of real-world news stories. These systems are based on theories of expert human reasoning and learning, and thus represent steps toward next-generation cognitive expert systems.","",""
1,"P. Jacobs","Text-based systems and information management: artificial intelligence confronts matters of scale",1994,"","","","",176,"2022-07-13 09:22:00","","10.1109/TAI.1994.346487","","",,,,,1,0.04,1,1,28,"Many of the more ambitious goals of artificial intelligence have proved unattainable because of the failure of the many small, successful systems to scale up. The general use of technologies such as natural language interfaces and expert systems has done little to alleviate the basic difficulties and overwhelming cost of knowledge engineering. At the same time, emerging text processing techniques, including data extraction from text and new text retrieval methods, offer a means of accessing stores of information many times larger than any organized knowledge base or database. Although knowledge acquisition from text is at the heart of the information management problem, interpreting text, paradoxically, requires large amounts of knowledge, mainly about the way words are used in context. In other words, before intelligent text processing systems can be trained to mine for useful knowledge, they must already have enough knowledge to interpret what they read. The point at which there is ""enough"", is still a matter of debate, as no real program seems close to having enough knowledge to achieve general human-like understanding. Current research in large-scale natural language processing has come, rightly, to focus on lexical acquisition as the key to future progress. Unfortunately, the current state of the art is quite far from the recipe for acquiring knowledge about words, because it leans too heavily on resources that are available, without consideration for what is needed.<<ETX>>","",""
1,"J. Delgado-Frias, W. Moore","A semantic network architecture for artificial intelligence processing",1989,"","","","",177,"2022-07-13 09:22:00","","10.1109/TAI.1989.65316","","",,,,,1,0.03,1,2,33,"A multiprocessor computer architecture for knowledge processing which is based on semantic network knowledge representation is described. The architecture has the capability of matching a semantic network query with the entire knowledge base in parallel; this in turn ensures that the best possible answer can be obtained. Applications in computer vision in general and in scene labeling in particular are described. It is shown that the introduction of weights to the links in the inference process gives some degree of fault tolerance and/or noise immunity. This architecture could have a significant impact on a wide range of artificial intelligence applications, such as knowledge bases, expert systems, high-level computer vision, and inference engines. The architecture might be implemented in wafer-scale integration technology.<<ETX>>","",""
1,"J. Delgado-Frias, W. Moore","A wafer-scale architecture for artificial intelligence",1989,"","","","",178,"2022-07-13 09:22:00","","10.1109/WAFER.1989.47543","","",,,,,1,0.03,1,2,33,"The architecture presented exploits the advantages of wafer-scale integration technology and has a defect-tolerant scheme to overcome silicon defects. It is in principle a two-dimensional array that is suited to process semantic network knowledge bases. The defect-tolerance approach is based on a combination of hardware redundancy and robust algorithms run on the architecture. The application that is presented here is the scene labeling that is used in computer vision. Due to the robustness of the scene labeling algorithms the machine can tolerate some hardware faults at run time.<<ETX>>","",""
0,"Jonathan Bean","Bridging knowledge and labor",2021,"","","","",179,"2022-07-13 09:22:00","","10.1145/3466164","","",,,,,0,0.00,0,1,1,"The artificial boundary between knowledge work and manual labor cuts two ways, affecting both the contours of the labor force and the potential for much-needed innovation. This boundary is artificial because even those in jobs that involve so-called menial labor do knowledge work. Grocery store cashiers, for example, interact with database systems (checkout registers) and perform calculations on the fly when products are mispriced. And this is in addition to the emotional labor involved in many service jobs: the cheery smile and small talk that can be creepy when replicated by a robot. For those who can choose, a career in knowledge work typically holds the promise of a higher income and greater status, bringing with it the trade-offs of geographic distance and potentially reduced job security. Meanwhile, the social ties in communities where jobs revolve around manual labor have been weakened, in part because younger generations have opted for a higher-status future doing knowledge work in distant cities. This pattern, while pronounced in the context of the U.S., is also visible in global flows of migrants for whom the wage differential for manual labor—as a housekeeper, construction worker, or health aide—leads many to earn money in another country, leaving family and community for years and sometimes decades. The other impact is not as immediately visible as geographic dislocation. As the example of Enzo illustrates, the line between manual labor and knowledge work is unclear. The explosion of cheap, everywhere computing, which futurists say has only just begun, suggests that it’s Enzo, an experienced machinist quoted in a report prepared by the Forrester consulting firm, felt out of place when he joined a new shop at age 56. He knew that he had changed over time, “but the machines had changed a lot more. They had gotten very high tech. The work had changed; there’s now a lot more prep: setting up the machine for the job, putting in codes, programming. But I wasn’t interested in learning the new machines. I hated coming every morning. Putting in the codes scared me; I thought I’d break something. For decades, I’d been a top performer at a top shop—but here, I was a slacker” [1]. As computational technology and artificial intelligence continue to seep into all aspects of everyday life and work, it’s not uncommon to hear concerns about robots replacing humans. This, according to the same Forrester report, is a misplaced fear. Not only do we lack the industrial capacity to produce enough robots to displace the world’s 340 million workers, robots lack the agility to deal with tasks involving fine motor skills, which are easily done by humans. Some jobs will surely be replaced by machines. My garbage and recycling, for example, are picked up by a robot arm, almost comical in its articulation, that is mounted to the side of a garbage truck. The two workers who used to wrangle the bin from the curb and hop on the truck between houses are no longer in sight, though there is still someone who at least appears to be driving the apparatus. Will this human, too, disappear, or will their presence be necessary to calm fears about such a massive and potentially destructive machine operating of its own accord? Putting a legitimate fear of physical safety aside, even highly skilled humans such as Enzo are not necessarily comfortable interacting with robots. While this is often attributed to a lack of skill, it’s a problem that’s equally rooted in Western culture, where what’s often referred to as knowledge work is held in tension with a broad category of work considered manual labor. Anyone who has watched a machinist in action knows that manual labor is only part of the job. A machinist draws from a robust knowledge base to configure the machines that cut thread into pipe, stamp and fold sheet metal, or operate a lathe, regardless of whether these tools are controlled by the direct manipulation of a skilled set of hands or the indirect manipulation of hands on a keyboard. The way these differences ripple into broader culture is especially apparent in American culture, where class status is inextricably linked to one’s work. The further the work is from manual labor, the higher the status. Elsewhere in the world, such as in India, this distinction is built into caste systems, where low-caste people have long been incorrectly regarded not only to be less intellectually capable than those born to a higher caste but also to prefer manual work. Aside from the deeply consequential problems engendered by caste and class, the fact remains that much work must be done by hand, and that leaving this work Bridging Knowledge and Labor","",""
0,"Zheng Xu, J. Abawajy","Editorial: Special issue on neural computing and applications in cyber intelligence: ATCI 2020",2021,"","","","",180,"2022-07-13 09:22:00","","10.1007/s00521-020-05653-5","","",,,,,0,0.00,0,2,1,"","",""
18,"D. Ovalle, Diana C. Restrepo, A. Montoya","Artificial Intelligence for Wireless Sensor Networks Enhancement",2010,"","","","",181,"2022-07-13 09:22:00","","10.5772/12962","","",,,,,18,1.50,6,3,12,"Whereas the main objective of Artificial Intelligence is to develop systems that emulate the intellectual and interaction abilities of a human being the Distributed Artificial Intelligence pursues the same objective but focusing on human being societies (O’Hare et al., 2006). A paradigm in current use for the development of Distributed Artificial Intelligence is based on the notion of multi-agent systems. A multi-agent system is formed by a number of interacting intelligent systems called agents, and can be implemented as a software program, as a dedicated computer, or as a robot (Russell & Norving, 2003). Intelligent agents in a multi-agent system interact among each other to organize their structure, assign tasks, and interchange knowledge. Concepts related to multi-agent systems, artificial societies, and simulated organizations, create a new and rising paradigm in computingwhich involves issues as cooperation and competition, coordination, collaboration, communication and language protocols, negotiation, consensus development, conflict detection and resolution, collective intelligence activities conducted by agents (e.g. problem resolution, planning, learning, and decision making in a distributed manner), cognitive multiple intelligence activities, social and dynamic structuring, decentralized administration and control, safety, reliability, and robustness (service quality parameters). Distributed intelligent sensor networks can be seen from the perspective of a system composed by multiple agents (sensor nodes), with sensors working among themselves and forming a collective system which function is to collect data from physical variables of systems. Thus, sensor networks can be seen as multi-agent systems or as artificial organized societies that can perceive their environment through sensors. But, the question is how to implement Artificial Intelligence mechanisms withinWireless Sensor Networks (WSNs)? There are two possible approaches to the problem: according to the first approach, designers have in mind the global objective to be accomplished and design both, the agents and the interaction mechanism of the multi-agent system. In the second approach, the designer conceives and constructs a set of self-interested agents whose then evolve and interact in a stable manner, in their structure, through evolutionary techniques for learning. The same difficulty applies when working with a WSN perspective seen from the 4","",""
8,"Tai T. L. Nguyen, Do-Van Nguyen, T. Le","Reinforcement Learning Based Navigation with Semantic Knowledge of Indoor Environments",2019,"","","","",182,"2022-07-13 09:22:00","","10.1109/KSE.2019.8919366","","",,,,,8,2.67,3,3,3,"Recent years have been witnessing a huge step of artificial intelligence towards being applied in autonomous robots. To build intelligent robots navigating in indoor environment, many research focus on deep reinforcement learning which help robot learn and plan by themselves. Different network architectures are proposed for training agents to navigate and find targeted objects in both real and simulated environments. Despite promising results, one key challenge remaining is that the agent has to perform well in unseen environments and objects. To solve this generalization problem, this work proposes a method using prior knowledge graph capturing relationships between target objects. Experiments on simulated environments show that not only the proposed method enhances the learning process but also significantly improves agents generalization. When compared to similar methods, proposed method has a competitive and even better performance while bringing computational advantages.","",""
3,"Haoran Wu, Wei Chen, Shuang Xu, Bo Xu","Counterfactual Supporting Facts Extraction for Explainable Medical Record Based Diagnosis with Graph Network",2021,"","","","",183,"2022-07-13 09:22:00","","10.18653/V1/2021.NAACL-MAIN.156","","",,,,,3,3.00,1,4,1,"Providing a reliable explanation for clinical diagnosis based on the Electronic Medical Record (EMR) is fundamental to the application of Artificial Intelligence in the medical field. Current methods mostly treat the EMR as a text sequence and provide explanations based on a precise medical knowledge base, which is disease-specific and difficult to obtain for experts in reality. Therefore, we propose a counterfactual multi-granularity graph supporting facts extraction (CMGE) method to extract supporting facts from irregular EMR itself without external knowledge bases in this paper. Specifically, we first structure the sequence of EMR into a hierarchical graph network and then obtain the causal relationship between multi-granularity features and diagnosis results through counterfactual intervention on the graph. Features having the strongest causal connection with the results provide interpretive support for the diagnosis. Experimental results on real Chinese EMR of the lymphedema demonstrate that our method can diagnose four types of EMR correctly, and can provide accurate supporting facts for the results. More importantly, the results on different diseases demonstrate the robustness of our approach, which represents the potential application in the medical field.","",""
1,"R. Kowalski","The Limitaions of Logic and Its Role in Artificial Intelligence",1989,"","","","",184,"2022-07-13 09:22:00","","10.1007/978-3-642-83397-7_22","","",,,,,1,0.03,1,1,33,"","",""
2,"Laura von Rueden, T. Wirtz, Fabian Hueger, Jan David Schneider, N. Piatkowski, C. Bauckhage","Street-Map Based Validation of Semantic Segmentation in Autonomous Driving",2021,"","","","",185,"2022-07-13 09:22:00","","10.1109/ICPR48806.2021.9413292","","",,,,,2,2.00,0,6,1,"Artificial intelligence for autonomous driving must meet strict requirements on safety and robustness, which motivates the thorough validation of learned models. However, current validation approaches mostly require ground truth data and are thus both cost-intensive and limited in their applicability. We propose to overcome these limitations by a model agnostic validation using a-priori knowledge from street maps. In particular, we show how to validate semantic segmentation masks and demonstrate the potential of our approach using OpenStreetMap. We introduce validation metrics that indicate false positive or negative road segments. Besides the validation approach, we present a method to correct the vehicle's GPS position so that a more accurate localization can be used for the street-map based validation. Lastly, we present quantitative results on the Cityscapes dataset indicating that our validation approach can indeed uncover errors in semantic segmentation masks.","",""
3,"M. G. Sánchez-Escribano","Engineering Computational Emotion - A Reference Model for Emotion in Artificial Systems",2017,"","","","",186,"2022-07-13 09:22:00","","10.1007/978-3-319-59430-9","","",,,,,3,0.60,3,1,5,"","",""
1,"Zhan Zhang, Y. Jiao, Mingxia Zhang, Bing Wei, Xiao Liu, Juan Zhao, Fengwei Tian, Jie Hu, Qin Zhang","AI-aided general clinical diagnoses verified by third-parties with dynamic uncertain causality graph extended to also include classification",2022,"","","","",187,"2022-07-13 09:22:00","","10.1007/s10462-021-10109-w","","",,,,,1,1.00,0,9,1,"","",""
1,"Jan Strohschein, A. Fischbach, Andreas Bunte, Heide Faeskorn-Woyke, N. Moriz, T. Bartz-Beielstein","Cognitive Capabilities for the CAAI in Cyber-Physical Production Systems",2020,"","","","",188,"2022-07-13 09:22:00","","10.1007/s00170-021-07248-3","","",,,,,1,0.50,0,6,2,"","",""
29,"A. Umbrico, A. Cesta, Gabriella Cortellessa, Andrea Orlandini","A Holistic Approach to Behavior Adaptation for Socially Assistive Robots",2020,"","","","",189,"2022-07-13 09:22:00","","10.1007/s12369-019-00617-9","","",,,,,29,14.50,7,4,2,"","",""
0,"C. Aggarwal","Machine Learning: The Inductive View",2021,"","","","",190,"2022-07-13 09:22:00","","10.1007/978-3-030-72357-6_6","","",,,,,0,0.00,0,1,1,"","",""
47706,"M. Garey, David S. Johnson","Computers and Intractability: A Guide to the Theory of NP-Completeness",1978,"","","","",191,"2022-07-13 09:22:00","","","","",,,,,47706,1084.23,23853,2,44,"Horn formulae play a prominent role in artificial intelligence and logic programming. In this paper we investigate the problem of optimal compression of propositional Horn production rule knowledge bases. The standard approach to this problem, consisting in the removal of redundant rules from a knowledge base, leads to an ""irredundant"" but not necessarily optimal knowledge base. We prove here that the number of rules in any irredundant Horn knowledge base involving n propositional variables is at most n 0 1 times the minimum possible number of rules. In order to formalize the optimal compression problem, we define a Boolean function of a knowledge base as being the function whose set of true points is the set of models of the knowledge base. In this way the optimal compression of production rule knowledge bases becomes a problem of Boolean function minimization. In this paper we prove that the minimization of Horn functions (i.e. Boolean functions associated to Horn knowledge bases) is...","",""
0,"Zixuan Li, Xiaolong Li","Target Tracking Research Hotspots and Frontier Trends Based on Citespace",2021,"","","","",192,"2022-07-13 09:22:00","","10.1109/ICDSBA53075.2021.00106","","",,,,,0,0.00,0,2,1,"With the continuous development of artificial intelligence technology, target tracking is a hot problem in the field of computer vision, which has a wide range of application prospects in industrial, military, transportation, medical and other fields. In this paper, Citespace software is used to conduct descriptive statistical analysis and knowledge mapping analysis of target tracking based on domestic CNKI database literature, and to explore the development status and frontier trends in the field of target tracking in China. On this basis, point out three shortcomings of current research: low accuracy of target tracking in complex environments, poor real-time target tracking, and few application directions, and give suggestions to improve algorithm robustness, real-time, accelerate engineering implementation, and focus on future research trends.","",""
0,"Mingu Kang, HyeungKyeom Kim, Suchul Lee, Seokmin Han","Resilience against Adversarial Examples: Data-Augmentation Exploiting Generative Adversarial Networks",2021,"","","","",193,"2022-07-13 09:22:00","","10.3837/tiis.2021.11.013","","",,,,,0,0.00,0,4,1,"Recently, malware classification based on Deep Neural Networks (DNN) has gained significant attention due to the rise in popularity of artificial intelligence (AI). DNN-based malware classifiers are a novel solution to combat never-before-seen malware families because this approach is able to classify malwares based on structural characteristics rather than requiring particular signatures like traditional malware classifiers. However, these DNNbased classifiers have been found to lack robustness against malwares that are carefully crafted to evade detection. These specially crafted pieces of malware are referred to as adversarial examples. We consider a clever adversary who has a thorough knowledge of DNN-based malware classifiers and will exploit it to generate a crafty malware to fool DNN-based classifiers. In this paper, we propose a DNN-based malware classifier that becomes resilient to these kinds of attacks by exploiting Generative Adversarial Network (GAN) based data augmentation. The experimental results show that the proposed scheme classifies malware, including AEs, with a false positive rate (FPR) of 3.0% and a balanced accuracy of 70.16%. These are respective 26.1% and 18.5% enhancements when compared to a traditional DNNbased classifier that does not exploit GAN.","",""
0,"Tongjie Pan, Ziwei Huang, Yalan Ye, Yunfei Cheng, Wenwen He, Chong Wang","Joint Transfer Strategy for Cross-Domain Human Activity Recognition",2021,"","","","",194,"2022-07-13 09:22:00","","10.1109/ICET51757.2021.9451080","","",,,,,0,0.00,0,6,1,"Human activity recognition based on wearable sensors has been considered as an important work in the increasingly developed artificial intelligence and Internet of Things. Since acquiring enough activity labels is often expensive and time-consuming, an intuitive way is to leverage existing data from one sensor (source domain) to recognize target data from another sensor (target domain). Unfortunately, the data from different domains may have a large discrepancy. Existing approaches typically consider reducing the domain discrepancy to transfer knowledge. However, these approaches do not take full advantage of data structure, which is important for activity recognition. In this paper, we propose an effective method, named Joint Transfer Strategy (JTS). The method has high accuracy and robustness for cross-domain human activity recognition. Specifically, JTS first obtains pseudo labels for the target domain via a majority voting technique. Then, it leverages a joint transfer strategy to map the source and target domain data into a shared domain-invariant subspace and preserve the data structure information. Finally, an adaptive classifier is learned to label target data. Comprehensive experiments on three large public activity recognition datasets demonstrate that JTS outperforms other state-of-the-art methods in terms of classification accuracy.","",""
26,"R. Serra, R. Cucchiara","AI*IA 2009: Emergent Perspectives in Artificial Intelligence, XIth International Conference of the Italian Association for Artificial Intelligence, Reggio Emilia, Italy, December 9-12, 2009, Proceedings",2009,"","","","",195,"2022-07-13 09:22:00","","10.1007/978-3-642-10291-2","","",,,,,26,2.00,13,2,13,"","",""
0,"C. Ezeofor, Onengiye M. Georgewill","Development of Knowledge Based Smart Home",2019,"","","","",196,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,2,3,"this paper presents the development of Knowledge Based Smart Home. Smart home came to existence the very moment Internet of Things (IoT) Technology was invented. Internet of Things (IoT) has been experimentally proven to work satisfactorily by connecting simple appliances to it and were successfully controlled remotely through the internet.This invention has led to automation of homes, offices, industries, robotics, artificial intelligence etc. and today, more robust systems are being developed. Two basic ways of controlling and monitoring home remotely had been implemented from research. The first is by using GSM phone to control home appliances via sms commands and the second is through web application platform via networked and internet based computers. This work covers both ways and integrated voice recognition as another way of controlling home appliances. In order to accommodate sms based, web based and voice based, knowledge based system is implemented.This system integrates various communication techonologies such as Bluetooth BLE, Wi-Fi, GSM and Voice Recognition for easy communication with smart devices like Android smartphones, tablets, PDA etc. and personal computers. The system is made up of security system, control system and communication system which houses various sensors, actuators, microcontrollers such as raspberry pi 3, ESp 32, STM32, ESP8266-01, LCDs, etc. The graphical user interface (GUI) for mobile and computer web applications is designed using Eclipse and brackets IDEs. Python, java, C++ languages are used to write control codes for both the GUI and the embedded system chips (microcontrollers).The system can also function as an intelligent personal assistant (IPA), thus answering query and performing actions via voice commands using natural language user interface. This is to assist people like elderly, sick and disabled with basic tasks to makes household decisions through stored information in the knowledge base of the system. The system is implemented with the aid of Artificial Intelligence for the effective and efficient management of the home. The complete system was tested successfully.","",""
0,"Edgar L Reinoso-Peláez, D. Gianola, O. González-Recio","Genome-Enabled Prediction Methods Based on Machine Learning.",2022,"","","","",197,"2022-07-13 09:22:00","","10.1007/978-1-0716-2205-6_7","","",,,,,0,0.00,0,3,1,"","",""
0,"J. Long","Analysis of Insurance Marketing Planning Based on BD-Guided Decision Tree Classification Algorithm",2022,"","","","",198,"2022-07-13 09:22:00","","","","",,,,,0,0.00,0,1,1,"(e emergence and development of Chinese insurance companies are affected by their own unique national conditions. (e modern marketing concept lags behind, lacks the practical experience of scientifically formulating marketing strategies, and insurance practitioners lack marketing knowledge and the ability to absorb modern marketing achievements to guide practice. (erefore, China’s insurance industry inevitably has many problems in insurance marketing. In recent years, with the rapid development of big data (BD) technology, artificial intelligence, and machine learning in engineering and academia, relevant data models have been well developed. (e advantages of the decision tree are its good robustness, full sample mining, high precision, fast implementation, fast running speed, and low implementation cost. (is paper studies the application of the decision tree classification algorithm under the guidance of BD in insurance marketing planning. (e running results of the decision tree classification algorithm model show what factors will affect the accuracy and recall rate of customer churn decision-making. (e predicted value and scoring value of users are extracted to test the model, and the results are within a reasonable range. (e running time of this model is 2,320.36 s, which is more efficient than the 34min 25 s of traditional SAS. (erefore, the model can be put into use, and it is necessary to establish a long-term and stable relationship with customers.","",""
9,"Avadh Kishor, P. Singh","Comparative Study of Artificial Bee Colony Algorithm and Real Coded Genetic Algorithm for Analysing Their Performances and Development of a New Algorithmic Framework",2015,"","","","",199,"2022-07-13 09:22:00","","10.1109/ISCMI.2015.29","","",,,,,9,1.29,5,2,7,"This paper compares performance of the artificial bee colony algorithm (ABC) and the real coded genetic algorithm (RCGA) on a suite of 9 standard benchmark problems. The problem suite comprises a diverse set of unimodal, multimodal and rotated multimodal numerical optimization functions and the comparison criteria include (i) solution quality, (ii) convergence speed, (iii) robustness, and (iv) scalability to test efficacy of the algorithms. To the best knowledge of the authors, such a comprehensive comparative study of the two algorithms is not available in the literature. An empirical study shows that the RCGA has advantages over the ABC in terms of all the criteria for the unimodal and the rotated multimodal functions. On other hand, the ABC outperforms the RCGA in terms of solution quality for the multimodal functions. Therefore, based on the insights gained out of this comparative study, the authors propose an algorithm ABC-GA with new algorithmic framework that comprises advantages of both the ABC and the GA. An empirical study of the proposed algorithm ABC-GA shows its promising performance as the obtained results are superior to both the comparative algorithms for all the problems in all the criteria.","",""
6,"F. Barber, M. Salido","Robustness, stability, recoverability, and reliability in constraint satisfaction problems",2015,"","","","",200,"2022-07-13 09:22:00","","10.1007/s10115-014-0778-3","","",,,,,6,0.86,3,2,7,"","",""
