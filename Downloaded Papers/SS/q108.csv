Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
31,"Chongchong Qi, Qiu-song Chen, S. Sonny Kim","Integrated and intelligent design framework for cemented paste backfill: A combination of robust machine learning modelling and multi-objective optimization",2020,"","","","",1,"2022-07-13 09:38:31","","10.1016/j.mineng.2020.106422","","",,,,,31,15.50,10,3,2,"","",""
53,"M. Shafique, Mahum Naseer, T. Theocharides, C. Kyrkou, O. Mutlu, Lois Orosa, Jungwook Choi","Robust Machine Learning Systems: Challenges,Current Trends, Perspectives, and the Road Ahead",2020,"","","","",2,"2022-07-13 09:38:31","","10.1109/MDAT.2020.2971217","","",,,,,53,26.50,8,7,2,"Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPS) and Internet-of-Things (IoT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques. —Partha Pratim Pande, Washington State University","",""
20,"Srisatja Vitayasak, P. Pongcharoen","Performance improvement of Teaching-Learning-Based Optimisation for robust machine layout design",2018,"","","","",3,"2022-07-13 09:38:31","","10.1016/j.eswa.2018.01.005","","",,,,,20,5.00,10,2,4,"","",""
34,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, Semeen Rehman, M. Shafique","Robust Machine Learning Systems: Reliability and Security for Deep Neural Networks",2018,"","","","",4,"2022-07-13 09:38:31","","10.1109/IOLTS.2018.8474192","","",,,,,34,8.50,7,5,4,"Machine learning is commonly being used in almost all the areas that involve advanced data analytics and intelligent control. From applications like Natural Language Processing (NLP) to autonomous driving are based upon machine learning algorithms. An increasing trend is observed in the use of Deep Neural Networks (DNNs) for such applications. While the slight inaccuracy in applications like NLP does not have any severe consequences, it is not the same for other safety-critical applications, like autonomous driving and smart healthcare, where a small error can lead to catastrophic effects. Apart from high-accuracy DNN algorithms, there is a significant need for robust machine learning systems and hardware architectures that can generate reliable and trustworthy results in the presence of hardware-level faults while also preserving security and privacy. This paper provides an overview of the challenges being faced in ensuring reliable and secure execution of DNNs. To address the challenges, we present several techniques for analyzing and mitigating the reliability and security threats in machine learning systems.","",""
5,"Tao Chen, M. Ludkovski","A Machine Learning Approach to Adaptive Robust Utility Maximization and Hedging",2019,"","","","",5,"2022-07-13 09:38:31","","10.1137/20m1336023","","",,,,,5,1.67,3,2,3,"We investigate the adaptive robust control framework for portfolio optimization and loss-based hedging under drift and volatility uncertainty. Adaptive robust problems offer many advantages but require handling a double optimization problem (infimum over market measures, supremum over the control) at each instance. Moreover, the underlying Bellman equations are intrinsically multi-dimensional. We propose a novel machine learning approach that solves for the local saddle-point at a chosen set of inputs and then uses a nonparametric (Gaussian process) regression to obtain a functional representation of the value function. Our algorithm resembles control randomization and regression Monte Carlo techniques but also brings multiple innovations, including adaptive experimental design, separate surrogates for optimal control and the local worst-case measure, and computational speed-ups for the sup-inf optimization. Thanks to the new scheme we are able to consider settings that have been previously computationally intractable and provide several new financial insights about learning and optimal trading under unknown market parameters. In particular, we demonstrate the financial advantages of adaptive robust framework compared to adaptive and static robust alternatives.","",""
11,"Sherri Rose","Robust Machine Learning Variable Importance Analyses of Medical Conditions for Health Care Spending",2018,"","","","",6,"2022-07-13 09:38:31","","10.1111/1475-6773.12848","","",,,,,11,2.75,11,1,4,"OBJECTIVE To propose nonparametric double robust machine learning in variable importance analyses of medical conditions for health spending.   DATA SOURCES 2011-2012 Truven MarketScan database.   STUDY DESIGN I evaluate how much more, on average, commercially insured enrollees with each of 26 of the most prevalent medical conditions cost per year after controlling for demographics and other medical conditions. This is accomplished within the nonparametric targeted learning framework, which incorporates ensemble machine learning. Previous literature studying the impact of medical conditions on health care spending has almost exclusively focused on parametric risk adjustment; thus, I compare my approach to parametric regression.   PRINCIPAL FINDINGS My results demonstrate that multiple sclerosis, congestive heart failure, severe cancers, major depression and bipolar disorders, and chronic hepatitis are the most costly medical conditions on average per individual. These findings differed from those obtained using parametric regression.   CONCLUSIONS The literature may be underestimating the spending contributions of several medical conditions, which is a potentially critical oversight. If current methods are not capturing the true incremental effect of medical conditions, undesirable incentives related to care may remain. Further work is needed to directly study these issues in the context of federal formulas.","",""
8,"Daniil Bash, Yongqiang Cai, Vijila Chellappan, S. L. Wong, Yang Xu, Pawan Kumar, J. Tan, Anas Abutaha, J. Cheng, Y. Lim, S. Tian, D. Ren, Flore Mekki-Barrada, W. Wong, J. Kumar, Saif A. Khan, Qianxiao Li, T. Buonassisi, K. Hippalgaonkar","Machine Learning and High-Throughput Robust Design of P3HT-CNT Composite Thin Films for High Electrical Conductivity",2020,"","","","",7,"2022-07-13 09:38:31","","10.26434/chemrxiv.13265288.v1","","",,,,,8,4.00,1,19,2,"Combining high-throughput experiments with machine learning allows quick optimization of parameter spaces towards achieving target properties. In this study, we demonstrate that machine learning, combined with multi-labeled datasets, can additionally be used for scientific understanding and hypothesis testing. We introduce an automated flow system with high-throughput drop-casting for thin film preparation, followed by fast characterization of optical and electrical properties, with the capability to complete one cycle of learning of fully labeled ~160 samples in a single day. We combine regio-regular poly-3-hexylthiophene with various carbon nanotubes to achieve electrical conductivities as high as 1200 S/cm. Interestingly, a non-intuitive local optimum emerges when 10% of double-walled carbon nanotubes are added with long single wall carbon nanotubes, where the conductivity is seen to be as high as 700 S/cm, which we subsequently explain with high fidelity optical characterization. Employing dataset resampling strategies and graph-based regressions allows us to account for experimental cost and uncertainty estimation of correlated multi-outputs, and supports the proving of the hypothesis linking charge delocalization to electrical conductivity. We therefore present a robust machine-learning driven high-throughput experimental scheme that can be applied to optimize and understand properties of composites, or hybrid organic-inorganic materials.","",""
26,"W. Gou, Chu-wen Ling, Yan He, Zengliang Jiang, Yuanqing Fu, Fengzhe Xu, Z. Miao, Ting-yu Sun, Jie-sheng Lin, Hui-lian Zhu, Hongwei Zhou, Yu-ming Chen, Ju-Sheng Zheng","Interpretable Machine Learning Framework Reveals Robust Gut Microbiome Features Associated With Type 2 Diabetes",2020,"","","","",8,"2022-07-13 09:38:31","","10.2337/dc20-1536","","",,,,,26,13.00,3,13,2,"OBJECTIVE To identify the core gut microbial features associated with type 2 diabetes risk and potential demographic, adiposity, and dietary factors associated with these features. RESEARCH DESIGN AND METHODS We used an interpretable machine learning framework to identify the type 2 diabetes–related gut microbiome features in the cross-sectional analyses of three Chinese cohorts: one discovery cohort (n = 1,832, 270 cases of type 2 diabetes) and two validation cohorts (cohort 1: n = 203, 48 cases; cohort 2: n = 7,009, 608 cases). We constructed a microbiome risk score (MRS) with the identified features. We examined the prospective association of the MRS with glucose increment in 249 participants without type 2 diabetes and assessed the correlation between the MRS and host blood metabolites (n = 1,016). We transferred human fecal samples with different MRS levels to germ-free mice to confirm the MRS–type 2 diabetes relationship. We then examined the prospective association of demographic, adiposity, and dietary factors with the MRS (n = 1,832). RESULTS The MRS (including 14 microbial features) consistently associated with type 2 diabetes, with risk ratio for per 1-unit change in MRS 1.28 (95% CI 1.23–1.33), 1.23 (1.13–1.34), and 1.12 (1.06–1.18) across three cohorts. The MRS was positively associated with future glucose increment (P < 0.05) and was correlated with a variety of gut microbiota–derived blood metabolites. Animal study further confirmed the MRS–type 2 diabetes relationship. Body fat distribution was found to be a key factor modulating the gut microbiome–type 2 diabetes relationship. CONCLUSIONS Our results reveal a core set of gut microbiome features associated with type 2 diabetes risk and future glucose increment.","",""
9,"Yanbin Li, G. Lei, G. Bramerdorfer, S. Peng, Xiaodong Sun, Jianguo Zhu","Machine Learning for Design Optimization of Electromagnetic Devices: Recent Developments and Future Directions",2021,"","","","",9,"2022-07-13 09:38:31","","10.3390/APP11041627","","",,,,,9,9.00,2,6,1,"This paper reviews the recent developments of design optimization methods for electromagnetic devices, with a focus on machine learning methods. First, the recent advances in multi-objective, multidisciplinary, multilevel, topology, fuzzy, and robust design optimization of electromagnetic devices are overviewed. Second, a review is presented to the performance prediction and design optimization of electromagnetic devices based on the machine learning algorithms, including artificial neural network, support vector machine, extreme learning machine, random forest, and deep learning. Last, to meet modern requirements of high manufacturing/production quality and lifetime reliability, several promising topics, including the application of cloud services and digital twin, are discussed as future directions for design optimization of electromagnetic devices.","",""
148,"Amedeo Sapio, M. Canini, Chen-Yu Ho, J. Nelson, Panos Kalnis, Changhoon Kim, A. Krishnamurthy, M. Moshref, Dan R. K. Ports, Peter Richtárik","Scaling Distributed Machine Learning with In-Network Aggregation",2019,"","","","",10,"2022-07-13 09:38:31","","","","",,,,,148,49.33,15,10,3,"Training complex machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide a robust, efficient solution that speeds up training by up to 300%, and at least by 20% for a number of real-world benchmark models.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",11,"2022-07-13 09:38:31","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
9,"Ruben F. Kranenburg, J. Verduin, Y. Weesepoel, M. Alewijn, Marcel Heerschop, G. Koomen, P. Keizers, Frank Bakker, Fionn Wallace, Annette van Esch, Annemieke Hulsbergen, A. V. van Asten","Rapid and robust on‐scene detection of cocaine in street samples using a handheld near‐infrared spectrometer and machine learning algorithms",2020,"","","","",12,"2022-07-13 09:38:31","","10.1002/dta.2895","","",,,,,9,4.50,1,12,2,"Abstract On‐scene drug detection is an increasingly significant challenge due to the fast‐changing drug market as well as the risk of exposure to potent drug substances. Conventional colorimetric cocaine tests involve handling of the unknown material and are prone to false‐positive reactions on common pharmaceuticals used as cutting agents. This study demonstrates the novel application of 740–1070 nm small‐wavelength‐range near‐infrared (NIR) spectroscopy to confidently detect cocaine in case samples. Multistage machine learning algorithms are used to exploit the limited spectral features and predict not only the presence of cocaine but also the concentration and sample composition. A model based on more than 10,000 spectra from case samples yielded 97% true‐positive and 98% true‐negative results. The practical applicability is shown in more than 100 case samples not included in the model design. One of the most exciting aspects of this on‐scene approach is that the model can almost instantly adapt to changes in the illicit‐drug market by updating metadata with results from subsequent confirmatory laboratory analyses. These results demonstrate that advanced machine learning strategies applied on limited‐range NIR spectra from economic handheld sensors can be a valuable procedure for rapid on‐site detection of illicit substances by investigating officers. In addition to forensics, this interesting approach could be beneficial for screening and classification applications in the pharmaceutical, food‐safety, and environmental domains.","",""
7,"Bingbing Sun, T. Alkhalifah","ML-misfit: Learning a robust misfit function for full-waveform inversion using machine learning",2020,"","","","",13,"2022-07-13 09:38:31","","10.3997/2214-4609.202010466","","",,,,,7,3.50,4,2,2,"Most of the available advanced misfit functions for full waveform inversion (FWI) are hand-crafted, and the performance of those misfit functions is data-dependent. Thus, we propose to learn a misfit function for FWI, entitled ML-misfit, based on machine learning. Inspired by the optimal transport of the matching filter misfit, we design a neural network (NN) architecture for the misfit function in a form similar to comparing the mean and variance for two distributions. To guarantee the resulting learned misfit is a metric, we accommodate the symmetry of the misfit with respect to its input and a Hinge loss regularization term in a meta-loss function to satisfy the ""triangle inequality"" rule. In the framework of meta-learning, we train the network by running FWI to invert for randomly generated velocity models and update the parameters of the NN by minimizing the meta-loss, which is defined as accumulated difference between the true and inverted models. We first illustrate the basic principle of the ML-misfit for learning a convex misfit function for travel-time shifted signals. Further, we train the NN on 2D horizontally layered models, and we demonstrate the effectiveness and robustness of the learned ML-misfit by applying it to the well-known Marmousi model.","",""
29,"Ajay-Vikram Singh, Daniel Rosenkranz, M. Ansari, Rishabh Singh, Anurag Kanase, Shubham Pratap Singh, Blair Johnston, J. Tentschert, P. Laux, A. Luch","Artificial Intelligence and Machine Learning Empower Advanced Biomedical Material Design to Toxicity Prediction",2020,"","","","",14,"2022-07-13 09:38:31","","10.1002/aisy.202000084","","",,,,,29,14.50,3,10,2,"Materials at the nanoscale exhibit specific physicochemical interactions with their environment. Therefore, evaluating their toxic potential is a primary requirement for regulatory purposes and for the safer development of nanomedicines. In this review, to aid the understanding of nano–bio interactions from environmental and health and safety perspectives, the potential, reality, challenges, and future advances that artificial intelligence (AI) and machine learning (ML) present are described. Herein, AI and ML algorithms that assist in the reporting of the minimum information required for biomaterial characterization and aid in the development and establishment of standard operating procedures are focused. ML tools and ab initio simulations adopted to improve the reproducibility of data for robust quantitative comparisons and to facilitate in silico modeling and meta‐analyses leading to a substantial contribution to safe‐by‐design development in nanotoxicology/nanomedicine are mainly focused. In addition, future opportunities and challenges in the application of ML in nanoinformatics, which is particularly well‐suited for the clinical translation of nanotherapeutics, are highlighted. This comprehensive review is believed that it will promote an unprecedented involvement of AI research in improvements in the field of nanotoxicology and nanomedicine.","",""
22,"F. Plisson, O. Ramírez-Sánchez, Cristina Martínez-Hernández","Machine learning-guided discovery and design of non-hemolytic peptides",2020,"","","","",15,"2022-07-13 09:38:31","","10.1038/s41598-020-73644-6","","",,,,,22,11.00,7,3,2,"","",""
10,"R. Armiento","Database-Driven High-Throughput Calculations and Machine Learning Models for Materials Design",2019,"","","","",16,"2022-07-13 09:38:31","","10.1007/978-3-030-40245-7_17","","",,,,,10,3.33,10,1,3,"","",""
7,"D. Vanpoucke, Onno S. J. van Knippenberg, K. Hermans, K. Bernaerts, S. Mehrkanoon","Small data materials design with machine learning: When the average model knows best",2020,"","","","",17,"2022-07-13 09:38:31","","10.1063/5.0012285","","",,,,,7,3.50,1,5,2,"Machine learning is quickly becoming an important tool in modern materials design. Where many of its successes are rooted in huge datasets, the most common applications in academic and industrial materials design deal with datasets of at best a few tens of data points. Harnessing the power of machine learning in this context is, therefore, of considerable importance. In this work, we investigate the intricacies introduced by these small datasets. We show that individual data points introduce a significant chance factor in both model training and quality measurement. This chance factor can be mitigated by the introduction of an ensemble-averaged model. This model presents the highest accuracy, while at the same time, it is robust with regard to changing the dataset size. Furthermore, as only a single model instance needs to be stored and evaluated, it provides a highly efficient model for prediction purposes, ideally suited for the practical materials scientist.","",""
18,"S. Fleming, A. Goodbody","A Machine Learning Metasystem for Robust Probabilistic Nonlinear Regression-Based Forecasting of Seasonal Water Availability in the US West",2019,"","","","",18,"2022-07-13 09:38:31","","10.1109/ACCESS.2019.2936989","","",,,,,18,6.00,9,2,3,"Hydroelectric power generation, water supplies for municipal, agricultural, manufacturing, and service industry uses including technology-sector requirements, dam safety, flood control, recreational uses, and ecological and legal constraints, all place simultaneous, competing demands on the heavily stressed water management infrastructure of the mostly arid American West. Optimally managing these resources depends on predicting water availability. We built a probabilistic nonlinear regression water supply forecast (WSF) technique for the US Department of Agriculture, which runs the largest stand-alone WSF system in the US West. Design criteria included improved accuracy over the existing system; uncertainty estimates that seamlessly handle complex (heteroscedastic, non-Gaussian) prediction errors; integration of physical hydrometeorological process knowledge and domain-specific expert experience; ability to accommodate nonlinearity, model selection uncertainty and equifinality, and predictor multicollinearity and high dimensionality; and relatively easy, low-cost implementation. Some methods satisfied some of these requirements but none met all, leading us to develop a novel, interdisciplinary, and pragmatic prediction metasystem through a carefully considered synthesis of well-established, off-the-shelf components and approaches, spanning supervised and unsupervised machine learning, nonparametric statistical modeling, ensemble learning, and evolutionary optimization, focusing on maintaining but radically updating the principal components regression framework widely used for WSF. Testing this integrated multi-method prediction engine demonstrated its value for river forecasting; USDA adoption is a landmark for transitioning machine learning from research into practice in this field. Its ability to handle all the foregoing design criteria and requirements, which are not unique to WSF, suggests potential for extension to complex probabilistic prediction problems in other fields.","",""
59,"Yang Long, Jie Ren, Yunhui Li, Hong Chen","Inverse design of photonic topological state via machine learning",2019,"","","","",19,"2022-07-13 09:38:31","","10.1063/1.5094838","","",,,,,59,19.67,15,4,3,"The photonics topological state plays an important role in recent optical physics and has led to devices with robust properties. However, the design of optical structures with the target topological states is a challenge for current research. Here, we propose an approach to achieve this goal by exploiting machine learning technologies. In our work, we focus on Zak phases, which are the topological properties of one-dimensional photonics crystals. After learning the principle between the geometrical parameters and the Zak phases, the neural network can obtain the appropriate structures of photonics crystals by applying the objective Zak phase properties. Our work would give more insights into the application of machine learning on the inverse design of the complex material properties and could be extended to other fields, i.e., advanced phononics devices.","",""
19,"T. Le, M. Penna, D. Winkler, I. Yarovsky","Quantitative design rules for protein-resistant surface coatings using machine learning",2019,"","","","",20,"2022-07-13 09:38:31","","10.1038/s41598-018-36597-5","","",,,,,19,6.33,5,4,3,"","",""
17,"Fan Li, Xiaoqi Peng, Zuo Wang, Yi Zhou, Yuxia Wu, Minlin Jiang, Min Xu","Machine Learning (ML)‐Assisted Design and Fabrication for Solar Cells",2019,"","","","",21,"2022-07-13 09:38:31","","10.1002/eem2.12049","","",,,,,17,5.67,2,7,3,"Photovoltaic (PV) technologies have attracted great interest due to their capability of generating electricity directly from sunlight. Machine learning (ML) is a technique for computer to learn how to perform a specific task using known data. It can be used in many areas and has become a hot research topic recently due to the rapid accumulation of data and advancement of computer hardware. The application of ML techniques in the design and fabrication of solar cells started slowly but has recently gained tremendous momentum. An exhaustive compilation of the literatures indicates that all the major aspects in the research and development of solar cells can be effectively assisted by ML techniques. If combined with other tools and fed with additional theoretical and experimental data, more accurate and robust results can be achieved from ML techniques. The aspects can be grouped into four categories: prediction of material properties, optimization of device structures, optimization of fabrication processes, and reconstruction of measurement data. A statistical analysis of the literatures shows that artificial neural network (ANN) and genetic algorithm (GA) are the two most applied ML techniques and the topics in the optimization of device structures and optimization of fabrication processes are more popular. This article can be used as a reference by all PV researchers who are interested in ML techniques.","",""
21,"Aurélien Appriou, A. Cichocki, F. Lotte","Towards Robust Neuroadaptive HCI: Exploring Modern Machine Learning Methods to Estimate Mental Workload From EEG Signals",2018,"","","","",22,"2022-07-13 09:38:31","","10.1145/3170427.3188617","","",,,,,21,5.25,7,3,4,"Estimating mental workload from brain signals such as Electroencephalography (EEG) has proven very promising in multiple Human-Computer Interaction (HCI) applications, e.g., to design games or educational applications with adaptive difficulty, or to assess how cognitively difficult to use an interface can be. However, current EEG-based workload estimation may not be robust enough for some practical applications. Indeed, the currently obtained workload classification accuracies are relatively low, making the resulting estimations not fully trustable. This paper thus studies promising modern machine learning algorithms, including Riemannian geometry-based methods and Deep Learning, to estimate workload from EEG signals. We study them with both user-specific and user-independent calibration, to go towards calibration-free systems. Our results suggested that a shallow Convolutional Neural Network obtained the best performance in both conditions, outperforming state-of-the-art methods on the used data sets. This suggests that Deep Learning can bring new possibilities in HCI.","",""
12,"Maziar Montazerian, Edgar Dutra Zanotto, J. Mauro","Model-driven design of bioactive glasses: from molecular dynamics through machine learning",2020,"","","","",23,"2022-07-13 09:38:31","","10.1080/09506608.2019.1694779","","",,,,,12,6.00,4,3,2,"ABSTRACT Research in bioactive glasses (BGs) has traditionally been performed through trial-and-error experimentation. However, several modelling techniques will accelerate the discovery of new BGs as part of the ongoing endeavour to ‘decode the glass genome.’ Here, we critically review recent publications applying molecular dynamics simulations, machine learning approaches, and other modelling techniques for understanding BGs. We argue that modelling should be utilised more frequently in the design of BGs to achieve properties such as high bioactivity, high fracture strength and toughness, low density, and controlled morphology. Another challenge is modelling the biological response to biomaterials, such as their ability to foster protein adsorption, cell adhesion, cell proliferation, osteogenesis, angiogenesis, and bactericidal effects. The development of databases integrated with robust computational tools will be indispensable to these efforts. Future challenges are thus envisaged in which the compositional design, synthesis, characterisation, and application of BGs can be greatly accelerated by computational modelling.","",""
9,"S. Aich, Sabyasachi Chakraborty, J. Sim, Dong-Jin Jang, Hee-Cheol Kim","The Design of an Automated System for the Analysis of the Activity and Emotional Patterns of Dogs with Wearable Sensors Using Machine Learning",2019,"","","","",24,"2022-07-13 09:38:31","","10.3390/app9224938","","",,,,,9,3.00,2,5,3,"The safety and welfare of companion animals such as dogs has become a large challenge in the last few years. To assess the well-being of a dog, it is very important for human beings to understand the activity pattern of the dog, and its emotional behavior. A wearable, sensor-based system is suitable for such ends, as it will be able to monitor the dogs in real-time. However, the question remains unanswered as to what kind of data should be used to detect the activity patterns and emotional patterns, as does another: what should be the location of the sensors for the collection of data and how should we automate the system? Yet these questions remain unanswered, because to date, there is no such system that can address the above-mentioned concerns. The main purpose of this study was (1) to develop a system that can detect the activities and emotions based on the accelerometer and gyroscope signals and (2) to automate the system with robust machine learning techniques for implementing it for real-time situations. Therefore, we propose a system which is based on the data collected from 10 dogs, including nine breeds of various sizes and ages, and both genders. We used machine learning classification techniques for automating the detection and evaluation process. The ground truth fetched for the evaluation process was carried out by taking video recording data in frame per second and the wearable sensors data were collected in parallel with the video recordings. Evaluation of the system was performed using an ANN (artificial neural network), random forest, SVM (support vector machine), KNN (k nearest neighbors), and a naïve Bayes classifier. The robustness of our system was evaluated by taking independent training and validation sets. We achieved an accuracy of 96.58% while detecting the activity and 92.87% while detecting emotional behavior, respectively. This system will help the owners of dogs to track their behavior and emotions in real-life situations for various breeds in different scenarios.","",""
12,"E. Talbi","Machine Learning into Metaheuristics",2021,"","","","",25,"2022-07-13 09:38:31","","10.1145/3459664","","",,,,,12,12.00,12,1,1,"During the past few years, research in applying machine learning (ML) to design efficient, effective, and robust metaheuristics has become increasingly popular. Many of those machine learning-supported metaheuristics have generated high-quality results and represent state-of-the-art optimization algorithms. Although various appproaches have been proposed, there is a lack of a comprehensive survey and taxonomy on this research topic. In this article, we will investigate different opportunities for using ML into metaheuristics. We define uniformly the various ways synergies that might be achieved. A detailed taxonomy is proposed according to the concerned search component: target optimization problem and low-level and high-level components of metaheuristics. Our goal is also to motivate researchers in optimization to include ideas from ML into metaheuristics. We identify some open research issues in this topic that need further in-depth investigations.","",""
82,"Qian Yang, Jina Suh, N. Chen, Gonzalo A. Ramos","Grounding Interactive Machine Learning Tool Design in How Non-Experts Actually Build Models",2018,"","","","",26,"2022-07-13 09:38:31","","10.1145/3196709.3196729","","",,,,,82,20.50,21,4,4,"Machine learning (ML) promises data-driven insights and solutions for people from all walks of life, but the skill of crafting these solutions is possessed by only a few. Emerging research addresses this issue by creating ML tools that are easy and accessible to people who are not formally trained in ML (non-experts). This work investigated how non-experts build ML solutions for themselves in real life. Our interviews and surveys revealed unique potentials of non-expert ML, as well several pitfalls that non-experts are susceptible to. For example, many perceived percentage accuracy as a sole measure of performance, thus problematic models proceeded to deployment. These observations suggested that, while challenging, making ML easy and robust should both be important goals of designing novice-facing ML tools. To advance on this insight, we discuss design implications and created a sensitizing concept to demonstrate how designers might guide non-experts to easily build robust solutions.","",""
10,"Chengjun Xu, G. Zhu","Intelligent manufacturing Lie Group Machine Learning: real-time and efficient inspection system based on fog computing",2020,"","","","",27,"2022-07-13 09:38:31","","10.1007/s10845-020-01570-5","","",,,,,10,5.00,5,2,2,"","",""
242,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",28,"2022-07-13 09:38:31","","10.1145/3308809.3308857","","",,,,,242,48.40,81,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server andm working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of them working machines suffer Byzantine faults - a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.","",""
24,"A. Hürkamp, S. Gellrich, T. Ossowski, Jan Beuscher, S. Thiede, C. Herrmann, K. Dröder","Combining Simulation and Machine Learning as Digital Twin for the Manufacturing of Overmolded Thermoplastic Composites",2020,"","","","",29,"2022-07-13 09:38:31","","10.3390/jmmp4030092","","",,,,,24,12.00,3,7,2,"The design and development of composite structures requires precise and robust manufacturing processes. Composite materials such as fiber reinforced thermoplastics (FRTP) provide a good balance between manufacturing time, mechanical performance and weight. In this contribution, we investigate the process combination of thermoforming FRTP sheets (organo sheets) and injection overmolding of short FRTP for automotive structures. The limiting factor in those structures is the bond strength between the organo sheet and the overmolded thermoplastic. Within this process chain, even small deviations of the process settings (e.g., temperature) can lead to significant defects in the structure. A cyber physical production system based framework for a digital twin combining simulation and machine learning is presented. Based on parametric Finite-Element-Method (FEM) studies, training data for machine learning methods are generated and a FEM surrogate is developed. A comparison of different data-driven methods yields information on the estimation accuracy of task-specific data-driven methods. Finally, in accordance with experimental cross tension tests, the investigated FEM surrogate model is able to predict the interface bond strength quality in dependence of the process settings. The visualization into different quality domains qualifies the presented approach as decision support.","",""
25,"Nastaran Meftahi, M. Klymenko, A. Christofferson, U. Bach, D. Winkler, S. Russo","Machine learning property prediction for organic photovoltaic devices",2020,"","","","",30,"2022-07-13 09:38:31","","10.1038/s41524-020-00429-w","","",,,,,25,12.50,4,6,2,"","",""
19,"Sidra Mehtab, Jaydip Sen","A Time Series Analysis-Based Stock Price Prediction Using Machine Learning and Deep Learning Models",2020,"","","","",31,"2022-07-13 09:38:31","","10.1504/IJBFMI.2020.115691","","",,,,,19,9.50,10,2,2,"Prediction of future movement of stock prices has always been a challenging task for the researchers. While the advocates of the efficient market hypothesis (EMH) believe that it is impossible to design any predictive framework that can accurately predict the movement of stock prices, there are seminal work in the literature that have clearly demonstrated that the seemingly random movement patterns in the time series of a stock price can be predicted with a high level of accuracy. Design of such predictive models requires choice of appropriate variables, right transformation methods of the variables, and tuning of the parameters of the models. In this work, we present a very robust and accurate framework of stock price prediction that consists of an agglomeration of statistical, machine learning and deep learning models. We use the daily stock price data, collected at five minutes interval of time, of a very well known company that is listed in the National Stock Exchange (NSE) of India. The granular data is aggregated into three slots in a day, and the aggregated data is used for building and training the forecasting models. We contend that the agglomerative approach of model building that uses a combination of statistical, machine learning, and deep learning approaches, can very effectively learn from the volatile and random movement patterns in a stock price data. We build eight classification and eight regression models based on statistical and machine learning approaches. In addition to these models, a deep learning regression model using a long-and-short-term memory (LSTM) network is also built. Extensive results have been presented on the performance of these models, and the results are critically analyzed.","",""
15,"E. Talbi","Machine learning into metaheuristics: A survey and taxonomy of data-driven metaheuristics",2020,"","","","",32,"2022-07-13 09:38:31","","","","",,,,,15,7.50,15,1,2,"During the last years, research in applying machine learning (ML) to design efficient, effective and robust metaheuristics became increasingly popular. Many of those data driven metaheuristics have generated high quality results and represent state-of-the-art optimization algorithms. Although various appproaches have been proposed, there is a lack of a comprehensive survey and taxonomy on this research topic. In this paper we will investigate different opportunities for using ML into metaheuristics. We define uniformly the various ways synergies which might be achieved. A detailed taxonomy is proposed according to the concerned search component: target optimization problem, low-level and high-level components of metaheuristics. Our goal is also to motivate researchers in optimization to include ideas from ML into metaheuristics. We identify some open research issues in this topic which needs further in-depth investigations.","",""
17,"Liangyi Gong, Zhenhua Li, Feng Qian, Zi-Mei Zhang, Qi Alfred Chen, Zhiyun Qian, Hao Lin, Yunhao Liu","Experiences of landing machine learning onto market-scale mobile malware detection",2020,"","","","",33,"2022-07-13 09:38:31","","10.1145/3342195.3387530","","",,,,,17,8.50,2,8,2,"App markets, being crucial and critical for today's mobile ecosystem, have also become a natural malware delivery channel since they actually ""lend credibility"" to malicious apps. In the past decade, machine learning (ML) techniques have been explored for automated, robust malware detection. Unfortunately, to date, we have yet to see an ML-based malware detection solution deployed at market scales. To better understand the real-world challenges, we conduct a collaborative study with a major Android app market (T-Market) offering us large-scale ground-truth data. Our study shows that the key to successfully developing such systems is manifold, including feature selection/engineering, app analysis speed, developer engagement, and model evolution. Failure in any of the above aspects would lead to the ""wooden barrel effect"" of the entire system. We discuss our careful design choices as well as our first-hand deployment experiences in building such an ML-powered malware detection system. We implement our design and examine its effectiveness in the T-Market for over one year, using a single commodity server to vet ~ 10K apps every day. The evaluation results show that this design achieves an overall precision of 98% and recall of 96% with an average per-app scan time of 1.3 minutes.","",""
15,"Itzel Nunez, Afshin Marani, M. Nehdi","Mixture Optimization of Recycled Aggregate Concrete Using Hybrid Machine Learning Model",2020,"","","","",34,"2022-07-13 09:38:31","","10.3390/ma13194331","","",,,,,15,7.50,5,3,2,"Recycled aggregate concrete (RAC) contributes to mitigating the depletion of natural aggregates, alleviating the carbon footprint of concrete construction, and averting the landfilling of colossal amounts of construction and demolition waste. However, complexities in the mixture optimization of RAC due to the variability of recycled aggregates and lack of accuracy in estimating its compressive strength require novel and sophisticated techniques. This paper aims at developing state-of-the-art machine learning models to predict the RAC compressive strength and optimize its mixture design. Results show that the developed models including Gaussian processes, deep learning, and gradient boosting regression achieved robust predictive performance, with the gradient boosting regression trees yielding highest prediction accuracy. Furthermore, a particle swarm optimization coupled with gradient boosting regression trees model was developed to optimize the mixture design of RAC for various compressive strength classes. The hybrid model achieved cost-saving RAC mixture designs with lower environmental footprint for different target compressive strength classes. The model could be further harvested to achieve sustainable concrete with optimal recycled aggregate content, least cost, and least environmental footprint.","",""
16,"Abinet Tesfaye Eseye, M. Lehtonen","Short-Term Forecasting of Heat Demand of Buildings for Efficient and Optimal Energy Management Based on Integrated Machine Learning Models",2020,"","","","",35,"2022-07-13 09:38:31","","10.1109/TII.2020.2970165","","",,,,,16,8.00,8,2,2,"The increasing growth in the energy demand calls for robust actions to design and optimize energy-related assets for efficient and economic energy supply and demand within a smart grid setup. This article proposes a novel integrated machine learning (ML) technique to forecast the heat demand of buildings in a district heating system. The proposed short-term (24h-ahead) heat demand forecasting model is based on the integration of empirical mode decomposition (EMD), imperialistic competitive algorithm (ICA), and support vector machine (SVM). The proposed model also embeds an ML-based feature selection (FS) technique combining binary genetic algorithm and Gaussian process regression to obtain the most important and nonredundant variables that can constitute the input predictor subset to the forecasting model. The model is developed using a two-year (2015–2016) hourly dataset of actual district heat demand obtained from various buildings in the Otaniemi area of Espoo, Finland. Several variables from different domains such as seasonality (calendar), weather, occupancy, and heat demand are used to construct the initial feature space for FS process. Short-term forecasting models are also implemented using the Persistence approach as a reference and other eight ML approaches: artificial neural network (ANN), genetic algorithm combined with ANN (GA-ANN), ICA-ANN, SVM, GA-SVM, ICA-SVM, EMD-GA-ANN, and EMD-ICA-ANN. The performance of the proposed EMD-ICA-SVM-based forecasting model is tested using an out-of-sample one-year (2017) hourly dataset of district heat consumption of various building types. Comparative analysis of the forecasting performance of the models was performed. The obtained results demonstrate that the devised model forecasts the heat demand with improved performance evaluated using various accuracy metrics. Moreover, the devised model achieves outperformed forecasting accuracy enhancement, compared to the other nine evaluated models.","",""
1203,"Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. B. McMahan, Sarvar Patel, D. Ramage, Aaron Segal, Karn Seth","Practical Secure Aggregation for Privacy-Preserving Machine Learning",2017,"","","","",36,"2022-07-13 09:38:31","","10.1145/3133956.3133982","","",,,,,1203,240.60,134,9,5,"We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.","",""
14,"A. Mahmoud, Salaheldin Elkatatny, D. Al Shehri","Application of Machine Learning in Evaluation of the Static Young’s Modulus for Sandstone Formations",2020,"","","","",37,"2022-07-13 09:38:31","","10.3390/su12051880","","",,,,,14,7.00,5,3,2,"Prediction of the mechanical characteristics of the reservoir formations, such as static Young’s modulus (Estatic), is very important for the evaluation of the wellbore stability and development of the earth geomechanical model. Estatic considerably varies with the change in the lithology. Therefore, a robust model for Estatic prediction is needed. In this study, the predictability of Estatic for sandstone formation using four machine learning models was evaluated. The design parameters of the machine learning models were optimized to improve their predictability. The machine learning models were trained to estimate Estatic based on bulk formation density, compressional transit time, and shear transit time. The machine learning models were trained and tested using 592 well log data points and their corresponding core-derived Estatic values collected from one sandstone formation in well-A and then validated on 38 data points collected from a sandstone formation in well-B. Among the machine learning models developed in this work, Mamdani fuzzy interference system was the highly accurate model to predict Estatic for the validation data with an average absolute percentage error of only 1.56% and R of 0.999. The developed static Young’s modulus prediction models could help the new generation to characterize the formation rock with less cost and safe operation.","",""
9,"Wajid Hassan, T. Chou, Omar Tamer, John Pickard, Patrick Appiah-Kubi, L. Pagliari","Cloud computing survey on services, enhancements and challenges in the era of machine learning and data science",2020,"","","","",38,"2022-07-13 09:38:31","","10.11591/IJICT.V9I2.PP117-139","","",,,,,9,4.50,2,6,2,"Cloud computing has sweeping impact on the human productivity. Today it’s used for Computing, Storage, Predictions and Intelligent Decision Making, among others. Intelligent Decision Making using Machine Learning has pushed for the Cloud Services to be even more fast, robust and accurate. Security remains one of the major concerns which affect the cloud computing growth however there exist various research challenges in cloud computing adoption such as lack of well managed service level agreement (SLA), frequent disconnections, resource scarcity, interoperability, privacy, and reliability. Tremendous amount of work still needs to be done to explore the security challenges arising due to widespread usage of cloud deployment using Containers. We also discuss Impact of Cloud Computing and Cloud Standards. Hence in this research paper, a detailed survey of cloud computing, concepts, architectural principles, key services, and implementation, design and deployment challenges of cloud computing are discussed in detail and important future research directions in the era of Machine Learning and Data Science have been identified.","",""
7,"Flávio Luis de Mello","A Survey on Machine Learning Adversarial Attacks",2020,"","","","",39,"2022-07-13 09:38:31","","10.17648/jisc.v7i1.76","","",,,,,7,3.50,7,1,2,"It is becoming notorious several types of adversaries based on their threat model leverage vulnerabilities to compromise a machine learning system. Therefore, it is important to provide robustness to machine learning algorithms and systems against these adversaries. However, there are only a few strong countermeasures, which can be used in all types of attack scenarios to design a robust artificial intelligence system. This paper is structured and comprehensive overview of the research on attacks to machine learning systems and it tries to call the attention from developers and software houses to the security issues concerning machine learning.","",""
11,"Huai-Ting Li, Ching-Yao Chou, Yi-Ta Chen, Sheng-Hui Wang, A. Wu","Robust and Lightweight Ensemble Extreme Learning Machine Engine Based on Eigenspace Domain for Compressed Learning",2019,"","","","",40,"2022-07-13 09:38:31","","10.1109/TCSI.2019.2940642","","",,,,,11,3.67,2,5,3,"Compressive sensing (CS) is applied to electrocardiography (ECG) telemonitoring system to address the energy constraint of signal acquisition in sensors. In addition, on-sensor-analysis transmitting only abnormal data further reduces the energy consumption. To combine both advantages, “On-CS-sensor-analysis” can be achieved by compressed learning (CL), which analyzes signals directly in compressed domain. Extreme learning machine (ELM) provides an effective solution to achieve the goal of low-complexity CL. However, single ELM model has limited accuracy and is sensitive to the quality of data. Furthermore, hardware non-idealities in CS sensors result in learning performance degradation. In this work, we propose the ensemble of sub-eigenspace-ELM (SE-ELM), including two novel approaches: 1) We develop the eigenspace transformation for compressed noisy data, and further utilize a subspace-based dictionary to remove the interferences, and 2) Hardware-friendly design for ensemble of ELM provides high accuracy while maintaining low complexity. The simulation results on ECG-based atrial fibrillation show the SE-ELM can achieve the highest accuracy with 61.9% savings of the required multiplications compared with conventional methods. Finally, we implement this engine in TSMC 90 nm technology. The postlayout results show the proposed CL engine can provide competitive area- and energy-efficiency compared to existing machine learning engines.","",""
101,"Andreas K Triantafyllidis, A. Tsanas","Applications of Machine Learning in Real-Life Digital Health Interventions: Review of the Literature",2019,"","","","",41,"2022-07-13 09:38:31","","10.2196/12286","","",,,,,101,33.67,51,2,3,"Background Machine learning has attracted considerable research interest toward developing smart digital health interventions. These interventions have the potential to revolutionize health care and lead to substantial outcomes for patients and medical professionals. Objective Our objective was to review the literature on applications of machine learning in real-life digital health interventions, aiming to improve the understanding of researchers, clinicians, engineers, and policy makers in developing robust and impactful data-driven interventions in the health care domain. Methods We searched the PubMed and Scopus bibliographic databases with terms related to machine learning, to identify real-life studies of digital health interventions incorporating machine learning algorithms. We grouped those interventions according to their target (ie, target condition), study design, number of enrolled participants, follow-up duration, primary outcome and whether this had been statistically significant, machine learning algorithms used in the intervention, and outcome of the algorithms (eg, prediction). Results Our literature search identified 8 interventions incorporating machine learning in a real-life research setting, of which 3 (37%) were evaluated in a randomized controlled trial and 5 (63%) in a pilot or experimental single-group study. The interventions targeted depression prediction and management, speech recognition for people with speech disabilities, self-efficacy for weight loss, detection of changes in biopsychosocial condition of patients with multiple morbidity, stress management, treatment of phantom limb pain, smoking cessation, and personalized nutrition based on glycemic response. The average number of enrolled participants in the studies was 71 (range 8-214), and the average follow-up study duration was 69 days (range 3-180). Of the 8 interventions, 6 (75%) showed statistical significance (at the P=.05 level) in health outcomes. Conclusions This review found that digital health interventions incorporating machine learning algorithms in real-life studies can be useful and effective. Given the low number of studies identified in this review and that they did not follow a rigorous machine learning evaluation methodology, we urge the research community to conduct further studies in intervention settings following evaluation principles and demonstrating the potential of machine learning in clinical practice.","",""
15,"Qiaolin Ye, Peng Huang, Zhao Zhang, Yuhui Zheng, L. Fu, Wankou Yang","Multiview Learning With Robust Double-Sided Twin SVM.",2021,"","","","",42,"2022-07-13 09:38:31","","10.1109/TCYB.2021.3088519","","",,,,,15,15.00,3,6,1,"Multiview learning (MVL), which enhances the learners' performance by coordinating complementarity and consistency among different views, has attracted much attention. The multiview generalized eigenvalue proximal support vector machine (MvGSVM) is a recently proposed effective binary classification method, which introduces the concept of MVL into the classical generalized eigenvalue proximal support vector machine (GEPSVM). However, this approach cannot guarantee good classification performance and robustness yet. In this article, we develop multiview robust double-sided twin SVM (MvRDTSVM) with SVM-type problems, which introduces a set of double-sided constraints into the proposed model to promote classification performance. To improve the robustness of MvRDTSVM against outliers, we take L1-norm as the distance metric. Also, a fast version of MvRDTSVM (called MvFRDTSVM) is further presented. The reformulated problems are complex, and solving them are very challenging. As one of the main contributions of this article, we design two effective iterative algorithms to optimize the proposed nonconvex problems and then conduct theoretical analysis on the algorithms. The experimental results verify the effectiveness of our proposed methods.","",""
52,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",43,"2022-07-13 09:38:31","","10.1145/3219617.3219655","","",,,,,52,10.40,17,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q łe m for an arbitrarily small but fixed constant ε>0. The parameter estimate converges in O(łog N) rounds with an estimation error on the order of max √dq/N, ~√d/N , which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q . The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log 3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
26,"Theja Tulabandhula, C. Rudin","Robust Optimization using Machine Learning for Uncertainty Sets",2014,"","","","",44,"2022-07-13 09:38:31","","","","",,,,,26,3.25,13,2,8,"Our goal is to build robust optimization problems for making decisions based on complex data from the past. In robust optimization (RO) generally, the goal is to create a policy for decision-making that is robust to our uncertainty about the future. In particular, we want our policy to best handle the the worst possible situation that could arise, out of an uncertainty set of possible situations. Classically, the uncertainty set is simply chosen by the user, or it might be estimated in overly simplistic ways with strong assumptions; whereas in this work, we learn the uncertainty set from data collected in the past. The past data are drawn randomly from an (unknown) possibly complicated high-dimensional distribution. We propose a new uncertainty set design and show how tools from statistical learning theory can be employed to provide probabilistic guarantees on the robustness of the policy.","",""
31,"Kristin V. Presnell, H. Alper","Systems Metabolic Engineering Meets Machine Learning: A New Era for Data‐Driven Metabolic Engineering",2019,"","","","",45,"2022-07-13 09:38:31","","10.1002/biot.201800416","","",,,,,31,10.33,16,2,3,"The recent increase in high‐throughput capacity of ‘omics datasets combined with advances and interest in machine learning (ML) have created great opportunities for systems metabolic engineering. In this regard, data‐driven modeling methods have become increasingly valuable to metabolic strain design. In this review, the nature of ‘omics is discussed and a broad introduction to the ML algorithms combining these datasets into predictive models of metabolism and metabolic rewiring is provided. Next, this review highlights recent work in the literature that utilizes such data‐driven methods to inform various metabolic engineering efforts for different classes of application including product maximization, understanding and profiling phenotypes, de novo metabolic pathway design, and creation of robust system‐scale models for biotechnology. Overall, this review aims to highlight the potential and promise of using ML algorithms with metabolic engineering and systems biology related datasets.","",""
21,"Mohammadreza Mirzahosseini, Pengcheng Jiao, Kaveh Barri, K. Riding, A. Alavi","New machine learning prediction models for compressive strength of concrete modified with glass cullet",2019,"","","","",46,"2022-07-13 09:38:31","","10.1108/EC-08-2018-0348","","",,,,,21,7.00,4,5,3,"PurposeRecycled waste glasses have been widely used in Portland cement and concrete as aggregate or supplementary cementitious material. Compressive strength is one of the most important properties of concrete containing waste glasses, providing information about the loading capacity, pozzolanic reaction and porosity of the mixture. This study aims to propose highly nonlinear models to predict the compressive strength of concrete containing finely ground glass particles.Design/methodology/approachA robust machine leaning method called genetic programming is used the build the compressive strength prediction models. The models are developed using a number of test results on 50-mm mortar cubes containing glass powder according to ASTM C109. Parametric and sensitivity analyses are conducted to evaluate the effect of the predictor variables on the compressive strength. Furthermore, a comparative study is performed to benchmark the proposed models against classical regression models.FindingsThe derived design equations accurately characterize the compressive strength of concrete with ground glass fillers and remarkably outperform the regression models. A key feature of the proposed models as compared to the previous studies is that they include the simultaneous effect of various parameters such as glass compositions, size distributions, curing age and isothermal temperatures. Parametric and sensitivity analyses indicate that compressive strength is very sensitive to the curing age, curing temperature and particle surface area.Originality/valueThis study presents accurate machine learning models for the prediction of one of the most important mechanical properties of cementitious mixtures modified by waste glass, i.e. compressive strength. In addition, it provides an insight into the effect of several parameters influencing the compressive strength. From a computing perspective, a robust machine learning technique that overcomes the shortcomings of existing soft computing methods is introduced.","",""
22,"F. Emmert‐Streib, M. Dehmer","A Machine Learning Perspective on Personalized Medicine: An Automized, Comprehensive Knowledge Base with Ontology for Pattern Recognition",2018,"","","","",47,"2022-07-13 09:38:31","","10.3390/MAKE1010009","","",,,,,22,5.50,11,2,4,"Personalized or precision medicine is a new paradigm that holds great promise for individualized patient diagnosis, treatment, and care. However, personalized medicine has only been described on an informal level rather than through rigorous practical guidelines and statistical protocols that would allow its robust practical realization for implementation in day-to-day clinical practice. In this paper, we discuss three key factors, which we consider dimensions that effect the experimental design for personalized medicine: (I) phenotype categories; (II) population size; and (III) statistical analysis. This formalization allows us to define personalized medicine from a machine learning perspective, as an automized, comprehensive knowledge base with an ontology that performs pattern recognition of patient profiles.","",""
12,"Devyani P. Bhamare, P. Suryawanshi","Review on Reliable Pattern Recognition with Machine Learning Techniques",2018,"","","","",48,"2022-07-13 09:38:31","","10.1080/16168658.2019.1611030","","",,,,,12,3.00,6,2,4,"ABSTRACT The primary goal of pattern recognition is supervised or unsupervised classification. Among the various frameworks in which pattern recognition has been traditionally formulated, the statistical approach has been most intensively studied and used in practice. More recently, neural network techniques and methods imported from statistical learning theory have deserved increasing attention. The design of a recognition system requires careful attention to the following issues: definition of pattern classes, sensing environment, pattern representation, feature extraction and selection, cluster analysis, classifier design and learning, selection of training and test samples and performance evaluation. The general problem of recognising complex pattern with arbitrary patterns with arbitrary orientation, location and scale remains unsolved. New and emerging application, such as data mining, web searching, retrieval of multimedia data, face recognition and cursive handwriting recognition, require robust and efficient pattern recognition techniques. The objective of this review paper is to summarise and review some of the well-known methods used in various stages of a pattern recognition system and identify research topics and applications which are at the forefront of this exciting and challenging field. In the literature, Pattern recognition frameworks have been drawn closer by different machine learning strategies. This part reviews 33 related examinations in the period between 2014 and 2017.","",""
142,"Hassan Rafique, Mingrui Liu, Qihang Lin, Tianbao Yang","Non-Convex Min-Max Optimization: Provable Algorithms and Applications in Machine Learning",2018,"","","","",49,"2022-07-13 09:38:31","","","","",,,,,142,35.50,36,4,4,"Min-max saddle-point problems have broad applications in many tasks in machine learning, e.g., distributionally robust learning, learning with non-decomposable loss, or learning with uncertain data. Although convex-concave saddle-point problems have been broadly studied with efficient algorithms and solid theories available, it remains a challenge to design provably efficient algorithms for non-convex saddle-point problems, especially when the objective function involves an expectation or a large-scale finite sum. Motivated by recent literature on non-convex non-smooth minimization, this paper studies a family of non-convex min-max problems where the minimization component is non-convex (weakly convex) and the maximization component is concave. We propose a proximally guided stochastic subgradient method and a proximally guided stochastic variance-reduced method for expected and finite-sum saddle-point problems, respectively. We establish the computation complexities of both methods for finding a nearly stationary point of the corresponding minimization problem.","",""
4,"M. Tong","Using Machine Learning to Predict Core Sizes of High-Efficiency Turbofan Engines",2019,"","","","",50,"2022-07-13 09:38:31","","10.1115/1.4044770","","",,,,,4,1.33,4,1,3,"  With the rise in big data and analytics, machine learning is transforming many industries. It is being increasingly employed to solve a wide range of complex problems, producing autonomous systems that support human decision-making. For the aircraft engine industry, machine learning of historical and existing engine data could provide insights that help drive for better engine design. This work explored the application of machine learning to engine preliminary design. Engine core-size prediction was chosen for the first study because of its relative simplicity in terms of number of input variables required (only three). Specifically, machine-learning predictive tools were developed for turbofan engine core-size prediction, using publicly available data of two hundred manufactured engines and engines that were studied previously in NASA aeronautics projects. The prediction results of these models show that, by bringing together big data, robust machine-learning algorithms and data science, a machine learning-based predictive model can be an effective tool for turbofan engine core-size prediction. The promising results of this first study paves the way for further exploration of the use of machine learning for aircraft engine preliminary design.","",""
7,"M. Joly, S. Sarkar, D. Mehta","Machine Learning Enabled Adaptive Optimization of a Transonic Compressor Rotor With Precompression",2019,"","","","",51,"2022-07-13 09:38:31","","10.1115/1.4041808","","",,,,,7,2.33,2,3,3,"In aerodynamic design, accurate and robust surrogate models are important to accelerate computationally expensive computational fluid dynamics (CFD)-based optimization. In this paper, a machine learning framework is presented to speed-up the design optimization of a highly loaded transonic compressor rotor. The approach is threefold: (1) dynamic selection and self-tuning among several surrogate models; (2) classification to anticipate failure of the performance evaluation; and (3) adaptive selection of new candidates to perform CFD evaluation for updating the surrogate, which facilitates design space exploration and reduces surrogate uncertainty. The framework is demonstrated with a multipoint optimization of the transonic NASA rotor 37, yielding increased compressor efficiency in less than 48 h on 100 central processing unit cores. The optimized rotor geometry features precompression that relocates and attenuates the shock, without the stability penalty or undesired reacceleration usually observed in the literature.","",""
4,"Chuanhe Jay Shan, A. Wahba, Li-C. Wang, N. Sumikawa","Deploying A Machine Learning Solution As A Surrogate",2019,"","","","",52,"2022-07-13 09:38:31","","10.1109/ITC44170.2019.9000109","","",,,,,4,1.33,1,4,3,"A machine learning (ML) solution can be non-robust and when it is deployed, can make mistakes on the future unseen data. Consequently, deployment of a ML solution might demand continuous service from its ML developer. Using wafer image classification as an example, this paper presents the design of a ML solution where its deployment is facilitated by the continuous service from its ML expert.","",""
79,"A. Nandy, Chenru Duan, J. Janet, Stefan Gugler, H. Kulik","Strategies and Software for Machine Learning Accelerated Discovery in Transition Metal Chemistry",2018,"","","","",53,"2022-07-13 09:38:31","","10.1021/ACS.IECR.8B04015","","",,,,,79,19.75,16,5,4,"Machine learning the electronic structure of open shell transition metal complexes presents unique challenges, including robust and automated data set generation. Here, we introduce tools that simplify data acquisition from density functional theory (DFT) and validation of trained machine learning models using the molSimplify automatic design (mAD) workflow. We demonstrate this workflow by training and comparing the performance of LASSO, kernel ridge regression (KRR), and artificial neural network (ANN) models using heuristic, topological revised autocorrelation (RAC) descriptors we have recently introduced for machine learning inorganic chemistry. On a series of open shell transition metal complexes, we evaluate set aside test errors of these models for predicting the HOMO level and HOMO-LUMO gap. The best performing models are ANNs, which show 0.15 and 0.25 eV test set mean absolute errors on the HOMO level and HOMO-LUMO gap, respectively. Poor performing KRR models using the full 153-feature RAC set are improved to nearly the same performance as the ANNs when trained on down-selected subsets of 20-30 features. Analysis of the essential descriptors for HOMO and HOMO-LUMO gap prediction as well as comparison to subsets previously obtained for other properties reveals the paramount importance of non-local, steric properties in determining frontier molecular orbital energetics. We demonstrate our model performance on diverse complexes and in the discovery of molecules with target HOMO-LUMO gaps from a large 15,000 molecule design space in minutes rather than days that full DFT evaluation would require.","",""
38,"D. Dong, Xi Xing, Hailan Ma, Chunlin Chen, Zhixin Liu, H. Rabitz","Learning-Based Quantum Robust Control: Algorithm, Applications, and Experiments",2020,"","","","",54,"2022-07-13 09:38:31","","10.1109/TCYB.2019.2921424","","",,,,,38,19.00,6,6,2,"Robust control design for quantum systems has been recognized as a key task in quantum information technology, molecular chemistry, and atomic physics. In this paper, an improved differential evolution algorithm, referred to as multiple-samples and mixed-strategy DE (msMS_DE), is proposed to search robust fields for various quantum control problems. In msMS_DE, multiple samples are used for fitness evaluation and a mixed strategy is employed for the mutation operation. In particular, the msMS_DE algorithm is applied to the control problems of: 1) open inhomogeneous quantum ensembles and 2) the consensus goal of a quantum network with uncertainties. Numerical results are presented to demonstrate the excellent performance of the improved machine learning algorithm for these two classes of quantum robust control problems. Furthermore, msMS_DE is experimentally implemented on femtosecond (fs) laser control applications to optimize two-photon absorption and control fragmentation of the molecule CH2BrI. The experimental results demonstrate the excellent performance of msMS_DE in searching for effective fs laser pulses for various tasks.","",""
9,"Kexin Pei, Jonas Guan, David Williams-King, Junfeng Yang, S. Jana","XDA: Accurate, Robust Disassembly with Transfer Learning",2020,"","","","",55,"2022-07-13 09:38:31","","10.14722/NDSS.2021.23112","","",,,,,9,4.50,2,5,2,"Accurate and robust disassembly of stripped binaries is challenging. The root of the difficulty is that high-level structures, such as instruction and function boundaries, are absent in stripped binaries and must be recovered based on incomplete information. Current disassembly approaches rely on heuristics or simple pattern matching to approximate the recovery, but these methods are often inaccurate and brittle, especially across different compiler optimizations.  We present XDA, a transfer-learning-based disassembly framework that learns different contextual dependencies present in machine code and transfers this knowledge for accurate and robust disassembly. We design a self-supervised learning task motivated by masked Language Modeling to learn interactions among byte sequences in binaries. The outputs from this task are byte embeddings that encode sophisticated contextual dependencies between input binaries' byte tokens, which can then be finetuned for downstream disassembly tasks.  We evaluate XDA's performance on two disassembly tasks, recovering function boundaries and assembly instructions, on a collection of 3,121 binaries taken from SPEC CPU2017, SPEC CPU2006, and the BAP corpus. The binaries are compiled by GCC, ICC, and MSVC on x86/x64 Windows and Linux platforms over 4 optimization levels. XDA achieves 99.0% and 99.7% F1 score at recovering function boundaries and instructions, respectively, surpassing the previous state-of-the-art on both tasks. It also maintains speed on par with the fastest ML-based approach and is up to 38x faster than hand-written disassemblers like IDA Pro.","",""
94,"Qiang Zhu, A. Samanta, Bingxi Li, R. Rudd, T. Frolov","Predicting phase behavior of grain boundaries with evolutionary search and machine learning",2017,"","","","",56,"2022-07-13 09:38:31","","10.1038/s41467-018-02937-2","","",,,,,94,18.80,19,5,5,"","",""
6,"Wenqi Wei, Ling Liu","Robust Deep Learning Ensemble Against Deception",2020,"","","","",57,"2022-07-13 09:38:31","","10.1109/TDSC.2020.3024660","","",,,,,6,3.00,3,2,2,"Deep neural network (DNN) models are known to be vulnerable to maliciously crafted adversarial examples and to out-of-distribution inputs drawn sufficiently far away from the training data. How to protect a machine learning model against deception of both types of destructive inputs remains an open challenge. This article presents XEnsemble, a diversity ensemble verification methodology for enhancing the adversarial robustness of DNN models against deception caused by either adversarial examples or out-of-distribution inputs. XEnsemble by design has three unique capabilities. First, XEnsemble builds diverse input denoising verifiers by leveraging different data cleaning techniques. Second, XEnsemble develops a disagreement-diversity ensemble learning methodology for guarding the output of the prediction model against deception. Third, XEnsemble provides a suite of algorithms to combine input verification and output verification to protect the DNN prediction models from both adversarial examples and out of distribution inputs. Evaluated using 11 popular adversarial attacks and two representative out-of-distribution datasets, we show that XEnsemble achieves a high defense success rate against adversarial examples and a high detection success rate against out-of-distribution data inputs, and outperforms existing representative defense methods with respect to robustness and defensibility.","",""
21,"Lie He, Sai Praneeth Karimireddy, Martin Jaggi","Byzantine-Robust Learning on Heterogeneous Datasets via Resampling",2020,"","","","",58,"2022-07-13 09:38:31","","","","",,,,,21,10.50,7,3,2,"In Byzantine robust distributed optimization, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages to the server. While this problem has received significant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers is heterogeneous (non-iid), we design new attacks which circumvent these defenses leading to significant loss of performance. We then propose a simple resampling scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. We theoretically and experimentally validate our approach, showing that combining resampling with existing robust algorithms is effective against challenging attacks.","",""
35,"Vasisht Duddu","A Survey of Adversarial Machine Learning in Cyber Warfare",2018,"","","","",59,"2022-07-13 09:38:31","","10.14429/DSJ.68.12371","","",,,,,35,8.75,35,1,4,"The changing nature of warfare has seen a paradigm shift from the conventional to asymmetric, contactless warfare such as information and cyber warfare. Excessive dependence on information and communication technologies, cloud infrastructures, big data analytics, data-mining and automation in decision making poses grave threats to business and economy in adversarial environments. Adversarial machine learning is a fast growing area of research which studies the design of Machine Learning algorithms that are robust in adversarial environments. This paper presents a comprehensive survey of this emerging area and the various techniques of adversary modelling. We explore the threat models for Machine Learning systems and describe the various techniques to attack and defend them. We present privacy issues in these models and describe a cyber-warfare test-bed to test the effectiveness of the various attack-defence strategies and conclude with some open problems in this area of research. ","",""
44,"Thiago Alves, R. Das, Thomas Morris","Embedding Encryption and Machine Learning Intrusion Prevention Systems on Programmable Logic Controllers",2018,"","","","",60,"2022-07-13 09:38:31","","10.1109/LES.2018.2823906","","",,,,,44,11.00,15,3,4,"During its nascent stages, programmable logic controllers (PLCs) were made robust to sustain tough industrial environments, but little care was taken to raise defenses against potential cyberthreats. The recent interconnectivity of legacy PLCs and supervisory control and data acquisition (SCADA) systems with corporate networks and the Internet has significantly increased the threats to critical infrastructure. To counter these threats, researchers have put their efforts in finding defense mechanisms that can protect the SCADA network and the PLCs. Encryption and intrusion prevention systems (IPSs) have been used by many organizations to protect data and the network against cyber-attacks. However, since PLC vendors do not make available information about their hardware or software, it becomes challenging for researchers to embed security mechanisms into their devices. This letter describes an alternative design using an open source PLC that was modified to encrypt all data it sends over the network, independently of the protocol used. Additionally, a machine learning-based IPS was added to the PLC network stack providing a secure mechanism against network flood attacks like denial of service (DoS). Experimental results indicated that the encryption layer and the IPS increased the security of the link between the PLC and the supervisory software, preventing interception, injection, and DoS attacks.","",""
33,"Ved P. Kafle, Y. Fukushima, P. Martinez-Julia, T. Miyazawa","Consideration On Automation of 5G Network Slicing with Machine Learning",2018,"","","","",61,"2022-07-13 09:38:31","","10.23919/ITU-WT.2018.8597639","","",,,,,33,8.25,8,4,4,"Machine learning has the capability to provide simpler solutions to complex problems by analyzing a huge volume of data in a short time, learning for adapting its functionality to dynamically changing environments, and predicting near future events with reasonably good accuracy. The 5G communication networks are getting complex due to emergence of unprecedentedly huge number of new connected devices and new types of services. Moreover, the requirements of creating virtual network slices suitable to provide optimal services for diverse users and applications are posing challenges to the efficient management of network resources, processing information about a huge volume of traffic, staying robust against all potential security threats, and adaptively adjustment of network functionality for time-varying workload. In this paper, we introduce about the envisioned 5G network slicing and elaborate the necessity of automation of network functions for the design, construction, deployment, operation, control and management of network slices. We then revisit the machine learning techniques that can be applied for the automation of network functions. We also discuss the status of artificial intelligence and machine learning related activities being progressed in standards development organizations and industrial forums.","",""
27,"Fufang Wen, Jiaqi Jiang, Jonathan A. Fan","Robust Freeform Metasurface Design Based on Progressively Growing Generative Networks",2020,"","","","",62,"2022-07-13 09:38:31","","10.1021/acsphotonics.0c00539","","",,,,,27,13.50,9,3,2,"A longstanding objective of machine learning-enabled inverse design is the realization of inverse neural networks that can instantaneously output a device given a desired optical function. For comp...","",""
33,"R. Trinchero, P. Manfredi, I. Stievano, F. Canavero","Machine Learning for the Performance Assessment of High-Speed Links",2018,"","","","",63,"2022-07-13 09:38:31","","10.1109/TEMC.2018.2797481","","",,,,,33,8.25,8,4,4,"This paper investigates the application of support vector machine to the modeling of high-speed interconnects with largely varying and/or highly uncertain design parameters. The proposed method relies on a robust and well-established mathematical framework, yielding accurate surrogates of complex dynamical systems. An identification procedure based on the observation of a small set of system responses allows generating compact parametric relations, which can be used for design optimization and/or stochastic analysis. The feasibility and strength of the method are demonstrated based on a benchmark function and on the statistical assessment of a realistic printed circuit board interconnect, highlighting the main features and benefits of this technique over state-of-the-art solutions. Emphasis is given to the effects of the initial sample size and of input noise on the model estimation.","",""
18,"M. Avila, W. F. Azevedo","Development of machine learning models to predict inhibition of 3‐dehydroquinate dehydratase",2018,"","","","",64,"2022-07-13 09:38:31","","10.1111/cbdd.13312","","",,,,,18,4.50,9,2,4,"In this study, we describe the development of new machine learning models to predict inhibition of the enzyme 3‐dehydroquinate dehydratase (DHQD). This enzyme is the third step of the shikimate pathway and is responsible for the synthesis of chorismate, which is a natural precursor of aromatic amino acids. The enzymes of shikimate pathway are absent in humans, which make them protein targets for the design of antimicrobial drugs. We focus our study on the crystallographic structures of DHQD in complex with competitive inhibitors, for which experimental inhibition constant data is available. Application of supervised machine learning techniques was able to elaborate a robust DHQD‐targeted model to predict binding affinity. Combination of high‐resolution crystallographic structures and binding information indicates that the prevalence of intermolecular electrostatic interactions between DHQD and competitive inhibitors is of pivotal importance for the binding affinity against this enzyme. The present findings can be used to speed up virtual screening studies focused on the DHQD structure.","",""
21,"Daniyal Amir Awan, R. L. Cavalcante, M. Yukawa, S. Stańczak","Detection for 5G-NOMA: An Online Adaptive Machine Learning Approach",2017,"","","","",65,"2022-07-13 09:38:31","","10.1109/ICC.2018.8422449","","",,,,,21,4.20,5,4,5,"Non-orthogonal multiple access (NOMA) has emerged as a promising radio access technique for enabling the performance enhancements promised by the fifth-generation (5G) networks in terms of connectivity, latency, and spectrum efficiency. In the NOMA uplink, detection based on successive interference cancellation (SIC) with device clustering has been suggested. If the receivers are equipped with multiple antennas, SIC can be combined with minimum mean-squared error (MMSE) beamforming. However, there exists a tradeoff between the NOMA cluster size and the incurred SIC error. Larger clusters lead to larger errors but they are desirable from the spectrum efficiency and connectivity point of view. To enable the deployment of large clusters, we propose a novel online learning detection method for the NOMA uplink. We design an online adaptive filter in the sum space of linear and Gaussian reproducing kernel Hilbert spaces (RKHSs). Such a sum space design is robust against variations of a dynamic wireless network that can deteriorate the performance of a purely nonlinear adaptive filter. We demonstrate by simulations that the proposed method outperforms (symbol level) MMSE-SIC based detection for large cluster sizes.","",""
67,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings",2017,"","","","",66,"2022-07-13 09:38:31","","10.1145/3154503","","",,,,,67,13.40,22,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q ≤ for an arbitrarily small but fixed constant ε > 0. The parameter estimate converges in O(log N) rounds with an estimation error on the order of max{√dq/N, √d/N, which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q. The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
8,"Yufei Han, Xiangliang Zhang","Robust Federated Training via Collaborative Machine Teaching using Trusted Instances",2019,"","","","",67,"2022-07-13 09:38:31","","","","",,,,,8,2.67,4,2,3,"Federated learning performs distributed model training using local data hosted by agents. It shares only model parameter updates for iterative aggregation at the server. Although it is privacy-preserving by design, federated learning is vulnerable to noise corruption of local agents, as demonstrated in the previous study on adversarial data poisoning threat against federated learning systems. Even a single noise-corrupted agent can bias the model training. In our work, we propose a collaborative and privacy-preserving machine teaching paradigm with multiple distributed teachers, to improve robustness of the federated training process against local data corruption. We assume that each local agent (teacher) have the resources to verify a small portions of trusted instances, which may not by itself be adequate for learning. In the proposed collaborative machine teaching method, these trusted instances guide the distributed agents to jointly select a compact while informative training subset from data hosted by their own. Simultaneously, the agents learn to add changes of limited magnitudes into the selected data instances, in order to improve the testing performances of the federally trained model despite of the training data corruption. Experiments on toy and real data demonstrate that our approach can identify training set bugs effectively and suggest appropriate changes to the labels. Our algorithm is a step toward trustworthy machine learning.","",""
9,"Kshira Sagar Sahoo, Amaan Iqbal, P. Maiti, B. Sahoo","A Machine Learning Approach for Predicting DDoS Traffic in Software Defined Networks",2018,"","","","",68,"2022-07-13 09:38:31","","10.1109/ICIT.2018.00049","","",,,,,9,2.25,2,4,4,"Software Defined Networks (SDN) paradigm was introduced to overcome the limitations of the traditional network such as vendor dependencies, inconsistency policies, etc. It becomes a promising network architecture that provides the operators more control over the network infrastructure. The controller also called the operating system of the SDN has the centralized control over the network. Despite all its capabilities, the introduction of various architectural entities poses many security threats to SDN layers. Among many such security issues, Distributed Denial of Services (DDoS) is a rapidly growing attack that poses a tremendous threat to SDN. It targets to the availability of the network, by flooding the controller with spoofed packets. It causes the controller to become paralyzed, and thereby the entire network becomes destabilize. Therefore, it is essential to design a robust DDoS detection mechanism to prevent the control plane attack. In this regard, we have used seven Machine Learning techniques to accurately classify and predict different DDoS attacks like Smurf, UDP flood, and HTTP flood. Experimental results with proper analysis have been presented in this work.","",""
8,"Xiangrong Wang, Pengcheng Wang, Xianghua Wang","Adaptive Sparse Array Reconfiguration based on Machine Learning Algorithms",2018,"","","","",69,"2022-07-13 09:38:31","","10.1109/ICASSP.2018.8461429","","",,,,,8,2.00,3,3,4,"The sparse array design for adaptive beamforming has been recently formulated into combinatorial antenna selection problems, which belong to notorious NP-hard problems. As the commonly deployed convex relaxation algorithms are susceptible to local optima, several trials with different initial points are conducted for the global optima. Moreover, the high computational load of optimization techniques prohibits the real-time adaptive array reconfiguration. In this work, we propose to utilize machine learning algorithms, specifically support vector machine (SVM) and artificial neural network (ANN), for solving combinatorial antenna selection problems. Numerical examples are presented to validate the effectiveness and efficiency of machine learning algorithms for sparse array design. Moreover, the SVM based antenna selection is robust against DOA estimate uncertainties.","",""
8,"Sai Praneeth Karimireddy, Lie He, Martin Jaggi","Byzantine-Robust Learning on Heterogeneous Datasets via Bucketing",2020,"","","","",70,"2022-07-13 09:38:31","","","","",,,,,8,4.00,3,3,2,"In Byzantine robust distributed or federated learning, a central server wants to train a machine learning model over data distributed across multiple workers. However, a fraction of these workers may deviate from the prescribed algorithm and send arbitrary messages. While this problem has received signiﬁcant attention recently, most current defenses assume that the workers have identical data. For realistic cases when the data across workers are heterogeneous (non-iid), we design new attacks which circumvent current defenses, leading to signiﬁcant loss of performance. We then propose a simple bucketing scheme that adapts existing robust algorithms to heterogeneous datasets at a negligible computational cost. We also theoretically and experimentally validate our approach, showing that combining bucketing with existing robust algorithms is effective against challenging attacks. Our work is the ﬁrst to establish guaranteed convergence for the non-iid Byzantine robust problem under realistic assumptions.","",""
63,"Judy Hoffman, Daniel A. Roberts, Sho Yaida","Robust Learning with Jacobian Regularization",2019,"","","","",71,"2022-07-13 09:38:31","","","","",,,,,63,21.00,21,3,3,"Design of reliable systems must guarantee stability against input perturbations. In machine learning, such guarantee entails preventing overfitting and ensuring robustness of models against corruption of input data. In order to maximize stability, we analyze and develop a computationally efficient implementation of Jacobian regularization that increases classification margins of neural networks. The stabilizing effect of the Jacobian regularizer leads to significant improvements in robustness, as measured against both random and adversarial input perturbations, without severely degrading generalization properties on clean data.","",""
27,"Johannes H. Uhl, S. Leyk, Yao-Yi Chiang, Weiwei Duan, Craig A. Knoblock","Extracting Human Settlement Footprint from Historical Topographic Map Series Using Context-Based Machine Learning",2017,"","","","",72,"2022-07-13 09:38:31","","10.1049/CP.2017.0144","","",,,,,27,5.40,5,5,5,"Information extraction from historical maps represents a persistent challenge due to inferior graphical quality and large data volume in digital map archives, which can hold thousands of digitized map sheets. In this paper, we describe an approach to extract human settlement symbols in United States Geological Survey (USGS) historical topographic maps using contemporary building data as the contextual spatial layer. The presence of a building in the contemporary layer indicates a high probability that the same building can be found at that location on the historical map. We describe the design of an automatic sampling approach using these contemporary data to collect thousands of graphical examples for the symbol of interest. These graphical examples are then used for robust learning to then carry out feature extraction in the entire map. We employ a Convolutional Neural Network (LeNet) for the recognition task. Results are promising and will guide the next steps in this research to provide an unsupervised approach to extracting features from historical maps.","",""
24,"A. Weinand, Michael Karrenbauer, R. Sattiraju, H. Schotten","Application of Machine Learning for Channel based Message Authentication in Mission Critical Machine Type Communication",2017,"","","","",73,"2022-07-13 09:38:31","","","","",,,,,24,4.80,6,4,5,"The design of robust wireless communication systems for industrial applications such as closed loop control processes has been considered manifold recently. Additionally, the ongoing advances in the area of connected mobility have similar or even higher requirements regarding system reliability and availability. Beside unfulfilled reliability requirements, the availability of a system can further be reduced, if it is under attack in the sense of violation of information security goals such as data authenticity or integrity. In order to guarantee the safe operation of an application, a system has at least to be able to detect these attacks. Though there are numerous techniques in the sense of conventional cryptography in order to achieve that goal, these are not always suited for the requirements of the applications mentioned due to resource inefficiency. In the present work, we show how the goal of message authenticity based on physical layer security (PHYSEC) can be achieved. The main idea for such techniques is to exploit user specific characteristics of the wireless channel, especially in spatial domain. Additionally, we show the performance of our machine learning based approach and compare it with other existing approaches.","",""
76,"Ningning Jia, E. Lam","Machine learning for inverse lithography: using stochastic gradient descent for robust photomask synthesis",2010,"","","","",74,"2022-07-13 09:38:31","","10.1088/2040-8978/12/4/045601","","",,,,,76,6.33,38,2,12,"Inverse lithography technology (ILT) synthesizes photomasks by solving an inverse imaging problem through optimization of an appropriate functional. Much effort on ILT is dedicated to deriving superior masks at a nominal process condition. However, the lower k1 factor causes the mask to be more sensitive to process variations. Robustness to major process variations, such as focus and dose variations, is desired. In this paper, we consider the focus variation as a stochastic variable, and treat the mask design as a machine learning problem. The stochastic gradient descent approach, which is a useful tool in machine learning, is adopted to train the mask design. Compared with previous work, simulation shows that the proposed algorithm is effective in producing robust masks.","",""
17,"Qunwei Li, B. Kailkhura, R. Goldhahn, P. Ray, P. Varshney","Robust Decentralized Learning Using ADMM With Unreliable Agents",2017,"","","","",75,"2022-07-13 09:38:31","","10.1109/tsp.2022.3178655","","",,,,,17,3.40,3,5,5,"Many signal processing and machine learning problems can be formulated as consensus optimization problems which can be solved efficiently via a cooperative multi-agent system. However, the agents in the system can be unreliable due to a variety of reasons: noise, faults and attacks. Providing erroneous updates leads the optimization process in a wrong direction, and degrades the performance of distributed machine learning algorithms. This paper considers the problem of decentralized learning using ADMM in the presence of unreliable agents. First, we rigorously analyze the effect of erroneous updates (in ADMM learning iterations) on the convergence behavior of the multi-agent system. We show that the algorithm linearly converges to a neighborhood of the optimal solution under certain conditions and characterize the neighborhood size analytically. Next, we provide guidelines for network design to achieve a faster convergence to the neighborhood. We also provide conditions on the erroneous updates for exact convergence to the optimal solution. Finally, to mitigate the influence of unreliable agents, we propose ROAD, a robust variant of ADMM, and show its resilience to unreliable agents with an exact convergence to the optimum.","",""
27,"Han Zou, Jianfei Yang, Hari Prasanna Das, Huihan Liu, Yuxun Zhou, C. Spanos","WiFi and Vision Multimodal Learning for Accurate and Robust Device-Free Human Activity Recognition",2019,"","","","",76,"2022-07-13 09:38:31","","10.1109/CVPRW.2019.00056","","",,,,,27,9.00,5,6,3,"Human activity recognition plays an indispensable role in a myriad of emerging applications in context-aware services. Accurate activity recognition systems usually require the user to carry mobile or wearable devices, which is inconvenient for long term usage. In this paper, we design WiVi, a novel human activity recognition scheme that is able to identify common human activities in an accurate and device-free manner via multimodal machine learning using only commercial WiFi-enabled IoT devices and camera. For sensing using WiFi, a new platform is developed to extract fine-grained WiFi channel information and transform them into WiFi frames. A tailored convolutional neural network model is designed to extract high-level representative features among the WiFi frames in order to provide human activity estimation. We utilized a variant of C3D model for activity sensing using vision. Following this, WiVi performs multimodal fusion at the decision level to combine the strength of WiFi and vision by constructing an ensembled DNN model. Extensive experiments are conducted in an indoor environment, demonstrating that WiVi achieves 97.5% activity recognition accuracy and is robust under unfavorable situations, as each modality provides the complementary sensing when the other faces its limiting conditions.","",""
29,"D. Pánek, T. Orosz, P. Karban","Artap: Robust Design Optimization Framework for Engineering Applications",2019,"","","","",77,"2022-07-13 09:38:31","","10.1109/ICDS47004.2019.8942318","","",,,,,29,9.67,10,3,3,"The main goal of the Artap project is to provide an extensive infrastructure for robust design optimization, where usually many different numerical solvers have to be used together and the impact of the manufacturing uncertainties have to be minimized. Artap is an open-source software platform, developed jointly with the coupled numerical field solver, Agros Suite. Artap ensures interfaces for a broad collection of optimization algorithms (genetic and evolutionary algorithms, various interfaces to libraries such as Nlopt, Bayesopt, etc.), tools for machine learning (neural networks, Gaussian processes, etc.), finite element solvers (Agros Suite, Comsol, Multiphysics, deal.II). The implemented tools offers an easy and straightforward solution not only for robust design optimization but parameter identification, model order reduction, and shape optimization, as well. Moreover, Artap provides automatic parallelization of the optimization process. The paper presents the structure of the framework and technologies powering the project. The main features of Artap are demonstrated on an induction brazing process design tasks.","",""
12,"M. Adnan, M. Latif, Abaid-ur-Rehman, M. Nazir","Estimating Evapotranspiration using Machine Learning Techniques",2017,"","","","",78,"2022-07-13 09:38:31","","10.14569/IJACSA.2017.080915","","",,,,,12,2.40,3,4,5,"The measurement of evapotranspiration is the most important factor in irrigation scheduling. Evapotranspiration means loss of water from the surface of plant and soil. Evaporation parameters are being used in studying water balances, water resource management, and irrigation system design and for estimating plant growth and height as well. Evapotranspiration is measured by different methods by using various parameters. Evapotranspiration varies with the climate change and as the climate has a lot of variation geographically, the pre-developed systems have not used all available meteorological data hence not robust models. In this research work, a model is developed to estimate evapotranspiration with more authentic and accurate reduced meteorological parameters using different machine learning techniques. The study reveals to learn and generalize the relationship among different parameters. The dataset with reduced dimension is modeled through time series neural network giving the regression value R=83%.","",""
35,"Hao-Fan Yang, T. Dillon, E. Chang, Yi-Ping Phoebe Chen","Optimized Configuration of Exponential Smoothing and Extreme Learning Machine for Traffic Flow Forecasting",2019,"","","","",79,"2022-07-13 09:38:31","","10.1109/TII.2018.2876907","","",,,,,35,11.67,9,4,3,"Traffic flow forecasting is a useful technology applied to solve traffic congestion problems and to improve transportation mobility. Neural networks related approaches have been applied to develop traffic forecasting models for more than two decades. Since neural networks are sensitivity in parameters selection, selecting appropriate modeling configuration is essential to improve the accuracy and efficiency of traffic flow prediction. However, this is usually conducted by the trial-and-error method, which is very time consuming while involving too many design factors. Therefore, this paper utilizes a robust and systematic optimization approach, the Taguchi method, for obtaining the optimized configuration of the proposed exponential smoothing and extreme learning machine forecasting model. The developed model is applied to real-world data collected from freeways and highways in the United Kingdom and is compared with three existing forecasting models. The results indicate that the Taguchi method is efficient and capable for the forecasting model design and the proposed model with the optimized configuration has superior performance in traffic flow forecasting with approximate 91% and 88% accuracy rate in freeway and highway in both peak and nonpeak traffic periods.","",""
6,"Ana Luiza B. P. Barros, G. Barreto","Building a Robust Extreme Learning Machine for Classification in the Presence of Outliers",2013,"","","","",80,"2022-07-13 09:38:31","","10.1007/978-3-642-40846-5_59","","",,,,,6,0.67,3,2,9,"","",""
16,"J. Vink, G. Haan","No-Reference Metric Design With Machine Learning for Local Video Compression Artifact Level",2011,"","","","",81,"2022-07-13 09:38:31","","10.1109/JSTSP.2010.2055832","","",,,,,16,1.45,8,2,11,"In decoded digital video, the local perceptual compression artifact level depends on the global compression ratio and the local video content. In this paper, we show how to build a highly relevant metric for video compression artifacts using supervised learning. To obtain the ground truth for training, we first build a reference metric for local estimation of the artifact level, which is robust to scaling and sensitive to all types of compression artifacts. Next, we design a large feature set and use AdaBoost to create no-reference metrics trained with the output of the reference metric. Two separate trained no-reference metrics, one for flat and one for detailed areas, respectively, are necessary to cover all types of artifacts. The relevance of these metrics is validated in a compression artifact reduction application, using objective scores like PSNR and BIM, but also a subjective evaluation as proof. We conclude that our created reference metric is an accurate local estimator of the compression artifact level. We were able to copy the performance to two no-reference metrics, based on a weighted mixture of low-level features. Our new metrics enable a far superior performance of artifact reduction compared to relevant alternative proposals.","",""
92,"Chun‐Teh Chen, Grace X. Gu","Generative Deep Neural Networks for Inverse Materials Design Using Backpropagation and Active Learning",2020,"","","","",82,"2022-07-13 09:38:31","","10.1002/advs.201902607","","",,,,,92,46.00,46,2,2,"In recent years, machine learning (ML) techniques are seen to be promising tools to discover and design novel materials. However, the lack of robust inverse design approaches to identify promising candidate materials without exploring the entire design space causes a fundamental bottleneck. A general‐purpose inverse design approach is presented using generative inverse design networks. This ML‐based inverse design approach uses backpropagation to calculate the analytical gradients of an objective function with respect to design variables. This inverse design approach is capable of overcoming local minima traps by using backpropagation to provide rapid calculations of gradient information and running millions of optimizations with different initial values. Furthermore, an active learning strategy is adopted in the inverse design approach to improve the performance of candidate materials and reduce the amount of training data needed to do so. Compared to passive learning, the active learning strategy is capable of generating better designs and reducing the amount of training data by at least an order‐of‐magnitude in the case study on composite materials. The inverse design approach is compared with conventional gradient‐based topology optimization and gradient‐free genetic algorithms and the pros and cons of each method are discussed when applied to materials discovery and design problems.","",""
12,"Weijing Shi, Mohamed Baker Alawieh, Xin Li, Huafeng Yu, N. Aréchiga, Nobuyuki Tomatsu","Efficient statistical validation of machine learning systems for autonomous driving",2016,"","","","",83,"2022-07-13 09:38:31","","10.1145/2966986.2980077","","",,,,,12,2.00,2,6,6,"Today's automotive industry is making a bold move to equip vehicles with intelligent driver assistance features. A modern automobile is now equipped with a powerful computing platform to run multiple machine learning algorithms for environment perception (e.g., pedestrian detection) and motion control (e.g., vehicle stabilization). These machine learning systems must be highly robust with extremely small failure rate in order to ensure safe and reliable driving. In this paper, we propose a novel Subset Sampling (SUS) algorithm to efficiently validate a machine learning system. In particular, a Markov Chain Monte Carlo algorithm based on graph mapping is developed to accurately estimate the rare failure rate with a minimal amount of test data, thereby minimizing the validation cost. Our numerical experiments show that SUS achieves 15.2× runtime speed-up over the conventional brute-force Monte Carlo method.","",""
7,"Ghislain Takam Tchendjou, Rshdee Alhakim, E. Simeu, F. Lebowsky","Evaluation of machine learning algorithms for image quality assessment",2016,"","","","",84,"2022-07-13 09:38:31","","10.1109/IOLTS.2016.7604697","","",,,,,7,1.17,2,4,6,"In this article, we apply different machine learning (ML) techniques for building objective models, that permit to automatically assess the image quality in agreement with human visual perception. The six ML methods proposed are discriminant analysis, k-nearest neighbors, artificial neural network, non-linear regression, decision tree and fuzzy logic. Both the stability and the robustness of designed models are evaluated by using Monte-Carlo cross-validation approach (MCCV). The simulation results demonstrate that fuzzy logic model provides the best prediction accuracy.","",""
7,"Peng Zhou, Fan Ye, Liang Du","Unsupervised Robust Multiple Kernel Learning via Extracting Local and Global Noises",2019,"","","","",85,"2022-07-13 09:38:31","","10.1109/ACCESS.2019.2904727","","",,,,,7,2.33,2,3,3,"Kernel-based clustering methods can capture the non-linear structure and identify arbitrarily shaped clusters, so they have been widely used in machine learning tasks. Since the performance of kernel methods critically depends on the choices of kernels, multiple kernel learning methods are proposed to alleviate the effort for kernel designing. The conventional multiple kernel learning methods learn a consensus kernel by linearly combining all candidate kernels, whereas ignoring the influence of the noises. To improve the robustness of multiple kernel learning methods, in this paper, we analyze the local and global noises and design regularized terms to characterize them respectively. Then, we propose a novel local and global de-noising multiple kernel learning method which can explicitly extract the local and global noises to recover the clean kernels for multiple kernel learning. After that, a block coordinate descent algorithm is presented to solve the optimization problem. Finally, the extensive experiments on benchmark data sets will demonstrate the effectiveness of the proposed algorithm.","",""
16,"Hadi Salman, Andrew Ilyas, Logan Engstrom, Sai Vemprala, A. Madry, Ashish Kapoor","Unadversarial Examples: Designing Objects for Robust Vision",2020,"","","","",86,"2022-07-13 09:38:31","","","","",,,,,16,8.00,3,6,2,"We study a class of realistic computer vision settings wherein one can inﬂuence the design of the objects being recognized. We develop a framework that leverages this capability to signiﬁcantly improve vision models’ performance and robustness. This framework exploits the sensitivity of modern machine learning algorithms to input perturbations in order to design “robust objects,” i.e., objects that are explicitly optimized to be conﬁdently detected or classiﬁed. We demonstrate the efﬁcacy of the framework on a wide variety of vision-based tasks ranging from standard benchmarks, to (in-simulation) robotics, to real-world experiments. Our code can be found at https://git.io/unadversarial .","",""
7,"Liangqiong Qu, Yuyin Zhou, Paul Pu Liang, Yingda Xia, Feifei Wang, L. Fei-Fei, E. Adeli, D. Rubin","Rethinking Architecture Design for Tackling Data Heterogeneity in Federated Learning",2021,"","","","",87,"2022-07-13 09:38:31","","","","",,,,,7,7.00,1,8,1,"Federated learning is an emerging research paradigm enabling collaborative training of machine learning models among different organizations while keeping data private at each institution. Despite recent progress, there remain fundamental challenges such as the lack of convergence and the potential for catastrophic forgetting across real-world heterogeneous devices. In this paper, we demonstrate that self-attention-based architectures ( e.g. , Transformers) are more robust to distribution shifts and hence improve federated learning over heterogeneous data. Concretely, we conduct the ﬁrst rigorous empirical investigation of different neural architectures across a range of federated algorithms, real-world benchmarks, and heterogeneous data splits. Our experiments show that simply replacing convolutional networks with Transformers can greatly reduce catastrophic forgetting of previous devices, accelerate convergence, and reach a better global model, especially when dealing with heterogeneous data. We release our code and pretrained models to encourage future exploration in robust architectures as an alternative to current research efforts on the optimization front.","",""
9,"Dirmanto Jap, W. He, S. Bhasin","Supervised and unsupervised machine learning for side-channel based Trojan detection",2016,"","","","",88,"2022-07-13 09:38:31","","10.1109/ASAP.2016.7760768","","",,,,,9,1.50,3,3,6,"Hardware Trojan (HT) has recently drawn much attention in both industry and academia due to the global outsourcing trend in semiconductor manufacturing, where a malicious logic can be inserted into the security critical ICs at almost any stages. HT severity mainly stems from its low-cost and stealthy nature where the HT only functions at a strict condition to purposely alter the logic or physical behavior for leaking secrets. This fact makes HT detection very challenging in practice. In this paper, we propose a novel HT detection technique based on machine learning approach. The described solution is constructed over one-class SVM and is shown to be more robust compared to the template based detection techniques. An unsupervised approach is also applied in our solution for mitigating the golden model dependencies. To evaluate the solution, a practical HT design was inserted into an AES coprocessor implemented in a Xilinx FPGA. Based on the partial reconfiguration, the HT size can be dynamically changed without altering cipher part, which helps to precisely evaluate the HT influence. The experimental results have shown that our proposed detection technique achieve a high performance accuracy.","",""
9,"A. Akansu, S. Kulkarni, D. Malioutov","Financial Signal Processing and Machine Learning",2016,"","","","",89,"2022-07-13 09:38:31","","","","",,,,,9,1.50,3,3,6,"The modern financial industry has been required to deal with large and diverse portfolios in a variety of asset classes often with limited market data available. Financial Signal Processing and Machine Learning unifies a number of recent advances made in signal processing and machine learning for the design and management of investment portfolios and financial engineering. This book bridges the gap between these disciplines, offering the latest information on key topics including characterizing statistical dependence and correlation in high dimensions, constructing effective and robust risk measures, and their use in portfolio optimization and rebalancing. The book focuses on signal processing approaches to model return, momentum, and mean reversion, addressing theoretical and implementation aspects. It highlights the connections between portfolio theory, sparse learning and compressed sensing, sparse eigen-portfolios, robust optimization, non-Gaussian data-driven risk measures, graphical models, causal analysis through temporal-causal modeling, and large-scale copula-based approaches. Key features: Highlights signal processing and machine learning as key approaches to quantitative finance. Offers advanced mathematical tools for high-dimensional portfolio construction, monitoring, and post-trade analysis problems. Presents portfolio theory, sparse learning and compressed sensing, sparsity methods for investment portfolios. including eigen-portfolios, model return, momentum, mean reversion and non-Gaussian data-driven risk measures with real-world applications of these techniques. Includes contributions from leading researchers and practitioners in both the signal and information processing communities, and the quantitative finance community.","",""
66,"D. Ramík, C. Sabourin, R. Moreno, K. Madani","A machine learning based intelligent vision system for autonomous object detection and recognition",2014,"","","","",90,"2022-07-13 09:38:31","","10.1007/s10489-013-0461-5","","",,,,,66,8.25,17,4,8,"","",""
6,"Philip Wijesinghe, K. Dholakia","Emergent physics-informed design of deep learning for microscopy",2021,"","","","",91,"2022-07-13 09:38:31","","10.1088/2515-7647/abf02c","","",,,,,6,6.00,3,2,1,"Deep learning has revolutionised microscopy, enabling automated means for image classification, tracking and transformation. Beyond machine vision, deep learning has recently emerged as a universal and powerful tool to address challenging and previously untractable inverse image recovery problems. In seeking accurate, learned means of inversion, these advances have transformed conventional deep learning methods to those cognisant of the underlying physics of image formation, enabling robust, efficient and accurate recovery even in severely ill-posed conditions. In this perspective, we explore the emergence of physics-informed deep learning that will enable universal and accessible computational microscopy.","",""
14,"Noor H. Awad, Neeratyoy Mallik, F. Hutter","DEHB: Evolutionary Hyberband for Scalable, Robust and Efficient Hyperparameter Optimization",2021,"","","","",92,"2022-07-13 09:38:31","","10.24963/ijcai.2021/296","","",,,,,14,14.00,5,3,1,"Modern machine learning algorithms crucially rely on several design decisions to achieve strong performance, making the problem of Hyperparameter Optimization (HPO) more important than ever. Here, we combine the advantages of the popular bandit-based HPO method Hyperband (HB) and the evolutionary search approach of Differential Evolution (DE) to yield a new HPO method which we call DEHB. Comprehensive results on a very broad range of HPO problems, as well as a wide range of tabular benchmarks from neural architecture search, demonstrate that DEHB achieves strong performance far more robustly than all previous HPO methods we are aware of, especially for high-dimensional problems with discrete input dimensions. For example, DEHB is up to 1000x faster than random search. It is also efficient in computational time, conceptually simple and easy to implement, positioning it well to become a new default HPO method.","",""
36,"Yazhou Ren, Peng Zhao, Yongpan Sheng, D. Yao, Zenglin Xu","Robust Softmax Regression for Multi-class Classification with Self-Paced Learning",2017,"","","","",93,"2022-07-13 09:38:31","","10.24963/ijcai.2017/368","","",,,,,36,7.20,7,5,5,"Softmax regression, a generalization of Logistic regression (LR) in the setting of multi-class classification, has been widely used in many machine learning applications. However, the performance of softmax regression is extremely sensitive to the presence of noisy data and outliers. To address this issue, we propose a model of robust softmax regression (RoSR) originated from the self-paced learning (SPL) paradigm for multi-class classification. Concretely, RoSR equipped with the soft weighting scheme is able to evaluate the importance of each data instance. Then, data instances participate in the classification problem according to their weights. In this way, the influence of noisy data and outliers (which are typically with small weights) can be significantly reduced. However, standard SPL may suffer from the imbalanced class influence problem, where some classes may have little influence in the training process if their instances are not sensitive to the loss. To alleviate this problem, we design two novel soft weighting schemes that assign weights and select instances locally for each class. Experimental results demonstrate the effectiveness of the proposed methods.","",""
10,"K. Kasper, L. Mathelin, H. Abou-Kandil","A machine learning approach for constrained sensor placement",2015,"","","","",94,"2022-07-13 09:38:31","","10.1109/ACC.2015.7172034","","",,,,,10,1.43,3,3,7,"Sensor placement is of pivotal importance in closed-loop control as measurements are key to design the control laws. In this article, a novel machine learning-based sensor placement algorithm is proposed in order to recover a high-dimensional field from a limited amount of local measurements with a linear estimator. Unlike many other methods, our algorithm does not rely on a reduced order model and achieves good results even with a small number of sensors. In many situations, sensors cannot be placed arbitrarily, either because of their geometry or because of the environment they are in. Our algorithm naturally accounts for these constraints as well as being robust to noise. Its performance is illustrated on a fluid flow example and compared to two state of the art methods, Effective Independence and FrameSense, on the recovery of the pressure field from limited noisy pressure measurements.","",""
14,"A. Causo, Zheng-Hao Chong, Ramamoorthy Luxman, Yuan Yik Kok, Zhao Yi, W. Pang, Ren Meixuan, Yee Seng Teoh, Wu Jing, Hendra Suratno Tju, I. Chen","A Robust Robot Design for Item Picking",2018,"","","","",95,"2022-07-13 09:38:31","","10.1109/ICRA.2018.8461057","","",,,,,14,3.50,1,11,4,"In order to build a stable and reliable system for the Amazon Robotics Challenge we went through a detailed study of the performance and system requirements based on the rules and our past experience of the challenge. The challenge was to build a robot that integrates grasping, vision, motion planning, among others, to be able to pick items from a shelf to specific order boxes. This paper presents the development process including component selection, module designs, and deployment. The resulting robot system has dual 6 degrees of freedom industrial arms mounted on fixed bases, which in turn are mounted on a calibrated table. The robot works with a custom-designed top-open extendable shelf. The vision system uses multiple stereo cameras mounted on a fixed calibrated frame. Feature-based comparison and machine-learning based matching are used to identify and determine item pose. The gripper system uses suction cup and the grasping strategy is pick from the top. Error recovery strategies were also implemented to ensure robust performance. During the competition, the robot was able to pick all target items with the shortest amount of time.","",""
405,"David Alvarez-Melis, T. Jaakkola","Towards Robust Interpretability with Self-Explaining Neural Networks",2018,"","","","",96,"2022-07-13 09:38:31","","","","",,,,,405,101.25,203,2,4,"Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general – explicitness, faithfulness, and stability – and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.","",""
191,"Ilias Diakonikolas, Gautam Kamath, D. Kane, Jerry Li, J. Steinhardt, Alistair Stewart","Sever: A Robust Meta-Algorithm for Stochastic Optimization",2018,"","","","",97,"2022-07-13 09:38:31","","","","",,,,,191,47.75,32,6,4,"In high dimensions, most machine learning methods are brittle to even a small fraction of structured outliers. To address this, we introduce a new meta-algorithm that can take in a base learner such as least squares or stochastic gradient descent, and harden the learner to be resistant to outliers. Our method, Sever, possesses strong theoretical guarantees yet is also highly scalable -- beyond running the base learner itself, it only requires computing the top singular vector of a certain $n \times d$ matrix. We apply Sever on a drug design dataset and a spam classification dataset, and find that in both cases it has substantially greater robustness than several baselines. On the spam dataset, with $1\%$ corruptions, we achieved $7.4\%$ test error, compared to $13.4\%-20.5\%$ for the baselines, and $3\%$ error on the uncorrupted dataset. Similarly, on the drug design dataset, with $10\%$ corruptions, we achieved $1.42$ mean-squared error test error, compared to $1.51$-$2.33$ for the baselines, and $1.23$ error on the uncorrupted dataset.","",""
14,"Kevin Li, C. Gibson, D. Ho, Qi Zhou, J. Kim, O. Buhisi, D. Brown, M. Gerber","Assessment of machine learning algorithms in cloud computing frameworks",2013,"","","","",98,"2022-07-13 09:38:31","","10.1109/SIEDS.2013.6549501","","",,,,,14,1.56,2,8,9,"In the past decade, digitization of information has led to a data explosion in both volume and complexity. While traditional computing frameworks have failed to provide adequate computing power for the now common data-intensive computing tasks, cloud computing provides an effective alternative to enhance computing power. Machine learning algorithms are powerful analytical methods that allow machines to recognize patterns and facilitate human learning. However, the performance of individual machine learning algorithms within each cloud computing framework remains largely unknown. Furthermore, the lack of a robust selection methodology matching input data with effective machine learning algorithms limits the ability of practitioners to make effective use of cloud computing. This research compares various machine learning algorithms on the widely adopted Apache Mahout framework and the recently introduced GraphLab framework. Whereas previous work has examined the computational architectures of various cloud computing frameworks, this work focuses on a problem-based approach to architecture selection. The experimental results demonstrate that GraphLab generally outperforms Mahout with respect to runtime, scalability, and usability. However, Mahout outperforms GraphLab when the experiment focus shifts to error measurement.","",""
9,"Lei Luo, Heng Huang","Matrix Variate Gaussian Mixture Distribution Steered Robust Metric Learning",2018,"","","","",99,"2022-07-13 09:38:31","","10.1609/aaai.v32i1.11801","","",,,,,9,2.25,5,2,4,"    Mahalanobis Metric Learning (MML) has been actively studied recently in machine learning community. Most of existing MML methods aim to learn a powerful Mahalanobis distance for computing similarity of two objects. More recently, multiple methods use matrix norm regularizers to constrain the learned distance matrixMto improve the performance. However, in real applications, the structure of the distance matrix M is complicated and cannot be characterized well by the simple matrix norm. In this paper, we propose a novel robust metric learning method with learning the structure of the distance matrix in a new and natural way. We partition M into blocks and consider each block as a random matrix variate, which is fitted by matrix variate Gaussian mixture distribution. Different from existing methods, our model has no any assumption on M and automatically learns the structure of M from the real data, where the distance matrix M often is neither sparse nor low-rank. We design an effective algorithm to optimize the proposed model and establish the corresponding theoretical guarantee. We conduct extensive evaluations on the real-world data. Experimental results show our method consistently outperforms the related state-of-the-art methods.   ","",""
13,"Jainendra Singh","Big Data Analytic and Mining with Machine Learning Algorithm",2014,"","","","",100,"2022-07-13 09:38:31","","","","",,,,,13,1.63,13,1,8,"Big Data concern large-volume, complex, growing data sets with multiple, autonomous sources. With the fast development of networking, data storage, and the data collection capacity, Big Data are now rapidly expanding in all science and engineering domains, including physical, biological and biomedical sciences. This datadriven model involves demand-driven aggregation of information sources, mining and analysis, user interest modeling, and security and privacy considerations. We analyze the challenging issues in the datadriven model and also in the Big Data revolution. Advances in Machine Learning (ML) provide new challenges and solutions to the security problems encountered in applications, technologies and theories. Machine Learning (ML) techniques have found widespread applications and implementations in security issues. Many ML techniques, approaches, algorithms, methods and tools are extensively used by security experts and researchers to achieve better results and to design robust systems.","",""
26,"Mohammad Saidur Rahman, M. Imani, Nate Mathews, M. Wright","Mockingbird: Defending Against Deep-Learning-Based Website Fingerprinting Attacks With Adversarial Traces",2019,"","","","",101,"2022-07-13 09:38:31","","10.1109/TIFS.2020.3039691","","",,,,,26,8.67,7,4,3,"Website Fingerprinting (WF) is a type of traffic analysis attack that enables a local passive eavesdropper to infer the victim’s activity, even when the traffic is protected by a VPN or an anonymity system like Tor. Leveraging a deep-learning classifier, a WF attacker can gain over 98% accuracy on Tor traffic. In this paper, we explore a novel defense, Mockingbird, based on the idea of adversarial examples that have been shown to undermine machine-learning classifiers in other domains. Since the attacker gets to design and train his attack classifier based on the defense, we first demonstrate that at a straightforward technique for generating adversarial-example based traces fails to protect against an attacker using adversarial training for robust classification. We then propose Mockingbird, a technique for generating traces that resists adversarial training by moving randomly in the space of viable traces and not following more predictable gradients. The technique drops the accuracy of the state-of-the-art attack hardened with adversarial training from 98% to 42–58% while incurring only 58% bandwidth overhead. The attack accuracy is generally lower than state-of-the-art defenses, and much lower when considering Top-2 accuracy, while incurring lower bandwidth overheads.","",""
4,"S. Izrailev, Jeremy M. Stanley","Machine Learning at Scale",2014,"","","","",102,"2022-07-13 09:38:31","","","","",,,,,4,0.50,2,2,8,"It takes skill to build a meaningful predictive model even with the abundance of implementations of modern machine learning algorithms and readily available computing resources. Building a model becomes challenging if hundreds of terabytes of data need to be processed to produce the training data set. In a digital advertising technology setting, we are faced with the need to build thousands of such models that predict user behavior and power advertising campaigns in a 24/7 chaotic real-time production environment. As data scientists, we also have to convince other internal departments critical to implementation success, our management, and our customers that our machine learning system works. In this paper, we present the details of the design and implementation of an automated, robust machine learning platform that impacts billions of advertising impressions monthly. This platform enables us to continuously optimize thousands of campaigns over hundreds of millions of users, on multiple continents, against varying performance objectives.","",""
17,"Chen Dan, Yuting Wei, Pradeep Ravikumar","Sharp Statistical Guarantees for Adversarially Robust Gaussian Classification",2020,"","","","",103,"2022-07-13 09:38:31","","","","",,,,,17,8.50,6,3,2,"Adversarial robustness has become a fundamental requirement in modern machine learning applications. Yet, there has been surprisingly little statistical understanding so far. In this paper, we provide the first result of the optimal minimax guarantees for the excess risk for adversarially robust classification, under Gaussian mixture model proposed by \cite{schmidt2018adversarially}. The results are stated in terms of the Adversarial Signal-to-Noise Ratio (AdvSNR), which generalizes a similar notion for standard linear classification to the adversarial setting. For the Gaussian mixtures with AdvSNR value of $r$, we establish an excess risk lower bound of order $\Theta(e^{-(\frac{1}{8}+o(1)) r^2} \frac{d}{n})$ and design a computationally efficient estimator that achieves this optimal rate. Our results built upon minimal set of assumptions while cover a wide spectrum of adversarial perturbations including $\ell_p$ balls for any $p \ge 1$.","",""
17,"Hanrui Wang, Yongshan Ding, Jiaqi Gu, Yujun Lin, D. Pan, F. Chong, Song Han","QuantumNAS: Noise-Adaptive Search for Robust Quantum Circuits",2021,"","","","",104,"2022-07-13 09:38:31","","10.1109/HPCA53966.2022.00057","","",,,,,17,17.00,2,7,1,"Quantum noise is the key challenge in Noisy Intermediate-Scale Quantum (NISQ) computers. Previous work for mitigating noise has primarily focused on gate-level or pulse-level noise-adaptive compilation. However, limited research has explored a higher level of optimization by making the quantum circuits themselves resilient to noise.In this paper, we propose QuantumNAS, a comprehensive framework for noise-adaptive co-search of the variational circuit and qubit mapping. Variational quantum circuits are a promising approach for constructing quantum neural networks for machine learning and variational ansatzes for quantum simulation. However, finding the best variational circuit and its optimal parameters is challenging due to the large design space and parameter training cost. We propose to decouple the circuit search from parameter training by introducing a novel SuperCircuit. The SuperCircuit is constructed with multiple layers of pre-defined parameterized gates (e.g., U3 and CU3) and trained by iteratively sampling and updating the parameter subsets (SubCircuits) of it. It provides an accurate estimation of SubCircuits performance trained from scratch. Then we perform an evolutionary co-search of SubCircuit and its qubit mapping. The SubCircuit performance is estimated with parameters inherited from SuperCircuit and simulated with real device noise models. Finally, we perform iterative gate pruning and finetuning to remove redundant gates in a fine-grained manner.Extensively evaluated with 12 quantum machine learning (QML) and variational quantum eigensolver (VQE) benchmarks on 14 quantum computers, QuantumNAS significantly outperforms noise-unaware search, human, random, and existing noise-adaptive qubit mapping baselines. For QML tasks, QuantumNAS is the first to demonstrate over 95% 2-class, 85% 4-class, and 32% 10-class classification accuracy on real quantum computers. It also achieves the lowest eigenvalue for VQE tasks on H2, H2O, LiH, CH4, BeH2 compared with UCCSD baselines. We also open-source the TorchQuantum library for fast training of parameterized quantum circuits to facilitate future research.","",""
19,"Kai Yang, Yuanming Shi, Wei Yu, Z. Ding","Energy-Efficient Processing and Robust Wireless Cooperative Transmission for Edge Inference",2019,"","","","",105,"2022-07-13 09:38:31","","10.1109/JIOT.2020.2979523","","",,,,,19,6.33,5,4,3,"Edge machine learning can deliver low-latency and private artificial intelligent (AI) services for mobile devices by leveraging computation and storage resources at the network edge. This article presents an energy-efficient edge processing framework to execute deep learning inference tasks at the edge computing nodes whose wireless connections to mobile devices are prone to channel uncertainties. Aimed at minimizing the sum of computation and transmission power consumption with probabilistic Quality-of-Service (QoS) constraints, we formulate the joint inference tasking and the downlink beamforming problem that is characterized by a group sparse objective function. We provide a statistical learning-based robust optimization approach to approximate the highly intractable probabilistic-QoS constraints by nonconvex quadratic constraints, which are further reformulated as matrix inequalities with a rank-one constraint via matrix lifting. We design a reweighted power minimization approach by iteratively reweighted $\ell _{1}$ minimization with difference-of-convex-functions (DC) regularization and updating weights, where the reweighted approach is adopted for enhancing group sparsity whereas the DC regularization is designed for inducing rank-one solutions. The numerical results demonstrate that the proposed approach outperforms other state-of-the-art approaches.","",""
120,"C. Granade, C. Ferrie, N. Wiebe, D. Cory","Robust Online Hamiltonian Learning",2012,"","","","",106,"2022-07-13 09:38:31","","10.1088/1367-2630/14/10/103013","","",,,,,120,12.00,30,4,10,"In this work we combine two distinct machine learning methodologies, sequential Monte Carlo and Bayesian experimental design, and apply them to the problem of inferring the dynamical parameters of a quantum system. We design the algorithm with practicality in mind by including parameters that control trade-offs between the requirements on computational and experimental resources. The algorithm can be implemented online (during experimental data collection), avoiding the need for storage and post-processing. Most importantly, our algorithm is capable of learning Hamiltonian parameters even when the parameters change from experiment-to-experiment, and also when additional noise processes are present and unknown. The algorithm also numerically estimates the Cramer?Rao lower bound, certifying its own performance.","",""
10,"P. Hatfield, S. Rose, R. Scott, I. Almosallam, S. Roberts, M. Jarvis","Using Sparse Gaussian Processes for Predicting Robust Inertial Confinement Fusion Implosion Yields",2019,"","","","",107,"2022-07-13 09:38:31","","10.1109/TPS.2019.2944416","","",,,,,10,3.33,2,6,3,"Here, we present the application of an advanced sparse Gaussian process-based machine learning algorithm to the challenge of predicting the yields of inertial confinement fusion (ICF) experiments. The algorithm is used to investigate the parameter space of an extremely robust ICF design for the National Ignition Facility, the “Simplest Design”; deuterium–tritium gas in a plastic ablator with a Gaussian, Planckian drive. In particular, we show that: 1) GPz has the potential to decompose uncertainty on predictions into uncertainty from lack of data and shot-to-shot variation; 2) it permits the incorporation of science-goal-specific cost-sensitive learning (CSL), e.g., focusing on the high-yield parts of parameter space; and 3) it is very fast and effective in high dimensions.","",""
3,"B. Németh, P. Gáspár","Ensuring performance requirements for semiactive suspension with nonconventional control systems via robust linear parameter varying framework",2020,"","","","",108,"2022-07-13 09:38:31","","10.1002/rnc.5282","","",,,,,3,1.50,2,2,2,"In the article a method which is able to provide the required performance level of a system is proposed. Its principle is to combine the results of conventional control methods with those of methods based on nonconventional, for example, machine‐learning‐based ones. In more detail, it designs a robust linear parameter varying (LPV) control in a predefined form, whose output is equivalent to the output of a machine‐learning‐based control inside a predefined operational range. Outside of the operation range the output of the machine‐learning‐based control is overridden, while the intervention with the performance level is guaranteed. The efficiency of the proposed method is illustrated through an example on the semiactive suspension control design. The nonlinearities in the dynamics of the magneto‐rheological damper are considered through a nonlinear parameter varying (NLPV) model. It designs an NLPV model‐based LPV control, which is combined with a neural network to achieve preview capability.","",""
96,"M. Yu, David M'Raïhi, R. Sowell, S. Devadas","Lightweight and Secure PUF Key Storage Using Limits of Machine Learning",2011,"","","","",109,"2022-07-13 09:38:31","","10.1007/978-3-642-23951-9_24","","",,,,,96,8.73,24,4,11,"","",""
42,"Quanming Yao, Hansi Yang, Bo Han, Gang Niu, J. Kwok","Searching to Exploit Memorization Effect in Learning with Noisy Labels",2020,"","","","",110,"2022-07-13 09:38:31","","","","",,,,,42,21.00,8,5,2,"Sample selection approaches are popular in robust learning from noisy labels. However, how to properly control the selection process so that deep networks can benefit from the memorization effect is a hard problem. In this paper, motivated by the success of automated machine learning (AutoML), we model this issue as a function approximation problem. Specifically, we design a domain-specific search space based on general patterns of the memorization effect and propose a novel Newton algorithm to solve the bi-level optimization problem efficiently. We further provide theoretical analysis of the algorithm, which ensures a good approximation to critical points. Experiments are performed on both benchmark and real-world data sets. Results demonstrate that the proposed method is much better than the stateof-the-art noisy-label-learning approaches, and also much more efficient than existing AutoML algorithms.","",""
42,"Bo Han, Quanming Yao, Tongliang Liu, Gang Niu, I. Tsang, J. Kwok, M. Sugiyama","A Survey of Label-noise Representation Learning: Past, Present and Future",2020,"","","","",111,"2022-07-13 09:38:31","","","","",,,,,42,21.00,6,7,2,"Classical machine learning implicitly assumes that labels of the training data are sampled from a clean distribution, which can be too restrictive for real-world scenarios. However, statistical learning-based methods may not train deep learning models robustly with these noisy labels. Therefore, it is urgent to design Label-Noise Representation Learning (LNRL) methods for robustly training deep models with noisy labels. To fully understand LNRL, we conduct a survey study. We first clarify a formal definition for LNRL from the perspective of machine learning. Then, via the lens of learning theory and empirical study, we figure out why noisy labels affect deep models' performance. Based on the theoretical guidance, we categorize different LNRL methods into three directions. Under this unified taxonomy, we provide a thorough discussion of the pros and cons of different categories. More importantly, we summarize the essential components of robust LNRL, which can spark new directions. Lastly, we propose possible research directions within LNRL, such as new datasets, instance-dependent LNRL, and adversarial LNRL. Finally, we envision potential directions beyond LNRL, such as learning with feature-noise, preference-noise, domain-noise, similarity-noise, graph-noise, and demonstration-noise.","",""
16,"Daniel Spikol, K. Avramides, M. Cukurova","Exploring the interplay between human and machine annotated multimodal learning analytics in hands-on STEM activities",2016,"","","","",112,"2022-07-13 09:38:31","","10.1145/2883851.2883920","","",,,,,16,2.67,5,3,6,"This poster explores how to develop a working framework for STEM education that uses both human annotated and machine data across a purpose-built learning environment. Our dual approach is to develop a robust framework for analysis and investigate how to design a learning analytics system to support hands-on engineering design tasks. Data from the first user tests are presented along with the framework for discussion.","",""
5,"Yongwon Lee, S. Clearwater","Tools for automating experiment design: a machine learning approach",1992,"","","","",113,"2022-07-13 09:38:31","","10.1109/TAI.1992.246423","","",,,,,5,0.17,3,2,30,"Work that uses an inductive learning tool, HEP-RL (high-energy-physics rule learner), in the design of a very complex artifact, a high-energy-physics experiment, is reported. The important contribution is the observation that the results of learning provide a more complete and robust design. This is because there were end users of the learning able to suggest constraints beyond the usual simple coverage metrics. This allowed for more confidence in the design.<<ETX>>","",""
145,"Manoj Ghuhan Arivazhagan, V. Aggarwal, Aaditya Singh, Sunav Choudhary","Federated Learning with Personalization Layers",2019,"","","","",114,"2022-07-13 09:38:31","","","","",,,,,145,48.33,36,4,3,"The emerging paradigm of federated learning strives to enable collaborative training of machine learning models on the network edge without centrally aggregating raw data and hence, improving data privacy. This sharply deviates from traditional machine learning and necessitates the design of algorithms robust to various sources of heterogeneity. Specifically, statistical heterogeneity of data across user devices can severely degrade the performance of standard federated averaging for traditional machine learning applications like personalization with deep learning. This paper pro-posesFedPer, a base + personalization layer approach for federated training of deep feedforward neural networks, which can combat the ill-effects of statistical heterogeneity. We demonstrate effectiveness ofFedPerfor non-identical data partitions ofCIFARdatasetsand on a personalized image aesthetics dataset from Flickr.","",""
26,"Xinyang Cao, L. Lai","Distributed Gradient Descent Algorithm Robust to an Arbitrary Number of Byzantine Attackers",2019,"","","","",115,"2022-07-13 09:38:31","","10.1109/TSP.2019.2946020","","",,,,,26,8.67,13,2,3,"Due to the growth of modern dataset size and the desire to harness computing power of multiple machines, there is a recent surge of interest in the design of distributed machine learning algorithms. However, distributed algorithms are sensitive to Byzantine attackers who can send falsified data to prevent the convergence of algorithms or lead the algorithms to converge to value of the attackers’ choice. Some recent work proposed interesting algorithms that can deal with the scenario when up to half of the workers are compromised. In this paper, we propose a novel algorithm that can deal with an arbitrary number of Byzantine attackers. The main idea is to ask the parameter server to randomly select a small clean dataset and compute noisy gradient using this small dataset. This noisy gradient will then be used as a ground truth to filter out information sent by compromised workers. We show that the proposed algorithm converges to the neighborhood of the population minimizer regardless the number of Byzantine attackers. We further provide numerical examples to show that the proposed algorithm can benefit from the presence of good workers and achieve better performance than existing algorithms.","",""
23,"M. Lerasle, Z. Szabó, Guillaume Lecué, Gaspar Massiot, É. Moulines","MONK - Outlier-Robust Mean Embedding Estimation by Median-of-Means",2018,"","","","",116,"2022-07-13 09:38:31","","","","",,,,,23,5.75,5,5,4,"Mean embeddings provide an extremely flexible and powerful tool in machine learning and statistics to represent probability distributions and define a semi-metric (MMD, maximum mean discrepancy; also called N-distance or energy distance), with numerous successful applications. The representation is constructed as the expectation of the feature map defined by a kernel. As a mean, its classical empirical estimator, however, can be arbitrary severely affected even by a single outlier in case of unbounded features. To the best of our knowledge, unfortunately even the consistency of the existing few techniques trying to alleviate this serious sensitivity bottleneck is unknown. In this paper, we show how the recently emerged principle of median-of-means can be used to design estimators for kernel mean embedding and MMD with excessive resistance properties to outliers, and optimal sub-Gaussian deviation bounds under mild assumptions.","",""
19,"Santiago Alonso, J. Bobadilla, F. Ortega, Ricardo Moya","Robust Model-Based Reliability Approach to Tackle Shilling Attacks in Collaborative Filtering Recommender Systems",2019,"","","","",117,"2022-07-13 09:38:31","","10.1109/ACCESS.2019.2905862","","",,,,,19,6.33,5,4,3,"As the use of recommender systems becomes generalized in society, the interest in varying the orientation of their recommendations is increasing. There are shilling attacks’ strategies that introduce malicious profiles in collaborative filtering recommender systems in order to promote the own products or services or to discredit those of the competition. Academic research against shilling attacks has been focused in statistical approaches to detect the unusual patterns in user ratings. Nowadays, there is a growing research area focused on the design of robust machine learning methods to neutralize the malicious profiles inserted into the system. This paper proposes an innovative robust method, based on matrix factorization, to neutralize the shilling attacks. Our method obtains the reliability value associated with each prediction of a user to an item. By monitoring the unusual reliability variations in the items prediction, we can avoid promoting the shilling predictions to the erroneous recommendations. This paper openly provides more than 13 000 individual experiments involving a wide range of attack strategies, both push, and nuke, in order to test the proposed approach. The results show that the proposed method is able to neutralize most of the existing attacks; its performance only decreases in the not relevant situations: when the attack size is not large enough to effectively affect the recommendations provided by the system.","",""
125,"P. Laskov, R. Lippmann","Machine learning in adversarial environments",2010,"","","","",118,"2022-07-13 09:38:31","","10.1007/s10994-010-5207-6","","",,,,,125,10.42,63,2,12,"","",""
5,"Wenbo Zheng, Lan Yan, Chao Gou, Fei-Yue Wang","Fighting fire with fire: A spatial–frequency ensemble relation network with generative adversarial learning for adversarial image classification",2021,"","","","",119,"2022-07-13 09:38:31","","10.1002/int.22372","","",,,,,5,5.00,1,4,1,"Adversarial images generated by generative adversarial networks are not close to any existing benign images, and contain nonrobust features that have been identified as critical to the robustness of a machine learning model. Since adversarial images have an underlying distribution that differs from normal images, these kinds of images can offer valuable features for training a robust model. To deal with these special features, we focus on a novel machine learning task of adversarial images classification, where adversarial images can be used to investigate the problem of classifying adversarial images themselves. In the setting of this novel task, adversarial images are the ONLY kind of data used in training and testing, rather than not just a set of testing images as usual. To this end, we propose a novel spatial–frequency ensemble relation network with generative adversarial learning. First, we present a spatial–frequency ensemble representation learning to extract the feature of training images. Second, we design a meta‐learning‐based relation model to gain the relationship between images. Third, to achieve a robust model, we utilize generative adversarial learning and transform the relationship into a Jacobian matrix. Finally, we design a discriminator model that determines whether an adversarial image is from the matching category or not. Experimental results demonstrate that our approach achieves significantly higher performance compared with other state‐of‐the‐arts.","",""
29,"Alexander Wei, Fred Zhang","Optimal Robustness-Consistency Trade-offs for Learning-Augmented Online Algorithms",2020,"","","","",120,"2022-07-13 09:38:31","","","","",,,,,29,14.50,15,2,2,"We study the problem of improving the performance of online algorithms by incorporating machine-learned predictions. The goal is to design algorithms that are both consistent and robust, meaning that the algorithm performs well when predictions are accurate and maintains worst-case guarantees. Such algorithms have been studied in a recent line of works due to Lykouris and Vassilvitskii (ICML '18) and Purohit et al (NeurIPS '18). They provide robustness-consistency trade-offs for a variety of online problems. However, they leave open the question of whether these trade-offs are tight, i.e., to what extent to such trade-offs are necessary. In this paper, we provide the first set of non-trivial lower bounds for competitive analysis using machine-learned predictions. We focus on the classic problems of ski-rental and non-clairvoyant scheduling and provide optimal trade-offs in various settings.","",""
32,"Alexander Wei","Better and Simpler Learning-Augmented Online Caching",2020,"","","","",121,"2022-07-13 09:38:31","","10.4230/LIPIcs.APPROX/RANDOM.2020.60","","",,,,,32,16.00,32,1,2,"Lykouris and Vassilvitskii (ICML 2018) introduce a model of online caching with machine-learned advice, where each page request additionally comes with a prediction of when that page will next be requested. In this model, a natural goal is to design algorithms that (1) perform well when the advice is accurate and (2) remain robust in the worst case a la traditional competitive analysis. Lykouris and Vassilvitskii give such an algorithm by adapting the Marker algorithm to the learning-augmented setting. In a recent work, Rohatgi (SODA 2020) improves on their result with an approach also inspired by randomized marking. We continue the study of this problem, but with a somewhat different approach: We consider combining the BlindOracle algorithm, which just naively follows the predictions, with an optimal competitive algorithm for online caching in a black-box manner. The resulting algorithm outperforms all existing approaches while being significantly simpler. Moreover, we show that combining BlindOracle with LRU is in fact optimal among deterministic algorithms for this problem.","",""
102,"Jiaqi Jiang, Ming-Keh Chen, Jonathan A. Fan","Deep neural networks for the evaluation and design of photonic devices",2020,"","","","",122,"2022-07-13 09:38:31","","10.1038/s41578-020-00260-1","","",,,,,102,51.00,34,3,2,"","",""
10,"Jing Ma, Si-Ahmed Naas, S. Sigg, X. Lyu","Privacy-preserving Federated Learning based on Multi-key Homomorphic Encryption",2021,"","","","",123,"2022-07-13 09:38:31","","10.1002/int.22818","","",,,,,10,10.00,3,4,1,"With the advance of machine learning and the internet of things (IoT), security and privacy have become key concerns in mobile services and networks. Transferring data to a central unit violates privacy as well as protection of sensitive data while increasing bandwidth demands. Federated learning mitigates this need to transfer local data by sharing model updates only. However, data leakage still remains an issue. In this paper, we propose xMK-CKKS, a multi-key homomorphic encryption protocol to design a novel privacy-preserving federated learning scheme. In this scheme, model updates are encrypted via an aggregated public key before sharing with a server for aggregation. For decryption, collaboration between all participating devices is required. This scheme prevents privacy leakage from publicly shared information in federated learning, and is robust to collusion between k < N − 1 participating devices and the server. Our experimental evaluation demonstrates that the scheme preserves model accuracy against traditional federated learning as well as secure federated learning with homomorphic encryption (MK-CKKS, Paillier) and reduces computational cost compared to Paillier based federated learning. The average energy consumption is 2.4 Watts, so that it is suited to IoT scenarios.","",""
13,"Andreu Sancho-Asensio, Albert Orriols-Puig, E. Golobardes","Robust on-line neural learning classifier system for data stream classification tasks",2014,"","","","",124,"2022-07-13 09:38:31","","10.1007/s00500-014-1233-9","","",,,,,13,1.63,4,3,8,"","",""
34,"Mark C. Ballandies, Marcus M. Dapp, Evangelos Pournaras","Decrypting distributed ledger design—taxonomy, classification and blockchain community evaluation",2018,"","","","",125,"2022-07-13 09:38:31","","10.1007/s10586-021-03256-w","","",,,,,34,8.50,11,3,4,"","",""
16,"Yujie Ying, J. Harley, J. Garrett, Yuanwei Jin, I. Oppenheim, Jun Shi, L. Soibelman","Applications of Machine Learning in Pipeline Monitoring",2011,"","","","",126,"2022-07-13 09:38:31","","10.1061/41182(416)30","","",,,,,16,1.45,2,7,11,"In the field of structural health monitoring, researchers focus on the design of systems and techniques capable of detecting damage in structures. However, most traditional detection methods fail under environmental and operational variations that tend to distort the signals and masquerade as damage. In this paper, we investigate the applications of machine learning techniques to developing a damage detection system robust to changes in the internal air pressure of a pipe. From each of the 240 experimental datasets, we extract 167 features and implement three classification algorithms for detecting damage: adaptive boosting, support vector machines, and a method combining the two. The performances of the three classifiers are evaluated over 30 detection trials with different combinations of training and testing data, resulting in the average accuracies of 87.7%, 92.5% and 93.5%, respectively. The combined method is a promising classifier for damage detection. Through feature selection, we also demonstrate the effectiveness of features related to the curve length, the shift-invariant correlation coefficient and the peak amplitude of the signal.","",""
82,"Seong-Yong Jeong, Junsik Kim, Jong‐Heun Lee","Rational Design of Semiconductor‐Based Chemiresistors and their Libraries for Next‐Generation Artificial Olfaction",2020,"","","","",127,"2022-07-13 09:38:31","","10.1002/adma.202002075","","",,,,,82,41.00,27,3,2,"Artificial olfaction based on gas sensor arrays aims to substitute for, support, and surpass human olfaction. Like mammalian olfaction, a larger number of sensors and more signal processing are crucial for strengthening artificial olfaction. Due to rapid progress in computing capabilities and machine‐learning algorithms, on‐demand high‐performance artificial olfaction that can eclipse human olfaction becomes inevitable once diverse and versatile gas sensing materials are provided. Here, rational strategies to design a myriad of different semiconductor‐based chemiresistors and to grow gas sensing libraries enough to identify a wide range of odors and gases are reviewed, discussed, and suggested. Key approaches include the use of p‐type oxide semiconductors, multinary perovskite and spinel oxides, carbon‐based materials, metal chalcogenides, their heterostructures, as well as heterocomposites as distinctive sensing materials, the utilization of bilayer sensor design, the design of robust sensing materials, and the high‐throughput screening of sensing materials. In addition, the state‐of‐the‐art and key issues in the implementation of electronic noses are discussed. Finally, a perspective on chemiresistive sensing materials for next‐generation artificial olfaction is provided.","",""
5,"Qi Guo, Tianshi Chen, Zhi-Hua Zhou, O. Temam, Ling Li, D. Qian, Yunji Chen","Robust Design Space Modeling",2015,"","","","",128,"2022-07-13 09:38:31","","10.1145/2668118","","",,,,,5,0.71,1,7,7,"Architectural design spaces of microprocessors are often exponentially large with respect to the pending processor parameters. To avoid simulating all configurations in the design space, machine learning and statistical techniques have been utilized to build regression models for characterizing the relationship between architectural configurations and responses (e.g., performance or power consumption). However, this article shows that the accuracy variability of many learning techniques over different design spaces and benchmarks can be significant enough to mislead the decision-making. This clearly indicates a high risk of applying techniques that work well on previous modeling tasks (each involving a design space, benchmark, and design objective) to a new task, due to which the powerful tools might be impractical.  Inspired by ensemble learning in the machine learning domain, we propose a robust framework called ELSE to reduce the accuracy variability of design space modeling. Rather than employing a single learning technique as in previous investigations, ELSE employs distinct learning techniques to build multiple base regression models for each modeling task. This is not a trivial combination of different techniques (e.g., always trusting the regression model with the smallest error). Instead, ELSE carefully maintains the diversity of base regression models and constructs a metamodel from the base models that can provide accurate predictions even when the base models are far from accurate. Consequently, we are able to reduce the number of cases in which the final prediction errors are unacceptably large. Experimental results validate the robustness of ELSE: compared with the widely used artificial neural network over 52 distinct modeling tasks, ELSE reduces the accuracy variability by about 62%. Moreover, ELSE reduces the average prediction error by 27% and 85% for the investigated MIPS and POWER design spaces, respectively.","",""
18,"Kai Liang Tan, Yasaman Esfandiari, Xian Yeow Lee, Aakanksha, S. Sarkar","Robustifying Reinforcement Learning Agents via Action Space Adversarial Training",2020,"","","","",129,"2022-07-13 09:38:31","","10.23919/ACC45564.2020.9147846","","",,,,,18,9.00,4,5,2,"Adoption of machine learning (ML)-enabled cyber-physical systems (CPS) are becoming prevalent in various sectors of modern society such as transportation, industrial, and power grids. Recent studies in deep reinforcement learning (DRL) have demonstrated its benefits in a large variety of data-driven decisions and control applications. As reliance on ML-enabled systems grows, it is imperative to study the performance of these systems under malicious state and actuator attacks. Traditional control systems employ resilient/fault-tolerant controllers that counter these attacks by correcting the system via error observations. However, in some applications, a resilient controller may not be sufficient to avoid a catastrophic failure. Ideally, a robust approach is more useful in these scenarios where a system is inherently robust (by design) to adversarial attacks. While robust control has a long history of development, robust ML is an emerging research area that has already demonstrated its relevance and urgency. However, the majority of robust ML research has focused on perception tasks and not on decision and control tasks, although the ML (specifically RL) models used for control applications are equally vulnerable to adversarial attacks. In this paper, we show that a well-performing DRL agent that is initially susceptible to action space perturbations (e.g. actuator attacks) can be robustified against similar perturbations through adversarial training.","",""
9,"Dmytro Katrychuk, Henry K. Griffith, Oleg V. Komogortsev","Power-efficient and shift-robust eye-tracking sensor for portable VR headsets",2019,"","","","",130,"2022-07-13 09:38:31","","10.1145/3314111.3319821","","",,,,,9,3.00,3,3,3,"Photosensor oculography (PSOG) is a promising solution for reducing the computational requirements of eye tracking sensors in wireless virtual and augmented reality platforms. This paper proposes a novel machine learning-based solution for addressing the known performance degradation of PSOG devices in the presence of sensor shifts. Namely, we introduce a convolutional neural network model capable of providing shift-robust end-to-end gaze estimates from the PSOG array output. Moreover, we propose a transfer-learning strategy for reducing model training time. Using a simulated workflow with improved realism, we show that the proposed convolutional model offers improved accuracy over a previously considered multilayer perceptron approach. In addition, we demonstrate that the transfer of initialization weights from pre-trained models can substantially reduce training time for new users. In the end, we provide the discussion regarding the design trade-offs between accuracy, training time, and power consumption among the considered models.","",""
262,"Yu Jiang, Zhong-Ping Jiang","Robust Adaptive Dynamic Programming and Feedback Stabilization of Nonlinear Systems",2014,"","","","",131,"2022-07-13 09:38:31","","10.1109/TNNLS.2013.2294968","","",,,,,262,32.75,131,2,8,"This paper studies the robust optimal control design for a class of uncertain nonlinear systems from a perspective of robust adaptive dynamic programming (RADP). The objective is to fill up a gap in the past literature of adaptive dynamic programming (ADP) where dynamic uncertainties or unmodeled dynamics are not addressed. A key strategy is to integrate tools from modern nonlinear control theory, such as the robust redesign and the backstepping techniques as well as the nonlinear small-gain theorem, with the theory of ADP. The proposed RADP methodology can be viewed as an extension of ADP to uncertain nonlinear systems. Practical learning algorithms are developed in this paper, and have been applied to the controller design problems for a jet engine and a one-machine power system.","",""
150,"Dipendra Jha, Logan T. Ward, Arindam Paul, W. Liao, A. Choudhary, C. Wolverton, Ankit Agrawal","ElemNet: Deep Learning the Chemistry of Materials From Only Elemental Composition",2018,"","","","",132,"2022-07-13 09:38:31","","10.1038/s41598-018-35934-y","","",,,,,150,37.50,21,7,4,"","",""
8,"Shao Cheng","Robust Stability of Optimal Iterative Learning Control and Application to Injection Molding Machine",2003,"","","","",133,"2022-07-13 09:38:31","","","","",,,,,8,0.42,8,1,19,"A design of robust iterative learning controller is presented. A sufficient and necessary condition to ensure robust BIBO (bounded-input bounded-output) stability is derived for the optimal iterative learning controllers when tracking arbitrary bounded output. A practical scheme of selecting weighting matrices is proposed for the process with uncertain initial resetting and disturbances to ensure improvement of system performance from batch to batch. An application to the injection molding control is given to demonstrate the effectiveness of the proposed results.","",""
6,"R. Michalski","Advances in Machine Learning II, Dedicated to the Memory of Professor Ryszard S. Michalski",2010,"","","","",134,"2022-07-13 09:38:31","","10.1007/978-3-642-05179-1","","",,,,,6,0.50,6,1,12,"","",""
67,"Dingjiang Huang, Junlong Zhou, B. Li, S. Hoi, Shuigeng Zhou","Robust Median Reversion Strategy for Online Portfolio Selection",2013,"","","","",135,"2022-07-13 09:38:31","","10.1109/TKDE.2016.2563433","","",,,,,67,7.44,13,5,9,"Online portfolio selection has attracted increasing attention from data mining and machine learning communities in recent years. An important theory in financial markets is mean reversion, which plays a critical role in some state-of-the-art portfolio selection strategies. Although existing mean reversion strategies have been shown to achieve good empirical performance on certain datasets, they seldom carefully deal with noise and outliers in the data, leading to suboptimal portfolios, and consequently yielding poor performance in practice. In this paper, we propose to exploit the reversion phenomenon by using robust <inline-formula><tex-math notation=""LaTeX""> $L_1$</tex-math><alternatives><inline-graphic xlink:type=""simple"" xlink:href=""huang-ieq1-2563433.gif""/></alternatives> </inline-formula>-median estimators, and design a novel online portfolio selection strategy named “Robust Median Reversion” (RMR), which constructs optimal portfolios based on the improved reversion estimator. We examine the performance of the proposed algorithms on various real markets with extensive experiments. Empirical results show that RMR can overcome the drawbacks of existing mean reversion algorithms and achieve significantly better results. Finally, RMR runs in linear time, and thus is suitable for large-scale real-time algorithmic trading applications.","",""
29,"Florian Hartmann, Sunah Suh, Arkadiusz Komarzewski, Tim Smith, I. Segall","Federated Learning for Ranking Browser History Suggestions",2019,"","","","",136,"2022-07-13 09:38:31","","","","",,,,,29,9.67,6,5,3,"Federated Learning is a new subfield of machine learning that allows fitting models without collecting the training data itself. Instead of sharing data, users collaboratively train a model by only sending weight updates to a server. To improve the ranking of suggestions in the Firefox URL bar, we make use of Federated Learning to train a model on user interactions in a privacy-preserving way. This trained model replaces a handcrafted heuristic, and our results show that users now type over half a character less to find what they are looking for. To be able to deploy our system to real users without degrading their experience during training, we design the optimization process to be robust. To this end, we use a variant of Rprop for optimization, and implement additional safeguards. By using a numerical gradient approximation technique, our system is able to optimize anything in Firefox that is currently based on handcrafted heuristics. Our paper shows that Federated Learning can be used successfully to train models in privacy-respecting ways.","",""
95,"U. Maulik, D. Chakraborty","Remote Sensing Image Classification: A survey of support-vector-machine-based advanced techniques",2017,"","","","",137,"2022-07-13 09:38:31","","10.1109/MGRS.2016.2641240","","",,,,,95,19.00,48,2,5,"Land-cover mapping in remote sensing (RS) applications renders rich information for decision support and environmental monitoring systems. The derivation of such information increasingly relies on robust classification methods for identifying the complex land-cover area of different categories. Numerous classification techniques have been designed for the analysis of RS imagery. In this context, support vector machines (SVMs) have recently received increasing interest. However, the need for a small-size training set remains a bottleneck to design efficient supervised classifiers, while an adequate number of unlabeled data is readily available in RS images and can be exploited as a supplementary source of information. To fully leverage these precious unlabeled data, a number of promising advanced SVM-based methods, such as active SVMs, semisupervised SVMs (S3VMs), and SVMs combined with other algorithms, have been developed to analyze satellite imagery. In this literature review, we have surveyed these learning techniques to explore RS images. Moreover, we have provided the empirical evidences of SVMs and three representative techniques. It is our hope that this review will provide guidelines to future researchers to enhance further algorithmic developments in RS applications.","",""
15,"J. Burton, I. Ijjaali, F. Petitet, A. Michel, D. P. Vercauteren","Virtual screening for cytochromes p450: successes of machine learning filters.",2009,"","","","",138,"2022-07-13 09:38:31","","10.2174/138620709788167935","","",,,,,15,1.15,3,5,13,"Cytochromes P450 (CYPs) are crucial targets when predicting the ADME properties (absorption, distribution, metabolism, and excretion) of drugs in development. Particularly, CYPs mediated drug-drug interactions are responsible for major failures in the drug design process. Accurate and robust screening filters are thus needed to predict interactions of potent compounds with CYPs as early as possible in the process. In recent years, more and more 3D structures of various CYP isoforms have been solved, opening the gate of accurate structure-based studies of interactions. Nevertheless, the ligand-based approach still remains popular. This success can be explained by the growing number of available data and the satisfying performances of existing machine learning (ML) methods. The aim of this contribution is to give an overview of the recent achievements in ML applications to CYP datasets. Particularly, popular methods such as support vector machine, decision trees, artificial neural networks, k-nearest neighbors, and partial least squares will be compared as well as the quality of the datasets and the descriptors used. Consensus of different methods will also be discussed. Often reaching 90% of accuracy, the models will be analyzed to highlight the key descriptors permitting the good prediction of CYPs binding.","",""
85,"Jessica G. Freeze, H. R. Kelly, V. Batista","Search for Catalysts by Inverse Design: Artificial Intelligence, Mountain Climbers, and Alchemists.",2019,"","","","",139,"2022-07-13 09:38:31","","10.1021/acs.chemrev.8b00759","","",,,,,85,28.33,28,3,3,"In silico catalyst design is a grand challenge of chemistry. Traditional computational approaches have been limited by the need to compute properties for an intractably large number of possible catalysts. Recently, inverse design methods have emerged, starting from a desired property and optimizing a corresponding chemical structure. Techniques used for exploring chemical space include gradient-based optimization, alchemical transformations, and machine learning. Though the application of these methods to catalysis is in its early stages, further development will allow for robust computational catalyst design. This review provides an overview of the evolution of inverse design approaches and their relevance to catalysis. The strengths and limitations of existing techniques are highlighted, and suggestions for future research are provided.","",""
15,"Hassan Musafer, Abdel-shakour Abuzneid, M. Faezipour, A. Mahmood","An Enhanced Design of Sparse Autoencoder for Latent Features Extraction Based on Trigonometric Simplexes for Network Intrusion Detection Systems",2020,"","","","",140,"2022-07-13 09:38:31","","10.3390/electronics9020259","","",,,,,15,7.50,4,4,2,"Despite the successful contributions in the field of network intrusion detection using machine learning algorithms and deep networks to learn the boundaries between normal traffic and network attacks, it is still challenging to detect various attacks with high performance. In this paper, we propose a novel mathematical model for further development of robust, reliable, and efficient software for practical intrusion detection applications. In this present work, we are concerned with optimal hyperparameters tuned for high performance sparse autoencoders for optimizing features and classifying normal and abnormal traffic patterns. The proposed framework allows the parameters of the back-propagation learning algorithm to be tuned with respect to the performance and architecture of the sparse autoencoder through a sequence of trigonometric simplex designs. These hyperparameters include the number of nodes in the hidden layer, learning rate of the hidden layer, and learning rate of the output layer. It is expected to achieve better results in extracting features and adapting to various levels of learning hierarchy as different layers of the autoencoder are characterized by different learning rates in the proposed framework. The idea is viewed such that every learning rate of a hidden layer is a dimension in a multidimensional space. Hence, a vector of the adaptive learning rates is implemented for the multiple layers of the network to accelerate the processing time that is required for the network to learn the mapping towards a combination of enhanced features and the optimal synaptic weights in the multiple layers for a given problem. The suggested framework is tested on CICIDS2017, a reliable intrusion detection dataset that covers all the common, updated intrusions and cyber-attacks. Experimental results demonstrate that the proposed architecture for intrusion detection yields superior performance compared to recently published algorithms in terms of classification accuracy and F-measure results.","",""
11,"He Yan, Qiaolin Ye, Dong-Jun Yu","Efficient and robust TWSVM classification via a minimum L1-norm distance metric criterion",2018,"","","","",141,"2022-07-13 09:38:31","","10.1007/s10994-018-5771-8","","",,,,,11,2.75,4,3,4,"","",""
33,"A. Beygelzimer, J. Langford, B. Zadrozny","Machine Learning Techniques—Reductions Between Prediction Quality Metrics",2008,"","","","",142,"2022-07-13 09:38:31","","10.1007/978-0-387-79361-0_1","","",,,,,33,2.36,11,3,14,"","",""
39,"B. Biggio, G. Fumera, F. Roli","Design of robust classifiers for adversarial environments",2011,"","","","",143,"2022-07-13 09:38:31","","10.1109/ICSMC.2011.6083796","","",,,,,39,3.55,13,3,11,"In adversarial classification tasks like spam filtering, intrusion detection in computer networks, and biometric identity verification, malicious adversaries can design attacks which exploit vulnerabilities of machine learning algorithms to evade detection, or to force a classification system to generate many false alarms, making it useless. Several works have addressed the problem of designing robust classifiers against these threats, although mainly focusing on specific applications and kinds of attacks. In this work, we propose a model of data distribution for adversarial classification tasks, and exploit it to devise a general method for designing robust classifiers, focusing on generative classifiers. Our method is then evaluated on two case studies concerning biometric identity verification and spam filtering.","",""
57,"Seung-Jean Kim, Stephen P. Boyd","A minimax theorem with applications to machine learning, signal processing, and finance",2007,"","","","",144,"2022-07-13 09:38:31","","10.1137/060677586","","",,,,,57,3.80,29,2,15,"This paper concerns a fractional function of the form xTalpha/radicxTBx, where B is positive definite. We consider the game of choosing x from a convex set, to maximize the function, and choosing (alpha, B) from a convex set, to minimize it. We prove the existence of a saddle point and describe an efficient method, based on convex optimization, for computing it. We describe applications in machine learning (robust Fisher linear discriminant analysis), signal processing (robust beam- forming, robust matched filtering), and finance (robust portfolio selection). In these applications, x corresponds to some design variables to be chosen, and the pair (alpha, B) corresponds to the statistical model, which is uncertain.","",""
23,"Chong Peng, Zhao Kang, Yunhong Hu, Jie Cheng, Q. Cheng","Robust Graph Regularized Nonnegative Matrix Factorization for Clustering",2017,"","","","",145,"2022-07-13 09:38:31","","10.1145/3003730","","",,,,,23,4.60,5,5,5,"Matrix factorization is often used for data representation in many data mining and machine-learning problems. In particular, for a dataset without any negative entries, nonnegative matrix factorization (NMF) is often used to find a low-rank approximation by the product of two nonnegative matrices. With reduced dimensions, these matrices can be effectively used for many applications such as clustering. The existing methods of NMF are often afflicted with their sensitivity to outliers and noise in the data. To mitigate this drawback, in this paper, we consider integrating NMF into a robust principal component model, and design a robust formulation that effectively captures noise and outliers in the approximation while incorporating essential nonlinear structures. A set of comprehensive empirical evaluations in clustering applications demonstrates that the proposed method has strong robustness to gross errors and superior performance to current state-of-the-art methods.","",""
23,"A. Mahabuba, M. A. Khan","Small signal stability enhancement of a multi-machine power system using robust and adaptive fuzzy neural network-based power system stabilizer",2009,"","","","",146,"2022-07-13 09:38:31","","10.1002/ETEP.276","","",,,,,23,1.77,12,2,13,"This paper presents a design procedure for a robust and adaptive fuzzy neural network-based power system stabilizer (RAFNNPSS) and investigates the robustness and adaptive feature of the RAFNNPSS for a single machine connected to an infinite bus system and multi-machine power systems in order to enhance the dynamic stability (small signal stability of the system). The parameters of RAFNNPSS are tuned by adaptive neural network (NN). This RAFNNPSS uses adaptive network-based fuzzy inference system (ANFIS) network, which provides a natural framework of multi-layered feed forward adaptive network using fuzzy logic inference system. In this approach, the hybrid-learning algorithm tunes the fuzzy rules and the membership functions of the RAFNNPSS. Speed deviation of synchronous generator and its derivative are chosen as the input signals to the RAFNNPSS. The dynamic performance of single-machine infinite bus (SMIB) system, a two-area, five-machine, eight-bus power system and a large power system (10-machine, 39-bus New England system) with the proposed RAFNNPSS under different operating conditions and change in system parameters have been investigated. The simulation results obtained from the conventional PSS (CPSS) and Fuzzy logic-based PSS (FPSS) are compared with the proposed RAFNNPSS. The simulation results demonstrate that the proposed RAFNNPSS performs well in damping and quicker response when compared with the other two PSSs. Copyright © 2008 John Wiley & Sons, Ltd.","",""
5,"Xinyang Cao, L. Lai","Robust Distributed Gradient Descent with Arbitrary Number of Byzantine Attackers",2018,"","","","",147,"2022-07-13 09:38:31","","10.1109/ICASSP.2018.8461691","","",,,,,5,1.25,3,2,4,"Due to the grow of modern dataset size and the desire to harness computing power of multiple machines, there is a recent surge of interest in the design of distributed machine learning algorithms. However, distributed algorithms are sensitive to Byzantine attackers who can send falsified data to prevent the convergence of algorithms or lead the algorithms to converge to value of the attackers' choice. Some recent work proposed interesting algorithms that can deal with the scenario when up to half of the workers are compromised. In this paper, we propose a novel algorithm that can deal with an arbitrary number of Byzantine attackers.","",""
12,"Jinlong Ji, Xuhui Chen, Qianlong Wang, Lixing Yu, Pan Li","Learning to Learn Gradient Aggregation by Gradient Descent",2019,"","","","",148,"2022-07-13 09:38:31","","10.24963/ijcai.2019/363","","",,,,,12,4.00,2,5,3,"In the big data era, distributed machine learning emerges as an important learning paradigm to mine large volumes of data by taking advantage of distributed computing resources. In this work, motivated by learning to learn, we propose a meta-learning approach to coordinate the learning process in the master-slave type of distributed systems. Specifically, we utilize a recurrent neural network (RNN) in the parameter server (the master) to learn to aggregate the gradients from the workers (the slaves). We design a coordinatewise preprocessing and postprocessing method to make the neural network based aggregator more robust. Besides, to address the fault tolerance, especially the Byzantine attack, in distributed machine learning systems, we propose an RNN aggregator with additional loss information (ARNN) to improve the system resilience. We conduct extensive experiments to demonstrate the effectiveness of the RNN aggregator, and also show that it can be easily generalized and achieve remarkable performance when transferred to other distributed systems. Moreover, under majoritarian Byzantine attacks, the ARNN aggregator outperforms the Krum, the state-of-art fault tolerance aggregation method, by 43.14%. In addition, our RNN aggregator enables the server to aggregate gradients from variant local models, which significantly improve the scalability of distributed learning.","",""
34,"Pu Zhao, Sijia Liu, Pin-Yu Chen, Nghia Hoang, Kaidi Xu, B. Kailkhura, Xue Lin","On the Design of Black-Box Adversarial Examples by Leveraging Gradient-Free Optimization and Operator Splitting Method",2019,"","","","",149,"2022-07-13 09:38:31","","10.1109/ICCV.2019.00021","","",,,,,34,11.33,5,7,3,"Robust machine learning is currently one of the most prominent topics which could potentially help shaping a future of advanced AI platforms that not only perform well in average cases but also in worst cases or adverse situations. Despite the long-term vision, however, existing studies on black-box adversarial attacks are still restricted to very specific settings of threat models (e.g., single distortion metric and restrictive assumption on target model's feedback to queries) and/or suffer from prohibitively high query complexity. To push for further advances in this field, we introduce a general framework based on an operator splitting method, the alternating direction method of multipliers (ADMM) to devise efficient, robust black-box attacks that work with various distortion metrics and feedback settings without incurring high query complexity. Due to the black-box nature of the threat model, the proposed ADMM solution framework is integrated with zeroth-order (ZO) optimization and Bayesian optimization (BO), and thus is applicable to the gradient-free regime. This results in two new black-box adversarial attack generation methods, ZO-ADMM and BO-ADMM. Our empirical evaluations on image classification datasets show that our proposed approaches have much lower function query complexities compared to state-of-the-art attack methods, but achieve very competitive attack success rates.","",""
58,"V. Sessions, M. Valtorta","The Effects of Data Quality on Machine Learning Algorithms",2006,"","","","",150,"2022-07-13 09:38:31","","","","",,,,,58,3.63,29,2,16,"Our research focuses on two goals. First, we seek to demonstrate that data quality is an important component of machine learning tools and that the quality of our data should be carefully considered when developing and using these tools. We believe that while the importance of data quality is now understood in the business community, where researchers have equated quality decision making to earnings, in the machine learning community this realization has not yet occurred. Therefore, we embark upon research into the effects of data quality upon these algorithms in an effort to demonstrate that data quality is a large factor in the outcomes of these algorithms and should be given more respect in their design. Second, after providing evidence that data quality is a large factor in these algorithms, we develop and test some preliminary methods which incorporate data quality assessments, thus creating more robust and useful algorithms. We believe these modifications and methods can have profound effects upon the usage of these machine learning algorithms in actual practice, particularly in the Law Enforcement community.","",""
7,"Takao Sato, H. Tajika, R. Vilanova, Y. Konishi","Adaptive PID control system with assigned robust stability",2018,"","","","",151,"2022-07-13 09:38:31","","10.1002/tee.22680","","",,,,,7,1.75,2,4,4,"This paper presents a proportional‐integral‐derivative (PID) control system design method for discrete‐time first‐order plus dead‐time plants. Because the relation between the tracking performance and the robust stability is the trade‐off, the PID parameters of the PID control system are decided such that the tracking performance is optimized subject to the assigned robust stability, where the reference or disturbance response is selected as the tracking performance. In the proposed method, the PID parameters decision method is designed using a neural network with an extreme learning machine. Hence, the optimal PID parameters are obtained based on the trade‐off between the tracking performance and the robust stability. Furthermore, the proposed design method is extended as the adaptive control, and hence an unknown or time‐varying plant, is well controlled using the proposed method. © 2018 Institute of Electrical Engineers of Japan. Published by John Wiley & Sons, Inc.","",""
14,"G. Currie, Helena N Angel-Scott, L. Colvin, Fala Cramond, Kaitlyn Hair, Laila Khandoker, Jing Liao, M. Macleod, S. McCann, R. Morland, N. Sherratt, R. Stewart, Ezgi Tanriver-Ayder, James Thomas, Qianying Wang, R. Wodarski, Ran Xiong, A. Rice, E. Sena","Animal models of chemotherapy-induced peripheral neuropathy: A machine-assisted systematic review and meta-analysis",2018,"","","","",152,"2022-07-13 09:38:31","","10.1101/293480","","",,,,,14,3.50,1,19,4,"Background and aims Chemotherapy-induced peripheral neuropathy (CIPN) can be a severely disabling side-effect of commonly used cancer chemotherapeutics, requiring cessation or dose reduction, impacting on survival and quality of life. Our aim was to conduct a systematic review and meta-analysis of research using animal models of CIPN to inform robust experimental design. Methods We systematically searched 5 online databases (PubMed, Web of Science, Citation Index, Biosis Previews and Embase (September 2012) to identify publications reporting in vivo CIPN modelling. Due to the number of publications and high accrual rate of new studies, we ran an updated search November 2015, using machine-learning and text mining to identify relevant studies. All data were abstracted by two independent reviewers. For each comparison we calculated a standardised mean difference effect size then combined effects in a random effects meta- analysis. The impact of study design factors and reporting of measures to reduce the risk of bias was assessed. We ran power analysis for the most commonly reported behavioural tests. Results 341 publications were included. The majority (84%) of studies reported using male animals to model CIPN; the most commonly reported strain was Sprague Dawley rat. In modelling experiments, Vincristine was associated with the greatest increase in pain-related behaviour (−3.22 SD [−3.88; −2.56], n=152, p=0). The most commonly reported outcome measure was evoked limb withdrawal to mechanical monofilaments. Pain-related complex behaviours were rarely reported. The number of animals required to obtain 80% power with a significance level of 0.05 varied substantially across behavioural tests. Overall, studies were at moderate risk of bias, with modest reporting of measures to reduce the risk of bias. Conclusions Here we provide a comprehensive summary of the field of animal models of CIPN and inform robust experimental design by highlighting measures to increase the internal and external validity of studies using animal models of CIPN. Power calculations and other factors, such as clinical relevance, should inform the choice of outcome measure in study design.","",""
51,"D. Yeung, Zhi-Qiang Liu, Xizhao Wang, Hong-yan Yan","Advances in Machine Learning and Cybernetics, 4th International Conference, ICMLC 2005, Guangzhou, China, August 18-21, 2005, Revised Selected Papers",2006,"","","","",153,"2022-07-13 09:38:31","","10.1007/11739685","","",,,,,51,3.19,13,4,16,"","",""
86,"Yu Jiang, Zhong-Ping Jiang","Robust Adaptive Dynamic Programming With an Application to Power Systems",2013,"","","","",154,"2022-07-13 09:38:31","","10.1109/TNNLS.2013.2249668","","",,,,,86,9.56,43,2,9,"This brief presents a novel framework of robust adaptive dynamic programming (robust-ADP) aimed at computing globally stabilizing and suboptimal control policies in the presence of dynamic uncertainties. A key strategy is to integrate ADP theory with techniques in modern nonlinear control with a unique objective of filling up a gap in the past literature of ADP without taking into account dynamic uncertainties. Neither the system dynamics nor the system order are required to be precisely known. As an illustrative example, the computational algorithm is applied to the controller design of a two-machine power system.","",""
169,"S. Kung, M. Mak, S. H. Lin","Biometric Authentication: A Machine Learning Approach",2004,"","","","",155,"2022-07-13 09:38:31","","","","",,,,,169,9.39,56,3,18,"A breakthrough approach to improving biometrics performance Constructing robust information processing systems for face and voice recognition Supporting high-performance data fusion in multimodal systems Algorithms, implementation techniques, and application examples Machine learning: driving significant improvements in biometric performance As they improve, biometric authentication systems are becoming increasingly indispensable for protecting life and property. This book introduces powerful machine learning techniques that significantly improve biometric performance in a broad spectrum of application domains. Three leading researchers bridge the gap between research, design, and deployment, introducing key algorithms as well as practical implementation techniques. They demonstrate how to construct robust information processing systems for biometric authentication in both face and voice recognition systems, and to support data fusion in multimodal systems. Coverage includes: How machine learning approaches differ from conventional template matching Theoretical pillars of machine learning for complex pattern recognition and classification Expectation-maximization (EM) algorithms and support vector machines (SVM) Multi-layer learning models and back-propagation (BP) algorithms Probabilistic decision-based neural networks (PDNNs) for face biometrics Flexible structural frameworks for incorporating machine learning subsystems in biometric applications Hierarchical mixture of experts and inter-class learning strategies based on class-based modular networks Multi-cue data fusion techniques that integrate face and voice recognition Application case studies","",""
10,"Russell Lee, M. Hajiesmaili, Jian Li","Learning-Assisted Competitive Algorithms for Peak-Aware Energy Scheduling",2019,"","","","",156,"2022-07-13 09:38:31","","","","",,,,,10,3.33,3,3,3,"In this paper, we study the peak-aware energy scheduling problem using the competitive framework with machine learning prediction. With the uncertainty of energy demand as the fundamental challenge, the goal is to schedule the energy output of local generation units such that the electricity bill is minimized. While this problem has been tackled using classic competitive design with worst-case guarantee, the goal of this paper is to develop learning-assisted competitive algorithms to improve the performance in a provable manner. We develop two deterministic and randomized algorithms that are provably robust against the poor performance of learning prediction, however, achieve the optimal performance as the error of prediction goes to zero. Extensive experiments using real data traces verify our theoretical observations and show 15.13% improved performance against pure online algorithms.","",""
14,"Rania M. Hathout, Heba A. Gad, A. Metwally","Gelatinized-core liposomes: Toward a more robust carrier for hydrophilic molecules.",2017,"","","","",157,"2022-07-13 09:38:31","","10.1002/jbm.a.36175","","",,,,,14,2.80,5,3,5,"The use of liposomes as a delivery system for hydrophobic and hydrophilic drugs is well recognized. However, they possess several limitations that remained unresolved, including stability problems, low entrapment of the hydrophilic drugs, and the subsequent rapid release. This study introduces a novel approach to incorporate gelatin in the liposomal core to overcome these limitations. A rheological study was conducted to select suitable masses of the gelatin used in the liposomal formulations. Moreover, a full-factorial experimental design was utilized to compare the newly produced gel-core liposomes to the conventional liposomes with respect to the amount of a model hydrophilic molecule loading. An advanced machine learning method, namely, artificial neural networks was utilized to capture the effects of gelatin and cholesterol incorporation in the liposomes on the entrapment efficiency. The results revealed the successful preparation of the novel vesicles and their superiority over the conventional liposomes in drug loading, sustaining the drug release and stability which pose the newly introduced liposomal system as a successful delivery carrier for hydrophilic molecules and drugs. © 2017 Wiley Periodicals, Inc. J Biomed Mater Res Part A: 105A: 3086-3092, 2017.","",""
92,"Florin C. Ghesu, Edward Krubasik, B. Georgescu, V. Singh, Yefeng Zheng, J. Hornegger, D. Comaniciu","Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing",2016,"","","","",158,"2022-07-13 09:38:31","","10.1109/TMI.2016.2538802","","",,,,,92,15.33,13,7,6,"Robust and fast solutions for anatomical object detection and segmentation support the entire clinical workflow from diagnosis, patient stratification, therapy planning, intervention and follow-up. Current state-of-the-art techniques for parsing volumetric medical image data are typically based on machine learning methods that exploit large annotated image databases. Two main challenges need to be addressed, these are the efficiency in scanning high-dimensional parametric spaces and the need for representative image features which require significant efforts of manual engineering. We propose a pipeline for object detection and segmentation in the context of volumetric image parsing, solving a two-step learning problem: anatomical pose estimation and boundary delineation. For this task we introduce Marginal Space Deep Learning (MSDL), a novel framework exploiting both the strengths of efficient object parametrization in hierarchical marginal spaces and the automated feature design of Deep Learning (DL) network architectures. In the 3D context, the application of deep learning systems is limited by the very high complexity of the parametrization. More specifically 9 parameters are necessary to describe a restricted affine transformation in 3D, resulting in a prohibitive amount of billions of scanning hypotheses. The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in clustered, high-probability regions in spaces of gradually increasing dimensionality. To further increase computational efficiency and robustness, in our system we learn sparse adaptive data sampling patterns that automatically capture the structure of the input. Given the object localization, we propose a DL-based active shape model to estimate the non-rigid object boundary. Experimental results are presented on the aortic valve in ultrasound using an extensive dataset of 2891 volumes from 869 patients, showing significant improvements of up to 45.2% over the state-of-the-art. To our knowledge, this is the first successful demonstration of the DL potential to detection and segmentation in full 3D data with parametrized representations.","",""
18,"Zhaoyuan Yang, N. Iyer, Johan Reimann, Nurali Virani","Design of intentional backdoors in sequential models",2019,"","","","",159,"2022-07-13 09:38:31","","","","",,,,,18,6.00,5,4,3,"Recent work has demonstrated robust mechanisms by which attacks can be orchestrated on machine learning models. In contrast to adversarial examples, backdoor or trojan attacks embed surgically modified samples with targeted labels in the model training process to cause the targeted model to learn to misclassify chosen samples in the presence of specific triggers, while keeping the model performance stable across other nominal samples. However, current published research on trojan attacks mainly focuses on classification problems, which ignores sequential dependency between inputs. In this paper, we propose methods to discreetly introduce and exploit novel backdoor attacks within a sequential decision-making agent, such as a reinforcement learning agent, by training multiple benign and malicious policies within a single long short-term memory (LSTM) network. We demonstrate the effectiveness as well as the damaging impact of such attacks through initial outcomes generated from our approach, employed on grid-world environments. We also provide evidence as well as intuition on how the trojan trigger and malicious policy is activated. Challenges with network size and unintentional triggers are identified and analogies with adversarial examples are also discussed. In the end, we propose potential approaches to defend against or serve as early detection for such attacks. Results of our work can also be extended to many applications of LSTM and recurrent networks.","",""
56,"A. Bietti, Alekh Agarwal, J. Langford","A Contextual Bandit Bake-off",2018,"","","","",160,"2022-07-13 09:38:31","","","","",,,,,56,14.00,19,3,4,"Contextual bandit algorithms are essential for solving many real-world interactive machine learning problems. Despite multiple recent successes on statistically and computationally efficient methods, the practical behavior of these algorithms is still poorly understood. We leverage the availability of large numbers of supervised learning datasets to compare and empirically optimize contextual bandit algorithms, focusing on practical methods that learn by relying on optimization oracles from supervised learning. We find that a recent method (Foster et al., 2018) using optimism under uncertainty works the best overall. A surprisingly close second is a simple greedy baseline that only explores implicitly through the diversity of contexts, followed by a variant of Online Cover (Agarwal et al., 2014) which tends to be more conservative but robust to problem specification by design. Along the way, we also evaluate and improve several internal components of contextual bandit algorithm design. Overall, this is a thorough study and review of contextual bandit methodology.","",""
50,"D. Tuia, C. Persello, L. Bruzzone","Recent Advances in Domain Adaptation for the Classification of Remote Sensing Data",2021,"","","","",161,"2022-07-13 09:38:31","","10.1109/MGRS.2016.2548504","","",,,,,50,50.00,17,3,1,"This is the pre-acceptance version, to read the final version published in 2016 in the IEEE Geoscience and Remote Sensing Magazine, please go to: 10.1109/MGRS.2016.2548504 The success of supervised classification of remotely sensed images acquired over large geographical areas or at short time intervals strongly depends on the representativity of the samples used to train the classification algorithm and to define the model. When training samples are collected from an image (or a spatial region) different from the one used for mapping, spectral shifts between the two distributions are likely to make the model fail. Such shifts are generally due to differences in acquisition and atmospheric conditions or to changes in the nature of the object observed. In order to design classification methods that are robust to data-set shifts, recent remote sensing literature has considered solutions based on domain adaptation (DA) approaches. Inspired by machine learning literature, several DA methods have been proposed to solve specific problems in remote sensing data classification. This paper provides a critical review of the recent advances in DA for remote sensing and presents an overview of methods divided into four categories: i) invariant feature selection; ii) representation matching; iii) adaptation of classifiers and iv) selective sampling. We provide an overview of recent methodologies, as well as examples of application of the considered techniques to real remote sensing images characterized by very high spatial and spectral resolution. Finally, we propose guidelines to the selection of the method to use in real application scenarios. Manuscript received 2015; D. Tuia is with the Department of Geography, University of Zurich, 8057 Zurich, Switzerland. Email: devis.tuia@geo.uzh.ch, C. Persello is with the faculty of Geo-Information Science and Earth Observation (ITC), University of Twente, The Netherlands. Email c.persello@utwente.nl, L. Bruzzone is with the Dipartimento di Ingegneria e Scienza dell’Informazione, University of Trento, Italy. Email: lorenzo.bruzzone@disi.unitn.it April 19, 2021 DRAFT ar X iv :2 10 4. 07 77 8v 1 [ cs .C V ] 1 5 A pr 2 02 1 IEEE GEOSCIENCE AND REMOTE SENSING MAGAZINE 2016, PREPRINT, FULL VERSION: 10.1109/MGRS.2016.2548504 2","",""
124,"Liam Li, Kevin G. Jamieson, Afshin Rostamizadeh, Ekaterina Gonina, Jonathan Ben-tzur, Moritz Hardt, B. Recht, Ameet S. Talwalkar","A System for Massively Parallel Hyperparameter Tuning",2018,"","","","",162,"2022-07-13 09:38:31","","","","",,,,,124,31.00,16,8,4,"Modern learning models are characterized by large hyperparameter spaces and long training times. These properties, coupled with the rise of parallel computing and the growing demand to productionize machine learning workloads, motivate the need to develop mature hyperparameter optimization functionality in distributed computing settings. We address this challenge by first introducing a simple and robust hyperparameter optimization algorithm called ASHA, which exploits parallelism and aggressive early-stopping to tackle large-scale hyperparameter optimization problems. Our extensive empirical results show that ASHA outperforms existing state-of-the-art hyperparameter optimization methods; scales linearly with the number of workers in distributed settings; and is suitable for massive parallelism, as demonstrated on a task with 500 workers. We then describe several design decisions we encountered, along with our associated solutions, when integrating ASHA in Determined AI's end-to-end production-quality machine learning system that offers hyperparameter tuning as a service.","",""
17,"Daniel J. Fremont, Edward J. Kim, T. Dreossi, S. Ghosh, Xiangyu Yue, A. Sangiovanni-Vincentelli, S. Seshia","Scenic: A Language for Scenario Specification and Data Generation",2020,"","","","",163,"2022-07-13 09:38:31","","10.1007/s10994-021-06120-5","","",,,,,17,8.50,2,7,2,"","",""
7,"Akinori Fujino, N. Ueda, M. Nagata","A robust semi-supervised classification method for transfer learning",2010,"","","","",164,"2022-07-13 09:38:31","","10.1145/1871437.1871488","","",,,,,7,0.58,2,3,12,"The transfer learning problem of designing good classifiers with a high generalization ability by using labeled samples whose distribution is different from that of test samples is an important and challenging research issue in the fields of machine learning and data mining. This paper focuses on designing a semi-supervised classifier trained by using unlabeled samples drawn by the same distribution as test samples, and presents a semi-supervised classification method to deal with the transfer learning problem, based on a hybrid discriminative and generative model. Although JESS-CM is one of the most successful semi-supervised classifier design frameworks and has achieved the best published results in NLP tasks, it has an overfitting problem in transfer learning settings that we consider in this paper. We expect the overfitting problem to be mitigated with the proposed method, which utilizes both labeled and unlabeled samples for the discriminative training of classifiers. We also present a refined objective that formalizes the training algorithm and classifier form. Our experimental results for text classification using three typical benchmark test collections confirmed that the proposed method outperformed the JESS-CM framework with most transfer learning settings.","",""
267,"Xiangru Lian, Wei Zhang, Ce Zhang, Ji Liu","Asynchronous Decentralized Parallel Stochastic Gradient Descent",2017,"","","","",165,"2022-07-13 09:38:31","","","","",,,,,267,53.40,67,4,5,"Most commonly used distributed machine learning systems are either synchronous or centralized asynchronous. Synchronous algorithms like AllReduce-SGD perform poorly in a heterogeneous environment, while asynchronous algorithms using a parameter server suffer from 1) communication bottleneck at parameter servers when workers are many, and 2) significantly worse convergence when the traffic to parameter server is congested. Can we design an algorithm that is robust in a heterogeneous environment, while being communication efficient and maintaining the best-possible convergence rate? In this paper, we propose an asynchronous decentralized stochastic gradient decent algorithm (AD-PSGD) satisfying all above expectations. Our theoretical analysis shows AD-PSGD converges at the optimal $O(1/\sqrt{K})$ rate as SGD and has linear speedup w.r.t. number of workers. Empirically, AD-PSGD outperforms the best of decentralized parallel SGD (D-PSGD), asynchronous parallel SGD (A-PSGD), and standard data parallel SGD (AllReduce-SGD), often by orders of magnitude in a heterogeneous environment. When training ResNet-50 on ImageNet with up to 128 GPUs, AD-PSGD converges (w.r.t epochs) similarly to the AllReduce-SGD, but each epoch can be up to 4-8X faster than its synchronous counterparts in a network-sharing HPC environment. To the best of our knowledge, AD-PSGD is the first asynchronous algorithm that achieves a similar epoch-wise convergence rate as AllReduce-SGD, at an over 100-GPU scale.","",""
191,"Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, J. Tenenbaum, Boris Katz","ObjectNet: A large-scale bias-controlled dataset for pushing the limits of object recognition models",2019,"","","","",166,"2022-07-13 09:38:31","","","","",,,,,191,63.67,24,8,3,"We collect a large real-world test set, ObjectNet, for object recognition with controls where object backgrounds, rotations, and imaging viewpoints are random. Most scientific experiments have controls, confounds which are removed from the data, to ensure that subjects cannot perform a task by exploiting trivial correlations in the data. Historically, large machine learning and computer vision datasets have lacked such controls. This has resulted in models that must be fine-tuned for new datasets and perform better on datasets than in real-world applications. When tested on ObjectNet, object detectors show a 40-45% drop in performance, with respect to their performance on other benchmarks, due to the controls for biases. Controls make ObjectNet robust to fine-tuning showing only small performance increases. We develop a highly automated platform that enables gathering datasets with controls by crowdsourcing image capturing and annotation. ObjectNet is the same size as the ImageNet test set (50,000 images), and by design does not come paired with a training set in order to encourage generalization. The dataset is both easier than ImageNet (objects are largely centered and unoccluded) and harder (due to the controls). Although we focus on object recognition here, data with controls can be gathered at scale using automated tools throughout machine learning to generate datasets that exercise models in new ways thus providing valuable feedback to researchers. This work opens up new avenues for research in generalizable, robust, and more human-like computer vision and in creating datasets where results are predictive of real-world performance.","",""
34,"Florin C. Ghesu, Edward Krubasik, B. Georgescu, V. Singh, Yefeng Zheng, J. Hornegger, D. Comaniciu","Marginal Space Deep Learning: Efficient Architecture for Volumetric Image Parsing.",2016,"","","","",167,"2022-07-13 09:38:31","","10.1109/TMI.2016.2538802","","",,,,,34,5.67,5,7,6,"Robust and fast solutions for anatomical object detection and segmentation support the entire clinical workflow from diagnosis, patient stratification, therapy planning, intervention and follow-up. Current state-of-the-art techniques for parsing volumetric medical image data are typically based on machine learning methods that exploit large annotated image databases. Two main challenges need to be addressed, these are the efficiency in scanning high-dimensional parametric spaces and the need for representative image features which require significant efforts of manual engineering. We propose a pipeline for object detection and segmentation in the context of volumetric image parsing, solving a two-step learning problem: anatomical pose estimation and boundary delineation. For this task we introduce Marginal Space Deep Learning (MSDL), a novel framework exploiting both the strengths of efficient object parametrization in hierarchical marginal spaces and the automated feature design of Deep Learning (DL) network architectures. In the 3D context, the application of deep learning systems is limited by the very high complexity of the parametrization. More specifically 9 parameters are necessary to describe a restricted affine transformation in 3D, resulting in a prohibitive amount of billions of scanning hypotheses. The mechanism of marginal space learning provides excellent run-time performance by learning classifiers in clustered, high-probability regions in spaces of gradually increasing dimensionality. To further increase computational efficiency and robustness, in our system we learn sparse adaptive data sampling patterns that automatically capture the structure of the input. Given the object localization, we propose a DL-based active shape model to estimate the non-rigid object boundary. Experimental results are presented on the aortic valve in ultrasound using an extensive dataset of 2891 volumes from 869 patients, showing significant improvements of up to 45.2% over the state-of-the-art. To our knowledge, this is the first successful demonstration of the DL potential to detection and segmentation in full 3D data with parametrized representations.","",""
8,"Rahul Pandey, C. Castillo, Hemant Purohit","Modeling Human Annotation Errors to Design Bias-Aware Systems for Social Stream Processing",2019,"","","","",168,"2022-07-13 09:38:31","","10.1145/3341161.3342931","","",,,,,8,2.67,3,3,3,"High-quality human annotations are necessary to create effective machine learning systems for social media. Low-quality human annotations indirectly contribute to the creation of inaccurate or biased learning systems. We show that human annotation quality is dependent on the ordering of instances shown to annotators (referred as ‘annotation schedule’), and can be improved by local changes in the instance ordering provided to the annotators, yielding a more accurate annotation of the data stream for efficient real-time social media analytics. We propose an error-mitigating active learning algorithm that is robust with respect to some cases of human errors when deciding an annotation schedule. We validate the human error model and evaluate the proposed algorithm against strong baselines by experimenting on classification tasks of relevant social media posts during crises. According to these experiments, considering the order in which data instances are presented to human annotators leads to both an increase in accuracy for machine learning and awareness toward some potential biases in human learning that may affect the automated classifier.","",""
51,"Siddhant Kumar, Stephanie Tan, Li Zheng, D. Kochmann","Inverse-designed spinodoid metamaterials",2020,"","","","",169,"2022-07-13 09:38:31","","10.1038/s41524-020-0341-6","","",,,,,51,25.50,13,4,2,"","",""
6,"Jayaraman J. Thiagarajan, Rushil Anirudh, B. Kailkhura, Nikhil Jain, T. Islam, A. Bhatele, Jae-Seung Yeom, T. Gamblin","PADDLE: Performance Analysis Using a Data-Driven Learning Environment",2018,"","","","",170,"2022-07-13 09:38:31","","10.1109/IPDPS.2018.00088","","",,,,,6,1.50,1,8,4,"The use of machine learning techniques to model execution time and power consumption, and, more generally, to characterize performance data is gaining traction in the HPC community. Although this signifies huge potential for automating complex inference tasks, a typical analytics pipeline requires selecting and extensively tuning multiple components ranging from feature learning to statistical inferencing to visualization. Further, the algorithmic solutions often do not generalize between problems, thereby making it cumbersome to design and validate machine learning techniques in practice. In order to address these challenges, we propose a unified machine learning framework, PADDLE, which is specifically designed for problems encountered during analysis of HPC data. The proposed framework uses an information-theoretic approach for hierarchical feature learning and can produce highly robust and interpretable models. We present user-centric workflows for using PADDLE and demonstrate its effectiveness in different scenarios: (a) identifying causes of network congestion; (b) determining the best performing linear solver for sparse matrices; and (c) comparing performance characteristics of parent and proxy application pairs.","",""
11,"Henry Kenlay, D. Thanou, Xiaowen Dong","Interpretable Stability Bounds for Spectral Graph Filters",2021,"","","","",171,"2022-07-13 09:38:31","","","","",,,,,11,11.00,4,3,1,"Graph-structured data arise in a variety of realworld context ranging from sensor and transportation to biological and social networks. As a ubiquitous tool to process graph-structured data, spectral graph filters have been used to solve common tasks such as denoising and anomaly detection, as well as design deep learning architectures such as graph neural networks. Despite being an important tool, there is a lack of theoretical understanding of the stability properties of spectral graph filters, which are important for designing robust machine learning models. In this paper, we study filter stability and provide a novel and interpretable upper bound on the change of filter output, where the bound is expressed in terms of the endpoint degrees of the deleted and newly added edges, as well as the spatial proximity of those edges. This upper bound allows us to reason, in terms of structural properties of the graph, when a spectral graph filter will be stable. We further perform extensive experiments to verify intuition that can be gained from the bound.","",""
33,"Youxuan Jiang, Jonathan K. Kummerfeld, Walter S. Lasecki","Understanding Task Design Trade-offs in Crowdsourced Paraphrase Collection",2017,"","","","",172,"2022-07-13 09:38:31","","10.18653/v1/P17-2017","","",,,,,33,6.60,11,3,5,"Linguistically diverse datasets are critical for training and evaluating robust machine learning systems, but data collection is a costly process that often requires experts. Crowdsourcing the process of paraphrase generation is an effective means of expanding natural language datasets, but there has been limited analysis of the trade-offs that arise when designing tasks. In this paper, we present the first systematic study of the key factors in crowdsourcing paraphrase collection. We consider variations in instructions, incentives, data domains, and workflows. We manually analyzed paraphrases for correctness, grammaticality, and linguistic diversity. Our observations provide new insight into the trade-offs between accuracy and diversity in crowd responses that arise as a result of task design, providing guidance for future paraphrase generation procedures.","",""
8,"David Alvarez-Melis, Harmanpreet Kaur, Hal Daum'e, H. Wallach, Jennifer Wortman Vaughan","From Human Explanation to Model Interpretability: A Framework Based on Weight of Evidence",2021,"","","","",173,"2022-07-13 09:38:31","","","","",,,,,8,8.00,2,5,1,"We take inspiration from the study of human explanation to inform the design and evaluation of interpretability methods in machine learning. First, we survey the literature on human explanation in philosophy, cognitive science, and the social sciences, and propose a list of design principles for machine- generated explanations that are meaningful to humans. Using the concept of weight of evidence from information theory, we develop a method for generating explanations that adhere to these principles. We show that this method can be adapted to handle high-dimensional, multi-class settings, yielding a ﬂexible framework for generating explanations. We demon- strate that these explanations can be estimated accurately from ﬁnite samples and are robust to small perturbations of the inputs. We also evaluate our method through a qualitative user study with machine learning practitioners, where we observe that the resulting explanations are usable despite some participants struggling with background concepts like prior class probabilities. Finally, we conclude by surfacing design implications for interpretability tools in general.","",""
117,"Jianyu Wang, Gauri Joshi","Adaptive Communication Strategies to Achieve the Best Error-Runtime Trade-off in Local-Update SGD",2018,"","","","",174,"2022-07-13 09:38:31","","","","",,,,,117,29.25,59,2,4,"Large-scale machine learning training, in particular distributed stochastic gradient descent, needs to be robust to inherent system variability such as node straggling and random communication delays. This work considers a distributed training framework where each worker node is allowed to perform local model updates and the resulting models are averaged periodically. We analyze the true speed of error convergence with respect to wall-clock time (instead of the number of iterations), and analyze how it is affected by the frequency of averaging. The main contribution is the design of AdaComm, an adaptive communication strategy that starts with infrequent averaging to save communication delay and improve convergence speed, and then increases the communication frequency in order to achieve a low error floor. Rigorous experiments on training deep neural networks show that AdaComm can take $3 \times$ less time than fully synchronous SGD, and still reach the same final training loss.","",""
36,"C. MacDonald, N. Tonellotto","Declarative Experimentation in Information Retrieval using PyTerrier",2020,"","","","",175,"2022-07-13 09:38:31","","10.1145/3409256.3409829","","",,,,,36,18.00,18,2,2,"The advent of deep machine learning platforms such as Tensorflow and Pytorch, developed in expressive high-level languages such as Python, have allowed more expressive representations of deep neural network architectures. We argue that such a powerful formalism is missing in information retrieval (IR), and propose a framework called PyTerrier that allows advanced retrieval pipelines to be expressed, and evaluated, in a declarative manner close to their conceptual design. Like the aforementioned frameworks that compile deep learning experiments into primitive GPU operations, our framework targets IR platforms as backends in order to execute and evaluate retrieval pipelines. Further, we can automatically optimise the retrieval pipelines to increase their efficiency to suite a particular IR platform backend. Our experiments, conducted on TREC Robust and ClueWeb09 test collections, demonstrate the efficiency benefits of these optimisations for retrieval pipelines involving both the Anserini and Terrier IR platforms.","",""
28,"Pinkal Patel, P. Aggarwal, Anubha Gupta","Classification of Schizophrenia versus normal subjects using deep learning",2016,"","","","",176,"2022-07-13 09:38:31","","10.1145/3009977.3010050","","",,,,,28,4.67,9,3,6,"Motivated by deep learning approaches to classify normal and neuro-diseased subjects in functional Magnetic Resonance Imaging (fMRI), we propose stacked autoencoder (SAE) based 2-stage architecture for disease diagnosis. In the proposed architecture, a separate 4-hidden layer autoencoder is trained in unsupervised manner for feature extraction corresponding to every brain region. Thereafter, these trained autoencoders are used to provide features on class-labeled input data for training a binary support vector machine (SVM) based classifier. In order to design a robust classifier, noisy or inactive gray matter voxels are filtered out using a proposed covariance based approach. We applied the proposed methodology on a public dataset, namely, 1000 Functional Connectomes Project Cobre dataset consisting of fMRI data of normal and Schizophrenia subjects. The proposed architecture is able to classify normal and Schizophrenia subjects with 10-fold cross-validation accuracy of 92% that is better compared to the existing methods used on the same dataset.","",""
53,"A. Bellet, Amaury Habrard, M. Sebban","Similarity Learning for Provably Accurate Sparse Linear Classification",2012,"","","","",177,"2022-07-13 09:38:31","","","","",,,,,53,5.30,18,3,10,"In recent years, the crucial importance of metrics in machine learning algorithms has led to an increasing interest for optimizing distance and similarity functions. Most of the state of the art focus on learning Mahalanobis distances (requiring to fulfill a constraint of positive semi-definiteness) for use in a local k-NN algorithm. However, no theoretical link is established between the learned metrics and their performance in classification. In this paper, we make use of the formal framework of (e, γ, τ)-good similarities introduced by Balcan et al. to design an algorithm for learning a non PSD linear similarity optimized in a nonlinear feature space, which is then used to build a global linear classifier. We show that our approach has uniform stability and derive a generalization bound on the classification error. Experiments performed on various datasets confirm the effectiveness of our approach compared to state-of-the-art methods and provide evidence that (i) it is fast, (ii) robust to overfitting and (iii) produces very sparse classifiers.","",""
93,"A. Agarwal, M. Dahleh, Tuhin Sarkar","A Marketplace for Data: An Algorithmic Solution",2018,"","","","",178,"2022-07-13 09:38:31","","10.1145/3328526.3329589","","",,,,,93,23.25,31,3,4,"In this work, we aim to design a data marketplace; a robust real-time matching mechanism to efficiently buy and sell training data for Machine Learning tasks. While the monetization of data and pre-trained models is an essential focus of industry today, there does not exist a market mechanism to price training data and match buyers to sellers while still addressing the associated (computational and other) complexity. The challenge in creating such a market stems from the very nature of data as an asset: (i) it is freely replicable; (ii) its value is inherently combinatorial due to correlation with signal in other data; (iii) prediction tasks and the value of accuracy vary widely; (iv) usefulness of training data is difficult to verify a priori without first applying it to a prediction task. As our main contributions we: (i) propose a mathematical model for a two-sided data market and formally define the key associated challenges; (ii) construct algorithms for such a market to function and analyze how they meet the challenges defined. We highlight two technical contributions: (i) a new notion of ""fairness"" required for cooperative games with freely replicable goods; (ii) a truthful, zero regret mechanism to auction a class of combinatorial goods based on utilizing Myerson's payment function and the Multiplicative Weights algorithm. These might be of independent interest.","",""
26,"Banghua Zhu, Jiantao Jiao, David Tse","Deconstructing Generative Adversarial Networks",2019,"","","","",179,"2022-07-13 09:38:31","","10.1109/TIT.2020.2983698","","",,,,,26,8.67,9,3,3,"Generative Adversarial Networks (GANs) are a thriving unsupervised machine learning technique that has led to significant advances in various fields such as computer vision, natural language processing, among others. However, GANs are known to be difficult to train and usually suffer from mode collapse and the discriminator winning problem. To interpret the empirical observations of GANs and design better ones, we deconstruct the study of GANs into three components and make the following contributions. •Formulation: we propose a perturbation view of the population target of GANs. Building on this interpretation, we show that GANs can be connected to the robust statistics framework, and propose a novel GAN architecture, termed as Cascade GANs, to provably recover meaningful low-dimensional generator approximations when the real distribution is high-dimensional and corrupted by outliers.•Generalization: given a population target of GANs, we design a systematic principle, projection under admissible distance, to design GANs to meet the population requirement using only finite samples. We implement our principle in three cases to achieve polynomial and sometimes near-optimal sample complexities: (1) learning an arbitrary generator under an arbitrary pseudonorm; (2) learning a Gaussian location family under total variation distance, where we utilize our principle to provide a new proof for the near-optimality of the Tukey median viewed as GANs; (3) learning a low-dimensional Gaussian approximation of a high-dimensional arbitrary distribution under Wasserstein distance. We demonstrate a fundamental trade-off in the approximation error and statistical error in GANs, and demonstrate how to apply our principle in practice with only empirical samples to predict how many samples would be sufficient for GANs in order not to suffer from the discriminator winning problem.•Optimization: we demonstrate alternating gradient descent is provably not locally asymptotically stable in optimizing the GAN formulation of PCA. We found that the minimax duality gap being non-zero might be one of the causes, and propose a new GAN architecture whose duality gap is zero, where the value of the game is equal to the previous minimax value (not the maximin value). We prove the new GAN architecture is globally asymptotically stable in solving PCA under alternating gradient descent.","",""
356,"S. Brunton, B. R. Noack","Closed-Loop Turbulence Control: Progress and Challenges",2015,"","","","",180,"2022-07-13 09:38:31","","10.1115/1.4031175","","",,,,,356,50.86,178,2,7,"Closed-loop turbulence control is a critical enabler of aerodynamic drag reduction, lift increase, mixing enhancement, and noise reduction. Current and future applications have epic proportion: cars, trucks, trains, airplanes, wind turbines, medical devices, combustion, chemical reactors, just to name a few. Methods to adaptively adjust open-loop parameters are continually improving toward shorter response times. However, control design for in-time response is challenged by strong nonlinearity, high-dimensionality, and time-delays. Recent advances in the field of model identification and system reduction, coupled with advances in control theory (robust, adaptive, and nonlinear) are driving significant progress in adaptive and in-time closed-loop control of fluid turbulence. In this review, we provide an overview of critical theoretical developments, highlighted by compelling experimental success stories. We also point to challenging open problems and propose potentially disruptive technologies of machine learning and compressive sensing.","",""
23,"S. Fujii, T. Yokoi, C. Fisher, H. Moriwake, M. Yoshiya","Quantitative prediction of grain boundary thermal conductivities from local atomic environments",2020,"","","","",181,"2022-07-13 09:38:31","","10.1038/s41467-020-15619-9","","",,,,,23,11.50,5,5,2,"","",""
24,"R. Batra, H. Dai, T. D. Huan, Lihua Chen, Chiho Kim, W. Gutekunst, Le Song, R. Ramprasad","Polymers for Extreme Conditions Designed Using Syntax-Directed Variational Autoencoders",2020,"","","","",182,"2022-07-13 09:38:31","","10.1021/acs.chemmater.0c03332","","",,,,,24,12.00,3,8,2,"The design/discovery of new materials is highly non-trivial owing to the near-infinite possibilities of material candidates, and multiple required property/performance objectives. Thus, machine learning tools are now commonly employed to virtually screen material candidates with desired properties by learning a theoretical mapping from material-to-property space, referred to as the \emph{forward} problem. However, this approach is inefficient, and severely constrained by the candidates that human imagination can conceive. Thus, in this work on polymers, we tackle the materials discovery challenge by solving the \emph{inverse} problem: directly generating candidates that satisfy desired property/performance objectives. We utilize syntax-directed variational autoencoders (VAE) in tandem with Gaussian process regression (GPR) models to discover polymers expected to be robust under three extreme conditions: (1) high temperatures, (2) high electric field, and (3) high temperature \emph{and} high electric field, useful for critical structural, electrical and energy storage applications. This approach to learn from (and augment) human ingenuity is general, and can be extended to discover polymers with other targeted properties and performance measures.","",""
5,"Ali A. Bataleblu, J. Roshanian","Robust trajectory optimization of space launch vehicle using computational intelligence",2015,"","","","",183,"2022-07-13 09:38:31","","10.1109/CEC.2015.7257318","","",,,,,5,0.71,3,2,7,"Metamodeling techniques using computational intelligence have been used in Uncertainty-based Design Optimization (UDO) to reduce the high computational cost of the uncertainty analysis and improve the performance of stochastic optimization problems with computationally expensive simulation models. Optimal trajectory generation is a major part of Space Launch Vehicle (SLV) design and if it is robust relative to uncertainties can improve vehicle reliability, safety and operational cost. This paper presents a combination of Latin Hypercube Sampling (LHS) and Extreme Learning Machine (ELM) in order to create an appropriate trajectory metamodel for reducing computational time of robust trajectory design optimization of a two-stage-to-orbit SLV. The sampled data of LHS is then used as training data for ELM. Complex and costly uncertainty analyses are replaced by an ELM Neural Network (NN) which is used to instantaneously estimate the mean and standard deviation of objective function and constraints. The evolutionary genetic algorithm is used for global optimization of layers' connection weights and biases to minimize the learning error during learning phase of NN. A Hybrid Search Algorithm (HSA), which associates Simulated Annealing (SA) as a global optimizer with Simplex as a local optimizer is employed to find robust optimum point of this metamodel. The optimal and robust trajectories are compared. The results show excellent approximation of highly non-linear design space and drastic reduction in overall UDO time, due to greatly reduced number of exact trajectory analyses.","",""
29,"Geqiang Li, Jian Cao, Biao Zhang, K. Zhao","Design of Robust Controller in Electrohydraulic Load Simulator",2006,"","","","",184,"2022-07-13 09:38:31","","10.1109/ICMLC.2006.258453","","",,,,,29,1.81,7,4,16,"Application of mu theory in the electrohydraulic load simulator is studied. Robust control strategy of the electrohydraulic load simulator is put forward. The electrohydraulic load simulator is a complicated mechanical-electronic-hydraulic compound system, moreover it is a strong coupling and time-varying controlled object. Synthetically considering uncertainties such as parameter perturbations, model perturbations and external disturbances etc., robust force controller of the electrohydraulic load simulator is designed using mu synthesis theory. Robust stability and robust performance are analyzed by mu values of the system in which robust force controller is used. Moreover, experiment results of systems using robust force controller and classical force controllers are shown. The results indicate that the designed robust force controller is efficiency and superiority","",""
18,"Debmalya Mandal, Samuel Deng, S. Jana, Jeannette M. Wing, Daniel J. Hsu","Ensuring Fairness Beyond the Training Data",2020,"","","","",185,"2022-07-13 09:38:31","","","","",,,,,18,9.00,4,5,2,"We initiate the study of fair classifiers that are robust to perturbations in the training distribution. Despite recent progress, the literature on fairness has largely ignored the design of fair and robust classifiers. In this work, we develop classifiers that are fair not only with respect to the training distribution, but also for a class of distributions that are weighted perturbations of the training samples. We formulate a min-max objective function whose goal is to minimize a distributionally robust training loss, and at the same time, find a classifier that is fair with respect to a class of distributions. We first reduce this problem to finding a fair classifier that is robust with respect to the class of distributions. Based on online learning algorithm, we develop an iterative algorithm that provably converges to such a fair and robust solution. Experiments on standard machine learning fairness datasets suggest that, compared to the state-of-the-art fair classifiers, our classifier retains fairness guarantees and test accuracy for a large class of perturbations on the test set. Furthermore, our experiments show that there is an inherent trade-off between fairness robustness and accuracy of such classifiers.","",""
14,"Paul Berube, J. N. Amaral","Benchmark Design for Robust Profile-Directed Optimization",2007,"","","","",186,"2022-07-13 09:38:31","","","","",,,,,14,0.93,7,2,15,"Profile-guided code transformations specialize program code according to the profile provided by execution on training data. Consequently, the performance of the code generated usind this feedback is sensitive to the selection of training data. Used in this fashion, the principle behind profileguided optimization techniques is the same as off-line learning commonly used in the field of machine learning. However, scant use of proper validation techniques for profileguided optimizations have appeared in the literature. Given the broad use of SPEC benchmarks in the computer architecture and optimizing compiler communities, SPEC is in a position to influence the proper evaluation and validation of profile-guided optimizations. Thus, we propose an evaluation methodology appropriate for profile-guided optimization based on cross-validation. Cross-validation is a methodology from machine learning that takes input sensitivity into account, and provides a measure of the generalizability of results.","",""
26,"M. Malekzadeh, S. Calinon, D. Bruno, D. Caldwell","Learning by imitation with the STIFF-FLOP surgical robot: a biomimetic approach inspired by octopus movements",2014,"","","","",187,"2022-07-13 09:38:31","","10.1186/S40638-014-0013-4","","",,,,,26,3.25,7,4,8,"","",""
11,"C. Atkeson","Efficient robust policy optimization",2012,"","","","",188,"2022-07-13 09:38:31","","10.1109/ACC.2012.6315619","","",,,,,11,1.10,11,1,10,"We provide efficient algorithms to calculate first and second order gradients of the cost of a control law with respect to its parameters, to speed up policy optimization. We achieve robustness by simultaneously designing one control law for multiple models with potentially different model structures, which represent model uncertainty and unmodeled dynamics. Providing explicit examples of possible unmodeled dynamics during the control design process is easier for the designer and is more effective than providing simulated perturbations to increase robustness, as is currently done in machine learning. Our approach supports the design of deterministic nonlinear and time varying controllers for both deterministic and stochastic nonlinear and time varying systems, including policies with internal state such as observers or other state estimators. We highlight the benefit of control laws made up of collections of simple policies where only one component policy is active at a time. Controller optimization and learning is particularly fast and effective in this situation because derivatives are decoupled.","",""
150,"A. D. Castro, J. Torres-Sánchez, J. Peña, F. M. Jiménez-Brenes, O. Csillik, F. López-Granados","An Automatic Random Forest-OBIA Algorithm for Early Weed Mapping between and within Crop Rows Using UAV Imagery",2018,"","","","",189,"2022-07-13 09:38:31","","10.3390/rs10020285","","",,,,,150,37.50,25,6,4,"Accurate and timely detection of weeds between and within crop rows in the early growth stage is considered one of the main challenges in site-specific weed management (SSWM). In this context, a robust and innovative automatic object-based image analysis (OBIA) algorithm was developed on Unmanned Aerial Vehicle (UAV) images to design early post-emergence prescription maps. This novel algorithm makes the major contribution. The OBIA algorithm combined Digital Surface Models (DSMs), orthomosaics and machine learning techniques (Random Forest, RF). OBIA-based plant heights were accurately estimated and used as a feature in the automatic sample selection by the RF classifier; this was the second research contribution. RF randomly selected a class balanced training set, obtained the optimum features values and classified the image, requiring no manual training, making this procedure time-efficient and more accurate, since it removes errors due to a subjective manual task. The ability to discriminate weeds was significantly affected by the imagery spatial resolution and weed density, making the use of higher spatial resolution images more suitable. Finally, prescription maps for in-season post-emergence SSWM were created based on the weed maps—the third research contribution—which could help farmers in decision-making to optimize crop management by rationalization of the herbicide application. The short time involved in the process (image capture and analysis) would allow timely weed control during critical periods, crucial for preventing yield loss.","",""
15,"K. Lavanya, Ahmed J. Obaid, I. Thaseen, K. Abhishek, Khushboo Saboo, R. Paturkar","Terrain Mapping of LandSat8 Images using MNF and Classifying Soil Properties using Ensemble Modelling",2020,"","","","",190,"2022-07-13 09:38:31","","10.22075/IJNAA.2020.4750","","",,,,,15,7.50,3,6,2,"Traditional technique for determining the soil texture and other soil properties is performed in laboratory which is a time consuming task. In this paper, machine learning algorithms are deployed to classify the soil texture and its properties without any intervention of laboratory equipment using the satellite images recorded by Landsat 8. These images are used to extract the terrain properties of the region which is integrated with weather data for the specific region and the vegetation index which are the major factors affecting the soil condition. A major aim of this paper is to design a robust technique for extracting, transforming Landsat images to numerical data and pre-processing the data for classifying the soil property. Minimum Noise Fraction (MNF) is utilized to segregate and remove noise from the Landsat images for subsequent processing. A significant amount of noise is present in the raw data which affects the accuracy of the analysis. Terrain features are extracted after noise removal from the MNF transformed images and merged with the weather data, and vegetation index for a period of time and then classified using voting classifier of the ensemble modeling or analysis of the soil texture of the region. The voting is performed by integrating the results of logistic regression, support vector machine and decision tree. With this study, the consolidated dependence of the soil texture on the environmental factors is analyzed and a cross validation accuracy of 94.44% is obtained.","",""
15,"Muhammad Ather Iqbal Hussain, Babar Khan, Zhijie Wang, Shenyi Ding","Woven Fabric Pattern Recognition and Classification Based on Deep Convolutional Neural Networks",2020,"","","","",191,"2022-07-13 09:38:31","","10.3390/electronics9061048","","",,,,,15,7.50,4,4,2,"The weave pattern (texture) of woven fabric is considered to be an important factor of the design and production of high-quality fabric. Traditionally, the recognition of woven fabric has a lot of challenges due to its manual visual inspection. Moreover, the approaches based on early machine learning algorithms directly depend on handcrafted features, which are time-consuming and error-prone processes. Hence, an automated system is needed for classification of woven fabric to improve productivity. In this paper, we propose a deep learning model based on data augmentation and transfer learning approach for the classification and recognition of woven fabrics. The model uses the residual network (ResNet), where the fabric texture features are extracted and classified automatically in an end-to-end fashion. We evaluated the results of our model using evaluation metrics such as accuracy, balanced accuracy, and F1-score. The experimental results show that the proposed model is robust and achieves state-of-the-art accuracy even when the physical properties of the fabric are changed. We compared our results with other baseline approaches and a pretrained VGGNet deep learning model which showed that the proposed method achieved higher accuracy when rotational orientations in fabric and proper lighting effects were considered.","",""
17,"Jorge Chang, P. Nikolaev, J. Carpena-Núñez, R. Rao, Kevin Decker, A. Islam, Jiseob Kim, M. Pitt, Jay I. Myung, B. Maruyama","Efficient Closed-loop Maximization of Carbon Nanotube Growth Rate using Bayesian Optimization",2020,"","","","",192,"2022-07-13 09:38:31","","10.1038/s41598-020-64397-3","","",,,,,17,8.50,2,10,2,"","",""
126,"A. Rahmani, Bryan Donyanavard, T. Mück, Kasra Moazzemi, A. Jantsch, O. Mutlu, N. Dutt","SPECTR",2018,"","","","",193,"2022-07-13 09:38:31","","10.1145/3296957.3173199","","",,,,,126,31.50,18,7,4,"Resource management strategies for many-core systems need to enable sharing of resources such as power, processing cores, and memory bandwidth while coordinating the priority and significance of system- and application-level objectives at runtime in a scalable and robust manner. State-of-the-art approaches use heuristics or machine learning for resource management, but unfortunately lack formalism in providing robustness against unexpected corner cases. While recent efforts deploy classical control-theoretic approaches with some guarantees and formalism, they lack scalability and autonomy to meet changing runtime goals. We present SPECTR, a new resource management approach for many-core systems that leverages formal supervisory control theory (SCT) to combine the strengths of classical control theory with state-of-the-art heuristic approaches to efficiently meet changing runtime goals. SPECTR is a scalable and robust control architecture and a systematic design flow for hierarchical control of many-core systems. SPECTR leverages SCT techniques such as gain scheduling to allow autonomy for individual controllers. It facilitates automatic synthesis of the high-level supervisory controller and its property verification. We implement SPECTR on an Exynos platform containing ARM»s big.LITTLE-based heterogeneous multi-processor (HMP) and demonstrate that SPECTR»s use of SCT is key to managing multiple interacting resources (e.g., chip power and processing cores) in the presence of competing objectives (e.g., satisfying QoS vs. power capping). The principles of SPECTR are easily applicable to any resource type and objective as long as the management problem can be modeled using dynamical systems theory (e.g., difference equations), discrete-event dynamic systems, or fuzzy dynamics.","",""
19,"J. Abernethy, Yiling Chen, Chien-Ju Ho, Bo Waggoner","Low-Cost Learning via Active Data Procurement",2015,"","","","",194,"2022-07-13 09:38:31","","10.1145/2764468.2764519","","",,,,,19,2.71,5,4,7,"We design mechanisms for online procurement of data held by strategic agents for machine learning tasks. We study a model in which agents cannot fabricate data, but may lie about their cost of furnishing their data. The challenge is to use past data to actively price future data in order to obtain learning guarantees, even when agents' costs can depend arbitrarily on the data itself. We show how to convert a large class of no-regret algorithms into online posted-price and learning mechanisms. Our results parallel classic sample complexity guarantees, but with the key resource constraint being money rather than quantity of data available. With a budget constraint B, we give robust risk (predictive error) bounds on the order of 1/√B. In many cases our guarantees are significantly better due to an active-learning approach that leverages correlations between costs and data. Our algorithms and analysis go through a model of no-regret learning with T arriving pairs (cost, data) and a budget constraint of B, coupled with the ""online to batch conversion"". Our regret bounds for this model are on the order of T/√B and we give lower bounds on the same order.","",""
17,"Florin C. Ghesu, B. Georgescu, Yefeng Zheng, J. Hornegger, D. Comaniciu","Marginal Space Deep Learning: Efficient Architecture for Detection in Volumetric Image Data",2015,"","","","",195,"2022-07-13 09:38:31","","10.1007/978-3-319-24553-9_87","","",,,,,17,2.43,3,5,7,"","",""
12,"Phasit Charoenkwan, Nuttapat Anuwongcharoen, C. Nantasenamat, M. Hasan, W. Shoombuatong","In silico approaches for the prediction and analysis of antiviral peptides: a review.",2020,"","","","",196,"2022-07-13 09:38:31","","10.2174/1381612826666201102105827","","",,,,,12,6.00,2,5,2,"In light of the growing resistance toward current antiviral drugs, efforts to discover novel and effective antiviral therapeutic agents remain a pressing scientific effort. Antiviral peptides (AVPs) represents promising therapeutic agents due to their extraordinary advantages in terms of potency, efficacy and pharmacokinetic properties. The growing volume of newly discovered peptide sequences in the post-genomic era requires computational approaches for timely and accurate identification of AVPs. Machine learning (ML) methods such as random forest and support vector machine represents robust learning algorithms that are instrumental in successful peptide-based drug discovery. Therefore, this review summarizes the current state-of-the-art on the application of ML methods for identifying AVPs directly from the sequence information. We compare the efficiency of these methods in terms of the underlying characteristics of the dataset used along with feature encoding methods, ML algorithms, cross-validation methods and prediction performance. Finally, guidelines for development of robust AVP models are also discussed. It is anticipated that this review will be serve as a useful guide for the design and development of robust AVP and related therapeutic peptide predictors in the future.","",""
50,"Joseph Aylett-Bullock, C. Cuesta-Lázaro, A. Quera-Bofarull","XNet: a convolutional neural network (CNN) implementation for medical x-ray image segmentation suitable for small datasets",2018,"","","","",197,"2022-07-13 09:38:31","","10.1117/12.2512451","","",,,,,50,12.50,17,3,4,"X-Ray image enhancement, along with many other medical image processing applications, requires the segmentation of images into bone, soft tissue, and open beam regions. We apply a machine learning approach to this problem, presenting an end-to-end solution which results in robust and efficient inference. Since medical institutions often do not have the resources to process and label the large quantity of X-Ray images usually needed for neural network training, we design an end-to-end solution for small datasets, while achieving state-of-the-art results. Our implementation produces an overall accuracy of 92%, F1 score of 0.92, and an AUC of 0.98, surpassing classical image processing techniques, such as clustering and entropy based methods, while improving upon the output of existing neural networks used for segmentation in non-medical contexts. The code used for this project is available online.1","",""
221,"D. Tuia, C. Persello, L. Bruzzone","Domain Adaptation for the Classification of Remote Sensing Data: An Overview of Recent Advances",2016,"","","","",198,"2022-07-13 09:38:31","","10.1109/MGRS.2016.2548504","","",,,,,221,36.83,74,3,6,"The success of the supervised classification of remotely sensed images acquired over large geographical areas or at short time intervals strongly depends on the representativity of the samples used to train the classification algorithm and to define the model. When training samples are collected from an image or a spatial region that is different from the one used for mapping, spectral shifts between the two distributions are likely to make the model fail. Such shifts are generally due to differences in acquisition and atmospheric conditions or to changes in the nature of the object observed. To design classification methods that are robust to data set shifts, recent remote sensing literature has considered solutions based on domain adaptation (DA) approaches. Inspired by machine-learning literature, several DA methods have been proposed to solve specific problems in remote sensing data classification. This article provides a critical review of the recent advances in DA approaches for remote sensing and presents an overview of DA methods divided into four categories: 1) invariant feature selection, 2) representation matching, 3) adaptation of classifiers, and 4) selective sampling. We provide an overview of recent methodologies, examples of applications of the considered techniques to real remote sensing images characterized by very high spatial and spectral resolution as well as possible guidelines for the selection of the method to use in real application scenarios.","",""
44,"Paulina Varshavskaya, L. Kaelbling, D. Rus","Automated Design of Adaptive Controllers for Modular Robots using Reinforcement Learning",2008,"","","","",199,"2022-07-13 09:38:31","","10.1177/0278364907084983","","",,,,,44,3.14,15,3,14,"Designing distributed controllers for self-reconfiguring modular robots has been consistently challenging. We have developed a reinforcement learning approach which can be used both to automate controller design and to adapt robot behavior on-line. In this paper, we report on our study of reinforcement learning in the domain of self-reconfigurable modular robots: the underlying assumptions, the applicable algorithms and the issues of partial observability, large search spaces and local optima. We propose and validate experimentally in simulation a number of techniques designed to address these and other scalability issues that arise in applying machine learning to distributed systems such as modular robots. We discuss ways to make learning faster, more robust and amenable to on-line application by giving scaffolding to the learning agents in the form of policy representation, structured experience and additional information. With enough structure modular robots can run learning algorithms to both automate the generation of distributed controllers, and adapt to the changing environment and deliver on the self-organization promise with less interference from human designers, programmers and operators.","",""
17,"Fei Wang, W. Liu, S. Chawla","On Sparse Feature Attacks in Adversarial Learning",2014,"","","","",200,"2022-07-13 09:38:31","","10.1109/ICDM.2014.117","","",,,,,17,2.13,6,3,8,"Adversarial learning is the study of machine learning techniques deployed in non-benign environments. Example applications include classifications for detecting spam email, network intrusion detection and credit card scoring. In fact as the gamut of application domains of machine learning grows, the possibility and opportunity for adversarial behavior will only increase. Till now, the standard assumption about modeling adversarial behavior has been to empower an adversary to change all features of the classifier sat will. The adversary pays a cost proportional to the size of ""attack"". We refer to this form of adversarial behavior as a dense feature attack. However, the aim of an adversary is not just to subvert a classifier but carry out data transformation in a way such that spam continues to appear like spam to the user as much as possible. We demonstrate that an adversary achieves this objective by carrying out a sparse feature attack. We design an algorithm to show how a classifier should be designed to be robust against sparse adversarial attacks. Our main insight is that sparse feature attacks are best defended by designing classifiers which use l1 regularizers.","",""
