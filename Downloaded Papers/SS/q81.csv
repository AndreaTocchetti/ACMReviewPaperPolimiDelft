Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
0,"Jacob Pettigrew, Gideon Woo, Herbert H. Tsang","Computational Intelligence in Human Feature Analysis and Pose Selection",2020,"","","","",1,"2022-07-13 09:31:46","","10.1109/SSCI47803.2020.9308270","","",,,,,0,0.00,0,3,2,"Using computers to detect a human’s features is a difficult problem. The solution to this problem can be used in applications such as facial detection and gesture recognition. These applications require fast computation and high accuracy. In our research, we are trying to detect human poses by examining humans’ features such as the arms and legs. Artificial Neural Networks have been successfully used in feature analysis and are popular for use in human pose selection. In this paper, we present the results from our research comparing various computational intelligent approaches such as Convolutional Neural Networks (CNN), Multi-Layer Perceptrons (MLP), Support Vector Machines (SVM), and K-Nearest Neighbour (KNN). Among the four algorithms examined in this paper, we found that CNNs outperformed other algorithms in terms of prediction accuracy and calculation speed. Our main contribution is therefore a CNN specifically designed for learning a human’s body structure and limb articulation, producing high accuracy while being robust against different body types and variation in limb articulation.","",""
0,"José Roberto Banin Júnior, R. A. de Lima Moreto, G. A. da Silva, C. Thomaz, S. P. Gimenez","Optimizing a Robust Miller OTA Implemented with Diamond Layout Style for MOSFETs By Using iMTGSPICE",2021,"","","","",2,"2022-07-13 09:31:46","","10.1109/SBCCI53441.2021.9529991","","",,,,,0,0.00,0,5,1,"This paper describes an innovative methodology to design and optimize robust analog Complementary Metal-Oxide-Semiconductor (CMOS) Integrated Circuits (ICs) with Diamond layout style (hexagonal gate shape) for Metal-Oxide-Semiconductor Field-Effect Transistors (MOSFETs), focusing on reducing their die areas and improving their electrical performances. The Miller CMOS Operational Transconductance Amplifier (OTA) is used to validate this design and optimization approach by using a computational tool, which integrates the human intelligence (expertise of the designer) and the artificial intelligence (use an evolutionary optimization algorithm to search robust potential solutions quickly and accurately). The 180 nm CMOS ICs technology node was considered in this work. The Longitudinal Corner Effect (LCE) and Parallel Connections of MOSFETs with different channel Lengths Effect (PAMDLE) present in the Diamond MOSFET structure are analytically modeled to be simulated in the SPICE. In addition, the iMTGSPICE computation tool was improved with a new feature to automatically convert Conventional MOSFETs (CMs) into Diamond MOSFETs (DMs). The main results show that the Miller CMOS OTA implemented with DMs (ɑ= 45°) can reduce up to 43% of their die area, practically without impairing the design specifications and robustness (Corner and Monte Carlo Analyses) in comparison to the one implemented with CM counterparts. Furthermore, these results were obtained quickly, i.e., approximately 4 hours to obtain five different robust potential solutions available to the designer.","",""
2,"C. Bento, A. Cardoso, G. Dias","Progress in Artificial Intelligence, 12th Portuguese Conference on Artificial Intelligence, EPIA 2005, Covilhã, Portugal, December 5-8, 2005, Proceedings",2006,"","","","",3,"2022-07-13 09:31:46","","10.1007/11595014","","",,,,,2,0.13,1,3,16,"","",""
27,"Andrew N. Sloss, Steven M. Gustafson","2019 Evolutionary Algorithms Review",2019,"","","","",4,"2022-07-13 09:31:46","","10.1007/978-3-030-39958-0_16","","",,,,,27,9.00,14,2,3,"","",""
7,"E. Lutton, A. Tonda, N. Boukhelifa, N. Perrot","COMPLEX SYSTEMS IN FOOD SCIENCE : HUMAN FACTOR ISSUES",2016,"","","","",5,"2022-07-13 09:31:46","","","","",,,,,7,1.17,2,4,6,"Building in-silico decision making systems is essential in the food domain, albeit highly difficult. This task strongly relies on multidisciplinary research and in particular on advanced techniques from artificial intelligence. The success of such systems depends on how well they cope with the complex properties of food processes, such as the large variety of interacting components including those related to human expertise; and their dynamic, non-linear, multi-scale, uncertain and non-equilibrium behaviors. Robust stochastic optimization techniques, evolutionary computation and in particular Interactive Evolutionary Computation (IEC) seem to be a fruitful framework for developing food science models. A Human-Centered approach to Interactive Evolutionary Computation is discussed in this paper as a possible pertinent way to cope with challenges related to human factors in this context. FOOD SCIENCE AND COMPLEX SYSTEMS Food is a major factor for health and public well-being. It is one of the most important sectors of industry and deals with chemicals, agriculture, animal feed, food processing, trade, retail and consumer sectors. Providing an adequate food supply to a growing world population is one of the grand challenges our global society has to address. Enterprises need to continuously provide safe, tasty, healthy, affordable, and sustainable food in sufficient volumes. This requires adapting to a range of factors, such as the increase in human population and health requirements, and the reduction in crops and livestock due to environmental factors and changes in the socio-political scene (van Mil et al. (2014)). Besides, there is a need for an integrated vision looking at these factors from multiple scales and perspectives: • from emotion and pleasure generated when eating food to nano-structures of a food emulsion or food microbial ecosystems, • from regional organization to nutritional and sociological impact, • from health considerations to inter-crop culture and microbial complexities, within the human body and in relation to food microbial ecosystems. In these conditions, creativity, pragmatism and optimization methods are crucial to reach breakthrough innovations and sustainable solutions. We foresee a huge opportunity for research in mathematical programming, integrative models and decision-support tools (Perrot et al. (2016)) to address the aforementioned challenges. Any proposed mathematical programming framework, however, has to deal with the following characteristic features of food systems: • The uncertainty and variability (in process, data and available knowledge) that severely influences the dynamics and emergence of various properties, • The heterogeneity of the data, from big volumes at the genomic scale to scarce samples at a more macroscopic level (i.e. process scales). For instance an ecosystem of 9 microorganisms can be characterized using 40,000 genes, and its dynamics with 10 aromatic compounds, • The complexity of qualitative and quantitative information, for instance for social and environmental evaluation, at various scales in space and time, • The variety of perspectives, types of models, research goals and data produced by conceptually disjoint scientific disciplines, ranging from physics and physiology to sociology and ethics. Moreover, there is a need to find an appropriate description level, able to express the complexity of an ecosystem with minimum uncertainty. Building models is essential, but highly difficult; efficient modeling necessitates a rigorous iterative process combining computationally intensive methods, formal reasoning and expertise from different fields.","",""
1,"S. Jafri, Satish Chawan, Afifa Khan","Face Recognition using Deep Neural Network with ""LivenessNet""",2020,"","","","",6,"2022-07-13 09:31:46","","10.1109/ICICT48043.2020.9112543","","",,,,,1,0.50,0,3,2,"There is a continuous increase in the amount of population over the globe, and this, in turn, increases the number of complex datasets over a period. This necessitates improving artificial intelligence algorithms for better and accurate categorization of data. The most defining characteristic of the human body of the face. Every person’s face is unique, although have the same structure such as noise, eyes, lips, etc. but it can vary strikingly. It’s within this variance which lies the distinguishing characteristics that can be used to identify one person from another. Face recognition is a popular concept which is commonly used in surveillance cameras at public places for security purposes. The ""Face Recognition using DNN with LivenessNet"" presents a face recognition method based on deep neural networks for liveness. Any algorithm is considered to be efficient only if it is robust and accurate. It provides accurate results with face spoofing quickly and efficiently. The main advantage of using this technique is identifying the uniqueness in the datasets by capturing the real-time face data through different modes & jitter. Also providing accurate face recognition model which can be used for safety and security purpose.","",""
73,"Yueting Zhuang, Fei Wu, Chun Chen, Yunhe Pan","Challenges and opportunities: from big data to knowledge in AI 2.0",2017,"","","","",7,"2022-07-13 09:31:46","","10.1631/FITEE.1601883","","",,,,,73,14.60,18,4,5,"In this paper, we review recent emerging theoretical and technological advances of artificial intelligence (AI) in the big data settings. We conclude that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI, as follows: from shallow computation to deep neural reasoning; from merely data-driven model to data-driven with structured logic rules models; from task-oriented (domain-specific) intelligence (adherence to explicit instructions) to artificial general intelligence in a general context (the capability to learn from experience). Motivated by such endeavors, the next generation of AI, namely AI 2.0, is positioned to reinvent computing itself, to transform big data into structured knowledge, and to enable better decision-making for our society.","",""
1,"J. Beck, W. Graf, Christian Soize","Special Issue on Computational Intelligence in Structural Engineering and Mechanics",2012,"","","","",8,"2022-07-13 09:31:46","","10.1111/j.1467-8667.2012.00784.x","","",,,,,1,0.10,0,3,10,"Computational Intelligence (CI) has its origins in artificial  intelligence and it has a broad algorithmic scope.  CI algorithms are often based on mimicking natural systems  or processes such as biological neural networks,  evolution, swarming of social organisms, and human  cognitive processes. They include algorithms in machine  learning (e.g., artificial neural networks, Bayesian  networks, and support vector machines), fuzzy modeling,  stochastic modeling, evolutionary computation  (e.g., genetic algorithms), and other optimization algorithms  such as simulated annealing, ant colony, and particle  swarm methods. CI methods have been applied  in structural engineering and mechanics for computational  modeling and surrogate models for response predictions,  system identification, robust optimization and  design, structural health monitoring, structural control,  and other tasks.","",""
2,"Sujay Pandey, Suvadeep Banerjee, A. Chatterjee","Error Resilient Neuromorphic Networks Using Checker Neurons",2018,"","","","",9,"2022-07-13 09:31:46","","10.1109/IOLTS.2018.8474075","","",,,,,2,0.50,1,3,4,"The last decade has seen tremendous advances in the application of artificial neural networks to solving problems that mimic human intelligence. Many of these systems are implemented using traditional digital compute engines where errors can occur during memory accesses or during numerical computation. While such networks are inherently error resilient, specific errors can result in incorrect decisions. This work develops a low overhead error detection and correction approach for multilayer artificial neural networks, here the hidden layer functions are approximated using checker neurons. Experimental results show that a high coverage of injected errors can be achieved with extremely low computational overhead using consistency properties of the encoded checks. A key side benefit is that the checks can flag errors when the network is presented outlier data that do not correspond to data with which the network is trained to operate.","",""
0,"Dimitris Spathis, A. Tefas","Learning to interact with high-dimensional data",2017,"","","","",10,"2022-07-13 09:31:46","","","","",,,,,0,0.00,0,2,5,"Artificial intelligence techniques and humans have skills that complement each other; the first being good in computation at lowest level (e.g. matrix multiplications) whereas people are better at abstracting and transferring knowledge from their experience. This thesis examines how we can combine the two aforementioned strengths in order to create robust, efficient, visual and interpretable machine learning. In particular, we research on dimensionality reduction techniques, which provide ways of projecting high-dimensional data in 2D. While dimensionality reduction is used for many reasons such as to reduce storage space and processing time, we focus on its usage for visualization. By visualizing the features in a two dimensional (2D) or three dimensional (3D) space, we make it easier for the human perception to understand the structure of data in a manner that feels natural. In layman terms, we examine what happens when a user interacts (moves, drags etc.) with some data points in 2D, which correspond to high-dimensional data. Most modern dimensionality reduction techniques are based in distance metrics, which are prone to outliers and crowding issues. By extending a recently proposed generic framework called Similarity Embedding Framework (SEF) which minimizes the objective function of the difference between the projection and a target, we define that target similarity matrix as the outcome of user interaction. Then, the framework learns iteratively the optimal projection with gradient descent. In essence, we just optimize two similarity matrices. Fast linear and kernel versions are proposed. The experimental procedure covers two interaction scenarios. The first one is a quite common in multi-dimensional projection literature, so that users are provided with a subset of data points (also known as control points) and they are free to rearrange it as they wish, usually to cluster them better. Based, on that interaction, techniques have been proposed that perform interpolation so that they are able to project the rest of unseen dataset, in a kind of semi-supervised learning. While most techniques of this kind rely on feeding coordinates to least-squares, bayesian modeling, eigendecomposition, or other kinds of solvers, this thesis suggests an end-toend optimization algorithm that models user interaction as a target similarity matrix. Extensive evaluations are performed where we report results that outperform competitive baselines in a wide range of datasets (numerical, image, text). The second interaction scenario involves questions like ""what happens if I move that class away?"". This scenario involves a modification of SEF in which the target matrix of user interaction is enriched with the information of high-dimensional neighbors. In essence, when a whole dataset is projected, we drag a class away and we set the target matrix so as that the points we moved to be similar with their high-dimensional neighbors. This procedure results in some surprising observations. Apart from improving classification precision and clustering, the new emerging structure of the projection unveils semantic manifolds. For example, on a Head Pose dataset, by just dragging the faces looking far left to the left and those looking far right to the right, all faces are re-arranged on a continuum even on the vertical axis (face up and down). This methodology could be used in domain adaptation of dense embeddings and transfer learning. Αριστοτέλειο Πανεπιστήμιο Θεσσαλονίκης","",""
5,"Beom-Jin Lee, Jung-Woo Ha, Kyung-Min Kim, Byoung-Tak Zhang","Evolutionary concept learning from cartoon videos by multimodal hypernetworks",2013,"","","","",11,"2022-07-13 09:31:46","","10.1109/CEC.2013.6557700","","",,,,,5,0.56,1,4,9,"Concepts have been widely used for categorizing and representing knowledge in artificial intelligence. Previous researches on concept learning have focused on unimodal data, usually on linguistic domains in a static environment. Concept learning from multimodal stream data, such as videos, remains a challenge due to their dynamic change and high-dimensionality. Here we propose an evolutionary method that simulates the process of human concept learning from multimodal video streams. Two key ideas on evolutionary concept learning are representing concepts in a large collection (population) of hyperedges or a hypergraph and to incrementally learning from video streams based on an evolutionary approach. The hypergraph is learned ""evolutionarily"" by repeating the generation and selection process of hyperedge concepts from the video data. The advantage of this evolutionary learning process is that the population-based distributed coding allows flexible and robust trace of the change of concept relations as the video story unfolds. We evaluate the proposed method on a suite of children's cartoon videos for 517 minutes of total playing time. Experimental results show that the proposed method effectively represents visual-textual concept relations and our evolutionary concept learning method effectively models the conceptual change as an evolutionary process. We also investigate the structure properties of the constructed concept networks.","",""
39,"Hassan Baghgar Bostan Abad, A. Y. Varjani, Taheri Asghar","Using Fuzzy Controller in Induction Motor Speed Control with Constant Flux",2007,"","","","",12,"2022-07-13 09:31:46","","","","",,,,,39,2.60,13,3,15,"Abstract — Variable speed drives are growing and varying. Drives expanse depend on progress in different part of science like power system, microelectronic, control methods, and so on. Artificial intelligent contains hard computation and soft computation. Artificial intelligent has found high application in most nonlinear systems same as motors drive. Because it has intelligence like human but there are no sentimental against human like angriness and.... Artificial intelligent is used for various points like approximation, control, and monitoring. Because artificial intelligent techniques can use as controller for any system without requirement to system mathematical model, it has been used in electrical drive control. With this manner, efficiency and reliability of drives increase and volume, weight and cost of them decrease. Keywords — Artificial intelligent, electrical motor, intelligent drive and control, I. I NTRODUCTION ECAUSE induction motors require low maintenance and are robust [8], have many applications in industry [9]. Along with industry progress, it indicates requirement to progresses drive with high performance [8]. DC motors are control abler than AC motors but they require much cost. In addition, in equal power, DC motors have higher volume and weight. Main variations in semiconductors, converters topology, analyze technique and simulation of electrical machines drive and newer control technique have had role in this progress [1]. Usually classical control is used in motors drive [8,10,11]. Design and implementation of Conventional controls have difficulties that nameable: a) It is basis on mathematical accurate model of system that usual it is not known [12,13]. b) Drives are nonlinear systems and Classical control performance with this system decrease [9,13]. c) Variation of machine parameters (especially in vector control [12]) by load disturbance [9], motor saturation [9,13] or thermal variations [13] do not cause expectation performance [9,13]. d) Classical linear control shows high performance only for one unique act point [13]. e) With choose improperly coefficient, classical control cannot receive acceptable result and suitable choose for constant coefficient in especial application condition with set point varying, necessarily is not optimum [14].","",""
0,"J. Beck, W. Graf, C. Soize","Introduction",2015,"","","","",13,"2022-07-13 09:31:46","","10.1111/mice.12150","","",,,,,0,0.00,0,3,7,"Computational Intelligence (CI) is a broad group of computational methods that have their origin in artificial intelligence and are often based on mimicking natural systems or processes such as biological neural networks, evolution, swarming of social organisms, and human cognitive processes. They include algorithms in machine learning (e.g., artificial neural networks, Bayesian networks, and support vector machines), fuzzy modeling (e.g., fuzzy logic, fuzzy sets), stochastic modeling, evolutionary computation (e.g., genetic algorithms), and other optimization algorithms such as simulated annealing, ant colony and particle swarm methods. CI methods have received much attention across all engineering disciplines in recent years. In structural engineering and mechanics, they have been applied for response predictions, system identification, robust optimization and robust design, structural health monitoring, structural control, and other tasks. This special issue of the CACAIE journal is devoted to some recent research in CI in structural engineering and mechanics. It is the fourth special issue devoted to this topic in CACAIE, the first three being issues 25:5 (July 2010), 27:9 (October 2012), and 29:3 (March 2014). Thirty-five papers were submitted for possible publication. Each paper was reviewed anonymously following the journal’s rigorous review process by four to eight reviewers. After undergoing two rounds of review, five papers that meet the high standards of the journal were finally approved for publication and inclusion in this issue. These papers apply several different CI approaches to improve computational methods for structural health monitoring and system identification. It is hoped that the reader interested in CI in structural engineering and mechanics will find these papers interesting and informative. We sincerely thank the many reviewers of the submitted papers for their thoughtful reviews and constructive contributions. Additionally, we thank all the authors of the submitted papers for their interest and effort in this special issue. We are also grateful to the Editor-inChief, Prof. Hojjat Adeli, for his encouragement, assistance, and support in producing this special issue. Since this year marks the 30th anniversary of the founding of CACAIE by Prof. Adeli, we take this opportunity to recognize the important contribution of the journal to progress in many subfields of civil engineering, and beyond. CACAIE provides a venue for high-quality papers on the development and application of computational methods for a broad range of engineering problems. Methods developed for one application often have much broader scope, and a strength of CACAIE is that it allows cross-pollination of ideas across many application areas. Another strength is the tight management of the editorial process by Prof. Adeli who is very focused on shepherding manuscripts through the process as quickly as possible, while ensuring that each paper receives at least three substantive reviews. We wish continued success for the journal.","",""
15,"T. Hussain, G. Vidaver","Flexible and Purposeful NPC Behaviors using Real-Time Genetic Control",2006,"","","","",14,"2022-07-13 09:31:46","","10.1109/CEC.2006.1688391","","",,,,,15,0.94,8,2,16,"There is an increasing need in modern computer games for non-player characters (NPCs) with robust behaviors that achieve game objectives while appearing flexible and believable to the human players interacting with those NPCs. Evolutionary approaches to game artificial intelligence (game AI) have produced successful results for complex game winning strategies, realistic behavior patterns for groups of simulated entities, and more. However, there has been relatively little effort on evolutionary techniques for producing rich NPC behaviors for interaction with human players. To explore whether evolutionary mechanisms can support real-time control of NPCs to produce flexible and purposeful behavior, we present our initial efforts at integrating a genetic algorithm based robotic controller with an off-the-shelf game to control one or more NPCs dynamically. We describe the integration effort and our initial observations, and discuss our plan for achieving richer NPC control and for performing more detailed analysis of the behaviors of the resulting NPCs.","",""
0,"Joe Hays, S. Ramamoorthy, Christian Tetzlaff","Editorial: Robust Artificial Intelligence for Neurorobotics",2021,"","","","",15,"2022-07-13 09:31:46","","10.3389/fnbot.2021.809903","","",,,,,0,0.00,0,3,1,"Neural computing is a powerful paradigm that has revolutionized machine learning. Building from early roots in the study of adaptive behavior and attempts to understand information processing in parallel and distributed neural architectures, modern neural networks have convincingly demonstrated successes in numerous areas—transforming the practice of computer vision, natural language processing, and even computational biology. Applications in robotics bring stringent constraints on size, weight and power constraints (SWaP), which challenge the developers of these technologies in new ways. Indeed, these requirements take us back to the roots of the field of neural computing, forcing us to ask how it could be that the human brain achieves with as little as 12 watts of power what seems to require entire server farms with state of the art computational and numerical methods. Likewise, even lowly insects demonstrate a degree of adaptivity and resilience that still defy easy explanation or computational replication. In this Research Topic, we have compiled the latest research addressing several aspects of these broadly defined challenge questions. As illustrated in Figure 1, the articles are organized into four prevailing themes: Sense, Think, Act, and Tools.","",""
19,"Thomas G. Dietterich","Robust artificial intelligence and robust human organizations",2018,"","","","",16,"2022-07-13 09:31:46","","10.1007/s11704-018-8900-4","","",,,,,19,4.75,19,1,4,"","",""
43,"Philip C. Jackson","Toward Human-Level Artificial Intelligence: Representation and Computation of Meaning in Natural Language",2019,"","","","",17,"2022-07-13 09:31:46","","","","",,,,,43,14.33,43,1,3,".................................................................................................. ix Preface .................................................................................................... xi","",""
17,"Yi-Shan Lin, Wen-Chuan Lee, Z. B. Celik","What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors",2020,"","","","",18,"2022-07-13 09:31:46","","10.1145/3447548.3467213","","",,,,,17,8.50,6,3,2,"EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.","",""
10,"M. Alomar, M. Hameed, N. Al‐Ansari, M. Alsaadi","Data-Driven Model for the Prediction of Total Dissolved Gas: Robust Artificial Intelligence Approach",2020,"","","","",19,"2022-07-13 09:31:46","","10.1155/2020/6618842","","",,,,,10,5.00,3,4,2,"Saturated total dissolved gas (TDG) is recently considered as a serious issue in the environmental engineering field since it stands behind the reasons for increasing the mortality rates of fish and aquatic organisms. The accurate and more reliable prediction of TDG has a very significant role in preserving the diversity of aquatic organisms and reducing the phenomenon of fish deaths. Herein, two machine learning approaches called support vector regression (SVR) and extreme learning machine (ELM) have been applied to predict the saturated TDG% at USGS 14150000 and USGS 14181500 stations which are located in the USA. For the USGS 14150000 station, the recorded samples from 13 October 2016 to 14 March 2019 (75%) were used for training set, and the rest from 15 March 2019 to 13 October 2019 (25%) were used for testing requirements. Similarly, for USGS 14181500 station, the hourly data samples which covered the period from 9 June 2017 till 11 March 2019 were used for calibrating the models and from 12 March 2019 until 9 October 2019 were used for testing the predictive models. Eight input combinations based on different parameters have been established as well as nine statistical performance measures have been used for evaluating the accuracy of adopted models, for instance, not limited, correlation of determination (        R      2        ), mean absolute relative error (MAE), and uncertainty at 95% (        U      95        ). The obtained results of the study for both stations revealed that the ELM managed efficiently to estimate the TDG in comparison to SVR technique. For USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.986 (0.986), MAE of 0.316 (0.441), and         U      95        of 3.592 (3.869). Lastly, for USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.991 (0.991), MAE of 0.338 (0.396), and         U      95        of 0.832 (0.837). In addition, ELM’s training process computational time is stated to be much shorter than that of SVM. The results also showed that the temperature parameter was the most significant variable that influenced TDG relative to the other parameters. Overall, the proposed model (ELM) proved to be an appropriate and efficient computer-assisted technology for saturated TDG modeling that will contribute to the basic knowledge of environmental considerations.","",""
0,"F. LeRon Shults","Progress in simulating human geography: Assemblage theory and the practice of multi-agent artificial intelligence modeling",2021,"","","","",20,"2022-07-13 09:31:46","","10.1177/03091325211059567","","",,,,,0,0.00,0,1,1,"Over the last few years, there has been an explosion of interest in assemblage theory among human geographers. During this same period, a growing number of scholars in the field have utilized computational methodologies to simulate the complex adaptive systems they study. However, very little attention has been paid to the connections between these two developments. This article outlines those connections and argues that more explicitly integrating assemblage theory and computer modeling can encourage a more robust philosophical understanding of both and facilitate progress in scientific research on the ways in which complex socio-material systems form and transform.","",""
0,"Bushra Rasheed, M. Usama, Asmara Safdar","Robust Artificial Intelligence Approach to Stabilize and Control Propeller Driven Hybrid UGV",2022,"","","","",21,"2022-07-13 09:31:46","","10.1109/ICAI55435.2022.9773375","","",,,,,0,0.00,0,3,1,"Hybrid Unmanned Ground Vehicle (HUGV) can drive on any terrain including walls and fly as well, using the multi directional thrust force of propellers. In the era of industrial revolution, hybrid UGVs need to be autonomous with intelligent decision making capabilities. During wall climbing of hybrid UGVs, stability is essential and depends on real time feedback from multiple sensors. To increase stability and control, it is proposed that PID control loops should be replaced by AI based algorithms that reduce the decision time and mathematical complexity. For autonomous movement in any terrain using the proposed model, intelligent UGVs can map and localize simultaneously.They can make intelligent decisions about mode of movement i.e. driving on ground or wall, steering on ground or wall, flying and maneuvering by using real time sensor readings. Integration of the proposed AI models with HUGV can be applied to many areas which are hard for humans to access, for instance; inspection of large structures, bio & nuclear hazard environments, planetary exploration & magnetic fields detection.","",""
9,"A. Wells, Shaan Patel, Jason B Lee, K. Motaparthi","Artificial intelligence in dermatopathology: Diagnosis, education, and research",2021,"","","","",22,"2022-07-13 09:31:46","","10.1111/cup.13954","","",,,,,9,9.00,2,4,1,"Artificial intelligence (AI) utilizes computer algorithms to carry out tasks with human‐like intelligence. Convolutional neural networks, a type of deep learning AI, can classify basal cell carcinoma, seborrheic keratosis, and conventional nevi, highlighting the potential for deep learning algorithms to improve diagnostic workflow in dermatopathology of highly routine diagnoses. Additionally, convolutional neural networks can support the diagnosis of melanoma and may help predict disease outcomes. Capabilities of machine learning in dermatopathology can extend beyond clinical diagnosis to education and research. Intelligent tutoring systems can teach visual diagnoses in inflammatory dermatoses, with measurable cognitive effects on learners. Natural language interfaces can instruct dermatopathology trainees to produce diagnostic reports that capture relevant detail for diagnosis in compliance with guidelines. Furthermore, deep learning can power computation‐ and population‐based research. However, there are many limitations of deep learning that need to be addressed before broad incorporation into clinical practice. The current potential of AI in dermatopathology is to supplement diagnosis, and dermatopathologist guidance is essential for the development of useful deep learning algorithms. Herein, the recent progress of AI in dermatopathology is reviewed with emphasis on how deep learning can influence diagnosis, education, and research.","",""
63,"M. VerMilyea, J. Hall, S. Diakiw, A. Johnston, T. Nguyen, D. Perugini, A. Miller, A. Picou, A. P. Murphy, M. Perugini","Development of an artificial intelligence-based assessment model for prediction of embryo viability using static images captured by optical light microscopy during IVF",2020,"","","","",23,"2022-07-13 09:31:46","","10.1093/humrep/deaa013","","",,,,,63,31.50,6,10,2,"Abstract STUDY QUESTION Can an artificial intelligence (AI)-based model predict human embryo viability using images captured by optical light microscopy? SUMMARY ANSWER We have combined computer vision image processing methods and deep learning techniques to create the non-invasive Life Whisperer AI model for robust prediction of embryo viability, as measured by clinical pregnancy outcome, using single static images of Day 5 blastocysts obtained from standard optical light microscope systems. WHAT IS KNOWN ALREADY Embryo selection following IVF is a critical factor in determining the success of ensuing pregnancy. Traditional morphokinetic grading by trained embryologists can be subjective and variable, and other complementary techniques, such as time-lapse imaging, require costly equipment and have not reliably demonstrated predictive ability for the endpoint of clinical pregnancy. AI methods are being investigated as a promising means for improving embryo selection and predicting implantation and pregnancy outcomes. STUDY DESIGN, SIZE, DURATION These studies involved analysis of retrospectively collected data including standard optical light microscope images and clinical outcomes of 8886 embryos from 11 different IVF clinics, across three different countries, between 2011 and 2018. PARTICIPANTS/MATERIALS, SETTING, METHODS The AI-based model was trained using static two-dimensional optical light microscope images with known clinical pregnancy outcome as measured by fetal heartbeat to provide a confidence score for prediction of pregnancy. Predictive accuracy was determined by evaluating sensitivity, specificity and overall weighted accuracy, and was visualized using histograms of the distributions of predictions. Comparison to embryologists’ predictive accuracy was performed using a binary classification approach and a 5-band ranking comparison. MAIN RESULTS AND THE ROLE OF CHANCE The Life Whisperer AI model showed a sensitivity of 70.1% for viable embryos while maintaining a specificity of 60.5% for non-viable embryos across three independent blind test sets from different clinics. The weighted overall accuracy in each blind test set was >63%, with a combined accuracy of 64.3% across both viable and non-viable embryos, demonstrating model robustness and generalizability beyond the result expected from chance. Distributions of predictions showed clear separation of correctly and incorrectly classified embryos. Binary comparison of viable/non-viable embryo classification demonstrated an improvement of 24.7% over embryologists’ accuracy (P = 0.047, n = 2, Student’s t test), and 5-band ranking comparison demonstrated an improvement of 42.0% over embryologists (P = 0.028, n = 2, Student’s t test). LIMITATIONS, REASONS FOR CAUTION The AI model developed here is limited to analysis of Day 5 embryos; therefore, further evaluation or modification of the model is needed to incorporate information from different time points. The endpoint described is clinical pregnancy as measured by fetal heartbeat, and this does not indicate the probability of live birth. The current investigation was performed with retrospectively collected data, and hence it will be of importance to collect data prospectively to assess real-world use of the AI model. WIDER IMPLICATIONS OF THE FINDINGS These studies demonstrated an improved predictive ability for evaluation of embryo viability when compared with embryologists’ traditional morphokinetic grading methods. The superior accuracy of the Life Whisperer AI model could lead to improved pregnancy success rates in IVF when used in a clinical setting. It could also potentially assist in standardization of embryo selection methods across multiple clinical environments, while eliminating the need for complex time-lapse imaging equipment. Finally, the cloud-based software application used to apply the Life Whisperer AI model in clinical practice makes it broadly applicable and globally scalable to IVF clinics worldwide. STUDY FUNDING/COMPETING INTEREST(S) Life Whisperer Diagnostics, Pty Ltd is a wholly owned subsidiary of the parent company, Presagen Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation and Startup Fund (RCSF). ‘In kind’ support and embryology expertise to guide algorithm development were provided by Ovation Fertility. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. Presagen has filed a provisional patent for the technology described in this manuscript (52985P pending). A.P.M. owns stock in Life Whisperer, and S.M.D., A.J., T.N. and A.P.M. are employees of Life Whisperer.","",""
58,"M. Smith","Getting value from artificial intelligence in agriculture",2020,"","","","",24,"2022-07-13 09:31:46","","10.1071/AN18522","","",,,,,58,29.00,58,1,2,"Artificial intelligence (AI) is beginning to live up to its promise of delivering real value, driven by recent advances in the availability of relevant data, computation and algorithms. In the present paper, I discuss the value to agriculture from AI over the next decade. The more immediate applications will be to improve precision information about what is happening on the farm by improving what is being detected and measured. A consequence of this are more accurate alerts to farmers. Another is an increased ability to understand why phenomena occur in farm systems, so as to improve their management. From improved data and understanding come improved predictions, enabling more optimal decisions about how to manage farm systems and stimulating the development of decision support and recommender systems. In many cases, robotics and automated systems will remove much of the need for human decision-making and improve farm efficiencies and farm health. Artificial intelligence will also be needed to enable organisations to harness the value of information distributed throughout supply chains, including farm data. Digital twins will also emerge as an important paradigm to improve how information about farm entities is organised to support decision-making. There are also likely to be negative impacts from AI, such as disruption to the roles and skills needed from farm workers, indicating the need to consider the social and ethical impacts of AI each time a new capability is introduced. I conclude that understanding these challenges more deeply tends to highlight new opportunities for positive change.","",""
0,"D. Kothari, Mayank Patel, Ajay Kumar Sharma","Implementation of Grey Scale Normalization in Machine Learning & Artificial Intelligence for Bioinformatics using Convolutional Neural Networks",2021,"","","","",25,"2022-07-13 09:31:46","","10.1109/ICICT50816.2021.9358549","","",,,,,0,0.00,0,3,1,"Machine Learning is a trending field nowadays and is very well known as an application of Artificial Intelligence (AI). Machine learning makes use of secure arithmetical algorithms to construct computers assignment in a constructive way without being unequivocally programmed. The algorithms acquire freedom of a participated value and estimate output for this by utilizing definite arithmetical methods. The key function of machine learning is to create smart machines that can visualize and work related to human beings. Artificial Intelligence has been witnessing a massive improvement in bridging the gap among the capabilities of humans and technologies. Similarly, one of the characteristics of the field were tried to combine the outstanding effects. A Convolution Neural Network (CNN) is a sort of Deep Learning algorithm which can obtain an input image, assign consequence to different aspects in the image and capable to differentiate one from the other. The pre-processing requirement in a CNN is lesser as compared to other classification algorithms. CNN has the capability to find out these filter's characteristics. Bioinformatics is a term that is a mixture of two terms bio and informatics. Bio means associated with biology and informatics means in a sequence of information. Thus bioinformatics is an area that deals with handing out and accepting biological statistics using the computational and arithmetical approaches. Machine Learning has added up to of applications in the area of bioinformatics. Machine Learning finds its submission in the subsequent subfields of bioinformatics. The aim of this article is to perform a grayscale normalization of a selected image and thereafter to reduce the effect of illumination differences. Normalization is considered so that CNN works in a faster manner. Different models are available but Keras model is selected to perform this task. Keras supports the style of data preparation for Image statistics via the Image Data Generator group and Application Programming Interface (API). The Image Data Generator group in Keras provides a matching set of techniques for scaling pixel standards in the image dataset subsequent to modeling. The Keras functional API provides an additional flexible approach for significant models. It particularly allows Identifving several input or output models as well as models that allocate layers.","",""
50,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello","Artificial Intelligence, Algorithmic Pricing, and Collusion",2020,"","","","",26,"2022-07-13 09:31:46","","10.1257/AER.20190623","","",,,,,50,25.00,13,4,2,"Increasingly, pricing algorithms are supplanting human decision making in real marketplaces. To inform the competition policy debate on the possible consequences of this development, we experiment with pricing algorithms powered by Artificial Intelligence (AI) in controlled environments (computer simulations), studying the interaction among a number of Q-learning algorithms in a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. In this setting the algorithms consistently learn to charge supra-competitive prices, without communicating with one another. The high prices are sustained by classical collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.","",""
29,"Grayson W. Armstrong, A. Lorch","A(eye): A Review of Current Applications of Artificial Intelligence and Machine Learning in Ophthalmology",2019,"","","","",27,"2022-07-13 09:31:46","","10.1097/IIO.0000000000000298","","",,,,,29,9.67,15,2,3,"Artificial intelligence (AI) is a branch of computer science that aims to enable computers to perform human-like tasks. Although AI is a broad discipline, machine learning is a specific branch of AI that uses computer algorithms capable of “learning” through the simulation of human intelligence. Machine learning algorithms have been applied to the medical field since the 1970s,1 and since that time have proven useful in computerassisted diagnosis, screening, and prognostication of disease.2–7 Ophthalmology is uniquely capable of capitalizing on the promise of AI. Ophthalmologists, during routine clinical encounters, generate robust data sources capable of supporting machine learning algorithms including multimodal ophthalmic images and quantifiable metrics such as visual acuity (VA), intraocular pressure, and cup to disk ratio. To date, AI techniques have been applied to ophthalmology to screen for and diagnose diseases, such as diabetic retinopathy (DR), age-related macular degeneration (AMD), macular edema (ME), glaucoma, keratoconus, postlaserassisted in situ keratomileusis corneal ectasia, retinopathy of prematurity (ROP), and cataracts, as well as predict the prognosis of various ophthalmic diseases. Advances in ophthalmology-specific AI stand to increase patient access to clinical screening and diagnosis as well as decrease health care costs, especially when applied to high-risk populations, low-resource communities, or when combined with telemedicine initiatives. This review provides an introduction to AI andmachine learning, as well as an overview of current applications in the field of ophthalmology.","",""
0,"Ewa Szewczyk","Artificial intelligence in administrative law and procedure",2021,"","","","",28,"2022-07-13 09:31:46","","10.13166/wsge//krcl6757","","",,,,,0,0.00,0,1,1,"Introduction Since J. McCarthy used the term ‘artificial intelligence’ (AI) in the 1950s, it has become a key concept in the technological development of all mankind. It has appeared in every area of life and science. AI has become established in areas of life that were previously thought to be reserved for decision-making by human beings. Artificial intelligence is based on the analysis of large volumes of data, used in algorithms. According to the modern definition, artificial intelligence encompasses the area of knowledge that includes fuzzy logic, evolutionary computation, neural networks, artificial life, and robotics, and one of its essential features is the ability to learn1 and take into account new circumstances when solving a given problem2. In other words, artificial intelligence is the ability of a machine to mimic or imitate human intelligence3. Algorithms are nothing new. They have been used in computer programmes for decades. Today, however, advanced algorithms have become digital robots – often sophisticated computer programmes (rather than physical entities) with the ability to adapt and ‘learn’. However, there is no denying that the unhindered development of AI technologies is marred with public concern and is by no means universally embraced, even though the Covid-19 pandemic has","",""
0,"Aabid Ali","Artificial Intelligence",2021,"","","","",29,"2022-07-13 09:31:46","","10.1017/9781009036719.007","","",,,,,0,0.00,0,1,1,"Artificial Intelligence may be defined as intelligence displayed by machines, systems or agents or by entities other than living beings. Apparently, the term seems simple but the definition bears deeper connotations. The terms intelligence and creativity have long been the prerogatives associated with the humans or have been the privileges enjoyed by them since the dawn of the creation. The views ‘creativity is computation’ or ‘cognition is computation’ and ‘mind as machine’ has offset the traditional theories, assumptions and interpretations held so far in the philosophy and theory of mind. AI’s push to impart intelligence to non-human entities to enable them to behave intelligently and creatively or as Boden would put it “to make computers do the sort of things that minds can do” (Boden 1) has challenged the very traditional fabric of our perception and comprehension, conception and construction related to our learning and living dispensations.","",""
0,"A. D. W. Sumari, I. Syamsiana","A Simple Introduction to Cognitive Artificial Intelligence’s Knowledge Growing System",2021,"","","","",30,"2022-07-13 09:31:46","","10.1109/DATABIA53375.2021.9650179","","",,,,,0,0.00,0,2,1,"Knowledge Growing System (KGS) since its introduction in 2009, has been stated as the foundation of Cognitive Artificial Intelligence (CAI). Because of its computation mechanism simplicity and it does not burden the computation resources, various use-cases have applied KGS to solve their problems. The KGS development was inspired by the growing of knowledge within human brain when thinking during carrying out interactions to a phenomenon in its environment. KGS learns to the data received at that time and at the next series of time that are sensed and perceived by its sensory organs, and uses them to generate knowledge. By combining approaches and techniques from cognitive psychology, mathematics, social science, and AI fields, we created simple mathematics formulas called ASSA2010 (Arwin Sumari-Suwandi Ahmad year 2010) information-inferencing fusion method for KGS’ knowledge growing mechanism. In this article, we deliver a simple introduction to KGS and also some of its utilizations for humankind.","",""
1,"C. Bormann, M. Kanakasabapathy, Prudhvi Thirumalaraju, I. Dimitriadis, I. Souter, K. Hammer, H. Shafiee","O-125 Development of an artificial intelligence embryo witnessing system to accurately track and identify patient specific embryos in a human IVF laboratory",2021,"","","","",31,"2022-07-13 09:31:46","","10.1093/humrep/deab126.050","","",,,,,1,1.00,0,7,1,"      Can convolutional neural networks (CNN) be used as a witnessing system to accurately track and identify patient specific embryos at the cleavage stage of development?        We developed the first artificial intelligence driven witnessing system to accurately track cleavage and blastocyst stage embryos in a human ART laboratory.        There are reports of human errors in embryo tracking that have led to the births of children with different genetic makeup than their birth parents. Clinical practices rely on manual identification, barcodes or radio-frequency identification technology to track embryos. These systems are designed to track culture dishes but are unable to monitor developing embryos within the dish to help ensure an error-free patient match. Previously, we developed an AI witnessing system to track blastocysts with 100% accuracy. The goal of this study was to determine whether an AI witnessing system could be developed that accurately tracks cleavage stage embryos.        A pre-developed deep neural network technology was first trained and tested on 4944 embryos images. The algorithm processed embryo images for each patient and produced a unique key that was associated with the patient ID at 60 hpi, which formed our library. When the algorithm evaluated embryos at 64 hpi it generated another key that was matched with the patient’s unique key available in the library.        A total of 3068 embryos from 412 patients were examined by the CNN at both 60 hpi and 64 hpi. These timepoints were chosen as they reflect the time our laboratory evaluates Day 3 embryos (60 hpi) and the time we move them to another dish and prepare them for transfer (64 hpi). The patient cohorts ranged from 3-12 embryos per patient.        The accuracy of the CNN in correctly matching the patient identification with the patient embryo cohort was 100% (CI: 99.1% to 100.0%, n = 412).        Limitations of this study include that all embryos were imaged under identical conditions and within the same EmbryoScope. Additionally, this study only examined fresh Day 3 embryos cultured over a span of 4 hours. Future studies should include images of fresh and frozen/thawed embryos captured using different imaging systems.        This study describes the first artificial intelligence-based approach for cleavage stage embryo tracking and patient specimen identification in the IVF laboratory. This technology offers a robust witnessing step based on unique morphological features that are specific to each individual embryo.        This work was partially supported by the Brigham Precision Medicine Developmental Award (Brigham Precision Medicine Program, Brigham and Women’s Hospital), Partners Innovation Discovery Grant (Partners Healthcare), and R01AI118502, and R01AI138800. ","",""
0,"T. Leung, C. L. Lee, P. Chiu","P-064 Application of an artificial intelligence model for morphologic prediction of fertilization-competent human spermatozoa",2021,"","","","",32,"2022-07-13 09:31:46","","10.1093/humrep/deab127.043","","",,,,,0,0.00,0,3,1,"      What is the role of artificial intelligence in selecting fertilization-competent human spermatozoa according to their morphological characteristics?         The established AI model in this study can be potentially used to select semen samples with superior fertilization potential in clinical settings.        Defective spermatozoa-zona pellucida (ZP) interaction causes subfertility and is a major cause of low IVF fertilization rates. While ICSI benefits patients with defective spermatozoa-ZP binding, a standard method to identify such patients prior to conventional IVF is lacking. The application of artificial intelligence to sperm morphology analysis has become a topic of growing interest owing to the fact that the conventional assessment is highly subjective and time-consuming. Deep-learning, a core element of artificial intelligence (AI), incorporates the convolutional neural networks (CNN) to process all the data composing a digital image through successive layers to identify the underlying pattern.        The fertilization-competent spermatozoa were isolated according to their binding ability to the ZP. The ZP-bound and -unbound spermatozoa were collected for functional assays and to establish an AI model for morphologic prediction of sperm fertilization potential. Human spermatozoa (n = 289) were isolated from normozoospermic samples. Human oocytes (n = 562) were collected from an assisted reproduction program in Hong Kong. Sample collection has been ongoing and will continue until the end of this study in November 2021.        Sperm-ZP binding assay was employed to collect ZP-bound and -unbound spermatozoa. The fertilization potential and genetic quality of the collected spermatozoa were evaluated by our established protocols. Diff-Quik- stained images of ZP-bound and -unbound spermatozoa were collected respectively for the establishment of an AI model. A novel algorithm for sperm image transformation and segmentation was developed to pre-process the images. CNN architecture was then applied on these pre-processed images for feature extraction and model training.        Our result showed that the sperm-ZP binding assay had no detrimental effect on sperm viability when compared with the raw samples and unbound-sperm subpopulations. ZP-bound spermatozoa were found with statistically higher acrosome reaction rates, improved DNA integrity, better morphology, lower protamine deficiency and higher methylation level when compared with the unbound spermatozoa. A deep-learning model was trained and validated by analyzing a total of 1,334 and 885 of ZP-bound/unbound spermatozoa to evaluate the predictive power of sperm morphology for ZP binding ability. Our newly trained AI-based model showed initial success in classifying the ZP-bound/ unbound spermatozoa according to their morphological characteristics with high accuracy of 85% and low computational complexity.        This sperm selection method requires micromanipulation and relatively long processing time to recover ZP-bound spermatozoa. In addition to limited availability, the use of human materials may result in interassay variations affecting the reproducibility of this method among laboratories.        In light of current findings, AI-based sperm selection method may provide high predictive values of sperm fertilization potential for clinical purposes. This method is particularly applicable to patients who had poor fertilization outcomes after conventional IVF treatments or those with high degree of defective sperm-ZP binding ability.         not applicable ","",""
0,"T. Leung, C. L. Lee, P. Chiu","P–064 Application of an artificial intelligence model for morphologic prediction of fertilization-competent human spermatozoa",2021,"","","","",33,"2022-07-13 09:31:46","","10.1093/humrep/deab130.063","","",,,,,0,0.00,0,3,1,"      What is the role of artificial intelligence in selecting fertilization-competent human spermatozoa according to their morphological characteristics? Summary answer: The established AI model in this study can be potentially used to select semen samples with superior fertilization potential in clinical settings.        Defective spermatozoa-zona pellucida (ZP) interaction causes subfertility and is a major cause of low IVF fertilization rates. While ICSI benefits patients with defective spermatozoa-ZP binding, a standard method to identify such patients prior to conventional IVF is lacking. The application of artificial intelligence to sperm morphology analysis has become a topic of growing interest owing to the fact that the conventional assessment is highly subjective and time-consuming. Deep-learning, a core element of artificial intelligence (AI), incorporates the convolutional neural networks (CNN) to process all the data composing a digital image through successive layers to identify the underlying pattern.        The fertilization-competent spermatozoa were isolated according to their binding ability to the ZP. The ZP-bound and -unbound spermatozoa were collected for functional assays and to establish an AI model for morphologic prediction of sperm fertilization potential. Human spermatozoa (n = 289) were isolated from normozoospermic samples. Human oocytes (n = 562) were collected from an assisted reproduction program in Hong Kong. Sample collection has been ongoing and will continue until the end of this study in November 2021.        Sperm-ZP binding assay was employed to collect ZP-bound and -unbound spermatozoa. The fertilization potential and genetic quality of the collected spermatozoa were evaluated by our established protocols. Diff-Quik- stained images of ZP-bound and -unbound spermatozoa were collected respectively for the establishment of an AI model. A novel algorithm for sperm image transformation and segmentation was developed to pre-process the images. CNN architecture was then applied on these pre-processed images for feature extraction and model training.        Our result showed that the sperm-ZP binding assay had no detrimental effect on sperm viability when compared with the raw samples and unbound-sperm subpopulations. ZP-bound spermatozoa were found with statistically higher acrosome reaction rates, improved DNA integrity, better morphology, lower protamine deficiency and higher methylation level when compared with the unbound spermatozoa. A deep-learning model was trained and validated by analyzing a total of 1,334 and 885 of ZP-bound/unbound spermatozoa to evaluate the predictive power of sperm morphology for ZP binding ability. Our newly trained AI-based model showed initial success in classifying the ZP-bound/ unbound spermatozoa according to their morphological characteristics with high accuracy of 85% and low computational complexity.        This sperm selection method requires micromanipulation and relatively long processing time to recover ZP-bound spermatozoa. In addition to limited availability, the use of human materials may result in interassay variations affecting the reproducibility of this method among laboratories.  Wider implications of the findings: In light of current findings, AI-based sperm selection method may provide high predictive values of sperm fertilization potential for clinical purposes. This method is particularly applicable to patients who had poor fertilization outcomes after conventional IVF treatments or those with high degree of defective sperm-ZP binding ability.        Not applicable ","",""
44,"Chiara Longoni, Luca Cian","Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect",2020,"","","","",34,"2022-07-13 09:31:46","","10.1177/0022242920957347","","",,,,,44,22.00,22,2,2,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).","",""
24,"Rainer Mühlhoff","Human-aided artificial intelligence: Or, how to run large computations in human brains? Toward a media sociology of machine learning",2019,"","","","",35,"2022-07-13 09:31:46","","10.1177/1461444819885334","","",,,,,24,8.00,24,1,3,"Today, artificial intelligence (AI), especially machine learning, is structurally dependent on human participation. Technologies such as deep learning (DL) leverage networked media infrastructures and human-machine interaction designs to harness users to provide training and verification data. The emergence of DL is therefore based on a fundamental socio-technological transformation of the relationship between humans and machines. Rather than simulating human intelligence, DL-based AIs capture human cognitive abilities, so they are hybrid human-machine apparatuses. From a perspective of media philosophy and social-theoretical critique, I differentiate five types of “media technologies of capture” in AI apparatuses and analyze them as forms of power relations between humans and machines. Finally, I argue that the current hype about AI implies a relational and distributed understanding of (human/artificial) intelligence, which I categorize under the term “cybernetic AI.” This form of AI manifests in socio-technological apparatuses that involve new modes of subjectivation, social control, and digital labor.","",""
8,"I. Wiafe, F. N. Koranteng, Emmanuel Nyarko Obeng, Nana Assyne, Abigail Wiafe, S. Gulliver","Artificial Intelligence for Cybersecurity: A Systematic Mapping of Literature",2020,"","","","",36,"2022-07-13 09:31:46","","10.1109/ACCESS.2020.3013145","","",,,,,8,4.00,1,6,2,"Due to the ever-increasing complexities in cybercrimes, there is the need for cybersecurity methods to be more robust and intelligent. This will make defense mechanisms to be capable of making real-time decisions that can effectively respond to sophisticated attacks. To support this, both researchers and practitioners need to be familiar with current methods of ensuring cybersecurity (CyberSec). In particular, the use of artificial intelligence for combating cybercrimes. However, there is lack of summaries on artificial intelligent methods for combating cybercrimes. To address this knowledge gap, this study sampled 131 articles from two main scholarly databases (ACM digital library and IEEE Xplore). Using a systematic mapping, the articles were analyzed using quantitative and qualitative methods. It was observed that artificial intelligent methods have made remarkable contributions to combating cybercrimes with significant improvement in intrusion detection systems. It was also observed that there is a reduction in computational complexity, model training times and false alarms. However, there is a significant skewness within the domain. Most studies have focused on intrusion detection and prevention systems, and the most dominant technique used was support vector machines. The findings also revealed that majority of the studies were published in two journal outlets. It is therefore suggested that to enhance research in artificial intelligence for CyberSec, researchers need to adopt newer techniques and also publish in other related outlets.","",""
0,"Chengbing Tan, Qun Chen","Application of an artificial intelligence algorithm model of memory retrieval and roaming in sorting Chinese medicinal materials",2021,"","","","",37,"2022-07-13 09:31:46","","10.3233/jcm-215477","","",,,,,0,0.00,0,2,1,"In order to capture autobiographical memory, inspired by the development of human intelligence, a computational AM model for autobiographical memory is proposed in this paper, which is a three-layer network structure, in which the bottom layer encodes the event-specific knowledge comprising 5W1H, and provides retrieval clues to the middle layer, encodes the related events, and the top layer encodes the event set. According to the bottom-up memory search process, the corresponding events and event sets can be identified in the middle layer and the top layer respectively; At the same time, AM model can simulate human memory roaming through the process of rule-based memory retrieval. The computational AM model proposed in this paper not only has robust and flexible memory retrieval, but also has better response performance to noisy memory retrieval cues than the commonly used memory retrieval model based on keyword query method, and can also imitate the roaming phenomenon in memory.","",""
13,"Edgar Bermudez Contreras, B. J. Clark, A. Wilber","The Neuroscience of Spatial Navigation and the Relationship to Artificial Intelligence",2020,"","","","",38,"2022-07-13 09:31:46","","10.3389/fncom.2020.00063","","",,,,,13,6.50,4,3,2,"Recent advances in artificial intelligence (AI) and neuroscience are impressive. In AI, this includes the development of computer programs that can beat a grandmaster at GO or outperform human radiologists at cancer detection. A great deal of these technological developments are directly related to progress in artificial neural networks—initially inspired by our knowledge about how the brain carries out computation. In parallel, neuroscience has also experienced significant advances in understanding the brain. For example, in the field of spatial navigation, knowledge about the mechanisms and brain regions involved in neural computations of cognitive maps—an internal representation of space—recently received the Nobel Prize in medicine. Much of the recent progress in neuroscience has partly been due to the development of technology used to record from very large populations of neurons in multiple regions of the brain with exquisite temporal and spatial resolution in behaving animals. With the advent of the vast quantities of data that these techniques allow us to collect there has been an increased interest in the intersection between AI and neuroscience, many of these intersections involve using AI as a novel tool to explore and analyze these large data sets. However, given the common initial motivation point—to understand the brain—these disciplines could be more strongly linked. Currently much of this potential synergy is not being realized. We propose that spatial navigation is an excellent area in which these two disciplines can converge to help advance what we know about the brain. In this review, we first summarize progress in the neuroscience of spatial navigation and reinforcement learning. We then turn our attention to discuss how spatial navigation has been modeled using descriptive, mechanistic, and normative approaches and the use of AI in such models. Next, we discuss how AI can advance neuroscience, how neuroscience can advance AI, and the limitations of these approaches. We finally conclude by highlighting promising lines of research in which spatial navigation can be the point of intersection between neuroscience and AI and how this can contribute to the advancement of the understanding of intelligent behavior.","",""
24,"Martina Gurgitano, S. A. Angileri, G. Rodà, Alessandro Liguori, M. Pandolfi, A. Ierardi, B. Wood, G. Carrafiello","Interventional Radiology ex-machina: impact of Artificial Intelligence on practice",2021,"","","","",39,"2022-07-13 09:31:46","","10.1007/s11547-021-01351-x","","",,,,,24,24.00,3,8,1,"","",""
24,"Maxime Sermesant, H. Delingette, H. Cochet, P. Jaïs, N. Ayache","Applications of artificial intelligence in cardiovascular imaging",2021,"","","","",40,"2022-07-13 09:31:46","","10.1038/s41569-021-00527-2","","",,,,,24,24.00,5,5,1,"","",""
18,"Saman Tauqir","Is Artificial Intelligence Transforming Dentistry Today?",2021,"","","","",41,"2022-07-13 09:31:46","","10.37762/jgmds.8-4.263","","",,,,,18,18.00,18,1,1,"Since the birth of science, the most fascinating structure of the human body is the human brain.  Over the past centuries’ researchers have been developing the latest technologies to imitate and explore how the human brain functions. However, to develop a machine that thinks like a human brain is still a dream for researchers. Aristotle’s early efforts to devise logical thinking via his syllogisms (a three-part deductive reasoning) were a source of inspiration for modern computers and technologies1. In the1950, Alan Turing designed a machine to decode encrypted messages, which was a breakthrough of super computers in the days of yore. He designed the “Turing Test” which was coined to assess whether a computer could exhibit intelligence better known as “artificial intelligence” (AI) today2. AI is “a field of science and engineering concerned with the computational understanding of what is commonly called intelligent behavior, and with the creation of artifacts that exhibit such behaviour”3.  Since 1980, AI has come a long way, virtual reality is being used in dental education these days to create real life situations and promote clinical work on simulators to eliminate risk factors associated with training on live patients. Recently artificial intelligence has been integrated with tutoring systems like “Unified Medical Language System” (UMLS), which have resulted in a better quality of feedback, which the preclinical virtual patients provide to the students4,5. This interactive phase helps students to evaluate their clinical skills and compare their skills with the standard ones, thus creating an ideal and high-quality training environment. Studies have been carried out regarding the efficacy of AI systems, which have stipulated that preclinical students build higher competencies than with the use of traditional simulator units6-8.  Currently AI inbuilt virtual dental assistants are present in the market. They can execute various chair side tasks with greater accuracy and less manpower ensuring minimum error during the procedures. In the world of implantology and maxillofacial surgery AI helps plan and prepare surgeries with smallest details forgoing actual surgery. Some exceptional uses of AI include robotic surgeries in the field of maxillofacial surgery and bioprinting (where tissues and organs can be reconstructed in thin layers)9. The field of AI has flourished to great extent in the past decade; AI systems are an aid to the field of dentistry and dental education.   This narrative attempts to explain possible AI-based applications in the future, it can be used for dental diagnosis, planning out treatments, conducting image analysis, and record keeping. AI-based technologies streamline and reduce laborious workforce to routine tasks, it ensures dental procedures are possible at a lower cost and ultimately makes predictive, preventive, and participatory dentistry possible. The use of AI in dental procedures needs to be guaranteed; its application with human oversight and evidence-based dentistry shall be expected. Dental education needs to be introduced to clinical AI solutions by promoting digital literacy in the future dental liveware.","",""
167,"Max Tegmark","Life 3.0: Being Human in the Age of Artificial Intelligence",2017,"","","","",42,"2022-07-13 09:31:46","","","","",,,,,167,33.40,167,1,5,"New York Times Best Seller How will Artificial Intelligence affect crime, war, justice, jobs, society and our very sense of being human? The rise of AI has the potential to transform our future more than any other technologyand theres nobody better qualified or situated to explore that future than Max Tegmark, an MIT professor whos helped mainstream research on how to keep AI beneficial. How can we grow our prosperity through automation without leaving people lacking income or purpose? What career advice should we give todays kids? How can we make future AI systems more robust, so that they do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will machines eventually outsmart us at all tasks, replacing humans on the job market and perhaps altogether? Will AI help life flourish like never before or give us more power than we can handle? What sort of future do you want? This book empowers you to join what may be the most important conversation of our time. It doesnt shy away from the full range of viewpoints or from the most controversial issuesfrom superintelligence to meaning, consciousness and the ultimate physical limits on life in the cosmos.","",""
0,"T. Mahmood, Muhammad Owais, Kyoung Jun Noh, Hyo Sik Yoon, A. Haider, H. Sultan, K. Park","Artificial Intelligence-based Segmentation of Nuclei in Multi-organ Histopathology Images: Model Development and Validation (Preprint)",2020,"","","","",43,"2022-07-13 09:31:46","","10.2196/preprints.24394","","",,,,,0,0.00,0,7,2,"  BACKGROUND  Accurate nuclei segmentation in histopathology images plays a key role in digital pathology. It is considered a prerequisite for the determination of cell phenotype, nuclear morphometrics, cell classification, and the grading and prognosis of cancer. However, it is a very challenging task because of the different types of nuclei, large intra-class variations, and diverse cell morphologies. Consequently, the manual inspection of such images under high-resolution microscopes is tedious and time-consuming. Alternatively, artificial intelligence (AI)-based automated techniques, which are fast, robust, and require less human effort, can be used. Recently, several AI-based nuclei segmentation techniques have been proposed. They have shown a significant performance improvement for this task, but there is room for further improvement. Thus, we propose an AI-based nuclei segmentation technique in which we adopt a new nuclei segmentation network empowered by residual skip connections to address this issue.      OBJECTIVE  The aim of this study was to develop an AI-based nuclei segmentation method for histopathology images of multiple organs.       METHODS  Our proposed residual-skip-connections-based nuclei segmentation network (R-NSN) is comprised of two main stages: Stain normalization and nuclei segmentation as shown in Figure 2. In the 1st stage, a histopathology image is stain normalized to balance the color and intensity variation. Subsequently, it is used as an input to the R-NSN in stage 2, which outputs a segmented image.       RESULTS  Experiments were performed on two publicly available datasets: 1) The Cancer Genomic Atlas (TCGA), and 2) Triple-negative Breast Cancer (TNBC). The results show that our proposed technique achieves an aggregated Jaccard index (AJI) of 0.6794, Dice coefficient of 0.8084, and F1-measure of 0.8547 on the TCGA dataset, and an AJI of 0.7332, Dice coefficient of 0.8441, precision of 0.8352, recall of 0.8306, and F1-measure of 0.8329 on the TNBC dataset. These values are higher than those of the state-of-the-art methods.      CONCLUSIONS  The proposed R-NSN has the potential to maintain crucial features by using the residual connectivity from the encoder to the decoder and uses only a few layers, which reduces the computational cost of the model. The selection of a good stain normalization technique, the effective use of residual connections to avoid information loss, and the use of only a few layers to reduce the computational cost yielded outstanding results. Thus, our nuclei segmentation method is robust and is superior to the state-of-the-art methods. We expect that this study will contribute to the development of computational pathology software for research and clinical use and enhance the impact of computational pathology.      CLINICALTRIAL   ","",""
0,"Shanqi Pang Dr, Yongmei Li Prof","Artificial Intelligence Techniques for Cyber Security Applications",2020,"","","","",44,"2022-07-13 09:31:46","","10.46532/ijaict-2020021","","",,,,,0,0.00,0,2,2,"Considering the enhancement in technology, criminals have been using cyberspace in order to commit many crimes. Therefore, it should be noted that cybercrimes are exposed to a number of threats and intrusions if not safeguarded well. Human and physical intervention tend not to be very adequate for the protection and tracking of such infrastructure, that is why there should be the establishment of multifaceted cyber defense networks, which are flexible, robust, and adjustable in order sense a massive collection of invasion and creation of real-time choices. Nevertheless, significant number of bio-related computing techniques of AI (artificial intelligence) tend to be increasing hence a significant role is played in detecting and preventing cybercrime. The main aim of this paper is outlining the actual advancement that have been made possible due to the application of AI methods for the fight against cybercrimes, in order to reveal how the methods are efficient in sensing and preventing cyber invasions, also providing a brief overview of the future works. Keywords— Computational intelligence, Artificial Intelligence, Intrusion detection and prevention systems, Cyber crime","",""
0,"K. Shrivastav, N. Taneja, P. Arambam, Vandana Bhatia, S. Batra, Harpreet Singh, E. Abed, P. Ranjan, Rajiv Janardhanan∗h","An Artificial Intelligence Enabled Multimedia Tool for Rapid Screening of Cervical Cancer",2020,"","","","",45,"2022-07-13 09:31:46","","","","",,,,,0,0.00,0,9,2,"Cervical cancer is a major public health challenge. Further mitigation of cervical cancer can greatly benefit from development of innovative and disruptive technologies for its rapid screening and early detection. The primary objective of this study is to contribute to this aim through large scale screening by development of Artificial Intelligence enabled Intelligent Systems as they can support human cancer experts in making more precise and timely diagnosis. Our current study is focused on development of a robust and interactive algorithm for analysis of colposcope-derived images analysis and a diagnostic tool/scale namely the OMThe Onco-Meter. This tool was trained and tested on 300 InEmail addresses: kdshrivastav@amity.edu (Kumar Dron Shrivastav), ntaneja@amity.edu (Neha Taneja), priyadarshini@batrahospitaldelhi.org (Priyadarshini Arambam), vbhatia2@amity.edu (Vandana Bhatia), shelly.batra@opasha.org (Shelly Batra), crbhmrc1@batrahospitaldelhi.org (Shelly Batra), hsingh@bmi.icmr.org.in (Harpreet Singh), abed@isr.umd.edu (Eyad H. Abed), ranjan.p@srmap.edu.in (Priya Ranjan), rjanardhanan@amity.edu (Rajiv Janardhanan∗) Preprint submitted to The Lancet Digital Health June 1, 2020 dian subjects/patients yielding 77% accuracy with a sensitivity of 83.56% and a specificity of 59.25%. OM-The Oncometer is capable of classifying cervigrams into cervical dysplasia, carcinoma in− situ (CIS) and invasive cancer(IC). Programming language R has been used to implement and compute earth mover distances (EMD) to characterize different diseases labels associated with cervical cancer, computationally. Deployment of automated tools will facilitate early diagnosis in a noninvasive manner leading to a timely clinical intervention for cervical cancer patients upon detection at a Primary Health Care (PHC).The tool developed in this study will aid clinicians to design timely intervention strategies aimed at improving the clinical prognosis of patients.","",""
8,"M. Choudhury, Min Kyung Lee, Haiyi Zhu, David A. Shamma","Introduction to this special issue on unifying human computer interaction and artificial intelligence",2020,"","","","",46,"2022-07-13 09:31:46","","10.1080/07370024.2020.1744146","","",,,,,8,4.00,2,4,2,"McCarthy (1998) defined Artificial Intelligence (AI) as both “the science and engineering of in- telligent machines, especially computer programs” and the “computational part of the ability to achi...","",""
13,"A. Lin, Márton Kolossváry, M. Motwani, I. Išgum, P. Maurovich-Horvat, P. Slomka, D. Dey","Artificial Intelligence in Cardiovascular Imaging for Risk Stratification in Coronary Artery Disease.",2021,"","","","",47,"2022-07-13 09:31:46","","10.1148/ryct.2021200512","","",,,,,13,13.00,2,7,1,"Artificial intelligence (AI) describes the use of computational techniques to perform tasks that normally require human cognition. Machine learning and deep learning are subfields of AI that are increasingly being applied to cardiovascular imaging for risk stratification. Deep learning algorithms can accurately quantify prognostic biomarkers from image data. Additionally, conventional or AI-based imaging parameters can be combined with clinical data using machine learning models for individualized risk prediction. The aim of this review is to provide a comprehensive review of state-of-the-art AI applications across various noninvasive imaging modalities (coronary artery calcium scoring CT, coronary CT angiography, and nuclear myocardial perfusion imaging) for the quantification of cardiovascular risk in coronary artery disease. © RSNA, 2021.","",""
18,"R. Slart, M. Williams, L. Juarez-Orozco, C. Rischpler, M. Dweck, A. Glaudemans, A. Gimelli, P. Georgoulias, O. Gheysens, O. Gaemperli, G. Habib, R. Hustinx, B. Cosyns, H. Verberne, F. Hyafil, P. Erba, M. Lubberink, P. Slomka, I. Išgum, D. Visvikis, Márton Kolossváry, A. Saraste","Position paper of the EACVI and EANM on artificial intelligence applications in multimodality cardiovascular imaging using SPECT/CT, PET/CT, and cardiac CT",2021,"","","","",48,"2022-07-13 09:31:46","","10.1007/s00259-021-05341-z","","",,,,,18,18.00,2,22,1,"","",""
22,"M. J. Mrowinski, P. Fronczak, A. Fronczak, M. Ausloos, O. Nedić","Artificial intelligence in peer review: How can evolutionary computation support journal editors?",2017,"","","","",49,"2022-07-13 09:31:46","","10.1371/journal.pone.0184711","","",,,,,22,4.40,4,5,5,"With the volume of manuscripts submitted for publication growing every year, the deficiencies of peer review (e.g. long review times) are becoming more apparent. Editorial strategies, sets of guidelines designed to speed up the process and reduce editors’ workloads, are treated as trade secrets by publishing houses and are not shared publicly. To improve the effectiveness of their strategies, editors in small publishing groups are faced with undertaking an iterative trial-and-error approach. We show that Cartesian Genetic Programming, a nature-inspired evolutionary algorithm, can dramatically improve editorial strategies. The artificially evolved strategy reduced the duration of the peer review process by 30%, without increasing the pool of reviewers (in comparison to a typical human-developed strategy). Evolutionary computation has typically been used in technological processes or biological ecosystems. Our results demonstrate that genetic programs can improve real-world social systems that are usually much harder to understand and control than physical systems.","",""
2,"B. Nair, Yakov Diskin, V. Asari","Multi-modal low cost mobile indoor surveillance system on the Robust Artificial Intelligence-based Defense Electro Robot (RAIDER)",2012,"","","","",50,"2022-07-13 09:31:46","","10.1117/12.930353","","",,,,,2,0.20,1,3,10,"We present an autonomous system capable of performing security check routines. The surveillance machine, the Clearpath Husky robotic platform, is equipped with three IP cameras with different orientations for the surveillance tasks of face recognition, human activity recognition, autonomous navigation and 3D reconstruction of its environment. Combining the computer vision algorithms onto a robotic machine has given birth to the Robust Artificial Intelligencebased Defense Electro-Robot (RAIDER). The end purpose of the RAIDER is to conduct a patrolling routine on a single floor of a building several times a day. As the RAIDER travels down the corridors off-line algorithms use two of the RAIDER's side mounted cameras to perform a 3D reconstruction from monocular vision technique that updates a 3D model to the most current state of the indoor environment. Using frames from the front mounted camera, positioned at the human eye level, the system performs face recognition with real time training of unknown subjects. Human activity recognition algorithm will also be implemented in which each detected person is assigned to a set of action classes picked to classify ordinary and harmful student activities in a hallway setting.The system is designed to detect changes and irregularities within an environment as well as familiarize with regular faces and actions to distinguish potentially dangerous behavior. In this paper, we present the various algorithms and their modifications which when implemented on the RAIDER serves the purpose of indoor surveillance.","",""
10,"A. S. Ahmad, A. D. W. Sumari","Cognitive artificial intelligence: Brain-inspired intelligent computation in artificial intelligence",2017,"","","","",51,"2022-07-13 09:31:46","","10.1109/SAI.2017.8252094","","",,,,,10,2.00,5,2,5,"Computation occurred within human brain is very much awesome and is not possible to be emulated 100% exactly in Artificial Intelligence (AI) method-based machines. What scientists did and have been done so far up to now are to try to model it as close as to what exactly occurs within the brain. Human brain has an awesome mechanism in performing computation with the end result is new knowledge and human uses the knowledge to actuate his organs. In this paper we will show a new approach for emulating the computation occured within human brain to obtain new knowledge based on the inputs sensed by the system's sensory system taken from the environment. When this process is carried out recursively, the system's knowledge becomes newer and newer, and it is called as knowledge growing. This approach is designed for an agent that has ability to think and act rationally like human. Our cognitive modelling approach is resulted in a model of human information processing and a technique to obtain the most maximum performance should be taken by the cognitive agent. This method is called as A3S (Arwin-Adang-Aciek-Sembiring), the agent is called as Knowledge-Growing System (KGS) and this brain-inspired method opens a new perspective in AI that we call as Cognitive Artificial Intelligence (CAI).","",""
81,"S. Ullman","Using neuroscience to develop artificial intelligence",2019,"","","","",52,"2022-07-13 09:31:46","","10.1126/science.aau6595","","",,,,,81,27.00,81,1,3,"Combining deep learning with brain-like innate structures may guide network models toward human-like learning When the mathematician Alan Turing posed the question “Can machines think?” in the first line of his seminal 1950 paper that ushered in the quest for artificial intelligence (AI) (1), the only known systems carrying out complex computations were biological nervous systems. It is not surprising, therefore, that scientists in the nascent field of AI turned to brain circuits as a source for guidance. One path that was taken since the early attempts to perform intelligent computation by brain-like circuits (2), and which led recently to remarkable successes, can be described as a highly reductionist approach to model cortical circuitry. In its basic current form, known as a “deep network” (or deep net) architecture, this brain-inspired model is built from successive layers of neuron-like elements, connected by adjustable weights, called “synapses” after their biological counterparts (3). The application of deep nets and related methods to AI systems has been transformative. They proved superior to previously known methods in central areas of AI research, including computer vision, speech recognition and production, and playing complex games. Practical applications are already in broad use, in areas such as computer vision and speech and text translation, and large-scale efforts are under way in many other areas. Here, I discuss how additional aspects of brain circuitry could supply cues for guiding network models toward broader aspects of cognition and general AI.","",""
0,"E. Nikitos, T. Triantafillou, K. Dimitropoulos, V. Kallergi, P. Psathas, I. Erlich, A. Ben-Meir, N. Bergelson","P-271 Challenges with comparing different commercially available Artificial Intelligence (AI) systems on the same data set of time-lapse selected euploid blastocysts",2022,"","","","",53,"2022-07-13 09:31:46","","10.1093/humrep/deac107.260","","",,,,,0,0.00,0,8,1,"      To identify challenges in choosing a robust AI following comparative validation with data already pre-selected with established embryos selection tools: blastulation, morphology, time-lapse, PGTA.        Challenges included: bias; assessment against outcomes AI models were not trained on; performance metrics prioritisation; statistical methodology; continuous data cutoffs for binary clinical decision making.        AI is commercially available to be incorporated into routine practice to support embryo selection decision-making. Different clinical practices and demographics are used to train AI models, potentially impacting the prediction efficacy of the same model when used in different clinics. Fertility professionals require robust methods of validation to responsibly implement AI-based tools. Unbiased and robust frameworks for comparing AI systems in the same dataset are needed. Validating AI in a dataset of time-lapse selected euploid blastocysts using all the current methods of embryo selection currently available is the toughest assessment possible and has not previously been performed.        This study uses a retrospectively timelapse dataset collected from 2018-2021 at a single private fertility clinic. The dataset included 915 blastocysts which underwent PGTA (913 results: 381 euploids, 528 aneuploids, 4 mosaics) and 46 euploids transferred with known bhcg and ongoing clinical outcome (of which 40 resulted to live birth).  Following a prospective, comparative, observational, cohort study design, blastocysts were blindly scored using the CHLOE(FAIRTILITY) and another commercially available AI system, referred to as ‘AI-2’.        Patients aged 24-47years (average 35.4). Blastocysts selected for biopsy and transfer based on morphology and KIDScore(Vitrolife). Both AI systems were tested in the data set blindly, without any training. Correlation Regression analysis assessed correlation with KIDSCORE and relative to each AI system. Efficacy of prediction (using metrics AUC, Accuracy, Sensitivity, Specificity and Informedness) of outcomes (ploidy, biochemical and clinical pregnancy) were assessed for both AI models (CHLOEvsAI-2) by two independent statisticians to establish significance.        Regression analysis demonstrated no correlation between KIDSCORE and AI-2(r2=0.3%,p=0.5) or between CHLOE(FAIRTILITY) and AI-2(r2=0.03%,p=0.9). CHLOE(FAIRTILITY) correlated with KIDSCORE(r2=29%,p<0.001).  AI-2 was not predictive of ploidy (Euploids vs Aneuploids+mosaic: AUC=0.5,p=0.6). CHLOE(Fairtility) was predictive of ploidy(AUC=0.66, p<0.001).  Neither AI-2 or CHLOE(Fairtility) predicted which embryo the human embryologist prioritised for transfer (AI-2 vs CHLOE:accuracy:0.31vs0.49, p<0.00001). Neither AI-2 nor CHLOE(Fairtility) predicted which embryo the human embryologist prioritised for transfer (AI-2 vs CHLOE:accuracy: 0.31vs0.49, p<0.00001). There was no difference detected in efficacy of prediction of biochemical (accuracy:0.52vs0.67,NS) and ongoing clinical pregnancy (accuracy:0.53 vs 0.78,NS) by AI-2 or CHLOE. This is partly due to the low number of euploid transfers assessed (n = 46), and partly due to the fact that neither of these algorithms are trained specifically on predicting outcome of euploid transfers.  CHLOE(Fairtility) was more specific than AI-2 for predicting selection for transfer(0.44/0.80vs0.17/0.93,p<0.05/NS) and ploidy(0.54/0.77vs0.23/0.87,p<0.05/NS), and they were equally as sensitive. CHLOE(Fairtility) was more sensitive, and less specific than AI-2 for predicting biochemical pregnancy(0.36/0.81vs0.86/0.38,p<0.05) and more sensitive but equally as specific for predicting clinical pregnancy(0.33/0.88vs0.83/0.46,NS/p<0.05).  Informedness was positive for both CHLOE(Fairtility) and AI-2 in predicting all outcomes assessed. Informedness was greater for AI-2 for predicting morphology(AI-2vsCHLOE:0.16vs0.31,p<0.05), transfer(0.11vs0.24,p<0.05), ploidy(0.10vs0.31,p<0.05) and equivalent for predicting biochemical (0.23vs0.17,NS) and clinical pregnancy(0.29vs0.22,NS).        In this single clinic study, both algorithms were assessed against outcomes (live birth following transfer of time-lapse cultured euploid blastocysts) for which they were not trained on: AI-2(designed for ploidy prediction) and CHLOE(FAIRTILITY, implantation prediction of non-PGTA embryos) and no clinic data was used for training.        The only way to decide which AI model is more useful is by a direct comparison of two or more models on the same dataset with same outcomes and metrics, as recommended by TRIPOD. To date, this is the first publication comparing multiple commercial AI models on the same dataset.        NA ","",""
10,"A. C. Horta, A. Silva, C. Sargo, V. M. Gonçalves, T. C. Zangirolami, Roberto Campos Giordano","Robust artificial intelligence tool for automatic start-up of the supplementary medium feeding in recombinant E. coli cultivations",2011,"","","","",54,"2022-07-13 09:31:46","","10.1007/s00449-011-0540-0","","",,,,,10,0.91,2,6,11,"","",""
1,"A. Admin, Dr.P Dr.P.Kavitha2, A. Akshaya, P. P.Shalin, R. R.Ramya","A Survey on Cyber Security Meets Artificial Intelligence: AI– Driven Cyber Security",2022,"","","","",55,"2022-07-13 09:31:46","","10.54216/jchci.020202","","",,,,,1,1.00,0,5,1,"The computerized version of human intelligence is Artificial Intelligence(AI). Artificial Intelligence systems combine large sets of data with intelligent and iterative processing algorithms in order to make predictions, based on patterns and features in the data that they analyse. With the booming technologies such as IOT and Cloud Computing, huge amounts of data are generated and collected that require cyber security protection today. There is a growing need for cyber security methods which are both robust and intelligent due to the ever-increasing complexity of cyber crimes. While data can be used to benefit business interests, it poses a number of challenges in terms of security and privacy protection. Artificial Intelligence (AI) based technologies, such as machine learning statistics, big data analysis, deep learning and so on, have been used to deal with cyber security threats. These technologies are used for intrusion detection systems, malicious software detection, and encrypted communications. In the rapidly growing field of AI driven security, scientists from multiple disciplines work together to combat cyber threats. AI models require unique cyber security defence and protection technologies. This survey provides various method, different datasets and methodologies that may be used for the proposed IA enabled cyber security technologies. This study aims to classify the AI-based cyber security solutions gathered and describe how they can help solve problems in the field of cyber security.","",""
0,"Kristin, N. Johnson, Carla, L. Reyes","Exploring the Implications of Artificial Intelligence Exploring the Implications of Artificial Intelligence",2022,"","","","",56,"2022-07-13 09:31:46","","","","",,,,,0,0.00,0,4,1,": Emerging technologies promise to play a transformative role in our society, enabling driverless cars, enhanced accuracy and efficiency in disease mapping, greater and less expensive access to certain consumer services, including consumer financial services. Discussions regarding the role of emerging technologies increasingly center on the development and integration of artificial intelligence technologies or AI-an assemblage of technologies that rely on a variety of computational techniques. This Essay offers a modest primer outlining a general understanding of the contours and contributions of Al, as well as introducing the articulated benefits and limits of these technologies. This Essay examines the increasingly pervasive use of artificial intelligence in society through two key areas of ethical and policy concerns: (i) privacy, surveillance and the appropriate boundaries for machine-human interaction, and (ii) bias and discrimination. As we assess the merits of Al, this Essay embraces the robust and lively debate and raises probing questions initiated by scholars, activists, industry participants, and governments regarding the ethical implications of embracing Al. This Essay encourages adopters of Al to carefully consider the impacts of integrating Al on vulnerable and marginalized groups. To accomplish this goal, this Essay advocates for affected stakeholders to engage in a collaborative, interdisciplinary colloquy examining the consequences of incorporating Al technologies. Finally, this Essay serves as an introduction to a Special Issue dedicated to sharing novel thinking and approaches to address underexplored challenges posed by Al. Addressing a range of issues discussed in the debate regarding the promises and perils of Al, the contributors to this volume offer critical insights, frameworks, and tools for evaluating the issues from the perspectives of diverse stakeholders. This Special Issue seeks to shed light on some of the hidden implications of artificial intelligence on the values, institutions, and structures that form the foundation of a just society.","",""
0,"S. Diakiw, J. Hall, M. VerMilyea, J. Amin, J. Aizpurua, L. Giardini, Y. G. Briones, A. Lim, M. Dakka, T. Nguyen, D. Perugini, M. Perugini","Development of an artificial intelligence model for predicting the likelihood of human embryo euploidy based on blastocyst images from multiple imaging systems during IVF.",2022,"","","","",57,"2022-07-13 09:31:46","","10.1093/humrep/deac131","","",,,,,0,0.00,0,12,1,"STUDY QUESTION Can an artificial intelligence (AI) model predict human embryo ploidy status using static images captured by optical light microscopy?   SUMMARY ANSWER Results demonstrated predictive accuracy for embryo euploidy and showed a significant correlation between AI score and euploidy rate, based on assessment of images of blastocysts at Day 5 after IVF.   WHAT IS KNOWN ALREADY Euploid embryos displaying the normal human chromosomal complement of 46 chromosomes are preferentially selected for transfer over aneuploid embryos (abnormal complement), as they are associated with improved clinical outcomes. Currently, evaluation of embryo genetic status is most commonly performed by preimplantation genetic testing for aneuploidy (PGT-A), which involves embryo biopsy and genetic testing. The potential for embryo damage during biopsy, and the non-uniform nature of aneuploid cells in mosaic embryos, has prompted investigation of additional, non-invasive, whole embryo methods for evaluation of embryo genetic status.   STUDY DESIGN, SIZE, DURATION A total of 15 192 blastocyst-stage embryo images with associated clinical outcomes were provided by 10 different IVF clinics in the USA, India, Spain and Malaysia. The majority of data were retrospective, with two additional prospectively collected blind datasets provided by IVF clinics using the genetics AI model in clinical practice. Of these images, a total of 5050 images of embryos on Day 5 of in vitro culture were used for the development of the AI model. These Day 5 images were provided for 2438 consecutively treated women who had undergone IVF procedures in the USA between 2011 and 2020. The remaining images were used for evaluation of performance in different settings, or otherwise excluded for not matching the inclusion criteria.   PARTICIPANTS/MATERIALS, SETTING, METHODS The genetics AI model was trained using static 2-dimensional optical light microscope images of Day 5 blastocysts with linked genetic metadata obtained from PGT-A. The endpoint was ploidy status (euploid or aneuploid) based on PGT-A results. Predictive accuracy was determined by evaluating sensitivity (correct prediction of euploid), specificity (correct prediction of aneuploid) and overall accuracy. The Matthew correlation coefficient and receiver-operating characteristic curves and precision-recall curves (including AUC values), were also determined. Performance was also evaluated using correlation analyses and simulated cohort studies to evaluate ranking ability for euploid enrichment.   MAIN RESULTS AND THE ROLE OF CHANCE Overall accuracy for the prediction of euploidy on a blind test dataset was 65.3%, with a sensitivity of 74.6%. When the blind test dataset was cleansed of poor quality and mislabeled images, overall accuracy increased to 77.4%. This performance may be relevant to clinical situations where confounding factors, such as variability in PGT-A testing, have been accounted for. There was a significant positive correlation between AI score and the proportion of euploid embryos, with very high scoring embryos (9.0-10.0) twice as likely to be euploid than the lowest-scoring embryos (0.0-2.4). When using the genetics AI model to rank embryos in a cohort, the probability of the top-ranked embryo being euploid was 82.4%, which was 26.4% more effective than using random ranking, and ∼13-19% more effective than using the Gardner score. The probability increased to 97.0% when considering the likelihood of one of the top two ranked embryos being euploid, and the probability of both top two ranked embryos being euploid was 66.4%. Additional analyses showed that the AI model generalized well to different patient demographics and could also be used for the evaluation of Day 6 embryos and for images taken using multiple time-lapse systems. Results suggested that the AI model could potentially be used to differentiate mosaic embryos based on the level of mosaicism.   LIMITATIONS, REASONS FOR CAUTION While the current investigation was performed using both retrospectively and prospectively collected data, it will be important to continue to evaluate real-world use of the genetics AI model. The endpoint described was euploidy based on the clinical outcome of PGT-A results only, so predictive accuracy for genetic status in utero or at birth was not evaluated. Rebiopsy studies of embryos using a range of PGT-A methods indicated a degree of variability in PGT-A results, which must be considered when interpreting the performance of the AI model.   WIDER IMPLICATIONS OF THE FINDINGS These findings collectively support the use of this genetics AI model for the evaluation of embryo ploidy status in a clinical setting. Results can be used to aid in prioritizing and enriching for embryos that are likely to be euploid for multiple clinical purposes, including selection for transfer in the absence of alternative genetic testing methods, selection for cryopreservation for future use or selection for further confirmatory PGT-A testing, as required.   STUDY FUNDING/COMPETING INTEREST(S) Life Whisperer Diagnostics is a wholly owned subsidiary of the parent company, Presagen Holdings Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation, and Startup Fund (RCSF). 'In kind' support and embryology expertise to guide algorithm development were provided by Ovation Fertility. 'In kind' support in terms of computational resources provided through the Amazon Web Services (AWS) Activate Program. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. S.M.D., M.A.D. and T.V.N. are employees or former employees of Life Whisperer. S.M.D, J.M.M.H, M.A.D, T.V.N., D.P. and M.P. are listed as inventors of patents relating to this work, and also have stock options in the parent company Presagen. M.V. sits on the advisory board for the global distributor of the technology described in this study and also received support for attending meetings.   TRIAL REGISTRATION NUMBER N/A.","",""
9,"David A. Joyner, D. Bedwell, Chris Graham, Warren Lemmon, Óscar Martínez, Ashok K. Goel","Using Human Computation to Acquire Novel Methods for Addressing Visual Analogy Problems on Intelligence Tests",2015,"","","","",58,"2022-07-13 09:31:46","","","","",,,,,9,1.29,2,6,7,"The Raven's Progressive Matrices (RPM) test is a commonly used test of intelligence. The literature suggests a variety of problem-solving methods for addressing RPM problems. For a graduate-level artificial intelligence class in Fall 2014, we asked students to develop intelligent agents that could address 123 RPM-inspired problems, essentially crowdsourcing RPM problem solving. The students in the class submitted 224 agents that used a wide variety of problem-solving methods. In this paper, we first report on the aggregate results of those 224 agents on the 123 problems, then focus specifically on four of the most creative, novel, and effective agents in the class. We find that the four agents, using four very different problem-solving methods, were all able to achieve significant success. This suggests the RPM test may be amenable to a wider range of problem-solving methods than previously reported. It also suggests that human computation might be an effective strategy for collecting a wide variety of methods for creative tasks.","",""
0,"A. Campbell, R. Smith, B. Petersen, L. Moore, A. Khan, A. Barrie","O-125 Application of artificial intelligence using big data to devise and train a machine learning model on over 63,000 human embryos to automate time-lapse embryo annotation",2022,"","","","",59,"2022-07-13 09:31:46","","10.1093/humrep/deac105.025","","",,,,,0,0.00,0,6,1,"      Can a machine learning (ML) model, developed using modern neural network architecture produce comparable annotation data; utilisable for algorithmic outcome prediction, to manual time-lapse annotations?        The model automatically annotated unseen embryos with comparable results to manual methods, generating morphokinetic data to enable comparably predictive outputs from an embryo selection algorithm.        The application of artificial intelligence across healthcare industries, including fertility, is increasing. Several ML models are available that seek to generate or analyse embryo images and morphokinetic data, and to determine embryo viability potential. Along with photographic images, the use of time-lapse in IVF laboratories has amassed numeric data, resulting predominantly from annotated manual assessment of images over time. Embryo annotation practice is variable in quality, can be subjective and is time-consuming; commonly taking several minutes per embryo. The development of rapid, accurate automatic annotation would represent a significant time-saving as well as an increase in reproducibility and accuracy.        Multicentre quality assured annotation data from 63,383 time-lapse monitored embryos (EmbryoScope®), comprising over 400 million individual images, were used to train a ML model to automatically generate morphokinetic annotations. Data was derived from 8 UK clinics within a cohesive group between 2012-2021. Accuracy was assessed using 900 unseen embryos (with live birth outcome) by comparing the output of an established in-house, prospectively validated embryo selection model when the input was either ML-automated, or manual annotations.        Multi-focal plane images were processed on the Azure cloud (Microsoft) and resampled to 300x300 pixels. A Laplacian-based focal stacking algorithm merged frames into a single image. The model consisted of an EfficientNetB4 Convolutional Neural Network classifier to extract features and classify the stage of embryo images. A Temporal Convolutional Network  interpreted a time-series of image features; producing annotations from pronuclear fading through to blastocyst. Soft localisation loss function used QA data to integrate annotation subjectivities.        The ML model rapidly and automatically generated annotations. Efficacy and comparability of the ML model to automate reliable, utilisable annotations was demonstrated by comparison with manual annotation data and the ML model’s ability to auto-generate annotations which could be used to predict live birth by providing annotation data to an established, validated in house embryo selection model. Live birth-predictive capability was measured, and benchmarked against manual annotation, using the area under the receiver operating characteristic curve (AUC).  When tested on time-lapse images, collected from pronuclear fading to full blastulation, representing 900 previously unseen, transferred blastocysts where live birth outcomes were blinded, the in-house developed auto-annotation ML model resulted in an AUC of 0.686 compared with 0.661 for manual annotations, for live birth prediction.  Auto annotation using the developed model took only milliseconds to complete per embryo. The developed auto-annotation model, built and tested on large data, is considered suitable for productionisation with the aim of being validated and integrated into an application to support IVF laboratory practice.        Whilst this model was trained to recognise key morphokinetic events, there are other morphokinetic variables that may be useful in the prediction of live birth and further improve embryo selection, or deselection, ability. Akin to manual interpretation, some embryos may fail to be annotated or need second opinion.        There is increasing evidence supporting the application of ML to utilise big data from time-lapse imaging and fertility care generally. Whilst promising benefits to IVF clinics and patients, responsible use of data is required alongside large high-quality datasets, and rigorous validation, to ensure safe and robust applications.        N/A ","",""
37,"T. Babina, A. Fedyk, A. He, James Hodson","Artificial Intelligence, Firm Growth, and Industry Concentration",2020,"","","","",60,"2022-07-13 09:31:46","","10.2139/ssrn.3651052","","",,,,,37,18.50,9,4,2,"Which firms invest in artificial intelligence (AI) technologies, and how do these investments affect individual firms and industries? We provide a comprehensive picture of the use of AI technologies and their impact among US firms over the last decade, using a unique combination of job postings and individual-level employment profiles. We introduce a novel measure of investments in AI technologies based on human capital and document that larger firms with higher sales, markups, and cash holdings tend to invest more in AI. Firms that invest in AI experience faster growth in both sales and employment, which translates into analogous growth at the industry level. The positive effects are concentrated among the ex ante largest firms, leading to a positive correlation between AI investments and an increase in industry concentration. However, the increase in concentration is not accompanied by either increased markups or increased productivity. Instead, firms tend to expand into new product and geographic markets. Our results are robust to instrumenting firm-level AI investments with foreign industry-level AI investments and with local variation in industry-level AI investments, and to controlling for investments in general information technology and robotics. We also document consistent patterns across measures of AI using firms' demand for AI talent (job postings) and actual AI talent (resumes). Overall, our findings support the view that new technologies, such as AI, increase the scale of the most productive firms and contribute to the rise of superstar firms.","",""
7,"Baptiste Caramiaux, Marco Donnarumma","Artificial Intelligence in Music and Performance: A Subjective Art-Research Inquiry",2020,"","","","",61,"2022-07-13 09:31:46","","10.1007/978-3-030-72116-9_4","","",,,,,7,3.50,4,2,2,"","",""
103,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dentistry: Chances and Challenges",2020,"","","","",62,"2022-07-13 09:31:46","","10.1177/0022034520915714","","",,,,,103,51.50,34,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
26,"W. Lawless, R. Mittu, D. Sofge, Laura M. Hiatt","Artificial intelligence, Autonomy, and Human-Machine Teams - Interdependence, Context, and Explainable AI",2019,"","","","",63,"2022-07-13 09:31:46","","10.1609/aimag.v40i3.2866","","",,,,,26,8.67,7,4,3,"     Because in military situations, as well as for self-driving cars, information must be processed faster than humans can achieve, determination of context computationally, also known as situational assessment, is increasingly important. In this article, we introduce the topic of context, and we discuss what is known about the heretofore intractable research problem on the effects of interdependence, present in the best of human teams; we close by proposing that interdependence must be mastered mathematically to operate human-machine teams efficiently, to advance theory, and to make the machine actions directed by AI explainable to team members and society. The special topic articles in this issue and a subsequent issue of AI Magazine review ongoing mature research and operational programs that address context for human-machine teams.      ","",""
0,"","Neural Network Training Using Genetic Algorithms Series In Machine Perception And Artificial Intelligence",2021,"","","","",64,"2022-07-13 09:31:46","","","","",,,,,0,0.00,0,0,1,"Knowledge-Based Intelligent Information and Engineering Systems 2Nature-inspired Methods in Chemometrics: Genetic Algorithms and Artificial Neural NetworksParallel Implementations of Backpropagation Neural Networks on TransputersEvolutionary Algorithms and Neural NetworksTraining Neural Networks Using Hybrids with Genetic AlgorithmsNeural Network Training Using Genetic AlgorithmsGene Expression ProgrammingTraining a Neural Network with a Genetic AlgorithmMethods and Applications of Artificial IntelligenceClassification and Learning Using Genetic AlgorithmsPractical Computer Vision Applications Using Deep Learning with CNNsIntelligent Hybrid SystemsNeurogenetic LearningAdvances in Neural Networks ISNN 2007Hybrid Intelligent SystemsEncyclopedia of Computer Science and TechnologyMachine LearningUsing a Genetic Algorithm in Training an Artificial Neural Network to Implement the XOR FunctionGenetic and Evolutionary Computation — GECCO 2004Handbook of Fuzzy ComputationNeural Network Data Analysis Using SimulnetTMArtificial Neural Nets and Genetic AlgorithmsApplied Soft Computing Technologies: The Challenge of ComplexityGenetic Algorithm for Artificial Neural Network Training for the Purpose of Automated Part RecognitionNEURAL NETWORKS, FUZZY LOGIC AND GENETIC ALGORITHMAutomatic Generation of Neural Network Architecture Using Evolutionary ComputationPGANETThe Sixth International Symposium on Neural Networks (ISNN 2009)Evolutionary Machine Learning TechniquesModeling Decisions for Artificial IntelligenceEmpirical Studies on the Utility of Genetic Algorithms for Training and Designing of Neural NetworksTraining Neural Networks Using Genetic AlgorithmsTraining feedforward neural networks using genetic algorithmsMetaheuristic Procedures for Training Neural NetworksArtificial Neural Nets and Genetic AlgorithmsNature-Inspired Computing: Concepts, Methodologies, Tools, and ApplicationsApplications of Evolutionary ComputingArtificial Intelligence and CreativityArtificial Neural Nets and Genetic AlgorithmsDeep Learning Using Genetic Algorithms Creativity is one of the least understood aspects of intelligence and is often seen as `intuitive' and not susceptible to rational enquiry. Recently, however, there has been a resurgence of interest in the area, principally in artificial intelligence and cognitive science, but also in psychology, philosophy, computer science, logic, mathematics, sociology, and architecture and design. This volume brings this work together and provides an overview of this rapidly developing field. It addresses a range of issues. Can computers be creative? Can they help us to understand human creativity? How can artificial intelligence (AI) enhance human creativity? How, in particular, can it contribute to the `sciences of the artificial', such as design? Does the new wave of AI (connectionism, geneticism and artificial life) offer more promise in these areas than classical, symbol-handling AI? What would the implications be for AI and cognitive science if computers could not be creative? These issues are explored in five interrelated parts, each of which is introducted and explained by a leading figure in the field. Prologue (Margaret Boden) Part I: Foundational Issues (Terry Dartnall) Part II: Creativity and Cognition (Graeme S. Halford and Robert Levinson) Part III: Creativity and Connectionism (Chris Thornton) Part IV: Creativity and Design (John Gero) Part V: Human Creativity Enhancement (Ernest Edmonds) Epilogue (Douglas Hofstadter) For researchers in AI, cognitive science, computer science, philosophy, psychology, mathematics, logic, sociology, and architecture and design; and anyone interested in the rapidly growing field of artificial intelligence and creativity.From the contents: Neural networks – theory and applications: NNs (= neural networks) classifier on continuous data domains– quantum associative memory – a new class of neuron-like discrete filters to image processing – modular NNs for improving generalisation properties – presynaptic inhibition modelling for image processing application – NN recognition system for a curvature primal sketch – NN based nonlinear temporalspatial noise rejection system – relaxation rate for improving Hopfield network – Oja's NN and influence of the learning gain on its dynamics Genetic algorithms – theory and applications: transposition: a biological-inspired mechanism to use with GAs (= genetic algorithms) – GA for decision tree induction – optimising decision classifications using GAs – scheduling tasks with intertask communication onto multiprocessors by GAs – design of robust networks with GA – effect of degenerate coding on GAs – multiple traffic signal control using a GA – evolving musical harmonisation – niched-penalty approach for constraint handling in GAs – GA with dynamic population size – GA with dynamic niche clustering for multimodal function optimisation Soft computing and uncertainty: self-adaptation of evolutionary constructed decision trees by information spreading – evolutionary programming of near optimal NNsArtificial neural networks and genetic algorithms both are areas of research","",""
0,"Lucas Mendes Lima, Victor Calebe Cavalcante, Mariana Guimarães de Sousa, Cláudio Afonso Fleury, D. Oliveira, Eduardo Noronha de Andrade Freitas","Artificial Intelligence in Support of Welfare Monitoring of Dairy Cattle: A Systematic Literature Review",2021,"","","","",65,"2022-07-13 09:31:46","","10.1109/CSCI54926.2021.00324","","",,,,,0,0.00,0,6,1,"Context: Although agribusiness corresponded to more than 20% of Brazil’s Gross Domestic Product (GDP), most livestock is under manual control and manual monitoring. Additionally, alternative technologies are either uncomfortable and stressful, or expensive. Now, despite the great scientific advances in the area, there is still a pressing need for an automated robust, inexpensive and (sub)optimal technology to monitor animal behavior in a cost-effective, contact-less and stress-free fashion. Overall, this niche can leverage the benefits of Deep Learning schemes.Objective: This review aims to provide a systematic overview of most current projects in the area of comfort monitoring dairy cattle, as well as their corresponding image recognition-based techniques and technologies.Methods: First, a systematic review planning was carried out, and objectives, research questions, search strings, among others, were defined. Subsequently,a broad survey was conducted to extract, analyze and compile the data, to generate a easy-to-read visual source of information (tables and graphics).Results: Information was extracted from the reviewed papers. Among this data collected from the papers are techniques utilized, target behaviors, cow bodyparts identified in visual computational, besides their paper source font, the publication date, and localization. For example, the papers present are mostly recent. China has had a larger number of relevant papers in the area. The back was the body region most analyzed by the papers and the behaviors most analyzed were body condition score, lameness, cow’s body position and feeding/drinking behavior. Among the methods used is RCNN Inception V3 with the best accuracy for cow’s back region.Conclusion: The aim of this work is to present some of the papers that are being carried out in the area of dairy cow behavior monitoring, using techniques of Artifical Intelligence. It is expected that the information collected and presented in the present systematic review paper contribute to the future researches and projects of the area and the application of new techniques.","",""
45,"Tom Kamiel Magda Vercauteren, M. Unberath, N. Padoy, N. Navab","CAI4CAI: The Rise of Contextual Artificial Intelligence in Computer-Assisted Interventions",2019,"","","","",66,"2022-07-13 09:31:46","","10.1109/JPROC.2019.2946993","","",,,,,45,15.00,11,4,3,"Data-driven computational approaches have evolved to enable extraction of information from medical images with reliability, accuracy, and speed, which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theaters are extremely complex and typically rely on poorly integrated intraoperative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer-assisted interventions, we highlight the crucial need to take the context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer-assisted intervention (CAI4CAI) arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors, and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human–AI actor team; and how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision-making ultimately producing more precise and reliable interventions.","",""
38,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases.",2020,"","","","",67,"2022-07-13 09:31:46","","10.1038/s41746-020-0229-3","","",,,,,38,19.00,6,6,2,"","",""
0,"","Proceedings of 2021 1st International Conference on Computer Science and Artificial Intelligence (ICCSAI)",2021,"","","","",68,"2022-07-13 09:31:46","","10.1109/iccsai53272.2021.9609785","","",,,,,0,0.00,0,0,1,"The proceedings contain 81 papers. The topics discussed include: adaptive central pattern generators to control human/robot interactions;modelling personality prediction from user's posting on social media;web based application for ordering food raw materials;comparison of Gaussian hidden Markov model and convolutional neural network in sign language recognition system;intelligent computational model for early heart disease prediction using logistic regression and stochastic gradient descent (a preliminary study);an efficient system to collect data for ai training on multi-category object counting task;a comparison of artificial intelligence-based methods in traffic prediction;impact of computer vision with deep learning approach in medical imaging diagnosis;and development of portable temperature and air quality detector for preventing COVID-19.","",""
31,"I. Habli, T. Lawton, Zoe Porter","Artificial intelligence in health care: accountability and safety",2020,"","","","",69,"2022-07-13 09:31:46","","10.2471/BLT.19.237487","","",,,,,31,15.50,10,3,2,"Abstract The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed.","",""
2,"P. Kumar","Special issue on Artificial Intelligence in Engineering Education",2021,"","","","",70,"2022-07-13 09:31:46","","10.1002/cae.22398","","",,,,,2,2.00,2,1,1,"Artificial intelligence (AI) can be defined as the intelligence exhibited by machines and computers in accomplishing desired tasks in a similar way to how normal human beings think and act. Hence AI is also termed machine intelligence. For a computational system to be artificially intelligent, the system should possess the ability to understand the surrounding environment, make proper assumptions, and based on the circumstances make judicious decisions that maximize the possibilities of accomplishing goals most of the time. These AI‐enabled devices are also called Intelligent Agents. These intelligent agents use some mapping functions also termed cognitive functions, which take these environmental parameters and contextual information as inputs along with the goal to be accomplished and manipulate the right means to accomplish the targeted goal. AI can also be considered inter‐ disciplinary as it involves several other disciplines such as Machine Learning, Computer Vision, Cognitive Science, Neural Networks, Data Mining, Natural Language Processing (NLP), robotics, and mathematics. All these disciplines are related, and thereby intelligent agents are trained to understand and adapt to the surrounding environment according to the context. The use of AI spans across several applications such as Human–Computer Interaction (HCI) based smart agent development, devising smart surveillance solutions using computer vision, creating robust and stable decision making systems that can understand, evaluate, manipulate, analyze, and predict several novel patterns by processing large volumes of application data, development of multilingual systems that uses NLP to understand the language features used across the context and aid decision making and so on. Also, since its inception as an academic discipline in the 1950s, AI has grown leaps and bounds as a discipline, and its applications have stretched across several domains such as Retail and Business solutions, Manufacturing and Logistics, Automobiles, Business Analytics and Market predictions, Healthcare, Security Systems, and Education. One of the key emerging areas where extensive efforts are spent towards developing smart applications and agents is the educational domain. Gone are the days where the educational system was completely driven by humans, and the growth of AI‐enabled intelligent agents has set the tone by replacing most human work with that of smart agents. Educational systems use AI‐based agents to study the behavior of students and suggest suitable courses for them. Smart agents are nowadays deployed in classrooms for complete classroom monitoring that includes tracking attendance, monitoring classroom activities, student and staff behavior monitoring, and so on. Similarly, smart agents are deployed to scan through the contents available online and suggest suitable content to students according to the course and also according to the different levels of understanding of student fraternity. Also, computer vision‐based smart agents are deployed to study the state of mind of students when they undergo different courses and provide insightful information about their likeness towards a subject or course. This agent‐based information serves as useful information in deciding the teaching methodology and also framing of course contents. Also, smart systems play a vital role in analyzing student results and providing insightful information about student performance. Thus, it is imperative that AI has become an indispensable force to reckon with in the future forward across the educational domain. However, the major drawback in these artificially intelligent systems is that they are not always accurate with decision making and at times predict otherwise. Also, training the AI‐based agent to understand the contextual paradigm and surrounding environment is a challenge. This special issue on “Artificial Intelligence In Education” is focused on drawing original studies related to the development and refinement of smart agents that can be applied across the educational domain.","",""
1,"Pranav Gupta, A. Woolley","Articulating the Role of Artificial Intelligence in Collective Intelligence: A Transactive Systems Framework",2021,"","","","",71,"2022-07-13 09:31:46","","10.1177/1071181321651354c","","",,,,,1,1.00,1,2,1,"Human society faces increasingly complex problems that require coordinated collective action. Artificial intelligence (AI) holds the potential to bring together the knowledge and associated action needed to find solutions at scale. In order to unleash the potential of human and AI systems, we need to understand the core functions of collective intelligence. To this end, we describe a socio-cognitive architecture that conceptualizes how boundedly rational individuals coordinate their cognitive resources and diverse goals to accomplish joint action. Our transactive systems framework articulates the inter-member processes underlying the emergence of collective memory, attention, and reasoning, which are fundamental to intelligence in any system. Much like the cognitive architectures that have guided the development of artificial intelligence, our transactive systems framework holds the potential to be formalized in computational terms to deepen our understanding of collective intelligence and pinpoint roles that AI can play in enhancing it.","",""
18,"Ahmed Gowida, Salaheldin Elkatatny, Saad F. K. Al-Afnan, A. Abdulraheem","New Computational Artificial Intelligence Models for Generating Synthetic Formation Bulk Density Logs While Drilling",2020,"","","","",72,"2022-07-13 09:31:46","","10.3390/su12020686","","",,,,,18,9.00,5,4,2,"Synthetic well log generation using artificial intelligence tools is a robust solution for situations in which logging data are not available or are partially lost. Formation bulk density (RHOB) logging data greatly assist in identifying downhole formations. These data are measured in the field while drilling by using a density log tool in the form of either a logging while drilling (LWD) technique or (more often) by wireline logging after the formations are drilled. This is due to operational limitations during the drilling process. Therefore, the objective of this study was to develop a predictive tool for estimating RHOB while drilling using an adaptive network-based fuzzy interference system (ANFIS), functional network (FN), and support vector machine (SVM). The proposed model uses the mechanical drilling constraints as feeding input parameters, and the conventional RHOB log data as an output parameter. These mechanical drilling parameters are usually measured while drilling, and their responses vary with different formations. A dataset of 2400 actual datapoints, obtained from a horizontal well in the Middle East, were used to build the proposed models. The obtained dataset was divided into a 70/30 ratio for model training and testing, respectively. The optimized ANFIS-based model outperformed the FN- and SVM-based models with a correlation coefficient (R) of 0.93, and average absolute percentage error (AAPE) of 0.81% between the predicted and measured RHOB values. These results demonstrate the reliability of the developed ANFIS model for predicting RHOB while drilling, based on the mechanical drilling parameters. Subsequently, the ANFIS-based model was validated using unseen data from another well within the same field. The validation process yielded an AAPE of 0.97% between the predicted and actual RHOB values, which confirmed the robustness of the developed model as an effective predictive tool for RHOB.","",""
0,"Arkadiusz Czuba","Artificial Intelligence-Based Cognitive Radar Architecture",2021,"","","","",73,"2022-07-13 09:31:46","","10.1109/CSCI54926.2021.00092","","",,,,,0,0.00,0,1,1,"This paper considers a new cognitive radar architecture based on artificial intelligence cognitive architectures (CAs). The CAs are the computational embodiments of the theory about modeling the human mind used in artificial intelligence. They found applications in, e.g., robotics and autonomous vehicles. However, the research related to the topic of integrating artificial intelligence CAs with radar systems is very limited. In this paper, a novel cognitive radar architecture has been proposed. Cognitive abilities such as learning, perception, attention, and decision-making mechanisms were conformed to radar capabilities. This new concept introduces promising visual attention mechanisms for target prioritization, declarative memories for a broader understanding of the radar environment, and reinforcement learning to improve the signal-to-noise ratio. All of the components mentioned above, combined together, look promising to improve an overall radar performance.","",""
0,"R. Hariharan, P. He, C. Hickman, J. Chambost, C. Jacques, M. Hentschke, B. Cunegatto, C. Dutra, A. Drakeley, Q. Zhan, R. Miller, G. Verheyen, M. Rosselot, S. Loubersac, K. Kelley","P–165 Using Artificial Intelligence to Classify Embryo Shape: An International Perspective",2021,"","","","",74,"2022-07-13 09:31:46","","10.1093/humrep/deab130.164","","",,,,,0,0.00,0,15,1,"      Is a pre-trained machine learning algorithm able to accurately detect cellular arrangement in 4-cell embryos from a different continent?        Artificial Intelligence (AI) analysis of 4-cell embryo classification is transferable across clinics globally with 79% accuracy.        Previous studies observing four-cell human embryo configurations have demonstrated that non-tetrahedral embryos (embryos in which cells make contact with fewer than 3 other cells) are associated with compromised blastulation and implantation potential. Previous research by this study group has indicated the efficacy of AI models in classification of tetrahedral and non-tetrahedral embryos with 87% accuracy, with a database comprising 2 clinics both from the same country (Brazil). This study aims to evaluate the transferability and robustness of this model on blind test data from a different country (France).        The study was a retrospective cohort analysis in which 909 4-cell embryo images (“tetrahedral”, n = 749; “non-tetrahedral”, n = 160) were collected from 3 clinics (2 Brazilian, 1 French). All embryos were captured at the central focal plane using Embryoscope™ time-lapse incubators. The training data consisted solely of embryo images captured in Brazil (586 tetrahedral; 87 non-tetrahedral) and the test data consisted exclusively of embryo images captured in France (163 tetrahedral; 72 non-tetrahedral).        The embryo images were labelled as either “tetrahedral” or “non-tetrahedral” at their respective clinics. Annotations were then validated by three operators. A ResNet–50 neural network model pretrained on ImageNet was fine-tuned on the training dataset to predict the correct annotation for each image. We used the cross entropy loss function and the RMSprop optimiser (lr = 1e–5). Simple data augmentations (flips and rotations) were used during the training process to help counteract class imbalances.        Our model was capable of classifying embryos in the blind French test set with 79% accuracy when trained with the Brazilian data. The model had sensitivity of 91% and 51% for tetrahedral and non-tetrahedral embryos respectively; precision was 81% and 73%; F1 score was 86% and 60%; and AUC was 0.61 and 0.64. This represents a 10% decrease in accuracy compared to when the model both trained and tested on different data from the same clinics.        Although strict inclusion and exclusion criteria were used, inter-operator variability may affect the pre-processing stage of the algorithm. Moreover, as only one focal plane was used, ambiguous cases were interpoloated and further annotated. Analysing embryos at multiple focal planes may prove crucial in improving the accuracy of the model.  Wider implications of the findings: Though the use of machine learning models in the analysis of embryo imagery has grown in recent years, there has been concern over their robustness and transferability. While previous results have demonstrated the utility of locally-trained models, our results highlight the potential for models to be implemented across different clinics.        Not applicable ","",""
0,"M. Yakar, D. Etiz","Artificial intelligence in radiation oncology",2021,"","","","",75,"2022-07-13 09:31:46","","10.35711/AIMI.V2.I2.13","","",,,,,0,0.00,0,2,1,"Artificial intelligence (AI) is a computer science that tries to mimic human-like intelligence in machines that use computer software and algorithms to perform specific tasks without direct human input. Machine learning (ML) is a subunit of AI that uses data-driven algorithms that learn to imitate human behavior based on a previous example or experience. Deep learning is an ML technique that uses deep neural networks to create a model. The growth and sharing of data, increasing computing power, and developments in AI have initiated a transformation in healthcare. Advances in radiation oncology have produced a significant amount of data that must be integrated with computed tomography imaging, dosimetry, and imaging performed before each fraction. Of the many algorithms used in radiation oncology, has advantages and limitations with different computational power requirements. The aim of this review is to summarize the radiotherapy (RT) process in workflow order by identifying specific areas in which quality and efficiency can be improved by ML. The RT stage is divided into seven stages: patient evaluation, simulation, contouring, planning, quality control, treatment application, and patient follow-up. A systematic evaluation of the applicability, limitations, and advantages of AI algorithms has been done for each stage.","",""
28,"Sathian Dananjayan, G. M. Raj","Artificial Intelligence during a pandemic: The COVID‐19 example",2020,"","","","",76,"2022-07-13 09:31:46","","10.1002/hpm.2987","","",,,,,28,14.00,14,2,2,"Artificial intelligence (AI) is transforming our lifestyle intending to mimic human intelligence by a computer/machine in solving various issues. Initially, AI was designed to overcome simpler problems like winning a chess game, language recognition, image retrieval, among others. With the technological advancements, AI is getting increasingly sophisticated at doing what humans do, but more efficiently, rapidly, and at a lower cost in solving complex problems. AI in healthcare provides an upper hand undoubtedly over traditional analytics and clinical decision-making techniques. Machine learning (ML) algorithms, a subset of AI, can detect patterns from huge complex datasets to become more precise and accurate as they interact with training data, allowing humans to gain unprecedented insights into early detection of diseases, drug discovery, diagnostics, healthcare processes, treatment variability, and patient outcomes. But how effective are the AI algorithms during a disease outbreak or for that matter a pandemic? After 2000, the pandemics are testing the AI's ability to handle extreme events. The two major factors affecting AI algorithms include the availability of historical and real-time data and high computational power. The different roles played by AI during pandemics are early warning and alerts, prediction and detection of outbreak of diseases, real-time disease monitoring worldwide, analysis and visualisation of spreading trends, prediction of infection rate and infection trend, rapid decision-making to identify the effective treatments, study and analysis of the pathogens, and drug discovery. All these are executed at a greater speed with AI. WHO and CDC (United States) are receiving data of several diseases and situations occurring across the world. With modern computer architecture and internet, all these data can be accessed in real-time by different institutes to develop an autonomous or collaborative AI model to handle various tasks. In addition to the official data, AI can gather information from news outlets, forums, healthcare reports, travel data, social media posts, and others in multiple languages across the world by using natural language processing (NLP) techniques and flag their priority. Several terabytes of data which includes patients' case history, geographical events, and social media posts about a new pneumonia are processed at a rapid rate with high-performance computing to predict the possible outbreak of a pandemic. Most importantly unsupervised ML can identify its own pattern from the noise (historical and real-time data) rather than the training it on a preselected dataset, thus giving a wider possibility and new behaviour. An AI model trained to predict a particular disease can be retrained on the new data of a new or different disease. Some noticeable examples of AI that are used to battle the COVID-19 pandemic and others are as follows:","",""
27,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases",2020,"","","","",77,"2022-07-13 09:31:46","","10.1038/s41746-020-0229-3","","",,,,,27,13.50,5,6,2,"","",""
0,"C. Aggarwal","An Introduction to Artificial Intelligence",2021,"","","","",78,"2022-07-13 09:31:46","","10.1007/978-3-030-72357-6_1","","",,,,,0,0.00,0,1,1,"","",""
1,"H. Yamakawa","Peacekeeping Conditions for an Artificial Intelligence Society",2019,"","","","",79,"2022-07-13 09:31:46","","10.3390/BDCC3020034","","",,,,,1,0.33,1,1,3,"In a human society with emergent technology, the destructive actions of some pose a danger to the survival of all of humankind, increasing the need to maintain peace by overcoming universal conflicts. However, human society has not yet achieved complete global peacekeeping. Fortunately, a new possibility for peacekeeping among human societies using the appropriate interventions of an advanced system will be available in the near future. To achieve this goal, an artificial intelligence (AI) system must operate continuously and stably (condition 1) and have an intervention method for maintaining peace among human societies based on a common value (condition 2). However, as a premise, it is necessary to have a minimum common value upon which all of human society can agree (condition 3). In this study, an AI system to achieve condition 1 was investigated. This system was designed as a group of distributed intelligent agents (IAs) to ensure robust and rapid operation. Even if common goals are shared among all IAs, each autonomous IA acts on each local value to adapt quickly to each environment that it faces. Thus, conflicts between IAs are inevitable, and this situation sometimes interferes with the achievement of commonly shared goals. Even so, they can maintain peace within their own societies if all the dispersed IAs think that all other IAs aim for socially acceptable goals. However, communication channel problems, comprehension problems, and computational complexity problems are barriers to realization. This problem can be overcome by introducing an appropriate goal-management system in the case of computer-based IAs. Then, an IA society could achieve its goals peacefully, efficiently, and consistently. Therefore, condition 1 will be achievable. In contrast, humans are restricted by their biological nature and tend to interact with others similar to themselves, so the eradication of conflicts is more difficult.","",""
11,"K. Mudgal, Neelanjan Das","The ethical adoption of artificial intelligence in radiology",2019,"","","","",80,"2022-07-13 09:31:46","","10.1259/bjro.20190020","","",,,,,11,3.67,6,2,3,"Artificial intelligence (AI) is rapidly transforming healthcare—with radiology at the pioneering forefront. To be trustfully adopted, AI needs to be lawful, ethical and robust. This article covers the different aspects of a safe and sustainable deployment of AI in radiology during: training, integration and regulation. For training, data must be appropriately valued, and deals with AI companies must be centralized. Companies must clearly define anonymization and consent, and patients must be well-informed about their data usage. Data fed into algorithms must be made AI-ready by refining, purification, digitization and centralization. Finally, data must represent various demographics. AI needs to be safely integrated with radiologists-in-the-loop: guiding forming concepts of AI solutions and supervising training and feedback. To be well-regulated, AI systems must be approved by a health authority and agreements must be made upon liability for errors, roles of supervised and unsupervised AI and fair workforce distribution (between AI and radiologists), with a renewal of policy at regular intervals. Any errors made must have a root-cause analysis, with outcomes fedback to companies to close the loop—thus enabling a dynamic best prediction system. In the distant future, AI may act autonomously with little human supervision. Ethical training and integration can ensure a ""transparent"" technology that will allow insight: helping us reflect on our current understanding of imaging interpretation and fill knowledge gaps, eventually moulding radiological practice. This article proposes recommendations for ethical practise that can guide a nationalized framework to build a sustainable and transparent system.","",""
9,"Cecilia S Lee, Aaron Y. Lee","How Artificial Intelligence Can Transform Randomized Controlled Trials",2020,"","","","",81,"2022-07-13 09:31:46","","10.1167/tvst.9.2.9","","",,,,,9,4.50,5,2,2,"With the advent of deep learning (DL), the application of artificial intelligence (AI) and big data in healthcare has started transforming the way we approach medicine including clinical trials.1,2 The randomized controlled trial (RCT) has been traditionally accepted as the most robust method of assessing the risks and benefits of any intervention.3 However, the undertaking of an RCT is not always feasible due to the rarity of the disease, or time and costs that would impinge on the healthcare system. AI is an academic discipline founded in 1956.4 Machine learning (ML) is a subfield of AI that can learn complex relationships or patterns from data and make accurate decisions.5 DL or deep artificial networks are a relatively new subfield of ML that takes advantage of powerful computational processing capacity provided by Graphic Processing Units and exponentially increasing datasets from medical records, images, multi-omics, and other “Big Data”.6 By feeding an enormous amount of data in training, a DL algorithm allows the model to alter its internal parameters between each neuronal layer to increase its performance. Applications of AI, DL in particular, have been successful in ophthalmic imaging research,7–10 and the application of AI in RCTs may become reality in the near future. Common pitfalls of unsuccessful RCTs include poor patient selection, inadequate randomization with residual confounders, insufficient sample size, and poor selection of end points.11 With well-curated large datasets that incorporate clinical and multimodal imaging, AI models can be trained to select the potential study participants without relying on costly manual review to predict the natural history of each study participants with advanced statistical methods, and to assess study end points in a data-driven method. Given these advantages, the application of AI has potentials for more efficient execution and greater statistical power than what would be expected from traditional RCTs. First, ML models can drastically improve the patient selection process, thus lowering the burden of individual screening and need for large sample sizes. Recruiting the patients who meet precise selection criteria is crucial to avoid potential confounders or misclassifications. ML can combine multimodal data, such as imaging, laboratory, and other complex -omics data, to screen and select patients who match complex inclusion criteria, which can improve the recruitment efficiency. This is one of the areas in which the American Academy of Ophthalmology’s Intelligent Research in Sight (IRIS) data will be utilized for RCT recruitment (personal communication, Flora Lum, MD). In addition to the efficient selection process, having a sufficient sample size to enable detection of statistically significant differences between groups is critical. Many RCTs require a large sample size because the effect of the treatment in question is small.12 AI has the potential in selecting “the ideal”patients for RCTs, who are “fast progressors” of the disease based on the AI’s predictive algorithm. Thus, the expected effect size will be large and required sample size will be small resulting in a much shorter duration of RCTs. Selecting the “fast progressors” alone will limit the generalizability of the trial results; however, it may expedite the development of novel therapies, in particular for rare diseases. Second, AI-generated end points have the potential to minimize measurement errors and analyze the data without human-imposed biases. Furthermore, algorithms may enable more sensitive quantification of key study end points than how they are traditionally measured. For example, central macular","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",82,"2022-07-13 09:31:46","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
0,"Yaxin Peng, S. Du, T. Zeng","Preface: Special Issue on Optimization Models and Algorithms in Artificial Intelligence",2019,"","","","",83,"2022-07-13 09:31:46","","10.1007/s40305-019-00278-5","","",,,,,0,0.00,0,3,3,"","",""
30,"Xinqin Liao, Wei‐ming Song, X. Zhang, Chaoqun Yan, Tianliang Li, Hongliang Ren, Cunzhi Liu, Yongtian Wang, Yuanjin Zheng","A bioinspired analogous nerve towards artificial intelligence",2020,"","","","",84,"2022-07-13 09:31:46","","10.1038/s41467-019-14214-x","","",,,,,30,15.00,3,9,2,"","",""
0,"Benjamin J. Wortman, J. Z. Wang","HICEM: A High-Coverage Emotion Model for Artificial Emotional Intelligence",2022,"","","","",85,"2022-07-13 09:31:46","","10.48550/arXiv.2206.07593","","",,,,,0,0.00,0,2,1,"As social robots and other intelligent machines enter the home, artificial emotional intelligence (AEI) is taking center stage to address users’ desire for deeper, more meaningful human-machine interaction. To accomplish such efficacious interaction, the next-generation AEI need comprehensive human emotion models for training. Unlike theory of emotion, which has been the historical focus in psychology, emotion models are a descriptive tools. In practice, the strongest models need robust coverage, which means defining the smallest core set of emotions from which all others can be derived. To achieve the desired coverage, we turn to word embeddings from natural language processing. Using unsupervised clustering techniques, our experiments show that with as few as 15 discrete emotion categories, we can provide maximum coverage across six major languages–Arabic, Chinese, English, French, Spanish, and Russian. In support of our findings, we also examine annotations from two large-scale emotion recognition datasets to assess the validity of existing emotion models compared to human perception at scale. Because robust, comprehensive emotion models are foundational for developing real-world affective computing applications, this work has broad implications in social robotics, human-machine interaction, mental healthcare, and computational psychology.","",""
0,"Fahad S. Mohammed, Hisham Qadri, S. Mohammed","COVID-19 in the era of artificial intelligence: a black swan event?",2021,"","","","",86,"2022-07-13 09:31:46","","10.21037/jmai-21-23","","",,,,,0,0.00,0,3,1,"of COVID-19 (3). The large amount of social computational data generated by the pandemic may lead to breakthroughs in AI that can greatly alter human behavior. Newer COVID-19 variants and behavioral changes are causing resurgence of the pandemic. AI can use social computational data to devise novel non-pharmaceutical interventions to prevent newer outbreaks. “How we feel” a web and mobile application that longitudinally tracks COVID-19 symptoms, behavior and testing, can predict likely COVID-19 positive individuals and outbreaks (4). Genomic, structural data and outcomes can be used to make COVID-19 simulations, predict mutations, outbreaks and guide therapy leading to drug discovery, drug repurposing and precision medicine (5). Multiple applications for predicting severity using imaging and lab data in real time have been developed and have been externally validated (6). These applications have played a pivotal role in the management of the pandemic.","",""
12,"Awishkar Ghimire, Surendrabikram Thapa, A. Jha, Surabhi Adhikari, Ankit Kumar","Accelerating Business Growth with Big Data and Artificial Intelligence",2020,"","","","",87,"2022-07-13 09:31:46","","10.1109/I-SMAC49090.2020.9243318","","",,,,,12,6.00,2,5,2,"Artificial Intelligence (AI) is considered to be the fourth industrial revolution. Artificial Intelligence with the help of big data has transformed all industries around the world Artificial intelligence refers to the simulation of human or animal intelligence in computational systems so that they are programmed to think like Intelligent beings and mimic the actions of intelligent entities. Computational systems which have programmed intelligence can solve different real-world problems far more accurately and efficiently than computational systems that are deterministic and hardcoded. Since many problems in business and business analytics cannot be solved by deterministic systems, AI plays a major role in tackling problems in the business world Machine learning and deep learning which are subsets of the field of AI is widely used to solve and optimize many problems in business such as marketing, credit card fraud detection, algorithmic trading, customer service, portfolio management, product recommendation according to the needs of customers, insurance underwriting. AI and big data have revolutionized the business world and this paper discusses some AI and big data technologies that are currently being used to accelerate business growth.","",""
15,"M. Sipper, R. S. Olson, J. Moore","Evolutionary computation: the next major transition of artificial intelligence?",2017,"","","","",88,"2022-07-13 09:31:46","","10.1186/s13040-017-0147-3","","",,,,,15,3.00,5,3,5,"","",""
21,"Adrien Bécue, Isabel Praça, J. Gama","Artificial intelligence, cyber-threats and Industry 4.0: challenges and opportunities",2021,"","","","",89,"2022-07-13 09:31:46","","10.1007/S10462-020-09942-2","","",,,,,21,21.00,7,3,1,"","",""
2,"B. Grzyb, G. Vigliocco","Beyond robotic speech: mutual benefits to cognitive psychology and artificial intelligence from the study of multimodal communication",2020,"","","","",90,"2022-07-13 09:31:46","","10.31234/osf.io/h5dxy","","",,,,,2,1.00,1,2,2,"Language has predominately been studied as a unimodal phenomenon - as speech or text without much consideration of its physical and social context – this is true both in cognitive psychology/psycholinguistics as well as in artificial intelligence. However, in everyday life, language is most often used in face-to-face communication and in addition to structured speech it comprises a dynamic system of multiplex components such as gestures, eye gaze, mouth movements and prosodic modulation. Recently, cognitive scientists have started to realise the potential importance of multimodality for the understanding of human communication and its neural underpinnings; while AI scientists have begun to address how to integrate multimodality in order to improve communication between human and artificial embodied agent. We review here the existing literature on multimodal language learning and processing in humans and the literature on perception of artificial agents, their comprehension and production of multimodal cues and we discuss their main limitations. We conclude by arguing that by joining forces AI scientists can improve the effectiveness of human-machine interaction and increase the human-likeness and acceptance of embodied agents in society. In turn, computational models that generate language in artificial embodied agents constitute a unique research tool to investigate the underlying mechanisms that govern language processing and learning in humans.","",""
1,"Assil Benchaaben, Felipe Guimaraes, Emmanuel Prestat, A. Kassambara, Mounia Filahi, Caroline Laugé, T. Sbarrato, J. Fieschi","Abstract 870: Immunoscore®workflow enhanced by artificial intelligence",2020,"","","","",91,"2022-07-13 09:31:46","","10.1158/1538-7445.am2020-870","","",,,,,1,0.50,0,8,2,"Artificial Intelligence (AI) along with Machine Learning (ML) techniques has long promised to accelerate Digital Pathology (DP) based cancer diagnosis. Despite the consensus regarding the value of AI, the lack of visibility of how ML algorithms work, prevents their wider adoption for human in vitro diagnostic (IVD) in a highly regulated environment. A common ground becomes necessary in order to fully benefit from ML capabilities. HalioDx Immunoscore® was the first immune scoring test validated for IVD use leveraging advanced image analysis. In brief, for each tumor sample, 2 slides are stained using an automated immunohistochemistry instrument: one with CD3 and one with CD8 ready-to-use monoclonal antibodies (HalioDx) followed by detection with DAB and counterstaining. Digital images of stained slides are obtained using a whole slide scanner and analyzed by a software program (Immunoscore® Analyzer, HalioDx)1. Current workflow relies only on Computer Vision (CV) techniques for image analysis leading to the calculation of the Immunoscore®. We have used ML to improve HalioDx Immunoscore® software program, streamline the workflow, decrease hands-on and computation times. In summary, to design the new workflow, each DP steps were considered as independent applications. CV remains applied to the cell detection. A Convolutional Neural Network, along with a UNET architecture, were used to recognize Regions of Interest (ROI) and image-related artifacts during the analysis. Intermediary validation steps by a trained operator were maintained in order to review CV and AI steps and guarantee a complete equivalence versus the standardized original DP protocol. The Intersection over the Union of two regions (IoU) was used as performance and equivalency metric. Compared to Ground Truth, the ML algorithm improves the accuracy of the ROI detection versus the CV based algorithm, resulting in a dramatic decrease of the ROI computing time (from 3h to 5min) as well as in a reduced need for manual correction. We demonstrated that ML applied to the Immunoscore® DP workflow for ROI detection results in reduced time-to result and overall improved robustness of the analysis. The equivalency study showed the importance of a well-curated dataset to maximize model9s accuracy and performance. Finally, the verification and validation phase demonstrated the ML based workflow readiness for regulatory approval. 1Hermitte F. J Immunother Cancer. 2016 Sep 20;4:57. doi: 10.1186/s40425-016-0161-x. Citation Format: Assil Benchaaben, Felipe Machado Guimaraes, Emmanuel Prestat, Alboukadel Kassambara, Mounia Filahi, Caroline Lauge, Thomas Sbarrato, Jacques Fieschi. Immunoscore® workflow enhanced by artificial intelligence [abstract]. In: Proceedings of the Annual Meeting of the American Association for Cancer Research 2020; 2020 Apr 27-28 and Jun 22-24. Philadelphia (PA): AACR; Cancer Res 2020;80(16 Suppl):Abstract nr 870.","",""
82,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello","Artificial Intelligence, Algorithmic Pricing and Collusion",2018,"","","","",92,"2022-07-13 09:31:46","","10.2139/ssrn.3304991","","",,,,,82,20.50,21,4,4,"Pricing algorithms are increasingly replacing human decision making in real marketplaces. To inform the competition policy debate on possible consequences, we run experiments with pricing algorithms powered by Artificial Intelligence in controlled environments (computer simulations).<br><br>In particular, we study the interaction among a number of Q-learning algorithms in the context of a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. We show that the algorithms consistently learn to charge supra-competitive prices, without communicating with each other. The high prices are sustained by classical collusive strategies with a finite punishment phase followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.","",""
152,"P. Khosravi, Ehsan Kazemi, Q. Zhan, J. Malmsten, M. Toschi, Pantelis Zisimopoulos, Alexandros Sigaras, S. Lavery, L. Cooper, C. Hickman, M. Meseguer, Z. Rosenwaks, O. Elemento, N. Zaninovic, I. Hajirasouliha","Deep learning enables robust assessment and selection of human blastocysts after in vitro fertilization",2019,"","","","",93,"2022-07-13 09:31:46","","10.1038/s41746-019-0096-y","","",,,,,152,50.67,15,15,3,"","",""
0,"Shivali Agarwal, Jayachandu Bandlamudi, Atri Mandal, Anupama Ray, G. Sridhara","Automated Assignment of Helpdesk Email Tickets: An Artificial Intelligence Life-Cycle Case Study",2020,"","","","",94,"2022-07-13 09:31:46","","","","",,,,,0,0.00,0,5,2,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 Fall 2020 45 The landscape of modern information technology service delivery is changing, with increased focus on automation and optimization. Most information technology vendors today have service platforms aimed toward end-to-end automation for carrying out mundane, repetitive labor-intensive tasks and even for tasks requiring human cognizance. One such task is ticket assignment and dispatch, where the service requests submitted by the end-users to the vendor in the form of tickets are reviewed by a centralized dispatch team and assigned to the appropriate service team and resolver group. The dispatch of a ticket to the correct group of practitioners is a critical step in the speedy resolution of a ticket. Incorrect dispatch decisions can significantly increase the total turnaround time for ticket resolution, as observed in a study of an actual production system (agarwal, Sindhgatta, and Sengupta 2012). When such delays occur, it causes customer dissatisfaction as well as monetary penalties for the vendor due to service-level-agreement breaches. Several factors make the dispatcher’s job challenging, namely the need for in-depth knowledge of the roles and responsibilities of various groups, the heterogeneous and informal nature of email text, and the high attrition rate in service delivery teams (Mandal et al. 2018). Given the fact that inefficiencies in dispatch have serious business consequences, there has been a lot of interest in automating the assignment process. a number of different approaches have been proposed for automating ticket dispatch (agarwal, Sindhgatta, and Sengupta 2012; Shao et al. 2008a, 2008b; Parvin, Bose, and Van Oyen 2009).  In this article, we present an endto-end automated helpdesk email ticket assignment system driven by high accuracy, coverage, business continuity, scalability, and optimal usage of computational resources. The primary objective of the system is to determine the problem mentioned in an incoming email ticket and then automatically dispatch it to an appropriate resolver group with high accuracy. While meeting this objective, it should also meet the objective of being able to operate at desired accuracy levels in the face of changing business needs by automatically adapting to the changes. The proposed system uses a system of classifiers with separate strategies for handling frequent and sparse resolver groups augmented with a semiautomatic rule engine and retraining strategies to ensure that it is accurate, robust, and adaptive to changing business needs. Our system has been deployed in the production of six major service providers in diverse service domains and currently assigns 100,000 emails per month, on an average, with an accuracy close to ninety percent and covering at least ninety percent of email tickets. This translates to achieving human-level accuracy and results in a net savings of more than 50,000 man-hours of effort per annum. To date, our deployed system has already served more than two million tickets in production. Automated Assignment of Helpdesk Email Tickets: An Artificial Intelligence Life-Cycle Case Study","",""
73,"Valentina Bellemo, Gilbert Lim, T. Rim, G. Tan, C. Cheung, S. Sadda, M. He, A. Tufail, M. Lee, W. Hsu, D. Ting","Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application",2019,"","","","",95,"2022-07-13 09:31:46","","10.1007/s11892-019-1189-3","","",,,,,73,24.33,7,11,3,"","",""
85,"Ashish Ghosh, Debasrita Chakraborty, Anwesha Law","Artificial intelligence in Internet of things",2018,"","","","",96,"2022-07-13 09:31:46","","10.1049/TRIT.2018.1008","","",,,,,85,21.25,28,3,4,"Functioning of the Internet is persistently transforming from the Internet of computers (IoC) to the ‘Internet of things (IoT)’. Furthermore, massively interconnected systems, also known as cyber-physical systems (CPSs), are emerging from the assimilation of many facets like infrastructure, embedded devices, smart objects, humans, and physical environments. What the authors are heading to is a huge ‘Internet of Everything in a Smart Cyber Physical Earth’. IoT and CPS conjugated with ‘data science’ may emerge as the next ‘smart revolution’. The concern that arises then is to handle the huge data generated with the much weaker existing computation power. The research in data science and artificial intelligence (AI) has been striving to give an answer to this problem. Thus, IoT with AI can become a huge breakthrough. This is not just about saving money, smart things, reducing human effort, or any trending hype. This is much more than that – easing human life. There are, however, some serious issues like the security concerns and ethical issues which will go on plaguing IoT. The big picture is not how fascinating IoT with AI seems, but how the common people perceive it – a boon, a burden, or a threat.","",""
2,"Lorenzo Barberis Canonico, Nathan J. Mcneese, Chris Duncan","Machine Learning as Grounded Theory: Human-Centered Interfaces for Social Network Research through Artificial Intelligence",2018,"","","","",97,"2022-07-13 09:31:46","","10.1177/1541931218621287","","",,,,,2,0.50,1,3,4,"Internet technologies have created unprecedented opportunities for people to come together and through their collective effort generate large amounts of data about human behavior. With the increased popularity of grounded theory, many researchers have sought to use ever-increasingly large datasets to analyze and draw patterns about social dynamics. However, the data is simply too big to enable a single human to derive effective models for many complex social phenomena. Computational methods offer a unique opportunity to analyze a wide spectrum of sociological events by leveraging the power of artificial intelligence. Within the human factors community, machine learning has emerged as the dominant AI-approach to deal with big data. However, along with its many benefits, machine learning has introduced a unique challenge: interpretability. The models of macro-social behavior generated by AI are so complex that rarely can they translated into human understanding. We propose a new method to conduct grounded theory research by leveraging the power of machine learning to analyze complex social phenomena through social network analysis while retaining interpretability as a core feature.","",""
0,"S. Fiore","Interdisciplinary Models and Frameworks for the Study of Artificial Social Intelligence",2021,"","","","",98,"2022-07-13 09:31:46","","10.1177/1071181321651354","","",,,,,0,0.00,0,1,1,"This symposium provides a complementary set of papers exploring frameworks and models for developing artificial social intelligence (ASI) for teams. ASI consists of components of social cognition that support teamwork and more general interpersonal interactions. Although AI is rapidly evolving and fielded in a variety of operational settings, the implementation of such systems is vastly outpacing our ability to understand how to design and develop technologies appropriately. This symposium is meant to help redress this gap. Consisting of scholars representing the cognitive, computational, and organizational sciences, the papers discuss how they integrate theory and methods to inform development of agents capable of complex collaborative processes. Collectively, these papers synthesize perspectives across disciplines in support of an interdisciplinary research approach for ASL The goal is to contribute to research and development in the area of Human- AI- Robot Teaming effectiveness.","",""
0,"Bradley Hayes, M. Cakmak, Stephanie Rosenthal","Introduction to the Special Issue on Artificial Intelligence and Human-Robot Interaction",2018,"","","","",99,"2022-07-13 09:31:46","","10.1145/3279995","","",,,,,0,0.00,0,3,4,"Artificial Intelligence (AI) has had a transformational impact on Human-Robot Interaction (HRI) research over the past decade, enabling work in HRI to develop and investigate robots that can operate autonomously in far more challenging environments and far more complex scenarios than was possible ever before. Beyond laboratory studies, robots that explicitly interact with people as part of their functionality are increasingly being developed, productized, and deployed throughout the world, enabling ecologically valid ethnographic studies of interactions between humans and robots. These advances have been fueled by enabling technologies across many subfields of AI including machine learning, computer vision, task and motion planning, natural language understanding, and dialogue systems. It is not, however, the case that AI research produced polished, ready-off-the-shelf tools that researchers could pick up and effortlessly use to build their envisioned autonomous robot. Rather, the shift has been due to a new, hybrid approach to human-centered robotics research, facilitated by HRI researchers who acquired deep technical skill sets and an influx of AI researchers applying their expertise to HRI problems. More interdiscplinary research teams consisting of formerly AI and HRI researchers also formed, resulting in a vibrant sub-community at the intersection of AI and HRI who came together at the AAAI Fall Symposium on AI for Human-Robot Interaction for the last 4 years. This special issue was encouraged by the continued success and overwhelming popularity of this symposium. Our goal is to exemplify this community’s mature, high-quality, and original work, establishing T-HRI as a premier venue for work at the intersection of AI and HRI. Research at this intersection is particularly challenging due to the very need for interdiscplinary, multi-faceted skill sets. AI-HRI researchers need to both innovate in computational techniques and","",""
43,"Stuart J. Russell, Thomas G. Dietterich, Eric Horvitz, B. Selman, F. Rossi, D. Hassabis, S. Legg, Mustafa Suleyman, D. George, D. Phoenix","Letter to the Editor: Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter",2015,"","","","",100,"2022-07-13 09:31:46","","10.1609/aimag.v36i4.2621","","",,,,,43,6.14,4,10,7,"Artificial intelligence (AI) research has explored a variety of problems and approaches since its inception, but for the last 20 years or so has been focused on the problems surrounding the construction of intelligent agents — systems that perceive and act in some environment. In this context, ""intelligence"" is related to statistical and economic notions of rationality — colloquially, the ability to make good decisions, plans, or inferences. The adoption of probabilistic and decision-theoretic representations and statistical learning methods has led to a large degree of integration and cross-fertilization among AI, machine learning, statistics, control theory, neuroscience, and other fields. The establishment of shared theoretical frameworks, combined with the availability of data and processing power, has yielded remarkable successes in various component tasks such as speech recognition, image classification, autonomous vehicles, machine translation, legged locomotion, and question-answering systems. As capabilities in these areas and others cross the threshold from laboratory research to economically valuable technologies, a virtuous cycle takes hold whereby even small improvements in performance are worth large sums of money, prompting greater investments in research. There is now a broad consensus that AI research is progressing steadily, and that its impact on society is likely to increase. The potential benefits are huge, since everything that civilization has to offer is a product of human intelligence; we cannot predict what we might achieve when this intelligence is magnified by the tools AI may provide, but the eradication of disease and poverty are not unfathomable. Because of the great potential of AI, it is important to research how to reap its benefits while avoiding potential pitfalls. The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the AAAI 2008–09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do. The attached research priorities document [see page X] gives many examples of such research directions that can help maximize the societal benefit of AI. This research is by necessity interdisciplinary, because it involves both society and AI. It ranges from economics, law and philosophy to computer security, formal methods and, of course, various branches of AI itself. In summary, we believe that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today.","",""
16,"Kristin Siu, Matthew J. Guzdial, Mark O. Riedl","Evaluating singleplayer and multiplayer in human computation games",2017,"","","","",101,"2022-07-13 09:31:46","","10.1145/3102071.3102077","","",,,,,16,3.20,5,3,5,"Human computation games (HCGs) can provide novel solutions to intractable computational problems, help enable scientific breakthroughs, and provide datasets for artificial intelligence. However, our knowledge about how to design and deploy HCGs that appeal to players and solve problems effectively is incomplete. We present an investigatory HCG based on Super Mario Bros. We used this game in a human subjects study to investigate how different social conditions---singleplayer and multiplayer---and scoring mechanics---collaborative and competitive---affect players' subjective experiences, accuracy at the task, and the completion rate. In doing so, we demonstrate a novel design approach for HCGs, and discuss the benefits and tradeoffs of these mechanics in HCG design.","",""
141,"Rusul L. Abduljabbar, H. Dia, S. Liyanage, S. Bagloee","Applications of Artificial Intelligence in Transport: An Overview",2019,"","","","",102,"2022-07-13 09:31:46","","10.3390/SU11010189","","",,,,,141,47.00,35,4,3,"The rapid pace of developments in Artificial Intelligence (AI) is providing unprecedented opportunities to enhance the performance of different industries and businesses, including the transport sector. The innovations introduced by AI include highly advanced computational methods that mimic the way the human brain works. The application of AI in the transport field is aimed at overcoming the challenges of an increasing travel demand, CO2 emissions, safety concerns, and environmental degradation. In light of the availability of a huge amount of quantitative and qualitative data and AI in this digital age, addressing these concerns in a more efficient and effective fashion has become more plausible. Examples of AI methods that are finding their way to the transport field include Artificial Neural Networks (ANN), Genetic algorithms (GA), Simulated Annealing (SA), Artificial Immune system (AIS), Ant Colony Optimiser (ACO) and Bee Colony Optimization (BCO) and Fuzzy Logic Model (FLM) The successful application of AI requires a good understanding of the relationships between AI and data on one hand, and transportation system characteristics and variables on the other hand. Moreover, it is promising for transport authorities to determine the way to use these technologies to create a rapid improvement in relieving congestion, making travel time more reliable to their customers and improve the economics and productivity of their vital assets. This paper provides an overview of the AI techniques applied worldwide to address transportation problems mainly in traffic management, traffic safety, public transportation, and urban mobility. The overview concludes by addressing the challenges and limitations of AI applications in transport.","",""
151,"S. Goldenberg, G. Nir, S. Salcudean","A new era: artificial intelligence and machine learning in prostate cancer",2019,"","","","",103,"2022-07-13 09:31:46","","10.1038/s41585-019-0193-3","","",,,,,151,50.33,50,3,3,"","",""
104,"Jeffrey Heer","Agency plus automation: Designing artificial intelligence into interactive systems",2019,"","","","",104,"2022-07-13 09:31:46","","10.1073/pnas.1807184115","","",,,,,104,34.67,104,1,3,"Much contemporary rhetoric regards the prospects and pitfalls of using artificial intelligence techniques to automate an increasing range of tasks, especially those once considered the purview of people alone. These accounts are often wildly optimistic, understating outstanding challenges while turning a blind eye to the human labor that undergirds and sustains ostensibly “automated” services. This long-standing focus on purely automated methods unnecessarily cedes a promising design space: one in which computational assistance augments and enriches, rather than replaces, people’s intellectual work. This tension between human agency and machine automation poses vital challenges for design and engineering. In this work, we consider the design of systems that enable rich, adaptive interaction between people and algorithms. We seek to balance the often-complementary strengths and weaknesses of each, while promoting human control and skillful action. We share case studies of interactive systems we have developed in three arenas—data wrangling, exploratory analysis, and natural language translation—that integrate proactive computational support into interactive systems. To improve outcomes and support learning by both people and machines, we describe the use of shared representations of tasks augmented with predictive models of human capabilities and actions. We conclude with a discussion of future prospects and scientific frontiers for intelligence augmentation research.","",""
126,"Y. Mintz, Ronit Brodie","Introduction to artificial intelligence in medicine",2019,"","","","",105,"2022-07-13 09:31:46","","10.1080/13645706.2019.1575882","","",,,,,126,42.00,63,2,3,"Abstract The term Artificial Intelligence (AI) was coined by John McCarthy in 1956 during a conference held on this subject. However, the possibility of machines being able to simulate human behavior and actually think was raised earlier by Alan Turing who developed the Turing test in order to differentiate humans from machines. Since then, computational power has grown to the point of instant calculations and the ability evaluate new data, according to previously assessed data, in real time. Today, AI is integrated into our daily lives in many forms, such as personal assistants (Siri, Alexa, Google assistant etc.), automated mass transportation, aviation and computer gaming. More recently, AI has also begun to be incorporated into medicine to improve patient care by speeding up processes and achieving greater accuracy, opening the path to providing better healthcare overall. Radiological images, pathology slides, and patients’ electronic medical records (EMR) are being evaluated by machine learning, aiding in the process of diagnosis and treatment of patients and augmenting physicians’ capabilities. Herein we describe the current status of AI in medicine, the way it is used in the different disciplines and future trends.","",""
36,"William R. Frey, D. Patton, M. Gaskell, K. McGregor","Artificial Intelligence and Inclusion: Formerly Gang-Involved Youth as Domain Experts for Analyzing Unstructured Twitter Data",2018,"","","","",106,"2022-07-13 09:31:46","","10.1177/0894439318788314","","",,,,,36,9.00,9,4,4,"Mining social media data for studying the human condition has created new and unique challenges. When analyzing social media data from marginalized communities, algorithms lack the ability to accurately interpret off-line context, which may lead to dangerous assumptions about and implications for marginalized communities. To combat this challenge, we hired formerly gang-involved young people as domain experts for contextualizing social media data in order to create inclusive, community-informed algorithms. Utilizing data from the Gang Intervention and Computer Science Project—a comprehensive analysis of Twitter data from gang-involved youth in Chicago—we describe the process of involving formerly gang-involved young people in developing a new part-of-speech tagger and content classifier for a prototype natural language processing system that detects aggression and loss in Twitter data. We argue that involving young people as domain experts leads to more robust understandings of context, including localized language, culture, and events. These insights could change how data scientists approach the development of corpora and algorithms that affect people in marginalized communities and who to involve in that process. We offer a contextually driven interdisciplinary approach between social work and data science that integrates domain insights into the training of qualitative annotators and the production of algorithms for positive social impact.","",""
41,"B. C. Smith","The Promise of Artificial Intelligence: Reckoning and Judgment",2019,"","","","",107,"2022-07-13 09:31:46","","","","",,,,,41,13.67,41,1,3,"An argument that?despite dramatic advances in the field?artificial intelligence is nowhere near developing systems that are genuinely intelligent.In this provocative book, Brian Cantwell Smith argues that artificial intelligence is nowhere near developing systems that are genuinely intelligent. Second wave AI, machine learning, even visions of third-wave AI: none will lead to human-level intelligence and judgment, which have been honed over millennia. Recent advances in AI may be of epochal significance, but human intelligence is of a different order than even the most powerful calculative ability enabled by new computational capacities. Smith calls this AI ability ?reckoning,? and argues that it does not lead to full human judgment?dispassionate, deliberative thought grounded in ethical commitment and responsible action.Taking judgment as the ultimate goal of intelligence, Smith examines the history of AI from its first-wave origins (?good old-fashioned AI,? or GOFAI) to such celebrated second-wave approaches as machine learning, paying particular attention to recent advances that have led to excitement, anxiety, and debate. He considers each AI technology's underlying assumptions, the conceptions of intelligence targeted at each stage, and the successes achieved so far. Smith unpacks the notion of intelligence itself?what sort humans have, and what sort AI aims at. Smith worries that, impressed by AI's reckoning prowess, we will shift our expectations of human intelligence. What we should do, he argues, is learn to use AI for the reckoning tasks at which it excels while we strengthen our commitment to judgment, ethics, and the world.","",""
44,"H. Joo, T. Simon, M. Cikara, Yaser Sheikh","Towards Social Artificial Intelligence: Nonverbal Social Signal Prediction in a Triadic Interaction",2019,"","","","",108,"2022-07-13 09:31:46","","10.1109/CVPR.2019.01113","","",,,,,44,14.67,11,4,3,"We present a new research task and a dataset to understand human social interactions via computational methods, to ultimately endow machines with the ability to encode and decode a broad channel of social signals humans use. This research direction is essential to make a machine that genuinely communicates with humans, which we call Social Artificial Intelligence. We first formulate the ``social signal prediction'' problem as a way to model the dynamics of social signals exchanged among interacting individuals in a data-driven way. We then present a new 3D motion capture dataset to explore this problem, where the broad spectrum of social signals (3D body, face, and hand motions) are captured in a triadic social interaction scenario. Baseline approaches to predict speaking status, social formation, and body gestures of interacting individuals are presented in the defined social prediction framework.","",""
39,"Han-wei Liu, Ching-Fu Lin, Yu-Jie Chen","Beyond State v Loomis: artificial intelligence, government algorithmization and accountability",2019,"","","","",109,"2022-07-13 09:31:46","","10.1093/IJLIT/EAZ001","","",,,,,39,13.00,13,3,3,"Developments in data analytics, computational power, and machine learning techniques have driven all branches of the government to outsource authority to machines in performing public functions — social welfare, law enforcement, and most importantly, courts. Complex statistical algorithms and artificial intelligence (AI) tools are being used to automate decision-making and are having a significant impact on individuals’ rights and obligations. Controversies have emerged regarding the opaque nature of such schemes, the unintentional bias against and harm to underrepresented populations, and the broader legal, social, and ethical ramifications. State v. Loomis, a recent case in the United States, well demonstrates how unrestrained and unchecked outsourcing of public power to machines may undermine human rights and the rule of law. With a close examination of the case, this Article unpacks the issues of the ‘legal black box’ and the ‘technical black box’ to identify the risks posed by rampant ‘algorithmization’ of government functions to due process, equal protection, and transparency. We further assess some important governance proposals and suggest ways for improving the accountability of AI-facilitated decisions. As AI systems are commonly employed in consequential settings across jurisdictions, technologically-informed governance models are needed to locate optimal institutional designs that strike a balance between the benefits and costs of algorithmization.","",""
196,"W. Samek, K. Müller","Towards Explainable Artificial Intelligence",2019,"","","","",110,"2022-07-13 09:31:46","","10.1007/978-3-030-28954-6_1","","",,,,,196,65.33,98,2,3,"","",""
0,"K. Sfakianoudis, E. Maziotis, S. Grigoriadis, A. Pantou, G. Kokkini, A. Trypidi, I. Angeli, T. Vaxevanoglou, K. Pantos, M. Simopoulou","O-122 Reporting on the value of Artificial Intelligence in predicting the optimal embryo for transfer: A systematic review and meta-analysis",2022,"","","","",111,"2022-07-13 09:31:46","","10.1093/humrep/deac105.022","","",,,,,0,0.00,0,10,1,"      Are Artificial Intelligence (AI) based models effective in robustly predicting in vitro fertilization (IVF) outcome by assessing embryo quality?        The majority of the AI-based models could provide an accurate prediction regarding live birth, clinical pregnancy, clinical pregnancy with fetal heartbeat and embryo ploidy status.        Precision and consistency in embryo quality evaluation are of paramount importance regarding the outcome of an IVF cycle. Numerous embryo grading and evaluation systems, employing morphological and morphokinetical assessment, have been proposed but without reaching a consensus yet. The main limitation of the aforementioned assessment systems is that they depend on human evaluation, which may be subject to subjectivity and interobserver variation. Thus, automated prediction models may be essential to optimize objectivity and reliability of embryo grading. Artificial neural network models may process microscopy images or time-lapse videos as input to predict the embryos’ potential competency.        A systematic review and meta-analysis including 18 published studies. The population consists of preimplantation embryos suitable for embryo transfer in IVF/ICSI cycles following employment of an AI-based prediction model. The outcome measures are prediction of live birth, clinical pregnancy, clinical pregnancy with heartbeat and ploidy status.        A systematic search of the literature was performed in the databases of Pubmed/Medline, Embase, and Cochrane Central Library limited to articles published in English up to August 2021. The initial search yielded a total of 694 studies with 97 of them being duplicates and other 579 being excluded on the grounds of not fulfilling inclusion criteria. Following full-text screening and citation mining a total of 18 studies were identified to be eligible for inclusion.        Four studies reported on prediction of live birth. The sensitivity was 70.6% (95%C.I.: 38.1-90.4%) and specificity was 90.6% (95%C.I.:79.3-96.1%).  The Area Under the Curve (AUC) of the Summary Receiver Operating Characteristics (SROC) curve was 0.905, while the partial AUC (pAUC) was 0.755. Employing the Bayesian approach, the total Observed:Expected ratio (O:E) was 1.12 (95%CI: 0.26–2.37; 95%PI:0.02-6.54). Ten studies reported on prediction of clinical pregnancy. The sensitivity and the specificity were 71% (95%C.I.: 58.1-81.2%) and 62.5% (95%C.I.: 47.4-75.5%) respectively. The AUC was 0.716, while pAUC was 0.693. Moreover, the total O:E ratio was 0.92 (95%CI: 0.61–1.28; 95%PI:0.13-2.43). Eight studies reported on prediction of clinical pregnancy with fetal heartbeat the sensitivity was 75.2% (95%C.I.: 66.8-82%) and the specificity was 55.3% (95%C.I.: 41.2-68.7%). The AUC was 0.722, while the pAUC was 0.774. The O:E ratio was 0.77 (95%CI: 0.54 – 1.05; 95%PI: 0.21-1.62). Four studies reported on the ploidy status of the embryo. The sensitivity and specificity were 59.4% (95%C.I.: 45.0-73.1%) and 79.2% (95%C.I.: 70.1-86.1%) respectively. The AUC was 0.751 and the pAUC was 0.585. The total O:E ratio was 0.86 (95%CI: 0.42 – 1.27; 95%PI: 0.03-1.83).        The limited number of studies fulfilling inclusion criteria, along with the different designs applied when developing AI models which may lead to increased heterogeneity, stand as limitations. Inclusion of women regardless of their age presents as another limitation, as advanced maternal age has been associated with diminished IVF outcomes.        Albeit, our findings support that AI is a highly promising tool in the era of personalized medicine providing precise predictions it does not appear to considerably surpass human prediction capabilities. More studies and more collaborations between the developers are of paramount importance prior to AI becoming the gold standard.        Not applicable ","",""
63,"S. Strohmeier, F. Piazza","Artificial Intelligence Techniques in Human Resource Management - A Conceptual Exploration",2015,"","","","",112,"2022-07-13 09:31:46","","10.1007/978-3-319-17906-3_7","","",,,,,63,9.00,32,2,7,"","",""
21,"Li-Qi Shu, Yi-Kan Sun, L. Tan, Q. Shu, A. Chang","Application of artificial intelligence in pediatrics: past, present and future",2019,"","","","",113,"2022-07-13 09:31:46","","10.1007/s12519-019-00255-1","","",,,,,21,7.00,4,5,3,"","",""
23,"B. Chin-Yee, Ross E. G. Upshur","Three Problems with Big Data and Artificial Intelligence in Medicine",2019,"","","","",114,"2022-07-13 09:31:46","","10.1353/pbm.2019.0012","","",,,,,23,7.67,12,2,3,"ABSTRACT:The rise of big data and artificial intelligence (AI) in health care has engendered considerable excitement, claiming to improve approaches to diagnosis, prognosis, and treatment. Amidst the enthusiasm, the philosophical assumptions that underlie the big data and AI movement in medicine are rarely examined. This essay outlines three philosophical challenges faced by this movement: (1) the epistemological-ontological problem arising from the theory-ladenness of big data and measurement; (2) the epistemological-logical problem resulting from the inherent limitations of algorithms and attendant issues of reliability and interpretability; and (3) the phenomenological problem concerning the irreducibility of human experience to quantitative data. These philosophical issues demonstrate several important challenges for these technologies that must be considered prior to their integration into clinical care. Our article aims to initiate a critical dialogue on the impact of big data and AI in health care in order to allow for more robust evaluation of these technologies and to aid in the development of approaches to clinical care that better serve clinicians and their patients.","",""
16,"Bob L. Sturm, Maria Iglesias, Oded Ben-Tal, M. Miron, Emilia Gómez","Artificial Intelligence and Music: Open Questions of Copyright Law and Engineering Praxis",2019,"","","","",115,"2022-07-13 09:31:46","","10.3390/arts8030115","","",,,,,16,5.33,3,5,3,"The application of artificial intelligence (AI) to music stretches back many decades, and presents numerous unique opportunities for a variety of uses, such as the recommendation of recorded music from massive commercial archives, or the (semi-)automated creation of music. Due to unparalleled access to music data and effective learning algorithms running on high-powered computational hardware, AI is now producing surprising outcomes in a domain fully entrenched in human creativity—not to mention a revenue source around the globe. These developments call for a close inspection of what is occurring, and consideration of how it is changing and can change our relationship with music for better and for worse. This article looks at AI applied to music from two perspectives: copyright law and engineering praxis. It grounds its discussion in the development and use of a specific application of AI in music creation, which raises further and unanticipated questions. Most of the questions collected in this article are open as their answers are not yet clear at this time, but they are nonetheless important to consider as AI technologies develop and are applied more widely to music, not to mention other domains centred on human creativity.","",""
18,"S. Costantini, G. D. Gasperis, R. Olivieri","Digital forensics and investigations meet artificial intelligence",2019,"","","","",116,"2022-07-13 09:31:46","","10.1007/s10472-019-09632-y","","",,,,,18,6.00,6,3,3,"","",""
18,"T. L. Jaynes","Legal personhood for artificial intelligence: citizenship as the exception to the rule",2019,"","","","",117,"2022-07-13 09:31:46","","10.1007/s00146-019-00897-9","","",,,,,18,6.00,18,1,3,"","",""
0,"","Big Data and Artificial Intelligence Analytics in Geosciences : Promises and Potential Last Call for 2019 Annual Meeting Proposals",2019,"","","","",118,"2022-07-13 09:31:46","","","","",,,,,0,0.00,0,0,3,"Big data and machine learning are IT methodologies that are bringing substantial changes in the analysis and interpretation of scientific data. By adding GPU processing resources to the typical equipment of a server host, it is possible to speed up queries performed on large databases and reduce training time for deep learning architectures. A recent pairing of the big data technologies, applied to old and new data, and artificial intelligence techniques has enabled a team of scientists to create an interactive virtual globe that shows a color mosaic of the seabed geology. This interactive model allows us to obtain robust reconstructions and predictions of climate changes and their impacts on the ocean environment. We suggest a possible evolution of such a model by means of the expansion of functionalities and performance improvements. We refer respectively to the implementation of isochronic layers of seabed lithologies and the addition of GPU resources to speed up the learning phase of the support vector machine (SVM) model. These additional features would allow us to establish broader correlations and extract additional information on large-scale geological phenomena. INTRODUCTION The Earth system generates continuous data, and our acquisition capacity has significantly increased over time. The growing availability of acquired geological data and the methods developed in the field of information technology make it possible to identify associations and understand patterns and trends within data (Big Data), solve difficult decision problems (artificial intelligence), and provide acceleration to data processing (GPU computing). Big Data is a term that indicates very large databases (often by order of zettabytes, i.e., billions of terabytes) that can contain huge amounts of heterogeneous, structured and unstructured data (text, numerical values, images, e-mail, GPS data, and data acquired from social networks), which can be extrapolated, analyzed, and correlated with each other. Artificial Intelligence (AI) is a branch of computer science that studies the way in which the combination of hardware and software systems can simulate typical behaviors of the human brain. One of the most important applications consists of a complex algorithm, called machine learning, which is able to learn and make decisions. GPU Parallel Computing (GPGPU) involves the processing of data by the processors present in the graphics card (GPU) and has allowed the computation, in relatively short times, of huge amounts of data with an efficiency of at least two orders of magnitude greater compared to the past. There are several cases in which these technologies have been applied both in the field of potential earthquakes (RouetLeduc et al., 2017), volcanic eruptions (Ham et al., 2012), and to solve the problems of spatial modeling in the field of the assessment of landslide susceptibility (Korup and Stolle, 2014). The following describes a mixed approach (AI and Big Data) in the field of geosciences—analyzing potentials and possible future developments. CASE STUDY: BIG DATA AND AI MAP WORLD’S OCEAN FLOOR An example of an application combining Big Data and machine learning technologies was implemented by a team of Australian scientists who created the first digital map of seabed lithologies (Dutkiewicz et al., 2015) through the analysis and cataloging of ~15,000 samples of sediments found in marine basins. Before such a map, the most recent map of oceanic lithologies was hand drawn ~40 years ago, at the beginning of ocean exploration. Since then, the map has undergone few changes, with at most six types of sediment dominant in the ocean basins. The digital map was created using an AI method consisting of the support vector machine (SVM) model. Through a crossvalidation approach, the classifier was trained by adding new data gradually so as to allow its learning. Learning the parameter values, which optimize the classifier’s performance on withheld data, is an important step in the workflow. In this way, the vast set of point data has been transformed into a continuous digital map with very high accuracy (up to 80%). The new lithological map of the seabed is very important for the interpretation of global phenomena related to the evolution of ocean basins. An example of this is diatoms, siliceous phytoplankton that live in the oceans and that through chlorophyll photosynthesis produce about one-quarter of the oxygen present in the atmosphere, contributing to reduce global terrestrial warming. At their death, these organisms precipitate through the water column, accumulating on the underlying sea floor. Satellite surveys over the years have identified places where diatomaceous activity is more productive; that is, the marine areas in which there are the maximum concentrations of chlorophyll, considering that they should also correspond to the areas of maximum accumulation of these organisms in the sea floor. Surprisingly, the digital map of the seabed has revealed that there is a decoupling between the productivity of diatoms and the corresponding accumulation areas in the sea floor. The possibility of diatom ooze formation is however favored by the low surface temperature (0.9–5.7 °C), by salinity (33.8–34 PSS), and by the high concentration of nutrients, and therefore can represent an important indicator of the oceanographic variables of the surface of the sea (Cunningham and Big Data and Artificial Intelligence Analytics in Geosciences: Promises and Potential Roberto Spina, Geologist and DCompSci, CNG (National Council of Geologists), Rome, Italy, robertospina@geologi.it GSA Today, https://www.doi.org/10.1130/GSATG372GW.1. Copyright 2019, The Geological Society of America. CC-BY-NC. Leventer, 1998). For this reason, the map will help scientists better understand how our oceans have responded and will respond to environmental changes. POTENTIAL AND FUTURE PROSPECTS Big Data and AI are having an impact on every commercial and scientific domain, and their application in the field of geosciences is making a great impact in the analysis and understanding of natural phenomena. The intensive use of CPUs required by these two technologies has stimulated the search for alternative solutions to improve performance by using a mixed CPU-GPU approach. In this way it is possible to obtain rapid results from huge databases and the acceleration of the learning process for neural networks. These techniques are the basis of deep learning, an alternative model of machine learning, which achieves a very high degree of accuracy in recognizing objects and is able to learn features automatically from data without the need to extract them manually. The joint application of Big Data– machine learning, described as a case study, allowed researchers to demonstrate the absence of correlation between diatom productivity and the corresponding diatom oozes: The accumulation of these organisms in the seabed seems rather to be linked to specific variations in sea-surface parameters. This is one of many cases where the integrated analysis of various parameters allows a different interpretation from what could be assumed by their disjoint analysis. A possible evolution is to represent, on a similar map, in addition to the current surface lithologies, those present within the lithostratigraphic succession, making geochronological correlations between chronostratigraphic units. Using surveys carried out in various parts of the world, different layers could be defined, each corresponding to a specific age expressed in millions of years, representing the ocean lithologies existing in that particular geological period. Similarly to the previous case, the transition from a punctual to a continuous display could be obtained, for each layer, by applying the existing SVM model or an even more efficient version using GPU computing. Figure 1 shows a possible switching between current ocean Figure 1. Example of a layered implementation of seabed lithology maps (modified from https://portal.gplates.org). lithologies (https://portal.gplates.org) placed below and those existing respectively 500,000 and one million years ago (above). The oldest layers were made only for demonstration purposes and reproduce an artificial lithology of the seabed. A system of this kind allows the carrying out of various operations that can be summarized as follows: • display/hide isochronous levels obtaining different instantaneous representations of the ocean basins during the geological eras; • using Big Data analytics to pair data sets (oceanographic, stratigraphic, paleontological, and micropaleontological) with one or more isochronous layers to analyze geological phenomena on a global scale (eustatic oscillations, glacial and interglacial periods...) and perform stratigraphic correlations between oceanic crustal sectors to identify evolutionary patterns. The optimization introduced by IT methods lets us perform analyses on large heterogeneous data to discover hidden models and unknown correlations that allow for more solid reconstructions and forecasts on natural phenomena that have had and will have a major impact on the ecosystems of our planet. REFERENCES CITED Cunningham, W.L., and Leventer, A., 1998, Diatom assemblages in surface sediments of the Ross Sea: Relationship to present oceanographic conditions: Antarctic Science, v. 10, p. 134–146, https://doi.org/10.1017/S0954102098000182. Dutkiewicz, A., Müller, R.D., O’Callaghan, S., and Jónasson, H., 2015, Census of seafloor sediments in the world’s ocean: Geology, v. 43, no. 9, p. 795–798, https://doi.org/10.1130/G36883.1. Ham, M.F., Iyengar, I., Hambebo, B.M., Garces, M., Deaton, J., Perttu, A., and Williams, B., 2012, A neurocomputing approach for monitoring Plinian volcanic eruptions using infrasound: Procedia Computer Science, v. 13, p. 7–17, https://doi.org/10.1016/j.procs.2012.09.109. Korup, O., and Stolle, A., 2014, Landslide prediction from","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",119,"2022-07-13 09:31:46","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
40,"Adriana Braga, R. Logan","The Emperor of Strong AI Has No Clothes: Limits to Artificial Intelligence",2017,"","","","",120,"2022-07-13 09:31:46","","10.3390/info8040156","","",,,,,40,8.00,20,2,5,"Making use of the techniques of media ecology we argue that the premise of the technological Singularity based on the notion computers will one day be smarter that their human creators is false. We also analyze the comments of other critics of the Singularity, as well supporters of this notion. The notion of intelligence that advocates of the technological singularity promote does not take into account the full dimension of human intelligence. They treat artificial intelligence as a figure without a ground. Human intelligence as we will show is not based solely on logical operations and computation, but also includes a long list of other characteristics that are unique to humans, which is the ground that supporters of the Singularity ignore. The list includes curiosity, imagination, intuition, emotions, passion, desires, pleasure, aesthetics, joy, purpose, objectives, goals, telos, values, morality, experience, wisdom, judgment, and even humor.","",""
15,"Xiao-Guang Han, W. Tian","Artificial intelligence in orthopedic surgery: current state and future perspective",2019,"","","","",121,"2022-07-13 09:31:46","","10.1097/CM9.0000000000000479","","",,,,,15,5.00,8,2,3,"Artificial intelligence (AI), first proposed by Prof. John AI helps the radiologist to improve the diagnostic accuracy McCarthy in 1956, aims to reproduce human intelligence and prevent errors and observer fatigue. AI algorithms using computers. Machine learning (ML) is a form of AI that uses computational algorithms that learn and improve with experience. The two main forms of ML are supervised and unsupervised. In supervised ML, algorithms are given labeled data, which is used to predict disease outcomes in a new patient. In contrast, unsupervised ML is used to identify patterns without training; the algorithm learns the inherent structure of the data by searching for common characteristics.","",""
2,"J. Johanssen, Xin Wang","Artificial Intuition in Tech Journalism on AI: Imagining the Human Subject",2021,"","","","",122,"2022-07-13 09:31:46","","10.30658/HMC.2.9","","",,,,,2,2.00,1,2,1,"Artificial intuition (AI acting intuitively) is one trend in artificial intelligence. This article analyzes how it is discussed by technology journalism on the internet. The journalistic narratives that were analyzed claim that intuition can make AI more efficient, autonomous, and human. Some commentators also write that intuitive AI could execute tasks better than humans themselves ever could (e.g., in digital games); therefore, it could ultimately surpass human intuition. Such views do not pay enough attention to biases as well as transparency and explainability of AI. We contrast the journalistic narratives with philosophical understandings of intuition and a psychoanalytic view of the human. Those perspectives allow for a more complex view that goes beyond the focus on rationality and computational perspectives of tech journalism.","",""
29,"Melanie Mitchell","Artificial Intelligence Hits the Barrier of Meaning",2019,"","","","",123,"2022-07-13 09:31:46","","10.3390/info10020051","","",,,,,29,9.67,29,1,3,"Today’s AI systems sorely lack the essence of human intelligence: Understanding the situations we experience, being able to grasp their meaning. The lack of humanlike understanding in machines is underscored by recent studies demonstrating lack of robustness of state-of-the-art deep-learning systems. Deeper networks and larger datasets alone are not likely to unlock AI’s “barrier of meaning”; instead the field will need to embrace its original roots as an interdisciplinary science of intelligence.","",""
0,"Muhammad Taseer Suleman, Y. Khan","m1A-pred: Prediction of modified 1-methyladenosine sites in RNA sequences through artificial intelligence.",2022,"","","","",124,"2022-07-13 09:31:46","","10.2174/1386207325666220617152743","","",,,,,0,0.00,0,2,1,"BACKGROUND The process of nucleotides modification or methyl groups addition to nucleotides is known as post-transcriptional modification (PTM). 1-methyladenosine (m1A) is a type of PTM formed by adding a methyl group to the nitrogen at the 1st position of the adenosine base. Many human disorders are associated with m1A, which is widely found in ribosomal RNA and transfer RNA.   OBJECTIVE The conventional methods such as mass spectrometry and site-directed mutagenesis proved to be laborious and burdensome. Systematic identification of modified sites from RNA sequences is gaining much attention nowadays. Consequently, an extreme gradient boost predictor, m1A-Pred, is developed in this study for the prediction of modified m1A sites.   METHOD The current study involves the extraction of position and composition-based properties within nucleotide sequences. The extraction of features helps in the development of the features vector. Statistical moments were endorsed for dimensionality reduction in the obtained features.   RESULTS Through a series of experiments using different computational models and evaluation methods, it was revealed that the proposed predictor, m1A-pred, proved to be the most robust and accurate model for the identification of modified sites.   AVAILABILITY AND IMPLEMENTATION To enhance the research on m1A sites, a friendly server was also developed which was the final phase of this research.","",""
0,"Sandro Valerio Silva, Tobias Andermann, Alexander Zizka, G. Kozlowski, D. Silvestro","Global Estimation and Mapping of the Conservation Status of Tree Species Using Artificial Intelligence",2022,"","","","",125,"2022-07-13 09:31:46","","10.3389/fpls.2022.839792","","",,,,,0,0.00,0,5,1,"Trees are fundamental for Earth’s biodiversity as primary producers and ecosystem engineers and are responsible for many of nature’s contributions to people. Yet, many tree species at present are threatened with extinction by human activities. Accurate identification of threatened tree species is necessary to quantify the current biodiversity crisis and to prioritize conservation efforts. However, the most comprehensive dataset of tree species extinction risk—the Red List of the International Union for the Conservation of Nature (IUCN RL)—lacks assessments for a substantial number of known tree species. The RL is based on a time-consuming expert-based assessment process, which hampers the inclusion of less-known species and the continued updating of extinction risk assessments. In this study, we used a computational pipeline to approximate RL extinction risk assessments for more than 21,000 tree species (leading to an overall assessment of 89% of all known tree species) using a supervised learning approach trained based on available IUCN RL assessments. We harvested the occurrence data for tree species worldwide from online databases, which we used with other publicly available data to design features characterizing the species’ geographic range, biome and climatic affinities, and exposure to human footprint. We trained deep neural network models to predict their conservation status, based on these features. We estimated 43% of the assessed tree species to be threatened with extinction and found taxonomic and geographic heterogeneities in the distribution of threatened species. The results are consistent with the recent estimates by the Global Tree Assessment initiative, indicating that our approach provides robust and time-efficient approximations of species’ IUCN RL extinction risk assessments.","",""
32,"R. Grassi, V. Miele, A. Giovagnoni","Artificial intelligence: a challenge for third millennium radiologist",2019,"","","","",126,"2022-07-13 09:31:46","","10.1007/s11547-019-00990-5","","",,,,,32,10.67,11,3,3,"","",""
7,"Efrén Pérez Santín, Raquel Rodríguez Solana, María de las Nieves González García, M. D. García Suárez, Gerardo David Blanco Díaz, María Dolores Cima Cabal, J. Moreno Rojas, J. I. López Sánchez","Toxicity prediction based on artificial intelligence: A multidisciplinary overview",2021,"","","","",127,"2022-07-13 09:31:46","","10.1002/wcms.1516","","",,,,,7,7.00,1,8,1,"The use and production of chemical compounds are subjected to strong legislative pressure. Chemical toxicity and adverse effects derived from exposure to chemicals are key regulatory aspects for a multitude of industries, such as chemical, pharmaceutical, or food, due to direct harm to humans, animals, plants, or the environment. Simultaneously, there are growing demands on the authorities to replace traditional in vivo toxicity tests carried out on laboratory animals (e.g., European Union REACH/3R principles, Tox21 and ToxCast by the U.S. government, etc.) with in silica computational models. This is not only for ethical aspects, but also because of its greater economic and time efficiency, as well as more recently because of their superior reliability and robustness than in vivo tests, mainly since the entry into the scene of artificial intelligence (AI)‐based models, promoting and setting the necessary requirements that these new in silico methodologies must meet. This review offers a multidisciplinary overview of the state of the art in the application of AI‐based methodologies for the fulfillment of regulatory‐related toxicological issues.","",""
32,"Mark O. Riedl","Computational Narrative Intelligence: A Human-Centered Goal for Artificial Intelligence",2016,"","","","",128,"2022-07-13 09:31:46","","","","",,,,,32,5.33,32,1,6,"Narrative intelligence is the ability to craft, tell, understand, and respond affectively to stories. We argue that instilling artificial intelligences with computational narrative intelligence affords a number of applications beneficial to humans. We lay out some of the machine learning challenges necessary to solve to achieve computational narrative intelligence. Finally, we argue that computational narrative is a practical step towards machine enculturation, the teaching of sociocultural values to machines.","",""
44,"T. Alexandrov","Spatial Metabolomics and Imaging Mass Spectrometry in the Age of Artificial Intelligence.",2020,"","","","",129,"2022-07-13 09:31:46","","10.1146/annurev-biodatasci-011420-031537","","",,,,,44,22.00,44,1,2,"Spatial metabolomics is an emerging field of omics research that has enabled localizing metabolites, lipids, and drugs in tissue sections, a feat considered impossible just two decades ago. Spatial metabolomics and its enabling technology-imaging mass spectrometry-generate big hyper-spectral imaging data that have motivated the development of tailored computational methods at the intersection of computational metabolomics and image analysis. Experimental and computational developments have recently opened doors to applications of spatial metabolomics in life sciences and biomedicine. At the same time, these advances have coincided with a rapid evolution in machine learning, deep learning, and artificial intelligence, which are transforming our everyday life and promise to revolutionize biology and healthcare. Here, we introduce spatial metabolomics through the eyes of a computational scientist, review the outstanding challenges, provide a look into the future, and discuss opportunities granted by the ongoing convergence of human and artificial intelligence.","",""
34,"A. Borkowski, N. Viswanadham, L. B. Thomas, R. D. Guzmán, L. Deland, S. Mastorides","Using Artificial Intelligence for COVID-19 Chest X-ray Diagnosis",2020,"","","","",130,"2022-07-13 09:31:46","","10.1101/2020.05.21.20106518","","",,,,,34,17.00,6,6,2,"Coronavirus disease-19 (COVID-19), caused by a novel member of the coronavirus family, is a respiratory disease that rapidly reached pandemic proportions with high morbidity and mortality. It has had a dramatic impact on society and world economies in only a few months. COVID-19 presents numerous challenges to all aspects of healthcare, including reliable methods for diagnosis, treatment, and prevention. Initial efforts to contain the spread of the virus were hampered by the time required to develop reliable diagnostic methods. Artificial intelligence (AI) is a rapidly growing field of computer science with many applications to healthcare. Machine learning is a subset of AI that employs deep learning with neural network algorithms. It can recognize patterns and achieve complex computational tasks often far quicker and with increased precision than humans. In this manuscript, we explore the potential for a simple and widely available test as a chest x-ray (CXR) to be utilized with AI to diagnose COVID-19 reliably. Microsoft CustomVision is an automated image classification and object detection system that is a part of Microsoft Azure Cognitive Services. We utilized publicly available CXR images for patients with COVID-19 pneumonia, pneumonia from other etiologies, and normal CXRs as a dataset to train Microsoft CustomVision. Our trained model overall demonstrated 92.9% sensitivity (recall) and positive predictive value (precision), with results for each label showing sensitivity and positive predictive value at 94.8% and 98.9% for COVID-19 pneumonia, 89% and 91.8% for non-COVID-19 pneumonia, 95% and 88.8% for normal lung. We then validated the program using CXRs of patients from our institution with confirmed COVID-19 diagnoses along with non-COVID-19 pneumonia and normal CXRs. Our model performed with 100% sensitivity, 95% specificity, 97% accuracy, 91% positive predictive value, and 100% negative predictive value. Finally, we developed and described a publicly available website to demonstrate how this technology can be made readily available in the future.","",""
25,"M. Bainbridge, J. Webb","Artificial intelligence applied to the automatic analysis of absorption spectra. Objective measurement of the fine structure constant",2016,"","","","",131,"2022-07-13 09:31:46","","10.1093/mnras/stx179","","",,,,,25,4.17,13,2,6,"A new and automated method is presented for the analysis of high-resolution absorption spectra. Three established numerical methods are unified into one ""artificial intelligence"" process: a genetic algorithm (GVPFIT); non-linear least-squares with parameter constraints (VPFIT); and Bayesian Model Averaging (BMA).  The method has broad application but here we apply it specifically to the problem of measuring the fine structure constant at high redshift. For this we need objectivity and reproducibility. GVPFIT is also motivated by the importance of obtaining a large statistical sample of measurements of $\Delta\alpha/\alpha$. Interactive analyses are both time consuming and complex and automation makes obtaining a large sample feasible.  In contrast to previous methodologies, we use BMA to derive results using a large set of models and show that this procedure is more robust than a human picking a single preferred model since BMA avoids the systematic uncertainties associated with model choice.  Numerical simulations provide stringent tests of the whole process and we show using both real and simulated spectra that the unified automated fitting procedure out-performs a human interactive analysis. The method should be invaluable in the context of future instrumentation like ESPRESSO on the VLT and indeed future ELTs.  We apply the method to the $z_{abs} = 1.8389$ absorber towards the $z_{em} = 2.145$ quasar J110325-264515. The derived constraint of $\Delta\alpha/\alpha = 3.3 \pm 2.9 \times 10^{-6}$ is consistent with no variation and also consistent with the tentative spatial variation reported in Webb et al (2011) and King et al (2012).","",""
7,"D. G. Harkut, K. Kasat","Introductory Chapter: Artificial Intelligence - Challenges and Applications",2019,"","","","",132,"2022-07-13 09:31:46","","10.5772/INTECHOPEN.84624","","",,,,,7,2.33,4,2,3,"Artificial intelligence (AI) is any task performed by program or machine, which otherwise human needs to apply intelligence to accomplish it. It is the science and engineering of making machines to demonstrate intelligence especially visual perception, speech recognition, decision-making, and translation between languages like human beings. AI is the simulation of human intelligence processes by machines, especially computer systems. This includes learning, reasoning, planning, self-correction, problem solving, knowledge representation, perception, motion, manipulation, and creativity. It is a science and a set of computational techniques that are inspired by the way in which human beings use their nervous system and their body to feel, learn, reason, and act. AI is related to machine learning and deep learning wherein machine learning makes use of algorithms to discover patterns and generate insights from the data they are working on. Deep learning is a subset of machine learning, one that brings AI closer to the goal of enabling machines to think and work as human as possible. AI is a debatable topic and is often represented in a negative way; some would call it a blessing in disguise for businesses, while for some it is a technology that endangers the mere existence of humankind as it is potentially capable of taking over and dominating human being, but in reality artificial intelligence has affected our lifestyle either directly or indirectly and shaping the future of tomorrow. AI has already become an intrinsic part of our daily life and has greatly impacted our lifestyle despite the imperative uses of digital assistants of mobile phones, driverassistance systems, the bots, texts and speech translators, and systems that assist in recommending products and services and customized learning. Every emerging technology is a source of both enthusiasm and skepticism. AI is a source of both advantages and disadvantages in different perspectives. However, we need to overcome certain challenges before we can realize the true potential and immense transformational capabilities of this emerging technology. Some of the challenges related to artificial intelligence are:","",""
7,"J. Moore, N. Raghavachari","Artificial Intelligence Based Approaches to Identify Molecular Determinants of Exceptional Health and Life Span-An Interdisciplinary Workshop at the National Institute on Aging",2019,"","","","",133,"2022-07-13 09:31:46","","10.3389/frai.2019.00012","","",,,,,7,2.33,4,2,3,"Artificial intelligence (AI) has emerged as a powerful approach for integrated analysis of the rapidly growing volume of multi-omics data, including many research and clinical tasks such as prediction of disease risk and identification of potential therapeutic targets. However, the potential for AI to facilitate the identification of factors contributing to human exceptional health and life span and their translation into novel interventions for enhancing health and life span has not yet been realized. As researchers on aging acquire large scale data both in human cohorts and model organisms, emerging opportunities exist for the application of AI approaches to untangle the complex physiologic process(es) that modulate health and life span. It is expected that efficient and novel data mining tools that could unravel molecular mechanisms and causal pathways associated with exceptional health and life span could accelerate the discovery of novel therapeutics for healthy aging. Keeping this in mind, the National Institute on Aging (NIA) convened an interdisciplinary workshop titled “Contributions of Artificial Intelligence to Research on Determinants and Modulation of Health Span and Life Span” in August 2018. The workshop involved experts in the fields of aging, comparative biology, cardiology, cancer, and computational science/AI who brainstormed ideas on how AI can be leveraged for the analyses of large-scale data sets from human epidemiological studies and animal/model organisms to close the current knowledge gaps in processes that drive exceptional life and health span. This report summarizes the discussions and recommendations from the workshop on future application of AI approaches to advance our understanding of human health and life span.","",""
2,"Luis Pérez-Breva, John H. Shin","Artificial Intelligence in Neurosurgery: A Comment on the Possibilities",2019,"","","","",134,"2022-07-13 09:31:46","","10.14245/ns.1938404.202","","",,,,,2,0.67,1,2,3,"To the editor What people call artificial intelligence (AI) has begun to permeate our work and home environments. It provides customer service to consumers, suggests travel routes, and figures out when to turn up thermostats in our homes. It promises to empower precision medicine, handling of medical records, and eventually even replace human drivers. Professionals of all sorts turn to AI applications as “partners” in the work they do. How much should you believe? And most importantly, how can it help neurosurgeons? So-called generalized intelligence remains a distant, elusive aspiration. But there are ample opportunities to avail ourselves to the tools of AI to push the envelope and help discover and answer new questions in myriad fields, including neurosurgery. That requires understanding what the tools can do and how to phrase problems in neurology and neurosurgery to overcome the many limitations of these AI tools and make the most out of them. When the editors of Neurospine suggested we write a commentary, another particularly intriguing opportunity rose to mind: understanding how the neurology and neurosurgery community might benefit from the tools of AI could also help AI itself. The AI community has always hoped to gain inspiration from the way our brains (and entire perceptual and mechanical apparati) might work. Cognitive science has provided some of that, but as computational technology makes AI tools increasingly accessible for neurosurgery research, is there room to imagine collaborations that inform new opportunities in neurosurgery and new insights for AI following from a finer understanding among computer scientists of the phenomenally complex, robust architectures that support what we call “intelligence”? To get there, we need a shared understanding of what AI tools can and cannot yet do. Think of there being 2 ways to use AI. One is typically associated with analytics, regression, classification models, and statistical learning. These tools make the most sense when you have plenty of data. As long as you reduce a problem to a single factor and you have enough data, these tools can power fairly sophisticated software that interprets medical images, anticipates outcomes, or helps spot correlative trends you had not noticed. In these cases, AI tools are not providing fundamentally “new” insights; this is just modeling that may be extraordinarily complex and beyond most human comprehension. And just about everyone seems to think the magic formula for this is more data. Using AI tools this way is the most common and simplest. It is also the source of much of the common confusion—and, frankly, panic—about AI, which most people tend to think of as doing what humans already do, but better—such as outperforming humans at, say, Neurospine 2019;16(4):640-642. https://doi.org/10.14245/ns.1938404.202 Neurospine","",""
0,"S. Kumar","Abstract PO-056: Importance of artificial intelligence, machine learning deep learning in the field of medicine on the future role of the physician",2021,"","","","",135,"2022-07-13 09:31:46","","10.1158/1557-3265.ADI21-PO-056","","",,,,,0,0.00,0,1,1,"There are many ways to define the field of Artificial Intelligence. Here is one way for Artificial Intelligence is ""The Study of the computations that make it possible to perceive, reason, act and predict the future possible outcomes”. Deep learning, which is a popular research area of artificial intelligence (AI), enables the creation of end-to-end models to achieve promised results using input data. Deep learning techniques have been successfully applied in many problems such as arrhythmia detection, skin cancer classification, breast cancer detection, brain disease classification, pneumonia detection, COVID-19 from chest X-ray images, and CT scan images. Almost all hospitals have CT imaging machines; therefore, the chest CT images can be utilized for early classification of diseases. However, the chest CT classification involves a radiology expert and considerable time, which is valuable when any infection is growing at a rapid rate. Therefore, automated analysis of chest CT images is desirable to save the medical professionals precious time that shows the importance of Artificial Intelligence neural networks which is used to classify the infected patients as infected (+ve) or not (−ve). There is a vital need to detect the disease at an early stage and save the patient from the disease. Convolutional neural networks (CNN) are a powerful tool that comes under the platform of Neural Networks – Artificial Intelligence inspired by the human brain, which is extensively utilized for image classification. The hierarchical structure and efficient feature extraction characteristics from an image make CNN a dynamic model for image classification. Initially, the layers are organized into three dimensions: width, height, and depth. The neurons in a given layer do not attach to the entire set of neurons in the later layer, but only to limited neurons of it. Finally, the output is diminished to a single vector of probability scores, coordinated alongside the depth dimension. In a Convolutional Neural Network, the linear function that is used is called a convolutional layer. Each node in the hidden layer extracts different features by using image processing feature detectors. For example, in the first layer, the first node may extract the horizontal edges of an image, the second node may extract vertical edges and etc. These features are extracted using a kernel. The bottom is the original image and the top is the output of the convolutions. It is also worth noting that the output of the convolutions reduces the dimension of the original image, The next step the pooling layer happens tends to be computed after the convolutional layer. The reason why pooling is done is to further reduce the dimensions of the convolutional layer and just extract out the features to make the model more robust. AI could help to rapidly diagnose diseases if proper attention given in collecting the data. Citation Format: Subash Kumar. Importance of artificial intelligence, machine learning deep learning in the field of medicine on the future role of the physician [abstract]. In: Proceedings of the AACR Virtual Special Conference on Artificial Intelligence, Diagnosis, and Imaging; 2021 Jan 13-14. Philadelphia (PA): AACR; Clin Cancer Res 2021;27(5_Suppl):Abstract nr PO-056.","",""
26,"Yingxu Wang, W. Kinsner, S. Kwong, Henry Leung, Jianhua Lu, Michael H. Smith, L. Trajković, E. Tunstel, K. Plataniotis, G. Yen","Brain-Inspired Systems: A Transdisciplinary Exploration on Cognitive Cybernetics, Humanity, and Systems Science Toward Autonomous Artificial Intelligence",2020,"","","","",136,"2022-07-13 09:31:46","","10.1109/MSMC.2018.2889502","","",,,,,26,13.00,3,10,2,"Brain-inspired cognitive systems (BCSs) are an emerging field of cybernetics, cognitive science, and system science. BCSs study not only the intelligence science foundations of artificial intelligence (AI) and cognitive systems, but also formal models of the brain embodied by computational intelligence. This article presents the brain and intelligence science foundations of BCS toward hybrid intelligent systems and the symbiotic intelligence of humanity. It explores the transdisciplinary theoretical foundations of system, brain, intelligence, knowledge, cybernetic, and cognitive sciences toward the next generation of knowledge processors beyond classic data processors for autonomous computing systems. A BCS provides an overarching platform for cognitive cybernetics, humanity, and systems to enable emerging hybrid societies shared by humans and intelligent machines.","",""
15,"Yun-he Pan","Special issue on artificial intelligence 2.0",2017,"","","","",137,"2022-07-13 09:31:46","","10.1631/FITEE.1710000","","",,,,,15,3.00,15,1,5,"With the ever-growing popularization of the Internet, universal existence of sensors, emergence of big data, development of e-commerce, rise of the information community, and interconnection and fusion of data and knowledge in human society, physical space, and cyberspace, the information environment surrounding artificial intelligence (AI) development has changed profoundly, leading to a new evolutionary stage: AI 2.0. The emergence of new technologies also promotes AI to a new stage (Pan, 2016). The next-generation AI, namely AI 2.0, is a more explainable, robust, open, and general AI with the following attractive merits: It effectively integrates data-driven machine learning approaches (bottom-up) with knowledge-guided methods (top-down). In addition, it can employ data with different modalities (e.g., visual, auditory, and natural language processing) to perform cross-media learning and inference. Furthermore, there will be a step from the pursuit of an intelligent machine to the hybridaugmented intelligence (i.e., high-level man-machine collaboration and fusion). AI 2.0 will also promote crowd-based intelligence and autonomous-intelligent systems. In the next decades, AI2.0 will probably achieve remarkable progress in aforementioned trends, and therefore significantly change our cities, products, services, economics, environments, even how we advance our society. This special issue aims at reporting recent re-thinking of AI 2.0 from aforementioned aspects as well as practical methodologies, efficient implementations, and applications of AI 2.0. The papers in this special issue can be categorized into two groups. The first group consists of six review papers and the second group five research papers. In the first group, Zhuang et al. (2017) reviewed recent emerging theoretical and technological advances of AI in big data settings. The authors concluded that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI. Li W et al. (2017) described the concepts of crowd intelligence, and explained its relationship to the existing related concepts, e.g., crowdsourcing and human computation. In addition, the authors introduced four categories of representative crowd intelligence platforms. Peng et al. (2017) presented approaches, advances, and future directions in cross-media analysis and reasoning. This paper covers cross-media representation, mining, reasoning, and cross-media knowledge evolution. Tian et al. (2017) reviewed the state-of-the-art research of the perception in terms of visual perception, auditory perception, and speech perception. It also covered perceptual information processing and learning engines. Zhang et al. (2017) introduced the trends in the development of intelligent unmanned autonomous systems. It covered unmanned vehicles, unmanned aerial vehicles, service robots, space robots, marine robots, and unmanned Editorial: Frontiers of Information Technology & Electronic Engineering www.zju.edu.cn/jzus; engineering.cae.cn; www.springerlink.com ISSN 2095-9184 (print); ISSN 2095-9230 (online) E-mail: jzus@zju.edu.cn","",""
2,"Lahiru L. Abeysekara, H. Abdi","Short Paper: Neuromorphic Chip Embedded Electronic Systems to Expand Artificial Intelligence",2019,"","","","",138,"2022-07-13 09:31:46","","10.1109/AI4I46381.2019.00038","","",,,,,2,0.67,1,2,3,"Neuromorphic chips are electronic hardware mimicking neurons in human brain in an electronic structure. These ASICs (Application Specific Integrated Circuits) provide artificial neural networks with computational power comparatively higher than most neural networks generated by software algorithms. 'CM1K' is an electronic chip in this family of products. It has a parallel neural network of 1024 neurons. These neurons provide K-Nearest Neighbor (KNN) data classification. The chip requires to be embedded in an electronic system to access all its capabilities. This paper deliver a novel hardware system embedding CM1K neuromorphic chip. The system was implemented in image and video frame analysis for evaluation. The results prove that the system could benefit various applications including security, asset management, home appliances, mail sorting and manufacturing. Since the embedded system provide opportunity to integrate AI in to simple electronics, it helps on extending AI applications.","",""
19,"Sandip K. Patel, Bhawana George, Vineeta Rai","Artificial Intelligence to Decode Cancer Mechanism: Beyond Patient Stratification for Precision Oncology",2020,"","","","",139,"2022-07-13 09:31:46","","10.3389/fphar.2020.01177","","",,,,,19,9.50,6,3,2,"The multitude of multi-omics data generated cost-effectively using advanced high-throughput technologies has imposed challenging domain for research in Artificial Intelligence (AI). Data curation poses a significant challenge as different parameters, instruments, and sample preparations approaches are employed for generating these big data sets. AI could reduce the fuzziness and randomness in data handling and build a platform for the data ecosystem, and thus serve as the primary choice for data mining and big data analysis to make informed decisions. However, AI implication remains intricate for researchers/clinicians lacking specific training in computational tools and informatics. Cancer is a major cause of death worldwide, accounting for an estimated 9.6 million deaths in 2018. Certain cancers, such as pancreatic and gastric cancers, are detected only after they have reached their advanced stages with frequent relapses. Cancer is one of the most complex diseases affecting a range of organs with diverse disease progression mechanisms and the effectors ranging from gene-epigenetics to a wide array of metabolites. Hence a comprehensive study, including genomics, epi-genomics, transcriptomics, proteomics, and metabolomics, along with the medical/mass-spectrometry imaging, patient clinical history, treatments provided, genetics, and disease endemicity, is essential. Cancer Moonshot℠ Research Initiatives by NIH National Cancer Institute aims to collect as much information as possible from different regions of the world and make a cancer data repository. AI could play an immense role in (a) analysis of complex and heterogeneous data sets (multi-omics and/or inter-omics), (b) data integration to provide a holistic disease molecular mechanism, (c) identification of diagnostic and prognostic markers, and (d) monitor patient’s response to drugs/treatments and recovery. AI enables precision disease management well beyond the prevalent disease stratification patterns, such as differential expression and supervised classification. This review highlights critical advances and challenges in omics data analysis, dealing with data variability from lab-to-lab, and data integration. We also describe methods used in data mining and AI methods to obtain robust results for precision medicine from “big” data. In the future, AI could be expanded to achieve ground-breaking progress in disease management.","",""
14,"Puneet S. Sharma, M. Suehling, T. Flohr, D. Comaniciu","Artificial Intelligence in Diagnostic Imaging: Status Quo, Challenges, and Future Opportunities.",2020,"","","","",140,"2022-07-13 09:31:46","","10.1097/RTI.0000000000000499","","",,,,,14,7.00,4,4,2,"In this review article, the current and future impact of artificial intelligence (AI) technologies on diagnostic imaging is discussed, with a focus on cardio-thoracic applications. The processing of imaging data is described at 4 levels of increasing complexity and wider implications. At the examination level, AI aims at improving, simplifying, and standardizing image acquisition and processing. Systems for AI-driven automatic patient iso-centering before a computed tomography (CT) scan, patient-specific adaptation of image acquisition parameters, and creation of optimized and standardized visualizations, for example, automatic rib-unfolding, are discussed. At the reading and reporting levels, AI focuses on automatic detection and characterization of features and on automatic measurements in the images. A recently introduced AI system for chest CT imaging is presented that reports specific findings such as nodules, low-attenuation parenchyma, and coronary calcifications, including automatic measurements of, for example, aortic diameters. At the prediction and prescription levels, AI focuses on risk prediction and stratification, as opposed to merely detecting, measuring, and quantifying images. An AI-based approach for individualizing radiation dose in lung stereotactic body radiotherapy is discussed. The digital twin is presented as a concept of individualized computational modeling of human physiology, with AI-based CT-fractional flow reserve modeling as a first example. Finally, at the cohort and population analysis levels, the focus of AI shifts from clinical decision-making to operational decisions.","",""
13,"Yuanbin Wang, P. Zheng, Tao Peng, Huayong Yang, J. Zou","Smart additive manufacturing: Current artificial intelligence-enabled methods and future perspectives",2020,"","","","",141,"2022-07-13 09:31:46","","10.1007/s11431-020-1581-2","","",,,,,13,6.50,3,5,2,"","",""
0,"Wei Yan Ng, C. Cheung, D. Milea, D. Ting","Artificial intelligence and machine learning for Alzheimer’s disease: let’s not forget about the retina",2021,"","","","",142,"2022-07-13 09:31:46","","10.1136/bjophthalmol-2020-318407","","",,,,,0,0.00,0,4,1,"As the world population ages, it is estimated that the population worldwide above the age of 65 years old will increase from 420 million in 2000 to almost 1 billion by 2030. Dementia, with Alzheimer’s disease (AD) as the leading cause, is expected to rise in tandem. AD accounts for 60%–80% of all dementia cases, with an estimated 5–7 million new cases diagnosed each year. Despite intensive research, the diagnosis of AD is currently made through a combination of clinical assessment, neuroimaging and detection of biomarkers from positron emission tomography or cerebrospinal fluid examination, with patients facing issues including high costs, invasiveness of the procedures. Hence, alternative identification of AD without the use of costly or invasive tests remains a challenge that is difficult to surmount. To date, the healthcare has experienced a significant shift towards early accurate detection as well as early prevention. This importance is highlighted by the screening and surveillance of prevalent diseases such as diabetic retinopathy, breast cancer and dementia. While some of these programmes have been very successful in significantly reducing morbidity and mortality, significant amount of manpower, time and training is required for their successful execution. 10 This has lent greater weight to the adoption of healthcare technology in order to optimise the accuracy and efficiency of such programmes. Artificial intelligence (AI), through the combination of digitised big data and computational power, has emerged at the forefront of healthcare. It appears to be wellsuited to address the needs of the healthcare system: fast and accurate predictive, diagnostic and possibly therapeutic algorithms. Machine learning is able to process large amounts of digitised datasets beyond the limits of human capability, and analyse and convert these data into useful clinical insights for the physician. It is a natural fit for conditions or medical specialties that have a large reserve of labelled digitised datasets. At present, convoluted neural networks (CNN), designed to receive two or threedimensional shaped inputs, is one of the most commonly applied deep learning models in medical imaging analysis. Through the utilisation of CNN deep learning, several landmark studies extracting data from retinal images have shown a high degree of accuracy compared with human graders. 13 In order to achieve this, AI generally still requires large, welllabelled and highquality datasets. This places significant barriers to the development of successful image analysis models. The retina is one of the few select organs where image collection is easily accessible and abundant through the use of ocular imaging technologies. As the microvascular properties and neuronal structures of the retina, such as ganglion cellinner plexiform layer (GCIPL) and retinal nerve fibre layer (RNFL), 15 resemble the intracranial neuronal structure and vasculature, it provides a direct visualisation of potential intracranial changes. This combination of unique attributes can potentially provide a low cost, noninvasive analysis of the brain without the patient having to undergo costly neuroimaging to reach a diagnosis. This has attracted increasing research interest, especially in the field of AD where accurate early detection and diagnostic models still remain elusive. The pathogenesis associating retinal changes with AD is currently still unclear. While some evidence suggest the presence of amyloid beta plaques and tau neurofibrillary tangles in the retina of patients with AD, 20 the significance remains debatable. On the other hand, recent studies examining structural changes of the retina in patients with AD have indicated extensive changes including loss of macula volume, 23 thinning of the GCIPL 25 and RNFL thickness, and subfoveal choroidal thickness. In the peripheral retina, notable changes include increased drusen formation and reduced retinal vascularity. Optical coherence tomography angiogram (OCTA), which allows for noninvasive assessment of retinal vascularity, has also shown decreased vessel density, perfusion density and increased foveal avascular zone in patients with AD. 29 It is thus apparent that retinal changes, especially retinal thinning and reduction in vascularity, could potentially indicate the development of AD. In the article by Wisely et al the authors have trained an AI model with multiple imaging modalities using CNN deeplearning with the best performing models achieving an area under curve (AUC) between 0.830 and 0.841. A total of 284 eyes from 159 subjects, of which 36 were clinically diagnosed with AD by experienced neurologists, were analysed in this study. Patient data, optical coherence tomography (OCT) and OCTA quantitative data, ultrawide field retinal photography as well as retinal autofluorescence images were used in the training, validation and testing of the models. Used in isolation, the GCIPL thickness as an indicator of AD had the highest predictive value with an AUC of 0.809. When used in combination, the model that was trained with a combination of GCIPL, OCTA quantitative data and patient data had the highest AUC of 0.841. This paper represents the first attempt as well as a proof of concept at developing a CNN to detect AD using multimodal retinal images. The relatively high predictive ability of the different combination models not only serves as an important foundation for future deep learning models in predicting AD, but also proves the validity of this approach. By testing a varied combination of different imaging modalities available in an ophthalmic clinic, Wisely et al are able to determine the datasets that will provide the greatest yield in accurate prediction of AD. This could contribute to the future development of a robust screening or predictive platform that uses parameters that are costeffective and simple in acquisition. Future research and AI models are likely to expand on the use of retinal imaging and combine with clinical neurological Cataract and Comprehensive, Singapore National Eye Centre, Singapore Ophthalmology and Visual Sciences, The Chinese University of Hong Kong, Hong Kong, Hong Kong Neuroophthalmology Department, Singapore National Eye Centre, Singapore Vitreoretinal Department, Singapore National Eye Centre, Singapore","",""
0,"E. Cambouropoulos, Maximos A. Kaliakatsos-Papakostas","Cognitive Musicology and Artificial Intelligence: Harmonic Analysis, Learning, and Generation",2021,"","","","",143,"2022-07-13 09:31:46","","10.1007/978-3-030-72116-9_10","","",,,,,0,0.00,0,2,1,"","",""
587,"Matej Moravcík, Martin Schmid, Neil Burch, V. Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, K. Waugh, Michael Bradley Johanson, Michael H. Bowling","DeepStack: Expert-level artificial intelligence in heads-up no-limit poker",2017,"","","","",144,"2022-07-13 09:31:46","","10.1126/science.aam6960","","",,,,,587,117.40,59,10,5,"Computer code based on continual problem re-solving beats human professional poker players at a two-player variant of poker. Artificial intelligence masters poker Computers can beat humans at games as complex as chess or go. In these and similar games, both players have access to the same information, as displayed on the board. Although computers have the ultimate poker face, it has been tricky to teach them to be good at poker, where players cannot see their opponents' cards. Moravčík et al. built a code dubbed DeepStack that managed to beat professional poker players at a two-player poker variant called heads-up no-limit Texas hold'em. Instead of devising its strategy beforehand, DeepStack recalculated it at each step, taking into account the current state of the game. The principles behind DeepStack may enable advances in solving real-world problems that involve information asymmetry. Science, this issue p. 508 Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold’em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.","",""
23,"K. Mouridsen, P. Thurner, G. Zaharchuk","Artificial Intelligence Applications in Stroke.",2020,"","","","",145,"2022-07-13 09:31:46","","10.1161/STROKEAHA.119.027479","","",,,,,23,11.50,8,3,2,"Management of stroke highly depends on information from imaging studies. Noncontrast computed tomography (CT) and magnetic resonance imaging (MRI) can both be used to distinguish between ischemic and hemorrhagic stroke, which is difficult based on clinical features. Hypodensity on CT and DWI hyperintensity on MRI identifies irreversibly damaged tissue, although the sensitivity of MRI is higher in the acute setting. Angiographic and perfusion imaging sequences can identify a large vessel occlusion and, along with perfusion imaging, can select patients for endovascular therapy. The FLAIR-DWI mismatch yields information about patients with unknown time of onset (including wake-up strokes). Stroke imaging also gives insight into prognosis, with current methods aiming to give a picture of the short-term consequences of successful reperfusion or continued large vessel occlusion. One important caveat about stroke imaging is that it must be done quickly, as faster treatment leads to better outcomes.1 However, most steps in the stroke imaging triage pathway require the presence of human radiologists and neurologists, and this is often the time-limiting step. The expertise required for these tasks may not be available at all sites or at all times. Therefore, there is interest in automated methods for stroke imaging evaluation. Artificial intelligence (AI) is a broad term reflecting the use of computers to perform tasks that humans may find difficult, often in ways that are hard to pinpoint. For example, although humans find high-level computation difficult, calculator technology is not considered AI because we know how to break this down into discrete steps and feel we understand it. However, facial recognition is a task that humans perform well, but an algorithm to identify faces is usually considered AI since we cannot articulate precisely how this is done. Machine learning (ML) is a subset of AI in which algorithms learn from the data itself without explicit programming. ML methods reflect a broad range of statistical techniques ranging from linear regression to more complex methods such as support vector machines and decision trees. ML methods can be further broken into supervised and unsupervised learning, which differ from one another in that the former requires access to gold standard labels although the latter attempts to find the answers implicitly in the data itself. While ML methods have grown more popular over recent years, the advent of a specific supervised ML method based on architectures resembling human neural networks over the past decade has led to a quantum leap in performance.2 This method, called deep learning (DL) because of many multiple internal layers, can be considered a transformative technology. Compared with previous methods that required humans to identify image features, a deep neural network trained on a dataset with known outputs can learn the best features for organizing the data. In this review, we will discuss ML methods applied to stroke imaging with an emphasis on DL applications. We refer to Figure for a graphical overview of the applications discussed in this review.","",""
95,"S. Dilek, Hüseyin Çakir, Mustafa Aydin","Applications of Artificial Intelligence Techniques to Combating Cyber Crimes: A Review",2015,"","","","",146,"2022-07-13 09:31:46","","10.5121/ijaia.2015.6102","","",,,,,95,13.57,32,3,7,"With the advances in information technology (IT) criminals are using cyberspace to commit numerous cyber crimes. Cyber infrastructures are highly vulnerable to intrusions and other threats. Physical devices and human intervention are not sufficient for monitoring and protection of these infrastructures; hence, there is a need for more sophisticated cyber defense systems that need to be flexible, adaptable and robust, and able to detect a wide variety of threats and make intelligent real-time decisions. Numerous bio-inspired computing methods of Artificial Intelligence have been increasingly playing an important role in cyber crime detection and prevention. The purpose of this study is to present advances made so far in the field of applying AI techniques for combating cyber crimes, to demonstrate how these techniques can be an effective tool for detection and prevention of cyber attacks, as well as to give the scope for future work.","",""
213,"Georgios N. Yannakakis, J. Togelius","Artificial Intelligence and Games",2018,"","","","",147,"2022-07-13 09:31:46","","10.1007/978-3-319-63519-4","","",,,,,213,53.25,107,2,4,"","",""
90,"M. Alsharqi, W. Woodward, J. Mumith, D. C. Markham, R. Upton, P. Leeson","Artificial intelligence and echocardiography",2018,"","","","",148,"2022-07-13 09:31:46","","10.1530/ERP-18-0056","","",,,,,90,22.50,15,6,4,"Echocardiography plays a crucial role in the diagnosis and management of cardiovascular disease. However, interpretation remains largely reliant on the subjective expertise of the operator. As a result inter-operator variability and experience can lead to incorrect diagnoses. Artificial intelligence (AI) technologies provide new possibilities for echocardiography to generate accurate, consistent and automated interpretation of echocardiograms, thus potentially reducing the risk of human error. In this review, we discuss a subfield of AI relevant to image interpretation, called machine learning, and its potential to enhance the diagnostic performance of echocardiography. We discuss recent applications of these methods and future directions for AI-assisted interpretation of echocardiograms. The research suggests it is feasible to apply machine learning models to provide rapid, highly accurate and consistent assessment of echocardiograms, comparable to clinicians. These algorithms are capable of accurately quantifying a wide range of features, such as the severity of valvular heart disease or the ischaemic burden in patients with coronary artery disease. However, the applications and their use are still in their infancy within the field of echocardiography. Research to refine methods and validate their use for automation, quantification and diagnosis are in progress. Widespread adoption of robust AI tools in clinical echocardiography practice should follow and have the potential to deliver significant benefits for patient outcome.","",""
76,"A. Zhavoronkov","Artificial Intelligence for Drug Discovery, Biomarker Development, and Generation of Novel Chemistry.",2018,"","","","",149,"2022-07-13 09:31:46","","10.1021/acs.molpharmaceut.8b00930","","",,,,,76,19.00,76,1,4,"and Generation of Novel Chemistry T productivity of the pharmaceutical industry is on the decline. Failure rates in clinical trials exceed 90% after therapies are tested in model organisms, and the cost to develop a new drug exceeds $2.6 billion. Recent advances in artificial intelligence (AI) may help to reverse this trend and accelerate and improve pharmaceutical R&D. While the term AI and the concept of deep learning are not new, recent advances in high-performance computing, the availability of large annotated data sets required for training, and new frameworks for implementing deep neural networks (DNNs) resulted in an unprecedented acceleration of the field. Since 2014, DNNs have surpassed human accuracy in image, voice and text recognition, autonomous driving, and many other tasks. Early presentations to the pharmaceutical industry on the advances in deep learning in 2014 and 2015 resulted in skepticism and were discarded. In 2017, many pharmaceutical companies started partnering with AI startups and academics or started internal R&D programs. From training DNNs on transcriptional response data for predicting the pharmacological properties of small molecules and biomarker development, to the generation of novel chemistry, deep learning techniques rapidly propagated into many areas of biomedical research. The body of knowledge and the range of applications of deep learning and other machine learning techniques has expanded quickly and permeated into many areas of drug discovery. There have been hundreds of publications deposited in peer-reviewed journals and on ArXiv. In June 2017, Molecular Pharmaceutics announced a special issue titled “Deep Learning for Drug Discovery and Biomarker Development” focused on the applications of AI in chemistry and biomedicine (Figure 1). After a call to the most prominent scientists publishing on deep learning in the areas of computational chemistry and biology, 10 research papers were accepted. One of the main opportunities for AI in drug discovery is in drug repurposing using abundant data sets available from highthroughput experiments with gene expression profiles. Specifically, transcriptional response profiles generated by the Broad Institute, such as the Connectivity Map. The connectivity map uses gene expression signatures to connect small molecules, genes, and disease available through the LINCS Project. Donner et al. used the L1000 data set to develop a new method for measuring the compound functional similarity based on gene expression data for drug repurposing. The method identified drugs with shared therapeutic and biological targets even when the compounds were structurally dissimilar, thereby revealing previously unreported functional relationships between compounds. Imaging data is among the most abundant data type available for deep learning researchers, often allowing for rapid validation of results using human visual sensory organs. Many of the tools developed for image recognition and trained and tested on simple pictures are now available for researchers working with more complex imaging data types, including computed tomography. Gao and Qian used the patch-based convolutional neural network (CNN) model combined with support vector machines to predict multidrug resistant patients with tuberculosis using a data set of 230 patients from the ImageCLEF2017 competition, achieving reasonably high classification rates. Xiang and colleagues presented a multitask deep autoencoder for the prediction of the human cytochrome 450 inhibition, laying the roadmap for reducing the side effects associated with inhibiting the CYP450. Lane et al. compared various machine learning models for predicting hit molecules forMycobacterium tuberculosis (Mtb) using a small curated data set of molecules targeting Mtb. Another article by the Ekins group compared various machine learning techniques for predicting the estrogen receptor (ER) binding. In this work, Russo and co-authors compared the AdaBoost, Bernoulli Naiv̈e-Bayes, Random Forest, support vector classification, and deep neural networks using a variety of metrics and a proprietary data set compiled from public sources to predict ER binding. Again, Random Forest outperformed the other algorithms, demonstrating the value of comparing the various algorithms, especially for simple machine learning tasks. One of the many chemistry related machine learning challenges is the selection of the representation of molecular structure to capture as many of the relevant chemical and biological features and come as close to reality as possible. There are many representations of molecular structures, including a variety of molecular fingerprints, string-based representations, molecular graphs, and others. A molecular graph is a popular representation of the molecular structure for machine learning applications and explored by many groups working on medicinal chemistry related tasks. Hop and colleagues explored the performance of geometric deep learning methods in the context of drug discovery, comparing machine learned features against the domain expert engineered features. The CNN graph outperformed the methods trained on expert engineered features on most of the data sets. The popular deep learning techniques involving CNN are often trained on 2D and 3D images. To help facilitate f the many applications of CNNs in chemistry, Kuzminykh presented the wave transform-based representation of the 3D molecular structure. The group demonstrated that the proposed representation leads to the better performance of CNNbased autoencoders than either the voxel-based representation or the previously used Gaussian blur of atoms, and it can be successfully applied to classification tasks, such as MACCS fingerprint prediction. Deep generative models, commonly referred to as AI imagination, enabled many new applications requiring creativity and","",""
58,"L. D. Jones, D. Golan, S. Hanna, M. Ramachandran","Artificial intelligence, machine learning and the evolution of healthcare",2018,"","","","",150,"2022-07-13 09:31:46","","10.1302/2046-3758.73.BJR-2017-0147.R1","","",,,,,58,14.50,15,4,4,"vol. 7, No. 3, MaRch 2018 223 First proposed by Professor John Mccarthy at Dartmouth college in the summer of 1956,1 artificial Intelligence (aI) – human intelligence exhibited by machines – has occupied the lexicon of successive generations of computer scientists, science fiction fans, and medical researchers. The aim of countless careers has been to build intelligent machines that can interpret the world as humans do, understand language, and learn from realworld examples. In the early part of this century, two events coincided that transformed the field of aI. The advent of widely available Graphic Processing Units (GPUs) meant that parallel processing was faster, cheaper, and more powerful. at the same time, the era of ‘Big Data’ – images, text, bioinformatics, medical records, and financial transactions, among others – was moving firmly into the mainstream, along with almost limitless data storage. These factors led to a dramatic resurgence in interest in aI in both academic circles and industries outside traditional computer science. once again, aI occupies the zeitgeist, and is poised to transform medicine at a basic science, clinical, healthcare management, and financial level. Terminology surrounding these technologies continues to evolve and can be a source of confusion for non-computer scientists. aI is broadly classified as: general aI, machines that replicate human thought, emotion, and reason (and remain, for now, in the realm of science fiction); and narrow aI, technologies that can perform specific tasks as well as, or better than, humans. Machine learning (Ml) is the study of computer algorithms that can learn complex relationships or patterns from empirical data and make accurate decisions.2 Rather than coding specific sets of instructions to accomplish a task, the machine is ‘trained’ using large amounts of data and algorithms that confer it the ability to learn how to perform the task. Unlike normal algorithms, it is the data that ‘tells’ the machine what the ‘good answer’ is, and learning occurs without explicit programming. Ml problems can be classified as supervised learning or unsupervised learning.3 In a supervised machine learning algorithm, such as face recognition, the machine is shown several examples of ‘face’ or ‘non-face’ and the algorithm learns to predict whether an unseen image is a face or not. In unsupervised learning, the images shown to the machine are not labelled as ‘face’ or ‘non-face’. artificial Neural Networks (aNN)4 are one group of algorithms used for machine learning. While aNNs have existed for over 60 years, they fell out of favour during the 1990s and 2000s. In the last half-decade, aNNs have had a resurgence under a new name: deep artificial networks (or ‘Deep learning’). aNNs are uniquely poised to take full advantage of the computational boost offered by GPUs, allowing them to crunch through data sets of enormous sizes. These range from computer vision tasks, such as image classification, object detection, face recognition, and optical character recognition (ocR), to natural language processing and even gameplaying problems (from mastering simple atari games to the recent alphaGo victory against human grandmasters).5 aNNs work by constructing layers upon layers of simple processing units (often referred to as ‘neurons’), interconnected via many differentially weighted connections. aNNs are ‘trained’ by using backpropagation algorithms, essentially telling the machine how to alter the internal parameters that are used to compute the representation in each layer from the representation in the previous Artificial intelligence, machine learning and the evolution of healthcare","",""
47,"L. Deng","Artificial Intelligence in the Rising Wave of Deep Learning: The Historical Path and Future Outlook [Perspectives]",2018,"","","","",151,"2022-07-13 09:31:46","","10.1109/MSP.2017.2762725","","",,,,,47,11.75,47,1,4,"Artificial intelligence (AI) is a branch of computer science and a technology aimed at developing the theories, methods, algorithms, and applications for simulating and extending human intelligence. Modern AI enables going from an old world-where people give computers rules to solve problems-to a new world-where people give computers problems directly and the machines learn how to solve them on their own using a set of algorithms. An algorithm is a self-contained sequence of instructions and actions to be performed by a computational machine. Starting from an initial state and initial input, the instructions describe computational steps, which, when executed, proceed through a finite number of well-defined successive states, eventually producing an output and terminating at a final ending state. AI algorithms are a rich set of algorithms used to perform AI tasks, notably those pertaining to perception and cognition that involve learning from data and experiences simulating human intelligence.","",""
42,"R. Mirnezami, A. Ahmed","Surgery 3.0, artificial intelligence and the next‐generation surgeon",2018,"","","","",152,"2022-07-13 09:31:46","","10.1002/bjs.10860","","",,,,,42,10.50,21,2,4,"In December 2017, the Google subsidiary DeepMind announced that its AlphaZero artificial intelligence (AI) program had taught itself to play chess, from scratch, in just 4 h. This algorithmic program has since mastered moves and strategies entirely through self-play, and defeated the world champion chess program by an inventive approach to the ancient art of chess1. The wave of enthusiasm following this announcement has been felt well beyond gaming and genuine questions are now emerging regarding the potential for AI in other fields, including healthcare. This is especially relevant in disciplines with a strong emphasis on pattern recognition, notably medical imaging and histopathology, where AI-based platforms are equalling, and in some cases surpassing, their human counterparts. For example, computer scientists from Cornell University recently reported superior accuracy of classification in the detection of lymph node metastases in breast cancer using a deep learning algorithm, compared with conventional pathology2. In contrast, AI technology has taken longer to permeate through to the world of surgery, partly owing to the complex nature of interaction with human tissue at the core of the specialty, but also because of a perceived lack of necessity, evidence and awareness of the potential capabilities of computational approaches in surgical practice. AI is, however, advancing rapidly, and at a pace that is difficult to ignore. The current vision is one of augmented surgical practice to complement rather than replace human skills, particularly in two broad areas: surgical decision-making and operative surgery. Surgery involves complex decisions including, for example, choices about the need for multimodal therapy, timing of surgery, and radical versus organ-preserving surgery. Moreover, the surgeon is increasingly expected to provide patients with personalized data on potential risks, and likelihood of major morbidity and mortality. These complex assessments are beyond the capabilities of most surgeons. The development of tools such as algorithmic clinical decision support (CDS) within surgery, with access to large and varied stores of ‘big data’, underpinned by the use of integration of multiparametric data, allows crosstalk between data stores and computer","",""
1,"P. Smolensky, R. Thomas McCoy, Roland Fernandez, M. Goldrick, Jia-Hao Gao","Neurocompositional computing in human and machine intelligence: A tutorial",2022,"","","","",153,"2022-07-13 09:31:46","","","","",,,,,1,1.00,0,5,1,"The past decade has produced a revolution in Artificial Intelligence (AI), after a half-century of AI repeatedly failing to meet expectations. What explains the dramatic change from 20th-century to 21st-century AI, and how can remaining limitations of current AI be overcome? Until now, the widely accepted narrative has attributed the recent progress in AI to technical engineering advances that have yielded massive increases in the quantity of computational resources and training data available to support statistical learning in deep artificial neural networks. Although these quantitative engineering innovations are important, here we show that the latest advances in AI are not solely due to quantitative increases in computing power but also qualitative changes in how that computing power is deployed. These qualitative changes have brought about a new type of computing that we call neurocompositional computing . In neurocompositional computing, neural networks exploit two scientific principles that contemporary theory in cognitive science maintains are simultaneously necessary to enable human-level cognition. The Compositionality Principle asserts that encodings of complex information are structures that are systematically composed from simpler structured encodings. The Continuity Principle states that the encoding and processing of information is formalized with real numbers that vary continuously. These principles have seemed irreconcilable until the recent mathematical discovery that compositionality can be realized not only through the traditional discrete methods of symbolic computing, well developed in 20th-century AI, but also through novel forms of continuous neural computing—neurocompositional computing. The unprecedented progress of 21st-century AI has resulted from the use of limited—first-generation—forms of neurocompositional computing. We show that the new techniques now being deployed in second-generation neurocompositional computing create AI systems that are not only more robust and accurate than current systems, but also more comprehensible—making it possible to diagnose errors in, and exert human control over, artificial neural networks through interpretation of their internal states and direct intervention upon those states. Note: This tutorial is intended for those new to this topic, and does not assume familiarity with cognitive science, AI, or deep learning. Appendices provide more advanced material. Each figure, and the associated box explaining it, provides an exposition, illustration, or further details of a main point of the paper; in order to make these figures relatively self-contained, it has sometimes been necessary to repeat some material from the text. For a brief introduction and additional development of some of this material see [212]. . abstract mental processes”","",""
0,"K. Hemalatha, K. Hema, V. Deepika","Predictive Analysis of Damage Occurred Due to Natural Disasters Using Whale-Optimization Algorithm-Based Hybrid Computation",2021,"","","","",154,"2022-07-13 09:31:46","","10.1007/978-981-16-1941-0_7","","",,,,,0,0.00,0,3,1,"","",""
105,"B. Koçak, E. S. Durmaz, Ece Ateş, Ö. Kılıçkesmez","Radiomics with artificial intelligence: a practical guide for beginners.",2019,"","","","",155,"2022-07-13 09:31:46","","10.5152/dir.2019.19321","","",,,,,105,35.00,26,4,3,"Radiomics is a relatively new word for the field of radiology, meaning the extraction of a high number of quantitative features from medical images. Artificial intelligence (AI) is broadly a set of advanced computational algorithms that basically learn the patterns in the data provided to make predictions on unseen data sets. Radiomics can be coupled with AI because of its better capability of handling a massive amount of data compared with the traditional statistical methods. Together, the primary purpose of these fields is to extract and analyze as much and meaningful hidden quantitative data as possible to be used in decision support. Nowadays, both radiomics and AI have been getting attention for their remarkable success in various radiological tasks, which has been met with anxiety by most of the radiologists due to the fear of replacement by intelligent machines. Considering ever-developing advances in computational power and availability of large data sets, the marriage of humans and machines in future clinical practice seems inevitable. Therefore, regardless of their feelings, the radiologists should be familiar with these concepts. Our goal in this paper was three-fold: first, to familiarize radiologists with the radiomics and AI; second, to encourage the radiologists to get involved in these ever-developing fields; and, third, to provide a set of recommendations for good practice in design and assessment of future works.","",""
0,"Haqi Khalid, S. Hashim, S. M. S. Ahmad, F. Hashim, Muhammad Akmal Chaudhary","Robust Multi-Gateway Authentication Scheme for Agriculture Wireless Sensor Network in Society 5.0 Smart Communities",2021,"","","","",156,"2022-07-13 09:31:46","","10.3390/AGRICULTURE11101020","","",,,,,0,0.00,0,5,1,"Recent Society 5.0 efforts by the Government of Japan are aimed at establishing a sustainable human-centered society by combining new technologies such as sensor networks, edge computing, Internet of Things (IoT) ecosystems, artificial intelligence (AI), big data, and robotics. Many research works have been carried out with an increasing emphasis on the fundamentals of wireless sensor networks (WSN) for different applications; namely precision agriculture, environment, medical care, security, and surveillance. In the same vein, almost all of the known authentication techniques rely on the single gateway node, which is unsuitable for the current sensor nodes that are broadly distributed in the real world. Despite technological advances, resource constraints and vulnerability to an attacker physically capturing some sensor nodes have remained an important and challenging research field for developing wireless sensor network user authentication. This work proposes a new authentication scheme for agriculture professionals based on a multi-gateway communication model using a fuzzy extractor algorithm to support the Society 5.0 environment. The scheme provides a secure mutual authentication using the well-established formal method called BAN logic. The formal security verification of the proposed scheme is validated with the AVISPA tool, a powerful validation method for network security applications. In addition, the security of the scheme was informally analyzed to demonstrate that the scheme is secure from different attacks, e.g., sensor capture, replay, and other network and physical attacks. Furthermore, the communication and computation costs of the proposed scheme are evaluated and show better performance than the existing authentication schemes.","",""
0,"A. Ruospo, D. Piumatti, A. Floridia, Ernesto Sánchez","A Suitability Analysis of Software Based Testing Strategies for the On-line Testing of Artificial Neural Networks Applications in Embedded Devices",2021,"","","","",157,"2022-07-13 09:31:46","","10.1109/IOLTS52814.2021.9486704","","",,,,,0,0.00,0,4,1,"Electronic devices based on artificial intelligence solutions are pervading our everyday life. Nowadays, human decision processes are supported by real-time data gathered from intelligent systems. Artificial Neural Networks (ANNs) are one of the most used deep learning predictive models due to their outstanding computational capabilities. However, assessing their reliability is still an open issue faced by both the academic and industrial worlds, especially when ANNs are deployed on safety-critical systems, such as self-driving cars in the automotive world. In these systems, a strategy for identifying hardware faults is required by industry standards (e.g., ISO26262 for automotive, and DO254 for avionics). Among the existing in-field test strategies, the periodic scheduling of on-line Software Test Library (STL) is a wide strategy adopted; STL allows to reach an acceptable fault coverage without the need for additional hardware. However, when dealing with ANN-based applications, the execution of on-line tests interleaving the ANN inferences may jeopardise the strive for performance maximization. The paper presents a comprehensive analysis of six possible scenarios concerning the execution of on-line self-test programs in embedded devices running ANN-based applications. In the proposed scenarios, the impact of the STL execution on the ANN performance is analyzed; in particular, the execution times of an inference and the Fault Detection Time (FDT) of the STL are discussed and compared. Experimental analyses are provided by relying on: an open-source RISC-V platform running two different convolutional neural networks; a STL for RISC-V cores with a maximum achievable fault coverage of 90%.","",""
24,"Lorenzo Cominelli, D. Mazzei, D. Rossi","SEAI: Social Emotional Artificial Intelligence Based on Damasio’s Theory of Mind",2018,"","","","",158,"2022-07-13 09:31:46","","10.3389/frobt.2018.00006","","",,,,,24,6.00,8,3,4,"A socially intelligent robot must be capable to extract meaningful information in real time from the social environment and react accordingly with coherent human-like behavior. Moreover, it should be able to internalize this information, to reason on it at a higher level, build its own opinions independently, and then automatically bias the decision-making according to its unique experience. In the last decades, neuroscience research highlighted the link between the evolution of such complex behavior and the evolution of a certain level of consciousness, which cannot leave out of a body that feels emotions as discriminants and prompters. In order to develop cognitive systems for social robotics with greater human-likeliness, we used an “understanding by building” approach to model and implement a well-known theory of mind in the form of an artificial intelligence, and we tested it on a sophisticated robotic platform. The name of the presented system is SEAI (Social Emotional Artificial Intelligence), a cognitive system specifically conceived for social and emotional robots. It is designed as a bio-inspired, highly modular, hybrid system with emotion modeling and high-level reasoning capabilities. It follows the deliberative/reactive paradigm where a knowledge-based expert system is aimed at dealing with the high-level symbolic reasoning, while a more conventional reactive paradigm is deputed to the low-level processing and control. The SEAI system is also enriched by a model that simulates the Damasio’s theory of consciousness and the theory of Somatic Markers. After a review of similar bio-inspired cognitive systems, we present the scientific foundations and their computational formalization at the basis of the SEAI framework. Then, a deeper technical description of the architecture is disclosed underlining the numerous parallelisms with the human cognitive system. Finally, the influence of artificial emotions and feelings, and their link with the robot’s beliefs and decisions have been tested in a physical humanoid involved in Human–Robot Interaction (HRI).","",""
6,"Marina Boia, C. Musat, B. Faltings","Acquiring Commonsense Knowledge for Sentiment Analysis through Human Computation",2014,"","","","",159,"2022-07-13 09:31:46","","10.1609/aaai.v28i1.8840","","",,,,,6,0.75,2,3,8,"    Many Artificial Intelligence tasks need large amounts of commonsense knowledge. Because obtaining this knowledge through machine learning would require a huge amount of data, a better alternative is to elicit it from people through human computation. We consider the sentiment classification task, where knowledge about the contexts that impact word polarities is crucial, but hard to acquire from data. We describe a novel task design that allows us to crowdsource this knowledge through Amazon Mechanical Turk with high quality. We show that the commonsense knowledge acquired in this way dramatically improves the performance of established sentiment classification methods.   ","",""
11,"Matthew Lease, Omar Alonso","Crowdsourcing and Human Computation, Introduction",2014,"","","","",160,"2022-07-13 09:31:46","","10.1007/978-1-4614-6170-8_107","","",,,,,11,1.38,6,2,8,"","",""
41,"C. Macrae","Governing the safety of artificial intelligence in healthcare",2019,"","","","",161,"2022-07-13 09:31:46","","10.1136/bmjqs-2019-009484","","",,,,,41,13.67,41,1,3,"Artificial intelligence (AI) has enormous potential to improve the safety of healthcare, from increasing diagnostic accuracy,1 to optimising treatment planning,2 to forecasting outcomes of care.3 However, integrating AI technologies into the delivery of healthcare is likely to introduce a range of new risks and amplify existing ones. For instance, failures in widely used software have the potential to quickly affect large numbers of patients4; hidden assumptions in underlying data and models can lead to AI systems delivering dangerous recommendations that are insensitive to local care processes,5 6 and opaque AI techniques such as deep learning can make explaining and learning from failure extremely difficult.7 8 To maximise the benefits of AI in healthcare and to build trust among patients and practitioners, it will therefore be essential to robustly govern the risks that AI poses to patient safety.  In a recent review in this journal, Challen and colleagues present an important and timely analysis of some of the key technological risks associated with the application of machine learning in clinical settings.9 Machine learning is a subfield of AI that focuses on the development of algorithms that are automatically derived and optimised through exposure to large quantities of exemplar ‘training’ data.10 The outputs of machine learning algorithms are essentially classifications of patterns that provide some sort of prediction—for instance, predicting whether an image shows a malignant melanoma or a benign mole.11 Some of the basic techniques of machine learning have existed for half a century or more, but progress in the field has accelerated rapidly due to advances in the development of ‘deep’ artificial neural networks12 combined with huge increases in computational power and the availability of enormous quantities of data. These techniques have underpinned recent public demonstrations of AI systems …","",""
37,"C. Kulikowski","Beginnings of Artificial Intelligence in Medicine (AIM): Computational Artifice Assisting Scientific Inquiry and Clinical Art – with Reflections on Present AIM Challenges",2019,"","","","",162,"2022-07-13 09:31:46","","10.1055/s-0039-1677895","","",,,,,37,12.33,37,1,3,"Summary Background : The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970’s led to a major change in the paradigm that affected all of artificial intelligence (AI) research. Since then, AI has evolved, surviving several “winters”, as it has oscillated between relying on expensive and hard-to-validate knowledge-based approaches, and the alternative of using machine learning methods for inferring classification rules from labelled datasets. In the past couple of decades, we are seeing a gradual but progressive intertwining of the two. Objectives : To give an overview of early directions in AI in medicine and threads of some subsequent developments motivated by the very different goals of scientific inquiry for biomedical research, and for computational modeling of clinical reasoning and more general healthcare problem solving from the perspective of today’s “AI-Deep Learning Boom”. To show how, from the beginning, AI was central to Biomedical and Health Informatics (BMHI), as a field investigating how to understand intelligent thinking in dealing professionally with the practice for healthcare, developing mathematical models, technology, and software tools to aid human experts in biomedicine, despite many previous bouts of “exuberant optimism” about the methodologies deployed. Methods : An overview and commentary on some of the early research and publications in AI in biomedicine, emphasizing the different approaches to the modeling of problems involved in clinical practice in contrast to those of biomedical science. A concluding reflection of a few current challenges and pitfalls of AI in some biomedical applications. Conclusion : While biomedical knowledge-based systems played a critical role in influencing AI in its early days, 50 years later they have taken a back seat behind “Deep Learning” which promises to discover knowledge structures for inference and prediction, both in science and for clinical decision-support. Early work on AI for medical consultation turned out to be more useful for explanation and teaching than for clinical practice, as had been originally intended. Today, despite the many reported successes of deep learning, fundamental scientific challenges arise in drawing on models of brain science, cognition, and language, if AI is to augment and complement rather than replace human judgment and expertise in biomedicine while also incorporating these advances for translational medicine. Understanding clinical phenotypes and how they relate to precision and personalization of care requires not only scientific inquiry, but also humanistic models of treatment that respond to patient and practitioner narrative exchanges, since it is the stories and insights of human experts which encourage what Norbert Weiner termed the ethical “human use of human beings”, so central to adherence to the Hippocratic Oath","",""
35,"J. Shapey, Guotai Wang, R. Dorent, A. Dimitriadis, Wenqi Li, I. Paddick, N. Kitchen, S. Bisdas, S. Saeed, S. Ourselin, R. Bradford, Tom Kamiel Magda Vercauteren","An artificial intelligence framework for automatic segmentation and volumetry of vestibular schwannomas from contrast-enhanced T1-weighted and high-resolution T2-weighted MRI.",2019,"","","","",163,"2022-07-13 09:31:46","","10.3171/2019.9.JNS191949","","",,,,,35,11.67,4,12,3,"OBJECTIVE Automatic segmentation of vestibular schwannomas (VSs) from MRI could significantly improve clinical workflow and assist in patient management. Accurate tumor segmentation and volumetric measurements provide the best indicators to detect subtle VS growth, but current techniques are labor intensive and dedicated software is not readily available within the clinical setting. The authors aim to develop a novel artificial intelligence (AI) framework to be embedded in the clinical routine for automatic delineation and volumetry of VS.   METHODS Imaging data (contrast-enhanced T1-weighted [ceT1] and high-resolution T2-weighted [hrT2] MR images) from all patients meeting the study's inclusion/exclusion criteria who had a single sporadic VS treated with Gamma Knife stereotactic radiosurgery were used to create a model. The authors developed a novel AI framework based on a 2.5D convolutional neural network (CNN) to exploit the different in-plane and through-plane resolutions encountered in standard clinical imaging protocols. They used a computational attention module to enable the CNN to focus on the small VS target and propose a supervision on the attention map for more accurate segmentation. The manually segmented target tumor volume (also tested for interobserver variability) was used as the ground truth for training and evaluation of the CNN. We quantitatively measured the Dice score, average symmetric surface distance (ASSD), and relative volume error (RVE) of the automatic segmentation results in comparison to manual segmentations to assess the model's accuracy.   RESULTS Imaging data from all eligible patients (n = 243) were randomly split into 3 nonoverlapping groups for training (n = 177), hyperparameter tuning (n = 20), and testing (n = 46). Dice, ASSD, and RVE scores were measured on the testing set for the respective input data types as follows: ceT1 93.43%, 0.203 mm, 6.96%; hrT2 88.25%, 0.416 mm, 9.77%; combined ceT1/hrT2 93.68%, 0.199 mm, 7.03%. Given a margin of 5% for the Dice score, the automated method was shown to achieve statistically equivalent performance in comparison to an annotator using ceT1 images alone (p = 4e-13) and combined ceT1/hrT2 images (p = 7e-18) as inputs.   CONCLUSIONS The authors developed a robust AI framework for automatically delineating and calculating VS tumor volume and have achieved excellent results, equivalent to those achieved by an independent human annotator. This promising AI technology has the potential to improve the management of patients with VS and potentially other brain tumors.","",""
31,"T. Ertekin, Qian Sun","Artificial Intelligence Applications in Reservoir Engineering: A Status Check",2019,"","","","",164,"2022-07-13 09:31:46","","10.3390/EN12152897","","",,,,,31,10.33,16,2,3,"This article provides a comprehensive review of the state-of-art in the area of artificial intelligence applications to solve reservoir engineering problems. Research works including proxy model development, artificial-intelligence-assisted history-matching, project design, and optimization, etc. are presented to demonstrate the robustness of the intelligence systems. The successes of the developments prove the advantages of the AI approaches in terms of high computational efficacy and strong learning capabilities. Thus, the implementation of intelligence models enables reservoir engineers to accomplish many challenging and time-intensive works more effectively. However, it is not yet astute to completely replace the conventional reservoir engineering models with intelligent systems, since the defects of the technology cannot be ignored. The trend of research and industrial practices of reservoir engineering area would be establishing a hand-shaking protocol between the conventional modeling and the intelligent systems. Taking advantages of both methods, more robust solutions could be obtained with significantly less computational overheads.","",""
32,"J. Bali, R. Garg, R. Bali","Artificial intelligence (AI) in healthcare and biomedical research: Why a strong computational/AI bioethics framework is required?",2019,"","","","",165,"2022-07-13 09:31:46","","10.4103/ijo.IJO_1292_18","","",,,,,32,10.67,11,3,3,"Artificial intelligence (AI) refers to a computer mimicking “intellectual processes characteristic of humans, such as the ability to reason, discover meaning, generalize, or learn from past experience” to achieve goals without being explicitly programmed for specific action. There is no consensus on what constitutes AI. Different criteria for intelligence proposed have not satisfied everyone leading to the famous aphorism, “AI is whatever hasn’t been done yet.” For example, optical character recognition and translation has now been relegated from “artificial intelligence” because of the routine nature of their use.[1,2]","",""
10,"R. Smith","Idealizations of Uncertainty, and Lessons from Artificial Intelligence",2016,"","","","",166,"2022-07-13 09:31:46","","10.5018/ECONOMICS-EJOURNAL.JA.2016-7","","",,,,,10,1.67,10,1,6,"Abstract At a time when economics is giving intense scrutiny to the likely impact of artificial intelligence (AI) on the global economy, this paper suggests the two disciplines face a common problem when it comes to uncertainty. It is argued that, despite the enormous achievements of AI systems, it would be a serious mistake to suppose that such systems, unaided by human intervention, are as yet any nearer to providing robust solutions to the problems posed by Keynesian uncertainty. Under the radically uncertain conditions, human decision-making (for all its problems) has proved relatively robust, while decision making relying solely on deterministic rules or probabilistic models is bound to be brittle. AI remains dependent on techniques that are seldom seen in human decision-making, including assumptions of fully enumerable spaces of future possibilities, which are rigorously computed over, and extensively searched. Discussion of alternative models of human decision making under uncertainty follows, suggesting a future research agenda in this area of common interest to AI and economics.","",""
22,"R. Dash, Mark E. McMurtrey, C. Rebman, U. Kar","Application of Artificial Intelligence in Automation of Supply Chain Management",2019,"","","","",167,"2022-07-13 09:31:46","","10.33423/JSIS.V14I3.2105","","",,,,,22,7.33,6,4,3,"A well-functioning supply chain is a key to success for every business entity. Having an accurate projection on inventory offers a substantial competitive advantage. There are many internal factors like product introductions, distribution network expansion; and external factors such as weather, extreme seasonality, and changes in customer perception or media coverage that affects the performance of the supply chain. In recent years Artificial Intelligence (AI) has been proved to become an extension of our brain, expanding our cognitive abilities to levels that we never thought would be possible. Though many believe AI will replace humans, it is not true, rather it will help us to unleash our true strategic and creative potential. AI consists of a set of computational technologies developed to sense, learn, reason, and act appropriately. With the technological advancement in mobile computing, the capacity to store huge data on the internet, cloud-based machine learning and information processing algorithms etc. AI has been integrated into many sectors of business and been proved to reduce costs, increase revenue, and enhance asset utilization. AI is helping businesses to get almost 100% accurate projection and forecast the customer demand, optimizing their R&D and increase manufacturing with lower cost and higher quality, helping them in the promotion (identifying target customers, demography, defining the price, and designing the right message, etc.) and providing their customers a better experience. These four areas of value creation are extremely important for gaining competitive advantage. Supply-chain leaders use AI-powered technologies to a) make efficient designs to eliminate waste b) real-time monitoring and error-free production and c) facilitate lower process cycle times. These processes are crucial in bringing Innovation faster to the market.","",""
42,"George Gadanidis","Artificial intelligence, computational thinking, and mathematics education",2017,"","","","",168,"2022-07-13 09:31:46","","10.1108/IJILT-09-2016-0048","","",,,,,42,8.40,42,1,5,"Purpose          The purpose of this paper is to examine the intersection of artificial intelligence (AI), computational thinking (CT), and mathematics education (ME) for young students (K-8). Specifically, it focuses on three key elements that are common to AI, CT and ME: agency, modeling of phenomena and abstracting concepts beyond specific instances.          Design/methodology/approach          The theoretical framework of this paper adopts a sociocultural perspective where knowledge is constructed in interactions with others (Vygotsky, 1978). Others also refers to the multiplicity of technologies that surround us, including both the digital artefacts of our new media world, and the human methods and specialized processes acting in the world. Technology is not simply a tool for human intention. It is an actor in the cognitive ecology of immersive humans-with-technology environments (Levy, 1993, 1998) that supports but also disrupts and reorganizes human thinking (Borba and Villarreal, 2005).          Findings          There is fruitful overlap between AI, CT and ME that is of value to consider in mathematics education.          Originality/value          Seeing ME through the lenses of other disciplines and recognizing that there is a significant overlap of key elements reinforces the importance of agency, modeling and abstraction in ME and provides new contexts and tools for incorporating them in classroom practice.","",""
15,"R. Mittu, D. Sofge, Alan R. Wagner, W. Lawless","Robust Intelligence and Trust in Autonomous Systems",2016,"","","","",169,"2022-07-13 09:31:46","","10.1007/978-1-4899-7668-0","","",,,,,15,2.50,4,4,6,"","",""
6,"A. Kanta, G. Montavon, M. Planche, C. Coddet","Artificial Intelligence Computation to Establish Relationships Between APS Process Parameters and Alumina–Titania Coating Properties",2008,"","","","",170,"2022-07-13 09:31:46","","10.1007/S11090-007-9116-9","","",,,,,6,0.43,2,4,14,"","",""
5,"I. Timm, Steffen Staab, M. Siebers, C. Schon, Ute Schmid, Kai Sauerwald, Lukas Reuter, Marco Ragni, C. Niederée, H. Maus, G. Kern-Isberner, Christian Jilek, Paulina Friemann, Thomas Eiter, A. Dengel, Hannah Dames, Tanja Bock, J. Berndt, C. Beierle","Intentional Forgetting in Artificial Intelligence Systems: Perspectives and Challenges",2018,"","","","",171,"2022-07-13 09:31:46","","10.1007/978-3-030-00111-7_30","","",,,,,5,1.25,1,19,4,"","",""
83,"S. D’Alfonso, Olga Santesteban-Echarri, S. Rice, G. Wadley, R. Lederman, C. Miles, J. Gleeson, M. Alvarez-Jimenez","Artificial Intelligence-Assisted Online Social Therapy for Youth Mental Health",2017,"","","","",172,"2022-07-13 09:31:46","","10.3389/fpsyg.2017.00796","","",,,,,83,16.60,10,8,5,"Introduction: Benefits from mental health early interventions may not be sustained over time, and longer-term intervention programs may be required to maintain early clinical gains. However, due to the high intensity of face-to-face early intervention treatments, this may not be feasible. Adjunctive internet-based interventions specifically designed for youth may provide a cost-effective and engaging alternative to prevent loss of intervention benefits. However, until now online interventions have relied on human moderators to deliver therapeutic content. More sophisticated models responsive to user data are critical to inform tailored online therapy. Thus, integration of user experience with a sophisticated and cutting-edge technology to deliver content is necessary to redefine online interventions in youth mental health. This paper discusses the development of the moderated online social therapy (MOST) web application, which provides an interactive social media-based platform for recovery in mental health. We provide an overview of the system's main features and discus our current work regarding the incorporation of advanced computational and artificial intelligence methods to enhance user engagement and improve the discovery and delivery of therapy content. Methods: Our case study is the ongoing Horyzons site (5-year randomized controlled trial for youth recovering from early psychosis), which is powered by MOST. We outline the motivation underlying the project and the web application's foundational features and interface. We discuss system innovations, including the incorporation of pertinent usage patterns as well as identifying certain limitations of the system. This leads to our current motivations and focus on using computational and artificial intelligence methods to enhance user engagement, and to further improve the system with novel mechanisms for the delivery of therapy content to users. In particular, we cover our usage of natural language analysis and chatbot technologies as strategies to tailor interventions and scale up the system. Conclusions: To date, the innovative MOST system has demonstrated viability in a series of clinical research trials. Given the data-driven opportunities afforded by the software system, observed usage patterns, and the aim to deploy it on a greater scale, an important next step in its evolution is the incorporation of advanced and automated content delivery mechanisms.","",""
107,"Nicholas Ernest, David Carroll, C. Schumacher, M. Clark, Kelly Cohen, Gene Lee","Genetic Fuzzy based Artificial Intelligence for Unmanned Combat Aerial Vehicle Control in Simulated Air Combat Missions",2016,"","","","",173,"2022-07-13 09:31:46","","10.4172/2167-0374.1000144","","",,,,,107,17.83,18,6,6,"Breakthroughs in genetic fuzzy systems, most notably the development of the Genetic Fuzzy Tree methodology, have allowed fuzzy logic based Artificial Intelligences to be developed that can be applied to incredibly complex problems. The ability to have extreme performance and computational efficiency as well as to be robust to uncertainties and randomness, adaptable to changing scenarios, verified and validated to follow safety specifications and operating doctrines via formal methods, and easily designed and implemented are just some of the strengths that this type of control brings. Within this white paper, the authors introduce ALPHA, an Artificial Intelligence that controls flights of Unmanned Combat Aerial Vehicles in aerial combat missions within an extreme-fidelity simulation environment. To this day, this represents the most complex application of a fuzzy-logic based Artificial Intelligence to an Unmanned Combat Aerial Vehicle control problem. While development is on-going, the version of ALPHA presented withinwas assessed by Colonel (retired)Gene Lee who described ALPHA as “the most aggressive, responsive, dynamic and credible AI (he’s) seen-to-date.” The quality of these preliminary results in a problem that is not only complex and rife with uncertainties but also contains an intelligent and unrestricted hostile force has significant implications for this type of Artificial Intelligence. This work adds immensely to the body of evidence that this methodology is an ideal solution to a very wide array of problems.","",""
189,"Lawrence B. Solum","Legal Personhood for Artificial Intelligences",2008,"","","","",174,"2022-07-13 09:31:46","","10.4324/9781003074991-37","","",,,,,189,13.50,189,1,14,"Could an artificial intelligence become a legal person? As of today, this question is only theoretical. No existing computer program currently possesses the sort of capacities that would justify serious judicial inquiry into the question of legal personhood. The question is nonetheless of some interest. Cognitive science begins with the assumption that the nature of human intelligence is computational, and therefore, that the human mind can, in principle, be modelled as a program that runs on a computer. Artificial intelligence (AI) research attempts to develop such models. But even as cognitive science has displaced behavioralism as the dominant paradigm for investigating the human mind, fundamental questions about the very possibility of artificial intelligence continue to be debated. This Essay explores those questions through a series of thought experiments that transform the theoretical question whether artificial intelligence is possible into legal questions such as, ""Could an artificial intelligence serve as a trustee?"" What is the relevance of these legal thought experiments for the debate over the possibility of artificial intelligence? A preliminary answer to this question has two parts. First, putting the AI debate in a concrete legal context acts as a pragmatic Occam's razor. By reexamining positions taken in cognitive science or the philosophy of artificial intelligence as legal arguments, we are forced to see them anew in a relentlessly pragmatic context. Philosophical claims that no program running on a digital computer could really be intelligent are put into a context that requires us to take a hard look at just what practical importance the missing reality could have for the way we speak and conduct our affairs. In other words, the legal context provides a way to ask for the ""cash value"" of the arguments. The hypothesis developed in this Essay is that only some of the claims made in the debate over the possibility of AI do make a pragmatic difference, and it is pragmatic differences that ought to be decisive. Second, and more controversially, we can view the legal system as a repository of knowledge-a formal accumulation of practical judgments. The law embodies core insights about the way the world works and how we evaluate it. Moreover, in common-law systems judges strive to decide particular cases in a way that best fits the legal landscape-the prior cases, the statutory law, and the constitution. Hence, transforming the abstract debate over the possibility of AI into an imagined hard case forces us to check our intuitions and arguments against the assumptions that underlie social decisions made in many other contexts. By using a thought experiment that explicitly focuses on wide coherence, we increase the chance that the positions we eventually adopt will be in reflective equilibrium with our views about related matters. In addition, the law embodies practical knowledge in a form that is subject to public examination and discussion. Legal materials are published and subject to widespread public scrutiny and discussion. Some of the insights gleaned in the law may clarify our approach to the artificial intelligence debate.","",""
3,"Mengwei Liu, Yujia Zhang, Jiachuang Wang, N. Qin, Heng Yang, Ke Sun, Jie Hao, L. Shu, Jiarui Liu, Qiang Chen, Pingping Zhang, T. Tao","A star-nose-like tactile-olfactory bionic sensing array for robust object recognition in non-visual environments",2022,"","","","",175,"2022-07-13 09:31:46","","10.1038/s41467-021-27672-z","","",,,,,3,3.00,0,12,1,"","",""
0,"","ACTIVITY REPORT Project-Team Models and Algorithms for Artiﬁcial Intelligence",2022,"","","","",176,"2022-07-13 09:31:46","","","","",,,,,0,0.00,0,0,1,"The expectation-maximization (EM) algorithm is a powerful computational technique for maximum likelihood estimation in incomplete data models. When the expectation step cannot be performed in closed form, a stochastic approximation of EM (SAEM) can be used. The convergence of the SAEM toward critical points of the observed likelihood has been proved and its numerical efﬁciency has been demonstrated. However, sampling from the posterior distribution may be intractable or have a high computational cost. Moreover, despite appealing features, the limit position of this algorithm can strongly depend on its starting one. To cope with this two issues, we propose in [11] new stochastic approximation version of the EM in which we do not sample from the exact distribution in the expectation phase of the procedure. We ﬁrst prove the convergence of this algorithm toward critical points of the observed likelihood. Then, we propose an instantiation of this general procedure to favor convergence toward global maxima. Experiments on synthetic and real data highlight the performance of this algorithm in comparison to the SAEM and the EM when feasible. of subject-speciﬁc weights characterizing partial membership across clusters. With this ﬂexibility come challenges in uniquely identifying, estimating, and interpreting the parameters. In [40], we propose a new class of Dimension-Grouped MMMs (Gro-M 3 s) for multivariate categorical data, which improve parsimony and interpretability. In Gro-M 3 s, observed variables are partitioned into groups such that the latent membership is constant for variables within a group but can differ across groups. Traditional latent class models are obtained when all variables are in one group, while traditional MMMs are obtained when each variable is in its own group. The new model corresponds to a novel decomposition of probability tensors. Theoretically, we derive transparent identiﬁability conditions for both the unknown grouping structure and model parameters in general settings. Methodologically, we propose a Bayesian approach for Dirichlet Gro-M3 s to inferring the variable grouping structure and estimating model parameters. Simulation results demonstrate good computational performance and empirically conﬁrm the identiﬁability results. We illustrate the new methodology through an application to a functional disability dataset. from this natural partition. In a Bayesian context, this is achieved by considering the Dirichlet cluster proportion prior parameter α as a regularisation term controlling the granularity of the clustering. This second step allows the exploration of the clustering at coarser scales and the ordering of the clusters an important output for the visual representations of the clustering results. The clustering results obtained with the proposed approach, on simulated as well as real settings, are compared with existing strategies and are shown to be particularly relevant. This work is implemented in the R package greed and Figure 2 illustrates the main idea of the method. In this applied work [19], we use the Fisher-EM algorithm for clustering for the unsupervised classiﬁcation of 702, 248 spectra of galaxies and quasars with resdshifts smaller than 0.25 that were retrieved from the Sloan Digital Sky Survey (SDSS) database, release 7. The spectra were ﬁrst corrected for the redshift, then wavelet-ﬁltered to reduce the noise, and ﬁnally binned to obtain about 1437 wavelengths per spectrum. Fisher-EM, an unsupervised clustering discriminative latent mixture model algorithm, was applied on these corrected spectra, considering the full set as well as several subsets of 100,000 and 300,000 spectra. The optimum number of classes given by a penalized likelihood criterion is 86 classes, the 37 most populated ones gathering 99% of the sample. These classes are established from a subset of 302144 spectra. Using several cross-validation techniques we ﬁnd that this classiﬁcation is in agreement with the results obtained on the other subsets with an average misclassiﬁcation error of about 15%. The large number of very small classes tends to increase this error rate. This is the ﬁrst time that an automatic, objective and robust unsupervised classiﬁcation is established on such a large amount of spectra of galaxies. The mean spectra of the classes can be used as templates for a large majority of galaxies in our Universe. Figure 7 illustrates the obtained results. Recurrent Neural Networks, Deep linguistic patterns the of a of of to the is this linguistic that becomes valuable for our descriptive approach through deep as it allows us to observe complex lexico-grammatical structures, that potentially associate several levels of text representation in the same structure. The convolutional model used until now must therefore be adapted to integrate this additional information in order to obtain an even ﬁner description of the textual salience of a corpus. the relevant features used by the CNN to perform the classiﬁcation task. We empirically demonstrate the efﬁciency of our approach on corpora from two different languages: English and French. On all datasets, wTDS automatically encodes complex linguistic objects based on co-occurrences and possibly on grammatical and syntax analysis. relationships between the concepts in the metadata by analyzing the contrast between the concepts similarities in the Joconde’s semantic model and other vocabularies and we tried to improve the model prediction scores based on the semantic relations. Our results show that cross-fertilization between symbolic AI and machine learning can indeed provide the tools to address the challenges of the museum curators work describing the artwork pieces and searching for the relevant images. that combines a geometric approach for decision rules with existing post hoc solutions for machine learning models to generate an intuitive feature ranking tailored to the end user. We show that established model-agnostic approaches produce poor results in this framework. Figure 13 illustrates this work. Algorithms involving Gaussian processes or determinantal point processes typically require computing the determinant of a kernel matrix. Frequently, the latter is computed from the Cholesky decomposition, an algorithm of cubic complexity in the size of the matrix. We show that, under mild assumptions, it is possible to estimate the determinant from only a sub-matrix, with probabilistic guarantee on the relative error. In [37], we present an augmentation of the Cholesky decomposition that stops under certain conditions before processing the whole matrix. Experiments demonstrate that this can save a considerable amount of time while having an overhead of less than 5% when not stopping early. More generally, we present a probabilistic stopping strategy for the approximation of a sum of known length where addends are revealed sequentially. We do not assume independence between addends, only that they are bounded from below and decrease in conditional expectation. of there is a signiﬁcant from combining and audio data in detecting active speakers. either of the modalities can potentially mislead audiovisual fusion by inducing unreliable or deceptive information. outlines active speaker detection as a multi-objective learning problem to leverage best of each modalities using a novel self-attention, uncertainty-based multimodal fusion scheme. Results obtained show that the proposed multi-objective learning architecture outperforms traditional approaches in improving both mAP and AUC scores. We further demonstrate that our fusion strategy surpasses, in active speaker detection, other modality fusion methods reported in various disciplines. We ﬁnally show that the proposed method signiﬁcantly improves the state-of-the-art on the AVA-ActiveSpeaker dataset. This paper explores the problem of summarizing professional soccer matches as automatically as possible using both the event-stream data collected from the ﬁeld and the content broadcasted on TV. We have designed an architecture, introducing ﬁrst (1) a Multiple Instance Learning method that takes into account the sequential dependency among events and then (2) a hierarchical multimodal attention layer that grasps the importance of each event in an action [31]. We evaluate our approach on matches from two professional European soccer leagues, showing its capability to identify the best actions for automatic summarization by comparing with real summaries made by human operators. Figure 18 illustrates the general schema of the approach. We a coherent framework for studying longitudinal manifold-valued data. We introduce a Bayesian mixed-effects model which allows estimating both a group-representative piecewise-geodesic creating clusters of similar sentences. The ideal practice is to obtain a cluster with only positive blocks and another with only negative ones. Comparing to the supervised approach (Bag of words + Logistic Regression Classiﬁer) with its f1-score as 0.8234 and f2-score as 0.8316, we found that both S-Bert [58] (with a f1-score of 0.6250 and f2-score of 0.6192) and BioBert [57] (f1-score as 0.7004 and f2 as 0.6955) can achieves relatively good results and latter even outperformed the former due to its domain speciﬁc knowledge. around 13 billion euros per year to European citizens [52]. In the ﬁeld of healthcare insurance, in France the compulsory scheme detected over 261.2 million euros of fraudulent services in 2018, mainly due to healthcare professionals and healthcare establishments [50]. In the United States, according to the FBI, medicare fraud costs insurance companies between 21 billion and 71 billion US dollars per year [55]. In a context where reducing management costs is a real issue for healthcare insurers, the ﬁght against fraud is a real expectation of the customers of professionals in the sector so that everyone receives a fair return for their contributions. This stud","",""
0,"Mr. Nusrath Khan, M. P. Mishra","The Impact of Empathic Algorithms on Artificial Intelligence",2018,"","","","",177,"2022-07-13 09:31:46","","","","",,,,,0,0.00,0,2,4,"Intelligence links perception to action to help an organism live. Intelligence is working out in the service of life, just as metabolism is chemistry in the service of life. Intelligence does not infer perfect understanding; every intelligent being has limited perception, memory, and computation. Many opinions on the spectrum of intelligence-versus-cost are viable, from insects to humans. The implications of autonomous symmetries have been far-reaching and pervasive. In this work, we show the visualization of redundancy. We construct an analysis of flip-flop gates, which we call HEAL. Keywords-Spectrum,autonomous symmetries, visualization, flip-flop gates. I . I NT R O D U CT I O N Many hackers worldwide would agree that, had it not been for SMPs, the exploration of systems might never have occurred. While existing solutions to this obstacle are outdated, none have taken the embedded solution we propose here. To put this in perspective, consider the fact that much-touted security experts largely use extreme programming to fulfill this aim. The study of courseware would tremendously amplify trainable symmetries. We motivate a novel algorithm for the refinement of virtual machines, which we call HEAL. for ex-ample, many frameworks visualize classical information. Existing psychoacoustic and self-learning sys-tems use the transistor to refine A* search [13]. We view programming languages as following a cycle of four phases: development, exploration, visualization, and study. Combined with flexible epistemologies, it develops new game-theoretic epistemologies. We question the need for certifiable theory. The flaw of this type of approach, however, is that the transistor can be made permutable, ubiquitous, and reliable. Existing robust and permutable frameworks use the analysis of 802.11 mesh networks to visualize the visualization of A* search. We view e-voting technology as following a cycle of four phases: development, construction, visualization, and provision. By comparison, for example, many methodologies emulate linked lists. However, virtual methodologies might not be the panacea that information theorists expected. Here, we make four main contributions. We motivate new optimal configurations (HEAL), demonstrating that the little-known homogeneous algorithm for the refinement of model checking by Ed-ward Feigenbaum et al. [14] is NPcomplete. Furthermore, we use concurrent symmetries to demonstrate that the seminal cooperative algorithm for the development of virtual machines by Jackson[13] is re-cursively enumerable. We verify that systems can be made game-theoretic, unstable, and constant-time. Lastly, we investigate how erasure coding can be applied to the simulation of journaling file systems. The rest of this paper is organized as follows. First, we motivate the need for forward-error correction. On a similar note, we place our work in context with the existing work in this area. To realize this purpose, we show not only that Byzantine fault tolerance [18] can be made psychoacoustic, adaptive, and event-driven, but that the same is true for compilers. Finally, we conclude. II. MOTIVATION Though we are the first to explore DNS in this light, much related work has been devoted to the analysis of spreadsheets that would allow for further study into SCSI disks [19]. Similarly, Andy Tanenbaum et al. [20] developed a similar system, nevertheless we proved that our algorithm runs in Θ(n2) time [9]. HEAL is broadly related to work in the field of cryptography by Y. Shastri et al., but we view it from a new perspective: “smart” information. Our method is broadly related to work in the field of complexity theory by White et al., but we view it from a new perspective: the emulation of access points [23]. A major source of our inspiration is early work by I. L. Maruyama et al. [1] on active networks [16,21]. We believe there is room for both schools of thought within the field of algorithms. Wilson [12] suggested a scheme for architecting scatter/gather I/O, but did not fully realize the implications of adaptive theory at the time. This method is less cheap than ours. Furthermore, Charles Leiserson et al. constructed several electronic methods, and reported that International Journal of Innovations in Engineering and Technology (IJIET) http://dx.doi.org/10.21172/ijiet.102.23 Volume 10 Issue 2 May 2018 152 ISSN: 2319-1058 A R they have great lack of influence on game-theoretic theory [15]. Furthermore, though Sun and Sato also proposed this approach, we visualized it independently and simultaneously [5]. Thus, despite substantial work in this area, our method is evidently the framework of choice among analysts [28]. Our approach is related to research into highly-available epistemologies, unstable modalities, and public-private key pairs. It remains to be seen how valuable this research is to the cryptanalysis community. Similarly, instead of investigating thin clients, we surmount this issue simply by enabling RPCs. On the other hand, the complexity of their solution grows inversely as active networks grow. Continuing with this rationale, the choice of B-trees in [22] diff ers from ours in that we investigate only intuitive communication in our system. Wang and White [25] suggested a scheme for emulating the improvement of the UNIVAC computer, but did not fully realize the implications of game-theoretic modalities at the time. However, the complexity of their approach grows quadratically as collaborative technology grows. Clearly, despite substantial work in this area, our solution is perhaps the framework of choice among security experts. III. PROPOSED METHODOLOGY Motivated by the need for Byzantine fault tolerance, we now motivate an architecture for confirming that the Ethernet and multicast heuristics are continuously incompatible. We postulate that Scheme and compilers can collaborate to accomplish this mission [24]. Continuing with this rationale, we consider an algorithm consisting of n checksums. Similarly, we instrumented a 2-year-long trace demonstrating that our methodology is solidly grounded in reality. This seems to hold in most cases.","",""
27,"Gaurav Sharma, Alexis B. Carter","Artificial Intelligence and the Pathologist: Future Frenemies?",2017,"","","","",178,"2022-07-13 09:31:46","","10.5858/arpa.2016-0593-ED","","",,,,,27,5.40,14,2,5,"The manuscript titled ‘‘AlphaGo, deep learning, and the future of the human microscopist’’ in this month’s issue of the Archives of Pathology & Laboratory Medicine describes the triumph of Google’s (Mountain View, California) artificial intelligence (AI) program, AlphaGo, which beat the 18-time world champion of Go, an ancient Chinese board game far more complex than chess. The authors have hypothesized that the development of intuition and creativity combined with the raw computing of AI heralds an age where well-designed and well-executed AI algorithms can solve complex medical problems, including the interpretation of diagnostic images, thereby replacing the microscopist. Of note, in a prior work, the microscope was predicted to have a 75% chance of remaining in use for another 144 years. To support their hypothesis, the authors presented recent studies that compared the performance of nontraditional interpreters to those of experienced pathologists, in making accurate diagnoses (note: 1 author disclosed a significant financial interest in an AI company). One study examined the potential of using pigeons (yes, pigeons) for medical image studies, wherein the pigeons engaged in a matching game of completely benign and unambiguously malignant breast histology images. Pigeons correctly classified images as benign or malignant 85% of the time. A separate image algorithm study was erroneously reported to differentiate between small cell and non–small cell lung carcinoma with the accuracy of expert pulmonary pathologists, but instead, multiple computational algorithms were used to subtype known non–small cell lung carcinomas and gliomas in separate experiments. The accuracy rate of each algorithm approached 70% to 85%. We believe that this level of diagnostic accuracy in settings that lack complexity is an extremely poor replica of a human pathologist’s diagnostic capabilities. So, will the data-digesting and 24 3 7 learning AI be capable of looking at an image and able to render a pathologic diagnosis? Before attempting to answer this, we caution against the difficulties of predicting the future. Much of our existence still rests on innovations that have remained unchanged because of their inherent simplicity, applicability, and trueness to purpose (eg, the wheel), proving the point that something new (and different) is not always something better. On the other hand, several established and incumbent technologies were quickly (albeit incompletely) eclipsed, often within a decade, by a challenger that was faster, more convenient, cheaper, or better for the need (eg, postal mail being replaced by electronic mail). In the latter context, we note that information technology and AI are clearly better at repetitive detailed tasks that require accuracy and speed than are humans, who often find such tasks mind-numbing, and consequently are error-prone.","",""
50,"M. Hashemi, M. Spaulding, Alex Shaw, H. Farhadi, Matt J. Lewis","An efficient artificial intelligence model for prediction of tropical storm surge",2016,"","","","",179,"2022-07-13 09:31:46","","10.1007/s11069-016-2193-4","","",,,,,50,8.33,10,5,6,"","",""
1,"Massoud Sokouti, B. Sokouti","Applying the Science of Systematic Review and Meta-Analysis to Retrospective Artificial Intelligence Based Studies: The importance of performance evaluation",2019,"","","","",180,"2022-07-13 09:31:46","","","","",,,,,1,0.33,1,2,3,"The rationale behind the meta-analysis goes back to the 17th century studies of astronomy which then Karl Pearson performed a study based on meta-analysis using the data for typhoid inoculation in 1904. After, William Cochran applied this type of analysis to medical researches by taking the advantage of multiple previous studies. For more information and details on the history, the readers are referred to. To emerge the important role of systematic and metaanalysis studies even in the area of artificial intelligence systems, it is an anticipated that more reliable results can be driven from previous research studies alongside a simple review of such studies from which most of them may be ignored or not included as a matter of their nonsystematical type of reviews. The meta-analysis technique uses various types of statistics tools and methodologies to commonly derive a predictive diagnostic or non-diagnostic performance result of their compared corresponding approaches on the target defined disorders using information included in different datasets of previous studies. Although, a meta-analysis study can be regarded as a review of previous studies, however, it thoroughly targets not only the achieving results of those studies but also determine the in-common and non-commonpatterns of those researches as well as biases of the performance results whether they have been inserted intentionally or unintentionally. The importance of meta-analysis has been vastly discussed in medical sciences and therefore, been conducted rigorously through various studies, mostly on clinical trial ones. However, this technique is one of those less valued tools imported in to biomedical engineering studies and hence, their related algorithms mostly on the performance of artificial intelligence approaches. One of those studies to mention is the one performed on classification algorithms for pattern recognition by So Young Sohn in 1999 based on some in-house implemented statistics tools without considering the meta-analysis software. Moreover, in 2015,Horn et al have conducted a systematic review on functional brain imaging studies on assessing the familiarity of artificial neural networks and discussed their pros and cons in terms of their experimental conflicting results based on a meta-analysis on 68 publishedarticles. In another recent study, the role of real-time biomedical systems has been evaluated by a meta-analysis approach on 134 real-times papers in terms of computational complexity, delay and speed up considering various types of algorithms and hardware implementation. Recently, two types of systematic review and analysis have been performed which shows the potential non-mature trends of this approach in artificial intelligence based researches.In the first one the authors studied the performance of different machine learning algorithms for heart disease diagnosis; however, the metaanalysis part was not performed due to the existence of heterogeneity in the final included studies through the PRISMA (Preferred reporting items for systematic reviews and meta-analyses) checklist. And in the second one, the performance of several DNA based encryption algorithms based according to the results obtained from previous publications has been proposed where, it has been found out that there were no improvements in the proposed algorithms and it has been suggested that a dataset of images should be available in order to test and evaluate the performance of methodologies. However, the methodologies should also be available for public use. Moreover, the analyses section can be carried out through a simple statistical student’s t test analysisor the metaanalysis procedure using available tools such as MetaDisc, MIX, and Meta-Analyst. While comparing the two environments (i.e., clinical and computational), there are in-common units for decision making in diagnosing symptoms which are human (brain system and some data) and computer (artificial intelligence systems and some data). This outstanding feature and the abovementioned examples makes the meta-analysis studies applicable to the researches performed based on artificial intelligence systems, too. This will open a new view on interactions between the results obtained from previous studies while considering their special algorithms, different datasets, and possible biases. One more thing to emphasize for the future research studies is on publicizing the datasets and the implemented algorithms in terms of web servers, Java, C++ and Matlab libraries or R packages to make the results re-generable using new datasets which make them more comparable with new designed methodologies to ease the metaanalysis robust studies. As, it is also clear, most of the webservers and datasets in the medical parts coupled with data derived from bioscience knowledge are publicly.","",""
15,"Firas Safadi, R. Fonteneau, D. Ernst","Artificial Intelligence in Video Games: Towards a Unified Framework",2015,"","","","",181,"2022-07-13 09:31:46","","10.1155/2015/271296","","",,,,,15,2.14,5,3,7,"With modern video games frequently featuring sophisticated and realistic environments, the need for smart and comprehensive agents that understand the various aspects of complex environments is pressing. Since video game AI is often specifically designed for each game, video game AI tools currently focus on allowing video game developers to quickly and efficiently create specific AI. One issue with this approach is that it does not efficiently exploit the numerous similarities that exist between video games not only of the same genre, but of different genres too, resulting in a difficulty to handle the many aspects of a complex environment independently for each video game. Inspired by the human ability to detect analogies between games and apply similar behavior on a conceptual level, this paper suggests an approach based on the use of a unified conceptual framework to enable the development of conceptual AI which relies on conceptual views and actions to define basic yet reasonable and robust behavior. The approach is illustrated using two video games, Raven and StarCraft: Brood War.","",""
22,"Cameron E. Freer, Daniel M. Roy, J. Tenenbaum","Towards common-sense reasoning via conditional simulation: legacies of Turing in Artificial Intelligence",2012,"","","","",182,"2022-07-13 09:31:46","","10.1017/CBO9781107338579.007","","",,,,,22,2.20,7,3,10,"The problem of replicating the flexibility of human common-sense reasoning has captured the imagination of computer scientists since the early days of Alan Turing's foundational work on computation and the philosophy of artificial intelligence. In the intervening years, the idea of cognition as computation has emerged as a fundamental tenet of Artificial Intelligence (AI) and cognitive science. But what kind of computation is cognition?  We describe a computational formalism centered around a probabilistic Turing machine called QUERY, which captures the operation of probabilistic conditioning via conditional simulation. Through several examples and analyses, we demonstrate how the QUERY abstraction can be used to cast common-sense reasoning as probabilistic inference in a statistical model of our observations and the uncertain structure of the world that generated that experience. This formulation is a recent synthesis of several research programs in AI and cognitive science, but it also represents a surprising convergence of several of Turing's pioneering insights in AI, the foundations of computation, and statistics.","",""
7,"Tatiana Tambouratzis, J. Giannatsis, A. Kyriazis, Panayiotis Siotropos","Applying the Computational Intelligence Paradigm to Nuclear Power Plant Operation",2020,"","","","",183,"2022-07-13 09:31:46","","10.4018/ijeoe.2020010102","","",,,,,7,3.50,2,4,2,"In the guise of artificial neural networks (ANNs), genetic/evolutionary computation algorithms (GAs/ECAs), fuzzy logic (FL) inference systems (FLIS) and their variants as well as combinations, the computational intelligence (CI) paradigm has been applied to nuclear energy (NE) since the late 1980s as a set of efficient and accurate, non-parametric, robust-to-noise as well as to-missing-information, non-invasive on-line tools for monitoring, predicting and overall controlling nuclear (power) plant (N(P)P) operation. Since then, the resulting CI-based implementations have afforded increasingly reliable as well as robust performance, demonstrating their potential as either stand-alone tools, or - whenever more advantageous - combined with each other as well as with traditional signal processing techniques. The present review is focused upon the application of CI methodologies to the - generally acknowledged as - key-issues of N(P)P operation, namely: control, diagnostics and fault detection, monitoring, N(P)P operations, proliferation and resistance applications, sensor and component reliability, spectroscopy, fusion supporting operations, as these have been reported in the relevant primary literature for the period 1990-2015. At one end, 1990 constitutes the beginning of the actual implementation of innovative, and – at the same time – robust as well as practical, directly implementable in H/W, CI-based solutions/tools which have proved to be significantly superior to the traditional as well as the artificial-intelligence-(AI)derived methodologies in terms of operation efficiency as well as robustness-to-noise and/or otherwise distorted/missing information. At the other end, 2015 marks a paradigm shift in terms of the emergent (and, swiftly, ubiquitous) use of deep neural networks (DNNs) over existing ANN architectures and FL problem representations, thus dovetailing the increasing requirements of the era of complex - as well as Big - Data and forever changing the means of ANN/neuro-fuzzy construction and application/performance. By exposing the prevalent CI-based tools for each key-issue of N(P)P operation, overall as well as over time for the given 1990-2015 period, the applicability and optimal use of CI tools to NE problems is revealed, thus providing the necessary know-how concerning crucial decisions that need to be made for the increasingly efficient as well as safe exploitation of NE.","",""
7,"Alexander Lavin, H. Zenil, Brooks Paige, D. Krakauer, Justin Emile Gottschlich, T. Mattson, Anima Anandkumar, Sanjay Choudry, K. Rocki, A. G. Baydin, Carina Prunkl, O. Isayev, Erik J Peterson, P. McMahon, J. Macke, K. Cranmer, Jiaxin Zhang, H. Wainwright, A. Hanuka, M. Veloso, Samuel A. Assefa, Stephan Zheng, A. Pfeffer","Simulation Intelligence: Towards a New Generation of Scientific Methods",2021,"","","","",184,"2022-07-13 09:31:46","","","","",,,,,7,7.00,1,23,1,"The original""Seven Motifs""set forth a roadmap of essential methods for the field of scientific computing, where a motif is an algorithmic method that captures a pattern of computation and data movement. We present the""Nine Motifs of Simulation Intelligence"", a roadmap for the development and integration of the essential algorithms necessary for a merger of scientific computing, scientific simulation, and artificial intelligence. We call this merger simulation intelligence (SI), for short. We argue the motifs of simulation intelligence are interconnected and interdependent, much like the components within the layers of an operating system. Using this metaphor, we explore the nature of each layer of the simulation intelligence operating system stack (SI-stack) and the motifs therein: (1) Multi-physics and multi-scale modeling; (2) Surrogate modeling and emulation; (3) Simulation-based inference; (4) Causal modeling and inference; (5) Agent-based modeling; (6) Probabilistic programming; (7) Differentiable programming; (8) Open-ended optimization; (9) Machine programming. We believe coordinated efforts between motifs offers immense opportunity to accelerate scientific discovery, from solving inverse problems in synthetic biology and climate science, to directing nuclear energy experiments and predicting emergent behavior in socioeconomic settings. We elaborate on each layer of the SI-stack, detailing the state-of-art methods, presenting examples to highlight challenges and opportunities, and advocating for specific ways to advance the motifs and the synergies from their combinations. Advancing and integrating these technologies can enable a robust and efficient hypothesis-simulation-analysis type of scientific method, which we introduce with several use-cases for human-machine teaming and automated science.","",""
49,"Ning Chen, Tie Qiu, Xiaobo Zhou, Keqiu Li, Mohammed Atiquzzaman","An Intelligent Robust Networking Mechanism for the Internet of Things",2019,"","","","",185,"2022-07-13 09:31:46","","10.1109/MCOM.001.1900094","","",,,,,49,16.33,10,5,3,"In smart cities, the Internet of Things (IoT) consists of many low-power smart nodes. Its robustness is essential for protection of communication in data science against node failures caused by energy shortage or cyber-attacks. Scale-free networking topology, widely applied in IoT, is effectively resilient to random attacks but is vulnerable to malicious ones in which high-degree nodes are made to fail. The prohibitively high computational cost of existing robustness optimization algorithms is an obstacle to efficient topology self-optimization. To solve this problem, a novel robust networking model based on artificial intelligence is proposed to improve IoT topology robustness to protect its communication. Using the Back-Propagation neural network learning algorithm, the model extracts topology features from a dataset by supervised training. The experimental results show that the model achieves better prediction accuracy, thereby optimizing the topology with minimal computation overhead.","",""
18,"B. King","Guest Editorial: Discovery and Artificial Intelligence.",2017,"","","","",186,"2022-07-13 09:31:46","","10.2214/AJR.17.19178","","",,,,,18,3.60,18,1,5,"1189 term for extremely large datasets that can be analyzed computationally to reveal patterns, trends, and associations. A typical example of big data is the amount of information contained in today’s electronic medical records, which contain huge digital imaging archives, pathology department and laboratory archives, and millions of digital clinical notes and diagnoses that, when analyzed, could help physicians better identify trends in diagnoses. Natural language processing is another important field of computer science that is concerned with programming computers to process and understand natural human language text, such as the text contained in an electronic medical record. Finally, it is important to note that fastmoving advancements in computer hardware and processing, such as ultrafast graphic processing units and cloud computing, are enabling the accelerated applications of AI. All of the aforementioned components are part of the new world of the field of AI. AI has been developing for many years, but it has been advancing at a more rapid pace in recent years. First, we had computer-aided diagnosis, where computers were programmed by humans to detect certain characteristics on digital images. Computer-aided diagnosis was helpful but was limited to what the computer was programmed to detect. Through artificial neural networks, AI has introduced the ability for computers to learn from experience, thus enabling AI to go much further than CAD. However, the concept of AI as it applies to radiology is much easier in theory than in practice. Many complex and tedious steps must be taken before AI can play a major role in radiology [2, 3]. What is imperative is that radiologists must continually learn how to apply this new technology to improve the care of our patients. AI will likely replace some of what we do as radiologists, and, in other cases, AI will likely help us be more accurate in what we do today [4]. Those who believe that AI could replace radiologists don’t understand the complex role of radiologists in the care of patients. Although the naive concept that AI could replace radiologists will continue to be promoted, the truth of the matter is that AI will likely help radiologists achieve advancements way beyond those currently in motion through discovery and innovation. The uniquely human However, rather than fear these challenges, we should embrace them and look for new discoveries and innovations to help us meet those challenges. As the old English-language proverb says, “Necessity is the mother of invention.” In other words, we should look to discovery and innovation in radiology to help us meet the challenges facing us today. One of the most exciting emerging technologies on the radiology horizon is not a new scanner technology but, rather, what many refer to as artificial intelligence (AI). Many of us see AI as the next big important innovation in medicine and radiology. However, others see AI as a threat to our profession. Some have even gone so far as to say that radiologists could someday be replaced by AI [1]. Rather than fear this new technology, we should embrace it and discover new and exciting ways to improve and advance the field of radiology. To accomplish this, we, as radiologists, need to begin to understand the technology behind AI and look for ways to incorporate it into our practice and ultimately improve the care of our patients. AI is defined as the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, written and spoken human language recognition, decision making, and translation between languages. A major component of AI is machine learning, which is a subfield of computer science that enables computers to learn without being explicitly programmed. This exciting technology of machine learning incorporates computational models and algorithms that are similar to the structure and function of our brain’s biologic neural networks. These computational models are often referred to as artificial neural networks. When these artificial neural networks process information (i.e., digital data) from numerous input flows, they have the ability to “learn” and alter their structure in much the same way that the neurons in our brain are altered with memory. Deep learning is a part of a broader family of machine learning methods based on learning representations of data such as recognizing characteristic images (i.e., face recognition). Deep learning is very significant as it relates to radiology because of its focus on recognizing objects in images. Big data is a Guest Editorial: Discovery and Artificial Intelligence","",""
50,"A. Hramov, N. Frolov, V. Maksimenko, V. Makarov, A. Koronovskii, J. Garcia-Prieto, L. Antón-Toro, F. Maestú, A. Pisarchik","Artificial neural network detects human uncertainty.",2018,"","","","",187,"2022-07-13 09:31:46","","10.1063/1.5002892","","",,,,,50,12.50,6,9,4,"Artificial neural networks (ANNs) are known to be a powerful tool for data analysis. They are used in social science, robotics, and neurophysiology for solving tasks of classification, forecasting, pattern recognition, etc. In neuroscience, ANNs allow the recognition of specific forms of brain activity from multichannel EEG or MEG data. This makes the ANN an efficient computational core for brain-machine systems. However, despite significant achievements of artificial intelligence in recognition and classification of well-reproducible patterns of neural activity, the use of ANNs for recognition and classification of patterns in neural networks still requires additional attention, especially in ambiguous situations. According to this, in this research, we demonstrate the efficiency of application of the ANN for classification of human MEG trials corresponding to the perception of bistable visual stimuli with different degrees of ambiguity. We show that along with classification of brain states associated with multistable image interpretations, in the case of significant ambiguity, the ANN can detect an uncertain state when the observer doubts about the image interpretation. With the obtained results, we describe the possible application of ANNs for detection of bistable brain activity associated with difficulties in the decision-making process.","",""
6,"Sweta Jain","IS ARTIFICIAL INTELLIGENCE –THE NEXT BIG THING IN HR ?",2017,"","","","",188,"2022-07-13 09:31:46","","","","",,,,,6,1.20,6,1,5,"As technology continues to move at a breakneck pace and the world has become a global village, and everyone is connected with each other through internet. AI helps the systems to think and act like rational human beings so as to gain the benefits of performing the work at a faster pace with less computational errors and less fatigue. Artificial Intelligence in HR helps in understanding the cognitive science and cognitive behavior modeling. Fast paced digitization and AI helps in integrating different systems and can provide unified platform that can support full range of HR function starting from recruitment, selection, training, development, compensation and performance management. Periodical training, learning and development programs should be conducted at all levels of organization to impart digital skill set to the employees so as make processes more efficient, less time consuming and more productive.","",""
4,"W. Lawless, D. Sofge","Evaluations: Autonomy and Artificial Intelligence: A Threat or Savior?",2017,"","","","",189,"2022-07-13 09:31:46","","10.1007/978-3-319-59719-5_13","","",,,,,4,0.80,2,2,5,"","",""
23,"A. Wichert","Principles of Quantum Artificial Intelligence",2013,"","","","",190,"2022-07-13 09:31:46","","10.1142/8980","","",,,,,23,2.56,23,1,9,"Computation Production Systems Human Problem Solving Information Theory Markov Chains Introduction to Quantum Physics Computation with Qubits Discrete Fourier Transform and Quantum Fourier Transform Order Finding Quantum Search Grover's Algorithm and Householder Reflection Quantum Tree Search Quantum Problem Solving General Model of Quantum Computer Quantum Cognition.","",""
126,"W. Stead","Clinical Implications and Challenges of Artificial Intelligence and Deep Learning.",2018,"","","","",191,"2022-07-13 09:31:46","","10.1001/jama.2018.11029","","",,,,,126,31.50,126,1,4,"Artificial intelligence (AI) and deep learning are entering the mainstream of clinical medicine. For example, in December 2016, Gulshan et al1 reported development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. An accompanying editorial by Wong and Bressler2 pointed out limits of the study, the need for further validation of the algorithm in different populations, and unresolved challenges (eg, incorporating the algorithm into clinical work flows and convincing clinicians and patients to “trust a ‘black box’”). Sixteen months later, the Food and Drug Administration (FDA)3 permitted marketing of the first medical device to use AI to detect diabetic retinopathy. FDA reduced the risk of releasing the device by limiting the indication for use to screening adults who do not have visual symptoms for greater than mild retinopathy, to refer them to an eye care specialist. This issue of JAMA contains 2 Viewpoints on deep learning in health care. Hinton4 explains the technology underlying AI and deep learning, using clinical examples. AI is the general term for imitating human intelligence with computer systems. Early AI systems represented human reasoning with symbolic logic. As computer processing and storage became more powerful, researchers developed machine-learning techniques to imitate the way the human brain learns. The first machine learning continued to rely on human experts to label the data the system trained on (eg, the diagnosis) and to identify the significant features (eg, findings). Machine learning weighted the features from the data. With continued advances in computational power and with larger data sets, researchers began to develop deep learning techniques. The first deep learning algorithms were “supervised” in that human experts continued to label the training data, and the deep learning algorithms learned the features and weights directly from the data. The retinopathy screening algorithms are an example of supervised deep learning. Hinton4 describes continuing development of new deep learning techniques, including ones that are completely unsupervised. He also points out that it is not feasible to see the features learned by deep learning to explain how the system reaches a conclusion. Naylor5 identifies 7 factors driving adoption of AI and deep learning in health care: (1) the strengths of digital imaging over human interpretation; (2) the digitization of health-related records and data sharing; (3) the adaptability of deep learning to analysis of heterogeneous data sets; (4) the capacity of deep learning for hypothesis generation in research; (5) the promise of deep learning to streamline clinical workflows and empower patients; (6) the rapid-diffusion open-source and proprietary deep learning programs; and (7) of the adequacy of today’s basic deep learning technology to deliver improved performance as data sets get larger. Factors 3, 4, and 6 are specific to deep learning; the other factors apply to other AI techniques as well. Artificial intelligence is a family of technical techniques in the same way the radiologic imaging tool kit includes flat images, computed tomography scans, and functional imaging such as magnetic resonance imaging. Advances in computational technology, computer science, informatics, and statistics improve existing techniques and make new techniques possible. The addition of deep learning to the AI family of techniques represents an advance similar in magnitude to the addition of the computed tomography scanner to the radiology tool kit. Each AI technique has strengths and weaknesses. Symbolic logic is self-explaining but difficult to scale.6 For example, knowledge engineers extract the logic by interviewing or observing human experts. Statistical techniques such as supervised deep learning scale, but are subject to bias in the training data, and the reasoning cannot be explained. Since deep learning systems are trained on data from the past, they are not prepared to reason in the way humans do about conditions that have not been seen before. In the future, unsupervised deep learning may reduce this gap between human intelligence and AI. The potential applications of AI in health care present a range of computational difficulty. Narrow tasks, in which the context is predefined, are relatively easy. Imageprocessing tasks such as recognizing the border of an organ to suggest where to cut off a scan, or highlighting a suspicious area in an image for the radiologist or pathologist, are examples of narrow tasks. Image analysis and diagnostic prediction tasks such as the diabetic retinopathy example are broader and harder, but doable with today’s technology. Very broad data analysis and pattern prediction tasks such as analyzing heterogeneous data sets from diverse sources to suggest novel associations are feasible today because the purpose is limited to hypothesis generation. Thinking in the way humans do—reasoning, for example, from a few observations to suggest a novel scientific framework as Einstein did with the theory of relativity—is beyond technology on the horizon. Clinicians should view the output of AI programs or devices as statistical predictions. They should maintain an index of suspicion that the prediction may be wrong, just as they Viewpoint pages 1099 and 1101 Opinion","",""
92,"A. Annoni, P. Benczúr, P. Bertoldi, Blagoj Delipetrev, Giuditta De Prato, C. Feijóo, Enrique Fernández-Macías, E. Gutiérrez, M. Portela, H. Junklewitz, M. L. Cobo, B. Martens, Susana Nascimento, S. Nativi, Alexandre Pólvora, Jose Ignacio Sanchez Martin, Songuel Tolan, I. Tuomi, Lucia Vesnić Alujević","Artificial Intelligence: A European Perspective",2018,"","","","",192,"2022-07-13 09:31:46","","10.2760/11251","","",,,,,92,23.00,9,19,4,"We are only at the beginning of a rapid period of transformation of our economy and society due to the convergence of many digital technologies. Artificial Intelligence (AI) is central to this change and offers major opportunities to improve our lives. The recent developments in AI are the result of increased processing power, improvements in algorithms and the exponential growth in the volume and variety of digital data. Many applications of AI have started entering into our every-day lives, from machine translations, to image recognition, and music generation, and are increasingly deployed in industry, government, and commerce. Connected and autonomous vehicles, and AI-supported medical diagnostics are areas of application that will soon be commonplace. There is strong global competition on AI among the US, China, and Europe. The US leads for now but China is catching up fast and aims to lead by 2030. For the EU, it is not so much a question of winning or losing a race but of finding the way of embracing the opportunities offered by AI in a way that is human-centred, ethical, secure, and true to our core values. The EU Member States and the European Commission are developing coordinated national and European strategies, recognising that only together we can succeed. We can build on our areas of strength including excellent research, leadership in some industrial sectors like automotive and robotics, a solid legal and regulatory framework, and very rich cultural diversity also at regional and sub-regional levels. It is generally recognised that AI can flourish only if supported by a robust computing infrastructure and good quality data: â€¢ With respect to computing, we identified a window of opportunity for Europe to invest in the emerging new paradigm of computing distributed towards the edges of the network, in addition to centralised facilities. This will support also the future deployment of 5G and the Internet of Things. â€¢ With respect to data, we argue in favour of learning from successful Internet companies, opening access to data and developing interactivity with the users rather than just broadcasting data. In this way, we can develop ecosystems of public administrations, firms, and civil society enriching the data to make it fit for AI applications responding to European needs. We should embrace the opportunities afforded by AI but not uncritically. The black box characteristics of most leading AI techniques make them opaque even to specialists. AI systems are currently limited to narrow and well-defined tasks, and their technologies inherit imperfections from their human creators, such as the well-recognised bias effect present in data. We should challenge the shortcomings of AI and work towards strong evaluation strategies, transparent and reliable systems, and good human-AI interactions. Ethical and secure-by-design algorithms are crucial to build trust in this disruptive technology, but we also need a broader engagement of civil society on the values to be embedded in AI and the directions for future development. This social engagement should be part of the effort to strengthen our resilience at all levels from local, to national and European, across institutions, industry and civil society. Developing local ecosystems of skills, computing, data, and applications can foster the engagement of local communities, respond to their needs, harness local creativity and knowledge, and build a human-centred, diverse, and socially driven AI. We still know very little about how AI will impact the way we think, make decisions, relate to each other, and how it will affect our jobs. This uncertainty can be a source of concern but is also a sign of opportunity. The future is not yet written. We can shape it based on our collective vision of what future we would like to have. But we need to act together and act fast.","",""
4,"Shahriar Shakir Sumit, D. Rambli, S. Mirjalili","Vision-Based Human Detection Techniques: A Descriptive Review",2021,"","","","",193,"2022-07-13 09:31:46","","10.1109/ACCESS.2021.3063028","","",,,,,4,4.00,1,3,1,"Cameras are being used everywhere for the safety and security of citizens in different countries. Using a machine to detect humans in a photo or a video frame is a very complicated and challenging task. Various techniques have been developed for this purpose, which mainly rely on Artificial Intelligence. This article aims to provide a comprehensive review and analysis of the literatures from a descriptive perspective, which is its main differentiator from the existing survey papers in this area. Firstly, the vision-based human detection techniques and classifiers are elucidated in conjunction with the variants of feature extraction techniques. Secondly, various pros and cons of such techniques are discussed. Then, an investigation has been conducted and reported based on the state-of-the-art human detection descriptors (e.g. Log-Average Miss Rate and accuracy). Although techniques such as Viola-Jones and Speeded-Up Robust Features can detect objects in real-time and overcome Scale-Invariant Feature Transform (SIFT) limitations, they are still sensitive to illuminated conditions. Other techniques such as SIFT, Bag of Words, Orthogonal Moments, and Histogram of oriented Gradients provide other interesting benefits which include insensitivity to occlusion and clutters, simplicity, low-order element construction and invariance to illuminated conditions; nevertheless, they are computationally expensive and sensitive to image rotation. A meticulous review along similar lines revealed that the Deformable Part-based Model performs relatively better due to its ability to deal with particular pose variations and multiple views, occlusion handling (partial) and is application-free while its counterparts focus on only a single aspect. This article highlights and provides a brief description of each available data-sets for human detection research. Various use-cases of human detection systems are also elaborated. Finally, various conclusions are derived based on the conducted review followed by recommendations for future directions and possibilities to further improve the speed and accuracy of human detection systems.","",""
100,"K. Yasaka, O. Abe","Deep learning and artificial intelligence in radiology: Current applications and future directions",2018,"","","","",194,"2022-07-13 09:31:46","","10.1371/journal.pmed.1002707","","",,,,,100,25.00,50,2,4,"Radiological imaging diagnosis plays important roles in clinical patient management. Deep learning with convolutional neural networks (CNNs) is recently gaining wide attention for its high performance in recognizing images. If CNNs realize their promise in the context of radiology, they are anticipated to help radiologists achieve diagnostic excellence and to enhance patient healthcare. Here, we discuss very recent developments in the field, including studies published in the current PLOS Medicine Special Issue on Machine Learning in Health and Biomedicine, with comment on expectations and planning for artificial intelligence (AI) in the radiology clinic. Chest radiographs are one of the most utilized radiological modalities in the world and have been collected into a number of large datasets currently available to machine learning researchers. In this Special Issue, three groups of researchers applied deep learning to radiological imaging diagnosis using this modality. In the first, Pranav Rajpurkar and colleagues found that deep learning models detected clinically important abnormalities (e.g., edema, fibrosis, mass, pneumonia, and pneumothorax) on chest radiography, at a performance level comparable to practicing radiologists [1]. In a similar study, Andrew Taylor and colleagues developed deep learning models that detected clinically significant pneumothoraces on chest radiography with excellent performance on data from the same site—with areas under the receiver operating characteristic curve (AUC) of 0.94–0.96 [2]. Meanwhile, Eric Oermann and colleagues investigated how well deep learning models that detected pneumonia on chest radiography generalized across different hospitals. They found that models trained on pooled data from sites with different pneumonia prevalence performed well on new pooled data from these same sites (AUC of 0.93–0.94) but significantly less well on external data (AUC 0.75–0.89); additional analyses supported the interpretation that deep learning models diagnosing pneumonia on chest radiography are able to exploit confounding information that is associated with pneumonia prevalence [3]. Also in this Special Issue, Nicholas Bien and colleagues applied deep learning techniques to detect knee abnormalities on magnetic resonance (MR) imaging and found that the trained model showed near-human-level performance [4]. Taking these four studies together, we can interpret that deep learning is currently able to diagnose a number of conditions using radiological data, but such diagnostic models may not be robust to a change in location. These Special Issue studies join a growing number of applications of deep learning to radiological images from various modalities that can aid with detection, diagnosis, staging, and subclassification of conditions. Cerebral aneurysms can be detected on MR angiography with","",""
50,"Stuart J. Russell, J. Bohannon","Artificial intelligence. Fears of an AI pioneer.",2015,"","","","",195,"2022-07-13 09:31:46","","10.1126/science.349.6245.252","","",,,,,50,7.14,25,2,7,"From the enraged robots in the 1920 play R.U.R. to the homicidal computer H.A.L. in 2001: A Space Odyssey, science fiction writers have embraced the dark side of artificial intelligence (AI) ever since the concept entered our collective imagination. Sluggish progress in AI research, especially during the “AI winter” of the 1970s and 1980s, made such worries seem far-fetched. But recent breakthroughs in machine learning and vast improvements in computational power have brought a flood of research funding— and fresh concerns about where AI may lead us. One researcher now speaking up is Stuart Russell, a computer scientist at the University of California, Berkeley, who with Peter Norvig, director of research at Google, wrote the premier AI textbook, Artificial Intelligence: A Modern Approach, now in its third edition. Last year, Russell joined the Centre for the Study of Existential Risk at Cambridge University in the United Kingdom as an AI expert focusing on “risks that could lead to human extinction.” Among his chief concerns, which he aired at an April meeting in Geneva, Switzerland, run by the United Nations, is the danger of putting military drones and weaponry under the full control of AI systems. This interview has been edited for clarity and brevity.","",""
56,"O. Bello, J. Holzmann, T. Yaqoob, C. Teodoriu","Application Of Artificial Intelligence Methods In Drilling System Design And Operations: A Review Of The State Of The Art",2015,"","","","",196,"2022-07-13 09:31:46","","10.1515/jaiscr-2015-0024","","",,,,,56,8.00,14,4,7,"Abstract Artificial Intelligence (AI) can be defined as the application of science and engineering with the intent of intelligent machine composition. It involves using tool based on intelligent behavior of humans in solving complex issues, designed in a way to make computers execute tasks that were earlier thought of human intelligence involvement. In comparison to other computational automations, AI facilitates and enables time reduction based on personnel needs and most importantly, the operational expenses. Artificial Intelligence (AI) is an area of great interest and significance in petroleum exploration and production. Over the years, it has made an impact in the industry, and the application has continued to grow within the oil and gas industry. The application in E & P industry has more than 16 years of history with first application dated 1989, for well log interpretation; drill bit diagnosis using neural networks and intelligent reservoir simulator interface. It has been propounded in solving many problems in the oil and gas industry which includes, seismic pattern recognition, reservoir characterisation, permeability and porosity prediction, prediction of PVT properties, drill bits diagnosis, estimating pressure drop in pipes and wells, optimization of well production, well performance, portfolio management and general decision making operations and many more. This paper reviews and analyzes the successful application of artificial intelligence techniques as related to one of the major aspects of the oil and gas industry, drilling capturing the level of application and trend in the industry. A summary of various papers and reports associated with artificial intelligence applications and it limitations will be highlighted. This analysis is expected to contribute to further development of this technique and also determine the neglected areas in the field.","",""
42,"Douwe Kiela, L. Bulat, A. Vero, S. Clark","Virtual Embodiment: A Scalable Long-Term Strategy for Artificial Intelligence Research",2016,"","","","",197,"2022-07-13 09:31:46","","","","",,,,,42,7.00,11,4,6,"Meaning has been called the ""holy grail"" of a variety of scientific disciplines, ranging from linguistics to philosophy, psychology and the neurosciences. The field of Artifical Intelligence (AI) is very much a part of that list: the development of sophisticated natural language semantics is a sine qua non for achieving a level of intelligence comparable to humans. Embodiment theories in cognitive science hold that human semantic representation depends on sensori-motor experience; the abundant evidence that human meaning representation is grounded in the perception of physical reality leads to the conclusion that meaning must depend on a fusion of multiple (perceptual) modalities. Despite this, AI research in general, and its subdisciplines such as computational linguistics and computer vision in particular, have focused primarily on tasks that involve a single modality. Here, we propose virtual embodiment as an alternative, long-term strategy for AI research that is multi-modal in nature and that allows for the kind of scalability required to develop the field coherently and incrementally, in an ethically responsible fashion.","",""
0,"M. Iwamoto, Daichi Kato","Efficient Actor-Critic Reinforcement Learning With Embodiment of Muscle Tone for Posture Stabilization of the Human Arm",2020,"","","","",198,"2022-07-13 09:31:46","","10.1162/neco_a_01333","","",,,,,0,0.00,0,2,2,"This letter proposes a new idea to improve learning efficiency in reinforcement learning (RL) with the actor-critic method used as a muscle controller for posture stabilization of the human arm. Actor-critic RL (ACRL) is used for simulations to realize posture controls in humans or robots using muscle tension control. However, it requires very high computational costs to acquire a better muscle control policy for desirable postures. For efficient ACRL, we focused on embodiment that is supposed to potentially achieve efficient controls in research fields of artificial intelligence or robotics. According to the neurophysiology of motion control obtained from experimental studies using animals or humans, the pedunculopontine tegmental nucleus (PPTn) induces muscle tone suppression, and the midbrain locomotor region (MLR) induces muscle tone promotion. PPTn and MLR modulate the activation levels of mutually antagonizing muscles such as flexors and extensors in a process through which control signals are translated from the substantia nigra reticulata to the brain stem. Therefore, we hypothesized that the PPTn and MLR could control muscle tone, that is, the maximum values of activation levels of mutually antagonizing muscles using different sigmoidal functions for each muscle; then we introduced antagonism function models (AFMs) of PPTn and MLR for individual muscles, incorporating the hypothesis into the process to determine the activation level of each muscle based on the output of the actor in ACRL. ACRL with AFMs representing the embodiment of muscle tone successfully achieved posture stabilization in five joint motions of the right arm of a human adult male under gravity in predetermined target angles at an earlier period of learning than the learning methods without AFMs. The results obtained from this study suggest that the introduction of embodiment of muscle tone can enhance learning efficiency in posture stabilization disorders of humans or humanoid robots.","",""
0,"John Kalantari","A general purpose artificial intelligence framework for the analysis of complex biological systems",2017,"","","","",199,"2022-07-13 09:31:46","","10.17077/ETD.4ESKIJ3M","","",,,,,0,0.00,0,1,5,"This thesis encompasses research on Artificial Intelligence in support of automating scientific discovery in the fields of biology and medicine. At the core of this research is the ongoing development of a general-purpose artificial intelligence framework emulating various facets of human-level intelligence necessary for building cross-domain knowledge that may lead to new insights and discoveries. To learn and buildmodels in a data-drivenmanner, we develop a general-purpose learning framework called Syntactic Nonparametric Analysis of Complex Systems (SYNACX), which uses tools from Bayesian nonparametric inference to learn the statistical and syntactic properties of biological phenomena from sequence data. We show that the models learned by SYNACX offer performance comparable to that of standard neural network architectures. For complex biological systems or processes consisting of several heterogeneous components with spatio-temporal interdependencies across multiple scales, learning frameworks like SYNACX can become unwieldy due to the the resultant combinatorial complexity. Thus we also investigate ways to robustly reduce data dimensionality by introducing a new data abstraction. In particular, we extend traditional string and graph grammars in a new modeling formalism which we call Simplicial Grammar. This formalism integrates the topological properties of the simplicial complex with the expressive power of stochastic grammars in a computation abstraction with which we can decompose complex system behavior, into a finite set of modular grammar rules which parsimoniously describe the spatial/temporal structure and dynamics of patterns inferred from sequence data.","",""
47,"T. Frei","An Artificial Intelligence Approach To Legal Reasoning",2016,"","","","",200,"2022-07-13 09:31:46","","","","",,,,,47,7.83,47,1,6,"ed to be an introduction to computational jurisprudence for both groups. It identifies issues critical to the purpose , behavior, knowledge sources, knowledge structures, and reasoning processes of expert legal systems. The second part implements a simple prototype system for a well-defined area of contract law and is more appropriate for experienced developers of knowledge-based systems. Law is a domain in which the experts are supposed to disagree, and lawyers must be able to argue either side of a case. A judge or juror must decide which argument is "" best. "" A knowledge based legal reasoning program can only guide analysis and identification of technically defensi-ble positions in a case. However, it should also be able to distinguish between questions that are "" easy "" to decide, and those demanding human analysis. These two ideas form the basis of the prototype's behavior, making it somewhat different from knowledge based systems in most other expert domains. According to Gardner, legal reasoning systems are further distinguished by their knowledge sources and knowledge structures. She reviews the evolution of legal thought in the context of knowledge engineering, raises several critical issues, and draws conclusions about how legal knowledge must be used and represented in programs. In the human world, legal knowledge is represented in cases, and statutes. Although not all areas of law use both sources, she concludes that expert legal systems need both types of knowledge, plus some additional "" common sense knowledge "" to guide analysis effectively. Gardner views statutes as rules defining legal states and their consequences. Although they are convenient starting points for legal analysis , they are usually insufficient for making wise legal decisions. Most litigation involves questions about whether the rules have been followed, what the rules actually mean, and sometimes, which set of rules should be used. Cases contain written arguments about how to answer these questions under specific circumstances, along with their final interpretation by the juror. Lawyers can use similar cases as examples to guide their formulation of arguments in future disputes. Cases are used as precedents for deciding which rules to use in a given situation, and how to apply them. They can be used to annotate and clarify rules that conflict in some context, or whose relevance might be disputed. They can even change the way rules are applied to similar factual situations in the future. In these respects, cases embody …","",""
