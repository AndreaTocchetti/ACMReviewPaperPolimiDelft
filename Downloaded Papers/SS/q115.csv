Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
3,"O. Dukes, S. Vansteelandt, D. Whitney","On doubly robust inference for double machine learning",2021,"","","","",1,"2022-07-13 09:39:44","","","","",,,,,3,3.00,1,3,1,"Due to concerns about parametric model misspecification, there is interest in using machine learning to adjust for confounding when evaluating the causal effect of an exposure on an outcome. Unfortunately, exposure effect estimators that rely on machine learning predictions are generally subject to so-called plug-in bias, which can render naive p-values and confidence intervals invalid. Progress has been made via proposals like targeted maximum likelihood estimation and more recently double machine learning, which rely on learning the conditional mean of both the outcome and exposure. Valid inference can then be obtained so long as both predictions converge (sufficiently fast) to the truth. Focusing on partially linear regression models, we show that a specific implementation of the machine learning techniques can yield exposure effect estimators that have small bias even when one of the firststage predictions does not converge to the truth. The resulting tests and confidence intervals are doubly robust. We also show that the proposed estimators may fail to be regular when only one nuisance parameter is consistently estimated; nevertheless, we observe in simulation studies that our proposal leads to reduced bias and improved confidence interval coverage in moderate samples.","",""
0,"Chi-ting Ho, Dawei Wang","Robust identification of topological phase transition by self-supervised machine learning approach",2021,"","","","",2,"2022-07-13 09:39:44","","10.1088/1367-2630/ac1709","","",,,,,0,0.00,0,2,1,"We propose a systematic methodology to identify the topological phase transition through a self-supervised machine learning model, which is trained to correlate system parameters to the non-local observables in time-of-flight experiments of ultracold atoms. Different from the conventional supervised learning approach, where the predicted phase transition point is very sensitive to the training region and data labeling, our self-supervised learning approach identifies the phase transition point by the largest deviation of the predicted results from the known system parameters and by the highest confidence through a systematic shift of the training regions. We demonstrate the robust application of this approach results in various 1D and 2D exactly solvable models, using different input features (time-of-flight images, spatial correlation function or density–density correlation function). As a result, our self-supervised approach should be a very general and reliable method for many condensed matter or solid state systems to observe new states of matters solely based on experimental measurements, even without a priori knowledge of the phase transition models.","",""
0,"B. Boguslawski, Matthieu Boujonnier, Loryne Bissuel-Beauvais, Fahd Saghir","Edge Analytics at the Wellhead: Designing Robust Machine Learning Models for Artificial Lift Failure Detection",2018,"","","","",3,"2022-07-13 09:39:44","","10.2118/192886-MS","","",,,,,0,0.00,0,4,4,"  This paper outlines the challenges and constraints related to deployment of Machine Learning solutions for rod pump abnormal states recognition and diagnosis at the wellhead. Those abnormal states may lead to a failure or to non-optimized production. Particular focus is on two main aspects: 1) Develop a robust Machine Learning model & IIoT architecture to predict rod pump failure directly at the wellhead, 2) Ensure high level of pump failure prediction through Machine Learning to ensure operator confidence.  To the best of our knowledge, this is the first-of-its-kind IIoT Edge Analytics solution which provides operators with the capability of automated Dynagraph Card recognition directly at the wellhead via Machine Learning models. This solution also addresses end-user requirements in terms of confidentiality and communication infrastructure.","",""
4,"Peng Kang, P. Lama","Robust Resource Scaling of Containerized Microservices with Probabilistic Machine learning",2020,"","","","",4,"2022-07-13 09:39:44","","10.1109/UCC48980.2020.00031","","",,,,,4,2.00,2,2,2,"Large-scale web services are increasingly being built with many small modular components (microservices), which can be deployed, updated and scaled seamlessly. These microservices are packaged to run in a lightweight isolated execution environment (containers) and deployed on computing resources rented from cloud providers. However, the complex interactions and the contention of shared hardware resources in cloud data centers pose significant challenges in managing web service performance. In this paper, we present RScale, a robust resource scaling system that provides end-to-end performance guarantee for containerized microservices deployed in the cloud. RScale employs a probabilistic machine learning-based performance model, which can quickly adapt to changing system dynamics and directly provide confidence bounds in the predictions with minimal overhead. It leverages multi-layered data collected from container-level resource usage metrics and virtual machine-level hardware performance counter metrics to capture changing resource demands in the presence of multi-tenant performance interference. We implemented and evaluated RScale on NSF Cloud's Chameleon testbed using KVM for virtualization, Docker Engine for containerization and Kubernetes for container orchestration. Experimental results with an open-source microservices benchmark, Robot Shop, demonstrate the superior prediction accuracy and adaptiveness of our modeling approach compared to popular machine learning techniques. RScale meets the performance SLO (service-level-objective) targets for various microservice workflows even in the presence of multi-tenant performance interference and changing system dynamics.","",""
1,"Edward H. Kennedy, Sivaraman Balakrishnan, L. Wasserman","Discussion of “On Nearly Assumption-Free Tests of Nominal Confidence Interval Coverage for Causal Parameters Estimated by Machine Learning”",2020,"","","","",5,"2022-07-13 09:39:44","","10.1214/20-sts796","","",,,,,1,0.50,0,3,2,"We congratulate the authors on their exciting paper, which introduces a novel idea for assessing the estimation bias in causal estimates. Doubly robust estimators are now part of the standard set of tools in causal inference, but a typical analysis stops with an estimate and a confidence interval. The authors give an approach for a unique type of model-checking that allows the user to check whether the bias is sufficiently small with respect to the standard error, which is generally required for confidence intervals to be reliable.","",""
7,"Lin Liu, R. Mukherjee, J. Robins","On Nearly Assumption-Free Tests of Nominal Confidence Interval Coverage for Causal Parameters Estimated by Machine Learning",2019,"","","","",6,"2022-07-13 09:39:44","","10.1214/20-sts786","","",,,,,7,2.33,2,3,3,"For many causal effect parameters of interest, doubly robust machine learning (DRML) estimators ψ^1 are the state-of-the-art, incorporating the good prediction performance of machine learning; the decreased bias of doubly robust estimators; and the analytic tractability and bias reduction of sample splitting with cross-fitting. Nonetheless, even in the absence of confounding by unmeasured factors, the nominal (1−α) Wald confidence interval ψ^1±zα/2s.e.ˆ[ψ^1] may still undercover even in large samples, because the bias of ψ^1 may be of the same or even larger order than its standard error of order n−1/2.  In this paper, we introduce essentially assumption-free tests that (i) can falsify the null hypothesis that the bias of ψ^1  is of smaller order than its standard error, (ii) can provide a upper confidence bound on the true coverage of the Wald interval, and (iii) are valid under the null under no smoothness/sparsity assumptions on the nuisance parameters. The tests, which we refer to as Assumption Free Empirical Coverage Tests (AFECTs), are based on a U-statistic that estimates part of the bias of ψ^1.  Our claims need to be tempered in several important ways. First no test, including ours, of the null hypothesis that the ratio of the bias to its standard error is smaller than some threshold δ  can be consistent [without additional assumptions (e.g., smoothness or sparsity) that may be incorrect]. Second, the above claims only apply to certain parameters in a particular class. For most of the others, our results are unavoidably less sharp. In particular, for these parameters, we cannot directly test whether the nominal Wald interval ψ^1±zα/2s.e.ˆ[ψ^1] undercovers. However, we can often test the validity of the smoothness and/or sparsity assumptions used by an analyst to justify a claim that the reported Wald interval’s actual coverage is no less than nominal. Third, in the main text, with the exception of the simulation study in Section 1, we assume we are in the semisupervised data setting (wherein there is a much larger dataset with information only on the covariates), allowing us to regard the covariance matrix of the covariates as known. In the simulation in Section 1, we consider the setting in which estimation of the covariance matrix is required. In the simulation, we used a data adaptive estimator which performs very well in our simulations, but the estimator’s theoretical sampling behavior remains unknown.","",""
2,"Lin Liu, R. Mukherjee, J. Robins","On assumption-free tests and confidence intervals for causal effects estimated by machine learning",2019,"","","","",7,"2022-07-13 09:39:44","","","","",,,,,2,0.67,1,3,3,"For many causal effect parameters $\psi$ of interest doubly robust machine learning estimators $\widehat\psi_1$ are the state-of-the-art, incorporating the benefits of the low prediction error of machine learning algorithms; the decreased bias of doubly robust estimators; and.the analytic tractability and bias reduction of cross fitting. When the potential confounders is high dimensional, the associated $(1 - \alpha)$ Wald intervals may still undercover even in large samples, because the bias may be of the same or even larger order than its standard error. In this paper, we introduce tests that can have the power to detect whether the bias of $\widehat\psi_1$ is of the same or even larger order than its standard error of order $n^{-1/2}$, can provide a lower confidence limit on the degree of under coverage of the interval and strikingly, are valid under essentially no assumptions. We also introduce an estimator with bias generally less than that of $\widehat\psi_1$, yet whose standard error is not much greater than $\widehat\psi_1$'s. The tests, as well as the estimator $\widehat\psi_2$, are based on a U-statistic that is the second-order influence function for the parameter that encodes the estimable part of the bias of $\widehat\psi_1$. Our impressive claims need to be tempered in several important ways. First no test, including ours, of the null hypothesis that the ratio of the bias to its standard error can be consistent [without making additional assumptions that may be incorrect]. Furthermore the above claims only apply to parameters in a particular class. For the others, our results are less sharp and require more careful interpretation.","",""
6,"C. Yeomans, R. Shail, S. Grebby, V. Nykänen, M. Middleton, P. Lusty","A machine learning approach to tungsten prospectivity modelling using knowledge-driven feature extraction and model confidence",2019,"","","","",8,"2022-07-13 09:39:44","","10.31223/osf.io/9fet8","","",,,,,6,2.00,1,6,3,"Abstract Novel mineral prospectivity modelling presented here applies knowledge-driven feature extraction to a data-driven machine learning approach for tungsten mineralisation. The method emphasises the importance of appropriate model evaluation and develops a new Confidence Metric to generate spatially refined and robust exploration targets. The data-driven Random Forest™ algorithm is employed to model tungsten mineralisation in SW England using a range of geological, geochemical and geophysical evidence layers which include a depth to granite evidence layer. Two models are presented, one using standardised input variables and a second that implements fuzzy set theory as part of an augmented feature extraction step. The use of fuzzy data transformations mean feature extraction can incorporate some user-knowledge about the mineralisation into the model. The typically subjective approach is guided using the Receiver Operating Characteristics (ROC) curve tool where transformed data are compared to known training samples. The modelling is conducted using 34 known true positive samples with 10 sets of randomly generated true negative samples to test the random effect on the model. The two models have similar accuracy but show different spatial distributions when identifying highly prospective targets. Areal analysis shows that the fuzzy-transformed model is a better discriminator and highlights three areas of high prospectivity that were not previously known. The Confidence Metric, derived from model variance, is employed to further evaluate the models. The new metric is useful for refining exploration targets and highlighting the most robust areas for follow-up investigation. The fuzzy-transformed model is shown to contain larger areas of high model confidence compared to the model using standardised variables. Finally, legacy mining data, from drilling reports and mine descriptions, is used to further validate the fuzzy-transformed model and gauge the depth of potential deposits. Descriptions of mineralisation corroborate that the targets generated in these models could be undercover at depths of less than 300 ​m. In summary, the modelling workflow presented herein provides a novel integration of knowledge-driven feature extraction with data-driven machine learning modelling, while the newly derived Confidence Metric generates reliable mineral exploration targets.","",""
21,"A. Naimi, Alan Mishler, Edward H. Kennedy","Challenges in Obtaining Valid Causal Effect Estimates with Machine Learning Algorithms.",2017,"","","","",9,"2022-07-13 09:39:44","","10.1093/aje/kwab201","","",,,,,21,4.20,7,3,5,"Unlike parametric regression, machine learning (ML) methods do not generally require precise knowledge of the true data generating mechanisms. As such, numerous authors have advocated for ML methods to estimate causal effects. Unfortunately, ML algorithmscan perform worse than parametric regression. We demonstrate the performance of ML-based single- and double-robust estimators. We use 100 Monte Carlo samples with sample sizes of 200, 1200, and 5000 to investigate bias and confidence interval coverage under several scenarios. In a simple confounding scenario, confounders were related to the treatment and the outcome via parametric models. In a complex confounding scenario, the simple confounders were transformed to induce complicated nonlinear relationships. In the simple scenario, when ML algorithms were used, double-robust estimators were superior to single-robust estimators. In the complex scenario, single-robust estimators with ML algorithms were at least as biased as estimators using misspecified parametric models. Double-robust estimators were less biased, but coverage was well below nominal. The use of sample splitting, inclusion of confounder interactions, reliance on a richly specified ML algorithm, and use of doubly robust estimators was the only explored approach that yielded negligible bias and nominal coverage. Our results suggest that ML based singly robust methods should be avoided.","",""
1,"Tianzhe Bao, Shengquan Xie, Pengfei Yang, P. Zhou, Zhiqiang Zhang","Towards Robust, Adaptive and Reliable Upper-limb Motion Estimation Using Machine Learning and Deep Learning--A Survey in Myoelectric Control.",2022,"","","","",10,"2022-07-13 09:39:44","","10.1109/JBHI.2022.3159792","","",,,,,1,1.00,0,5,1,"To develop multi-functional human-machine interfaces that can help disabled people reconstruct lost functions of upper-limbs, machine learning (ML) and deep learning (DL) techniques have been widely implemented to decode human movement intentions from surface electromyography (sEMG) signals. However, due to the high complexity of upper-limb movements and the inherent non-stable characteristics of sEMG, the usability of ML/DL based control schemes is still greatly limited in practical scenarios. To this end, tremendous efforts have been made to improve model robustness, adaptation, and reliability. In this article, we provide a systematic review on recent achievements, mainly from three categories: multi-modal sensing fusion to gain additional information of the user, transfer learning (TL) methods to eliminate domain shift impacts on estimation models, and post-processing approaches to obtain more reliable outcomes. Special attention is given to fusion strategies, deep TL frameworks, and confidence estimation. \textcolor{red}{Research challenges and emerging opportunities, with respect to hardware development, public resources, and decoding strategies, are also analysed to provide perspectives for future developments.","",""
0,"Tareq Aziz AL-Qutami, M. A. Ishak, Lars Wollebaek, W. A. W. Ahmad","Combining Physics and Machine Learning for Multimodal Virtual Flow Metering with Confidence",2022,"","","","",11,"2022-07-13 09:39:44","","10.2523/iptc-22431-ms","","",,,,,0,0.00,0,4,1,"  This paper introduces a multimodal virtual flow meter (VFM) that merges physics-driven multiphase flow simulations with machine learning models to accurately estimate flow rates in oil and gas wells. The combining algorithm takes advantage of the confidence decay and historical performance factors to assign confidence and contribution weights to the base estimators and then aggregates their estimates to arrive at more accurate flow rate estimates. Furthermore, the proposed multimodal VFM provides an indication of the confidence level for each estimate based on the underlying agreement of the base estimates and the historical performance. The proposed VFM was tested in a 6 months online pilot in two oil wells. The proposed multimodal algorithm resulted in almost 50% improvements in performance compared to individual VFMs. The proposed robust multimodal approach can provide a complimentary benefit as an optimal VFM and reduce the overall system uncertainty. The developed VFM can be used for real-time production monitoring, verification and backup of physical meters, and well-test validation.","",""
0,"ShanShan Hu, Shengying Gu, Shuowen Wang, C. Qi, Chenyang Shi, Feng-huan Qian, G. Fan","Robust Prediction of Prognosis and Immunotherapy Response for Bladder Cancer through Machine Learning Algorithm",2022,"","","","",12,"2022-07-13 09:39:44","","10.3390/genes13061073","","",,,,,0,0.00,0,7,1,"The important roles of machine learning and ferroptosis in bladder cancer (BCa) are still poorly understood. In this study, a comprehensive analysis of 19 ferroptosis-related genes (FRGs) was performed in 1322 patients with BCa from four independent patient cohorts and a pan-cancer cohort of 9824 patients. Twelve FRGs were selected through machine learning algorithm to construct the prognosis model. Significantly differential survival outcomes (hazard ratio (HR) = 2.09, 95% confidence interval (CI): 1.55–2.82, p < 0.0001) were observed between patients with high and low ferroptosis scores in the TCGA cohort, which was also verified in the E-MTAB-4321 cohort (HR = 4.71, 95% CI: 1.58–14.03, p < 0.0001), the GSE31684 cohort (HR = 1.76, 95% CI: 1.08–2.87, p = 0.02), and the pan-cancer cohort (HR = 1.15, 95% CI: 1.07–1.24, p < 0.0001). Tumor immunity-related pathways, including the IL-17 signaling pathway and JAK-STAT signaling pathway, were found to be associated with the ferroptosis score in BCa through a functional enrichment analysis. Further verification in the IMvigor210 cohort revealed the BCa patients with high ferroptosis scores tended to have worse survival outcome after receiving tumor immunotherapy. Significantly different ferroptosis scores could also be found between BCa patients with different reactions to treatment with immune checkpoint inhibitors.","",""
186,"Seung-Hwan Bae, Kuk-jin Yoon","Confidence-Based Data Association and Discriminative Deep Appearance Learning for Robust Online Multi-Object Tracking",2018,"","","","",13,"2022-07-13 09:39:44","","10.1109/TPAMI.2017.2691769","","",,,,,186,46.50,93,2,4,"Online multi-object tracking aims at estimating the tracks of multiple objects instantly with each incoming frame and the information provided up to the moment. It still remains a difficult problem in complex scenes, because of the large ambiguity in associating multiple objects in consecutive frames and the low discriminability between objects appearances. In this paper, we propose a robust online multi-object tracking method that can handle these difficulties effectively. We first define the tracklet confidence using the detectability and continuity of a tracklet, and decompose a multi-object tracking problem into small subproblems based on the tracklet confidence. We then solve the online multi-object tracking problem by associating tracklets and detections in different ways according to their confidence values. Based on this strategy, tracklets sequentially grow with online-provided detections, and fragmented tracklets are linked up with others without any iterative and expensive association steps. For more reliable association between tracklets and detections, we also propose a deep appearance learning method to learn a discriminative appearance model from large training datasets, since the conventional appearance learning methods do not provide rich representation that can distinguish multiple objects with large appearance variations. In addition, we combine online transfer learning for improving appearance discriminability by adapting the pre-trained deep model during online tracking. Experiments with challenging public datasets show distinct performance improvement over other state-of-the-arts batch and online tracking methods, and prove the effect and usefulness of the proposed methods for online multi-object tracking.","",""
0,"Adrià Soldevila Coma","Robust leak localization in water distribution networks using machine learning techniques",2018,"","","","",14,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,1,4,"This PhD thesis presents a methodology to detect, estimate and localize water leaks (with the main focus in the localization problem) in water distribution networks using hydraulic models and machine learning techniques. The actual state of the art is introduced, the theoretical basis of the machine learning techniques applied are explained and the hydraulic model is also detailed. The whole methodology is presented and tested into different water distribution networks and district metered areas based on simulated and real case studies and compared with published methods. The focus of the contributions is to bring more robust methods against the uncertainties that effects the problem of leak detection, by dealing with them using the self-similarity to create features monitored by the change detection technique intersection-of-confidence-interval, and the leak localization where the problem is tackled using machine learning techniques. By using those techniques, it is expected to learn the leak behavior considering their uncertainty to be used in the diagnosis stage after the training phase. One method for the leak detection problem is presented that is able to estimate the leak size and the time that the leak has been produced. This method captures the normal, leak-free, behavior and contrast it with the new measurements in order to evaluate the state of the network. If the behavior is not normal check if it is due to a leak. To have a more robust leak detection method, a specific validation is designed to operate specifically with leaks and in the temporal region where the leak is most apparent. A methodology to extent the current model-based approach to localize water leaks by means of classifiers is proposed where the non-parametric k-nearest neighbors classifier and the parametric multi-class Bayesian classifier are proposed. A new data-driven approach to localize leaks using a multivariate regression technique without the use of hydraulic models is also introduced. This method presents a clear benefit over the model-based technique by removing the need of the hydraulic model despite of the topological information is still required. Also, the information of the expected leaks is not required since information of the expected hydraulic behavior with leak is exploited to find the place where the leak is more suitable. This method has a good performance in practice, but is very sensitive to the number of sensor in the network and their sensor placement. The proposed sensor placement techniques reduce the computational load required to take into account the amount of data needed to model the uncertainty compared with other optimization approaches while are designed to work with the leak localization problem. More precisely, the proposed hybrid feature selection technique for sensor placement is able to work with any method that can be evaluated with confusion matrix and still being specialized for the leak localization task. This last method is good for a few sensors, but lacks of precision when the number of sensors to place is large. To overcome this problem an incremental sensor placement is proposed which is better for a larger number of sensors to place but worse when the number is small.","",""
2,"Jiayi Tang, Alex Henderson, P. Gardner","Exploring AdaBoost and Random Forests machine learning approaches for infrared pathology on unbalanced data sets.",2021,"","","","",15,"2022-07-13 09:39:44","","10.1039/D0AN02155E","","",,,,,2,2.00,1,3,1,"The use of infrared spectroscopy to augment decision-making in histopathology is a promising direction for the diagnosis of many disease types. Hyperspectral images of healthy and diseased tissue, generated by infrared spectroscopy, are used to build chemometric models that can provide objective metrics of disease state. It is important to build robust and stable models to provide confidence to the end user. The data used to develop such models can have a variety of characteristics which can pose problems to many model-building approaches. Here we have compared the performance of two machine learning algorithms - AdaBoost and Random Forests - on a variety of non-uniform data sets. Using samples of breast cancer tissue, we devised a range of training data capable of describing the problem space. Models were constructed from these training sets and their characteristics compared. In terms of separating infrared spectra of cancerous epithelium tissue from normal-associated tissue on the tissue microarray, both AdaBoost and Random Forests algorithms were shown to give excellent classification performance (over 95% accuracy) in this study. AdaBoost models were more robust when datasets with large imbalance were provided. The outcomes of this work are a measure of classification accuracy as a function of training data available, and a clear recommendation for choice of machine learning approach.","",""
11,"M. Park, Kuk-jin Yoon","Learning and Selecting Confidence Measures for Robust Stereo Matching",2019,"","","","",16,"2022-07-13 09:39:44","","10.1109/TPAMI.2018.2837760","","",,,,,11,3.67,6,2,3,"We present a robust approach for computing disparity maps with a supervised learning-based confidence prediction. This approach takes into consideration following features. First, we analyze the characteristics of various confidence measures in the random forest framework to select effective confidence measures depending on the characteristics of the training data and matching strategies, such as similarity measures and parameters. We then train a random forest using the selected confidence measures to improve the efficiency of confidence prediction and to build a better prediction model. Second, we present a confidence-based matching cost modulation scheme, based on predicted confidence values, to improve the robustness and accuracy of the (semi-) global stereo matching algorithms. Finally, we apply the proposed modulation scheme to popularly used algorithms to make them robust against unexpected difficulties that could occur in an uncontrolled environment using challenging outdoor datasets. The proposed confidence measure selection and cost modulation schemes are experimentally verified from various perspectives using the KITTI and Middlebury datasets.","",""
1,"Khaled A. Ismail, M. A. E. Ghany","Survey on Machine Learning Algorithms Enhancing the Functional Verification Process",2021,"","","","",17,"2022-07-13 09:39:44","","10.3390/electronics10212688","","",,,,,1,1.00,1,2,1,"The continuing increase in functional requirements of modern hardware designs means the traditional functional verification process becomes inefficient in meeting the time-to-market goal with sufficient level of confidence in the design. Therefore, the need for enhancing the process is evident. Machine learning (ML) models proved to be valuable for automating major parts of the process, which have typically occupied the bandwidth of engineers; diverting them from adding new coverage metrics to make the designs more robust. Current research of deploying different (ML) models prove to be promising in areas such as stimulus constraining, test generation, coverage collection and bug detection and localization. An example of deploying artificial neural network (ANN) in test generation shows 24.5× speed up in functionally verifying a dual-core RISC processor specification. Another study demonstrates how k-means clustering can reduce redundancy of simulation trace dump of an AHB-to-WHISHBONE bridge by 21%, thus reducing the debugging effort by not having to inspect unnecessary waveforms. The surveyed work demonstrates a comprehensive overview of current (ML) models enhancing the functional verification process from which an insight of promising future research areas is inferred.","",""
9,"Min-Gyu Park, Kuk-Jin Yoon","Learning and Selecting Confidence Measures for Robust Stereo Matching.",2019,"","","","",18,"2022-07-13 09:39:44","","10.1109/TPAMI.2018.2837760","","",,,,,9,3.00,5,2,3,"We present a robust approach for computing disparity maps with a supervised learning-based confidence prediction. This approach takes into consideration following features. First, we analyze the characteristics of various confidence measures in the random forest framework to select effective confidence measures depending on the characteristics of the training data and matching strategies, such as similarity measures and parameters. We then train a random forest using the selected confidence measures to improve the efficiency of confidence prediction and to build a better prediction model. Second, we present a confidence-based matching cost modulation scheme, based on predicted confidence values, to improve the robustness and accuracy of the (semi-) global stereo matching algorithms. Finally, we apply the proposed modulation scheme to popularly used algorithms to make them robust against unexpected difficulties that could occur in an uncontrolled environment using challenging outdoor datasets. The proposed confidence measure selection and cost modulation schemes are experimentally verified from various perspectives using the KITTI and Middlebury datasets.","",""
0,"M. Ntampaka, Matthew Ho, B. Nord","Building Trustworthy Machine Learning Models for Astronomy",2021,"","","","",19,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,3,1,". Astronomy is entering an era of data-driven discovery, due in part to modern machine learning (ML) techniques enabling powerful new ways to interpret observations. This shift in our scientiﬁc approach requires us to consider whether we can trust the black box. Here, we overview methods for an often-overlooked step in the development of ML models: building community trust in the algorithms. Trust is an essential ingredient not just for creating more robust data analysis techniques, but also for building conﬁdence within the astronomy community to embrace machine learning methods and results.","",""
0,"S. Saha, H. Singh, A. Soliman, S. Rajasekaran","A novel computational methodology for GWAS multi-locus analysis based on graph theory and machine learning",2021,"","","","",20,"2022-07-13 09:39:44","","10.1101/2021.10.22.21265388","","",,,,,0,0.00,0,4,1,"Background: Current form of genome-wide association studies (GWAS) is inadequate to accurately explain the genetics of complex traits due to the lack of sufficient statistical power. It explores each variant individually, but current studies show that multiple variants with varying effect sizes actually act in a concerted way to develop a complex disease. To address this issue, we have developed an algorithmic framework that can effectively solve the multi-locus problem in GWAS with a very high level of confidence. Our methodology consists of three novel algorithms based on graph theory and machine learning. It identifies a set of highly discriminating variants that are stable and robust with little (if any) spuriousness. Consequently, likely these variants should be able to interpret missing heritability of a convoluted disease as an entity. Results: To demonstrate the efficacy of our proposed algorithms, we have considered astigmatism case-control GWAS dataset. Astigmatism is a common eye condition that causes blurred vision because of an error in the shape of the cornea. The cause of astigmatism is not entirely known but a sizable inheritability is assumed. Clinical studies show that developmental disorders (such as, autism) and astigmatism co-occur in a statistically significant number of individuals. By performing classical GWAS analysis, we didn't find any genome-wide statistically significant variants. Conversely, we have identified a set of stable, robust, and highly predictive variants that can together explain the genetics of astigmatism. We have performed a set of biological enrichment analyses based on gene ontology (GO) terms, disease ontology (DO) terms, biological pathways, network of pathways, and so forth to manifest the accuracy and novelty of our findings. Conclusions: Rigorous experimental evaluations show that our proposed methodology can solve GWAS multi-locus problem effectively and efficiently. It can identify signals from the GWAS dataset having small number of samples with a high level of accuracy. We believe that the proposed methodology based on graph theory and machine learning is the most comprehensive one compared to any other machine learning based tools in this domain.","",""
657,"Wieland Brendel, Jonas Rauber, M. Bethge","Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",2017,"","","","",21,"2022-07-13 09:39:44","","","","",,,,,657,131.40,219,3,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .","",""
15,"Lin Li, X. Guo, N. Ansari","SmartLoc: Smart Wireless Indoor Localization Empowered by Machine Learning",2020,"","","","",22,"2022-07-13 09:39:44","","10.1109/TIE.2019.2931261","","",,,,,15,7.50,5,3,2,"Recently, machine learning (ML) has been widely adopted for fingerprint-based indoor localization because of its potency in delineating relationships between received signal strength (RSS) information and labels accurately. Existing ML-based indoor localization systems are less robust because they only adopt the output with the highest probability. This affects the final location estimate, hence compromising accuracy due to the severity of RSS fluctuations. Since different ML algorithms (MLAs) yield different performances, it is therefore intuitive to fuse predictions from multiple MLAs to improve the positioning performance in the presence of signal fluctuation. In this article, we propose SmartLoc, a smart wireless indoor localization framework to enhance indoor localization. In the offline phase, multiple MLAs are trained by utilizing an offline database. We further apply probability alignment to guarantee the predicted probabilities of each MLA at the same confidence level. In the online phase, given a testing RSS sample of a user at an unknown location, we extract the labels with probabilities greater than a certain threshold from each MLA to construct the space of candidate labels (SCL). The size of SCL can be adaptively determined by using our proposed dynamic size determination algorithm. Based on the SCL, we propose a probabilistic model to intelligently estimate the user's location by evaluating the label credibility simultaneously. A high label credibility indicates that the frequently occurred label is more likely to be true. Experimental results in a real changing environment verify the superiority of SmartLoc, outperforming the best among comparative methods by 10.8% in 75th percentile accuracy.","",""
4,"G. Whelan, D. McDowell","Machine Learning-Enabled Uncertainty Quantification for Modeling Structure–Property Linkages for Fatigue Critical Engineering Alloys Using an ICME Workflow",2020,"","","","",23,"2022-07-13 09:39:44","","10.1007/s40192-020-00192-2","","",,,,,4,2.00,2,2,2,"","",""
3,"Xubo Leng, Margot Wohl, Kenichi Ishii, Pavan Nayak, Kenta Asahina","Quantitative comparison of Drosophila behavior annotations by human observers and a machine learning algorithm",2020,"","","","",24,"2022-07-13 09:39:44","","10.1101/2020.06.16.153130","","",,,,,3,1.50,1,5,2,"Automated quantification of behavior is increasingly prevalent in neuroscience research. Human judgments can influence machine-learning-based behavior classification at multiple steps in the process, for both supervised and unsupervised approaches. Such steps include the design of the algorithm for machine learning, the methods used for animal tracking, the choice of training images, and the benchmarking of classification outcomes. However, how these design choices contribute to the interpretation of automated behavioral classifications has not been extensively characterized. Here, we quantify the effects of experimenter choices on the outputs of automated classifiers of Drosophila social behaviors. Drosophila behaviors contain a considerable degree of variability, which was reflected in the confidence levels associated with both human and computer classifications. We found that a diversity of sex combinations and tracking features was important for robust performance of the automated classifiers. In particular, features concerning the relative position of flies contained useful information for training a machine-learning algorithm. These observations shed light on the importance of human influence on tracking algorithms, the selection of training images, and the quality of annotated sample images used to benchmark the performance of a classifier (the ‘ground truth’). Evaluation of these factors is necessary for researchers to accurately interpret behavioral data quantified by a machine-learning algorithm and to further improve automated classifications. Significance Statement Accurate quantification of animal behaviors is fundamental to neuroscience. Here, we quantitatively assess how human choices influence the performance of automated classifiers trained by a machine-learning algorithm. We found that human decisions about the computational tracking method, the training images, and the images used for performance evaluation impact both the classifier outputs and how human observers interpret the results. These factors are sometimes overlooked but are critical, especially because animal behavior is itself inherently variable. Automated quantification of animal behavior is becoming increasingly prevalent: our results provide a model for bridging the gap between traditional human annotations and computer-based annotations. Systematic assessment of human choices is important for developing behavior classifiers that perform robustly in a variety of experimental conditions.","",""
2,"Xubo Leng, Margot Wohl, Kenichi Ishii, Pavan Nayak, Kenta Asahina","Quantifying influence of human choice on the automated detection of Drosophila behavior by a supervised machine learning algorithm",2020,"","","","",25,"2022-07-13 09:39:44","","10.1371/journal.pone.0241696","","",,,,,2,1.00,0,5,2,"Automated quantification of behavior is increasingly prevalent in neuroscience research. Human judgments can influence machine-learning-based behavior classification at multiple steps in the process, for both supervised and unsupervised approaches. Such steps include the design of the algorithm for machine learning, the methods used for animal tracking, the choice of training images, and the benchmarking of classification outcomes. However, how these design choices contribute to the interpretation of automated behavioral classifications has not been extensively characterized. Here, we quantify the effects of experimenter choices on the outputs of automated classifiers of Drosophila social behaviors. Drosophila behaviors contain a considerable degree of variability, which was reflected in the confidence levels associated with both human and computer classifications. We found that a diversity of sex combinations and tracking features was important for robust performance of the automated classifiers. In particular, features concerning the relative position of flies contained useful information for training a machine-learning algorithm. These observations shed light on the importance of human influence on tracking algorithms, the selection of training images, and the quality of annotated sample images used to benchmark the performance of a classifier (the ‘ground truth’). Evaluation of these factors is necessary for researchers to accurately interpret behavioral data quantified by a machine-learning algorithm and to further improve automated classifications.","",""
2,"Victor Venturi, Holden L Parks, Zeeshan Ahmad, V. Viswanathan","Machine Learning Enabled Discovery of Application Dependent Design Principles for Two-dimensional Materials",2020,"","","","",26,"2022-07-13 09:39:44","","10.1088/2632-2153/ABA002","","",,,,,2,1.00,1,4,2,"The large-scale search for high-performing candidate 2D materials is limited to calculating a few simple descriptors, usually with first-principles density functional theory calculations. In this work, we alleviate this issue by extending and generalizing crystal graph convolutional neural networks to systems with planar periodicity, and train an ensemble of models to predict thermodynamic, mechanical, and electronic properties. To demonstrate the utility of this approach, we carry out a screening of nearly 45,000 structures for two largely disjoint applications: namely, mechanically robust composites and photovoltaics. An analysis of the uncertainty associated with our methods indicates the ensemble of neural networks is well-calibrated and has errors comparable with those from accurate first-principles density functional theory calculations. The ensemble of models allows us to gauge the confidence of our predictions, and to find the candidates most likely to exhibit effective performance in their applications. Since the datasets used in our screening were combinatorically generated, we are also able to investigate, using an innovative method, structural and compositional design principles that impact the properties of the structures surveyed and which can act as a generative model basis for future material discovery through reverse engineering. Our approach allowed us to recover some well-accepted design principles: for instance, we find that hybrid organic-inorganic perovskites with lead and tin tend to be good candidates for solar cell applications.","",""
0,"J. Z. Pan, Nicholas Zufelt","On Intrinsic Dataset Properties for Adversarial Machine Learning",2020,"","","","",27,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,2,2,"Deep neural networks (DNNs) have played a key role in a wide range of machine learning applications. However, DNN classifiers are vulnerable to human-imperceptible adversarial perturbations, which can cause them to misclassify inputs with high confidence. Thus, creating robust DNNs which can defend against malicious examples is critical in applications where security plays a major role. In this paper, we study the effect of intrinsic dataset properties on the performance of adversarial attack and defense methods, testing on five popular image classification datasets - MNIST, Fashion-MNIST, CIFAR10/CIFAR100, and ImageNet. We find that input size and image contrast play key roles in attack and defense success. Our discoveries highlight that dataset design and data preprocessing steps are important to boost the adversarial robustness of DNNs. To our best knowledge, this is the first comprehensive work that studies the effect of intrinsic dataset properties on adversarial machine learning.","",""
0,"A. Chakrabarty, K. Berntorp, S. D. Cairano","Learning-based Parameter-Adaptive Reference Governors /Author=Chakrabarty, Ankush; Berntorp, Karl; Di Cairano, Stefano /CreationDate=July 3, 2020 /Subject=Control, Machine Learning",2020,"","","","",28,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,3,2,"Reference governors (RGs) provide an effective method for ensuring safety via constraint enforcement in closedloop control systems. When the parameters of the underlying systems are unknown, but constant or slowly-varying, robust formulations of RGs that consider only the worst-case effect may be overly conservative and exhibit poor performance. This paper proposes a parameter-adaptive reference governor (PARG) architecture that is capable of generating safe trajectories in spite of parameter uncertainties without being as conservative as robust RGs. The proposed approach leverages on-line data to inform algorithms for robust parameter estimation. Subsequently, confidence bounds around parameter estimates are fed to supervised machine learners for approximating robust constraint admissible sets leveraged by the PARG. While initially, due to the absence of on-line data, the PARG may be as conservative as a robust RG, as more data is gathered and the confidence bounds become tighter, such conservativeness reduces, as demonstrated in a simulation example. American Control Conference (ACC) This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Copyright c © Mitsubishi Electric Research Laboratories, Inc., 2020 201 Broadway, Cambridge, Massachusetts 02139 Learning-based Parameter-Adaptive Reference Governors Ankush Chakrabarty†, Karl Berntorp, Stefano Di Cairano Abstract—Reference governors (RGs) provide an effective method for ensuring safety via constraint enforcement in closedloop control systems. When the parameters of the underlying systems are unknown, but constant or slowly-varying, robust formulations of RGs that consider only the worst-case effect may be overly conservative and exhibit poor performance. This paper proposes a parameter-adaptive reference governor (PARG) architecture that is capable of generating safe trajectories in spite of parameter uncertainties without being as conservative as robust RGs. The proposed approach leverages on-line data to inform algorithms for robust parameter estimation. Subsequently, confidence bounds around parameter estimates are fed to supervised machine learners for approximating robust constraint admissible sets leveraged by the PARG. While initially, due to the absence of on-line data, the PARG may be as conservative as a robust RG, as more data is gathered and the confidence bounds become tighter, such conservativeness reduces, as demonstrated in a simulation example.","",""
34,"Narathip Reamaroon, M. Sjoding, Kaiwen Lin, T. Iwashyna, K. Najarian","Accounting for Label Uncertainty in Machine Learning for Detection of Acute Respiratory Distress Syndrome",2019,"","","","",29,"2022-07-13 09:39:44","","10.1109/JBHI.2018.2810820","","",,,,,34,11.33,7,5,3,"When training a machine learning algorithm for a supervised-learning task in some clinical applications, uncertainty in the correct labels of some patients may adversely affect the performance of the algorithm. For example, even clinical experts may have less confidence when assigning a medical diagnosis to some patients because of ambiguity in the patient's case or imperfect reliability of the diagnostic criteria. As a result, some cases used in algorithm training may be mislabeled, adversely affecting the algorithm's performance. However, experts may also be able to quantify their diagnostic uncertainty in these cases. We present a robust method implemented with support vector machines (SVM) to account for such clinical diagnostic uncertainty when training an algorithm to detect patients who develop the acute respiratory distress syndrome (ARDS). ARDS is a syndrome of the critically ill that is diagnosed using clinical criteria known to be imperfect. We represent uncertainty in the diagnosis of ARDS as a graded weight of confidence associated with each training label. We also performed a novel time-series sampling method to address the problem of intercorrelation among the longitudinal clinical data from each patient used in model training to limit overfitting. Preliminary results show that we can achieve meaningful improvement in the performance of algorithm to detect patients with ARDS on a hold-out sample, when we compare our method that accounts for the uncertainty of training labels with a conventional SVM algorithm.","",""
0,"Mark A. Miller, Hayden Freedman, C. Stoeckert","A Robust, Self-Training Classifier for Medication Strings, with Quantitative and Semantic Confidence Metrics",2020,"","","","",30,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,3,2,"The TURBO Medication Mapper (TMM) identifies terms from RxNorm that best represent a list of medication strings, like one would find in a Clinical Data Warehouse (CDW). TMM has several differentiating characteristics, compared to other tools: the machine learning component does not require a human-curated gold standard for training; normalizations are applied to source-specific language in the strings (instead of just excluding them); the confidence of each mapping is represented as a relationship to the absolute truth, along with a 0.0-1.0 score; the results, along with supporting knowledge, are saved into an RDF graph and a Solr document database is generated. Queries for drug classes like “statins” are based on OBO foundry ontologies like the Drug Ontology (DrOn) and ChEBI, and they return more results than multiple SQL search strategies over the CDW, with few false positives. TMM is available for download from GitHub.","",""
14,"B. Ansell, B. Pope, P. Georgeson, Samantha J. Emery-Corbin, A. Jex","Annotation of the Giardia proteome through structure-based homology and machine learning",2018,"","","","",31,"2022-07-13 09:39:44","","10.1093/gigascience/giy150","","",,,,,14,3.50,3,5,4,"Abstract Background Large-scale computational prediction of protein structures represents a cost-effective alternative to empirical structure determination with particular promise for non-model organisms and neglected pathogens. Conventional sequence-based tools are insufficient to annotate the genomes of such divergent biological systems. Conversely, protein structure tolerates substantial variation in primary amino acid sequence and is thus a robust indicator of biochemical function. Structural proteomics is poised to become a standard part of pathogen genomics research; however, informatic methods are now required to assign confidence in large volumes of predicted structures. Aims Our aim was to predict the proteome of a neglected human pathogen, Giardia duodenalis, and stratify predicted structures into high- and lower-confidence categories using a variety of metrics in isolation and combination. Methods We used the I-TASSER suite to predict structural models for ∼5,000 proteins encoded in G. duodenalis and identify their closest empirically-determined structural homologues in the Protein Data Bank. Models were assigned to high- or lower-confidence categories depending on the presence of matching protein family (Pfam) domains in query and reference peptides. Metrics output from the suite and derived metrics were assessed for their ability to predict the high-confidence category individually, and in combination through development of a random forest classifier. Results We identified 1,095 high-confidence models including 212 hypothetical proteins. Amino acid identity between query and reference peptides was the greatest individual predictor of high-confidence status; however, the random forest classifier outperformed any metric in isolation (area under the receiver operating characteristic curve = 0.976) and identified a subset of 305 high-confidence-like models, corresponding to false-positive predictions. High-confidence models exhibited greater transcriptional abundance, and the classifier generalized across species, indicating the broad utility of this approach for automatically stratifying predicted structures. Additional structure-based clustering was used to cross-check confidence predictions in an expanded family of Nek kinases. Several high-confidence-like proteins yielded substantial new insight into mechanisms of redox balance in G. duodenalis—a system central to the efficacy of limited anti-giardial drugs. Conclusion Structural proteomics combined with machine learning can aid genome annotation for genetically divergent organisms, including human pathogens, and stratify predicted structures to promote efficient allocation of limited resources for experimental investigation.","",""
1,"Sicheng Jiang, Sirui Lu, D. Deng","Adversarial Machine Learning Phases of Matter",2019,"","","","",32,"2022-07-13 09:39:44","","","","",,,,,1,0.33,0,3,3,"We study the robustness of machine learning approaches to adversarial perturbations, with a focus on supervised learning scenarios. We find that typical phase classifiers based on deep neural networks are extremely vulnerable to adversarial perturbations: adding a tiny amount of carefully crafted noises into the original legitimate examples will cause the classifiers to make incorrect predictions at a notably high confidence level. Through the lens of activation maps, we find that some important underlying physical principles and symmetries remain to be adequately captured for classifiers with even near-perfect performance. This explains why adversarial perturbations exist for fooling these classifiers. In addition, we find that, after adversarial training the classifiers will become more consistent with physical laws and consequently more robust to certain kinds of adversarial perturbations. Our results provide valuable guidance for both theoretical and experimental future studies on applying machine learning techniques to condensed matter physics.","",""
0,"S. Mordensky, J. Lipor, J. DeAngelo, E. Burns, Cary R. Lindsey","Predicting Geothermal Favorability in the Western United States by Using Machine Learning: Addressing Challenges and Developing Solutions",2022,"","","","",33,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,5,1,"Previous moderate- and high-temperature geothermal resource assessments of the western United States utilized weight-of-evidence and logistic regression methods to estimate resource favorability, but these analyses relied upon some expert decisions. While expert decisions can add confidence to aspects of the modeling process by ensuring only reasonable models are employed, expert decisions also introduce human bias into assessments. This bias presents a source of error that may affect the performance of the models and resulting resource estimates. Our study aims to reduce expert input through robust data-driven analyses and better-suited data science techniques, with the goals of saving time, reducing bias, and improving predictive ability. We present six favorability maps for geothermal resources in the western United States created using two strategies applied to three modern machine learning algorithms (logistic regression, support-vector machines, and XGBoost). To provide a direct comparison to previous assessments, we use the same input data as the 2008 U.S. Geological Survey (USGS) conventional moderate- to high-temperature geothermal resource assessment. The six new favorability maps required far less expert decision-making, but broadly agree with the previous assessment. Despite the fact that the 2008 assessment results employed linear methods, the non-linear machine learning algorithms ( i.e., support-vector machines and XGBoost) produced greater agreement with the previous assessment than the linear machine learning algorithm ( i.e., logistic regression). It is not surprising that geothermal systems depend on non-linear combinations of features, and we postulate that the expert decisions during the 2008 assessment accounted for system non-linearities. Substantial challenges to applying machine learning algorithms to predict geothermal resource favorability include severe class imbalance ( i.e., there are very few known geothermal systems compared to the large area considered), and while there are known geothermal systems ( i.e., positive labels), all other sites have an unknown status ( i.e., they are unlabeled), instead of receiving a negative label ( i.e., the known/proven absence of a geothermal resource). We address both challenges through a custom undersampling strategy that can be used with any algorithm and then evaluated using F1 scores. for XGBoost: class weight, learning rate, number of estimators, and maximum depth of estimators. Class weight in XGBoost differs in exact implementation compared with logistic regression and SVMs, but this hyperparameter serves much the same purpose: a greater class weight places greater emphasis on accurately predicting positive labels ( i.e., known geothermal systems) than non-positive labels ( i.e., unknown resource potential). The other parameters are used to maximize prediction performance while also avoiding overfitting (Chen and Guestrin, 2016). We leave the other parameters of XGBoost at the default values found in Python’s XGBoost module as they pertain to the specifics of the optimization routine and have only a modest impact on performance (Chen and Guestrin, 2016).","",""
0,"James Hammond, Nick Pepper, F. Montomoli, V. Michelassi","Machine Learning Methods in CFD for Turbomachinery: A Review",2022,"","","","",34,"2022-07-13 09:39:44","","10.3390/ijtpp7020016","","",,,,,0,0.00,0,4,1,"Computational Fluid Dynamics is one of the most relied upon tools in the design and analysis of components in turbomachines. From the propulsion fan at the inlet, through the compressor and combustion sections, to the turbines at the outlet, CFD is used to perform fluid flow and heat transfer analyses to help designers extract the highest performance out of each component. In some cases, such as the design point performance of the axial compressor, current methods are capable of delivering good predictive accuracy. However, many areas require improved methods to give reliable predictions in order for the relevant design spaces to be further explored with confidence. This paper illustrates recent developments in CFD for turbomachinery which make use of machine learning techniques to augment prediction accuracy, speed up prediction times, analyse and manage uncertainty and reconcile simulations with available data. Such techniques facilitate faster and more robust searches of the design space, with or without the help of optimization methods, and enable innovative designs which keep pace with the demand for improved efficiency and sustainability as well as parts and asset operation cost reduction.","",""
327,"Nicolas Papernot, P. Mcdaniel","Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning",2018,"","","","",35,"2022-07-13 09:39:44","","","","",,,,,327,81.75,164,2,4,"Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.","",""
3,"Jiantao Liu, Xiaoxiang Yang, Mingzhu Zhu","Neural Network with Confidence Kernel for Robust Vibration Frequency Prediction",2019,"","","","",36,"2022-07-13 09:39:44","","10.1155/2019/6573513","","",,,,,3,1.00,1,3,3,"Image-based measurement has received increasing attention as it can substantially reduce the cost of labor, measurement equipment, and installation process. Instead of using optical flow, pattern, or marker tracking to extract a displacement signal, in this study, a novel noncontact machine learning-based system was proposed to directly predict vibration frequency with high accuracy and good reliability by using image sequences acquired from a single camera. The performance of the proposed method was demonstrated through experiments conducted in a laboratory and under real-field conditions and compared with those obtained using a contacted sensor. The vibration frequency prediction results of the proposed method are compared with industry-level vibration sensor results in the frequency domain, demonstrating that the proposed method could predict the target-object-vibration frequency as accurately as an industry-level vibration sensor, even under uncontrollable real-field conditions with no additional enhancement or extra signal processing techniques. However, only the principal vibration frequency of a measurement target is predicted, and the measurement range is limited by the trained model. Nonetheless, if these limitations are resolved, this method can potentially be used in real engineering applications in mechanical or civil structural health monitoring thanks to the simple deployment and concise pipeline of this method.","",""
89,"Huichen Lihuichen","DECISION-BASED ADVERSARIAL ATTACKS: RELIABLE ATTACKS AGAINST BLACK-BOX MACHINE LEARNING MODELS",2017,"","","","",37,"2022-07-13 09:39:44","","","","",,,,,89,17.80,89,1,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradientor score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available at XXXXXX. Gradient-based Model M Untargeted Flip to any label Targeted Flip to target label FGSM, DeepFool L-BFGS-B, Houdini, JSMA, Carlini & Wagner, Iterative Gradient Descent Score-based Detailed Model Prediction Y (e.g. probabilities or logits) ZOO Local Search Decision-based Final Model Prediction Ymax (e.g. max class label) this work (Boundary Attack) Transfer-based Training Data T","",""
0,"Hidetoshi Kawaguchi","Dummy training data generation method towards robust estimation of confidence value of semi- automatic agents for multi-class classification",2019,"","","","",38,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,1,3,"In this paper, we propose a dummy training data generation method towards robust estimation confidence value of semi-automatic agents for multi-class classification for the purpose of improving the performance. In the case where machine-learning is used for calculating the confidence value, there is a problem that can calculate accurately only with existing training data. Our approach is using simple random values. As a result of experiments of actual data, we confirmed the improvement of the performance.","",""
0,"Jiashuo Liu, Zheyan Shen, Peng Cui, Linjun Zhou, Kun Kuang, B. Li","Distributionally Robust Learning with Stable Adversarial Training",2021,"","","","",39,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,6,1,"Machine learning algorithms with empirical risk minimization are vulnerable under distributional shifts due to the greedy adoption of all the correlations found in training data. There is an emerging literature on tackling this problem by minimizing the worst-case risk over an uncertainty set. However, existing methods mostly construct ambiguity sets by treating all variables equally regardless of the stability of their correlations with the target, resulting in the overwhelmingly-large uncertainty set and low confidence of the learner. In this paper, we propose a novel Stable Adversarial Learning (SAL) algorithm that leverages heterogeneous data sources to construct a more practical uncertainty set and conduct differentiated robustness optimization, where covariates are differentiated according to the stability of their correlations with the target. We theoretically show that our method is tractable for stochastic gradient-based optimization and provide the performance guarantees for our method. Empirical studies on both simulation and real datasets validate the effectiveness of our method in terms of uniformly good performance across unknown distributional shifts.","",""
13,"Rajnish Kumar, Anju Sharma, M. H. Siddiqui, R. Tiwari","Promises of Machine Learning Approaches in Prediction of Absorption of Compounds.",2018,"","","","",40,"2022-07-13 09:39:44","","10.2174/1389557517666170315150116","","",,,,,13,3.25,3,4,4,"The Machine Learning (ML) is one of the fastest developing techniques in the prediction and evaluation of important pharmacokinetic properties such as absorption, distribution, metabolism and excretion. The availability of a large number of robust validation techniques for prediction models devoted to pharmacokinetics has significantly enhanced the trust and authenticity in ML approaches. There is a series of prediction models generated and used for rapid screening of compounds on the basis of absorption in last one decade. Prediction of absorption of compounds using ML models has great potential across the pharmaceutical industry as a non-animal alternative to predict absorption. However, these prediction models still have to go far ahead to develop the confidence similar to conventional experimental methods for estimation of drug absorption. Some of the general concerns are selection of appropriate ML methods and validation techniques in addition to selecting relevant descriptors and authentic data sets for the generation of prediction models. The current review explores published models of ML for the prediction of absorption using physicochemical properties as descriptors and their important conclusions. In addition, some critical challenges in acceptance of ML models for absorption are also discussed.","",""
12,"Zhongyang Zhang, Haoxiang Cheng, X. Hong, A. D. Narzo, O. Franzén, Shouneng Peng, A. Ruusalepp, J. Kovacic, J. Bjorkegren, Xiaobin Wang, K. Hao","EnsembleCNV: an ensemble machine learning algorithm to identify and genotype copy number variation using SNP array data",2018,"","","","",41,"2022-07-13 09:39:44","","10.1101/356667","","",,,,,12,3.00,1,11,4,"The associations between diseases/traits and copy number variants (CNVs) have not been systematically investigated in genome-wide association studies (GWASs), primarily due to a lack of robust and accurate tools for CNV genotyping. Herein, we propose a novel ensemble learning framework, ensembleCNV, to detect and genotype CNVs using single nucleotide polymorphism (SNP) array data. EnsembleCNV a) identifies and eliminates batch effects at raw data level; b) assembles individual CNV calls into CNV regions (CNVRs) from multiple existing callers with complementary strengths by a heuristic algorithm; c) re-genotypes each CNVR with local likelihood model adjusted by global information across multiple CNVRs; d) refines CNVR boundaries by local correlation structure in copy number intensities; e) provides direct CNV genotyping accompanied with confidence score, directly accessible for downstream quality control and association analysis. Benchmarked on two large datasets, ensembleCNV outperformed competing methods and achieved a high call rate (93.3%) and reproducibility (98.6%), while concurrently achieving high sensitivity by capturing 85% of common CNVs documented in the 1000 Genomes Project. Given this CNV call rate and accuracy, which are comparable to SNP genotyping, we suggest ensembleCNV holds significant promise for performing genome-wide CNV association studies and investigating how CNVs predispose to human diseases.","",""
8,"J. Coyle, N. Hejazi, I. Malenica, Rachael V. Phillips, B. Arnold, A. Mertens, J. Benjamin-Chung, Weixin Cai, Sonali Dayal, J. Colford, A. Hubbard, M. Laan","Targeting Learning: Robust Statistics for Reproducible Research",2020,"","","","",42,"2022-07-13 09:39:44","","","","",,,,,8,4.00,1,12,2,"Targeted Learning is a subfield of statistics that unifies advances in causal inference, machine learning and statistical theory to help answer scientifically impactful questions with statistical confidence. Targeted Learning is driven by complex problems in data science and has been implemented in a diversity of real-world scenarios: observational studies with missing treatments and outcomes, personalized interventions, longitudinal settings with time-varying treatment regimes, survival analysis, adaptive randomized trials, mediation analysis, and networks of connected subjects. In contrast to the (mis)application of restrictive modeling strategies that dominate the current practice of statistics, Targeted Learning establishes a principled standard for statistical estimation and inference (i.e., confidence intervals and p-values). This multiply robust approach is accompanied by a guiding roadmap and a burgeoning software ecosystem, both of which provide guidance on the construction of estimators optimized to best answer the motivating question. The roadmap of Targeted Learning emphasizes tailoring statistical procedures so as to minimize their assumptions, carefully grounding them only in the scientific knowledge available. The end result is a framework that honestly reflects the uncertainty in both the background knowledge and the available data in order to draw reliable conclusions from statistical analyses - ultimately enhancing the reproducibility and rigor of scientific findings.","",""
34,"Petrônio L. Braga, Adriano Oliveira, S. Meira","Software Effort Estimation Using Machine Learning Techniques with Robust Confidence Intervals",2007,"","","","",43,"2022-07-13 09:39:44","","10.1109/HIS.2007.56","","",,,,,34,2.27,11,3,15,"The precision and reliability of the estimation of the effort of software projects is very important for the competitiveness of software companies. Good estimates play a very important role in the management of software projects. Most methods proposed for effort estimation, including methods based on machine learning, provide only an estimate of the effort for a novel project. In this paper we introduce a method based on machine learning which gives the estimation of the effort together with a confidence interval for it. In our method, we propose to employ robust confidence intervals, which do not depend on the form of probability distribution of the errors in the training set. We report on a number of experiments using two datasets aimed to compare machine learning techniques for software effort estimation and to show that robust confidence intervals for the effort estimation can be successfully built.","",""
1,"Faiq Khalid, Muhammad Abdullah Hanif, Semeen Rehman, M. Shafique","ISA4ML: Training Data-Unaware Imperceptible Security Attacks on Machine Learning Modules of Autonomous Vehicles",2018,"","","","",44,"2022-07-13 09:39:44","","","","",,,,,1,0.25,0,4,4,"Due to big data analysis ability, machine learning (ML) algorithms are becoming popular for several applications in autonomous vehicles. However, ML algorithms possessinherent security vulnerabilities which increase the demand for robust ML algorithms. Recently, various groups have demonstrated how vulnerabilities in ML can be exploited to perform several security attacks for confidence reduction and random/targeted misclassification, by using the data manipulation techniques. These traditional data manipulation techniques, especially during the training stage, introduce the random visual noise. However, such visual noise can be detected during the attack or testing through noise detection/filtering or human-in-the-loop. In this paper, we propose a novel methodology to automatically generate an ""imperceptible attack"" by exploiting the back-propagation property of trained deep neural networks (DNNs). Unlike state-of-the-art inference attacks, our methodology does not require any knowledge of the training data set during the attack image generation. To illustrate the effectiveness of the proposed methodology, we present a case study for traffic sign detection in an autonomous driving use case. We deploy the state-of-the-art VGGNet DNN trained for German Traffic Sign Recognition Benchmarks (GTSRB) datasets. Our experimental results show that the generated attacks are imperceptible in both subjective tests (i.e., visual perception) and objective tests (i.e., without any noticeable change in the correlation and structural similarity index) but still performs successful misclassification attacks.","",""
33,"Petrônio L. Braga, Adriano Oliveira, S. Meira","Software Effort Estimation using Machine Learning Techniques with Robust Confidence Intervals",2007,"","","","",45,"2022-07-13 09:39:44","","10.1109/ICTAI.2007.172","","",,,,,33,2.20,11,3,15,"The precision and reliability of the estimation of the effort of software projects is very important for the competitiveness of software companies. Good estimates play a very important role in the management of software projects. Most methods proposed for effort estimation, including methods based on machine learning, provide only an estimate of the effort for a novel project. In this paper we introduce a method based on machine learning which gives the estimation of the effort together with a confidence interval for it. In our method, we propose to employ robust confidence intervals, which do not depend on the form of probability distribution of the errors in the training set. We report on a number of experiments using two datasets aimed to compare machine learning techniques for software effort estimation and to show that robust confidence intervals can be successfully built.","",""
142,"John C. Duchi, Hongseok Namkoong","Learning Models with Uniform Performance via Distributionally Robust Optimization",2018,"","","","",46,"2022-07-13 09:39:44","","10.1214/20-aos2004","","",,,,,142,35.50,71,2,4,"A common goal in statistics and machine learning is to learn models that can perform well against distributional shifts, such as latent heterogeneous subpopulations, unknown covariate shifts, or unmodeled temporal effects. We develop and analyze a distributionally robust stochastic optimization (DRO) framework that learns a model providing good performance against perturbations to the data-generating distribution. We give a convex formulation for the problem, providing several convergence guarantees. We prove finite-sample minimax upper and lower bounds, showing that distributional robustness sometimes comes at a cost in convergence rates. We give limit theorems for the learned parameters, where we fully specify the limiting distribution so that confidence intervals can be computed. On real tasks including generalizing to unknown subpopulations, fine-grained recognition, and providing good tail performance, the distributionally robust approach often exhibits improved performance.","",""
12,"W. Zhang, Hao Quan, Oktoviano Gandhi, Carlos D. Rodríguez-Gallegos, Anurag Sharma, D. Srinivasan","An ensemble machine learning based approach for constructing probabilistic PV generation forecasting",2017,"","","","",47,"2022-07-13 09:39:44","","10.1109/APPEEC.2017.8308947","","",,,,,12,2.40,2,6,5,"Photovoltaic (PV) generation forecasting plays an important role in accommodating more distributed PV sites into power systems. However, due to the stochastic nature of PV generation, conventional point forecast methods can hardly quantify the uncertainties of PV generation. Being capable of quantifying uncertainties, probabilistic forecasting tools, like prediction intervals (PIs), are receiving increasing attention. This paper proposes a new framework to construct PIs and make point forecasts. In the proposed framework, an efficient and robust algorithm is employed to perform quantile regression. Based on the quantile regression results, PIs for multiple confidence levels are constructed utilizing different quantiles. Simulation results on a PV generation system reveal that the proposed framework is more reliable and accurate, compared with state-of-the-art methods, as measured by multiple performance indices.","",""
39,"Indrajit Mandal, N. Sairam","New machine-learning algorithms for prediction of Parkinson's disease",2014,"","","","",48,"2022-07-13 09:39:44","","10.1080/00207721.2012.724114","","",,,,,39,4.88,20,2,8,"This article presents an enhanced prediction accuracy of diagnosis of Parkinson's disease (PD) to prevent the delay and misdiagnosis of patients using the proposed robust inference system. New machine-learning methods are proposed and performance comparisons are based on specificity, sensitivity, accuracy and other measurable parameters. The robust methods of treating Parkinson's disease (PD) includes sparse multinomial logistic regression, rotation forest ensemble with support vector machines and principal components analysis, artificial neural networks, boosting methods. A new ensemble method comprising of the Bayesian network optimised by Tabu search algorithm as classifier and Haar wavelets as projection filter is used for relevant feature selection and ranking. The highest accuracy obtained by linear logistic regression and sparse multinomial logistic regression is 100% and sensitivity, specificity of 0.983 and 0.996, respectively. All the experiments are conducted over 95% and 99% confidence levels and establish the results with corrected t-tests. This work shows a high degree of advancement in software reliability and quality of the computer-aided diagnosis system and experimentally shows best results with supportive statistical inference.","",""
2,"Enhan Liu, Yan Chu, Liying Zheng","Object Tracking Based on Compressive Features and Extreme Learning Machine",2019,"","","","",49,"2022-07-13 09:39:44","","10.1109/ACCESS.2019.2909667","","",,,,,2,0.67,1,3,3,"Effective object tracking is a great challenging task in computer vision due to the appearance changes of an object. Extreme learning machine (ELM) can analytically determine the weights of feed-forward neural networks, while compressive features can be extracted via a random sparse matrix. Thus, their common advantages are running fast. Moreover, the strong learning ability of ELM is helpful to make a tracker robust target appearance change. In this paper, we propose an effective tracking method using ELM and compressive features. First, a bank of compressive Haar-like features is extracted with the sparse random matrix. To make the feature invariant to illumination, the mean illumination of each image patch is removed from each Haar-like feature. Second, a bank of ELM-based weak classifiers are combined together to form a strong classifier to complete tracking task. Considering the discrimination performance of features is uneven, the training accuracy of each ELM is employed as the confidence (weight) of the weak classifier. Thus, less discriminative features will be weakened. The proposed tracker has been tested on several representative sequences, and the results show that it achieves the highest tracking success rate and the tracking accuracy among the compared trackers including the FCT, CNT, MIL, KCF, and ESPT.","",""
11,"Alexander Meinke, Julian Bitterwolf, Matthias Hein","Provably Robust Detection of Out-of-distribution Data (almost) for free",2021,"","","","",50,"2022-07-13 09:39:44","","","","",,,,,11,11.00,4,3,1,"When applying machine learning in safety-critical systems, a reliable assessment of the uncertainy of a classifier is required. However, deep neural networks are known to produce highly overconfident predictions on out-of-distribution (OOD) data and even if trained to be non-confident on OOD data one can still adversarially manipulate OOD data so that the classifer again assigns high confidence to the manipulated samples. In this paper we propose a novel method where from first principles we combine a certifiable OOD detector with a standard classifier into an OOD aware classifier. In this way we achieve the best of two worlds: certifiably adversarially robust OOD detection, even for OOD samples close to the in-distribution, without loss in prediction accuracy and close to state-of-the-art OOD detection performance for non-manipulated OOD data. Moreover, due to the particular construction our classifier provably avoids the asymptotic overconfidence problem of standard neural networks.","",""
513,"Ludwig Schmidt, Shibani Santurkar, Dimitris Tsipras, Kunal Talwar, A. Madry","Adversarially Robust Generalization Requires More Data",2018,"","","","",51,"2022-07-13 09:39:44","","","","",,,,,513,128.25,103,5,4,"Machine learning models are often susceptible to adversarial perturbations of their inputs. Even small perturbations can cause state-of-the-art classifiers with high ""standard"" accuracy to produce an incorrect prediction with high confidence. To better understand this phenomenon, we study adversarially robust learning from the viewpoint of generalization. We show that already in a simple natural data model, the sample complexity of robust learning can be significantly larger than that of ""standard"" learning. This gap is information theoretic and holds irrespective of the training algorithm or the model family. We complement our theoretical results with experiments on popular image classification datasets and show that a similar gap exists here as well. We postulate that the difficulty of training robust classifiers stems, at least partially, from this inherently larger sample complexity.","",""
153,"Dong Yin, K. Ramchandran, P. Bartlett","Rademacher Complexity for Adversarially Robust Generalization",2018,"","","","",52,"2022-07-13 09:39:44","","","","",,,,,153,38.25,51,3,4,"Many machine learning models are vulnerable to adversarial attacks; for example, adding adversarial perturbations that are imperceptible to humans can often make machine learning models produce wrong predictions with high confidence. Moreover, although we may obtain robust models on the training dataset via adversarial training, in some problems the learned models cannot generalize well to the test data. In this paper, we focus on $\ell_\infty$ attacks, and study the adversarially robust generalization problem through the lens of Rademacher complexity. For binary linear classifiers, we prove tight bounds for the adversarial Rademacher complexity, and show that the adversarial Rademacher complexity is never smaller than its natural counterpart, and it has an unavoidable dimension dependence, unless the weight vector has bounded $\ell_1$ norm. The results also extend to multi-class linear classifiers. For (nonlinear) neural networks, we show that the dimension dependence in the adversarial Rademacher complexity also exists. We further consider a surrogate adversarial loss for one-hidden layer ReLU network and prove margin bounds for this setting. Our results indicate that having $\ell_1$ norm constraints on the weight matrices might be a potential way to improve generalization in the adversarial setting. We demonstrate experimental results that validate our theoretical findings.","",""
2,"Aditya Jonnalagadda, I. Frosio, Seth Schneider, M. McGuire, Joohwan Kim","Robust Vision-Based Cheat Detection in Competitive Gaming",2021,"","","","",53,"2022-07-13 09:39:44","","10.1145/3451259","","",,,,,2,2.00,0,5,1,"Game publishers and anti-cheat companies have been unsuccessful in blocking cheating in online gaming. We propose a novel, vision-based approach that captures the frame buffer's final state and detects illicit overlays. To this aim, we train and evaluate a DNN detector on a new dataset, collected using two first-person shooter games and three cheating software. We study the advantages and disadvantages of different DNN architectures operating on a local or global scale. We use output confidence analysis to avoid unreliable detections and inform when network retraining is required. In an ablation study, we show how to use Interval Bound Propagation (IBP) to build a detector that is also resistant to potential adversarial attacks and study IBP's interaction with confidence analysis. Our results show that robust and effective anti-cheating through machine learning is practically feasible and can be used to guarantee fair play in online gaming.","",""
6,"Olivier Deiss, S. Biswal, Jing Jin, Haoqi Sun, M. Westover, Jimeng Sun","HAMLET: Interpretable Human And Machine co-LEarning Technique",2018,"","","","",54,"2022-07-13 09:39:44","","","","",,,,,6,1.50,1,6,4,"Efficient label acquisition processes are key to obtaining robust classifiers. However, data labeling is often challenging and subject to high levels of label noise. This can arise even when classification targets are well defined, if instances to be labeled are more difficult than the prototypes used to define the class, leading to disagreements among the expert community. Here, we enable efficient training of deep neural networks. From low-confidence labels, we iteratively improve their quality by simultaneous learning of machines and experts. We call it Human And Machine co-LEarning Technique (HAMLET). Throughout the process, experts become more consistent, while the algorithm provides them with explainable feedback for confirmation. HAMLET uses a neural embedding function and a memory module filled with diverse reference embeddings from different classes. Its output includes classification labels and highly relevant reference embeddings as explanation. We took the study of brain monitoring at intensive care unit (ICU) as an application of HAMLET on continuous electroencephalography (cEEG) data. Although cEEG monitoring yields large volumes of data, labeling costs and difficulty make it hard to build a classifier. Additionally, while experts agree on the labels of clear-cut examples of cEEG patterns, labeling many real-world cEEG data can be extremely challenging. Thus, a large minority of sequences might be mislabeled. HAMLET has shown significant performance gain against deep learning and other baselines, increasing accuracy from 7.03% to 68.75% on challenging inputs. Besides improved performance, clinical experts confirmed the interpretability of those reference embeddings in helping explaining the classification results by HAMLET.","",""
0,"Jian Hu","Data-Driven Distributionally Preference Robust Optimization Models Based on Random Utility Representation in Multi-Attribute Decision Making",2021,"","","","",55,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,1,1,"Preference robust optimization (PRO) has recently been studied to deal with utility based decision making problems under ambiguity in the characterization of the decision maker’s (DM) preference. In this paper, we propose a novel PRO modeling paradigm which combines the stochastic utility theory with distributionally robust optimization technique. Based on the stochastic utility theory, our model is applicable to problems with inconsistent and mutable preference representations which are ubiquitous in practice, particularly in group or social decision making. In the framework of distributionally robust optimization, data-driven approaches are discussed to construct two ambiguity sets of the probability distributions of the DM’s preference: one is an ellipsoidal moment region with a sample mean and sample covariance matrix, the other is a nonparametric percentile-t bootsrap confidence region. A numerical example of vehicle design demonstrates the effectiveness of the proposed model working with machine learning methods. We first depict the random preference in the structure of piecewise linear additive multi-attribute utility functions, which are either nondecreasing or risk averse, and next develop tractable reformulations and solution algorithms. Finally we extend to general continuous random utility functions and carry out convergence analysis of the proposed piecewise linear approximation as the size of sample data increases.","",""
0,"Dipti Mishra, Kumar, Kallurupalli","Building a Robust QA System that Knows When it Doesn ’ t Know",2021,"","","","",56,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,3,1,"Machine Learning models have a hard time knowing when they shouldn’t be confident about their output. They falter when they see data that is out of their training distribution. Unfortunately, they falter in a way that the scores don’t properly reflect. For example, an MNIST model is capable of predicting a chicken is actually the number 5 with 99% certainty.! While models are capable of generalizing to data they haven’t seen, they have a hard time knowing what data they can and can’t generalize to. A robust QA module should not only be able to do a good job of handling data it wasn’t modeled to handle, it should also be able to do a good job of knowing what data it can’t handle. The goal of our project is to initially build a good Question Answering model with an architecture that relies on a base of DistilBERT, improve on it through model fine-tuning, better optimization, and then augment the predictions of the model with a confidence score that is high when the model is reliable and low when the model is basically guessing on data it hasn’t seen. (M)","",""
0,"Yeongmin Lee, C. Kyung","A Memory- and Accuracy-Aware Gaussian Parameter-Based Stereo Matching Using Confidence Measure",2019,"","","","",57,"2022-07-13 09:39:44","","10.1109/TPAMI.2019.2959613","","",,,,,0,0.00,0,2,3,"Accurate stereo matching requires a large amount of memory at a high bandwidth, which restricts its use in resource-limited systems such as mobile devices. This problem is compounded by the recent trend of applications requiring significantly high pixel resolution and disparity levels. To alleviate this, we present a memory-efficient and robust stereo matching algorithm. For cost aggregation, we employ the semiglobal parametric approach, which significantly reduces the memory bandwidth by representing the costs of all disparities as a Gaussian mixture model. All costs on multiple paths in an image are aggregated by updating the Gaussian parameters. The aggregation is performed during the scanning in the forward and backward directions. To reduce the amount of memory for the intermediate results during the forward scan, we suggest to store only the Gaussian parameters which contribute significantly to the final disparity selection. We also propose a method to enhance the overall procedure through a learning-based confidence measure. The random forest framework is used to train various features which are extracted from the cost and intensity profile. The experimental results on KITTI dataset show that the proposed method reduces the memory requirement to less than 3 percent of that of semiglobal matching (SGM) while providing a robust depth map compared to those of state-of-the-art SGM-based algorithms.","",""
0,"Hemant Rathore","Designing Adversarial Robust and Explainable Malware Detection System for Android based Smartphones: PhD Forum Abstract",2021,"","","","",58,"2022-07-13 09:39:44","","10.1145/3412382.3459209","","",,,,,0,0.00,0,1,1,"Android smartphones and malware have grown exponentially in the last decade. Literature suggests that the current malware detection systems cannot cope with the present security challenges. Thus researchers are developing next-generation malware detection systems/models using the machine and deep learning. However, the proposed systems/models have poor explainability and are vulnerable against adversarial attacks, which will jeopardize their adoption in the future security ecosystem. Thus, we aim to construct adversarial robust malware detection models by first acting as an adversary to find vulnerabilities in models and then proposing preventive countermeasures. We also aim to improve models' explain-ability to win security community confidence before real-world implementation.","",""
0,"Yaping Wang, Zhicheng Peng, Riquan Zhang, Qian Xiao","Robust sequential design for piecewise-stationary multi-armed bandit problem in the presence of outliers",2021,"","","","",59,"2022-07-13 09:39:44","","10.1080/24754269.2021.1902687","","",,,,,0,0.00,0,4,1,"ABSTRACT The multi-armed bandit (MAB) problem studies the sequential decision making in the presence of uncertainty and partial feedback on rewards. Its name comes from imagining a gambler at a row of slot machines who needs to decide the best strategy on the number of times as well as the orders to play each machine. It is a classic reinforcement learning problem which is fundamental to many online learning problems. In many practical applications of the MAB, the reward distributions may change at unknown time steps and the outliers (extreme rewards) often exist. Current sequential design strategies may struggle in such cases, as they tend to infer additional change points to fit the outliers. In this paper, we propose a robust change-detection upper confidence bound (RCD-UCB) algorithm which can distinguish the real change points from the outliers in piecewise-stationary MAB settings. We show that the proposed RCD-UCB algorithm can achieve a nearly optimal regret bound on the order of , where T is the number of time steps, K is the number of arms and S is the number of stationary segments. We demonstrate its superior performance compared to some state-of-the-art algorithms in both simulation experiments and real data analysis. (See https://github.com/woaishufenke/MAB_STRF.git for the codes used in this paper.)","",""
6,"Pedro Cisneros-Velarde, Sang-Yun Oh, Alexander Petersen","Distributionally Robust Formulation and Model Selection for the Graphical Lasso",2019,"","","","",60,"2022-07-13 09:39:44","","","","",,,,,6,2.00,2,3,3,"Building on a recent framework for distributionally robust optimization in machine learning, we develop a similar framework for estimation of the inverse covariance matrix for multivariate data. We provide a novel notion of a Wasserstein ambiguity set specifically tailored to this estimation problem, from which we obtain a representation for a tractable class of regularized estimators. Special cases include penalized likelihood estimators for Gaussian data, specifically the graphical lasso estimator. As a consequence of this formulation, a natural relationship arises between the radius of the Wasserstein ambiguity set and the regularization parameter in the estimation problem. Using this relationship, one can directly control the level of robustness of the estimation procedure by specifying a desired level of confidence with which the ambiguity set contains a distribution with the true population covariance. Furthermore, a unique feature of our formulation is that the radius can be expressed in closed-form as a function of the ordinary sample covariance matrix. Taking advantage of this finding, we develop a simple algorithm to determine a regularization parameter for graphical lasso, using only the bootstrapped sample covariance matrices, meaning that computationally expensive repeated evaluation of the graphical lasso algorithm is not necessary. Alternatively, the distributionally robust formulation can also quantify the robustness of the corresponding estimator if one uses an off-the-shelf method such as cross-validation. Finally, we numerically study the obtained regularization criterion and analyze the robustness of other automated tuning procedures used in practice.","",""
1,"Michael Guarino, Pablo Rivas, C. DeCusatis","Towards Adversarially Robust DDoS-Attack Classification",2020,"","","","",61,"2022-07-13 09:39:44","","10.1109/UEMCON51285.2020.9298167","","",,,,,1,0.50,0,3,2,"On the frontier of cybersecurity are a class of emergent security threats that learn to find vulnerabilities in machine learning systems. A supervised machine learning classifier learns a mapping from x to y where x is the input features and y is a vector of associated labels. Neural Networks are state of the art performers on most vision, audio, and natural language processing tasks. Neural Networks have been shown to be vulnerable to adversarial perturbations of the input, which cause them to misclassify with high confidence. Adversarial perturbations are small but targeted modifications to the input often undetectable by the human eye. Adversarial perturbations pose risk to applications that rely on machine learning models. Neural Networks have been shown to be able to classify distributed denial of service (DDoS) attacks by learning a dataset of attack characteristics visualized using three-axis hive plots. In this work we present a novel application of a classifier trained to classify DDoS attacks that is robust to some of the most common, known, classes of gradient-based and gradient-free adversarial attacks.","",""
0,"Kamran Ghasedi Dizaji, Hongchang Gao, Yanhua Yang, Heng Huang, Cheng Deng","Robust Cumulative Crowdsourcing Framework Using New Incentive Payment Function and Joint Aggregation Model",2020,"","","","",62,"2022-07-13 09:39:44","","10.1109/TNNLS.2019.2956523","","",,,,,0,0.00,0,5,2,"In recent years, crowdsourcing has gained tremendous attention in the machine learning community due to the increasing demand for labeled data. However, the labels collected by crowdsourcing are usually unreliable and noisy. This issue is mainly caused by: 1) nonflexible data collection mechanisms; 2) nonincentive payment functions; and 3) inexpert crowd workers. We propose a new robust crowdsourcing framework as a comprehensive solution for all these challenging problems. Our unified framework consists of three novel components. First, we introduce a new flexible data collection mechanism based on the cumulative voting system, allowing crowd workers to express their confidence for each option in multi-choice questions. Second, we design a novel payment function regarding the settings of our data collection mechanism. The payment function is theoretically proved to be incentive-compatible, encouraging crowd workers to disclose truthfully their beliefs to get the maximum payment. Third, we propose efficient aggregation models, which are compatible with both single-option and multi-option crowd labels. We define a new aggregation model, called simplex constrained majority voting (SCMV), and enhance it by using the probabilistic generative model. Furthermore, fast optimization algorithms are derived for the proposed aggregation models. Experimental results indicate higher quality for the crowd labels collected by our proposed mechanism without increasing the cost. Our aggregation models also outperform the state-of-the-art models on multiple crowdsourcing data sets in terms of accuracy and convergence speed.","",""
11,"F. Cabitza, A. Campagner, D. Albano, A. Aliprandi, A. Bruno, V. Chianca, A. Corazza, F. Di Pietto, A. Gambino, S. Gitto, C. Messina, D. Orlandi, L. Pedone, M. Zappia, L. Sconfienza","The Elephant in the Machine: Proposing a New Metric of Data Reliability and its Application to a Medical Case to Assess Classification Reliability",2020,"","","","",63,"2022-07-13 09:39:44","","10.3390/app10114014","","",,,,,11,5.50,1,15,2,"In this paper, we present and discuss a novel reliability metric to quantify the extent a ground truth, generated in multi-rater settings, as a reliable basis for the training and validation of machine learning predictive models. To define this metric, three dimensions are taken into account: agreement (that is, how much a group of raters mutually agree on a single case); confidence (that is, how much a rater is certain of each rating expressed); and competence (that is, how accurate a rater is). Therefore, this metric produces a reliability score weighted for the raters’ confidence and competence, but it only requires the former information to be actually collected, as the latter can be obtained by the ratings themselves, if no further information is available. We found that our proposal was both more conservative and robust to known paradoxes than other existing agreement measures, by virtue of a more articulated notion of the agreement due to chance, which was based on an empirical estimation of the reliability of the single raters involved. We discuss the above metric within a realistic annotation task that involved 13 expert radiologists in labeling the MRNet dataset. We also provide a nomogram by which to assess the actual accuracy of a classification model, given the reliability of its ground truth. In this respect, we also make the point that theoretical estimates of model performance are consistently overestimated if ground truth reliability is not properly taken into account.","",""
7,"Jiashuo Liu, Zheyan Shen, Peng Cui, Linjun Zhou, Kun Kuang, B. Li, Yishi Lin","Stable Adversarial Learning under Distributional Shifts",2020,"","","","",64,"2022-07-13 09:39:44","","","","",,,,,7,3.50,1,7,2,"Machine learning algorithms with empirical risk minimization are vulnerable under distributional shifts due to the greedy adoption of all the correlations found in training data. Recently, there are robust learning methods aiming at this problem by minimizing the worst-case risk over an uncertainty set. However, they equally treat all covariates to form the decision sets regardless of the stability of their correlations with the target, resulting in the overwhelmingly large set and low confidence of the learner. In this paper, we propose Stable Adversarial Learning (SAL) algorithm that leverages heterogeneous data sources to construct a more practical uncertainty set and conduct differentiated robustness optimization, where covariates are differentiated according to the stability of their correlations with the target. We theoretically show that our method is tractable for stochastic gradientbased optimization and provide the performance guarantees for our method. Empirical studies on both simulation and real datasets validate the effectiveness of our method in terms of uniformly good performance across unknown distributional shifts.","",""
1,"Michael Austin Langford, B. Cheng","“Know What You Know”: Predicting Behavior for Learning-Enabled Systems When Facing Uncertainty",2021,"","","","",65,"2022-07-13 09:39:44","","10.1109/SEAMS51251.2021.00020","","",,,,,1,1.00,1,2,1,"Since deep learning systems do not generalize well when training data is incomplete and missing coverage of corner cases, it is difficult to ensure the robustness of safety-critical self-adaptive systems with deep learning components. Stakeholders require a reasonable level of confidence that a safety-critical system will behave as expected in all contexts. However, uncertainty in the behavior of safety-critical Learning-Enabled Systems (LESs) arises when run-time contexts deviate from training and validation data. To this end, this paper proposes an approach to develop a more robust safety-critical LES by predicting its learned behavior when exposed to uncertainty and thereby enabling mitigating countermeasures for predicted failures. By combining evolutionary computation with machine learning, an automated method is introduced to assess and predict the behavior of an LES when faced with previously unseen environmental conditions. By experimenting with Deep Neural Networks (DNNs) under a variety of adverse environmental changes, the proposed method is compared to a Monte Carlo (i.e., random sampling) method. Results indicate that when Monte Carlo sampling fails to capture uncommon system behavior, the proposed method is better at training behavior models with fewer training examples required.","",""
1,"Athindran Ramesh Kumar, Sulin Liu, J. Fisac, Ryan P. Adams, P. Ramadge","ProBF: Learning Probabilistic Safety Certificates with Barrier Functions",2021,"","","","",66,"2022-07-13 09:39:44","","","","",,,,,1,1.00,0,5,1,"Safety-critical applications require controllers/policies that can guarantee safety with high confidence. The control barrier function is a useful tool to guarantee safety if we have access to the ground-truth system dynamics. In practice, we have inaccurate knowledge of the system dynamics, which can lead to unsafe behaviors due to unmodeled residual dynamics. Learning the residual dynamics with deterministic machine learning models can prevent the unsafe behavior but can fail when the predictions are imperfect. In this situation, a probabilistic learning method that reasons about the uncertainty of its predictions can help provide robust safety margins. In this work, we use a Gaussian process to model the projection of the residual dynamics onto a control barrier function. We propose a novel optimization procedure to generate safe controls that can guarantee safety with high probability. The safety filter is provided with the ability to reason about the uncertainty of the predictions from the GP. We show the efficacy of this method through experiments on Segway and Quadrotor simulations. Our proposed probabilistic approach is able to reduce the number of safety violations significantly as compared to the deterministic approach with a neural network.","",""
0,"Renhai Xu, Wenxin Li, Keqiu Li, Xiaobo Zhou, Heng Qi","DarkTE: Towards Dark Traffic Engineering in Data Center Networks with Ensemble Learning",2021,"","","","",67,"2022-07-13 09:39:44","","10.1109/IWQOS52092.2021.9521298","","",,,,,0,0.00,0,5,1,"Over the last decade, traffic engineering (TE) has always been a research hotspot in data center networks. For routing flows efficiently and practically, existing TE schemes explore experience-driven heuristics or machine learning (ML) techniques to predict/identify network flows’ size information. However, these TE schemes have significant limitations: they either identify the flow size information too late or are unaware of the ML models’ prediction errors. In this paper, we present DarkTE, a novel TE solution that can learn to predict flow size information timely for achieving better routing performance while being robust to the prediction errors. At its heart, DarkTE employs an ensemble learning technique (i.e., random forest) to classify flows into mice and elephant flows with high accuracy. It then leverages a confidence-based rate allocation and path selection scheme to mitigate the occasional classification errors. Large-scale simulations demonstrate that DarkTE classifies flows within hundreds of microseconds, and the classification accuracy is at least 86.4% over three different realistic workloads. Further, DarkTE completes flows 2.94 times faster on average and makes more links to experience over 90% bandwidth utilization than the Hedera solution.","",""
11,"Cheng Ju, D. Benkeser, M. J. van der Laan","Robust inference on the average treatment effect using the outcome highly adaptive lasso",2018,"","","","",68,"2022-07-13 09:39:44","","10.1111/biom.13121","","",,,,,11,2.75,4,3,4,"Many estimators of the average effect of a treatment on an outcome require estimation of the propensity score, the outcome regression, or both. It is often beneficial to utilize flexible techniques, such as semiparametric regression or machine learning, to estimate these quantities. However, optimal estimation of these regressions does not necessarily lead to optimal estimation of the average treatment effect, particularly in settings with strong instrumental variables. A recent proposal addressed these issues via the outcome‐adaptive lasso, a penalized regression technique for estimating the propensity score that seeks to minimize the impact of instrumental variables on treatment effect estimators. However, a notable limitation of this approach is that its application is restricted to parametric models. We propose a more flexible alternative that we call the outcome highly adaptive lasso. We discuss the large sample theory for this estimator and propose closed‐form confidence intervals based on the proposed estimator. We show via simulation that our method offers benefits over several popular approaches.","",""
642,"Kaihua Zhang, Lei Zhang, Qingshan Liu, D. Zhang, Ming-Hsuan Yang","Fast Visual Tracking via Dense Spatio-temporal Context Learning",2014,"","","","",69,"2022-07-13 09:39:44","","10.1007/978-3-319-10602-1_9","","",,,,,642,80.25,128,5,8,"","",""
5,"Muhammad Rehan Abbas, M. Nadeem, Aliya Shaheen, Abdulrahman A. Alshdadi, Riad Alharbey, Seong-O Shim, W. Aziz","Accuracy Rejection Normalized-Cost Curves (ARNCCs): A Novel 3-Dimensional Framework for Robust Classification",2019,"","","","",70,"2022-07-13 09:39:44","","10.1109/ACCESS.2019.2950244","","",,,,,5,1.67,1,7,3,"Machine learning (ML) offers several supervised learning algorithms to build classifiers for developing accurate decision support systems. However, the selection of robust classifier for reliable decision making in healthcare domain is three-fold: accuracy, refraining for low confidence decisions, and the cost of decisions. In the field of medical science, there are costs associated with the incorrect and the refraining from decisions, which can have negative implications in devising adequate therapeutic interventions. For example, it may be life threating if a cancer patient is declared as healthy one (misclassification cost) or decision remains pending for some time (rejection cost). In this work we proposed the concept of Accuracy Rejection Normalized-Cost Curves (ARNCCs), which is an extension of Accuracy Rejection Curves (ARCs); a three-dimensional visualization technique to demonstrate the strengths and weaknesses of classification algorithms over different rejection regions and normalized-cost (NC) to select the robust classifier. ARNCCs method holds ARCs plot on two dimensions, in addition it computes NC at third dimension against ratio of false positive costs and ratio of rejection costs obtained at different rejection rates. The proposed three-dimensional graphs have the potential to answer a variety of questions regarding accuracy, rejection rate and NC of a classifier. Six publicly available cancer datasets (four breast cancer and two lung cancer) having clinical parameters obtained from ML repository of University of California, Irvine (UCI) were used to assess the performance of proposed ARNCCs in this study. Empirical results show that ARNCCs provide broad range of decisions to choose the desired parameters (accuracy, rejection rate and NC) for further necessary actions as compared to traditional ARCs method. ARNCCs framework has the ability to more logically compare the performances of classification algorithms in terms of accuracy, rejection rate and NC based scenarios.","",""
1,"Ilija Bogunovic","Robust Adaptive Decision Making: Bayesian Optimization and Beyond",2019,"","","","",71,"2022-07-13 09:39:44","","10.5075/epfl-thesis-9147","","",,,,,1,0.33,1,1,3,"The central task in many interactive machine learning systems can be formalized as the sequential optimization of a black-box function. Bayesian optimization (BO) is a powerful model-based framework for adaptive experimentation, where the primary goal is the optimization of the black-box function via sequentially chosen decisions. In many real-world tasks, it is essential for the decisions to be robust against, e.g., adversarial failures and perturbations, dynamic and timevarying phenomena, a mismatch between simulations and reality, etc. Under such requirements, the standard methods and BO algorithms become inadequate. In this dissertation, we consider four research directions with the goal of enhancing robust and adaptive decision making in BO and associated problems. First, we study the related problem of level-set estimation (LSE) with Gaussian Processes (GPs). While in BO the goal is to find a maximizer of the unknown function, in LSE one seeks to find all ""sufficiently good"" solutions. We propose an efficient confidence-bound based algorithm that treats BO and LSE in a unified fashion. It is effective in settings that are non-trivial to incorporate into existing algorithms, including cases with pointwise costs, heteroscedastic noise, and multi-fidelity setting. Our main result is a general regret guarantee that covers these aspects. Next, we consider GP optimization with robustness requirement: An adversary may perturb the returned design, and so we seek to find a robust maximizer in the case this occurs. This requirement is motivated by, e.g., settings where the functions during optimization and implementation stages are different. We propose a novel robust confidence-bound based algorithm. The rigorous regret guarantees for this algorithm are established and complemented with an algorithm-independent lower bound. We experimentally demonstrate that our robust approach consistently succeeds in finding a robust maximizer while standard BO methods fail. We then investigate the problem of GP optimization in which the reward function varies with time. The setting is motivated by many practical applications in which the function to be optimized is not static. We model the unknown reward function via a GP whose evolution obeys a simple Markov model. Two confidence-bound based algorithms with the ability to ""forget"" about old data are proposed. We obtain regret bounds for these algorithms that jointly depend on the time horizon and the rate at which the function varies. Finally, we consider the maximization of a set function subject to a cardinality constraint k in the case a number of items τ from the returned set may be removed. One notable application is in batch BO where we need to select experiments to run, but some of them can fail. Our focus is on the worst-case adversarial setting, and we consider both submodular (i.e., satisfies a natural notion of diminishing returns) and non-submodular objectives. We propose robust","",""
1,"I. Kulikovskikh, S. Prokhorov, T. Lipić, T. Legovic, T. Šmuc","BioGD: Bio-inspired robust gradient descent",2019,"","","","",72,"2022-07-13 09:39:44","","10.1371/journal.pone.0219004","","",,,,,1,0.33,0,5,3,"Recent research in machine learning pointed to the core problem of state-of-the-art models which impedes their widespread adoption in different domains. The models’ inability to differentiate between noise and subtle, yet significant variation in data leads to their vulnerability to adversarial perturbations that cause wrong predictions with high confidence. The study is aimed at identifying whether the algorithms inspired by biological evolution may achieve better results in cases where brittle robustness properties are highly sensitive to the slight noise. To answer this question, we introduce the new robust gradient descent inspired by the stability and adaptability of biological systems to unknown and changing environments. The proposed optimization technique involves an open-ended adaptation process with regard to two hyperparameters inherited from the generalized Verhulst population growth equation. The hyperparameters increase robustness to adversarial noise by penalizing the degree to which hardly visible changes in gradients impact prediction. The empirical evidence on synthetic and experimental datasets confirmed the viability of the bio-inspired gradient descent and suggested promising directions for future research. The code used for computational experiments is provided in a repository at https://github.com/yukinoi/bio_gradient_descent.","",""
55,"N. Edwards, Xue Wu, C. Tseng","An Unsupervised, Model-Free, Machine-Learning Combiner for Peptide Identifications from Tandem Mass Spectra",2009,"","","","",73,"2022-07-13 09:39:44","","10.1007/s12014-009-9024-5","","",,,,,55,4.23,18,3,13,"","",""
0,"Yunxia Liu, Anjie Zhang, Q. Meng, Yingjie Chen, Yang Yang, Yuehui Chen","Robust Circulating Tumor Cells Detection in Scanned Microscopic Images with Cascaded Morphological and Faster R-CNN Deep Detectors",2019,"","","","",74,"2022-07-13 09:39:44","","10.1007/978-3-030-26969-2_70","","",,,,,0,0.00,0,6,3,"","",""
3,"Anirbit Mukherjee, Ramchandran Muthukumar","Guarantees on learning depth-2 neural networks under a data-poisoning attack",2020,"","","","",75,"2022-07-13 09:39:44","","","","",,,,,3,1.50,2,2,2,"In recent times many state-of-the-art machine learning models have been shown to be fragile to adversarial attacks. In this work we attempt to build our theoretical understanding of adversarially robust learning with neural nets. We demonstrate a specific class of neural networks of finite size and a non-gradient stochastic algorithm which tries to recover the weights of the net generating the realizable true labels in the presence of an oracle doing a bounded amount of malicious additive distortion to the labels. We prove (nearly optimal) trade-offs among the magnitude of the adversarial attack, the accuracy and the confidence achieved by the proposed algorithm.","",""
46,"M. Staib, S. Jegelka","Robust Budget Allocation Via Continuous Submodular Functions",2017,"","","","",76,"2022-07-13 09:39:44","","10.1007/S00245-019-09567-0","","",,,,,46,9.20,23,2,5,"","",""
1,"Jiashuo Liu, Zheyan Shen, Peng Cui, Linjun Zhou, Kun Kuang, Bo Li, Yishi Lin","Invariant Adversarial Learning for Distributional Robustness",2020,"","","","",77,"2022-07-13 09:39:44","","","","",,,,,1,0.50,0,7,2,"Machine learning algorithms with empirical risk minimization are vulnerable to distributional shifts due to the greedy adoption of all the correlations found in training data. Recently, there are robust learning methods aiming at this problem by minimizing the worst-case risk over an uncertainty set. However, they equally treat all covariates to form the uncertainty sets regardless of the stability of their correlations with the target, resulting in the overwhelmingly large set and low confidence of the learner. In this paper, we propose the Invariant Adversarial Learning (IAL) algorithm that leverages heterogeneous data sources to construct a more practical uncertainty set and conduct robustness optimization, where covariates are differentiated according to the stability of their correlations with the target. We theoretically show that our method is tractable for stochastic gradient-based optimization and provide the performance guarantees for our method. Empirical studies on both simulation and real datasets validate the effectiveness of our method in terms of robust performance across unknown distributional shifts.","",""
1,"A. Chakrabarty, K. Berntorp, S. D. Cairano","Learning-based Parameter-Adaptive Reference Governors",2020,"","","","",78,"2022-07-13 09:39:44","","10.23919/ACC45564.2020.9147615","","",,,,,1,0.50,0,3,2,"Reference governors (RGs) provide an effective method for ensuring safety via constraint enforcement in closed-loop control systems. When the parameters of the underlying systems are unknown, but constant or slowly-varying, robust formulations of RGs that consider only the worst-case effect may be overly conservative and exhibit poor performance. This paper proposes a parameter-adaptive reference governor (PARG) architecture that is capable of generating safe trajectories in spite of parameter uncertainties without being as conservative as robust RGs. The proposed approach leverages on-line data to inform algorithms for robust parameter estimation. Subsequently, confidence bounds around parameter estimates are fed to supervised machine learners for approximating robust constraint admissible sets leveraged by the PARG. While initially, due to the absence of on-line data, the PARG may be as conservative as a robust RG, as more data is gathered and the confidence bounds become tighter, such conservativeness reduces, as demonstrated in a simulation example.","",""
1,"Zhi-Hua Zhou, T. Washio","Advances in machine learning : First Asian Conference on Machine Learning, ACML 2009, Nanjing, China, November 2-4, 2009 ; proceedings",2009,"","","","",79,"2022-07-13 09:39:44","","","","",,,,,1,0.08,1,2,13,"Keynote and Invited Talks.- Machine Learning and Ecosystem Informatics: Challenges and Opportunities.- Density Ratio Estimation: A New Versatile Tool for Machine Learning.- Transfer Learning beyond Text Classification.- Regular Papers.- Improving Adaptive Bagging Methods for Evolving Data Streams.- A Hierarchical Face Recognition Algorithm.- Estimating Likelihoods for Topic Models.- Conditional Density Estimation with Class Probability Estimators.- Linear Time Model Selection for Mixture of Heterogeneous Components.- Max-margin Multiple-Instance Learning via Semidefinite Programming.- A Reformulation of Support Vector Machines for General Confidence Functions.- Robust Discriminant Analysis Based on Nonparametric Maximum Entropy.- Context-Aware Online Commercial Intention Detection.- Feature Selection via Maximizing Neighborhood Soft Margin.- Accurate Probabilistic Error Bound for Eigenvalues of Kernel Matrix.- Community Detection on Weighted Networks: A Variational Bayesian Method.- Averaged Naive Bayes Trees: A New Extension of AODE.- Automatic Choice of Control Measurements.- Coupled Metric Learning for Face Recognition with Degraded Images.- Cost-Sensitive Boosting: Fitting an Additive Asymmetric Logistic Regression Model.- On Compressibility and Acceleration of Orthogonal NMF for POMDP Compression.- Building a Decision Cluster Forest Model to Classify High Dimensional Data with Multi-classes.- Query Selection via Weighted Entropy in Graph-Based Semi-supervised Classification.- Learning Algorithms for Domain Adaptation.- Mining Multi-label Concept-Drifting Data Streams Using Dynamic Classifier Ensemble.- Learning Continuous-Time Information Diffusion Model for Social Behavioral Data Analysis.- Privacy-Preserving Evaluation of Generalization Error and Its Application to Model and Attribute Selection.- Coping with Distribution Change in the Same Domain Using Similarity-Based Instance Weighting.- Monte-Carlo Tree Search in Poker Using Expected Reward Distributions.- Injecting Structured Data to Generative Topic Model in Enterprise Settings.- Weighted Nonnegative Matrix Co-Tri-Factorization for Collaborative Prediction.","",""
9,"Zhi Xiao, Zhe Luo, Bo Zhong, Xin Dang","Robust and Efficient Boosting Method Using the Conditional Risk",2018,"","","","",80,"2022-07-13 09:39:44","","10.1109/TNNLS.2017.2711028","","",,,,,9,2.25,2,4,4,"Well known for its simplicity and effectiveness in classification, AdaBoost, however, suffers from overfitting when class-conditional distributions have significant overlap. Moreover, it is very sensitive to noise that appears in the labels. This paper tackles the above limitations simultaneously via optimizing a modified loss function (i.e., the conditional risk). The proposed approach has the following two advantages. First, it is able to directly take into account label uncertainty with an associated label confidence. Second, it introduces a trustworthiness measure on training samples via the Bayesian risk rule, and hence the resulting classifier tends to have finite sample performance that is superior to that of the original AdaBoost when there is a large overlap between class conditional distributions. Theoretical properties of the proposed method are investigated. Extensive experimental results using synthetic data and real-world data sets from UCI machine learning repository are provided. The empirical study shows the high competitiveness of the proposed method in predication accuracy and robustness when compared with the original AdaBoost and several existing robust AdaBoost algorithms.","",""
16,"A. M. Ticlavilca, M. McKee, W. Walker","Real-time forecasting of short-term irrigation canal demands using a robust multivariate Bayesian learning model",2013,"","","","",81,"2022-07-13 09:39:44","","10.1007/s00271-011-0300-6","","",,,,,16,1.78,5,3,9,"","",""
140,"V. G. Krishnan, D. Westhead","A comparative study of machine-learning methods to predict the effects of single nucleotide polymorphisms on protein function",2003,"","","","",82,"2022-07-13 09:39:44","","10.1093/bioinformatics/btg297","","",,,,,140,7.37,70,2,19,"MOTIVATION The large volume of single nucleotide polymorphism data now available motivates the development of methods for distinguishing neutral changes from those which have real biological effects. Here, two different machine-learning methods, decision trees and support vector machines (SVMs), are applied for the first time to this problem. In common with most other methods, only non-synonymous changes in protein coding regions of the genome are considered.   RESULTS In detailed cross-validation analysis, both learning methods are shown to compete well with existing methods, and to out-perform them in some key tests. SVMs show better generalization performance, but decision trees have the advantage of generating interpretable rules with robust estimates of prediction confidence. It is shown that the inclusion of protein structure information produces more accurate methods, in agreement with other recent studies, and the effect of using predicted rather than actual structure is evaluated.   AVAILABILITY Software is available on request from the authors.","",""
142,"Kaihua Zhang, Lei Zhang, Ming-Hsuan Yang, David Zhang","Fast Tracking via Spatio-Temporal Context Learning",2013,"","","","",83,"2022-07-13 09:39:44","","","","",,,,,142,15.78,36,4,9,"In this paper, we present a simple yet fast and robust algorithm which exploits the spatio-temporal context for visual tracking. Our approach formulates the spatio-temporal relationships between the object of interest and its local context based on a Bayesian framework, which models the statistical correlation between the low-level features (i.e., image intensity and position) from the target and its surrounding regions. The tracking problem is posed by computing a confidence map, and obtaining the best target location by maximizing an object location likelihood function. The Fast Fourier Transform is adopted for fast learning and detection in this work. Implemented in MATLAB without code optimization, the proposed tracker runs at 350 frames per second on an i7 machine. Extensive experimental results show that the proposed algorithm performs favorably against state-of-the-art methods in terms of efficiency, accuracy and robustness.","",""
13,"X. Nguyen, S. Erfani, S. Paisitkriangkrai, J. Bailey, C. Leckie, K. Ramamohanarao","Training robust models using Random Projection",2016,"","","","",84,"2022-07-13 09:39:44","","10.1109/ICPR.2016.7899688","","",,,,,13,2.17,2,6,6,"Regularization plays an important role in machine learning systems. We propose a novel methodology for model regularization using random projection. We demonstrate the technique on neural networks, since such models usually comprise a very large number of parameters, calling for strong regularizers. It has been shown recently that neural networks are sensitive to two kinds of samples: (i) adversarial samples, which are generated by imperceptible perturbations of previously correctly-classified samples—yet the network will misclassify them; and (ii) fooling samples, which are completely unrecognizable, yet the network will classify them with extremely high confidence. In this paper, we show how robust neural networks can be trained using random projection. We show that while random projection acts as a strong regularizer, boosting model accuracy similar to other regularizers, such as weight decay and dropout, it is far more robust to adversarial noise and fooling samples. We further show that random projection also helps to improve the robustness of traditional classifiers, such as Random Forrest and Gradient Boosting Machines.","",""
0,"Fernando Pereira, A. McCallum, John Pereira","Machine Learning for Sequences and Structured Data : Tools for Non-Experts",2004,"","","","",85,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,3,18,"Project Summary This project proposes to dramatically improve the ability of people who are not experts in machine learning to design and automatically train models for analyzing and transforming sequences and other structured data such as text, signals, handwriting, and biological sequences. Working in the context of recent successes with conditional random fields (CRFs) and other conditional models of structured data, the proposed work will achieve its goals through scientific advances in model definition and combination, robust parameter estimation, and data-efficient training procedures, supported by an innovative compositional software architecture. Intellectual merit: Automatic classification and function fitting (regression) are mature techniques used in many scientific, engineering, and national security applications. Free and commercial software packages allow novice users to train with reasonable confidence of attaining useful results, and to use these results in sophisticated predictive and decision-making systems. However, for sequence and structured data problems, while task-specific machine-learning software has become increasingly available (e.g., speech recognition and genomics software tools), there are no modular, easy-to-use packages for domain experts working in science and engineering problems such as predictive data modeling of spatio-temporal patterns of brain activity or extraction of conceptual and citation networks from the scientific literature, or applications in national and homeland security such as the extraction of social networks from the open Web, or dynamically recognizing suspicious temporal patterns in network logs. The proposed work will make available for the first time a user-oriented software toolkit for integrated analysis and transformation of sequential and graph-structured data, which will be a major innovation in the data, models, and communications technical focus area. What makes this possible is the convergence of three scientific innovations in learning from structured data. First, powerful, trainable analyzers and transformers for sequences and other structured data can be built by combining simpler conditional models with general composition and product operations based on the theory of weighted automata. Second, a range of capacity-control techniques (feature induction, margin maximization, Bayesian automatic relevance determination) can be generalized to these complex models to control overfitting without the need for extensive hyperparameter adjustments. Third, the need for fully annotated training data can be reduced by combining partial evidence for multiple sequences into a single graph labeling problem. Together, these ideas provide a framework for flexibly specifying and learning parameters for multistep probabilistic transformations from complex data to their structured representations, and for designing and documenting efficient and usable software for …","",""
9,"Chi-Hoon Lee","Learning to combine discriminative classifiers: confidence based",2010,"","","","",86,"2022-07-13 09:39:44","","10.1145/1835804.1835899","","",,,,,9,0.75,9,1,12,"Much of research in data mining and machine learning has led to numerous practical applications. Spam filtering, fraud detection, and user query-intent analysis has relied heavily on machine learned classifiers, and resulted in improvements in robust classification accuracy. Combining multiple classifiers (a.k.a. Ensemble Learning) is a well studied and has been known to improve effectiveness of a classifier. To address two key challenges in Ensemble Learning-- (1) learning weights of individual classifiers and (2) the combination rule of their weighted responses, this paper proposes a novel Ensemble classifier, EnLR, that computes weights of responses from discriminative classifiers and combines their weighted responses to produce a single response for a test instance. The combination rule is based on aggregating weighted responses, where a weight of an individual classifier is inversely based on their respective variances around their responses. Here, variance quantifies the uncertainty of the discriminative classifiers' parameters, which in turn depends on the training samples. As opposed to other ensemble methods where the weight of each individual classifier is learned as a part of parameter learning and thus the same weight is applied to all testing instances, our model is actively adjusted as individual classifiers become confident at its decision for a test instance. Our empirical experiments on various data sets demonstrate that our combined classifier produces ""effective"" results when compared with a single classifier. Our novel classifier shows statistically significant better accuracy when compared to well known Ensemble methods -- Bagging and AdaBoost. In addition to robust accuracy, our model is extremely efficient dealing with high volumes of training samples due to the independent learning paradigm among its multiple classifiers. It is simple to implement in a distributed computing environment such as Hadoop.","",""
5,"Yongwon Lee, S. Clearwater","Tools for automating experiment design: a machine learning approach",1992,"","","","",87,"2022-07-13 09:39:44","","10.1109/TAI.1992.246423","","",,,,,5,0.17,3,2,30,"Work that uses an inductive learning tool, HEP-RL (high-energy-physics rule learner), in the design of a very complex artifact, a high-energy-physics experiment, is reported. The important contribution is the observation that the results of learning provide a more complete and robust design. This is because there were end users of the learning able to suggest constraints beyond the usual simple coverage metrics. This allowed for more confidence in the design.<<ETX>>","",""
8,"Qinglong Wang, Wenbo Guo, Alexander Ororbia, Xinyu Xing, Lin Lin, C. Lee Giles, Xue Liu, Peng Liu, Gang Xiong","Using Non-invertible Data Transformations to Build Adversarial-Robust Neural Networks",2016,"","","","",88,"2022-07-13 09:39:44","","","","",,,,,8,1.33,1,9,6,"Deep neural networks have proven to be quite effective in a wide variety of machine learning tasks, ranging from improved speech recognition systems to advancing the development of autonomous vehicles. However, despite their superior performance in many applications, these models have been recently shown to be susceptible to a particular type of attack possible through the generation of particular synthetic examples referred to as adversarial samples. These samples are constructed by manipulating real examples from the training data distribution in order to ""fool"" the original neural model, resulting in misclassification (with high confidence) of previously correctly classified samples. Addressing this weakness is of utmost importance if deep neural architectures are to be applied to critical applications, such as those in the domain of cybersecurity. In this paper, we present an analysis of this fundamental flaw lurking in all neural architectures to uncover limitations of previously proposed defense mechanisms. More importantly, we present a unifying framework for protecting deep neural models using a non-invertible data transformation--developing two adversary-resilient architectures utilizing both linear and nonlinear dimensionality reduction. Empirical results indicate that our framework provides better robustness compared to state-of-art solutions while having negligible degradation in accuracy.","",""
19,"Jie Wan, MingSong Li, M. O'Grady, Xiang Gu, Munassar A.A.H Alawlaqi, G. O’hare","Time-Bounded Activity Recognition for Ambient Assisted Living",2018,"","","","",89,"2022-07-13 09:39:44","","10.1109/TETC.2018.2870047","","",,,,,19,4.75,3,6,4,"Robust activity recognition in near real-time is a prerequisite for delivering the smartness intrinsic to the pragmatic realisation of smart homes, environments and so forth. Many of the physical devices necessary for equipping a smart home are already available as consumer electronic devices and certified for use by the public. Yet activity recognition remains the preserve of the research community, despite the array of machine learning and other AI techniques currently available. To-date, research has been dominated by the use of pre-segmented data, resulting in the recognition of an arbitrary activity subsequent to its completion. For assistive paradigms dependent on smart technologies, for example Ambient Assisted Living, such approaches are insufficient. The overall objective must be the identification of an activity within an appropriative confidence level as soon as possible after activity commencement. This paper presents a novel approach, Cumulatively Overlapping windowing approach for AmBient Recognition of Activities (COBRA), for near real-time activity recognition, specifically within 10, 30, 60 or 120 seconds of the commencement of an activity. COBRA utilizes an innovative combination of sliding windows augmented with a logistic regression model. The approach is evaluated using the well-established, open, CASAS dataset.","",""
214,"Mingkun Li, I. Sethi","Confidence-based active learning",2006,"","","","",90,"2022-07-13 09:39:44","","10.1109/TPAMI.2006.156","","",,,,,214,13.38,107,2,16,"This paper proposes a new active learning approach, confidence-based active learning, for training a wide range of classifiers. This approach is based on identifying and annotating uncertain samples. The uncertainty value of each sample is measured by its conditional error. The approach takes advantage of current classifiers' probability preserving and ordering properties. It calibrates the output scores of classifiers to conditional error. Thus, it can estimate the uncertainty value for each input sample according to its output score from a classifier and select only samples with uncertainty value above a user-defined threshold. Even though we cannot guarantee the optimality of the proposed approach, we find it to provide good performance. Compared with existing methods, this approach is robust without additional computational effort. A new active learning method for support vector machines (SVMs) is implemented following this approach. A dynamic bin width allocation method is proposed to accurately estimate sample conditional error and this method adapts to the underlying probabilities. The effectiveness of the proposed approach is demonstrated using synthetic and real data sets and its performance is compared with the widely used least certain active learning method","",""
26,"M. D. Cattaneo, M. Farrell, Yingjie Feng","Large sample properties of partitioning-based series estimators",2018,"","","","",91,"2022-07-13 09:39:44","","10.1214/19-aos1865","","",,,,,26,6.50,9,3,4,"We present large sample results for partitioning-based least squares nonparametric regression, a popular method for approximating conditional expectation functions in statistics, econometrics, and machine learning. First, we obtain a general characterization of their leading asymptotic bias. Second, we establish integrated mean squared error approximations for the point estimator and propose feasible tuning parameter selection. Third, we develop pointwise inference methods based on undersmoothing and robust bias correction. Fourth, employing different coupling approaches, we develop uniform distributional approximations for the undersmoothed and robust bias-corrected t-statistic processes and construct valid confidence bands. In the univariate case, our uniform distributional approximations require seemingly minimal rate restrictions and improve on approximation rates known in the literature. Finally, we apply our general results to three partitioning-based estimators: splines, wavelets, and piecewise polynomials. The supplemental appendix includes several other general and example-specific technical and methodological results. A companion R package is provided.","",""
2,"Matthew J. Holland","Robustness and scalability under heavy tails, without strong convexity",2021,"","","","",92,"2022-07-13 09:39:44","","","","",,,,,2,2.00,2,1,1,"Real-world data is laden with outlying values. The challenge for machine learning is that the learner typically has no prior knowledge of whether the feedback it receives (losses, gradients, etc.) will be heavy-tailed or not. In this work, we study a simple, cost-efficient algorithmic strategy that can be leveraged when both losses and gradients can be heavy-tailed. The core technique introduces a simple robust validation sub-routine, which is used to boost the confidence of inexpensive gradient-based sub-processes. Compared with recent robust gradient descent methods from the literature, dimension dependence (both risk bounds and cost) is substantially improved, without relying upon strong convexity or expensive perstep robustification. We also empirically show that the proposed procedure cannot simply be replaced with naive cross-validation.","",""
20,"I. Cortés-Ciriano, A. Bender","Concepts and Applications of Conformal Prediction in Computational Drug Discovery",2019,"","","","",93,"2022-07-13 09:39:44","","10.1039/9781788016841-00063","","",,,,,20,6.67,10,2,3,"Estimating the reliability of individual predictions is key to increase the adoption of computational models and artificial intelligence in preclinical drug discovery, as well as to foster its application to guide decision making in clinical settings. Among the large number of algorithms developed over the last decades to compute prediction errors, Conformal Prediction (CP) has gained increasing attention in the computational drug discovery community. A major reason for its recent popularity is the ease of interpretation of the computed prediction errors in both classification and regression tasks. For instance, at a confidence level of 90% the true value will be within the predicted confidence intervals in at least 90% of the cases. This so called validity of conformal predictors is guaranteed by the robust mathematical foundation underlying CP. The versatility of CP relies on its minimal computational footprint, as it can be easily coupled to any machine learning algorithm at little computational cost. In this review, we summarize underlying concepts and practical applications of CP with a particular focus on virtual screening and activity modelling, and list open source implementations of relevant software. Finally, we describe the current limitations in the field, and provide a perspective on future opportunities for CP in preclinical and clinical drug discovery.","",""
1,"Yogesh Kalakoti, Shashank Yadav, D. Sundar","SurvCNN: A Discrete Time-to-Event Cancer Survival Estimation Framework Using Image Representations of Omics Data",2021,"","","","",94,"2022-07-13 09:39:44","","10.3390/cancers13133106","","",,,,,1,1.00,0,3,1,"Simple Summary Robust methods for modelling and estimation of cancer survival could be relevant in understanding and limiting the impact of cancer. This study was aimed at developing an efficient Machine learning (ML) pipeline that could model survival in Lung Adenocarcinoma (LUAD) patients. Image transformations of multi omics data were employed for training a machine vision-based model capable of segregating patients into high-risk and low-risk subgroups. The performance was evaluated using concordance index, Brier score, and other similar metrices. The proposed model was able to outperform similar methods with a high degree of confidence. Furthermore, critical modules in cell cycle and pathways were also identified. Abstract The utility of multi-omics in personalized therapy and cancer survival analysis has been debated and demonstrated extensively in the recent past. Most of the current methods still suffer from data constraints such as high-dimensionality, unexplained interdependence, and subpar integration methods. Here, we propose SurvCNN, an alternative approach to process multi-omics data with robust computer vision architectures, to predict cancer prognosis for Lung Adenocarcinoma patients. Numerical multi-omics data were transformed into their image representations and fed into a Convolutional Neural network with a discrete-time model to predict survival probabilities. The framework also dichotomized patients into risk subgroups based on their survival probabilities over time. SurvCNN was evaluated on multiple performance metrics and outperformed existing methods with a high degree of confidence. Moreover, comprehensive insights into the relative performance of various combinations of omics datasets were probed. Critical biological processes, pathways and cell types identified from downstream processing of differentially expressed genes suggested that the framework could elucidate elements detrimental to a patient’s survival. Such integrative models with high predictive power would have a significant impact and utility in precision oncology.","",""
1,"Utkarsh Sarawgi, Rishab Khincha, Wazeer Zulfikar, Satrajit S. Ghosh, P. Maes","Uncertainty-Aware Boosted Ensembling in Multi-Modal Settings",2021,"","","","",95,"2022-07-13 09:39:44","","10.1109/IJCNN52387.2021.9534161","","",,,,,1,1.00,0,5,1,"Reliability of machine learning (ML) systems is crucial in safety-critical applications such as healthcare, and uncertainty estimation is a widely researched method to highlight the confidence of ML systems in deployment. Sequential and parallel ensemble techniques have shown improved performance of ML systems in multi-modal settings by leveraging the feature sets together. We propose an uncertainty-aware boosting technique for multi-modal ensembling in order to focus on the data points with higher associated uncertainty estimates, rather than the ones with higher loss values. We evaluate this method on healthcare tasks related to Dementia and Parkinson's disease which involve real-world multi-modal speech and text data, wherein our method shows an improved performance. Additional analysis suggests that introducing uncertainty-awareness into the boosted ensembles decreases the overall entropy of the system, making it more robust to heteroscedasticity in the data, as well as better calibrating each of the modalities along with high quality prediction intervals. We open-source our entire codebase at https://github.com/usarawgi911//Uncertainty-aware-boosting.","",""
0,"J. Bottieau, K. Bruninx, Anibal Sanjab, Z. De Grève, F. Vallée, J. Toubeau","Decision letter for ""Automatic risk adjustment for profit maximization in renewable dominated short‐term electricity markets""",2021,"","","","",96,"2022-07-13 09:39:44","","10.1002/2050-7038.13152/v1/decision1","","",,,,,0,0.00,0,6,1,"State-of-the-art trading strategies in short-term electricity markets employ risk awareness for reducing, inter alia, their exposure to the volatility of electricity prices. To ensure an optimal balance between risk and profit, risk-aversion parameters are traditionally fine-tuned via an offline out-of-sample analysis. Such a computationally-intensive analysis is typically run once, which yields time-invariant risk policies. Instead, this paper proposes the use of machine learning to select, in an online fashion, optimal risk-aversion parameters. This novel automatic risk-tuning approach offers the benefit of continuously adjusting the risk policy based on the dynamically changing market operating conditions. Our approach is tested on two risk-aversion parameters, i.e., the confidence level of the conditional value-at-risk and the budget of uncertainty, respectively considering scenario-based and robust optimization frameworks. A set of performed case studies – focusing on the very short-term dispatch of a market actor participating in electricity markets – using realworld market data from the Belgian power system demonstrate the ability of the proposed methodology to outperform traditional offline risk policies.","",""
0,"Rituraj, A. Tiwari, S. Chaudhury, Sanjay Singh, Sumeet Saurav","Video Classification using SlowFast Network via Fuzzy rule",2021,"","","","",97,"2022-07-13 09:39:44","","10.1109/FUZZ45933.2021.9494542","","",,,,,0,0.00,0,5,1,"Anomalous events occur rarely and are challenging to model. Therefore, automatic recognition of abnormal activities in surveillance videos is a non-trivial task. Though with the availability of video datasets of abnormal activities, there has been some progress, recognition of abnormal activities in real-time with high confidence remains unsolved. Existing video-based anomaly detection techniques using traditional machine learning and deep-learning are compute-intensive and give low recognition accuracy. This paper presents a robust and computationally efficient deep learning-based framework to recognize different real-world anomalies from the video. The proposed scheme uses a Fuzzy rule to summarize the video to scale the problem into fewer frames and the slow-fast neural network for classification. Intuitively, the designed pipeline aims to solve two significant problems that arise with video classification; one is to reduce the redundant frames and avoid the computation of optical flow for a video that has a substantial computational requirement. The proposed scheme tested on the UCF-crime dataset and has achieved recognition accuracy of 53%.","",""
0,"Obada Ghassan Al-Zibak, Khalid Sulaiman Al-Jibreen, F. Al-Ismail","Day-Ahead Market Self-Scheduling of a Virtual Power Plant under Uncertainties",2021,"","","","",98,"2022-07-13 09:39:44","","10.1109/SeFet48154.2021.9375741","","",,,,,0,0.00,0,3,1,"In this paper, the problem of a Virtual Power Plant participating in the Day-Ahead energy market was addressed with the aim of maximizing its profits. The problem considered the uncertainties in the wind and prices of the day-ahead market. The uncertainties were addressed by forecasting and by applying the confidence interval statistical theory. Machine learning that is based on the Gaussian Processes was employed for estimating and forecasting the uncertain variables. Moreover, the forecasting algorithms used the exponential kernel in the regression process. The self-scheduling problem was modeled using the MILP Robust optimization model. The proposed models were tested on an illustrative case study that comprises a conventional power plant, a wind power farm, and a flexible demand.","",""
0,"V. Belov, V. Kozyrev, Aditya Singh, M. Sacchet, R. Goya-Maldonado","High frequency rTMS modulates functional connectivity nodes and boundaries differently in the human brain",2021,"","","","",99,"2022-07-13 09:39:44","","10.1101/2021.03.09.434571","","",,,,,0,0.00,0,5,1,"Repetitive transcranial magnetic stimulation (rTMS) has gained considerable importance in the treatment of disorders, e.g. depression. However, it is not yet understood how rTMS alters brain’s functional connectivity. Here we report the changes captured by resting state functional magnetic resonance imaging (rsfMRI) within the first hour after 10Hz rTMS in (1) nodes, where the strongest functional connectivity of regions is seen, and (2) boundaries, where functional transitions between regions occur. We use support vector machines (SVM), a widely used machine learning algorithm that has been proven to be robust and effective, for the classification and characterization of time intervals of major changes in node and boundary maps, while respecting the variability between subjects. Particularly in the posterior cingulate cortex and precuneus, our results reveal that the changes in connectivity at the boundaries are slower and more complex than in those observed in the nodes, but of similar magnitude according to accuracy confidence intervals. As the network boundaries are under studied in comparison to nodes in connectomics research, our results highlight their contribution for functional adjustments to rTMS.","",""
0,"M. Glenski, Ellyn Ayton, Robin Cosbey, Dustin L. Arendt, Svitlana Volkova","Evaluating Deception Detection Model Robustness To Linguistic Variation",2021,"","","","",100,"2022-07-13 09:39:44","","10.18653/V1/2021.SOCIALNLP-1.6","","",,,,,0,0.00,0,5,1,"With the increasing use of machine-learning driven algorithmic judgements, it is critical to develop models that are robust to evolving or manipulated inputs. We propose an extensive analysis of model robustness against linguistic variation in the setting of deceptive news detection, an important task in the context of misinformation spread online. We consider two prediction tasks and compare three state-of-the-art embeddings to highlight consistent trends in model performance, high confidence misclassifications, and high impact failures. By measuring the effectiveness of adversarial defense strategies and evaluating model susceptibility to adversarial attacks using character- and word-perturbed text, we find that character or mixed ensemble models are the most effective defenses and that character perturbation-based attack tactics are more successful.","",""
0,"Rituraj, A. Tiwari, S. Chaudhury, Sanjay Singh, Sumeet Saurav","Video Classification using SlowFast Network via Fuzzy rule",2021,"","","","",101,"2022-07-13 09:39:44","","10.1109/FUZZ45933.2021.9494542","","",,,,,0,0.00,0,5,1,"Anomalous events occur rarely and are challenging to model. Therefore, automatic recognition of abnormal activities in surveillance videos is a non-trivial task. Though with the availability of video datasets of abnormal activities, there has been some progress, recognition of abnormal activities in real-time with high confidence remains unsolved. Existing video-based anomaly detection techniques using traditional machine learning and deep-learning are compute-intensive and give low recognition accuracy. This paper presents a robust and computationally efficient deep learning-based framework to recognize different real-world anomalies from the video. The proposed scheme uses a Fuzzy rule to summarize the video to scale the problem into fewer frames and the slow-fast neural network for classification. Intuitively, the designed pipeline aims to solve two significant problems that arise with video classification; one is to reduce the redundant frames and avoid the computation of optical flow for a video that has a substantial computational requirement. The proposed scheme tested on the UCF-crime dataset and has achieved recognition accuracy of 53%.","",""
0,"Erick Galinkin","Who's Afraid of Thomas Bayes?",2021,"","","","",102,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,1,1,"In many cases, neural networks perform well on test data, but tend to overestimate their confidence on out-of-distribution data. This has led to adoption of Bayesian neural networks, which better capture uncertainty and therefore more accurately reflect the model’s confidence. For machine learning security researchers, this raises the natural question of how making a model Bayesian affects the security of the model. In this work, we explore the interplay between Bayesianism and two measures of security: model privacy and adversarial robustness. We demonstrate that Bayesian neural networks are more vulnerable to membership inference attacks in general, but are at least as robust as their non-Bayesian counterparts to adversarial examples.","",""
0,"Obada Ghassan Al-Zibak, Khalid Sulaiman Al-Jibreen, F. Al-Ismail","Day-Ahead Market Self-Scheduling of a Virtual Power Plant under Uncertainties",2021,"","","","",103,"2022-07-13 09:39:44","","10.1109/SeFet48154.2021.9375741","","",,,,,0,0.00,0,3,1,"In this paper, the problem of a Virtual Power Plant participating in the Day-Ahead energy market was addressed with the aim of maximizing its profits. The problem considered the uncertainties in the wind and prices of the day-ahead market. The uncertainties were addressed by forecasting and by applying the confidence interval statistical theory. Machine learning that is based on the Gaussian Processes was employed for estimating and forecasting the uncertain variables. Moreover, the forecasting algorithms used the exponential kernel in the regression process. The self-scheduling problem was modeled using the MILP Robust optimization model. The proposed models were tested on an illustrative case study that comprises a conventional power plant, a wind power farm, and a flexible demand.","",""
0,"J. Bottieau, K. Bruninx, Anibal Sanjab, Z. De Grève, F. Vallée, J. Toubeau","Decision letter for ""Automatic risk adjustment for profit maximization in renewable dominated short‐term electricity markets""",2021,"","","","",104,"2022-07-13 09:39:44","","10.1002/2050-7038.13152/v1/decision1","","",,,,,0,0.00,0,6,1,"State-of-the-art trading strategies in short-term electricity markets employ risk awareness for reducing, inter alia, their exposure to the volatility of electricity prices. To ensure an optimal balance between risk and profit, risk-aversion parameters are traditionally fine-tuned via an offline out-of-sample analysis. Such a computationally-intensive analysis is typically run once, which yields time-invariant risk policies. Instead, this paper proposes the use of machine learning to select, in an online fashion, optimal risk-aversion parameters. This novel automatic risk-tuning approach offers the benefit of continuously adjusting the risk policy based on the dynamically changing market operating conditions. Our approach is tested on two risk-aversion parameters, i.e., the confidence level of the conditional value-at-risk and the budget of uncertainty, respectively considering scenario-based and robust optimization frameworks. A set of performed case studies – focusing on the very short-term dispatch of a market actor participating in electricity markets – using realworld market data from the Belgian power system demonstrate the ability of the proposed methodology to outperform traditional offline risk policies.","",""
0,"Tianyu Pang, Huishuai Zhang, Di He, Yinpeng Dong, Hang Su, Wei Chen, Jun Zhu, Tie-Yan Liu","Two Coupled Rejection Metrics Can Tell Adversarial Examples Apart",2021,"","","","",105,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,8,1,"Correctly classifying adversarial examples is an essential but challenging requirement for safely deploying machine learning models. As reported in RobustBench, even the state-of-the-art adversarially trained models struggle to exceed 67% robust test accuracy on CIFAR-10, which is far from practical. A complementary way towards robustness is to introduce a rejection option, allowing the model to not return predictions on uncertain inputs, where conﬁdence is a commonly used certainty proxy. Along with this routine, we ﬁnd that conﬁdence and a rectiﬁed conﬁdence (R-Con) can form two coupled rejection metrics, which could provably distinguish wrongly classiﬁed inputs from correctly classiﬁed ones. This intriguing property sheds light on using coupling strategies to better detect and reject adversarial examples. We evaluate our rectiﬁed rejection (RR) module on CIFAR-10, CIFAR-10-C, and CIFAR-100 under several attacks including adaptive ones, and demonstrate that the RR module is compatible with different adversarial training frameworks on improving robustness, with little extra computation.","",""
0,"Tapadhir Das, R. Shukla, S. Sengupta","The Devil is in the Details: Confident & Explainable Anomaly Detector for Software-Defined Networks",2021,"","","","",106,"2022-07-13 09:39:44","","10.1109/nca53618.2021.9685157","","",,,,,0,0.00,0,3,1,"Deployment of SDN control plane in high-end servers allow many network applications to be automated and easily managed. In this paper, we propose an SDN anomaly detection application, Confident and Explainable Anomaly Detector (CEAD), that automatically detects malicious network flows in SDN-based network architectures. The proposed application employs a set of Machine Learning (ML) classifiers to improve the confidence score of a prediction, thereby creating improved trust upon the prediction, while providing interpretability to the anomaly detector. The method utilizes the Explainable Artificial Intelligence (XAI) framework to provide interpretation to predictions to unearth network features that establish the most influence between predicted anomaly types. Results show that the proposed framework can achieve efficient anomaly detection performance, with near perfect confidence scores. Analysis with XAI highlights that byte and packet transmissions, and their robust statistics, can be significant indicators for prevalence of any attacks. Results also indicate that a subset of influential features can generally be used to decipher between normal and anomalous flow, while certain dataset features can be specifically influential in detecting specific attack types. This can lead to more efficient network resource utilization.","",""
0,"F. B. J. R. Dallaqua, F. Faria, Á. Fazenda","Building Data Sets for Rainforest Deforestation Detection Through a Citizen Science Project",2022,"","","","",107,"2022-07-13 09:39:44","","10.1109/lgrs.2020.3032098","","",,,,,0,0.00,0,3,1,"Originally, the ForestEyes project aims to detect deforestation in tropical forests based on citizen science (CS) and machine learning (ML) approaches, in which the volunteers analyze and label segments of remote sensing images to build new training sets for creating different classification models. In previous work, only three modules related to CS have been proposed. In this letter, two new modules are created: 1) organization and selection and 2) ML. Therefore, these modules turn the ForestEyes project a more robust system in the deforestation detection task, building high-confidence labeled collections, increasing the monitoring coverage, and decreasing volunteer dependence. Performed experiments show that volunteers create better data sets than those based on automatic PRODES-based approaches, selecting the most relevant samples and discarding noisy segments that might disrupt ML techniques. Finally, the results showed the feasibility of allying CS with ML for rainforest deforestation detection task.","",""
0,"Bhavna Soman, A. Torkamani, Michael J. Morais, Jeffrey Bickford, B. Coskun","Firenze: Model Evaluation Using Weak Signals",2022,"","","","",108,"2022-07-13 09:39:44","","10.48550/arXiv.2207.00827","","",,,,,0,0.00,0,5,1,"Data labels in the security ﬁeld are frequently noisy, limited, or biased towards a subset of the population. As a result, commonplace evaluation methods such as accuracy, precision and recall metrics, or analysis of performance curves computed from labeled datasets do not provide sufﬁcient conﬁdence in the real-world performance of a machine learning (ML) model. This has slowed the adoption of machine learning in the ﬁeld. In the industry today, we rely on domain expertise and lengthy manual evaluation to build this conﬁdence before shipping a new model for security applications. In this paper, we introduce Firenze, a novel framework for comparative evaluation of ML models’ performance using domain expertise, encoded into scalable functions called markers . We show that markers computed and combined over select subsets of samples called regions of interest can provide a robust estimate of their real-world performances. Critically, we use statistical hypothesis testing to ensure that observed differences—and therefore conclusions emerging from our framework—are more prominent than that observable from the noise alone. Using simulations and two real-world datasets for malware and domain-name-service reputation detection, we illustrate our approach’s effective-ness, limitations, and insights. Taken together, we propose Firenze as a resource for fast, interpretable, and collaborative model development and evaluation by mixed teams of researchers, domain experts, and business owners. no likely benign domains that are deemed highly malicious by either model. The results from these markers indicate that the real-world FP rate for the detections from both models are likely to be similar, and the test model preserves the low-FP quality of the reference model. With these data points we can show with a high degree of explainability that the test model is not performing better than the reference model at scoring malicious domains.","",""
0,"Shu Yao, Stanislav Minsker","Median of Means Principle for Bayesian Inference",2022,"","","","",109,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,2,1,"The topic of robustness is experiencing a resurgence of interest in the statistical and machine learning communities. In particular, robust algorithms making use of the so-called median of means estimator were shown to satisfy strong performance guarantees for many problems, including estimation of the mean, covariance structure as well as linear regression. In this work, we propose an extension of the median of means principle to the Bayesian framework, leading to the notion of the robust posterior distribution. In particular, we (a) quantify robustness of this posterior to outliers, (b) show that it satisﬁes a version of the Bernstein-von Mises theorem that con-nects Bayesian credible sets to the traditional conﬁdence intervals, and (c) demonstrate that our approach performs well in applications.","",""
1,"Juanjuan Wang, Haoran Yang, Ning Xu, Chengqin Wu, Zengshun Zhao, Jixiang Zhang, Dapeng Oliver Wu","Long-term target tracking combined with re-detection",2020,"","","","",110,"2022-07-13 09:39:44","","10.21203/rs.3.rs-51036/v1","","",,,,,1,0.50,0,7,2,"Long-term visual tracking undergoes more challenges and is closer to realistic applications than short-term tracking. However, the performances of most existing methods have been limited in the long-term tracking tasks. In this work, we present a reliable yet simple long-term tracking method, which extends the state-of-the-art learning adaptive discriminative correlation filters (LADCF) tracking algorithm with a re-detection component based on the support vector machine (SVM) model. The LADCF tracking algorithm localizes the target in each frame, and the re-detector is able to efficiently re-detect the target in the whole image when the tracking fails. We further introduce a robust confidence degree evaluation criterion that combines the maximum response criterion and the average peak-to-correlation energy (APCE) to judge the confidence level of the predicted target. When the confidence degree is generally high, the SVM is updated accordingly. If the confidence drops sharply, the SVM re-detects the target. We perform extensive experiments on the OTB-2015 and UAV123 datasets. The experimental results demonstrate the effectiveness of our algorithm in long-term tracking.","",""
1,"S. Daniel, A. Connolly, J. Schneider","DETERMINING FREQUENTIST CONFIDENCE LIMITS USING A DIRECTED PARAMETER SPACE SEARCH",2012,"","","","",111,"2022-07-13 09:39:44","","10.1088/0004-637X/794/1/38","","",,,,,1,0.10,0,3,10,"We consider the problem of inferring constraints on a high-dimensional parameter space with a computationally expensive likelihood function. We propose a machine learning algorithm that maps out the Frequentist confidence limit on parameter space by intelligently targeting likelihood evaluations so as to quickly and accurately characterize the likelihood surface in both low- and high-likelihood regions. We compare our algorithm to Bayesian credible limits derived by the well-tested Markov Chain Monte Carlo (MCMC) algorithm using both multi-modal toy likelihood functions and the seven yr Wilkinson Microwave Anisotropy Probe cosmic microwave background likelihood function. We find that our algorithm correctly identifies the location, general size, and general shape of high-likelihood regions in parameter space while being more robust against multi-modality than MCMC.","",""
11,"M. Callaghan, Finn Müller-Hansen","Statistical stopping criteria for automated screening in systematic reviews",2019,"","","","",112,"2022-07-13 09:39:44","","10.1186/s13643-020-01521-4","","",,,,,11,3.67,6,2,3,"","",""
0,"A. Christmann","Robust Learning from Bites",2005,"","","","",113,"2022-07-13 09:39:44","","10.17877/DE290R-6210","","",,,,,0,0.00,0,1,17,"Many robust statistical procedures have two drawbacks. Firstly, they are computer-intensive such that they can hardly be used for massive data sets. Sec- ondly, robust confidence intervals for the estimated parameters or robust pre- dictions according to the fitted models are often unknown. Here, we propose a general method to overcome these problems of robust estimation in the con- text of huge data sets. The method is scalable to the memory of the computer, can be distributed on several processors if available, and can help to reduce the computation time substantially. The method additionally oers distribution-free confidence intervals for the median of the predictions. The method is illustrated for two situations: robust estimation in linear regression and kernel logistic re- gression from statistical machine learning. and scoring methods for credit risk management, respectively. Other examples are data mining projects and micro-arrays. For such data sets parametric assumptions are often violated, outliers are present, or some variables can only be measured in an imprecise manner. The application of robust sta- tistical methods is important in such situations. However, many robust methods have the following drawbacks which are serious limitations for the application of robust methods. (a) They are computer-intensive such that they can hardly be used for massive data sets, say for several millions of observations with many explanatory variables. (b) Robust standard errors and robust confidence intervals for the estimated parameters or for robust predic- tions are often unknown. (c) Some statistical software packages like S-PLUS or R contain state-of-the-art algorithms for robust statistical methods, but the implemented numerical algorithms usually require that the whole data set fits into the memory of the computer. In this paper a simple but quite general method for robust estimation in the context of huge data sets is proposed. The goal of the proposal is to broaden in application of robust methods for massive data. The idea is to split the huge data set S by random into disjoint subsets Sb, b = 1;:::;B. Then the robust method is applied to each subset, and the results are summarized in a robust manner. The proposal yields robust predictions for the median together with distribution-free confidence intervals. The method is scalable to the memory of the computer by choosing B appropriately and the computation can easily distributed on several processors which helps to reduce the computation time substantially.","",""
2,"Sachin Saxena","TextDecepter: Hard Label Black Box Attack on Text Classifiers",2020,"","","","",114,"2022-07-13 09:39:44","","","","",,,,,2,1.00,2,1,2,"Machine learning has been proven to be susceptible to carefully crafted samples, known as adversarial examples. The generation of these adversarial examples helps to make the models more robust and give as an insight of the underlying decision making of these models. Over the years, researchers have successfully attacked image classifiers in, both, white and black-box setting. Although, these methods are not directly applicable to texts as text data is discrete in nature. In recent years, research on crafting adversarial examples against textual applications has been on the rise. In this paper, we present a novel approach for hard label black-box attacks against Natural Language Processing (NLP) classifiers, where no model information is disclosed, and an attacker can only query the model to get final decision of the classifier, without confidence scores of the classes involved. Such attack scenario is applicable to real world black-box models being used for security-sensitive applications such as sentiment analysis and toxic content detection","",""
2,"Hyun Kwon, J. Lee","AdvGuard: Fortifying Deep Neural Networks against Optimized Adversarial Example Attack",2020,"","","","",115,"2022-07-13 09:39:44","","10.1109/access.2020.3042839","","",,,,,2,1.00,1,2,2,"Deep neural networks (DNNs) provide excellent performance in image recognition, speech recognition, video recognition, and pattern analysis. However, they are vulnerable to adversarial example attacks. An adversarial example, which is input to which a little bit of noise has been strategically added, appears normal to the human eye but will be misrecognized by the DNN. In this paper, we propose AdvGuard, a method for resisting adversarial example attacks. This defense method prevents the generation of adversarial examples by constructing a robust DNN that provides random confidence values. This method does not require training of adversarial examples, use of other processing modules, or the ability to perform input data filtering. In addition, a DNN constructed using the proposed scheme can defend against adversarial examples while maintaining its accuracy on the original samples. In the experimental evaluation, MNIST and CIFAR10 were used as datasets, and TensorFlow was used as a machine learning library. The results show that a DNN constructed using the proposed method can correctly classify adversarial examples with 100% and 99.5% accuracy on MNIST and CIFAR10, respectively. INDEX TERMS Adversarial example, Evasion attack, Deep neural network, Defense method","",""
2,"Marcel Stehle, Mario Lasseck, Omid Khorramshahi, U. Sturm","Evaluation of acoustic pattern recognition of nightingale (Luscinia megarhynchos) recordings by citizens",2020,"","","","",116,"2022-07-13 09:39:44","","10.3897/rio.6.e50233","","",,,,,2,1.00,1,4,2,"Acoustic pattern recognition methods introduce new perspectives for species identification, biodiversity monitoring and data validation in citizen science but are rarely evaluated in real world scenarios. In this case study we analysed the performance of a machine learning algorithm for automated bird identification to reliably identify common nightingales (Luscinia megarhynchos) in field recordings taken by users of the smartphone app Naturblick. We found that the performance of the automated identification tool was overall robust in our selected recordings. Although most of the recordings had a relatively low confidence score, a large proportion of the recordings were identified correctly.","",""
1,"Matthew J. Holland","Improved scalability under heavy tails, without strong convexity",2020,"","","","",117,"2022-07-13 09:39:44","","","","",,,,,1,0.50,1,1,2,"Real-world data is laden with outlying values. The challenge for machine learning is that the learner typically has no prior knowledge of whether the feedback it receives (losses, gradients, etc.) will be heavy-tailed or not. In this work, we study a simple algorithmic strategy that can be leveraged when both losses and gradients can be heavy-tailed. The core technique introduces a simple robust validation sub-routine, which is used to boost the confidence of inexpensive gradient-based sub-processes. Compared with recent robust gradient descent methods from the literature, dimension dependence (both risk bounds and cost) is substantially improved, without relying upon strong convexity or expensive per-step robustification. Empirically, we also show that under heavy-tailed losses, the proposed procedure cannot simply be replaced with naive cross-validation. Taken together, we have a scalable method with transparent guarantees, which performs well without prior knowledge of how ""convenient"" the feedback it receives will be.","",""
0,"Jamie Hayes","Unique properties of adversarially trained linear classifiers on Gaussian data",2020,"","","","",118,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,1,2,"Machine learning models are vulnerable to adversarial perturbations, that when added to an input, can cause high confidence misclassifications. The adversarial learning research community has made remarkable progress in the understanding of the root causes of adversarial perturbations. However, most problems that one may consider important to solve for the deployment of machine learning in safety critical tasks involve high dimensional complex manifolds that are difficult to characterize and study. It is common to develop adversarially robust learning theory on simple problems, in the hope that insights will transfer to `real world datasets'. In this work, we discuss a setting where this approach fails. In particular, we show with a linear classifier, it is always possible to solve a binary classification problem on Gaussian data under arbitrary levels of adversarial corruption during training, and that this property is not observed with non-linear classifiers on the CIFAR-10 dataset.","",""
0,"Fahd Saghir, Helenio Gilabert, Bernardo Martin Mancuso","Application of Augmented Intelligence and Edge Analytics In Upstream Production Operations: An Innovative Approach for Optimizing Artificial Lift Systems Performance",2020,"","","","",119,"2022-07-13 09:39:44","","10.2118/201516-ms","","",,,,,0,0.00,0,3,2,"  With the advent of robust and powerful IIoT Edge Devices, it is now possible to deploy Machine Learning models at the boundaries of a production network, i.e. using Smart Nodes directly at the wellhead that runs analytics in near real-time. These Smart Nodes, when paired with Augmented Intelligence capabilities, allow subject matter experts to interact with Machine Learning models and help improve their accuracy over time. This, in turn, helps increase confidence in data-based results and enables operators to make informed decisions.  This paper will define and discuss an end-to-end architecture on how Augmented Intelligence, in tandem with Edge Analytics, can be implemented in the upstream production environment. Results, methodologies and lessons learnt from an Edge Analytics solution deployed on Rod Pump wells will be discussed in this paper.","",""
12,"D. Jacob","Group Average Treatment Effects for Observational Studies",2019,"","","","",120,"2022-07-13 09:39:44","","","","",,,,,12,4.00,12,1,3,"The paper proposes an estimator to make inference of heterogeneous treatment effects sorted by impact groups (GATES) for non-randomised experiments. The groups can be understood as a broader aggregation of the conditional average treatment effect (CATE) where the number of groups is set in advance. In economics, this approach is similar to pre-analysis plans. Observational studies are standard in policy evaluation from labour markets, educational surveys and other empirical studies. To control for a potential selection-bias, we implement a doubly-robust estimator in the first stage. We use machine learning methods to learn the conditional mean functions as well as the propensity score. The group average treatment effect is then estimated via a linear projection model. The linear model is easy to interpret, provides p-values and confidence intervals, and limits the danger of finding spurious heterogeneity due to small subgroups in the CATE. To control for confounding in the linear model, we use Neyman-orthogonal moments to partial out the effect that covariates have on both, the treatment assignment and the outcome. The result is a best linear predictor for effect heterogeneity based on impact groups. We find that our proposed method has lower absolute errors as well as smaller bias than the benchmark doubly-robust estimator. We further introduce a bagging type averaging for the CATE function for each observation to avoid biases through sample splitting. The advantage of the proposed method is a robust linear estimation of heterogeneous group treatment effects in observational studies.","",""
104,"Jie Wan, M. O'Grady, Gregory M. P. O'Hare","Dynamic sensor event segmentation for real-time activity recognition in a smart home context",2015,"","","","",121,"2022-07-13 09:39:44","","10.1007/s00779-014-0824-x","","",,,,,104,14.86,35,3,7,"","",""
45,"Thomas Raffinot","Hierarchical Clustering-Based Asset Allocation",2017,"","","","",122,"2022-07-13 09:39:44","","10.3905/jpm.2018.44.2.089","","",,,,,45,9.00,45,1,5,"This article proposes a hierarchical clustering-based asset allocation method, which uses graph theory and machine learning techniques. Hierarchical clustering refers to the formation of a recursive clustering, suggested by the data, not defined a priori. Several hierarchical clustering methods are presented and tested. Once the assets are hierarchically clustered, the authors compute a simple and efficient capital allocation within and across clusters of assets, so that many correlated assets receive the same total allocation as a single uncorrelated one. The out-of-sample performances of hierarchical clustering-based portfolios and more traditional risk-based portfolios are evaluated across three disparate datasets, which differ in term of the number of assets and the assets’ composition. To avoid data snooping, the authors assess the comparison of profit measures using the bootstrap-based model confidence set procedure. Their empirical results indicate that hierarchical clustering-based portfolios are robust and truly diversified and achieve statistically better risk-adjusted performances than commonly used portfolio optimization techniques.","",""
91,"J. Verrelst, L. Alonso, J. Caicedo, J. Moreno, G. Camps-Valls","Gaussian Process Retrieval of Chlorophyll Content From Imaging Spectroscopy Data",2013,"","","","",123,"2022-07-13 09:39:44","","10.1109/JSTARS.2012.2222356","","",,,,,91,10.11,18,5,9,"Precise and spatially-explicit knowledge of leaf chlorophyll content (Chl) is crucial to adequately interpret the chlorophyll fluorescence (ChF) signal from space. Accompanying information about the reliability of the Chl estimation becomes more important than ever. Recently, a new statistical method was proposed within the family of nonparametric Bayesian statistics, namely Gaussian Processes regression (GPR). GPR is simpler and more robust than their machine learning family members while maintaining very good numerical performance and stability. Other features include: (i) GPR requires a relatively small training data set and can adopt very flexible kernels, (ii) GPR identifies the relevant bands and observations in establishing relationships with a variable, and finally (iii) along with pixelwise estimations GPR provides accompanying confidence intervals. We used GPR to retrieve Chl from hyperspectral reflectance data and evaluated the portability of the regression model to other images. Based on field Chl measurements from the SPARC dataset and corresponding spaceborne CHRIS spectra (acquired in 2003, Barrax, Spain), GPR developed a regression model that was excellently validated (r2: 0.96, RMSE: 3.82 μg/cm2). The SPARC-trained GPR model was subsequently applied to CHRIS images (Barrax, 2003, 2009) and airborne CASI flightlines (Barrax 2009) to generate Chl maps. The accompanying confidence maps provided insight in the robustness of the retrievals. Similar confidences were achieved by both sensors, which is encouraging for upscaling Chl estimates from field to landscape scale. Because of its robustness and ability to deliver confidence intervals, GPR is evaluated as a promising candidate for implementation into ChF processing chains.","",""
8,"Maggie L. DiNome, J. Orozco, C. Matsuba, Ayla O Manughian-Peter, Miquel Ensenyat-Mendez, Shu-ching Chang, J. Jalas, M. Salomon, D. Marzese","Clinicopathological Features of Triple-Negative Breast Cancer Epigenetic Subtypes",2019,"","","","",124,"2022-07-13 09:39:44","","10.1245/s10434-019-07565-8","","",,,,,8,2.67,1,9,3,"","",""
70,"U. Johansson, Henrik Boström, Tuwe Löfström, H. Linusson","Regression conformal prediction with random forests",2014,"","","","",125,"2022-07-13 09:39:44","","10.1007/s10994-014-5453-0","","",,,,,70,8.75,18,4,8,"","",""
4,"Taimur Bakhshi, S. Shahid","Securing Internet of Bio-Nano Things: ML-Enabled Parameter Profiling of Bio-Cyber Interfaces",2019,"","","","",126,"2022-07-13 09:39:44","","10.1109/INMIC48123.2019.9022753","","",,,,,4,1.33,2,2,3,"The Internet of Bio-Nano Things (IoBNT) is an emerging paradigm at the cross-roads of Internet of Things (IoT), E-healthcare and synthetic biology. IoB Nt relies on innovating molecular communication between biologically synthesized nanodevices to facilitate in vivo drug delivery, remote monitoring and healthcare management via. the Internet. IoBNT therefore, requires robust security primitives to adequately address patient privacy concerns, increase physician confidence as well as limit (any) malicious operation resulting in bio-terrorism. The present work focused on securing the bio-cyber interfacing of IoB Nt technologies and considered three prevalent bio-electrical transduction techniques including bio-luminescence, redox modality and biological field effect transistors (BioFETs). Using the state-of-the-art machine learning (ML) algorithms, the proposed security framework employed parameter profiling the distinct operating features of bio-cyber interfaces and aimed to identify anomalous operation. During validation, an optimal profiling accuracy ranging between 88–91 % was recorded. The corresponding time complexity, scalability and qualitative analysis of the proposed parameter profiling-based framework leads to us to recommend it for further adoption in improving IoBNT security.","",""
0,"A. Chambaz, Sherri Rose, J. Bouyer, M. J. Laan","Targeted Learning of The Probability of Success of An In Vitro Fertilization Program Controlling for Time-dependent Confounders",2012,"","","","",127,"2022-07-13 09:39:44","","","","",,,,,0,0.00,0,4,10,"Infertility is a global public health issue and various treatments are available. In vitro fertilization (IVF) is an increasingly common treatment method, but accurately assessing the success of IVF programs has proven challenging since they consist of multiple cycles. We present a double robust semiparametric method that incorporates machine learning to estimate the probability of success (i.e., delivery resulting from embryo transfer) of a program of at most four IVF cycles in the French Devenir Apr‘es Interruption de la FIV (DAIFI) study and several simulation studies, controlling for time-dependent confounders. We find that the probability of success in the DAIFI study is 50% (95% confidence interval [0.48, 0.53]), therefore approximately half of future participants in a program of at most four IVF cycles can expect a delivery resulting from embryo transfer. (2012), 0, 0, pp. 1–24 Targeted learning of the probability of success of an in vitro fertilization program controlling for time-dependent confounders ANTOINE CHAMBAZ∗,1, SHERRI ROSE, JEAN BOUYER, MARK J. VAN DER LAAN 1 Laboratoire MODAL’X, Universite Paris-Ouest, 200 av. de la Republique, 92001 Nanterre, France 2 Department of Biostatistics, Johns Hopkins Bloomberg School of Public Health, 615 N Wolfe St, Baltimore, MD 21205, USA 3 Inserm, CESP Centre for research in Epidemiology and Population Health, U1018, Reproduction and child development, F-94807, Villejuif, France 4 Universite Paris-Sud, UMRS 1018, F-94807, Villejuif, France 5 Division of Biostatistics, University of California, Berkeley, School of Public Health, 101 Haviland Hall, Berkeley, CA 94720, USA ∗achambaz@u-paris10.fr Summary Infertility is a global public health issue and various treatments are available. In vitro fertilization (IVF) is an increasingly common treatment method, but accurately assessing the success of IVF programs has proven challenging since they consist of multiple cycles. We present a double robust semiparametric method that incorporates machine learning to estimate the probability of success (i.e., delivery resulting from embryo transfer) of a program of at most four IVF cycles in the French Devenir Apres Interruption de la FIV (DAIFI) study and several simulation studies, controlling for time-dependent confounders. We find that the probability of success in the DAIFI study is 50% (95% confidence interval [0.48, 0.53]), therefore approximately half of future participants in ∗To whom correspondence should be addressed. Hosted by The Berkeley Electronic Press 2 A. Chambaz and others a program of at most four IVF cycles can expect a delivery resulting from embryo transfer.","",""
16,"Pei-Hsuan Lu, Pin-Yu Chen, Kang-Cheng Chen, Chia-Mu Yu","On the Limitation of MagNet Defense Against L1-Based Adversarial Examples",2018,"","","","",128,"2022-07-13 09:39:44","","10.1109/DSN-W.2018.00065","","",,,,,16,4.00,4,4,4,"In recent years, defending adversarial perturbations to natural examples in order to build robust machine learning models trained by deep neural networks (DNNs) has become an emerging research field in the conjunction of deep learning and security. In particular, MagNet consisting of an adversary detector and a data reformer is by far one of the strongest defenses in the black-box oblivious attack setting, where the attacker aims to craft transferable adversarial examples from an undefended DNN model to bypass an unknown defense module deployed on the same DNN model. Under this setting, MagNet can successfully defend a variety of attacks in DNNs, including the high-confidence adversarial examples generated by the Carlini and Wagner's attack based on the L2 distortion metric. However, in this paper, under the same attack setting we show that adversarial examples crafted based on the L1 distortion metric can easily bypass MagNet and mislead the target DNN image classifiers on MNIST and CIFAR-10. We also provide explanations on why the considered approach can yield adversarial examples with superior attack performance and conduct extensive experiments on variants of MagNet to verify its lack of robustness to L1 distortion based attacks. Notably, our results substantially weaken the assumption of effective threat models on MagNet that require knowing the deployed defense technique when attacking DNNs (i.e., the gray-box attack setting).","",""
17,"Mohit Singh, Weijun Xie","Approximate Positively Correlated Distributions and Approximation Algorithms for D-optimal Design",2018,"","","","",129,"2022-07-13 09:39:44","","10.1137/1.9781611975031.145","","",,,,,17,4.25,9,2,4,"Experimental design is a classical area in statistics [21] and has also found new applications in machine learning[2]. In the combinatorial experimental design problem, the aim is to estimate an unknown m-dimensional vector x from linear measurements where a Gaussian noise is introduced in each measurement. The goal is to pick k out of the given n experiments so as to make the most accurate estimate of the unknown parameter x. Given a set S of chosen experiments, the most likelihood estimate x' can be obtained by a least squares computation. One of the robust measures of error estimation is the D-optimality criterion [27] which aims to minimize the generalized variance of the estimator. This corresponds to minimizing the volume of the standard confidence ellipsoid for the estimation error x − x'. The problem gives rise to two natural variants depending on whether repetitions of experiments is allowed or not. The latter variant, while being more general, has also found applications in geographical location of sensors [19]. We show a close connection between approximation algorithms for the D-optimal design problem and constructions of approximately m-wise positively correlated distributions. This connection allows us to obtain a [EQUATION]-approximation for the D-optimal design problem with and without repetitions giving the first constant factor approximation for the problem. We then consider the case when the number of experiments chosen is much larger than the dimension m and show one can obtain (1 − ϵ)-approximation if [EQUATION] when repetitions are allowed and if [EQUATION] when no repetitions are allowed improving on previous work.","",""
16,"Del BufaloAurelia, PauloinThierry, AlepeeNathalie, ClouzeauJacques, DetroyerAnn, EilsteinJoan, GomesCharles, NocairiHicham, PiroirdCécile, RoussetFrancoise, TourneixFleur, BasketterDavid, Martinozzi TeissierSilvia","Alternative Integrated Testing for Skin Sensitization: Assuring Consumer Safety",2018,"","","","",130,"2022-07-13 09:39:44","","10.1089/AIVT.2017.0023","","",,,,,16,4.00,2,13,4,"Abstract Cosmetics legislation in Europe has driven the validation and acceptance of non-animal alternatives, most recently in the area of skin sensitization. Despite use of these methods to meet regulatory needs, it is also essential that they allow evaluation regarding human safety. For cosmetic product safety, it is necessary to understand how they can be used and with what limitations, and thereby reveal what remains to be addressed. A dataset of 165 ingredients (137 cosmetic ingredients +28 reference substances) has been identified, curated, and subjected to testing using accepted in vitro methods, with additional information, including physicochemical data and in silico results. The inputs from multiple determinants of skin sensitizing activity have been used in five individual supervised classification models (or machine learning approaches), which were then collated in a robust statistical manner, a stacking meta-model, to deliver a prediction with an optimized level of confidence. For the trainin...","",""
7,"B. A. Miller, Mustafa Çamurcu, Alexander J. Gomez, K. Chan, Tina Eliassi-Rad","Improving Robustness to Attacks Against Vertex Classification",2019,"","","","",131,"2022-07-13 09:39:44","","","","",,,,,7,2.33,1,5,3,"Vertex classification—the problem of identifying the class labels of nodes in a graph—has applicability in a wide variety of domains. Examples include classifying subject areas of papers in citation networks or roles of machines in a computer network. Recent work has demonstrated that vertex classification using graph convolutional networks is susceptible to targeted poisoning attacks, in which both graph structure and node attributes can be changed in an attempt to misclassify a target node. This vulnerability decreases users’ confidence in the learning method and can prevent adoption in high-stakes contexts. This paper presents work in progress aiming to make vertex classification robust to these types of attacks. We investigate two aspects of this problem: (1) the classification model and (2) themethod for selecting training data. Our alternative classifier is a support vector machine (with a radial basis function kernel), which is applied to an augmented node feature-vector obtained by appending the node’s attributes to a Euclidean vector representing the node based on the graph structure. Our alternative methods of selecting training data are (1) to select the highestdegree nodes in each class and (2) to iteratively select the node with the most neighbors minimally connected to the training set. In the datasets on which the original attack was demonstrated, we show that changing the training set can make the network much harder to attack. To maintain a given probability of attack success, the adversary must use far more perturbations; often a factor of 2–4 over the random training baseline. Even in cases where success is relatively easy for the attacker, we show that the classification and training alternatives allow classification performance to degrade much more gradually, with weaker incorrect predictions for the attacked nodes.","",""
2,"M. Benosman","Towards Stability in Learning-based Control : A Bayesian Optimization-based Adaptive Controller",,"","","","",132,"2022-07-13 09:39:44","","","","",,,,,2,0.00,2,1,,"We propose to merge together techniques from control theory and machine learning to design a stable learning-based controller for a class of nonlinear systems. We adopt a modular adaptive control design approach that has two components. The first is a model-based robust nonlinear state feedback, which guarantees stability during learning, by rendering the closed-loop system input-to-state stable (ISS). The input is considered to be the error in the estimation of the uncertain parameters of the dynamics, and the state is considered to be the closed-loop output tracking error. The second component is a data-driven Bayesian optimization method for estimating the uncertain parameters of the dynamics, and improving the overall performance of the closed-loop system. In particular, we suggest using Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, which is a method for trading-off exploration-exploitation in continuous-armed bandits. GP-UCB searches the space of uncertain parameters and gradually finds the parameters that maximize the performance of the closed-loop system. These two systems together ensure that we have a stable learning-based control algorithm. The Multi-disciplinary Conference on Reinforcement Learning and Decision Making This work may not be copied or reproduced in whole or in part for any commercial purpose. Permission to copy in whole or in part without payment of fee is granted for nonprofit educational and research purposes provided that all such whole or partial copies include the following: a notice that such copying is by permission of Mitsubishi Electric Research Laboratories, Inc.; an acknowledgment of the authors and individual contributions to the work; and all applicable portions of the copyright notice. Copying, reproduction, or republishing for any other purpose shall require a license with payment of fee to Mitsubishi Electric Research Laboratories, Inc. All rights reserved. Copyright c © Mitsubishi Electric Research Laboratories, Inc., 2017 201 Broadway, Cambridge, Massachusetts 02139 Towards Stability in Learning-based Control: A Bayesian Optimization-based Adaptive Controller Mouhacine Benosman Mitsubishi Electric Research Laboratories (MERL) Cambridge, MA 02139, USA m benosman@ieee.org Amir-massoud Farahmand Mitsubishi Electric Research Laboratories (MERL) Cambridge, MA 02139, USA farahmand@merl.com","",""
2,"O. Elsayed, Noura Ahmed Mohamed Marzouky, Esraa Atef, M. Salem","Abnormal Action detection in video surveillance",2019,"","","","",133,"2022-07-13 09:39:44","","10.1109/ICICIS46948.2019.9014712","","",,,,,2,0.67,1,4,3,"The growing number of anomalies happening in indoor and outdoor environments calls for accurate and robust action recognition systems. These anomalies could vary from theft, destruction of public property or even fighting innocents. The aim of this paper is to introduce a new algorithm based on machine learning paradigm to detect human actions and to label them as normal or abnormal. The algorithm starts by testing two different human detectors, cascade object detector and Faster Region Convolutional Neural Network for Human Detection (FRCNNHD). Both detectors were trained using widely available datasets. Afterwards, detected human figures are ex-tracted to form a video patch that represents human motion. For action recognition, we applied the Motion History Image to extract static features of motion. The actions are then classified using the Support Vector Machine (SVM). Finally, actions with low recognition confidence are labeled as “abnormal actions”. Experimental results on two datasets show the accuracy of our algorithm on learned actions.","",""
116,"Pabitra Mitra, C. A. Murthy, S. Pal","A probabilistic active support vector learning algorithm",2004,"","","","",134,"2022-07-13 09:39:44","","10.1109/TPAMI.2004.1262340","","",,,,,116,6.44,39,3,18,"The paper describes a probabilistic active learning strategy for support vector machine (SVM) design in large data applications. The learning strategy is motivated by the statistical query model. While most existing methods of active SVM learning query for points based on their proximity to the current separating hyperplane, the proposed method queries for a set of points according to a distribution as determined by the current separating hyperplane and a newly defined concept of an adaptive confidence factor. This enables the algorithm to have more robust and efficient learning capabilities. The confidence factor is estimated from local information using the k nearest neighbor principle. The effectiveness of the method is demonstrated on real-life data sets both in terms of generalization performance, query complexity, and training time.","",""
21,"A. B. Abdessalem, N. Dervilis, D. Wagg, K. Worden","Automatic Kernel Selection for Gaussian Processes Regression with Approximate Bayesian Computation and Sequential Monte Carlo",2017,"","","","",135,"2022-07-13 09:39:44","","10.3389/fbuil.2017.00052","","",,,,,21,4.20,5,4,5,"The current work introduces a novel combination of two Bayesian tools, Gaussian Processes (GPs) and the use of the Approximate Bayesian Computation (ABC) algorithm for kernel selection and parameter estimation for machine learning applications. The combined methodology that this research paper proposes and investigates offers the possibility to use different metrics and summary statistics of the kernels used for Bayesian regression. The presented work moves a step towards online, robust, consistent and automated mechanism to formulate optimal kernels (or even mean functions) and their hyperparameters simultaneously offering confidence evaluation when these tools are used for mathematical or engineering problems such as structural health monitoring (SHM) or system identification (SI).","",""
5,"Fan Yang, Hua-zhen Wang, Hong Mi","A novel classification method of microarray with reliability and confidence",2008,"","","","",136,"2022-07-13 09:39:44","","10.1109/ICMLC.2008.4620684","","",,,,,5,0.36,2,3,14,"Most of state-of-the-art machine learning algorithms cannot provide a reliable measure of their classifications and predictions. This paper addresses the importance of reliability and confidence for classification, and presents a novel method based on a combination of the unexcelled ensemble method, random forest (RF), and transductive confidence machine (TCM) which we call TCM-RF. The new algorithm hedges the predictions of RF and gives a well-calibrated region prediction by using the proximity matrix generated with RF as a nonconformity measure of examples. The new method takes advantage of RF and possesses a more precise and robust nonconformity measure. It can deal with redundant and noisy data with mixed types of variables, and is less sensitive to parameter settings. Experiments on benchmark datasets show it is more effective and robust than other TCMs. Further study on a real-world lymphoma microarray dataset shows its superiority over SVM with the ability of controlling the risk of error.","",""
25,"N. Kreif, Susan Gruber, R. Radice, R. Grieve, J. Sekhon","Evaluating treatment effectiveness under model misspecification: A comparison of targeted maximum likelihood estimation with bias-corrected matching",2014,"","","","",137,"2022-07-13 09:39:44","","10.1177/0962280214521341","","",,,,,25,3.13,5,5,8,"Statistical approaches for estimating treatment effectiveness commonly model the endpoint, or the propensity score, using parametric regressions such as generalised linear models. Misspecification of these models can lead to biased parameter estimates. We compare two approaches that combine the propensity score and the endpoint regression, and can make weaker modelling assumptions, by using machine learning approaches to estimate the regression function and the propensity score. Targeted maximum likelihood estimation is a double-robust method designed to reduce bias in the estimate of the parameter of interest. Bias-corrected matching reduces bias due to covariate imbalance between matched pairs by using regression predictions. We illustrate the methods in an evaluation of different types of hip prosthesis on the health-related quality of life of patients with osteoarthritis. We undertake a simulation study, grounded in the case study, to compare the relative bias, efficiency and confidence interval coverage of the methods. We consider data generating processes with non-linear functional form relationships, normal and non-normal endpoints. We find that across the circumstances considered, bias-corrected matching generally reported less bias, but higher variance than targeted maximum likelihood estimation. When either targeted maximum likelihood estimation or bias-corrected matching incorporated machine learning, bias was much reduced, compared to using misspecified parametric models.","",""
34,"Indrajit Mandal, N. Sairam","Accurate Prediction of Coronary Artery Disease Using Reliable Diagnosis System",2012,"","","","",138,"2022-07-13 09:39:44","","10.1007/s10916-012-9828-0","","",,,,,34,3.40,17,2,10,"","",""
13,"A. Naimi, Edward H. Kennedy","Nonparametric Double Robustness",2017,"","","","",139,"2022-07-13 09:39:44","","","","",,,,,13,2.60,7,2,5,"Use of nonparametric techniques (e.g., machine learning, kernel smoothing, stacking) are increasingly appealing because they do not require precise knowledge of the true underlying models that generated the data under study. Indeed, numerous authors have advocated for their use with standard methods (e.g., regression, inverse probability weighting) in epidemiology. However, when used in the context of such singly robust approaches, nonparametric methods can lead to suboptimal statistical properties, including inefficiency and no valid confidence intervals. Using extensive Monte Carlo simulations, we show how doubly robust methods offer improvements over singly robust approaches when implemented via nonparametric methods. We use 10,000 simulated samples and 50, 100, 200, 600, and 1200 observations to investigate the bias and mean squared error of singly robust (g Computation, inverse probability weighting) and doubly robust (augmented inverse probability weighting, targeted maximum likelihood estimation) estimators under four scenarios: correct and incorrect model specification; and parametric and nonparametric estimation. As expected, results show best performance with g computation under correctly specified parametric models. However, even when based on complex transformed covariates, double robust estimation performs better than singly robust estimators when nonparametric methods are used. Our results suggest that nonparametric methods should be used with doubly instead of singly robust estimation techniques.","",""
4,"E. Isong, Udonyah Kingsley, Godwin Ansa","Cognitive Factors in Students' Academic Performance Evaluation using Artificial Neural Networks",2018,"","","","",140,"2022-07-13 09:39:44","","","","",,,,,4,1.00,1,3,4,"Performance evaluation based on some cognitive factors especially Students’ Intelligent Quotient rating (IQR), Confidence Level (CoL) and Time Management ability gives an equal platform for better evaluation of students’ performance using Artificial Neural Network. Artificial Neural Networks (ANN) models, which has the advantage of being trained, offers a more robust methodology and tool for predicting, forecasting and modeling phenomena to ascertain conformance to desired standards as well as assist in decision making. This work employs Machine Learning and cognitive science which uses Artificial Neural networks (ANNs) to evaluated students’ academic performance in the Department of Computer Science, Akwa Ibom State University. It presents a survey of the design, building and functionalities of Artificial Neural Network for the evaluation of students’ academic performance using cognitive factors that could affect student’s performances. Keywords : Cognitive, Intelligent Quotient Rating, Machine Learning, Artificial Neural Network.","",""
2,"Joshua B. Cohen, M. Simi, F. Campagne","GenotypeTensors: Efficient Neural Network Genotype Callers",2018,"","","","",141,"2022-07-13 09:39:44","","10.1101/338780","","",,,,,2,0.50,1,3,4,"We studied the problem of calling genotypes using neural networks. A machine learning approach to calling genotypes requires a training set, an approach to convert genomic sites into tensors and robust model development and evaluation protocols. We discuss each of these components of our approach and compare four types of neural network training protocols, two fully supervised and two semi-supervised approaches. Semi-supervised approaches use unlabeled data to supplement limited quantities of labeled data. Random hyper-parameter searches identified highly performing models that reach indel F1 of 99.4% on a chromosomes 20, 21, 22 and X of NA12878/HG001. We further validate these models by evaluating performance on HG002, an independent sample used in the PrecisionFDA challenge. We apply GenotypeTensors to evaluate the impact of (1) training with small datasets, (2) training models only with sites inside confidence regions, or (3) training with improved true label annotations. A PyTorch open-source implementation of GenotypeTensors is available at https://github.com/CampagneLaboratory/GenotypeTensors. DNANexus cloud applications are provided to help process new datasets both to train model or call genotypes with trained models.","",""
2,"Diego Aparicio, M. L. Prado","How Hard Is It to Pick the Right Model? MCS and Backtest Overfitting",2017,"","","","",142,"2022-07-13 09:39:44","","10.2139/ssrn.3044740","","",,,,,2,0.40,1,2,5,"Recent advances in machine learning, artificial intelligence, and the availability of billions of high frequency data signals have made model selection a challenging and pressing need. However, most of the model selection methods available in modern finance are subject to backtest overfitting. This is the probability that one will select a financial strategy that outperforms during backtest, but underperforms in practice. We evaluate the performance of the novel model confidence set (MCS) introduced in Hansen et al. (2011a) in a simple machine learning trading strategy problem. We find that MCS is not robust to multiple testing and that it requires a very high signal-to-noise ratio to be utilizable. More generally, we raise awareness on the limitations of model selection in finance.","",""
7,"Aleksander Bapst, Jonathan Tran, M. W. Koch, M. M. Moya, Robert Swahn","Open set recognition of aircraft in aerial imagery using synthetic template models",2017,"","","","",143,"2022-07-13 09:39:44","","10.1117/12.2262150","","",,,,,7,1.40,1,5,5,"Fast, accurate and robust automatic target recognition (ATR) in optical aerial imagery can provide game-changing advantages to military commanders and personnel. ATR algorithms must reject non-targets with a high degree of confidence in a world with an infinite number of possible input images. Furthermore, they must learn to recognize new targets without requiring massive data collections. Whereas most machine learning algorithms classify data in a closed set manner by mapping inputs to a fixed set of training classes, open set recognizers incorporate constraints that allow for inputs to be labelled as unknown. We have adapted two template-based open set recognizers to use computer generated synthetic images of military aircraft as training data, to provide a baseline for military-grade ATR: (1) a frequentist approach based on probabilistic fusion of extracted image features, and (2) an open set extension to the one-class support vector machine (SVM). These algorithms both use histograms of oriented gradients (HOG) as features as well as artificial augmentation of both real and synthetic image chips to take advantage of minimal training data. Our results show that open set recognizers trained with synthetic data and tested with real data can successfully discriminate real target inputs from non-targets. However, there is still a requirement for some knowledge of the real target in order to calibrate the relationship between synthetic template and target score distributions. We conclude by proposing algorithm modifications that may improve the ability of synthetic data to represent real data.","",""
6,"E. Chatzilari, G. Liaros, K. Georgiadis, S. Nikolopoulos, Y. Kompatsiaris","Combining the Benefits of CCA and SVMs for SSVEP-based BCIs in Real-world Conditions",2017,"","","","",144,"2022-07-13 09:39:44","","10.1145/3132635.3132636","","",,,,,6,1.20,1,5,5,"In this paper we propose a novel method for SSVEP classification that combines the benefits of the inherently multi-channel CCA, the state-of-the-art method for detecting SSVEPs, with the robust SVMs, one of the most popular machine learning algorithms. The employment of SVMs, except for the benefit of robustness, provides us also with a confidence score allowing to dynamically trade-off the trial length with the accuracy of the classifier, and vice versa. By balancing this trade-off we are able to offer personalized self-paced BCIs that maximize the ITR of the system. Furthermore, we propose to perturb the template frequencies of CCA so as to accommodate with real world BCI applications requirements, where the environmental conditions may not be ideal compared to existing methods that rely on the assumption of soundproof and distraction-free environments.","",""
0,"Xiaohua Huang, Wenming Zheng","Effective discriminative TCM-KNN for incremental learning",2009,"","","","",145,"2022-07-13 09:39:44","","10.1117/12.832567","","",,,,,0,0.00,0,2,13,"Incremental learning is an efficient scheme for reducing computational complexity of batch learning. Label information in each update is helpful to update discriminative model in incremental learning. However, the procedure of labeling samples is always a time-consuming and tedious task. In this paper, we propose two labeling algorithms for unknown samples, one is discriminative Transductive Confidence Machine for K-Nearest Neighbor (TCM-KNN), the other is its improved algorithm for choosing good quality discriminative samples and enhancing the performance of the procedure of labeling samples; and then these methods is applied in the incremental learning[2] before updating model. Experiment on PIE database has been carried out for comparing their recognition rate and complexity. Extensive experimental results show that the proposed method for incremental learning is more robust and effective than batch learning.","",""
69,"Yang Li, Binxing Fang, Li Guo, You Chen","Network anomaly detection based on TCM-KNN algorithm",2007,"","","","",146,"2022-07-13 09:39:44","","10.1145/1229285.1229292","","",,,,,69,4.60,17,4,15,"Intrusion detection is a critical component of secure information systems. Network anomaly detection has been an active and difficult research topic in the field of Intrusion Detection for many years. However, it still has some problems unresolved. They include high false alarm rate, difficulties in obtaining exactly clean data for the modeling of normal patterns and the deterioration of detection rate because of some ""noisy"" data in the training set. In this paper, we propose a novel network anomaly detection method based on improved TCM-KNN (Transductive Confidence Machines for K-Nearest Neighbors) machine learning algorithm. A series of experimental results on the well-known KDD Cup 1999 dataset demonstrate it can effectively detect anomalies with high true positive rate, low false positive rate and high confidence than the state-of-the-art anomaly detection methods. In addition, even interfered by ""noisy"" data (unclean data), the proposed method is robust and effective. Moreover, it still retains good detection performance after employing feature selection aiming at avoiding the ""curse of dimensionality"".","",""
57,"A. Lobley, Timothy Nugent, C. Orengo, David T. Jones","FFPred: an integrated feature-based function prediction server for vertebrate proteomes",2008,"","","","",147,"2022-07-13 09:39:44","","10.1093/nar/gkn193","","",,,,,57,4.07,14,4,14,"One of the challenges of the post-genomic era is to provide accurate function annotations for large volumes of data resulting from genome sequencing projects. Most function prediction servers utilize methods that transfer existing database annotations between orthologous sequences. In contrast, there are few methods that are independent of homology and can annotate distant and orphan protein sequences. The FFPred server adopts a machine-learning approach to perform function prediction in protein feature space using feature characteristics predicted from amino acid sequence. The features are scanned against a library of support vector machines representing over 300 Gene Ontology (GO) classes and probabilistic confidence scores returned for each annotation term. The GO term library has been modelled on human protein annotations; however, benchmark performance testing showed robust performance across higher eukaryotes. FFPred offers important advantages over traditional function prediction servers in its ability to annotate distant homologues and orphan protein sequences, and achieves greater coverage and classification accuracy than other feature-based prediction servers. A user may upload an amino acid and receive annotation predictions via email. Feature information is provided as easy to interpret graphics displayed on the sequence of interest, allowing for back-interpretation of the associations between features and function classes.","",""
84,"T. Melluish, C. Saunders, I. Nouretdinov, V. Vovk","Comparing the Bayes and Typicalness Frameworks",2001,"","","","",148,"2022-07-13 09:39:44","","10.1007/3-540-44795-4_31","","",,,,,84,4.00,21,4,21,"","",""
1,"M. Montazery, Nic Wilson","Rescale-Invariant SVM for Binary Classification",2017,"","","","",149,"2022-07-13 09:39:44","","10.24963/ijcai.2017/348","","",,,,,1,0.20,1,2,5,"Support Vector Machines (SVM) are among the best-known machine learning methods, with broad use in different scientific areas. However, one necessary pre-processing phase for SVM is normalization (scaling) of features, since SVM is not invariant to the scales of the features’ spaces, i.e., different ways of scaling may lead to different results. We define a more robust decision-making approach for binary classification, in which one sample strongly belongs to a class if it belongs to that class for all possible rescalings of features. We derive a way of characterising the approach for binary SVM that allows determining when an instance strongly belongs to a class and when the classification is invariant to rescaling. The characterisation leads to a computational method to determine whether one sample is strongly positive, strongly negative or neither. Our experimental results back up the intuition that being strongly positive suggests stronger confidence that an instance really is positive.","",""
7,"Xin Chen, E. Moschidis, C. Taylor, S. Astley","Breast Cancer Risk Analysis Based on a Novel Segmentation Framework for Digital Mammograms",2014,"","","","",150,"2022-07-13 09:39:44","","10.1007/978-3-319-10404-1_67","","",,,,,7,0.88,2,4,8,"","",""
5,"B. Gillen, H. Moon","BLP-Lasso for Aggregate Discrete Choice Models Applied to Elections with Rich Demographic Covariates ∗",2015,"","","","",151,"2022-07-13 09:39:44","","","","",,,,,5,0.71,3,2,7,"Economists often study consumers’ aggregate behavior across markets choosing from a menu of differentiated products. In this analysis, local demographic characteristics can serve as controls for market-specific heterogeneity in product preferences. Given rich demographic data, implementing these models requires specifying which variables to include in the analysis, an ad hoc process typically guided primarily by a researcher’s intuition. We propose a data-driven approach to estimate these models applying penalized estimation algorithms imported from the machine learning literature along with confidence intervals that are robust to variable selection. Our application explores the effect of campaign spending on vote shares in data from Mexican elections.","",""
5,"B. Gillen, Sergio Montero, H. Moon, M. Shum","BLP-Lasso for Aggregate Discrete Choice Models of Elections with Rich Demographic Covariates",2015,"","","","",152,"2022-07-13 09:39:44","","10.2139/ssrn.2703602","","",,,,,5,0.71,1,4,7,"Economists often study consumers' aggregate behavior across markets choosing from a menu of differentiated products. In this analysis, local demographic characteristics can serve as controls for market-specific heterogeneity in product preferences. Given rich demographic data, implementing these models requires specifying which variables to include in the analysis, an ad hoc process typically guided primarily by a researcher's intuition. We propose a data-driven approach to estimate these models applying penalized estimation algorithms imported from the machine learning literature along with confidence intervals that are robust to variable selection. Our application explores the effect of campaign spending on vote shares in data from Mexican elections.","",""
33,"O. Debeir, Ivan Adanja, N. Warzée, P. V. Ham, C. Decaestecker","Phase contrast image segmentation by weak watershed transform assembly",2008,"","","","",153,"2022-07-13 09:39:44","","10.1109/ISBI.2008.4541098","","",,,,,33,2.36,7,5,14,"We present here a method giving a robust segmentation for in vitro cells observed under standard phase-contrast microscopy. We tackle the problem using the watershed transform. Watershed transform is known for its ability to generate closed contours and its extreme sensitivity to image borders. One main drawback of this method is over- segmentation. In order to circumvent this, marked watershed based on the ""modified gradient"" method has been developed. However, the choice of the watershed mark locations is critical and their inadequacy may cause wrong results. Similarly to randomization and combination procedures used in the machine learning field, the present paper promotes the use of an assembly of marked watershed transforms, in order to increase the segmentation robustness. This results in the definition of candidate segmentations margins (expressed in terms of object border confidence) from which final segmentation can be chosen by means of thresholding.","",""
6,"Jing Peng, B. Bhanu","Learning to Perceive Objects for Autonomous Navigation",1999,"","","","",154,"2022-07-13 09:39:44","","10.1023/A:1008887511945","","",,,,,6,0.26,3,2,23,"","",""
0,"Vasileios Asthenopoulos, Pavlos Kosmides, E. Adamopoulou, K. Demestichas","Intelligent energy consumption estimation for electric vehicles: Business processes and services",2014,"","","","",155,"2022-07-13 09:39:44","","10.1109/ICCVE.2014.7297559","","",,,,,0,0.00,0,4,8,"Nowadays, Fully Electric Vehicles are in the spotlight of energy-efficient and sustainable mobility. Their overall efficiency however, as well as their commercial viability, depend strongly on the degree of confidence they offer to the driver in terms of energy savings and range characteristics. To this end, advanced consumption prediction mechanisms must be implemented in order to enable the provision of energy-based routing functionalities. In this context, this paper presents an innovative energy consumption estimation service that relies on the vehicles' travelling history and experience and deploys machine learning mechanisms in order to obtain accurate, robust and cost-efficient estimations.","",""
23,"T. Melluish, C. Saunders, I. Nouretdinov, V. Vovk","The typicalness framework: a comparison with the Bayesian approach",2004,"","","","",156,"2022-07-13 09:39:44","","","","",,,,,23,1.28,6,4,18,"When correct priors are known, Bayesian algorithms give optimal decisions, and accurate confidence values for predictions can be obtained. If the prior is incorrect however, these confidence values have no theoretical base – even though the algorithms’ predictive performance may be good. There also exist many successful learning algorithms which only depend on the iid assumption. Often however they produce no confidence values for their predictions. Bayesian frameworks are often applied to these algorithms in order to obtain such values, however they can rely on unjustified priors. In this paper we outline the typicalness framework which can be used in conjunction with many other machine learning algorithms. The framework provides confidence information based only on the standard iid assumption and so is much more robust to different underlying data distributions. We show how the framework can be applied to existing algorithms. We also present experimental results which show that the typicalness approach performs close to Bayes when the prior is known to be correct. Unlike Bayes however, the method still gives accurate confidence values even when different data distributions are considered.","",""
2,"S. Nadgeri, Vidya P. Hulsure, A. Gawande","Comparative Study of Various Regression Methods for Software Effort Estimation",2010,"","","","",157,"2022-07-13 09:39:44","","10.1109/ICETET.2010.22","","",,,,,2,0.17,1,3,12,"Machine Learning deals with the issue of how to build programs that improve their performance at some task through experience. This paper deals with the subject of applying machine learning methods to software engineering. For effort estimation which not only provide an estimation but also confidence interval for it. The robust confidence intervals do not depend on the form of probability distribution of the errors in the training set. This paper compares various regression methods for software effort estimation with the help of number of experiments performed using NASA datasets and to show that robust confidence intervals can be successfully built.","",""
9,"F. Y. Yeh, M. Gallagher","An Empirical Study of Hoeffding Racing for Model Selection in k-Nearest Neighbor Classification",2005,"","","","",158,"2022-07-13 09:39:44","","10.1007/11508069_29","","",,,,,9,0.53,5,2,17,"","",""
0,"D. Ruta","A Generic Methodology for Classification of Complex Data Structures in Automotive Industry",2008,"","","","",159,"2022-07-13 09:39:44","","10.1007/978-3-540-85563-7_58","","",,,,,0,0.00,0,1,14,"","",""
1,"Xiang Meng","Doubly robust, machine learning effect estimation in real-world clinical sciences: A practical evaluation of performance in molecular epidemiology cohort settings",2021,"","","","",160,"2022-07-13 09:39:44","","","","",,,,,1,1.00,1,1,1,"Modern efficient, semi-parametric estimators such as AIPW and TMLE facilitate the application of flexible (e.g. data-adaptive, non-smooth, machine learning) algorithms to improve treatment and outcome model fit, allowing for some model misspecification while still maintaining desired bias and variance properties. Recent simulation work has pointed to essential conditions for effective application including: the need for sample splitting (Chernozhukov et al. (2018), Naimi et al. (2021)), cross-fitting (Newey and Robins (2018), Zivich and Breskin (2021)), using of a broad library of well-tuned, flexible learners (Naimi et al. (2021), Balzer and Westling (2021)), and sufficiently large sample sizes (Benkeser et al. (2017), Pang et al. (2016), Balzer and Westling (2021)). In these settings, cross-fit, efficient estimators fit with ensemble flexible learning appear to be broadly superior to conventional alternatives, as theorized. However, commonly simulated conditions differ in important ways from settings in which these estimators may be most useful (Balzer and Westling (2021)) namely in high-dimensional, observational settings (Pang et al. (2016)). In such settings, such computationally-intensive and challenging to optimize estimators may have less of a practical advantage over simpler approaches. Here we present extensive simulation results drawing data on 331 covariates from 1178 subjects of a multi-omic, longitudinal birth cohort while fixing treatment and outcome effects. We attempt to estimate average causal effects under various misspecification conditions. In real-world data structures, we find in nearly every scenario that efficient estimators fit with smooth learners outperform those that include non-smooth, data-adaptive learners on the basis of bias and confidence interval coverage. For the typical setting where correct model specification is unlikely, it is possible that the use of double-cross-fit efficient estimators fit with ensembles of smooth learners may be a generally practical choice, taking 2-5 times less computation time than models fit with non-smooth algorithms while having equal or better performance.","",""
0,"J. Huang, Xiang Meng","521Performance of doubly-robust, machine learning effect estimators in realistic epidemiologic data settings and practical recommendations",2021,"","","","",161,"2022-07-13 09:39:44","","10.1093/ije/dyab168.293","","",,,,,0,0.00,0,2,1,"      Flexible, data-adaptive algorithms (machine learning; ML) for nuisance parameter estimation in epidemiologic causal inference have promising asymptotic properties for complex, high-dimensional data. However, recently proposed applications (e.g. targeted maximum likelihood estimation; TMLE) may produce biases parameter and standard error estimates in common real-world cohort settings. The relative performance of these novel estimators over simpler approaches in such settings is unclear.        We apply double-crossfit TMLE, augmented inverse probability weighting (AIPW), and standard IPW to simple simulations (5 covariates) and “real-world” data using covariate-structure-preserving (“plasmode”) simulations of 1,178 subjects and 331 covariates from a longitudinal birth cohort. We evaluate various data generating and estimation scenarios including: under- and over- (e.g. excess orthogonal covariates) identification, poor data support, near-instruments, and mis-specified biological interactions. We also track representative computation times.        We replicate optimal performance of cross-fit, doubly robust estimators in simple data generating processes. However, in nearly every real world-based scenario, estimators fit with parametric learners outperform those that include non-parametric learners in terms of mean bias and confidence interval coverage. Even when correctly specified, estimators fit with non-parametric algorithms (xgboost, random forest) performed poorly (e.g. 24% bias, 57% coverage vs. 10% bias, 79% coverage for parametric fit), at times underperforming simple IPW.        In typical epidemiologic data sets, double-crossfit estimators fit with simple smooth, parametric learners may be the optimal solution, taking 2-5 times less computation time than flexible non-parametric models, while having equal or better performance. No approaches are optimal, and estimators should be compared on simulations close to the source data.        In epidemiologic studies, use of flexible non-parametric algorithms for effect estimation should be strongly justified (i.e. high-dimensional covariates) and performed with care. Parametric learners may be a safer option with few drawbacks. ","",""
0,"Ransalu Senanayake, Daniel J. Fremont, Mykel J. Kochenderfer, A. Lomuscio, D. Margineantu, Cheng Soon Ong","Guest Editorial: Special issue on robust machine learning",2021,"","","","",162,"2022-07-13 09:39:44","","10.1007/s10994-021-06113-4","","",,,,,0,0.00,0,6,1,"","",""
1,"Jonathan Koss, Anthony Jiang, Patrick W Sweeney, N. Rios, A. Dollar","Robust Machine Learning Classification of Unlabeled Biological Data: A case study with herbaria sheets",2021,"","","","",163,"2022-07-13 09:39:44","","10.3897/biss.5.73833","","",,,,,1,1.00,0,5,1,"There is much excitement across a broad range of biological disciplines over the prospect of using deep learning and similar modern statistical methods to label research data. The extensive time, effort, and cost required for humans to label a dataset drastically limits the type and amount of data that can be reasonably utilized, and is currently a major bottleneck to the extensive application of biological datasets such as specimen imagery, video and audio recordings. While a number of researchers have shown how deep convolutional neural networks (CNN) can be trained to classify image data with 80-90% accuracy, that range of accuracy is still too low for most research applications. Furthermore, applying these classifiers to new, unlabeled data from a dataset other than the one used for training the classifier would likely result in even lower accuracy. As a result, these classifiers have still not generally been applied to unlabeled data—which is where they could be most useful.  In this talk, we will present a method for determining a confidence metric on predicted classifications (i.e. ""labels"") from a deep CNN classifier that can inform a user whether to trust a particular automatic label or to discard it, thereby giving a reasonable and straight-forward method to label a previously unlabeled dataset with high confidence.  Essentially, it is an approach that allows an imperfect method of classification to be used in a useful way that can save an enormous amount of time and effort and/or greatly increase the amount of data that can be reasonably utilized.  In this work, the training dataset consisted of a set of records of flowering plant species that collectively exhibited a range of reproductive morphologies, represented multiple taxonomic groups, and could be easily scored by humans for reproductive condition by examination of specimen images. The records were labeled as reproductive, budding, flowering and/or fruiting. All of the data and images were obtained from the Consortium of Northeastern Herbaria portal (CNH). There were two unscored datasets that were used to evaluate the classifiers. One dataset contained the same taxa that were in the training dataset and the second dataset contained all remaining flowering plant taxa in the CNH portal database that were not included in the other two datasets. Records of families with flowers that are obscure (i.e., they lack petals & sepals or have vestigial structures) were excluded.   To label the reproductive state of the plants, we trained one deep CNN classifier using the XCeption architecture for the binary classification of each state (e.g., budding vs. not budding). This method and architecture was chosen because of its success in similar image-classification tasks.   Each of these networks takes an image of a herbarium sheet as input, and outputs a value in the interval [0,1]. In these networks, the output is typically thresholded to generate a binary label, but we found it could also be used to approximate a measure of confidence in the network’s classification. By treating this value as a confidence metric, we are able to input a large unlabeled dataset into the classifier and then trust the labels that were assigned a “high confidence” and leave the remainder unlabeled.   After training the network, the performance of the four classifiers (reproductive, budding, flowering, fruiting) achieved 85-90% accuracy compared to expert-labeled data. However, as described above, the real value of these approaches comes from their prospects for labelling previously unlabeled data, thus helping to replace expensive and time-consuming human labor. We then applied our confidence-interval-based approach to a collection of 600k images and were able to label 35-70% of the samples with a chosen confidence threshold of 95%. In other words, we could then use the high-confidence labels and simply not automatically label the remaining unclassifiable samples. The data from these samples could then be labeled manually, or, if appropriate, not labeled at all.","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",164,"2022-07-13 09:39:44","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
1,"Bappaditya Dey, Stewart Wu, Sayantan Das, Kasem Khalil, S. Halder, P. Leray, Samir Bhamidipati, Kiarash Ahi, Mark Pereira, G. Fenger, M. Bayoumi","Unsupervised machine learning based SEM image denoising for robust contour detection",2021,"","","","",165,"2022-07-13 09:39:44","","10.1117/12.2600945","","",,,,,1,1.00,0,11,1,"Contour detection of an object is a fundamental computer vision problem in image processing domain. The goal is to find a concrete boundary for pixel ownership between an OOI (object-of-interest) and its corresponding background. However, contour extraction from low SN SEM images is a very challenging problem as different sources of noise shadow the estimation of underlying structural geometries. As device scaling continues to 3nm node and below, the extraction of accurate CD contour geometries from SEM images especially ADI (after developed inspection) is of utmost importance for a qualitative lithographic process as well as to verify device characterization in aggressive pitches. In this paper, we have applied a U-Net architecture based unsupervised machine learning approach for de-noising CD-SEM images. Unlike other discriminative deep-learning based de-noising approaches, the proposed method does not require any ground-truth as clean/noiseless images or synthetic noiseless images for training. Simultaneously, we have also attempted to demonstrate how de-noising is helping to improve the contour detection accuracy. We have analyzed and validated our result by using a programmable tool (SEMSuiteTM) for contour extraction. We have de-noised SEM images with categorically different geometrical patterns such as L/S (line-space), T2T (tip-to-tip), pillars with different scan types etc. and extracted the contours in both noisy and de-noised images. The comparative analysis demonstrates that de-noised images have higher confidence contour metric than their noisy twins while keeping the same parameter settings for both data input. When the ML algorithm is applied, the contour extraction results would have higher confidence numbers comparing with the ones only applied the conventional Gaussian or Median blur de-noise method. The final goal of this work is to establish a robust de-noising method to reduce the dependency of SEM image acquisition settings and provide more accurate metrology data for OPC calibration.","",""
101,"G. Lecu'e, M. Lerasle","Robust machine learning by median-of-means: Theory and practice",2017,"","","","",166,"2022-07-13 09:39:44","","10.1214/19-AOS1828","","",,,,,101,20.20,51,2,5,"We introduce new estimators for robust machine learning based on median-of-means (MOM) estimators of the mean of real valued random variables. These estimators achieve optimal rates of convergence under minimal assumptions on the dataset. The dataset may also have been corrupted by outliers on which no assumption is granted. We also analyze these new estimators with standard tools from robust statistics. In particular, we revisit the concept of breakdown point. We modify the original definition by studying the number of outliers that a dataset can contain without deteriorating the estimation properties of a given estimator. This new notion of breakdown number, that takes into account the statistical performances of the estimators, is non-asymptotic in nature and adapted for machine learning purposes. We proved that the breakdown number of our estimator is of the order of (number of observations)*(rate of convergence). For instance, the breakdown number of our estimators for the problem of estimation of a d-dimensional vector with a noise variance sigma^2 is sigma^2d and it becomes sigma^2 s log(d/s) when this vector has only s non-zero component. Beyond this breakdown point, we proved that the rate of convergence achieved by our estimator is (number of outliers) divided by (number of observation).  Besides these theoretical guarantees, the major improvement brought by these new estimators is that they are easily computable in practice. In fact, basically any algorithm used to approximate the standard Empirical Risk Minimizer (or its regularized versions) has a robust version approximating our estimators. As a proof of concept, we study many algorithms for the classical LASSO estimator. A byproduct of the MOM algorithms is a measure of depth of data that can be used to detect outliers.","",""
34,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, Semeen Rehman, M. Shafique","Robust Machine Learning Systems: Reliability and Security for Deep Neural Networks",2018,"","","","",167,"2022-07-13 09:39:44","","10.1109/IOLTS.2018.8474192","","",,,,,34,8.50,7,5,4,"Machine learning is commonly being used in almost all the areas that involve advanced data analytics and intelligent control. From applications like Natural Language Processing (NLP) to autonomous driving are based upon machine learning algorithms. An increasing trend is observed in the use of Deep Neural Networks (DNNs) for such applications. While the slight inaccuracy in applications like NLP does not have any severe consequences, it is not the same for other safety-critical applications, like autonomous driving and smart healthcare, where a small error can lead to catastrophic effects. Apart from high-accuracy DNN algorithms, there is a significant need for robust machine learning systems and hardware architectures that can generate reliable and trustworthy results in the presence of hardware-level faults while also preserving security and privacy. This paper provides an overview of the challenges being faced in ensuring reliable and secure execution of DNNs. To address the challenges, we present several techniques for analyzing and mitigating the reliability and security threats in machine learning systems.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",168,"2022-07-13 09:39:44","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
209,"J. Blanchet, Yang Kang, M. KarthyekRajhaaA.","Robust Wasserstein profile inference and applications to machine learning",2016,"","","","",169,"2022-07-13 09:39:44","","10.1017/jpr.2019.49","","",,,,,209,34.83,70,3,6,"We show that several machine learning estimators, including square-root least absolute shrinkage and selection and regularized logistic regression, can be represented as solutions to distributionally robust optimization problems. The associated uncertainty regions are based on suitably defined Wasserstein distances. Hence, our representations allow us to view regularization as a result of introducing an artificial adversary that perturbs the empirical distribution to account for out-of-sample effects in loss estimation. In addition, we introduce RWPI (robust Wasserstein profile inference), a novel inference methodology which extends the use of methods inspired by empirical likelihood to the setting of optimal transport costs (of which Wasserstein distances are a particular case). We use RWPI to show how to optimally select the size of uncertainty regions, and as a consequence we are able to choose regularization parameters for these machine learning estimators without the use of cross validation. Numerical experiments are also given to validate our theoretical findings.","",""
168,"D. Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, Soroosh Shafieezadeh-Abadeh","Wasserstein Distributionally Robust Optimization: Theory and Applications in Machine Learning",2019,"","","","",170,"2022-07-13 09:39:44","","10.1287/EDUC.2019.0198","","",,,,,168,56.00,42,4,3,"Many decision problems in science, engineering and economics are affected by uncertain parameters whose distribution is only indirectly observable through samples. The goal of data-driven decision-making is to learn a decision from finitely many training samples that will perform well on unseen test samples. This learning task is difficult even if all training and test samples are drawn from the same distribution---especially if the dimension of the uncertainty is large relative to the training sample size. Wasserstein distributionally robust optimization seeks data-driven decisions that perform well under the most adverse distribution within a certain Wasserstein distance from a nominal distribution constructed from the training samples. In this tutorial we will argue that this approach has many conceptual and computational benefits. Most prominently, the optimal decisions can often be computed by solving tractable convex optimization problems, and they enjoy rigorous out-of-sample and asymptotic consistency guarantees. We will also show that Wasserstein distributionally robust optimization has interesting ramifications for statistical learning and motivates new approaches for fundamental learning tasks such as classification, regression, maximum likelihood estimation or minimum mean square error estimation, among others.","",""
24,"P. Zivich, A. Breskin","Machine Learning for Causal Inference: On the Use of Cross-fit Estimators",2020,"","","","",171,"2022-07-13 09:39:44","","10.1097/EDE.0000000000001332","","",,,,,24,12.00,12,2,2,"Supplemental Digital Content is available in the text. Background: Modern causal inference methods allow machine learning to be used to weaken parametric modeling assumptions. However, the use of machine learning may result in complications for inference. Doubly robust cross-fit estimators have been proposed to yield better statistical properties. Methods: We conducted a simulation study to assess the performance of several different estimators for the average causal effect. The data generating mechanisms for the simulated treatment and outcome included log-transforms, polynomial terms, and discontinuities. We compared singly robust estimators (g-computation, inverse probability weighting) and doubly robust estimators (augmented inverse probability weighting, targeted maximum likelihood estimation). We estimated nuisance functions with parametric models and ensemble machine learning separately. We further assessed doubly robust cross-fit estimators. Results: With correctly specified parametric models, all of the estimators were unbiased and confidence intervals achieved nominal coverage. When used with machine learning, the doubly robust cross-fit estimators substantially outperformed all of the other estimators in terms of bias, variance, and confidence interval coverage. Conclusions: Due to the difficulty of properly specifying parametric models in high-dimensional data, doubly robust estimators with ensemble learning and cross-fitting may be the preferred approach for estimation of the average causal effect in most epidemiologic studies. However, these approaches may require larger sample sizes to avoid finite-sample issues.","",""
53,"Bethany M. Moore, Peipei Wang, P. Fan, Bryan J. Leong, Craig A. Schenck, J. P. Lloyd, Melissa D. Lehti-Shiu, R. Last, E. Pichersky, S. Shiu","Robust predictions of specialized metabolism genes through machine learning",2018,"","","","",172,"2022-07-13 09:39:44","","10.1073/pnas.1817074116","","",,,,,53,13.25,5,10,4,"Significance Specialized metabolites are critical for plant–environment interactions, e.g., attracting pollinators or defending against herbivores, and are important sources of plant-based pharmaceuticals. However, it is unclear what proportion of enzyme-encoding genes play a role in specialized metabolism (SM) as opposed to general metabolism (GM) in any plant species. This is because of the diversity of specialized metabolites and the considerable number of incompletely characterized pathways responsible for their production. In addition, SM gene ancestors frequently played roles in GM. We evaluate features distinguishing SM and GM genes and build a computational model that accurately predicts SM genes. Our predictions provide candidates for experimental studies, and our modeling approach can be applied to other species that produce medicinally or industrially useful compounds. Plant specialized metabolism (SM) enzymes produce lineage-specific metabolites with important ecological, evolutionary, and biotechnological implications. Using Arabidopsis thaliana as a model, we identified distinguishing characteristics of SM and GM (general metabolism, traditionally referred to as primary metabolism) genes through a detailed study of features including duplication pattern, sequence conservation, transcription, protein domain content, and gene network properties. Analysis of multiple sets of benchmark genes revealed that SM genes tend to be tandemly duplicated, coexpressed with their paralogs, narrowly expressed at lower levels, less conserved, and less well connected in gene networks relative to GM genes. Although the values of each of these features significantly differed between SM and GM genes, any single feature was ineffective at predicting SM from GM genes. Using machine learning methods to integrate all features, a prediction model was established with a true positive rate of 87% and a true negative rate of 71%. In addition, 86% of known SM genes not used to create the machine learning model were predicted. We also demonstrated that the model could be further improved when we distinguished between SM, GM, and junction genes responsible for reactions shared by SM and GM pathways, indicating that topological considerations may further improve the SM prediction model. Application of the prediction model led to the identification of 1,220 A. thaliana genes with previously unknown functions, each assigned a confidence measure called an SM score, providing a global estimate of SM gene content in a plant genome.","",""
0,"H. Anh, Cao Van Kien","Robust extreme learning machine neural approach for uncertain nonlinear hyper‐chaotic system identification",2021,"","","","",173,"2022-07-13 09:39:44","","10.1002/rnc.5756","","",,,,,0,0.00,0,2,1,"This paper proposes a novel nonlinearly parameterized advanced single‐hidden layer neural extreme learning machine (ASHLN‐ELM) model in which the hidden and output weighting values are simultaneously updated using adaptively robust rules that are implemented based on Lyapunov stability principle. The proposed scheme guarantees the fast convergence speed of the state‐estimation residual errors bounded to null regarding to the influence of time‐varied disturbances. Additionally, proposed method needs no any knowledge related to desired weighting values or required approximating error. Typical uncertain hyper‐chaotic benchmark systems are used as to verify the new ASHLN‐ELM approach and to demonstrate the efficiency and the robustness of proposed method.","",""
220,"Ian J. Goodfellow, P. Mcdaniel, Nicolas Papernot","Making machine learning robust against adversarial inputs",2018,"","","","",174,"2022-07-13 09:39:44","","10.1145/3134599","","",,,,,220,55.00,73,3,4,"Such inputs distort how machine-learning-based systems are able to function in the world as it is.","",""
1146,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Efficient and Robust Automated Machine Learning",2015,"","","","",175,"2022-07-13 09:39:44","","","","",,,,,1146,163.71,191,6,7,"The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.","",""
62,"Matteo Poggi, F. Tosi, S. Mattoccia","Quantitative Evaluation of Confidence Measures in a Machine Learning World",2017,"","","","",176,"2022-07-13 09:39:44","","10.1109/ICCV.2017.559","","",,,,,62,12.40,21,3,5,"Confidence measures aim at detecting unreliable depth measurements and play an important role for many purposes and in particular, as recently shown, to improve stereo accuracy. This topic has been thoroughly investigated by Hu and Mordohai in 2010 (and 2012) considering 17 confidence measures and two local algorithms on the two datasets available at that time. However, since then major breakthroughs happened in this field: the availability of much larger and challenging datasets, novel and more effective stereo algorithms including ones based on deep learning and confidence measures leveraging on machine learning techniques. Therefore, this paper aims at providing an exhaustive and updated review and quantitative evaluation of 52 (actually, 76 considering variants) stateof- the-art confidence measures - focusing on recent ones mostly based on random-forests and deep learning - with three algorithms on the challenging datasets available today. Moreover we deal with problems inherently induced by learning-based confidence measures. How are these methods able to generalize to new data? How a specific training improves their effectiveness? How more effective confidence measures can actually improve the overall stereo accurac?","",""
11,"B. Celik, J. Vanschoren","Adaptation Strategies for Automated Machine Learning on Evolving Data",2020,"","","","",177,"2022-07-13 09:39:44","","10.1109/TPAMI.2021.3062900","","",,,,,11,5.50,6,2,2,"Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust to changes in the underlying data. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on a variety of AutoML approaches for building machine learning pipelines, including Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.","",""
23,"J. Gröhl, T. Kirchner, T. Adler, L. Maier-Hein","Confidence Estimation for Machine Learning-Based Quantitative Photoacoustics",2018,"","","","",178,"2022-07-13 09:39:44","","10.3390/jimaging4120147","","",,,,,23,5.75,6,4,4,"In medical applications, the accuracy and robustness of imaging methods are of crucial importance to ensure optimal patient care. While photoacoustic imaging (PAI) is an emerging modality with promising clinical applicability, state-of-the-art approaches to quantitative photoacoustic imaging (qPAI), which aim to solve the ill-posed inverse problem of recovering optical absorption from the measurements obtained, currently cannot comply with these high standards. This can be attributed to the fact that existing methods often rely on several simplifying a priori assumptions of the underlying physical tissue properties or cannot deal with realistic noise levels. In this manuscript, we address this issue with a new method for estimating an indicator of the uncertainty of an estimated optical property. Specifically, our method uses a deep learning model to compute error estimates for optical parameter estimations of a qPAI algorithm. Functional tissue parameters, such as blood oxygen saturation, are usually derived by averaging over entire signal intensity-based regions of interest (ROIs). Therefore, we propose to reduce the systematic error of the ROI samples by additionally discarding those pixels for which our method estimates a high error and thus a low confidence. In silico experiments show an improvement in the accuracy of optical absorption quantification when applying our method to refine the ROI, and it might thus become a valuable tool for increasing the robustness of qPAI methods.","",""
0,"Jianlong Zhou, Kun Yu, Fang Chen","Revealing User Confidence in Machine Learning-Based Decision Making",2018,"","","","",179,"2022-07-13 09:39:44","","10.1007/978-3-319-90403-0_11","","",,,,,0,0.00,0,3,4,"","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",180,"2022-07-13 09:39:44","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",181,"2022-07-13 09:39:44","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
2,"Yifeng Gao, Hosein Mohammadi Makrani, Mehrdad Aliasgari, Amin Rezaei, Jessica Lin, H. Homayoun, H. Sayadi","Adaptive-HMD: Accurate and Cost-Efficient Machine Learning-Driven Malware Detection using Microarchitectural Events",2021,"","","","",182,"2022-07-13 09:39:44","","10.1109/IOLTS52814.2021.9486701","","",,,,,2,2.00,0,7,1,"To address the high complexity and computational overheads of conventional software-based detection techniques, Hardware Malware Detection (HMD) has shown promising results as an alternative anomaly detection solution. HMD methods apply Machine Learning (ML) classifiers on microarchitectural events monitored by built-in Hardware Performance Counter (HPC) registers available in modern microprocessors to recognize the patterns of anomalies (e.g., signatures of malicious applications). Existing hardware malware detection solutions have mainly focused on utilizing standard ML algorithms to detect the existence of malware without considering an adaptive and cost-efficient approach for online malware detection. Our comprehensive analysis across a wide range of malicious software applications and different branches of machine learning algorithms indicates that the type of adopted ML algorithm to detect malicious applications at the hardware level highly correlates with the type of the examined malware, and the ultimate performance evaluation metric (F-measure, robustness, latency, detection rate/cost, etc.) to select the most efficient ML model for distinguishing the target malware from benign program. Therefore, in this work we propose Adaptive-HMD, an accurate and cost-efficient machine learning-driven framework for online malware detection using low-level microarchitectural events collected from HPC registers. Adaptive-HMD is equipped with a lightweight tree-based decision-making algorithm that accurately selects the most efficient ML model to be used for the inference in online malware detection according to the users' preference and optimal performance vs. cost (hardware overhead and latency) criteria. The experimental results demonstrate that Adaptive-HMD achieves up to 94% detection rate (F-measure) while improving the cost-efficiency of ML-based malware detection by more than 5X as compared to existing ensemble-based malware detection methods.","",""
16,"Phil Legg, Jim E. Smith, A. Downing","Visual analytics for collaborative human-machine confidence in human-centric active learning tasks",2019,"","","","",183,"2022-07-13 09:39:44","","10.1186/s13673-019-0167-8","","",,,,,16,5.33,5,3,3,"","",""
4,"Alexandra Renouard, A. Maggi, M. Grunberg, C. Doubre, C. Hibert","Toward False Event Detection and Quarry Blast versus Earthquake Discrimination in an Operational Setting Using Semiautomated Machine Learning",2021,"","","","",184,"2022-07-13 09:39:44","","10.1785/0220200305","","",,,,,4,4.00,1,5,1,"  Small-magnitude earthquakes shed light on the spatial and magnitude distribution of natural seismicity, as well as its rate and occurrence, especially in stable continental regions where natural seismicity remains difficult to explain under slow strain-rate conditions. However, capturing them in catalogs is strongly hindered by signal-to-noise ratio issues, resulting in high rates of false and man-made events also being detected. Accurate and robust discrimination of these events is critical for optimally detecting small earthquakes. This requires uncovering recurrent salient features that can rapidly distinguish first false events from real events, then earthquakes from man-made events (mainly quarry blasts), despite high signal variability and noise content. In this study, we combined the complementary strengths of human and interpretable rule-based machine-learning algorithms for solving this classification problem. We used human expert knowledge to co-create two reliable machine-learning classifiers through human-assisted selection of classification features and review of events with uncertain classifier predictions. The two classifiers are integrated into the SeisComP3 operational monitoring system. The first one discards false events from the set of events obtained with a low short-term average/long-term average threshold; the second one labels the remaining events as either earthquakes or quarry blasts. When run in an operational setting, the first classifier correctly detected more than 99% of false events and just over 93% of earthquakes; the second classifier correctly labeled 95% of quarry blasts and 96% of earthquakes. After a manual review of the second classifier low-confidence outputs, the final catalog contained fewer than 2% of misclassified events. These results confirm that machine learning strengthens the quality of earthquake catalogs and that the performance of machine-learning classifiers can be improved through human expertise. Our study promotes a broader implication of hybrid intelligence monitoring within seismological observatories.","",""
4,"Rahul Singh","A Finite Sample Theorem for Longitudinal Causal Inference with Machine Learning: Long Term, Dynamic, and Mediated Effects",2021,"","","","",185,"2022-07-13 09:39:44","","","","",,,,,4,4.00,4,1,1,"I construct and justify confidence intervals for longitudinal causal parameters estimated with machine learning. Longitudinal parameters include long term, dynamic, and mediated effects. I provide a nonasymptotic theorem for any longitudinal causal parameter estimated with any machine learning algorithm that satisfies a few simple, interpretable conditions. The main result encompasses local parameters defined for specific demographics as well as proximal parameters defined in the presence of unobserved confounding. Formally, I prove consistency, Gaussian approximation, and semiparametric efficiency. The rate of convergence is n for global parameters, and it degrades gracefully for local parameters. I articulate a simple set of conditions to translate mean square rates into statistical inference. A key feature of the main result is a new multiple robustness to ill posedness for proximal causal inference in longitudinal settings.","",""
1,"W. Weinzierl","Attribute Assisted Interpretation Confidence Classification Using Machine Learning",2017,"","","","",186,"2022-07-13 09:39:44","","10.1109/ICMLA.2017.0-178","","",,,,,1,0.20,1,1,5,"An attribute assisted classification deriving estimates of interpretation confidence was performed. Instantaneous and coherency attributes were used in a supervised followed by an unsupervised classification resulting in an error envelope of the interpretation. In an initial approximation, confidence weights for a signal and background response are estimated using support vector machine learning. Subsequently, a weighted discrimination based on several coherency attributes using self-organizing maps is obtained. The resulting quantization is used as additional input and constraint in a final probability assessment of signal confidence using instantaneous attributes in support vector machine learning. The additional input in the form of quantization vectors and possible reduction in dimensionality of the input attribute vector space, allows to combine highly non-linear correlations in a multivariate discrimination. The trained classification is used to assign signal confidence probabilities to an interpreted seismic horizon. The proposed methodology is applied to an onshore data set from Wyoming, USA, revealing how single- and multi-trace attributes can be used to quantitatively assess the uncertainty of an interpretation often lost during project maturation.","",""
0,"A. Nair, F. Yu, P. C. Jost, P. DeMott, E. Levin, J. Jimenez, J. Peischl, I. Pollack, C. Fredrickson, A. Beyersdorf, B. Nault, Minsu Park, S. Yum, B. Palm, Lu Xu, I. Bourgeois, B. Anderson, A. Nenes, L. Ziemba, R. Moore, Taehyoung Lee, T. Park, C. Thompson, F. Flocke, L. Huey, Michelle J. Kim, Q. Peng","Machine learning uncovers aerosol size information from chemistry and meteorology to quantify potential cloud-forming particles",2021,"","","","",187,"2022-07-13 09:39:44","","10.21203/rs.3.rs-244416/v1","","",,,,,0,0.00,0,27,1,"  Cloud condensation nuclei (CCN) are mediators of aerosol–cloud interactions, which contribute to the largest uncertainty in climate change prediction. Here, we present a machine learning/artificial intelligence model that quantifies CCN from variables of aerosol composition, atmospheric trace gases, and meteorology. Comprehensive multi-campaign airborne measurements, covering varied physicochemical regimes in the troposphere, confirm the validity of and help probe the inner workings of this machine learning model: revealing for the first time that different ranges of atmospheric aerosol composition and mass correspond to distinct aerosol number size distributions. Machine learning extracts this information, important for accurate quantification of CCN, additionally from both chemistry and meteorology. This can provide a physicochemically explainable, computationally efficient, robust machine learning pathway in global climate models that only resolve aerosol composition; potentially mitigating the uncertainty of effective radiative forcing due to aerosol–cloud interactions (ERFaci) and improving confidence in assessment of anthropogenic contributions and climate change projections.","",""
1,"Yue Gao, Kassem Fawaz","Scale-Adv: A Joint Attack on Image-Scaling and Machine Learning Classifiers",2021,"","","","",188,"2022-07-13 09:39:44","","","","",,,,,1,1.00,1,2,1,"As real-world images come in varying sizes, the machine learning model is part of a larger system that includes an upstream image scaling algorithm. In this system, the model and the scaling algorithm have become attractive targets for numerous attacks, such as adversarial examples and the recent imagescaling attack. In response to these attacks, researchers have developed defense approaches that are tailored to attacks at each processing stage. As these defenses are developed in isolation, their underlying assumptions become questionable when viewing them from the perspective of an end-to-end machine learning system. In this paper, we investigate whether defenses against scaling attacks and adversarial examples are still robust when an adversary targets the entire machine learning system. In particular, we propose Scale-Adv, a novel attack framework that jointly targets the image-scaling and classification stages. This framework packs several novel techniques, including novel representations of the scaling defenses. It also defines two integrations that allow for attacking the machine learning system pipeline in the white-box and black-box settings. Based on this framework, we evaluate cutting-edge defenses at each processing stage. For scaling attacks, we show that Scale-Adv can evade four out of five state-of-the-art defenses by incorporating adversarial examples. For classification, we show that Scale-Adv can significantly improve the performance of machine learning attacks by leveraging weaknesses in the scaling algorithm. We empirically observe that Scale-Adv can produce adversarial examples with less perturbation and higher confidence than vanilla black-box and white-box attacks. We further demonstrate the transferability of Scale-Adv on a commercial online API.","",""
2,"Sarath Shekkizhar, Antonio Ortega","Revisiting Local Neighborhood Methods in Machine Learning",2021,"","","","",189,"2022-07-13 09:39:44","","10.1109/DSLW51110.2021.9523409","","",,,,,2,2.00,1,2,1,"Several machine learning methods leverage the idea of locality by using k-nearest neighbor (KNN) techniques to design better pattern recognition models. However, the choice of KNN parameters such as k is often made experimentally, e.g., via cross-validation, leading to local neighborhoods without a clear geometric interpretation. In this paper, we replace KNN with our recently introduced polytope neighborhood scheme - Non Negative Kernel regression (NNK). NNK formulates neighborhood selection as a sparse signal approximation problem and is adaptive to the local distribution of samples in the neighborhood of the data point of interest. We analyze the benefits of local neighborhood construction based on NNK. In particular, we study the generalization properties of local interpolation using NNK and present data dependent bounds in the non asymptotic setting. The applicability of NNK in transductive few shot learning setting and for measuring distance between two datasets is demonstrated. NNK exhibits robust, superior performance in comparison to standard locally weighted neighborhood methods.","",""
1,"M. Frizzarin, T. O’Callaghan, T. Murphy, D. Hennessy, A. Casa","Application of machine-learning methods to milk mid-infrared spectra for discrimination of cow milk from pasture or total mixed ration diets.",2021,"","","","",190,"2022-07-13 09:39:44","","10.3168/jds.2021-20812","","",,,,,1,1.00,0,5,1,"The prevalence of ""grass-fed"" labeled food products on the market has increased in recent years, often commanding a premium price. To date, the majority of methods used for the authentication of grass-fed source products are driven by auditing and inspection of farm records. As such, the ability to verify grass-fed source claims to ensure consumer confidence will be important in the future. Mid-infrared (MIR) spectroscopy is widely used in the dairy industry as a rapid method for the routine monitoring of individual herd milk composition and quality. Further harnessing the data from individual spectra offers a promising and readily implementable strategy to authenticate the milk source at both farm and processor levels. Herein, a comprehensive comparison of the robustness, specificity, and accuracy of 11 machine-learning statistical analysis methods were tested for the discrimination of grass-fed versus non-grass-fed milks based on the MIR spectra of 4,320 milk samples collected from cows on pasture or indoor total mixed ration-based feeding systems over a 3-yr period. Linear discriminant analysis and partial least squares discriminant analysis (PLS-DA) were demonstrated to offer the greatest level of accuracy for the prediction of cow diet from MIR spectra. Parsimonious strategies for the selection of the most discriminating wavelengths within the spectra are also highlighted.","",""
0,"Snehal Prabhudesai, Nicholas C. Wang, Vinayak S. Ahluwalia, X. Huan, J. Bapuraj, Nikola Banovic, A. Rao","Stratification by Tumor Grade Groups in a Holistic Evaluation of Machine Learning for Brain Tumor Segmentation",2021,"","","","",191,"2022-07-13 09:39:44","","10.3389/fnins.2021.740353","","",,,,,0,0.00,0,7,1,"Accurate and consistent segmentation plays an important role in the diagnosis, treatment planning, and monitoring of both High Grade Glioma (HGG), including Glioblastoma Multiforme (GBM), and Low Grade Glioma (LGG). Accuracy of segmentation can be affected by the imaging presentation of glioma, which greatly varies between the two tumor grade groups. In recent years, researchers have used Machine Learning (ML) to segment tumor rapidly and consistently, as compared to manual segmentation. However, existing ML validation relies heavily on computing summary statistics and rarely tests the generalizability of an algorithm on clinically heterogeneous data. In this work, our goal is to investigate how to holistically evaluate the performance of ML algorithms on a brain tumor segmentation task. We address the need for rigorous evaluation of ML algorithms and present four axes of model evaluation—diagnostic performance, model confidence, robustness, and data quality. We perform a comprehensive evaluation of a glioma segmentation ML algorithm by stratifying data by specific tumor grade groups (GBM and LGG) and evaluate these algorithms on each of the four axes. The main takeaways of our work are—(1) ML algorithms need to be evaluated on out-of-distribution data to assess generalizability, reflective of tumor heterogeneity. (2) Segmentation metrics alone are limited to evaluate the errors made by ML algorithms and their describe their consequences. (3) Adoption of tools in other domains such as robustness (adversarial attacks) and model uncertainty (prediction intervals) lead to a more comprehensive performance evaluation. Such a holistic evaluation framework could shed light on an algorithm's clinical utility and help it evolve into a more clinically valuable tool.","",""
0,"A. Rathore, Anumeet Saini, Navjot Kaur, Aparna Singh, Ojasvi Dutta, Mrinal Bamhotra, A. Saini, Sandeep Saini","Amino Acid Composition and Charge Based Prediction of Antisepsis Peptides by Random Forest Machine Learning Algorithm",2021,"","","","",192,"2022-07-13 09:39:44","","10.1101/2021.09.26.461860","","",,,,,0,0.00,0,8,1,"Sepsis is a severe infectious disease with high mortality, and it occurs when chemicals released in the bloodstream to fight an infection trigger inflammation throughout the body and it can cause a cascade of changes that damage multiple organ systems, leading them to fail, even resulting in death. In order to reduce the possibility of sepsis or infection antiseptics are used and process is known as antisepsis. Antiseptic peptides (ASPs) show properties similar to antigram-negative peptides, antigram-positive peptides and many more. Machine learning algorithms are useful in screening and identification of therapeutic peptides and thus provide initial filters or built confidence before using time consuming and laborious experimental approaches. In this study, various machine learning algorithms like Support Vector Machine (SVM), Random Forest (RF), K-Nearest Neighbour (KNN) and Logistic Regression (LR) were evaluated for prediction of ASPs. Moreover, the characteristics physicochemical features of ASPs were also explored to use them in machine learning. Both manual and automatic feature selection methodology was employed to achieve best performance of machine learning algorithms. A 5-fold cross validation and independent data set validation proved RF as the best model for prediction of ASPs. Our RF model showed an accuracy of 97%, Matthew’s Correlation Coefficient (MCC) of 0.93, which are indication of a robust and good model. To our knowledge this is the first attempt to build a machine learning classifier for prediction of ASPs.","",""
0,"Uma Gunasilan","Debate as a learning activity for teaching programming: a case in the subject of machine learning",2021,"","","","",193,"2022-07-13 09:39:44","","10.1108/heswbl-01-2021-0006","","",,,,,0,0.00,0,1,1,"PurposeDebates are well known to encompass a variety of skills we would like higher education candidates to embody when they graduate.Design/methodology/approachDebates in a classroom with computer science as the main subject has been popular in high schools particularly with emerging issues around the area, however it does not have as an extensive similar documented outreach in tertiary education, particularly in the area of hard computer sciences and more recent concentrations of computer science, such as machine learning, artificial intelligence and cloud computing.FindingsTo explore further, the debate dataset had more methodologies applied and was split into training and testing sets, whose results were then compared by a standardized measure: Root Mean Square Error (RMSE) which is currently standard in the industry. The rationale of the approach is to quantify that debate activities have an immensely positive impact towards both the teaching and learning in technical subjects and needs to be more often and robustly used within higher education.Originality/valueThe rationale of the approach is that classroom debate activities equip students with verbal and social learning styles and an opportunity to engage with content in a way that is more comfortable than working with traditional lecture-and-laboratory style learning.","",""
0,"Yan Zhou, Murat Kantarcioglu, B. Xi","A Game Theoretic Perspective on Adversarial Machine Learning and Related Cybersecurity Applications",2021,"","","","",194,"2022-07-13 09:39:44","","10.1002/9781119723950.ch13","","",,,,,0,0.00,0,3,1,"In cybersecurity applications where machine learning algorithms are increasingly used to detect vulnerabilities, a somewhat unique challenge arises as exploits targeting machine learning models are constantly devised by the attackers. Traditional machine learning models are no longer robust and reliable when they are under attack. The action and reaction between machine learning systems and the adversary can be modeled as a game between two or more players. Under well‐defined attack models, game theory can provide robustness guarantee for machine learning models that are otherwise vulnerable to application‐time data corruption. We review two cases of game theory‐based machine learning techniques: in one case, players play a zero sum game by following a minimax strategy, while in the other case, players play a sequential game with one player as the leader and the rest as the followers. Experimental results on e‐mail spam and web spam datasets are presented. In the zero sum game, we demonstrate that an adversarial SVM model built upon the minimax strategy is much more resilient to adversarial attacks than standard SVM and one‐class SVM models. We also show that optimal learning strategies derived to counter overly pessimistic attack models can produce unsatisfactory results when the real attacks are much weaker. In the sequential game, we demonstrate that the mixed strategy, allowing a player to randomize over available strategies, is the best solution in general without knowing what types of adversaries machine learning applications are facing in the wild. We also discuss scenarios where players' behavior may derail rational decision making and models that consider such decision risks.","",""
0,"S. Taha-Mehlitz, L. Wentzler, F. Angehrn, A. Hendie, V. Ochs, V. Staartjes, M. von Fluee, A. Taha, D. Steinemann","Machine-learning based preoperative analytics for the prediction of anastomotic insufficiency in colorectal surgery: a single-centre pilot study",2021,"","","","",195,"2022-07-13 09:39:44","","10.1101/2021.12.11.21267569","","",,,,,0,0.00,0,9,1,"Background: Anastomotic insufficiency (AI) is a relatively common but grave complication after colorectal surgery. This study aims to determine whether AI can be predicted from simple preoperative data using machine learning (ML) algorithms. Methods: In this retrospective analysis, patients undergoing colorectal surgery with creation of a bowel anastomosis from the University Hospital of Basel were included. Data was split into a training set (80%) and a test set (20%). The group of patients with AI was oversampled to a ratio of 50:50 in the training set and missing values were imputed. Known predictors of AI were included as inputs: age, gender, BMI, smoking status, alcohol abuse, prior abdominal surgery, leukocytosis, haemoglobin and albumin levels, steroid use, the Charlson Comorbidity Index, the American Society of Anesthesiologists score, and renal function. Results: Of the 593 included patients, 88 experienced AI. At internal validation on unseen patients from the test set, area under the curve (AUC) was 0.64 (95% confidence interval [CI]: 0.44-0.82), calibration slope was 0.21 (95% CI: -0.02-0.46) and calibration intercept was 0.06 (95% CI: 0.01-0.1). We observed a specificity of 0.76 (95% CI: 0.68-0.84), sensitivity of 0.36 (95% CI: 0.08-0.7), and accuracy of 0.72 (95% CI: 0.65-0.8). Conclusion: By using 13 patient-related risk factors associated with AI, we demonstrate the feasibility of ML-based prediction of AI after colorectal surgery. Nevertheless, it is crucial to include multicenter data and higher sample sizes to develop a robust and generalizable model, which will subsequently allow for deployment of the algorithm in a web-based application.","",""
0,"Aurélien Olivier, C. Hoffmann, A. Mansour, L. Bressollette, Benoit Clement","Survey on machine learning applied to medical image analysis",2021,"","","","",196,"2022-07-13 09:39:44","","10.1109/CISP-BMEI53629.2021.9624442","","",,,,,0,0.00,0,5,1,"This paper presents a selective survey on recent advances in machine learning applied to medical imaging. It aims to highlight both innovations that increase the performance of the models and methods that ensure certainty, interpretability and robustness of the trained models. The paper focuses particularly on new concepts such as attention modules that allow to gather specific features considering global context. Its second main focus is given to domain adaptation methods to enhance model robustness to distribution shifts. Finally, we discuss uncertainty estimation and interpretability methods to evaluate confidence in a trained model.","",""
0,"A. Nair, F. Yu, P. Campuzano‐Jost, P. DeMott, E. Levin, J. Jimenez, J. Peischl, I. Pollack, C. Fredrickson, A. Beyersdorf, B. Nault, Minsu Park, S. Yum, B. Palm, Lu Xu, I. Bourgeois, B. Anderson, A. Nenes, L. Ziemba, R. Moore, Taehyoung Lee, T. Park, C. Thompson, F. Flocke, L. G. Huey, Michelle J. Kim, Q. Peng","Machine Learning Uncovers Aerosol Size Information From Chemistry and Meteorology to Quantify Potential Cloud‐Forming Particles",2021,"","","","",197,"2022-07-13 09:39:44","","10.1029/2021GL094133","","",,,,,0,0.00,0,27,1,"Cloud condensation nuclei (CCN) are mediators of aerosol‐cloud interactions, which contribute to the largest uncertainty in climate change prediction. Here, we present a machine learning (ML)/artificial intelligence (AI) model that quantifies CCN from model‐simulated aerosol composition, atmospheric trace gas, and meteorological variables. Comprehensive multi‐campaign airborne measurements, covering varied physicochemical regimes in the troposphere, confirm the validity of and help probe the inner workings of this ML model: revealing for the first time that different ranges of atmospheric aerosol composition and mass correspond to distinct aerosol number size distributions. ML extracts this information, important for accurate quantification of CCN, additionally from both chemistry and meteorology. This can provide a physicochemically explainable, computationally efficient, robust ML pathway in global climate models that only resolve aerosol composition; potentially mitigating the uncertainty of effective radiative forcing due to aerosol‐cloud interactions (ERFaci) and improving confidence in assessment of anthropogenic contributions and climate change projections.","",""
0,"Yinyihong Liu","Airbnb Pricing Based on Statistical Machine Learning Models",2021,"","","","",198,"2022-07-13 09:39:44","","10.1109/CONF-SPML54095.2021.00042","","",,,,,0,0.00,0,1,1,"Being one of the largest online accommodation booking platforms, Airbnb has many hosts who are seeking for more proper prices to increase their booking rate. To develop a good pricing prediction model, this paper has employed machine learning models including KNN, MLR, LASSO regression, Ridge regression, Random Forest, Gradient Boosting and XGBoost etc. While past studies on Airbnb pricing have applied quantitative pricing, some face the problems that the models are not robust enough and some face the problem of not training the model plentily. To fill this gap, we give careful consideration in exploratory data analysis to make the dataset more reasonable, apply many robust models ranging from regularized regression to ensemble models and use cross validation and random search to tune each parameter in each model. In this way, we not only select XGBoost as the best model for price prediction with R2 score 0.6321, but also uncover the features which have statistical significance with the target price.","",""
0,"Weiqi Chen, Qi Wu, Chen Yu, Haiming Wang, W. Hong, Weishuang Yin","Multipath Machine Learning Assisted Optimization and Its Application for Antenna Design",2021,"","","","",199,"2022-07-13 09:39:44","","10.1109/APS/URSI47566.2021.9704532","","",,,,,0,0.00,0,6,1,"A multipath machine learning assisted optimization (MP-MLAO) method is proposed for antenna applications. In practical antenna design tasks, the modulation of the balance between exploration and exploitation of the conventional MLAO method always relies on experience of designers, which largely impacts the performance and robustness of the algorithm. A multipath strategy is introduced based on the implementation of multiple lower confidence bound constants to enhance the robustness of the MLAO algorithm. Moreover, with the aid of variable-fidelity-based modeling, the increase of the computational burden is largely avoided. Compared with the conventional MLAO method, the proposed MP-MLAO achieves obvious performance improvement, which is validated by applying it to design a practical cavity-backed slot antenna.","",""
0,"L. Gallagher, Jill M. Williams, Drew Lazzeri, Calla Chennault, S. Jourdain, P. O'Leary, L. Condon, R. Maxwell","Sandtank-ML: An Educational Tool at the Interface of Hydrology and Machine Learning",2021,"","","","",200,"2022-07-13 09:39:44","","10.3390/w13233328","","",,,,,0,0.00,0,8,1,"Hydrologists and water managers increasingly face challenges associated with extreme climatic events. At the same time, historic datasets for modeling contemporary and future hydrologic conditions are increasingly inadequate. Machine learning is one promising technological tool for navigating the challenges of understanding and managing contemporary hydrological systems. However, in addition to the technical challenges associated with effectively leveraging ML for understanding subsurface hydrological processes, practitioner skepticism and hesitancy surrounding ML presents a significant barrier to adoption of ML technologies among practitioners. In this paper, we discuss an educational application we have developed—Sandtank-ML—to be used as a training and educational tool aimed at building user confidence and supporting adoption of ML technologies among water managers. We argue that supporting the adoption of ML methods and technologies for subsurface hydrological investigations and management requires not only the development of robust technologic tools and approaches, but educational strategies and tools capable of building confidence among diverse users.","",""
