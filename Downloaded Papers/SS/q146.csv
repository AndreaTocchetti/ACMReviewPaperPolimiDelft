Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
1,"Y. Lu, Ilgiz Murzakhanov, Spyros Chatzivasileiadis","Neural network interpretability for forecasting of aggregated renewable generation",2021,"","","","",1,"2022-07-13 10:10:07","","10.1109/SmartGridComm51999.2021.9631993","","",,,,,1,1.00,0,3,1,"With the rapid growth of renewable energy, lots of small photovoltaic (PV) prosumers emerge. Due to the uncertainty of solar power generation, there is a need for aggregated prosumers to predict solar power generation and whether solar power generation will be larger than load. This paper presents two interpretable neural networks to solve the problem: one binary classification neural network and one regression neural network. The neural networks are built using TensorFlow. The global feature importance and local feature contributions are examined by three gradient-based methods: Integrated Gradients, Expected Gradients, and DeepLIFT. Moreover, we detect abnormal cases when predictions might fail by estimating the prediction uncertainty using Bayesian neural networks. Neural networks, which are interpreted by the gradient-based methods and complemented with uncertainty estimation, provide robust and explainable forecasting for decision-makers.","",""
0,"Lech Szymanski, B. McCane, C. Atkinson","Switched linear projections and inactive state sensitivity for deep neural network interpretability",2019,"","","","",2,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,3,3,"We introduce switched linear projections for expressing the activity of a neuron in a ReLU-based deep neural network in terms of a single linear projection in the input space. The method works by isolating the active subnetwork, a series of linear transformations, that completely determine the entire computation of the deep network for a given input instance. We also propose that for interpretability it is more instructive and meaningful to focus on the patterns that deactive the neurons in the network, which are ignored by the exisiting methods that implicitly track only the active aspect of the network's computation. We introduce a novel interpretability method for the inactive state sensitivity (Insens). Comparison against existing methods shows that Insens is more robust (in the presence of noise), more complete (in terms of patterns that affect the computation) and a very effective interpretability method for deep neural networks.","",""
5,"Hui Guo, Shu Hu, Xin Wang, Ming-Ching Chang, Siwei Lyu","Robust Attentive Deep Neural Network for Exposing GAN-generated Faces",2021,"","","","",3,"2022-07-13 10:10:07","","","","",,,,,5,5.00,1,5,1,"GAN-based techniques that generate and synthesize realistic faces have caused severe social concerns and security problems. Existing methods for detecting GAN-generated faces can perform well on limited public datasets. However, images from existing public datasets do not represent real-world scenarios well enough in terms of view variations and data distributions (where real faces largely outnumber synthetic faces). The stateof-the-art methods do not generalize well in real-world problems and lack the interpretability of detection results. Performance of existing GAN-face detection models degrades significantly when facing imbalanced data distributions. To address these shortcomings, we propose a robust, attentive, end-to-end network that can spot GAN-generated faces by analyzing their eye inconsistencies. Specifically, our model learns to identify inconsistent eye components by localizing and comparing the iris artifacts between the two eyes automatically. Our deep network addresses the imbalance learning issues by considering the AUC loss and the traditional cross-entropy loss jointly. Comprehensive evaluations of the FFHQ dataset in terms of both balanced and imbalanced scenarios demonstrate the superiority of the proposed method.","",""
9,"Adam Noack, Isaac Ahern, D. Dou, Boyang Li","An Empirical Study on the Relation Between Network Interpretability and Adversarial Robustness",2020,"","","","",4,"2022-07-13 10:10:07","","10.1007/s42979-020-00390-x","","",,,,,9,4.50,2,4,2,"","",""
17,"Yi-Shan Lin, Wen-Chuan Lee, Z. B. Celik","What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors",2020,"","","","",5,"2022-07-13 10:10:07","","10.1145/3447548.3467213","","",,,,,17,8.50,6,3,2,"EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.","",""
22,"Akhilan Boopathy, Sijia Liu, Gaoyuan Zhang, Cynthia Liu, Pin-Yu Chen, Shiyu Chang, L. Daniel","Proper Network Interpretability Helps Adversarial Robustness in Classification",2020,"","","","",6,"2022-07-13 10:10:07","","","","",,,,,22,11.00,3,7,2,"Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability (namely, making network interpretation maps visually similar), or interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with a proper measurement of interpretation, it is actually difficult to prevent prediction-evasion adversarial attacks from causing interpretation discrepancy, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop an interpretability-aware defensive scheme built only on promoting robust interpretation (without the need for resorting to adversarial loss minimization). We show that our defense achieves both robust classification and robust interpretation, outperforming state-of-the-art adversarial training methods against attacks of large perturbation in particular.","",""
6,"Souvik Kundu, K. Sim, M. Gales","Incorporating a Generative Front-End Layer to Deep Neural Network for Noise Robust Automatic Speech Recognition",2016,"","","","",7,"2022-07-13 10:10:07","","10.21437/Interspeech.2016-760","","",,,,,6,1.00,2,3,6,"It is difficult to apply well-formulated model-based noise adaptation approaches to Deep Neural Network (DNN) due to the lack of interpretability of the model parameters. In this paper, we propose incorporating a generative front-end layer (GFL), which is parameterised by Gaussian Mixture Model (GMM), into the DNN. A GFL can be easily adapted to different noise conditions by applying the model-based Vector Taylor Series (VTS) to the underlying GMM. We show that incorporating a GFL to DNN yields 12.1% relative improvement over a baseline multi-condition DNN. We also show that the proposed system performs significantly better than the noise aware training method, where the per-utterance estimated noise parameters are appended to the acoustic features.","",""
2,"Gurpreet Singh, Soumyajit Gupta, Matt Lease, Clint N. Dawson","TIME: A Transparent, Interpretable, Model-Adaptive and Explainable Neural Network for Dynamic Physical Processes",2020,"","","","",8,"2022-07-13 10:10:07","","","","",,,,,2,1.00,1,4,2,"Partial Differential Equations are infinite dimensional encoded representations of physical processes. However, imbibing multiple observation data towards a coupled representation presents significant challenges. We present a fully convolutional architecture that captures the invariant structure of the domain to reconstruct the observable system. The proposed architecture is significantly low-weight compared to other networks for such problems. Our intent is to learn coupled dynamic processes interpreted as deviations from true kernels representing isolated processes for model-adaptivity. Experimental analysis shows that our architecture is robust and transparent in capturing process kernels and system anomalies. We also show that high weights representation is not only redundant but also impacts network interpretability. Our design is guided by domain knowledge, with isolated process representations serving as ground truths for verification. These allow us to identify redundant kernels and their manifestations in activation maps to guide better designs that are both interpretable and explainable unlike traditional deep-nets.","",""
28,"Nicholas Carlini","Is AmI (Attacks Meet Interpretability) Robust to Adversarial Examples?",2019,"","","","",9,"2022-07-13 10:10:07","","","","",,,,,28,9.33,28,1,3,"No. I. ATTACKING “ATTACKS MEET INTERPRETABILITY” AmI (Attacks meet Interpretability) is an “attribute-steered” defense [3] to detect [1] adversarial examples [2] on facerecognition models. By applying interpretability techniques to a pre-trained neural network, AmI identifies “important” neurons. It then creates a second augmented neural network with the same parameters but increases the weight activations of important neurons. AmI rejects inputs where the original and augmented neural network disagree. We find that this defense (presented at at NeurIPS 2018 as a spotlight paper—the top 3% of submissions) is completely ineffective, and even defense-oblivious1 attacks reduce the detection rate to 0% on untargeted attacks. That is, AmI is no more robust to untargeted attacks than the undefended original network. Figure 1 shows selected adversarial examples that fool the AmI defense. We are incredibly grateful to the authors for releasing their source code2 which we build on3. We hope that future work will continue to release source code by publication time to accelerate progress in this field.","",""
1,"Huijun Wu, Chen Wang, R. Nock, Wei Wang, Jie Yin, Kai Lu, Liming Zhu","SMINT: Toward Interpretable and Robust Model Sharing for Deep Neural Networks",2020,"","","","",10,"2022-07-13 10:10:07","","10.1145/3381833","","",,,,,1,0.50,0,7,2,"Sharing a pre-trained machine learning model, particularly a deep neural network via prediction APIs, is becoming a common practice on machine learning as a service (MLaaS) platforms nowadays. Although deep neural networks (DNN) have shown remarkable successes in many tasks, they are also criticized for the lack of interpretability and transparency. Interpreting a shared DNN model faces two additional challenges compared with interpreting a general model. (1) Limited training data can be disclosed to users. (2) The internal structure of the models may not be available. These two challenges impede the application of most existing interpretability approaches, such as saliency maps or influence functions, for DNN models. Case-based reasoning methods have been used for interpreting decisions; however, how to select and organize the data points under the constraints of shared DNN models is not discussed. Moreover, simply providing cases as explanations may not be sufficient for supporting instance level interpretability. Meanwhile, existing interpretation methods for DNN models generally lack the means to evaluate the reliability of the interpretation. In this article, we propose a framework named Shared Model INTerpreter (SMINT) to address the above limitations. We propose a new data structure called a boundary graph to organize training points to mimic the predictions of DNN models. We integrate local features, such as saliency maps and interpretable input masks, into the data structure to help users to infer the model decision boundaries. We show that the boundary graph is able to address the reliability issues in many local interpretation methods. We further design an algorithm named hidden-layer aware p-test to measure the reliability of the interpretations. Our experiments show that SMINT is able to achieve above 99% fidelity to corresponding DNN models on both MNIST and ImageNet by sharing only a tiny fraction of training data to make these models interpretable. The human pilot study demonstrates that SMINT provides better interpretability compared with existing methods. Moreover, we demonstrate that SMINT is able to assist model tuning for better performance on different user data.","",""
11,"Ming Jin, Heng Chang, Wenwu Zhu, S. Sojoudi","Power up! Robust Graph Convolutional Network via Graph Powering",2019,"","","","",11,"2022-07-13 10:10:07","","","","",,,,,11,3.67,3,4,3,"Graph convolutional networks (GCNs) are powerful tools for graph-structured data. However, they have been recently shown to be vulnerable to topological attacks. To enhance adversarial robustness, we go beyond spectral graph theory to robust graph theory. By challenging the classical graph Laplacian, we propose a new convolution operator that is provably robust in the spectral domain and is incorporated in the GCN architecture to improve expressivity and interpretability. By extending the original graph to a sequence of graphs, we also propose a robust training paradigm that encourages transferability across graphs that span a range of spatial and spectral characteristics. The proposed approaches are demonstrated in extensive experiments to simultaneously improve performance in both benign and adversarial situations. Introduction Graph convolutional networks (GCNs) are powerful extensions of convolutional neural networks (CNN) to graphstructured data. Recently, GCNs and variants have been applied to a wide range of domains, achieving state-of-the-art performances in social networks (Kipf and Welling 2017), traffic prediction (Rahimi, Cohn, and Baldwin 2018), recommendation systems (Ying et al. 2018), applied chemistry and biology (Kearnes et al. 2016; Fout et al. 2017), and natural language processing (Atwood and Towsley 2016; Hamilton, Ying, and Leskovec 2017; Bastings et al. 2017; Marcheggiani and Titov 2017), just to name a few (Zhou et al. 2018; Wu et al. 2019). GCNs belong to a family of spectral methods that deal with spectral representations of graphs (Zhou et al. 2018; Wu et al. 2019). A fundamental ingredient of GCNs is the graph convolution operation defined by the graph Laplacian in the Fourier domain: gθ ? x := ĝθ(L)x, (1) where x ∈ R is the graph signal on the set of vertices V and ĝθ is a spectral function applied to the graph Laplacian L := D − A (where D and A are the degree matrix and *The two first authors made equal contributions. This work was conducted during Heng Chang’s visit to Professor Somayeh Sojoudi’s group at UC Berkeley. Copyright © 2021, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved. the adjacency matrix, respectively). Because this operation is computational intensive for large graphs and non-spatially localized (Bruna et al. 2014), early attempts relied on a parameterization with smooth coefficients (Henaff, Bruna, and LeCun 2015) or a truncated expansion in terms of of Chebyshev polynomials (Hammond, Vandergheynst, and Gribonval 2011). By further restricting the Chebyshev polynomial order by 2, the approach in (Kipf and Welling 2017) referred henceforth as the vanilla GCN pushed the state-of-the-art performance of semi-supervised learning. The network has the following layer-wise update rule: H := ψ ( AHW (l) ) , (2) where H is the l-th layer hidden state (with H := X as nodal features),W (l) is the l-th layer weight matrix, ψ is the usual point-wise activation function, and A is the convolution operator chosen to be the degree weighted Laplacian with some slight modifications (Kipf and Welling 2017). Subsequent GCN variants have different architectures, but they all share the use of the Laplacian matrix as the convolution operator (Zhou et al. 2018; Wu et al. 2019). Why Not Graph Laplacian? Undoubtedly, the Laplacian operator (and its variants, e.g., normalized/powered Laplacian) plays a central role in spectral theory, and is a natural choice for a variety of spectral algorithms such as principal component analysis, clustering and linear embeddings (Chung and Graham 1997; Belkin and Niyogi 2002). So what can be problematic? From a spatial perspective, GCNs with d layers cannot acquire nodal information beyond its d-distance neighbors; hence, it severely limits its scope of data fusion. Recent works (Lee et al. 2018; Abu-El-Haija et al. 2018, 2019; Wu et al. 2019) alleviated this issue by directly powering the graph Laplacian. From a spectral perspective, one could demand better spectral properties, given that GCN is fundamentally a particular (yet effective) approximation of the spectral convolution (1). A key desirable property for generic spectral methods is known as “spectral separation,” namely the spectrum should comprise a few dominant eigenvalues whose associated eigenvectors reveal the sought structure in the graph. A well-known prototype is the Ramanujan property, for which the second The Thirty-Fifth AAAI Conference on Artificial Intelligence (AAAI-21)","",""
40,"J. Hao, Youngsoon Kim, Tae-Kyung Kim, Mingon Kang","PASNet: pathway-associated sparse deep neural network for prognosis prediction from high-throughput data",2018,"","","","",12,"2022-07-13 10:10:07","","10.1186/s12859-018-2500-z","","",,,,,40,10.00,10,4,4,"","",""
13,"S. Saralajew, Lars Holdijk, Maike Rees, T. Villmann","Prototype-based Neural Network Layers: Incorporating Vector Quantization",2018,"","","","",13,"2022-07-13 10:10:07","","","","",,,,,13,3.25,3,4,4,"Neural networks currently dominate the machine learning community and they do so for good reasons. Their accuracy on complex tasks such as image classification is unrivaled at the moment and with recent improvements they are reasonably easy to train. Nevertheless, neural networks are lacking robustness and interpretability. Prototype-based vector quantization methods on the other hand are known for being robust and interpretable. For this reason, we propose techniques and strategies to merge both approaches. This contribution will particularly highlight the similarities between them and outline how to construct a prototype-based classification layer for multilayer networks. Additionally, we provide an alternative, prototype-based, approach to the classical convolution operation. Numerical results are not part of this report, instead the focus lays on establishing a strong theoretical framework. By publishing our framework and the respective theoretical considerations and justifications before finalizing our numerical experiments we hope to jump-start the incorporation of prototype-based learning in neural networks and vice versa.","",""
0,"Zahra Kadkhodaie, Sreyas Mohan, Eero P. Simoncelli, C. Fernandez-Granda","Interpretable and robust blind image denoising with bias-free convolutional neural networks",2019,"","","","",14,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,4,3,"Deep convolutional networks often append additive constant (""bias"") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of ""batch normalization""). Recent state-of-the-art blind denoising methods seem to require these terms for their success. Here, however, we show that bias terms used in most CNNs (additive constants, including those used for batch normalization) interfere with the interpretability of these networks, do not help performance, and in fact prevent generalization of performance to noise levels not including in the training data. In particular, bias-free CNNs (BF-CNNs) are locally linear, and hence amenable to direct analysis with linear-algebraic tools. These analyses provide interpretations of network functionality in terms of projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology. Additionally, BF-CNNs generalize robustly, achieving near-state-of-the-art performance at noise levels well beyond the range over which they have been trained.","",""
0,"Henry Eigen, Amir Sadovnik","TopKConv: Increased Adversarial Robustness Through Deeper Interpretability",2021,"","","","",15,"2022-07-13 10:10:07","","10.1109/ICMLA52953.2021.00011","","",,,,,0,0.00,0,2,1,"Vulnerability to adversarial inputs remains an issue for deep neural networks. Attackers can slightly modify inputs in order to cause adverse behavior in otherwise highly accurate networks. In addition to making these networks less secure for real world applications, this also emphasizes a misalignment between the features the network uses to make decisions and the ones humans use. In this work we propose that more interpretable networks should yield more robust ones since they are able to rely on features that are more understandable to humans. More specifically, we take inspiration from interpretability based approaches to adversarial robustness, and propose a sparsity based defense to counter the impact of overparameterization on adversarial vulnerability. Building off of the work of the Dynamic-K algorithm, which introduces dynamic routing to fully connected layers in order to encourage sparse, interpretable predictions, we propose TopKConv, a novel method of reducing the number of activation channels used to construct each convolutional feature map. The incorporation of TopKConv alongside Dynamic-k results in a significant increase in adversarial accuracy at no cost to benign accuracy. Further, this is achieved with no fine tuning of or adversarial training.","",""
3,"O. Intrator, N. Intrator","Robust Interpretation of Neural-Network Models",1997,"","","","",16,"2022-07-13 10:10:07","","","","",,,,,3,0.12,2,2,25,"Na than Intrator Tel-Aviv University and Brown University nin©cns.brown.edu Artificial Neural Network seem very promising for regression and classification, especially for large covariate spaces. These methods represent a non-linear function as a composition of low dimensional ridge functions and therefore appear to be less sensitive to the dimensionality of the covariate space. However, due to non uniqueness of a global minimum and the existence of (possibly) many local minima, the model revealed by the network is non stable. We introduce a method to interpret neural network results which uses novel robustification techniques. This results in a robust interpretation of the model employed by the network. Simulated data from known models is used to demonstrate the interpretability results and to demonstrate the effects of different regularization methods on the robustness of the model. Graphical methods are introduced to present the interpretation results. We further demonstrate how interaction between covariates can be revealed. From this study we conclude that the interpretation method works well, but that NN models may sometimes be misinterpreted, especially if the approximations to the true model are less robust.","",""
9,"Jingyuan Wang, Yufan Wu, Mingxuan Li, Xin Lin, Junjie Wu, Chao Li","Interpretability is a Kind of Safety: An Interpreter-based Ensemble for Adversary Defense",2020,"","","","",17,"2022-07-13 10:10:07","","10.1145/3394486.3403044","","",,,,,9,4.50,2,6,2,"While having achieved great success in rich real-life applications, deep neural network (DNN) models have long been criticized for their vulnerability to adversarial attacks. Tremendous research efforts have been dedicated to mitigating the threats of adversarial attacks, but the essential trait of adversarial examples is not yet clear, and most existing methods are yet vulnerable to hybrid attacks and suffer from counterattacks. In light of this, in this paper, we first reveal a gradient-based correlation between sensitivity analysis-based DNN interpreters and the generation process of adversarial examples, which indicates the Achilles's heel of adversarial attacks and sheds light on linking together the two long-standing challenges of DNN: fragility and unexplainability. We then propose an interpreter-based ensemble framework called X-Ensemble for robust adversary defense. X-Ensemble adopts a novel detection-rectification process and features in building multiple sub-detectors and a rectifier upon various types of interpretation information toward target classifiers. Moreover, X-Ensemble employs the Random Forests (RF) model to combine sub-detectors into an ensemble detector for adversarial hybrid attacks defense. The non-differentiable property of RF further makes it a precious choice against the counterattack of adversaries. Extensive experiments under various types of state-of-the-art attacks and diverse attack scenarios demonstrate the advantages of X-Ensemble to competitive baseline methods.","",""
9,"Andrei Margeloiu, N. Simidjievski, M. Jamnik, Adrian Weller","Improving Interpretability in Medical Imaging Diagnosis using Adversarial Training",2020,"","","","",18,"2022-07-13 10:10:07","","","","",,,,,9,4.50,2,4,2,"We investigate the influence of adversarial training on the interpretability of convolutional neural networks (CNNs), specifically applied to diagnosing skin cancer. We show that gradient-based saliency maps of adversarially trained CNNs are significantly sharper and more visually coherent than those of standardly trained CNNs. Furthermore, we show that adversarially trained networks highlight regions with significant color variation within the lesion, a common characteristic of melanoma. We find that fine-tuning a robust network with a small learning rate further improves saliency maps' sharpness. Lastly, we provide preliminary work suggesting that robustifying the first layers to extract robust low-level features leads to visually coherent explanations.","",""
80,"Tianyang Xu, Zhenhua Feng, Xiaojun Wu, J. Kittler","Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking",2019,"","","","",19,"2022-07-13 10:10:07","","10.1109/ICCV.2019.00804","","",,,,,80,26.67,20,4,3,"We propose a new Group Feature Selection method for Discriminative Correlation Filters (GFS-DCF) based visual object tracking. The key innovation of the proposed method is to perform group feature selection across both channel and spatial dimensions, thus to pinpoint the structural relevance of multi-channel features to the filtering system. In contrast to the widely used spatial regularisation or feature selection methods, to the best of our knowledge, this is the first time that channel selection has been advocated for DCF-based tracking. We demonstrate that our GFS-DCF method is able to significantly improve the performance of a DCF tracker equipped with deep neural network features. In addition, our GFS-DCF enables joint feature selection and filter learning, achieving enhanced discrimination and interpretability of the learned filters. To further improve the performance, we adaptively integrate historical information by constraining filters to be smooth across temporal frames, using an efficient low-rank approximation. By design, specific temporal-spatial-channel configurations are dynamically learned in the tracking process, highlighting the relevant features, and alleviating the performance degrading impact of less discriminative representations and reducing information redundancy. The experimental results obtained on OTB2013, OTB2015, VOT2017, VOT2018 and TrackingNet demonstrate the merits of our GFS-DCF and its superiority over the state-of-the-art trackers. The code is publicly available at \url{https://github.com/XU-TIANYANG/GFS-DCF}.","",""
4,"Theodoros Tsiligkaridis, Jay Roberts","Second Order Optimization for Adversarial Robustness and Interpretability",2020,"","","","",20,"2022-07-13 10:10:07","","","","",,,,,4,2.00,2,2,2,"Deep neural networks are easily fooled by small perturbations known as adversarial attacks. Adversarial Training (AT) is a technique aimed at learning features robust to such attacks and is widely regarded as a very effective defense. However, the computational cost of such training can be prohibitive as the network size and input dimensions grow. Inspired by the relationship between robustness and curvature, we propose a novel regularizer which incorporates first and second order information via a quadratic approximation to the adversarial loss. The worst case quadratic loss is approximated via an iterative scheme. It is shown that using only a single iteration in our regularizer achieves stronger robustness than prior gradient and curvature regularization schemes, avoids gradient obfuscation, and, with additional iterations, achieves strong robustness with significantly lower training time than AT. Further, it retains the interesting facet of AT that networks learn features which are well-aligned with human perception. We demonstrate experimentally that our method produces higher quality human-interpretable features than other geometric regularization techniques. These robust features are then used to provide human-friendly explanations to model predictions.","",""
1,"L. Krishna, Guillermo A. Castillo, Utkarsh Aashu Mishra, Ayonga Hereid, Shishir Kolathaya","Linear Policies are Sufficient to Realize Robust Bipedal Walking on Challenging Terrains",2021,"","","","",21,"2022-07-13 10:10:07","","10.1109/lra.2022.3143227","","",,,,,1,1.00,0,5,1,"In this work, we demonstrate robust walking in the bipedal robot Digit on uneven terrains by just learning a single linear policy. In particular, we propose a new control pipeline, wherein the high-level trajectory modulator shapes the end-foot ellipsoidal trajectories, and the low-level gait controller regulates the torso and ankle orientation. The foot-trajectory modulator uses a linear policy and the regulator uses a linear PD control law. As opposed to neural network based policies, the proposed linear policy has only 13 learnable parameters, thereby not only guaranteeing sample efficient learning but also enabling simplicity and interpretability of the policy. This is achieved with no loss of performance on challenging terrains like slopes, stairs and outdoor landscapes. We first demonstrate robust walking in the custom simulation environment, MuJoCo, and then directly transfer to hardware with no modification of the control pipeline. We subject the biped to a series of pushes and terrain height changes, both indoors and outdoors, thereby validating the presented work.","",""
5,"Tsung-Yen Yang, Karthik Narasimham","Robust and Interpretable Grounding of Spatial References with Relation Networks",2020,"","","","",22,"2022-07-13 10:10:07","","10.18653/v1/2020.findings-emnlp.172","","",,,,,5,2.50,3,2,2,"Learning representations of spatial references in natural language is a key challenge in tasks like autonomous navigation and robotic manipulation. Recent work has investigated various neural architectures for learning multi-modal representations for spatial concepts. However, the lack of explicit reasoning over entities makes such approaches vulnerable to noise in input text or state observations. In this paper, we develop effective models for understanding spatial references in text that are robust and interpretable, without sacrificing performance. We design a text-conditioned relation network whose parameters are dynamically computed with a cross-modal attention module to capture fine-grained spatial relations between entities. This design choice provides interpretability of learned intermediate outputs. Experiments across three tasks demonstrate that our model achieves superior performance, with a 17% improvement in predicting goal locations and a 15% improvement in robustness compared to state-of-the-art systems.","",""
0,"","VISUAL INTERPRETABILITY ALONE HELPS ADVER-",2019,"","","","",23,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,0,3,"Recent works have empirically shown that there exist adversarial examples that can be hidden from neural network interpretability, and interpretability is itself susceptible to adversarial attacks. In this paper, we theoretically show that with the correct measurement of interpretation, it is actually difficult to hide adversarial examples, as confirmed by experiments on MNIST, CIFAR-10 and Restricted ImageNet. Spurred by that, we develop a novel defensive scheme built only on robust interpretation (without resorting to adversarial loss minimization). We show that our defense achieves similar classification robustness to state-of-the-art robust training methods while attaining higher interpretation robustness under various settings of adversarial attacks.","",""
1,"André Ferreira, S. Madeira, M. Gromicho, M. Carvalho, S. Vinga, Alexandra M. Carvalho","Predictive Medicine Using Interpretable Recurrent Neural Networks",2020,"","","","",24,"2022-07-13 10:10:07","","10.1007/978-3-030-68763-2_14","","",,,,,1,0.50,0,6,2,"","",""
7,"Ashkan Khakzar, Shadi Albarqouni, Nassir Navab","Learning Interpretable Features via Adversarially Robust Optimization",2019,"","","","",25,"2022-07-13 10:10:07","","10.1007/978-3-030-32226-7_88","","",,,,,7,2.33,2,3,3,"","",""
4,"Z. Su, M. Pietikäinen, Li Liu","BIRD: Learning Binary and Illumination Robust Descriptor for Face Recognition",2019,"","","","",26,"2022-07-13 10:10:07","","","","",,,,,4,1.33,1,3,3,"Recently face recognition has made significantly progress due to the advancement of large scale Deep Convolutional Neural Network (DeepCNNs). Despite the great success, the known deficiencies of DeepCNNs have not been addressed, such as the need for too much labeled training data, energy hungry, lack of theoretical interpretability, lack of robustness to image transformations and degradations, and vulnerable to attacks, which limit DeepCNNs to be used in many real world applications. Therefore, these factors make previous predominating Local Binary Patterns (LBP) based face recognition methods still irreplaceable. In this paper we propose a novel approach called BIRD (learning Binary and Illumination Robust Descriptor) for face representation, which nicely balances the three criteria: distinctiveness, robustness, and computationally inexpensive cost. We propose to learn discriminative and compact binary codes directly from six types of Pixel Difference Vectors (PDVs). For each type of binary codes, we cluster and pool these compact binary codes to obtain a histogram representation of each face image. Six global histograms derived from six types of learned compact binary codes are fused for the final face recognition. Experimental results on the CAS_PERL_R1 and LFW databases indicate the performance of our BIRD surpasses all previous binary based face recognition methods on the two evaluated datasets. More impressively, the proposed BIRD is shown to be highly robust to illumination changes, and produces 89.5% on the CAS_PEAL_R1 illumination subset, which, we believe, is so far the best reported results on this dataset. Our code is made available 1.","",""
1,"H. Cooper, G. Iyengar, Ching-Yung Lin","Deep Influence Diagrams: An Interpretable and Robust Decision Support System",2019,"","","","",27,"2022-07-13 10:10:07","","10.1007/978-3-030-20485-3_35","","",,,,,1,0.33,0,3,3,"","",""
1,"Meet H. Soni, I. Sheikh, Sunil Kumar Kopparapu","Label-Driven Time-Frequency Masking for Robust Speech Command Recognition",2019,"","","","",28,"2022-07-13 10:10:07","","10.1007/978-3-030-27947-9_29","","",,,,,1,0.33,0,3,3,"","",""
8,"Chi Zhang, S. A. Hosseini, S. Moeller, Sebastian Weingärtner, K. Uğurbil, M. Akçakaya","Scan-Specific Residual Convolutional Neural Networks for Fast MRI Using Residual RAKI",2019,"","","","",29,"2022-07-13 10:10:07","","10.1109/IEEECONF44664.2019.9048706","","",,,,,8,2.67,1,6,3,"Parallel imaging is a widely-used acceleration technique for magnetic resonance imaging (MRI). Conventional linear reconstruction approaches in parallel imaging suffer from noise amplification. Recently, a non-linear method that utilizes subject- specific convolutional neural networks for k-space reconstruction, Robust Artificial-neural-networks for k-space Interpolation (RAKI) was proposed and shown to improve noise resilience over linear methods. However, the linear convolutions still provide a sufficient baseline image quality and interpretability. In this paper, we sought to utilize a residual network architecture to combine the benefits of both the linear and non-linear RAKI reconstructions. This hybrid method, called residual RAKI (rRAKI) offers significant improvement in image quality compared to linear method, and improves upon RAKI in highly- accelerated simultaneous multi-slice imaging. Furthermore, it establishes an interpretable view for the use of CNNs in parallel imaging, as the CNN component in the residual network removes the noise amplification arising from the linear part.","",""
12,"Tomas Van Pottelbergh, G. Drion, R. Sepulchre","Robust Modulation of Integrate-and-Fire Models",2017,"","","","",30,"2022-07-13 10:10:07","","10.1162/neco_a_01065","","",,,,,12,2.40,4,3,5,"By controlling the state of neuronal populations, neuromodulators ultimately affect behavior. A key neuromodulation mechanism is the alteration of neuronal excitability via the modulation of ion channel expression. This type of neuromodulation is normally studied with conductance-based models, but those models are computationally challenging for large-scale network simulations needed in population studies. This article studies the modulation properties of the multiquadratic integrate-and-fire model, a generalization of the classical quadratic integrate-and-fire model. The model is shown to combine the computational economy of integrate-and-fire modeling and the physiological interpretability of conductance-based modeling. It is therefore a good candidate for affordable computational studies of neuromodulation in large networks.","",""
4,"Z. Xue, Lixin Duan, Wen Li, Lin Chen, Jiebo Luo","Region Comparison Network for Interpretable Few-shot Image Classification",2020,"","","","",31,"2022-07-13 10:10:07","","","","",,,,,4,2.00,1,5,2,"While deep learning has been successfully applied to many real-world computer vision tasks, training robust classifiers usually requires a large amount of well-labeled data. However, the annotation is often expensive and time-consuming. Few-shot image classification has thus been proposed to effectively use only a limited number of labeled examples to train models for new classes. Recent works based on transferable metric learning methods have achieved promising classification performance through learning the similarity between the features of samples from the query and support sets. However, rare of them explicitly considers the model interpretability, which can actually be revealed during the training phase.  For that, in this work, we propose a metric learning based method named Region Comparison Network (RCN), which is able to reveal how few-shot learning works as in a neural network as well as to find out specific regions that are related to each other in images coming from the query and support sets. Moreover, we also present a visualization strategy named Region Activation Mapping (RAM) to intuitively explain what our method has learned by visualizing intermediate variables in our network. We also present a new way to generalize the interpretability from the level of tasks to categories, which can also be viewed as a method to find the prototypical parts for supporting the final decision of our RCN. Extensive experiments on four benchmark datasets clearly show the effectiveness of our method over existing baselines.","",""
2,"H. Cooper, G. Iyengar, Ching-Yung Lin","Interpretable Robust Decision Making",2018,"","","","",32,"2022-07-13 10:10:07","","","","",,,,,2,0.50,1,3,4,"Interpretable decision making frameworks allow us to easily endow agents with specific goals, risk tolerances, and understanding. Existing decision making systems either forgo interpretability, or pay for it with severely reduced efficiency and large memory requirements. In this paper, we outline DeepID, a neural network approximation of Influence Diagrams, that avoids both pitfalls.","",""
51,"A. Garcez, L. Lamb","Neurosymbolic AI: The 3rd Wave",2020,"","","","",33,"2022-07-13 10:10:07","","","","",,,,,51,25.50,26,2,2,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.","",""
23,"Lizhi Wang, Chen Sun, Maoqing Zhang, Ying Fu, Hua Huang","DNU: Deep Non-Local Unrolling for Computational Spectral Imaging",2020,"","","","",34,"2022-07-13 10:10:07","","10.1109/cvpr42600.2020.00173","","",,,,,23,11.50,5,5,2,"Computational spectral imaging has been striving to capture the spectral information of the dynamic world in the last few decades. In this paper, we propose an interpretable neural network for computational spectral imaging. First, we introduce a novel data-driven prior that can adaptively exploit both the local and non-local correlations among the spectral image. Our data-driven prior is integrated as a regularizer into the reconstruction problem. Then, we propose to unroll the reconstruction problem into an optimization-inspired deep neural network. The architecture of the network has high interpretability by explicitly characterizing the image correlation and the system imaging model. Finally, we learn the complete parameters in the network through end-to-end training, enabling robust performance with high spatial-spectral fidelity. Extensive simulation and hardware experiments validate the superior performance of our method over state-of-the-art methods.","",""
3,"Javier Viaña, Kelly Cohen","Extension to Multidimensional Problems of a Fuzzy- based Explainable & Noise-Resilient Algorithm",2021,"","","","",35,"2022-07-13 10:10:07","","","","",,,,,3,3.00,2,2,1,"While Deep Neural Networks (DNNs) have shown incredible performance in a variety of data, they are brittle and opaque: easily fooled by the presence of noise, and difficult to understand the underlying reasoning for their predictions or choices. This focus on accuracy at the expense of interpretability and robustness caused little concern since, until recently, DNNs were employed primarily for scientific and limited commercial work. An increasing, widespread use of artificial intelligence and growing emphasis on user data protections, however, motivates the need for robust solutions with explainable methods and results. In this work, we extend a novel fuzzy based algorithm for regression to multidimensional problems. Previous research demonstrated that this approach outperforms neural network benchmarks while using only 5% of the number of the parameters.","",""
1,"Peishuo Sun, Ying Wu, Chaoyi Yin, H. Jiang, Ying Xu, Huiyan Sun","Molecular Subtyping of Cancer Based on Distinguishing Co-Expression Modules and Machine Learning",2022,"","","","",36,"2022-07-13 10:10:07","","10.3389/fgene.2022.866005","","",,,,,1,1.00,0,6,1,"Molecular subtyping of cancer is recognized as a critical and challenging step towards individualized therapy. Most existing computational methods solve this problem via multi-classification of gene-expressions of cancer samples. Although these methods, especially deep learning, perform well in data classification, they usually require large amounts of data for model training and have limitations in interpretability. Besides, as cancer is a complex systemic disease, the phenotypic difference between cancer samples can hardly be fully understood by only analyzing single molecules, and differential expression-based molecular subtyping methods are reportedly not conserved. To address the above issues, we present here a new framework for molecular subtyping of cancer through identifying a robust specific co-expression module for each subtype of cancer, generating network features for each sample by perturbing correlation levels of specific edges, and then training a deep neural network for multi-class classification. When applied to breast cancer (BRCA) and stomach adenocarcinoma (STAD) molecular subtyping, it has superior classification performance over existing methods. In addition to improving classification performance, we consider the specific co-expressed modules selected for subtyping to be biologically meaningful, which potentially offers new insight for diagnostic biomarker design, mechanistic studies of cancer, and individualized treatment plan selection.","",""
1,"Chih-Hsu Lin, O. Lichtarge","Using interpretable deep learning to model cancer dependencies",2021,"","","","",37,"2022-07-13 10:10:07","","10.1093/bioinformatics/btab137","","",,,,,1,1.00,1,2,1,"Abstract Motivation Cancer dependencies provide potential drug targets. Unfortunately, dependencies differ among cancers and even individuals. To this end, visible neural networks (VNNs) are promising due to robust performance and the interpretability required for the biomedical field. Results We design Biological visible neural network (BioVNN) using pathway knowledge to predict cancer dependencies. Despite having fewer parameters, BioVNN marginally outperforms traditional neural networks (NNs) and converges faster. BioVNN also outperforms an NN based on randomized pathways. More importantly, dependency predictions can be explained by correlating with the neuron output states of relevant pathways, which suggest dependency mechanisms. In feature importance analysis, BioVNN recapitulates known reaction partners and proposes new ones. Such robust and interpretable VNNs may facilitate the understanding of cancer dependency and the development of targeted therapies. Availability and implementation Code and data are available at https://github.com/LichtargeLab/BioVNN Supplementary information Supplementary data are available at Bioinformatics online.","",""
0,"Keke Du, Shane Chang, Huixiang Wen, Hao Zhang","Fighting Adversarial Images With Interpretable Gradients",2021,"","","","",38,"2022-07-13 10:10:07","","10.1145/3472634.3472644","","",,,,,0,0.00,0,4,1,"Adversarial images are specifically designed to fool neural networks into making a wrong decision about what they are looking at, which severely degrade neural network accuracy. Recently, empirical and theoretical evidence suggests that robust neural network models tend to have better interpretable gradients. Therefore, we speculate that improving the interpretability of the gradients of the neural network models may also help to improve the robustness of the models. Two methods are used to add gradient-dependent constraint terms to the loss function of neural network models and both improve the robustness of the models. The first method adds the fussed lasso penalty term of the saliency maps to the loss function of the neural network models, which makes the saliency maps arrange in a natural way to improve the interpretability of the saliency maps, and uses the gradient enhancement for relu instead of relu to strengthen the constraint of regularization term on saliency maps. In the second method, the cosine similarity penalty term between the input gradients and the image contour is added to the loss function of the model to constrain the approximation between the input gradients and the image contour. This method has a certain biological significance, because the contour information of the image is used in the human visual system to recognize the image. Both methods improve the interpretability of model‘s gradients and the first method exceeds most regularization methods except adversarial training on MNIST and the second method even exceeds the adversarial training under white-box attacks on CIFAR-10 and CIFAR-100.","",""
9,"J. Stember, Hrithwik Shalu","Deep reinforcement learning to detect brain lesions on MRI: a proof-of-concept application of reinforcement learning to medical images",2020,"","","","",39,"2022-07-13 10:10:07","","","","",,,,,9,4.50,5,2,2,"Purpose: AI in radiology is hindered chiefly by: 1) Requiring large annotated data sets. 2) Non-generalizability that limits deployment to new scanners / institutions. And 3) Inadequate explainability and interpretability. We believe that reinforcement learning can address all three shortcomings, with robust and intuitive algorithms trainable on small datasets. To the best of our knowledge, reinforcement learning has not been directly applied to computer vision tasks for radiological images. In this proof-of-principle work, we train a deep reinforcement learning network to predict brain tumor location.  Materials and Methods: Using the BraTS brain tumor imaging database, we trained a deep Q network on 70 post-contrast T1-weighted 2D image slices. We did so in concert with image exploration, with rewards and punishments designed to localize lesions. To compare with supervised deep learning, we trained a keypoint detection convolutional neural network on the same 70 images. We applied both approaches to a separate 30 image testing set.  Results: Reinforcement learning predictions consistently improved during training, whereas those of supervised deep learning quickly diverged. Reinforcement learning predicted testing set lesion locations with 85% accuracy, compared to roughly 7% accuracy for the supervised deep network.  Conclusion: Reinforcement learning predicted lesions with high accuracy, which is unprecedented for such a small training set. We believe that reinforcement learning can propel radiology AI well past the inherent limitations of supervised deep learning, with more clinician-driven research and finally toward true clinical applicability.","",""
78,"Edward Chou, Florian Tramèr, Giancarlo Pellegrino, D. Boneh","SentiNet: Detecting Physical Attacks Against Deep Learning Systems",2018,"","","","",40,"2022-07-13 10:10:07","","","","",,,,,78,19.50,20,4,4,"SentiNet is a novel detection framework for physical attacks on neural networks, a class of attacks that constrains an adversarial region to a visible portion of an image. Physical attacks have been shown to be robust and flexible techniques suited for deployment in real-world scenarios. Unlike most other adversarial detection works, SentiNet does not require training a model or preknowledge of an attack prior to detection. This attack-agnostic approach is appealing due to the large number of possible mechanisms and vectors of attack an attack-specific defense would have to consider. By leveraging the neural network’s susceptibility to attacks and by using techniques from model interpretability and object detection as detection mechanisms, SentiNet turns a weakness of a model into a strength. We demonstrate the effectiveness of SentiNet on three different attacks— i.e., adversarial examples, data poisoning attacks, and trojaned networks—that have large variations in deployment mechanisms, and show that our defense is able to achieve very competitive performance metrics for all three threats, even against strong adaptive adversaries with full knowledge of SentiNet.","",""
8,"Laura Rieger, L. K. Hansen","A simple defense against adversarial attacks on heatmap explanations",2020,"","","","",41,"2022-07-13 10:10:07","","","","",,,,,8,4.00,4,2,2,"With machine learning models being used for more sensitive applications, we rely on interpretability methods to prove that no discriminating attributes were used for classification. A potential concern is the so-called ""fair-washing"" - manipulating a model such that the features used in reality are hidden and more innocuous features are shown to be important instead.  In our work we present an effective defence against such adversarial attacks on neural networks. By a simple aggregation of multiple explanation methods, the network becomes robust against manipulation. This holds even when the attacker has exact knowledge of the model weights and the explanation methods used.","",""
6,"Amit Dhurandhar, Karthikeyan Shanmugam, Ronny Luss","Enhancing Simple Models by Exploiting What They Already Know",2019,"","","","",42,"2022-07-13 10:10:07","","","","",,,,,6,2.00,2,3,3,"There has been recent interest in improving performance of simple models for multiple reasons such as interpretability, robust learning from small data, deployment in memory constrained settings as well as environmental considerations. In this paper, we propose a novel method SRatio that can utilize information from high performing complex models (viz. deep neural networks, boosted trees, random forests) to reweight a training dataset for a potentially low performing simple model of much lower complexity such as a decision tree or a shallow network enhancing its performance. Our method also leverages the per sample hardness estimate of the simple model which is not the case with the prior works which primarily consider the complex model's confidences/predictions and is thus conceptually novel. Moreover, we generalize and formalize the concept of attaching probes to intermediate layers of a neural network to other commonly used classifiers and incorporate this into our method. The benefit of these contributions is witnessed in the experiments where on 6 UCI datasets and CIFAR-10 we outperform competitors in a majority (16 out of 27) of the cases and tie for best performance in the remaining cases. In fact, in a couple of cases, we even approach the complex model's performance. We also conduct further experiments to validate assertions and intuitively understand why our method works. Theoretically, we motivate our approach by showing that the weighted loss minimized by simple models using our weighting upper bounds the loss of the complex model.","",""
5,"Xinjian Gao, Tingting Mu, J. Y. Goulermas, Jeyarajan Thiyagalingam, Meng Wang","An Interpretable Deep Architecture for Similarity Learning Built Upon Hierarchical Concepts",2020,"","","","",43,"2022-07-13 10:10:07","","10.1109/TIP.2020.2965275","","",,,,,5,2.50,1,5,2,"In general, development of adequately complex mathematical models, such as deep neural networks, can be an effective way to improve the accuracy of learning models. However, this is achieved at the cost of reduced post-hoc model interpretability, because what is learned by the model can become less intelligible and tractable to humans as the model complexity increases. In this paper, we target a similarity learning task in the context of image retrieval, with a focus on the model interpretability issue. An effective similarity neural network (SNN) is proposed not only to seek robust retrieval performance but also to achieve satisfactory post-hoc interpretability. The network is designed by linking the neuron architecture with the organization of a concept tree and by formulating neuron operations to pass similarity information between concepts. Various ways of understanding and visualizing what is learned by the SNN neurons are proposed. We also exhaustively evaluate the proposed approach using a number of relevant datasets against a number of state-of-the-art approaches to demonstrate the effectiveness of the proposed network. Our results show that the proposed approach can offer superior performance when compared against state-of-the-art approaches. Neuron visualization results are demonstrated to support the understanding of the trained neurons.","",""
2,"A. Garcez, L. Lamb","A I ] 1 0 D ec 2 02 0 Neurosymbolic AI : The 3 rd Wave",2020,"","","","",44,"2022-07-13 10:10:07","","","","",,,,,2,1.00,1,2,2,"Current advances in Artificial Intelligence (AI) and Machine Learning (ML) have achieved unprecedented impact across research communities and industry. Nevertheless, concerns about trust, safety, interpretability and accountability of AI were raised by influential thinkers. Many have identified the need for well-founded knowledge representation and reasoning to be integrated with deep learning and for sound explainability. Neural-symbolic computing has been an active area of research for many years seeking to bring together robust learning in neural networks with reasoning and explainability via symbolic representations for network models. In this paper, we relate recent and early research results in neurosymbolic AI with the objective of identifying the key ingredients of the next wave of AI systems. We focus on research that integrates in a principled way neural network-based learning with symbolic knowledge representation and logical reasoning. The insights provided by 20 years of neural-symbolic computing are shown to shed new light onto the increasingly prominent role of trust, safety, interpretability and accountability of AI. We also identify promising directions and challenges for the next decade of AI research from the perspective of neural-symbolic systems.","",""
2,"S. Siltanen, Takanori Ide","Electrical Impedance Tomography, Enclosure Method and Machine Learning",2020,"","","","",45,"2022-07-13 10:10:07","","10.1109/MLSP49062.2020.9231717","","",,,,,2,1.00,1,2,2,"Electrical impedance tomography (EIT) is a non-destructive imaging method, where a physical body is probed with electric measurements at the boundary, and information about the internal conductivity is extracted from the data. The enclosure method of Ikehata [J. Inv. III-Posed Prob. 8(2000)] recovers the convex hull of an inclusion of unknown conductivity embedded in known background conductivity. Practical implementations of the enclosure method are based on least-squares (LS) fitting of lines to noise-robust values of the so-called indicator function. It is shown how a convolutional neural network instead of LS fitting improves the accuracy of the enclosure method significantly while retaining interpretability.","",""
2,"A. Kallipolitis, A. Stratigos, A. Zarras, Ilias Maglogiannis","Explainable Fully Connected Visual Words for the Classification of Skin Cancer Confocal Images: Interpreting the influence of visual words in classifying benign vs malignant pattern",2020,"","","","",46,"2022-07-13 10:10:07","","10.1145/3411408.3411435","","",,,,,2,1.00,1,4,2,"Skin cancer is affecting the lives of million people worldwide. Early detection and treatment of the cause can reduce drastically morbidity. Although the main workflow in dermatology clinics includes invasive skin removal procedures for diagnostic purposes, Reflectance Confocal Microscopy (RCM) provides an ancillary, non-invasive methodology for reviewing areas of interest of the human skin at a high resolution. In this paper, we propose a method for the classification and the interpretation of visual patterns in skin cancer confocal images. Both tasks are based on the formation of a visual vocabulary from Speeded up Robust Features (SURF) and the utilization of simple shallow artificial neural network with fully connected layers. Interpretability of the predictive models is also quite important, since it improves their reliability, accountability, transparency and provides useful insight of how to evolve the predictive model towards better performance. The paper discusses the technical details of both approaches along with some initial results.","",""
15,"Chen Liu, Ryota Tomioka, V. Cevher","On Certifying Non-uniform Bound against Adversarial Attacks",2019,"","","","",47,"2022-07-13 10:10:07","","","","",,,,,15,5.00,5,3,3,"This work studies the robustness certification problem of neural network models, which aims to find certified adversary-free regions as large as possible around data points. In contrast to the existing approaches that seek regions bounded uniformly along all input features, we consider non-uniform bounds and use it to study the decision boundary of neural network models. We formulate our target as an optimization problem with nonlinear constraints. Then, a framework applicable for general feedforward neural networks is proposed to bound the output logits so that the relaxed problem can be solved by the augmented Lagrangian method. Our experiments show the non-uniform bounds have larger volumes than uniform ones. Compared with normal models, the robust models have even larger non-uniform bounds and better interpretability. Further, the geometric similarity of the non-uniform bounds gives a quantitative, data-agnostic metric of input features’ robustness.","",""
3,"D. Adjodah, Tim Klinger","Symbolic Relation Networks for Reinforcement Learning",2018,"","","","",48,"2022-07-13 10:10:07","","","","",,,,,3,0.75,2,2,4,"In recent years, reinforcement learning techniques have enjoyed considerable success in a variety of challenging domains, but are typically sample inefficient and often fail to generalize well to new environments or tasks. Humans, by contrast, are able to learn robust skills with orders of magnitude less training. One hypothesis for this discrepancy is that humans view the world in terms of objects and relations between them. Such a bias may be useful reducing sample complexity and improving interpretability and generalization. In this paper, we present a novel relational architecture which has multiple neural network sub-modules called relational units which operate on objects and output values in the unit interval. Our model transforms the input state representation into a relational representation, which is then supplied as input to a Q-learner. Experiments on a goal-seeking game with random boards show better performance over several baselines: a multiheaded attention model, a standard MLP, a pixel MLP and a symbolic RL model. We also find that the relations learned in the network are interpretable.","",""
19,"Mohsen Hajiloo, H. Rabiee, Mahdi Anooshahpour","Fuzzy support vector machine: an efficient rule-based classification technique for microarrays",2013,"","","","",49,"2022-07-13 10:10:07","","10.1186/1471-2105-14-S13-S4","","",,,,,19,2.11,6,3,9,"","",""
20,"S. Kundu, D. Parhi, B. Deepak","Fuzzy-Neuro based Navigational Strategy for Mobile Robot",2012,"","","","",50,"2022-07-13 10:10:07","","","","",,,,,20,2.00,7,3,10,"A new paradigm of intelligent navigation system for mobile robot has been enriched with some common features like: criteria for optimal performance and ways to optimize design, structure and control of robot. With the growing need for the deployment of intelligent and highly autonomous systems, it would be beneficial to flawlessly combine robust learning capabilities of artificial neural networks with a high level of knowledge interpretability provided by fuzzy-logic. Fuzzy-neural network is able to build comprehensive knowledge bases considering sensor-rich system with real time constraints by adaptive learning, rule extraction and insertion, and neural/fuzzy reasoning. This technique is simulated and also compared with other simulation studies by previous researcher . The training for back propagation algorithm and its navigational performances analysis has been done in real experimental setup. As experimental result matches well with the simulation result, the realism of method is verified.","",""
3,"R. Kothari","Robust regression based training of ANFIS",1999,"","","","",51,"2022-07-13 10:10:07","","10.1109/NAFIPS.1999.781765","","",,,,,3,0.13,3,1,23,"The Adaptive Neuro Fuzzy Inference System (ANFIS) is an attractive compromise between the adaptability of a neural network and the interpretability of a fuzzy inference system. Typically, the membership functions of some of the variables can be determined a priori based on domain knowledge. Membership functions of the other variables are adapted using a hybrid learning rule. The hybrid learning rule is based on a decomposition of the parameter set and learning is based on interleaving of two phases. In one phase, the consequent parameters are adjusted using a least squares algorithm, assuming the premise parameters are fixed. In the second phase the premise parameters are adjusted using gradient descent, assuming the consequent parameters are fixed. However, the least squares algorithm used in adjusting the consequent parameters is susceptible to outliers and often leads to premise parameters (membership functions) that are less meaningful. We study this effect using noisy data sets and propose a hybrid learning algorithm based on robust regression for training the ANFIS.","",""
1,"M. Fontaine, D. Smith","Fuzzy logic for controlling call admission in ATM networks",1997,"","","","",52,"2022-07-13 10:10:07","","10.1007/978-0-387-35318-0_12","","",,,,,1,0.04,1,2,25,"","",""
1,"P. Hahn","A Tree-structured Approach to Medical Diagnosis Tasks",1996,"","","","",53,"2022-07-13 10:10:07","","","","",,,,,1,0.04,1,1,26,"A general approach for the generation of structured knowledge by integrating tree-structured and connec-tionist mechanisms is proposed. We present our main ideas concerning the amalgamation of symbolic tree structures and topology preserving neural networks. Goals of this research are to develop a model that is explainable in symbolic terms and supports direct interaction with the user as well as being trainable and robust like connectionist systems. As a report on the progress of our work, we introduce a dynamic hierarchical neural architecture that provides topology preservation and structural interpretability. It consists of a number of Kohonen chains which grow by inserting neurons according to a quantization error criterion. The hierarchical structure is developed mainly by detecting topological defects in a chain and resolving them by splitting of the chain. Neighboring chains remain connected and this topology preserving feature provides the interpretability of the network structure. We evaluate the topologic eeect by using a known measure for the topology preservation and give a simple example for the training result in two dimensions. We then demonstrate the capabilities of our approach in the medical diagnosis of Ulnar nerve lesions, which lead to motoric dysfunction of the ring and the little nger. Visual inspection by medical experts is supposed to be supported by an automatic diagnosis method that is based on the experience of many other previous cases. It is shown how our network model can be used to reach the goals of medical diagnosis such as distinction of diierent dysfunctions or judgement of success after surgical reconstruction. We present the current state of our research and outline future work.","",""
77,"Yu Zhang, P. Tiňo, A. Leonardis, K. Tang","A Survey on Neural Network Interpretability",2020,"","","","",54,"2022-07-13 10:10:07","","10.1109/TETCI.2021.3100641","","",,,,,77,38.50,19,4,2,"Along with the great success of deep neural networks, there is also growing concern about their black-box nature. The interpretability issue affects people's trust on deep learning systems. It is also related to many ethical problems, e.g., algorithmic discrimination. Moreover, interpretability is a desired property for deep networks to become powerful tools in other research fields, e.g., drug discovery and genomics. In this survey, we conduct a comprehensive review of the neural network interpretability research. We first clarify the definition of interpretability as it has been used in many different contexts. Then we elaborate on the importance of interpretability and propose a novel taxonomy organized along three dimensions: type of engagement (passive vs. active interpretation approaches), the type of explanation, and the focus (from local to global interpretability). This taxonomy provides a meaningful 3D view of distribution of papers from the relevant literature as two of the dimensions are not simply categorical but allow ordinal subcategories. Finally, we summarize the existing interpretability evaluation methods and suggest possible research directions inspired by our new taxonomy.","",""
2,"Patrick McClure, D. Moraczewski, K. Lam, Adam Thomas, Francisco Pereira","Evaluating Adversarial Robustness for Deep Neural Network Interpretability using fMRI Decoding",2020,"","","","",55,"2022-07-13 10:10:07","","","","",,,,,2,1.00,0,5,2,"While deep neural networks (DNNs) are being increasingly used to make predictions from high-dimensional, complex data, they are widely seen as uninterpretable ""black boxes"", since it can be difficult to discover what input information is used to make predictions. This ability is particularly important for applications in cognitive neuroscience and neuroinformatics. A saliency map is a common approach for producing interpretable visualizations of the relative importance of input features for a prediction. However, many methods for creating these maps fail due to focusing too much on the input or being extremely sensitive to small input noise. It is also challenging to quantitatively evaluate how well saliency maps correspond to the truly relevant input information. In this paper, we develop two quantitative evaluation procedures for saliency methods, using the fact that the Human Connectome Project (HCP) dataset contains functional magnetic resonance imaging (fMRI) data from multiple tasks per subject to create ground truth saliency maps. We then introduce an adversarial training method that makes DNNs robust to small input noise, and demonstrate that it measurably improves interpretability.","",""
4,"Jacob M. Springer, M. Mitchell, Garrett T. Kenyon","Adversarial Perturbations Are Not So Weird: Entanglement of Robust and Non-Robust Features in Neural Network Classifiers",2021,"","","","",56,"2022-07-13 10:10:07","","10.2172/1823733","","",,,,,4,4.00,1,3,1,"Neural networks trained on visual data are wellknown to be vulnerable to often imperceptible adversarial perturbations. The reasons for this vulnerability are still being debated in the literature. Recently Ilyas et al. (2019) showed that this vulnerability arises, in part, because neural network classifiers rely on highly predictive but brittle “non-robust” features. In this paper we extend the work of Ilyas et al. by investigating the nature of the input patterns that give rise to these features. In particular, we hypothesize that in a neural network trained in a standard way, non-robust features respond to small, “non-semantic” patterns that are typically entangled with larger, robust patterns, known to be more human-interpretable, as opposed to solely responding to statistical artifacts in a dataset. Thus, adversarial examples can be formed via minimal perturbations to these small, entangled patterns. In addition, we demonstrate a corollary of our hypothesis: robust classifiers are more effective than standard (non-robust) ones as a source for generating transferable adversarial examples in both the untargeted and targeted settings. The results we present in this paper provide new insight into the nature of the non-robust features responsible for adversarial vulnerability of neural network classifiers.","",""
405,"David Alvarez-Melis, T. Jaakkola","Towards Robust Interpretability with Self-Explaining Neural Networks",2018,"","","","",57,"2022-07-13 10:10:07","","","","",,,,,405,101.25,203,2,4,"Most recent work on interpretability of complex machine learning models has focused on estimating a posteriori explanations for previously trained models around specific predictions. Self-explaining models where interpretability plays a key role already during learning have received much less attention. We propose three desiderata for explanations in general – explicitness, faithfulness, and stability – and show that existing methods do not satisfy them. In response, we design self-explaining models in stages, progressively generalizing linear classifiers to complex yet architecturally explicit models. Faithfulness and stability are enforced via regularization specifically tailored to such models. Experimental results across various benchmark datasets show that our framework offers a promising direction for reconciling model complexity and interpretability.","",""
0,"K. Supekar, S. Ryali, R. Yuan, D. Kumar, C. de los Angeles, V. Menon","Identification of robust and interpretable brain signatures of autism and clinical symptom severity using a dynamic time-series deep neural network",2021,"","","","",58,"2022-07-13 10:10:07","","10.1192/j.eurpsy.2021.397","","",,,,,0,0.00,0,6,1,"Introduction Autism spectrum disorder (ASD) is among the most common and pervasive neurodevelopmental disorders. Yet, despite decades of research, the neurobiology of ASD is still poorly understood, as inconsistent findings preclude the identification of robust and interpretable neurobiological markers and predictors of clinical symptoms. Objectives Identify robust and interpretable dynamic brain markers that distinguish children with ASD from typically-developing (TD) children and predict clinical symptom severity. Methods We leverage multiple functional brain imaging cohorts (ABIDE, Stanford; N = 1004) and exciting recent advances in explainable artificial intelligence (xAI), to develop a novel multivariate time series deep neural network model that extracts informative brain dynamics features that accurately distinguish between ASD and TD children, and predict clinical symptom severity. Results Our model achieved consistently high classification accuracies in cross-validation analysis of data from the ABIDE cohort. Crucially, despite the differences in symptom profiles, age, and data acquisition protocols, our model also accurately classified data from an independent Stanford cohort without additional training. xAI analyses revealed that brain features associated with the default mode network, and the human voice/face processing and communication systems, most clearly distinguished ASD from TD children in both cohorts. Furthermore, the posterior cingulate cortex emerged as robust predictor of the severity of social and communication deficits in ASD in both cohorts. Conclusions Our findings, replicated across two independent cohorts, reveal robust and neurobiologically interpretable brain features that detect ASD and predict core phenotypic features of ASD, and have the potential to transform our understanding of the etiology and treatment of the disorder. Disclosure No significant relationships.","",""
22,"Hantao Huang, Jingye Zhou, Qingxun Di, Jiawei Zhou, Jiawang Li","Robust neural network–based tracking control and stabilization of a wheeled mobile robot with input saturation",2018,"","","","",59,"2022-07-13 10:10:07","","10.1002/rnc.4396","","",,,,,22,5.50,4,5,4,"This paper presents a robust neural network–based control scheme to deal with the problem of tracking and stabilization simultaneously for a wheeled mobile robot subject to parametric uncertainties, external disturbances, and input saturation. At first, a new error‐state transformation scheme is designed by introducing some auxiliary variables as an additional virtual control signals to reduce the adverse effect caused by the underactuation. These variables can change their structures for different desired trajectories to be tracked. Then, a robust control law is proposed combining with a kinematic controller and a dynamic controller, while a three‐layer neural network system is applied to approximate model uncertainties. Stability analysis via the Lyapunov theory shows that the proposed controller can make tracking errors converge to bounded neighborhoods of the origin. Finally, some simulation results are illustrated to verify the effectiveness of the proposed control strategy.","",""
0,"Yipeng Du, Jian Liu","IENet: a robust convolutional neural network for EEG based brain-computer interfaces",2022,"","","","",60,"2022-07-13 10:10:07","","10.1088/1741-2552/ac7257","","",,,,,0,0.00,0,2,1,"Objective. Brain-computer interfaces (BCIs) based on electroencephalogram (EEG) develop into novel application areas with more complex scenarios, which put forward higher requirements for the robustness of EEG signal processing algorithms. Deep learning can automatically extract discriminative features and potential dependencies via deep structures, demonstrating strong analytical capabilities in numerous domains such as computer vision and natural language processing. Making full use of deep learning technology to design a robust algorithm that is capable of analyzing EEG across BCI paradigms is our main work in this paper. Approach. Inspired by InceptionV4 and InceptionTime architecture, we introduce a neural network ensemble named InceptionEEG-Net (IENet), where multi-scale convolutional layer and convolution of length 1 enable model to extract rich high-dimensional features with limited parameters. In addition, we propose the average receptive field (RF) gain for convolutional neural networks (CNNs), which optimizes IENet to detect long patterns at a smaller cost. We compare with the current state-of-the-art methods across five EEG-BCI paradigms: steady-state visual evoked potentials (VEPs), epilepsy EEG, overt attention P300 VEPs, covert attention P300 visual-EPs and movement-related cortical potentials. Main results. The classification results show that the generalizability of IENet is on par with the state-of-the-art paradigm-agnostic models on test datasets. Furthermore, the feature explainability analysis of IENet illustrates its capability to extract neurophysiologically interpretable features for different BCI paradigms, ensuring the reliability of algorithm. Significance. It can be seen from our results that IENet can generalize to different BCI paradigms. And it is essential for deep CNNs to increase the RF size using average RF gain.","",""
0,"Ben Zhang, Zhetong Dong, Junsong Zhang, Hongwei Lin","Functional Network: A Novel Framework for Interpretability of Deep Neural Networks",2022,"","","","",61,"2022-07-13 10:10:07","","10.48550/arXiv.2205.11702","","",,,,,0,0.00,0,4,1,"The layered structure of deep neural networks hinders the use of numerous analysis tools and thus the development of its interpretability. Inspired by the success of functional brain networks, we propose a novel framework for interpretability of deep neural networks, that is, the functional network. We construct the functional network of fully connected networks and explore its small-worldness. In our experiments, the mechanisms of regularization methods, namely, batch normalization and dropout, are revealed using graph theoretical analysis and topological data analysis. Our empirical analysis shows the following: (1) Batch normalization enhances model performance by increasing the global eﬃciency and the number of loops but reduces adversarial robustness by lowering the fault tolerance. (2) Dropout improves generalization and robustness of models by improving the functional specialization and fault tolerance. (3) The models with diﬀerent regularizations can be clustered correctly according to their functional topological diﬀerences, reﬂecting the great potential of the functional network and topological data analysis in interpretability.","",""
1,"Can Udomcharoenchaikit, P. Boonkwan, P. Vateekul","Adversarial Evaluation of Robust Neural Sequential Tagging Methods for Thai Language",2020,"","","","",62,"2022-07-13 10:10:07","","10.1145/3383201","","",,,,,1,0.50,0,3,2,"Sequential tagging tasks, such as Part-Of-Speech (POS) tagging and Named-Entity Recognition, are the building blocks of many natural language processing applications. Although prior works have reported promising results in standard settings, they often underperform on non-standard text, such as microblogs and social media. In this article, we introduce an adversarial evaluation scheme for the Thai language by creating adversarial examples based on known spelling errors. Furthermore, we propose novel methods including UNK masking, condition initialization with affixation embeddings, and untied-directional self-attention mechanism to enhance robustness and interpretability of the neural networks. We conducted experiments on two Thai corpora: BEST2010 and ORCHID. Our adversarial evaluation schemes reveal that bidirectional LSTM (BiLSTM) do not perform well on adversarial examples. Our best methods match the performance of the BiLSTM baseline model and outperform it on adversarial examples.","",""
24,"Tianyu Kang, W. Ding, Luoyan Zhang, D. Ziemek, Kourosh Zarringhalam","A biological network-based regularized artificial neural network model for robust phenotype prediction from gene expression data",2017,"","","","",63,"2022-07-13 10:10:07","","10.1186/s12859-017-1984-2","","",,,,,24,4.80,5,5,5,"","",""
367,"V. Lawhern, Amelia J. Solon, Nicholas R. Waytowich, S. Gordon, C. Hung, Brent Lance","EEGNet: A Compact Convolutional Neural Network for EEG-based Brain-Computer Interfaces",2021,"","","","",64,"2022-07-13 10:10:07","","","","",,,,,367,367.00,61,6,1,"Objective: Brain computer interfaces (BCI) enable direct communication with a computer, using neural activity as the control signal. This neural signal is generally chosen from a variety of well-studied electroencephalogram (EEG) signals. For a given BCI paradigm, feature extractors and classifiers are tailored to the distinct characteristics of its expected EEG control signal, limiting its application to that specific signal. Convolutional Neural Networks (CNNs), which have been used in computer vision and speech recognition to perform automatic feature extraction and classification, have successfully been applied to EEG-based BCIs; however, they have mainly been applied to single BCI paradigms and thus it remains unclear how these architectures generalize to other paradigms. Here, we ask if we can design a single CNN architecture to accurately classify EEG signals from different BCI paradigms, while simultaneously being as compact as possible (defined as the number of parameters in the model). Approach: In this work we introduce EEGNet, a compact convolutional neural network for EEG-based BCIs. We introduce the use of depthwise and separable convolutions to construct an EEG-specific model which encapsulates well-known EEG feature extraction concepts for BCI. We compare EEGNet, both for within-subject and cross-subject classification, to current state-of-the-art approaches across four BCI paradigms: P300 visual-evoked potentials, error-related negativity responses (ERN), movement-related cortical potentials (MRCP), and sensory motor rhythms (SMR). Results: We show that EEGNet generalizes across paradigms better than, and achieves comparably high performance to, the reference algorithms when only limited training data is available. We also show that EEGNet effectively generalizes to both ERP and oscillatory-based BCIs. In addition, we demonstrate three different approaches to visualize the contents of a trained EEGNet model to enable interpretation of the learned features. Significance: Our results suggest that EEGNet is robust enough to learn a wide variety of interpretable features over a range of BCI tasks, suggesting that the observed performances were not due to artifact or noise sources in the data. Our models can be found at: https://github.com/vlawhern/arl-eegmodels. 1 ar X iv :1 61 1. 08 02 4v 4 [ cs .L G ] 1 6 M ay 2 01 8","",""
7,"Yuzhe Li, Shiyi Cheng, Yujia Xue, L. Tian","Displacement-agnostic coherent imaging through scatter with an interpretable deep neural network.",2020,"","","","",65,"2022-07-13 10:10:07","","10.1364/oe.411291","","",,,,,7,3.50,2,4,2,"Coherent imaging through scatter is a challenging task. Both model-based and data-driven approaches have been explored to solve the inverse scattering problem. In our previous work, we have shown that a deep learning approach can make high-quality and highly generalizable predictions through unseen diffusers. Here, we propose a new deep neural network model that is agnostic to a broader class of perturbations including scatterer change, displacements, and system defocus up to 10× depth of field. In addition, we develop a new analysis framework for interpreting the mechanism of our deep learning model and visualizing its generalizability based on an unsupervised dimension reduction technique. We show that our model can unmix the scattering-specific information and extract the object-specific information and achieve generalization under different scattering conditions. Our work paves the way to a robust and interpretable deep learning approach to imaging through scattering media.","",""
3,"Tom Dupré la Tour, Mi Lu, Michael Eickenberg, J. Gallant","A finer mapping of convolutional neural network layers to the visual cortex",2021,"","","","",66,"2022-07-13 10:10:07","","","","",,,,,3,3.00,1,4,1,"There is increasing interest in understanding similarities and differences between convolutional neural networks (CNNs) and the visual cortex. A common approach is to use some specific layer of a pre-trained CNN as a source of features to predict brain activity recorded during a visual task. Associating each brain region to the best predicting CNN layer reveals a gradual change over the visual cortex. However, this winner-take-all mapping is non-robust, because consecutive CNN layers are strongly correlated and have similar prediction accuracies. Moreover, this mapping is usually performed on static stimuli, which ignores the temporal component of human vision. When the mapping is performed on video stimuli, the features are extracted frame-by-frame and downsampled using an anti-aliasing low-pass filter, which removes high temporal frequencies that could be informative. To address the first issue and improve the non-robust winner-take-all approach, we propose to fit a joint model on all layers simultaneously. The model is fit with banded ridge regression, where a separate regularization hyperparameter is learned for each layer. By performing a selection over layers, this model effectively removes non-predictive or redundant layers and disentangles the contributions of each layer. We show that using a joint model increases prediction accuracy and leads to finer mappings from CNN layers to the visual cortex. To address the second issue and preserve more high frequency information, we propose to filter the features with a set of band-pass filters. We show that using the envelopes of the filtered signals as additional features further increases prediction accuracy. Introduction Convolutional neural networks (CNNs) were inspired originally by the anatomy of the brain, and they have been remarkably successful in computer vision [1, 2]. However, these networks still fail in many tasks that humans can perform easily. Therefore, there is increasing interest in understanding the similarities and differences between CNNs and the brain. To investigate this issue, a common method is to use some specific layer of a pre-trained CNN as a source of features to fit a brain encoding model [3]. With this approach, many studies have shown that early CNN layers best predict brain activity in low-level visual areas, while late layers best predict brain activity in intermediate and higher-level visual areas, with gradual changes of layer mapping over the cortical surface [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]. A similar approach has also been applied to speech [14, 15] and language tasks [16, 17]. One problem with this approach is that there are strong correlations between CNN activations from one layer to the next. This confound causes different layers to have similar predictive power in encoding models [8, 16, 17, 18]. It is thus hard to separate which part of the predictive power is specific to a layer and which part is shared with other layers. Most studies ignore this issue and select the best-predicting layer for each voxel [5, 6, 8, 9, 14, 19, 11], but this winner-take-all approach is not robust and it ignores potential complementarities between layers. Some studies use variance partitioning [20] or canonical component analysis [21] to disentangle the different layers, but these approaches cannot disentangle more than two or three layers. 3rd Workshop on Shared Visual Representations in Human and Machine Intelligence (SVRHM 2021) of the Neural Information Processing Systems (NeurIPS) conference, Virtual. To address this issue, we use banded ridge regression, which has been shown to disentangle contributions from correlated feature spaces in encoding models [22]. Specifically, we fit a predictive model using features from all layers at once, grouping the features by layers, and learning optimal regularization for each layer through cross-validation. We show empirically that this joint model performs a selection over layers, effectively removing non-predictive or redundant layers, and disentangling the contributions of each layer on each voxel. Using this joint model increases prediction accuracy, and leads to smoother cortical maps of layer mapping. A second problem of the conventional approach is that it oversimplifies the temporal aspect of visual processing. Indeed, most studies only use static image stimuli [5, 6, 7, 8, 11, 13], which entirely ignores the temporal component of human vision. Some studies use video stimuli [8, 9] and extract features frame by frame from an image-based CNN. Then, the features are downsampled to the brain imaging sampling frequency (typically 0.5 Hz), using an anti-aliasing low-pass filter [8, 9]. However, this low-pass filter is suboptimal, because it removes valuable high-frequency information contained in the CNN activations. Indeed, a video stimulus induces brain activity linked to movement, and this brain activity has been shown to be poorly predicted by low-pass filtered static features [23]. To address this issue, we first use video stimuli and extract features frame by frame from an imagebased CNN. Then, to preserve more high temporal frequency information, we filter the features with a set of band-pass filters, and extract the envelope of the filtered signals. The envelopes are then used as additional features which increase prediction accuracy of the model. Note that we specifically do not use a video-based CNN to be able to compare both approaches on the same CNN architecture. We expect further gain in prediction accuracy by using the set of band-pass filters on features extracted from a video-based CNN. These two methodological improvements pave the way for high-precision mappings between CNNs and human brains, which may help both designing and interpreting CNNs, and defining highresolution information pathways over the cortical surface. 1 Conventional approach The conventional approach for mapping CNN layers to brain regions [4, 5, 6, 7, 8, 9, 19, 11, 12, 13] follows the voxelwise encoding model framework [24, 25]. First, brain activity is recorded while subjects perceive a visual stimulus. Then, the same stimulus is presented to a pretrained CNN, activations are extracted from intermediate CNN layers and preprocessed into features (see below). Finally, a regression model is trained on each voxel to predict brain activity from the features. 1.1 Feature extraction To extract features, each image of the stimulus is first presented to a pretrained image-based CNN. Then, the activations of one convolutional or fully-connected layer are extracted (typically after ReLU and max-pooling layers). With a video stimulus, features are extracted frame by frame from an imagebased CNN, before being down-sampled to the brain imaging sampling frequency (typically 0.5 Hz). Next, a compressive nonlinearity x 7→ log(1 + x) is applied, and features are centered individually along the train set. Then, to account for the delay between the stimulus and the hemodynamic response, features are either convolved with a hemodynamic response function, or duplicated with multiple temporal delays. This process is repeated for each CNN layer. Limitations. With a video stimulus, the feature extraction process contains a down-sampling step to match the brain imaging sampling frequency (typically 0.5 Hz). This down-sampling is typically done with an anti-aliasing low-pass filter [8, 9]. However, this low-pass filter likely removes valuable information from the CNN activations. Indeed, a video stimulus induces brain activity linked to movement, and this activity has been shown to be poorly predicted by low-pass filtered features [23]. 1.2 Winner-take-all model In the conventional approach, a separate ridge regression [26] is fit to the features extracted from each layer of the CNN independently. Then, the best layer is selected for each voxel, based on cross-validated prediction accuracy. Finally, differences in terms of selected layers are analyzed across the brain. The approach thus produces a mapping from CNN layers to brain regions.","",""
0,"R. Ramos, Patr'icia Pereira, Helena Moniz, J. P. Carvalho, Bruno Martins","Retrieval Augmentation to Improve Robustness and Interpretability of Deep Neural Networks",2021,"","","","",67,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,5,1,"Deep neural network models have achieved state-ofthe-art results in various tasks related to vision and/or language. Despite the use of large training data, most models are trained by iterating over single input-output pairs, discarding the remaining examples for the current prediction. In this work, we actively exploit the training data to improve the robustness and interpretability of deep neural networks, using the information from nearest training examples to aid the prediction both during training and testing. Specifically, the proposed approach uses the target of the nearest input example to initialize the memory state of an LSTM model or to guide attention mechanisms. We apply this approach to image captioning and sentiment analysis, conducting experiments with both image and text retrieval. Results show the effectiveness of the proposed models for the two tasks, on the widely used Flickr8 and IMDB datasets, respectively. Our code is publicly available1.","",""
11,"Peter K. Koo, Sharon Qian, Gal Kaplun, Verena Volf, Dimitris Kalimeris","Robust Neural Networks are More Interpretable for Genomics",2019,"","","","",68,"2022-07-13 10:10:07","","10.1101/657437","","",,,,,11,3.67,2,5,3,"Deep neural networks (DNNs) have been applied to a variety of regulatory genomics tasks. For interpretability, attribution methods are employed to provide importance scores for each nucleotide in a given sequence. However, even with state-of-the-art DNNs, there is no guarantee that these methods can recover interpretable, biological representations. Here we perform systematic experiments on synthetic genomic data to raise awareness of this issue. We find that deeper networks have better generalization performance, but attribution methods recover less interpretable representations. Then, we show training methods promoting robustness – including regularization, injecting random noise into the data, and adversarial training – significantly improve interpretability of DNNs, especially for smaller datasets.","",""
0,"Patrick McClure, D. Moraczewski, K. Lam, Adam Thomas, Francisco Pereira","Improving the Interpretability of fMRI Decoding using Deep Neural Networks and Adversarial Robustness.",2020,"","","","",69,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,5,2,"Deep neural networks (DNNs) are being increasingly used to make predictions from functional magnetic resonance imaging (fMRI) data. However, they are widely seen as uninterpretable ""black boxes"", as it can be difficult to discover what input information is used by the DNN in the process, something important in both cognitive neuroscience and clinical applications. A saliency map is a common approach for producing interpretable visualizations of the relative importance of input features for a prediction. However, methods for creating maps often fail due to DNNs being sensitive to input noise, or by focusing too much on the input and too little on the model. It is also challenging to evaluate how well saliency maps correspond to the truly relevant input information, as ground truth is not always available. In this paper, we review a variety of methods for producing gradient-based saliency maps, and present a new adversarial training method we developed to make DNNs robust to input noise, with the goal of improving interpretability. We introduce two quantitative evaluation procedures for saliency map methods in fMRI, applicable whenever a DNN or linear model is being trained to decode some information from imaging data. We evaluate the procedures using a synthetic dataset where the complex activation structure is known, and on saliency maps produced for DNN and linear models for task decoding in the Human Connectome Project (HCP) dataset. Our key finding is that saliency maps produced with different methods vary widely in interpretability, in both in synthetic and HCP fMRI data. Strikingly, even when DNN and linear models decode at comparable levels of performance, DNN saliency maps score higher on interpretability than linear model saliency maps (derived via weights or gradient). Finally, saliency maps produced with our adversarial training method outperform those from other methods.","",""
8,"D. Valeriani, K. Simonyan","A microstructural neural network biomarker for dystonia diagnosis identified by a DystoniaNet deep learning platform",2020,"","","","",70,"2022-07-13 10:10:07","","10.1073/pnas.2009165117","","",,,,,8,4.00,4,2,2,"Significance This research identified a microstructural neural network biomarker for objective and accurate diagnosis of isolated dystonia based on the disorder pathophysiology using an advanced deep learning algorithm, DystoniaNet, and raw structural brain images of large cohorts of patients with isolated focal dystonia and healthy controls. DystoniaNet significantly outperformed shallow machine-learning pipelines and substantially exceeded the current agreement rates between clinicians, reaching an overall accuracy of 98.8% in diagnosing different forms of isolated focal dystonia. These results suggest that DystoniaNet could serve as an objective, robust, and generalizable algorithmic platform of dystonia diagnosis for enhanced clinical decision-making. Implementation of the identified biomarker for objective and accurate diagnosis of dystonia may be transformative for clinical management of this disorder. Isolated dystonia is a neurological disorder of heterogeneous pathophysiology, which causes involuntary muscle contractions leading to abnormal movements and postures. Its diagnosis is remarkably challenging due to the absence of a biomarker or gold standard diagnostic test. This leads to a low agreement between clinicians, with up to 50% of cases being misdiagnosed and diagnostic delays extending up to 10.1 y. We developed a deep learning algorithmic platform, DystoniaNet, to automatically identify and validate a microstructural neural network biomarker for dystonia diagnosis from raw structural brain MRIs of 612 subjects, including 392 patients with three different forms of isolated focal dystonia and 220 healthy controls. DystoniaNet identified clusters in corpus callosum, anterior and posterior thalamic radiations, inferior fronto-occipital fasciculus, and inferior temporal and superior orbital gyri as the biomarker components. These regions are known to contribute to abnormal interhemispheric information transfer, heteromodal sensorimotor processing, and executive control of motor commands in dystonia pathophysiology. The DystoniaNet-based biomarker showed an overall accuracy of 98.8% in diagnosing dystonia, with a referral of 3.5% of cases due to diagnostic uncertainty. The diagnostic decision by DystoniaNet was computed in 0.36 s per subject. DystoniaNet significantly outperformed shallow machine-learning algorithms in benchmark comparisons, showing nearly a 20% increase in its diagnostic performance. Importantly, the microstructural neural network biomarker and its DystoniaNet platform showed substantial improvement over the current 34% agreement on dystonia diagnosis between clinicians. The translational potential of this biomarker is in its highly accurate, interpretable, and generalizable performance for enhanced clinical decision-making.","",""
1,"Haonan Sun, Luqi Liang, Chunlin Wang, Yi Wu, Fei Yang, M. Rong","Prediction of the Electrical Strength and Boiling Temperature of the Substitutes for Greenhouse Gas SF₆ Using Neural Network and Random Forest",2020,"","","","",71,"2022-07-13 10:10:07","","10.1109/ACCESS.2020.3004519","","",,,,,1,0.50,0,6,2,"Finding substitutes for sulfur hexafluoride (SF6), a gas with extremely high global warming potential, has been a persistent effort for years in the field of high voltage power equipment, which focuses on the evaluation of the electrical strength and boiling temperature for the practical purpose. Following up the previous proposed linear regression models, this work introduces machine learning algorithms including artificial neural network (ANN) and random forest (RF) as the potential approaches to predict the electrical strength and boiling temperature. Based on a series of descriptors derived from the molecular structure of 74 molecules, the performance of three different methods: multiple linear regression, artificial neural network and random forest are compared and assessed in terms of the sensitivity to the sample size, prediction accuracy and stability, and the interpretability of predictors. Considering the available data are limited, random forest shows superior performance with higher robustness and efficiency. The same approaches were applied to the boiling temperature and random forest produced better results as well. Besides, the variable importance ranked by RF improves understanding of the correlation between the molecular properties and electrical strength. It provides important insights to analyze the properties of the SF6 substitutes during the design and synthesis of the new eco-friendly gases in power equipment.","",""
1,"Federico Amato, Fabian Guignard, P. Jacquet, M. Kanevski","On Feature Selection Using Anisotropic General Regression Neural Network",2020,"","","","",72,"2022-07-13 10:10:07","","","","",,,,,1,0.50,0,4,2,"The presence of irrelevant features in the input dataset tends to reduce the interpretability and predictive quality of machine learning models. Therefore, the development of feature selection methods to recognize irrelevant features is a crucial topic in machine learning. Here we show how the General Regression Neural Network used with an anisotropic Gaussian Kernel can be used to perform feature selection. A number of numerical experiments are conducted using simulated data to study the robustness of the proposed methodology and its sensitivity to sample size. Finally, a comparison with four other feature selection methods is performed on several real world datasets.","",""
2,"G. Baudat, John B. Hayes","A star-test wavefront sensor using neural network analysis",2020,"","","","",73,"2022-07-13 10:10:07","","10.1117/12.2568018","","",,,,,2,1.00,1,2,2,"We describe a new, simple wavefront sensing method that uses a single measurement of a defocused star and a neural network to determine low-order wavefront components. The neural net is trained on computed diffracted star image data at 640 nm to output annular Zernike terms for an obscured circular aperture over a discrete range of all values. In the context of an actual star, the neural-net also provides the Fried’s parameter as an estimation of atmospheric turbulence. It is shown that the neural-net can produce a robust, high accuracy solution of the wavefront based on a single measurement. The method can also be used to simultaneously determine both on-axis and fielddependent wavefront performance from a single measurement of stars throughout the field. The prototype system can run at a rate of about 1 Hz with Python interpreted code, but higher speeds, up to video rates, are possible with compilation, proper hardware and optimization. This technique is particularly useful for low-order active-optics control and for optical alignment. A key advantage of this new method is that it only requires a single camera making it a simple cost-effective solution that can take advantage of an existing camera that may already be in an optical system. Results for this method are compared to high-precision interferometric data taken with a 4D Technology, PhaseCam interferometer and with an Innovations Foresight StarWave Shack Hartmann sensor from ALCOR SYSTEM under well-controlled conditions to validate performance. We also look at how the system has been implemented to use starlight for aligning multiple mirror telescopes in the presence of atmospheric seeing.","",""
41,"Sreyas Mohan, Zahra Kadkhodaie, Eero P. Simoncelli, C. Fernandez-Granda","Robust and interpretable blind image denoising via bias-free convolutional neural networks",2019,"","","","",74,"2022-07-13 10:10:07","","","","",,,,,41,13.67,10,4,3,"Deep convolutional networks often append additive constant (""bias"") terms to their convolution operations, enabling a richer repertoire of functional mappings. Biases are also used to facilitate training, by subtracting mean response over batches of training images (a component of ""batch normalization""). Recent state-of-the-art blind denoising methods (e.g., DnCNN) seem to require these terms for their success. Here, however, we show that these networks systematically overfit the noise levels for which they are trained: when deployed at noise levels outside the training range, performance degrades dramatically. In contrast, a bias-free architecture -- obtained by removing the constant terms in every layer of the network, including those used for batch normalization-- generalizes robustly across noise levels, while preserving state-of-the-art performance within the training range. Locally, the bias-free network acts linearly on the noisy image, enabling direct analysis of network behavior via standard linear-algebraic tools. These analyses provide interpretations of network functionality in terms of nonlinear adaptive filtering, and projection onto a union of low-dimensional subspaces, connecting the learning-based method to more traditional denoising methodology.","",""
1,"Zhucheng Zhan, Noshad Hossenei, Olivier B. Poirion, M. Westerhoff, Eun-Young Choi, T. Ching, L. Garmire","Two-stage Neural-network based Prognosis Models using Pathological Image and Transcriptomic Data: An Application in Hepatocellular Carcinoma Patient Survival Prediction",2020,"","","","",75,"2022-07-13 10:10:07","","10.1101/2020.01.25.20016832","","",,,,,1,0.50,0,7,2,"Pathological images are easily accessible data type with potential as prognostic biomarkers. Here we extend Cox-nnet, a neural network based prognosis method previously used for transcriptomics data, to predict patient survival using hepatocellular carcinoma (HCC) pathological images. Cox-nnet based imaging predictions are more robust and accurate than Cox-PH. Moreover, using a novel two-stage Cox-nnet complex model, we are able to combine pathology image and transcriptomics RNA-Seq data to make impressively accurate prognosis predictions, with C-index close to 0.90 and log-ranked p-value of 4e-21 in the testing dataset. This work provides a new, biologically relevant and relatively interpretable solution to the challenge of integrating multi-modal and multiple types of data, particularly for survival prediction.","",""
1,"Ying L. Becker, Lingfeng Guo, Odilbek Nurmamatov","Assessing Asset Tail Risk with Artificial Intelligence: The Application of Artificial Neural Network",2020,"","","","",76,"2022-07-13 10:10:07","","10.1108/s2514-465020200000008002","","",,,,,1,0.50,0,3,2,"Value at risk (VaR) and expected shortfall (ES) are popular market risk measurements. The former is not coherent but robust, whereas the latter is coherent but less interpretable, only conditionally backtestable and less robust. In this chapter, we compare an innovative artificial neural network (ANN) model with a time series model in the context of forecasting VaR and ES of the univariate time series of four asset classes: US large capitalization equity index, European large cap equity index, US bond index, and US dollar versus euro exchange rate price index for the period of January 4, 1999, to December 31, 2018. In general, the ANN model has more favorable backtesting results as compared to the autoregressive moving average, generalized autoregressive conditional heteroscedasticity (ARMA-GARCH) time series model. In terms of forecasting accuracy, the ANN model has much fewer in-sample and out-of-sample exceptions than those of the ARMA-GARCH model.","",""
0,"Amit Sahu, Noelia V'allez, Rosana Rodr'iguez-Bobada, Mohamad Alhaddad, Omar Moured, G. Neugschwandtner","Application of the Neural Network Dependability Kit in Real-World Environments",2020,"","","","",77,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,6,2,"In this paper, we provide a guideline for using the Neural Network Dependability Kit (NNDK) during the development process of NN models, and show how the algorithm is applied in two image classification use cases. The case studies demonstrate the usage of the dependability kit to obtain insights about the NN model and how they informed the development process of the neural network model. After interpreting neural networks via the different metrics available in the NNDK, the developers were able to increase the NNs' accuracy, trust the developed networks, and make them more robust. In addition, we obtained a novel application-oriented technique to provide supporting evidence for an NN's classification result to the user. In the medical image classification use case, it was used to retrieve case images from the training dataset that were similar to the current patient's image and could therefore act as a support for the NN model's decision and aid doctors in interpreting the results.","",""
1,"Adam D. Cobb, Brian Jalaian, Nathaniel D. Bastian, Stephen Russell","Robust Decision-Making in the Internet of Battlefield Things Using Bayesian Neural Networks",2021,"","","","",78,"2022-07-13 10:10:07","","10.1109/WSC52266.2021.9715532","","",,,,,1,1.00,0,4,1,"The Internet of Battlefield Things (IoBT) is a dynamically composed network of intelligent sensors and actuators that operate as a command and control, communications, computers, and intelligence complex-system with the aim to enable multi-domain operations. The use of artificial intelligence can help transform the IoBT data into actionable insight to create information and decision advantage on the battlefield. In this work, we focus on how accounting for uncertainty in IoBT systems can result in more robust and safer systems. Human trust in these systems requires the ability to understand and interpret how machines make decisions. Most real-world applications currently use deterministic machine learning techniques that cannot incorporate uncertainty. In this work, we focus on the machine learning task of classifying vehicles from their audio recordings, comparing deterministic convolutional neural networks (CNNs) with Bayesian CNNs to show that correctly estimating the uncertainty can help lead to robust decision-making in IoBT.","",""
0,"G. Henter, S. Ronanki, O. Watts, M. Wester, Zhizheng Wu, Simon King","Robust text-to-speech duration modelling with a deep neural network",2016,"","","","",79,"2022-07-13 10:10:07","","10.1121/1.4969147","","",,,,,0,0.00,0,6,6,"Accurate modeling and prediction of speech-sound durations is important for generating more natural synthetic speech. Deep neural networks (DNNs) offer powerful models, and large, found corpora of natural speech are easily acquired for training them. Unfortunately, poor quality control (e.g., transcription errors) and phenomena such as reductions and filled pauses complicate duration modelling from found speech data. To mitigate issues caused by these idiosyncrasies, we propose to incorporate methods from robust statistics into speech synthesis. Robust methods can disregard ill-fitting training-data points—errors or other outliers—to describe the typical case better. For instance, parameter estimation can be made robust by replacing maximum likelihood with a robust estimation criterion based on the density power divergence (a.k.a. the β-divergence). Alternatively, a standard approximation for output generation with mixture density networks (MDNs) can be interpreted as a robust output generation heuristic....","",""
21,"Weiqi Ji, Sili Deng","Autonomous Discovery of Unknown Reaction Pathways from Data by Chemical Reaction Neural Network",2020,"","","","",80,"2022-07-13 10:10:07","","10.1021/acs.jpca.0c09316","","",,,,,21,10.50,11,2,2,"Chemical reactions occur in energy, environmental, biological, and many other natural systems, and the inference of the reaction networks is essential to understand and design the chemical processes in engineering and life sciences. Yet, revealing the reaction pathways for complex systems and processes is still challenging because of the lack of knowledge of the involved species and reactions. Here, we present a neural network approach that autonomously discovers reaction pathways from the time-resolved species concentration data. The proposed chemical reaction neural network (CRNN), by design, satisfies the fundamental physics laws, including the law of mass action and the Arrhenius law. Consequently, the CRNN is physically interpretable such that the reaction pathways can be interpreted, and the kinetic parameters can be quantified simultaneously from the weights of the neural network. The inference of the chemical pathways is accomplished by training the CRNN with species concentration data via stochastic gradient descent. We demonstrate the successful implementations and the robustness of the approach in elucidating the chemical reaction pathways of several chemical engineering and biochemical systems. The autonomous inference by the CRNN approach precludes the need for expert knowledge in proposing candidate networks and addresses the curse of dimensionality in complex systems. The physical interpretability also makes the CRNN capable of not only fitting the data for a given system but also developing knowledge of unknown pathways that could be generalized to similar chemical systems.","",""
64,"H. C. Liaw, B. Shirinzadeh, Julian Smith","Robust Neural Network Motion Tracking Control of Piezoelectric Actuation Systems for Micro/Nanomanipulation",2009,"","","","",81,"2022-07-13 10:10:07","","10.1109/TNN.2008.2004406","","",,,,,64,4.92,21,3,13,"This paper presents a robust neural network motion tracking control methodology for piezoelectric actuation systems employed in micro/nanomanipulation. This control methodology is proposed for tracking of desired motion trajectories in the presence of unknown system parameters, nonlinearities including the hysteresis effect and external disturbances in the control systems. In this paper, the related control issues are investigated, and a control methodology is established including the neural networks and a sliding control scheme. In particular, the radial basis function (RBF) neural networks are chosen for function approximations. The stability of the closed-loop system, as well as the convergence of the position and velocity tracking errors to zero, is assured by the control methodology in the presence of the aforementioned conditions. An offline learning procedure is also proposed for the improvement of the motion tracking performance. Precise tracking results of the proposed control methodology for a desired motion trajectory are demonstrated in the experimental study. With such a motion tracking capability, the proposed control methodology promises the realization of high-performance piezoelectric actuated micro/nanomanipulation systems.","",""
5,"Jan Rudy, Weiguang Ding, Daniel Jiwoong Im, Graham W. Taylor","Neural Network Regularization via Robust Weight Factorization",2014,"","","","",82,"2022-07-13 10:10:07","","","","",,,,,5,0.63,1,4,8,"Regularization is essential when training large neural networks. As deep neural networks can be mathematically interpreted as universal function approximators, they are effective at memorizing sampling noise in the training data. This results in poor generalization to unseen data. Therefore, it is no surprise that a new regularization technique, Dropout, was partially responsible for the now-ubiquitous winning entry to ImageNet 2012 by the University of Toronto. Currently, Dropout (and related methods such as DropConnect) are the most effective means of regularizing large neural networks. These amount to efficiently visiting a large number of related models at training time, while aggregating them to a single predictor at test time. The proposed FaMe model aims to apply a similar strategy, yet learns a factorization of each weight matrix such that the factors are robust to noise.","",""
4,"Taeheon Lee, Jeonghwan Hwang, Honggu Lee","TRIER: Template-Guided Neural Networks for Robust and Interpretable Sleep Stage Identification from EEG Recordings",2020,"","","","",83,"2022-07-13 10:10:07","","","","",,,,,4,2.00,1,3,2,"Neural networks often obtain sub-optimal representations during training, which degrade robustness as well as classification performances. This is a severe problem in applying deep learning to bio-medical domains, since models are vulnerable to being harmed by irregularities and scarcities in data. In this study, we propose a pre-training technique that handles this challenge in sleep staging tasks. Inspired by conventional methods that experienced physicians have used to classify sleep states from the existence of characteristic waveform shapes, or template patterns, our method introduces a cosine similarity based convolutional neural network to extract representative waveforms from training data. Afterwards, these features guide a model to construct representations based on template patterns. Through extensive experiments, we demonstrated that guiding a neural network with template patterns is an effective approach for sleep staging, since (1) classification performances are significantly enhanced and (2) robustness in several aspects are improved. Last but not least, interpretations on models showed that notable features exploited by trained experts are correctly addressed during prediction in the proposed method.","",""
22,"Adam Kortylewski, Qing Liu, Angtian Wang, Yihong Sun, A. Yuille","Compositional Convolutional Neural Networks: A Robust and Interpretable Model for Object Recognition under Occlusion",2020,"","","","",84,"2022-07-13 10:10:07","","10.1007/s11263-020-01401-3","","",,,,,22,11.00,4,5,2,"","",""
1,"Rahul Soni, Naresh Shah, Chua Tat Seng, J. D. Moore","Adversarial TCAV - Robust and Effective Interpretation of Intermediate Layers in Neural Networks",2020,"","","","",85,"2022-07-13 10:10:07","","","","",,,,,1,0.50,0,4,2,"Interpreting neural network decisions and the information learned in intermediate layers is still a challenge due to the opaque internal state and shared non-linear interactions. Although (Kim et al, 2017) proposed to interpret intermediate layers by quantifying its ability to distinguish a user-defined concept (from random examples), the questions of robustness (variation against the choice of random examples) and effectiveness (retrieval rate of concept images) remain. We investigate these two properties and propose improvements to make concept activations reliable for practical use.  Effectiveness: If the intermediate layer has effectively learned a user-defined concept, it should be able to recall --- at the testing step --- most of the images containing the proposed concept. For instance, we observed that the recall rate of Tiger shark and Great white shark from the ImageNet dataset with ""Fins"" as a user-defined concept was only 18.35% for VGG16. To increase the effectiveness of concept learning, we propose A-CAV --- the Adversarial Concept Activation Vector --- this results in larger margins between user concepts and (negative) random examples. This approach improves the aforesaid recall to 76.83% for VGG16.  For robustness, we define it as the ability of an intermediate layer to be consistent in its recall rate (the effectiveness) for different random seeds. We observed that TCAV has a large variance in recalling a concept across different random seeds. For example, the recall of cat images (from a layer learning the concept of tail) varies from 18% to 86% with 20.85% standard deviation on VGG16. We propose a simple and scalable modification that employs a Gram-Schmidt process to sample random noise from concepts and learn an average ""concept classifier"". This approach improves the aforesaid standard deviation from 20.85% to 6.4%.","",""
9,"Adam Noack, Isaac Ahern, D. Dou, Boyang Li","Does Interpretability of Neural Networks Imply Adversarial Robustness?",2019,"","","","",86,"2022-07-13 10:10:07","","","","",,,,,9,3.00,2,4,3,"The success of deep neural networks is clouded by two issues: (1) a vulnerability to adversarial examples and (2) a tendency to be uninterpretable. Interestingly, recent empirical evidence in the literature as well as theoretical analysis on simple models suggest these two seemingly disparate issues are actually connected. In particular, robust models tend to be more interpretable than non-robust models.  In this paper, we provide evidence for the claim that this relationship is bidirectional. Viz., models that are optimized to have interpretable gradients are more robust to adversarial examples than models trained in a standard manner. With further analysis and experiments on standard image classification datasets, we identify two factors behind this phenomenon---namely the suppression of the gradient's magnitude and the selective use of features guided by high-quality interpretations---which explain model behaviors under various regularization and target interpretation settings.","",""
6,"Fengchao Xiong, Jun Zhou, Shuyin Tao, Jianfeng Lu, Y. Qian","SNMF-Net: Learning a Deep Alternating Neural Network for Hyperspectral Unmixing",2021,"","","","",87,"2022-07-13 10:10:07","","10.1109/TGRS.2021.3081177","","",,,,,6,6.00,1,5,1,"Hyperspectral unmixing is recognized as an important tool to learn the constituent materials and corresponding distribution in a scene. The physical spectral mixture model is always important to tackle this problem because of its highly ill-posed nature. In this article, we introduce a linear spectral mixture model (LMM)-based end-to-end deep neural network named SNMF-Net for hyperspectral unmixing. SNMF-Net shares an alternating architecture and benefits from both model-based methods and learning-based methods. On the one hand, SNMF-Net is of high physical interpretability as it is built by unrolling <inline-formula> <tex-math notation=""LaTeX"">$L_{p}$ </tex-math></inline-formula> sparsity constrained nonnegative matrix factorization (<inline-formula> <tex-math notation=""LaTeX"">$L_{p}$ </tex-math></inline-formula>-NMF) model belonging to LMM families. On the other hand, all the parameters and submodules of SNMF-Net can be seamlessly linked with the alternating optimization algorithm of <inline-formula> <tex-math notation=""LaTeX"">$L_{p}$ </tex-math></inline-formula>-NMF and unmixing problem. This enables us to reasonably integrate the prior knowledge on unmixing, the optimization algorithm, and the sparse representation theory into the network for robust learning, so as to improve unmixing. Experimental results on the synthetic and real-world data show the advantages of the proposed SNMF-Net over many state-of-the-art methods.","",""
0,"Ben Qi, Liguo Zhang, Jin'gang Liang, J. Tong","Combinatorial Techniques for Fault Diagnosis in Nuclear Power Plants Based on Bayesian Neural Network and Simplified Bayesian Network-Artificial Neural Network",2022,"","","","",88,"2022-07-13 10:10:07","","10.3389/fenrg.2022.920194","","",,,,,0,0.00,0,4,1,"Knowledge-driven and data-driven methods are the two representative categories of intelligent technologies used in fault diagnosis in nuclear power plants. Knowledge-driven methods have advantages in interpretability and robustness, while data-driven methods have better performance in ease of modeling and inference efficiency. Given the complementarity of the two methods, a combination of them is a worthwhile investigation. In this work, we introduce two new techniques based on Bayesian theory (knowledge-driven) and artificial neural network (data-driven) for fault diagnosis in nuclear power plants. The first approach exploits an integrated technique, Bayesian Neural Network (BNN), which introduces Bayesian theory into the neural network to provide confidence in diagnosis. The second approach, denoted as Simplified Bayesian Network-Artificial Neural Network (SBN-ANN), adopts a hierarchical diagnosis idea, which firstly uses a simplified Bayesian network to diagnose fault types and then a neural network to diagnose the severity of faults. The two new techniques are implemented and verified with simulated faults data of a typical pressurized water reactor. Compared with single-algorithmic diagnostic approaches such as Bayesian network and neural network, the new combinatorial techniques show better performance in diagnostic precision. The results suggest the feasibility to develop the data and knowledge dual-drive technologies for fault diagnosis.","",""
31,"Q. Song, J. Spall, Y. Soh, Jie-ke Ni","Robust Neural Network Tracking Controller Using Simultaneous Perturbation Stochastic Approximation",2008,"","","","",89,"2022-07-13 10:10:07","","10.1109/TNN.2007.912315","","",,,,,31,2.21,8,4,14,"This paper considers the design of robust neural network tracking controllers for nonlinear systems. The neural network is used in the closed-loop system to estimate the nonlinear system function. We introduce the conic sector theory to establish a robust neural control system, with guaranteed boundedness for both the input/output (I/O) signals and the weights of the neural network. The neural network is trained by the simultaneous perturbation stochastic approximation (SPSA) method instead of the standard backpropagation (BP) algorithm. The proposed neural control system guarantees closed-loop stability of the estimation system, and a good tracking performance. The performance improvement of the proposed system over existing systems can be quantified in terms of preventing weight shifts, fast convergence, and robustness against system disturbance.","",""
0,"Bingxin Zhou, Yuanhong Jiang, Yu Guang Wang, Jingwei Liang, Junbin Gao, Shirui Pan, Xiaoqun Zhang","Graph Neural Network for Local Corruption Recovery",2022,"","","","",90,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,7,1,"Graph neural networks (GNNs) have seen a surge of development for exploiting the relational information of input graphs. Nevertheless, messages propagating through a graph contain both interpretable patterns and small perturbations. Despite global noise could be distributed over the entire graph data, it is not uncommon that corruptions appear well-concealed and merely pollute local regions while still having a vital influence on the GNN learning and prediction performance. This work tackles the graph recovery problem from local poisons by a robustness representation learning. Our developed strategy identifies regional graph perturbations and formulates a robust hidden feature representation for GNNs. A mask function pinpointed the anomalies without prior knowledge, and an `p,q regularizer defends local poisonings through pursuing sparsity in the framelet domain while maintaining a conditional closeness between the observation and new representation. The proposed robust computational unit alleviates the inertial alternating direction method of multipliers to achieve an efficient solution. Extensive experiments show that our new model recovers graph representations from local pollution and achieves excellent performance.","",""
28,"Zeon Trevor Fernando, Jaspreet Singh, Avishek Anand","A study on the Interpretability of Neural Retrieval Models using DeepSHAP",2019,"","","","",91,"2022-07-13 10:10:07","","10.1145/3331184.3331312","","",,,,,28,9.33,9,3,3,"A recent trend in IR has been the usage of neural networks to learn retrieval models for text based adhoc search. While various approaches and architectures have yielded significantly better performance than traditional retrieval models such as BM25, it is still difficult to understand exactly why a document is relevant to a query. In the ML community several approaches for explaining decisions made by deep neural networks have been proposed -- including DeepSHAP which modifies the DeepLift algorithm to estimate the relative importance (shapley values) of input features for a given decision by comparing the activations in the network for a given image against the activations caused by a reference input. In image classification, the reference input tends to be a plain black image. While DeepSHAP has been well studied for image classification tasks, it remains to be seen how we can adapt it to explain the output of Neural Retrieval Models (NRMs). In particular, what is a good ""black"" image in the context of IR? In this paper we explored various reference input document construction techniques. Additionally, we compared the explanations generated by DeepSHAP to LIME (a model agnostic approach) and found that the explanations differ considerably. Our study raises concerns regarding the robustness and accuracy of explanations produced for NRMs. With this paper we aim to shed light on interesting problems surrounding interpretability in NRMs and highlight areas of future work.","",""
13,"Igor Kvasić, N. Mišković, Z. Vukic","Convolutional Neural Network Architectures for Sonar-Based Diver Detection and Tracking",2019,"","","","",92,"2022-07-13 10:10:07","","10.1109/OCEANSE.2019.8867461","","",,,,,13,4.33,4,3,3,"Autonomous underwater navigation presents a whole set of challenges to be resolved in order to become adequately accurate and reliable. That is particularly critical when human divers work in close collaboration with autonomous underwater vehicles (AUVs). In absence of global positioning signals underwater, acoustic based sensors such as LBL (long-baseline), SBL (short-baseline) and USBL (ultrashort-baseline) are commonly used for navigation and localization. In addition to these low-bandwidth and high latency technologies, cameras and sonars can provide position measurements relative to the vehicle which can be used as an aid for navigation as well as for keeping a safe working distance between the diver and the AUV. While optical cameras are highly affected by water turbidity and lighting conditions, sonar images often become hard to interpret using conventional image processing methods due to image granulation and high noise levels.This paper focuses on finding a robust and reliable sonar image processing method for detection and tracking of human divers using convolutional neural networks. Machine learning algorithms are making a huge impact in computer vision applications but are not always considered when it comes to sonar image processing. After presenting commonly used image processing techniques the paper will focus on giving an overview of state-of-the-art machine learning algorithms and explore their performance in custom sonar image dataset processing. Finally, the performance of these algorithms will be compared on a set of sonar recordings to determine their reliability and applicability in a real-time operation.","",""
17,"Sercan Ö. Arik, Tomas Pfister","Attention-Based Prototypical Learning Towards Interpretable, Confident and Robust Deep Neural Networks",2019,"","","","",93,"2022-07-13 10:10:07","","","","",,,,,17,5.67,9,2,3,"We propose a new framework for prototypical learning that bases decision-making on few relevant examples that we call prototypes. Our framework utilizes an attention mechanism that relates the encoded representations to determine the prototypes. This results in a model that: (1) enables interpretability by outputting samples most relevant to the decision-making in addition to outputting the classification results; (2) allows confidence-controlled prediction by quantifying the mismatch across prototype labels; (3) permits detection of distribution mismatch; and (4) improves robustness to label noise. We demonstrate that our model is able to maintain comparable performance to baseline models while enabling all these benefits.","",""
33,"A. Szab'o, C. Castelnovo","Neural network wave functions and the sign problem",2020,"","","","",94,"2022-07-13 10:10:07","","10.1103/PHYSREVRESEARCH.2.033075","","",,,,,33,16.50,17,2,2,"Neural quantum states (NQS) are a promising approach to study many-body quantum physics. However, they face a major challenge when applied to lattice models: Convolutional networks struggle to converge to ground states with a nontrivial sign structure. We tackle this problem by proposing a neural network architecture with a simple, explicit, and interpretable phase ansatz, which can robustly represent such states and achieve state-of-the-art variational energies for both conventional and frustrated antiferromagnets. In the latter case, our approach uncovers low-energy states that exhibit the Marshall sign rule and are therefore inconsistent with the expected ground state. Such states are the likely cause of the obstruction for NQS-based variational Monte Carlo to access the true ground states of these systems. We discuss the implications of this observation and suggest potential strategies to overcome the problem.","",""
11,"Chaithanya Kumar Mummadi, Ranjitha Subramaniam, Robin Hutmacher, J. Vitay, Volker Fischer, J. H. Metzen","Does enhanced shape bias improve neural network robustness to common corruptions?",2021,"","","","",95,"2022-07-13 10:10:07","","","","",,,,,11,11.00,2,6,1,"Convolutional neural networks (CNNs) learn to extract representations of complex features, such as object shapes and textures to solve image recognition tasks. Recent work indicates that CNNs trained on ImageNet are biased towards features that encode textures and that these alone are sufficient to generalize to unseen test data from the same distribution as the training data but often fail to generalize to out-of-distribution data. It has been shown that augmenting the training data with different image styles decreases this texture bias in favor of increased shape bias while at the same time improving robustness to common corruptions, such as noise and blur. Commonly, this is interpreted as shape bias increasing corruption robustness. However, this relationship is only hypothesized. We perform a systematic study of different ways of composing inputs based on natural images, explicit edge information, and stylization. While stylization is essential for achieving high corruption robustness, we do not find a clear correlation between shape bias and robustness. We conclude that the data augmentation caused by style-variation accounts for the improved corruption robustness and increased shape bias is only a byproduct.","",""
5,"Chengjun Xu, G. Zhu, Jin Shu","A Lightweight and Robust Lie Group-Convolutional Neural Networks Joint Representation for Remote Sensing Scene Classification",2021,"","","","",96,"2022-07-13 10:10:07","","10.1109/TGRS.2020.3048024","","",,,,,5,5.00,2,3,1,"The existing convolutional neural network (CNN) models have shown excellent performance in remote sensing scene classification. However, the structure of such models is becoming more and more complex, and the learning of low-level features is difficult to interpret. To address this problem, in this study, we introduce lie group machine learning into the CNN model, try to combine both approaches to extract more distinguishing ability and effective features, and propose a novel network model, namely, the lie group regional influence network (LGRIN). First, manifold space samples of the lie group are obtained by mapping, and then, the features of the lie group are extracted after the operations of image decomposition and integral image calculation. Second, the multidilation pooling is integrated into the CNN architecture. At the same time, the image regional influence network module is designed to guide the attention of the classification model by using the regional-level supervision of the decomposition. Finally, the fusion features are classified, and the predicted results are obtained. Our model takes full advantage of regional influence, lie group kernel function, and lie group feature learning. Moreover, our model produces satisfactory performance on three public and challenging data sets: Aerial Image Dataset (AID), UC Merced, and NWPU-RESISC45. The experimental results verify that, compared with the state-of-the-art methods, this method is more explanatory and achieves higher accuracy.","",""
94,"C. Kwan, F. Lewis, D. Dawson","Robust neural-network control of rigid-link electrically driven robots",1998,"","","","",97,"2022-07-13 10:10:07","","10.1109/72.701172","","",,,,,94,3.92,31,3,24,"A robust neural-network (NN) controller is proposed for the motion control of rigid-link electrically driven (RLED) robots. Two-layer NN's are used to approximate two very complicated nonlinear functions. The main advantage of our approach is that the NN weights are tuned on-line, with no off-line learning phase required. Most importantly, we can guarantee the uniformly ultimately bounded (UUB) stability of tracking errors and NN weights. When compared with standard adaptive robot controllers, we do not require lengthy and tedious preliminary analysis to determine a regression matrix. The controller can be regarded as a universal reusable controller because the same controller can be applied to any type of RLED robots without any modifications.","",""
6,"Yashas B L Samaga, Shampa Raghunathan, U. D. Priyakumar","SCONES: Self-Consistent Neural Network for Protein Stability Prediction Upon Mutation.",2021,"","","","",98,"2022-07-13 10:10:07","","10.26434/CHEMRXIV.14729445.V1","","",,,,,6,6.00,2,3,1,"Engineering proteins to have desired properties by mutating amino acids at specific sites is commonplace. Such engineered proteins must be stable to function. Experimental methods used to determine stability at throughputs required to scan the protein sequence space thoroughly are laborious. To this end, many machine learning based methods have been developed to predict thermodynamic stability changes upon mutation. These methods have been evaluated for symmetric consistency by testing with hypothetical reverse mutations. In this work, we propose transitive data augmentation, evaluating transitive consistency with our new Stransitive data set, and a new machine learning based method, the first of its kind, that incorporates both symmetric and transitive properties into the architecture. Our method, called SCONES, is an interpretable neural network that predicts small relative protein stability changes for missense mutations that do not significantly alter the structure. It estimates a residue's contributions toward protein stability (ΔG) in its local structural environment, and the difference between independently predicted contributions of the reference and mutant residues is reported as ΔΔG. We show that this self-consistent machine learning architecture is immune to many common biases in data sets, relies less on data than existing methods, is robust to overfitting, and can explain a substantial portion of the variance in experimental data.","",""
1,"Shaojie Xu, J. Vaughan, Jie Chen, Aijun Zhang, A. Sudjianto","Traversing the Local Polytopes of ReLU Neural Networks: A Unified Approach for Network Verification",2021,"","","","",99,"2022-07-13 10:10:07","","","","",,,,,1,1.00,0,5,1,"Although neural networks (NNs) with ReLU activation functions have found success in a wide range of applications, their adoption in risk-sensitive settings has been limited by the concerns on robustness and interpretability. Previous works to examine robustness and to improve interpretability partially exploited the piecewise linear function form of ReLU NNs. In this paper, we explore the unique topological structure that ReLU NNs create in the input space, identifying the adjacency among the partitioned local polytopes and developing a traversing algorithm based on this adjacency. Our polytope traversing algorithm can be adapted to verify a wide range of network properties related to robustness and interpretability, providing an unified approach to examine the network behavior. As the traversing algorithm explicitly visits all local polytopes, it returns a clear and full picture of the network behavior within the traversed region. The time and space complexity of the traversing algorithm is determined by the number of a ReLU NN’s partitioning hyperplanes passing through the traversing region.","",""
0,"Mark R. P. Thomas, B. Martin, Katie A. Kowarski, B. Gaudet, S. Matwin","Interpreting the latent representations of a convolutional neural network trained on spectrograms",2019,"","","","",100,"2022-07-13 10:10:07","","10.1121/1.5137277","","",,,,,0,0.00,0,5,3,"Recent work [1,2] has shown that Convolutional Neural Networks (CNNs) trained on spectrograms of acoustic signals are capable of learning high-level latent representations for the purpose of detecting and classifying the vocalizations of endangered baleen whales. The aforementioned latent representations were used in the development of an automated system that was capable of detecting the vocalizations of blue, fin, and sei whales against non-biological and ambient noise sources to a high degree of accuracy (0.961, F-1 Score = 0.899). In this work, we conduct an exploratory analysis of the same latent representations using statistical machine learning approaches as well as by visualizing the convolutional feature maps learned by the CNN. Through this analysis we attempt to interpret what properties of a spectrogram are easily and/or most often exploited by the CNN during training in order to improve upon the state-of-the-art and develop more robust detection systems going forward. [1] M. Thomas, B. Martin, K. Kowarski, B. Gaudet, and S. Matwin, Marine Mammal Species Classification using Convolutional Neural Networks and a Novel Acoustic Representation, ECML PKDD 2019 (Springer, Cham, 2019). [2] M. Thomas, ""Towards a novel data representation for classifying acoustic signals,"" in Canadian Conference on Artificial Intelligence (Springer, Cham, 2019).","",""
180,"Noah D. Brenowitz, C. Bretherton","Prognostic Validation of a Neural Network Unified Physics Parameterization",2018,"","","","",101,"2022-07-13 10:10:07","","10.1029/2018GL078510","","",,,,,180,45.00,90,2,4,"Weather and climate models approximate diabatic and sub‐grid‐scale processes in terms of grid‐scale variables using parameterizations. Current parameterizations are designed by humans based on physical understanding, observations, and process modeling. As a result, they are numerically efficient and interpretable, but potentially oversimplified. However, the advent of global high‐resolution simulations and observations enables a more robust approach based on machine learning. In this letter, a neural network‐based parameterization is trained using a near‐global aqua‐planet simulation with a 4‐km resolution (NG‐Aqua). The neural network predicts the apparent sources of heat and moisture averaged onto (160 km)2 grid boxes. A numerically stable scheme is obtained by minimizing the prediction error over multiple time steps rather than single one. In prognostic single‐column model tests, this scheme matches both the fluctuations and equilibrium of NG‐Aqua simulation better than the Community Atmosphere Model does.","",""
4,"Ying Du, Yadong Liu, Xuhong Wang, Jian Fang, G. Sheng, Xiuchen Jiang","Predicting Weather-Related Failure Risk in Distribution Systems Using Bayesian Neural Network",2021,"","","","",102,"2022-07-13 10:10:07","","10.1109/TSG.2020.3019263","","",,,,,4,4.00,1,6,1,"The reliability of distribution systems is often challenged under unfavorable weather conditions, where weather-related failures occur with high probability. Predicting the number of weather-related failures in distribution systems can provide guiding information for operation and maintenance decisions, improving the risk management capability of utility companies. This article proposes a novel Bayesian Neural Network (BNN) based model to predict weather-related failures caused by wind, rain and lightning. Superior prediction performance of the BNN based model is verified by contrast experiments with other advanced prediction models under four different evaluation metrics. BNN based prediction model presents remarkable robustness, especially in the prediction of high failure levels. In addition, compared to most previous used prediction models without any prediction confidence feedback, BNN based prediction model has the capability of uncertainty estimation. The confidence interval of prediction results can be obtained, which provides sufficient information for guiding risk management of utility companies. An effective operation and maintenance guiding scheme based on the analysis of prediction uncertainty is proposed, which fully excavates the interpretability of the proposed model and enrich the application value of the model.","",""
3,"A. Piccardo, R. Cappuccio, G. Bottoni, D. Cecchin, L. Mazzella, A. Cirone, S. Righi, Martina Ugolini, P. Bianchi, P. Bertolaccini, E. Lorenzini, M. Massollo, A. Castaldi, F. Fiz, L. Strada, A. Cistaro, M. Del Sette","The role of the deep convolutional neural network as an aid to interpreting brain [18F]DOPA PET/CT in the diagnosis of Parkinson’s disease",2021,"","","","",103,"2022-07-13 10:10:07","","10.1007/s00330-021-07779-z","","",,,,,3,3.00,0,17,1,"","",""
1,"Yanghuan Xu, Dong-cheng Wang, Bo Duan, Hua-xin Yu, Hongmin Liu","Copper Strip Surface Defect Detection Model Based on Deep Convolutional Neural Network",2021,"","","","",104,"2022-07-13 10:10:07","","10.3390/app11198945","","",,,,,1,1.00,0,5,1,"Surface defect automatic detection has great significance for copper strip production. The traditional machine vision for surface defect automatic detection of copper strip needs artificial feature design, which has a long cycle, and poor ability of versatility and robustness. However, deep learning can effectively solve these problems. Therefore, based on the deep convolution neural network and the transfer learning strategy, an intelligent recognition model of surface defects of copper strip is established in this paper. Firstly, the defects were classified in accordance with the mechanism and morphology, and the surface defect dataset of copper strip was established by comprehensively adopting image acquisition and image augmentation. Then, a two-class discrimination model was established to achieve the accurate discrimination of perfect and defect images. On this basis, four CNN models were adopted for the recognition of defect images. Among these models, the EfficientNet model through transfer learning strategy had the best comprehensive performance with a recognition accuracy rate of 93.05%. Finally, the interpretability and deficiency of the model were analysed by the class activation map and confusion matrix, which point toward the direction of further optimization for future research.","",""
1,"Peng Lu, Yang Gao, Hao Xi, Yabin Zhang, Chao Gao, Bing Zhou, Hongpo Zhang, Liwei Chen, Xiaobo Mao","KecNet: A Light Neural Network for Arrhythmia Classification Based on Knowledge Reinforcement",2021,"","","","",105,"2022-07-13 10:10:07","","10.1155/2021/6684954","","",,,,,1,1.00,0,9,1,"Acquiring electrocardiographic (ECG) signals and performing arrhythmia classification in mobile device scenarios have the advantages of short response time, almost no network bandwidth consumption, and human resource savings. In recent years, deep neural networks have become a popular method to efficiently and accurately simulate nonlinear patterns of ECG data in a data-driven manner but require more resources. Therefore, it is crucial to design deep learning (DL) algorithms that are more suitable for resource-constrained mobile devices. In this paper, KecNet, a lightweight neural network construction scheme based on domain knowledge, is proposed to model ECG data by effectively leveraging signal analysis and medical knowledge. To evaluate the performance of KecNet, we use the Association for the Advancement of Medical Instrumentation (AAMI) protocol and the MIT-BIH arrhythmia database to classify five arrhythmia categories. The result shows that the ACC, SEN, and PRE achieve 99.31%, 99.45%, and 98.78%, respectively. In addition, it also possesses high robustness to noisy environments, low memory usage, and physical interpretability advantages. Benefiting from these advantages, KecNet can be applied in practice, especially wearable and lightweight mobile devices for arrhythmia classification.","",""
2,"Zhifang Liao, Haihui Pan, Xiaoping Fan, Yan Zhang, Li Kuang","Multiple Wavelet Convolutional Neural Network for Short-Term Load Forecasting",2020,"","","","",106,"2022-07-13 10:10:07","","10.1109/JIOT.2020.3026733","","",,,,,2,1.00,0,5,2,"Although the accuracy of load forecasting has been studied by many works, the actual deployability of a model is rarely considered. In this work, we consider the actual deployability of a model from four aspects: 1) the prediction performance of the model; 2) the robustness of the model; 3) the dependence of the model on external data; and 4) the storage size of the model. From these four aspects, we propose a multiple wavelet convolutional neural network (MWCNN) for load forecasting. On two public data sets, we verified the performance and robustness of the MWCNN. The MWCNN only uses load data, and the storage size of the model is only 497 kB, which shows that MWCNN has good deployability. In addition, our MWCNN prediction results are interpretable. The experimental results show that the MWCNN can effectively capture the periodic characteristics of load data.","",""
2,"S. Barland, Franccois Gustave","Convolutional neural network for self-mixing interferometric displacement sensing.",2021,"","","","",107,"2022-07-13 10:10:07","","10.1364/OE.419844","","",,,,,2,2.00,1,2,1,"Self-mixing interferometry is a well established interferometric measurement technique. In spite of the robustness and simplicity of the concept, interpreting the self-mixing signal is often complicated in practice, which is detrimental to measurement availability. Here we discuss the use of a convolutional neural network to reconstruct the displacement of a target from the self-mixing signal in a semiconductor laser. The network, once trained on periodic displacement patterns, can reconstruct arbitrarily complex displacement in different alignment conditions and setups. The approach validated here is amenable to generalization to modulated schemes or even to totally different self-mixing sensing tasks.","",""
0,"Ethan Harris, Daniela Mihai, Jonathon S. Hare","How Convolutional Neural Network Architecture Biases Learned Opponency and Color Tuning",2021,"","","","",108,"2022-07-13 10:10:07","","10.1162/neco_a_01356","","",,,,,0,0.00,0,3,1,"Recent work suggests that changing convolutional neural network (CNN) architecture by introducing a bottleneck in the second layer can yield changes in learned function. To understand this relationship fully requires a way of quantitatively comparing trained networks. The fields of electrophysiology and psychophysics have developed a wealth of methods for characterizing visual systems that permit such comparisons. Inspired by these methods, we propose an approach to obtaining spatial and color tuning curves for convolutional neurons that can be used to classify cells in terms of their spatial and color opponency. We perform these classifications for a range of CNNs with different depths and bottleneck widths. Our key finding is that networks with a bottleneck show a strong functional organization: almost all cells in the bottleneck layer become both spatially and color opponent, and cells in the layer following the bottleneck become nonopponent. The color tuning data can further be used to form a rich understanding of how color a network encodes color. As a concrete demonstration, we show that shallower networks without a bottleneck learn a complex nonlinear color system, whereas deeper networks with tight bottlenecks learn a simple channel opponent code in the bottleneck layer. We develop a method of obtaining a hue sensitivity curve for a trained CNN that enables high-level insights that complement the low-level findings from the color tuning data. We go on to train a series of networks under different conditions to ascertain the robustness of the discussed results. Ultimately our methods and findings coalesce with prior art, strengthening our ability to interpret trained CNNs and furthering our understanding of the connection between architecture and learned representation. Trained models and code for all experiments are available at https://github.com/ecs-vlc/opponency.","",""
2,"S. Tan, Runpei Dong, Kaisheng Ma","Multi-Glimpse Network: A Robust and Efficient Classification Architecture based on Recurrent Downsampled Attention",2021,"","","","",109,"2022-07-13 10:10:07","","","","",,,,,2,2.00,1,3,1,"Most feedforward convolutional neural networks spend roughly the same efforts for each pixel. Yet human visual recognition is an interaction between eye movements and spatial attention, which we will have several glimpses of an object in different regions. Inspired by this observation, we propose an end-to-end trainable M ulti- G limpse N etwork ( MGNet ) which aims to tackle the challenges of high computation and the lack of robustness based on recurrent downsampled attention mechanism. Speciﬁcally, MGNet sequentially selects task-relevant regions of an image to focus on and then adaptively combines all collected information for the ﬁnal prediction. MGNet expresses higher resistance against adversarial attacks and common corruptions with less computation. Also, MGNet is inherently more interpretable as it explicitly informs us where it focuses during each iteration. Our experiments on ImageNet100 demonstrate the potential of recurrent downsampled attention mechanisms to improve a single feedforward manner. For example, MGNet improves 4.76% accuracy on average in common corruptions with only 36.9% computational cost. Moreover, while the baseline incurs an accuracy drop to 7.6%, MGNet manages to maintain 44.2% accuracy in the same PGD attack strength with ResNet-50 backbone. Our code is available at https: //github.com/siahuat0727/MGNet .","",""
14,"T. Briegel, Volker Tresp","Robust Neural Network Regression for Offline and Online Learning",1999,"","","","",110,"2022-07-13 10:10:07","","","","",,,,,14,0.61,7,2,23,"We replace the commonly used Gaussian noise model in nonlinear regression by a more flexible noise model based on the Student-t- distribution. The degrees of freedom of the t-distribution can be chosen such that as special cases either the Gaussian distribution or the Cauchy distribution are realized. The latter is commonly used in robust regression. Since the t-distribution can be interpreted as being an infinite mixture of Gaussians, parameters and hyperparameters such as the degrees of freedom of the t-distribution can be learned from the data based on an EM-learning algorithm. We show that modeling using the t-distribution leads to improved predictors on real world data sets. In particular, if outliers are present, the t-distribution is superior to the Gaussian noise model. In effect, by adapting the degrees of freedom, the system can ""learn"" to distinguish between outliers and non-outliers. Especially for online learning tasks, one is interested in avoiding inappropriate weight changes due to measurement outliers to maintain stable online learning capability. We show experimentally that using the t-distribution as a noise model leads to stable online learning algorithms and outperforms state-of-the art online learning methods like the extended Kalman filter algorithm.","",""
44,"Minh Nguyen Nhat To, Q. Vu, B. Turkbey, P. Choyke, J. T. Kwak","Deep dense multi-path neural network for prostate segmentation in magnetic resonance imaging",2018,"","","","",111,"2022-07-13 10:10:07","","10.1007/s11548-018-1841-4","","",,,,,44,11.00,9,5,4,"","",""
32,"Jiaming Shen, Zhihong Shen, Chenyan Xiong, Chi Wang, Kuansan Wang, Jiawei Han","TaxoExpan: Self-supervised Taxonomy Expansion with Position-Enhanced Graph Neural Network",2020,"","","","",112,"2022-07-13 10:10:07","","10.1145/3366423.3380132","","",,,,,32,16.00,5,6,2,"Taxonomies consist of machine-interpretable semantics and provide valuable knowledge for many web applications. For example, online retailers (e.g., Amazon and eBay) use taxonomies for product recommendation, and web search engines (e.g., Google and Bing) leverage taxonomies to enhance query understanding. Enormous efforts have been made on constructing taxonomies either manually or semi-automatically. However, with the fast-growing volume of web content, existing taxonomies will become outdated and fail to capture emerging knowledge. Therefore, in many applications, dynamic expansions of an existing taxonomy are in great demand. In this paper, we study how to expand an existing taxonomy by adding a set of new concepts. We propose a novel self-supervised framework, named TaxoExpan, which automatically generates a set of ⟨query concept, anchor concept⟩ pairs from the existing taxonomy as training data. Using such self-supervision data, TaxoExpan learns a model to predict whether a query concept is the direct hyponym of an anchor concept. We develop two innovative techniques in TaxoExpan: (1) a position-enhanced graph neural network that encodes the local structure of an anchor concept in the existing taxonomy, and (2) a noise-robust training objective that enables the learned model to be insensitive to the label noise in the self-supervision data. Extensive experiments on three large-scale datasets from different domains demonstrate both the effectiveness and the efficiency of TaxoExpan for taxonomy expansion.","",""
356,"M. Mirman, Timon Gehr, Martin T. Vechev","Differentiable Abstract Interpretation for Provably Robust Neural Networks",2018,"","","","",113,"2022-07-13 10:10:07","","","","",,,,,356,89.00,119,3,4,"We introduce a scalable method for training robust neural networks based on abstract interpretation. We present several abstract transformers which balance efficiency with precision and show these can be used to train large neural networks that are certifiably robust to adversarial perturbations.","",""
6,"Kaveri A. Thakoor, Sharath C. Koorathota, D. Hood, P. Sajda","Robust and Interpretable Convolutional Neural Networks to Detect Glaucoma in Optical Coherence Tomography Images",2020,"","","","",114,"2022-07-13 10:10:07","","10.1109/tbme.2020.3043215","","",,,,,6,3.00,2,4,2,"Recent studies suggest that deep learning systems can now achieve performance on par with medical experts in diagnosis of disease. A prime example is in the field of ophthalmology, where convolutional neural networks (CNNs) have been used to detect retinal and ocular diseases. However, this type of artificial intelligence (AI) has yet to be adopted clinically due to questions regarding robustness of the algorithms to datasets collected at new clinical sites and a lack of explainability of AI-based predictions, especially relative to those of human expert counterparts. In this work, we develop CNN architectures that demonstrate robust detection of glaucoma in optical coherence tomography (OCT) images and test with concept activation vectors (TCAVs) to infer what image concepts CNNs use to generate predictions. Furthermore, we compare TCAV results to eye fixations of clinicians, to identify common decision-making features used by both AI and human experts. We find that employing fine-tuned transfer learning and CNN ensemble learning create end-to-end deep learning models with superior robustness compared to previously reported hybrid deep-learning/machine-learning models, and TCAV/eye-fixation comparison suggests the importance of three OCT report sub-images that are consistent with areas of interest fixated upon by OCT experts to detect glaucoma. The pipeline described here for evaluating CNN robustness and validating interpretable image concepts used by CNNs with eye movements of experts has the potential to help standardize the acceptance of new AI tools for use in the clinic.","",""
3,"Artur Petrosyan, M. Sinkin, M. Lebedev, A. Ossadtchi","Decoding and interpreting cortical signals with a compact convolutional neural network",2021,"","","","",115,"2022-07-13 10:10:07","","10.1088/1741-2552/abe20e","","",,,,,3,3.00,1,4,1,"Objective. Brain–computer interfaces (BCIs) decode information from neural activity and send it to external devices. The use of Deep Learning approaches for decoding allows for automatic feature engineering within the specific decoding task. Physiologically plausible interpretation of the network parameters ensures the robustness of the learned decision rules and opens the exciting opportunity for automatic knowledge discovery. Approach. We describe a compact convolutional network-based architecture for adaptive decoding of electrocorticographic (ECoG) data into finger kinematics. We also propose a novel theoretically justified approach to interpreting the spatial and temporal weights in the architectures that combine adaptation in both space and time. The obtained spatial and frequency patterns characterizing the neuronal populations pivotal to the specific decoding task can then be interpreted by fitting appropriate spatial and dynamical models. Main results. We first tested our solution using realistic Monte-Carlo simulations. Then, when applied to the ECoG data from Berlin BCI competition IV dataset, our architecture performed comparably to the competition winners without requiring explicit feature engineering. Using the proposed approach to the network weights interpretation we could unravel the spatial and the spectral patterns of the neuronal processes underlying the successful decoding of finger kinematics from an ECoG dataset. Finally we have also applied the entire pipeline to the analysis of a 32-channel EEG motor-imagery dataset and observed physiologically plausible patterns specific to the task. Significance. We described a compact and interpretable CNN architecture derived from the basic principles and encompassing the knowledge in the field of neural electrophysiology. For the first time in the context of such multibranch architectures with factorized spatial and temporal processing we presented theoretically justified weights interpretation rules. We verified our recipes using simulations and real data and demonstrated that the proposed solution offers a good decoder and a tool for investigating motor control neural mechanisms.","",""
1,"Michael Yeung, L. Rundo, Yang Nan, E. Sala, C. Schönlieb, Guang Yang","Calibrating the Dice loss to handle neural network overconfidence for biomedical image segmentation",2021,"","","","",116,"2022-07-13 10:10:07","","","","",,,,,1,1.00,0,6,1,"The Dice similarity coefficient (DSC) is both a widely used metric and loss function for biomedical image segmentation due to its robustness to class imbalance. However, it is well known that the DSC loss is poorly calibrated, resulting in overconfident predictions that cannot be usefully interpreted in biomedical and clinical practice. Performance is often the only metric used to evaluate segmentations produced by deep neural networks, and calibration is often neglected. However, calibration is important for translation into biomedical and clinical practice, providing crucial contextual information to model predictions for interpretation by scientists and clinicians. In this study, we identify poor calibration as an emerging challenge of deep learning based biomedical image segmentation. We provide a simple yet effective extension of the DSC loss, named the DSC++ loss, that selectively modulates the penalty associated with overconfident, incorrect predictions. As a standalone loss function, the DSC++ loss achieves significantly improved calibration over the conventional DSC loss across five well-validated open-source biomedical imaging datasets. Similarly, we observe significantly improved when integrating the DSC++ loss into four DSC-based loss functions. Finally, we use softmax thresholding to illustrate that well calibrated outputs enable tailoring of precision-recall bias, an important post-processing technique to adapt the model predictions to suit the biomedical or clinical task. The DSC++ loss overcomes the major limitation of the DSC, providing a suitable loss function for training deep learning segmentation models for use in biomedical and clinical practice.","",""
5,"Dimitrios Sakkos, Kevin D. McCay, Claire Marcroft, N. Embleton, Samiran Chattopadhyay, Edmond S. L. Ho","Identification of Abnormal Movements in Infants: A Deep Neural Network for Body Part-Based Prediction of Cerebral Palsy",2021,"","","","",117,"2022-07-13 10:10:07","","10.1109/ACCESS.2021.3093469","","",,,,,5,5.00,1,6,1,"The early diagnosis of cerebral palsy is an area which has recently seen significant multi-disciplinary research. Diagnostic tools such as the General Movements Assessment (GMA), have produced some very promising results, however these manual methods can be laborious. The prospect of automating these processes is seen as key in advancing this field of study. In our previous works, we examined the viability of using pose-based features extracted from RGB video sequences to undertake classification of infant body movements based upon the GMA. In this paper, we propose a new deep learning framework for this classification task. We also propose a visualization framework which identifies body-parts with the greatest contribution towards a classification decision. The inclusion of a visualization framework is an important step towards automation as it helps make the decisions made by the machine learning framework interpretable. We directly compare the proposed framework’s classification with several other methods from the literature using two independent datasets. Our experimental results show that the proposed method performs more consistently and more robustly than our previous pose-based techniques as well as other features from related works in this setting. We also find that our visualization framework helps provide greater interpretability, enhancing the likelihood of the adoption of these technologies within the medical domain.","",""
619,"Mou Chen, S. Ge, B. How","Robust Adaptive Neural Network Control for a Class of Uncertain MIMO Nonlinear Systems With Input Nonlinearities",2010,"","","","",118,"2022-07-13 10:10:07","","10.1109/TNN.2010.2042611","","",,,,,619,51.58,206,3,12,"In this paper, robust adaptive neural network (NN) control is investigated for a general class of uncertain multiple-input-multiple-output (MIMO) nonlinear systems with unknown control coefficient matrices and input nonlinearities. For nonsymmetric input nonlinearities of saturation and deadzone, variable structure control (VSC) in combination with backstepping and Lyapunov synthesis is proposed for adaptive NN control design with guaranteed stability. In the proposed adaptive NN control, the usual assumption on nonsingularity of NN approximation for unknown control coefficient matrices and boundary assumption between NN approximation error and control input have been eliminated. Command filters are presented to implement physical constraints on the virtual control laws, then the tedious analytic computations of time derivatives of virtual control laws are canceled. It is proved that the proposed robust backstepping control is able to guarantee semiglobal uniform ultimate boundedness of all signals in the closed-loop system. Finally, simulation results are presented to illustrate the effectiveness of the proposed adaptive NN control.","",""
2,"A. Petrosino, Giuseppe Salvi","A Robust Neural Network Based Object Recognition System and Its SIMD Implementation",1999,"","","","",119,"2022-07-13 10:10:07","","10.1007/3-540-48311-X_135","","",,,,,2,0.09,1,2,23,"","",""
21,"G. Portwood, B. Nadiga, J. Saenz, D. Livescu","Interpreting neural network models of residual scalar flux",2020,"","","","",120,"2022-07-13 10:10:07","","10.1017/jfm.2020.861","","",,,,,21,10.50,5,4,2,"Abstract We show that, in addition to providing effective and competitive closures, when analysed in terms of the dynamics and physically relevant diagnostics, artificial neural networks (ANNs) can be both interpretable and provide useful insights into the on-going task of developing and improving turbulence closures. In the context of large-eddy simulations (LES) of a passive scalar in homogeneous isotropic turbulence, exact subfilter fluxes obtained by filtering direct numerical simulations are used both to train deep ANN models as a function of filtered variables, and to optimise the coefficients of a turbulent Prandtl number LES closure. A priori analysis of the subfilter scalar variance transfer rate demonstrates that learnt ANN models outperform optimised turbulent Prandtl number closures and Clark-type gradient models. Next, a posteriori solutions are obtained with each model over several integral time scales. These experiments reveal, with single- and multi-point diagnostics, that ANN models temporally track exact resolved scalar variance with greater accuracy compared to other subfilter flux models for a given filter length scale. Finally, we interpret the artificial neural networks statistically with differential sensitivity analysis to show that the ANN models feature a dynamics reminiscent of so-called ‘mixed models’, where mixed models are understood as comprising both a structural and functional component. Besides enabling enhanced-accuracy LES of passive scalars henceforth, we anticipate this work to contribute to utilising neural network models as a tool in interpretability, robustness and model discovery.","",""
8,"S. Candemir, Richard D. White, M. Demirer, Vikash Gupta, M. Bigelow, L. Prevedello, B. Erdal","Automated coronary artery atherosclerosis detection and weakly supervised localization on coronary CT angiography with a deep 3-dimensional convolutional neural network",2019,"","","","",121,"2022-07-13 10:10:07","","","","",,,,,8,2.67,1,7,3,"We propose a fully automated algorithm based on a deep learning framework enabling screening of a Coronary Computed Tomography Angiography (CCTA) examination for confident detection of the presence or absence of coronary artery atherosclerosis. The system starts with extracting the coronary arteries and their branches from CCTA datasets and representing them with multi-planar reformatted volumes; pre-processing and augmentation techniques are then applied to increase the robustness and generalization ability of the system. A 3-Dimensional Convolutional Neural Network (3D CNN) is utilized to model pathological changes (e.g., atherosclerotic plaques) in coronary vessels. The system learns the discriminatory features between vessels with and without atherosclerosis. The discriminative features at the final convolutional layer are visualized with a saliency map approach to provide visual clues related to atherosclerosis likelihood and location. We have evaluated the system on a reference dataset representing 247 patients with atherosclerosis and 246 patients free of atherosclerosis. With 5-fold cross-validation, an Accuracy = 90:9%, Positive Predictive Value = 58:8%, Sensitivity = 68:9%, Specificity of 93:6%, and Negative Predictive Value (NPV) = 96:1% are achieved at the artery/branch level with threshold 0.5. The average area under the receiver operating characteristic curve is 0.91. The system indicates a high NPV, which may be potentially useful for assisting interpreting physicians in excluding coronary atherosclerosis in patients with acute chest pain.","",""
2,"Sara Aqab, Muhammad Usman","Handwriting Recognition using Artificial Intelligence Neural Network and Image Processing",2020,"","","","",122,"2022-07-13 10:10:07","","10.14569/ijacsa.2020.0110719","","",,,,,2,1.00,1,2,2,"Due to increased usage of digital technologies in all sectors and in almost all day to day activities to store and pass information, Handwriting character recognition has become a popular subject of research. Handwriting remains relevant, but people still want to have Handwriting copies converted into electronic copies that can be communicated and stored electronically. Handwriting character recognition refers to the computer's ability to detect and interpret intelligible Handwriting input from Handwriting sources such as touch screens, photographs, paper documents, and other sources. Handwriting characters remain complex since different individuals have different handwriting styles. This paper aims to report the development of a Handwriting character recognition system that will be used to read students and lectures Handwriting notes. The development is based on an artificial neural network, which is a field of study in artificial intelligence. Different techniques and methods are used to develop a Handwriting character recognition system. However, few of them focus on neural networks. The use of neural networks for recognizing Handwriting characters is more efficient and robust compared with other computing techniques. The paper also outlines the methodology, design, and architecture of the Handwriting character recognition system and testing and results of the system development. The aim is to demonstrate the effectiveness of neural networks for Handwriting character recognition.","",""
4,"Praveenram Balachandar, K. Michmizos","A Spiking Neural Network Emulating the Structure of the Oculomotor System Requires No Learning to Control a Biomimetic Robotic Head",2020,"","","","",123,"2022-07-13 10:10:07","","10.1109/BioRob49111.2020.9224303","","",,,,,4,2.00,2,2,2,"Robotic vision introduces requirements for real-time processing of fast-varying, noisy information in a continuously changing environment. In a real-world environment, convenient assumptions, such as static camera systems and deep learning algorithms devouring high volumes of ideally slightlyvarying data are hard to survive. Leveraging on recent studies on the neural connectome associated with eye movements, we designed a neuromorphic oculomotor controller and placed it at the heart of our in-house biomimetic robotic head prototype. The controller is unique in the sense that (1) all data are encoded and processed by a spiking neural network (SNN), and (2) by mimicking the associated brain areas’ topology, the SNN is biologically interpretable and requires no training to operate. Here, we report the robot’s target tracking ability, demonstrate that its eye kinematics are similar to those reported in human eye studies and show that a biologically-constrained learning, although not required for the SNN’s function, can be used to further refine its performance. This work aligns with our ongoing effort to develop energy-efficient neuromorphic SNNs and harness their emerging intelligence to control biomimetic robots with versatility and robustness.","",""
1,"Ethan Harris, Daniela Mihai, Jonathon S. Hare","How Convolutional Neural Network Architecture Biases Learned Opponency and Colour Tuning",2020,"","","","",124,"2022-07-13 10:10:07","","","","",,,,,1,0.50,0,3,2,"Recent work suggests that changing Convolutional Neural Network (CNN) architecture by introducing a bottleneck in the second layer can yield changes in learned function. To understand this relationship fully requires a way of quantitatively comparing trained networks. The fields of electrophysiology and psychophysics have developed a wealth of methods for characterising visual systems which permit such comparisons. Inspired by these methods, we propose an approach to obtaining spatial and colour tuning curves for convolutional neurons, which can be used to classify cells in terms of their spatial and colour opponency. We perform these classifications for a range of CNNs with different depths and bottleneck widths. Our key finding is that networks with a bottleneck show a strong functional organisation: almost all cells in the bottleneck layer become both spatially and colour opponent, cells in the layer following the bottleneck become non-opponent. The colour tuning data can further be used to form a rich understanding of how colour is encoded by a network. As a concrete demonstration, we show that shallower networks without a bottleneck learn a complex non-linear colour system, whereas deeper networks with tight bottlenecks learn a simple channel opponent code in the bottleneck layer. We further develop a method of obtaining a hue sensitivity curve for a trained CNN which enables high level insights that complement the low level findings from the colour tuning data. We go on to train a series of networks under different conditions to ascertain the robustness of the discussed results. Ultimately, our methods and findings coalesce with prior art, strengthening our ability to interpret trained CNNs and furthering our understanding of the connection between architecture and learned representation. Trained models and code for all experiments are available at https://github.com/ecs-vlc/opponency.","",""
41,"K. Islam, R. G. Raj","Real-Time (Vision-Based) Road Sign Recognition Using an Artificial Neural Network",2017,"","","","",125,"2022-07-13 10:10:07","","10.3390/s17040853","","",,,,,41,8.20,21,2,5,"Road sign recognition is a driver support function that can be used to notify and warn the driver by showing the restrictions that may be effective on the current stretch of road. Examples for such regulations are ‘traffic light ahead’ or ‘pedestrian crossing’ indications. The present investigation targets the recognition of Malaysian road and traffic signs in real-time. Real-time video is taken by a digital camera from a moving vehicle and real world road signs are then extracted using vision-only information. The system is based on two stages, one performs the detection and another one is for recognition. In the first stage, a hybrid color segmentation algorithm has been developed and tested. In the second stage, an introduced robust custom feature extraction method is used for the first time in a road sign recognition approach. Finally, a multilayer artificial neural network (ANN) has been created to recognize and interpret various road signs. It is robust because it has been tested on both standard and non-standard road signs with significant recognition accuracy. This proposed system achieved an average of 99.90% accuracy with 99.90% of sensitivity, 99.90% of specificity, 99.90% of f-measure, and 0.001 of false positive rate (FPR) with 0.3 s computational time. This low FPR can increase the system stability and dependability in real-time applications.","",""
2,"Shushan He, H. Zha, X. Ye","Network Diffusions via Neural Mean-Field Dynamics",2020,"","","","",126,"2022-07-13 10:10:07","","","","",,,,,2,1.00,1,3,2,"We propose a novel learning framework based on neural mean-field dynamics for inference and estimation problems of diffusion on networks. Our new framework is derived from the Mori-Zwanzig formalism to obtain an exact evolution of the node infection probabilities, which renders a delay differential equation with memory integral approximated by learnable time convolution operators, resulting in a highly structured and interpretable RNN. Directly using cascade data, our framework can jointly learn the structure of the diffusion network and the evolution of infection probabilities, which are cornerstone to important downstream applications such as influence maximization. Connections between parameter learning and optimal control are also established. Empirical study shows that our approach is versatile and robust to variations of the underlying diffusion network models, and significantly outperform existing approaches in accuracy and efficiency on both synthetic and real-world data.","",""
101,"Fatemeh Fahimi, Zhuo Zhang, Wooi-Boon Goh, Tih-Shih Lee, K. Ang, Cuntai Guan","Inter-subject transfer learning with an end-to-end deep convolutional neural network for EEG-based BCI.",2019,"","","","",127,"2022-07-13 10:10:07","","10.1088/1741-2552/aaf3f6","","",,,,,101,33.67,17,6,3,"OBJECTIVE Despite the effective application of deep learning (DL) in brain-computer interface (BCI) systems, the successful execution of this technique, especially for inter-subject classification, in cognitive BCI has not been accomplished yet. In this paper, we propose a framework based on the deep convolutional neural network (CNN) to detect the attentive mental state from single-channel raw electroencephalography (EEG) data.   APPROACH We develop an end-to-end deep CNN to decode the attentional information from an EEG time series. We also explore the consequences of input representations on the performance of deep CNN by feeding three different EEG representations into the network. To ensure the practical application of the proposed framework and avoid time-consuming re-training, we perform inter-subject transfer learning techniques as a classification strategy. Eventually, to interpret the learned attentional patterns, we visualize and analyse the network perception of the attention and non-attention classes.   MAIN RESULTS The average classification accuracy is 79.26%, with only 15.83% of 120 subjects having an accuracy below 70% (a generally accepted threshold for BCI). This is while with the inter-subject approach, it is literally difficult to output high classification accuracy. This end-to-end classification framework surpasses conventional classification methods for attention detection. The visualization results demonstrate that the learned patterns from the raw data are meaningful.   SIGNIFICANCE This framework significantly improves attention detection accuracy with inter-subject classification. Moreover, this study sheds light on the research on end-to-end learning; the proposed network is capable of learning from raw data with the least amount of pre-processing, which in turn eliminates the extensive computational load of time-consuming data preparation and feature extraction.","",""
2,"G. Portwood, B. Nadiga, J. Saenz, D. Livescu","Analysis and interpretation of out-performing neural network residual flux models",2020,"","","","",128,"2022-07-13 10:10:07","","","","",,,,,2,1.00,1,4,2,"We present novel approaches for the development, evaluation and interpretation of artificial neural networks (ANNs) for subfilter closures and demonstrate their usage in the context of large-eddy simulations (LES) of a passive scalar in homogeneous isotropic turbulence. Exact subfilter fluxes obtained by filtering direct numerical simulations (DNS) are used both to train deep ANN models as a function of filtered variables, and to optimise the coefficients of common spatio-temporally local LES closures. \textit{A-priori} analysis with respect to important dynamical features such as backscatter and subfilter scalar variance transfer rate, reveals that learnt ANN models out-performs optimised, turbulent Prandtl number closure models and gradient models. Next, \textit{a-posteriori} solutions are obtained with each model over several integral timescales. These solutions are obtained by explicitly filtering DNS-resolved velocity in order to isolate sources of error to subfilter flux closure. These experiments reveal that ANN models temporally track resolved scalar variance with greater accuracy compared to other subfilter flux models for a given filter length scale. Similarly, moments of scalar two-point structure functions reveal that trained neural network models reproduce statistics of ground-truth DNS with greater fidelity compared to common algebraic closure models. Finally, we interpret the artificial neural networks statistically with differential sensitivity analysis to show that the ANN models learns dynamics reminiscent of so-called ""mixed models"", where mixed models are understood as comprising both a structural and functional component. Besides enabling enhanced-accuracy LES of passive scalars henceforth, we anticipate this work to contribute to utilising well-performing neural network models as a tool in interpretability, robustness and model discovery.","",""
25,"K. Islam, R. G. Raj, G. Mujtaba","Recognition of Traffic Sign Based on Bag-of-Words and Artificial Neural Network",2017,"","","","",129,"2022-07-13 10:10:07","","10.3390/SYM9080138","","",,,,,25,5.00,8,3,5,"The traffic sign recognition system is a support system that can be useful to give notification and warning to drivers. It may be effective for traffic conditions on the current road traffic system. A robust artificial intelligence based traffic sign recognition system can support the driver and significantly reduce driving risk and injury. It performs by recognizing and interpreting various traffic sign using vision-based information. This study aims to recognize the well-maintained, un-maintained, standard, and non-standard traffic signs using the Bag-of-Words and the Artificial Neural Network techniques. This research work employs a Bag-of-Words model on the Speeded Up Robust Features descriptors of the road traffic signs. A robust classifier Artificial Neural Network has been employed to recognize the traffic sign in its respective class. The proposed system has been trained and tested to determine the suitable neural network architecture. The experimental results showed high accuracy of classification of traffic signs including complex background images. The proposed traffic sign detection and recognition system obtained 99.00% classification accuracy with a 1.00% false positive rate. For real-time implementation and deployment, this marginal false positive rate may increase reliability and stability of the proposed system.","",""
409,"Shuai Li, W. Li, Chris Cook, Ce Zhu, Yanbo Gao","Independently Recurrent Neural Network (IndRNN): Building A Longer and Deeper RNN",2018,"","","","",130,"2022-07-13 10:10:07","","10.1109/CVPR.2018.00572","","",,,,,409,102.25,82,5,4,"Recurrent neural networks (RNNs) have been widely used for processing sequential data. However, RNNs are commonly difficult to train due to the well-known gradient vanishing and exploding problems and hard to learn long-term patterns. Long short-term memory (LSTM) and gated recurrent unit (GRU) were developed to address these problems, but the use of hyperbolic tangent and the sigmoid action functions results in gradient decay over layers. Consequently, construction of an efficiently trainable deep network is challenging. In addition, all the neurons in an RNN layer are entangled together and their behaviour is hard to interpret. To address these problems, a new type of RNN, referred to as independently recurrent neural network (IndRNN), is proposed in this paper, where neurons in the same layer are independent of each other and they are connected across layers. We have shown that an IndRNN can be easily regulated to prevent the gradient exploding and vanishing problems while allowing the network to learn long-term dependencies. Moreover, an IndRNN can work with non-saturated activation functions such as relu (rectified linear unit) and be still trained robustly. Multiple IndRNNs can be stacked to construct a network that is deeper than the existing RNNs. Experimental results have shown that the proposed IndRNN is able to process very long sequences (over 5000 time steps), can be used to construct very deep networks (21 layers used in the experiment) and still be trained robustly. Better performances have been achieved on various tasks by using IndRNNs compared with the traditional RNN and LSTM.","",""
0,"Justin A. Goodwin, Olivia M. Brown, Victoria Helus","Fast Training of Deep Neural Networks Robust to Adversarial Perturbations",2020,"","","","",131,"2022-07-13 10:10:07","","10.1109/HPEC43674.2020.9286256","","",,,,,0,0.00,0,3,2,"Despite their promising performance, deep neural networks have shown sensitivities to perturbations of their inputs (e.g., adversarial examples) and their learned feature representations are often difficult to interpret, raising concerns about their true capability and trustworthiness. Recent work in adversarial training, a form of robust optimization in which the model is optimized against adversarial examples, demonstrates the ability to improve performance sensitivities to perturbations and yield feature representations that are more interpretable. Adversarial training, however, comes with an increased computational cost over that of standard (i.e., nonrobust) training, rendering it impractical for use in large-scale problems. Recent work suggests that a fast approximation to adversarial training shows promise for reducing training time and maintaining robustness in the presence of perturbations bounded by the infinity norm. In this work, we demonstrate that this approach extends to the Euclidean norm and preserves the human-aligned feature representations that are common for robust models. Additionally, we show that using a distributed training scheme can further reduce the time to train robust deep networks. Fast adversarial training is a promising approach that will provide increased security and explainability in machine learning applications for which robust optimization was previously thought to be impractical.","",""
1,"Haozhe Lin, Yushun Fan, Jia Zhang, Bing Bai","MSP-RNN: Multi-Step Piecewise Recurrent Neural Network for Predicting the Tendency of Services Invocation",2022,"","","","",132,"2022-07-13 10:10:07","","10.1109/tsc.2020.2966487","","",,,,,1,1.00,0,4,1,"Driven by the widespread application of Service-Oriented Architecture (SOA), an increasing number of services and mashups have been developed and published onto the Internet in the past decades. With the number keeping on burgeoning, predicting the tendency of services invocation will provide various roles in service ecosystems with promising opportunities. However, services invocation bear three unique characteristics, which give rise to difficulties in predicting them. First, enormous services show different and complicated traits, like periodicity, nonlinearity and nonstationarity. Second, services providing similar or compensatory functions make up intricate relationship. Third, the combination dependencies between mashups and their comprising component services further amplify the difficulty. Given these factors, we have developed a tailored model Multi-Step Piecewise Recurrent Neural Network (MSP-RNN) to predict the tendency of services invocation. In MSP-RNN, Long Short Term Memory (LSTM) units are used to extract universal features. Based on these features, we have developed a piecewise regressive mechanism to make prediction discriminatingly. Besides, we have developed a multi-step prediction strategy to further enhance prediction accuracy and robustness. Extensive experiments in real-world data set with interpretable analysis show that MSP-RNN predicts the tendency of services invocation more accurately, i.e., by 3.7 percent in terms of symmetric mean absolute percentage error (SMAPE), than state-of-the-art baseline methods.","",""
46,"Xiaoxiao Li, N. Dvornek, Yuan Zhou, Juntang Zhuang, P. Ventola, J. Duncan","Graph Neural Network for Interpreting Task-fMRI Biomarkers",2019,"","","","",133,"2022-07-13 10:10:07","","10.1007/978-3-030-32254-0_54","","",,,,,46,15.33,8,6,3,"","",""
393,"A. Ross, Finale Doshi-Velez","Improving the Adversarial Robustness and Interpretability of Deep Neural Networks by Regularizing their Input Gradients",2017,"","","","",134,"2022-07-13 10:10:07","","10.1609/aaai.v32i1.11504","","",,,,,393,78.60,197,2,5,"    Deep neural networks have proven remarkably effective at solving many classification problems, but have been criticized recently for two major weaknesses: the reasons behind their predictions are uninterpretable, and the predictions themselves can often be fooled by small adversarial perturbations. These problems pose major obstacles for the adoption of neural networks in domains that require security or transparency. In this work, we evaluate the effectiveness of defenses that differentiably penalize the degree to which small changes in inputs can alter model predictions. Across multiple attacks, architectures, defenses, and datasets, we find that neural networks trained with this input gradient regularization exhibit robustness to transferred adversarial examples generated to fool all of the other models. We also find that adversarial examples generated to fool gradient-regularized models fool all other models equally well, and actually lead to more ""legitimate,"" interpretable misclassifications as rated by people (which we confirm in a human subject experiment). Finally, we demonstrate that regularizing input gradients makes them more naturally interpretable as rationales for model predictions. We conclude by discussing this relationship between interpretability and robustness in deep neural networks.   ","",""
12,"Diego Manzanas Lopez, Patrick Musau, Hoang-Dung Tran, Taylor T. Johnson","Verification of Closed-loop Systems with Neural Network Controllers",2019,"","","","",135,"2022-07-13 10:10:07","","10.29007/btv1","","",,,,,12,4.00,3,4,3,"This benchmark suite presents a detailed description of a series of closed-loop control systems with artificial neural network controllers. In many applications, feed-forward neural networks are heavily involved in the implementation of controllers by learning and representing control laws through several methods such as model predictive control (MPC) and reinforcement learning (RL). The type of networks that we consider in this manuscript are feed-forward neural networks consisting of multiple hidden layers with ReLU activation functions and a linear activation function in the output layer. While neural network controllers have been able to achieve desirable performance in many contexts, they also present a unique challenge in that it is difficult to provide any guarantees about the correctness of their behavior or reason about the stability a system that employs their use. Thus, from a controls perspective, it is necessary to verify them in conjunction with their corresponding plants in closed-loop. While there have been a handful of works proposed towards the verification of closed-loop systems with feed-forward neural network controllers, this area still lacks attention and a unified set of benchmark examples on which verification techniques can be evaluated and compared. Thus, to this end, we present a range of closed-loop control systems ranging from two to six state variables, and a range of controllers with sizes in the range of eleven neurons to a few hundred neurons in more complex systems. Category: Academic Difficulty: High Acknowledgement The material presented in this paper is based upon work supported by the National Science Foundation (NSF) under grant number SHF 1736323, the Air Force Office of Scientific Research (AFOSR) through contract numbers FA9550-15-1-0258, FA9550-16-10246, and FA9550-18-1-0122, and the Defense Advanced Research Projects Agency (DARPA) through contract number FA8750-18-C-0089. The U.S. Government is authorized to reproduce and distribute reprints for Government purposes notwithstanding any copyright notation thereon. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either expressed or implied, of AFOSR, DARPA, or NSF G. Frehse and M. Althoff (eds.), ARCH19 (EPiC Series in Computing, vol. 61), pp. 201–210 Closed-loop Systems with Neural Network Controllers Manzanas Lopez, Musau, Tran and Johnson 1 Context and Origins. In recent years, advances in Artficial Intelligence (AI) have enabled a diverse range of technologies that are directly impacting people’s everyday lives [16]. Particularly, within this space, machine learning methods such as Deep Learning (DL) have achieved levels of accuracy and performance that are competitive or better than humans for tasks such as pattern and image recognition [12], natural language processing [7], and knowledge representation and reasoning [15,22]. Despite these achievements, there have been reservations about incorporating them into safety critical systems [11] due to their susceptibility to unexpected and errant behavior from a slight perturbation in their inputs [18]. Furthermore, neural networks are often viewed as ""black boxes"" since the underlying operation of the neuron activations is often indiscernible [22]. In light of these challenges, there has been significant work towards the creation of methods and verification tools that can formally reason about the behavior of neural networks [22]. However, the vast majority of these techniques have only been able to deal with feed-forward neural networks with piecewise-linear activation functions [4]. Additionally, the bulk of these methods have primarily considered the verification of input-output properties of neural networks in isolation [22], and there are only a handful of works that have explicitly addressed the verification of closed-loop control systems with neural network controllers [5, 8, 19–21]. One of the central challenges in verifying neural network control systems is that applying existing methodology to these systems is not straightforward [9], and a simple combination of verification tools for non-linear ordinary differential equations along with a neural network reachability tool suffers from severe overestimation errors [5]. Still, the verification of closed loop neural network systems is deeply important as they naturally arise in safety critical systems [5] such as autonomous vehicles, and complex control systems that make use of model predictive control and reinforcement learning [16]. Thus, there is a compelling need for methods and advanced software tools that can effectively deal with the complexities exhibited by these systems [5]. Inspired by a shortage of verification methods for closed-loop neural network control systems in the research literature, the central contribution of this paper is the provision of a set of executable benchmarks that have been synthesized using methods such as reinforcement learning [17], and model predictive control [14]. The problems elucidated in the paper are modeled using Simulink/Stateflow (SLSF) and are available at the following github repository1. We aim to provide a thorough problem description to which the numerous tools and approaches for non-linear systems and neural network verification present in the research community can be evaluated and compared [22]. If the research community is able to devise acceptable solutions to the aformentioned challenges they will stimulate the development of robust and intelligent systems with the potential to bring unparalleled benefits to numerous application domains. 2 Description of benchmarks. In this manuscript, we present a set of linear and non-linear closed-loop systems with continuoustime plants and feedforward neural networks controllers trained using different controls schemes such as reinforcement learning or model predictive control (MPC). A typical architecture describing the structure of these systems is displayed in Figure 2.1. All the neural networks 1https://github.com/verivital/ARCH-2019","",""
27,"Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang, Xiang Chen","Interpreting and Evaluating Neural Network Robustness",2019,"","","","",136,"2022-07-13 10:10:07","","10.24963/ijcai.2019/583","","",,,,,27,9.00,5,6,3,"Recently, adversarial deception becomes one of the most considerable threats to deep neural networks. However, compared to extensive research in new designs of various adversarial attacks and defenses, the neural networks' intrinsic robustness property is still lack of thorough investigation. This work aims to qualitatively interpret the adversarial attack and defense mechanisms through loss visualization, and establish a quantitative metric to evaluate the model's intrinsic robustness. The proposed robustness metric identifies the upper bound of a model's prediction divergence in the given domain and thus indicates whether the model can maintain a stable prediction. With extensive experiments, our metric demonstrates several advantages over conventional testing accuracy based robustness estimation: (1) it provides a uniformed evaluation to models with different structures and parameter scales; (2) it over-performs conventional accuracy based robustness evaluation and provides a more reliable evaluation that is invariant to different test settings; (3) it can be fast generated without considerable testing cost.","",""
10,"Sampo Kuutti, R. Bowden, Harita Joshi, Robert de Temple, Saber Fallah","Safe Deep Neural Network-Driven Autonomous Vehicles Using Software Safety Cages",2019,"","","","",137,"2022-07-13 10:10:07","","10.1007/978-3-030-33617-2_17","","",,,,,10,3.33,2,5,3,"","",""
7,"D. Barić, P. Fumić, D. Horvatic, T. Lipić","Benchmarking Attention-Based Interpretability of Deep Learning in Multivariate Time Series Predictions",2021,"","","","",138,"2022-07-13 10:10:07","","10.3390/e23020143","","",,,,,7,7.00,2,4,1,"The adaptation of deep learning models within safety-critical systems cannot rely only on good prediction performance but needs to provide interpretable and robust explanations for their decisions. When modeling complex sequences, attention mechanisms are regarded as the established approach to support deep neural networks with intrinsic interpretability. This paper focuses on the emerging trend of specifically designing diagnostic datasets for understanding the inner workings of attention mechanism based deep learning models for multivariate forecasting tasks. We design a novel benchmark of synthetically designed datasets with the transparent underlying generating process of multiple time series interactions with increasing complexity. The benchmark enables empirical evaluation of the performance of attention based deep neural networks in three different aspects: (i) prediction performance score, (ii) interpretability correctness, (iii) sensitivity analysis. Our analysis shows that although most models have satisfying and stable prediction performance results, they often fail to give correct interpretability. The only model with both a satisfying performance score and correct interpretability is IMV-LSTM, capturing both autocorrelations and crosscorrelations between multiple time series. Interestingly, while evaluating IMV-LSTM on simulated data from statistical and mechanistic models, the correctness of interpretability increases with more complex datasets.","",""
0,"S. Wein, A. Schüller, Ana Maria Tom'e, W. M. Malloni, M. Greenlee, E. Lang","Forecasting Brain Activity Based on Models of Spatio-Temporal Brain Dynamics: A Comparison of Graph Neural Network Architectures",2021,"","","","",139,"2022-07-13 10:10:07","","10.1162/netn_a_00252","","",,,,,0,0.00,0,6,1,"  Comprehending the interplay between spatial and temporal characteristics of neural dynamics can contribute to our understanding of information processing in the human brain. Graph neural networks (GNNs) provide a new possibility to interpret graph structured signals like those observed in complex brain networks. In our study we compare different spatio-temporal GNN architectures and study their ability to model neural activity distributions obtained in functional MRI (fMRI) studies. We evaluate the performance of the GNN models on a variety of scenarios in MRI studies and also compare it to a VAR model, which is currently often used for directed functional connectivity analysis. We show that by learning localized functional interactions on the anatomical substrate, GNN based approaches are able to robustly scale to large network studies, even when available data are scarce. By including anatomical connectivity as the physical substrate for information propagation, such GNNs also provide a multi-modal perspective on directed connectivity analysis, offering a novel possibility to investigate the spatio-temporal dynamics in brain networks.","",""
7,"Yongbing Zhang, Yangzhe Liu, Xiu Li, Shaowei Jiang, Krishna Dixit, Xinfeng Zhang, Xiangyang Ji","PgNN: Physics-guided Neural Network for Fourier Ptychographic Microscopy",2019,"","","","",140,"2022-07-13 10:10:07","","","","",,,,,7,2.33,1,7,3,"Fourier ptychography (FP) is a newly developed computational imaging approach that achieves both high resolution and wide field of view by stitching a series of low-resolution images captured under angle-varied illumination. So far, many supervised data-driven models have been applied to solve inverse imaging problems. These models need massive amounts of data to train, and are limited by the dataset characteristics. In FP problems, generic datasets are always scarce, and the optical aberration varies greatly under different acquisition conditions. To address these dilemmas, we model the forward physical imaging process as an interpretable physics-guided neural network (PgNN), where the reconstructed image in the complex domain is considered as the learnable parameters of the neural network. Since the optimal parameters of the PgNN can be derived by minimizing the difference between the model-generated images and real captured angle-varied images corresponding to the same scene, the proposed PgNN can get rid of the problem of massive training data as in traditional supervised methods. Applying the alternate updating mechanism and the total variation regularization, PgNN can flexibly reconstruct images with improved performance. In addition, the Zernike mode is incorporated to compensate for optical aberrations to enhance the robustness of FP reconstructions. As a demonstration, we show our method can reconstruct images with smooth performance and detailed information in both simulated and experimental datasets. In particular, when validated in an extension of a high-defocus, high-exposure tissue section dataset, PgNN outperforms traditional FP methods with fewer artifacts and distinguishable structures.","",""
2,"F. Hu, Jiaxin Jiang, P. Yin","Interpretable Prediction of Protein-Ligand Interaction by Convolutional Neural Network",2019,"","","","",141,"2022-07-13 10:10:07","","10.1109/BIBM47256.2019.8982989","","",,,,,2,0.67,1,3,3,"Evaluation of protein-ligand interaction is a crucial step in the process of drug discovery. Recently, several methods based on deep learning have gained impressive binary classification performance on protein-ligand binding prediction. However, lack of three-dimensional complex data still limits the accuracy and robustness of evaluation of protein-ligand binding affinity, as well as the prediction of their binding sites. In this paper, we propose a novel convolutional neural network based method for estimating the binding affinity between protein and ligand using only 1D sequence data. Even with the same amount of sample size, this model outperforms other structure-dependent traditional and machine learning based methods in terms of both binary classification and regression task. Furthermore, we use this model to identify the key amino acid residues of protein that are vital for binding interaction, which provides biological interpretation.","",""
2,"Mingxing Xu, Wenrui Dai, Yangmei Shen, H. Xiong","MSGCNN: Multi-scale Graph Convolutional Neural Network for Point Cloud Segmentation",2019,"","","","",142,"2022-07-13 10:10:07","","10.1109/BigMM.2019.00-35","","",,,,,2,0.67,1,4,3,"Point cloud has emerged as a scalable and flexible geometric representation for 3D data. Graph convolutional neural networks (GCNNs) have shown superior performance and robustness in point cloud processing with structure-awareness and permutation invariance. However, naive graph convolution networks are limited in point cloud segmentation tasks especially in the border areas of multiple segmentation instances due to the lack of multi-scale feature extraction ability. In this paper, we propose a novel multi-scale graph convolutional neural network (MSGCNN) to allow multi-scale feature learning for fine-grained point cloud segmentation. The proposed geometrical interpretable multi-scale point cloud processing framework is able to considerately enlarge the graph filters receptive fields and exploit discriminative multi-scale structure-aware point features for the superior segmentation performance against naive graph convolution networks especially in border area. Experimental results for part segmentation task on ShapeNet datasets show that MSGCNN achieves competitive performance with state-of-the-arts. In comparison to naive graph convolution networks, MSGCNN is shown to obtain better visual quality in the border area. We further validate that our model is robust to data point missing and noise perturbation with the learned multi-scale structure-aware point features.","",""
1,"Noor D. Al-shakarchy, I. H. Ali","Abnormal head movement classification using deep neural network DNN",2019,"","","","",143,"2022-07-13 10:10:07","","10.1063/1.5123123","","",,,,,1,0.33,1,2,3,"Abnormal head movements play a crucial role in diagnoisis of varity diseases. Moreover, different studies considered with these type of information. In addition, the gestures based mainly on head movement which can be employed in many applications such as using head-nodding or shaking to feedback content-related feedback, detect and interpret the emotion, gaze orientation, focus of attention, driver assistance system and so on. In this paper, a new method proposed to detect and classify the flopping head movements as normal or abnormal based on Convolution Neural Network CNN in the term of special sense interaction and behavioral studies with this movement. The proposed system based on deep learning employing the Convolution Neural Network CNN as most promising approach to deal with lighting condition (illumination change) and distortion and noise. Normal Abnormal Head Movement Dataset (NAHM) static images dataset gathered and used in proposed system implementation. This dataset provided the various image conditions and subjects to prevent the overfitting and under fitting problem that appears with publically datasets. The proposed frame work presents robust learning ability based on accuracy and lose functions which achieved training loss: 0.0106, training accuracy: 0.9980, validation loss: 0.0968 and validation accuracy: 0.9831.Abnormal head movements play a crucial role in diagnoisis of varity diseases. Moreover, different studies considered with these type of information. In addition, the gestures based mainly on head movement which can be employed in many applications such as using head-nodding or shaking to feedback content-related feedback, detect and interpret the emotion, gaze orientation, focus of attention, driver assistance system and so on. In this paper, a new method proposed to detect and classify the flopping head movements as normal or abnormal based on Convolution Neural Network CNN in the term of special sense interaction and behavioral studies with this movement. The proposed system based on deep learning employing the Convolution Neural Network CNN as most promising approach to deal with lighting condition (illumination change) and distortion and noise. Normal Abnormal Head Movement Dataset (NAHM) static images dataset gathered and used in proposed system implementation. This dataset provided the various image...","",""
3,"S. Bersimis, Aggeliki Sgora, S. Psarakis","A robust meta‐method for interpreting the out‐of‐control signal of multivariate control charts using artificial neural networks",2021,"","","","",144,"2022-07-13 10:10:07","","10.1002/qre.2955","","",,,,,3,3.00,1,3,1,"Multivariate control charts are an effective mean to identify an out‐of‐control process in several (industrial or non‐industrial) fields, where the quality depends on many related variables. However, the main shortcoming of these charts is that they fail to indicate which measured variable or variables has or have shifted. In order to address this issue, several alternative analytical approaches that aim to diagnose the responsible variable or variables for the out‐of‐control signal and help identify aberrant variables have been proposed. However, there is no particular method that can be considered as panacea, since its performance depends on several parameters, such as the correlation among the variables, and so forth. In this paper, a meta‐method is proposed that combines the results of several well‐known analytical methods in order to identify robustly the out‐of‐control variables. The obtained results show that the proposed meta‐method achieves high performance and it is extremely robust under different scenarios of out‐of‐control processes.","",""
14,"P. V. C. Souza, L. Torres, A. J. Guimarães, Vanessa Souza Araújo","Pulsar Detection for Wavelets SODA and Regularized Fuzzy Neural Networks Based on Andneuron and Robust Activation Function",2019,"","","","",145,"2022-07-13 10:10:07","","10.1142/S0218213019500039","","",,,,,14,4.67,4,4,3,"The use of intelligent models may be slow because of the number of samples involved in the problem. The identification of pulsars (stars that emit Earth-catchable signals) involves collecting thousands of signals by professionals of astronomy and their identification may be hampered by the nature of the problem, which requires many dimensions and samples to be analyzed. This paper proposes the use of hybrid models based on concepts of regularized fuzzy neural networks that use the representativeness of input data to define the groupings that make up the neurons of the initial layers of the model. The andneurons are used to aggregate the neurons of the first layer and can create fuzzy rules. The training uses fast extreme learning machine concepts to generate the weights of neurons that use robust activation functions to perform pattern classification. To solve large-scale problems involving the nature of pulsar detection problems, the model proposes a fast and highly accurate approach to address complex issues. In the execution of the tests with the proposed model, experiments were conducted explanation in two databases of pulsars, and the results prove the viability of the fast and interpretable approach in identifying such involved stars.","",""
1,"Cedrique Rovile Njieutcheu Tassi","Bayesian Convolutional Neural Network: Robustly Quantify Uncertainty for Misclassifications Detection",2019,"","","","",146,"2022-07-13 10:10:07","","10.1007/978-3-030-37548-5_10","","",,,,,1,0.33,1,1,3,"","",""
16,"Fan Zhang, Fábio Duarte, Ruixian Ma, Dimitrios Milioris, Hui-Ching Lin, C. Ratti","Indoor Space Recognition using Deep Convolutional Neural Network: A Case Study at MIT Campus",2016,"","","","",147,"2022-07-13 10:10:07","","","","",,,,,16,2.67,3,6,6,"In this paper, we propose a robust and parsimonious approach using Deep Convolutional Neural Network (DCNN) to recognize and interpret interior space. DCNN has achieved incredible success in object and scene recognition. In this study we design and train a DCNN to classify a pre-zoning indoor space, and from a single phone photo to recognize the learned space features, with no need of additional assistive technology. We collect more than 600,000 images inside MIT campus buildings to train our DCNN model, and achieved 97.9% accuracy in validation dataset and 81.7% accuracy in test dataset based on spatial-scale fixed model. Furthermore, the recognition accuracy and spatial resolution can be potentially improved through multiscale classification model. We identify the discriminative image regions through Class Activating Mapping (CAM) technique, to observe the model's behavior in how to recognize space and interpret it in an abstract way. By evaluating the results with misclassification matrix, we investigate the visual spatial feature of interior space by looking into its visual similarity and visual distinctiveness, giving insights into interior design and human indoor perception and wayfinding research. The contribution of this paper is threefold. First, we propose a robust and parsimonious approach for indoor navigation using DCNN. Second, we demonstrate that DCNN also has a potential capability in space feature learning and recognition, even under severe appearance changes. Third, we introduce a DCNN based approach to look into the visual similarity and visual distinctiveness of interior space.","",""
1,"F. Santos, C. Zanchettin, L. Matos, P. Novais","On the Impact of Interpretability Methods in Active Image Augmentation Method",2021,"","","","",148,"2022-07-13 10:10:07","","10.1093/jigpal/jzab006","","",,,,,1,1.00,0,4,1,"Robustness is a significant constraint in machine learning models. The performance of the algorithms must not deteriorate when training and testing with slightly different data. Deep neural network models achieve awe-inspiring results in a wide range of applications of computer vision. Still, in the presence of noise or region occlusion, some models exhibit inaccurate performance even with data handled in training. Besides, some experiments suggest deep learning models sometimes use incorrect parts of the input information to perform inference. Activate Image Augmentation (ADA) is an augmentation method that uses interpretability methods to augment the training data and improve its robustness to face the described problems. Although ADA presented interesting results, its original version only used the Vanilla Backpropagation interpretability to train the U-Net model. In this work, we propose an extensive experimental analysis of the interpretability method’s impact on ADA. We use five interpretability methods: Vanilla Backpropagation, Guided Backpropagation, GradCam, Guided GradCam, and InputXGradient. The results show that all methods achieve similar performance at the ending of training, but when combining ADA with GradCam, the U-Net model presented an impressive fast convergence.","",""
0,"R. Sathish, Debdoot Sheet","Unit Impulse Response as an Explainer of Redundancy in a Deep Convolutional Neural Network",2019,"","","","",149,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,2,3,"Convolutional neural networks (CNN) are generally designed with a heuristic initialization of network architecture and trained for a certain task. This often leads to overparametrization after learning and induces redundancy in the information flow paths within the network. This robustness and reliability is at the increased cost of redundant computations. Several methods have been proposed which leverage metrics that quantify the redundancy in each layer. However, layer-wise evaluation in these methods disregards the long-range redundancy which exists across depth on account of the distributed nature of the features learned by the model. In this paper, we propose (i) a mechanism to empirically demonstrate the robustness in performance of a CNN on account of redundancy across its depth, (ii) a method to identify the systemic redundancy in response of a CNN across depth using the understanding of unit impulse response, we subsequently demonstrate use of these methods to interpret redundancy in few networks as example. These techniques provide better insights into the internal dynamics of a CNN","",""
0,"Pau","Neural network signal understanding for instrumentation",2019,"","","","",150,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,1,3,"This paper reports on the use of neural signal interpretation theory and techniques for the purpose of classifying the shapes of a set of instrumentation signals, in order to calibrate devices, diagnose anomalies, generate tuning/settings, and interpret the measurement results. Neural signal understanding research is surveyed, and the selected implementation is described with its performance in terms of correct classification rates and robustness to noise. Formal results on neural net training time and sensitivity to weights are given. A theory for neural control is given using functional link nets and an explanation technique is designed to help neural signal understanding. The results of this are compared to those of a knowledge-based signal interpretation system within the context of the same specific instrument and data. Keywords-Neural understanding, calibration, signal understanding, control theory, neural control, training time, sensitivity to noise, explanation facilities, knowledge-based signal interpretation, instrumentation, analytical instrumentation.","",""
22,"Matthew L. Leavitt, Ari S. Morcos","Towards falsifiable interpretability research",2020,"","","","",151,"2022-07-13 10:10:07","","","","",,,,,22,11.00,11,2,2,"Methods for understanding the decisions of and mechanisms underlying deep neural networks (DNNs) typically rely on building intuition by emphasizing sensory or semantic features of individual examples. For instance, methods aim to visualize the components of an input which are ""important"" to a network's decision, or to measure the semantic properties of single neurons. Here, we argue that interpretability research suffers from an over-reliance on intuition-based approaches that risk-and in some cases have caused-illusory progress and misleading conclusions. We identify a set of limitations that we argue impede meaningful progress in interpretability research, and examine two popular classes of interpretability methods-saliency and single-neuron-based approaches-that serve as case studies for how overreliance on intuition and lack of falsifiability can undermine interpretability research. To address these concerns, we propose a strategy to address these impediments in the form of a framework for strongly falsifiable interpretability research. We encourage researchers to use their intuitions as a starting point to develop and test clear, falsifiable hypotheses, and hope that our framework yields robust, evidence-based interpretability methods that generate meaningful advances in our understanding of DNNs.","",""
692,"W. Samek, Alexander Binder, G. Montavon, S. Lapuschkin, K. Müller","Evaluating the Visualization of What a Deep Neural Network Has Learned",2015,"","","","",152,"2022-07-13 10:10:07","","10.1109/TNNLS.2016.2599820","","",,,,,692,98.86,138,5,7,"Deep neural networks (DNNs) have demonstrated impressive performance in complex machine learning tasks such as image classification or speech recognition. However, due to their multilayer nonlinear structure, they are not transparent, i.e., it is hard to grasp what makes them arrive at a particular classification or recognition decision, given a new unseen data sample. Recently, several approaches have been proposed enabling one to understand and interpret the reasoning embodied in a DNN for a single test image. These methods quantify the “importance” of individual pixels with respect to the classification decision and allow a visualization in terms of a heatmap in pixel/input space. While the usefulness of heatmaps can be judged subjectively by a human, an objective quality measure is missing. In this paper, we present a general methodology based on region perturbation for evaluating ordered collections of pixels such as heatmaps. We compare heatmaps computed by three different methods on the SUN397, ILSVRC2012, and MIT Places data sets. Our main result is that the recently proposed layer-wise relevance propagation algorithm qualitatively and quantitatively provides a better explanation of what made a DNN arrive at a particular classification decision than the sensitivity-based approach or the deconvolution method. We provide theoretical arguments to explain this result and discuss its practical implications. Finally, we investigate the use of heatmaps for unsupervised assessment of the neural network performance.","",""
38,"G. Lai, Zhi Liu, Yun Zhang, C. L. Chen","Adaptive Position/Attitude Tracking Control of Aerial Robot With Unknown Inertial Matrix Based on a New Robust Neural Identifier",2016,"","","","",153,"2022-07-13 10:10:07","","10.1109/TNNLS.2015.2406812","","",,,,,38,6.33,10,4,6,"This paper presents a novel adaptive controller for controlling an autonomous helicopter with unknown inertial matrix to asymptotically track the desired trajectory. To identify the unknown inertial matrix included in the attitude dynamic model, this paper proposes a new structural identifier that differs from those previously proposed in that it additionally contains a neural networks (NNs) mechanism and a robust adaptive mechanism, respectively. Using the NNs to compensate the unknown aerodynamic forces online and the robust adaptive mechanism to cancel the combination of the overlarge NNs compensation error and the external disturbances, the new robust neural identifier exhibits a better identification performance in the complex flight environment. Moreover, an optimized algorithm is included in the NNs mechanism to alleviate the burdensome online computation. By the strict Lyapunov argument, the asymptotic convergence of the inertial matrix identification error, position tracking error, and attitude tracking error to arbitrarily small neighborhood of the origin is proved. The simulation and implementation results are provided to evaluate the performance of the proposed controller.","",""
3,"Juhong Min, Seungwook Kim, Minsu Cho","Convolutional Hough Matching Networks for Robust and Efficient Visual Correspondence",2021,"","","","",154,"2022-07-13 10:10:07","","","","",,,,,3,3.00,1,3,1,"Despite advances in feature representation, leveraging geometric relations is crucial for establishing reliable visual correspondences under large variations of images. In this work we introduce a Hough transform perspective on convolutional matching and propose an effective geometric matching algorithm, dubbed Convolutional Hough Matching (CHM). The method distributes similarities of candidate matches over a geometric transformation space and evaluates them in a convolutional manner. We cast it into a trainable neural layer with a semi-isotropic high-dimensional kernel, which learns non-rigid matching with a small number of interpretable parameters. To further improve the efficiency of high-dimensional voting, we also propose to use an efficient kernel decomposition with center-pivot neighbors, which significantly sparsifies the proposed semi-isotropic kernels without performance degradation. To validate the proposed techniques, we develop the neural network with CHM layers that perform convolutional matching in the space of translation and scaling. Our method sets a new state of the art on standard benchmarks for semantic visual correspondence, proving its strong robustness to challenging intra-class variations.","",""
14,"Xuhong Li, Haoyi Xiong, Xingjian Li, Xuanyu Wu, Xiao Zhang, Ji Liu, Jiang Bian, D. Dou","Interpretable Deep Learning: Interpretations, Interpretability, Trustworthiness, and Beyond",2021,"","","","",155,"2022-07-13 10:10:07","","","","",,,,,14,14.00,2,8,1,"Deep neural networks have been well-known for their superb performance in handling various machine learning and artificial intelligence tasks. However, due to their over-parameterized black-box nature, it is often difficult to understand the prediction results of deep models. In recent years, many interpretation tools have been proposed to explain or reveal the ways that deep models make decisions. In this paper, we review this line of research and try to make a comprehensive survey. Specifically, we introduce and clarify two basic concepts— interpretations and interpretability—that people usually get confused. First of all, to address the research efforts in interpretations, we elaborate the design of several recent interpretation algorithms, from different perspectives, through proposing a new taxonomy. Then, to understand the results of interpretation, we also survey the performance metrics for evaluating interpretation algorithms. Further, we summarize the existing work in evaluating models’ interpretability using “trustworthy” interpretation algorithms. Finally, we review and discuss the connections between deep models’ interpretations and other factors, such as adversarial robustness and data augmentations, and we introduce several open-source libraries for interpretation algorithms and evaluation approaches.","",""
158,"A. Vemuri, M. Polycarpou","Neural-network-based robust fault diagnosis in robotic systems",1997,"","","","",156,"2022-07-13 10:10:07","","10.1109/72.641464","","",,,,,158,6.32,79,2,25,"Fault diagnosis plays an important role in the operation of modern robotic systems. A number of researchers have proposed fault diagnosis architectures for robotic manipulators using the model-based analytical redundancy approach. One of the key issues in the design of such fault diagnosis schemes is the effect of modeling uncertainties on their performance. This paper investigates the problem of fault diagnosis in rigid-link robotic manipulators with modeling uncertainties. A learning architecture with sigmoidal neural networks is used to monitor the robotic system for any off-nominal behavior due to faults. The robustness and stability properties of the fault diagnosis scheme are rigorously established. Simulation examples are presented to illustrate the ability of the neural-network-based robust fault diagnosis scheme to detect and accommodate faults in a two-link robotic manipulator.","",""
5,"Naoya Takeishi, Alexandros Kalousis","Physics-Integrated Variational Autoencoders for Robust and Interpretable Generative Modeling",2021,"","","","",157,"2022-07-13 10:10:07","","","","",,,,,5,5.00,3,2,1,"Integrating physics models within machine learning models holds considerable promise toward learning robust models with improved interpretability and abilities to extrapolate. In this work, we focus on the integration of incomplete physics models into deep generative models. In particular, we introduce an architecture of variational autoencoders (VAEs) in which a part of the latent space is grounded by physics. A key technical challenge is to strike a balance between the incomplete physics and trainable components such as neural networks for ensuring that the physics part is used in a meaningful manner. To this end, we propose a regularized learning method that controls the effect of the trainable components and preserves the semantics of the physics-based latent variables as intended. We not only demonstrate generative performance improvements over a set of synthetic and real-world datasets, but we also show that we learn robust models that can consistently extrapolate beyond the training distribution in a meaningful manner. Moreover, we show that we can control the generative process in an interpretable manner.","",""
75,"Vu Thi Yen, Yaonan Wang, C. Pham","Recurrent fuzzy wavelet neural networks based on robust adaptive sliding mode control for industrial robot manipulators",2019,"","","","",158,"2022-07-13 10:10:07","","10.1007/s00521-018-3520-3","","",,,,,75,25.00,25,3,3,"","",""
490,"M. Matsugu, Katsuhiko Mori, Y. Mitari, Yuji Kaneda","Subject independent facial expression recognition with robust face detection using a convolutional neural network",2003,"","","","",159,"2022-07-13 10:10:07","","10.1016/S0893-6080(03)00115-1","","",,,,,490,25.79,123,4,19,"","",""
32,"Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, Jingdong Wang","Informative Dropout for Robust Representation Learning: A Shape-bias Perspective",2020,"","","","",160,"2022-07-13 10:10:07","","","","",,,,,32,16.00,5,6,2,"Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at this https URL.","",""
147,"K. Liano","Robust error measure for supervised neural network learning with outliers",1996,"","","","",161,"2022-07-13 10:10:07","","10.1109/72.478411","","",,,,,147,5.65,147,1,26,"Most supervised neural networks (NNs) are trained by minimizing the mean squared error (MSE) of the training set. In the presence of outliers, the resulting NN model can differ significantly from the underlying system that generates the data. Two different approaches are used to study the mechanism by which outliers affect the resulting models: influence function and maximum likelihood. The mean log squared error (MLSE) is proposed as the error criteria that can be easily adapted by most supervised learning algorithms. Simulation results indicate that the proposed method is robust against outliers.","",""
4,"Jay Roberts, Theodoros Tsiligkaridis","Controllably Sparse Perturbations of Robust Classifiers for Explaining Predictions and Probing Learned Concepts",2021,"","","","",162,"2022-07-13 10:10:07","","","","",,,,,4,4.00,2,2,1,"Explaining the predictions of a deep neural network (DNN) in image classification is an active area of research. Many methods focus on localizing pixels, or groups of pixels, which maximize a relevance metric for the prediction. Others aim at creating local ""proxy"" explainers which aim to account for an individual prediction of a model. We aim to explore ""why"" a model made a prediction by perturbing inputs to robust classifiers and interpreting the semantically meaningful results. For such an explanation to be useful for humans it is desirable for it to be sparse; however, generating sparse perturbations can computationally expensive and infeasible on high resolution data. Here we introduce controllably sparse explanations that can be efficiently generated on higher resolution data to provide improved counter-factual explanations. Further we use these controllably sparse explanations to probe what the robust classifier has learned. These explanations could provide insight for model developers as well as assist in detecting dataset bias. CCS Concepts • Computing methodologies → Machine learning; Artificial intelligence;","",""
3,"Andreas Leitherer, A. Ziletti, L. Ghiringhelli","Robust recognition and exploratory analysis of crystal structures via Bayesian deep learning",2021,"","","","",163,"2022-07-13 10:10:07","","10.1038/s41467-021-26511-5","","",,,,,3,3.00,1,3,1,"","",""
12,"Javier Martínez García, Dominik Zoeke, M. Vossiek","MIMO-FMCW Radar-Based Parking Monitoring Application With a Modified Convolutional Neural Network With Spatial Priors",2018,"","","","",164,"2022-07-13 10:10:07","","10.1109/ACCESS.2018.2857007","","",,,,,12,3.00,4,3,4,"Radar imaging is a competitive option for smart city applications over optical approaches, as it raises no privacy concerns. The inherent difficulty of interpreting radar signals can be overcome using deep learning techniques to leverage the capabilities of monitoring sensors with a minimum of human intervention. In this paper, we use a modified convolutional neural network (CNN) for classifying radar images in order to detect vacant parking spaces with a 77-GHz imaging radar. Although training CNNs for radar-image classification is challenging due to poor generalization performance caused by the lack of labeled training data, the modified architecture takes into account the properties of the radar image in order to introduce prior information into the model and improve performance. A MIMO-FMCW radar is utilized to render a slant-range image of a parking scenario, and the image patches corresponding to each parking location are classified independently in the CNN. Since the radiation pattern of a MIMO array varies as a function of the scanning angle, the corresponding spatial coordinate of each patch is included as an additional feature in the upper layers of the network. This allows the model to combine local features from each patch with global scenario information in order to learn robust features that generalize properly to new scenarios. Several models are trained end to end with data from four different parking scenarios and evaluated in a 4-fold cross-validation scheme, and performance is improved when spatial prior information is included.","",""
3,"A. Heinlein, A. Klawonn, M. Lanser, J. Weber","Combining Machine Learning and Adaptive Coarse Spaces---A Hybrid Approach for Robust FETI-DP Methods in Three Dimensions",2020,"","","","",165,"2022-07-13 10:10:07","","10.1137/20m1344913","","",,,,,3,1.50,1,4,2,"The hybrid ML-FETI-DP algorithm combines the advantages of adaptive coarse spaces in domain decomposition methods and certain supervised machine learning techniques. Adaptive coarse spaces ensure robustness of highly scalable domain decomposition solvers, even for highly heterogeneous coefficient distributions with arbitrary coefficient jumps. However, their construction requires the setup and solution of local generalized eigenvalue problems, which is typically computationally expensive. The idea of ML-FETI-DP is to interpret the coefficient distribution as image data and predict whether an eigenvalue problem has to be solved or can be neglected while still maintaining robustness of the adaptive FETI-DP method. For this purpose, neural networks are used as image classifiers. In the present work, the ML-FETI-DP algorithm is extended to three dimensions, which requires both a complex data preprocessing procedure to construct consistent input data for the neural network as well as a representative training and validation data set to ensure generalization properties of the machine learning model. Numerical experiments for stationary diffusion and linear elasticity problems with realistic coefficient distributions show that a large number of eigenvalue problems can be saved; in the best case of the numerical results presented here, 97% of the eigenvalue problems can be avoided to be set up and solved.","",""
4,"Theodoros Tsiligkaridis, Jay Roberts","On Frank-Wolfe Optimization for Adversarial Robustness and Interpretability",2020,"","","","",166,"2022-07-13 10:10:07","","","","",,,,,4,2.00,2,2,2,"Deep neural networks are easily fooled by small perturbations known as adversarial attacks. Adversarial Training (AT) is a technique that approximately solves a robust optimization problem to minimize the worst-case loss and is widely regarded as the most effective defense against such attacks. While projected gradient descent (PGD) has received most attention for approximately solving the inner maximization of AT, Frank-Wolfe (FW) optimization is projection-free and can be adapted to any L norm. A Frank-Wolfe adversarial training approach is presented and is shown to provide as competitive level of robustness as PGD-AT without much tuning for a variety of architectures. We empirically show that robustness is strongly connected to the L magnitude of the adversarial perturbation and that more locally linear loss landscapes tend to have larger L distortions despite having the same L∞ distortion. We provide theoretical guarantees on the magnitude of the distortion for FW that depend on local geometry which FW-AT exploits. It is empirically shown that FW-AT achieves strong robustness to white-box attacks and black-box attacks and offers improved resistance to gradient masking. Further, FW-AT allows networks to learn highquality human-interpretable features which are then used to generate counterfactual explanations to model predictions by using dense and sparse adversarial perturbations.","",""
10,"Julian Viereck, Jules Kozolinsky, Alexander Herzog, L. Righetti","Learning a Structured Neural Network Policy for a Hopping Task",2017,"","","","",167,"2022-07-13 10:10:07","","10.1109/LRA.2018.2861466","","",,,,,10,2.00,3,4,5,"In this letter, we present a method for learning a reactive policy for a simple dynamic locomotion task involving hard impact and switching contacts where we assume the contact location and contact timing to be unknown. To learn such a policy, we use optimal control to optimize a local controller for a fixed environment and contacts. We learn the contact-rich dynamics for our underactuated systems along these trajectories in a sample efficient manner. We use the optimized policies to learn the reactive policy in form of a neural network. Using a new neural network architecture, we are able to preserve more information from the local policy and make its output interpretable in the sense that its output in terms of desired trajectories, feedforward commands and gains can be interpreted. Extensive simulations demonstrate the robustness of the approach to changing environments, outperforming a model-free gradient policy based methods on the same tasks in simulation. Finally, we show that the learned policy can be robustly transferred on a real robot.","",""
50,"Kenneth P. Smith, A. D. Kang, J. Kirby","Automated Interpretation of Blood Culture Gram Stains by Use of a Deep Convolutional Neural Network",2017,"","","","",168,"2022-07-13 10:10:07","","10.1128/JCM.01521-17","","",,,,,50,10.00,17,3,5,"ABSTRACT Microscopic interpretation of stained smears is one of the most operator-dependent and time-intensive activities in the clinical microbiology laboratory. Here, we investigated application of an automated image acquisition and convolutional neural network (CNN)-based approach for automated Gram stain classification. Using an automated microscopy platform, uncoverslipped slides were scanned with a 40× dry objective, generating images of sufficient resolution for interpretation. We collected 25,488 images from positive blood culture Gram stains prepared during routine clinical workup. These images were used to generate 100,213 crops containing Gram-positive cocci in clusters, Gram-positive cocci in chains/pairs, Gram-negative rods, or background (no cells). These categories were targeted for proof-of-concept development as they are associated with the majority of bloodstream infections. Our CNN model achieved a classification accuracy of 94.9% on a test set of image crops. Receiver operating characteristic (ROC) curve analysis indicated a robust ability to differentiate between categories with an area under the curve of >0.98 for each. After training and validation, we applied the classification algorithm to new images collected from 189 whole slides without human intervention. Sensitivity and specificity were 98.4% and 75.0% for Gram-positive cocci in chains and pairs, 93.2% and 97.2% for Gram-positive cocci in clusters, and 96.3% and 98.1% for Gram-negative rods. Taken together, our data support a proof of concept for a fully automated classification methodology for blood-culture Gram stains. Importantly, the algorithm was highly adept at identifying image crops with organisms and could be used to present prescreened, classified crops to technologists to accelerate smear review. This concept could potentially be extended to all Gram stain interpretive activities in the clinical laboratory.","",""
30,"Run Wang, Felix Juefei-Xu, Yihao Huang, Qing Guo, Xiaofei Xie, L. Ma, Yang Liu","DeepSonar: Towards Effective and Robust Detection of AI-Synthesized Fake Voices",2020,"","","","",169,"2022-07-13 10:10:07","","10.1145/3394171.3413716","","",,,,,30,15.00,4,7,2,"With the recent advances in voice synthesis, AI-synthesized fake voices are indistinguishable to human ears and widely are applied to produce realistic and natural DeepFakes, exhibiting real threats to our society. However, effective and robust detectors for synthesized fake voices are still in their infancy and are not ready to fully tackle this emerging threat. In this paper, we devise a novel approach, named DeepSonar, based on monitoring neuron behaviors of speaker recognition (SR) system, i.e., a deep neural network (DNN), to discern AI-synthesized fake voices. Layer-wise neuron behaviors provide an important insight to meticulously catch the differences among inputs, which are widely employed for building safety, robust, and interpretable DNNs. In this work, we leverage the power of layer-wise neuron activation patterns with a conjecture that they can capture the subtle differences between real and AI-synthesized fake voices, in providing a cleaner signal to classifiers than raw inputs. Experiments are conducted on three datasets (including commercial products from Google, Baidu, etc) containing both English and Chinese languages to corroborate the high detection rates (98.1% average accuracy) and low false alarm rates (about 2% error rate) of DeepSonar in discerning fake voices. Furthermore, extensive experimental results also demonstrate its robustness against manipulation attacks (e.g., voice conversion and additive real-world noises). Our work further poses a new insight into adopting neuron behaviors for effective and robust AI aided multimedia fakes forensics as an inside-out approach instead of being motivated and swayed by various artifacts introduced in synthesizing fakes.","",""
0,"Preetam Prabhu Srikar Dammu, S. Chalamala, A. Singh, B. Yegnanarayana","Interpretable and Robust Face Verification",2021,"","","","",170,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,4,1,"Advances in deep learning have been instrumental in enhancing the performance of face verification systems. Despite their ability to attain high accuracy, most of these systems fail to provide interpretations of their decisions. With the increased demands in making deep learning models more interpretable, numerous post-hoc methods have been proposed to probe the workings of these systems. Yet, the quest for face verification systems that inherently provide interpretations still remains largely unexplored. Additionally, most of the existing face recognition models are highly susceptible to adversarial attacks. In this work, we propose a face verification system which addresses the issue of interpretability by employing modular neural networks. In this, representations for each individual facial parts such as nose, mouth, eyes etc. are learned separately. We also show that our method is significantly more resistant to adversarial attacks, thereby addressing another crucial weakness concerning deep learning models.","",""
82,"Christian Etmann, Sebastian Lunz, P. Maass, C. Schönlieb","On the Connection Between Adversarial Robustness and Saliency Map Interpretability",2019,"","","","",171,"2022-07-13 10:10:07","","","","",,,,,82,27.33,21,4,3,"Recent studies on the adversarial vulnerability of neural networks have shown that models trained to be more robust to adversarial attacks exhibit more interpretable saliency maps than their non-robust counterparts. We aim to quantify this behavior by considering the alignment between input image and saliency map. We hypothesize that as the distance to the decision boundary grows,so does the alignment. This connection is strictly true in the case of linear models. We confirm these theoretical findings with experiments based on models trained with a local Lipschitz regularization and identify where the non-linear nature of neural networks weakens the relation.","",""
0,"Ju-Hwan Kim, Ji-Eun Woo, So-Yeon Park, Soo-Jin Kim, Donghyeon Han","Robust Deep Learning-Based Profiling Side-Channel Analysis for Jitter*",2021,"","","","",172,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,5,1,"Deep learning-based profiling side-channel analysis is a powerful analysis method that utilizes the neural network to profile the relationship between the side-channel information and the intermediate value. Since the neural network interprets each point of the signal in a different dimension, jitter makes it much hard that the neural network with dimension-wise weights learns the relationship. This paper shows that replacing the fully-connected layer of the traditional CNN (Convolutional Neural Network) with global average pooling (GAP) allows us to design the inherently robust neural network inherently for jitter. We experimented with the ChipWhisperer-Lite board to demonstrate the proposed method: as a result, the validation accuracy of the CNN with a fully-connected layer was only up to 1.4%; contrastively, the validation accuracy of the CNN with GAP was very high at up to 41.7%.","",""
1,"Z. Gou, C. Fyfe","A robust canonical correlation neural network",2002,"","","","",173,"2022-07-13 10:10:07","","10.1109/NNSP.2002.1030035","","",,,,,1,0.05,1,2,20,"We review a neural implementation of canonical correlation analysis and show, using ideas suggested by ridge regression, how to make the algorithm robust. The network is shown to operate on data sets which exhibit multicollinearity. We develop a second model which not only performs as well on multicollinear data but also on general data sets. This model allows us to vary a single parameter so that the network is capable of performing partial least squares regression (at one extreme) to canonical correlation analysis (at the other) and every intermediate operation between the two. On multicollinear data, the parameter setting is shown to be important but on more general data no particular parameter setting is required. Finally, the algorithm acts on such data as a smoother in that the resulting weight vectors are much smoother and more interpretable than the weights without the robustification term.","",""
252,"Yansong Gao, Chang Xu, Derui Wang, Shiping Chen, D. Ranasinghe, S. Nepal","STRIP: a defence against trojan attacks on deep neural networks",2019,"","","","",174,"2022-07-13 10:10:07","","10.1145/3359789.3359790","","",,,,,252,84.00,42,6,3,"A recent trojan attack on deep neural network (DNN) models is one insidious variant of data poisoning attacks. Trojan attacks exploit an effective backdoor created in a DNN model by leveraging the difficulty in interpretability of the learned model to misclassify any inputs signed with the attacker's chosen trojan trigger. Since the trojan trigger is a secret guarded and exploited by the attacker, detecting such trojan inputs is a challenge, especially at run-time when models are in active operation. This work builds STRong Intentional Perturbation (STRIP) based run-time trojan attack detection system and focuses on vision system. We intentionally perturb the incoming input, for instance by superimposing various image patterns, and observe the randomness of predicted classes for perturbed inputs from a given deployed model---malicious or benign. A low entropy in predicted classes violates the input-dependence property of a benign model and implies the presence of a malicious input---a characteristic of a trojaned input. The high efficacy of our method is validated through case studies on three popular and contrasting datasets: MNIST, CIFAR10 and GTSRB. We achieve an overall false acceptance rate (FAR) of less than 1%, given a preset false rejection rate (FRR) of 1%, for different types of triggers. Using CIFAR10 and GTSRB, we have empirically achieved result of 0% for both FRR and FAR. We have also evaluated STRIP robustness against a number of trojan attack variants and adaptive attacks.","",""
0,"Lingkun Kong, Dewang Chen, Ruijun Cheng","WRNFS: Width Residual Neuro Fuzzy System, a Fast-Learning Algorithm with High Interpretability",2022,"","","","",175,"2022-07-13 10:10:07","","10.3390/app12125810","","",,,,,0,0.00,0,3,1,"Although the deep neural network has a strong fitting ability, it is difficult to be applied to safety-critical fields because of its poor interpretability. Based on the adaptive neuro-fuzzy inference system (ANFIS) and the concept of residual network, a width residual neuro-fuzzy system (WRNFS) is proposed to improve the interpretability performance in this paper. WRNFS is used to transform a regression problem of high-dimensional data into the sum of several low-dimensional neuro-fuzzy systems. The ANFIS model in the next layer is established based on the low dimensional data and the residual of the ANFIS model in the former layer. The performance of WRNFS is compared with traditional ANFIS on three data sets. The results showed that WRNFS has high interpretability (fewer layers, fewer fuzzy rules, and fewer adjustable parameters) on the premise of satisfying the fitting accuracy. The interpretability, complexity, time efficiency, and robustness of WRNFS are greatly improved when the input number of single low-dimensional systems decreases.","",""
0,"O. Kitouni, N. Nolte, Mike Williams","Robust and Provably Monotonic Networks",2021,"","","","",176,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,3,1,"The Lipschitz constant of the map between the input and output space represented by a neural network is a natural metric for assessing the robustness of the model. We present a new method to constrain the Lipschitz constant of dense deep learning models that can also be generalized to other architectures. The method relies on a simple weight normalization scheme during training that ensures the Lipschitz constant of every layer is below an upper limit specified by the analyst. A simple residual connection can then be used to make the model monotonic in any subset of its inputs, which is useful in scenarios where domain knowledge dictates such dependence. Examples can be found in algorithmic fairness requirements or, as presented here, in the classification of the decays of subatomic particles produced at the CERN Large Hadron Collider. Our normalization is minimally constraining and allows the underlying architecture to maintain higher expressiveness compared to other techniques which aim to either control the Lipschitz constant of the model or ensure its monotonicity. We show how the algorithm was used to train a powerful, robust, and interpretable discriminator for heavy-flavor decays in the LHCb realtime data-processing system.","",""
0,"Phong Le, W. Zuidema","DoLFIn: Distributions over Latent Features for Interpretability",2020,"","","","",177,"2022-07-13 10:10:07","","10.18653/V1/2020.COLING-MAIN.127","","",,,,,0,0.00,0,2,2,"Interpreting the inner workings of neural models is a key step in ensuring the robustness and trustworthiness of the models, but work on neural network interpretability typically faces a trade-off: either the models are too constrained to be very useful, or the solutions found by the models are too complex to interpret. We propose a novel strategy for achieving interpretability that – in our experiments – avoids this trade-off. Our approach builds on the success of using probability as the central quantity, such as for instance within the attention mechanism. In our architecture, DoLFIn (Distributions over Latent Features for Interpretability), we do no determine beforehand what each feature represents, and features go altogether into an unordered set. Each feature has an associated probability ranging from 0 to 1, weighing its importance for further processing. We show that, unlike attention and saliency map approaches, this set-up makes it straight-forward to compute the probability with which an input component supports the decision the neural model makes. To demonstrate the usefulness of the approach, we apply DoLFIn to text classification, and show that DoLFIn not only provides interpretable solutions, but even slightly outperforms the classical CNN and BiLSTM text classifiers on the SST2 and AG-news datasets.","",""
3,"Hao Shen, Sihong Chen, Ran Wang","A study on the uncertainty of convolutional layers in deep neural networks",2020,"","","","",178,"2022-07-13 10:10:07","","10.1007/S13042-021-01278-9","","",,,,,3,1.50,1,3,2,"","",""
3,"A. Habibnia, E. Maasoumi","Forecasting in Big Data Environments: an Adaptable and Automated Shrinkage Estimation of Neural Networks (AAShNet)",2019,"","","","",179,"2022-07-13 10:10:07","","10.1007/s40953-021-00275-7","","",,,,,3,1.00,2,2,3,"","",""
1,"Benjamin Filtjens, P. Ginis, A. Nieuwboer, M. R. Afzal, J. Spildooren, B. Vanrumste, P. Slaets","Modelling and identification of characteristic kinematic features preceding freezing of gait with convolutional neural networks and layer-wise relevance propagation",2021,"","","","",180,"2022-07-13 10:10:07","","10.1186/s12911-021-01699-0","","",,,,,1,1.00,0,7,1,"","",""
3,"Ana F. Sequeira, Tiago Gonçalves, W. Silva, João Ribeiro Pinto, Jaime S. Cardoso","An exploratory study of interpretability for face presentation attack detection",2021,"","","","",181,"2022-07-13 10:10:07","","10.1049/BME2.12045","","",,,,,3,3.00,1,5,1,"Fundação para a Ciência e a Tecnologia within project UIDB/50014/2020 and the PhD grants, Grant/Award Number: SFRH/BD/137720/2018; SFRH/BD/139468/2018; and 2020.06434.BD Abstract Biometric recognition and presentation attack detection (PAD) methods strongly rely on deep learning algorithms. Though often more accurate, these models operate as complex black boxes. Interpretability tools are now being used to delve deeper into the operation of these methods, which is why this work advocates their integration in the PAD scenario. Building upon previous work, a face PAD model based on convolutional neural networks was implemented and evaluated both through traditional PAD metrics and with interpretability tools. An evaluation on the stability of the explanations obtained from testing models with attacks known and unknown in the learning step is made. To overcome the limitations of direct comparison, a suitable representation of the explanations is constructed to quantify how much two explanations differ from each other. From the point of view of interpretability, the results obtained in intra and inter class comparisons led to the conclusion that the presence of more attacks during training has a positive effect in the generalisation and robustness of the models. This is an exploratory study that confirms the urge to establish new approaches in biometrics that incorporate interpretability tools. Moreover, there is a need for methodologies to assess and compare the quality of explanations.","",""
0,"Pau","Neural network signal understanding for instrumentation",2018,"","","","",182,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,1,4,"This paper reports on the use of neural signal interpretation theory and techniques for the purpose of classifying the shapes of a set of instrumentation signals, in order to calibrate devices, diagnose anomalies, generate tuning/settings, and interpret the measurement results. Neural signal understanding research is surveyed, and the selected implementation is described with its performance in terms of correct classification rates and robustness to noise. Formal results on neural net training time and sensitivity to weights are given. A theory for neural control is given using functional link nets and an explanation technique is designed to help neural signal understanding. The results of this are compared to those of a knowledge-based signal interpretation system within the context of the same specific instrument and data. Keywords-Neural understanding, calibration, signal understanding, control theory, neural control, training time, sensitivity to noise, explanation facilities, knowledge-based signal interpretation, instrumentation, analytical instrumentation.","",""
0,"Pau","Neural network signal understanding for instrumentation",2018,"","","","",183,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,1,4,"This paper reports on the use of neural signal interpretation theory and techniques for the purpose of classifying the shapes of a set of instrumentation signals, in order to calibrate devices, diagnose anomalies, generate tuning/settings, and interpret the measurement results. Neural signal understanding research is surveyed, and the selected implementation is described with its performance in terms of correct classification rates and robustness to noise. Formal results on neural net training time and sensitivity to weights are given. A theory for neural control is given using functional link nets and an explanation technique is designed to help neural signal understanding. The results of this are compared to those of a knowledge-based signal interpretation system within the context of the same specific instrument and data. Keywords-Neural understanding, calibration, signal understanding, control theory, neural control, training time, sensitivity to noise, explanation facilities, knowledge-based signal interpretation, instrumentation, analytical instrumentation.","",""
0,"Pau","Neural network signal understanding for instrumentation",2018,"","","","",184,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,1,4,"This paper reports on the use of neural signal interpretation theory and techniques for the purpose of classifying the shapes of a set of instrumentation signals, in order to calibrate devices, diagnose anomalies, generate tuning/settings, and interpret the measurement results. Neural signal understanding research is surveyed, and the selected implementation is described with its performance in terms of correct classification rates and robustness to noise. Formal results on neural net training time and sensitivity to weights are given. A theory for neural control is given using functional link nets and an explanation technique is designed to help neural signal understanding. The results of this are compared to those of a knowledge-based signal interpretation system within the context of the same specific instrument and data. Keywords-Neural understanding, calibration, signal understanding, control theory, neural control, training time, sensitivity to noise, explanation facilities, knowledge-based signal interpretation, instrumentation, analytical instrumentation.","",""
19,"Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, Tianlin Li","Interpreting and Improving Adversarial Robustness of Deep Neural Networks With Neuron Sensitivity",2019,"","","","",185,"2022-07-13 10:10:07","","10.1109/TIP.2020.3042083","","",,,,,19,6.33,3,7,3,"Deep neural networks (DNNs) are vulnerable to adversarial examples where inputs with imperceptible perturbations mislead DNNs to incorrect results. Despite the potential risk they bring, adversarial examples are also valuable for providing insights into the weakness and blind-spots of DNNs. Thus, the interpretability of a DNN in the adversarial setting aims to explain the rationale behind its decision-making process and makes deeper understanding which results in better practical applications. To address this issue, we try to explain adversarial robustness for deep models from a new perspective of neuron sensitivity which is measured by neuron behavior variation intensity against benign and adversarial examples. In this paper, we first draw the close connection between adversarial robustness and neuron sensitivities, as sensitive neurons make the most non-trivial contributions to model predictions in the adversarial setting. Based on that, we further propose to improve adversarial robustness by stabilizing the behaviors of sensitive neurons. Moreover, we demonstrate that state-of-the-art adversarial training methods improve model robustness by reducing neuron sensitivities, which in turn confirms the strong connections between adversarial robustness and neuron sensitivity. Extensive experiments on various datasets demonstrate that our algorithm effectively achieves excellent results. To the best of our knowledge, we are the first to study adversarial robustness using neuron sensitivities.","",""
0,"Nikola Janjusevic, Amirhossein Khalilian-Gourtani, Yao Wang","CDLNet: Robust and Interpretable Denoising Through Deep Convolutional Dictionary Learning",2021,"","","","",186,"2022-07-13 10:10:07","","","","",,,,,0,0.00,0,3,1,"Deep learning based methods hold state-of-the-art results in image denoising, but remain difficult to interpret due to their construction from poorly understood building blocks such as batch-normalization, residual learning, and feature domain processing. Unrolled optimization networks propose an interpretable alternative to constructing deep neural networks by deriving their architecture from classical iterative optimization methods, without use of tricks from the standard deep learning tool-box. So far, such methods have demonstrated performance close to that of state-of-the-art models while using their interpretable construction to achieve a comparably low learned parameter count. In this work, we propose an unrolled convolutional dictionary learning network (CDLNet) and demonstrate its competitive denoising performance in both low and high parameter count regimes. Specifically, we show that the proposed model outperforms the state-of-the-art denoising models when scaled to similar parameter count. In addition, we leverage the model’s interpretable construction to propose an augmentation of the network’s thresholds that enables state-of-the-art blind denoising performance and near-perfect generalization on noiselevels unseen during training.","",""
6,"M. Mohamed, A. Almehdhar, M. Bamatraf, M. Girgis","Enhanced Self-Organizing Map Neural Network for DNA Sequence Classification",2013,"","","","",187,"2022-07-13 10:10:07","","10.4236/IIM.2013.51004","","",,,,,6,0.67,2,4,9,"The artificial neural networks (ANNs), among different soft computing methodologies are widely used to meet the challenges thrown by the main objectives of data mining classification techniques, due to their robust, powerful, distributed, fault tolerant computing and capability to learn in a data-rich environment. ANNs has been used in several fields, showing high performance as classifiers. The problem of dealing with non numerical data is one major obstacle prevents using them with various data sets and several domains. Another problem is their complex structure and how hands to interprets. Self-Organizing Map (SOM) is type of neural systems that can be easily interpreted, but still can’t be used with non numerical data directly. This paper presents an enhanced SOM structure to cope with non numerical data. It used DNA sequences as the training dataset. Results show very good performance compared to other classifiers. For better evaluation both micro-array structure and their sequential representation as proteins were targeted as dataset accuracy is measured accordingly.","",""
0,"A. Chistyakova, Maria Cherepnina, K. Arkhipenko, Sergey D. Kuznetsov, Chang-Seok Oh, Sebeom Park","Evaluation of interpretability methods for adversarial robustness on real-world datasets",2021,"","","","",188,"2022-07-13 10:10:07","","10.1109/ivmem53963.2021.00007","","",,,,,0,0.00,0,6,1,"Adversarial training is considered the most powerful approach for robustness against attacks on deep neural networks involving adversarial examples. However, recent works have shown that the similar robustness level can be achieved by other means, namely interpretability-based regularization. We evaluate these interpretability-based approaches on real-world ResNet models trained on CIFAR-10 and ImageNet datasets. Our results show that interpretability can marginally improve robustness when combined with adversarial training, however, they bring additional computational complexity making these approaches questionable for such models and datasets.","",""
1,"Yuyang Gao, Tong Sun, Guang-ying Bai, Siyi Gu, S. Hong, Liang Zhao","RES: A Robust Framework for Guiding Visual Explanation",2022,"","","","",189,"2022-07-13 10:10:07","","10.1145/3534678.3539419","","",,,,,1,1.00,0,6,1,"Despite the fast progress of explanation techniques in modern Deep Neural Networks (DNNs) where the main focus is handling “how to generate the explanations”, advanced research questions that exam-ine the quality of the explanation itself (e.g., “whether the explanations are accurate”) and improve the explanation quality (e.g., “how to adjust the model to generate more accurate explanations when explanations are inaccurate”) are still relatively under-explored. To guide the model toward better explanations, techniques in explanation supervision—which add supervision signals on the model explanation—have started to show promising effects on improving both the generalizability as and intrinsic interpretability of Deep Neural Networks. However, the research on supervising explanations, especially in vision-based applications represented through saliency maps, is in its early stage due to several inherent challenges: 1) inaccuracy of the human explanation annotation boundary, 2) incompleteness of the human explanation annotation region, and 3) inconsistency of the data distribution between human annotation and model explanation maps. To address the challenges, we propose a generic RES 1 framework for guiding visual explanation by developing a novel objective that handles inaccurate boundary, incomplete region, and inconsistent distribution of human annotations, with a theoretical justification on model generalizability. Extensive experiments on two real-world image datasets demonstrate the effectiveness of the proposed framework on enhancing","",""
327,"Nicolas Papernot, P. Mcdaniel","Deep k-Nearest Neighbors: Towards Confident, Interpretable and Robust Deep Learning",2018,"","","","",190,"2022-07-13 10:10:07","","","","",,,,,327,81.75,164,2,4,"Deep neural networks (DNNs) enable innovative applications of machine learning like image recognition, machine translation, or malware detection. However, deep learning is often criticized for its lack of robustness in adversarial settings (e.g., vulnerability to adversarial inputs) and general inability to rationalize its predictions. In this work, we exploit the structure of deep learning to enable new learning-based inference and decision strategies that achieve desirable properties such as robustness and interpretability. We take a first step in this direction and introduce the Deep k-Nearest Neighbors (DkNN). This hybrid classifier combines the k-nearest neighbors algorithm with representations of the data learned by each layer of the DNN: a test input is compared to its neighboring training points according to the distance that separates them in the representations. We show the labels of these neighboring points afford confidence estimates for inputs outside the model's training manifold, including on malicious inputs like adversarial examples--and therein provides protections against inputs that are outside the models understanding. This is because the nearest neighbors can be used to estimate the nonconformity of, i.e., the lack of support for, a prediction in the training data. The neighbors also constitute human-interpretable explanations of predictions. We evaluate the DkNN algorithm on several datasets, and show the confidence estimates accurately identify inputs outside the model, and that the explanations provided by nearest neighbors are intuitive and useful in understanding model failures.","",""
1,"R. Lin, Jie Ran, Ngai Wong","EZCrop: Energy-Zoned Channels for Robust Output Pruning",2021,"","","","",191,"2022-07-13 10:10:07","","10.1109/WACV51458.2022.00365","","",,,,,1,1.00,0,3,1,"Recent results have revealed an interesting observation in a trained convolutional neural network (CNN), namely, the rank of a feature map channel matrix remains surprisingly constant despite the input images. This has led to an effective rank-based channel pruning algorithm [23], yet the constant rank phenomenon remains mysterious and unexplained. This work aims at demystifying and interpreting such rank behavior from a frequency-domain perspective, which as a bonus suggests an extremely efficient Fast Fourier Transform (FFT)-based metric for measuring channel importance without explicitly computing its rank. We achieve remarkable CNN channel pruning based on this analytically sound and computationally efficient metric, and adopt it for repetitive pruning to demonstrate robustness via our scheme named Energy-Zoned Channels for Robust Output Pruning (EZCrop), which shows consistently better results than other state-of-the-art channel pruning methods. The codes and Appendix are publicly available at: https://github.com/ruilin0212/EZCrop.","",""
1,"Xi Wang, M. Christie, É. Marchand","Binary Graph Descriptor for Robust Relocalization on Heterogeneous Data",2022,"","","","",192,"2022-07-13 10:10:07","","10.1109/lra.2022.3142854","","",,,,,1,1.00,0,3,1,"In this letter, we propose a novelbinary graph descriptor to improve loop detection for visual SLAM systems. Our contribution is twofold: i) a graph embedding technique for generating binary descriptors which conserve both spatial and histogram information extracted from images; ii) a generic mean of combining multiple layers of heterogeneous data into the proposed binary graph descriptor, coupled with a matching and geometric checking method. We also introduce an implementation of our descriptor into an incremental Bag-of-Words (iBoW) structure that improves efficiency and scalability, and propose a method to interpret Deep Neural Network (DNN) results. We evaluate our system on synthetic and real datasets across different lighting and seasonal conditions. The proposed method outperforms state-of-the-art loop detection frameworks in terms of relocalization precision and computational performance, as well as displays high robustness against cross-condition datasets.","",""
4,"S. Yu, Yulei Niu, Shuohang Wang, Jing Jiang, Qianru Sun","Counterfactual Variable Control for Robust and Interpretable Question Answering",2020,"","","","",193,"2022-07-13 10:10:07","","","","",,,,,4,2.00,1,5,2,"Deep neural network based question answering (QA) models are neither robust nor explainable in many cases. For example, a multiple-choice QA model, tested without any input of question, is surprisingly ""capable"" to predict the most of correct options. In this paper, we inspect such spurious ""capability"" of QA models using causal inference. We find the crux is the shortcut correlation, e.g., unrobust word alignment between passage and options learned by the models. We propose a novel approach called Counterfactual Variable Control (CVC) that explicitly mitigates any shortcut correlation and preserves the comprehensive reasoning for robust QA. Specifically, we leverage multi-branch architecture that allows us to disentangle robust and shortcut correlations in the training process of QA. We then conduct two novel CVC inference methods (on trained models) to capture the effect of comprehensive reasoning as the final prediction. For evaluation, we conduct extensive experiments using two BERT backbones on both multi-choice and span-extraction QA benchmarks. The results show that our CVC achieves high robustness against a variety of adversarial attacks in QA while maintaining good interpretation ability.","",""
6,"D. Raimondi, J. Simm, A. Arany, P. Fariselli, I. Cleynen, Y. Moreau","An interpretable low-complexity machine learning framework for robust exome-based in-silico diagnosis of Crohn’s disease patients",2020,"","","","",194,"2022-07-13 10:10:07","","10.1093/nargab/lqaa011","","",,,,,6,3.00,1,6,2,"Abstract Whole exome sequencing (WES) data are allowing researchers to pinpoint the causes of many Mendelian disorders. In time, sequencing data will be crucial to solve the genome interpretation puzzle, which aims at uncovering the genotype-to-phenotype relationship, but for the moment many conceptual and technical problems need to be addressed. In particular, very few attempts at the in-silico diagnosis of oligo-to-polygenic disorders have been made so far, due to the complexity of the challenge, the relative scarcity of the data and issues such as batch effects and data heterogeneity, which are confounder factors for machine learning (ML) methods. Here, we propose a method for the exome-based in-silico diagnosis of Crohn’s disease (CD) patients which addresses many of the current methodological issues. First, we devise a rational ML-friendly feature representation for WES data based on the gene mutational burden concept, which is suitable for small sample sizes datasets. Second, we propose a Neural Network (NN) with parameter tying and heavy regularization, in order to limit its complexity and thus the risk of over-fitting. We trained and tested our NN on 3 CD case-controls datasets, comparing the performance with the participants of previous CAGI challenges. We show that, notwithstanding the limited NN complexity, it outperforms the previous approaches. Moreover, we interpret the NN predictions by analyzing the learned patterns at the variant and gene level and investigating the decision process leading to each prediction.","",""
5,"Zicheng Hu, A. Tang, Jaiveer Singh, Sanchita Bhattacharya, A. Butte","A robust and interpretable, end-to-end deep learning model for cytometry data",2020,"","","","",195,"2022-07-13 10:10:07","","10.1101/2020.02.05.934521","","",,,,,5,2.50,1,5,2,"Cytometry technologies are essential tools for immunology research, providing high-throughput measurements of the immune cells at the single-cell level. Traditional approaches in interpreting and using cytometry measurements include manual or automated gating to identify cell subsets from the cytometry data, providing highly intuitive results but may lead to significant information loss, in that additional details in measured or correlated cell signals might be missed. In this study, we propose and test a deep convolutional neural network for analyzing cytometry data in an end-to-end fashion, allowing a direct association between raw cytometry data and the clinical outcome of interest. Using nine large CyTOF studies from the open-access ImmPort database, we demonstrated that the deep convolutional neural network model can accurately diagnose the latent cytomegalovirus (CMV) in healthy individuals, even when using highly heterogeneous data from different studies. In addition, we developed a permutation-based method for interpreting the deep convolutional neural network model and identified a CD27-CD94+ CD8+ T cell population significantly associated with latent CMV infection. Finally, we provide a tutorial for creating, training and interpreting the tailored deep learning model for cytometry data using Keras and TensorFlow (github.com/hzc363/DeepLearningCyTOF).","",""
4,"R. Blything, Valerio Biscione, J. Bowers","A case for robust translation tolerance in humans and CNNs. A commentary on Han et al.",2020,"","","","",196,"2022-07-13 10:10:07","","","","",,,,,4,2.00,1,3,2,"Han et al. (2020) reported a behavioral experiment that assessed the extent to which the human visual system can identify novel images at unseen retinal locations (what the authors call ""intrinsic translation invariance"") and developed a novel convolutional neural network model (an Eccentricity Dependent Network or ENN) to capture key aspects of the behavioral results. Here we show that their analysis of behavioral data used inappropriate baseline conditions, leading them to underestimate intrinsic translation invariance. When the data are correctly interpreted they show near complete translation tolerance extending to 14° in some conditions, consistent with earlier work (Bowers et al., 2016) and more recent work Blything et al. (in press). We describe a simpler model that provides a better account of translation invariance.","",""
0,"S. Sengupta, C. Abbey, Kaiyan Li, M. Anastasio","Investigation of adversarial robust training for establishing interpretable CNN-based numerical observers",2022,"","","","",197,"2022-07-13 10:10:07","","10.1117/12.2613220","","",,,,,0,0.00,0,4,1,"The use of convolutional neural networks (CNNs) for establishing anthropomorphic numerical observers (ANOs) is being actively explored. In these data-driven approaches, CNNs are trained in a standard supervised way with human-labeled training data; hence, the anthropomorphic component of this procedure resides only in the training labels. However, it is well-known that such traditionally trained CNNs can rely on image features that are highly specific to the training distribution and may not align with features exploited by human perception. While being able to predict human observer performance under certain specified conditions, traditionally-trained CNNs lack the interpretability and robustness that may be desired for an ANO. To address this, in this work we investigate the use of an adversarial robust training strategy for training CNN-based observers. As recently demonstrated in the computer vision literature, this training strategy can result in CNNs that exploit more human-interpretable features than would be employed by a standard CNN. Robustly trained CNNs are systematically investigated for performing a signal-known-exactly (SKE) and background-known-statistically (BKS) binary detection task. Additionally, a differential evolution-based optimization procedure is developed to establish robustly trained CNNs that achieve a specified performance, which may provide a new approach to establishing ANOs.","",""
2,"Chenming Yang, Liang Zhou, Hui Wen, Zhiheng Zhou, Yue Wu","H-VGRAE: A Hierarchical Stochastic Spatial-Temporal Embedding Method for Robust Anomaly Detection in Dynamic Networks",2020,"","","","",198,"2022-07-13 10:10:07","","","","",,,,,2,1.00,0,5,2,"Detecting anomalous edges and nodes in dynamic networks is critical in various areas, such as social media, computer networks, and so on. Recent approaches leverage network embedding technique to learn how to generate node representations for normal training samples and detect anomalies deviated from normal patterns. However, most existing network embedding approaches learn deterministic node representations, which are sensitive to fluctuations of the topology and attributes due to the high flexibility and stochasticity of dynamic networks. In this paper, a stochastic neural network, named by Hierarchical Variational Graph Recurrent Autoencoder (H-VGRAE), is proposed to detect anomalies in dynamic networks by the learned robust node representations in the form of random variables. H-VGRAE is a semi-supervised model to capture normal patterns in training set by maximizing the likelihood of the adjacency matrix and node attributes via variational inference. Comparing with existing methods, H-VGRAE has three main advantages: 1) H-VGRAE learns robust node representations through stochasticity modeling and the extraction of multi-scale spatial-temporal features; 2) H-VGRAE can be extended to deep structure with the increase of the dynamic network scale; 3) the anomalous edge and node can be located and interpreted from the probabilistic perspective. Extensive experiments on four real-world datasets demonstrate the outperformance of H-VGRAE on anomaly detection in dynamic networks compared with state-of-the-art competitors.","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",199,"2022-07-13 10:10:07","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
1,"Zhe Xu","Robust Inference and Verification of Temporal Logic Classifier-in-the-loop Systems",2020,"","","","",200,"2022-07-13 10:10:07","","","","",,,,,1,0.50,1,1,2,"Autonomous systems embedded with machine learning modules often rely on deep neural networks for classifying different objects of interest in the environment or different actions or strategies to take for the system. Due to the non-linearity and high-dimensionality of deep neural networks, the interpretability of the autonomous systems is compromised. Besides, the machine learning methods in autonomous systems are mostly data-intensive and lack commonsense knowledge and reasoning that are natural to humans. In this paper, we propose the framework of temporal logic classifier-in-the-loop systems. The temporal logic classifiers can output different actions to take for an autonomous system based on the environment, such that the behavior of the autonomous system can satisfy a given temporal logic specification. Our approach is robust and provably-correct, as we can prove that the behavior of the autonomous system can satisfy a given temporal logic specification in the presence of (bounded) disturbances.","",""
