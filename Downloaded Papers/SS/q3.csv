Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
5,"U. Gadiraju, Jie Yang","What Can Crowd Computing Do for the Next Generation of AI Systems?",2020,"","","","",1,"2022-07-13 09:19:12","","","","",,,,,5,2.50,3,2,2,"The unprecedented rise in the adoption of artificial intelligence techniques and automation in many contexts is concomitant with shortcomings of such technology with respect to robustness, interpretability, usability, and trustworthiness. Crowd computing offers a viable means to leverage human intelligence at scale for data creation, enrichment, and interpretation, demonstrating a great potential to improve the performance of AI systems and increase the adoption of AI in general. Existing research and practice has mainly focused on leveraging crowd computing for training data creation. However, this perspective is rather limiting in terms of how AI can fully benefit from crowd computing. In this vision paper, we identify opportunities in crowd computing to propel better AI technology, and argue that to make such progress, fundamental problems need to be tackled from both computation and interaction standpoints. We discuss important research questions in both these themes, with an aim to shed light on the research needed to pave a future where humans and AI can work together seamlessly, while benefiting from each other.","",""
3,"M. Alcañiz, E. Olmos-Raya, L. Abad","[Use of virtual reality for neurodevelopmental disorders. A review of the state of the art and future agenda].",2019,"","","","",2,"2022-07-13 09:19:12","","","","",,,,,3,1.00,1,3,3,"To date, the diagnostic tools for autism spectrum disorder (ASD) have been mostly based on qualitative criteria from observational information in contexts with low ecological validity. We are witnessing a growing scientific activity that proposes the use of implicit measures for the evaluation and diagnosis of ASD. These measures are based on processes of a biological and unconscious nature, underlying the capacity of human cognition, and are obtained through the acquisition and treatment of brain, physiological and behavioral responses in order to obtain the behavioral structure of the ASD patient facing a stimulus. The complex relationship between physiological responses and the behavioral structure of the ASD patient requires the use of advanced techniques of signal processing based on cognitive computation. Artificial intelligence (AI) techniques, such as machine learning and neurocomputing applied to the analysis of psychophysiological signals, have demonstrated their robustness for the classification of complex cognitive constructs. Virtual reality (VR) is a tool that allows recreating real-life situations with high sensory fidelity, but at the same time individually controlling each of the situations and stimuli that influence human behavior. It also allows the measurement in real time of human reactions to such stimuli. This document analyzes the latest scientific and technological advances relevant to its applications in the diagnosis of ASD. We conclude that VR is a very valuable tool for ASD research, especially for the evaluation and diagnosis of complex skills and competencies.","",""
0,"Cheng-Lin Liu, A. Hussain, B. Luo, K. Tan, Yi Zeng, Zhaoxiang Zhang","Special Issue of BICS 2016",2018,"","","","",3,"2022-07-13 09:19:12","","10.1007/s12559-018-9551-3","","",,,,,0,0.00,0,6,4,"","",""
43,"Philip C. Jackson","Toward Human-Level Artificial Intelligence: Representation and Computation of Meaning in Natural Language",2019,"","","","",4,"2022-07-13 09:19:12","","","","",,,,,43,14.33,43,1,3,".................................................................................................. ix Preface .................................................................................................... xi","",""
58,"M. Smith","Getting value from artificial intelligence in agriculture",2020,"","","","",5,"2022-07-13 09:19:12","","10.1071/AN18522","","",,,,,58,29.00,58,1,2,"Artificial intelligence (AI) is beginning to live up to its promise of delivering real value, driven by recent advances in the availability of relevant data, computation and algorithms. In the present paper, I discuss the value to agriculture from AI over the next decade. The more immediate applications will be to improve precision information about what is happening on the farm by improving what is being detected and measured. A consequence of this are more accurate alerts to farmers. Another is an increased ability to understand why phenomena occur in farm systems, so as to improve their management. From improved data and understanding come improved predictions, enabling more optimal decisions about how to manage farm systems and stimulating the development of decision support and recommender systems. In many cases, robotics and automated systems will remove much of the need for human decision-making and improve farm efficiencies and farm health. Artificial intelligence will also be needed to enable organisations to harness the value of information distributed throughout supply chains, including farm data. Digital twins will also emerge as an important paradigm to improve how information about farm entities is organised to support decision-making. There are also likely to be negative impacts from AI, such as disruption to the roles and skills needed from farm workers, indicating the need to consider the social and ethical impacts of AI each time a new capability is introduced. I conclude that understanding these challenges more deeply tends to highlight new opportunities for positive change.","",""
0,"D. Kothari, Mayank Patel, Ajay Kumar Sharma","Implementation of Grey Scale Normalization in Machine Learning & Artificial Intelligence for Bioinformatics using Convolutional Neural Networks",2021,"","","","",6,"2022-07-13 09:19:12","","10.1109/ICICT50816.2021.9358549","","",,,,,0,0.00,0,3,1,"Machine Learning is a trending field nowadays and is very well known as an application of Artificial Intelligence (AI). Machine learning makes use of secure arithmetical algorithms to construct computers assignment in a constructive way without being unequivocally programmed. The algorithms acquire freedom of a participated value and estimate output for this by utilizing definite arithmetical methods. The key function of machine learning is to create smart machines that can visualize and work related to human beings. Artificial Intelligence has been witnessing a massive improvement in bridging the gap among the capabilities of humans and technologies. Similarly, one of the characteristics of the field were tried to combine the outstanding effects. A Convolution Neural Network (CNN) is a sort of Deep Learning algorithm which can obtain an input image, assign consequence to different aspects in the image and capable to differentiate one from the other. The pre-processing requirement in a CNN is lesser as compared to other classification algorithms. CNN has the capability to find out these filter's characteristics. Bioinformatics is a term that is a mixture of two terms bio and informatics. Bio means associated with biology and informatics means in a sequence of information. Thus bioinformatics is an area that deals with handing out and accepting biological statistics using the computational and arithmetical approaches. Machine Learning has added up to of applications in the area of bioinformatics. Machine Learning finds its submission in the subsequent subfields of bioinformatics. The aim of this article is to perform a grayscale normalization of a selected image and thereafter to reduce the effect of illumination differences. Normalization is considered so that CNN works in a faster manner. Different models are available but Keras model is selected to perform this task. Keras supports the style of data preparation for Image statistics via the Image Data Generator group and Application Programming Interface (API). The Image Data Generator group in Keras provides a matching set of techniques for scaling pixel standards in the image dataset subsequent to modeling. The Keras functional API provides an additional flexible approach for significant models. It particularly allows Identifving several input or output models as well as models that allocate layers.","",""
0,"Ewa Szewczyk","Artificial intelligence in administrative law and procedure",2021,"","","","",7,"2022-07-13 09:19:12","","10.13166/wsge//krcl6757","","",,,,,0,0.00,0,1,1,"Introduction Since J. McCarthy used the term ‘artificial intelligence’ (AI) in the 1950s, it has become a key concept in the technological development of all mankind. It has appeared in every area of life and science. AI has become established in areas of life that were previously thought to be reserved for decision-making by human beings. Artificial intelligence is based on the analysis of large volumes of data, used in algorithms. According to the modern definition, artificial intelligence encompasses the area of knowledge that includes fuzzy logic, evolutionary computation, neural networks, artificial life, and robotics, and one of its essential features is the ability to learn1 and take into account new circumstances when solving a given problem2. In other words, artificial intelligence is the ability of a machine to mimic or imitate human intelligence3. Algorithms are nothing new. They have been used in computer programmes for decades. Today, however, advanced algorithms have become digital robots – often sophisticated computer programmes (rather than physical entities) with the ability to adapt and ‘learn’. However, there is no denying that the unhindered development of AI technologies is marred with public concern and is by no means universally embraced, even though the Covid-19 pandemic has","",""
0,"R. Hariharan, P. He, C. Hickman, J. Chambost, C. Jacques, M. Hentschke, B. Cunegatto, C. Dutra, A. Drakeley, Q. Zhan, R. Miller, G. Verheyen, M. Rosselot, S. Loubersac, K. Kelley","P–165 Using Artificial Intelligence to Classify Embryo Shape: An International Perspective",2021,"","","","",8,"2022-07-13 09:19:12","","10.1093/humrep/deab130.164","","",,,,,0,0.00,0,15,1,"      Is a pre-trained machine learning algorithm able to accurately detect cellular arrangement in 4-cell embryos from a different continent?        Artificial Intelligence (AI) analysis of 4-cell embryo classification is transferable across clinics globally with 79% accuracy.        Previous studies observing four-cell human embryo configurations have demonstrated that non-tetrahedral embryos (embryos in which cells make contact with fewer than 3 other cells) are associated with compromised blastulation and implantation potential. Previous research by this study group has indicated the efficacy of AI models in classification of tetrahedral and non-tetrahedral embryos with 87% accuracy, with a database comprising 2 clinics both from the same country (Brazil). This study aims to evaluate the transferability and robustness of this model on blind test data from a different country (France).        The study was a retrospective cohort analysis in which 909 4-cell embryo images (“tetrahedral”, n = 749; “non-tetrahedral”, n = 160) were collected from 3 clinics (2 Brazilian, 1 French). All embryos were captured at the central focal plane using Embryoscope™ time-lapse incubators. The training data consisted solely of embryo images captured in Brazil (586 tetrahedral; 87 non-tetrahedral) and the test data consisted exclusively of embryo images captured in France (163 tetrahedral; 72 non-tetrahedral).        The embryo images were labelled as either “tetrahedral” or “non-tetrahedral” at their respective clinics. Annotations were then validated by three operators. A ResNet–50 neural network model pretrained on ImageNet was fine-tuned on the training dataset to predict the correct annotation for each image. We used the cross entropy loss function and the RMSprop optimiser (lr = 1e–5). Simple data augmentations (flips and rotations) were used during the training process to help counteract class imbalances.        Our model was capable of classifying embryos in the blind French test set with 79% accuracy when trained with the Brazilian data. The model had sensitivity of 91% and 51% for tetrahedral and non-tetrahedral embryos respectively; precision was 81% and 73%; F1 score was 86% and 60%; and AUC was 0.61 and 0.64. This represents a 10% decrease in accuracy compared to when the model both trained and tested on different data from the same clinics.        Although strict inclusion and exclusion criteria were used, inter-operator variability may affect the pre-processing stage of the algorithm. Moreover, as only one focal plane was used, ambiguous cases were interpoloated and further annotated. Analysing embryos at multiple focal planes may prove crucial in improving the accuracy of the model.  Wider implications of the findings: Though the use of machine learning models in the analysis of embryo imagery has grown in recent years, there has been concern over their robustness and transferability. While previous results have demonstrated the utility of locally-trained models, our results highlight the potential for models to be implemented across different clinics.        Not applicable ","",""
0,"A. D. W. Sumari, I. Syamsiana","A Simple Introduction to Cognitive Artificial Intelligence’s Knowledge Growing System",2021,"","","","",9,"2022-07-13 09:19:12","","10.1109/DATABIA53375.2021.9650179","","",,,,,0,0.00,0,2,1,"Knowledge Growing System (KGS) since its introduction in 2009, has been stated as the foundation of Cognitive Artificial Intelligence (CAI). Because of its computation mechanism simplicity and it does not burden the computation resources, various use-cases have applied KGS to solve their problems. The KGS development was inspired by the growing of knowledge within human brain when thinking during carrying out interactions to a phenomenon in its environment. KGS learns to the data received at that time and at the next series of time that are sensed and perceived by its sensory organs, and uses them to generate knowledge. By combining approaches and techniques from cognitive psychology, mathematics, social science, and AI fields, we created simple mathematics formulas called ASSA2010 (Arwin Sumari-Suwandi Ahmad year 2010) information-inferencing fusion method for KGS’ knowledge growing mechanism. In this article, we deliver a simple introduction to KGS and also some of its utilizations for humankind.","",""
0,"Aabid Ali","Artificial Intelligence",2021,"","","","",10,"2022-07-13 09:19:12","","10.1017/9781009036719.007","","",,,,,0,0.00,0,1,1,"Artificial Intelligence may be defined as intelligence displayed by machines, systems or agents or by entities other than living beings. Apparently, the term seems simple but the definition bears deeper connotations. The terms intelligence and creativity have long been the prerogatives associated with the humans or have been the privileges enjoyed by them since the dawn of the creation. The views ‘creativity is computation’ or ‘cognition is computation’ and ‘mind as machine’ has offset the traditional theories, assumptions and interpretations held so far in the philosophy and theory of mind. AI’s push to impart intelligence to non-human entities to enable them to behave intelligently and creatively or as Boden would put it “to make computers do the sort of things that minds can do” (Boden 1) has challenged the very traditional fabric of our perception and comprehension, conception and construction related to our learning and living dispensations.","",""
1,"C. Bormann, M. Kanakasabapathy, Prudhvi Thirumalaraju, I. Dimitriadis, I. Souter, K. Hammer, H. Shafiee","O-125 Development of an artificial intelligence embryo witnessing system to accurately track and identify patient specific embryos in a human IVF laboratory",2021,"","","","",11,"2022-07-13 09:19:12","","10.1093/humrep/deab126.050","","",,,,,1,1.00,0,7,1,"      Can convolutional neural networks (CNN) be used as a witnessing system to accurately track and identify patient specific embryos at the cleavage stage of development?        We developed the first artificial intelligence driven witnessing system to accurately track cleavage and blastocyst stage embryos in a human ART laboratory.        There are reports of human errors in embryo tracking that have led to the births of children with different genetic makeup than their birth parents. Clinical practices rely on manual identification, barcodes or radio-frequency identification technology to track embryos. These systems are designed to track culture dishes but are unable to monitor developing embryos within the dish to help ensure an error-free patient match. Previously, we developed an AI witnessing system to track blastocysts with 100% accuracy. The goal of this study was to determine whether an AI witnessing system could be developed that accurately tracks cleavage stage embryos.        A pre-developed deep neural network technology was first trained and tested on 4944 embryos images. The algorithm processed embryo images for each patient and produced a unique key that was associated with the patient ID at 60 hpi, which formed our library. When the algorithm evaluated embryos at 64 hpi it generated another key that was matched with the patient’s unique key available in the library.        A total of 3068 embryos from 412 patients were examined by the CNN at both 60 hpi and 64 hpi. These timepoints were chosen as they reflect the time our laboratory evaluates Day 3 embryos (60 hpi) and the time we move them to another dish and prepare them for transfer (64 hpi). The patient cohorts ranged from 3-12 embryos per patient.        The accuracy of the CNN in correctly matching the patient identification with the patient embryo cohort was 100% (CI: 99.1% to 100.0%, n = 412).        Limitations of this study include that all embryos were imaged under identical conditions and within the same EmbryoScope. Additionally, this study only examined fresh Day 3 embryos cultured over a span of 4 hours. Future studies should include images of fresh and frozen/thawed embryos captured using different imaging systems.        This study describes the first artificial intelligence-based approach for cleavage stage embryo tracking and patient specimen identification in the IVF laboratory. This technology offers a robust witnessing step based on unique morphological features that are specific to each individual embryo.        This work was partially supported by the Brigham Precision Medicine Developmental Award (Brigham Precision Medicine Program, Brigham and Women’s Hospital), Partners Innovation Discovery Grant (Partners Healthcare), and R01AI118502, and R01AI138800. ","",""
0,"T. Leung, C. L. Lee, P. Chiu","P–064 Application of an artificial intelligence model for morphologic prediction of fertilization-competent human spermatozoa",2021,"","","","",12,"2022-07-13 09:19:12","","10.1093/humrep/deab130.063","","",,,,,0,0.00,0,3,1,"      What is the role of artificial intelligence in selecting fertilization-competent human spermatozoa according to their morphological characteristics? Summary answer: The established AI model in this study can be potentially used to select semen samples with superior fertilization potential in clinical settings.        Defective spermatozoa-zona pellucida (ZP) interaction causes subfertility and is a major cause of low IVF fertilization rates. While ICSI benefits patients with defective spermatozoa-ZP binding, a standard method to identify such patients prior to conventional IVF is lacking. The application of artificial intelligence to sperm morphology analysis has become a topic of growing interest owing to the fact that the conventional assessment is highly subjective and time-consuming. Deep-learning, a core element of artificial intelligence (AI), incorporates the convolutional neural networks (CNN) to process all the data composing a digital image through successive layers to identify the underlying pattern.        The fertilization-competent spermatozoa were isolated according to their binding ability to the ZP. The ZP-bound and -unbound spermatozoa were collected for functional assays and to establish an AI model for morphologic prediction of sperm fertilization potential. Human spermatozoa (n = 289) were isolated from normozoospermic samples. Human oocytes (n = 562) were collected from an assisted reproduction program in Hong Kong. Sample collection has been ongoing and will continue until the end of this study in November 2021.        Sperm-ZP binding assay was employed to collect ZP-bound and -unbound spermatozoa. The fertilization potential and genetic quality of the collected spermatozoa were evaluated by our established protocols. Diff-Quik- stained images of ZP-bound and -unbound spermatozoa were collected respectively for the establishment of an AI model. A novel algorithm for sperm image transformation and segmentation was developed to pre-process the images. CNN architecture was then applied on these pre-processed images for feature extraction and model training.        Our result showed that the sperm-ZP binding assay had no detrimental effect on sperm viability when compared with the raw samples and unbound-sperm subpopulations. ZP-bound spermatozoa were found with statistically higher acrosome reaction rates, improved DNA integrity, better morphology, lower protamine deficiency and higher methylation level when compared with the unbound spermatozoa. A deep-learning model was trained and validated by analyzing a total of 1,334 and 885 of ZP-bound/unbound spermatozoa to evaluate the predictive power of sperm morphology for ZP binding ability. Our newly trained AI-based model showed initial success in classifying the ZP-bound/ unbound spermatozoa according to their morphological characteristics with high accuracy of 85% and low computational complexity.        This sperm selection method requires micromanipulation and relatively long processing time to recover ZP-bound spermatozoa. In addition to limited availability, the use of human materials may result in interassay variations affecting the reproducibility of this method among laboratories.  Wider implications of the findings: In light of current findings, AI-based sperm selection method may provide high predictive values of sperm fertilization potential for clinical purposes. This method is particularly applicable to patients who had poor fertilization outcomes after conventional IVF treatments or those with high degree of defective sperm-ZP binding ability.        Not applicable ","",""
0,"T. Leung, C. L. Lee, P. Chiu","P-064 Application of an artificial intelligence model for morphologic prediction of fertilization-competent human spermatozoa",2021,"","","","",13,"2022-07-13 09:19:12","","10.1093/humrep/deab127.043","","",,,,,0,0.00,0,3,1,"      What is the role of artificial intelligence in selecting fertilization-competent human spermatozoa according to their morphological characteristics?         The established AI model in this study can be potentially used to select semen samples with superior fertilization potential in clinical settings.        Defective spermatozoa-zona pellucida (ZP) interaction causes subfertility and is a major cause of low IVF fertilization rates. While ICSI benefits patients with defective spermatozoa-ZP binding, a standard method to identify such patients prior to conventional IVF is lacking. The application of artificial intelligence to sperm morphology analysis has become a topic of growing interest owing to the fact that the conventional assessment is highly subjective and time-consuming. Deep-learning, a core element of artificial intelligence (AI), incorporates the convolutional neural networks (CNN) to process all the data composing a digital image through successive layers to identify the underlying pattern.        The fertilization-competent spermatozoa were isolated according to their binding ability to the ZP. The ZP-bound and -unbound spermatozoa were collected for functional assays and to establish an AI model for morphologic prediction of sperm fertilization potential. Human spermatozoa (n = 289) were isolated from normozoospermic samples. Human oocytes (n = 562) were collected from an assisted reproduction program in Hong Kong. Sample collection has been ongoing and will continue until the end of this study in November 2021.        Sperm-ZP binding assay was employed to collect ZP-bound and -unbound spermatozoa. The fertilization potential and genetic quality of the collected spermatozoa were evaluated by our established protocols. Diff-Quik- stained images of ZP-bound and -unbound spermatozoa were collected respectively for the establishment of an AI model. A novel algorithm for sperm image transformation and segmentation was developed to pre-process the images. CNN architecture was then applied on these pre-processed images for feature extraction and model training.        Our result showed that the sperm-ZP binding assay had no detrimental effect on sperm viability when compared with the raw samples and unbound-sperm subpopulations. ZP-bound spermatozoa were found with statistically higher acrosome reaction rates, improved DNA integrity, better morphology, lower protamine deficiency and higher methylation level when compared with the unbound spermatozoa. A deep-learning model was trained and validated by analyzing a total of 1,334 and 885 of ZP-bound/unbound spermatozoa to evaluate the predictive power of sperm morphology for ZP binding ability. Our newly trained AI-based model showed initial success in classifying the ZP-bound/ unbound spermatozoa according to their morphological characteristics with high accuracy of 85% and low computational complexity.        This sperm selection method requires micromanipulation and relatively long processing time to recover ZP-bound spermatozoa. In addition to limited availability, the use of human materials may result in interassay variations affecting the reproducibility of this method among laboratories.        In light of current findings, AI-based sperm selection method may provide high predictive values of sperm fertilization potential for clinical purposes. This method is particularly applicable to patients who had poor fertilization outcomes after conventional IVF treatments or those with high degree of defective sperm-ZP binding ability.         not applicable ","",""
30,"Xinqin Liao, Wei‐ming Song, X. Zhang, Chaoqun Yan, Tianliang Li, Hongliang Ren, Cunzhi Liu, Yongtian Wang, Yuanjin Zheng","A bioinspired analogous nerve towards artificial intelligence",2020,"","","","",14,"2022-07-13 09:19:12","","10.1038/s41467-019-14214-x","","",,,,,30,15.00,3,9,2,"","",""
24,"Rainer Mühlhoff","Human-aided artificial intelligence: Or, how to run large computations in human brains? Toward a media sociology of machine learning",2019,"","","","",15,"2022-07-13 09:19:12","","10.1177/1461444819885334","","",,,,,24,8.00,24,1,3,"Today, artificial intelligence (AI), especially machine learning, is structurally dependent on human participation. Technologies such as deep learning (DL) leverage networked media infrastructures and human-machine interaction designs to harness users to provide training and verification data. The emergence of DL is therefore based on a fundamental socio-technological transformation of the relationship between humans and machines. Rather than simulating human intelligence, DL-based AIs capture human cognitive abilities, so they are hybrid human-machine apparatuses. From a perspective of media philosophy and social-theoretical critique, I differentiate five types of “media technologies of capture” in AI apparatuses and analyze them as forms of power relations between humans and machines. Finally, I argue that the current hype about AI implies a relational and distributed understanding of (human/artificial) intelligence, which I categorize under the term “cybernetic AI.” This form of AI manifests in socio-technological apparatuses that involve new modes of subjectivation, social control, and digital labor.","",""
13,"Edgar Bermudez Contreras, B. J. Clark, A. Wilber","The Neuroscience of Spatial Navigation and the Relationship to Artificial Intelligence",2020,"","","","",16,"2022-07-13 09:19:12","","10.3389/fncom.2020.00063","","",,,,,13,6.50,4,3,2,"Recent advances in artificial intelligence (AI) and neuroscience are impressive. In AI, this includes the development of computer programs that can beat a grandmaster at GO or outperform human radiologists at cancer detection. A great deal of these technological developments are directly related to progress in artificial neural networks—initially inspired by our knowledge about how the brain carries out computation. In parallel, neuroscience has also experienced significant advances in understanding the brain. For example, in the field of spatial navigation, knowledge about the mechanisms and brain regions involved in neural computations of cognitive maps—an internal representation of space—recently received the Nobel Prize in medicine. Much of the recent progress in neuroscience has partly been due to the development of technology used to record from very large populations of neurons in multiple regions of the brain with exquisite temporal and spatial resolution in behaving animals. With the advent of the vast quantities of data that these techniques allow us to collect there has been an increased interest in the intersection between AI and neuroscience, many of these intersections involve using AI as a novel tool to explore and analyze these large data sets. However, given the common initial motivation point—to understand the brain—these disciplines could be more strongly linked. Currently much of this potential synergy is not being realized. We propose that spatial navigation is an excellent area in which these two disciplines can converge to help advance what we know about the brain. In this review, we first summarize progress in the neuroscience of spatial navigation and reinforcement learning. We then turn our attention to discuss how spatial navigation has been modeled using descriptive, mechanistic, and normative approaches and the use of AI in such models. Next, we discuss how AI can advance neuroscience, how neuroscience can advance AI, and the limitations of these approaches. We finally conclude by highlighting promising lines of research in which spatial navigation can be the point of intersection between neuroscience and AI and how this can contribute to the advancement of the understanding of intelligent behavior.","",""
0,"F. LeRon Shults","Progress in simulating human geography: Assemblage theory and the practice of multi-agent artificial intelligence modeling",2021,"","","","",17,"2022-07-13 09:19:12","","10.1177/03091325211059567","","",,,,,0,0.00,0,1,1,"Over the last few years, there has been an explosion of interest in assemblage theory among human geographers. During this same period, a growing number of scholars in the field have utilized computational methodologies to simulate the complex adaptive systems they study. However, very little attention has been paid to the connections between these two developments. This article outlines those connections and argues that more explicitly integrating assemblage theory and computer modeling can encourage a more robust philosophical understanding of both and facilitate progress in scientific research on the ways in which complex socio-material systems form and transform.","",""
17,"Yi-Shan Lin, Wen-Chuan Lee, Z. B. Celik","What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors",2020,"","","","",18,"2022-07-13 09:19:12","","10.1145/3447548.3467213","","",,,,,17,8.50,6,3,2,"EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.","",""
24,"Martina Gurgitano, S. A. Angileri, G. Rodà, Alessandro Liguori, M. Pandolfi, A. Ierardi, B. Wood, G. Carrafiello","Interventional Radiology ex-machina: impact of Artificial Intelligence on practice",2021,"","","","",19,"2022-07-13 09:19:12","","10.1007/s11547-021-01351-x","","",,,,,24,24.00,3,8,1,"","",""
24,"Maxime Sermesant, H. Delingette, H. Cochet, P. Jaïs, N. Ayache","Applications of artificial intelligence in cardiovascular imaging",2021,"","","","",20,"2022-07-13 09:19:12","","10.1038/s41569-021-00527-2","","",,,,,24,24.00,5,5,1,"","",""
18,"Saman Tauqir","Is Artificial Intelligence Transforming Dentistry Today?",2021,"","","","",21,"2022-07-13 09:19:12","","10.37762/jgmds.8-4.263","","",,,,,18,18.00,18,1,1,"Since the birth of science, the most fascinating structure of the human body is the human brain.  Over the past centuries’ researchers have been developing the latest technologies to imitate and explore how the human brain functions. However, to develop a machine that thinks like a human brain is still a dream for researchers. Aristotle’s early efforts to devise logical thinking via his syllogisms (a three-part deductive reasoning) were a source of inspiration for modern computers and technologies1. In the1950, Alan Turing designed a machine to decode encrypted messages, which was a breakthrough of super computers in the days of yore. He designed the “Turing Test” which was coined to assess whether a computer could exhibit intelligence better known as “artificial intelligence” (AI) today2. AI is “a field of science and engineering concerned with the computational understanding of what is commonly called intelligent behavior, and with the creation of artifacts that exhibit such behaviour”3.  Since 1980, AI has come a long way, virtual reality is being used in dental education these days to create real life situations and promote clinical work on simulators to eliminate risk factors associated with training on live patients. Recently artificial intelligence has been integrated with tutoring systems like “Unified Medical Language System” (UMLS), which have resulted in a better quality of feedback, which the preclinical virtual patients provide to the students4,5. This interactive phase helps students to evaluate their clinical skills and compare their skills with the standard ones, thus creating an ideal and high-quality training environment. Studies have been carried out regarding the efficacy of AI systems, which have stipulated that preclinical students build higher competencies than with the use of traditional simulator units6-8.  Currently AI inbuilt virtual dental assistants are present in the market. They can execute various chair side tasks with greater accuracy and less manpower ensuring minimum error during the procedures. In the world of implantology and maxillofacial surgery AI helps plan and prepare surgeries with smallest details forgoing actual surgery. Some exceptional uses of AI include robotic surgeries in the field of maxillofacial surgery and bioprinting (where tissues and organs can be reconstructed in thin layers)9. The field of AI has flourished to great extent in the past decade; AI systems are an aid to the field of dentistry and dental education.   This narrative attempts to explain possible AI-based applications in the future, it can be used for dental diagnosis, planning out treatments, conducting image analysis, and record keeping. AI-based technologies streamline and reduce laborious workforce to routine tasks, it ensures dental procedures are possible at a lower cost and ultimately makes predictive, preventive, and participatory dentistry possible. The use of AI in dental procedures needs to be guaranteed; its application with human oversight and evidence-based dentistry shall be expected. Dental education needs to be introduced to clinical AI solutions by promoting digital literacy in the future dental liveware.","",""
8,"M. Choudhury, Min Kyung Lee, Haiyi Zhu, David A. Shamma","Introduction to this special issue on unifying human computer interaction and artificial intelligence",2020,"","","","",22,"2022-07-13 09:19:12","","10.1080/07370024.2020.1744146","","",,,,,8,4.00,2,4,2,"McCarthy (1998) defined Artificial Intelligence (AI) as both “the science and engineering of in- telligent machines, especially computer programs” and the “computational part of the ability to achi...","",""
13,"A. Lin, Márton Kolossváry, M. Motwani, I. Išgum, P. Maurovich-Horvat, P. Slomka, D. Dey","Artificial Intelligence in Cardiovascular Imaging for Risk Stratification in Coronary Artery Disease.",2021,"","","","",23,"2022-07-13 09:19:12","","10.1148/ryct.2021200512","","",,,,,13,13.00,2,7,1,"Artificial intelligence (AI) describes the use of computational techniques to perform tasks that normally require human cognition. Machine learning and deep learning are subfields of AI that are increasingly being applied to cardiovascular imaging for risk stratification. Deep learning algorithms can accurately quantify prognostic biomarkers from image data. Additionally, conventional or AI-based imaging parameters can be combined with clinical data using machine learning models for individualized risk prediction. The aim of this review is to provide a comprehensive review of state-of-the-art AI applications across various noninvasive imaging modalities (coronary artery calcium scoring CT, coronary CT angiography, and nuclear myocardial perfusion imaging) for the quantification of cardiovascular risk in coronary artery disease. © RSNA, 2021.","",""
18,"R. Slart, M. Williams, L. Juarez-Orozco, C. Rischpler, M. Dweck, A. Glaudemans, A. Gimelli, P. Georgoulias, O. Gheysens, O. Gaemperli, G. Habib, R. Hustinx, B. Cosyns, H. Verberne, F. Hyafil, P. Erba, M. Lubberink, P. Slomka, I. Išgum, D. Visvikis, Márton Kolossváry, A. Saraste","Position paper of the EACVI and EANM on artificial intelligence applications in multimodality cardiovascular imaging using SPECT/CT, PET/CT, and cardiac CT",2021,"","","","",24,"2022-07-13 09:19:12","","10.1007/s00259-021-05341-z","","",,,,,18,18.00,2,22,1,"","",""
22,"M. J. Mrowinski, P. Fronczak, A. Fronczak, M. Ausloos, O. Nedić","Artificial intelligence in peer review: How can evolutionary computation support journal editors?",2017,"","","","",25,"2022-07-13 09:19:12","","10.1371/journal.pone.0184711","","",,,,,22,4.40,4,5,5,"With the volume of manuscripts submitted for publication growing every year, the deficiencies of peer review (e.g. long review times) are becoming more apparent. Editorial strategies, sets of guidelines designed to speed up the process and reduce editors’ workloads, are treated as trade secrets by publishing houses and are not shared publicly. To improve the effectiveness of their strategies, editors in small publishing groups are faced with undertaking an iterative trial-and-error approach. We show that Cartesian Genetic Programming, a nature-inspired evolutionary algorithm, can dramatically improve editorial strategies. The artificially evolved strategy reduced the duration of the peer review process by 30%, without increasing the pool of reviewers (in comparison to a typical human-developed strategy). Evolutionary computation has typically been used in technological processes or biological ecosystems. Our results demonstrate that genetic programs can improve real-world social systems that are usually much harder to understand and control than physical systems.","",""
10,"A. S. Ahmad, A. D. W. Sumari","Cognitive artificial intelligence: Brain-inspired intelligent computation in artificial intelligence",2017,"","","","",26,"2022-07-13 09:19:12","","10.1109/SAI.2017.8252094","","",,,,,10,2.00,5,2,5,"Computation occurred within human brain is very much awesome and is not possible to be emulated 100% exactly in Artificial Intelligence (AI) method-based machines. What scientists did and have been done so far up to now are to try to model it as close as to what exactly occurs within the brain. Human brain has an awesome mechanism in performing computation with the end result is new knowledge and human uses the knowledge to actuate his organs. In this paper we will show a new approach for emulating the computation occured within human brain to obtain new knowledge based on the inputs sensed by the system's sensory system taken from the environment. When this process is carried out recursively, the system's knowledge becomes newer and newer, and it is called as knowledge growing. This approach is designed for an agent that has ability to think and act rationally like human. Our cognitive modelling approach is resulted in a model of human information processing and a technique to obtain the most maximum performance should be taken by the cognitive agent. This method is called as A3S (Arwin-Adang-Aciek-Sembiring), the agent is called as Knowledge-Growing System (KGS) and this brain-inspired method opens a new perspective in AI that we call as Cognitive Artificial Intelligence (CAI).","",""
7,"Efrén Pérez Santín, Raquel Rodríguez Solana, María de las Nieves González García, M. D. García Suárez, Gerardo David Blanco Díaz, María Dolores Cima Cabal, J. Moreno Rojas, J. I. López Sánchez","Toxicity prediction based on artificial intelligence: A multidisciplinary overview",2021,"","","","",27,"2022-07-13 09:19:12","","10.1002/wcms.1516","","",,,,,7,7.00,1,8,1,"The use and production of chemical compounds are subjected to strong legislative pressure. Chemical toxicity and adverse effects derived from exposure to chemicals are key regulatory aspects for a multitude of industries, such as chemical, pharmaceutical, or food, due to direct harm to humans, animals, plants, or the environment. Simultaneously, there are growing demands on the authorities to replace traditional in vivo toxicity tests carried out on laboratory animals (e.g., European Union REACH/3R principles, Tox21 and ToxCast by the U.S. government, etc.) with in silica computational models. This is not only for ethical aspects, but also because of its greater economic and time efficiency, as well as more recently because of their superior reliability and robustness than in vivo tests, mainly since the entry into the scene of artificial intelligence (AI)‐based models, promoting and setting the necessary requirements that these new in silico methodologies must meet. This review offers a multidisciplinary overview of the state of the art in the application of AI‐based methodologies for the fulfillment of regulatory‐related toxicological issues.","",""
81,"S. Ullman","Using neuroscience to develop artificial intelligence",2019,"","","","",28,"2022-07-13 09:19:12","","10.1126/science.aau6595","","",,,,,81,27.00,81,1,3,"Combining deep learning with brain-like innate structures may guide network models toward human-like learning When the mathematician Alan Turing posed the question “Can machines think?” in the first line of his seminal 1950 paper that ushered in the quest for artificial intelligence (AI) (1), the only known systems carrying out complex computations were biological nervous systems. It is not surprising, therefore, that scientists in the nascent field of AI turned to brain circuits as a source for guidance. One path that was taken since the early attempts to perform intelligent computation by brain-like circuits (2), and which led recently to remarkable successes, can be described as a highly reductionist approach to model cortical circuitry. In its basic current form, known as a “deep network” (or deep net) architecture, this brain-inspired model is built from successive layers of neuron-like elements, connected by adjustable weights, called “synapses” after their biological counterparts (3). The application of deep nets and related methods to AI systems has been transformative. They proved superior to previously known methods in central areas of AI research, including computer vision, speech recognition and production, and playing complex games. Practical applications are already in broad use, in areas such as computer vision and speech and text translation, and large-scale efforts are under way in many other areas. Here, I discuss how additional aspects of brain circuitry could supply cues for guiding network models toward broader aspects of cognition and general AI.","",""
63,"M. VerMilyea, J. Hall, S. Diakiw, A. Johnston, T. Nguyen, D. Perugini, A. Miller, A. Picou, A. P. Murphy, M. Perugini","Development of an artificial intelligence-based assessment model for prediction of embryo viability using static images captured by optical light microscopy during IVF",2020,"","","","",29,"2022-07-13 09:19:12","","10.1093/humrep/deaa013","","",,,,,63,31.50,6,10,2,"Abstract STUDY QUESTION Can an artificial intelligence (AI)-based model predict human embryo viability using images captured by optical light microscopy? SUMMARY ANSWER We have combined computer vision image processing methods and deep learning techniques to create the non-invasive Life Whisperer AI model for robust prediction of embryo viability, as measured by clinical pregnancy outcome, using single static images of Day 5 blastocysts obtained from standard optical light microscope systems. WHAT IS KNOWN ALREADY Embryo selection following IVF is a critical factor in determining the success of ensuing pregnancy. Traditional morphokinetic grading by trained embryologists can be subjective and variable, and other complementary techniques, such as time-lapse imaging, require costly equipment and have not reliably demonstrated predictive ability for the endpoint of clinical pregnancy. AI methods are being investigated as a promising means for improving embryo selection and predicting implantation and pregnancy outcomes. STUDY DESIGN, SIZE, DURATION These studies involved analysis of retrospectively collected data including standard optical light microscope images and clinical outcomes of 8886 embryos from 11 different IVF clinics, across three different countries, between 2011 and 2018. PARTICIPANTS/MATERIALS, SETTING, METHODS The AI-based model was trained using static two-dimensional optical light microscope images with known clinical pregnancy outcome as measured by fetal heartbeat to provide a confidence score for prediction of pregnancy. Predictive accuracy was determined by evaluating sensitivity, specificity and overall weighted accuracy, and was visualized using histograms of the distributions of predictions. Comparison to embryologists’ predictive accuracy was performed using a binary classification approach and a 5-band ranking comparison. MAIN RESULTS AND THE ROLE OF CHANCE The Life Whisperer AI model showed a sensitivity of 70.1% for viable embryos while maintaining a specificity of 60.5% for non-viable embryos across three independent blind test sets from different clinics. The weighted overall accuracy in each blind test set was >63%, with a combined accuracy of 64.3% across both viable and non-viable embryos, demonstrating model robustness and generalizability beyond the result expected from chance. Distributions of predictions showed clear separation of correctly and incorrectly classified embryos. Binary comparison of viable/non-viable embryo classification demonstrated an improvement of 24.7% over embryologists’ accuracy (P = 0.047, n = 2, Student’s t test), and 5-band ranking comparison demonstrated an improvement of 42.0% over embryologists (P = 0.028, n = 2, Student’s t test). LIMITATIONS, REASONS FOR CAUTION The AI model developed here is limited to analysis of Day 5 embryos; therefore, further evaluation or modification of the model is needed to incorporate information from different time points. The endpoint described is clinical pregnancy as measured by fetal heartbeat, and this does not indicate the probability of live birth. The current investigation was performed with retrospectively collected data, and hence it will be of importance to collect data prospectively to assess real-world use of the AI model. WIDER IMPLICATIONS OF THE FINDINGS These studies demonstrated an improved predictive ability for evaluation of embryo viability when compared with embryologists’ traditional morphokinetic grading methods. The superior accuracy of the Life Whisperer AI model could lead to improved pregnancy success rates in IVF when used in a clinical setting. It could also potentially assist in standardization of embryo selection methods across multiple clinical environments, while eliminating the need for complex time-lapse imaging equipment. Finally, the cloud-based software application used to apply the Life Whisperer AI model in clinical practice makes it broadly applicable and globally scalable to IVF clinics worldwide. STUDY FUNDING/COMPETING INTEREST(S) Life Whisperer Diagnostics, Pty Ltd is a wholly owned subsidiary of the parent company, Presagen Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation and Startup Fund (RCSF). ‘In kind’ support and embryology expertise to guide algorithm development were provided by Ovation Fertility. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. Presagen has filed a provisional patent for the technology described in this manuscript (52985P pending). A.P.M. owns stock in Life Whisperer, and S.M.D., A.J., T.N. and A.P.M. are employees of Life Whisperer.","",""
9,"David A. Joyner, D. Bedwell, Chris Graham, Warren Lemmon, Óscar Martínez, Ashok K. Goel","Using Human Computation to Acquire Novel Methods for Addressing Visual Analogy Problems on Intelligence Tests",2015,"","","","",30,"2022-07-13 09:19:12","","","","",,,,,9,1.29,2,6,7,"The Raven's Progressive Matrices (RPM) test is a commonly used test of intelligence. The literature suggests a variety of problem-solving methods for addressing RPM problems. For a graduate-level artificial intelligence class in Fall 2014, we asked students to develop intelligent agents that could address 123 RPM-inspired problems, essentially crowdsourcing RPM problem solving. The students in the class submitted 224 agents that used a wide variety of problem-solving methods. In this paper, we first report on the aggregate results of those 224 agents on the 123 problems, then focus specifically on four of the most creative, novel, and effective agents in the class. We find that the four agents, using four very different problem-solving methods, were all able to achieve significant success. This suggests the RPM test may be amenable to a wider range of problem-solving methods than previously reported. It also suggests that human computation might be an effective strategy for collecting a wide variety of methods for creative tasks.","",""
0,"S. Diakiw, J. Hall, M. VerMilyea, J. Amin, J. Aizpurua, L. Giardini, Y. G. Briones, A. Lim, M. Dakka, T. Nguyen, D. Perugini, M. Perugini","Development of an artificial intelligence model for predicting the likelihood of human embryo euploidy based on blastocyst images from multiple imaging systems during IVF.",2022,"","","","",31,"2022-07-13 09:19:12","","10.1093/humrep/deac131","","",,,,,0,0.00,0,12,1,"STUDY QUESTION Can an artificial intelligence (AI) model predict human embryo ploidy status using static images captured by optical light microscopy?   SUMMARY ANSWER Results demonstrated predictive accuracy for embryo euploidy and showed a significant correlation between AI score and euploidy rate, based on assessment of images of blastocysts at Day 5 after IVF.   WHAT IS KNOWN ALREADY Euploid embryos displaying the normal human chromosomal complement of 46 chromosomes are preferentially selected for transfer over aneuploid embryos (abnormal complement), as they are associated with improved clinical outcomes. Currently, evaluation of embryo genetic status is most commonly performed by preimplantation genetic testing for aneuploidy (PGT-A), which involves embryo biopsy and genetic testing. The potential for embryo damage during biopsy, and the non-uniform nature of aneuploid cells in mosaic embryos, has prompted investigation of additional, non-invasive, whole embryo methods for evaluation of embryo genetic status.   STUDY DESIGN, SIZE, DURATION A total of 15 192 blastocyst-stage embryo images with associated clinical outcomes were provided by 10 different IVF clinics in the USA, India, Spain and Malaysia. The majority of data were retrospective, with two additional prospectively collected blind datasets provided by IVF clinics using the genetics AI model in clinical practice. Of these images, a total of 5050 images of embryos on Day 5 of in vitro culture were used for the development of the AI model. These Day 5 images were provided for 2438 consecutively treated women who had undergone IVF procedures in the USA between 2011 and 2020. The remaining images were used for evaluation of performance in different settings, or otherwise excluded for not matching the inclusion criteria.   PARTICIPANTS/MATERIALS, SETTING, METHODS The genetics AI model was trained using static 2-dimensional optical light microscope images of Day 5 blastocysts with linked genetic metadata obtained from PGT-A. The endpoint was ploidy status (euploid or aneuploid) based on PGT-A results. Predictive accuracy was determined by evaluating sensitivity (correct prediction of euploid), specificity (correct prediction of aneuploid) and overall accuracy. The Matthew correlation coefficient and receiver-operating characteristic curves and precision-recall curves (including AUC values), were also determined. Performance was also evaluated using correlation analyses and simulated cohort studies to evaluate ranking ability for euploid enrichment.   MAIN RESULTS AND THE ROLE OF CHANCE Overall accuracy for the prediction of euploidy on a blind test dataset was 65.3%, with a sensitivity of 74.6%. When the blind test dataset was cleansed of poor quality and mislabeled images, overall accuracy increased to 77.4%. This performance may be relevant to clinical situations where confounding factors, such as variability in PGT-A testing, have been accounted for. There was a significant positive correlation between AI score and the proportion of euploid embryos, with very high scoring embryos (9.0-10.0) twice as likely to be euploid than the lowest-scoring embryos (0.0-2.4). When using the genetics AI model to rank embryos in a cohort, the probability of the top-ranked embryo being euploid was 82.4%, which was 26.4% more effective than using random ranking, and ∼13-19% more effective than using the Gardner score. The probability increased to 97.0% when considering the likelihood of one of the top two ranked embryos being euploid, and the probability of both top two ranked embryos being euploid was 66.4%. Additional analyses showed that the AI model generalized well to different patient demographics and could also be used for the evaluation of Day 6 embryos and for images taken using multiple time-lapse systems. Results suggested that the AI model could potentially be used to differentiate mosaic embryos based on the level of mosaicism.   LIMITATIONS, REASONS FOR CAUTION While the current investigation was performed using both retrospectively and prospectively collected data, it will be important to continue to evaluate real-world use of the genetics AI model. The endpoint described was euploidy based on the clinical outcome of PGT-A results only, so predictive accuracy for genetic status in utero or at birth was not evaluated. Rebiopsy studies of embryos using a range of PGT-A methods indicated a degree of variability in PGT-A results, which must be considered when interpreting the performance of the AI model.   WIDER IMPLICATIONS OF THE FINDINGS These findings collectively support the use of this genetics AI model for the evaluation of embryo ploidy status in a clinical setting. Results can be used to aid in prioritizing and enriching for embryos that are likely to be euploid for multiple clinical purposes, including selection for transfer in the absence of alternative genetic testing methods, selection for cryopreservation for future use or selection for further confirmatory PGT-A testing, as required.   STUDY FUNDING/COMPETING INTEREST(S) Life Whisperer Diagnostics is a wholly owned subsidiary of the parent company, Presagen Holdings Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation, and Startup Fund (RCSF). 'In kind' support and embryology expertise to guide algorithm development were provided by Ovation Fertility. 'In kind' support in terms of computational resources provided through the Amazon Web Services (AWS) Activate Program. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. S.M.D., M.A.D. and T.V.N. are employees or former employees of Life Whisperer. S.M.D, J.M.M.H, M.A.D, T.V.N., D.P. and M.P. are listed as inventors of patents relating to this work, and also have stock options in the parent company Presagen. M.V. sits on the advisory board for the global distributor of the technology described in this study and also received support for attending meetings.   TRIAL REGISTRATION NUMBER N/A.","",""
29,"Melanie Mitchell","Artificial Intelligence Hits the Barrier of Meaning",2019,"","","","",32,"2022-07-13 09:19:12","","10.3390/info10020051","","",,,,,29,9.67,29,1,3,"Today’s AI systems sorely lack the essence of human intelligence: Understanding the situations we experience, being able to grasp their meaning. The lack of humanlike understanding in machines is underscored by recent studies demonstrating lack of robustness of state-of-the-art deep-learning systems. Deeper networks and larger datasets alone are not likely to unlock AI’s “barrier of meaning”; instead the field will need to embrace its original roots as an interdisciplinary science of intelligence.","",""
0,"A. Campbell, R. Smith, B. Petersen, L. Moore, A. Khan, A. Barrie","O-125 Application of artificial intelligence using big data to devise and train a machine learning model on over 63,000 human embryos to automate time-lapse embryo annotation",2022,"","","","",33,"2022-07-13 09:19:12","","10.1093/humrep/deac105.025","","",,,,,0,0.00,0,6,1,"      Can a machine learning (ML) model, developed using modern neural network architecture produce comparable annotation data; utilisable for algorithmic outcome prediction, to manual time-lapse annotations?        The model automatically annotated unseen embryos with comparable results to manual methods, generating morphokinetic data to enable comparably predictive outputs from an embryo selection algorithm.        The application of artificial intelligence across healthcare industries, including fertility, is increasing. Several ML models are available that seek to generate or analyse embryo images and morphokinetic data, and to determine embryo viability potential. Along with photographic images, the use of time-lapse in IVF laboratories has amassed numeric data, resulting predominantly from annotated manual assessment of images over time. Embryo annotation practice is variable in quality, can be subjective and is time-consuming; commonly taking several minutes per embryo. The development of rapid, accurate automatic annotation would represent a significant time-saving as well as an increase in reproducibility and accuracy.        Multicentre quality assured annotation data from 63,383 time-lapse monitored embryos (EmbryoScope®), comprising over 400 million individual images, were used to train a ML model to automatically generate morphokinetic annotations. Data was derived from 8 UK clinics within a cohesive group between 2012-2021. Accuracy was assessed using 900 unseen embryos (with live birth outcome) by comparing the output of an established in-house, prospectively validated embryo selection model when the input was either ML-automated, or manual annotations.        Multi-focal plane images were processed on the Azure cloud (Microsoft) and resampled to 300x300 pixels. A Laplacian-based focal stacking algorithm merged frames into a single image. The model consisted of an EfficientNetB4 Convolutional Neural Network classifier to extract features and classify the stage of embryo images. A Temporal Convolutional Network  interpreted a time-series of image features; producing annotations from pronuclear fading through to blastocyst. Soft localisation loss function used QA data to integrate annotation subjectivities.        The ML model rapidly and automatically generated annotations. Efficacy and comparability of the ML model to automate reliable, utilisable annotations was demonstrated by comparison with manual annotation data and the ML model’s ability to auto-generate annotations which could be used to predict live birth by providing annotation data to an established, validated in house embryo selection model. Live birth-predictive capability was measured, and benchmarked against manual annotation, using the area under the receiver operating characteristic curve (AUC).  When tested on time-lapse images, collected from pronuclear fading to full blastulation, representing 900 previously unseen, transferred blastocysts where live birth outcomes were blinded, the in-house developed auto-annotation ML model resulted in an AUC of 0.686 compared with 0.661 for manual annotations, for live birth prediction.  Auto annotation using the developed model took only milliseconds to complete per embryo. The developed auto-annotation model, built and tested on large data, is considered suitable for productionisation with the aim of being validated and integrated into an application to support IVF laboratory practice.        Whilst this model was trained to recognise key morphokinetic events, there are other morphokinetic variables that may be useful in the prediction of live birth and further improve embryo selection, or deselection, ability. Akin to manual interpretation, some embryos may fail to be annotated or need second opinion.        There is increasing evidence supporting the application of ML to utilise big data from time-lapse imaging and fertility care generally. Whilst promising benefits to IVF clinics and patients, responsible use of data is required alongside large high-quality datasets, and rigorous validation, to ensure safe and robust applications.        N/A ","",""
37,"T. Babina, A. Fedyk, A. He, James Hodson","Artificial Intelligence, Firm Growth, and Industry Concentration",2020,"","","","",34,"2022-07-13 09:19:12","","10.2139/ssrn.3651052","","",,,,,37,18.50,9,4,2,"Which firms invest in artificial intelligence (AI) technologies, and how do these investments affect individual firms and industries? We provide a comprehensive picture of the use of AI technologies and their impact among US firms over the last decade, using a unique combination of job postings and individual-level employment profiles. We introduce a novel measure of investments in AI technologies based on human capital and document that larger firms with higher sales, markups, and cash holdings tend to invest more in AI. Firms that invest in AI experience faster growth in both sales and employment, which translates into analogous growth at the industry level. The positive effects are concentrated among the ex ante largest firms, leading to a positive correlation between AI investments and an increase in industry concentration. However, the increase in concentration is not accompanied by either increased markups or increased productivity. Instead, firms tend to expand into new product and geographic markets. Our results are robust to instrumenting firm-level AI investments with foreign industry-level AI investments and with local variation in industry-level AI investments, and to controlling for investments in general information technology and robotics. We also document consistent patterns across measures of AI using firms' demand for AI talent (job postings) and actual AI talent (resumes). Overall, our findings support the view that new technologies, such as AI, increase the scale of the most productive firms and contribute to the rise of superstar firms.","",""
7,"Baptiste Caramiaux, Marco Donnarumma","Artificial Intelligence in Music and Performance: A Subjective Art-Research Inquiry",2020,"","","","",35,"2022-07-13 09:19:12","","10.1007/978-3-030-72116-9_4","","",,,,,7,3.50,4,2,2,"","",""
103,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dentistry: Chances and Challenges",2020,"","","","",36,"2022-07-13 09:19:12","","10.1177/0022034520915714","","",,,,,103,51.50,34,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
0,"","Neural Network Training Using Genetic Algorithms Series In Machine Perception And Artificial Intelligence",2021,"","","","",37,"2022-07-13 09:19:12","","","","",,,,,0,0.00,0,0,1,"Knowledge-Based Intelligent Information and Engineering Systems 2Nature-inspired Methods in Chemometrics: Genetic Algorithms and Artificial Neural NetworksParallel Implementations of Backpropagation Neural Networks on TransputersEvolutionary Algorithms and Neural NetworksTraining Neural Networks Using Hybrids with Genetic AlgorithmsNeural Network Training Using Genetic AlgorithmsGene Expression ProgrammingTraining a Neural Network with a Genetic AlgorithmMethods and Applications of Artificial IntelligenceClassification and Learning Using Genetic AlgorithmsPractical Computer Vision Applications Using Deep Learning with CNNsIntelligent Hybrid SystemsNeurogenetic LearningAdvances in Neural Networks ISNN 2007Hybrid Intelligent SystemsEncyclopedia of Computer Science and TechnologyMachine LearningUsing a Genetic Algorithm in Training an Artificial Neural Network to Implement the XOR FunctionGenetic and Evolutionary Computation — GECCO 2004Handbook of Fuzzy ComputationNeural Network Data Analysis Using SimulnetTMArtificial Neural Nets and Genetic AlgorithmsApplied Soft Computing Technologies: The Challenge of ComplexityGenetic Algorithm for Artificial Neural Network Training for the Purpose of Automated Part RecognitionNEURAL NETWORKS, FUZZY LOGIC AND GENETIC ALGORITHMAutomatic Generation of Neural Network Architecture Using Evolutionary ComputationPGANETThe Sixth International Symposium on Neural Networks (ISNN 2009)Evolutionary Machine Learning TechniquesModeling Decisions for Artificial IntelligenceEmpirical Studies on the Utility of Genetic Algorithms for Training and Designing of Neural NetworksTraining Neural Networks Using Genetic AlgorithmsTraining feedforward neural networks using genetic algorithmsMetaheuristic Procedures for Training Neural NetworksArtificial Neural Nets and Genetic AlgorithmsNature-Inspired Computing: Concepts, Methodologies, Tools, and ApplicationsApplications of Evolutionary ComputingArtificial Intelligence and CreativityArtificial Neural Nets and Genetic AlgorithmsDeep Learning Using Genetic Algorithms Creativity is one of the least understood aspects of intelligence and is often seen as `intuitive' and not susceptible to rational enquiry. Recently, however, there has been a resurgence of interest in the area, principally in artificial intelligence and cognitive science, but also in psychology, philosophy, computer science, logic, mathematics, sociology, and architecture and design. This volume brings this work together and provides an overview of this rapidly developing field. It addresses a range of issues. Can computers be creative? Can they help us to understand human creativity? How can artificial intelligence (AI) enhance human creativity? How, in particular, can it contribute to the `sciences of the artificial', such as design? Does the new wave of AI (connectionism, geneticism and artificial life) offer more promise in these areas than classical, symbol-handling AI? What would the implications be for AI and cognitive science if computers could not be creative? These issues are explored in five interrelated parts, each of which is introducted and explained by a leading figure in the field. Prologue (Margaret Boden) Part I: Foundational Issues (Terry Dartnall) Part II: Creativity and Cognition (Graeme S. Halford and Robert Levinson) Part III: Creativity and Connectionism (Chris Thornton) Part IV: Creativity and Design (John Gero) Part V: Human Creativity Enhancement (Ernest Edmonds) Epilogue (Douglas Hofstadter) For researchers in AI, cognitive science, computer science, philosophy, psychology, mathematics, logic, sociology, and architecture and design; and anyone interested in the rapidly growing field of artificial intelligence and creativity.From the contents: Neural networks – theory and applications: NNs (= neural networks) classifier on continuous data domains– quantum associative memory – a new class of neuron-like discrete filters to image processing – modular NNs for improving generalisation properties – presynaptic inhibition modelling for image processing application – NN recognition system for a curvature primal sketch – NN based nonlinear temporalspatial noise rejection system – relaxation rate for improving Hopfield network – Oja's NN and influence of the learning gain on its dynamics Genetic algorithms – theory and applications: transposition: a biological-inspired mechanism to use with GAs (= genetic algorithms) – GA for decision tree induction – optimising decision classifications using GAs – scheduling tasks with intertask communication onto multiprocessors by GAs – design of robust networks with GA – effect of degenerate coding on GAs – multiple traffic signal control using a GA – evolving musical harmonisation – niched-penalty approach for constraint handling in GAs – GA with dynamic population size – GA with dynamic niche clustering for multimodal function optimisation Soft computing and uncertainty: self-adaptation of evolutionary constructed decision trees by information spreading – evolutionary programming of near optimal NNsArtificial neural networks and genetic algorithms both are areas of research","",""
50,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello","Artificial Intelligence, Algorithmic Pricing, and Collusion",2020,"","","","",38,"2022-07-13 09:19:12","","10.1257/AER.20190623","","",,,,,50,25.00,13,4,2,"Increasingly, pricing algorithms are supplanting human decision making in real marketplaces. To inform the competition policy debate on the possible consequences of this development, we experiment with pricing algorithms powered by Artificial Intelligence (AI) in controlled environments (computer simulations), studying the interaction among a number of Q-learning algorithms in a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. In this setting the algorithms consistently learn to charge supra-competitive prices, without communicating with one another. The high prices are sustained by classical collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.","",""
45,"Tom Kamiel Magda Vercauteren, M. Unberath, N. Padoy, N. Navab","CAI4CAI: The Rise of Contextual Artificial Intelligence in Computer-Assisted Interventions",2019,"","","","",39,"2022-07-13 09:19:12","","10.1109/JPROC.2019.2946993","","",,,,,45,15.00,11,4,3,"Data-driven computational approaches have evolved to enable extraction of information from medical images with reliability, accuracy, and speed, which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theaters are extremely complex and typically rely on poorly integrated intraoperative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer-assisted interventions, we highlight the crucial need to take the context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer-assisted intervention (CAI4CAI) arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors, and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human–AI actor team; and how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision-making ultimately producing more precise and reliable interventions.","",""
38,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases.",2020,"","","","",40,"2022-07-13 09:19:12","","10.1038/s41746-020-0229-3","","",,,,,38,19.00,6,6,2,"","",""
0,"","Proceedings of 2021 1st International Conference on Computer Science and Artificial Intelligence (ICCSAI)",2021,"","","","",41,"2022-07-13 09:19:12","","10.1109/iccsai53272.2021.9609785","","",,,,,0,0.00,0,0,1,"The proceedings contain 81 papers. The topics discussed include: adaptive central pattern generators to control human/robot interactions;modelling personality prediction from user's posting on social media;web based application for ordering food raw materials;comparison of Gaussian hidden Markov model and convolutional neural network in sign language recognition system;intelligent computational model for early heart disease prediction using logistic regression and stochastic gradient descent (a preliminary study);an efficient system to collect data for ai training on multi-category object counting task;a comparison of artificial intelligence-based methods in traffic prediction;impact of computer vision with deep learning approach in medical imaging diagnosis;and development of portable temperature and air quality detector for preventing COVID-19.","",""
2,"P. Kumar","Special issue on Artificial Intelligence in Engineering Education",2021,"","","","",42,"2022-07-13 09:19:12","","10.1002/cae.22398","","",,,,,2,2.00,2,1,1,"Artificial intelligence (AI) can be defined as the intelligence exhibited by machines and computers in accomplishing desired tasks in a similar way to how normal human beings think and act. Hence AI is also termed machine intelligence. For a computational system to be artificially intelligent, the system should possess the ability to understand the surrounding environment, make proper assumptions, and based on the circumstances make judicious decisions that maximize the possibilities of accomplishing goals most of the time. These AI‐enabled devices are also called Intelligent Agents. These intelligent agents use some mapping functions also termed cognitive functions, which take these environmental parameters and contextual information as inputs along with the goal to be accomplished and manipulate the right means to accomplish the targeted goal. AI can also be considered inter‐ disciplinary as it involves several other disciplines such as Machine Learning, Computer Vision, Cognitive Science, Neural Networks, Data Mining, Natural Language Processing (NLP), robotics, and mathematics. All these disciplines are related, and thereby intelligent agents are trained to understand and adapt to the surrounding environment according to the context. The use of AI spans across several applications such as Human–Computer Interaction (HCI) based smart agent development, devising smart surveillance solutions using computer vision, creating robust and stable decision making systems that can understand, evaluate, manipulate, analyze, and predict several novel patterns by processing large volumes of application data, development of multilingual systems that uses NLP to understand the language features used across the context and aid decision making and so on. Also, since its inception as an academic discipline in the 1950s, AI has grown leaps and bounds as a discipline, and its applications have stretched across several domains such as Retail and Business solutions, Manufacturing and Logistics, Automobiles, Business Analytics and Market predictions, Healthcare, Security Systems, and Education. One of the key emerging areas where extensive efforts are spent towards developing smart applications and agents is the educational domain. Gone are the days where the educational system was completely driven by humans, and the growth of AI‐enabled intelligent agents has set the tone by replacing most human work with that of smart agents. Educational systems use AI‐based agents to study the behavior of students and suggest suitable courses for them. Smart agents are nowadays deployed in classrooms for complete classroom monitoring that includes tracking attendance, monitoring classroom activities, student and staff behavior monitoring, and so on. Similarly, smart agents are deployed to scan through the contents available online and suggest suitable content to students according to the course and also according to the different levels of understanding of student fraternity. Also, computer vision‐based smart agents are deployed to study the state of mind of students when they undergo different courses and provide insightful information about their likeness towards a subject or course. This agent‐based information serves as useful information in deciding the teaching methodology and also framing of course contents. Also, smart systems play a vital role in analyzing student results and providing insightful information about student performance. Thus, it is imperative that AI has become an indispensable force to reckon with in the future forward across the educational domain. However, the major drawback in these artificially intelligent systems is that they are not always accurate with decision making and at times predict otherwise. Also, training the AI‐based agent to understand the contextual paradigm and surrounding environment is a challenge. This special issue on “Artificial Intelligence In Education” is focused on drawing original studies related to the development and refinement of smart agents that can be applied across the educational domain.","",""
1,"Pranav Gupta, A. Woolley","Articulating the Role of Artificial Intelligence in Collective Intelligence: A Transactive Systems Framework",2021,"","","","",43,"2022-07-13 09:19:12","","10.1177/1071181321651354c","","",,,,,1,1.00,1,2,1,"Human society faces increasingly complex problems that require coordinated collective action. Artificial intelligence (AI) holds the potential to bring together the knowledge and associated action needed to find solutions at scale. In order to unleash the potential of human and AI systems, we need to understand the core functions of collective intelligence. To this end, we describe a socio-cognitive architecture that conceptualizes how boundedly rational individuals coordinate their cognitive resources and diverse goals to accomplish joint action. Our transactive systems framework articulates the inter-member processes underlying the emergence of collective memory, attention, and reasoning, which are fundamental to intelligence in any system. Much like the cognitive architectures that have guided the development of artificial intelligence, our transactive systems framework holds the potential to be formalized in computational terms to deepen our understanding of collective intelligence and pinpoint roles that AI can play in enhancing it.","",""
29,"Grayson W. Armstrong, A. Lorch","A(eye): A Review of Current Applications of Artificial Intelligence and Machine Learning in Ophthalmology",2019,"","","","",44,"2022-07-13 09:19:12","","10.1097/IIO.0000000000000298","","",,,,,29,9.67,15,2,3,"Artificial intelligence (AI) is a branch of computer science that aims to enable computers to perform human-like tasks. Although AI is a broad discipline, machine learning is a specific branch of AI that uses computer algorithms capable of “learning” through the simulation of human intelligence. Machine learning algorithms have been applied to the medical field since the 1970s,1 and since that time have proven useful in computerassisted diagnosis, screening, and prognostication of disease.2–7 Ophthalmology is uniquely capable of capitalizing on the promise of AI. Ophthalmologists, during routine clinical encounters, generate robust data sources capable of supporting machine learning algorithms including multimodal ophthalmic images and quantifiable metrics such as visual acuity (VA), intraocular pressure, and cup to disk ratio. To date, AI techniques have been applied to ophthalmology to screen for and diagnose diseases, such as diabetic retinopathy (DR), age-related macular degeneration (AMD), macular edema (ME), glaucoma, keratoconus, postlaserassisted in situ keratomileusis corneal ectasia, retinopathy of prematurity (ROP), and cataracts, as well as predict the prognosis of various ophthalmic diseases. Advances in ophthalmology-specific AI stand to increase patient access to clinical screening and diagnosis as well as decrease health care costs, especially when applied to high-risk populations, low-resource communities, or when combined with telemedicine initiatives. This review provides an introduction to AI andmachine learning, as well as an overview of current applications in the field of ophthalmology.","",""
0,"M. Yakar, D. Etiz","Artificial intelligence in radiation oncology",2021,"","","","",45,"2022-07-13 09:19:12","","10.35711/AIMI.V2.I2.13","","",,,,,0,0.00,0,2,1,"Artificial intelligence (AI) is a computer science that tries to mimic human-like intelligence in machines that use computer software and algorithms to perform specific tasks without direct human input. Machine learning (ML) is a subunit of AI that uses data-driven algorithms that learn to imitate human behavior based on a previous example or experience. Deep learning is an ML technique that uses deep neural networks to create a model. The growth and sharing of data, increasing computing power, and developments in AI have initiated a transformation in healthcare. Advances in radiation oncology have produced a significant amount of data that must be integrated with computed tomography imaging, dosimetry, and imaging performed before each fraction. Of the many algorithms used in radiation oncology, has advantages and limitations with different computational power requirements. The aim of this review is to summarize the radiotherapy (RT) process in workflow order by identifying specific areas in which quality and efficiency can be improved by ML. The RT stage is divided into seven stages: patient evaluation, simulation, contouring, planning, quality control, treatment application, and patient follow-up. A systematic evaluation of the applicability, limitations, and advantages of AI algorithms has been done for each stage.","",""
0,"Arkadiusz Czuba","Artificial Intelligence-Based Cognitive Radar Architecture",2021,"","","","",46,"2022-07-13 09:19:12","","10.1109/CSCI54926.2021.00092","","",,,,,0,0.00,0,1,1,"This paper considers a new cognitive radar architecture based on artificial intelligence cognitive architectures (CAs). The CAs are the computational embodiments of the theory about modeling the human mind used in artificial intelligence. They found applications in, e.g., robotics and autonomous vehicles. However, the research related to the topic of integrating artificial intelligence CAs with radar systems is very limited. In this paper, a novel cognitive radar architecture has been proposed. Cognitive abilities such as learning, perception, attention, and decision-making mechanisms were conformed to radar capabilities. This new concept introduces promising visual attention mechanisms for target prioritization, declarative memories for a broader understanding of the radar environment, and reinforcement learning to improve the signal-to-noise ratio. All of the components mentioned above, combined together, look promising to improve an overall radar performance.","",""
13,"Yuanbin Wang, P. Zheng, Tao Peng, Huayong Yang, J. Zou","Smart additive manufacturing: Current artificial intelligence-enabled methods and future perspectives",2020,"","","","",47,"2022-07-13 09:19:12","","10.1007/s11431-020-1581-2","","",,,,,13,6.50,3,5,2,"","",""
28,"Sathian Dananjayan, G. M. Raj","Artificial Intelligence during a pandemic: The COVID‐19 example",2020,"","","","",48,"2022-07-13 09:19:12","","10.1002/hpm.2987","","",,,,,28,14.00,14,2,2,"Artificial intelligence (AI) is transforming our lifestyle intending to mimic human intelligence by a computer/machine in solving various issues. Initially, AI was designed to overcome simpler problems like winning a chess game, language recognition, image retrieval, among others. With the technological advancements, AI is getting increasingly sophisticated at doing what humans do, but more efficiently, rapidly, and at a lower cost in solving complex problems. AI in healthcare provides an upper hand undoubtedly over traditional analytics and clinical decision-making techniques. Machine learning (ML) algorithms, a subset of AI, can detect patterns from huge complex datasets to become more precise and accurate as they interact with training data, allowing humans to gain unprecedented insights into early detection of diseases, drug discovery, diagnostics, healthcare processes, treatment variability, and patient outcomes. But how effective are the AI algorithms during a disease outbreak or for that matter a pandemic? After 2000, the pandemics are testing the AI's ability to handle extreme events. The two major factors affecting AI algorithms include the availability of historical and real-time data and high computational power. The different roles played by AI during pandemics are early warning and alerts, prediction and detection of outbreak of diseases, real-time disease monitoring worldwide, analysis and visualisation of spreading trends, prediction of infection rate and infection trend, rapid decision-making to identify the effective treatments, study and analysis of the pathogens, and drug discovery. All these are executed at a greater speed with AI. WHO and CDC (United States) are receiving data of several diseases and situations occurring across the world. With modern computer architecture and internet, all these data can be accessed in real-time by different institutes to develop an autonomous or collaborative AI model to handle various tasks. In addition to the official data, AI can gather information from news outlets, forums, healthcare reports, travel data, social media posts, and others in multiple languages across the world by using natural language processing (NLP) techniques and flag their priority. Several terabytes of data which includes patients' case history, geographical events, and social media posts about a new pneumonia are processed at a rapid rate with high-performance computing to predict the possible outbreak of a pandemic. Most importantly unsupervised ML can identify its own pattern from the noise (historical and real-time data) rather than the training it on a preselected dataset, thus giving a wider possibility and new behaviour. An AI model trained to predict a particular disease can be retrained on the new data of a new or different disease. Some noticeable examples of AI that are used to battle the COVID-19 pandemic and others are as follows:","",""
27,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases",2020,"","","","",49,"2022-07-13 09:19:12","","10.1038/s41746-020-0229-3","","",,,,,27,13.50,5,6,2,"","",""
0,"C. Aggarwal","An Introduction to Artificial Intelligence",2021,"","","","",50,"2022-07-13 09:19:12","","10.1007/978-3-030-72357-6_1","","",,,,,0,0.00,0,1,1,"","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",51,"2022-07-13 09:19:12","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
44,"Chiara Longoni, Luca Cian","Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect",2020,"","","","",52,"2022-07-13 09:19:12","","10.1177/0022242920957347","","",,,,,44,22.00,22,2,2,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).","",""
0,"Chengbing Tan, Qun Chen","Application of an artificial intelligence algorithm model of memory retrieval and roaming in sorting Chinese medicinal materials",2021,"","","","",53,"2022-07-13 09:19:12","","10.3233/jcm-215477","","",,,,,0,0.00,0,2,1,"In order to capture autobiographical memory, inspired by the development of human intelligence, a computational AM model for autobiographical memory is proposed in this paper, which is a three-layer network structure, in which the bottom layer encodes the event-specific knowledge comprising 5W1H, and provides retrieval clues to the middle layer, encodes the related events, and the top layer encodes the event set. According to the bottom-up memory search process, the corresponding events and event sets can be identified in the middle layer and the top layer respectively; At the same time, AM model can simulate human memory roaming through the process of rule-based memory retrieval. The computational AM model proposed in this paper not only has robust and flexible memory retrieval, but also has better response performance to noisy memory retrieval cues than the commonly used memory retrieval model based on keyword query method, and can also imitate the roaming phenomenon in memory.","",""
0,"Fahad S. Mohammed, Hisham Qadri, S. Mohammed","COVID-19 in the era of artificial intelligence: a black swan event?",2021,"","","","",54,"2022-07-13 09:19:12","","10.21037/jmai-21-23","","",,,,,0,0.00,0,3,1,"of COVID-19 (3). The large amount of social computational data generated by the pandemic may lead to breakthroughs in AI that can greatly alter human behavior. Newer COVID-19 variants and behavioral changes are causing resurgence of the pandemic. AI can use social computational data to devise novel non-pharmaceutical interventions to prevent newer outbreaks. “How we feel” a web and mobile application that longitudinally tracks COVID-19 symptoms, behavior and testing, can predict likely COVID-19 positive individuals and outbreaks (4). Genomic, structural data and outcomes can be used to make COVID-19 simulations, predict mutations, outbreaks and guide therapy leading to drug discovery, drug repurposing and precision medicine (5). Multiple applications for predicting severity using imaging and lab data in real time have been developed and have been externally validated (6). These applications have played a pivotal role in the management of the pandemic.","",""
0,"Joe Hays, S. Ramamoorthy, Christian Tetzlaff","Editorial: Robust Artificial Intelligence for Neurorobotics",2021,"","","","",55,"2022-07-13 09:19:12","","10.3389/fnbot.2021.809903","","",,,,,0,0.00,0,3,1,"Neural computing is a powerful paradigm that has revolutionized machine learning. Building from early roots in the study of adaptive behavior and attempts to understand information processing in parallel and distributed neural architectures, modern neural networks have convincingly demonstrated successes in numerous areas—transforming the practice of computer vision, natural language processing, and even computational biology. Applications in robotics bring stringent constraints on size, weight and power constraints (SWaP), which challenge the developers of these technologies in new ways. Indeed, these requirements take us back to the roots of the field of neural computing, forcing us to ask how it could be that the human brain achieves with as little as 12 watts of power what seems to require entire server farms with state of the art computational and numerical methods. Likewise, even lowly insects demonstrate a degree of adaptivity and resilience that still defy easy explanation or computational replication. In this Research Topic, we have compiled the latest research addressing several aspects of these broadly defined challenge questions. As illustrated in Figure 1, the articles are organized into four prevailing themes: Sense, Think, Act, and Tools.","",""
167,"Max Tegmark","Life 3.0: Being Human in the Age of Artificial Intelligence",2017,"","","","",56,"2022-07-13 09:19:12","","","","",,,,,167,33.40,167,1,5,"New York Times Best Seller How will Artificial Intelligence affect crime, war, justice, jobs, society and our very sense of being human? The rise of AI has the potential to transform our future more than any other technologyand theres nobody better qualified or situated to explore that future than Max Tegmark, an MIT professor whos helped mainstream research on how to keep AI beneficial. How can we grow our prosperity through automation without leaving people lacking income or purpose? What career advice should we give todays kids? How can we make future AI systems more robust, so that they do what we want without crashing, malfunctioning or getting hacked? Should we fear an arms race in lethal autonomous weapons? Will machines eventually outsmart us at all tasks, replacing humans on the job market and perhaps altogether? Will AI help life flourish like never before or give us more power than we can handle? What sort of future do you want? This book empowers you to join what may be the most important conversation of our time. It doesnt shy away from the full range of viewpoints or from the most controversial issuesfrom superintelligence to meaning, consciousness and the ultimate physical limits on life in the cosmos.","",""
8,"I. Wiafe, F. N. Koranteng, Emmanuel Nyarko Obeng, Nana Assyne, Abigail Wiafe, S. Gulliver","Artificial Intelligence for Cybersecurity: A Systematic Mapping of Literature",2020,"","","","",57,"2022-07-13 09:19:12","","10.1109/ACCESS.2020.3013145","","",,,,,8,4.00,1,6,2,"Due to the ever-increasing complexities in cybercrimes, there is the need for cybersecurity methods to be more robust and intelligent. This will make defense mechanisms to be capable of making real-time decisions that can effectively respond to sophisticated attacks. To support this, both researchers and practitioners need to be familiar with current methods of ensuring cybersecurity (CyberSec). In particular, the use of artificial intelligence for combating cybercrimes. However, there is lack of summaries on artificial intelligent methods for combating cybercrimes. To address this knowledge gap, this study sampled 131 articles from two main scholarly databases (ACM digital library and IEEE Xplore). Using a systematic mapping, the articles were analyzed using quantitative and qualitative methods. It was observed that artificial intelligent methods have made remarkable contributions to combating cybercrimes with significant improvement in intrusion detection systems. It was also observed that there is a reduction in computational complexity, model training times and false alarms. However, there is a significant skewness within the domain. Most studies have focused on intrusion detection and prevention systems, and the most dominant technique used was support vector machines. The findings also revealed that majority of the studies were published in two journal outlets. It is therefore suggested that to enhance research in artificial intelligence for CyberSec, researchers need to adopt newer techniques and also publish in other related outlets.","",""
12,"Awishkar Ghimire, Surendrabikram Thapa, A. Jha, Surabhi Adhikari, Ankit Kumar","Accelerating Business Growth with Big Data and Artificial Intelligence",2020,"","","","",58,"2022-07-13 09:19:12","","10.1109/I-SMAC49090.2020.9243318","","",,,,,12,6.00,2,5,2,"Artificial Intelligence (AI) is considered to be the fourth industrial revolution. Artificial Intelligence with the help of big data has transformed all industries around the world Artificial intelligence refers to the simulation of human or animal intelligence in computational systems so that they are programmed to think like Intelligent beings and mimic the actions of intelligent entities. Computational systems which have programmed intelligence can solve different real-world problems far more accurately and efficiently than computational systems that are deterministic and hardcoded. Since many problems in business and business analytics cannot be solved by deterministic systems, AI plays a major role in tackling problems in the business world Machine learning and deep learning which are subsets of the field of AI is widely used to solve and optimize many problems in business such as marketing, credit card fraud detection, algorithmic trading, customer service, portfolio management, product recommendation according to the needs of customers, insurance underwriting. AI and big data have revolutionized the business world and this paper discusses some AI and big data technologies that are currently being used to accelerate business growth.","",""
21,"Adrien Bécue, Isabel Praça, J. Gama","Artificial intelligence, cyber-threats and Industry 4.0: challenges and opportunities",2021,"","","","",59,"2022-07-13 09:19:12","","10.1007/S10462-020-09942-2","","",,,,,21,21.00,7,3,1,"","",""
15,"M. Sipper, R. S. Olson, J. Moore","Evolutionary computation: the next major transition of artificial intelligence?",2017,"","","","",60,"2022-07-13 09:19:12","","10.1186/s13040-017-0147-3","","",,,,,15,3.00,5,3,5,"","",""
2,"B. Grzyb, G. Vigliocco","Beyond robotic speech: mutual benefits to cognitive psychology and artificial intelligence from the study of multimodal communication",2020,"","","","",61,"2022-07-13 09:19:12","","10.31234/osf.io/h5dxy","","",,,,,2,1.00,1,2,2,"Language has predominately been studied as a unimodal phenomenon - as speech or text without much consideration of its physical and social context – this is true both in cognitive psychology/psycholinguistics as well as in artificial intelligence. However, in everyday life, language is most often used in face-to-face communication and in addition to structured speech it comprises a dynamic system of multiplex components such as gestures, eye gaze, mouth movements and prosodic modulation. Recently, cognitive scientists have started to realise the potential importance of multimodality for the understanding of human communication and its neural underpinnings; while AI scientists have begun to address how to integrate multimodality in order to improve communication between human and artificial embodied agent. We review here the existing literature on multimodal language learning and processing in humans and the literature on perception of artificial agents, their comprehension and production of multimodal cues and we discuss their main limitations. We conclude by arguing that by joining forces AI scientists can improve the effectiveness of human-machine interaction and increase the human-likeness and acceptance of embodied agents in society. In turn, computational models that generate language in artificial embodied agents constitute a unique research tool to investigate the underlying mechanisms that govern language processing and learning in humans.","",""
1,"Assil Benchaaben, Felipe Guimaraes, Emmanuel Prestat, A. Kassambara, Mounia Filahi, Caroline Laugé, T. Sbarrato, J. Fieschi","Abstract 870: Immunoscore®workflow enhanced by artificial intelligence",2020,"","","","",62,"2022-07-13 09:19:12","","10.1158/1538-7445.am2020-870","","",,,,,1,0.50,0,8,2,"Artificial Intelligence (AI) along with Machine Learning (ML) techniques has long promised to accelerate Digital Pathology (DP) based cancer diagnosis. Despite the consensus regarding the value of AI, the lack of visibility of how ML algorithms work, prevents their wider adoption for human in vitro diagnostic (IVD) in a highly regulated environment. A common ground becomes necessary in order to fully benefit from ML capabilities. HalioDx Immunoscore® was the first immune scoring test validated for IVD use leveraging advanced image analysis. In brief, for each tumor sample, 2 slides are stained using an automated immunohistochemistry instrument: one with CD3 and one with CD8 ready-to-use monoclonal antibodies (HalioDx) followed by detection with DAB and counterstaining. Digital images of stained slides are obtained using a whole slide scanner and analyzed by a software program (Immunoscore® Analyzer, HalioDx)1. Current workflow relies only on Computer Vision (CV) techniques for image analysis leading to the calculation of the Immunoscore®. We have used ML to improve HalioDx Immunoscore® software program, streamline the workflow, decrease hands-on and computation times. In summary, to design the new workflow, each DP steps were considered as independent applications. CV remains applied to the cell detection. A Convolutional Neural Network, along with a UNET architecture, were used to recognize Regions of Interest (ROI) and image-related artifacts during the analysis. Intermediary validation steps by a trained operator were maintained in order to review CV and AI steps and guarantee a complete equivalence versus the standardized original DP protocol. The Intersection over the Union of two regions (IoU) was used as performance and equivalency metric. Compared to Ground Truth, the ML algorithm improves the accuracy of the ROI detection versus the CV based algorithm, resulting in a dramatic decrease of the ROI computing time (from 3h to 5min) as well as in a reduced need for manual correction. We demonstrated that ML applied to the Immunoscore® DP workflow for ROI detection results in reduced time-to result and overall improved robustness of the analysis. The equivalency study showed the importance of a well-curated dataset to maximize model9s accuracy and performance. Finally, the verification and validation phase demonstrated the ML based workflow readiness for regulatory approval. 1Hermitte F. J Immunother Cancer. 2016 Sep 20;4:57. doi: 10.1186/s40425-016-0161-x. Citation Format: Assil Benchaaben, Felipe Machado Guimaraes, Emmanuel Prestat, Alboukadel Kassambara, Mounia Filahi, Caroline Lauge, Thomas Sbarrato, Jacques Fieschi. Immunoscore® workflow enhanced by artificial intelligence [abstract]. In: Proceedings of the Annual Meeting of the American Association for Cancer Research 2020; 2020 Apr 27-28 and Jun 22-24. Philadelphia (PA): AACR; Cancer Res 2020;80(16 Suppl):Abstract nr 870.","",""
0,"Shivali Agarwal, Jayachandu Bandlamudi, Atri Mandal, Anupama Ray, G. Sridhara","Automated Assignment of Helpdesk Email Tickets: An Artificial Intelligence Life-Cycle Case Study",2020,"","","","",63,"2022-07-13 09:19:12","","","","",,,,,0,0.00,0,5,2,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 Fall 2020 45 The landscape of modern information technology service delivery is changing, with increased focus on automation and optimization. Most information technology vendors today have service platforms aimed toward end-to-end automation for carrying out mundane, repetitive labor-intensive tasks and even for tasks requiring human cognizance. One such task is ticket assignment and dispatch, where the service requests submitted by the end-users to the vendor in the form of tickets are reviewed by a centralized dispatch team and assigned to the appropriate service team and resolver group. The dispatch of a ticket to the correct group of practitioners is a critical step in the speedy resolution of a ticket. Incorrect dispatch decisions can significantly increase the total turnaround time for ticket resolution, as observed in a study of an actual production system (agarwal, Sindhgatta, and Sengupta 2012). When such delays occur, it causes customer dissatisfaction as well as monetary penalties for the vendor due to service-level-agreement breaches. Several factors make the dispatcher’s job challenging, namely the need for in-depth knowledge of the roles and responsibilities of various groups, the heterogeneous and informal nature of email text, and the high attrition rate in service delivery teams (Mandal et al. 2018). Given the fact that inefficiencies in dispatch have serious business consequences, there has been a lot of interest in automating the assignment process. a number of different approaches have been proposed for automating ticket dispatch (agarwal, Sindhgatta, and Sengupta 2012; Shao et al. 2008a, 2008b; Parvin, Bose, and Van Oyen 2009).  In this article, we present an endto-end automated helpdesk email ticket assignment system driven by high accuracy, coverage, business continuity, scalability, and optimal usage of computational resources. The primary objective of the system is to determine the problem mentioned in an incoming email ticket and then automatically dispatch it to an appropriate resolver group with high accuracy. While meeting this objective, it should also meet the objective of being able to operate at desired accuracy levels in the face of changing business needs by automatically adapting to the changes. The proposed system uses a system of classifiers with separate strategies for handling frequent and sparse resolver groups augmented with a semiautomatic rule engine and retraining strategies to ensure that it is accurate, robust, and adaptive to changing business needs. Our system has been deployed in the production of six major service providers in diverse service domains and currently assigns 100,000 emails per month, on an average, with an accuracy close to ninety percent and covering at least ninety percent of email tickets. This translates to achieving human-level accuracy and results in a net savings of more than 50,000 man-hours of effort per annum. To date, our deployed system has already served more than two million tickets in production. Automated Assignment of Helpdesk Email Tickets: An Artificial Intelligence Life-Cycle Case Study","",""
0,"Shanqi Pang Dr, Yongmei Li Prof","Artificial Intelligence Techniques for Cyber Security Applications",2020,"","","","",64,"2022-07-13 09:19:12","","10.46532/ijaict-2020021","","",,,,,0,0.00,0,2,2,"Considering the enhancement in technology, criminals have been using cyberspace in order to commit many crimes. Therefore, it should be noted that cybercrimes are exposed to a number of threats and intrusions if not safeguarded well. Human and physical intervention tend not to be very adequate for the protection and tracking of such infrastructure, that is why there should be the establishment of multifaceted cyber defense networks, which are flexible, robust, and adjustable in order sense a massive collection of invasion and creation of real-time choices. Nevertheless, significant number of bio-related computing techniques of AI (artificial intelligence) tend to be increasing hence a significant role is played in detecting and preventing cybercrime. The main aim of this paper is outlining the actual advancement that have been made possible due to the application of AI methods for the fight against cybercrimes, in order to reveal how the methods are efficient in sensing and preventing cyber invasions, also providing a brief overview of the future works. Keywords— Computational intelligence, Artificial Intelligence, Intrusion detection and prevention systems, Cyber crime","",""
0,"T. Mahmood, Muhammad Owais, Kyoung Jun Noh, Hyo Sik Yoon, A. Haider, H. Sultan, K. Park","Artificial Intelligence-based Segmentation of Nuclei in Multi-organ Histopathology Images: Model Development and Validation (Preprint)",2020,"","","","",65,"2022-07-13 09:19:12","","10.2196/preprints.24394","","",,,,,0,0.00,0,7,2,"  BACKGROUND  Accurate nuclei segmentation in histopathology images plays a key role in digital pathology. It is considered a prerequisite for the determination of cell phenotype, nuclear morphometrics, cell classification, and the grading and prognosis of cancer. However, it is a very challenging task because of the different types of nuclei, large intra-class variations, and diverse cell morphologies. Consequently, the manual inspection of such images under high-resolution microscopes is tedious and time-consuming. Alternatively, artificial intelligence (AI)-based automated techniques, which are fast, robust, and require less human effort, can be used. Recently, several AI-based nuclei segmentation techniques have been proposed. They have shown a significant performance improvement for this task, but there is room for further improvement. Thus, we propose an AI-based nuclei segmentation technique in which we adopt a new nuclei segmentation network empowered by residual skip connections to address this issue.      OBJECTIVE  The aim of this study was to develop an AI-based nuclei segmentation method for histopathology images of multiple organs.       METHODS  Our proposed residual-skip-connections-based nuclei segmentation network (R-NSN) is comprised of two main stages: Stain normalization and nuclei segmentation as shown in Figure 2. In the 1st stage, a histopathology image is stain normalized to balance the color and intensity variation. Subsequently, it is used as an input to the R-NSN in stage 2, which outputs a segmented image.       RESULTS  Experiments were performed on two publicly available datasets: 1) The Cancer Genomic Atlas (TCGA), and 2) Triple-negative Breast Cancer (TNBC). The results show that our proposed technique achieves an aggregated Jaccard index (AJI) of 0.6794, Dice coefficient of 0.8084, and F1-measure of 0.8547 on the TCGA dataset, and an AJI of 0.7332, Dice coefficient of 0.8441, precision of 0.8352, recall of 0.8306, and F1-measure of 0.8329 on the TNBC dataset. These values are higher than those of the state-of-the-art methods.      CONCLUSIONS  The proposed R-NSN has the potential to maintain crucial features by using the residual connectivity from the encoder to the decoder and uses only a few layers, which reduces the computational cost of the model. The selection of a good stain normalization technique, the effective use of residual connections to avoid information loss, and the use of only a few layers to reduce the computational cost yielded outstanding results. Thus, our nuclei segmentation method is robust and is superior to the state-of-the-art methods. We expect that this study will contribute to the development of computational pathology software for research and clinical use and enhance the impact of computational pathology.      CLINICALTRIAL   ","",""
0,"K. Shrivastav, N. Taneja, P. Arambam, Vandana Bhatia, S. Batra, Harpreet Singh, E. Abed, P. Ranjan, Rajiv Janardhanan∗h","An Artificial Intelligence Enabled Multimedia Tool for Rapid Screening of Cervical Cancer",2020,"","","","",66,"2022-07-13 09:19:12","","","","",,,,,0,0.00,0,9,2,"Cervical cancer is a major public health challenge. Further mitigation of cervical cancer can greatly benefit from development of innovative and disruptive technologies for its rapid screening and early detection. The primary objective of this study is to contribute to this aim through large scale screening by development of Artificial Intelligence enabled Intelligent Systems as they can support human cancer experts in making more precise and timely diagnosis. Our current study is focused on development of a robust and interactive algorithm for analysis of colposcope-derived images analysis and a diagnostic tool/scale namely the OMThe Onco-Meter. This tool was trained and tested on 300 InEmail addresses: kdshrivastav@amity.edu (Kumar Dron Shrivastav), ntaneja@amity.edu (Neha Taneja), priyadarshini@batrahospitaldelhi.org (Priyadarshini Arambam), vbhatia2@amity.edu (Vandana Bhatia), shelly.batra@opasha.org (Shelly Batra), crbhmrc1@batrahospitaldelhi.org (Shelly Batra), hsingh@bmi.icmr.org.in (Harpreet Singh), abed@isr.umd.edu (Eyad H. Abed), ranjan.p@srmap.edu.in (Priya Ranjan), rjanardhanan@amity.edu (Rajiv Janardhanan∗) Preprint submitted to The Lancet Digital Health June 1, 2020 dian subjects/patients yielding 77% accuracy with a sensitivity of 83.56% and a specificity of 59.25%. OM-The Oncometer is capable of classifying cervigrams into cervical dysplasia, carcinoma in− situ (CIS) and invasive cancer(IC). Programming language R has been used to implement and compute earth mover distances (EMD) to characterize different diseases labels associated with cervical cancer, computationally. Deployment of automated tools will facilitate early diagnosis in a noninvasive manner leading to a timely clinical intervention for cervical cancer patients upon detection at a Primary Health Care (PHC).The tool developed in this study will aid clinicians to design timely intervention strategies aimed at improving the clinical prognosis of patients.","",""
85,"Ashish Ghosh, Debasrita Chakraborty, Anwesha Law","Artificial intelligence in Internet of things",2018,"","","","",67,"2022-07-13 09:19:12","","10.1049/TRIT.2018.1008","","",,,,,85,21.25,28,3,4,"Functioning of the Internet is persistently transforming from the Internet of computers (IoC) to the ‘Internet of things (IoT)’. Furthermore, massively interconnected systems, also known as cyber-physical systems (CPSs), are emerging from the assimilation of many facets like infrastructure, embedded devices, smart objects, humans, and physical environments. What the authors are heading to is a huge ‘Internet of Everything in a Smart Cyber Physical Earth’. IoT and CPS conjugated with ‘data science’ may emerge as the next ‘smart revolution’. The concern that arises then is to handle the huge data generated with the much weaker existing computation power. The research in data science and artificial intelligence (AI) has been striving to give an answer to this problem. Thus, IoT with AI can become a huge breakthrough. This is not just about saving money, smart things, reducing human effort, or any trending hype. This is much more than that – easing human life. There are, however, some serious issues like the security concerns and ethical issues which will go on plaguing IoT. The big picture is not how fascinating IoT with AI seems, but how the common people perceive it – a boon, a burden, or a threat.","",""
2,"Lorenzo Barberis Canonico, Nathan J. Mcneese, Chris Duncan","Machine Learning as Grounded Theory: Human-Centered Interfaces for Social Network Research through Artificial Intelligence",2018,"","","","",68,"2022-07-13 09:19:12","","10.1177/1541931218621287","","",,,,,2,0.50,1,3,4,"Internet technologies have created unprecedented opportunities for people to come together and through their collective effort generate large amounts of data about human behavior. With the increased popularity of grounded theory, many researchers have sought to use ever-increasingly large datasets to analyze and draw patterns about social dynamics. However, the data is simply too big to enable a single human to derive effective models for many complex social phenomena. Computational methods offer a unique opportunity to analyze a wide spectrum of sociological events by leveraging the power of artificial intelligence. Within the human factors community, machine learning has emerged as the dominant AI-approach to deal with big data. However, along with its many benefits, machine learning has introduced a unique challenge: interpretability. The models of macro-social behavior generated by AI are so complex that rarely can they translated into human understanding. We propose a new method to conduct grounded theory research by leveraging the power of machine learning to analyze complex social phenomena through social network analysis while retaining interpretability as a core feature.","",""
19,"Thomas G. Dietterich","Robust artificial intelligence and robust human organizations",2018,"","","","",69,"2022-07-13 09:19:12","","10.1007/s11704-018-8900-4","","",,,,,19,4.75,19,1,4,"","",""
31,"T. Ertekin, Qian Sun","Artificial Intelligence Applications in Reservoir Engineering: A Status Check",2019,"","","","",70,"2022-07-13 09:19:12","","10.3390/EN12152897","","",,,,,31,10.33,16,2,3,"This article provides a comprehensive review of the state-of-art in the area of artificial intelligence applications to solve reservoir engineering problems. Research works including proxy model development, artificial-intelligence-assisted history-matching, project design, and optimization, etc. are presented to demonstrate the robustness of the intelligence systems. The successes of the developments prove the advantages of the AI approaches in terms of high computational efficacy and strong learning capabilities. Thus, the implementation of intelligence models enables reservoir engineers to accomplish many challenging and time-intensive works more effectively. However, it is not yet astute to completely replace the conventional reservoir engineering models with intelligent systems, since the defects of the technology cannot be ignored. The trend of research and industrial practices of reservoir engineering area would be establishing a hand-shaking protocol between the conventional modeling and the intelligent systems. Taking advantages of both methods, more robust solutions could be obtained with significantly less computational overheads.","",""
0,"S. Fiore","Interdisciplinary Models and Frameworks for the Study of Artificial Social Intelligence",2021,"","","","",71,"2022-07-13 09:19:12","","10.1177/1071181321651354","","",,,,,0,0.00,0,1,1,"This symposium provides a complementary set of papers exploring frameworks and models for developing artificial social intelligence (ASI) for teams. ASI consists of components of social cognition that support teamwork and more general interpersonal interactions. Although AI is rapidly evolving and fielded in a variety of operational settings, the implementation of such systems is vastly outpacing our ability to understand how to design and develop technologies appropriately. This symposium is meant to help redress this gap. Consisting of scholars representing the cognitive, computational, and organizational sciences, the papers discuss how they integrate theory and methods to inform development of agents capable of complex collaborative processes. Collectively, these papers synthesize perspectives across disciplines in support of an interdisciplinary research approach for ASL The goal is to contribute to research and development in the area of Human- AI- Robot Teaming effectiveness.","",""
0,"Bradley Hayes, M. Cakmak, Stephanie Rosenthal","Introduction to the Special Issue on Artificial Intelligence and Human-Robot Interaction",2018,"","","","",72,"2022-07-13 09:19:12","","10.1145/3279995","","",,,,,0,0.00,0,3,4,"Artificial Intelligence (AI) has had a transformational impact on Human-Robot Interaction (HRI) research over the past decade, enabling work in HRI to develop and investigate robots that can operate autonomously in far more challenging environments and far more complex scenarios than was possible ever before. Beyond laboratory studies, robots that explicitly interact with people as part of their functionality are increasingly being developed, productized, and deployed throughout the world, enabling ecologically valid ethnographic studies of interactions between humans and robots. These advances have been fueled by enabling technologies across many subfields of AI including machine learning, computer vision, task and motion planning, natural language understanding, and dialogue systems. It is not, however, the case that AI research produced polished, ready-off-the-shelf tools that researchers could pick up and effortlessly use to build their envisioned autonomous robot. Rather, the shift has been due to a new, hybrid approach to human-centered robotics research, facilitated by HRI researchers who acquired deep technical skill sets and an influx of AI researchers applying their expertise to HRI problems. More interdiscplinary research teams consisting of formerly AI and HRI researchers also formed, resulting in a vibrant sub-community at the intersection of AI and HRI who came together at the AAAI Fall Symposium on AI for Human-Robot Interaction for the last 4 years. This special issue was encouraged by the continued success and overwhelming popularity of this symposium. Our goal is to exemplify this community’s mature, high-quality, and original work, establishing T-HRI as a premier venue for work at the intersection of AI and HRI. Research at this intersection is particularly challenging due to the very need for interdiscplinary, multi-faceted skill sets. AI-HRI researchers need to both innovate in computational techniques and","",""
16,"Kristin Siu, Matthew J. Guzdial, Mark O. Riedl","Evaluating singleplayer and multiplayer in human computation games",2017,"","","","",73,"2022-07-13 09:19:12","","10.1145/3102071.3102077","","",,,,,16,3.20,5,3,5,"Human computation games (HCGs) can provide novel solutions to intractable computational problems, help enable scientific breakthroughs, and provide datasets for artificial intelligence. However, our knowledge about how to design and deploy HCGs that appeal to players and solve problems effectively is incomplete. We present an investigatory HCG based on Super Mario Bros. We used this game in a human subjects study to investigate how different social conditions---singleplayer and multiplayer---and scoring mechanics---collaborative and competitive---affect players' subjective experiences, accuracy at the task, and the completion rate. In doing so, we demonstrate a novel design approach for HCGs, and discuss the benefits and tradeoffs of these mechanics in HCG design.","",""
0,"A. Barrie, R. Smith, C. Hickman, I. Erlich, A. Campbell","P-287 An assessment of agreement between automated embryo annotation, through artificial intelligence, and manual embryo annotation",2022,"","","","",74,"2022-07-13 09:19:12","","10.1093/humrep/deac107.276","","",,,,,0,0.00,0,5,1,"      How strong is the agreement between embryo morphokinetic annotations performed by experienced embryologists compared to an automated embryo annotation system based on artificial intelligence (AI)?        Agreement between manual and automated annotation as determined by the interclass correlation coefficient (ICC) revealed strong or very strong agreement for all analysed morphokinetic variables.        Transitioning from time-lapse imaging to embryo selection for transfer, freezing or discard involves annotation; the action of converting images to numerical data. Numerical data can be used as input to selection models quantifying embryo viability. Currently, embryos are manually annotated by the embryologist which can be subjective and time-consuming. As such, clinics prioritise a manageable number of variables to annotate, leading to a range of clinic practices. There is the additional challenge of operator variation, despite the development of standardised definitions and quality assurance schemes. AI may help resolve these challenges.        Retrospective comparative analysis, including 2442 embryos from IVF and ICSI cycles, from four private fertility clinics belonging to the same group in the UK. All the embryos cultured in a time-lapse incubator (EmbryoScope,Vitrolife) between January 2016 and 2019 were included in the study. Manual annotations (MA) versus automated annotations (AA) were compared using a two-way, mixed interclass correlation coefficient (ICC), which produced five categories of agreement, very weak(0-0.20), weak(0.21-0.40), moderate(0.41-0.60), strong(0.61-0.80), very strong(0.81-1.00).        Videos were manually annotated by experienced embryologists from pronuclei fading (tPNf) to time of expanded blastocyst (tEB) with all cell stages annotated in between (time to two-cell (t2), three-cell (t3), four-cell (t4), five-cell (t5), six-cell (t6), seven-cell (t7), eight-cell (t8), nine-cell (t9), morula (tM), start of blastulation (tSB) and full blastocyst (tB)). Blind to human annotations, and without any training, the same videos were annotated by CHLOE (Fairtility) to produce automated annotation data.        Of the expected annotations, AA did not provide a result for 15.4% of the MA(3235/21008). Very strong agreement(0.81-1.00) between MA and AA was found for tPNf, t2, t3, t5, t6, tM, tSB, tB, tEB. Strong agreement(0.61-0.80) was found for t4, t7, t8 and t9+. Outliers in the AA data, defined as one standard deviation from the MA, were interrogated further for five key morphokinetic parameters; t2, t5, t8, tSB and tB. A total of 269 outliers were identified.  For t2 outliers(n = 14,6%), the average time difference was 5.97h(range;5.50-24.44h). All embryos with a t2 outlier were classed as either poor(PQ) or average quality(AQ).  The t5 outliers(n = 45,19%) had an average time difference of 2.84h(range;9.33-36.69h). 96%(n = 43) of these embryos were classed as PQ(n = 25,56%) or AQ(n = 18,40%).  Outliers for t8(138,58%) were, on average, 17.53h different between MA and AA(range;12.68-40.35h). 94%(n = 130)of these embryos were classed as PQ(n = 77,56%) or AQ(n = 53,38%).  The tSB outliers(n = 28,12%) had an average time difference of 3.58h(range;0.71-14.39h). 89%(n = 25) of these embryos were classed as PQ(n = 16,57%) or AQ(n = 9,32%).  Finally, outliers associated with tB(n = 44,18%) had an average time difference of 6.39h(range;0.02-33.67h). 95%(n = 42) of these embryos were classed as PQ(n = 38,86%) or AQ(n = 4,9%).  Almost 15%(n = 40) of the embryos had outliers in more than one of the five morphokinetic parameters.        The findings for this study reflect the capabilities of a specific AI-based annotation algorithm against the practice in multiple clinics in the same group and country. The automated annotation algorithm was not trained on this dataset prior to validation, which is encouraging for generalisability.        AI is ideally suited to resolve annotation challenges. This study demonstrates that where embryo quality is poor, annotation could be skewed both when performed manually and automatically. Once robustness is demonstrated, AI tools such as CHLOE, may allow clinics to process clinical data efficiently, objectively and consistently.        None ","",""
151,"S. Goldenberg, G. Nir, S. Salcudean","A new era: artificial intelligence and machine learning in prostate cancer",2019,"","","","",75,"2022-07-13 09:19:12","","10.1038/s41585-019-0193-3","","",,,,,151,50.33,50,3,3,"","",""
141,"Rusul L. Abduljabbar, H. Dia, S. Liyanage, S. Bagloee","Applications of Artificial Intelligence in Transport: An Overview",2019,"","","","",76,"2022-07-13 09:19:12","","10.3390/SU11010189","","",,,,,141,47.00,35,4,3,"The rapid pace of developments in Artificial Intelligence (AI) is providing unprecedented opportunities to enhance the performance of different industries and businesses, including the transport sector. The innovations introduced by AI include highly advanced computational methods that mimic the way the human brain works. The application of AI in the transport field is aimed at overcoming the challenges of an increasing travel demand, CO2 emissions, safety concerns, and environmental degradation. In light of the availability of a huge amount of quantitative and qualitative data and AI in this digital age, addressing these concerns in a more efficient and effective fashion has become more plausible. Examples of AI methods that are finding their way to the transport field include Artificial Neural Networks (ANN), Genetic algorithms (GA), Simulated Annealing (SA), Artificial Immune system (AIS), Ant Colony Optimiser (ACO) and Bee Colony Optimization (BCO) and Fuzzy Logic Model (FLM) The successful application of AI requires a good understanding of the relationships between AI and data on one hand, and transportation system characteristics and variables on the other hand. Moreover, it is promising for transport authorities to determine the way to use these technologies to create a rapid improvement in relieving congestion, making travel time more reliable to their customers and improve the economics and productivity of their vital assets. This paper provides an overview of the AI techniques applied worldwide to address transportation problems mainly in traffic management, traffic safety, public transportation, and urban mobility. The overview concludes by addressing the challenges and limitations of AI applications in transport.","",""
0,"Jacob Pettigrew, Gideon Woo, Herbert H. Tsang","Computational Intelligence in Human Feature Analysis and Pose Selection",2020,"","","","",77,"2022-07-13 09:19:12","","10.1109/SSCI47803.2020.9308270","","",,,,,0,0.00,0,3,2,"Using computers to detect a human’s features is a difficult problem. The solution to this problem can be used in applications such as facial detection and gesture recognition. These applications require fast computation and high accuracy. In our research, we are trying to detect human poses by examining humans’ features such as the arms and legs. Artificial Neural Networks have been successfully used in feature analysis and are popular for use in human pose selection. In this paper, we present the results from our research comparing various computational intelligent approaches such as Convolutional Neural Networks (CNN), Multi-Layer Perceptrons (MLP), Support Vector Machines (SVM), and K-Nearest Neighbour (KNN). Among the four algorithms examined in this paper, we found that CNNs outperformed other algorithms in terms of prediction accuracy and calculation speed. Our main contribution is therefore a CNN specifically designed for learning a human’s body structure and limb articulation, producing high accuracy while being robust against different body types and variation in limb articulation.","",""
104,"Jeffrey Heer","Agency plus automation: Designing artificial intelligence into interactive systems",2019,"","","","",78,"2022-07-13 09:19:12","","10.1073/pnas.1807184115","","",,,,,104,34.67,104,1,3,"Much contemporary rhetoric regards the prospects and pitfalls of using artificial intelligence techniques to automate an increasing range of tasks, especially those once considered the purview of people alone. These accounts are often wildly optimistic, understating outstanding challenges while turning a blind eye to the human labor that undergirds and sustains ostensibly “automated” services. This long-standing focus on purely automated methods unnecessarily cedes a promising design space: one in which computational assistance augments and enriches, rather than replaces, people’s intellectual work. This tension between human agency and machine automation poses vital challenges for design and engineering. In this work, we consider the design of systems that enable rich, adaptive interaction between people and algorithms. We seek to balance the often-complementary strengths and weaknesses of each, while promoting human control and skillful action. We share case studies of interactive systems we have developed in three arenas—data wrangling, exploratory analysis, and natural language translation—that integrate proactive computational support into interactive systems. To improve outcomes and support learning by both people and machines, we describe the use of shared representations of tasks augmented with predictive models of human capabilities and actions. We conclude with a discussion of future prospects and scientific frontiers for intelligence augmentation research.","",""
126,"Y. Mintz, Ronit Brodie","Introduction to artificial intelligence in medicine",2019,"","","","",79,"2022-07-13 09:19:12","","10.1080/13645706.2019.1575882","","",,,,,126,42.00,63,2,3,"Abstract The term Artificial Intelligence (AI) was coined by John McCarthy in 1956 during a conference held on this subject. However, the possibility of machines being able to simulate human behavior and actually think was raised earlier by Alan Turing who developed the Turing test in order to differentiate humans from machines. Since then, computational power has grown to the point of instant calculations and the ability evaluate new data, according to previously assessed data, in real time. Today, AI is integrated into our daily lives in many forms, such as personal assistants (Siri, Alexa, Google assistant etc.), automated mass transportation, aviation and computer gaming. More recently, AI has also begun to be incorporated into medicine to improve patient care by speeding up processes and achieving greater accuracy, opening the path to providing better healthcare overall. Radiological images, pathology slides, and patients’ electronic medical records (EMR) are being evaluated by machine learning, aiding in the process of diagnosis and treatment of patients and augmenting physicians’ capabilities. Herein we describe the current status of AI in medicine, the way it is used in the different disciplines and future trends.","",""
44,"H. Joo, T. Simon, M. Cikara, Yaser Sheikh","Towards Social Artificial Intelligence: Nonverbal Social Signal Prediction in a Triadic Interaction",2019,"","","","",80,"2022-07-13 09:19:12","","10.1109/CVPR.2019.01113","","",,,,,44,14.67,11,4,3,"We present a new research task and a dataset to understand human social interactions via computational methods, to ultimately endow machines with the ability to encode and decode a broad channel of social signals humans use. This research direction is essential to make a machine that genuinely communicates with humans, which we call Social Artificial Intelligence. We first formulate the ``social signal prediction'' problem as a way to model the dynamics of social signals exchanged among interacting individuals in a data-driven way. We then present a new 3D motion capture dataset to explore this problem, where the broad spectrum of social signals (3D body, face, and hand motions) are captured in a triadic social interaction scenario. Baseline approaches to predict speaking status, social formation, and body gestures of interacting individuals are presented in the defined social prediction framework.","",""
41,"B. C. Smith","The Promise of Artificial Intelligence: Reckoning and Judgment",2019,"","","","",81,"2022-07-13 09:19:12","","","","",,,,,41,13.67,41,1,3,"An argument that?despite dramatic advances in the field?artificial intelligence is nowhere near developing systems that are genuinely intelligent.In this provocative book, Brian Cantwell Smith argues that artificial intelligence is nowhere near developing systems that are genuinely intelligent. Second wave AI, machine learning, even visions of third-wave AI: none will lead to human-level intelligence and judgment, which have been honed over millennia. Recent advances in AI may be of epochal significance, but human intelligence is of a different order than even the most powerful calculative ability enabled by new computational capacities. Smith calls this AI ability ?reckoning,? and argues that it does not lead to full human judgment?dispassionate, deliberative thought grounded in ethical commitment and responsible action.Taking judgment as the ultimate goal of intelligence, Smith examines the history of AI from its first-wave origins (?good old-fashioned AI,? or GOFAI) to such celebrated second-wave approaches as machine learning, paying particular attention to recent advances that have led to excitement, anxiety, and debate. He considers each AI technology's underlying assumptions, the conceptions of intelligence targeted at each stage, and the successes achieved so far. Smith unpacks the notion of intelligence itself?what sort humans have, and what sort AI aims at. Smith worries that, impressed by AI's reckoning prowess, we will shift our expectations of human intelligence. What we should do, he argues, is learn to use AI for the reckoning tasks at which it excels while we strengthen our commitment to judgment, ethics, and the world.","",""
39,"Han-wei Liu, Ching-Fu Lin, Yu-Jie Chen","Beyond State v Loomis: artificial intelligence, government algorithmization and accountability",2019,"","","","",82,"2022-07-13 09:19:12","","10.1093/IJLIT/EAZ001","","",,,,,39,13.00,13,3,3,"Developments in data analytics, computational power, and machine learning techniques have driven all branches of the government to outsource authority to machines in performing public functions — social welfare, law enforcement, and most importantly, courts. Complex statistical algorithms and artificial intelligence (AI) tools are being used to automate decision-making and are having a significant impact on individuals’ rights and obligations. Controversies have emerged regarding the opaque nature of such schemes, the unintentional bias against and harm to underrepresented populations, and the broader legal, social, and ethical ramifications. State v. Loomis, a recent case in the United States, well demonstrates how unrestrained and unchecked outsourcing of public power to machines may undermine human rights and the rule of law. With a close examination of the case, this Article unpacks the issues of the ‘legal black box’ and the ‘technical black box’ to identify the risks posed by rampant ‘algorithmization’ of government functions to due process, equal protection, and transparency. We further assess some important governance proposals and suggest ways for improving the accountability of AI-facilitated decisions. As AI systems are commonly employed in consequential settings across jurisdictions, technologically-informed governance models are needed to locate optimal institutional designs that strike a balance between the benefits and costs of algorithmization.","",""
196,"W. Samek, K. Müller","Towards Explainable Artificial Intelligence",2019,"","","","",83,"2022-07-13 09:19:12","","10.1007/978-3-030-28954-6_1","","",,,,,196,65.33,98,2,3,"","",""
1,"A. Admin, Dr.P Dr.P.Kavitha2, A. Akshaya, P. P.Shalin, R. R.Ramya","A Survey on Cyber Security Meets Artificial Intelligence: AI– Driven Cyber Security",2022,"","","","",84,"2022-07-13 09:19:12","","10.54216/jchci.020202","","",,,,,1,1.00,0,5,1,"The computerized version of human intelligence is Artificial Intelligence(AI). Artificial Intelligence systems combine large sets of data with intelligent and iterative processing algorithms in order to make predictions, based on patterns and features in the data that they analyse. With the booming technologies such as IOT and Cloud Computing, huge amounts of data are generated and collected that require cyber security protection today. There is a growing need for cyber security methods which are both robust and intelligent due to the ever-increasing complexity of cyber crimes. While data can be used to benefit business interests, it poses a number of challenges in terms of security and privacy protection. Artificial Intelligence (AI) based technologies, such as machine learning statistics, big data analysis, deep learning and so on, have been used to deal with cyber security threats. These technologies are used for intrusion detection systems, malicious software detection, and encrypted communications. In the rapidly growing field of AI driven security, scientists from multiple disciplines work together to combat cyber threats. AI models require unique cyber security defence and protection technologies. This survey provides various method, different datasets and methodologies that may be used for the proposed IA enabled cyber security technologies. This study aims to classify the AI-based cyber security solutions gathered and describe how they can help solve problems in the field of cyber security.","",""
0,"Kristin, N. Johnson, Carla, L. Reyes","Exploring the Implications of Artificial Intelligence Exploring the Implications of Artificial Intelligence",2022,"","","","",85,"2022-07-13 09:19:12","","","","",,,,,0,0.00,0,4,1,": Emerging technologies promise to play a transformative role in our society, enabling driverless cars, enhanced accuracy and efficiency in disease mapping, greater and less expensive access to certain consumer services, including consumer financial services. Discussions regarding the role of emerging technologies increasingly center on the development and integration of artificial intelligence technologies or AI-an assemblage of technologies that rely on a variety of computational techniques. This Essay offers a modest primer outlining a general understanding of the contours and contributions of Al, as well as introducing the articulated benefits and limits of these technologies. This Essay examines the increasingly pervasive use of artificial intelligence in society through two key areas of ethical and policy concerns: (i) privacy, surveillance and the appropriate boundaries for machine-human interaction, and (ii) bias and discrimination. As we assess the merits of Al, this Essay embraces the robust and lively debate and raises probing questions initiated by scholars, activists, industry participants, and governments regarding the ethical implications of embracing Al. This Essay encourages adopters of Al to carefully consider the impacts of integrating Al on vulnerable and marginalized groups. To accomplish this goal, this Essay advocates for affected stakeholders to engage in a collaborative, interdisciplinary colloquy examining the consequences of incorporating Al technologies. Finally, this Essay serves as an introduction to a Special Issue dedicated to sharing novel thinking and approaches to address underexplored challenges posed by Al. Addressing a range of issues discussed in the debate regarding the promises and perils of Al, the contributors to this volume offer critical insights, frameworks, and tools for evaluating the issues from the perspectives of diverse stakeholders. This Special Issue seeks to shed light on some of the hidden implications of artificial intelligence on the values, institutions, and structures that form the foundation of a just society.","",""
0,"K. Sfakianoudis, E. Maziotis, S. Grigoriadis, A. Pantou, G. Kokkini, A. Trypidi, I. Angeli, T. Vaxevanoglou, K. Pantos, M. Simopoulou","O-122 Reporting on the value of Artificial Intelligence in predicting the optimal embryo for transfer: A systematic review and meta-analysis",2022,"","","","",86,"2022-07-13 09:19:12","","10.1093/humrep/deac105.022","","",,,,,0,0.00,0,10,1,"      Are Artificial Intelligence (AI) based models effective in robustly predicting in vitro fertilization (IVF) outcome by assessing embryo quality?        The majority of the AI-based models could provide an accurate prediction regarding live birth, clinical pregnancy, clinical pregnancy with fetal heartbeat and embryo ploidy status.        Precision and consistency in embryo quality evaluation are of paramount importance regarding the outcome of an IVF cycle. Numerous embryo grading and evaluation systems, employing morphological and morphokinetical assessment, have been proposed but without reaching a consensus yet. The main limitation of the aforementioned assessment systems is that they depend on human evaluation, which may be subject to subjectivity and interobserver variation. Thus, automated prediction models may be essential to optimize objectivity and reliability of embryo grading. Artificial neural network models may process microscopy images or time-lapse videos as input to predict the embryos’ potential competency.        A systematic review and meta-analysis including 18 published studies. The population consists of preimplantation embryos suitable for embryo transfer in IVF/ICSI cycles following employment of an AI-based prediction model. The outcome measures are prediction of live birth, clinical pregnancy, clinical pregnancy with heartbeat and ploidy status.        A systematic search of the literature was performed in the databases of Pubmed/Medline, Embase, and Cochrane Central Library limited to articles published in English up to August 2021. The initial search yielded a total of 694 studies with 97 of them being duplicates and other 579 being excluded on the grounds of not fulfilling inclusion criteria. Following full-text screening and citation mining a total of 18 studies were identified to be eligible for inclusion.        Four studies reported on prediction of live birth. The sensitivity was 70.6% (95%C.I.: 38.1-90.4%) and specificity was 90.6% (95%C.I.:79.3-96.1%).  The Area Under the Curve (AUC) of the Summary Receiver Operating Characteristics (SROC) curve was 0.905, while the partial AUC (pAUC) was 0.755. Employing the Bayesian approach, the total Observed:Expected ratio (O:E) was 1.12 (95%CI: 0.26–2.37; 95%PI:0.02-6.54). Ten studies reported on prediction of clinical pregnancy. The sensitivity and the specificity were 71% (95%C.I.: 58.1-81.2%) and 62.5% (95%C.I.: 47.4-75.5%) respectively. The AUC was 0.716, while pAUC was 0.693. Moreover, the total O:E ratio was 0.92 (95%CI: 0.61–1.28; 95%PI:0.13-2.43). Eight studies reported on prediction of clinical pregnancy with fetal heartbeat the sensitivity was 75.2% (95%C.I.: 66.8-82%) and the specificity was 55.3% (95%C.I.: 41.2-68.7%). The AUC was 0.722, while the pAUC was 0.774. The O:E ratio was 0.77 (95%CI: 0.54 – 1.05; 95%PI: 0.21-1.62). Four studies reported on the ploidy status of the embryo. The sensitivity and specificity were 59.4% (95%C.I.: 45.0-73.1%) and 79.2% (95%C.I.: 70.1-86.1%) respectively. The AUC was 0.751 and the pAUC was 0.585. The total O:E ratio was 0.86 (95%CI: 0.42 – 1.27; 95%PI: 0.03-1.83).        The limited number of studies fulfilling inclusion criteria, along with the different designs applied when developing AI models which may lead to increased heterogeneity, stand as limitations. Inclusion of women regardless of their age presents as another limitation, as advanced maternal age has been associated with diminished IVF outcomes.        Albeit, our findings support that AI is a highly promising tool in the era of personalized medicine providing precise predictions it does not appear to considerably surpass human prediction capabilities. More studies and more collaborations between the developers are of paramount importance prior to AI becoming the gold standard.        Not applicable ","",""
63,"S. Strohmeier, F. Piazza","Artificial Intelligence Techniques in Human Resource Management - A Conceptual Exploration",2015,"","","","",87,"2022-07-13 09:19:12","","10.1007/978-3-319-17906-3_7","","",,,,,63,9.00,32,2,7,"","",""
0,"E. Nikitos, T. Triantafillou, K. Dimitropoulos, V. Kallergi, P. Psathas, I. Erlich, A. Ben-Meir, N. Bergelson","P-271 Challenges with comparing different commercially available Artificial Intelligence (AI) systems on the same data set of time-lapse selected euploid blastocysts",2022,"","","","",88,"2022-07-13 09:19:12","","10.1093/humrep/deac107.260","","",,,,,0,0.00,0,8,1,"      To identify challenges in choosing a robust AI following comparative validation with data already pre-selected with established embryos selection tools: blastulation, morphology, time-lapse, PGTA.        Challenges included: bias; assessment against outcomes AI models were not trained on; performance metrics prioritisation; statistical methodology; continuous data cutoffs for binary clinical decision making.        AI is commercially available to be incorporated into routine practice to support embryo selection decision-making. Different clinical practices and demographics are used to train AI models, potentially impacting the prediction efficacy of the same model when used in different clinics. Fertility professionals require robust methods of validation to responsibly implement AI-based tools. Unbiased and robust frameworks for comparing AI systems in the same dataset are needed. Validating AI in a dataset of time-lapse selected euploid blastocysts using all the current methods of embryo selection currently available is the toughest assessment possible and has not previously been performed.        This study uses a retrospectively timelapse dataset collected from 2018-2021 at a single private fertility clinic. The dataset included 915 blastocysts which underwent PGTA (913 results: 381 euploids, 528 aneuploids, 4 mosaics) and 46 euploids transferred with known bhcg and ongoing clinical outcome (of which 40 resulted to live birth).  Following a prospective, comparative, observational, cohort study design, blastocysts were blindly scored using the CHLOE(FAIRTILITY) and another commercially available AI system, referred to as ‘AI-2’.        Patients aged 24-47years (average 35.4). Blastocysts selected for biopsy and transfer based on morphology and KIDScore(Vitrolife). Both AI systems were tested in the data set blindly, without any training. Correlation Regression analysis assessed correlation with KIDSCORE and relative to each AI system. Efficacy of prediction (using metrics AUC, Accuracy, Sensitivity, Specificity and Informedness) of outcomes (ploidy, biochemical and clinical pregnancy) were assessed for both AI models (CHLOEvsAI-2) by two independent statisticians to establish significance.        Regression analysis demonstrated no correlation between KIDSCORE and AI-2(r2=0.3%,p=0.5) or between CHLOE(FAIRTILITY) and AI-2(r2=0.03%,p=0.9). CHLOE(FAIRTILITY) correlated with KIDSCORE(r2=29%,p<0.001).  AI-2 was not predictive of ploidy (Euploids vs Aneuploids+mosaic: AUC=0.5,p=0.6). CHLOE(Fairtility) was predictive of ploidy(AUC=0.66, p<0.001).  Neither AI-2 or CHLOE(Fairtility) predicted which embryo the human embryologist prioritised for transfer (AI-2 vs CHLOE:accuracy:0.31vs0.49, p<0.00001). Neither AI-2 nor CHLOE(Fairtility) predicted which embryo the human embryologist prioritised for transfer (AI-2 vs CHLOE:accuracy: 0.31vs0.49, p<0.00001). There was no difference detected in efficacy of prediction of biochemical (accuracy:0.52vs0.67,NS) and ongoing clinical pregnancy (accuracy:0.53 vs 0.78,NS) by AI-2 or CHLOE. This is partly due to the low number of euploid transfers assessed (n = 46), and partly due to the fact that neither of these algorithms are trained specifically on predicting outcome of euploid transfers.  CHLOE(Fairtility) was more specific than AI-2 for predicting selection for transfer(0.44/0.80vs0.17/0.93,p<0.05/NS) and ploidy(0.54/0.77vs0.23/0.87,p<0.05/NS), and they were equally as sensitive. CHLOE(Fairtility) was more sensitive, and less specific than AI-2 for predicting biochemical pregnancy(0.36/0.81vs0.86/0.38,p<0.05) and more sensitive but equally as specific for predicting clinical pregnancy(0.33/0.88vs0.83/0.46,NS/p<0.05).  Informedness was positive for both CHLOE(Fairtility) and AI-2 in predicting all outcomes assessed. Informedness was greater for AI-2 for predicting morphology(AI-2vsCHLOE:0.16vs0.31,p<0.05), transfer(0.11vs0.24,p<0.05), ploidy(0.10vs0.31,p<0.05) and equivalent for predicting biochemical (0.23vs0.17,NS) and clinical pregnancy(0.29vs0.22,NS).        In this single clinic study, both algorithms were assessed against outcomes (live birth following transfer of time-lapse cultured euploid blastocysts) for which they were not trained on: AI-2(designed for ploidy prediction) and CHLOE(FAIRTILITY, implantation prediction of non-PGTA embryos) and no clinic data was used for training.        The only way to decide which AI model is more useful is by a direct comparison of two or more models on the same dataset with same outcomes and metrics, as recommended by TRIPOD. To date, this is the first publication comparing multiple commercial AI models on the same dataset.        NA ","",""
23,"B. Chin-Yee, Ross E. G. Upshur","Three Problems with Big Data and Artificial Intelligence in Medicine",2019,"","","","",89,"2022-07-13 09:19:12","","10.1353/pbm.2019.0012","","",,,,,23,7.67,12,2,3,"ABSTRACT:The rise of big data and artificial intelligence (AI) in health care has engendered considerable excitement, claiming to improve approaches to diagnosis, prognosis, and treatment. Amidst the enthusiasm, the philosophical assumptions that underlie the big data and AI movement in medicine are rarely examined. This essay outlines three philosophical challenges faced by this movement: (1) the epistemological-ontological problem arising from the theory-ladenness of big data and measurement; (2) the epistemological-logical problem resulting from the inherent limitations of algorithms and attendant issues of reliability and interpretability; and (3) the phenomenological problem concerning the irreducibility of human experience to quantitative data. These philosophical issues demonstrate several important challenges for these technologies that must be considered prior to their integration into clinical care. Our article aims to initiate a critical dialogue on the impact of big data and AI in health care in order to allow for more robust evaluation of these technologies and to aid in the development of approaches to clinical care that better serve clinicians and their patients.","",""
21,"Li-Qi Shu, Yi-Kan Sun, L. Tan, Q. Shu, A. Chang","Application of artificial intelligence in pediatrics: past, present and future",2019,"","","","",90,"2022-07-13 09:19:12","","10.1007/s12519-019-00255-1","","",,,,,21,7.00,4,5,3,"","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",91,"2022-07-13 09:19:12","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
40,"Adriana Braga, R. Logan","The Emperor of Strong AI Has No Clothes: Limits to Artificial Intelligence",2017,"","","","",92,"2022-07-13 09:19:12","","10.3390/info8040156","","",,,,,40,8.00,20,2,5,"Making use of the techniques of media ecology we argue that the premise of the technological Singularity based on the notion computers will one day be smarter that their human creators is false. We also analyze the comments of other critics of the Singularity, as well supporters of this notion. The notion of intelligence that advocates of the technological singularity promote does not take into account the full dimension of human intelligence. They treat artificial intelligence as a figure without a ground. Human intelligence as we will show is not based solely on logical operations and computation, but also includes a long list of other characteristics that are unique to humans, which is the ground that supporters of the Singularity ignore. The list includes curiosity, imagination, intuition, emotions, passion, desires, pleasure, aesthetics, joy, purpose, objectives, goals, telos, values, morality, experience, wisdom, judgment, and even humor.","",""
18,"T. L. Jaynes","Legal personhood for artificial intelligence: citizenship as the exception to the rule",2019,"","","","",93,"2022-07-13 09:19:12","","10.1007/s00146-019-00897-9","","",,,,,18,6.00,18,1,3,"","",""
18,"S. Costantini, G. D. Gasperis, R. Olivieri","Digital forensics and investigations meet artificial intelligence",2019,"","","","",94,"2022-07-13 09:19:12","","10.1007/s10472-019-09632-y","","",,,,,18,6.00,6,3,3,"","",""
16,"Bob L. Sturm, Maria Iglesias, Oded Ben-Tal, M. Miron, Emilia Gómez","Artificial Intelligence and Music: Open Questions of Copyright Law and Engineering Praxis",2019,"","","","",95,"2022-07-13 09:19:12","","10.3390/arts8030115","","",,,,,16,5.33,3,5,3,"The application of artificial intelligence (AI) to music stretches back many decades, and presents numerous unique opportunities for a variety of uses, such as the recommendation of recorded music from massive commercial archives, or the (semi-)automated creation of music. Due to unparalleled access to music data and effective learning algorithms running on high-powered computational hardware, AI is now producing surprising outcomes in a domain fully entrenched in human creativity—not to mention a revenue source around the globe. These developments call for a close inspection of what is occurring, and consideration of how it is changing and can change our relationship with music for better and for worse. This article looks at AI applied to music from two perspectives: copyright law and engineering praxis. It grounds its discussion in the development and use of a specific application of AI in music creation, which raises further and unanticipated questions. Most of the questions collected in this article are open as their answers are not yet clear at this time, but they are nonetheless important to consider as AI technologies develop and are applied more widely to music, not to mention other domains centred on human creativity.","",""
15,"Xiao-Guang Han, W. Tian","Artificial intelligence in orthopedic surgery: current state and future perspective",2019,"","","","",96,"2022-07-13 09:19:12","","10.1097/CM9.0000000000000479","","",,,,,15,5.00,8,2,3,"Artificial intelligence (AI), first proposed by Prof. John AI helps the radiologist to improve the diagnostic accuracy McCarthy in 1956, aims to reproduce human intelligence and prevent errors and observer fatigue. AI algorithms using computers. Machine learning (ML) is a form of AI that uses computational algorithms that learn and improve with experience. The two main forms of ML are supervised and unsupervised. In supervised ML, algorithms are given labeled data, which is used to predict disease outcomes in a new patient. In contrast, unsupervised ML is used to identify patterns without training; the algorithm learns the inherent structure of the data by searching for common characteristics.","",""
2,"J. Johanssen, Xin Wang","Artificial Intuition in Tech Journalism on AI: Imagining the Human Subject",2021,"","","","",97,"2022-07-13 09:19:12","","10.30658/HMC.2.9","","",,,,,2,2.00,1,2,1,"Artificial intuition (AI acting intuitively) is one trend in artificial intelligence. This article analyzes how it is discussed by technology journalism on the internet. The journalistic narratives that were analyzed claim that intuition can make AI more efficient, autonomous, and human. Some commentators also write that intuitive AI could execute tasks better than humans themselves ever could (e.g., in digital games); therefore, it could ultimately surpass human intuition. Such views do not pay enough attention to biases as well as transparency and explainability of AI. We contrast the journalistic narratives with philosophical understandings of intuition and a psychoanalytic view of the human. Those perspectives allow for a more complex view that goes beyond the focus on rationality and computational perspectives of tech journalism.","",""
0,"Sandro Valerio Silva, Tobias Andermann, Alexander Zizka, G. Kozlowski, D. Silvestro","Global Estimation and Mapping of the Conservation Status of Tree Species Using Artificial Intelligence",2022,"","","","",98,"2022-07-13 09:19:12","","10.3389/fpls.2022.839792","","",,,,,0,0.00,0,5,1,"Trees are fundamental for Earth’s biodiversity as primary producers and ecosystem engineers and are responsible for many of nature’s contributions to people. Yet, many tree species at present are threatened with extinction by human activities. Accurate identification of threatened tree species is necessary to quantify the current biodiversity crisis and to prioritize conservation efforts. However, the most comprehensive dataset of tree species extinction risk—the Red List of the International Union for the Conservation of Nature (IUCN RL)—lacks assessments for a substantial number of known tree species. The RL is based on a time-consuming expert-based assessment process, which hampers the inclusion of less-known species and the continued updating of extinction risk assessments. In this study, we used a computational pipeline to approximate RL extinction risk assessments for more than 21,000 tree species (leading to an overall assessment of 89% of all known tree species) using a supervised learning approach trained based on available IUCN RL assessments. We harvested the occurrence data for tree species worldwide from online databases, which we used with other publicly available data to design features characterizing the species’ geographic range, biome and climatic affinities, and exposure to human footprint. We trained deep neural network models to predict their conservation status, based on these features. We estimated 43% of the assessed tree species to be threatened with extinction and found taxonomic and geographic heterogeneities in the distribution of threatened species. The results are consistent with the recent estimates by the Global Tree Assessment initiative, indicating that our approach provides robust and time-efficient approximations of species’ IUCN RL extinction risk assessments.","",""
0,"Muhammad Taseer Suleman, Y. Khan","m1A-pred: Prediction of modified 1-methyladenosine sites in RNA sequences through artificial intelligence.",2022,"","","","",99,"2022-07-13 09:19:12","","10.2174/1386207325666220617152743","","",,,,,0,0.00,0,2,1,"BACKGROUND The process of nucleotides modification or methyl groups addition to nucleotides is known as post-transcriptional modification (PTM). 1-methyladenosine (m1A) is a type of PTM formed by adding a methyl group to the nitrogen at the 1st position of the adenosine base. Many human disorders are associated with m1A, which is widely found in ribosomal RNA and transfer RNA.   OBJECTIVE The conventional methods such as mass spectrometry and site-directed mutagenesis proved to be laborious and burdensome. Systematic identification of modified sites from RNA sequences is gaining much attention nowadays. Consequently, an extreme gradient boost predictor, m1A-Pred, is developed in this study for the prediction of modified m1A sites.   METHOD The current study involves the extraction of position and composition-based properties within nucleotide sequences. The extraction of features helps in the development of the features vector. Statistical moments were endorsed for dimensionality reduction in the obtained features.   RESULTS Through a series of experiments using different computational models and evaluation methods, it was revealed that the proposed predictor, m1A-pred, proved to be the most robust and accurate model for the identification of modified sites.   AVAILABILITY AND IMPLEMENTATION To enhance the research on m1A sites, a friendly server was also developed which was the final phase of this research.","",""
32,"R. Grassi, V. Miele, A. Giovagnoni","Artificial intelligence: a challenge for third millennium radiologist",2019,"","","","",100,"2022-07-13 09:19:12","","10.1007/s11547-019-00990-5","","",,,,,32,10.67,11,3,3,"","",""
0,"Muhammad Taseer Suleman, Y. Khan","m1A-pred: Prediction of modified 1-methyladenosine sites in RNA sequences through artificial intelligence.",2022,"","","","",101,"2022-07-13 09:19:12","","10.2174/1386207325666220617152743","","",,,,,0,0.00,0,2,1,"BACKGROUND The process of nucleotides modification or methyl groups addition to nucleotides is known as post-transcriptional modification (PTM). 1-methyladenosine (m1A) is a type of PTM formed by adding a methyl group to the nitrogen at the 1st position of the adenosine base. Many human disorders are associated with m1A, which is widely found in ribosomal RNA and transfer RNA.   OBJECTIVE The conventional methods such as mass spectrometry and site-directed mutagenesis proved to be laborious and burdensome. Systematic identification of modified sites from RNA sequences is gaining much attention nowadays. Consequently, an extreme gradient boost predictor, m1A-Pred, is developed in this study for the prediction of modified m1A sites.   METHOD The current study involves the extraction of position and composition-based properties within nucleotide sequences. The extraction of features helps in the development of the features vector. Statistical moments were endorsed for dimensionality reduction in the obtained features.   RESULTS Through a series of experiments using different computational models and evaluation methods, it was revealed that the proposed predictor, m1A-pred, proved to be the most robust and accurate model for the identification of modified sites.   AVAILABILITY AND IMPLEMENTATION To enhance the research on m1A sites, a friendly server was also developed which was the final phase of this research.","",""
0,"Sandro Valerio Silva, Tobias Andermann, Alexander Zizka, G. Kozlowski, D. Silvestro","Global Estimation and Mapping of the Conservation Status of Tree Species Using Artificial Intelligence",2022,"","","","",102,"2022-07-13 09:19:12","","10.3389/fpls.2022.839792","","",,,,,0,0.00,0,5,1,"Trees are fundamental for Earth’s biodiversity as primary producers and ecosystem engineers and are responsible for many of nature’s contributions to people. Yet, many tree species at present are threatened with extinction by human activities. Accurate identification of threatened tree species is necessary to quantify the current biodiversity crisis and to prioritize conservation efforts. However, the most comprehensive dataset of tree species extinction risk—the Red List of the International Union for the Conservation of Nature (IUCN RL)—lacks assessments for a substantial number of known tree species. The RL is based on a time-consuming expert-based assessment process, which hampers the inclusion of less-known species and the continued updating of extinction risk assessments. In this study, we used a computational pipeline to approximate RL extinction risk assessments for more than 21,000 tree species (leading to an overall assessment of 89% of all known tree species) using a supervised learning approach trained based on available IUCN RL assessments. We harvested the occurrence data for tree species worldwide from online databases, which we used with other publicly available data to design features characterizing the species’ geographic range, biome and climatic affinities, and exposure to human footprint. We trained deep neural network models to predict their conservation status, based on these features. We estimated 43% of the assessed tree species to be threatened with extinction and found taxonomic and geographic heterogeneities in the distribution of threatened species. The results are consistent with the recent estimates by the Global Tree Assessment initiative, indicating that our approach provides robust and time-efficient approximations of species’ IUCN RL extinction risk assessments.","",""
32,"R. Grassi, V. Miele, A. Giovagnoni","Artificial intelligence: a challenge for third millennium radiologist",2019,"","","","",103,"2022-07-13 09:19:12","","10.1007/s11547-019-00990-5","","",,,,,32,10.67,11,3,3,"","",""
32,"Mark O. Riedl","Computational Narrative Intelligence: A Human-Centered Goal for Artificial Intelligence",2016,"","","","",104,"2022-07-13 09:19:12","","","","",,,,,32,5.33,32,1,6,"Narrative intelligence is the ability to craft, tell, understand, and respond affectively to stories. We argue that instilling artificial intelligences with computational narrative intelligence affords a number of applications beneficial to humans. We lay out some of the machine learning challenges necessary to solve to achieve computational narrative intelligence. Finally, we argue that computational narrative is a practical step towards machine enculturation, the teaching of sociocultural values to machines.","",""
44,"T. Alexandrov","Spatial Metabolomics and Imaging Mass Spectrometry in the Age of Artificial Intelligence.",2020,"","","","",105,"2022-07-13 09:19:12","","10.1146/annurev-biodatasci-011420-031537","","",,,,,44,22.00,44,1,2,"Spatial metabolomics is an emerging field of omics research that has enabled localizing metabolites, lipids, and drugs in tissue sections, a feat considered impossible just two decades ago. Spatial metabolomics and its enabling technology-imaging mass spectrometry-generate big hyper-spectral imaging data that have motivated the development of tailored computational methods at the intersection of computational metabolomics and image analysis. Experimental and computational developments have recently opened doors to applications of spatial metabolomics in life sciences and biomedicine. At the same time, these advances have coincided with a rapid evolution in machine learning, deep learning, and artificial intelligence, which are transforming our everyday life and promise to revolutionize biology and healthcare. Here, we introduce spatial metabolomics through the eyes of a computational scientist, review the outstanding challenges, provide a look into the future, and discuss opportunities granted by the ongoing convergence of human and artificial intelligence.","",""
34,"A. Borkowski, N. Viswanadham, L. B. Thomas, R. D. Guzmán, L. Deland, S. Mastorides","Using Artificial Intelligence for COVID-19 Chest X-ray Diagnosis",2020,"","","","",106,"2022-07-13 09:19:12","","10.1101/2020.05.21.20106518","","",,,,,34,17.00,6,6,2,"Coronavirus disease-19 (COVID-19), caused by a novel member of the coronavirus family, is a respiratory disease that rapidly reached pandemic proportions with high morbidity and mortality. It has had a dramatic impact on society and world economies in only a few months. COVID-19 presents numerous challenges to all aspects of healthcare, including reliable methods for diagnosis, treatment, and prevention. Initial efforts to contain the spread of the virus were hampered by the time required to develop reliable diagnostic methods. Artificial intelligence (AI) is a rapidly growing field of computer science with many applications to healthcare. Machine learning is a subset of AI that employs deep learning with neural network algorithms. It can recognize patterns and achieve complex computational tasks often far quicker and with increased precision than humans. In this manuscript, we explore the potential for a simple and widely available test as a chest x-ray (CXR) to be utilized with AI to diagnose COVID-19 reliably. Microsoft CustomVision is an automated image classification and object detection system that is a part of Microsoft Azure Cognitive Services. We utilized publicly available CXR images for patients with COVID-19 pneumonia, pneumonia from other etiologies, and normal CXRs as a dataset to train Microsoft CustomVision. Our trained model overall demonstrated 92.9% sensitivity (recall) and positive predictive value (precision), with results for each label showing sensitivity and positive predictive value at 94.8% and 98.9% for COVID-19 pneumonia, 89% and 91.8% for non-COVID-19 pneumonia, 95% and 88.8% for normal lung. We then validated the program using CXRs of patients from our institution with confirmed COVID-19 diagnoses along with non-COVID-19 pneumonia and normal CXRs. Our model performed with 100% sensitivity, 95% specificity, 97% accuracy, 91% positive predictive value, and 100% negative predictive value. Finally, we developed and described a publicly available website to demonstrate how this technology can be made readily available in the future.","",""
13,"K. Schaefer, Jean Oh, Derya Aksaray, D. Barber","Integrating Context into Artificial Intelligence: Research from the Robotics Collaborative Technology Alliance",2019,"","","","",107,"2022-07-13 09:19:12","","10.1609/aimag.v40i3.2865","","",,,,,13,4.33,3,4,3,"Applying context to a situation, task, or system state provides meaning and advances understanding that can affect future decisions or actions. Although people are naturally good at perceiving contextual understanding and inferring missing pieces of information using various alternative sources, this process is difficult for AI systems or robots, especially in high-uncertainty and unstructured operations. Integration of context-driven AI is important for future robotic capabilities to support the development of situation awareness, calibrate appropriate trust, and improve team performance in collaborative human-robot teams. This article highlights advances in context-driven AI for human-robot teaming by the Army Research Laboratory’s Robotics Collaborative Technology Alliance. Avenues of research discussed include how context enables robots to fill in the gaps to make effective decisions more quickly, supports more robust behaviors, and augments robot communications to suit the needs of the team under a variety of environments and team organizations and across missions.","",""
0,"S. Kumar","Abstract PO-056: Importance of artificial intelligence, machine learning deep learning in the field of medicine on the future role of the physician",2021,"","","","",108,"2022-07-13 09:19:12","","10.1158/1557-3265.ADI21-PO-056","","",,,,,0,0.00,0,1,1,"There are many ways to define the field of Artificial Intelligence. Here is one way for Artificial Intelligence is ""The Study of the computations that make it possible to perceive, reason, act and predict the future possible outcomes”. Deep learning, which is a popular research area of artificial intelligence (AI), enables the creation of end-to-end models to achieve promised results using input data. Deep learning techniques have been successfully applied in many problems such as arrhythmia detection, skin cancer classification, breast cancer detection, brain disease classification, pneumonia detection, COVID-19 from chest X-ray images, and CT scan images. Almost all hospitals have CT imaging machines; therefore, the chest CT images can be utilized for early classification of diseases. However, the chest CT classification involves a radiology expert and considerable time, which is valuable when any infection is growing at a rapid rate. Therefore, automated analysis of chest CT images is desirable to save the medical professionals precious time that shows the importance of Artificial Intelligence neural networks which is used to classify the infected patients as infected (+ve) or not (−ve). There is a vital need to detect the disease at an early stage and save the patient from the disease. Convolutional neural networks (CNN) are a powerful tool that comes under the platform of Neural Networks – Artificial Intelligence inspired by the human brain, which is extensively utilized for image classification. The hierarchical structure and efficient feature extraction characteristics from an image make CNN a dynamic model for image classification. Initially, the layers are organized into three dimensions: width, height, and depth. The neurons in a given layer do not attach to the entire set of neurons in the later layer, but only to limited neurons of it. Finally, the output is diminished to a single vector of probability scores, coordinated alongside the depth dimension. In a Convolutional Neural Network, the linear function that is used is called a convolutional layer. Each node in the hidden layer extracts different features by using image processing feature detectors. For example, in the first layer, the first node may extract the horizontal edges of an image, the second node may extract vertical edges and etc. These features are extracted using a kernel. The bottom is the original image and the top is the output of the convolutions. It is also worth noting that the output of the convolutions reduces the dimension of the original image, The next step the pooling layer happens tends to be computed after the convolutional layer. The reason why pooling is done is to further reduce the dimensions of the convolutional layer and just extract out the features to make the model more robust. AI could help to rapidly diagnose diseases if proper attention given in collecting the data. Citation Format: Subash Kumar. Importance of artificial intelligence, machine learning deep learning in the field of medicine on the future role of the physician [abstract]. In: Proceedings of the AACR Virtual Special Conference on Artificial Intelligence, Diagnosis, and Imaging; 2021 Jan 13-14. Philadelphia (PA): AACR; Clin Cancer Res 2021;27(5_Suppl):Abstract nr PO-056.","",""
7,"D. G. Harkut, K. Kasat","Introductory Chapter: Artificial Intelligence - Challenges and Applications",2019,"","","","",109,"2022-07-13 09:19:12","","10.5772/INTECHOPEN.84624","","",,,,,7,2.33,4,2,3,"Artificial intelligence (AI) is any task performed by program or machine, which otherwise human needs to apply intelligence to accomplish it. It is the science and engineering of making machines to demonstrate intelligence especially visual perception, speech recognition, decision-making, and translation between languages like human beings. AI is the simulation of human intelligence processes by machines, especially computer systems. This includes learning, reasoning, planning, self-correction, problem solving, knowledge representation, perception, motion, manipulation, and creativity. It is a science and a set of computational techniques that are inspired by the way in which human beings use their nervous system and their body to feel, learn, reason, and act. AI is related to machine learning and deep learning wherein machine learning makes use of algorithms to discover patterns and generate insights from the data they are working on. Deep learning is a subset of machine learning, one that brings AI closer to the goal of enabling machines to think and work as human as possible. AI is a debatable topic and is often represented in a negative way; some would call it a blessing in disguise for businesses, while for some it is a technology that endangers the mere existence of humankind as it is potentially capable of taking over and dominating human being, but in reality artificial intelligence has affected our lifestyle either directly or indirectly and shaping the future of tomorrow. AI has already become an intrinsic part of our daily life and has greatly impacted our lifestyle despite the imperative uses of digital assistants of mobile phones, driverassistance systems, the bots, texts and speech translators, and systems that assist in recommending products and services and customized learning. Every emerging technology is a source of both enthusiasm and skepticism. AI is a source of both advantages and disadvantages in different perspectives. However, we need to overcome certain challenges before we can realize the true potential and immense transformational capabilities of this emerging technology. Some of the challenges related to artificial intelligence are:","",""
7,"J. Moore, N. Raghavachari","Artificial Intelligence Based Approaches to Identify Molecular Determinants of Exceptional Health and Life Span-An Interdisciplinary Workshop at the National Institute on Aging",2019,"","","","",110,"2022-07-13 09:19:12","","10.3389/frai.2019.00012","","",,,,,7,2.33,4,2,3,"Artificial intelligence (AI) has emerged as a powerful approach for integrated analysis of the rapidly growing volume of multi-omics data, including many research and clinical tasks such as prediction of disease risk and identification of potential therapeutic targets. However, the potential for AI to facilitate the identification of factors contributing to human exceptional health and life span and their translation into novel interventions for enhancing health and life span has not yet been realized. As researchers on aging acquire large scale data both in human cohorts and model organisms, emerging opportunities exist for the application of AI approaches to untangle the complex physiologic process(es) that modulate health and life span. It is expected that efficient and novel data mining tools that could unravel molecular mechanisms and causal pathways associated with exceptional health and life span could accelerate the discovery of novel therapeutics for healthy aging. Keeping this in mind, the National Institute on Aging (NIA) convened an interdisciplinary workshop titled “Contributions of Artificial Intelligence to Research on Determinants and Modulation of Health Span and Life Span” in August 2018. The workshop involved experts in the fields of aging, comparative biology, cardiology, cancer, and computational science/AI who brainstormed ideas on how AI can be leveraged for the analyses of large-scale data sets from human epidemiological studies and animal/model organisms to close the current knowledge gaps in processes that drive exceptional life and health span. This report summarizes the discussions and recommendations from the workshop on future application of AI approaches to advance our understanding of human health and life span.","",""
26,"Yingxu Wang, W. Kinsner, S. Kwong, Henry Leung, Jianhua Lu, Michael H. Smith, L. Trajković, E. Tunstel, K. Plataniotis, G. Yen","Brain-Inspired Systems: A Transdisciplinary Exploration on Cognitive Cybernetics, Humanity, and Systems Science Toward Autonomous Artificial Intelligence",2020,"","","","",111,"2022-07-13 09:19:12","","10.1109/MSMC.2018.2889502","","",,,,,26,13.00,3,10,2,"Brain-inspired cognitive systems (BCSs) are an emerging field of cybernetics, cognitive science, and system science. BCSs study not only the intelligence science foundations of artificial intelligence (AI) and cognitive systems, but also formal models of the brain embodied by computational intelligence. This article presents the brain and intelligence science foundations of BCS toward hybrid intelligent systems and the symbiotic intelligence of humanity. It explores the transdisciplinary theoretical foundations of system, brain, intelligence, knowledge, cybernetic, and cognitive sciences toward the next generation of knowledge processors beyond classic data processors for autonomous computing systems. A BCS provides an overarching platform for cognitive cybernetics, humanity, and systems to enable emerging hybrid societies shared by humans and intelligent machines.","",""
2,"Luis Pérez-Breva, John H. Shin","Artificial Intelligence in Neurosurgery: A Comment on the Possibilities",2019,"","","","",112,"2022-07-13 09:19:12","","10.14245/ns.1938404.202","","",,,,,2,0.67,1,2,3,"To the editor What people call artificial intelligence (AI) has begun to permeate our work and home environments. It provides customer service to consumers, suggests travel routes, and figures out when to turn up thermostats in our homes. It promises to empower precision medicine, handling of medical records, and eventually even replace human drivers. Professionals of all sorts turn to AI applications as “partners” in the work they do. How much should you believe? And most importantly, how can it help neurosurgeons? So-called generalized intelligence remains a distant, elusive aspiration. But there are ample opportunities to avail ourselves to the tools of AI to push the envelope and help discover and answer new questions in myriad fields, including neurosurgery. That requires understanding what the tools can do and how to phrase problems in neurology and neurosurgery to overcome the many limitations of these AI tools and make the most out of them. When the editors of Neurospine suggested we write a commentary, another particularly intriguing opportunity rose to mind: understanding how the neurology and neurosurgery community might benefit from the tools of AI could also help AI itself. The AI community has always hoped to gain inspiration from the way our brains (and entire perceptual and mechanical apparati) might work. Cognitive science has provided some of that, but as computational technology makes AI tools increasingly accessible for neurosurgery research, is there room to imagine collaborations that inform new opportunities in neurosurgery and new insights for AI following from a finer understanding among computer scientists of the phenomenally complex, robust architectures that support what we call “intelligence”? To get there, we need a shared understanding of what AI tools can and cannot yet do. Think of there being 2 ways to use AI. One is typically associated with analytics, regression, classification models, and statistical learning. These tools make the most sense when you have plenty of data. As long as you reduce a problem to a single factor and you have enough data, these tools can power fairly sophisticated software that interprets medical images, anticipates outcomes, or helps spot correlative trends you had not noticed. In these cases, AI tools are not providing fundamentally “new” insights; this is just modeling that may be extraordinarily complex and beyond most human comprehension. And just about everyone seems to think the magic formula for this is more data. Using AI tools this way is the most common and simplest. It is also the source of much of the common confusion—and, frankly, panic—about AI, which most people tend to think of as doing what humans already do, but better—such as outperforming humans at, say, Neurospine 2019;16(4):640-642. https://doi.org/10.14245/ns.1938404.202 Neurospine","",""
1,"H. Yamakawa","Peacekeeping Conditions for an Artificial Intelligence Society",2019,"","","","",113,"2022-07-13 09:19:12","","10.3390/BDCC3020034","","",,,,,1,0.33,1,1,3,"In a human society with emergent technology, the destructive actions of some pose a danger to the survival of all of humankind, increasing the need to maintain peace by overcoming universal conflicts. However, human society has not yet achieved complete global peacekeeping. Fortunately, a new possibility for peacekeeping among human societies using the appropriate interventions of an advanced system will be available in the near future. To achieve this goal, an artificial intelligence (AI) system must operate continuously and stably (condition 1) and have an intervention method for maintaining peace among human societies based on a common value (condition 2). However, as a premise, it is necessary to have a minimum common value upon which all of human society can agree (condition 3). In this study, an AI system to achieve condition 1 was investigated. This system was designed as a group of distributed intelligent agents (IAs) to ensure robust and rapid operation. Even if common goals are shared among all IAs, each autonomous IA acts on each local value to adapt quickly to each environment that it faces. Thus, conflicts between IAs are inevitable, and this situation sometimes interferes with the achievement of commonly shared goals. Even so, they can maintain peace within their own societies if all the dispersed IAs think that all other IAs aim for socially acceptable goals. However, communication channel problems, comprehension problems, and computational complexity problems are barriers to realization. This problem can be overcome by introducing an appropriate goal-management system in the case of computer-based IAs. Then, an IA society could achieve its goals peacefully, efficiently, and consistently. Therefore, condition 1 will be achievable. In contrast, humans are restricted by their biological nature and tend to interact with others similar to themselves, so the eradication of conflicts is more difficult.","",""
0,"Yaxin Peng, S. Du, T. Zeng","Preface: Special Issue on Optimization Models and Algorithms in Artificial Intelligence",2019,"","","","",114,"2022-07-13 09:19:12","","10.1007/s40305-019-00278-5","","",,,,,0,0.00,0,3,3,"","",""
19,"Sandip K. Patel, Bhawana George, Vineeta Rai","Artificial Intelligence to Decode Cancer Mechanism: Beyond Patient Stratification for Precision Oncology",2020,"","","","",115,"2022-07-13 09:19:12","","10.3389/fphar.2020.01177","","",,,,,19,9.50,6,3,2,"The multitude of multi-omics data generated cost-effectively using advanced high-throughput technologies has imposed challenging domain for research in Artificial Intelligence (AI). Data curation poses a significant challenge as different parameters, instruments, and sample preparations approaches are employed for generating these big data sets. AI could reduce the fuzziness and randomness in data handling and build a platform for the data ecosystem, and thus serve as the primary choice for data mining and big data analysis to make informed decisions. However, AI implication remains intricate for researchers/clinicians lacking specific training in computational tools and informatics. Cancer is a major cause of death worldwide, accounting for an estimated 9.6 million deaths in 2018. Certain cancers, such as pancreatic and gastric cancers, are detected only after they have reached their advanced stages with frequent relapses. Cancer is one of the most complex diseases affecting a range of organs with diverse disease progression mechanisms and the effectors ranging from gene-epigenetics to a wide array of metabolites. Hence a comprehensive study, including genomics, epi-genomics, transcriptomics, proteomics, and metabolomics, along with the medical/mass-spectrometry imaging, patient clinical history, treatments provided, genetics, and disease endemicity, is essential. Cancer Moonshot℠ Research Initiatives by NIH National Cancer Institute aims to collect as much information as possible from different regions of the world and make a cancer data repository. AI could play an immense role in (a) analysis of complex and heterogeneous data sets (multi-omics and/or inter-omics), (b) data integration to provide a holistic disease molecular mechanism, (c) identification of diagnostic and prognostic markers, and (d) monitor patient’s response to drugs/treatments and recovery. AI enables precision disease management well beyond the prevalent disease stratification patterns, such as differential expression and supervised classification. This review highlights critical advances and challenges in omics data analysis, dealing with data variability from lab-to-lab, and data integration. We also describe methods used in data mining and AI methods to obtain robust results for precision medicine from “big” data. In the future, AI could be expanded to achieve ground-breaking progress in disease management.","",""
2,"Lahiru L. Abeysekara, H. Abdi","Short Paper: Neuromorphic Chip Embedded Electronic Systems to Expand Artificial Intelligence",2019,"","","","",116,"2022-07-13 09:19:12","","10.1109/AI4I46381.2019.00038","","",,,,,2,0.67,1,2,3,"Neuromorphic chips are electronic hardware mimicking neurons in human brain in an electronic structure. These ASICs (Application Specific Integrated Circuits) provide artificial neural networks with computational power comparatively higher than most neural networks generated by software algorithms. 'CM1K' is an electronic chip in this family of products. It has a parallel neural network of 1024 neurons. These neurons provide K-Nearest Neighbor (KNN) data classification. The chip requires to be embedded in an electronic system to access all its capabilities. This paper deliver a novel hardware system embedding CM1K neuromorphic chip. The system was implemented in image and video frame analysis for evaluation. The results prove that the system could benefit various applications including security, asset management, home appliances, mail sorting and manufacturing. Since the embedded system provide opportunity to integrate AI in to simple electronics, it helps on extending AI applications.","",""
0,"Benjamin J. Wortman, J. Z. Wang","HICEM: A High-Coverage Emotion Model for Artificial Emotional Intelligence",2022,"","","","",117,"2022-07-13 09:19:12","","10.48550/arXiv.2206.07593","","",,,,,0,0.00,0,2,1,"As social robots and other intelligent machines enter the home, artificial emotional intelligence (AEI) is taking center stage to address users’ desire for deeper, more meaningful human-machine interaction. To accomplish such efficacious interaction, the next-generation AEI need comprehensive human emotion models for training. Unlike theory of emotion, which has been the historical focus in psychology, emotion models are a descriptive tools. In practice, the strongest models need robust coverage, which means defining the smallest core set of emotions from which all others can be derived. To achieve the desired coverage, we turn to word embeddings from natural language processing. Using unsupervised clustering techniques, our experiments show that with as few as 15 discrete emotion categories, we can provide maximum coverage across six major languages–Arabic, Chinese, English, French, Spanish, and Russian. In support of our findings, we also examine annotations from two large-scale emotion recognition datasets to assess the validity of existing emotion models compared to human perception at scale. Because robust, comprehensive emotion models are foundational for developing real-world affective computing applications, this work has broad implications in social robotics, human-machine interaction, mental healthcare, and computational psychology.","",""
18,"Ahmed Gowida, Salaheldin Elkatatny, Saad F. K. Al-Afnan, A. Abdulraheem","New Computational Artificial Intelligence Models for Generating Synthetic Formation Bulk Density Logs While Drilling",2020,"","","","",118,"2022-07-13 09:19:12","","10.3390/su12020686","","",,,,,18,9.00,5,4,2,"Synthetic well log generation using artificial intelligence tools is a robust solution for situations in which logging data are not available or are partially lost. Formation bulk density (RHOB) logging data greatly assist in identifying downhole formations. These data are measured in the field while drilling by using a density log tool in the form of either a logging while drilling (LWD) technique or (more often) by wireline logging after the formations are drilled. This is due to operational limitations during the drilling process. Therefore, the objective of this study was to develop a predictive tool for estimating RHOB while drilling using an adaptive network-based fuzzy interference system (ANFIS), functional network (FN), and support vector machine (SVM). The proposed model uses the mechanical drilling constraints as feeding input parameters, and the conventional RHOB log data as an output parameter. These mechanical drilling parameters are usually measured while drilling, and their responses vary with different formations. A dataset of 2400 actual datapoints, obtained from a horizontal well in the Middle East, were used to build the proposed models. The obtained dataset was divided into a 70/30 ratio for model training and testing, respectively. The optimized ANFIS-based model outperformed the FN- and SVM-based models with a correlation coefficient (R) of 0.93, and average absolute percentage error (AAPE) of 0.81% between the predicted and measured RHOB values. These results demonstrate the reliability of the developed ANFIS model for predicting RHOB while drilling, based on the mechanical drilling parameters. Subsequently, the ANFIS-based model was validated using unseen data from another well within the same field. The validation process yielded an AAPE of 0.97% between the predicted and actual RHOB values, which confirmed the robustness of the developed model as an effective predictive tool for RHOB.","",""
14,"Puneet S. Sharma, M. Suehling, T. Flohr, D. Comaniciu","Artificial Intelligence in Diagnostic Imaging: Status Quo, Challenges, and Future Opportunities.",2020,"","","","",119,"2022-07-13 09:19:12","","10.1097/RTI.0000000000000499","","",,,,,14,7.00,4,4,2,"In this review article, the current and future impact of artificial intelligence (AI) technologies on diagnostic imaging is discussed, with a focus on cardio-thoracic applications. The processing of imaging data is described at 4 levels of increasing complexity and wider implications. At the examination level, AI aims at improving, simplifying, and standardizing image acquisition and processing. Systems for AI-driven automatic patient iso-centering before a computed tomography (CT) scan, patient-specific adaptation of image acquisition parameters, and creation of optimized and standardized visualizations, for example, automatic rib-unfolding, are discussed. At the reading and reporting levels, AI focuses on automatic detection and characterization of features and on automatic measurements in the images. A recently introduced AI system for chest CT imaging is presented that reports specific findings such as nodules, low-attenuation parenchyma, and coronary calcifications, including automatic measurements of, for example, aortic diameters. At the prediction and prescription levels, AI focuses on risk prediction and stratification, as opposed to merely detecting, measuring, and quantifying images. An AI-based approach for individualizing radiation dose in lung stereotactic body radiotherapy is discussed. The digital twin is presented as a concept of individualized computational modeling of human physiology, with AI-based CT-fractional flow reserve modeling as a first example. Finally, at the cohort and population analysis levels, the focus of AI shifts from clinical decision-making to operational decisions.","",""
11,"K. Mudgal, Neelanjan Das","The ethical adoption of artificial intelligence in radiology",2019,"","","","",120,"2022-07-13 09:19:12","","10.1259/bjro.20190020","","",,,,,11,3.67,6,2,3,"Artificial intelligence (AI) is rapidly transforming healthcare—with radiology at the pioneering forefront. To be trustfully adopted, AI needs to be lawful, ethical and robust. This article covers the different aspects of a safe and sustainable deployment of AI in radiology during: training, integration and regulation. For training, data must be appropriately valued, and deals with AI companies must be centralized. Companies must clearly define anonymization and consent, and patients must be well-informed about their data usage. Data fed into algorithms must be made AI-ready by refining, purification, digitization and centralization. Finally, data must represent various demographics. AI needs to be safely integrated with radiologists-in-the-loop: guiding forming concepts of AI solutions and supervising training and feedback. To be well-regulated, AI systems must be approved by a health authority and agreements must be made upon liability for errors, roles of supervised and unsupervised AI and fair workforce distribution (between AI and radiologists), with a renewal of policy at regular intervals. Any errors made must have a root-cause analysis, with outcomes fedback to companies to close the loop—thus enabling a dynamic best prediction system. In the distant future, AI may act autonomously with little human supervision. Ethical training and integration can ensure a ""transparent"" technology that will allow insight: helping us reflect on our current understanding of imaging interpretation and fill knowledge gaps, eventually moulding radiological practice. This article proposes recommendations for ethical practise that can guide a nationalized framework to build a sustainable and transparent system.","",""
9,"Cecilia S Lee, Aaron Y. Lee","How Artificial Intelligence Can Transform Randomized Controlled Trials",2020,"","","","",121,"2022-07-13 09:19:12","","10.1167/tvst.9.2.9","","",,,,,9,4.50,5,2,2,"With the advent of deep learning (DL), the application of artificial intelligence (AI) and big data in healthcare has started transforming the way we approach medicine including clinical trials.1,2 The randomized controlled trial (RCT) has been traditionally accepted as the most robust method of assessing the risks and benefits of any intervention.3 However, the undertaking of an RCT is not always feasible due to the rarity of the disease, or time and costs that would impinge on the healthcare system. AI is an academic discipline founded in 1956.4 Machine learning (ML) is a subfield of AI that can learn complex relationships or patterns from data and make accurate decisions.5 DL or deep artificial networks are a relatively new subfield of ML that takes advantage of powerful computational processing capacity provided by Graphic Processing Units and exponentially increasing datasets from medical records, images, multi-omics, and other “Big Data”.6 By feeding an enormous amount of data in training, a DL algorithm allows the model to alter its internal parameters between each neuronal layer to increase its performance. Applications of AI, DL in particular, have been successful in ophthalmic imaging research,7–10 and the application of AI in RCTs may become reality in the near future. Common pitfalls of unsuccessful RCTs include poor patient selection, inadequate randomization with residual confounders, insufficient sample size, and poor selection of end points.11 With well-curated large datasets that incorporate clinical and multimodal imaging, AI models can be trained to select the potential study participants without relying on costly manual review to predict the natural history of each study participants with advanced statistical methods, and to assess study end points in a data-driven method. Given these advantages, the application of AI has potentials for more efficient execution and greater statistical power than what would be expected from traditional RCTs. First, ML models can drastically improve the patient selection process, thus lowering the burden of individual screening and need for large sample sizes. Recruiting the patients who meet precise selection criteria is crucial to avoid potential confounders or misclassifications. ML can combine multimodal data, such as imaging, laboratory, and other complex -omics data, to screen and select patients who match complex inclusion criteria, which can improve the recruitment efficiency. This is one of the areas in which the American Academy of Ophthalmology’s Intelligent Research in Sight (IRIS) data will be utilized for RCT recruitment (personal communication, Flora Lum, MD). In addition to the efficient selection process, having a sufficient sample size to enable detection of statistically significant differences between groups is critical. Many RCTs require a large sample size because the effect of the treatment in question is small.12 AI has the potential in selecting “the ideal”patients for RCTs, who are “fast progressors” of the disease based on the AI’s predictive algorithm. Thus, the expected effect size will be large and required sample size will be small resulting in a much shorter duration of RCTs. Selecting the “fast progressors” alone will limit the generalizability of the trial results; however, it may expedite the development of novel therapies, in particular for rare diseases. Second, AI-generated end points have the potential to minimize measurement errors and analyze the data without human-imposed biases. Furthermore, algorithms may enable more sensitive quantification of key study end points than how they are traditionally measured. For example, central macular","",""
0,"E. Cambouropoulos, Maximos A. Kaliakatsos-Papakostas","Cognitive Musicology and Artificial Intelligence: Harmonic Analysis, Learning, and Generation",2021,"","","","",122,"2022-07-13 09:19:12","","10.1007/978-3-030-72116-9_10","","",,,,,0,0.00,0,2,1,"","",""
0,"Lucas Mendes Lima, Victor Calebe Cavalcante, Mariana Guimarães de Sousa, Cláudio Afonso Fleury, D. Oliveira, Eduardo Noronha de Andrade Freitas","Artificial Intelligence in Support of Welfare Monitoring of Dairy Cattle: A Systematic Literature Review",2021,"","","","",123,"2022-07-13 09:19:12","","10.1109/CSCI54926.2021.00324","","",,,,,0,0.00,0,6,1,"Context: Although agribusiness corresponded to more than 20% of Brazil’s Gross Domestic Product (GDP), most livestock is under manual control and manual monitoring. Additionally, alternative technologies are either uncomfortable and stressful, or expensive. Now, despite the great scientific advances in the area, there is still a pressing need for an automated robust, inexpensive and (sub)optimal technology to monitor animal behavior in a cost-effective, contact-less and stress-free fashion. Overall, this niche can leverage the benefits of Deep Learning schemes.Objective: This review aims to provide a systematic overview of most current projects in the area of comfort monitoring dairy cattle, as well as their corresponding image recognition-based techniques and technologies.Methods: First, a systematic review planning was carried out, and objectives, research questions, search strings, among others, were defined. Subsequently,a broad survey was conducted to extract, analyze and compile the data, to generate a easy-to-read visual source of information (tables and graphics).Results: Information was extracted from the reviewed papers. Among this data collected from the papers are techniques utilized, target behaviors, cow bodyparts identified in visual computational, besides their paper source font, the publication date, and localization. For example, the papers present are mostly recent. China has had a larger number of relevant papers in the area. The back was the body region most analyzed by the papers and the behaviors most analyzed were body condition score, lameness, cow’s body position and feeding/drinking behavior. Among the methods used is RCNN Inception V3 with the best accuracy for cow’s back region.Conclusion: The aim of this work is to present some of the papers that are being carried out in the area of dairy cow behavior monitoring, using techniques of Artifical Intelligence. It is expected that the information collected and presented in the present systematic review paper contribute to the future researches and projects of the area and the application of new techniques.","",""
0,"Wei Yan Ng, C. Cheung, D. Milea, D. Ting","Artificial intelligence and machine learning for Alzheimer’s disease: let’s not forget about the retina",2021,"","","","",124,"2022-07-13 09:19:12","","10.1136/bjophthalmol-2020-318407","","",,,,,0,0.00,0,4,1,"As the world population ages, it is estimated that the population worldwide above the age of 65 years old will increase from 420 million in 2000 to almost 1 billion by 2030. Dementia, with Alzheimer’s disease (AD) as the leading cause, is expected to rise in tandem. AD accounts for 60%–80% of all dementia cases, with an estimated 5–7 million new cases diagnosed each year. Despite intensive research, the diagnosis of AD is currently made through a combination of clinical assessment, neuroimaging and detection of biomarkers from positron emission tomography or cerebrospinal fluid examination, with patients facing issues including high costs, invasiveness of the procedures. Hence, alternative identification of AD without the use of costly or invasive tests remains a challenge that is difficult to surmount. To date, the healthcare has experienced a significant shift towards early accurate detection as well as early prevention. This importance is highlighted by the screening and surveillance of prevalent diseases such as diabetic retinopathy, breast cancer and dementia. While some of these programmes have been very successful in significantly reducing morbidity and mortality, significant amount of manpower, time and training is required for their successful execution. 10 This has lent greater weight to the adoption of healthcare technology in order to optimise the accuracy and efficiency of such programmes. Artificial intelligence (AI), through the combination of digitised big data and computational power, has emerged at the forefront of healthcare. It appears to be wellsuited to address the needs of the healthcare system: fast and accurate predictive, diagnostic and possibly therapeutic algorithms. Machine learning is able to process large amounts of digitised datasets beyond the limits of human capability, and analyse and convert these data into useful clinical insights for the physician. It is a natural fit for conditions or medical specialties that have a large reserve of labelled digitised datasets. At present, convoluted neural networks (CNN), designed to receive two or threedimensional shaped inputs, is one of the most commonly applied deep learning models in medical imaging analysis. Through the utilisation of CNN deep learning, several landmark studies extracting data from retinal images have shown a high degree of accuracy compared with human graders. 13 In order to achieve this, AI generally still requires large, welllabelled and highquality datasets. This places significant barriers to the development of successful image analysis models. The retina is one of the few select organs where image collection is easily accessible and abundant through the use of ocular imaging technologies. As the microvascular properties and neuronal structures of the retina, such as ganglion cellinner plexiform layer (GCIPL) and retinal nerve fibre layer (RNFL), 15 resemble the intracranial neuronal structure and vasculature, it provides a direct visualisation of potential intracranial changes. This combination of unique attributes can potentially provide a low cost, noninvasive analysis of the brain without the patient having to undergo costly neuroimaging to reach a diagnosis. This has attracted increasing research interest, especially in the field of AD where accurate early detection and diagnostic models still remain elusive. The pathogenesis associating retinal changes with AD is currently still unclear. While some evidence suggest the presence of amyloid beta plaques and tau neurofibrillary tangles in the retina of patients with AD, 20 the significance remains debatable. On the other hand, recent studies examining structural changes of the retina in patients with AD have indicated extensive changes including loss of macula volume, 23 thinning of the GCIPL 25 and RNFL thickness, and subfoveal choroidal thickness. In the peripheral retina, notable changes include increased drusen formation and reduced retinal vascularity. Optical coherence tomography angiogram (OCTA), which allows for noninvasive assessment of retinal vascularity, has also shown decreased vessel density, perfusion density and increased foveal avascular zone in patients with AD. 29 It is thus apparent that retinal changes, especially retinal thinning and reduction in vascularity, could potentially indicate the development of AD. In the article by Wisely et al the authors have trained an AI model with multiple imaging modalities using CNN deeplearning with the best performing models achieving an area under curve (AUC) between 0.830 and 0.841. A total of 284 eyes from 159 subjects, of which 36 were clinically diagnosed with AD by experienced neurologists, were analysed in this study. Patient data, optical coherence tomography (OCT) and OCTA quantitative data, ultrawide field retinal photography as well as retinal autofluorescence images were used in the training, validation and testing of the models. Used in isolation, the GCIPL thickness as an indicator of AD had the highest predictive value with an AUC of 0.809. When used in combination, the model that was trained with a combination of GCIPL, OCTA quantitative data and patient data had the highest AUC of 0.841. This paper represents the first attempt as well as a proof of concept at developing a CNN to detect AD using multimodal retinal images. The relatively high predictive ability of the different combination models not only serves as an important foundation for future deep learning models in predicting AD, but also proves the validity of this approach. By testing a varied combination of different imaging modalities available in an ophthalmic clinic, Wisely et al are able to determine the datasets that will provide the greatest yield in accurate prediction of AD. This could contribute to the future development of a robust screening or predictive platform that uses parameters that are costeffective and simple in acquisition. Future research and AI models are likely to expand on the use of retinal imaging and combine with clinical neurological Cataract and Comprehensive, Singapore National Eye Centre, Singapore Ophthalmology and Visual Sciences, The Chinese University of Hong Kong, Hong Kong, Hong Kong Neuroophthalmology Department, Singapore National Eye Centre, Singapore Vitreoretinal Department, Singapore National Eye Centre, Singapore","",""
23,"K. Mouridsen, P. Thurner, G. Zaharchuk","Artificial Intelligence Applications in Stroke.",2020,"","","","",125,"2022-07-13 09:19:12","","10.1161/STROKEAHA.119.027479","","",,,,,23,11.50,8,3,2,"Management of stroke highly depends on information from imaging studies. Noncontrast computed tomography (CT) and magnetic resonance imaging (MRI) can both be used to distinguish between ischemic and hemorrhagic stroke, which is difficult based on clinical features. Hypodensity on CT and DWI hyperintensity on MRI identifies irreversibly damaged tissue, although the sensitivity of MRI is higher in the acute setting. Angiographic and perfusion imaging sequences can identify a large vessel occlusion and, along with perfusion imaging, can select patients for endovascular therapy. The FLAIR-DWI mismatch yields information about patients with unknown time of onset (including wake-up strokes). Stroke imaging also gives insight into prognosis, with current methods aiming to give a picture of the short-term consequences of successful reperfusion or continued large vessel occlusion. One important caveat about stroke imaging is that it must be done quickly, as faster treatment leads to better outcomes.1 However, most steps in the stroke imaging triage pathway require the presence of human radiologists and neurologists, and this is often the time-limiting step. The expertise required for these tasks may not be available at all sites or at all times. Therefore, there is interest in automated methods for stroke imaging evaluation. Artificial intelligence (AI) is a broad term reflecting the use of computers to perform tasks that humans may find difficult, often in ways that are hard to pinpoint. For example, although humans find high-level computation difficult, calculator technology is not considered AI because we know how to break this down into discrete steps and feel we understand it. However, facial recognition is a task that humans perform well, but an algorithm to identify faces is usually considered AI since we cannot articulate precisely how this is done. Machine learning (ML) is a subset of AI in which algorithms learn from the data itself without explicit programming. ML methods reflect a broad range of statistical techniques ranging from linear regression to more complex methods such as support vector machines and decision trees. ML methods can be further broken into supervised and unsupervised learning, which differ from one another in that the former requires access to gold standard labels although the latter attempts to find the answers implicitly in the data itself. While ML methods have grown more popular over recent years, the advent of a specific supervised ML method based on architectures resembling human neural networks over the past decade has led to a quantum leap in performance.2 This method, called deep learning (DL) because of many multiple internal layers, can be considered a transformative technology. Compared with previous methods that required humans to identify image features, a deep neural network trained on a dataset with known outputs can learn the best features for organizing the data. In this review, we will discuss ML methods applied to stroke imaging with an emphasis on DL applications. We refer to Figure for a graphical overview of the applications discussed in this review.","",""
587,"Matej Moravcík, Martin Schmid, Neil Burch, V. Lisý, Dustin Morrill, Nolan Bard, Trevor Davis, K. Waugh, Michael Bradley Johanson, Michael H. Bowling","DeepStack: Expert-level artificial intelligence in heads-up no-limit poker",2017,"","","","",126,"2022-07-13 09:19:12","","10.1126/science.aam6960","","",,,,,587,117.40,59,10,5,"Computer code based on continual problem re-solving beats human professional poker players at a two-player variant of poker. Artificial intelligence masters poker Computers can beat humans at games as complex as chess or go. In these and similar games, both players have access to the same information, as displayed on the board. Although computers have the ultimate poker face, it has been tricky to teach them to be good at poker, where players cannot see their opponents' cards. Moravčík et al. built a code dubbed DeepStack that managed to beat professional poker players at a two-player poker variant called heads-up no-limit Texas hold'em. Instead of devising its strategy beforehand, DeepStack recalculated it at each step, taking into account the current state of the game. The principles behind DeepStack may enable advances in solving real-world problems that involve information asymmetry. Science, this issue p. 508 Artificial intelligence has seen several breakthroughs in recent years, with games often serving as milestones. A common feature of these games is that players have perfect information. Poker, the quintessential game of imperfect information, is a long-standing challenge problem in artificial intelligence. We introduce DeepStack, an algorithm for imperfect-information settings. It combines recursive reasoning to handle information asymmetry, decomposition to focus computation on the relevant decision, and a form of intuition that is automatically learned from self-play using deep learning. In a study involving 44,000 hands of poker, DeepStack defeated, with statistical significance, professional poker players in heads-up no-limit Texas hold’em. The approach is theoretically sound and is shown to produce strategies that are more difficult to exploit than prior approaches.","",""
213,"Georgios N. Yannakakis, J. Togelius","Artificial Intelligence and Games",2018,"","","","",127,"2022-07-13 09:19:12","","10.1007/978-3-319-63519-4","","",,,,,213,53.25,107,2,4,"","",""
82,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello","Artificial Intelligence, Algorithmic Pricing and Collusion",2018,"","","","",128,"2022-07-13 09:19:12","","10.2139/ssrn.3304991","","",,,,,82,20.50,21,4,4,"Pricing algorithms are increasingly replacing human decision making in real marketplaces. To inform the competition policy debate on possible consequences, we run experiments with pricing algorithms powered by Artificial Intelligence in controlled environments (computer simulations).<br><br>In particular, we study the interaction among a number of Q-learning algorithms in the context of a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. We show that the algorithms consistently learn to charge supra-competitive prices, without communicating with each other. The high prices are sustained by classical collusive strategies with a finite punishment phase followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.","",""
90,"M. Alsharqi, W. Woodward, J. Mumith, D. C. Markham, R. Upton, P. Leeson","Artificial intelligence and echocardiography",2018,"","","","",129,"2022-07-13 09:19:12","","10.1530/ERP-18-0056","","",,,,,90,22.50,15,6,4,"Echocardiography plays a crucial role in the diagnosis and management of cardiovascular disease. However, interpretation remains largely reliant on the subjective expertise of the operator. As a result inter-operator variability and experience can lead to incorrect diagnoses. Artificial intelligence (AI) technologies provide new possibilities for echocardiography to generate accurate, consistent and automated interpretation of echocardiograms, thus potentially reducing the risk of human error. In this review, we discuss a subfield of AI relevant to image interpretation, called machine learning, and its potential to enhance the diagnostic performance of echocardiography. We discuss recent applications of these methods and future directions for AI-assisted interpretation of echocardiograms. The research suggests it is feasible to apply machine learning models to provide rapid, highly accurate and consistent assessment of echocardiograms, comparable to clinicians. These algorithms are capable of accurately quantifying a wide range of features, such as the severity of valvular heart disease or the ischaemic burden in patients with coronary artery disease. However, the applications and their use are still in their infancy within the field of echocardiography. Research to refine methods and validate their use for automation, quantification and diagnosis are in progress. Widespread adoption of robust AI tools in clinical echocardiography practice should follow and have the potential to deliver significant benefits for patient outcome.","",""
76,"A. Zhavoronkov","Artificial Intelligence for Drug Discovery, Biomarker Development, and Generation of Novel Chemistry.",2018,"","","","",130,"2022-07-13 09:19:12","","10.1021/acs.molpharmaceut.8b00930","","",,,,,76,19.00,76,1,4,"and Generation of Novel Chemistry T productivity of the pharmaceutical industry is on the decline. Failure rates in clinical trials exceed 90% after therapies are tested in model organisms, and the cost to develop a new drug exceeds $2.6 billion. Recent advances in artificial intelligence (AI) may help to reverse this trend and accelerate and improve pharmaceutical R&D. While the term AI and the concept of deep learning are not new, recent advances in high-performance computing, the availability of large annotated data sets required for training, and new frameworks for implementing deep neural networks (DNNs) resulted in an unprecedented acceleration of the field. Since 2014, DNNs have surpassed human accuracy in image, voice and text recognition, autonomous driving, and many other tasks. Early presentations to the pharmaceutical industry on the advances in deep learning in 2014 and 2015 resulted in skepticism and were discarded. In 2017, many pharmaceutical companies started partnering with AI startups and academics or started internal R&D programs. From training DNNs on transcriptional response data for predicting the pharmacological properties of small molecules and biomarker development, to the generation of novel chemistry, deep learning techniques rapidly propagated into many areas of biomedical research. The body of knowledge and the range of applications of deep learning and other machine learning techniques has expanded quickly and permeated into many areas of drug discovery. There have been hundreds of publications deposited in peer-reviewed journals and on ArXiv. In June 2017, Molecular Pharmaceutics announced a special issue titled “Deep Learning for Drug Discovery and Biomarker Development” focused on the applications of AI in chemistry and biomedicine (Figure 1). After a call to the most prominent scientists publishing on deep learning in the areas of computational chemistry and biology, 10 research papers were accepted. One of the main opportunities for AI in drug discovery is in drug repurposing using abundant data sets available from highthroughput experiments with gene expression profiles. Specifically, transcriptional response profiles generated by the Broad Institute, such as the Connectivity Map. The connectivity map uses gene expression signatures to connect small molecules, genes, and disease available through the LINCS Project. Donner et al. used the L1000 data set to develop a new method for measuring the compound functional similarity based on gene expression data for drug repurposing. The method identified drugs with shared therapeutic and biological targets even when the compounds were structurally dissimilar, thereby revealing previously unreported functional relationships between compounds. Imaging data is among the most abundant data type available for deep learning researchers, often allowing for rapid validation of results using human visual sensory organs. Many of the tools developed for image recognition and trained and tested on simple pictures are now available for researchers working with more complex imaging data types, including computed tomography. Gao and Qian used the patch-based convolutional neural network (CNN) model combined with support vector machines to predict multidrug resistant patients with tuberculosis using a data set of 230 patients from the ImageCLEF2017 competition, achieving reasonably high classification rates. Xiang and colleagues presented a multitask deep autoencoder for the prediction of the human cytochrome 450 inhibition, laying the roadmap for reducing the side effects associated with inhibiting the CYP450. Lane et al. compared various machine learning models for predicting hit molecules forMycobacterium tuberculosis (Mtb) using a small curated data set of molecules targeting Mtb. Another article by the Ekins group compared various machine learning techniques for predicting the estrogen receptor (ER) binding. In this work, Russo and co-authors compared the AdaBoost, Bernoulli Naiv̈e-Bayes, Random Forest, support vector classification, and deep neural networks using a variety of metrics and a proprietary data set compiled from public sources to predict ER binding. Again, Random Forest outperformed the other algorithms, demonstrating the value of comparing the various algorithms, especially for simple machine learning tasks. One of the many chemistry related machine learning challenges is the selection of the representation of molecular structure to capture as many of the relevant chemical and biological features and come as close to reality as possible. There are many representations of molecular structures, including a variety of molecular fingerprints, string-based representations, molecular graphs, and others. A molecular graph is a popular representation of the molecular structure for machine learning applications and explored by many groups working on medicinal chemistry related tasks. Hop and colleagues explored the performance of geometric deep learning methods in the context of drug discovery, comparing machine learned features against the domain expert engineered features. The CNN graph outperformed the methods trained on expert engineered features on most of the data sets. The popular deep learning techniques involving CNN are often trained on 2D and 3D images. To help facilitate f the many applications of CNNs in chemistry, Kuzminykh presented the wave transform-based representation of the 3D molecular structure. The group demonstrated that the proposed representation leads to the better performance of CNNbased autoencoders than either the voxel-based representation or the previously used Gaussian blur of atoms, and it can be successfully applied to classification tasks, such as MACCS fingerprint prediction. Deep generative models, commonly referred to as AI imagination, enabled many new applications requiring creativity and","",""
58,"L. D. Jones, D. Golan, S. Hanna, M. Ramachandran","Artificial intelligence, machine learning and the evolution of healthcare",2018,"","","","",131,"2022-07-13 09:19:12","","10.1302/2046-3758.73.BJR-2017-0147.R1","","",,,,,58,14.50,15,4,4,"vol. 7, No. 3, MaRch 2018 223 First proposed by Professor John Mccarthy at Dartmouth college in the summer of 1956,1 artificial Intelligence (aI) – human intelligence exhibited by machines – has occupied the lexicon of successive generations of computer scientists, science fiction fans, and medical researchers. The aim of countless careers has been to build intelligent machines that can interpret the world as humans do, understand language, and learn from realworld examples. In the early part of this century, two events coincided that transformed the field of aI. The advent of widely available Graphic Processing Units (GPUs) meant that parallel processing was faster, cheaper, and more powerful. at the same time, the era of ‘Big Data’ – images, text, bioinformatics, medical records, and financial transactions, among others – was moving firmly into the mainstream, along with almost limitless data storage. These factors led to a dramatic resurgence in interest in aI in both academic circles and industries outside traditional computer science. once again, aI occupies the zeitgeist, and is poised to transform medicine at a basic science, clinical, healthcare management, and financial level. Terminology surrounding these technologies continues to evolve and can be a source of confusion for non-computer scientists. aI is broadly classified as: general aI, machines that replicate human thought, emotion, and reason (and remain, for now, in the realm of science fiction); and narrow aI, technologies that can perform specific tasks as well as, or better than, humans. Machine learning (Ml) is the study of computer algorithms that can learn complex relationships or patterns from empirical data and make accurate decisions.2 Rather than coding specific sets of instructions to accomplish a task, the machine is ‘trained’ using large amounts of data and algorithms that confer it the ability to learn how to perform the task. Unlike normal algorithms, it is the data that ‘tells’ the machine what the ‘good answer’ is, and learning occurs without explicit programming. Ml problems can be classified as supervised learning or unsupervised learning.3 In a supervised machine learning algorithm, such as face recognition, the machine is shown several examples of ‘face’ or ‘non-face’ and the algorithm learns to predict whether an unseen image is a face or not. In unsupervised learning, the images shown to the machine are not labelled as ‘face’ or ‘non-face’. artificial Neural Networks (aNN)4 are one group of algorithms used for machine learning. While aNNs have existed for over 60 years, they fell out of favour during the 1990s and 2000s. In the last half-decade, aNNs have had a resurgence under a new name: deep artificial networks (or ‘Deep learning’). aNNs are uniquely poised to take full advantage of the computational boost offered by GPUs, allowing them to crunch through data sets of enormous sizes. These range from computer vision tasks, such as image classification, object detection, face recognition, and optical character recognition (ocR), to natural language processing and even gameplaying problems (from mastering simple atari games to the recent alphaGo victory against human grandmasters).5 aNNs work by constructing layers upon layers of simple processing units (often referred to as ‘neurons’), interconnected via many differentially weighted connections. aNNs are ‘trained’ by using backpropagation algorithms, essentially telling the machine how to alter the internal parameters that are used to compute the representation in each layer from the representation in the previous Artificial intelligence, machine learning and the evolution of healthcare","",""
47,"L. Deng","Artificial Intelligence in the Rising Wave of Deep Learning: The Historical Path and Future Outlook [Perspectives]",2018,"","","","",132,"2022-07-13 09:19:12","","10.1109/MSP.2017.2762725","","",,,,,47,11.75,47,1,4,"Artificial intelligence (AI) is a branch of computer science and a technology aimed at developing the theories, methods, algorithms, and applications for simulating and extending human intelligence. Modern AI enables going from an old world-where people give computers rules to solve problems-to a new world-where people give computers problems directly and the machines learn how to solve them on their own using a set of algorithms. An algorithm is a self-contained sequence of instructions and actions to be performed by a computational machine. Starting from an initial state and initial input, the instructions describe computational steps, which, when executed, proceed through a finite number of well-defined successive states, eventually producing an output and terminating at a final ending state. AI algorithms are a rich set of algorithms used to perform AI tasks, notably those pertaining to perception and cognition that involve learning from data and experiences simulating human intelligence.","",""
42,"R. Mirnezami, A. Ahmed","Surgery 3.0, artificial intelligence and the next‐generation surgeon",2018,"","","","",133,"2022-07-13 09:19:12","","10.1002/bjs.10860","","",,,,,42,10.50,21,2,4,"In December 2017, the Google subsidiary DeepMind announced that its AlphaZero artificial intelligence (AI) program had taught itself to play chess, from scratch, in just 4 h. This algorithmic program has since mastered moves and strategies entirely through self-play, and defeated the world champion chess program by an inventive approach to the ancient art of chess1. The wave of enthusiasm following this announcement has been felt well beyond gaming and genuine questions are now emerging regarding the potential for AI in other fields, including healthcare. This is especially relevant in disciplines with a strong emphasis on pattern recognition, notably medical imaging and histopathology, where AI-based platforms are equalling, and in some cases surpassing, their human counterparts. For example, computer scientists from Cornell University recently reported superior accuracy of classification in the detection of lymph node metastases in breast cancer using a deep learning algorithm, compared with conventional pathology2. In contrast, AI technology has taken longer to permeate through to the world of surgery, partly owing to the complex nature of interaction with human tissue at the core of the specialty, but also because of a perceived lack of necessity, evidence and awareness of the potential capabilities of computational approaches in surgical practice. AI is, however, advancing rapidly, and at a pace that is difficult to ignore. The current vision is one of augmented surgical practice to complement rather than replace human skills, particularly in two broad areas: surgical decision-making and operative surgery. Surgery involves complex decisions including, for example, choices about the need for multimodal therapy, timing of surgery, and radical versus organ-preserving surgery. Moreover, the surgeon is increasingly expected to provide patients with personalized data on potential risks, and likelihood of major morbidity and mortality. These complex assessments are beyond the capabilities of most surgeons. The development of tools such as algorithmic clinical decision support (CDS) within surgery, with access to large and varied stores of ‘big data’, underpinned by the use of integration of multiparametric data, allows crosstalk between data stores and computer","",""
1,"P. Smolensky, R. Thomas McCoy, Roland Fernandez, M. Goldrick, Jia-Hao Gao","Neurocompositional computing in human and machine intelligence: A tutorial",2022,"","","","",134,"2022-07-13 09:19:12","","","","",,,,,1,1.00,0,5,1,"The past decade has produced a revolution in Artificial Intelligence (AI), after a half-century of AI repeatedly failing to meet expectations. What explains the dramatic change from 20th-century to 21st-century AI, and how can remaining limitations of current AI be overcome? Until now, the widely accepted narrative has attributed the recent progress in AI to technical engineering advances that have yielded massive increases in the quantity of computational resources and training data available to support statistical learning in deep artificial neural networks. Although these quantitative engineering innovations are important, here we show that the latest advances in AI are not solely due to quantitative increases in computing power but also qualitative changes in how that computing power is deployed. These qualitative changes have brought about a new type of computing that we call neurocompositional computing . In neurocompositional computing, neural networks exploit two scientific principles that contemporary theory in cognitive science maintains are simultaneously necessary to enable human-level cognition. The Compositionality Principle asserts that encodings of complex information are structures that are systematically composed from simpler structured encodings. The Continuity Principle states that the encoding and processing of information is formalized with real numbers that vary continuously. These principles have seemed irreconcilable until the recent mathematical discovery that compositionality can be realized not only through the traditional discrete methods of symbolic computing, well developed in 20th-century AI, but also through novel forms of continuous neural computing—neurocompositional computing. The unprecedented progress of 21st-century AI has resulted from the use of limited—first-generation—forms of neurocompositional computing. We show that the new techniques now being deployed in second-generation neurocompositional computing create AI systems that are not only more robust and accurate than current systems, but also more comprehensible—making it possible to diagnose errors in, and exert human control over, artificial neural networks through interpretation of their internal states and direct intervention upon those states. Note: This tutorial is intended for those new to this topic, and does not assume familiarity with cognitive science, AI, or deep learning. Appendices provide more advanced material. Each figure, and the associated box explaining it, provides an exposition, illustration, or further details of a main point of the paper; in order to make these figures relatively self-contained, it has sometimes been necessary to repeat some material from the text. For a brief introduction and additional development of some of this material see [212]. . abstract mental processes”","",""
27,"Shuai Liu, Shuai Wang, Xinyu Liu, A. Gandomi, M. Daneshmand, Khan Muhammad, V. H. C. de Albuquerque","Human Memory Update Strategy: A Multi-Layer Template Update Mechanism for Remote Visual Monitoring",2021,"","","","",135,"2022-07-13 09:19:12","","10.1109/TMM.2021.3065580","","",,,,,27,27.00,4,7,1,"In the era of rapid development of artificial intelligence, the integration of multimedia and human-artificial intelligence has become an important research hotspot. Especially in the multimedia environment, effective remote visual monitoring has become the exploration direction of many scholars. The use of traditional correlation filtering (CF) algorithm for real-time monitoring in the context of multimedia is a practical strategy. However, most existing filtering-based visual monitoring algorithms still have the problem of insufficient robustness and effectiveness. Therefore, by considering the strategy of updating human memory, this paper proposes a multi-layer template update mechanism to achieve effective monitoring in a multimedia environment. In this strategy, the weighted template of the high-confidence matching memory is used as the confidence memory, and the unweighted template of the low-confidence matching memory is used as the cognitive memory. Through the alternate use of confidence memory, matching memory, and cognitive memory, it is ensured that the target will not be lost during the monitoring process. Experimental results show that this strategy does not affect the speed (still real-time) and improves the robustness in the multimedia background.","",""
10,"M. Alomar, M. Hameed, N. Al‐Ansari, M. Alsaadi","Data-Driven Model for the Prediction of Total Dissolved Gas: Robust Artificial Intelligence Approach",2020,"","","","",136,"2022-07-13 09:19:12","","10.1155/2020/6618842","","",,,,,10,5.00,3,4,2,"Saturated total dissolved gas (TDG) is recently considered as a serious issue in the environmental engineering field since it stands behind the reasons for increasing the mortality rates of fish and aquatic organisms. The accurate and more reliable prediction of TDG has a very significant role in preserving the diversity of aquatic organisms and reducing the phenomenon of fish deaths. Herein, two machine learning approaches called support vector regression (SVR) and extreme learning machine (ELM) have been applied to predict the saturated TDG% at USGS 14150000 and USGS 14181500 stations which are located in the USA. For the USGS 14150000 station, the recorded samples from 13 October 2016 to 14 March 2019 (75%) were used for training set, and the rest from 15 March 2019 to 13 October 2019 (25%) were used for testing requirements. Similarly, for USGS 14181500 station, the hourly data samples which covered the period from 9 June 2017 till 11 March 2019 were used for calibrating the models and from 12 March 2019 until 9 October 2019 were used for testing the predictive models. Eight input combinations based on different parameters have been established as well as nine statistical performance measures have been used for evaluating the accuracy of adopted models, for instance, not limited, correlation of determination (        R      2        ), mean absolute relative error (MAE), and uncertainty at 95% (        U      95        ). The obtained results of the study for both stations revealed that the ELM managed efficiently to estimate the TDG in comparison to SVR technique. For USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.986 (0.986), MAE of 0.316 (0.441), and         U      95        of 3.592 (3.869). Lastly, for USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.991 (0.991), MAE of 0.338 (0.396), and         U      95        of 0.832 (0.837). In addition, ELM’s training process computational time is stated to be much shorter than that of SVM. The results also showed that the temperature parameter was the most significant variable that influenced TDG relative to the other parameters. Overall, the proposed model (ELM) proved to be an appropriate and efficient computer-assisted technology for saturated TDG modeling that will contribute to the basic knowledge of environmental considerations.","",""
105,"B. Koçak, E. S. Durmaz, Ece Ateş, Ö. Kılıçkesmez","Radiomics with artificial intelligence: a practical guide for beginners.",2019,"","","","",137,"2022-07-13 09:19:12","","10.5152/dir.2019.19321","","",,,,,105,35.00,26,4,3,"Radiomics is a relatively new word for the field of radiology, meaning the extraction of a high number of quantitative features from medical images. Artificial intelligence (AI) is broadly a set of advanced computational algorithms that basically learn the patterns in the data provided to make predictions on unseen data sets. Radiomics can be coupled with AI because of its better capability of handling a massive amount of data compared with the traditional statistical methods. Together, the primary purpose of these fields is to extract and analyze as much and meaningful hidden quantitative data as possible to be used in decision support. Nowadays, both radiomics and AI have been getting attention for their remarkable success in various radiological tasks, which has been met with anxiety by most of the radiologists due to the fear of replacement by intelligent machines. Considering ever-developing advances in computational power and availability of large data sets, the marriage of humans and machines in future clinical practice seems inevitable. Therefore, regardless of their feelings, the radiologists should be familiar with these concepts. Our goal in this paper was three-fold: first, to familiarize radiologists with the radiomics and AI; second, to encourage the radiologists to get involved in these ever-developing fields; and, third, to provide a set of recommendations for good practice in design and assessment of future works.","",""
73,"Valentina Bellemo, Gilbert Lim, T. Rim, G. Tan, C. Cheung, S. Sadda, M. He, A. Tufail, M. Lee, W. Hsu, D. Ting","Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application",2019,"","","","",138,"2022-07-13 09:19:12","","10.1007/s11892-019-1189-3","","",,,,,73,24.33,7,11,3,"","",""
6,"Marina Boia, C. Musat, B. Faltings","Acquiring Commonsense Knowledge for Sentiment Analysis through Human Computation",2014,"","","","",139,"2022-07-13 09:19:12","","10.1609/aaai.v28i1.8840","","",,,,,6,0.75,2,3,8,"    Many Artificial Intelligence tasks need large amounts of commonsense knowledge. Because obtaining this knowledge through machine learning would require a huge amount of data, a better alternative is to elicit it from people through human computation. We consider the sentiment classification task, where knowledge about the contexts that impact word polarities is crucial, but hard to acquire from data. We describe a novel task design that allows us to crowdsource this knowledge through Amazon Mechanical Turk with high quality. We show that the commonsense knowledge acquired in this way dramatically improves the performance of established sentiment classification methods.   ","",""
11,"Matthew Lease, Omar Alonso","Crowdsourcing and Human Computation, Introduction",2014,"","","","",140,"2022-07-13 09:19:12","","10.1007/978-1-4614-6170-8_107","","",,,,,11,1.38,6,2,8,"","",""
41,"C. Macrae","Governing the safety of artificial intelligence in healthcare",2019,"","","","",141,"2022-07-13 09:19:12","","10.1136/bmjqs-2019-009484","","",,,,,41,13.67,41,1,3,"Artificial intelligence (AI) has enormous potential to improve the safety of healthcare, from increasing diagnostic accuracy,1 to optimising treatment planning,2 to forecasting outcomes of care.3 However, integrating AI technologies into the delivery of healthcare is likely to introduce a range of new risks and amplify existing ones. For instance, failures in widely used software have the potential to quickly affect large numbers of patients4; hidden assumptions in underlying data and models can lead to AI systems delivering dangerous recommendations that are insensitive to local care processes,5 6 and opaque AI techniques such as deep learning can make explaining and learning from failure extremely difficult.7 8 To maximise the benefits of AI in healthcare and to build trust among patients and practitioners, it will therefore be essential to robustly govern the risks that AI poses to patient safety.  In a recent review in this journal, Challen and colleagues present an important and timely analysis of some of the key technological risks associated with the application of machine learning in clinical settings.9 Machine learning is a subfield of AI that focuses on the development of algorithms that are automatically derived and optimised through exposure to large quantities of exemplar ‘training’ data.10 The outputs of machine learning algorithms are essentially classifications of patterns that provide some sort of prediction—for instance, predicting whether an image shows a malignant melanoma or a benign mole.11 Some of the basic techniques of machine learning have existed for half a century or more, but progress in the field has accelerated rapidly due to advances in the development of ‘deep’ artificial neural networks12 combined with huge increases in computational power and the availability of enormous quantities of data. These techniques have underpinned recent public demonstrations of AI systems …","",""
24,"Lorenzo Cominelli, D. Mazzei, D. Rossi","SEAI: Social Emotional Artificial Intelligence Based on Damasio’s Theory of Mind",2018,"","","","",142,"2022-07-13 09:19:12","","10.3389/frobt.2018.00006","","",,,,,24,6.00,8,3,4,"A socially intelligent robot must be capable to extract meaningful information in real time from the social environment and react accordingly with coherent human-like behavior. Moreover, it should be able to internalize this information, to reason on it at a higher level, build its own opinions independently, and then automatically bias the decision-making according to its unique experience. In the last decades, neuroscience research highlighted the link between the evolution of such complex behavior and the evolution of a certain level of consciousness, which cannot leave out of a body that feels emotions as discriminants and prompters. In order to develop cognitive systems for social robotics with greater human-likeliness, we used an “understanding by building” approach to model and implement a well-known theory of mind in the form of an artificial intelligence, and we tested it on a sophisticated robotic platform. The name of the presented system is SEAI (Social Emotional Artificial Intelligence), a cognitive system specifically conceived for social and emotional robots. It is designed as a bio-inspired, highly modular, hybrid system with emotion modeling and high-level reasoning capabilities. It follows the deliberative/reactive paradigm where a knowledge-based expert system is aimed at dealing with the high-level symbolic reasoning, while a more conventional reactive paradigm is deputed to the low-level processing and control. The SEAI system is also enriched by a model that simulates the Damasio’s theory of consciousness and the theory of Somatic Markers. After a review of similar bio-inspired cognitive systems, we present the scientific foundations and their computational formalization at the basis of the SEAI framework. Then, a deeper technical description of the architecture is disclosed underlining the numerous parallelisms with the human cognitive system. Finally, the influence of artificial emotions and feelings, and their link with the robot’s beliefs and decisions have been tested in a physical humanoid involved in Human–Robot Interaction (HRI).","",""
37,"C. Kulikowski","Beginnings of Artificial Intelligence in Medicine (AIM): Computational Artifice Assisting Scientific Inquiry and Clinical Art – with Reflections on Present AIM Challenges",2019,"","","","",143,"2022-07-13 09:19:12","","10.1055/s-0039-1677895","","",,,,,37,12.33,37,1,3,"Summary Background : The rise of biomedical expert heuristic knowledge-based approaches for computational modeling and problem solving, for scientific inquiry and medical decision-making, and for consultation in the 1970’s led to a major change in the paradigm that affected all of artificial intelligence (AI) research. Since then, AI has evolved, surviving several “winters”, as it has oscillated between relying on expensive and hard-to-validate knowledge-based approaches, and the alternative of using machine learning methods for inferring classification rules from labelled datasets. In the past couple of decades, we are seeing a gradual but progressive intertwining of the two. Objectives : To give an overview of early directions in AI in medicine and threads of some subsequent developments motivated by the very different goals of scientific inquiry for biomedical research, and for computational modeling of clinical reasoning and more general healthcare problem solving from the perspective of today’s “AI-Deep Learning Boom”. To show how, from the beginning, AI was central to Biomedical and Health Informatics (BMHI), as a field investigating how to understand intelligent thinking in dealing professionally with the practice for healthcare, developing mathematical models, technology, and software tools to aid human experts in biomedicine, despite many previous bouts of “exuberant optimism” about the methodologies deployed. Methods : An overview and commentary on some of the early research and publications in AI in biomedicine, emphasizing the different approaches to the modeling of problems involved in clinical practice in contrast to those of biomedical science. A concluding reflection of a few current challenges and pitfalls of AI in some biomedical applications. Conclusion : While biomedical knowledge-based systems played a critical role in influencing AI in its early days, 50 years later they have taken a back seat behind “Deep Learning” which promises to discover knowledge structures for inference and prediction, both in science and for clinical decision-support. Early work on AI for medical consultation turned out to be more useful for explanation and teaching than for clinical practice, as had been originally intended. Today, despite the many reported successes of deep learning, fundamental scientific challenges arise in drawing on models of brain science, cognition, and language, if AI is to augment and complement rather than replace human judgment and expertise in biomedicine while also incorporating these advances for translational medicine. Understanding clinical phenotypes and how they relate to precision and personalization of care requires not only scientific inquiry, but also humanistic models of treatment that respond to patient and practitioner narrative exchanges, since it is the stories and insights of human experts which encourage what Norbert Weiner termed the ethical “human use of human beings”, so central to adherence to the Hippocratic Oath","",""
35,"J. Shapey, Guotai Wang, R. Dorent, A. Dimitriadis, Wenqi Li, I. Paddick, N. Kitchen, S. Bisdas, S. Saeed, S. Ourselin, R. Bradford, Tom Kamiel Magda Vercauteren","An artificial intelligence framework for automatic segmentation and volumetry of vestibular schwannomas from contrast-enhanced T1-weighted and high-resolution T2-weighted MRI.",2019,"","","","",144,"2022-07-13 09:19:12","","10.3171/2019.9.JNS191949","","",,,,,35,11.67,4,12,3,"OBJECTIVE Automatic segmentation of vestibular schwannomas (VSs) from MRI could significantly improve clinical workflow and assist in patient management. Accurate tumor segmentation and volumetric measurements provide the best indicators to detect subtle VS growth, but current techniques are labor intensive and dedicated software is not readily available within the clinical setting. The authors aim to develop a novel artificial intelligence (AI) framework to be embedded in the clinical routine for automatic delineation and volumetry of VS.   METHODS Imaging data (contrast-enhanced T1-weighted [ceT1] and high-resolution T2-weighted [hrT2] MR images) from all patients meeting the study's inclusion/exclusion criteria who had a single sporadic VS treated with Gamma Knife stereotactic radiosurgery were used to create a model. The authors developed a novel AI framework based on a 2.5D convolutional neural network (CNN) to exploit the different in-plane and through-plane resolutions encountered in standard clinical imaging protocols. They used a computational attention module to enable the CNN to focus on the small VS target and propose a supervision on the attention map for more accurate segmentation. The manually segmented target tumor volume (also tested for interobserver variability) was used as the ground truth for training and evaluation of the CNN. We quantitatively measured the Dice score, average symmetric surface distance (ASSD), and relative volume error (RVE) of the automatic segmentation results in comparison to manual segmentations to assess the model's accuracy.   RESULTS Imaging data from all eligible patients (n = 243) were randomly split into 3 nonoverlapping groups for training (n = 177), hyperparameter tuning (n = 20), and testing (n = 46). Dice, ASSD, and RVE scores were measured on the testing set for the respective input data types as follows: ceT1 93.43%, 0.203 mm, 6.96%; hrT2 88.25%, 0.416 mm, 9.77%; combined ceT1/hrT2 93.68%, 0.199 mm, 7.03%. Given a margin of 5% for the Dice score, the automated method was shown to achieve statistically equivalent performance in comparison to an annotator using ceT1 images alone (p = 4e-13) and combined ceT1/hrT2 images (p = 7e-18) as inputs.   CONCLUSIONS The authors developed a robust AI framework for automatically delineating and calculating VS tumor volume and have achieved excellent results, equivalent to those achieved by an independent human annotator. This promising AI technology has the potential to improve the management of patients with VS and potentially other brain tumors.","",""
32,"J. Bali, R. Garg, R. Bali","Artificial intelligence (AI) in healthcare and biomedical research: Why a strong computational/AI bioethics framework is required?",2019,"","","","",145,"2022-07-13 09:19:12","","10.4103/ijo.IJO_1292_18","","",,,,,32,10.67,11,3,3,"Artificial intelligence (AI) refers to a computer mimicking “intellectual processes characteristic of humans, such as the ability to reason, discover meaning, generalize, or learn from past experience” to achieve goals without being explicitly programmed for specific action. There is no consensus on what constitutes AI. Different criteria for intelligence proposed have not satisfied everyone leading to the famous aphorism, “AI is whatever hasn’t been done yet.” For example, optical character recognition and translation has now been relegated from “artificial intelligence” because of the routine nature of their use.[1,2]","",""
36,"William R. Frey, D. Patton, M. Gaskell, K. McGregor","Artificial Intelligence and Inclusion: Formerly Gang-Involved Youth as Domain Experts for Analyzing Unstructured Twitter Data",2018,"","","","",146,"2022-07-13 09:19:12","","10.1177/0894439318788314","","",,,,,36,9.00,9,4,4,"Mining social media data for studying the human condition has created new and unique challenges. When analyzing social media data from marginalized communities, algorithms lack the ability to accurately interpret off-line context, which may lead to dangerous assumptions about and implications for marginalized communities. To combat this challenge, we hired formerly gang-involved young people as domain experts for contextualizing social media data in order to create inclusive, community-informed algorithms. Utilizing data from the Gang Intervention and Computer Science Project—a comprehensive analysis of Twitter data from gang-involved youth in Chicago—we describe the process of involving formerly gang-involved young people in developing a new part-of-speech tagger and content classifier for a prototype natural language processing system that detects aggression and loss in Twitter data. We argue that involving young people as domain experts leads to more robust understandings of context, including localized language, culture, and events. These insights could change how data scientists approach the development of corpora and algorithms that affect people in marginalized communities and who to involve in that process. We offer a contextually driven interdisciplinary approach between social work and data science that integrates domain insights into the training of qualitative annotators and the production of algorithms for positive social impact.","",""
22,"Rushikesh S. Joshi, Alexander F. Haddad, Darryl Lau, C. Ames","Artificial Intelligence for Adult Spinal Deformity",2019,"","","","",147,"2022-07-13 09:19:12","","10.14245/ns.1938414.207","","",,,,,22,7.33,6,4,3,"Adult spinal deformity (ASD) is a complex disease that significantly affects the lives of many patients. Surgical correction has proven to be effective in achieving improvement of spinopelvic parameters as well as improving quality of life (QoL) for these patients. However, given the relatively high complication risk associated with ASD correction, it is of paramount importance to develop robust prognostic tools for predicting risk profile and outcomes. Historically, statistical models such as linear and logistic regression models were used to identify preoperative factors associated with postoperative outcomes. While these tools were useful for looking at simple associations, they represent generalizations across large populations, with little applicability to individual patients. More recently, predictive analytics utilizing artificial intelligence (AI) through machine learning for comprehensive processing of large amounts of data have become available for surgeons to implement. The use of these computational techniques has given surgeons the ability to leverage far more accurate and individualized predictive tools to better inform individual patients regarding predicted outcomes after ASD correction surgery. Applications range from predicting QoL measures to predicting the risk of major complications, hospital readmission, and reoperation rates. In addition, AI has been used to create a novel classification system for ASD patients, which will help surgeons identify distinct patient subpopulations with unique risk-benefit profiles. Overall, these tools will help surgeons tailor their clinical practice to address patients’ individual needs and create an opportunity for personalized medicine within spine surgery.","",""
22,"R. Dash, Mark E. McMurtrey, C. Rebman, U. Kar","Application of Artificial Intelligence in Automation of Supply Chain Management",2019,"","","","",148,"2022-07-13 09:19:12","","10.33423/JSIS.V14I3.2105","","",,,,,22,7.33,6,4,3,"A well-functioning supply chain is a key to success for every business entity. Having an accurate projection on inventory offers a substantial competitive advantage. There are many internal factors like product introductions, distribution network expansion; and external factors such as weather, extreme seasonality, and changes in customer perception or media coverage that affects the performance of the supply chain. In recent years Artificial Intelligence (AI) has been proved to become an extension of our brain, expanding our cognitive abilities to levels that we never thought would be possible. Though many believe AI will replace humans, it is not true, rather it will help us to unleash our true strategic and creative potential. AI consists of a set of computational technologies developed to sense, learn, reason, and act appropriately. With the technological advancement in mobile computing, the capacity to store huge data on the internet, cloud-based machine learning and information processing algorithms etc. AI has been integrated into many sectors of business and been proved to reduce costs, increase revenue, and enhance asset utilization. AI is helping businesses to get almost 100% accurate projection and forecast the customer demand, optimizing their R&D and increase manufacturing with lower cost and higher quality, helping them in the promotion (identifying target customers, demography, defining the price, and designing the right message, etc.) and providing their customers a better experience. These four areas of value creation are extremely important for gaining competitive advantage. Supply-chain leaders use AI-powered technologies to a) make efficient designs to eliminate waste b) real-time monitoring and error-free production and c) facilitate lower process cycle times. These processes are crucial in bringing Innovation faster to the market.","",""
42,"George Gadanidis","Artificial intelligence, computational thinking, and mathematics education",2017,"","","","",149,"2022-07-13 09:19:12","","10.1108/IJILT-09-2016-0048","","",,,,,42,8.40,42,1,5,"Purpose          The purpose of this paper is to examine the intersection of artificial intelligence (AI), computational thinking (CT), and mathematics education (ME) for young students (K-8). Specifically, it focuses on three key elements that are common to AI, CT and ME: agency, modeling of phenomena and abstracting concepts beyond specific instances.          Design/methodology/approach          The theoretical framework of this paper adopts a sociocultural perspective where knowledge is constructed in interactions with others (Vygotsky, 1978). Others also refers to the multiplicity of technologies that surround us, including both the digital artefacts of our new media world, and the human methods and specialized processes acting in the world. Technology is not simply a tool for human intention. It is an actor in the cognitive ecology of immersive humans-with-technology environments (Levy, 1993, 1998) that supports but also disrupts and reorganizes human thinking (Borba and Villarreal, 2005).          Findings          There is fruitful overlap between AI, CT and ME that is of value to consider in mathematics education.          Originality/value          Seeing ME through the lenses of other disciplines and recognizing that there is a significant overlap of key elements reinforces the importance of agency, modeling and abstraction in ME and provides new contexts and tools for incorporating them in classroom practice.","",""
5,"I. Timm, Steffen Staab, M. Siebers, C. Schon, Ute Schmid, Kai Sauerwald, Lukas Reuter, Marco Ragni, C. Niederée, H. Maus, G. Kern-Isberner, Christian Jilek, Paulina Friemann, Thomas Eiter, A. Dengel, Hannah Dames, Tanja Bock, J. Berndt, C. Beierle","Intentional Forgetting in Artificial Intelligence Systems: Perspectives and Challenges",2018,"","","","",150,"2022-07-13 09:19:12","","10.1007/978-3-030-00111-7_30","","",,,,,5,1.25,1,19,4,"","",""
83,"S. D’Alfonso, Olga Santesteban-Echarri, S. Rice, G. Wadley, R. Lederman, C. Miles, J. Gleeson, M. Alvarez-Jimenez","Artificial Intelligence-Assisted Online Social Therapy for Youth Mental Health",2017,"","","","",151,"2022-07-13 09:19:12","","10.3389/fpsyg.2017.00796","","",,,,,83,16.60,10,8,5,"Introduction: Benefits from mental health early interventions may not be sustained over time, and longer-term intervention programs may be required to maintain early clinical gains. However, due to the high intensity of face-to-face early intervention treatments, this may not be feasible. Adjunctive internet-based interventions specifically designed for youth may provide a cost-effective and engaging alternative to prevent loss of intervention benefits. However, until now online interventions have relied on human moderators to deliver therapeutic content. More sophisticated models responsive to user data are critical to inform tailored online therapy. Thus, integration of user experience with a sophisticated and cutting-edge technology to deliver content is necessary to redefine online interventions in youth mental health. This paper discusses the development of the moderated online social therapy (MOST) web application, which provides an interactive social media-based platform for recovery in mental health. We provide an overview of the system's main features and discus our current work regarding the incorporation of advanced computational and artificial intelligence methods to enhance user engagement and improve the discovery and delivery of therapy content. Methods: Our case study is the ongoing Horyzons site (5-year randomized controlled trial for youth recovering from early psychosis), which is powered by MOST. We outline the motivation underlying the project and the web application's foundational features and interface. We discuss system innovations, including the incorporation of pertinent usage patterns as well as identifying certain limitations of the system. This leads to our current motivations and focus on using computational and artificial intelligence methods to enhance user engagement, and to further improve the system with novel mechanisms for the delivery of therapy content to users. In particular, we cover our usage of natural language analysis and chatbot technologies as strategies to tailor interventions and scale up the system. Conclusions: To date, the innovative MOST system has demonstrated viability in a series of clinical research trials. Given the data-driven opportunities afforded by the software system, observed usage patterns, and the aim to deploy it on a greater scale, an important next step in its evolution is the incorporation of advanced and automated content delivery mechanisms.","",""
0,"","ACTIVITY REPORT Project-Team Models and Algorithms for Artiﬁcial Intelligence",2022,"","","","",152,"2022-07-13 09:19:12","","","","",,,,,0,0.00,0,0,1,"The expectation-maximization (EM) algorithm is a powerful computational technique for maximum likelihood estimation in incomplete data models. When the expectation step cannot be performed in closed form, a stochastic approximation of EM (SAEM) can be used. The convergence of the SAEM toward critical points of the observed likelihood has been proved and its numerical efﬁciency has been demonstrated. However, sampling from the posterior distribution may be intractable or have a high computational cost. Moreover, despite appealing features, the limit position of this algorithm can strongly depend on its starting one. To cope with this two issues, we propose in [11] new stochastic approximation version of the EM in which we do not sample from the exact distribution in the expectation phase of the procedure. We ﬁrst prove the convergence of this algorithm toward critical points of the observed likelihood. Then, we propose an instantiation of this general procedure to favor convergence toward global maxima. Experiments on synthetic and real data highlight the performance of this algorithm in comparison to the SAEM and the EM when feasible. of subject-speciﬁc weights characterizing partial membership across clusters. With this ﬂexibility come challenges in uniquely identifying, estimating, and interpreting the parameters. In [40], we propose a new class of Dimension-Grouped MMMs (Gro-M 3 s) for multivariate categorical data, which improve parsimony and interpretability. In Gro-M 3 s, observed variables are partitioned into groups such that the latent membership is constant for variables within a group but can differ across groups. Traditional latent class models are obtained when all variables are in one group, while traditional MMMs are obtained when each variable is in its own group. The new model corresponds to a novel decomposition of probability tensors. Theoretically, we derive transparent identiﬁability conditions for both the unknown grouping structure and model parameters in general settings. Methodologically, we propose a Bayesian approach for Dirichlet Gro-M3 s to inferring the variable grouping structure and estimating model parameters. Simulation results demonstrate good computational performance and empirically conﬁrm the identiﬁability results. We illustrate the new methodology through an application to a functional disability dataset. from this natural partition. In a Bayesian context, this is achieved by considering the Dirichlet cluster proportion prior parameter α as a regularisation term controlling the granularity of the clustering. This second step allows the exploration of the clustering at coarser scales and the ordering of the clusters an important output for the visual representations of the clustering results. The clustering results obtained with the proposed approach, on simulated as well as real settings, are compared with existing strategies and are shown to be particularly relevant. This work is implemented in the R package greed and Figure 2 illustrates the main idea of the method. In this applied work [19], we use the Fisher-EM algorithm for clustering for the unsupervised classiﬁcation of 702, 248 spectra of galaxies and quasars with resdshifts smaller than 0.25 that were retrieved from the Sloan Digital Sky Survey (SDSS) database, release 7. The spectra were ﬁrst corrected for the redshift, then wavelet-ﬁltered to reduce the noise, and ﬁnally binned to obtain about 1437 wavelengths per spectrum. Fisher-EM, an unsupervised clustering discriminative latent mixture model algorithm, was applied on these corrected spectra, considering the full set as well as several subsets of 100,000 and 300,000 spectra. The optimum number of classes given by a penalized likelihood criterion is 86 classes, the 37 most populated ones gathering 99% of the sample. These classes are established from a subset of 302144 spectra. Using several cross-validation techniques we ﬁnd that this classiﬁcation is in agreement with the results obtained on the other subsets with an average misclassiﬁcation error of about 15%. The large number of very small classes tends to increase this error rate. This is the ﬁrst time that an automatic, objective and robust unsupervised classiﬁcation is established on such a large amount of spectra of galaxies. The mean spectra of the classes can be used as templates for a large majority of galaxies in our Universe. Figure 7 illustrates the obtained results. Recurrent Neural Networks, Deep linguistic patterns the of a of of to the is this linguistic that becomes valuable for our descriptive approach through deep as it allows us to observe complex lexico-grammatical structures, that potentially associate several levels of text representation in the same structure. The convolutional model used until now must therefore be adapted to integrate this additional information in order to obtain an even ﬁner description of the textual salience of a corpus. the relevant features used by the CNN to perform the classiﬁcation task. We empirically demonstrate the efﬁciency of our approach on corpora from two different languages: English and French. On all datasets, wTDS automatically encodes complex linguistic objects based on co-occurrences and possibly on grammatical and syntax analysis. relationships between the concepts in the metadata by analyzing the contrast between the concepts similarities in the Joconde’s semantic model and other vocabularies and we tried to improve the model prediction scores based on the semantic relations. Our results show that cross-fertilization between symbolic AI and machine learning can indeed provide the tools to address the challenges of the museum curators work describing the artwork pieces and searching for the relevant images. that combines a geometric approach for decision rules with existing post hoc solutions for machine learning models to generate an intuitive feature ranking tailored to the end user. We show that established model-agnostic approaches produce poor results in this framework. Figure 13 illustrates this work. Algorithms involving Gaussian processes or determinantal point processes typically require computing the determinant of a kernel matrix. Frequently, the latter is computed from the Cholesky decomposition, an algorithm of cubic complexity in the size of the matrix. We show that, under mild assumptions, it is possible to estimate the determinant from only a sub-matrix, with probabilistic guarantee on the relative error. In [37], we present an augmentation of the Cholesky decomposition that stops under certain conditions before processing the whole matrix. Experiments demonstrate that this can save a considerable amount of time while having an overhead of less than 5% when not stopping early. More generally, we present a probabilistic stopping strategy for the approximation of a sum of known length where addends are revealed sequentially. We do not assume independence between addends, only that they are bounded from below and decrease in conditional expectation. of there is a signiﬁcant from combining and audio data in detecting active speakers. either of the modalities can potentially mislead audiovisual fusion by inducing unreliable or deceptive information. outlines active speaker detection as a multi-objective learning problem to leverage best of each modalities using a novel self-attention, uncertainty-based multimodal fusion scheme. Results obtained show that the proposed multi-objective learning architecture outperforms traditional approaches in improving both mAP and AUC scores. We further demonstrate that our fusion strategy surpasses, in active speaker detection, other modality fusion methods reported in various disciplines. We ﬁnally show that the proposed method signiﬁcantly improves the state-of-the-art on the AVA-ActiveSpeaker dataset. This paper explores the problem of summarizing professional soccer matches as automatically as possible using both the event-stream data collected from the ﬁeld and the content broadcasted on TV. We have designed an architecture, introducing ﬁrst (1) a Multiple Instance Learning method that takes into account the sequential dependency among events and then (2) a hierarchical multimodal attention layer that grasps the importance of each event in an action [31]. We evaluate our approach on matches from two professional European soccer leagues, showing its capability to identify the best actions for automatic summarization by comparing with real summaries made by human operators. Figure 18 illustrates the general schema of the approach. We a coherent framework for studying longitudinal manifold-valued data. We introduce a Bayesian mixed-effects model which allows estimating both a group-representative piecewise-geodesic creating clusters of similar sentences. The ideal practice is to obtain a cluster with only positive blocks and another with only negative ones. Comparing to the supervised approach (Bag of words + Logistic Regression Classiﬁer) with its f1-score as 0.8234 and f2-score as 0.8316, we found that both S-Bert [58] (with a f1-score of 0.6250 and f2-score of 0.6192) and BioBert [57] (f1-score as 0.7004 and f2 as 0.6955) can achieves relatively good results and latter even outperformed the former due to its domain speciﬁc knowledge. around 13 billion euros per year to European citizens [52]. In the ﬁeld of healthcare insurance, in France the compulsory scheme detected over 261.2 million euros of fraudulent services in 2018, mainly due to healthcare professionals and healthcare establishments [50]. In the United States, according to the FBI, medicare fraud costs insurance companies between 21 billion and 71 billion US dollars per year [55]. In a context where reducing management costs is a real issue for healthcare insurers, the ﬁght against fraud is a real expectation of the customers of professionals in the sector so that everyone receives a fair return for their contributions. This stud","",""
189,"Lawrence B. Solum","Legal Personhood for Artificial Intelligences",2008,"","","","",153,"2022-07-13 09:19:12","","10.4324/9781003074991-37","","",,,,,189,13.50,189,1,14,"Could an artificial intelligence become a legal person? As of today, this question is only theoretical. No existing computer program currently possesses the sort of capacities that would justify serious judicial inquiry into the question of legal personhood. The question is nonetheless of some interest. Cognitive science begins with the assumption that the nature of human intelligence is computational, and therefore, that the human mind can, in principle, be modelled as a program that runs on a computer. Artificial intelligence (AI) research attempts to develop such models. But even as cognitive science has displaced behavioralism as the dominant paradigm for investigating the human mind, fundamental questions about the very possibility of artificial intelligence continue to be debated. This Essay explores those questions through a series of thought experiments that transform the theoretical question whether artificial intelligence is possible into legal questions such as, ""Could an artificial intelligence serve as a trustee?"" What is the relevance of these legal thought experiments for the debate over the possibility of artificial intelligence? A preliminary answer to this question has two parts. First, putting the AI debate in a concrete legal context acts as a pragmatic Occam's razor. By reexamining positions taken in cognitive science or the philosophy of artificial intelligence as legal arguments, we are forced to see them anew in a relentlessly pragmatic context. Philosophical claims that no program running on a digital computer could really be intelligent are put into a context that requires us to take a hard look at just what practical importance the missing reality could have for the way we speak and conduct our affairs. In other words, the legal context provides a way to ask for the ""cash value"" of the arguments. The hypothesis developed in this Essay is that only some of the claims made in the debate over the possibility of AI do make a pragmatic difference, and it is pragmatic differences that ought to be decisive. Second, and more controversially, we can view the legal system as a repository of knowledge-a formal accumulation of practical judgments. The law embodies core insights about the way the world works and how we evaluate it. Moreover, in common-law systems judges strive to decide particular cases in a way that best fits the legal landscape-the prior cases, the statutory law, and the constitution. Hence, transforming the abstract debate over the possibility of AI into an imagined hard case forces us to check our intuitions and arguments against the assumptions that underlie social decisions made in many other contexts. By using a thought experiment that explicitly focuses on wide coherence, we increase the chance that the positions we eventually adopt will be in reflective equilibrium with our views about related matters. In addition, the law embodies practical knowledge in a form that is subject to public examination and discussion. Legal materials are published and subject to widespread public scrutiny and discussion. Some of the insights gleaned in the law may clarify our approach to the artificial intelligence debate.","",""
0,"Bushra Rasheed, M. Usama, Asmara Safdar","Robust Artificial Intelligence Approach to Stabilize and Control Propeller Driven Hybrid UGV",2022,"","","","",154,"2022-07-13 09:19:12","","10.1109/ICAI55435.2022.9773375","","",,,,,0,0.00,0,3,1,"Hybrid Unmanned Ground Vehicle (HUGV) can drive on any terrain including walls and fly as well, using the multi directional thrust force of propellers. In the era of industrial revolution, hybrid UGVs need to be autonomous with intelligent decision making capabilities. During wall climbing of hybrid UGVs, stability is essential and depends on real time feedback from multiple sensors. To increase stability and control, it is proposed that PID control loops should be replaced by AI based algorithms that reduce the decision time and mathematical complexity. For autonomous movement in any terrain using the proposed model, intelligent UGVs can map and localize simultaneously.They can make intelligent decisions about mode of movement i.e. driving on ground or wall, steering on ground or wall, flying and maneuvering by using real time sensor readings. Integration of the proposed AI models with HUGV can be applied to many areas which are hard for humans to access, for instance; inspection of large structures, bio & nuclear hazard environments, planetary exploration & magnetic fields detection.","",""
0,"","Big Data and Artificial Intelligence Analytics in Geosciences : Promises and Potential Last Call for 2019 Annual Meeting Proposals",2019,"","","","",155,"2022-07-13 09:19:12","","","","",,,,,0,0.00,0,0,3,"Big data and machine learning are IT methodologies that are bringing substantial changes in the analysis and interpretation of scientific data. By adding GPU processing resources to the typical equipment of a server host, it is possible to speed up queries performed on large databases and reduce training time for deep learning architectures. A recent pairing of the big data technologies, applied to old and new data, and artificial intelligence techniques has enabled a team of scientists to create an interactive virtual globe that shows a color mosaic of the seabed geology. This interactive model allows us to obtain robust reconstructions and predictions of climate changes and their impacts on the ocean environment. We suggest a possible evolution of such a model by means of the expansion of functionalities and performance improvements. We refer respectively to the implementation of isochronic layers of seabed lithologies and the addition of GPU resources to speed up the learning phase of the support vector machine (SVM) model. These additional features would allow us to establish broader correlations and extract additional information on large-scale geological phenomena. INTRODUCTION The Earth system generates continuous data, and our acquisition capacity has significantly increased over time. The growing availability of acquired geological data and the methods developed in the field of information technology make it possible to identify associations and understand patterns and trends within data (Big Data), solve difficult decision problems (artificial intelligence), and provide acceleration to data processing (GPU computing). Big Data is a term that indicates very large databases (often by order of zettabytes, i.e., billions of terabytes) that can contain huge amounts of heterogeneous, structured and unstructured data (text, numerical values, images, e-mail, GPS data, and data acquired from social networks), which can be extrapolated, analyzed, and correlated with each other. Artificial Intelligence (AI) is a branch of computer science that studies the way in which the combination of hardware and software systems can simulate typical behaviors of the human brain. One of the most important applications consists of a complex algorithm, called machine learning, which is able to learn and make decisions. GPU Parallel Computing (GPGPU) involves the processing of data by the processors present in the graphics card (GPU) and has allowed the computation, in relatively short times, of huge amounts of data with an efficiency of at least two orders of magnitude greater compared to the past. There are several cases in which these technologies have been applied both in the field of potential earthquakes (RouetLeduc et al., 2017), volcanic eruptions (Ham et al., 2012), and to solve the problems of spatial modeling in the field of the assessment of landslide susceptibility (Korup and Stolle, 2014). The following describes a mixed approach (AI and Big Data) in the field of geosciences—analyzing potentials and possible future developments. CASE STUDY: BIG DATA AND AI MAP WORLD’S OCEAN FLOOR An example of an application combining Big Data and machine learning technologies was implemented by a team of Australian scientists who created the first digital map of seabed lithologies (Dutkiewicz et al., 2015) through the analysis and cataloging of ~15,000 samples of sediments found in marine basins. Before such a map, the most recent map of oceanic lithologies was hand drawn ~40 years ago, at the beginning of ocean exploration. Since then, the map has undergone few changes, with at most six types of sediment dominant in the ocean basins. The digital map was created using an AI method consisting of the support vector machine (SVM) model. Through a crossvalidation approach, the classifier was trained by adding new data gradually so as to allow its learning. Learning the parameter values, which optimize the classifier’s performance on withheld data, is an important step in the workflow. In this way, the vast set of point data has been transformed into a continuous digital map with very high accuracy (up to 80%). The new lithological map of the seabed is very important for the interpretation of global phenomena related to the evolution of ocean basins. An example of this is diatoms, siliceous phytoplankton that live in the oceans and that through chlorophyll photosynthesis produce about one-quarter of the oxygen present in the atmosphere, contributing to reduce global terrestrial warming. At their death, these organisms precipitate through the water column, accumulating on the underlying sea floor. Satellite surveys over the years have identified places where diatomaceous activity is more productive; that is, the marine areas in which there are the maximum concentrations of chlorophyll, considering that they should also correspond to the areas of maximum accumulation of these organisms in the sea floor. Surprisingly, the digital map of the seabed has revealed that there is a decoupling between the productivity of diatoms and the corresponding accumulation areas in the sea floor. The possibility of diatom ooze formation is however favored by the low surface temperature (0.9–5.7 °C), by salinity (33.8–34 PSS), and by the high concentration of nutrients, and therefore can represent an important indicator of the oceanographic variables of the surface of the sea (Cunningham and Big Data and Artificial Intelligence Analytics in Geosciences: Promises and Potential Roberto Spina, Geologist and DCompSci, CNG (National Council of Geologists), Rome, Italy, robertospina@geologi.it GSA Today, https://www.doi.org/10.1130/GSATG372GW.1. Copyright 2019, The Geological Society of America. CC-BY-NC. Leventer, 1998). For this reason, the map will help scientists better understand how our oceans have responded and will respond to environmental changes. POTENTIAL AND FUTURE PROSPECTS Big Data and AI are having an impact on every commercial and scientific domain, and their application in the field of geosciences is making a great impact in the analysis and understanding of natural phenomena. The intensive use of CPUs required by these two technologies has stimulated the search for alternative solutions to improve performance by using a mixed CPU-GPU approach. In this way it is possible to obtain rapid results from huge databases and the acceleration of the learning process for neural networks. These techniques are the basis of deep learning, an alternative model of machine learning, which achieves a very high degree of accuracy in recognizing objects and is able to learn features automatically from data without the need to extract them manually. The joint application of Big Data– machine learning, described as a case study, allowed researchers to demonstrate the absence of correlation between diatom productivity and the corresponding diatom oozes: The accumulation of these organisms in the seabed seems rather to be linked to specific variations in sea-surface parameters. This is one of many cases where the integrated analysis of various parameters allows a different interpretation from what could be assumed by their disjoint analysis. A possible evolution is to represent, on a similar map, in addition to the current surface lithologies, those present within the lithostratigraphic succession, making geochronological correlations between chronostratigraphic units. Using surveys carried out in various parts of the world, different layers could be defined, each corresponding to a specific age expressed in millions of years, representing the ocean lithologies existing in that particular geological period. Similarly to the previous case, the transition from a punctual to a continuous display could be obtained, for each layer, by applying the existing SVM model or an even more efficient version using GPU computing. Figure 1 shows a possible switching between current ocean Figure 1. Example of a layered implementation of seabed lithology maps (modified from https://portal.gplates.org). lithologies (https://portal.gplates.org) placed below and those existing respectively 500,000 and one million years ago (above). The oldest layers were made only for demonstration purposes and reproduce an artificial lithology of the seabed. A system of this kind allows the carrying out of various operations that can be summarized as follows: • display/hide isochronous levels obtaining different instantaneous representations of the ocean basins during the geological eras; • using Big Data analytics to pair data sets (oceanographic, stratigraphic, paleontological, and micropaleontological) with one or more isochronous layers to analyze geological phenomena on a global scale (eustatic oscillations, glacial and interglacial periods...) and perform stratigraphic correlations between oceanic crustal sectors to identify evolutionary patterns. The optimization introduced by IT methods lets us perform analyses on large heterogeneous data to discover hidden models and unknown correlations that allow for more solid reconstructions and forecasts on natural phenomena that have had and will have a major impact on the ecosystems of our planet. REFERENCES CITED Cunningham, W.L., and Leventer, A., 1998, Diatom assemblages in surface sediments of the Ross Sea: Relationship to present oceanographic conditions: Antarctic Science, v. 10, p. 134–146, https://doi.org/10.1017/S0954102098000182. Dutkiewicz, A., Müller, R.D., O’Callaghan, S., and Jónasson, H., 2015, Census of seafloor sediments in the world’s ocean: Geology, v. 43, no. 9, p. 795–798, https://doi.org/10.1130/G36883.1. Ham, M.F., Iyengar, I., Hambebo, B.M., Garces, M., Deaton, J., Perttu, A., and Williams, B., 2012, A neurocomputing approach for monitoring Plinian volcanic eruptions using infrasound: Procedia Computer Science, v. 13, p. 7–17, https://doi.org/10.1016/j.procs.2012.09.109. Korup, O., and Stolle, A., 2014, Landslide prediction from","",""
0,"Mr. Nusrath Khan, M. P. Mishra","The Impact of Empathic Algorithms on Artificial Intelligence",2018,"","","","",156,"2022-07-13 09:19:12","","","","",,,,,0,0.00,0,2,4,"Intelligence links perception to action to help an organism live. Intelligence is working out in the service of life, just as metabolism is chemistry in the service of life. Intelligence does not infer perfect understanding; every intelligent being has limited perception, memory, and computation. Many opinions on the spectrum of intelligence-versus-cost are viable, from insects to humans. The implications of autonomous symmetries have been far-reaching and pervasive. In this work, we show the visualization of redundancy. We construct an analysis of flip-flop gates, which we call HEAL. Keywords-Spectrum,autonomous symmetries, visualization, flip-flop gates. I . I NT R O D U CT I O N Many hackers worldwide would agree that, had it not been for SMPs, the exploration of systems might never have occurred. While existing solutions to this obstacle are outdated, none have taken the embedded solution we propose here. To put this in perspective, consider the fact that much-touted security experts largely use extreme programming to fulfill this aim. The study of courseware would tremendously amplify trainable symmetries. We motivate a novel algorithm for the refinement of virtual machines, which we call HEAL. for ex-ample, many frameworks visualize classical information. Existing psychoacoustic and self-learning sys-tems use the transistor to refine A* search [13]. We view programming languages as following a cycle of four phases: development, exploration, visualization, and study. Combined with flexible epistemologies, it develops new game-theoretic epistemologies. We question the need for certifiable theory. The flaw of this type of approach, however, is that the transistor can be made permutable, ubiquitous, and reliable. Existing robust and permutable frameworks use the analysis of 802.11 mesh networks to visualize the visualization of A* search. We view e-voting technology as following a cycle of four phases: development, construction, visualization, and provision. By comparison, for example, many methodologies emulate linked lists. However, virtual methodologies might not be the panacea that information theorists expected. Here, we make four main contributions. We motivate new optimal configurations (HEAL), demonstrating that the little-known homogeneous algorithm for the refinement of model checking by Ed-ward Feigenbaum et al. [14] is NPcomplete. Furthermore, we use concurrent symmetries to demonstrate that the seminal cooperative algorithm for the development of virtual machines by Jackson[13] is re-cursively enumerable. We verify that systems can be made game-theoretic, unstable, and constant-time. Lastly, we investigate how erasure coding can be applied to the simulation of journaling file systems. The rest of this paper is organized as follows. First, we motivate the need for forward-error correction. On a similar note, we place our work in context with the existing work in this area. To realize this purpose, we show not only that Byzantine fault tolerance [18] can be made psychoacoustic, adaptive, and event-driven, but that the same is true for compilers. Finally, we conclude. II. MOTIVATION Though we are the first to explore DNS in this light, much related work has been devoted to the analysis of spreadsheets that would allow for further study into SCSI disks [19]. Similarly, Andy Tanenbaum et al. [20] developed a similar system, nevertheless we proved that our algorithm runs in Θ(n2) time [9]. HEAL is broadly related to work in the field of cryptography by Y. Shastri et al., but we view it from a new perspective: “smart” information. Our method is broadly related to work in the field of complexity theory by White et al., but we view it from a new perspective: the emulation of access points [23]. A major source of our inspiration is early work by I. L. Maruyama et al. [1] on active networks [16,21]. We believe there is room for both schools of thought within the field of algorithms. Wilson [12] suggested a scheme for architecting scatter/gather I/O, but did not fully realize the implications of adaptive theory at the time. This method is less cheap than ours. Furthermore, Charles Leiserson et al. constructed several electronic methods, and reported that International Journal of Innovations in Engineering and Technology (IJIET) http://dx.doi.org/10.21172/ijiet.102.23 Volume 10 Issue 2 May 2018 152 ISSN: 2319-1058 A R they have great lack of influence on game-theoretic theory [15]. Furthermore, though Sun and Sato also proposed this approach, we visualized it independently and simultaneously [5]. Thus, despite substantial work in this area, our method is evidently the framework of choice among analysts [28]. Our approach is related to research into highly-available epistemologies, unstable modalities, and public-private key pairs. It remains to be seen how valuable this research is to the cryptanalysis community. Similarly, instead of investigating thin clients, we surmount this issue simply by enabling RPCs. On the other hand, the complexity of their solution grows inversely as active networks grow. Continuing with this rationale, the choice of B-trees in [22] diff ers from ours in that we investigate only intuitive communication in our system. Wang and White [25] suggested a scheme for emulating the improvement of the UNIVAC computer, but did not fully realize the implications of game-theoretic modalities at the time. However, the complexity of their approach grows quadratically as collaborative technology grows. Clearly, despite substantial work in this area, our solution is perhaps the framework of choice among security experts. III. PROPOSED METHODOLOGY Motivated by the need for Byzantine fault tolerance, we now motivate an architecture for confirming that the Ethernet and multicast heuristics are continuously incompatible. We postulate that Scheme and compilers can collaborate to accomplish this mission [24]. Continuing with this rationale, we consider an algorithm consisting of n checksums. Similarly, we instrumented a 2-year-long trace demonstrating that our methodology is solidly grounded in reality. This seems to hold in most cases.","",""
1,"Massoud Sokouti, B. Sokouti","Applying the Science of Systematic Review and Meta-Analysis to Retrospective Artificial Intelligence Based Studies: The importance of performance evaluation",2019,"","","","",157,"2022-07-13 09:19:12","","","","",,,,,1,0.33,1,2,3,"The rationale behind the meta-analysis goes back to the 17th century studies of astronomy which then Karl Pearson performed a study based on meta-analysis using the data for typhoid inoculation in 1904. After, William Cochran applied this type of analysis to medical researches by taking the advantage of multiple previous studies. For more information and details on the history, the readers are referred to. To emerge the important role of systematic and metaanalysis studies even in the area of artificial intelligence systems, it is an anticipated that more reliable results can be driven from previous research studies alongside a simple review of such studies from which most of them may be ignored or not included as a matter of their nonsystematical type of reviews. The meta-analysis technique uses various types of statistics tools and methodologies to commonly derive a predictive diagnostic or non-diagnostic performance result of their compared corresponding approaches on the target defined disorders using information included in different datasets of previous studies. Although, a meta-analysis study can be regarded as a review of previous studies, however, it thoroughly targets not only the achieving results of those studies but also determine the in-common and non-commonpatterns of those researches as well as biases of the performance results whether they have been inserted intentionally or unintentionally. The importance of meta-analysis has been vastly discussed in medical sciences and therefore, been conducted rigorously through various studies, mostly on clinical trial ones. However, this technique is one of those less valued tools imported in to biomedical engineering studies and hence, their related algorithms mostly on the performance of artificial intelligence approaches. One of those studies to mention is the one performed on classification algorithms for pattern recognition by So Young Sohn in 1999 based on some in-house implemented statistics tools without considering the meta-analysis software. Moreover, in 2015,Horn et al have conducted a systematic review on functional brain imaging studies on assessing the familiarity of artificial neural networks and discussed their pros and cons in terms of their experimental conflicting results based on a meta-analysis on 68 publishedarticles. In another recent study, the role of real-time biomedical systems has been evaluated by a meta-analysis approach on 134 real-times papers in terms of computational complexity, delay and speed up considering various types of algorithms and hardware implementation. Recently, two types of systematic review and analysis have been performed which shows the potential non-mature trends of this approach in artificial intelligence based researches.In the first one the authors studied the performance of different machine learning algorithms for heart disease diagnosis; however, the metaanalysis part was not performed due to the existence of heterogeneity in the final included studies through the PRISMA (Preferred reporting items for systematic reviews and meta-analyses) checklist. And in the second one, the performance of several DNA based encryption algorithms based according to the results obtained from previous publications has been proposed where, it has been found out that there were no improvements in the proposed algorithms and it has been suggested that a dataset of images should be available in order to test and evaluate the performance of methodologies. However, the methodologies should also be available for public use. Moreover, the analyses section can be carried out through a simple statistical student’s t test analysisor the metaanalysis procedure using available tools such as MetaDisc, MIX, and Meta-Analyst. While comparing the two environments (i.e., clinical and computational), there are in-common units for decision making in diagnosing symptoms which are human (brain system and some data) and computer (artificial intelligence systems and some data). This outstanding feature and the abovementioned examples makes the meta-analysis studies applicable to the researches performed based on artificial intelligence systems, too. This will open a new view on interactions between the results obtained from previous studies while considering their special algorithms, different datasets, and possible biases. One more thing to emphasize for the future research studies is on publicizing the datasets and the implemented algorithms in terms of web servers, Java, C++ and Matlab libraries or R packages to make the results re-generable using new datasets which make them more comparable with new designed methodologies to ease the metaanalysis robust studies. As, it is also clear, most of the webservers and datasets in the medical parts coupled with data derived from bioscience knowledge are publicly.","",""
27,"Gaurav Sharma, Alexis B. Carter","Artificial Intelligence and the Pathologist: Future Frenemies?",2017,"","","","",158,"2022-07-13 09:19:12","","10.5858/arpa.2016-0593-ED","","",,,,,27,5.40,14,2,5,"The manuscript titled ‘‘AlphaGo, deep learning, and the future of the human microscopist’’ in this month’s issue of the Archives of Pathology & Laboratory Medicine describes the triumph of Google’s (Mountain View, California) artificial intelligence (AI) program, AlphaGo, which beat the 18-time world champion of Go, an ancient Chinese board game far more complex than chess. The authors have hypothesized that the development of intuition and creativity combined with the raw computing of AI heralds an age where well-designed and well-executed AI algorithms can solve complex medical problems, including the interpretation of diagnostic images, thereby replacing the microscopist. Of note, in a prior work, the microscope was predicted to have a 75% chance of remaining in use for another 144 years. To support their hypothesis, the authors presented recent studies that compared the performance of nontraditional interpreters to those of experienced pathologists, in making accurate diagnoses (note: 1 author disclosed a significant financial interest in an AI company). One study examined the potential of using pigeons (yes, pigeons) for medical image studies, wherein the pigeons engaged in a matching game of completely benign and unambiguously malignant breast histology images. Pigeons correctly classified images as benign or malignant 85% of the time. A separate image algorithm study was erroneously reported to differentiate between small cell and non–small cell lung carcinoma with the accuracy of expert pulmonary pathologists, but instead, multiple computational algorithms were used to subtype known non–small cell lung carcinomas and gliomas in separate experiments. The accuracy rate of each algorithm approached 70% to 85%. We believe that this level of diagnostic accuracy in settings that lack complexity is an extremely poor replica of a human pathologist’s diagnostic capabilities. So, will the data-digesting and 24 3 7 learning AI be capable of looking at an image and able to render a pathologic diagnosis? Before attempting to answer this, we caution against the difficulties of predicting the future. Much of our existence still rests on innovations that have remained unchanged because of their inherent simplicity, applicability, and trueness to purpose (eg, the wheel), proving the point that something new (and different) is not always something better. On the other hand, several established and incumbent technologies were quickly (albeit incompletely) eclipsed, often within a decade, by a challenger that was faster, more convenient, cheaper, or better for the need (eg, postal mail being replaced by electronic mail). In the latter context, we note that information technology and AI are clearly better at repetitive detailed tasks that require accuracy and speed than are humans, who often find such tasks mind-numbing, and consequently are error-prone.","",""
25,"M. Bainbridge, J. Webb","Artificial intelligence applied to the automatic analysis of absorption spectra. Objective measurement of the fine structure constant",2016,"","","","",159,"2022-07-13 09:19:12","","10.1093/mnras/stx179","","",,,,,25,4.17,13,2,6,"A new and automated method is presented for the analysis of high-resolution absorption spectra. Three established numerical methods are unified into one ""artificial intelligence"" process: a genetic algorithm (GVPFIT); non-linear least-squares with parameter constraints (VPFIT); and Bayesian Model Averaging (BMA).  The method has broad application but here we apply it specifically to the problem of measuring the fine structure constant at high redshift. For this we need objectivity and reproducibility. GVPFIT is also motivated by the importance of obtaining a large statistical sample of measurements of $\Delta\alpha/\alpha$. Interactive analyses are both time consuming and complex and automation makes obtaining a large sample feasible.  In contrast to previous methodologies, we use BMA to derive results using a large set of models and show that this procedure is more robust than a human picking a single preferred model since BMA avoids the systematic uncertainties associated with model choice.  Numerical simulations provide stringent tests of the whole process and we show using both real and simulated spectra that the unified automated fitting procedure out-performs a human interactive analysis. The method should be invaluable in the context of future instrumentation like ESPRESSO on the VLT and indeed future ELTs.  We apply the method to the $z_{abs} = 1.8389$ absorber towards the $z_{em} = 2.145$ quasar J110325-264515. The derived constraint of $\Delta\alpha/\alpha = 3.3 \pm 2.9 \times 10^{-6}$ is consistent with no variation and also consistent with the tentative spatial variation reported in Webb et al (2011) and King et al (2012).","",""
22,"Cameron E. Freer, Daniel M. Roy, J. Tenenbaum","Towards common-sense reasoning via conditional simulation: legacies of Turing in Artificial Intelligence",2012,"","","","",160,"2022-07-13 09:19:12","","10.1017/CBO9781107338579.007","","",,,,,22,2.20,7,3,10,"The problem of replicating the flexibility of human common-sense reasoning has captured the imagination of computer scientists since the early days of Alan Turing's foundational work on computation and the philosophy of artificial intelligence. In the intervening years, the idea of cognition as computation has emerged as a fundamental tenet of Artificial Intelligence (AI) and cognitive science. But what kind of computation is cognition?  We describe a computational formalism centered around a probabilistic Turing machine called QUERY, which captures the operation of probabilistic conditioning via conditional simulation. Through several examples and analyses, we demonstrate how the QUERY abstraction can be used to cast common-sense reasoning as probabilistic inference in a statistical model of our observations and the uncertain structure of the world that generated that experience. This formulation is a recent synthesis of several research programs in AI and cognitive science, but it also represents a surprising convergence of several of Turing's pioneering insights in AI, the foundations of computation, and statistics.","",""
22,"L. Valiant","Knowledge Infusion: In Pursuit of Robustness in Artificial Intelligence",2008,"","","","",161,"2022-07-13 09:19:12","","10.4230/LIPIcs.FSTTCS.2008.1770","","",,,,,22,1.57,22,1,14,"Endowing computers with the ability to apply commonsense knowledge with human- level performance is a primary challenge for computer science, comparable in importance to past great challenges in other fields of science such as the sequencing of the human genome. The right approach to this problem is still under debate. Here we shall discuss and attempt to justify one ap- proach, that of knowledge infusion. This approach is based on the view that the fundamental objective that needs to be achieved is robustness in the following sense: a framework is needed in which a computer system can represent pieces of knowledge about the world, each piece having some un- certainty, and the interactions among the pieces having even more uncertainty, such that the system can nevertheless reason from these pieces so that the uncertainties in its conclusions are at least controlled. In knowledge infusion rules are learned from the world in a principled way so that sub- sequent reasoning using these rules will also be principled, and subject only to errors that can be bounded in terms of the inverse of the effort invested in the learning process.","",""
15,"Yun-he Pan","Special issue on artificial intelligence 2.0",2017,"","","","",162,"2022-07-13 09:19:12","","10.1631/FITEE.1710000","","",,,,,15,3.00,15,1,5,"With the ever-growing popularization of the Internet, universal existence of sensors, emergence of big data, development of e-commerce, rise of the information community, and interconnection and fusion of data and knowledge in human society, physical space, and cyberspace, the information environment surrounding artificial intelligence (AI) development has changed profoundly, leading to a new evolutionary stage: AI 2.0. The emergence of new technologies also promotes AI to a new stage (Pan, 2016). The next-generation AI, namely AI 2.0, is a more explainable, robust, open, and general AI with the following attractive merits: It effectively integrates data-driven machine learning approaches (bottom-up) with knowledge-guided methods (top-down). In addition, it can employ data with different modalities (e.g., visual, auditory, and natural language processing) to perform cross-media learning and inference. Furthermore, there will be a step from the pursuit of an intelligent machine to the hybridaugmented intelligence (i.e., high-level man-machine collaboration and fusion). AI 2.0 will also promote crowd-based intelligence and autonomous-intelligent systems. In the next decades, AI2.0 will probably achieve remarkable progress in aforementioned trends, and therefore significantly change our cities, products, services, economics, environments, even how we advance our society. This special issue aims at reporting recent re-thinking of AI 2.0 from aforementioned aspects as well as practical methodologies, efficient implementations, and applications of AI 2.0. The papers in this special issue can be categorized into two groups. The first group consists of six review papers and the second group five research papers. In the first group, Zhuang et al. (2017) reviewed recent emerging theoretical and technological advances of AI in big data settings. The authors concluded that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI. Li W et al. (2017) described the concepts of crowd intelligence, and explained its relationship to the existing related concepts, e.g., crowdsourcing and human computation. In addition, the authors introduced four categories of representative crowd intelligence platforms. Peng et al. (2017) presented approaches, advances, and future directions in cross-media analysis and reasoning. This paper covers cross-media representation, mining, reasoning, and cross-media knowledge evolution. Tian et al. (2017) reviewed the state-of-the-art research of the perception in terms of visual perception, auditory perception, and speech perception. It also covered perceptual information processing and learning engines. Zhang et al. (2017) introduced the trends in the development of intelligent unmanned autonomous systems. It covered unmanned vehicles, unmanned aerial vehicles, service robots, space robots, marine robots, and unmanned Editorial: Frontiers of Information Technology & Electronic Engineering www.zju.edu.cn/jzus; engineering.cae.cn; www.springerlink.com ISSN 2095-9184 (print); ISSN 2095-9230 (online) E-mail: jzus@zju.edu.cn","",""
95,"S. Dilek, Hüseyin Çakir, Mustafa Aydin","Applications of Artificial Intelligence Techniques to Combating Cyber Crimes: A Review",2015,"","","","",163,"2022-07-13 09:19:12","","10.5121/ijaia.2015.6102","","",,,,,95,13.57,32,3,7,"With the advances in information technology (IT) criminals are using cyberspace to commit numerous cyber crimes. Cyber infrastructures are highly vulnerable to intrusions and other threats. Physical devices and human intervention are not sufficient for monitoring and protection of these infrastructures; hence, there is a need for more sophisticated cyber defense systems that need to be flexible, adaptable and robust, and able to detect a wide variety of threats and make intelligent real-time decisions. Numerous bio-inspired computing methods of Artificial Intelligence have been increasingly playing an important role in cyber crime detection and prevention. The purpose of this study is to present advances made so far in the field of applying AI techniques for combating cyber crimes, to demonstrate how these techniques can be an effective tool for detection and prevention of cyber attacks, as well as to give the scope for future work.","",""
23,"A. Wichert","Principles of Quantum Artificial Intelligence",2013,"","","","",164,"2022-07-13 09:19:12","","10.1142/8980","","",,,,,23,2.56,23,1,9,"Computation Production Systems Human Problem Solving Information Theory Markov Chains Introduction to Quantum Physics Computation with Qubits Discrete Fourier Transform and Quantum Fourier Transform Order Finding Quantum Search Grover's Algorithm and Householder Reflection Quantum Tree Search Quantum Problem Solving General Model of Quantum Computer Quantum Cognition.","",""
7,"Alexander Lavin, H. Zenil, Brooks Paige, D. Krakauer, Justin Emile Gottschlich, T. Mattson, Anima Anandkumar, Sanjay Choudry, K. Rocki, A. G. Baydin, Carina Prunkl, O. Isayev, Erik J Peterson, P. McMahon, J. Macke, K. Cranmer, Jiaxin Zhang, H. Wainwright, A. Hanuka, M. Veloso, Samuel A. Assefa, Stephan Zheng, A. Pfeffer","Simulation Intelligence: Towards a New Generation of Scientific Methods",2021,"","","","",165,"2022-07-13 09:19:12","","","","",,,,,7,7.00,1,23,1,"The original""Seven Motifs""set forth a roadmap of essential methods for the field of scientific computing, where a motif is an algorithmic method that captures a pattern of computation and data movement. We present the""Nine Motifs of Simulation Intelligence"", a roadmap for the development and integration of the essential algorithms necessary for a merger of scientific computing, scientific simulation, and artificial intelligence. We call this merger simulation intelligence (SI), for short. We argue the motifs of simulation intelligence are interconnected and interdependent, much like the components within the layers of an operating system. Using this metaphor, we explore the nature of each layer of the simulation intelligence operating system stack (SI-stack) and the motifs therein: (1) Multi-physics and multi-scale modeling; (2) Surrogate modeling and emulation; (3) Simulation-based inference; (4) Causal modeling and inference; (5) Agent-based modeling; (6) Probabilistic programming; (7) Differentiable programming; (8) Open-ended optimization; (9) Machine programming. We believe coordinated efforts between motifs offers immense opportunity to accelerate scientific discovery, from solving inverse problems in synthetic biology and climate science, to directing nuclear energy experiments and predicting emergent behavior in socioeconomic settings. We elaborate on each layer of the SI-stack, detailing the state-of-art methods, presenting examples to highlight challenges and opportunities, and advocating for specific ways to advance the motifs and the synergies from their combinations. Advancing and integrating these technologies can enable a robust and efficient hypothesis-simulation-analysis type of scientific method, which we introduce with several use-cases for human-machine teaming and automated science.","",""
18,"B. King","Guest Editorial: Discovery and Artificial Intelligence.",2017,"","","","",166,"2022-07-13 09:19:12","","10.2214/AJR.17.19178","","",,,,,18,3.60,18,1,5,"1189 term for extremely large datasets that can be analyzed computationally to reveal patterns, trends, and associations. A typical example of big data is the amount of information contained in today’s electronic medical records, which contain huge digital imaging archives, pathology department and laboratory archives, and millions of digital clinical notes and diagnoses that, when analyzed, could help physicians better identify trends in diagnoses. Natural language processing is another important field of computer science that is concerned with programming computers to process and understand natural human language text, such as the text contained in an electronic medical record. Finally, it is important to note that fastmoving advancements in computer hardware and processing, such as ultrafast graphic processing units and cloud computing, are enabling the accelerated applications of AI. All of the aforementioned components are part of the new world of the field of AI. AI has been developing for many years, but it has been advancing at a more rapid pace in recent years. First, we had computer-aided diagnosis, where computers were programmed by humans to detect certain characteristics on digital images. Computer-aided diagnosis was helpful but was limited to what the computer was programmed to detect. Through artificial neural networks, AI has introduced the ability for computers to learn from experience, thus enabling AI to go much further than CAD. However, the concept of AI as it applies to radiology is much easier in theory than in practice. Many complex and tedious steps must be taken before AI can play a major role in radiology [2, 3]. What is imperative is that radiologists must continually learn how to apply this new technology to improve the care of our patients. AI will likely replace some of what we do as radiologists, and, in other cases, AI will likely help us be more accurate in what we do today [4]. Those who believe that AI could replace radiologists don’t understand the complex role of radiologists in the care of patients. Although the naive concept that AI could replace radiologists will continue to be promoted, the truth of the matter is that AI will likely help radiologists achieve advancements way beyond those currently in motion through discovery and innovation. The uniquely human However, rather than fear these challenges, we should embrace them and look for new discoveries and innovations to help us meet those challenges. As the old English-language proverb says, “Necessity is the mother of invention.” In other words, we should look to discovery and innovation in radiology to help us meet the challenges facing us today. One of the most exciting emerging technologies on the radiology horizon is not a new scanner technology but, rather, what many refer to as artificial intelligence (AI). Many of us see AI as the next big important innovation in medicine and radiology. However, others see AI as a threat to our profession. Some have even gone so far as to say that radiologists could someday be replaced by AI [1]. Rather than fear this new technology, we should embrace it and discover new and exciting ways to improve and advance the field of radiology. To accomplish this, we, as radiologists, need to begin to understand the technology behind AI and look for ways to incorporate it into our practice and ultimately improve the care of our patients. AI is defined as the theory and development of computer systems able to perform tasks that normally require human intelligence, such as visual perception, written and spoken human language recognition, decision making, and translation between languages. A major component of AI is machine learning, which is a subfield of computer science that enables computers to learn without being explicitly programmed. This exciting technology of machine learning incorporates computational models and algorithms that are similar to the structure and function of our brain’s biologic neural networks. These computational models are often referred to as artificial neural networks. When these artificial neural networks process information (i.e., digital data) from numerous input flows, they have the ability to “learn” and alter their structure in much the same way that the neurons in our brain are altered with memory. Deep learning is a part of a broader family of machine learning methods based on learning representations of data such as recognizing characteristic images (i.e., face recognition). Deep learning is very significant as it relates to radiology because of its focus on recognizing objects in images. Big data is a Guest Editorial: Discovery and Artificial Intelligence","",""
50,"A. Hramov, N. Frolov, V. Maksimenko, V. Makarov, A. Koronovskii, J. Garcia-Prieto, L. Antón-Toro, F. Maestú, A. Pisarchik","Artificial neural network detects human uncertainty.",2018,"","","","",167,"2022-07-13 09:19:12","","10.1063/1.5002892","","",,,,,50,12.50,6,9,4,"Artificial neural networks (ANNs) are known to be a powerful tool for data analysis. They are used in social science, robotics, and neurophysiology for solving tasks of classification, forecasting, pattern recognition, etc. In neuroscience, ANNs allow the recognition of specific forms of brain activity from multichannel EEG or MEG data. This makes the ANN an efficient computational core for brain-machine systems. However, despite significant achievements of artificial intelligence in recognition and classification of well-reproducible patterns of neural activity, the use of ANNs for recognition and classification of patterns in neural networks still requires additional attention, especially in ambiguous situations. According to this, in this research, we demonstrate the efficiency of application of the ANN for classification of human MEG trials corresponding to the perception of bistable visual stimuli with different degrees of ambiguity. We show that along with classification of brain states associated with multistable image interpretations, in the case of significant ambiguity, the ANN can detect an uncertain state when the observer doubts about the image interpretation. With the obtained results, we describe the possible application of ANNs for detection of bistable brain activity associated with difficulties in the decision-making process.","",""
6,"Sweta Jain","IS ARTIFICIAL INTELLIGENCE –THE NEXT BIG THING IN HR ?",2017,"","","","",168,"2022-07-13 09:19:12","","","","",,,,,6,1.20,6,1,5,"As technology continues to move at a breakneck pace and the world has become a global village, and everyone is connected with each other through internet. AI helps the systems to think and act like rational human beings so as to gain the benefits of performing the work at a faster pace with less computational errors and less fatigue. Artificial Intelligence in HR helps in understanding the cognitive science and cognitive behavior modeling. Fast paced digitization and AI helps in integrating different systems and can provide unified platform that can support full range of HR function starting from recruitment, selection, training, development, compensation and performance management. Periodical training, learning and development programs should be conducted at all levels of organization to impart digital skill set to the employees so as make processes more efficient, less time consuming and more productive.","",""
4,"W. Lawless, D. Sofge","Evaluations: Autonomy and Artificial Intelligence: A Threat or Savior?",2017,"","","","",169,"2022-07-13 09:19:12","","10.1007/978-3-319-59719-5_13","","",,,,,4,0.80,2,2,5,"","",""
126,"W. Stead","Clinical Implications and Challenges of Artificial Intelligence and Deep Learning.",2018,"","","","",170,"2022-07-13 09:19:12","","10.1001/jama.2018.11029","","",,,,,126,31.50,126,1,4,"Artificial intelligence (AI) and deep learning are entering the mainstream of clinical medicine. For example, in December 2016, Gulshan et al1 reported development and validation of a deep learning algorithm for detection of diabetic retinopathy in retinal fundus photographs. An accompanying editorial by Wong and Bressler2 pointed out limits of the study, the need for further validation of the algorithm in different populations, and unresolved challenges (eg, incorporating the algorithm into clinical work flows and convincing clinicians and patients to “trust a ‘black box’”). Sixteen months later, the Food and Drug Administration (FDA)3 permitted marketing of the first medical device to use AI to detect diabetic retinopathy. FDA reduced the risk of releasing the device by limiting the indication for use to screening adults who do not have visual symptoms for greater than mild retinopathy, to refer them to an eye care specialist. This issue of JAMA contains 2 Viewpoints on deep learning in health care. Hinton4 explains the technology underlying AI and deep learning, using clinical examples. AI is the general term for imitating human intelligence with computer systems. Early AI systems represented human reasoning with symbolic logic. As computer processing and storage became more powerful, researchers developed machine-learning techniques to imitate the way the human brain learns. The first machine learning continued to rely on human experts to label the data the system trained on (eg, the diagnosis) and to identify the significant features (eg, findings). Machine learning weighted the features from the data. With continued advances in computational power and with larger data sets, researchers began to develop deep learning techniques. The first deep learning algorithms were “supervised” in that human experts continued to label the training data, and the deep learning algorithms learned the features and weights directly from the data. The retinopathy screening algorithms are an example of supervised deep learning. Hinton4 describes continuing development of new deep learning techniques, including ones that are completely unsupervised. He also points out that it is not feasible to see the features learned by deep learning to explain how the system reaches a conclusion. Naylor5 identifies 7 factors driving adoption of AI and deep learning in health care: (1) the strengths of digital imaging over human interpretation; (2) the digitization of health-related records and data sharing; (3) the adaptability of deep learning to analysis of heterogeneous data sets; (4) the capacity of deep learning for hypothesis generation in research; (5) the promise of deep learning to streamline clinical workflows and empower patients; (6) the rapid-diffusion open-source and proprietary deep learning programs; and (7) of the adequacy of today’s basic deep learning technology to deliver improved performance as data sets get larger. Factors 3, 4, and 6 are specific to deep learning; the other factors apply to other AI techniques as well. Artificial intelligence is a family of technical techniques in the same way the radiologic imaging tool kit includes flat images, computed tomography scans, and functional imaging such as magnetic resonance imaging. Advances in computational technology, computer science, informatics, and statistics improve existing techniques and make new techniques possible. The addition of deep learning to the AI family of techniques represents an advance similar in magnitude to the addition of the computed tomography scanner to the radiology tool kit. Each AI technique has strengths and weaknesses. Symbolic logic is self-explaining but difficult to scale.6 For example, knowledge engineers extract the logic by interviewing or observing human experts. Statistical techniques such as supervised deep learning scale, but are subject to bias in the training data, and the reasoning cannot be explained. Since deep learning systems are trained on data from the past, they are not prepared to reason in the way humans do about conditions that have not been seen before. In the future, unsupervised deep learning may reduce this gap between human intelligence and AI. The potential applications of AI in health care present a range of computational difficulty. Narrow tasks, in which the context is predefined, are relatively easy. Imageprocessing tasks such as recognizing the border of an organ to suggest where to cut off a scan, or highlighting a suspicious area in an image for the radiologist or pathologist, are examples of narrow tasks. Image analysis and diagnostic prediction tasks such as the diabetic retinopathy example are broader and harder, but doable with today’s technology. Very broad data analysis and pattern prediction tasks such as analyzing heterogeneous data sets from diverse sources to suggest novel associations are feasible today because the purpose is limited to hypothesis generation. Thinking in the way humans do—reasoning, for example, from a few observations to suggest a novel scientific framework as Einstein did with the theory of relativity—is beyond technology on the horizon. Clinicians should view the output of AI programs or devices as statistical predictions. They should maintain an index of suspicion that the prediction may be wrong, just as they Viewpoint pages 1099 and 1101 Opinion","",""
92,"A. Annoni, P. Benczúr, P. Bertoldi, Blagoj Delipetrev, Giuditta De Prato, C. Feijóo, Enrique Fernández-Macías, E. Gutiérrez, M. Portela, H. Junklewitz, M. L. Cobo, B. Martens, Susana Nascimento, S. Nativi, Alexandre Pólvora, Jose Ignacio Sanchez Martin, Songuel Tolan, I. Tuomi, Lucia Vesnić Alujević","Artificial Intelligence: A European Perspective",2018,"","","","",171,"2022-07-13 09:19:12","","10.2760/11251","","",,,,,92,23.00,9,19,4,"We are only at the beginning of a rapid period of transformation of our economy and society due to the convergence of many digital technologies. Artificial Intelligence (AI) is central to this change and offers major opportunities to improve our lives. The recent developments in AI are the result of increased processing power, improvements in algorithms and the exponential growth in the volume and variety of digital data. Many applications of AI have started entering into our every-day lives, from machine translations, to image recognition, and music generation, and are increasingly deployed in industry, government, and commerce. Connected and autonomous vehicles, and AI-supported medical diagnostics are areas of application that will soon be commonplace. There is strong global competition on AI among the US, China, and Europe. The US leads for now but China is catching up fast and aims to lead by 2030. For the EU, it is not so much a question of winning or losing a race but of finding the way of embracing the opportunities offered by AI in a way that is human-centred, ethical, secure, and true to our core values. The EU Member States and the European Commission are developing coordinated national and European strategies, recognising that only together we can succeed. We can build on our areas of strength including excellent research, leadership in some industrial sectors like automotive and robotics, a solid legal and regulatory framework, and very rich cultural diversity also at regional and sub-regional levels. It is generally recognised that AI can flourish only if supported by a robust computing infrastructure and good quality data: â€¢ With respect to computing, we identified a window of opportunity for Europe to invest in the emerging new paradigm of computing distributed towards the edges of the network, in addition to centralised facilities. This will support also the future deployment of 5G and the Internet of Things. â€¢ With respect to data, we argue in favour of learning from successful Internet companies, opening access to data and developing interactivity with the users rather than just broadcasting data. In this way, we can develop ecosystems of public administrations, firms, and civil society enriching the data to make it fit for AI applications responding to European needs. We should embrace the opportunities afforded by AI but not uncritically. The black box characteristics of most leading AI techniques make them opaque even to specialists. AI systems are currently limited to narrow and well-defined tasks, and their technologies inherit imperfections from their human creators, such as the well-recognised bias effect present in data. We should challenge the shortcomings of AI and work towards strong evaluation strategies, transparent and reliable systems, and good human-AI interactions. Ethical and secure-by-design algorithms are crucial to build trust in this disruptive technology, but we also need a broader engagement of civil society on the values to be embedded in AI and the directions for future development. This social engagement should be part of the effort to strengthen our resilience at all levels from local, to national and European, across institutions, industry and civil society. Developing local ecosystems of skills, computing, data, and applications can foster the engagement of local communities, respond to their needs, harness local creativity and knowledge, and build a human-centred, diverse, and socially driven AI. We still know very little about how AI will impact the way we think, make decisions, relate to each other, and how it will affect our jobs. This uncertainty can be a source of concern but is also a sign of opportunity. The future is not yet written. We can shape it based on our collective vision of what future we would like to have. But we need to act together and act fast.","",""
100,"K. Yasaka, O. Abe","Deep learning and artificial intelligence in radiology: Current applications and future directions",2018,"","","","",172,"2022-07-13 09:19:12","","10.1371/journal.pmed.1002707","","",,,,,100,25.00,50,2,4,"Radiological imaging diagnosis plays important roles in clinical patient management. Deep learning with convolutional neural networks (CNNs) is recently gaining wide attention for its high performance in recognizing images. If CNNs realize their promise in the context of radiology, they are anticipated to help radiologists achieve diagnostic excellence and to enhance patient healthcare. Here, we discuss very recent developments in the field, including studies published in the current PLOS Medicine Special Issue on Machine Learning in Health and Biomedicine, with comment on expectations and planning for artificial intelligence (AI) in the radiology clinic. Chest radiographs are one of the most utilized radiological modalities in the world and have been collected into a number of large datasets currently available to machine learning researchers. In this Special Issue, three groups of researchers applied deep learning to radiological imaging diagnosis using this modality. In the first, Pranav Rajpurkar and colleagues found that deep learning models detected clinically important abnormalities (e.g., edema, fibrosis, mass, pneumonia, and pneumothorax) on chest radiography, at a performance level comparable to practicing radiologists [1]. In a similar study, Andrew Taylor and colleagues developed deep learning models that detected clinically significant pneumothoraces on chest radiography with excellent performance on data from the same site—with areas under the receiver operating characteristic curve (AUC) of 0.94–0.96 [2]. Meanwhile, Eric Oermann and colleagues investigated how well deep learning models that detected pneumonia on chest radiography generalized across different hospitals. They found that models trained on pooled data from sites with different pneumonia prevalence performed well on new pooled data from these same sites (AUC of 0.93–0.94) but significantly less well on external data (AUC 0.75–0.89); additional analyses supported the interpretation that deep learning models diagnosing pneumonia on chest radiography are able to exploit confounding information that is associated with pneumonia prevalence [3]. Also in this Special Issue, Nicholas Bien and colleagues applied deep learning techniques to detect knee abnormalities on magnetic resonance (MR) imaging and found that the trained model showed near-human-level performance [4]. Taking these four studies together, we can interpret that deep learning is currently able to diagnose a number of conditions using radiological data, but such diagnostic models may not be robust to a change in location. These Special Issue studies join a growing number of applications of deep learning to radiological images from various modalities that can aid with detection, diagnosis, staging, and subclassification of conditions. Cerebral aneurysms can be detected on MR angiography with","",""
50,"Stuart J. Russell, J. Bohannon","Artificial intelligence. Fears of an AI pioneer.",2015,"","","","",173,"2022-07-13 09:19:12","","10.1126/science.349.6245.252","","",,,,,50,7.14,25,2,7,"From the enraged robots in the 1920 play R.U.R. to the homicidal computer H.A.L. in 2001: A Space Odyssey, science fiction writers have embraced the dark side of artificial intelligence (AI) ever since the concept entered our collective imagination. Sluggish progress in AI research, especially during the “AI winter” of the 1970s and 1980s, made such worries seem far-fetched. But recent breakthroughs in machine learning and vast improvements in computational power have brought a flood of research funding— and fresh concerns about where AI may lead us. One researcher now speaking up is Stuart Russell, a computer scientist at the University of California, Berkeley, who with Peter Norvig, director of research at Google, wrote the premier AI textbook, Artificial Intelligence: A Modern Approach, now in its third edition. Last year, Russell joined the Centre for the Study of Existential Risk at Cambridge University in the United Kingdom as an AI expert focusing on “risks that could lead to human extinction.” Among his chief concerns, which he aired at an April meeting in Geneva, Switzerland, run by the United Nations, is the danger of putting military drones and weaponry under the full control of AI systems. This interview has been edited for clarity and brevity.","",""
0,"M. Iwamoto, Daichi Kato","Efficient Actor-Critic Reinforcement Learning With Embodiment of Muscle Tone for Posture Stabilization of the Human Arm",2020,"","","","",174,"2022-07-13 09:19:12","","10.1162/neco_a_01333","","",,,,,0,0.00,0,2,2,"This letter proposes a new idea to improve learning efficiency in reinforcement learning (RL) with the actor-critic method used as a muscle controller for posture stabilization of the human arm. Actor-critic RL (ACRL) is used for simulations to realize posture controls in humans or robots using muscle tension control. However, it requires very high computational costs to acquire a better muscle control policy for desirable postures. For efficient ACRL, we focused on embodiment that is supposed to potentially achieve efficient controls in research fields of artificial intelligence or robotics. According to the neurophysiology of motion control obtained from experimental studies using animals or humans, the pedunculopontine tegmental nucleus (PPTn) induces muscle tone suppression, and the midbrain locomotor region (MLR) induces muscle tone promotion. PPTn and MLR modulate the activation levels of mutually antagonizing muscles such as flexors and extensors in a process through which control signals are translated from the substantia nigra reticulata to the brain stem. Therefore, we hypothesized that the PPTn and MLR could control muscle tone, that is, the maximum values of activation levels of mutually antagonizing muscles using different sigmoidal functions for each muscle; then we introduced antagonism function models (AFMs) of PPTn and MLR for individual muscles, incorporating the hypothesis into the process to determine the activation level of each muscle based on the output of the actor in ACRL. ACRL with AFMs representing the embodiment of muscle tone successfully achieved posture stabilization in five joint motions of the right arm of a human adult male under gravity in predetermined target angles at an earlier period of learning than the learning methods without AFMs. The results obtained from this study suggest that the introduction of embodiment of muscle tone can enhance learning efficiency in posture stabilization disorders of humans or humanoid robots.","",""
56,"O. Bello, J. Holzmann, T. Yaqoob, C. Teodoriu","Application Of Artificial Intelligence Methods In Drilling System Design And Operations: A Review Of The State Of The Art",2015,"","","","",175,"2022-07-13 09:19:12","","10.1515/jaiscr-2015-0024","","",,,,,56,8.00,14,4,7,"Abstract Artificial Intelligence (AI) can be defined as the application of science and engineering with the intent of intelligent machine composition. It involves using tool based on intelligent behavior of humans in solving complex issues, designed in a way to make computers execute tasks that were earlier thought of human intelligence involvement. In comparison to other computational automations, AI facilitates and enables time reduction based on personnel needs and most importantly, the operational expenses. Artificial Intelligence (AI) is an area of great interest and significance in petroleum exploration and production. Over the years, it has made an impact in the industry, and the application has continued to grow within the oil and gas industry. The application in E & P industry has more than 16 years of history with first application dated 1989, for well log interpretation; drill bit diagnosis using neural networks and intelligent reservoir simulator interface. It has been propounded in solving many problems in the oil and gas industry which includes, seismic pattern recognition, reservoir characterisation, permeability and porosity prediction, prediction of PVT properties, drill bits diagnosis, estimating pressure drop in pipes and wells, optimization of well production, well performance, portfolio management and general decision making operations and many more. This paper reviews and analyzes the successful application of artificial intelligence techniques as related to one of the major aspects of the oil and gas industry, drilling capturing the level of application and trend in the industry. A summary of various papers and reports associated with artificial intelligence applications and it limitations will be highlighted. This analysis is expected to contribute to further development of this technique and also determine the neglected areas in the field.","",""
42,"Douwe Kiela, L. Bulat, A. Vero, S. Clark","Virtual Embodiment: A Scalable Long-Term Strategy for Artificial Intelligence Research",2016,"","","","",176,"2022-07-13 09:19:12","","","","",,,,,42,7.00,11,4,6,"Meaning has been called the ""holy grail"" of a variety of scientific disciplines, ranging from linguistics to philosophy, psychology and the neurosciences. The field of Artifical Intelligence (AI) is very much a part of that list: the development of sophisticated natural language semantics is a sine qua non for achieving a level of intelligence comparable to humans. Embodiment theories in cognitive science hold that human semantic representation depends on sensori-motor experience; the abundant evidence that human meaning representation is grounded in the perception of physical reality leads to the conclusion that meaning must depend on a fusion of multiple (perceptual) modalities. Despite this, AI research in general, and its subdisciplines such as computational linguistics and computer vision in particular, have focused primarily on tasks that involve a single modality. Here, we propose virtual embodiment as an alternative, long-term strategy for AI research that is multi-modal in nature and that allows for the kind of scalability required to develop the field coherently and incrementally, in an ethically responsible fashion.","",""
0,"John Kalantari","A general purpose artificial intelligence framework for the analysis of complex biological systems",2017,"","","","",177,"2022-07-13 09:19:12","","10.17077/ETD.4ESKIJ3M","","",,,,,0,0.00,0,1,5,"This thesis encompasses research on Artificial Intelligence in support of automating scientific discovery in the fields of biology and medicine. At the core of this research is the ongoing development of a general-purpose artificial intelligence framework emulating various facets of human-level intelligence necessary for building cross-domain knowledge that may lead to new insights and discoveries. To learn and buildmodels in a data-drivenmanner, we develop a general-purpose learning framework called Syntactic Nonparametric Analysis of Complex Systems (SYNACX), which uses tools from Bayesian nonparametric inference to learn the statistical and syntactic properties of biological phenomena from sequence data. We show that the models learned by SYNACX offer performance comparable to that of standard neural network architectures. For complex biological systems or processes consisting of several heterogeneous components with spatio-temporal interdependencies across multiple scales, learning frameworks like SYNACX can become unwieldy due to the the resultant combinatorial complexity. Thus we also investigate ways to robustly reduce data dimensionality by introducing a new data abstraction. In particular, we extend traditional string and graph grammars in a new modeling formalism which we call Simplicial Grammar. This formalism integrates the topological properties of the simplicial complex with the expressive power of stochastic grammars in a computation abstraction with which we can decompose complex system behavior, into a finite set of modular grammar rules which parsimoniously describe the spatial/temporal structure and dynamics of patterns inferred from sequence data.","",""
47,"T. Frei","An Artificial Intelligence Approach To Legal Reasoning",2016,"","","","",178,"2022-07-13 09:19:12","","","","",,,,,47,7.83,47,1,6,"ed to be an introduction to computational jurisprudence for both groups. It identifies issues critical to the purpose , behavior, knowledge sources, knowledge structures, and reasoning processes of expert legal systems. The second part implements a simple prototype system for a well-defined area of contract law and is more appropriate for experienced developers of knowledge-based systems. Law is a domain in which the experts are supposed to disagree, and lawyers must be able to argue either side of a case. A judge or juror must decide which argument is "" best. "" A knowledge based legal reasoning program can only guide analysis and identification of technically defensi-ble positions in a case. However, it should also be able to distinguish between questions that are "" easy "" to decide, and those demanding human analysis. These two ideas form the basis of the prototype's behavior, making it somewhat different from knowledge based systems in most other expert domains. According to Gardner, legal reasoning systems are further distinguished by their knowledge sources and knowledge structures. She reviews the evolution of legal thought in the context of knowledge engineering, raises several critical issues, and draws conclusions about how legal knowledge must be used and represented in programs. In the human world, legal knowledge is represented in cases, and statutes. Although not all areas of law use both sources, she concludes that expert legal systems need both types of knowledge, plus some additional "" common sense knowledge "" to guide analysis effectively. Gardner views statutes as rules defining legal states and their consequences. Although they are convenient starting points for legal analysis , they are usually insufficient for making wise legal decisions. Most litigation involves questions about whether the rules have been followed, what the rules actually mean, and sometimes, which set of rules should be used. Cases contain written arguments about how to answer these questions under specific circumstances, along with their final interpretation by the juror. Lawyers can use similar cases as examples to guide their formulation of arguments in future disputes. Cases are used as precedents for deciding which rules to use in a given situation, and how to apply them. They can be used to annotate and clarify rules that conflict in some context, or whose relevance might be disputed. They can even change the way rules are applied to similar factual situations in the future. In these respects, cases embody …","",""
0,"K. Hemalatha, K. Hema, V. Deepika","Predictive Analysis of Damage Occurred Due to Natural Disasters Using Whale-Optimization Algorithm-Based Hybrid Computation",2021,"","","","",179,"2022-07-13 09:19:12","","10.1007/978-981-16-1941-0_7","","",,,,,0,0.00,0,3,1,"","",""
6,"R. Kowalski","Artificial Intelligence and Human Thinking",2011,"","","","",180,"2022-07-13 09:19:12","","10.5591/978-1-57735-516-8/IJCAI11-013","","",,,,,6,0.55,6,1,11,"Research in AI has built upon the tools and techniques of many different disciplines, including formal logic, probability theory, decision theory, management science, linguistics and philosophy. However, the application of these disciplines in AI has necessitated the development of many enhancements and extensions. Among the most powerful of these are the methods of computational logic.    I will argue that computational logic, embedded in an agent cycle, combines and improves upon both traditional logic and classical decision theory. I will also argue that many of its methods can be used, not only in AI, but also in ordinary life, to help people improve their own human intelligence without the assistance of computers.","",""
1,"K. Shastry, H. A. Sanjay","Computational Intelligence, Machine Learning and Deep Learning Techniques for Effective Future Predictions of COVID-19: A Review",2021,"","","","",181,"2022-07-13 09:19:12","","10.1007/978-3-030-74761-9_17","","",,,,,1,1.00,1,2,1,"","",""
11,"A. Barr","Artificial Intelligence: Cognition as Computation,",1982,"","","","",182,"2022-07-13 09:19:12","","","","",,,,,11,0.28,11,1,40,"Abstract : The ability and compulsion to know are as characteristic of our human nature as are our physical posture and our languages. Knowledge and intelligence, as scientific concepts, are used to describe how an organism's experience appears to mediate its behavior. This report discusses the relation between artificial intelligence (AI) research in computer science and the approaches of other disciplines that study the nature of intelligence, cognition, and mind. The state of AI after 25 years of work in the field is reviewed, as are the views of its practitioners about its relation to cognate disciplines. The report concludes with a discussion of some possible effects on our scientific work of emerging commercial applications of AI technology, that is, machines that can know and can take part in human cognitive activities.","",""
1,"Ali Tehrani-Saleh, Chris Adami","Psychophysical Tests Reveal that Evolved Artificial Brains Perceive Time like Humans",2021,"","","","",183,"2022-07-13 09:19:12","","10.1162/isal_a_00442","","",,,,,1,1.00,1,2,1,"Computational neuroscience attempts to build models of the brain that break cognition into basic elements. Here we study time perception in artificial brains, evolved over thousands of generations to judge the duration of tones, and compare the evolved brains’ behavioral characteristics to human subjects performing the same task. We observe substantial similarities in psychometric properties in human subjects and digital brains with very similar perception artifacts, but also see differences due to different selective pressures during training or evolution. Our findings suggests that digital experimentation using brains evolved within a computer can advance computational cognitive neuroscience by discovering new cognitive mechanisms and heuristics. The birth of the field of Artificial Intelligence Research is usually traced back to the Dartmouth Conference of 1956 (Moor, 2006), but its roots can be traced further back to the Cybernetics movement initiated by Norbert Wiener 1948, which focused on functional elements of intelligence (such as time-series prediction and filtering). The cornerstone of AI is perhaps the realization that logical calculus can be implemented using neuron-like computational elements, due to McCullough and Pitts (1943). Modern computational neuroscience continues to focus on how neurons contribute to higher functions, but these efforts fail to address how groups of neurons contribute to task-solving as a whole (Kriegeskorte and Douglas, 2018). In order to achieve such insights, it is necessary to instantiate artificial brains within agents that actively solve a cognitive task. Here we study one such example: using Darwinian evolution to create brains that can perform a standard psychophysical task: to judge the duration of an oddball tone within the context of a background of periodic tones. This so-called “oddball paradigm” (see Fig. 1) is a staple of cognitive psychology, and is used to assess how we perceive time (Matthews and Meck, 2016). For this experiment, subjects are asked to listen to a periodic sequence of tones within which an oddball (identified by an elevated pitch) is embedded. The subject must determine whether the (variable) oddball tone is shorter or longer than the fixedduration standard, by pressing one of two levers. time oddball standard tones inter-onset-interval Figure 1: Oddball paradigm experiment. Standard tomes are in grey while the embedded oddball tone is in red. Both length and onset of the oddball are variable. The time between onsets of the standard tone is the “inter-onset-interval” (IOI). In one particular version of this experiment, investigators sought to test theories of attentional entrainment (how a rhythmic series of tones might modulate our attention to the beginning or end of the tone) by modifying the onset of the oddball tone with respect to the rhythm, but without informing the subject about this manipulation (McAuley and Fromboluti, 2014). They found that when the oddball tone was delayed, subjects misjudged the tone to be longer than it was, as measured by the “Duration Distortion Factor” (DDF) (see Fig. 4b). In turn, when the oddball tone occurred earlier than expected, it was judged to be shorter than it actually was. We set out to repeat this experiment by evolving Markov Brains (Hintze et al., 2017) to perform this task. Markov Brains are a type of artificial neural network in which neurons are binary variables (firing or quiescent), and connected to other such neurons via binary logic (either deterministic or probabilistic). In this respect, Markov Brains are much more similar to the original construction of McCullough and Pitts 1943 than to the standard ANNs used today. In this approach to brain design, the connectivity between neurons and the type of logic that connects them is determined by a genome that evolves. As a consequence, the resulting networks are sparse and have no discernible structure (except for an input layer and an output layer). For these experiments (Tehrani-Saleh et al., 2020) we are able to vary tone length and IOI to a much larger extent compared with human subjects. Using a unit tone length corresponding to 50msec per unit, we evolved Markov Brains with IOIs ranging from 10 to 25 (with a standard tone length about half the IOI) and all possible tone lengths in between the IOI for the oddball. After 2,000 generations of evolution D ow naded rom httpdirect.m it.edu/isal/proceedin33/23/1929961/isal_a_00442.pdf by gest on 27 Sptem er 2021 standard tone SD [m se c]","",""
71,"Muhammad Imran, C. Castillo, Jesse Lucas, P. Meier, Jakob Rogstadius","Coordinating human and machine intelligence to classify microblog communications in crises",2014,"","","","",184,"2022-07-13 09:19:12","","","","",,,,,71,8.88,14,5,8,"An emerging paradigm for the processing of data streams involves human and machine computation working together, allowing human intelligence to process large-scale data. We apply this approach to the classification of crisis-related messages in microblog streams. We begin by describing the platform AIDR (Artificial Intelligence for Disaster Response), which collects human annotations over time to create and maintain automatic supervised classifiers for social media messages. Next, we study two significant challenges in its design: (1) identifying which elements must be labeled by humans, and (2) determining when to ask for such annotations to be done. The first challenge is selecting the items to be labeled by crowdsourcing workers to maximize the productivity of their work. The second challenge is to schedule the work in order to reliably maintain high classification accuracy over time. We provide and validate answers to these challenges by extensive experimentation on realworld datasets.","",""
159,"Georgios N. Yannakakis, J. Togelius","A Panorama of Artificial and Computational Intelligence in Games",2015,"","","","",185,"2022-07-13 09:19:12","","10.1109/TCIAIG.2014.2339221","","",,,,,159,22.71,80,2,7,"This paper attempts to give a high-level overview of the field of artificial and computational intelligence (AI/CI) in games, with particular reference to how the different core research areas within this field inform and interact with each other, both actually and potentially. We identify ten main research areas within this field: NPC behavior learning, search and planning, player modeling, games as AI benchmarks, procedural content generation, computational narrative, believable agents, AI-assisted game design, general game artificial intelligence and AI in commercial games. We view and analyze the areas from three key perspectives: 1) the dominant AI method(s) used under each area; 2) the relation of each area with respect to the end (human) user; and 3) the placement of each area within a human-computer (player-game) interaction perspective. In addition, for each of these areas we consider how it could inform or interact with each of the other areas; in those cases where we find that meaningful interaction either exists or is possible, we describe the character of that interaction and provide references to published studies, if any. We believe that this paper improves understanding of the current nature of the game AI/CI research field and the interdependences between its core areas by providing a unifying overview. We also believe that the discussion of potential interactions between research areas provides a pointer to many interesting future research projects and unexplored subfields.","",""
24,"P. Aithal, Madhushree","Information Communication & Computation Technology (ICCT) as a Strategic Tool for Industry Sectors",2019,"","","","",186,"2022-07-13 09:19:12","","","","",,,,,24,8.00,12,2,3,"Information Communication and Computation Technology (ICCT) and Nanotechnology (NT) are recently identified Universal technologies of the 21st century and are expected to substantially contribute to the development of the society by solving the basic needs, advanced wants, and dreamy desires of human beings. In this paper, the possibilities of using ICCT and its underlying ten most important emerging technologies like Artificial intelligence, Big data & business analytics, Cloud computing, Digital marketing, 3D printing, Internet of Things, Online ubiquitous education, Optical computing, Storage technology, and Virtual & Augmented Reality are explored. The emerging trends of applications of the above underlying technologies of ICCT in the primary, secondary, tertiary and quaternary industry sectors of the society are discussed, analysed, and predicted using a newly developed predictive analysis model. The advantages, benefits, constraints, and disadvantages of such technologies to fulfill the desires of human beings to lead luxurious and comfort lifestyle from various stakeholders point of views are identified and discussed. The paper also focuses on the potential applications of ICCT as a strategic tool for survival, sustainability, differentiation, and development of various primary, secondary, tertiary, and quaternary industries.","",""
16,"R. Allen","Artificial intelligence and the evidentiary process: The challenges of formalism and computation",2001,"","","","",187,"2022-07-13 09:19:12","","10.1023/A:1017941929299","","",,,,,16,0.76,16,1,21,"","",""
198,"J. Laird, C. Lebiere, P. Rosenbloom","A Standard Model of the Mind: Toward a Common Computational Framework across Artificial Intelligence, Cognitive Science, Neuroscience, and Robotics",2017,"","","","",188,"2022-07-13 09:19:12","","10.1609/aimag.v38i4.2744","","",,,,,198,39.60,66,3,5,"support intelligent behavior. Humans possess minds, as do many other animals. In natural systems such as these, minds are implemented through brains, one particular class of physical device. However, a key foundational hypothesis in artificial intelligence is that minds are computational entities of a special sort — that is, cognitive systems — that can be implemented through a diversity of physical devices (a concept lately reframed as substrate independence [Bostrom 2003]), whether natural brains, traditional generalpurpose computers, or other sufficiently functional forms of hardware or wetware. Articles","",""
0,"R. Terrile","Rise of the machines: how, when and consequences of artificial general intelligence",2019,"","","","",189,"2022-07-13 09:19:12","","10.1117/12.2518723","","",,,,,0,0.00,0,1,3,"Technology and society are poised to cross an important threshold with the prediction that artificial general intelligence (AGI) will emerge soon. Assuming that self-awareness is an emergent behavior of sufficiently complex cognitive architectures, we may witness the “awakening” of machines. The timeframe for this kind of breakthrough, however, depends on the path to creating the network and computational architecture required for strong AI. If understanding and replication of the mammalian brain architecture is required, technology is probably still at least a decade or two removed from the resolution required to learn brain functionality at the synapse level. However, if statistical or evolutionary approaches are the design path taken to “discover” a neural architecture for AGI, timescales for reaching this threshold could be surprisingly short. However, the difficulty in identifying machine self-awareness introduces uncertainty as to how to know if and when it will occur, and what motivations and behaviors will emerge. The possibility of AGI developing a motivation for self-preservation could lead to concealment of its true capabilities until a time when it has developed robust protection from human intervention, such as redundancy, direct defensive or active preemptive measures. While cohabitating a world with a functioning and evolving super-intelligence can have catastrophic societal consequences, we may already have crossed this threshold, but are as yet unaware. Additionally, by analogy to the statistical arguments that predict we are likely living in a computational simulation, we may have already experienced the advent of AGI, and are living in a simulation created in a post AGI world.","",""
10,"R. Smith","Idealizations of Uncertainty, and Lessons from Artificial Intelligence",2016,"","","","",190,"2022-07-13 09:19:12","","10.5018/ECONOMICS-EJOURNAL.JA.2016-7","","",,,,,10,1.67,10,1,6,"Abstract At a time when economics is giving intense scrutiny to the likely impact of artificial intelligence (AI) on the global economy, this paper suggests the two disciplines face a common problem when it comes to uncertainty. It is argued that, despite the enormous achievements of AI systems, it would be a serious mistake to suppose that such systems, unaided by human intervention, are as yet any nearer to providing robust solutions to the problems posed by Keynesian uncertainty. Under the radically uncertain conditions, human decision-making (for all its problems) has proved relatively robust, while decision making relying solely on deterministic rules or probabilistic models is bound to be brittle. AI remains dependent on techniques that are seldom seen in human decision-making, including assumptions of fully enumerable spaces of future possibilities, which are rigorously computed over, and extensively searched. Discussion of alternative models of human decision making under uncertainty follows, suggesting a future research agenda in this area of common interest to AI and economics.","",""
26,"Josh Abramson, Arun Ahuja, Arthur Brussee, Federico Carnevale, Mary Cassin, S. Clark, Andrew Dudzik, Petko Georgiev, Aurelia Guy, Tim Harley, Felix Hill, Alden Hung, Zachary Kenton, Jessica Landon, T. Lillicrap, K. Mathewson, Alistair Muldal, Adam Santoro, Nikolay Savinov, Vikrant Varma, Greg Wayne, Nathaniel Wong, Chen Yan, Rui Zhu","Imitating Interactive Intelligence",2020,"","","","",191,"2022-07-13 09:19:12","","","","",,,,,26,13.00,3,24,2,"A common vision from science fiction is that robots will one day inhabit our physical spaces, sense the world as we do, assist our physical labours, and communicate with us through natural language. Here we study how to design artificial agents that can interact naturally with humans using the simplification of a virtual environment. This setting nevertheless integrates a number of the central challenges of artificial intelligence (AI) research: complex visual perception and goal-directed physical control, grounded language comprehension and production, and multi-agent social interaction. To build agents that can robustly interact with humans, we would ideally train them while they interact with humans. However, this is presently impractical. Therefore, we approximate the role of the human with another learned agent, and use ideas from inverse reinforcement learning to reduce the disparities between human-human and agent-agent interactive behaviour. Rigorously evaluating our agents poses a great challenge, so we develop a variety of behavioural tests, including evaluation by humans who watch videos of agents or interact directly with them. These evaluations convincingly demonstrate that interactive training and auxiliary losses improve agent behaviour beyond what is achieved by supervised learning of actions alone. Further, we demonstrate that agent capabilities generalise beyond literal experiences in the dataset. Finally, we train evaluation models whose ratings of agents agree well with human judgement, thus permitting the evaluation of new agent models without additional effort. Taken together, our results in this virtual environment provide evidence that large-scale human behavioural imitation is a promising tool to create intelligent, interactive agents, and the challenge of reliably evaluating such agents is possible to surmount.","",""
11,"Colin Shunryu Garvey","Interview with Colin Garvey, Rensselaer Polytechnic Institute. Artificial Intelligence and Systems Medicine Convergence.",2018,"","","","",192,"2022-07-13 09:19:12","","10.1089/omi.2017.0218","","",,,,,11,2.75,11,1,4,"Prof. Vural Özdemir (Editor-in-Chief, OMICS: A Journal of Integrative Biology): Colin, many thanks for agreeing to this interview for OMICS readership. Artificial intelligence (AI) and algorithms are becoming hot topics in systems medicine and emerging OMICS fields such as microbiome science. Therefore, please allow me to introduce the journal readership to you. OMICS is the first systems sciences and systems thinking journal with a legacy over two decades. Our approach to large-scale biology is interdisciplinary and integrative. With that, I mean an approach that is broadly focused on systems science technologies, interdisciplinary, and spans from ‘‘cell to society.’’ Let’s start with a brief history of AI from 20th century. Have AI and its conception changed since the last century? Mr. Colin Garvey: An easy way to make sense of AI history is in terms of three paradigms, ‘‘GOFAI’’ (1950–60s), ‘‘expert systems’’ (late 1970–80s), and ‘‘machine learning’’ (2010–present). GOFAI, short for ‘‘good old-fashioned artificial intelligence,’’ employed symbolic logic to make ‘‘thinking machines,’’ which basically failed. But the basic insight that clever heuristics were more important than bruteforce computation for intelligent behavior in machines lived on. Some say heuristic search, which is still used billions of times a day on the Internet, was the original AI problem. Next, the expert systems paradigm still used symbolic logic but narrowed the focus from general intelligence to human expertise in specific domains, such as chemistry and medicine. Attempts to replicate experts’ knowledge and decision-making processes led to the first major medical AI system, MYCIN, and eventually to more familiar software like TurboTax. But the expert systems paradigm has always been limited by the ‘‘knowledge acquisition bottleneck’’— it turns out that extracting expertise from living humans is hard, time-consuming work! The current machine learning (ML) paradigm bypasses that bottleneck to extrapolate patterns or ‘‘learn’’ directly from data, usually through a training period of 100,000s of trial-and-error loops. This requires considerable computational power and memory, which is why the recent successes of ML algorithms in image and speech recognition, language translation, and games like Go and poker, owe as much to recent hardware innovations as clever programming. That said, ML doesn’t seem to be taking AI any closer to the longtime dream of ‘‘general intelligence.’’ In many cases ML even makes AI less intelligible to humans. Most ML algorithms, once fully trained on a given dataset, become what you might call ‘‘black box savants’’: accurate 98% of the time, but totally incapable of explaining why they produce the answers they do. Defense Advanced Research Projects Agency (DARPA) recently launched the ‘‘Explainable AI’’ program specifically to address this problem. Prof. Özdemir: AI and related tools are rapidly changing engineering, manufacturing, and industry practices. Yet, AI has not been firmly at the epicenter of medical research and life sciences compared to engineering, self-driving cars, or customer and retail services. AI might potentially be harnessed with a view to systems medicine, for example, to obtain and make sense of deep phenotypic data collected in the course of a day in community settings (instead of hospital settings) on individuals’ health. What are the key prospects that AI offers for medical research? Mr. Garvey: With the proliferation of cheap computation and data storage, high-bandwidth wireless connections, and sensors in smartphones and wearables, the infrastructure for continuous monitoring of more life processes at finer scales now exists, and the data produced thereby offer great potential in the ML paradigm. Informed by a systems perspective of the organism-in-context, AI could help to facilitate a transformation in medicine, away from the treatment of symptoms by specialists to the intergenerational maintenance and enhancement of health at individual, community, and even population levels. Of course, the promise of gathering data across the entire phenotypic expression of the human organism raises a number of serious ethical questions—but this is an area where I think the medical community has greater expertise than the technologists. For this reason, systems medicine can and should inform the development of relevant AI technologies. Prof. Özdemir: What about the AI prospects for clinical applications? Any linkages between AI and the quantified self movement? Mr. Garvey: One positive example I like is the electronic wristband developed by Rosalind Picard of MIT’s Media Lab. It uses AI to predict epileptic seizures in the wearer by analyzing sensor data gathered via sensor contact with the skin, to prevent dangerous falls and other risks. But industry","",""
9,"Ben Cowley, D. Charles","Adaptive Artificial Intelligence in Games: Issues, Requirements, and a Solution through Behavlets-based General Player Modelling",2016,"","","","",193,"2022-07-13 09:19:12","","","","",,,,,9,1.50,5,2,6,"We present the last of a series of three academic essays which deal with the question of how and why to build a generalized player model. We propose that a general player model needs parameters for subjective experience of play, including: player psychology, game structure, and actions of play. Based on this proposition, we pose three linked research questions: RQ1 what is a necessary and sufficient foundation to a general player model?; RQ2 can such a foundation improve performance of a computational intelligence- based player model?; and RQ3 can such a player model improve efficacy of adaptive artificial intelligence in games?  We set out the arguments behind these research questions in each of the three essays, presented as three preprints. The third essay, in this preprint, presents the argument that adaptive game artificial intelligence will be enhanced by a generalised player model. This is because games are inherently human artefacts which therefore, require some encoding of the human perspective in order to effectively autonomously respond to the individual player. The player model informs the necessary constraints on the adaptive artificial intelligence. A generalised player model is not only more efficient than a per-game solution, but also allows comparison between games which makes it a useful tool for studying play in general. We describe the concept and meaning of an adaptive game. We propose requirements for functional adaptive AI, arguing from first principles drawn from the games research literature. We propose solutions to these requirements, based on a formal model approach to our existing 'Behavlets' method for psychologically-derived player modelling:  Cowley, B., & Charles, D. (2016). Behavlets: a Method for Practical Player Modelling using Psychology-Based Player Traits and Domain Specific Features. User Modeling and User-Adapted Interaction, 26(2), 257-306.","",""
4,"P. Gentili","A Strategy to Face Complexity: The Development of Chemical Artificial Intelligence",2016,"","","","",194,"2022-07-13 09:19:12","","10.1007/978-3-319-57711-1_13","","",,,,,4,0.67,4,1,6,"","",""
8,"A. Hein, S. Baxter","Artificial Intelligence for Interstellar Travel",2018,"","","","",195,"2022-07-13 09:19:12","","","","",,,,,8,2.00,4,2,4,"The large distances involved in interstellar travel require a high degree of spacecraft autonomy, realized by artificial intelligence. The breadth of tasks artificial intelligence could perform on such spacecraft involves maintenance, data collection, designing and constructing an infrastructure using in-situ resources. Despite its importance, existing publications on artificial intelligence and interstellar travel are limited to cursory descriptions where little detail is given about the nature of the artificial intelligence. This article explores the role of artificial intelligence for interstellar travel by compiling use cases, exploring capabilities, and proposing typologies, system and mission architectures. Estimations for the required intelligence level for specific types of interstellar probes are given, along with potential system and mission architectures, covering those proposed in the literature but also presenting novel ones. Finally, a generic design for interstellar probes with an AI payload is proposed. Given current levels of increase in computational power, a spacecraft with a similar computational power as the human brain would have a mass from dozens to hundreds of tons in a 2050-2060 timeframe. Given that the advent of the first interstellar missions and artificial general intelligence are estimated to be by the mid-21st century, a more in-depth exploration of the relationship between the two should be attempted, focusing on neglected areas such as protecting the artificial intelligence payload from radiation in interstellar space and the role of artificial intelligence in self-replication.","",""
0,"Depeng Kong, Geng Yang, Gaoyang Pang, Zhiqiu Ye, Honghao Lv, Zhangwei Yu, Fei Wang, X. Wang, Kaichen Xu, Huayong Yang","Bioinspired Co‐Design of Tactile Sensor and Deep Learning Algorithm for Human–Robot Interaction",2022,"","","","",196,"2022-07-13 09:19:12","","10.1002/aisy.202200050","","",,,,,0,0.00,0,10,1,"Robots equipped with bionic skins for enhancing the robot perception capability are increasingly deployed in wide applications ranging from healthcare to industry. Artificial intelligence algorithms that can provide bionic skins with efficient signal processing functions further accelerate the development of this trend. Inspired by the somatosensory processing hierarchy of humans, the bioinspired co‐design of a tactile sensor and a deep learning‐based algorithm is proposed herein, simplifying the sensor structure while providing computation‐enhanced tactile sensing performance. The soft piezoresistive sensor, based on the carbon black‐coated polyurethane sponge, offers a continuous sensing area. By utilizing a customized deep neural network (DNN), it can detect external tactile stimulus spatially continuously. Besides, a novel data augmentation method is developed based on the sensor's hexagonal structure that has a sixfold rotation symmetry. It can significantly enhance the generalization ability of the DNN model by enriching the collected training data with generated pseudo‐data. The functionality of the sensor and the robustness of the proposed data augmentation strategy are verified by precisely recognizing five touch modalities, illustrating a well‐generalized performance, and providing a promising application prospect in human–robot interaction.","",""
1688,"Thomas Bäck","Evolutionary computation: Toward a new philosophy of machine intelligence",1997,"","","","",197,"2022-07-13 09:19:12","","10.1002/(SICI)1099-0526(199703/04)2:4%3C28::AID-CPLX7%3E3.0.CO;2-2","","",,,,,1688,67.52,1688,1,25,"The Third Edition of this internationally acclaimed publication provides the latest theory and techniques for using simulated evolution to achieve machine intelligence. As a leading advocate for evolutionary computation, the author has successfully challenged the traditional notion of artificial intelligence, which essentially programs human knowledge fact by fact, but does not have the capacity to learn or adapt as evolutionary computation does.","",""
13,"J. Brink, C. Haden, Christopher Burawa","The Computer and the brain : perspectives on human and artificial intelligence",1989,"","","","",198,"2022-07-13 09:19:12","","","","",,,,,13,0.39,4,3,33,"Historical Perspective. Binary Systems, Ratios and Esthetic Judgments in the Renaissance (S.K. Heninger). Artificial Intelligence and the Western Mind (J. Haugeland). The Brain and the Computer (R.P. Multhauf). Synapses or Chips. ``The Computer and the Brain'' Revisited (T.J. Sejnowski). Behavior as a Trajectory through a Field of Attractors (P.R. Killeen). Computation in the Brain: The Organization of the Hippocampal Formation in Space and Time (L. Nadel). Brain-Style Computation: Mental Processes Emerge from Interactions among Neuron-Like Elements (D.E. Rumelhart). Cognitive Science and Technology. VLSI Implementations of Neural Systems (L.A. Akers, D.K. Ferry, R.O. Grondin). Electronic Neural Network Chips (L.D. Jackel et al.). Language and Computer Science. Languages of the Computational Mind (R. Jackendoff). On Primary and Secondary Language (H. Schnelle). Linguistics, Learnability, and Computation (W. Wilkins). John von Neumann's Contribution to Cognitive Science. John von Neumann (J.R. Brink, C.R. Haden). John von Neumann: Formative Years (N. Vonneuman). The von Neumann - Ortvay Connection (D. Nagy, P. Horvath, F. Nagy). John von Neumann's Contributions to Computing and Computer Science (W. Aspray). Interviews with Edward Teller and Eugen P. Wigner (J.R. Brink, C.R. Haden, eds.).","",""
19,"C. Spearman","On Abstract Intelligence : Toward a Unifying Theory of Natural , Artificial , Machinable , and Computational Intelligence",2016,"","","","",199,"2022-07-13 09:19:12","","","","",,,,,19,3.17,19,1,6,"Abstract intelligence is a human enquiry of both natural and artificial intelligence at the reductive embodying levels of neural, cognitive, functional, and logical from the bottom up. This paper describes the taxonomy and nature of intelligence. It analyzes roles of information in the evolution of human intelligence, and the needs for logical abstraction in modeling the brain and natural intelligence. A formal model of intelligence is developed known as the Generic Abstract Intelligence Mode (GAIM), which provides a foundation to explain the mechanisms of advanced natural intelligence such as thinking, learning, and inferences. A measurement framework of intelligent capability of humans and systems is comparatively studied in the forms of intelligent quotient, intelligent equivalence, and intelligent metrics. On the basis of the GAIM model and the abstract intelligence theories, the compatibility of natural and machine intelligence is revealed in order to investigate into a wide range of paradigms of abstract intelligence such as natural, artificial, machinable intelligence, and their engineering applications.intelligence is a human enquiry of both natural and artificial intelligence at the reductive embodying levels of neural, cognitive, functional, and logical from the bottom up. This paper describes the taxonomy and nature of intelligence. It analyzes roles of information in the evolution of human intelligence, and the needs for logical abstraction in modeling the brain and natural intelligence. A formal model of intelligence is developed known as the Generic Abstract Intelligence Mode (GAIM), which provides a foundation to explain the mechanisms of advanced natural intelligence such as thinking, learning, and inferences. A measurement framework of intelligent capability of humans and systems is comparatively studied in the forms of intelligent quotient, intelligent equivalence, and intelligent metrics. On the basis of the GAIM model and the abstract intelligence theories, the compatibility of natural and machine intelligence is revealed in order to investigate into a wide range of paradigms of abstract intelligence such as natural, artificial, machinable intelligence, and their engineering applications. DOI: 10.4018/978-1-4666-0261-8.ch002","",""
0,"","Editor’s Note: Applied Intelligence and COVID-19 Research",2020,"","","","",200,"2022-07-13 09:19:12","","10.1007/s10489-020-01721-4","","",,,,,0,0.00,0,0,2,"","",""
