Summarised,Classification Keywords,Title,Cites,Authors,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL,Query,Papers,Citations,Years,Cites_Year,Cites_Paper,Cites_Author,Papers_Author,Authors_Paper,h_index,g_index,hc_index,hI_index,hI_norm,AWCR,AW_index,AWCRpA,e_index,hm_index,Cites_Author_Year,hI_annual,h_coverage,g_coverage,star_count,year_first,year_last,acc1,acc2,acc5,acc20,hA,tokens_agg
Yes,"neural networks, adversarial robustness",Towards Evaluating the Robustness of Neural Networks,2534.0,N. Carlini,2017.0,Proceedings - IEEE Symposium on Security and Privacy,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85024480368&origin=inward,1.0,2022-07-12 16:26:13,Conference Paper,10.1109/SP.2017.49,1081-6011,https://api.elsevier.com/content/abstract/scopus_id/85024480368,,,39.0,57.0,2534,506.8,2534.0,1.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'neural networks']"
Yes,"Computer Vision, Certified Robustness, Perturbations",Learning perturbation sets for robust machine learning,38.0,"E Wong, JZ Kolter",2020.0,arXiv preprint arXiv:2007.08450,arxiv.org,https://arxiv.org/abs/2007.08450,https://scholar.google.com/scholar?cites=14923687105877479161&as_sdt=2005&sciodt=2007&hl=en,2.0,2022-07-14 10:13:06,,,,,,,,,38,19.0,19.0,2.0,2.0,"… which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and …",https://arxiv.org/pdf/2007.08450,https://scholar.google.com/scholar?q=related:-Y7njFqWG88J:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'perturbations', 'robust']"
No,"Certified Robustness, Smoothing, Neural Network",Input-Specific Robustness Certification for Randomized Smoothing,1.0,"Ruoxin Chen, Jie Li, Junchi Yan, Ping Li, Bin Sheng",2021.0,,,,,2.0,2022-07-13 09:20:12,,10.1609/aaai.v36i6.20579,,,,,,,1,1.0,0.0,5.0,1.0,"Although randomized smoothing has demonstrated high certified robustness and superior scalability to other certified defenses, the high computational overhead of the robustness certification bottlenecks the practical applicability, as it depends heavily on the large sample approximation for estimating the confidence interval. In existing works, the sample size for the confidence interval is universally set and agnostic to the input for prediction. This Input-Agnostic Sampling (IAS) scheme may yield a poor Average Certified Radius (ACR)-runtime trade-off which calls for improvement. In this paper, we propose Input-Specific Sampling (ISS) acceleration to achieve the cost-effectiveness for robustness certification, in an adaptive way of reducing the sampling size based on the input characteristic. Furthermore, our method universally controls the certified radius decline from the ISS sample size reduction. The empirical results on CIFAR-10 and ImageNet show that ISS can speed up the certification by more than three times at a limited cost of 0.05 certified radius. Meanwhile, ISS surpasses IAS on the average certified radius across the extensive hyperparameter settings. Specifically, ISS achieves ACR=0.958 on ImageNet in 250 minutes, compared to ACR=0.917 by IAS under the same condition. We release our code in https://github.com/roy-ch/Input-Specific-Certification.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural network', 'robust', 'smoothing']"
Yes,"Classifier, Robust, Benchmark",Benchmarking Neural Network Robustness to Common Corruptions and Perturbations,1185.0,"Dan Hendrycks, Thomas G. Dietterich",2018.0,,,,,2.0,2022-07-13 09:26:34,,,,,,,,,1185,296.25.00,593.0,2.0,4.0,"In this paper we establish rigorous benchmarks for image classifier robustness. Our first benchmark, ImageNet-C, standardizes and expands the corruption robustness topic, while showing which classifiers are preferable in safety-critical applications. Then we propose a new dataset called ImageNet-P which enables researchers to benchmark a classifier's robustness to common perturbations. Unlike recent robustness research, this benchmark evaluates performance on common corruptions and perturbations not worst-case adversarial perturbations. We find that there are negligible changes in relative corruption robustness from AlexNet classifiers to ResNet classifiers. Afterward we discover ways to enhance corruption and perturbation robustness. We even find that a bypassed adversarial defense provides substantial common perturbation robustness. Together our benchmarks may aid future work toward networks that robustly generalize.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['benchmark', 'classifier', 'robust']"
No,"Certified Robustness, Adversarial Attacks, Neural Network",Neural Network Robustness Verification on GPUs,13.0,"Christoph Müller, Gagandeep Singh, Markus Püschel, Martin T. Vechev",2020.0,,,,,3.0,2022-07-13 09:26:25,,,,,,,,,13,6.5,3.0,4.0,2.0,"Certifying the robustness of neural networks against adversarial attacks is critical to their reliable adoption in real-world systems including autonomous driving and medical diagnosis. Unfortunately, state-of-the-art verifiers either do not scale to larger networks or are too imprecise to prove robustness, which limits their practical adoption. In this work, we introduce GPUPoly, a scalable verifier that can prove the robustness of significantly larger deep neural networks than possible with prior work. The key insight behind GPUPoly is the design of custom, sound polyhedra algorithms for neural network verification on a GPU. Our algorithms leverage the available GPU parallelism and the inherent sparsity of the underlying neural network verification task. GPUPoly scales to very large networks: for example, it can prove the robustness of a 1M neuron, 34-layer deep residual network in about 1 minute. We believe GPUPoly is a promising step towards the practical verification of large real-world networks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'neural network', 'robust']"
No,"Accuracy, Noisy Labels",Robustness of accuracy metric and its inspirations in learning with noisy labels,14.0,"P Chen, J Ye, G Chen, J Zhao, PA Heng",2021.0,… on Artificial Intelligence,ojs.aaai.org,https://ojs.aaai.org/index.php/AAAI/article/view/17364,https://scholar.google.com/scholar?cites=10244312252705347407&as_sdt=2005&sciodt=2007&hl=en,3.0,2022-07-12 13:44:06,,,,,,,,,14,14.0,3.0,5.0,1.0,"… accuracy metric itself is robust for common diagonally-dominant class-conditional noise, ie, a classifier maximizing its accuracy … h) by showing the robustness of the accuracy metric. With …",https://ojs.aaai.org/index.php/AAAI/article/view/17364/17171,https://scholar.google.com/scholar?q=related:TxevlrcbK44J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'label noise']"
Yes,"Assessment, Robust",Assessing the Adversarial Robustness of Monte Carlo and Distillation Methods for Deep Bayesian Neural Network Classification,5.0,"Meet P. Vadera, Satya Narayan Shukla, B. Jalaeian, Benjamin M Marlin",2020.0,,,,,3.0,2022-07-13 09:30:23,,,,,,,,,5,2.5,1.0,4.0,2.0,"In this paper, we consider the problem of assessing the adversarial robustness of deep neural network models under both Markov chain Monte Carlo (MCMC) and Bayesian Dark Knowledge (BDK) inference approximations. We characterize the robustness of each method to two types of adversarial attacks: the fast gradient sign method (FGSM) and projected gradient descent (PGD). We show that full MCMC-based inference has excellent robustness, significantly outperforming standard point estimation-based learning. On the other hand, BDK provides marginal improvements. As an additional contribution, we present a storage-efficient approach to computing adversarial examples for large Monte Carlo ensembles using both the FGSM and PGD attacks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'robust']"
Yes,"NLP, adversarial robustness, semantic robustness, neural networks",Adversarial explanations for understanding image classification decisions and improved neural network robustness,0.0,"Walt Woods, Jack Chen, Christof Teuscher",2019.0,Nature Machine Intelligence,,,,3.0,2022-07-13 15:07:01,Article,10.1038/s42256-019-0104-6,,,1.0,11.0,508.0,516.0,0,0.0,0.0,3.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'neural networks', 'nlp', 'robust']"
Yes,"dependency parsing, robust",Evaluating and Enhancing the Robustness of Neural Network-based Dependency Parsing Models with Adversarial Examples,20.0,"Xiaoqing Zheng, Jiehang Zeng, Yi Zhou, Cho-Jui Hsieh, Minhao Cheng, Xuanjing Huang",2020.0,,,,,4.0,2022-07-13 09:26:34,,10.18653/v1/2020.acl-main.590,,,,,,,20,10.0,3.0,6.0,2.0,"Despite achieving prominent performance on many important tasks, it has been reported that neural networks are vulnerable to adversarial examples. Previously studies along this line mainly focused on semantic tasks such as sentiment analysis, question answering and reading comprehension. In this study, we show that adversarial examples also exist in dependency parsing: we propose two approaches to study where and how parsers make mistakes by searching over perturbations to existing texts at sentence and phrase levels, and design algorithms to construct such examples in both of the black-box and white-box settings. Our experiments with one of state-of-the-art parsers on the English Penn Treebank (PTB) show that up to 77% of input examples admit adversarial perturbations, and we also show that the robustness of parsing models can be improved by crafting high-quality adversaries and including them in the training stage, while suffering little to no performance drop on the clean input data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['nlp', 'robust']"
Yes,"Label Noise, Early Stopping, Deep Learning",Gradient Descent with Early Stopping is Provably Robust to Label Noise for Overparameterized Neural Networks,181.0,"Mingchen Li, M. Soltanolkotabi, Samet Oymak",2019.0,,,,,4.0,2022-07-13 10:10:34,,,,,,,,,181,60.33.00,60.0,3.0,3.0,"Modern neural networks are typically trained in an over-parameterized regime where the parameters of the model far exceed the size of the training data. Such neural networks in principle have the capacity to (over)fit any set of labels including pure noise. Despite this, somewhat paradoxically, neural network models trained via first-order methods continue to predict well on yet unseen test data. This paper takes a step towards demystifying this phenomena. Under a rich dataset model, we show that gradient descent is provably robust to noise/corruption on a constant fraction of the labels despite overparameterization. In particular, we prove that: (i) In the first few iterations where the updates are still in the vicinity of the initialization gradient descent only fits to the correct labels essentially ignoring the noisy labels. (ii) to start to overfit to the noisy labels network must stray rather far from from the initialization which can only occur after many more iterations. Together, these results show that gradient descent with early stopping is provably robust to label noise and shed light on the empirical robustness of deep networks as well as commonly adopted heuristics to prevent overfitting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'early stopping', 'label noise']"
No,"adversarial robustness, explainability, NLP",Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction,0.0,"D Li, B Hu, Q Chen, T Xu, J Tao, Y Zhang",2022.0,… on Artificial Intelligence,ojs.aaai.org,https://ojs.aaai.org/index.php/AAAI/article/view/21342,,4.0,2022-07-12 11:56:04,,,,,,,,,0,0.0,0.0,6.0,1.0,"… work that considers explainability and robustness both in one text classification model. As a step towards understanding the connection between explainability and robustness, we pro…",https://ojs.aaai.org/index.php/AAAI/article/view/21342/21091,https://scholar.google.com/scholar?q=related:0xS8oDvlEBkJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explainability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'explainability', 'nlp']"
Yes,"Assessment, Measure, Adversarial, Robust",Evaluating the robustness of neural networks: An extreme value theory approach,299.0,"TW Weng, H Zhang, PY Chen, J Yi, D Su, Y Gao…",2018.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/1801.10578,https://scholar.google.com/scholar?cites=2078120094241692942&as_sdt=2005&sciodt=2007&hl=en,4.0,2022-07-13 14:59:54,,,,,,,,,299,75.15.00,43.0,7.0,4.0,"… analysis of an arbitrary neural network classifier, we … our knowledge, CLEVER is the first robustness metric that is attack-independent and can be applied to any arbitrary neural network …",https://arxiv.org/pdf/1801.10578,https://scholar.google.com/scholar?q=related:Dp3gITf31hwJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'evaluation', 'metric', 'robust']"
Yes,"Classifier, Robust, Adversarial",Analysis of classifiers' robustness to adversarial perturbations,0.0,"Alhussein Fawzi, Omar Fawzi, Pascal Frossard",2018.0,Machine Learning,,,,4.0,2022-07-13 15:12:02,Article,10.1007/s10994-017-5663-3,0885-6125,,107.0,3.0,481.0,508.0,0,0.0,0.0,3.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'classifier', 'robust']"
Yes,"assessment, robust, adversarial, graph neural network",Graph Robustness Benchmark: Benchmarking the Adversarial Robustness of Graph Machine Learning,6.0,"Qinkai Zheng, Xu Zou, Yuxiao Dong, Yukuo Cen, Da Yin, Jiarong Xu, Yang Yang, Jie Tang",2021.0,,,,,4.0,2022-07-13 09:22:57,,,,,,,,,6,6.0,1.0,8.0,1.0,"Adversarial attacks on graphs have posed a major threat to the robustness of graph machine learning (GML) models. Naturally, there is an ever-escalating arms race between attackers and defenders. However, the strategies behind both sides are often not fairly compared under the same and realistic conditions. To bridge this gap, we present the Graph Robustness Benchmark (GRB) with the goal of providing a scalable, unified, modular, and reproducible evaluation for the adversarial robustness of GML models. GRB standardizes the process of attacks and defenses by 1) developing scalable and diverse datasets, 2) modularizing the attack and defense implementations, and 3) unifying the evaluation protocol in refined scenarios. By leveraging the GRB pipeline, the end-users can focus on the development of robust GML models with automated data processing and experimental evaluations. To support open and reproducible research on graph adversarial learning, GRB also hosts public leaderboards across different scenarios. As a starting point, we conduct extensive experiments to benchmark baseline techniques. GRB is open-source and welcomes contributions from the community. Datasets, codes, leaderboards are available at https://cogdl.ai/grb/home.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'evaluation', 'graph neural network', 'robust']"
No,"robustness to hyperparameter changes, spiking networks",The geometry of robustness in spiking neural networks.,0.0,"Nuno Calaim, F. Dehmelt, P. J. Gonçalves, Christian K. Machens",2022.0,,,,,5.0,2022-07-13 09:26:09,,10.7554/eLife.73276,,,,,,,0,0.0,0.0,4.0,1.0,"Neural systems are remarkably robust against various perturbations, a phenomenon that still requires a clear explanation. Here, we graphically illustrate howneural networks can become robust. We study spiking networks that generate low-dimensional representations, and we show that the neurons; subthreshold voltages are confined to a convex region in a lower-dimensional voltage subspace, which we call a 'bounding box'. Any changes in network parameters (such as number of neurons, dimensionality of inputs, firing thresholds, synapticweights, or transmission delays) can all be understood as deformations of this bounding box. Using these insights, we showthat functionality is preserved as long as perturbations do not destroy the integrity of the bounding box. We suggest that the principles underlying robustness in these networks-low-dimensional representations, heterogeneity of tuning, and precise negative feedback-may be key to understanding the robustness of neural systems at the circuit level.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['hyperparameters', 'spiking neural network']"
Discussion,"Explainability, Reliability, Active Learning",Select Wisely and Explain: Active Learning and Probabilistic Local Post-hoc Explainability,0.0,"Aditya Saini, Ranjitha Prasad",2021.0,,,,,5.0,2022-07-13 09:31:23,,,,,,,,,0,0.0,0.0,2.0,1.0,"Albeit the tremendous performance improvements in designing complex artificial intelligence (AI) systems in data-intensive do-mains, the black-box nature of these systems leads to the lack of trustworthiness. Post-hoc interpretability methods explain the prediction of a black-box ML model for a single instance, and such explanations are being leveraged by domain experts to diagnose the underlying biases of these models. Despite their efficacy in providing valuable insights, existing approaches fail to deliver consistent and reliable explanations. In this paper, we propose an active learning-based technique called UnRAvEL (Uncertainty driven Robust Active Learning Based Locally Faithful Explanations), which consists of a novel acquisition function that is locally faithful and uses uncertainty-driven sampling based on the posterior distribution on the probabilistic locality using Gaussian process regression (GPR). We present a theoretical analysis of UnRAvEL by treating it as a local optimizer and analyzing its regret in terms of instantaneous regrets over a global optimizer. We demonstrate the efficacy of the local samples generated by UnRAvEL by incorporating different kernels such as the Matern and linear kernels in GPR. Through a series of experiments, we show that UnRAvEL outperforms the baselines with respect to stability and local fidelity on several real-world models and datasets. We show that UnRAvEL is an efficient surrogate dataset generator by deriving importance scores on this surrogate dataset using sparse linear models. We also showcase the sample efficiency and flexibility of the developed framework on the Imagenet dataset using a pre-trained ResNet model.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['active learning', 'explainability', 'reliability']"
Yes,"Robust, Adversarial, Recurrent Neural Network",DeepState: Selecting Test Suites to Enhance the Robustness of Recurrent Neural Networks,0.0,"Zixi Liu, Yang Feng, Yining Yin, Zhenyu Chen",2022.0,,,,,6.0,2022-07-13 09:26:17,,10.1145/3510003.3510231,,,,,,,0,0.0,0.0,4.0,1.0,"Deep Neural Networks (DNN) have achieved tremendous success in various software applications. However, accompanied by outstanding effectiveness, DNN-driven software systems could also exhibit incorrect behaviors and result in some critical accidents and losses. The testing and optimization of DNN-driven software systems rely on a large number of labeled data that often require many human efforts, resulting in high test costs and low efficiency. Although plenty of coverage-based criteria have been proposed to assist in the data selection of convolutional neural networks, it is difficult to apply them on Recurrent Neural Network (RNN) models due to the difference between the working nature. In this paper, we propose a test suite selection tool DeepState towards the particular neural network structures of RNN models for reducing the data labeling and computation cost. DeepState selects data based on a stateful perspective of RNN, which identifies the possibly misclassified test by capturing the state changes of neurons in RNN models. We further design a test selection method to enable testers to obtain a test suite with strong fault detection and model improvement capability from a large dataset. To evaluate DeepState, we conduct an extensive empirical study on popular datasets and prevalent RNN models containing image and text processing tasks. The experimental results demonstrate that DeepState outperforms existing coverage-based techniques in selecting tests regarding effectiveness and the inclusiveness of bug cases. Meanwhile, we observe that the selected data can improve the robustness of RNN models effectively.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'recurrent neural network', 'robust']"
No,"Robust, Classification, Support Vector Machine, Benchmark",A robust ensemble approach to learn from positive and unlabeled data using SVM base models,0.0,"Marc Claesen, Frank De Smet, Johan A. K. Suykens, Bart De Moor",2015.0,Neurocomputing,,,,6.0,2022-07-13 12:51:04,Article,10.1016/j.neucom.2014.10.081,0925-2312,,160.0,,73.0,84.0,0,0.0,0.0,4.0,7.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['benchmark', 'classification', 'robust', 'support vector machine']"
Yes,"adversarial robustness, explainability",TopKConv: Increased Adversarial Robustness Through Deeper Interpretability,0.0,"Henry Eigen, Amir Sadovnik",2021.0,,,,,6.0,2022-07-13 09:28:50,,10.1109/ICMLA52953.2021.00011,,,,,,,0,0.0,0.0,2.0,1.0,"Vulnerability to adversarial inputs remains an issue for deep neural networks. Attackers can slightly modify inputs in order to cause adverse behavior in otherwise highly accurate networks. In addition to making these networks less secure for real world applications, this also emphasizes a misalignment between the features the network uses to make decisions and the ones humans use. In this work we propose that more interpretable networks should yield more robust ones since they are able to rely on features that are more understandable to humans. More specifically, we take inspiration from interpretability based approaches to adversarial robustness, and propose a sparsity based defense to counter the impact of overparameterization on adversarial vulnerability. Building off of the work of the Dynamic-K algorithm, which introduces dynamic routing to fully connected layers in order to encourage sparse, interpretable predictions, we propose TopKConv, a novel method of reducing the number of activation channels used to construct each convolutional feature map. The incorporation of TopKConv alongside Dynamic-k results in a significant increase in adversarial accuracy at no cost to benign accuracy. Further, this is achieved with no fine tuning of or adversarial training.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'explainability']"
Yes,"adversarial robustness, mitigation method",Do Wider Neural Networks Really Help Adversarial Robustness?,26.0,"Boxi Wu, Jinghui Chen, Deng Cai, Xiaofei He, Quanquan Gu",2020.0,,,,,6.0,2022-07-13 09:26:34,,,,,,,,,26,13.0,5.0,5.0,2.0,"Adversarial training is a powerful type of defense against adversarial examples. Previous empirical results suggest that adversarial training requires wider networks for better performances. However, it remains elusive how does neural network width affect model robustness. In this paper, we carefully examine the relationship between network width and model robustness. Specifically, we show that the model robustness is closely related to the tradeoff between natural accuracy and perturbation stability, which is controlled by the robust regularization parameter λ. With the same λ, wider networks can achieve better natural accuracy but worse perturbation stability, leading to a potentially worse overall model robustness. To understand the origin of this phenomenon, we further relate the perturbation stability with the network’s local Lipschitzness. By leveraging recent results on neural tangent kernels, we theoretically show that wider networks tend to have worse perturbation stability. Our analyses suggest that: 1) the common strategy of first fine-tuning λ on small networks and then directly use it for wide model training could lead to deteriorated model robustness; 2) one needs to properly enlarge λ to unleash the robustness potential of wider models fully. Finally, we propose a new Width Adjusted Regularization (WAR) method that adaptively enlarges λ on wide models and significantly saves the tuning time.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'mitigation method']"
No,"Feature Selection, Object Tracking, Computer Vision",Joint Group Feature Selection and Discriminative Filter Learning for Robust Visual Object Tracking,0.0,"Tianyang Xu, Zhen-Hua Feng, Xiao-Jun Wu, Josef Kittler, IEEE",2019.0,2019 Ieee/Cvf International Conference On Computer Vision (Iccv 2019),,,,7.0,2022-07-15 11:26:18,Proceedings Paper,10.1109/ICCV.2019.00804,1550-5499,,,,7949.0,7959.0,0,0.0,0.0,5.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'features']"
Yes,"Adversarial Training, Malware, Robust",Improving the Robustness of AI-Based Malware Detection Using Adversarial Machine Learning,1.0,"S. Patil, V. Vijayakumar, D. Walimbe, Siddharth Gulechha, Sushant Shenoy, Aditya Raina, K. Kotecha",2021.0,,,,,7.0,2022-07-13 09:22:57,,10.3390/a14100297,,,,,,,1,1.0,0.0,7.0,1.0,"Cyber security is used to protect and safeguard computers and various networks from ill-intended digital threats and attacks. It is getting more difficult in the information age due to the explosion of data and technology. There is a drastic rise in the new types of attacks where the conventional signature-based systems cannot keep up with these attacks. Machine learning seems to be a solution to solve many problems, including problems in cyber security. It is proven to be a very useful tool in the evolution of malware detection systems. However, the security of AI-based malware detection models is fragile. With advancements in machine learning, attackers have found a way to work around such detection systems using an adversarial attack technique. Such attacks are targeted at the data level, at classifier models, and during the testing phase. These attacks tend to cause the classifier to misclassify the given input, which can be very harmful in real-time AI-based malware detection. This paper proposes a framework for generating the adversarial malware images and retraining the classification models to improve malware detection robustness. Different classification models were implemented for malware detection, and attacks were established using adversarial images to analyze the model’s behavior. The robustness of the models was improved by means of adversarial training, and better attack resistance is observed.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial training', 'cybersecurity', 'robust']"
Yes,"Graph Neural Network, Robust",Certified Robustness of Graph Neural Networks against Adversarial Structural Perturbation,18.0,"Binghui Wang, Jinyuan Jia, Xiaoyu Cao, N. Gong",2020.0,,,,,8.0,2022-07-13 09:26:34,,10.1145/3447548.3467295,,,,,,,18,9.0,5.0,4.0,2.0,"Graph neural networks (GNNs) have recently gained much attention for node and graph classification tasks on graph-structured data. However, multiple recent works showed that an attacker can easily make GNNs predict incorrectly via perturbing the graph structure, i.e., adding or deleting edges in the graph. We aim to defend against such attacks via developing certifiably robust GNNs. Specifically, we prove the first certified robustness guarantee of any GNN for both node and graph classifications against structural perturbation. Moreover, we show that our certified robustness guarantee is tight. Our results are based on a recently proposed technique called randomized smoothing, which we extend to graph data. We also empirically evaluate our method for both node and graph classifications on multiple GNNs and multiple benchmark datasets. For instance, on the Cora dataset, Graph Convolutional Network with our randomized smoothing can achieve a certified accuracy of 0.49 when the attacker can arbitrarily add/delete at most 15 edges in the graph.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['graph neural network', 'robust']"
Discussion,"Security, Robust, Adversarial",Adversarial Attacks and Defenses in Deep Learning,133.0,K. Ren,2020.0,Engineering,,https://api.elsevier.com/content/article/eid/1-s2.0-S209580991930503X,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85078475784&origin=inward,8.0,2022-07-12 16:24:02,Article,10.1016/j.eng.2019.12.012,2095-8099,https://api.elsevier.com/content/abstract/scopus_id/85078475784,6.0,3.0,346.0,360.0,133,66.5,133.0,1.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'cybersecurity', 'robust']"
Yes,"Attack Algorithms, Perturbations, Robust",Learning to generate noise for multi-attack robustness,3.0,"D Madaan, J Shin, SJ Hwang",2021.0,International Conference on …,proceedings.mlr.press,http://proceedings.mlr.press/v139/madaan21a.html,https://scholar.google.com/scholar?cites=10029031126071377800&as_sdt=2005&sciodt=2007&hl=en,8.0,2022-07-12 13:50:33,,,,,,,,,3,3.0,1.0,3.0,1.0,… to generate noise to improve the model’s robustness against multiple types of attacks. Its key component is Meta Noise Generator (MNG) that outputs optimal noise to stochastically …,http://proceedings.mlr.press/v139/madaan21a/madaan21a.pdf,https://scholar.google.com/scholar?q=related:iP_RTqxGLosJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'perturbations', 'robust']"
Yes,"Robust, Adversarial, Privacy",Certified Robustness to Adversarial Examples with Differential Privacy,0.0,"Mathias Lecuyer, Vaggelis Atlidakis, Roxana Geambasu, Daniel Hsu, Suman Jana, IEEE",2019.0,2019 Ieee Symposium On Security And Privacy (Sp 2019),,,,11.0,2022-07-13 13:06:07,Proceedings Paper,10.1109/SP.2019.00044,1081-6011,,,,656.0,656.0,0,0.0,0.0,6.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'privacy', 'robust']"
Yes,"Data Augmentation, Autoencoder, Projected Gradient Descent",Improving Robustness of Deep Learning Systems with Fast and Customizable Adversarial Data Generation,0.0,"Mehmet Melih Arıcı, A. Sen",2021.0,,,,,11.0,2022-07-13 09:19:22,,10.1109/AITEST52744.2021.00017,,,,,,,0,0.0,0.0,2.0,1.0,"Deep Learning (DL) is the force behind the success of solving many complicated tasks in recent years. With the use of DL systems in safety-critical applications, it has become of great importance to make these systems robust against adversarial attacks. Adversarial data generation is an effective tool to make DL systems robust against such attacks, with the help of adversarial training. Recent studies focus on gradient-based adversarial attacks. Although they can successfully generate adversarial samples, high computation cost and lack of flexibility over input generation arise the need for an efficient and flexible adversarial attack methodology. In this paper, we present DeepCustom, a fast and customizable adversarial data generation framework towards bridging this gap. Convolutional autoencoders with custom loss functions, enable user-configurable data generation within a much shorter time compared to the state-of-the-art attack method called PGD. Experiments show that our technique produces adversarial samples faster than PGD and using these samples in adversarial training, allows comparable robustness against adversarial attacks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['autoencoder', 'data augmentation', 'projected gradient descent']"
No,"Robust, Classifier, Prediction",An Empirical Study of Robustness and Stability of Machine Learning Classifiers in Software Defect Prediction,15.0,"A. Kaur, Kamaldeep Kaur",2014.0,,,,,11.0,2022-07-13 09:23:56,,10.1007/978-3-319-11218-3_35,,,,,,,15,2.28,8.0,2.0,8.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classifier', 'prediction', 'robust']"
No,"Adversarial, Robust, Generative Adversarial Model",ADVERSARIAL LEARNING ON ROBUSTNESS AND GENERATIVE MODELS,0.0,Qingyi Gao,2021.0,,,,,12.0,2022-07-13 09:23:14,,10.25394/PGS.15085227.V1,,,,,,,0,0.0,0.0,1.0,1.0,"In this dissertation, we study two important problems in the area of modern deep learning: adversarial robustness and adversarial generative model. In the first part, we study the generalization performance of deep neural networks (DNNs) in adversarial learning. Recent studies have shown that many machine learning models are vulnerable to adversarial attacks, but much remains unknown concerning its generalization error in this scenario. We focus on the $\ell_\infty$ adversarial attacks produced under the fast gradient sign method (FGSM). We establish a tight bound for the adversarial Rademacher complexity of DNNs based on both spectral norms and ranks of weight matrices. The spectral norm and rank constraints imply that this class of networks can be realized as a subset of the class of a shallow network composed with a low dimensional Lipschitz continuous function. This crucial observation leads to a bound that improves the dependence on the network width compared to previous works and achieves depth independence. We show that adversarial Rademacher complexity is always larger than its natural counterpart, but the effect of adversarial perturbations can be limited under our weight normalization framework. In the second part, we study deep generative models that receive great success in many fields. It is well-known that the complex data usually does not populate its ambient Euclidean space but resides in a lower-dimensional manifold instead. Thus, misspecifying the latent dimension in generative models will result in a mismatch of latent representations and poor generative qualities. To address these problems, we propose a novel framework called Latent Wasserstein GAN (LWGAN) to fuse the auto-encoder and WGAN such that the intrinsic dimension of data manifold can be adaptively learned by an informative latent distribution. In particular, we show that there exist an encoder network and a generator network in such a way that the intrinsic dimension of the learned encodes distribution is equal to the dimension of the data manifold. Theoretically, we prove the consistency of the estimation for the intrinsic dimension of the data manifold and derive a generalization error bound for LWGAN. Comprehensive empirical experiments verify our framework and show that LWGAN is able to identify the correct intrinsic dimension under several scenarios, and simultaneously generate high-quality synthetic data by samples from the learned latent distribution.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'generative adversarial networks', 'robust']"
Discussion,"review, adversarial robustness, mitigation method, identification method",On evaluating adversarial robustness,561.0,"N Carlini, A Athalye, N Papernot, W Brendel…",2019.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/1902.06705,https://scholar.google.com/scholar?cites=4715738924964583360&as_sdt=2005&sciodt=2007&hl=en,12.0,2022-07-12 11:56:54,,,,,,,,,561,187.0,112.0,5.0,3.0,"… the robustness of a model from the perspective of an adversary, we can estimate the worst-case robustness … However, analyzing the worst-case robustness can discover a difference. If a …",https://arxiv.org/pdf/1902.06705,https://scholar.google.com/scholar?q=related:wBf_M5GrcUEJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'identification', 'mitigation method', 'survey']"
Yes,"Design, Training, Adversarial, Robust, Explainability",Adversarially Robust and Explainable Model Compression with On-Device Personalization for Text Classification,0.0,"Yao Qiang, Supriya Tumkur Suresh Kumar, Marco Brocanelli, D. Zhu",2021.0,,,,,13.0,2022-07-13 09:26:01,,,,,,,,,0,0.0,0.0,4.0,1.0,"On-device Deep Neural Networks (DNNs) have recently gained more attention due to the increasing computing power of the mobile devices and the number of applications in Computer Vision (CV), Natural Language Processing (NLP), and Internet of Things (IoTs). Unfortunately, the existing efficient convolutional neural network (CNN) architectures designed for CV tasks are not directly applicable to NLP tasks and the tiny Recurrent Neural Network (RNN) architectures have been designed primarily for IoT applications. In NLP applications, although model compression has seen initial success in on-device text classification, there are at least three major challenges yet to be addressed: adversarial robustness, explainability, and personalization. Here we attempt to tackle these challenges by designing a new training scheme for model compression and adversarial robustness, including the optimization of an explainable feature mapping objective, a knowledge distillation objective, and an adversarially robustness objective. The resulting compressed model is personalized using on-device private training data via fine-tuning. We perform extensive experiments to compare our approach with both compact RNN (e.g., FastGRNN) and compressed RNN (e.g., PRADO) architectures in both natural and adversarial NLP test settings.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'design', 'explainability', 'robust', 'training']"
Yes,"Pruning, Robust, Performance",""" Understanding Robustness Lottery"": A Comparative Visual Analysis of Neural Network Pruning Approaches",0.0,"Z Li, S Liu, X Yu, K Bhavya, J Cao, DJ Daniel…",2022.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2206.07918,,13.0,2022-07-13 12:03:09,,,,,,,,,0,0.0,0.0,7.0,1.0,"… assumption that a well optimized neural network model reach to a function’… robustness in relationship to model’s internal representation. Particularly, our framework provides explanation …",https://arxiv.org/pdf/2206.07918,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['performance', 'pruning', 'robust']"
No,"Deep Learning, Noise, Speech Recognition",Instantaneous Frequency Features for Noise Robust Speech Recognition,1.0,"Shekhar Nayak, D. Shashank, Saurabhchand Bhati, Koilakuntla Bramhendra, K. S. R. Murty",2019.0,,,,,14.0,2022-07-13 09:26:17,,10.1109/NCC.2019.8732216,,,,,,,1,0.33,0.0,5.0,3.0,"Analytic phase of the speech signal plays an important role in human speech perception, specially in the presence of noise. Generally, phase information is ignored in most of the recent speech recognition systems. In this paper, we illustrate the importance of analytic phase of the speech signal for noise robust automatic speech recognition. To avoid phase wrapping problem involved in the computation of analytic phase, features are extracted from instantaneous frequency (IF) which is time derivative of analytic phase. Deep neural network (DNN) based acoustic models are trained on clean speech using features extracted from the IF of speech signals. Robustness of IF features in combination with mel-frequency cepstral coefficients (MFCCs) was evaluated in varied noisy conditions. System combination using minimum Bayes risk decoding of IF features with MFCCs delivered absolute improvements of upto 13% over MFCC features alone for DNN based systems under noisy conditions. The impact of the system combination of magnitude and phase based features on different phonetic classes was studied under noisy conditions and was found to model both voiced and unvoiced phonetic classes efficiently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'noise', 'time series']"
Yes,"robust, framework",HyDREA: Towards More Robust and Efficient Machine Learning Systems with Hyperdimensional Computing,3.0,"Justin Morris, Kazim Ergun, Behnam Khaleghi, M. Imani, Baris Aksanli, T. Simunic",2021.0,,,,,14.0,2022-07-13 09:24:47,,10.23919/DATE51398.2021.9474218,,,,,,,3,3.0,1.0,6.0,1.0,"Today's systems, especially in the age of federated learning, rely on sending all the data to the cloud, and then use complex algorithms, such as Deep Neural Networks, which require billions of parameters and many hours to train a model. In contrast, the human brain can do much of this learning effortlessly. Hyperdimensional (HD) Computing aims to mimic the behavior of the human brain by utilizing high dimensional representations. This leads to various desirable properties that other Machine Learning (ML) algorithms lack such as: robustness to noise in the system and simple, highly parallel operations. In this paper, we propose HyDREA, a HD computing system that is Robust, Efficient, and Accurate. To evaluate the feasibility of HyDREA in a federated learning environment with wireless communication noise, we utilize NS-3, a popular network simulator that models a real world environment with wireless communication noise. We found that HyDREA is 48× more robust to noise than other comparable ML algorithms. We additionally propose a Processing-in-Memory (PIM) architecture that adaptively changes the bitwidth of the model based on the signal to noise ratio (SNR) of the incoming sample to maintain the robustness of the HD model while achieving high accuracy and energy efficiency. Our results indicate that our proposed system loses less than 1% classification accuracy, even in scenarios with an SNR of 6.64. Our PIM architecture is also able to achieve 255× better energy efficiency and speed up execution time by 28× compared to the baseline PIM architecture.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['framework', 'robust']"
Yes,"Framework, Robust, Neural Network",Efficient Neural Network Robustness Certification with General Activation Functions,351.0,"Huan Zhang, Tsui-Wei Weng, Pin-Yu Chen, Cho-Jui Hsieh, L. Daniel",2018.0,,,,,14.0,2022-07-13 09:26:34,,,,,,,,,351,88.15.00,70.0,5.0,4.0,"Finding minimum distortion of adversarial examples and thus certifying robustness in neural network classifiers for given data points is known to be a challenging problem. Nevertheless, recently it has been shown to be possible to give a non-trivial certified lower bound of minimum adversarial distortion, and some recent progress has been made towards this direction by exploiting the piece-wise linear nature of ReLU activations. However, a generic robustness certification for general activation functions still remains largely unexplored. To address this issue, in this paper we introduce CROWN, a general framework to certify robustness of neural networks with general activation functions for given input data points. The novelty in our algorithm consists of bounding a given activation function with linear and quadratic functions, hence allowing it to tackle general activation functions including but not limited to four popular choices: ReLU, tanh, sigmoid and arctan. In addition, we facilitate the search for a tighter certified lower bound by adaptively selecting appropriate surrogates for each neuron activation. Experimental results show that CROWN on ReLU networks can notably improve the certified lower bounds compared to the current state-of-the-art algorithm Fast-Lin, while having comparable computational efficiency. Furthermore, CROWN also demonstrates its effectiveness and flexibility on networks with general activation functions, including tanh, sigmoid and arctan.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['framework', 'neural network', 'robust']"
Yes,"robustness, assessment",Evaluating Robustness of AI Models against Adversarial Attacks,2.0,"Chih-Ling Chang, Jui-Lung Hung, Chin-Wei Tien, Chia-Wei Tien, S. Kuo",2020.0,,,,,15.0,2022-07-13 09:19:22,,10.1145/3385003.3410920,,,,,,,2,1.0,0.0,5.0,2.0,"Recently developed adversarial attacks on neural networks have become more aggressive and dangerous, because of which Artificial Intelligence (AI) models are no longer sufficiently robust against them. It is important to have a set of effective and reliable methods to detect malicious attacks to ensure the security of AI models. Such standardized methods can also serve as a reference for researchers to develop robust models and new kinds of attacks. This study proposes a method to assess the robustness of AI models. Six commonly used image classification CNN models were evaluated when subjected to 13 types of adversarial attacks. The robustness of the models is calculated unbiased and can be used as a reference for further improvement. It is distinguished from prior related works that our algorithm is attack-agnostic and is applicable to neural network model.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'robust']"
Yes,"Defence, Adversarial Attack, Evaluation",Increasing Confidence in Adversarial Robustness Evaluations,0.0,"Roland S. Zimmermann, Wieland Brendel, Florian Tramèr, Nicholas Carlini",2022.0,,,,,15.0,2022-07-13 09:27:43,,10.48550/arXiv.2206.13991,,,,,,,0,0.0,0.0,4.0,1.0,"Hundreds of defenses have been proposed to make deep neural networks robust against minimal (adversarial) input perturbations. However, only a handful of these defenses held up their claims because correctly evaluating robustness is extremely challenging: Weak attacks often fail to ﬁnd adversarial examples even if they unknowingly exist, thereby making a vulnerable network look robust. In this paper, we propose a test to identify weak attacks, and thus weak defense evaluations. Our test slightly modiﬁes a neural network to guarantee the existence of an adversarial example for every sample. Consequentially, any correct attack must succeed in breaking this modiﬁed network. For eleven out of thirteen previously-published defenses, the original evaluation of the defense fails our test, while stronger attacks that break these defenses pass it. We hope that attack unit tests — such as ours — will be a major component in future robustness evaluations and increase conﬁdence in an empirical ﬁeld that is currently riddled with skepticism. Online version & code: zimmerrol.github.io/active-tests/",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'defense', 'evaluation']"
Yes,"Training, Robust",Attributional Robustness Training Using Input-Gradient Spatial Alignment,7.0,"M. Singh, Nupur Kumari, Puneet Mangla, Abhishek Sinha, V. Balasubramanian, Balaji Krishnamurthy",2019.0,,,,,15.0,2022-07-13 09:22:33,,10.1007/978-3-030-58583-9_31,,,,,,,7,2.33,1.0,6.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['robust', 'training']"
Yes,"Adversarial, Generative Adversarial Network, Robust, Accuracy",ARGAN: Adversarially Robust Generative Adversarial Networks for Deep Neural Networks Against Adversarial Examples,0.0,"Seok-Hwan Choi, Jinmyeong Shin, Peng Liu, Yoon-Ho Choi",2022.0,,,,,15.0,2022-07-13 09:32:18,,10.1109/access.2022.3160283,,,,,,,0,0.0,0.0,4.0,1.0,"An adversarial example, which is an input instance with small, intentional feature perturbations to machine learning models, represents a concrete problem in Artificial intelligence safety. As an emerging defense method to defend against adversarial examples, generative adversarial networks-based defense methods have recently been studied. However, the performance of the state-of-the-art generative adversarial networks-based defense methods is limited because the target deep neural network models with generative adversarial networks-based defense methods are robust against adversarial examples but make a false decision for legitimate input data. To solve the accuracy degradation of the generative adversarial networks-based defense methods for legitimate input data, we propose a new generative adversarial networks-based defense method, which is called Adversarially Robust Generative Adversarial Networks(ARGAN). While converting input data to machine learning models using the two-step transformation architecture, ARGAN learns the generator model to reflect the vulnerability of the target deep neural network model against adversarial examples and optimizes parameter values of the generator model for a joint loss function. From the experimental results under various datasets collected from diverse applications, we show that the accuracy of ARGAN for legitimate input data is good-enough while keeping the target deep neural network model robust against adversarial examples. We also show that the accuracy of ARGAN outperforms the accuracy of the state-of-the-art generative adversarial networks-based defense methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial', 'generative adversarial networks', 'robust']"
No,"Speech Recognition, Robust, Error Rate",Improving Noise Robust Automatic Speech Recognition with Single-Channel Time-Domain Enhancement Network,35.0,"K. Kinoshita, Tsubasa Ochiai, Marc Delcroix, T. Nakatani",2020.0,,,,,15.0,2022-07-13 10:10:34,,10.1109/ICASSP40776.2020.9053266,,,,,,,35,17.5,9.0,4.0,2.0,"With the advent of deep learning, research on noise-robust automatic speech recognition (ASR) has progressed rapidly. However, ASR performance in noisy conditions of single-channel systems remains unsatisfactory. Indeed, most single-channel speech enhancement (SE) methods (denoising) have brought only limited performance gains over state-of-the-art ASR back-end trained on multi-condition training data. Recently, there has been much research on neural network-based SE methods working in the time-domain showing levels of performance never attained before. However, it has not been established whether the high enhancement performance achieved by such time-domain approaches could be translated into ASR. In this paper, we show that a single-channel time-domain denoising approach can significantly improve ASR performance, providing more than 30 % relative word error reduction over a strong ASR back-end on the real evaluation data of the single-channel track of the CHiME-4 dataset. These positive results demonstrate that single-channel noise reduction can still improve ASR performance, which should open the door to more research in that direction.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['error rate', 'robust', 'time series']"
Yes,"robust, adversarial, fairness",Fairness through robustness: Investigating robustness disparity in deep learning,8.0,V. Nanda,2021.0,"FAccT 2021 - Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency",,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85102612039&origin=inward,15.0,2022-07-12 16:28:30,Conference Paper,10.1145/3442188.3445910,,https://api.elsevier.com/content/abstract/scopus_id/85102612039,,,466.0,477.0,8,8.0,8.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'fairness', 'robust']"
Yes,"Training, Recommendation System, Robust, Adversarial, Framework",Adversarial Collaborative Neural Network for Robust Recommendation,29.0,"Feng Yuan, Lina Yao, B. Benatallah",2019.0,,,,,15.0,2022-07-13 09:26:25,,10.1145/3331184.3331321,,,,,,,29,10.07,10.0,3.0,3.0,"Most of recent neural network(NN)-based recommendation techniques mainly focus on improving the overall performance, such as hit ratio for top-N recommendation, where the users' feedbacks are considered as the ground-truth. In real-world applications, those feedbacks are possibly contaminated by imperfect user behaviours, posing challenges on the design of robust recommendation methods. Some methods apply man-made noises on the input data to train the networks more effectively (e.g. the collaborative denoising auto-encoder). In this work, we propose a general adversarial training framework for NN-based recommendation models, improving both the model robustness and the overall performance. We apply our approach on the collaborative auto-encoder model, and show that the combination of adversarial training and NN-based models outperforms highly competitive state-of-the-art recommendation methods on three public datasets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'framework', 'recommender systems', 'robust', 'training']"
Yes,"formal verification, robustness, neural networks",Eager Falsification for Accelerating Robustness Verification of Deep Neural Networks,1.0,"Xingwu Guo, Wenjie Wan, Zhaodi Zhang, Min Zhang, Fu Song, Xuejun Wen",2021.0,,,,,16.0,2022-07-13 09:28:33,,10.1109/ISSRE52982.2021.00044,,,,,,,1,1.0,0.0,6.0,1.0,"Formal robustness verification of deep neural networks (DNNs) is a promising approach for achieving a provable reliability guarantee to AI-enabled software systems. Limited scalability is one of the main obstacles to the verification problem. In this paper, we propose eager falsification to accelerate the robustness verification of DNNs. It divides the verification problem into a set of independent subproblems and solves them in descending order of their falsification probabilities. Once a subproblem is falsified, the verification terminates with a conclusion that the network is not robust. We introduce a notion of label affinity to measure the falsification probability and present an approach to computing the probability based on symbolic interval propagation. Our approach is orthogonal to existing verification techniques. We integrate it into four state-of-the-art verification tools, i.e., MIPVerify, Neurify, DeepZ, and DeepPoly, and conduct extensive experiments on 8 benchmark datasets. The experimental results show that our approach can significantly improve these tools by up to 200x speedup when the perturbation distance is in a reasonable range.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['formal verification', 'neural networks', 'robust']"
Yes,"adversarial robustness, robustness to natural perturbations",Understanding and mitigating the tradeoff between robustness and accuracy,11.0,A. Raghunathan,2020.0,"37th International Conference on Machine Learning, ICML 2020",,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85098425117&origin=inward,16.0,2022-07-12 16:25:58,Conference Paper,,,https://api.elsevier.com/content/abstract/scopus_id/85098425117,,,7865.0,7875.0,11,5.5,11.0,1.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'perturbations']"
Yes,"Robust, Adversarial",DeepCloak: Masking Deep Neural Network Models for Robustness Against Adversarial Samples,62.0,"Ji Gao, Beilun Wang, Zeming Lin, Weilin Xu, Yanjun Qi",2017.0,,,,,17.0,2022-07-13 09:26:34,,,,,,,,,62,12.4,12.0,5.0,5.0,"Recent studies have shown that deep neural networks (DNN) are vulnerable to adversarial samples: maliciously-perturbed samples crafted to yield incorrect model outputs. Such attacks can severely undermine DNN systems, particularly in security-sensitive settings. It was observed that an adversary could easily generate adversarial samples by making a small perturbation on irrelevant feature dimensions that are unnecessary for the current classification task. To overcome this problem, we introduce a defensive mechanism called DeepCloak. By identifying and removing unnecessary features in a DNN model, DeepCloak limits the capacity an attacker can use generating adversarial samples and therefore increase the robustness against such inputs. Comparing with other defensive approaches, DeepCloak is easy to implement and computationally efficient. Experimental results show that DeepCloak can increase the performance of state-of-the-art DNN models against adversarial samples.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
Yes,"Mixup, Classification, Out-of-Distribution Data",Improved Robustness to Open Set Inputs via Tempered Mixup,1.0,"R Roady, TL Hayes, C Kanan",2020.0,European Conference on Computer Vision,Springer,https://link.springer.com/chapter/10.1007/978-3-030-66415-2_12,https://scholar.google.com/scholar?cites=17210946141413122281&as_sdt=2005&sciodt=2007&hl=en,17.0,2022-07-13 13:39:40,,10.1007/978-3-030-66415-2_12,,,,,,,1,0.5,0.0,3.0,2.0,"… existing convolutional neural network architectures that improves open set robustness without … to enable it to better separate known classes from potential unknowns [8, 15]. For the latter …",https://arxiv.org/pdf/2009.04659,https://scholar.google.com/scholar?q=related:6SBP_1OQ2e4J:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22unknowns%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'mixup', 'out-of-distribution data']"
Yes,robustness to label noise,Trust Your Model: Iterative Label Improvement and Robust Training by Confidence Based Filtering and Dataset Partitioning,1.0,"Christian Haase-Schuetz, Rainer Stal, H. Hertlein, B. Sick",2020.0,,,,,18.0,2022-07-13 10:09:16,,,,,,,,,1,0.5,0.0,4.0,2.0,"State-of-the-art, high capacity deep neural networks not only require large amounts of labelled training data, they are also highly susceptible to label errors in this data, typically resulting in large efforts and costs and therefore limiting the applicability of deep learning. To alleviate this issue, we propose a novel meta training and labelling scheme that is able to use inexpensive unlabelled data by taking advantage of the generalization power of deep neural networks. We show experimentally that by solely relying on one network architecture and our proposed scheme of iterative training and prediction steps, both label quality and resulting model accuracy can be improved significantly. Our method achieves state-of-the-art results, while being architecture agnostic and therefore broadly applicable. Compared to other methods dealing with erroneous labels, our approach does neither require another network to be trained, nor does it necessarily need an additional, highly accurate reference label set. Instead of removing samples from a labelled set, our technique uses additional sensor data without the need for manual labelling.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['label noise']
Yes,"Spiking Neural Network, Adversarial Robustness",Inherent Adversarial Robustness of Deep Spiking Neural Networks: Effects of Discrete Input Encoding and Non-Linear Activations,18.0,"Saima Sharmin, Nitin Rathi, P. Panda, K. Roy",2020.0,,,,,18.0,2022-07-13 09:26:34,,10.1007/978-3-030-58526-6_24,,,,,,,18,9.0,5.0,4.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'spiking neural network']"
Yes,"Data Augmentation, Robust",Adaptive Data Augmentation to Achieve Noise Robustness and Overcome Data Deficiency for Deep Learning,2.0,"E Kim, J Kim, H Lee, S Kim",2021.0,Applied Sciences,mdpi.com,https://www.mdpi.com/2076-3417/11/12/5586,https://scholar.google.com/scholar?cites=11208542821773225838&as_sdt=2005&sciodt=2007&hl=en,18.0,2022-07-12 13:44:06,,,,,,,,,2,2.0,1.0,4.0,1.0,… Artificial intelligence technologies and robot vision systems are core … accuracy by causing a malfunction that is related to defect and longer working time. To achieve robustness against …,https://www.mdpi.com/2076-3417/11/12/5586/pdf,https://scholar.google.com/scholar?q=related:bi-MnU--jJsJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['data augmentation', 'robust']"
Yes,"Robust, Explainability, Deep Neural Network, Knowledge",A Framework for Explainable Deep Neural Models Using External Knowledge Graphs,0.0,"Zachary A. Daniels, Logan D. Frank, Christopher J. Menart, Michael Raymer, Pascal Hitzler",2020.0,Artificial Intelligence And Machine Learning For Multi-Domain Operations Applications Ii,,,,18.0,2022-07-13 15:07:01,Proceedings Paper,10.1117/12.2558083,0277-786X,,11413.0,,,,0,0.0,0.0,5.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'explainability', 'knowledge', 'robust']"
No,"Noise, Resilience, Explainability","Fuzzy-Based, Noise-Resilient, Explainable Algorithm for Regression",1.0,J. Viaña,2022.0,Lecture Notes in Networks and Systems,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85113412382&origin=inward,19.0,2022-07-12 16:23:42,Conference Paper,10.1007/978-3-030-82099-2_42,2367-3370,https://api.elsevier.com/content/abstract/scopus_id/85113412382,258.0,,461.0,472.0,1,1.0,1.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'noise', 'resilience']"
No,"Robust, Adversarial, Neural Network, Pruning, Interpretability",A Principled Approach to Trustworthy Machine Learning,0.0,YY Yang,2022.0,,escholarship.org,https://escholarship.org/uc/item/3x23g3c4,,19.0,2022-07-13 11:13:23,,,,,,,,,0,0.0,0.0,1.0,1.0,"… highest accuracy under robustness constraints. Finally, we connect robustness and interpretability … Finally, we connect robustness and interpretability on decision trees by designing an …",https://escholarship.org/content/qt3x23g3c4/qt3x23g3c4.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'interpretability', 'neural network', 'pruning', 'robust']"
No,"Robust, Noise, Accuracy",Improving Robustness Without Sacrificing Accuracy with Patch Gaussian Augmentation,110.0,"Raphael Gontijo Lopes, Dong Yin, Ben Poole, J. Gilmer, E. D. Cubuk",2019.0,,,,,19.0,2022-07-13 09:24:12,,,,,,,,,110,37.07.00,22.0,5.0,3.0,"Deploying machine learning systems in the real world requires both high accuracy on clean data and robustness to naturally occurring corruptions. While architectural advances have led to improved accuracy, building robust models remains challenging. Prior work has argued that there is an inherent trade-off between robustness and accuracy, which is exemplified by standard data augment techniques such as Cutout, which improves clean accuracy but not robustness, and additive Gaussian noise, which improves robustness but hurts accuracy. To overcome this trade-off, we introduce Patch Gaussian, a simple augmentation scheme that adds noise to randomly selected patches in an input image. Models trained with Patch Gaussian achieve state of the art on the CIFAR-10 and ImageNetCommon Corruptions benchmarks while also improving accuracy on clean data. We find that this augmentation leads to reduced sensitivity to high frequency noise(similar to Gaussian) while retaining the ability to take advantage of relevant high frequency information in the image (similar to Cutout). Finally, we show that Patch Gaussian can be used in conjunction with other regularization methods and data augmentation policies such as AutoAugment, and improves performance on the COCO object detection benchmark.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'noise', 'robust']"
Yes,"Robust, Neural Architecture, Training",Improving the Robustness of Deep Neural Networks via Stability Training,0.0,"Stephan Zheng, Yang Song, Thomas Leung, Ian Goodfellow, IEEE",2016.0,2016 Ieee Conference On Computer Vision And Pattern Recognition (Cvpr),,,,19.0,2022-07-13 16:02:14,Proceedings Paper,10.1109/CVPR.2016.485,1063-6919,,,,4480.0,4488.0,0,0.0,0.0,5.0,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural architecture', 'robust', 'training']"
Yes,"LIME, Explainability, Generative Adversarial Network",Improving lime robustness with smarter locality sampling,8.0,"S Saito, E Chua, N Capel, R Hu",2020.0,arXiv preprint arXiv:2006.12302,arxiv.org,https://arxiv.org/abs/2006.12302,https://scholar.google.com/scholar?cites=7003023579302372208&as_sdt=2005&sciodt=2007&hl=en,20.0,2022-07-13 09:27:33,,,,,,,,,8,4.0,2.0,4.0,2.0,"… machine learning has shown that even these explainability … We measure the robustness of an explainability algorithm … In other words, this represents how well the explainability …",https://arxiv.org/pdf/2006.12302,https://scholar.google.com/scholar?q=related:cKMAfte8L2EJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explainability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'generative adversarial networks', 'lime']"
No,"Explainability, Robust, Adversarial Perturbations",Machine Learning Explainability and Robustness: Connected at the Hip,0.0,"Anupam Datta, Matt Fredrikson, Klas Leino, Kaiji Lu, Shayak Sen, Zifan Wang, ASSOC COMP MACHINERY",2021.0,Kdd '21: Proceedings Of The 27th Acm Sigkdd Conference On Knowledge Discovery & Data Mining,,,,20.0,2022-07-13 12:46:02,Proceedings Paper,10.1145/3447548.3470806,,,,,4035.0,4036.0,0,0.0,0.0,7.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'explainability', 'robust']"
Yes,"Robust, Adversarial",Exploiting joint robustness to adversarial perturbations,16.0,"A Dabouei, S Soleymani…",2020.0,Proceedings of the …,openaccess.thecvf.com,http://openaccess.thecvf.com/content_CVPR_2020/html/Dabouei_Exploiting_Joint_Robustness_to_Adversarial_Perturbations_CVPR_2020_paper.html,https://scholar.google.com/scholar?cites=15434727618344741324&as_sdt=2005&sciodt=2007&hl=en,21.0,2022-07-12 11:56:54,,,,,,,,,16,8.0,5.0,3.0,2.0,"… Considering ℓp-norm as the distance metric to measure the magnitude of perturbations, we define the robustness to adversarial examples or, more specifically, adversarial perturbations …",https://openaccess.thecvf.com/content_CVPR_2020/papers/Dabouei_Exploiting_Joint_Robustness_to_Adversarial_Perturbations_CVPR_2020_paper.pdf,https://scholar.google.com/scholar?q=related:zGlZAP4qM9YJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
No,"Feature Selection, Interpretability, Robust",Interpretable robust feature selection via joint-norms minimization,2.0,"J Lu, S Yi, J Zhao, Y Liang, W Liu",2021.0,2021 13th International Conference …,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3457682.3457693,https://scholar.google.com/scholar?cites=10451966454098965635&as_sdt=2005&sciodt=2007&hl=en,21.0,2022-07-13 17:59:05,,10.1145/3457682.3457693,,,,,,,2,2.0,0.0,5.0,1.0,"… This requires the proposed model to reasonably explain the importance of features and be robust to noise. In order to solve this problem, this paper proposes an interpretable robust …",,https://scholar.google.com/scholar?q=related:gzTe9BzYDJEJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22interpretable%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['features', 'interpretability', 'robust']"
Yes,"Transformer Network, NLP, Adversarial Attacks",Is BERT Really Robust? A Strong Baseline for Natural Language Attack on Text Classification and Entailment,0.0,"Di Jin, Zhijing Jin, Joey Tianyi Zhou, Peter Szolovits, Assoc Advancement Artificial Intelligence",2020.0,"Thirty-Fourth Aaai Conference On Artificial Intelligence, The Thirty-Second Innovative Applications Of Artificial Intelligence Conference And The Tenth Aaai Symposium On Educational Advances In Artificial Intelligence",,,,22.0,2022-07-13 13:06:07,Proceedings Paper,,2159-5399,,34.0,,8018.0,8025.0,0,0.0,0.0,5.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'nlp', 'transformer network']"
Yes,"Robust, Classification, Convolutional Neural Network",A Robust System for Noisy Image Classification Combining Denoising Autoencoder and Convolutional Neural Network,17.0,"S. Roy, Sk. Imran Hossain, M. Akhand, K. Murase",2018.0,,,,,22.0,2022-07-13 10:10:34,,10.14569/IJACSA.2018.090131,,,,,,,17,4.25,4.0,4.0,4.0,"Image classification, a complex perceptual task with many real life important applications, faces a major challenge in presence of noise. Noise degrades the performance of the classifiers and makes them less suitable in real life scenarios. To solve this issue, several researches have been conducted utilizing denoising autoencoder (DAE) to restore original images from noisy images and then Convolutional Neural Network (CNN) is used for classification. The existing models perform well only when the noise level present in the training set and test set are same or differs only a little. To fit a model in real life applications, it should be independent to level of noise. The aim of this study is to develop a robust image classification system which performs well at regular to massive noise levels. The proposed method first trains a DAE with low-level noise-injected images and a CNN with noiseless native images independently. Then it arranges these two trained models in three different combinational structures: CNN, DAE-CNN, and DAE-DAE-CNN to classify images corrupted with zero, regular and massive noises, accordingly. Final system outcome is chosen by applying the winner-takes-all combination on individual outcomes of the three structures. Although proposed system consists of three DAEs and three CNNs in different structure layers, the DAEs and CNNs are the copy of same DAE and CNN trained initially which makes it computationally efficient as well. In DAE-DAE-CNN, two identical DAEs are arranged in a cascaded structure to make the structure well suited for classifying massive noisy data while the DAE is trained with low noisy image data. The proposed method is tested with MNIST handwritten numeral dataset with different noise levels. Experimental results revealed the effectiveness of the proposed method showing better results than individual structures as well as the other related methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'convolutional neural network', 'robust']"
No,"Robust, Design, Neural Network",A new robust design method using neural network,10.0,"S Shin, TT Hoang, TH Le…",2016.0,Journal of Nanoelectronics …,ingentaconnect.com,https://www.ingentaconnect.com/contentone/asp/jno/2016/00000011/00000001/art00013,https://scholar.google.com/scholar?cites=1356035343012064571&as_sdt=2005&sciodt=2007&hl=en,23.0,2022-07-14 12:19:18,,,,,,,,,10,2.07,3.0,4.0,6.0,"… design of experiments (DoE) step, information about the relationship between input variables and output responses is exploited to obtain the final solutions for the robust design … robust …",https://www.researchgate.net/profile/Tuan-Ho-Le-2/publication/283697186_A_New_Robust_Design_Method_Using_Neural_Network/links/5707544e08aed73c8548ac4c/A-New-Robust-Design-Method-Using-Neural-Network.pdf,https://scholar.google.com/scholar?q=related:O-0bu_Wa0RIJ:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['design', 'neural network', 'robust']"
No,"adversarial robustness, evaluation of explanations",Double Backpropagation with Applications to Robustness and Saliency Map Interpretability,3.0,C Etmann,2019.0,,core.ac.uk,https://core.ac.uk/download/pdf/322818060.pdf,https://scholar.google.com/scholar?cites=9392274173339511078&as_sdt=2005&sciodt=2007&hl=en,23.0,2022-07-13 10:52:14,PDF,,,,,,,,3,1.0,3.0,1.0,3.0,… Such an increase in adversarial robustness has been shown … While ’classical’ methods from machine learning have been … The area of machine learning we will focus on is predictive in …,https://core.ac.uk/download/pdf/322818060.pdf,https://scholar.google.com/scholar?q=related:Jin6JpkPWIIJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22interpretability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'evaluation', 'explainability']"
Yes,"Neural Machine Translation, Training, Robust",AdvAug: Robust Adversarial Augmentation for Neural Machine Translation,50.0,"Yong Cheng, Lu Jiang, Wolfgang Macherey, Jacob Eisenstein",2020.0,,,,,23.0,2022-07-13 09:38:44,,10.18653/v1/2020.acl-main.529,,,,,,,50,25.0,13.0,4.0,2.0,"In this paper, we propose a new adversarial augmentation method for Neural Machine Translation (NMT). The main idea is to minimize the vicinal risk over virtual sentences sampled from two vicinity distributions, in which the crucial one is a novel vicinity distribution for adversarial sentences that describes a smooth interpolated embedding space centered around observed training sentence pairs. We then discuss our approach, AdvAug, to train NMT models using the embeddings of virtual sentences in sequence-to-sequence learning. Experiments on Chinese-English, English-French, and English-German translation benchmarks show that AdvAug achieves significant improvements over theTransformer (up to 4.9 BLEU points), and substantially outperforms other data augmentation techniques (e.g.back-translation) without using extra corpora.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['nlp', 'robust', 'training']"
Yes,"Training, Adversarial Attacks, Smoothing",SPROUT: Self-Progressing Robust Training,0.0,"Minhao Cheng, Pin-Yu Chen, Sijia Liu, Shiyu Chang, Cho-Jui Hsieh, Payel Das",2019.0,,,,,23.0,2022-07-13 09:25:03,,,,,,,,,0,0.0,0.0,6.0,3.0,"Enhancing model robustness under new and even adversarial environments is a crucial milestone toward building trustworthy and reliable machine learning systems. Current robust training methods such as adversarial training explicitly specify an “attack” (e.g., `∞-norm bounded perturbation) to generate adversarial examples during model training in order to improve adversarial robustness. In this paper, we take a different perspective and propose a new framework SPROUT, self-progressing robust training. During model training, SPROUT progressively adjusts training label distribution via our proposed parametrized label smoothing technique, making training free of attack generation and more scalable. We also motivate SPROUT using a general formulation based on vicinity risk minimization, which includes many robust training methods as special cases. Compared with stateof-the-art adversarial training methods (PGD-`∞ and TRADES) under `∞-norm bounded attacks and various invariance tests, SPROUT consistently attains superior performance and is more scalable to large neural networks. Our results shed new light on scalable, effective and attack-independent robust training methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'smoothing', 'training']"
No,"accuracy, human-in-the-loop, active learning",Designing a Human-in-the-Loop System for Object Detection in Floor Plans,0.0,"J Jakubik, P Hemmer, M Vössing, B Blumenstiel…",2022.0,… of Artificial Intelligence …,aaai.org,https://www.aaai.org/AAAI22Papers/IAAI-00068-JakubikJ.pdf,,23.0,2022-07-13 09:24:19,PDF,,,,,,,,0,0.0,0.0,5.0,1.0,"… (AEC) industry have started exploring how artificial intelligence (AI) can reduce time-consuming … In this paper, we therefore propose a human-in-the-loop approach for the detection and …",https://www.aaai.org/AAAI22Papers/IAAI-00068-JakubikJ.pdf,https://scholar.google.com/scholar?q=related:pl1PLvrDbloJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22human+in+the+loop%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'active learning', 'hci']"
Discussion,evaluation of explanations,Towards Quantification of Explainability in Explainable Artificial Intelligence Methods,11.0,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor",2019.0,,,,,23.0,2022-07-13 09:19:02,,,,,,,,,11,"0,1715277778",4.0,3.0,3.0,"Artificial Intelligence (AI) has become an integral part of domains such as security, finance, healthcare, medicine, and criminal justice. Explaining the decisions of AI systems in human terms is a key challenge--due to the high complexity of the model, as well as the potential implications on human interests, rights, and lives . While Explainable AI is an emerging field of research, there is no consensus on the definition, quantification, and formalization of explainability. In fact, the quantification of explainability is an open challenge. In our previous work, we incorporated domain knowledge for better explainability, however, we were unable to quantify the extent of explainability. In this work, we (1) briefly analyze the definitions of explainability from the perspective of different disciplines (e.g., psychology, social science), properties of explanation, explanation methods, and human-friendly explanations; and (2) propose and formulate an approach to quantify the extent of explainability. Our experimental result suggests a reasonable and model-agnostic way to quantify explainability",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'explainability']"
Yes,"Explainability, Adversarial Attacks, NLP",Robustness of Explanation Methods for NLP Models,0.0,"S Atmakuri, T Chheda, D Kandula, N Yadav…",2022.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2206.12284,,23.0,2022-07-12 11:49:54,,,,,,,,,0,0.0,0.0,5.0,1.0,"… In this paper, we particularly aim to understand the robustness of explanation methods in … the adversarial robustness of an explanation method. Our experiments show the explanation …",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'explainability', 'nlp']"
Yes,"NLP, neural networks, explainability",Trusting deep learning natural-language models via local and global explanations,0.0,"F. Ventura, Salvatore Greco, D. Apiletti, T. Cerquitelli",2022.0,,,,,23.0,2022-07-13 09:31:23,,10.1007/s10115-022-01690-9,,,,,,,0,0.0,0.0,4.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'neural networks', 'nlp']"
Yes,"Metric, Neural Network, Robust",Measuring neural net robustness with constraints,170.0,O. Bastani,2016.0,Advances in Neural Information Processing Systems,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85018916971&origin=inward,24.0,2022-07-12 16:28:25,Conference Paper,,1049-5258,https://api.elsevier.com/content/abstract/scopus_id/85018916971,,,2621.0,2629.0,170,28.33.00,170.0,1.0,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['metric', 'neural network', 'robust']"
No,"adversarial robustness, explainability",Unifying Model Explainability and Robustness via Machine-Checkable Concepts,2.0,"Vedant Nanda, Till Speicher, John P. Dickerson, K. Gummadi, M. B. Zafar",2020.0,,,,,24.0,2022-07-13 09:26:01,,,,,,,,,2,1.0,0.0,5.0,2.0,"As deep neural networks (DNNs) get adopted in an ever-increasing number of applications, explainability has emerged as a crucial desideratum for these models. In many real-world tasks, one of the principal reasons for requiring explainability is to in turn assess prediction robustness, where predictions (i.e., class labels) that do not conform to their respective explanations (e.g., presence or absence of a concept in the input) are deemed to be unreliable. However, most, if not all, prior methods for checking explanation-conformity (e.g., LIME, TCAV, saliency maps) require significant manual intervention, which hinders their large-scale deployability. In this paper, we propose a robustness-assessment framework, at the core of which is the idea of using machine-checkable concepts. Our framework defines a large number of concepts that the DNN explanations could be based on and performs the explanation-conformity check at test time to assess prediction robustness. Both steps are executed in an automated manner without requiring any human intervention and are easily scaled to datasets with a very large number of classes. Experiments on real-world datasets and human surveys show that our framework is able to enhance prediction robustness significantly: the predictions marked to be robust by our framework have significantly higher accuracy and are more robust to adversarial perturbations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'explainability']"
Yes,"Visual, Adversarial, Robust, Accuracy",Active dropblock: Method to enhance deep model accuracy and robustness,2.0,"J Yao, W Xing, D Wang, J Xing, L Wang",2021.0,Neurocomputing,Elsevier,https://www.sciencedirect.com/science/article/pii/S0925231221006597,https://scholar.google.com/scholar?cites=7367894921480402119&as_sdt=2005&sciodt=2007&hl=en,25.0,2022-07-12 13:44:06,HTML,,,,,,,,2,2.0,0.0,5.0,1.0,… the robustness of deep network training on visual recognition tasks without sacrificing accuracy. The … interests mainly include intelligent information processing and artificial intelligence. …,https://www.sciencedirect.com/science/article/pii/S0925231221006597,https://scholar.google.com/scholar?q=related:x3Q-8WQFQGYJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial', 'robust', 'visual']"
Yes,"Quantization, Neural Network, Adversarial Attacks",Improving Adversarial Robustness in Weight-quantized Neural Networks,5.0,"Chang Song, E. Fallon, Hai Helen Li",2020.0,,,,,25.0,2022-07-13 09:26:34,,,,,,,,,5,2.5,2.0,3.0,2.0,"Neural networks are getting deeper and more computationintensive nowadays. Quantization is a useful technique in deploying neural networks on hardware platforms and saving computation costs with negligible performance loss. However, recent research reveals that neural network models, no matter full-precision or quantized, are vulnerable to adversarial attacks. In this work, we analyze both adversarial and quantization losses and then introduce criteria to evaluate them. We propose a boundary-based retraining method to mitigate adversarial and quantization losses together and adopt a nonlinear mapping method to defend against white-box gradient-based adversarial attacks. The evaluations demonstrate that our method can better restore accuracy after quantization than other baseline methods on both black-box and white-box adversarial attacks. The results also show that adversarial training suffers quantization loss and does not cooperate well with other training methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'neural network', 'quantization']"
Yes,"Training, Adversarial, Robust, Classification",Adversarially Robust Learning via Entropic Regularization,4.0,"Gauri Jagatap, Animesh Basak Chowdhury, S. Garg, C. Hegde",2020.0,,,,,26.0,2022-07-13 09:32:18,,10.3389/frai.2021.780843,,,,,,,4,2.0,1.0,4.0,2.0,"In this paper we propose a new family of algorithms, ATENT, for training adversarially robust deep neural networks. We formulate a new loss function that is equipped with an additional entropic regularization. Our loss function considers the contribution of adversarial samples that are drawn from a specially designed distribution in the data space that assigns high probability to points with high loss and in the immediate neighborhood of training samples. Our proposed algorithms optimize this loss to seek adversarially robust valleys of the loss landscape. Our approach achieves competitive (or better) performance in terms of robust classification accuracy as compared to several state-of-the-art robust learning approaches on benchmark datasets such as MNIST and CIFAR-10.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'classification', 'robust', 'training']"
Yes,"Dataset, Robust, Interpretability, Trustworthiness",DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation,0.0,"L Wang, H Liu, S Peng, H Tang, X Xiao…",2021.0,arXiv e …,ui.adsabs.harvard.edu,https://ui.adsabs.harvard.edu/abs/2021arXiv210813140W/abstract,,26.0,2022-07-12 13:46:44,,,,,,,,,0,0.0,0.0,6.0,1.0,"… the performance of most artificial intelligence tasks, they are … dataset to evaluate robustness and interpretability. To evaluate … further proposed for interpretability and robustness. Based …",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['dataset', 'interpretability', 'robust', 'trustworthy']"
Yes,"adversarial, robust, classifier",Formal guarantees on the robustness of a classifier against adversarial Manipulation,160.0,M. Hein,2017.0,Advances in Neural Information Processing Systems,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85047021059&origin=inward,26.0,2022-07-12 16:28:25,Conference Paper,,1049-5258,https://api.elsevier.com/content/abstract/scopus_id/85047021059,2017.0,,2267.0,2277.0,160,32.0,160.0,1.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'classifier', 'robust']"
No,explainability,Designing an Interpretability-Based Model to Explain the Artificial Intelligence Algorithms in Healthcare,0.0,"M Ennab, H Mcheick",2022.0,Diagnostics,mdpi.com,https://www.mdpi.com/2075-4418/12/7/1557,,26.0,2022-07-14 08:54:02,,,,,,,,,0,0.0,0.0,2.0,1.0,"… on a set of statistical rules and assumptions, interpretability is critical because it is the … Furthermore, model interpretability is a critical means of verifying that the assumptions are robust …",https://www.mdpi.com/2075-4418/12/7/1557/pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['explainability']
Discussion ,"adversarial robustness, mitigation method",Directional Adversarial Training for Robust Ownership-Based Recommendation System,0.0,"Zhefu Wu, Agyemang Paul, Junzhuo Cao, Luping Fang",2022.0,,,,,27.0,2022-07-13 09:40:06,,10.1109/access.2022.3140352,,,,,,,0,0.0,0.0,4.0,1.0,"Machine learning algorithms are susceptible to cyberattacks, posing security problems in computer vision, speech recognition, and recommendation systems. So far, researchers have made great strides in adopting adversarial training as a defensive strategy. Single-step adversarial training methods have been proposed as viable solutions for improving model generality and resilience. However, there has been little study to address this issue in the context of ownership-based recommendations, which may fail to capture stable results. In this work, we adapt the single-step adversarial training for ownership recommendation systems. Our main technical contributions are as follows: (1) We propose Adversarial Consumption and Production Relationship (ACPR), a model that combines factorization machine and single-step adversarial training for ownership recommendations. It enables us to take advantage of modeling consumption-production interactions with a factorization machine instead of the conventional matrix factorization method for ownership recommendations. (2) We enrich the ACPR technique with directional adversarial training and call our technique Adversarial Consumption and Production Relationship-Aware Directional Adversarial Model (ACPR-ADAM). The idea behind our ACPR-ADAM is that instead of the worst perturbation direction, the perturbation direction in the embedding space is restricted to other examples in the current embedding space, allowing us to incorporate the collaborative signal into the training process. Lastly, through extensive evaluations on Reddit and Pinterest, we demonstrate that our proposed method outperforms state-of-the-art methods. Compared with CPR and ACPR on Reddit and Pinterest datasets, our proposed ACPR-ADAM achieves 93%, 88%, and 72%, 69% enhancement in terms of AUC and HR, respectively.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'mitigation method']"
No,"Feature Selection, Explainability, K-means",Robust multi-view feature selection,22.0,H. Liu,2017.0,"Proceedings - IEEE International Conference on Data Mining, ICDM",,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85014525414&origin=inward,27.0,2022-07-12 16:32:32,Conference Paper,10.1109/ICDM.2016.37,1550-4786,https://api.elsevier.com/content/abstract/scopus_id/85014525414,,,281.0,290.0,22,4.4,22.0,1.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['clustering', 'explainability', 'features']"
Yes,"Privacy, adversarial attack",Robust unlearnable examples: Protecting data privacy against adversarial learning,2.0,"S Fu, F He, Y Liu, L Shen, D Tao",2021.0,International Conference on …,openreview.net,https://openreview.net/forum?id=baUQQPwQiAg,https://scholar.google.com/scholar?cites=1608441887966191980&as_sdt=2005&sciodt=2007&hl=en,27.0,2022-07-13 17:19:08,,,,,,,,,2,2.0,0.0,5.0,1.0,"… unlearnability is found fragile to adversarial training. In this paper, we design new methods to generate robust unlearnable examples that are protected from adversarial training. We first …",https://openreview.net/pdf?id=baUQQPwQiAg,https://scholar.google.com/scholar?q=related:bGEVDmBVUhYJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'privacy']"
Yes,"Adversarial, Robust, Visual, Classifier",Contextual Fusion For Adversarial Robustness,0.0,"Aiswarya Akumalla, S. Haney, M. Bazhenov",2020.0,,,,,28.0,2022-07-13 09:27:16,,,,,,,,,0,0.0,0.0,3.0,2.0,"Mammalian brains handle complex reasoning tasks in a gestalt manner by integrating information from regions of the brain that are specialised to individual sensory modalities. This allows for improved robustness and better generalisation ability. In contrast, deep neural networks are usually designed to process one particular information stream and susceptible to various types of adversarial perturbations. While many methods exist for detecting and defending against adversarial attacks, they do not generalise across a range of attacks and negatively affect performance on clean, unperturbed data. We developed a fusion model using a combination of background and foreground features extracted in parallel from Places-CNN and Imagenet-CNN. We tested the benefits of the fusion approach on preserving adversarial robustness for human perceivable (e.g., Gaussian blur) and network perceivable (e.g., gradient-based) attacks for CIFAR-10 and MS COCO data sets. For gradient based attacks, our results show that fusion allows for significant improvements in classification without decreasing performance on unperturbed data and without need to perform adversarial retraining. Our fused model revealed improvements for Gaussian blur type perturbations as well. The increase in performance from fusion approach depended on the variability of the image contexts; larger increases were seen for classes of images with larger differences in their contexts. We also demonstrate the effect of regularization to bias the classifier decision in the presence of a known adversary. We propose that this biologically inspired approach to integrate information across multiple modalities provides a new way to improve adversarial robustness that can be complementary to current state of the art approaches.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'classifier', 'robust', 'visual']"
Yes,"Computer Vision, Noise, Deep Learning",Robustness of deep convolutional neural networks for image degradations,37.0,"S Ghosh, R Shet, P Amon, A Hutter…",2018.0,2018 IEEE International …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8461907/?casa_token=SHXG1vtgfj0AAAAA:KTAp9imfP3sSQsW27GGRiUTVDBZ3PAnpFKg2pBgWYtbJG5Musxb76RMejOPM8dkDet1ugDgjXA,https://scholar.google.com/scholar?cites=3718479366662679447&as_sdt=2005&sciodt=2007&hl=en,28.0,2022-07-13 14:12:47,,,,,,,,,37,9.25,7.0,5.0,4.0,"… robustness of CNNs [6], [7]. Zheng et al. [8] propose a method for increasing the robustness of deep neural networks by stability … Hence, a neural network whose weights adapt to various …",https://ieeexplore.ieee.org/iel7/8450881/8461260/08461907.pdf?casa_token=ze9zppk3yYAAAAAA:picFiA7lALwrvnyCIluZTPmY1nWN9rkkVpe_GPvVgYG4jJXMiSCwA974BISMXVyPaHM2WfwWSA,https://scholar.google.com/scholar?q=related:l2uMWkixmjMJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22stability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'deep learning', 'noise']"
Yes,"Interpretability, trustworthy ML",On the benefits of attributional robustness,6.0,"M Singh, N Kumari, P Mangla, A Sinha…",2019.0,arXiv preprint arXiv …,researchgate.net,https://www.researchgate.net/profile/Mayank-Singh-30/publication/337671257_On_the_Benefits_of_Attributional_Robustness/links/5e4e2fa3299bf1cdb938dc71/On-the-Benefits-of-Attributional-Robustness.pdf,https://scholar.google.com/scholar?cites=3819856751012266670&as_sdt=2005&sciodt=2007&hl=en,28.0,2022-07-13 09:30:22,PDF,,,,,,,,6,2.0,1.0,5.0,3.0,… Application of machine learning algorithms in various safety-… Robustness and interpretability of machine learning methods … robustness with respect to integrated gradients explanation. …,https://www.researchgate.net/profile/Mayank-Singh-30/publication/337671257_On_the_Benefits_of_Attributional_Robustness/links/5e4e2fa3299bf1cdb938dc71/On-the-Benefits-of-Attributional-Robustness.pdf,https://scholar.google.com/scholar?q=related:rsL2AHrbAjUJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['interpretability', 'trustworthy']"
Yes,"Fairness, optimization, noisy data",Robust optimization for fairness with noisy protected groups,57.0,"S Wang, W Guo, H Narasimhan…",2020.0,Advances in …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2020/hash/37d097caf1299d9aa79c2c2b843d2d78-Abstract.html,https://scholar.google.com/scholar?cites=5111841011798470081&as_sdt=2005&sciodt=2007&hl=en,29.0,2022-07-14 10:15:05,,,,,,,,,57,28.5,14.0,4.0,2.0,"… Many existing fairness criteria for machine learning involve … Second, we introduce two new approaches using robust … Using two case studies, we show empirically that the robust …",https://proceedings.neurips.cc/paper/2020/file/37d097caf1299d9aa79c2c2b843d2d78-Paper.pdf,https://scholar.google.com/scholar?q=related:wW2FCFHo8EYJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22transparency%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['fairness', 'noise', 'optimization']"
Yes,"robustness, global, framework",Global Robustness Verification Networks,0.0,"Weidi Sun, Yuteng Lu, Xiyue Zhang, Zhanxing Zhu, Meng Sun",2020.0,,,,,29.0,2022-07-13 09:27:16,,,,,,,,,0,0.0,0.0,5.0,2.0,"The wide deployment of deep neural networks, though achieving great success in many domains, has severe safety and reliability concerns. Existing adversarial attack generation and automatic verification techniques cannot formally verify whether a network is globally robust, i.e., the absence or not of adversarial examples in the input space. To address this problem, we develop a global robustness verification framework with three components: 1) a novel rule-based ``back-propagation'' finding which input region is responsible for the class assignment by logic reasoning; 2) a new network architecture Sliding Door Network (SDN) enabling feasible rule-based ``back-propagation''; 3) a region-based global robustness verification (RGRV) approach. Moreover, we demonstrate the effectiveness of our approach on both synthetic and real datasets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['framework', 'global', 'robust']"
Yes,Robustness to label noise,On the resistance of neural nets to label noise,14.0,"A Drory, S Avidan, R Giryes",2018.0,arXiv preprint arXiv:1803.11410,researchgate.net,https://www.researchgate.net/profile/Amnon-Drory/publication/324150989_On_the_Resistance_of_Neural_Nets_to_Label_Noise/links/5ad6f66daca272fdaf7e4fb4/On-the-Resistance-of-Neural-Nets-to-Label-Noise.pdf,https://scholar.google.com/scholar?cites=992748699251194081&as_sdt=2005&sciodt=2007&hl=en,29.0,2022-07-13 12:21:07,PDF,,,,,,,,14,3.5,5.0,3.0,4.0,"… for the expected accuracy of a neural network, which is in fact the … In this work, we have studied the robustness of neural … Our conclusion is that CNN robustness to label noise depends …",https://www.researchgate.net/profile/Amnon-Drory/publication/324150989_On_the_Resistance_of_Neural_Nets_to_Label_Noise/links/5ad6f66daca272fdaf7e4fb4/On-the-Resistance-of-Neural-Nets-to-Label-Noise.pdf,https://scholar.google.com/scholar?q=related:4TgZoq7zxg0J:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22human+computation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['label noise']
Yes,"Adversarial, Robust",Image Transformation can make Neural Networks more robust against Adversarial Examples,8.0,"D. D. Thang, Toshihiro Matsui",2019.0,,,,,29.0,2022-07-13 10:08:07,,,,,,,,,8,3.07,4.0,2.0,3.0,"Neural networks are being applied in many tasks related to IoT with encouraging results. For example, neural networks can precisely detect human, objects and animal via surveillance camera for security purpose. However, neural networks have been recently found vulnerable to well-designed input samples that called adversarial examples. Such issue causes neural networks to misclassify adversarial examples that are imperceptible to humans. We found giving a rotation to an adversarial example image can defeat the effect of adversarial examples. Using MNIST number images as the original images, we first generated adversarial examples to neural network recognizer, which was completely fooled by the forged examples. Then we rotated the adversarial image and gave them to the recognizer to find the recognizer to regain the correct recognition. Thus, we empirically confirmed rotation to images can protect pattern recognizer based on neural networks from adversarial example attacks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
Yes,Robustness evaluation framework,Robustart: Benchmarking robustness on architecture design and training techniques,16.0,"S Tang, R Gong, Y Wang, A Liu, J Wang…",2021.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2109.05211,https://scholar.google.com/scholar?cites=8485046656923809530&as_sdt=2005&sciodt=2007&hl=en,29.0,2022-07-12 11:55:13,,,,,,,,,16,16.0,3.0,6.0,1.0,"… 4.1 Architecture design towards robustness In this section, we first study the influence of architecture design on robustness. … In Thirty-first AAAI conference on artificial intelligence, 2017. …",https://arxiv.org/pdf/2109.05211,https://scholar.google.com/scholar?q=related:-jaPkPrwwHUJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'framework']"
No,"Robust, Deep Neural Network, Speech Recognition",A STUDY ON DATA AUGMENTATION OF REVERBERANT SPEECH FOR ROBUST SPEECH RECOGNITION,0.0,"Tom Ko, Vijayaditya Peddinti, Daniel Povey, Michael L. Seltzer, Sanjeev Khudanpur, IEEE",2017.0,"2017 Ieee International Conference On Acoustics, Speech And Signal Processing (Icassp)",,,,30.0,2022-07-13 16:32:21,Proceedings Paper,,1520-6149,,,,5220.0,5224.0,0,0.0,0.0,6.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'robust', 'time series']"
Yes,"Speech Recognition, Invariant Representation, Robust",Learning noise-invariant representations for robust speech recognition,46.0,"D Liang, Z Huang, ZC Lipton",2018.0,2018 IEEE Spoken Language …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8639575/?casa_token=vEc2TH6-rD8AAAAA:IdMYLNMW60GSmONXuEPhDhFEW_bkdLXS7HQ3VUg_ltNwiUOfbuZBjTASHuYgWdHvFuTpKj2PDw,https://scholar.google.com/scholar?cites=14506006666209071802&as_sdt=2005&sciodt=2007&hl=en,30.0,2022-07-14 11:17:40,,,,,,,,,46,11.5,15.0,3.0,4.0,"… task, produce models that are robust to out-of-domain noise, and improve convergence speed. … methods are equally applicable to other machine learning fields, notably computer vision. …",https://ieeexplore.ieee.org/iel7/8632666/8639030/08639575.pdf?casa_token=FQ4nEEkMsLMAAAAA:zB1GInGKfO_YM2pQVOA7zoW-DPHRowhfDjfhC6CgcdU5PahV941KseT0aBv1oVbryHJl1lLtmA,https://scholar.google.com/scholar?q=related:uhLX8DSwT8kJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['invariant representation', 'robust', 'time series']"
Yes,"assessment, robust, adversarial",Evolving robust neural architectures to defend from adversarial attacks,4.0,"S Kotyan, DV Vargas",2019.0,arXiv preprint arXiv:1906.11667,arxiv.org,https://arxiv.org/abs/1906.11667,https://scholar.google.com/scholar?cites=15908670418167875491&as_sdt=2005&sciodt=2007&hl=en,30.0,2022-07-14 12:23:15,,,,,,,,,4,1.33,2.0,2.0,3.0,"… 4% respectively when the fitness is only based on the neural network’s testing accuracy. However, when the accuracy on adversarial samples is included in the evaluation function, the …",https://arxiv.org/pdf/1906.11667,https://scholar.google.com/scholar?q=related:o0epI3XzxtwJ:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'evaluation', 'robust']"
No,"Spiking Neural Network, Robust",Improving Robustness of ReRAM-based Spiking Neural Network Accelerator with Stochastic Spike-timing-dependent-plasticity,6.0,"Xueyuan She, Yun Long, S. Mukhopadhyay",2019.0,,,,,30.0,2022-07-13 09:19:17,,10.1109/IJCNN.2019.8851825,,,,,,,6,2.0,2.0,3.0,3.0,"Spike-timing-dependent-plasticity (STDP) is an unsupervised learning algorithm for spiking neural network (SNN), which promises to achieve deeper understanding of human brain and more powerful artificial intelligence. While conventional computing system fails to simulate SNN efficiently, process-inmemory (PIM) based on devices such as ReRAM can be used in designing fast and efficient STDP based SNN accelerators, as it operates in high resemblance with biological neural network. However, the real-life implementation of such design still suffers from impact of input noise and device variation. In this work, we present a novel stochastic STDP algorithm that uses spiking frequency information to dynamically adjust synaptic behavior. The algorithm is tested in pattern recognition task with noisy input and shows accuracy improvement over deterministic STDP. In addition, we show that the new algorithm can be used for designing a robust ReRAM based SNN accelerator that has strong resilience to device variation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['robust', 'spiking neural network']"
Yes,"neural networks, adversarial robustness, mitigation method",Differentiable Abstract Interpretation for Provably Robust Neural Networks,0.0,"Matthew Mirman, Timon Gehr, Martin Vechev",2018.0,"International Conference On Machine Learning, Vol 80",,,,31.0,2022-07-15 10:31:07,Proceedings Paper,,2640-3498,,80.0,,,,0,0.0,0.0,3.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'mitigation method', 'neural networks']"
Yes,"Probabilistic Robustness, Neural Network, Verification",Robustness of neural networks: A probabilistic and practical approach,35.0,"R Mangal, AV Nori, A Orso",2019.0,2019 IEEE/ACM 41st International …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8805674/,https://scholar.google.com/scholar?cites=2761884113933862766&as_sdt=2005&sciodt=2007&hl=en,31.0,2022-07-13 12:24:11,,,,,,,,,35,12.07,12.0,3.0,3.0,"… non-adversarial settings, we are only interested in robustness of a neural network for pairs of δ-close inputs that are likely to be generated in the real-world. Consequently, instead of …",https://arxiv.org/pdf/1902.05983,https://scholar.google.com/scholar?q=related:bofS8fcuVCYJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural network', 'robust', 'verification']"
Yes,"Deep Learning, Neural Architecture, Robust",Fool the COOL - On the Robustness of Deep Learning SAR ATR Systems,0.0,"Simon Wagner, Chandana Panati, Stefan Brüggenwirth",2021.0,,,,,31.0,2022-07-13 09:27:43,,10.1109/RadarConf2147009.2021.9455231,,,,,,,0,0.0,0.0,3.0,1.0,"Over the last years, deep learning automatic target recognition systems have become very popular for synthetic aperture radar images. These systems achieve very high classification rates with common datasets, like the Moving and Stationary Target Acquisition and Recognition (MSTAR) data. A point that is normally not considered is the robustness of these systems, which typically use a softmax layer without rejection class for classification. It has been reported in the past that small variations in the training and test data of deep neural networks might lead to a change in the result. To avoid this situation, several methods to increase the robustness are presented in this paper. These methods vary from simple, like training with noisy samples, to changes in the network structure, particularly the Competitive Overcomplete Output Layer (COOL) is proposed. The COOL gives an output value that also represents a confidence, but with a larger variation than the softmax output. To evaluate the robustness, the DeepFool algorithm is used to creates adversarial examples, i.e. images that look similar to the original data, but cause a wrong classification result. This algorithm is applied to the known training data of the different networks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'neural architecture', 'robust']"
No,"Adversarial Perturbations, Dataset, Generalization",Robustness to adversarial perturbations in learning from incomplete data,80.0,"A Najafi, S Maeda, M Koyama…",2019.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2019/hash/60ad83801910ec976590f69f638e0d6d-Abstract.html,https://scholar.google.com/scholar?cites=10146569317905505663&as_sdt=2005&sciodt=2007&hl=en,31.0,2022-07-12 11:56:54,,,,,,,,,80,27.07.00,20.0,4.0,3.0,"… Robustness to adversarial attacks is an essential feature in the design of modern classifiers —in particular, of deep neural networks [1, 2]. Adversarial … could improve the robustness of a …",https://proceedings.neurips.cc/paper/2019/file/60ad83801910ec976590f69f638e0d6d-Paper.pdf,https://scholar.google.com/scholar?q=related:f52iKgnbz4wJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'dataset', 'generalization']"
Yes,"Adversarial Robustness, Training, Dataset",Improving adversarial robustness using proxy distributions,20.0,"V Sehwag, S Mahloujifar…",2021.0,arXiv preprint …,aisecure-workshop.github.io,https://aisecure-workshop.github.io/aml-iclr2021/papers/39.pdf,https://scholar.google.com/scholar?cites=16612302926147460426&as_sdt=2005&sciodt=2007&hl=en,31.0,2022-07-13 09:47:56,PDF,,,,,,,,20,20.0,7.0,3.0,1.0,"… of training samples, adversarial training can continue to improve the robustness of deep neural networks. Next, we prove the relationship between the robustness achieved on the proxy …",https://aisecure-workshop.github.io/aml-iclr2021/papers/39.pdf,https://scholar.google.com/scholar?q=related:Sp0z3n7BiuYJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'dataset', 'training']"
Yes,"Fine Tuning, Out-of-Distribution Data, Convolutional Neural Network",Improving the Robustness of a Convolutional Neural Network with Out-of-Distribution Data Fine-Tuning and Image Preprocessing,2.0,"S Haque, AW Liu, S Liu, J H. Chan",2021.0,The 12th International Conference …,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3468784.3470655,https://scholar.google.com/scholar?cites=7127411326712702847&as_sdt=2005&sciodt=2007&hl=en,32.0,2022-07-13 14:02:43,,10.1145/3468784.3470655,,,,,,,2,2.0,1.0,4.0,1.0,"… This study explored methods that will increase the robustness of a CNN model trained on … of the confidence scores with perturbations computed with ODIN (ie, not raw confidence) were …",,https://scholar.google.com/scholar?q=related:f2fOyNam6WIJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'fine tuning', 'out-of-distribution data']"
Yes,"Convolutional Neural Network, Noise, Dataset",Noise immunity and robustness study of image recognition using a convolutional neural network,2.0,"V Ziyadinov, M Tereshonok",2022.0,Sensors,mdpi.com,https://www.mdpi.com/1486738,https://scholar.google.com/scholar?cites=11753076321491086579&as_sdt=2005&sciodt=2007&hl=en,32.0,2022-07-13 11:05:20,,,,,,,,,2,2.0,1.0,2.0,1.0,"… robustness … machine learning—the work describes a largely simplified model, thereby summarizing its conclusions for most cases solved by neural networks and other machine learning …",https://www.mdpi.com/1424-8220/22/3/1241/htm,https://scholar.google.com/scholar?q=related:8-yH2KRQG6MJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'dataset', 'noise']"
Yes,"Accuracy, Metric, Evaluation",Interpreting and Evaluating Neural Network Robustness,27.0,"Fuxun Yu, Zhuwei Qin, Chenchen Liu, Liang Zhao, Yanzhi Wang, Xiang Chen",2019.0,,,,,32.0,2022-07-13 09:26:34,,10.24963/ijcai.2019/583,,,,,,,27,9.0,5.0,6.0,3.0,"Recently, adversarial deception becomes one of the most considerable threats to deep neural networks. However, compared to extensive research in new designs of various adversarial attacks and defenses, the neural networks' intrinsic robustness property is still lack of thorough investigation. This work aims to qualitatively interpret the adversarial attack and defense mechanisms through loss visualization, and establish a quantitative metric to evaluate the model's intrinsic robustness. The proposed robustness metric identifies the upper bound of a model's prediction divergence in the given domain and thus indicates whether the model can maintain a stable prediction. With extensive experiments, our metric demonstrates several advantages over conventional testing accuracy based robustness estimation: (1) it provides a uniformed evaluation to models with different structures and parameter scales; (2) it over-performs conventional accuracy based robustness evaluation and provides a more reliable evaluation that is invariant to different test settings; (3) it can be fast generated without considerable testing cost.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'evaluation', 'metric']"
Yes,"Adversarial Robustness, Sensitivity, Perturbations",Interpreting and Improving Adversarial Robustness of Deep Neural Networks With Neuron Sensitivity,19.0,"Chongzhi Zhang, Aishan Liu, Xianglong Liu, Yitao Xu, Hang Yu, Yuqing Ma, Tianlin Li",2019.0,,,,,32.0,2022-07-13 09:26:01,,10.1109/TIP.2020.3042083,,,,,,,19,6.33,3.0,7.0,3.0,"Deep neural networks (DNNs) are vulnerable to adversarial examples where inputs with imperceptible perturbations mislead DNNs to incorrect results. Despite the potential risk they bring, adversarial examples are also valuable for providing insights into the weakness and blind-spots of DNNs. Thus, the interpretability of a DNN in the adversarial setting aims to explain the rationale behind its decision-making process and makes deeper understanding which results in better practical applications. To address this issue, we try to explain adversarial robustness for deep models from a new perspective of neuron sensitivity which is measured by neuron behavior variation intensity against benign and adversarial examples. In this paper, we first draw the close connection between adversarial robustness and neuron sensitivities, as sensitive neurons make the most non-trivial contributions to model predictions in the adversarial setting. Based on that, we further propose to improve adversarial robustness by stabilizing the behaviors of sensitive neurons. Moreover, we demonstrate that state-of-the-art adversarial training methods improve model robustness by reducing neuron sensitivities, which in turn confirms the strong connections between adversarial robustness and neuron sensitivity. Extensive experiments on various datasets demonstrate that our algorithm effectively achieves excellent results. To the best of our knowledge, we are the first to study adversarial robustness using neuron sensitivities.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'perturbations', 'sensitivity']"
Yes,"Adversarial, Benchmark",ImageNet-Patch: A Dataset for Benchmarking Machine Learning Robustness against Adversarial Patches,2.0,"M Pintor, D Angioni, A Sotgiu, L Demetrio…",2022.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2203.04412,https://scholar.google.com/scholar?cites=17017923935017385309&as_sdt=2005&sciodt=2007&hl=en,32.0,2022-07-13 09:47:56,,,,,,,,,2,2.0,0.0,5.0,1.0,"… robustness evaluations. To overcome these issues, we propose ImageNet-Patch, a dataset to benchmark machinelearning models against adversarial … yet faster robustness evaluation, …",https://arxiv.org/pdf/2203.04412,https://scholar.google.com/scholar?q=related:XT1UcafPK-wJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'benchmark']"
No,"Time Series, Explainability, Robust Features",Robust Feature Based Model Agnostic Explainability For Time Series Forecasting,0.0,"V Raykar, S Mukherjee, B Vinzamuri…",2021.0,INFORMS Annual …,research.ibm.com,https://research.ibm.com/publications/robust-feature-based-model-agnostic-explainability-for-time-series-forecasting,,32.0,2022-07-12 11:56:04,,,,,,,,,0,0.0,0.0,4.0,1.0,… Explainability is the degree to which a human can understand the cause of a decision made by a model. Various notions of explainability has … For robustness we aggregate multiple …,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'features', 'time series']"
Yes,"Robust, Invariant Representation, Convolutional Neural Network",Is Robustness To Transformations Driven by Invariant Neural Representations?,3.0,"Syed Suleman Abbas Zaidi, X. Boix, Neeraj Prasad, Sharon Gilad-Gutnick, Shlomit Ben-Ami, P. Sinha",2020.0,,,,,33.0,2022-07-13 09:26:01,,,,,,,,,3,1.5,1.0,6.0,2.0,"Deep Convolutional Neural Networks (DCNNs) have demonstrated impressive robustness to recognize objects under transformations (e.g. blur or noise) when these transformations are included in the training set. A hypothesis to explain such robustness is that DCNNs develop invariant neural representations that remain unaltered when the image is transformed. Yet, to what extent this hypothesis holds true is an outstanding question, as including transformations in the training set could lead to properties different from invariance, e.g. parts of the network could be specialized to recognize either transformed or non-transformed images. In this paper, we analyze the conditions under which invariance emerges. To do so, we leverage that invariant representations facilitate robustness to transformations for object categories that are not seen transformed during training. Our results with state-of-the-art DCNNs indicate that invariant representations strengthen as the number of transformed categories in the training set is increased. This is much more prominent with local transformations such as blurring and high-pass filtering, compared to geometric transformations such as rotation and thinning, that entail changes in the spatial arrangement of the object. Our results contribute to a better understanding of invariant representations in deep learning, and the conditions under which invariance spontaneously emerges.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'invariant representation', 'robust']"
No,"Matrix Factorization, Feature Selection, Robust",Robust Unsupervised Feature Selection,206.0,M. Qian,2013.0,IJCAI International Joint Conference on Artificial Intelligence,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=84896063375&origin=inward,33.0,2022-07-12 16:30:31,Conference Paper,,1045-0823,https://api.elsevier.com/content/abstract/scopus_id/84896063375,,,1621.0,1627.0,206,23.29,206.0,1.0,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['features', 'matrix factorization', 'robust']"
Yes,"adversarial robustness, neural networks, mitigation method",Doshi-VelezF. Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients,2.0,S RossA,2018.0,… of 2018 AAAI Conference on Artificial Intelligence,,,https://scholar.google.com/scholar?cites=2510434800888518971&as_sdt=2005&sciodt=2007&hl=en,33.0,2022-07-12 13:46:44,CITATION,,,,,,,,2,0.5,2.0,1.0,4.0,,,https://scholar.google.com/scholar?q=related:Oxm9iibb1iIJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22interpretability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'mitigation method', 'neural networks']"
No,"Feature Selection, Robust, Classification, Accuracy",A Robust Cost-Sensitive Feature Selection Via Self-Paced Learning Regularization,0.0,"Yangding Li, Chaoqun Ma, Yiling Tao, Zehui Hu, Zidong Su, Meiling Liu",2021.0,Neural Processing Letters,,,,33.0,2022-07-13 11:50:52,Article,10.1007/s11063-021-10479-w,1370-4621,,,,,,0,0.0,0.0,6.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'classification', 'features', 'robust']"
No,"adversarial robustness, fairness, privacy",When data lie: Fairness and robustness in contested environments,1.0,R. Raghavendra,2018.0,Proceedings of SPIE - The International Society for Optical Engineering,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85049672220&origin=inward,34.0,2022-07-12 16:26:18,Conference Paper,10.1117/12.2306978,0277-786X,https://api.elsevier.com/content/abstract/scopus_id/85049672220,10653.0,,,,1,"0,01736111111",1.0,1.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'fairness', 'privacy']"
Discussion,"Face Recognition, Adversarial Attacks, Generative Adversarial Networks",On the robustness of face recognition algorithms against attacks and bias,25.0,R. Singh,2020.0,AAAI 2020 - 34th AAAI Conference on Artificial Intelligence,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85106459865&origin=inward,34.0,2022-07-12 16:24:02,Conference Paper,,,https://api.elsevier.com/content/abstract/scopus_id/85106459865,,,13583.0,13589.0,25,12.5,25.0,1.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'computer vision', 'generative adversarial networks']"
Yes,"adversarial, robust",How Does Noise Help Robustness? Explanation and Exploration under the Neural SDE Framework,0.0,"Xuanqing Liu, Tesi Xiao, Si Si, Qin Cao, Sanjiv Kumar, Cho-Jui Hsieh, IEEE",2020.0,2020 Ieee/Cvf Conference On Computer Vision And Pattern Recognition (Cvpr),,,,34.0,2022-07-13 15:12:02,Proceedings Paper,10.1109/CVPR42600.2020.00036,1063-6919,,,,279.0,287.0,0,0.0,0.0,7.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
Yes,"Randomized Smoothing, Robust",A Framework for robustness Certification of Smoothed Classifiers using F-Divergences,39.0,"Krishnamurthy Dvijotham, Jamie Hayes, Borja Balle, J. Z. Kolter, Chongli Qin, A. György, Kai Y. Xiao, Sven Gowal, Pushmeet Kohli",2020.0,,,,,34.0,2022-07-13 09:25:19,,,,,,,,,39,19.5,4.0,9.0,2.0,"Formal verification techniques that compute provable guarantees on properties of machine learning models, like robustness to norm-bounded adversarial perturbations, have yielded impressive results. Although most techniques developed so far requires knowledge of the architecture of the machine learning model and remains hard to scale to complex prediction pipelines, the method of randomized smoothing has been shown to overcome many of these obstacles. By requiring only black-box access to the underlying model, randomized smoothing scales to large architectures and is agnostic to the internals of the network. However, past work on randomized smoothing has focused on restricted classes of smoothing measures or perturbations (like Gaussian or discrete) and has only been able to prove robustness with respect to simple norm bounds. In this paper we introduce a general framework for proving robustness properties of smoothed machine learning models in the black-box setting. Specifically, we extend randomized smoothing procedures to handle arbitrary smoothing measures and prove robustness of the smoothed classifier by using $f$-divergences. Our methodology achieves state-of-the-art}certified robustness on MNIST, CIFAR-10 and ImageNet and also audio classification task, Librispeech, with respect to several classes of adversarial perturbations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['robust', 'smoothing']"
Yes,"Robust, Noise",Exponentiated Gradient Reweighting for Robust Training Under Label Noise and Beyond,5.0,"Negin Majidi, E. Amid, Hossein Talebi, Manfred K. Warmuth",2021.0,,,,,35.0,2022-07-13 10:05:55,,,,,,,,,5,5.0,1.0,4.0,1.0,"Many learning tasks in machine learning can be viewed as taking a gradient step towards minimizing the average loss of a batch of examples in each training iteration. When noise is prevalent in the data, this uniform treatment of examples can lead to overfitting to noisy examples with larger loss values and result in poor generalization. Inspired by the expert setting in on-line learning, we present a flexible approach to learning from noisy examples. Specifically, we treat each training example as an expert and maintain a distribution over all examples. We alternate between updating the parameters of the model using gradient descent and updating the example weights using the exponentiated gradient update. Unlike other related methods, our approach handles a general class of loss functions and can be applied to a wide range of noise types and applications. We show the efficacy of our approach for multiple learning settings, namely noisy principal component analysis and a variety of noisy classification problems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['noise', 'robust']"
Yes,"Neural Networks, Pruning, Training",On Pruning Adversarially Robust Neural Networks,18.0,"Vikash Sehwag, Shiqi Wang, Prateek Mittal, S. Jana",2020.0,,,,,36.0,2022-07-13 10:08:07,,,,,,,,,18,9.0,5.0,4.0,2.0,"In safety-critical but computationally resourceconstrained applications, deep learning faces two key challenges: lack of robustness against adversarial attacks and large neural network size (often millions of parameters). While the research community has extensively explored the use of robust training and network pruning independently to address one of these challenges, we show that integrating existing pruning techniques with multiple types of robust training techniques, including verifiably robust training, leads to poor robust accuracy even though such techniques can preserve high regular accuracy. We further demonstrate that making pruning techniques aware of the robust learning objective can lead to a large improvement in performance. We realize this insight by formulating the pruning objective as an empirical risk minimization problem which is then solved using SGD. We demonstrate the success of the proposed pruning technique across CIFAR-10, SVHN, and ImageNet dataset with four different robust training techniques: iterative adversarial training, randomized smoothing, MixTrain, and CROWN-IBP. Specifically, at 99% connection pruning ratio, we achieve gains up to 3.2, 10.0, and 17.8 percentage points in robust accuracy under state-of-the-art adversarial attacks for ImageNet, CIFAR-10, and SVHN dataset, respectively. Our code and compressed networks are publicly available1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural networks', 'pruning', 'training']"
No,"federated learning, fairness, trustworthy, review",Towards Blockchain-Based Fair and Trustworthy Federated Learning Systems,0.0,"AM Dirir, K Salah, D Svetinovic",2021.0,Federated Learning Systems,Springer,https://link.springer.com/chapter/10.1007/978-3-030-70604-3_7,,36.0,2022-07-14 11:28:06,,10.1007/978-3-030-70604-3_7,,,,,,,0,0.0,0.0,3.0,1.0,… mechanism to train Machine Learning models on unseen data. … for the development of fair and trustworthy FL systems. … in turn will help build better robust machine learning models. …,,https://scholar.google.com/scholar?q=related:H4Q5YbcwohMJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22trustworthy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['fairness', 'federated learning', 'survey', 'trustworthy']"
Yes,"measure, framework, adversarial, robust",Fundamental limits on adversarial robustness,68.0,"A Fawzi, O Fawzi, P Frossard",2015.0,"Proc. ICML, Workshop on Deep Learning",epfl.ch,https://www.epfl.ch/labs/lts4/wp-content/uploads/2018/10/icml2015a.pdf,https://scholar.google.com/scholar?cites=8293169021539437548&as_sdt=2005&sciodt=2007&hl=en,36.0,2022-07-13 13:05:38,PDF,,,,,,,,68,10.11,23.0,3.0,7.0,"… The focus of this paper is to study the robustness of classifiers to adversarial perturbations in the ambient space Rd. Given a datapoint x ∈ Rd sampled from µ, we denote by ∆adv(x; f) …",https://www.epfl.ch/labs/lts4/wp-content/uploads/2018/10/icml2015a.pdf,https://scholar.google.com/scholar?q=related:7MshN0lBF3MJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'framework', 'metric', 'robust']"
No,"Dropout, Training, Out-of-Distribution Data",Machine Learning's Dropout Training is Distributionally Robust Optimal,3.0,"J. Blanchet, Yang Kang, J. L. M. Olea, Viet Anh Nguyen, Xuhui Zhang",2020.0,,,,,37.0,2022-07-13 09:38:44,,,,,,,,,3,1.5,1.0,5.0,2.0,"This paper shows that dropout training in Generalized Linear Models is the minimax solution of a two-player, zero-sum game where an adversarial nature corrupts a statistician's covariates using a multiplicative nonparametric errors-in-variables model. In this game---known as a Distributionally Robust Optimization problem---nature's least favorable distribution is dropout noise, where nature independently deletes entries of the covariate vector with some fixed probability $\delta$. Our decision-theoretic analysis shows that dropout training---the statistician's minimax strategy in the game---indeed provides out-of-sample expected loss guarantees for distributions that arise from multiplicative perturbations of in-sample data.  This paper also provides a novel, parallelizable, Unbiased Multi-Level Monte Carlo algorithm to speed-up the implementation of dropout training. Our algorithm has a much smaller computational cost compared to the naive implementation of dropout, provided the number of data points is much smaller than the dimension of the covariate vector.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['dropout', 'out-of-distribution data', 'training']"
Yes,"Adversarial Robustness, Perturbations",Improving model robustness by adaptively correcting perturbation levels with active queries,2.0,"KP Ning, L Tao, S Chen, SJ Huang",2021.0,… on Artificial Intelligence,ojs.aaai.org,https://ojs.aaai.org/index.php/AAAI/article/view/17106,https://scholar.google.com/scholar?cites=3025186297027305214&as_sdt=2005&sciodt=2007&hl=en,37.0,2022-07-12 13:35:23,,,,,,,,,2,2.0,1.0,4.0,1.0,"… In addition to high accuracy, robustness is becoming increas… to improving the model robustness by training with noise … helpful information for improving the robustness. Motivated by this …",https://ojs.aaai.org/index.php/AAAI/article/view/17106/16913,https://scholar.google.com/scholar?q=related:_h6xteie-ykJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22human+knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'perturbations']"
Yes,"Robust, Noise, Spiking Neural Network",LISNN: Improving Spiking Neural Networks with Lateral Interactions for Robust Object Recognition,28.0,"Xiang Cheng, Yunzhe Hao, Jiaming Xu, Bo Xu",2020.0,,,,,37.0,2022-07-13 10:10:34,,10.24963/ijcai.2020/211,,,,,,,28,14.0,7.0,4.0,2.0,"Spiking Neural Network (SNN) is considered more biologically plausible and energy-efficient on emerging neuromorphic hardware. Recently backpropagation algorithm has been utilized for training SNN, which allows SNN to go deeper and achieve higher performance. However, most existing SNN models for object recognition are mainly convolutional structures or fully-connected structures, which only have inter-layer connections, but no intra-layer connections. Inspired by Lateral Interactions in neuroscience, we propose a highperformance and noise-robust Spiking Neural Network (dubbed LISNN). Based on the convolutional SNN, we model the lateral interactions between spatially adjacent neurons and integrate it into the spiking neuron membrane potential formula, then build a multi-layer SNN on a popular deep learning framework, i. e., PyTorch. We utilize the pseudo-derivative method to solve the nondifferentiable problem when applying backpropagation to train LISNN and test LISNN on multiple standard datasets. Experimental results demonstrate that the proposed model can achieve competitive or better performance compared to current state-of-the-art spiking neural networks on MNIST, Fashion-MNIST, and N-MNIST datasets. Besides, thanks to lateral interactions, our model processes stronger noise-robustness than other SNN. Our work brings a biologically plausible mechanism into SNN, hoping that it can help us understand the visual information processing in the brain.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['noise', 'robust', 'spiking neural network']"
No,"Adversarial, Robust, Federated Learning, Fairness",A Reputation Mechanism Is All You Need: Collaborative Fairness and Adversarial Robustness in Federated Learning,8.0,"Xinyi Xu, Lingjuan Lyu",2020.0,,,,,38.0,2022-07-13 09:22:57,,,,,,,,,8,4.0,4.0,2.0,2.0,"Federated learning (FL) is an emerging practical framework for effective and scalable machine learning among multiple participants, such as end users, organizations and companies. However, most existing FL or distributed learning frameworks have not well addressed two important issues together: collaborative fairness and adversarial robustness (e.g. free-riders and malicious participants). In conventional FL, all participants receive the global model (equal rewards), which might be unfair to the high-contributing participants. Furthermore, due to the lack of a safeguard mechanism, free-riders or malicious adversaries could game the system to access the global model for free or to sabotage it. In this paper, we propose a novel Robust and Fair Federated Learning (RFFL) framework to achieve collaborative fairness and adversarial robustness simultaneously via a reputation mechanism. RFFL maintains a reputation for each participant by examining their contributions via their uploaded gradients (using vector similarity) and thus identifies non-contributing or malicious participants to be removed. Our approach differentiates itself by not requiring any auxiliary/validation dataset. Extensive experiments on benchmark datasets show that RFFL can achieve high fairness and is very robust to different types of adversaries while achieving competitive predictive accuracy. Equal contribution Department of Computer Science, University of Singapore, Singapore, Singapore Ant financial. Correspondence to: Xinyi Xu <xinyi.xu@u.nus.edu>, Lingjuan Lyu <lingjuanlvsmile@gmail.com>. This work was presented at the International Workshop on Federated Learning for User Privacy and Data Confidentiality in Conjunction with ICML 2021 (FL-ICML’21). This workshop does not have official proceedings and this paper is non-archival. Copyright 2021 by the author(s).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'fairness', 'federated learning', 'robust']"
No,"Generative Adversarial Networks, Data Augmentation, Noise",Realistic galaxy images and improved robustness in machine learning tasks from generative modelling,0.0,"Benjamin Holzschuh, C. M. O’Riordan, S. Vegetti, V. Rodriguez-Gomez, Nils Thuerey",2022.0,,,,,38.0,2022-07-13 09:24:54,,,,,,,,,0,0.0,0.0,5.0,1.0,"We examine the capability of generative models to produce realistic galaxy images. We show that mixing generated data with the original data improves the robustness in downstream machine learning tasks. We focus on three different data sets; analytical Sérsic profiles, real galaxies from the COSMOS survey, and galaxy images produced with the SKIRT code, from the IllustrisTNG simulation. We quantify the performance of each generative model using the Wasserstein distance between the distributions of morphological properties (e.g. the Gini-coefficient, the asymmetry, and ellipticity), the surface brightness distribution on various scales (as encoded by the power-spectrum), the bulge statistic and the colour for the generated and source data sets. With an average Wasserstein distance (Fréchet Inception Distance) of 7.19 × 10−2 (0.55), 5.98 × 10−2 (1.45) and 5.08 × 10−2 (7.76) for the Sérsic, COSMOS and SKIRT data set, respectively, our best models convincingly reproduce even themost complicated galaxy properties and create images that are visually indistinguishable from the source data. We demonstrate that by supplementing the training data set with generated data, it is possible to significantly improve the robustness against domain-shifts and out-ofdistribution data. In particular, we train a convolutional neural network to denoise a data set of mock observations. By mixing generated images into the original training data, we obtain an improvement of 11 and 45 per cent in the model performance regarding domain-shifts in the physical pixel size and background noise level, respectively.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['data augmentation', 'generative adversarial networks', 'noise']"
No,"Time Series, Generative Adversarial Network, Robust",Improving robustness on seasonality-heavy multivariate time series anomaly detection,3.0,"F Khoshnevisan, Z Fan, VR Carvalho",2020.0,arXiv preprint arXiv:2007.14254,arxiv.org,https://arxiv.org/abs/2007.14254,https://scholar.google.com/scholar?cites=12688301826551867895&as_sdt=2005&sciodt=2007&hl=en,38.0,2022-07-13 14:36:56,,,,,,,,,3,1.5,1.0,3.0,2.0,"… This paper explores some of the challenges in such data, and proposes a new approach that makes inroads towards increased robustness on seasonal and contaminated data, while …",https://arxiv.org/pdf/2007.14254,https://scholar.google.com/scholar?q=related:92mQJknnFbAJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22accountability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generative adversarial networks', 'robust', 'time series']"
No,"Classification, Adversarial Attacks, Perturbations",Improving Robustness to Attacks Against Vertex Classification,7.0,"B. A. Miller, Mustafa Çamurcu, Alexander J. Gomez, K. Chan, Tina Eliassi-Rad",2019.0,,,,,38.0,2022-07-13 09:23:47,,,,,,,,,7,2.33,1.0,5.0,3.0,"Vertex classification—the problem of identifying the class labels of nodes in a graph—has applicability in a wide variety of domains. Examples include classifying subject areas of papers in citation networks or roles of machines in a computer network. Recent work has demonstrated that vertex classification using graph convolutional networks is susceptible to targeted poisoning attacks, in which both graph structure and node attributes can be changed in an attempt to misclassify a target node. This vulnerability decreases users’ confidence in the learning method and can prevent adoption in high-stakes contexts. This paper presents work in progress aiming to make vertex classification robust to these types of attacks. We investigate two aspects of this problem: (1) the classification model and (2) themethod for selecting training data. Our alternative classifier is a support vector machine (with a radial basis function kernel), which is applied to an augmented node feature-vector obtained by appending the node’s attributes to a Euclidean vector representing the node based on the graph structure. Our alternative methods of selecting training data are (1) to select the highestdegree nodes in each class and (2) to iteratively select the node with the most neighbors minimally connected to the training set. In the datasets on which the original attack was demonstrated, we show that changing the training set can make the network much harder to attack. To maintain a given probability of attack success, the adversary must use far more perturbations; often a factor of 2–4 over the random training baseline. Even in cases where success is relatively easy for the attacker, we show that the classification and training alternatives allow classification performance to degrade much more gradually, with weaker incorrect predictions for the attacked nodes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'classification', 'perturbations']"
Yes,"Data Augmentation, Spurious Correlation, Counterfactual",Robustness to spurious correlations in text classification via automatically generated counterfactuals,17.0,"Z Wang, A Culotta",2021.0,… of the AAAI Conference on Artificial Intelligence,ojs.aaai.org,https://ojs.aaai.org/index.php/AAAI/article/view/17651,https://scholar.google.com/scholar?cites=11598894796576579417&as_sdt=2005&sciodt=2007&hl=en,39.0,2022-07-12 11:49:54,,,,,,,,,17,17.0,9.0,2.0,1.0,"… Using this framework, we can easily improve classifier robustness even with few causal … this framework to other tasks such as topic classification robustness. To do so, we would need to …",https://ojs.aaai.org/index.php/AAAI/article/download/17651/17458,https://scholar.google.com/scholar?q=related:Wd8SKV2N96AJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['counterfactual', 'data augmentation', 'spurious correlation']"
Yes,"Interpretability, Deep Learning, Adversarial Robustness",Improving the Interpretability of fMRI Decoding using Deep Neural Networks and Adversarial Robustness,0.0,"P McClure, D Moraczewski, KC Lam, A Thomas…",2020.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2004.11114,,39.0,2022-07-13 10:52:14,,,,,,,,,0,0.0,0.0,5.0,2.0,… vary widely in interpretability. We also apply several saliency-based interpretability methods to … This should increase the interpretability of complex machine learning models and their …,https://arxiv.org/pdf/2004.11114,https://scholar.google.com/scholar?q=related:_MJinMZ1AqAJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22interpretability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'deep learning', 'interpretability']"
Yes,"robustness to noisy labels, neural networks, computer vision, object detection",Dropout sampling for robust object detection in open-set conditions,121.0,"D Miller, L Nicholson, F Dayoub…",2018.0,… on Robotics and …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8460700/?casa_token=O8Eq7NvbeJgAAAAA:iz3BDHnFFrgrlcRV1j_UWawWwd57bOS7XipLCqE_ObOPHk-VOYa7z4iaJHTBivyQoO3qgNvulw,https://scholar.google.com/scholar?cites=14714432469954040683&as_sdt=2005&sciodt=2007&hl=en,39.0,2022-07-14 10:19:14,,,,,,,,,121,30.25.00,30.0,4.0,4.0,"… Current attempts at improving open-set performance of machine learning systems have focused on formally accounting for unknown unknowns [4], [5], [15] by identifying and rejecting …",https://ieeexplore.ieee.org/iel7/8449910/8460178/08460700.pdf?casa_token=Go0bo4iJNZEAAAAA:mWefdxbwAgwUlbdHXA2qsXkl9uohRJp9XvWPa2wToSVxHzjezNkW9YUHhXwgk19lXw2WXpJSZw,https://scholar.google.com/scholar?q=related:a5MV-14qNMwJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22unknowns%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'label noise', 'neural networks']"
Yes,"Survey, Robust, Accuracy",Is Robustness the Cost of Accuracy? - A Comprehensive Study on the Robustness of 18 Deep Image Classification Models,0.0,"Dong Su, Huan Zhang, Hongge Chen, Jinfeng Yi, Pin-Yu Chen, Yupeng Gao",2018.0,"Computer Vision - Eccv 2018, Pt Xii",,,,39.0,2022-07-13 15:27:05,Proceedings Paper,10.1007/978-3-030-01258-8_39,0302-9743,,11216.0,,644.0,661.0,0,0.0,0.0,6.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'robust', 'survey']"
Yes,"Adversarial Perturbations, Explainability, Fidelity",Robust and stable black box explanations,38.0,"H Lakkaraju, N Arsov, O Bastani",2020.0,… on Machine Learning,proceedings.mlr.press,http://proceedings.mlr.press/v119/lakkaraju20a.html?ref=https://githubhelp.com,https://scholar.google.com/scholar?cites=13638658442863351598&as_sdt=2005&sciodt=2007&hl=en,40.0,2022-07-13 10:06:51,,,,,,,,,38,19.0,13.0,3.0,2.0,"… shown to lack stability and robustness to distribution shifts. We … approach substantially improves robustness of explanations … where the black box is also an interpretable model B∗ ∈ E, …",http://proceedings.mlr.press/v119/lakkaraju20a/lakkaraju20a.pdf,https://scholar.google.com/scholar?q=related:LpfA05c_Rr0J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22interpretable%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'explainability']"
Yes,"Label Noise, Generative Adversarial Networks, Robust",Label-noise robust generative adversarial networks,49.0,"T Kaneko, Y Ushiku, T Harada",2019.0,Proceedings of the IEEE …,openaccess.thecvf.com,http://openaccess.thecvf.com/content_CVPR_2019/html/Kaneko_Label-Noise_Robust_Generative_Adversarial_Networks_CVPR_2019_paper.html,https://scholar.google.com/scholar?cites=832067215173341045&as_sdt=2005&sciodt=2007&hl=en,40.0,2022-07-14 11:17:40,,,,,,,,,49,16.33,16.0,3.0,3.0,"… In computer vision and machine learning, generative modeling has been actively studied to generate or reproduce samples indistinguishable from real data. Recently, deep generative …",http://openaccess.thecvf.com/content_CVPR_2019/papers/Kaneko_Label-Noise_Robust_Generative_Adversarial_Networks_CVPR_2019_paper.pdf,https://scholar.google.com/scholar?q=related:dc-nRbkYjAsJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generative adversarial networks', 'label noise', 'robust']"
Yes,"robust, out-of-distribution",Fool Me Once: Robust Selective Segmentation via Out-of-Distribution Detection with Contrastive Learning,2.0,"David Williams, Matthew Gadd, D. Martini, P. Newman",2021.0,,,,,41.0,2022-07-13 10:09:16,,10.1109/ICRA48506.2021.9561165,,,,,,,2,2.0,1.0,4.0,1.0,"In this work, a neural network is trained to simultaneously perform segmentation and pixel-wise Out-of-Distribution (OoD) detection, such that the segmentation of unknown regions of scenes can be rejected. This is made possible by leveraging an OoD dataset with a novel contrastive objective and data augmentation scheme. By including unknown classes in the training data, a more robust feature representation is learned with known classes represented distinctly from those unknown. In comparison, when presented with unknown classes or conditions, many current approaches for segmentation frequently exhibit high confidence in their inaccurate segmentations and cannot be trusted in many operational environments. We validate our system on a real-world dataset of unusual driving scenes, and show that by selectively segmenting scenes based on what is predicted as OoD, we can increase the segmentation accuracy by an IoU of 0.2 with respect to alternative techniques.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['out-of-distribution data', 'robust']"
Yes,"Certified Robustness, Pruning, Deep Learning",Linearity Grafting: How Neuron Pruning Helps Certifiable Robustness,0.0,"T Chen, H Zhang, Z Zhang, S Chang…",2022.0,International …,research.ibm.com,https://research.ibm.com/publications/linearity-grafting-how-neuron-pruning-helps-certifiable-robustness,,41.0,2022-07-13 08:53:52,,,,,,,,,0,0.0,0.0,5.0,1.0,"… intercept, that might overly restrict the network flexibility and sacrifice its performance … effectively tighten certified bounds; (2) achieve competitive certifiable robustness without certified …",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'pruning', 'robust']"
Yes,"adversarial robustness, explainability",Which neural network makes more explainable decisions? An approach towards measuring explainability,0.0,"M Zhang, J Sun, J Wang",2022.0,Automated Software Engineering,Springer,https://link.springer.com/article/10.1007/s10515-022-00338-w,,42.0,2022-07-13 12:01:57,,10.1007/s10515-022-00338-w,,,,,,,0,0.0,0.0,3.0,1.0,… on the decision explainability of neural … neural network models trained on benchmark datasets. The results show that existing neural networks’ decisions often have low explainability …,https://ink.library.smu.edu.sg/cgi/viewcontent.cgi?article=8163&context=sis_research,https://scholar.google.com/scholar?q=related:HHGtCu-hycQJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22explainability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'explainability']"
Yes,"Deep Learning, Adversarial Robustness, Jacobian",Improving DNN Robustness to Adversarial Attacks Using Jacobian Regularization,0.0,"Daniel Jakubovitz, Raja Girye",2018.0,"Computer Vision - Eccv 2018, Pt Xii",,,,42.0,2022-07-13 15:47:10,Proceedings Paper,10.1007/978-3-030-01258-8_32,0302-9743,,11216.0,,525.0,541.0,0,0.0,0.0,2.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'deep learning', 'jacobian']"
Yes,"Explainability, Robsut, Adversarial",An Assessment of Robustness for Adversarial Attacks and Physical Distortions on Image Classification using Explainable AI,0.0,K.T.Y. Mahima,2021.0,CEUR Workshop Proceedings,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85128749013&origin=inward,42.0,2022-07-12 16:28:04,Conference Paper,,1613-0073,https://api.elsevier.com/content/abstract/scopus_id/85128749013,3125.0,,14.0,28.0,0,0.0,0.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'explainability', 'robust']"
No,adversarial robustness,Trustworthy ML: Robustness and Foresight,0.0,S Kadavath,2021.0,,digitalassets.lib.berkeley.edu,https://digitalassets.lib.berkeley.edu/techreports/ucb/incoming/EECS-2021-245.pdf,,43.0,2022-07-13 11:13:23,PDF,,,,,,,,0,0.0,0.0,1.0,1.0,"… To measure this ability in machine learning models, we introduce MATH, a new dataset of 12,500 challenging competition mathematics problems. Each problem in MATH has a full step-…",https://digitalassets.lib.berkeley.edu/techreports/ucb/incoming/EECS-2021-245.pdf,https://scholar.google.com/scholar?q=related:a5NSRI1rVXoJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22trustworthy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['adversarial robustness']
Yes,"Adversarial, Training, Robust, Accuracy",Cat: Customized adversarial training for improved robustness,43.0,"M Cheng, Q Lei, PY Chen, I Dhillon…",2020.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2002.06789,https://scholar.google.com/scholar?cites=5677713869977603912&as_sdt=2005&sciodt=2007&hl=en,44.0,2022-07-12 11:56:54,,,,,,,,,43,21.5,9.0,5.0,2.0,"… However, it has been found that DNNs are highly vulnerable to adversarial examples (… To enhance the robustness of DNNs against adversarial examples, adversarial training (…",https://arxiv.org/pdf/2002.06789,https://scholar.google.com/scholar?q=related:SM9S1K5Ky04J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial', 'robust', 'training']"
Yes,"Loss Function, Deep Learning, Robust",Improving Deep Network Robustness to Unknown Inputs with Objectosphere,3.0,"A. Dhamija, Manuel Günther, T. Boult",2019.0,,,,,44.0,2022-07-13 09:26:50,,,,,,,,,3,1.0,1.0,3.0,3.0,"Deep Neural Networks trained on academic datasets often fail when applied to the real world. These failures generally arise from unknown inputs that are not of interest to the system. The mis-classification of these unknown inputs as one of the known classes highlights the need for more robust deep networks. The problem of identifying samples that are not of interest to the system has previously been tackled by either thresholding softmax, which by construction cannot return none of the known classes itself, or by learning new features for the unknown inputs using an additional background or garbage class. As demonstrated, both of these approaches help but are generally insufficient when previously unseen classes are encountered. This paper overviews our recent publication Reducing Network Agnostophobia, NeurIPS 2018. The paper presented two novel loss functions that effectively handle unseen classes while providing a new measure for uncertainty. The ability to identify unknown samples plays a crucial role in developing robust networks that may be used in open-world problems. The paper also introduced an evaluation metric that focused on comparing performance of multiple approaches in an open-set setting.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'loss function', 'robust']"
Yes,"Adversarial, Training, Robust",A Self-supervised Approach for Adversarial Robustness,53.0,"Muzammal Naseer, S. Khan, Munawar Hayat, F. Khan, F. Porikli",2020.0,,,,,44.0,2022-07-13 09:26:25,,10.1109/cvpr42600.2020.00034,,,,,,,53,26.5,11.0,5.0,2.0,"Adversarial examples can cause catastrophic mistakes in Deep Neural Network (DNNs) based vision systems e.g., for classification, segmentation and object detection. The vulnerability of DNNs against such attacks can prove a major roadblock towards their real-world deployment. Transferability of adversarial examples demand generalizable defenses that can provide cross-task protection. Adversarial training that enhances robustness by modifying target model's parameters lacks such generalizability. On the other hand, different input processing based defenses fall short in the face of continuously evolving attacks. In this paper, we take the first step to combine the benefits of both approaches and propose a self-supervised adversarial training mechanism in the input space. By design, our defense is a generalizable approach and provides significant robustness against the \textbf{unseen} adversarial attacks (\eg by reducing the success rate of translation-invariant \textbf{ensemble} attack from 82.6\% to 31.9\% in comparison to previous state-of-the-art). It can be deployed as a plug-and-play solution to protect a variety of vision systems, as we demonstrate for the case of classification, segmentation and detection.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust', 'training']"
Yes,"Robust, Smoothing",Certified Robustness to Label-Flipping Attacks via Randomized Smoothing,45.0,"Elan Rosenfeld, Ezra Winston, Pradeep Ravikumar, J. Z. Kolter",2020.0,,,,,44.0,2022-07-13 09:25:10,,,,,,,,,45,22.5,11.0,4.0,2.0,"Machine learning algorithms are known to be susceptible to data poisoning attacks, where an adversary manipulates the training data to degrade performance of the resulting classifier. In this work, we present a unifying view of randomized smoothing over arbitrary functions, and we leverage this novel characterization to propose a new strategy for building classifiers that are pointwise-certifiably robust to general data poisoning attacks. As a specific instantiation, we utilize our framework to build linear classifiers that are robust to a strong variant of label flipping, where each test example is targeted independently. In other words, for each test point, our classifier includes a certification that its prediction would be the same had some number of training labels been changed adversarially. Randomized smoothing has previously been used to guarantee---with high probability---test-time robustness to adversarial manipulation of the input to a classifier; we derive a variant which provides a deterministic, analytical bound, sidestepping the probabilistic certificates that traditionally result from the sampling subprocedure. Further, we obtain these certified bounds with minimal additional runtime complexity over standard classification and no assumptions on the train or test distributions. We generalize our results to the multi-class case, providing the first multi-class classification algorithm that is certifiably robust to label-flipping attacks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['robust', 'smoothing']"
Yes,"Generative Adversarial Network, Robust, Adversarial",A direct approach to robust deep learning using adversarial networks,49.0,"H Wang, CN Yu",2019.0,arXiv preprint arXiv:1905.09591,arxiv.org,https://arxiv.org/abs/1905.09591,https://scholar.google.com/scholar?cites=2332293430655643076&as_sdt=2005&sciodt=2007&hl=en,45.0,2022-07-13 17:19:08,,,,,,,,,49,16.33,25.0,2.0,3.0,"… Our adversarial network is not as robust as adversarial PGD under white box attack, … robust than the undefended network. Interestingly, in addition to transferring well to the adversarial …",https://arxiv.org/pdf/1905.09591,https://scholar.google.com/scholar?q=related:xO00poT4XSAJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'generative adversarial networks', 'robust']"
Discussion,"review, human-in-the-loop, trustworthy, explainability",A Human-Centered Agenda for Intelligible Machine Learning,36.0,"Jennifer Wortman Vaughan, H. Wallach",2021.0,,,,,45.0,2022-07-13 09:25:03,,10.7551/MITPRESS/12186.003.0014,,,,,,,36,36.0,18.0,2.0,1.0,"To build machine learning systems that are reliable, trustworthy, and fair, we must be able to provide relevant stakeholders with an understanding of how these systems work. Yet what makes a system “intelligible” is difficult to pin down. Intelligibility is a fundamentally human-centered concept that lacks a one-size-fits-all solution. Although many intelligibility techniques have been proposed in the machine learning literature, there are many more open questions about how best to provide stakeholders with the information they need to achieve their desired goals. In this chapter, we begin with an overview of the intelligible machine learning landscape and give several examples of the diverse ways in which needs for intelligibility can arise. We provide an overview of the techniques for achieving intelligibility that have been proposed in the machine learning literature. We discuss the importance of taking a human-centered strategy when designing intelligibility techniques or when verifying that these techniques achieve their intended goals. We also argue that the notion of intelligibility should be expanded beyond machine learning models to other components of machine learning systems, such as datasets and performance metrics. Finally, we emphasize the necessity of tight integration between the machine learning and human–computer interaction communities.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'hci', 'survey', 'trustworthy']"
Yes,individual fairness,Training individually fair ML models with sensitive subspace robustness,46.0,"Mikhail Yurochkin, Amanda Bower, Yuekai Sun",2019.0,,,,,45.0,2022-07-13 09:25:10,,,,,,,,,46,"0,6479166667",15.0,3.0,3.0,"We propose an approach to training machine learning models that are fair in the sense that their performance is invariant under certain perturbations to the features. For example, the performance of a resume screening system should be invariant under changes to the name of the applicant. We formalize this intuitive notion of fairness by connecting it to the original notion of individual fairness put forth by Dwork et al and show that the proposed approach achieves this notion of fairness. We also demonstrate the effectiveness of the approach on two machine learning tasks that are susceptible to gender and racial biases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['fairness']
Yes,"Classifier, Visual, Robust",Compositional Convolutional Neural Networks: A Robust and Interpretable Model for Object Recognition Under Occlusion,5.0,A. Kortylewski,2021.0,International Journal of Computer Vision,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85096439182&origin=inward,45.0,2022-07-12 16:28:40,Article,10.1007/s11263-020-01401-3,0920-5691,https://api.elsevier.com/content/abstract/scopus_id/85096439182,129.0,3.0,736.0,760.0,5,5.0,5.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classifier', 'robust', 'visual']"
Yes,"Visual, Robust",BIRD: Learning Binary and Illumination Robust Descriptor for Face Recognition,4.0,"Z. Su, M. Pietikäinen, Li Liu",2019.0,,,,,46.0,2022-07-13 09:28:50,,,,,,,,,4,1.33,1.0,3.0,3.0,"Recently face recognition has made significantly progress due to the advancement of large scale Deep Convolutional Neural Network (DeepCNNs). Despite the great success, the known deficiencies of DeepCNNs have not been addressed, such as the need for too much labeled training data, energy hungry, lack of theoretical interpretability, lack of robustness to image transformations and degradations, and vulnerable to attacks, which limit DeepCNNs to be used in many real world applications. Therefore, these factors make previous predominating Local Binary Patterns (LBP) based face recognition methods still irreplaceable. In this paper we propose a novel approach called BIRD (learning Binary and Illumination Robust Descriptor) for face representation, which nicely balances the three criteria: distinctiveness, robustness, and computationally inexpensive cost. We propose to learn discriminative and compact binary codes directly from six types of Pixel Difference Vectors (PDVs). For each type of binary codes, we cluster and pool these compact binary codes to obtain a histogram representation of each face image. Six global histograms derived from six types of learned compact binary codes are fused for the final face recognition. Experimental results on the CAS_PERL_R1 and LFW databases indicate the performance of our BIRD surpasses all previous binary based face recognition methods on the two evaluated datasets. More impressively, the proposed BIRD is shown to be highly robust to illumination changes, and produces 89.5% on the CAS_PEAL_R1 illumination subset, which, we believe, is so far the best reported results on this dataset. Our code is made available 1.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['robust', 'visual']"
Yes,"speech recognition, robustness to noice, mitigation method",Dual-Path Style Learning for End-to-End Noise-Robust Speech Recognition,0.0,"Y Hu, N Hou, C Chen, ES Chng",2022.0,arXiv preprint arXiv:2203.14838,arxiv.org,https://arxiv.org/abs/2203.14838,,46.0,2022-07-14 09:04:21,,,,,,,,,0,0.0,0.0,4.0,1.0,"… Noise-robust automatic speech recognition degrades signifi… end-to-end noise-robust automatic speech recognition (DPSL… , noiserobust speech recognition, over-suppression problem …",https://arxiv.org/pdf/2203.14838,https://scholar.google.com/scholar?q=related:V3UveJIf4BsJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['mitigation method', 'noise', 'time series']"
Yes,"pre-processing, training, robust, out.of-distribution",GCOOD: A Generic Coupled Out-of-Distribution Detector for Robust Classification,0.0,"Rogerio Ferreira De Moraes, Raphael S. Evangelista, Leandro A. F. Fernandes, Luis Martí",2021.0,,,,,47.0,2022-07-13 10:09:16,,10.1109/sibgrapi54419.2021.00062,,,,,,,0,0.0,0.0,4.0,1.0,"Neural networks have achieved high degrees of accuracy in classification tasks. However, when an out-of-distribution (OOD) sample (i.e., entries from unknown classes) is submitted to the classification process, the result is the association of the sample to one or more of the trained classes with different degrees of confidence. If any of these confidence values are more significant than the user-defined threshold, the network will mislabel the sample, affecting the model credibility. The definition of the acceptance threshold itself is a sensitive issue in the face of the classifier’s overconfidence. This paper presents the Generic Coupled OOD Detector (GCOOD), a novel Convolutional Neural Network (CNN) tailored to detect whether an entry submitted to a trained classification model is an OOD sample for that model. From the analysis of the Softmax output of any classifier, our approach can indicate whether the resulting classification should be considered or not as a sample of some of the trained classes. To train our CNN, we had to develop a novel training strategy based on Voronoi diagrams of the location of representative entries in the latent space of the classification model and graph coloring. We evaluated our approach using ResNet, VGG, DenseNet, and SqueezeNet classifiers with images from the CIFAR-10 dataset.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['out-of-distribution data', 'pre-processing', 'robust', 'training']"
No,"Label Noise, Deep Learning, Robust",Making deep neural networks robust to label noise: A loss correction approach,869.0,"G Patrini, A Rozza, A Krishna Menon…",2017.0,Proceedings of the …,openaccess.thecvf.com,http://openaccess.thecvf.com/content_cvpr_2017/html/Patrini_Making_Deep_Neural_CVPR_2017_paper.html,https://scholar.google.com/scholar?cites=15043338876316879408&as_sdt=2005&sciodt=2007&hl=en,47.0,2022-07-14 11:17:40,,,,,,,,,869,173.8,217.0,4.0,5.0,"… Large datasets used in training modern machine learning models, such … noise, with few exceptions [42, 8, 25]. We aim to modify a loss ℓ so as to make it robust to asymmetric label noise…",http://openaccess.thecvf.com/content_cvpr_2017/papers/Patrini_Making_Deep_Neural_CVPR_2017_paper.pdf,https://scholar.google.com/scholar?q=related:MAqxRgGtxNAJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'label noise', 'robust']"
Yes,"Visual, Robust, Framework, Data Augmentation, Convolutional Neural Network",Dealing with Robustness of Convolutional Neural Networks for Image Classification,6.0,"Paolo Arcaini, A. Bombarda, S. Bonfanti, A. Gargantini",2020.0,,,,,47.0,2022-07-13 09:19:22,,10.1109/AITEST49225.2020.00009,,,,,,,6,3.0,2.0,4.0,2.0,"SW-based systems depend more and more on AI also for critical tasks. For instance, the use of machine learning, especially for image recognition, is increasing ever more. As state-of-the-art, Convolutional Neural Networks (CNNs) are the most adopted techniques for image classification. Although they are proved to have optimal results, it is not clear what happens when unforeseen modifications during the image acquisition and elaboration occur. Thus, it is very important to assess the robustness of a CNN, especially when it is used in a safety critical system, as, e.g., in the medical domain or in automated driving systems. Most of the analyses made about the robustness of CNNs are focused on adversarial examples which are created by exploiting the CNN internal structure; however, these are not the only problems we can encounter with CNNs and, moreover, they may be unlikely in some fields. This is why, in this paper, we focus on the robustness analysis when plausible alterations caused by an error during the acquisition of the input images occur. We give a novel definition of robustness w.r.t. possible input alterations for a CNN and we propose a framework to compute it. Moreover, we analyse four methods (data augmentation, limited data augmentation, network parallelization, and limited network parallelization) which can be used to improve the robustness of a CNN for image classification. Analyses are conducted over a dataset of histologic images.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'data augmentation', 'framework', 'robust', 'visual']"
Yes,"Siamese Network, Explainability, Robust",Self-learn to Explain Siamese Networks Robustly,0.0,C. Chen,2021.0,"Proceedings - IEEE International Conference on Data Mining, ICDM",,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85125204490&origin=inward,48.0,2022-07-12 16:28:04,Conference Paper,10.1109/ICDM51629.2021.00116,1550-4786,https://api.elsevier.com/content/abstract/scopus_id/85125204490,2021.0,,1018.0,1023.0,0,0.0,0.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'robust', 'siamese network']"
No,"Noise, Robust",Employing Robust Principal Component Analysis for Noise-Robust Speech Feature Extraction in Automatic Speech Recognition with the Structure of a Deep Neural Network,8.0,"J. Hung, Jung-Shan Lin, Po-Jen Wu",2018.0,,,,,48.0,2022-07-13 09:30:23,,10.3390/ASI1030028,,,,,,,8,2.0,3.0,3.0,4.0,"In recent decades, researchers have been focused on developing noise-robust methods in order to compensate for noise effects in automatic speech recognition (ASR) systems and enhance their performance. In this paper, we propose a feature-based noise-robust method that employs a novel data analysis technique—robust principal component analysis (RPCA). In the proposed scenario, RPCA is employed to process a noise-corrupted speech feature matrix, and the obtained sparse partition is shown to reveal speech-dominant characteristics. One apparent advantage of using RPCA for enhancing noise robustness is that no prior knowledge about the noise is required. The proposed RPCA-based method is evaluated with the Aurora-4 database and a task using a state-of-the-art deep neural network (DNN) architecture as the acoustic models. The evaluation results indicate that the newly proposed method can provide the original speech feature with significant recognition accuracy improvement, and can be cascaded with mean normalization (MN), mean and variance normalization (MVN), and relative spectral (RASTA)—three well-known and widely used feature robustness algorithms—to achieve better performance compared with the individual component method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['noise', 'robust']"
Yes,"short survey, reliability, robustness, trust","On mismatched detection and safe, trustworthy machine learning",8.0,KR Varshney,2020.0,2020 54th Annual Conference on Information …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9086253/,https://scholar.google.com/scholar?cites=11253855951992435464&as_sdt=2005&sciodt=2007&hl=en,49.0,2022-07-13 08:51:06,,,,,,,,,8,4.0,8.0,1.0,2.0,"… theory, data poisoning, adver- sarial robustness, distribution shift … Despite artificial intelligence's promise to reshape different sectors … how should we model trustworthy machine learning …",http://krvarshney.github.io/pubs/Varshney_ciss2020.pdf,https://scholar.google.com/scholar?q=related:CLPs41y6LZwJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22trustworthy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['reliability', 'robust', 'survey', 'trustworthy']"
No,"robustness, adversarial, measure",Evaluating Model Robustness to Adversarial Samples in Network Intrusion Detection,0.0,"Madeleine Schneider, David Aspinall, Nathaniel D. Bastian",2021.0,2021 Ieee International Conference On Big Data (Big Data),,,,49.0,2022-07-13 10:55:42,Proceedings Paper,10.1109/BigData52589.2021.9671580,2639-1589,,,,3343.0,3352.0,0,0.0,0.0,3.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'metric', 'robust']"
Discussion,"Robust, Design, Optimization, Survey",A Critical Review of Surrogate Assisted Robust Design Optimization,0.0,"Tanmoy Chatterjee, Souvik Chakraborty, Rajib Chowdhury",2019.0,Archives Of Computational Methods In Engineering,,,,50.0,2022-07-15 11:21:17,Review,10.1007/s11831-017-9240-5,1134-3060,,26.0,1.0,245.0,274.0,0,0.0,0.0,3.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['design', 'optimization', 'robust', 'survey']"
No,robustness to label noise,A Comparative Analysis of Robustness to Noise in Machine Learning Classifiers,1.0,"S Ishii, D Ljunggren",2021.0,,diva-portal.org,https://www.diva-portal.org/smash/record.jsf?pid=diva2:1597519,https://scholar.google.com/scholar?cites=8608312723744804514&as_sdt=2005&sciodt=2007&hl=en,50.0,2022-07-12 13:50:33,,,,,,,,,1,1.0,1.0,2.0,1.0,"… Artificial intelligence and machine learning have been developing rapidly in recent times, and the strength of their applications have become more apparent. Some modern applications …",https://www.diva-portal.org/smash/get/diva2:1597519/FULLTEXT01.pdf,https://scholar.google.com/scholar?q=related:os5IPtDedncJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['label noise']
Yes,"Adversarial Robustness, Convolutional Neural Network, Loss Function",Improving adversarial robustness via probabilistically compact loss with logit constraints,8.0,"X Li, X Li, D Pan, D Zhu",2021.0,… the AAAI Conference on Artificial Intelligence,ojs.aaai.org,https://ojs.aaai.org/index.php/AAAI/article/view/17030,https://scholar.google.com/scholar?cites=11619359049424214800&as_sdt=2005&sciodt=2007&hl=en,50.0,2022-07-12 11:56:54,,,,,,,,,8,8.0,2.0,4.0,1.0,"… vulnerable to carefully crafted adversarial samples and suffer … to improve adversarial robustness (eg, adversarial training … that they tend to misclassify adversarial samples into the most …",https://ojs.aaai.org/index.php/AAAI/article/view/17030/16837,https://scholar.google.com/scholar?q=related:EJumUn5BQKEJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'convolutional neural network', 'loss function']"
Discussion,human-in-the-loop,The future of human-AI collaboration: a taxonomy of design knowledge for hybrid intelligence systems,94.0,"D Dellermann, A Calma, N Lipusch, T Weber…",2021.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2105.03354,https://scholar.google.com/scholar?cites=4905817702939702885&as_sdt=2005&sciodt=2007&hl=en,50.0,2022-07-12 11:53:57,,,,,,,,,94,94.0,19.0,5.0,1.0,"… by combining human and artificial intelligence to collectively … dimensions and characteristics to differentiate (robustness). … by leveraging mechanisms of human computation (eg [68, 66, …",https://arxiv.org/pdf/2105.03354,https://scholar.google.com/scholar?q=related:ZTaLbDX3FEQJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22human+computation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['hci']
Yes,"Adversarial Attack, Defence, Feature Selection",Investigating Resistance of Deep Learning-based IDS against Adversaries using min-max Optimization,0.0,"Rana Abou Khamis, M. Omair Shafiq, Ashraf Matrawy, IEEE",2020.0,Icc 2020 - 2020 Ieee International Conference On Communications (Icc),,,,50.0,2022-07-13 13:56:07,Proceedings Paper,,1550-3607,,,,,,0,0.0,0.0,4.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'defense', 'features']"
No,"Object Detection, Robust, Dataset",Increased Robustness of Object Detection on Aerial Image Datasets using Simulated Imagery,0.0,"K Konen, T Hecking",2021.0,… Conference on Artificial Intelligence and …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9723671/,,50.0,2022-07-12 13:40:41,,,,,,,,,0,0.0,0.0,2.0,1.0,"… of 0.43 at a confidence threshold of 0.95. … robustness. Consequently, augmenting training-data with simulated images can constitute a significant improvement regarding the robustness …",https://elib.dlr.de/147100/1/AIKE_2021___Increased_Robustness_of_Object_Detection_on_Aerial_Image_Datasets_using_Simulated_Imagery__private_.pdf,https://scholar.google.com/scholar?q=related:oEw-vV8JTZ4J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'dataset', 'robust']"
No,"Framework, Assesment, Robust, Prediction",Assessing and Enhancing Adversarial Robustness of Predictive Analytics: An Empirically Tested Design Framework,0.0,"Weifeng Li, Yidong Chai",2022.0,Journal Of Management Information Systems,,,,51.0,2022-07-13 10:55:42,Article,10.1080/07421222.2022.2063549,0742-1222,,39.0,2.0,542.0,572.0,0,0.0,0.0,2.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'framework', 'prediction', 'robust']"
Yes,"Adversarial, Pre-Processing",ComDefend: An Efficient Image Compression Model to Defend Adversarial Examples,0.0,"Xiaojun Jia, Xingxing Wei, Xiaochun Cao, Hassan Foroosh, IEEE Comp Soc",2019.0,2019 Ieee/Cvf Conference On Computer Vision And Pattern Recognition (Cvpr 2019),,,,51.0,2022-07-13 15:27:05,Proceedings Paper,10.1109/CVPR.2019.00624,1063-6919,,,,6077.0,6085.0,0,0.0,0.0,5.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'pre-processing']"
Yes,"Adversarial Robustness, Regularizer, Ensemble Classifier",Improving Adversarial Robustness via Promoting Ensemble Diversity,0.0,"Tianyu Pang, Kun Xu, Chao Du, Ning Chen, Jun Zhu",2019.0,"International Conference On Machine Learning, Vol 97",,,,53.0,2022-07-13 15:27:05,Proceedings Paper,,2640-3498,,97.0,,,,0,0.0,0.0,5.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'ensemble classifier', 'regularization']"
No,Human-AI decision making,Rapid trust calibration through interpretable and uncertainty-aware AI,49.0,"R Tomsett, A Preece, D Braines, F Cerutti…",2020.0,Patterns,Elsevier,https://www.sciencedirect.com/science/article/pii/S266638992030060X,https://scholar.google.com/scholar?cites=6061329878486729053&as_sdt=2005&sciodt=2007&hl=en,53.0,2022-07-13 17:59:05,HTML,,,,,,,,49,24.5,10.0,5.0,2.0,… This article is about artificial intelligence (AI) used to inform high… Artificial intelligence (AI) systems hold great promise as … and uncertainty awareness for robust AI-supported decision …,https://www.sciencedirect.com/science/article/pii/S266638992030060X,https://scholar.google.com/scholar?q=related:XcGpAWkrHlQJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22interpretable%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['hci']
Yes,"Assessment, Robust, Neural Network, Adversarial",Evaluating Robustness of Neural Networks with Mixed Integer Programming,445.0,"Vincent Tjeng, Kai Y. Xiao, Russ Tedrake",2017.0,,,,,54.0,2022-07-13 09:26:34,,,,,,,,,445,89.0,148.0,3.0,5.0,"Neural networks have demonstrated considerable success on a wide variety of real-world problems. However, neural networks can be fooled by adversarial examples  – slightly perturbed inputs that are misclassified with high confidence. Verification of networks enables us to gauge their vulnerability to such adversarial examples. We formulate verification of piecewise-linear neural networks as a mixed integer program. Our verifier finds minimum adversarial distortions two to three orders of magnitude more quickly than the state-of-the-art. We achieve this via tight formulations for non-linearities, as well as a novel presolve algorithm that makes full use of all information available. The computational speedup enables us to verify properties on convolutional networks with an order of magnitude more ReLUs than had been previously verified by any complete verifier, and we determine for the first time the exact adversarial accuracy of an MNIST classifier to perturbations with bounded l[infinity] norm e = 0:1. On this network, we find an adversarial example for 4.38% of samples, and a certificate of robustness for the remainder. Across a variety of robust training procedures, we are able to certify more samples than the state-of-the-art and find more adversarial examples than a strong first-order attack for every network.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'evaluation', 'neural network', 'robust']"
No,"Training, Adversarial, Robust, Accuracy",Confidence-aware Training of Smoothed Classifiers for Certified Robustness,0.0,"J Jeong, S Kim, J Shin",2021.0,,openreview.net,https://openreview.net/forum?id=qLqeb9AjD2o,,54.0,2022-07-13 10:25:08,,,,,,,,,0,0.0,0.0,3.0,1.0,"… 2014) is still one of the most significant aspects that reveals the gap between machine learning systems and humans: for a given input x (eg, an image) to a classifier f, say a neural …",https://openreview.net/pdf?id=qLqeb9AjD2o,https://scholar.google.com/scholar?q=related:j6wT3gjZJtsJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial', 'robust', 'training']"
Yes,"Loss Function, Label Noise, Deep Learning",L_dmi: A novel information-theoretic loss function for training deep nets robust to label noise,94.0,"Y Xu, P Cao, Y Kong, Y Wang",2019.0,Advances in neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2019/hash/8a1ee9f2b7abe6e88d1a479ab6a42c5e-Abstract.html,https://scholar.google.com/scholar?cites=18122740520845285462&as_sdt=2005&sciodt=2007&hl=en,54.0,2022-07-14 09:04:21,,,,,,,,,94,31.33.00,24.0,4.0,3.0,"… information theoretic noise-robust loss function LDMI based on a generalized information measure, DMI. Theoretically we show that LDMI is robust to instanceindependent label noise. …",https://proceedings.neurips.cc/paper/2019/file/8a1ee9f2b7abe6e88d1a479ab6a42c5e-Paper.pdf,https://scholar.google.com/scholar?q=related:VpiPPXzogPsJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'label noise', 'loss function']"
Yes,"Security, Robust, Adversarial",Do gradient-based explanations tell anything about adversarial robustness to android malware?,0.0,M. Melis,2022.0,International Journal of Machine Learning and Cybernetics,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85117733938&origin=inward,55.0,2022-07-12 16:23:47,Article,10.1007/s13042-021-01393-7,1868-8071,https://api.elsevier.com/content/abstract/scopus_id/85117733938,13.0,1.0,217.0,232.0,0,0.0,0.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'cybersecurity', 'robust']"
No,"Robust, Neural Network",A Priori Guaranteed Evolution Within the Neural Network Approximation Set and Robustness Expansion via Prescribed Performance Control,38.0,"C. Bechlioulis, G. Rovithakis",2012.0,,,,,56.0,2022-07-13 09:26:09,,10.1109/TNNLS.2012.2186152,,,,,,,38,3.8,19.0,2.0,10.0,"A neuroadaptive control scheme for strict feedback systems is designed, which is capable of achieving prescribed performance guarantees for the output error while keeping all closed-loop signals bounded, despite the presence of unknown system nonlinearities and external disturbances. The aforementioned properties are induced without resorting to a special initialization procedure or a tricky control gains selection, but addressing through a constructive methodology the longstanding problem in neural network control of a priori guaranteeing that the system states evolve strictly within the compact region in which the approximation capabilities of neural networks hold. Moreover, it is proven that robustness against external disturbances is significantly expanded, with the only practical constraint being the magnitude of the required control effort. A comparative simulation study clarifies and verifies the approach.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural network', 'robust']"
Yes,"Data Augmentation, Convolutional Neural Network",Data augmentation using artificial Immune systems for noise-robust CNN models,1.0,"M Ofori-Oduro, MA Amer",2020.0,2020 IEEE International Conference …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9191116/,https://scholar.google.com/scholar?cites=6339673852081169606&as_sdt=2005&sciodt=2007&hl=en,56.0,2022-07-14 09:04:21,,,,,,,,,1,0.5,1.0,2.0,2.0,"… of noise … noise greatly dropped by an average mAP (mean Average Precision) of 10.34 on the PASCAL VOC 2007/2012 [11]. To make CNN based object detection more robust to noise, …",,https://scholar.google.com/scholar?q=related:xmQhiccL-1cJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'data augmentation']"
Yes,"semi-supervised learning, robustness to noise labels, mitigation method",Noise-robust semi-supervised learning via fast sparse coding,34.0,"Z Lu, L Wang",2015.0,Pattern Recognition,Elsevier,https://www.sciencedirect.com/science/article/pii/S0031320314003331?casa_token=_e2q50DSJPgAAAAA:W5P4eefNzDHsWl-1YPsxGDpLG8_mqCvTE4S9yGzUK6vfXeOygGurmUUvF0oDj4SBO9_rYNor-OE,https://scholar.google.com/scholar?cites=10764783197903162916&as_sdt=2005&sciodt=2007&hl=en,56.0,2022-07-14 11:17:40,HTML,,,,,,,,34,5.26,17.0,2.0,7.0,"… This paper presents a novel noise-robust graph-based semi-… Inspired by the successful use of sparse coding for noise … Finally, we evaluate the proposed algorithm in noise-robust …",https://www.sciencedirect.com/science/article/pii/S0031320314003331?casa_token=_e2q50DSJPgAAAAA:W5P4eefNzDHsWl-1YPsxGDpLG8_mqCvTE4S9yGzUK6vfXeOygGurmUUvF0oDj4SBO9_rYNor-OE,https://scholar.google.com/scholar?q=related:JI4cw0gxZJUJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['label noise', 'mitigation method', 'semi-supervised learning']"
Yes,"Adversarial, Training, Robust",ALICE++: Adversarial Training for Robust and Effective Temporal Reasoning,0.0,"L Pereira, F Cheng, M Asahara…",2021.0,Proceedings of the 35th …,aclanthology.org,https://aclanthology.org/2021.paclic-1.40.pdf,,57.0,2022-07-14 10:29:30,PDF,,,,,,,,0,0.0,0.0,4.0,1.0,"… In this section, we describe the temporal reasoning tasks we tackle in this work. All tasks are challenging since they require deep understanding of the temporal properties of language. …",https://aclanthology.org/2021.paclic-1.40.pdf,https://scholar.google.com/scholar?q=related:ulOy_EqXnl8J:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22reasoning%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust', 'training']"
Discussion,"Deep Learning, Robust, Adversarial, Interpretability",Adaptive iterative attack towards explainable adversarial robustness,23.0,"Y Shi, Y Han, Q Zhang, X Kuang",2020.0,Pattern Recognition,Elsevier,https://www.sciencedirect.com/science/article/pii/S0031320320301138?casa_token=Y7L0mbWObeIAAAAA:46NiGoCHto0VTx4osQEkJ8Z5exnYwryUAIpRpzlHDiBupeaaUlz40_mv0VItLz2adXeKy6dYFpI,https://scholar.google.com/scholar?cites=11582596946027568263&as_sdt=2005&sciodt=2007&hl=en,57.0,2022-07-13 09:27:33,HTML,,,,,,,,23,11.5,6.0,4.0,2.0,… of the attack mechanism help uncover the reasons for the effectiveness of adversarial examples and support the further improvement of adversarial machine learning’s explainability. …,https://www.sciencedirect.com/science/article/pii/S0031320320301138?casa_token=Y7L0mbWObeIAAAAA:46NiGoCHto0VTx4osQEkJ8Z5exnYwryUAIpRpzlHDiBupeaaUlz40_mv0VItLz2adXeKy6dYFpI,https://scholar.google.com/scholar?q=related:h1x_II6mvaAJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explainability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'deep learning', 'interpretability', 'robust']"
Yes,"semantic robustness, NLP",The King is Naked: on the Notion of Robustness for Natural Language Processing,0.0,"Emanuele La Malfa, M. Kwiatkowska",2021.0,,,,,57.0,2022-07-13 09:19:22,,10.1609/aaai.v36i10.21353,,,,,,,0,0.0,0.0,2.0,1.0,"There is growing evidence that the classical notion of adversarial robustness originally introduced for images has been adopted as a de facto standard by a large part of the NLP research community.  We show that this notion is problematic in the context of NLP as it considers a narrow spectrum of linguistic phenomena. In this paper, we argue for semantic robustness, which is better aligned with the human concept of linguistic fidelity. We characterize semantic robustness in terms of biases that it is expected to induce in a model. We study semantic robustness of a range of vanilla and robustly trained architectures using a template-based generative test bed. We complement the analysis with empirical evidence that, despite being harder to implement, semantic robustness can improve performance %gives guarantees for on complex linguistic phenomena where models robust in the classical sense fail.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['nlp', 'robust']"
No,"explainability, survey",Toward Interpretable Machine Learning: Transparent Deep Neural Networks and Beyond,64.0,"W. Samek, G. Montavon, S. Lapuschkin, Christopher J. Anders, K. Müller",2020.0,,,,,57.0,2022-07-13 09:22:33,,,,,,,,,64,32.0,13.0,5.0,2.0,"With the broader and highly successful usage of machine learning in industry and the sciences, there has been a growing demand for explainable AI. Interpretability and explanation methods for gaining a better understanding about the problem solving abilities and strategies of nonlinear Machine Learning such as Deep Learning (DL), LSTMs, and kernel methods are therefore receiving increased attention. In this work we aim to (1) provide a timely overview of this active emerging field and explain its theoretical foundations, (2) put interpretability algorithms to a test both from a theory and comparative evaluation perspective using extensive simulations, (3) outline best practice aspects i.e. how to best include interpretation methods into the standard usage of machine learning and (4) demonstrate successful usage of explainable AI in a representative selection of application scenarios. Finally, we discuss challenges and possible future directions of this exciting foundational field of machine learning.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'survey']"
Discussion,evaluation of explanations,Towards Robust Interpretability with Self-Explaining Neural Networks,0.0,"David Alvarez-Melis, Tommi S. Jaakkola",2018.0,Advances In Neural Information Processing Systems 31 (Nips 2018),,,,58.0,2022-07-14 15:50:46,Proceedings Paper,,1049-5258,,31.0,,,,0,0.0,0.0,2.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'explainability']"
Yes,"Adversarial Attacks, NLP, Embedding",Robust Textual Embedding against Word-level Adversarial Attacks,1.0,"Y Yang, X Wang, K He",2022.0,arXiv preprint arXiv:2202.13817,arxiv.org,https://arxiv.org/abs/2202.13817,https://scholar.google.com/scholar?cites=1427796438267745155&as_sdt=2005&sciodt=2007&hl=en,58.0,2022-07-13 17:19:08,,,,,,,,,1,1.0,0.0,3.0,1.0,"… , we could train a robust model against the synonym substitutions based adversarial attacks. … In this way, we could train a robust model that has similar representations for similar input …",https://arxiv.org/pdf/2202.13817,https://scholar.google.com/scholar?q=related:g1OEukuN0BMJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'embedding', 'nlp']"
Yes,"Adversarial Robustness, Out-of-Domain Data, Classification",Improving adversarial robustness via unlabeled out-of-domain data,5.0,"Z Deng, L Zhang, A Ghorbani…",2021.0,… on Artificial Intelligence …,proceedings.mlr.press,https://proceedings.mlr.press/v130/deng21b.html,https://scholar.google.com/scholar?cites=15114495224915167133&as_sdt=2005&sciodt=2007&hl=en,58.0,2022-07-12 11:56:54,,,,,,,,,5,5.0,1.0,4.0,1.0,"… Next, we show that in certain settings, one can achieve even better adversarial robustness when the unlabeled data comes from a shifted domain rather than the same domain as the …",http://proceedings.mlr.press/v130/deng21b/deng21b.pdf,https://scholar.google.com/scholar?q=related:nQdouFN5wdEJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'classification', 'out-of-distribution data']"
Yes,"Data Augmentation, Generalization, Robustness",Maximum-entropy adversarial data augmentation for improved generalization and robustness,43.0,"L Zhao, T Liu, X Peng…",2020.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2020/hash/a5bfc9e07964f8dddeb95fc584cd965d-Abstract.html,https://scholar.google.com/scholar?cites=7615895385729702903&as_sdt=2005&sciodt=2007&hl=en,59.0,2022-07-12 11:56:54,,,,,,,,,43,21.5,11.0,4.0,2.0,"… In this paper, our main idea is to incorporate the IB principle into adversarial data augmentation so as to improve model robustness to large domain shifts. We start by adapting the IB …",https://proceedings.neurips.cc/paper/2020/file/a5bfc9e07964f8dddeb95fc584cd965d-Paper.pdf,https://scholar.google.com/scholar?q=related:95v0qIAYsWkJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['data augmentation', 'generalization', 'robust']"
No,"Adversarial Perturbations, Deep Learning, Robust",On the robustness of skeleton detection against adversarial attacks,2.0,X. Bai,2020.0,Neural Networks,,https://api.elsevier.com/content/article/eid/1-s2.0-S0893608020303415,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85092035349&origin=inward,59.0,2022-07-12 16:26:29,Article,10.1016/j.neunet.2020.09.018,0893-6080,https://api.elsevier.com/content/abstract/scopus_id/85092035349,132.0,,416.0,427.0,2,1.0,2.0,1.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'deep learning', 'robust']"
Yes,"Graph Neural Network, Robust",Expressive 1-lipschitz neural networks for robust multiple graph learning against adversarial attacks,8.0,"X Zhao, Z Zhang, Z Zhang, L Wu, J Jin…",2021.0,International …,proceedings.mlr.press,http://proceedings.mlr.press/v139/zhao21e.html,https://scholar.google.com/scholar?cites=13407308401883474864&as_sdt=2005&sciodt=2007&hl=en,59.0,2022-07-13 17:19:08,,,,,,,,,8,8.0,1.0,6.0,1.0,… expressive and robust multiple graph learning against adversarial attacks. … robust graph learning models and Lipschitz-bound neural architectures. We validate that the proposed robust …,http://proceedings.mlr.press/v139/zhao21e/zhao21e.pdf,https://scholar.google.com/scholar?q=related:sOtf4PRTELoJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['graph neural network', 'robust']"
Yes,"Speech Recognition, Robust, Noise",Robustness to noise for speech emotion classification using CNNs and attention mechanisms,8.0,"L Wijayasingha, JA Stankovic",2021.0,Smart Health,Elsevier,https://www.sciencedirect.com/science/article/pii/S235264832030057X,https://scholar.google.com/scholar?cites=7146956485966650285&as_sdt=2005&sciodt=2007&hl=en,59.0,2022-07-13 14:41:59,,,,,,,,,8,8.0,4.0,2.0,1.0,"… effects of noise on CNN (Convolutional Neural Network) … noise types at different noise levels with signal to noise ratios of 10,15,20,25,30 and 35. We show that the noise robustness of …",https://www.cs.virginia.edu/~stankovic/psfiles/CHASE_paper_finalLahiru.pdf,https://scholar.google.com/scholar?q=related:rVs3yw4XL2MJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['noise', 'robust', 'time series']"
Yes,"Interpretability, explanations",On the robustness of interpretability methods,290.0,"D Alvarez-Melis, TS Jaakkola",2018.0,arXiv preprint arXiv:1806.08049,arxiv.org,https://arxiv.org/abs/1806.08049,https://scholar.google.com/scholar?cites=11950983066902532220&as_sdt=2005&sciodt=2007&hl=en,59.0,2022-07-12 11:49:54,,,,,,,,,290,72.5,145.0,2.0,4.0,… The notion of robustness we seek concerns variations of a prediction’s “explanation” with … much—then we would hope that the explanation provided by the interpretability method for that …,https://arxiv.org/pdf/1806.08049,https://scholar.google.com/scholar?q=related:fBBvw8dr2qUJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'interpretability']"
Yes,"Adversarial, Performance",A Simple Way to Make Neural Networks Robust Against Diverse Image Corruptions,97.0,"E. Rusak, Lukas Schott, Roland S. Zimmermann, Julian Bitterwolf, O. Bringmann, M. Bethge, Wieland Brendel",2020.0,,,,,60.0,2022-07-13 10:11:42,,10.1007/978-3-030-58580-8_4,,,,,,,97,48.5,14.0,7.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'performance']"
Yes,"adversarial robustness, neural networks, traditional classifiers",Training Robust Models Using Random Projection,0.0,"Nguyen Xuan Vinh, Sarah Erfani, Sakrapee Paisitkriangkrai, James Bailey, Christopher Leckie, Kotagiri Ramamohanarao, IEEE",2016.0,2016 23rd International Conference On Pattern Recognition (Icpr),,,,60.0,2022-07-13 13:36:14,Proceedings Paper,,1051-4651,,,,531.0,536.0,0,0.0,0.0,7.0,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'classification', 'neural networks']"
Yes,"Federated Learning, Adversarial, Robust",Curse or redemption? how data heterogeneity affects the robustness of federated learning,9.0,"S Zawad, A Ali, PY Chen, A Anwar, Y Zhou…",2021.0,… on Artificial Intelligence,ojs.aaai.org,https://ojs.aaai.org/index.php/AAAI/article/view/17291,https://scholar.google.com/scholar?cites=9019067838784609622&as_sdt=2005&sciodt=2007&hl=en,61.0,2022-07-12 11:55:13,,,,,,,,,9,9.0,2.0,6.0,1.0,"… but often overlooked in the lens of robustness to adversarial attacks. This paper focuses on … the attack less efficient, more challenging to design effective attack strategies, and the attack …",https://ojs.aaai.org/index.php/AAAI/article/view/17291/17098,https://scholar.google.com/scholar?q=related:Vh3oD2oqKn0J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'federated learning', 'robust']"
Discussion,"robust, adversarial",How to Make Machine Learning Robust Against Adversarial Inputs,2.0,"G. Muela, C. Servin, V. Kreinovich",2016.0,,,,,61.0,2022-07-13 09:38:44,,,,,,,,,2,0.33,1.0,3.0,6.0,"It has been recently shown that it is possible to “cheat” many machine learning algorithms – i.e., to perform minor modifications of the inputs that would lead to a wrong classification. This feature can be used by adversaries to avoid spam detection, to create a wrong identification allowing access to classified information, etc. In this paper, we propose a solution to this problem: namely, instead of applying the original machine learning algorithm to the original inputs, we should first perform a random modification of these inputs. Since machine learning algorithms perform well on random data, such a random modification ensures us that the algorithm will, with a high probability, work correctly on the modified inputs. An additional advantage of this idea is that it also provides an additional privacy protection. 1 Adversarial Inputs to Machine Learning Algorithms: Formulation of the Problem Machine learning algorithms have been very successful. Machine learning algorithms allow us, based on the known examples of different phenomena, to develop a general algorithm for detecting this phenomenon; see, e.g., [1]. For example, when presented with data from different patients with different diagnoses, machine learning can help diagnose new patients. When presented with",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
Yes,"Face Recognition, Open Environment, Robust",On the Robustness of Deep Learning Based Face Recognition,1.0,"W Bailer, M Winter",2019.0,Proceedings of the 1st International Workshop on AI …,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3347449.3357483,https://scholar.google.com/scholar?cites=12657093507401411869&as_sdt=2005&sciodt=2007&hl=en,61.0,2022-07-13 10:04:11,,10.1145/3347449.3357483,,,,,,,1,0.33,1.0,2.0,3.0,… of FaceNet features with incremental machine learning approaches to automatically train … the known faces (FPk ) is low and most of the faces are erroneously classified as unknowns. …,https://www.researchgate.net/profile/Werner-Bailer/publication/336929115_On_the_Robustness_of_Deep_Learning_Based_Face_Recognition/links/5f2a661392851cd302dc1347/On-the-Robustness-of-Deep-Learning-Based-Face-Recognition.pdf,https://scholar.google.com/scholar?q=related:HW2QHXwHp68J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22unknowns%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'open environment', 'robust']"
Yes,"CNN, adversarial noise",Robust convolutional neural networks under adversarial noise,61.0,"J Jin, A Dundar, E Culurciello",2015.0,arXiv preprint arXiv:1511.06306,arxiv.org,https://arxiv.org/abs/1511.06306,https://scholar.google.com/scholar?cites=15379544080822759515&as_sdt=2005&sciodt=2007&hl=en,61.0,2022-07-14 12:23:15,,,,,,,,,61,9.11,20.0,3.0,7.0,"… The main contribution of this work is to propose a robust feedforward CNN model under adversarial noise; a noise that affects the performance the most. In order to achieve this, we add …",https://arxiv.org/pdf/1511.06306,https://scholar.google.com/scholar?q=related:W_DmcNsdb9UJ:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'noise']"
Yes,"robust, benchmark, out-of-distribution",Imagenet performance correlates with pose estimation robustness and generalization on out-of-domain data,4.0,"A Mathis, T Biasi, Y Mert, B Rogers…",2020.0,… on Machine Learning,gatsby.ucl.ac.uk,http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-047.pdf,https://scholar.google.com/scholar?cites=11777146032531642842&as_sdt=2005&sciodt=2007&hl=en,61.0,2022-07-13 11:16:49,PDF,,,,,,,,4,2.0,1.0,5.0,2.0,"… To probe the impact of ImageNet performance on pose estimation robustness, we selected … performance (see Methods; 13 models spanning from 60% to 84% ImageNet performance). …",http://www.gatsby.ucl.ac.uk/~balaji/udl2020/accepted-papers/UDL2020-paper-047.pdf,https://scholar.google.com/scholar?q=related:2hnzKOvTcKMJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22performance%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['benchmark', 'out-of-distribution data', 'robust']"
Yes,"Graph Neural Network, Noise, Dataset",How robust are graph neural networks to structural noise?,13.0,"J Fox, S Rajamanickam",2019.0,arXiv preprint arXiv:1912.10206,arxiv.org,https://arxiv.org/abs/1912.10206,https://scholar.google.com/scholar?cites=11190653749781891306&as_sdt=2005&sciodt=2007&hl=en,63.0,2022-07-14 11:17:40,,,,,,,,,13,4.33,7.0,2.0,3.0,"… We also show that the same GNN model is not robust to addition of structural noise, through a … -augmented training is capable of significantly improving robustness to structural noise. …",https://arxiv.org/pdf/1912.10206,https://scholar.google.com/scholar?q=related:6ogkeEswTZsJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['dataset', 'graph neural network', 'noise']"
Yes,"neural networks, robustness to label noise",A Spectral Perspective of DNN Robustness to Label Noise,1.0,"O Bar, A Drory, R Giryes",2022.0,… on Artificial Intelligence and …,proceedings.mlr.press,https://proceedings.mlr.press/v151/bar22a.html,https://scholar.google.com/scholar?cites=1068088145612023327&as_sdt=2005&sciodt=2007&hl=en,63.0,2022-07-12 11:49:54,,,,,,,,,1,1.0,0.0,3.0,1.0,"… Note though that the robustness of the neural networks to label … the robustness of networks to such types of label noise. … Following that, we show that further robustness to label noise is …",https://proceedings.mlr.press/v151/bar22a/bar22a.pdf,https://scholar.google.com/scholar?q=related:H6YCbIKc0g4J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['label noise', 'neural networks']"
Yes,"Computer Vision, Centroid, Noise",Robustness of Supervised Learning Based on Combined Centroids,0.0,"J Kalina, C Matonoha",2021.0,International Conference on Artificial Intelligence …,Springer,https://link.springer.com/chapter/10.1007/978-3-030-87897-9_16,,63.0,2022-07-12 13:50:33,,10.1007/978-3-030-87897-9_16,,,,,,,0,0.0,0.0,2.0,1.0,"… interested in studying the robustness of the method to … artificially modified by additional noise, occlusion, changed … out to ensure robustness with respect to the presence of noise or …",,https://scholar.google.com/scholar?q=related:VcqYCQQN11AJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['clustering', 'computer vision', 'noise']"
Yes,"Data Augmentation, Out-of-Distribution Data, Robust",Memo: Test time robustness via adaptation and augmentation,8.0,"M Zhang, S Levine, C Finn",2021.0,arXiv preprint arXiv:2110.09506,arxiv.org,https://arxiv.org/abs/2110.09506,https://scholar.google.com/scholar?cites=1448618539109048791&as_sdt=2005&sciodt=2007&hl=en,63.0,2022-07-13 10:25:08,,,,,,,,,8,8.0,3.0,3.0,1.0,"… performance on many machine learning problems, such as … information for improving model robustness. Recently, several … Conversely, as an objective that encourages confidence but …",https://arxiv.org/pdf/2110.09506,https://scholar.google.com/scholar?q=related:1-Gj3uKGGhQJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['data augmentation', 'out-of-distribution data', 'robust']"
No,"RNN, adversarial robustness, robustness to distribution shifts, fairness",Towards Robust and Reliable Machine Learning: Theory and Algorithms,0.0,Z Tu,2022.0,,ses.library.usyd.edu.au,https://ses.library.usyd.edu.au/handle/2123/28832,,64.0,2022-07-14 11:02:11,,,,,,,,,0,0.0,0.0,1.0,1.0,"… topics on robustness and reliability in machine learning, with a focus … robust generalization properties of machine learning models. For distributional shift, we focus on learning a robust …",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'distribution shift', 'fairness', 'recurrent neural network']"
Yes,"Decision trees, adversarial examples",Robust decision trees against adversarial examples,74.0,"H Chen, H Zhang, D Boning…",2019.0,… Conference on Machine …,proceedings.mlr.press,http://proceedings.mlr.press/v97/chen19m.html?ref=https://githubhelp.com,https://scholar.google.com/scholar?cites=18298482644739407816&as_sdt=2005&sciodt=2007&hl=en,64.0,2022-07-13 17:19:08,,,,,,,,,74,25.07.00,19.0,4.0,3.0,"… In our paper, we shed light on the adversarial robustness of an important class of machine … models under adversarial attacks, and more importantly, we propose a novel robust training …",http://proceedings.mlr.press/v97/chen19m/chen19m.pdf,https://scholar.google.com/scholar?q=related:yL9XEARF8f0J:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'decision trees']"
Yes,"Interpretability, Optimization, Adversarial Attacks",Learning Interpretable Features via Adversarially Robust Optimization,3.0,A. Khakzar,2019.0,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85075822214&origin=inward,64.0,2022-07-12 16:28:40,Conference Paper,10.1007/978-3-030-32226-7_88,0302-9743,https://api.elsevier.com/content/abstract/scopus_id/85075822214,11769.0,,793.0,800.0,3,1.0,3.0,1.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'interpretability', 'optimization']"
Yes,"Robust, Machine Translations, Transformer Network",Improving Robustness of Neural Machine Translation with Multi-task Learning,15.0,"Shuyan Zhou, Xiangkai Zeng, Yingqi Zhou, Antonios Anastasopoulos, Graham Neubig",2019.0,,,,,64.0,2022-07-13 09:24:04,,10.18653/v1/W19-5368,,,,,,,15,5.0,3.0,5.0,3.0,"While neural machine translation (NMT) achieves remarkable performance on clean, in-domain text, performance is known to degrade drastically when facing text which is full of typos, grammatical errors and other varieties of noise. In this work, we propose a multi-task learning algorithm for transformer-based MT systems that is more resilient to this noise. We describe our submission to the WMT 2019 Robustness shared task based on this method. Our model achieves a BLEU score of 32.8 on the shared task French to English dataset, which is 7.1 BLEU points higher than the baseline vanilla transformer trained with clean text.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['nlp', 'robust', 'transformer network']"
Yes,"Speech Recognition, Noise, Reconstruction",Improving noise robustness of contrastive speech representation learning with speech reconstruction,2.0,"H Wang, Y Qian, X Wang, Y Wang…",2022.0,ICASSP 2022-2022 …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9746220/,https://scholar.google.com/scholar?cites=9337900834304000717&as_sdt=2005&sciodt=2007&hl=en,64.0,2022-07-13 14:41:59,,,,,,,,,2,2.0,0.0,5.0,1.0,"… of suppressing background noise with a conventional cascaded pipeline, we employ a noise-… The reconstruction module is used for auxiliary learning to improve the noise robustness of …",https://arxiv.org/pdf/2110.15430,https://scholar.google.com/scholar?q=related:zYKDoFXjloEJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['noise', 'reconstruction', 'time series']"
Yes,"Classifier, Robust",Adding Gaussian Noise to DeepFool for Robustness based on Perturbation Directionality.,2.0,"T Xie, Y Li",2019.0,Aust. J. Intell. Inf. Process. Syst.,ajiips.com.au,http://ajiips.com.au/papers/V16.3/v16n3_46-55.pdf,https://scholar.google.com/scholar?cites=15867645659041780644&as_sdt=2005&sciodt=2007&hl=en,64.0,2022-07-12 13:50:33,PDF,,,,,,,,2,1.07,1.0,2.0,3.0,"… It is the core of artificial intelligence and a fundamental way to make the computer more intelligent. For machine learning, there are two main tasks: classification and regression [14]. …",http://ajiips.com.au/papers/V16.3/v16n3_46-55.pdf,https://scholar.google.com/scholar?q=related:pFP0EKgzNdwJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classifier', 'robust']"
Yes,"Classification, Matrix Factorization, Robust",Hyperspectral image unsupervised classification by robust manifold matrix factorization,161.0,L. Zhang,2019.0,Information Sciences,,https://api.elsevier.com/content/article/eid/1-s2.0-S0020025519301094,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85061389947&origin=inward,65.0,2022-07-12 16:33:33,Article,10.1016/j.ins.2019.02.008,0020-0255,https://api.elsevier.com/content/abstract/scopus_id/85061389947,485.0,,154.0,169.0,161,54.07.00,161.0,1.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'matrix factorization', 'robust']"
Yes,"Accuracy, Adversarial Perturbations",Revisiting adversarial risk,21.0,"AS Suggala, A Prasad, V Nagarajan…",2019.0,… Artificial Intelligence …,proceedings.mlr.press,http://proceedings.mlr.press/v89/suggala19a.html,https://scholar.google.com/scholar?cites=16626337542332403058&as_sdt=2005&sciodt=2007&hl=en,65.0,2022-07-13 17:19:08,,,,,,,,,21,7.0,5.0,4.0,3.0,"… Specifically, they show that no classifier can simultaneously be robust to adversarial perturbations and achieve high standard test accuracy. However, this is contrary to the standard …",http://proceedings.mlr.press/v89/suggala19a/suggala19a.pdf,https://scholar.google.com/scholar?q=related:cs12luedvOYJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial perturbations']"
Yes,"Feature, Robust",Detecting Robust Co-Saliency with Recurrent Co-Attention Neural Network.,33.0,"B Li, Z Sun, L Tang, Y Sun, J Shi",2019.0,IJCAI,ijcai.org,https://www.ijcai.org/proceedings/2019/0115.pdf,https://scholar.google.com/scholar?cites=17675793410944930864&as_sdt=2005&sciodt=2007&hl=en,65.0,2022-07-14 12:48:27,PDF,,,,,,,,33,11.0,7.0,5.0,3.0,"… the group to gradually learn the robust group representation and … to facilitate the robust cosaliency reasoning. Moreover, to … superiority of our approach in robust co-saliency detection as …",https://www.ijcai.org/proceedings/2019/0115.pdf,https://scholar.google.com/scholar?q=related:MBSQ9nQITfUJ:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22reasoning%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['features', 'robust']"
Yes,"human-in-the-loop, unknown unknowns",Towards hybrid human-AI workflows for unknown unknown detection,13.0,"A Liu, S Guerra, I Fung, G Matute, E Kamar…",2020.0,Proceedings of The …,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3366423.3380306,https://scholar.google.com/scholar?cites=11332481256639619541&as_sdt=2005&sciodt=2007&hl=en,65.0,2022-07-13 12:21:07,,10.1145/3366423.3380306,,,,,,,13,6.5,2.0,6.0,2.0,… of an intermediate layer of the neural network. The idea of using … We test the robustness of pattern expansion by using the … future research on human computation algorithms that enable …,,https://scholar.google.com/scholar?q=related:1eHGLakPRZ0J:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22human+computation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['hci', 'unknown unknowns']"
Yes,"Convolutional Neural Network, Defense, Adversarial Attacks",Improving Network Robustness against Adversarial Attacks with Compact Convolution,11.0,"Rajeev Ranjan, S. Sankaranarayanan, Carlos D. Castillo, R. Chellappa",2017.0,,,,,67.0,2022-07-13 09:26:34,,,,,,,,,11,2.2,3.0,4.0,5.0,"Though Convolutional Neural Networks (CNNs) have surpassed human-level performance on tasks such as object classification and face verification, they can easily be fooled by adversarial attacks. These attacks add a small perturbation to the input image that causes the network to mis-classify the sample. In this paper, we focus on neutralizing adversarial attacks by exploring the effect of different loss functions such as CenterLoss and L2-Softmax Loss for enhanced robustness to adversarial perturbations. Additionally, we propose power convolution, a novel method of convolution that when incorporated in conventional CNNs improve their robustness. Power convolution ensures that features at every layer are bounded and close to each other. Extensive experiments show that Power Convolutional Networks (PCNs) neutralize multiple types of attacks, and perform better than existing methods for defending adversarial attacks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'convolutional neural network', 'defense']"
Yes,adversarial robustness,Towards efficient data-centric robust machine learning with noise-based augmentation,2.0,"X Liu, H Wang, Y Zhang, F Wu, S Hu",2022.0,arXiv preprint arXiv:2203.03810,arxiv.org,https://arxiv.org/abs/2203.03810,https://scholar.google.com/scholar?cites=6591865772393088711&as_sdt=2005&sciodt=2007&hl=en,67.0,2022-07-14 09:04:21,,,,,,,,,2,2.0,0.0,5.0,1.0,"… which is composed of Gaussian Noise, Salt-and-Pepper noise, and the PGD adversarial … In addition, we share our insights about the data-centric robust machine learning gained …",https://arxiv.org/pdf/2203.03810,https://scholar.google.com/scholar?q=related:x8aLqv4Ce1sJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['adversarial robustness']
Yes,"Robust, Pruning, Deep Neural Network, Classifier",DNR: A Tunable Robust Pruning Framework Through Dynamic Network Rewiring of DNNs,19.0,"Souvik Kundu, M. Nazemi, P. Beerel, M. Pedram",2020.0,,,,,68.0,2022-07-13 10:08:07,,10.1145/3394885.3431542,,,,,,,19,9.5,5.0,4.0,2.0,"This paper presents a dynamic network rewiring (DNR) method to generate pruned deep neural network (DNN) models that are robust against adversarial attacks yet maintain high accuracy on clean images. In particular, the disclosed DNR method is based on a unified constrained optimization formulation using a hybrid loss function that merges ultra-high model compression with robust adversarial training. This training strategy dynamically adjusts inter-layer connectivity based on per-layer normalized momentum computed from the hybrid loss function. In contrast to existing robust pruning frameworks that require multiple training iterations, the proposed learning strategy achieves an overall target pruning ratio with only a single training iteration and can be tuned to support both irregular and structured channel pruning. To evaluate the merits of DNR, experiments were performed with two widely accepted models, namely VGG16 and ResNet-18, on CIFAR-10, CIFAR-100 as well as with VGG16 on Tiny-ImageNet. Compared to the baseline un-compressed models, DNR provides over 20× compression on all the datasets with no significant drop in either clean or adversarial classification accuracy. Moreover, our experiments show that DNR consistently finds compressed models with better clean and adversarial image classification performance than what is achievable through state-of-the-art alternatives. Our models and test codes are available at https://github.com/ksouvik52/DNR_ASP_DAC2021.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classifier', 'deep learning', 'pruning', 'robust']"
Yes,"Resilience, Convolutional Neural Network, Classification",Fruit-classification model resilience under adversarial attack,1.0,R. Siddiqi,2022.0,SN Applied Sciences,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85121564974&origin=inward,68.0,2022-07-12 16:29:05,Article,10.1007/s42452-021-04917-6,2523-3971,https://api.elsevier.com/content/abstract/scopus_id/85121564974,4.0,1.0,,,1,1.0,1.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'convolutional neural network', 'resilience']"
Yes,"Visual Question Answering, Robust, Measure",A novel framework for robustness analysis of visual QA models,9.0,J.H. Huang,2019.0,"33rd AAAI Conference on Artificial Intelligence, AAAI 2019, 31st Innovative Applications of Artificial Intelligence Conference, IAAI 2019 and the 9th AAAI Symposium on Educational Advances in Artificial Intelligence, EAAI 2019",,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85086889377&origin=inward,68.0,2022-07-12 16:24:02,Conference Paper,,,https://api.elsevier.com/content/abstract/scopus_id/85086889377,,,8449.0,8456.0,9,3.0,9.0,1.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['metric', 'robust', 'visual question answering']"
Yes,"Robust, Domain Adaptation, Feature Alignment",Limitations of Post-Hoc Feature Alignment for Robustness,7.0,"Collin Burns, J. Steinhardt",2021.0,,,,,69.0,2022-07-13 09:26:01,,10.1109/CVPR46437.2021.00255,,,,,,,7,7.0,4.0,2.0,1.0,"Feature alignment is an approach to improving robustness to distribution shift that matches the distribution of feature activations between the training distribution and test distribution. A particularly simple but effective approach to feature alignment involves aligning the batch normalization statistics between the two distributions in a trained neural network. This technique has received renewed interest lately because of its impressive performance on robustness benchmarks. However, when and why this method works is not well understood. We investigate the approach in more detail and identify several limitations. We show that it only significantly helps with a narrow set of distribution shifts and we identify several settings in which it even degrades performance. We also explain why these limitations arise by pinpointing why this approach can be so effective in the first place. Our findings call into question the utility of this approach and Unsupervised Domain Adaptation more broadly for improving robustness in practice.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['domain adaptation', 'features', 'robust']"
Yes,"natural language inference, robust features",Supervising model attention with human explanations for robust natural language inference,6.0,"J Stacey, Y Belinkov, M Rei",2022.0,… AAAI Conference on Artificial Intelligence,ojs.aaai.org,https://ojs.aaai.org/index.php/AAAI/article/view/21386,https://scholar.google.com/scholar?cites=3467923394867101821&as_sdt=2005&sciodt=2007&hl=en,69.0,2022-07-13 15:31:15,,,,,,,,,6,6.0,2.0,3.0,1.0,"… Creating More Robust NLI Models Previous work on creating more robust NLI models has fo… humans believe are important, creating more robust and better performing NLI models. …",https://ojs.aaai.org/index.php/AAAI/article/download/21386/21135,https://scholar.google.com/scholar?q=related:fQBSK_SJIDAJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['features', 'nlp']"
Yes,"adversarial robustness, mitigation method",Directional adversarial training for cost sensitive deep learning classification applications,9.0,M. Terzi,2020.0,Engineering Applications of Artificial Intelligence,,https://api.elsevier.com/content/article/eid/1-s2.0-S0952197620300452,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85080100070&origin=inward,69.0,2022-07-12 16:24:02,Article,10.1016/j.engappai.2020.103550,0952-1976,https://api.elsevier.com/content/abstract/scopus_id/85080100070,91.0,,,,9,4.5,9.0,1.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'mitigation method']"
Yes,"Noise, Feature Selection, Dataset",Robust Unsupervised Feature Selection Based on Sparse Reconstruction of Learned Clean Data,0.0,"Jingjing Lu, Shuangyan Yi, Yongsheng Liang, Wei Liu, Jiaoyan Zhao, Qiangqiang Shen",2021.0,,,,,70.0,2022-07-13 09:24:29,,10.1109/CCISP52774.2021.9639338,,,,,,,0,0.0,0.0,6.0,1.0,"Unsupervised feature selection is an essential task in machine learning. Current methods select representative features under slight noise interference. However, their performance is degraded when the data is grossly corrupted. To fundamentally improve the robustness, we propose a robust unsupervised feature selection method based on the sparse reconstruction of learned clean data(SRCD). In this method, we choose the clean data learned by the low-rank constrain as the reconstruction object. Using the noiseless data, SRCD is then capable of obtaining effective features. To further enhance the interpretability, the projection matrix in the reconstruction term is constrained to column sparse patterns for consistent feature selection. Experiment results based on the benchmark and noisy data confirm the effectiveness of the proposed method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['dataset', 'features', 'noise']"
Yes,"Classifier, Adversarial, Robust",Are Generative Classifiers More Robust to Adversarial Attacks?,61.0,"Yingzhen Li, Yash Sharma",2018.0,,,,,70.0,2022-07-13 10:08:07,,,,,,,,,61,15.25,31.0,2.0,4.0,"There is a rising interest in studying the robustness of deep neural network classifiers against adversaries, with both advanced attack and defence techniques being actively developed. However, most recent work focuses on discriminative classifiers which only models the conditional distribution of the labels given the inputs. In this abstract we propose deep Bayes classifier that improves the classical naive Bayes with conditional deep generative models, and verifies its robustness against a number of existing attacks. We further developed a detection method for adversarial examples based on conditional deep generative models. Our initial results on MNIST suggest that deep Bayes classifiers might be more robust when compared with deep discriminative classifiers, and the proposed detection method achieves high detection rates against two commonly used attacks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'classifier', 'robust']"
Yes,"Robust, Spurious Correlation",Correct-N-Contrast: A Contrastive Approach for Improving Robustness to Spurious Correlations,2.0,"M Zhang, NS Sohoni, HR Zhang, C Finn…",2022.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2203.01517,https://scholar.google.com/scholar?cites=8960959356014477531&as_sdt=2005&sciodt=2007&hl=en,71.0,2022-07-13 14:36:56,,,,,,,,,2,2.0,0.0,5.0,1.0,… CnC can be used to train any standard classification model architecture; for any given neural network we just apply different optimization objectives to the encoder and classifier layers. …,https://arxiv.org/pdf/2203.01517,https://scholar.google.com/scholar?q=related:28D0nQ65W3wJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22accountability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['robust', 'spurious correlation']"
Yes,"Graph Neural Network, Robust",Certifiable Robustness and Robust Training for Graph Convolutional Networks,0.0,"Daniel Zuegner, Stephan Guennemann, Assoc Comp Machinery",2019.0,Kdd'19: Proceedings Of The 25th Acm Sigkdd International Conferencce On Knowledge Discovery And Data Mining,,,,71.0,2022-07-13 15:27:05,Proceedings Paper,10.1145/3292500.3330905,,,,,246.0,256.0,0,0.0,0.0,3.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['graph neural network', 'robust']"
No,"robustness to label noise, neural networks",Toward Robustness against Label Noise in Training Deep Discriminative Neural Networks,125.0,A. Vahdat,2017.0,Advances in Neural Information Processing Systems,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85047006830&origin=inward,72.0,2022-07-12 16:29:30,Conference Paper,,1049-5258,https://api.elsevier.com/content/abstract/scopus_id/85047006830,2017.0,,5597.0,5606.0,125,25.0,125.0,1.0,5.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['label noise', 'neural networks']"
No,"Extreme Learning Machine, Robust",A robust AdaBoost. RT based ensemble extreme learning machine,24.0,"P Zhang, Z Yang",2015.0,Mathematical Problems in Engineering,hindawi.com,https://www.hindawi.com/journals/mpe/2015/260970/,https://scholar.google.com/scholar?cites=15658661859528355282&as_sdt=2005&sciodt=2007&hl=en,72.0,2022-07-14 10:47:24,HTML,,,,,,,,24,3.43,12.0,2.0,7.0,"… stability and accuracy of ELM shall be further enhanced. In this paper, a new hybrid machine learning method called robust … combined ELM with the novel robust AdaBoost.RT algorithm …",https://www.hindawi.com/journals/mpe/2015/260970/,https://scholar.google.com/scholar?q=related:0i0lg_-9TtkJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22stability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['extreme learning machine', 'robust']"
Yes,"robust, adversarial, learning",Feedback learning for improving the robustness of neural networks,6.0,"C Song, Z Wang, H Li",2019.0,… Conference On Machine Learning And …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8999302/?casa_token=WudACo0H_6AAAAAA:Fr8DjZvfKjxi_FN25-8wnStHS2iPaxU5Gjt-QQlfZwMhvd1zrUEwiJ9N1p2Fjqo5zZTN_qw9lQ,https://scholar.google.com/scholar?cites=18436610087433061062&as_sdt=2005&sciodt=2007&hl=en,72.0,2022-07-13 09:30:22,,,,,,,,,6,2.0,2.0,3.0,3.0,"… , the robustness of neural … robustness is defending not only adversarial attacks, but any types of evasion attacks that attackers may conduct. One possible solution and explanation are …",https://ieeexplore.ieee.org/iel7/8974348/8998966/08999302.pdf?casa_token=-w6GvNK26xwAAAAA:tVZOMsr2FeJryhjIwMCOno5kHg7FEh1AmEAMi77hjWT62w4jCKIOR3wxOx-EzyxWz1zNouG01A,https://scholar.google.com/scholar?q=related:xsLUczH_2_8J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'learning', 'robust']"
Yes,"Robust, Adversarial, Natural Language Processing",Can Rationalization Improve Robustness?,2.0,"H Chen, J He, K Narasimhan, D Chen",2022.0,arXiv preprint arXiv:2204.11790,arxiv.org,https://arxiv.org/abs/2204.11790,https://scholar.google.com/scholar?cites=4903102654298189324&as_sdt=2005&sciodt=2007&hl=en,73.0,2022-07-12 11:49:54,,,,,,,,,2,2.0,1.0,4.0,1.0,… models can also provide robustness to adversarial attacks in … show promise in improving robustness but struggle in certain … and robustness in the rationalize-then-predict framework.…,https://arxiv.org/pdf/2204.11790,https://scholar.google.com/scholar?q=related:DO635uJRC0QJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'nlp', 'robust']"
Discussion,"explainability, HCI",What Do We Want From Explainable Artificial Intelligence (XAI)? - A Stakeholder Perspective on XAI and a Conceptual Model Guiding Interdisciplinary XAI Research,61.0,"Markus Langer, Daniel Oster, Timo Speith, H. Hermanns, Lena Kästner, Eva Schmidt, Andreas Sesing, Kevin Baum",2021.0,,,,,74.0,2022-07-13 09:19:02,,10.1016/j.artint.2021.103473,,,,,,,61,61.0,8.0,8.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'hci']"
No,"Clustering, Accuracy, Robust",J-Score: A Robust Measure of Clustering Accuracy,1.0,"N Ahmadinejad, L Liu",2021.0,arXiv preprint arXiv:2109.01306,arxiv.org,https://arxiv.org/abs/2109.01306,https://scholar.google.com/scholar?cites=5086978653338177857&as_sdt=2005&sciodt=2007&hl=en,75.0,2022-07-14 08:45:17,,,,,,,,,1,1.0,1.0,2.0,1.0,"… Calculating overall accuracy: To quantify the overall accuracy, we aggregate Jaccard … International symposium on distributed computing and artificial intelligence; 2017: Springer. …",https://arxiv.org/pdf/2109.01306,https://scholar.google.com/scholar?q=related:QZE3HCKUmEYJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'clustering', 'robust']"
Yes,"Speech Recognition, Robust",A curriculum learning method for improved noise robustness in automatic speech recognition,52.0,"S Braun, D Neil, SC Liu",2017.0,2017 25th European Signal …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8081267/,https://scholar.google.com/scholar?cites=2374410430770139490&as_sdt=2005&sciodt=2007&hl=en,75.0,2022-07-12 13:50:33,,,,,,,,,52,10.4,17.0,3.0,5.0,… methods for improving noise robustness in recurrent neural … noise robustness when tested under a wide range of SNR levels. This work also investigates the usefulness of adding noise …,https://ieeexplore.ieee.org/iel7/8067434/8081145/08081267.pdf,https://scholar.google.com/scholar?q=related:YlEJ5bSZ8yAJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['robust', 'time series']"
Yes,"Graph Convolutional Network, Robust",A Feature-Importance-Aware and Robust Aggregator for GCN,3.0,L. Zhang,2020.0,"International Conference on Information and Knowledge Management, Proceedings",,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85095864528&origin=inward,75.0,2022-07-12 16:28:09,Conference Paper,10.1145/3340531.3411983,,https://api.elsevier.com/content/abstract/scopus_id/85095864528,,,1813.0,1822.0,3,1.5,3.0,1.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['graph neural network', 'robust']"
Yes,"Fairness, change in data",Robust fairness under covariate shift,20.0,"A Rezaei, A Liu, O Memarrast, BD Ziebart",2021.0,Proceedings of the AAAI …,ojs.aaai.org,https://ojs.aaai.org/index.php/AAAI/article/view/17135,https://scholar.google.com/scholar?cites=10571294488002446621&as_sdt=2005&sciodt=2007&hl=en,76.0,2022-07-14 10:15:05,,,,,,,,,20,20.0,5.0,4.0,1.0,… of individuals interacting with the machine learning system change. We investigate fairness … We propose an approach that obtains the predictor that is robust to the worst-case testing …,https://ojs.aaai.org/index.php/AAAI/article/view/17135/16942,https://scholar.google.com/scholar?q=related:HUVu5VPItJIJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22transparency%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['distribution shift', 'fairness']"
Yes,"Deep Learning, Classification, Adversarial Perturbations",Metrics and methods for robustness evaluation of neural networks with generative models,6.0,"I Buzhinsky, A Nerinovsky, S Tripakis",2021.0,Machine Learning,Springer,https://link.springer.com/article/10.1007/s10994-021-05994-9,https://scholar.google.com/scholar?cites=12585603600300479845&as_sdt=2005&sciodt=2007&hl=en,76.0,2022-07-13 09:30:22,HTML,10.1007/s10994-021-05994-9,,,,,,,6,6.0,2.0,3.0,1.0,… robustness are associated with the accuracy of the classifier rather than its conventional adversarial robustness… A speculative explanation of the found connection between the accuracy …,https://link.springer.com/article/10.1007/s10994-021-05994-9,https://scholar.google.com/scholar?q=related:ZZXJB8sLqa4J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'classification', 'deep learning']"
Yes,"adversarial robustness, CNN, identification method",Efficient computation of robustness of convolutional neural networks,1.0,"P Arcaini, A Bombarda, S Bonfanti…",2021.0,… Artificial Intelligence …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9564330/,https://scholar.google.com/scholar?cites=14563596704268535398&as_sdt=2005&sciodt=2007&hl=en,76.0,2022-07-12 11:55:13,,,,,,,,,1,1.0,0.0,4.0,1.0,"… In [4], we proposed a robustness definition for CNNs that … robustness analysis, implementing the robustness definition and supporting different types of input data. Computing robustness …",https://ieeexplore.ieee.org/iel7/9564290/9564099/09564330.pdf,https://scholar.google.com/scholar?q=related:Zsb5kQpKHMoJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'convolutional neural network', 'identification']"
Yes,"training, robust, adversarial",FaiR-N: Fair and Robust Neural Networks for Structured Data,2.0,"Shubham Sharma, A. Gee, D. Paydarfar, J. Ghosh",2020.0,,,,,76.0,2022-07-13 09:19:22,,10.1145/3461702.3462559,,,,,,,2,1.0,1.0,4.0,2.0,"Fairness and robustness in machine learning are crucial when individuals are subject to automated decisions made by models in high-stake domains. To promote ethical artificial intelligence, fairness metrics that rely on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias. However, fairness measures that rely on comparing the ability to achieve recourse have been relatively unexplored. In this paper, we present a novel formulation for training neural networks that considers the distance of data observations to the decision boundary such that the new objective: (1) reduces the disparity in the average ability of recourse between individuals in each protected group, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that models trained with this new objective are more fair and adversarially robust neural networks, with similar accuracies, when compared to models without it. We also investigate a trade-off between the recourse-based fairness and robustness objectives. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across protected groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse disparity across groups are considered to train fairer neural networks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust', 'training']"
No,"Neural Network, Training, Dataset",Robustness of neural networks,3.0,K KrishnaKumar,2021.0,World Congress On Neural Networks-San …,taylorfrancis.com,https://www.taylorfrancis.com/chapters/edit/10.4324/9781315784076-77/robustness-neural-networks-krishnakumar,https://scholar.google.com/scholar?cites=7208217854340084204&as_sdt=2005&sciodt=2007&hl=en,77.0,2022-07-13 14:12:47,,10.4324/9781315784076-77,,,,,,,3,3.0,3.0,1.0,1.0,… to define relative robustness of different neural network structures. … We first define robustness given a NN structure and then … Here we use the Lyapunov stability condition presented in …,,https://scholar.google.com/scholar?q=related:7LGRn_K7CGQJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22stability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['dataset', 'neural network', 'training']"
No,"neural networks, efficiency, adversarial robustness",Towards Compact and Robust Deep Neural Networks,21.0,"Vikash Sehwag, Shiqi Wang, Prateek Mittal, S. Jana",2019.0,,,,,77.0,2022-07-13 09:26:25,,,,,,,,,21,7.0,5.0,4.0,3.0,"Deep neural networks have achieved impressive performance in many applications but their large number of parameters lead to significant computational and storage overheads. Several recent works attempt to mitigate these overheads by designing compact networks using pruning of connections. However, we observe that most of the existing strategies to design compact networks fail to preserve network robustness against adversarial examples. In this work, we rigorously study the extension of network pruning strategies to preserve both benign accuracy and robustness of a network. Starting with a formal definition of the pruning procedure, including pre-training, weights pruning, and fine-tuning, we propose a new pruning method that can create compact networks while preserving both benign accuracy and robustness. Our method is based on two main insights: (1) we ensure that the training objectives of the pre-training and fine-tuning steps match the training objective of the desired robust model (e.g., adversarial robustness/verifiable robustness), and (2) we keep the pruning strategy agnostic to pre-training and fine-tuning objectives. We evaluate our method on four different networks on the CIFAR-10 dataset and measure benign accuracy, empirical robust accuracy, and verifiable robust accuracy. We demonstrate that our pruning method can preserve on average 93\% benign accuracy, 92.5\% empirical robust accuracy, and 85.0\% verifiable robust accuracy while compressing the tested network by 10$\times$.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'efficiency', 'neural networks']"
Yes,"Reliable, trustworthy, out-of-distribution dataset, human interpretable",Reliable and trustworthy machine learning for health using dataset shift detection,8.0,"C Park, A Awadalla, T Kohno…",2021.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2021/hash/17e23e50bedc63b4095e3d8204ce063b-Abstract.html,https://scholar.google.com/scholar?cites=16529249886251964832&as_sdt=2005&sciodt=2007&hl=en,77.0,2022-07-14 11:28:06,,,,,,,,,8,8.0,2.0,4.0,1.0,"… systems causal [6, 55], explainable [2, 36, 45, 34, 35], fair [3, 59, 17], robust [18, 21, 28, 19, … trustworthy AI by improving reliability and user-perceived trustworthiness of machine learning …",https://proceedings.neurips.cc/paper/2021/file/17e23e50bedc63b4095e3d8204ce063b-Paper.pdf,https://scholar.google.com/scholar?q=related:oNm2ITKxY-UJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22trustworthy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['interpretability', 'out-of-distribution data', 'reliability', 'trustworthy']"
No,"Assessment, Robust",Adversarial Robustness Assessment : Why both L 0 and L ∞ Attacks Are Necessary,4.0,"Shashank Kotyan, Danilo Vasconcellos Vargas",2021.0,,,,,77.0,2022-07-13 09:22:57,,,,,,,,,4,4.0,2.0,2.0,1.0,"There exists a vast number of adversarial attacks and defences for machine learning algorithms of various types which makes assessing the robustness of algorithms a daunting task. To make matters worse, there is an intrinsic bias in these adversarial algorithms. Here, we organise the problems faced: a) Model Dependence, b) Insufficient Evaluation, c) False Adversarial Samples, and d) Perturbation Dependent Results). Based on this, we propose a model agnostic dual quality assessment method, together with the concept of robustness levels to tackle them. We validate the dual quality assessment on state-of-the-art neural networks (WideResNet, ResNet, AllConv, DenseNet, NIN, LeNet and CapsNet) as well as adversarial defences for image classification problem. We further show that current networks and defences are vulnerable at all levels of robustness. The proposed robustness assessment reveals that depending on the metric used (i.e., L0 or L∞), the robustness may vary significantly. Hence, the duality should be taken into account for a correct evaluation. Moreover, a mathematical derivation, as well as a counter-example, suggest that L1 and L2 metrics alone are not sufficient to avoid spurious adversarial samples. Interestingly, the threshold attack of the proposed assessment is a novel L∞ black-box adversarial method which requires even less perturbation than the One-Pixel Attack (only 12% of One-Pixel Attack’s amount of perturbation) to achieve similar results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'robust']"
No,"formal verification, adversarial robustness",On robustness to adversarial examples and polynomial optimization,28.0,"P Awasthi, A Dutta…",2019.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2019/hash/107878346e1d8f8fe6af7a7a588aa807-Abstract.html,https://scholar.google.com/scholar?cites=14449715261251259195&as_sdt=2005&sciodt=2007&hl=en,77.0,2022-07-13 09:41:18,,,,,,,,,28,9.33,9.0,3.0,3.0,"… In particular, we leverage this connection to (a) design … of achieving robustness in a computationally efficient manner for these classes, (c) design efficient algorithms to certify robustness …",https://proceedings.neurips.cc/paper/2019/file/107878346e1d8f8fe6af7a7a588aa807-Paper.pdf,https://scholar.google.com/scholar?q=related:Owts5Hizh8gJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'formal verification']"
No,"Training, Out-of-Distribution, Adversarial, Robust",Adversarial-Mixup: Increasing Robustness to Out-of-Distribution Data and Reliability of Inference,0.0,"K Gwon, J Yo",2021.0,IEMEK Journal of Embedded Systems and …,koreascience.or.kr,https://www.koreascience.or.kr/article/JAKO202106957556147.page,,77.0,2022-07-13 10:50:02,,,,,,,,,0,0.0,0.0,2.0,1.0,"Detecting Out-of-Distribution (OOD) data is fundamentally required when Deep Neural Network (DNN) is applied to real-world AI such as autonomous driving. However, modern DNNs …",https://www.koreascience.or.kr/article/JAKO202106957556147.pdf,https://scholar.google.com/scholar?q=related:krl-49KCvcUJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22reliability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'out-of-distribution data', 'robust', 'training']"
Yes,"adversarial robustness, mitigation method",Distilling robust and non-robust features in adversarial examples by information bottleneck,5.0,"J Kim, BK Lee, YM Ro",2021.0,Advances in Neural Information …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2021/hash/8e5e15c4e6d09c8333a17843461041a9-Abstract.html,https://scholar.google.com/scholar?cites=5846557975157001548&as_sdt=2005&sciodt=2007&hl=en,78.0,2022-07-13 17:19:08,,,,,,,,,5,5.0,2.0,3.0,1.0,"… features affect the intermediate feature representation under adversarial perturbation, we demonstrate that the non-robust features are highly correlated with the adversarial prediction. …",https://proceedings.neurips.cc/paper/2021/file/8e5e15c4e6d09c8333a17843461041a9-Paper.pdf,https://scholar.google.com/scholar?q=related:TM31g4AlI1EJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'mitigation method']"
Yes,"Deep Learning, Robust, Adversarial Attacks",Investigating the robustness of multi-view detection to current adversarial patch threats,0.0,"B Tarchoun, AB Khalifa…",2022.0,2022 6th International …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9805870/,,80.0,2022-07-12 13:45:32,,,,,,,,,0,0.0,0.0,3.0,1.0,"… in our daily lives, the safety and reliability of their results has become of paramount … Therefore, studying these attacks has become a rapidly growing field of artificial intelligence …",https://ieeexplore.ieee.org/iel7/9805808/9805811/09805870.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'deep learning', 'robust']"
Yes,"Classification, Robust",Building a Robust Extreme Learning Machine for Classification in the Presence of Outliers,6.0,"Ana Luiza B. P. Barros, G. Barreto",2013.0,,,,,80.0,2022-07-13 09:38:31,,10.1007/978-3-642-40846-5_59,,,,,,,6,1.07,3.0,2.0,9.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'robust']"
No,"Accuracy, Robust, Fairness",Ditto: Fair and robust federated learning through personalization,120.0,"T Li, S Hu, A Beirami, V Smith",2021.0,… on Machine Learning,proceedings.mlr.press,https://proceedings.mlr.press/v139/li21h.html,https://scholar.google.com/scholar?cites=11515326237813489969&as_sdt=2005&sciodt=2007&hl=en,80.0,2022-07-13 10:14:47,,,,,,,,,120,120.0,30.0,4.0,1.0,"… robustness are two important concerns for federated learning systems. In this work, we identify that robustness … is in line with our reasoning that personalization can provide benefits for …",http://proceedings.mlr.press/v139/li21h/li21h.pdf,https://scholar.google.com/scholar?q=related:McGb1TOozp8J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22reasoning%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'fairness', 'robust']"
Yes,"classifier, evasion, robust",Enhancing robustness of machine learning systems via data transformations,155.0,"AN Bhagoji, D Cullina, C Sitawarin…",2018.0,2018 52nd Annual …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8362326/,https://scholar.google.com/scholar?cites=12153399130293984368&as_sdt=2005&sciodt=2007&hl=en,81.0,2022-07-12 13:42:35,,,,,,,,,155,39.15.00,39.0,4.0,4.0,"… We will give some intuition about why dimensionality reduction should improve resilience for SVMs. … of the 2nd ACM workshop on Security and artificial intelligence. ACM, 2009, pp. 1–4. …",https://ieeexplore.ieee.org/iel7/8359988/8362188/08362326.pdf,https://scholar.google.com/scholar?q=related:cNy2Fh6MqagJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22resilience%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classifier', 'evasion', 'robust']"
Yes,"Adversarial, Robust, Measure, Data Augmentation",Generalized but not Robust? Comparing the Effects of Data Modification Methods on Out-of-Domain Generalization and Adversarial Robustness,3.0,"T Gokhale, S Mishra, M Luo, BS Sachdeva…",2022.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2203.07653,https://scholar.google.com/scholar?cites=7761901102673929375&as_sdt=2005&sciodt=2007&hl=en,81.0,2022-07-13 14:36:56,,,,,,,,,3,3.0,1.0,5.0,1.0,… with log-loss and evaluate the robustness of each model in terms of in-domain and OOD accuracies. We also evaluate adversarial robustness by using standard PGD attacks. Results …,https://arxiv.org/pdf/2203.07653,https://scholar.google.com/scholar?q=related:n6SXtu3Pt2sJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22accountability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'data augmentation', 'metric', 'robust']"
Yes,"Adversarial, Robust, Training",Achieving model robustness through discrete adversarial training,3.0,"M Ivgi, J Berant",2021.0,arXiv preprint arXiv:2104.05062,arxiv.org,https://arxiv.org/abs/2104.05062,https://scholar.google.com/scholar?cites=3182542021014957718&as_sdt=2005&sciodt=2007&hl=en,81.0,2022-07-12 11:56:54,,,,,,,,,3,3.0,2.0,2.0,1.0,"… We consider both offline and online data augmentation and focus on improving robustness with adversarial examples. Given a training set {(xj,yj)}N j=1, offline data augmentation …",https://arxiv.org/pdf/2104.05062,https://scholar.google.com/scholar?q=related:lq5ZNRupKiwJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust', 'training']"
No,"Neural Network, Robust, Lipschitz",Improving Neural Network Robustness via Persistency of Excitation,2.0,"K Sridhar, O Sokolsky, I Lee, J Weimer",2021.0,arXiv preprint arXiv:2106.02078,arxiv.org,https://arxiv.org/abs/2106.02078,https://scholar.google.com/scholar?cites=17090006082328023650&as_sdt=2005&sciodt=2007&hl=en,81.0,2022-07-13 12:24:11,,,,,,,,,2,2.0,1.0,4.0,1.0,… that adversarial robustness can be improved by leveraging the fact that every neural network … We posit (and empirically demonstrate) that this implies increased adversarial robustness …,https://arxiv.org/pdf/2106.02078,https://scholar.google.com/scholar?q=related:YlJtMvzlK-0J:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['lipschitz', 'neural network', 'robust']"
Yes,"robustness verification, robustness to input perturbations",Tightening robustness verification of convolutional neural networks with fine-grained linear approximation,4.0,"Y Wu, M Zhang",2021.0,Proceedings of the AAAI Conference on Artificial …,ojs.aaai.org,https://ojs.aaai.org/index.php/AAAI/article/view/17388,https://scholar.google.com/scholar?cites=14067790347406194363&as_sdt=2005&sciodt=2007&hl=en,81.0,2022-07-13 14:46:28,,,,,,,,,4,4.0,2.0,2.0,1.0,… compute a lower bound for a neural network and an input. All the perturbed inputs around … A certified lower bound quantitatively describes the robustness of a neural network. …,https://ojs.aaai.org/index.php/AAAI/article/view/17388/17195,https://scholar.google.com/scholar?q=related:u7bQX8nUOsMJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22trustworthy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['perturbations', 'verification']"
Yes,"Noise, Robust, Classification",Exploiting Context for Robustness to Label Noise in Active Learning,2.0,"S Paul, S Chandrasekaran, BS Manjunath…",2020.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2010.09066,https://scholar.google.com/scholar?cites=1078588678279443821&as_sdt=2005&sciodt=2007&hl=en,81.0,2022-07-13 11:05:20,,,,,,,,,2,1.0,1.0,4.0,2.0,"… learning strategy, we analyze the robustness of different active learning techniques when combined with CNLD. Figure 4 illustrates the robustness of our proposed approach for different …",https://arxiv.org/pdf/2010.09066,https://scholar.google.com/scholar?q=related:bbWaM7Dq9w4J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'noise', 'robust']"
No,"Robust, Generalization",Counterbalancing Teacher: Regularizing Batch Normalized Models for Robustness,0.0,"S Asgari, F Khani, A Gholami, K Choi, L Tran, R Zhang",2021.0,,openreview.net,https://openreview.net/forum?id=sTkY-RVYBz,,82.0,2022-07-13 14:36:56,,,,,,,,,0,0.0,0.0,6.0,1.0,"… For an arbitrary neural network encoder F parametrized by ζ with one or more batch normalization layers, we first create a clone of the network. We refer to the cloned network as G and …",https://openreview.net/pdf?id=sTkY-RVYBz,https://scholar.google.com/scholar?q=related:CNH0oIQxVZ4J:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22accountability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generalization', 'robust']"
No,adversarial robustness,A Finer Calibration Analysis for Adversarial Robustness,4.0,"Pranjal Awasthi, Anqi Mao, M. Mohri, Yutao Zhong",2021.0,,,,,82.0,2022-07-13 09:22:57,,,,,,,,,4,4.0,1.0,4.0,1.0,"We present a more general analysis of H-calibration for adversarially robust classification. By adopting a finer definition of calibration, we can cover settings beyond the restricted hypothesis sets studied in previous work. In particular, our results hold for most common hypothesis sets used in machine learning. We both fix some previous calibration results (Bao et al., 2020) and generalize others (Awasthi et al., 2021). Moreover, our calibration results, combined with the previous study of consistency by Awasthi et al. (2021), also lead to more general H-consistency results covering common hypothesis sets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['adversarial robustness']
Yes,"Interpretability, Accuracy, Classification",Leveraging Model Interpretability and Stability to increase Model Robustness,0.0,"Wu Fei, CentraleSupélec",2019.0,,,,,82.0,2022-07-13 09:28:50,,,,,,,,,0,0.0,0.0,2.0,3.0,"State of the art Deep Neural Networks (DNN) can now achieve above human level accuracy on image classification tasks. However their outstanding performances come along with a complex inference mechanism making them arduously interpretable models. In order to understand the underlying prediction rules of DNNs, Dhamdhere et al. [3] propose an interpretability method to break down a DNN prediction score as sum of its hidden unit contributions, in the form of a metric called conductance. Analyzing conductances of DNN hidden units, we find out there is a difference in how wrong and correct predictions are inferred. We identify distinguishable patterns of hidden unit activations for wrong and correct predictions. We then use an error detector in the form of a binary classifier on top of the DNN to automatically discriminate wrong and correct predictions of the DNN based on their hidden unit activations. Detected wrong predictions are discarded, increasing the model robustness. A different approach to distinguish wrong and correct predictions of DNNs is proposed by Wang et al. [20] whose method is based on the premise that input samples leading a DNN into making wrong predictions are less stable to the DNN weight changes than correctly classified input samples. In our study, we compare both methods and find out by combining them that better detection of wrong predictions can be achieved.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'classification', 'interpretability']"
No,"neural networks, robustness to missing data",A Modulation Layer to Increase Neural Network Robustness Against Data Quality Issues,0.0,"M Abdelhack, J Zhang, S Tripathi, B Fritz…",2021.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2107.08574,,82.0,2022-07-12 13:40:41,,,,,,,,,0,0.0,0.0,5.0,1.0,… layer can enable the deployment of artificial intelligence systems in real-time settings. … it as a problem that leads to lower confidence in outputs. Our approach is superficially similar to …,https://arxiv.org/pdf/2107.08574,https://scholar.google.com/scholar?q=related:AKtgKLlRNAQJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural networks', 'robust']"
No,"Label Noise, Ensemble Classifier, Robust",Improving the robustness of bagging with reduced sampling size,11.0,"M Sabzevari, G Martínez-Muñoz, A Suárez",2014.0,,repositorio.uam.es,https://repositorio.uam.es/bitstream/handle/10486/667698/improving_sabzevari_ESANN_2014.pdf?sequence=1,https://scholar.google.com/scholar?cites=5157548172073804020&as_sdt=2005&sciodt=2007&hl=en,83.0,2022-07-13 11:05:20,PDF,,,,,,,,11,1.38,4.0,3.0,8.0,… presence of class label noise. This algorithm builds … robustness of bagging in the presence of class label noise. An empirical analysis on two datasets is carried out using different noise …,https://repositorio.uam.es/bitstream/handle/10486/667698/improving_sabzevari_ESANN_2014.pdf?sequence=1,https://scholar.google.com/scholar?q=related:9KBslLxKk0cJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['ensemble classifier', 'label noise', 'robust']"
Yes,"Regularization, Robust, Accuracy, Adversarial",Consistency regularization for adversarial robustness,8.0,"J Tack, S Yu, J Jeong, M Kim, SJ Hwang…",2022.0,… on Artificial Intelligence,ojs.aaai.org,https://ojs.aaai.org/index.php/AAAI/article/view/20817,https://scholar.google.com/scholar?cites=9081981343970826286&as_sdt=2005&sciodt=2007&hl=en,83.0,2022-07-12 11:55:13,,,,,,,,,8,8.0,1.0,6.0,1.0,"… overfitting, ie, the robustness starts to decrease significantly … a significant generalization gap in the robustness. In this paper, … to generalize its robustness against unseen adversaries, eg, …",https://ojs.aaai.org/index.php/AAAI/article/download/20817/20576,https://scholar.google.com/scholar?q=related:Lhgb-OmtCX4J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial', 'regularization', 'robust']"
No,"recommender systems, fairness, explainability",Transparency in Fair Machine Learning: the Case of Explainable Recommender Systems,33.0,"B. Abdollahi, O. Nasraoui",2018.0,,,,,84.0,2022-07-13 09:22:24,,10.1007/978-3-319-90403-0_2,,,,,,,33,"0,3506944444",17.0,2.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'fairness', 'recommender systems']"
Yes,"Explainability, Counterfactual, Graph Neural Network",Robust Counterfactual Explanations on Graph Neural Networks,1.0,M. Bajaj,2021.0,Advances in Neural Information Processing Systems,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85131713149&origin=inward,84.0,2022-07-12 16:34:43,Conference Paper,,1049-5258,https://api.elsevier.com/content/abstract/scopus_id/85131713149,7.0,,5644.0,5655.0,1,1.0,1.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['counterfactual', 'explainability', 'graph neural network']"
Yes,"Assessment, Robust",A statistical approach to assessing neural network robustness,43.0,"S Webb, T Rainforth, YW Teh, MP Kumar",2018.0,arXiv preprint arXiv:1811.07209,arxiv.org,https://arxiv.org/abs/1811.07209,https://scholar.google.com/scholar?cites=7897732150648450452&as_sdt=2005&sciodt=2007&hl=en,84.0,2022-07-13 14:02:43,,,,,,,,,43,11.15,11.0,4.0,4.0,"… We have introduced a new measure for the intrinsic robustness of a neural network, and … for satisfiable properties and provide high confidence, accurate predictions for properties which …",https://arxiv.org/pdf/1811.07209,https://scholar.google.com/scholar?q=related:lC2VCoxhmm0J:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'robust']"
No,"Framework, Convolutional Neural Network, Robust",A Robust Backpropagation-Free Framework for Images,0.0,"Timothy Zee, Alexander Ororbia, A. Mali, Ifeoma Nwogu",2022.0,,,,,84.0,2022-07-13 09:19:22,,10.48550/arXiv.2206.01820,,,,,,,0,0.0,0.0,4.0,1.0,"While current deep learning algorithms have been successful for a wide variety of artiﬁcial intelligence (AI) tasks, including those involving structured image data, they present deep neurophysiological conceptual issues due to their reliance on the gradients computed by backpropagation of errors (backprop) to obtain synaptic weight adjustments; hence are biologically implausible. We present a more biologically plausible approach, the error-kernel driven activation alignment (EKDAA) algorithm, to train convolution neural networks (CNNs) using locally derived error transmission kernels and error maps. We demonstrate the efﬁcacy of EKDAA by performing the task of visual-recognition on the Fashion MNIST, CIFAR-10 and SVHN benchmarks as well as conducting blackbox robustness tests on adversarial examples derived from these datasets. Furthermore, we also present results for a CNN trained using a non-differentiable activation function. All recognition results nearly matches that of backprop and exhibit greater adversarial robustness compared to backprop.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'framework', 'robust']"
Yes,"Robust, Noise, Uncertainty",Certifying neural network robustness to random input noise from samples,4.0,"BG Anderson, S Sojoudi",2020.0,arXiv preprint arXiv:2010.07532,arxiv.org,https://arxiv.org/abs/2010.07532,https://scholar.google.com/scholar?cites=10034362074634965645&as_sdt=2005&sciodt=2007&hl=en,84.0,2022-07-13 11:05:20,,,,,,,,,4,2.0,2.0,2.0,2.0,… Methods to certify the robustness of neural networks in the presence of input … robustness certification method that upper bounds the probability of misclassification when the input noise …,https://arxiv.org/pdf/2010.07532,https://scholar.google.com/scholar?q=related:jdJEfiQ3QYsJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['noise', 'robust', 'uncertainty']"
No,"Robust, Sensitivity, Stability, Generalization",An Understanding-Oriented Robust Machine Reading Comprehension Model,0.0,"Feiliang Ren, Yongkang Liu, Bochao Li, Shilei Liu, Bingchao Wang, Jiaqi Wang, Chunchao Liu, Qi Ma",2022.0,,,,,85.0,2022-07-13 09:23:56,,10.1145/3546190,,,,,,,0,0.0,0.0,8.0,1.0,"Although existing machine reading comprehension models are making rapid progress on many datasets, they are far from robust. In this paper, we propose an understanding-oriented machine reading comprehension model to address three kinds of robustness issues, which are over sensitivity, over stability and generalization. Specifically, we first use a natural language inference module to help the model understand the accurate semantic meanings of input questions so as to address the issues of over sensitivity and over stability. Then in the machine reading comprehension module, we propose a memory-guided multi-head attention method that can further well understand the semantic meanings of input questions and passages. Third, we propose a multi-language learning mechanism to address the issue of generalization. Finally, these modules are integrated with a multi-task learning based method. We evaluate our model on three benchmark datasets that are designed to measure models’ robustness, including DuReader (robust) and two SQuAD-related datasets. Extensive experiments show that our model can well address the mentioned three kinds of robustness issues. And it achieves much better results than the compared state-of-the-art models on all these datasets under different evaluation metrics, even under some extreme and unfair evaluations. The source code of our work is available at: https://github.com/neukg/RobustMRC.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generalization', 'robust', 'sensitivity', 'stability']"
No,"Perturbations, Robust, Graph Convolutional Network",Node Feature Kernels Increase Graph Convolutional Network Robustness,1.0,"MEA Seddik, C Wu, JF Lutzeyer…",2022.0,… Artificial Intelligence …,proceedings.mlr.press,https://proceedings.mlr.press/v151/el-amine-seddik22a.html,https://scholar.google.com/scholar?cites=3396819270477207757&as_sdt=2005&sciodt=2007&hl=en,85.0,2022-07-12 13:50:33,,,,,,,,,1,1.0,0.0,4.0,1.0,"… We observe that only when the node feature noise is five times larger than the graph structure noise (γ = 5), the addition of the node feature kernel stops to benefit the model …",https://proceedings.mlr.press/v151/el-amine-seddik22a/el-amine-seddik22a.pdf,https://scholar.google.com/scholar?q=related:zdj-HiHtIy8J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['graph neural network', 'perturbations', 'robust']"
Yes,"Robust, Hierachical CNN, Object Detection",Hierarchical and Robust Convolutional Neural Network for Very High-Resolution Remote Sensing Object Detection,97.0,Y. Zhang,2019.0,IEEE Transactions on Geoscience and Remote Sensing,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85069769380&origin=inward,86.0,2022-07-12 16:26:59,Article,10.1109/TGRS.2019.2900302,0196-2892,https://api.elsevier.com/content/abstract/scopus_id/85069769380,57.0,8.0,5535.0,5548.0,97,32.33.00,97.0,1.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'convolutional neural network', 'robust']"
Yes,"training, adversarial, generative adversarial network",GADoT: GAN-based Adversarial Training for Robust DDoS Attack Detection,1.0,"Maged AbdelAty, S. Scott-Hayward, R. D. Corin, D. Siracusa",2021.0,,,,,86.0,2022-07-13 09:38:44,,10.1109/CNS53000.2021.9705040,,,,,,,1,1.0,0.0,4.0,1.0,"Machine Learning (ML) has proven to be effective in many application domains. However, ML methods can be vulnerable to adversarial attacks, in which an attacker tries to fool the classification/prediction mechanism by crafting the input data. In the case of ML-based Network Intrusion Detection Systems (NIDSs), the attacker might use their knowledge of the intrusion detection logic to generate malicious traffic that remains undetected. One way to solve this issue is to adopt adversarial training, in which the training set is augmented with adversarial traffic samples. This paper presents an adversarial training approach called GADoT, which leverages a Generative Adversarial Network (GAN) to generate adversarial DDoS samples for training. We show that a state-of-the-art NIDS with high accuracy on popular datasets can experience more than 60% undetected malicious flows under adversarial attacks. We then demonstrate how this score drops to 1.8% or less after adversarial training using GADoT.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'generative adversarial networks', 'training']"
Yes,Vision transformers,On the adversarial robustness of vision transformers,49.0,"R Shao, Z Shi, J Yi, PY Chen, CJ Hsieh",2021.0,arXiv preprint arXiv:2103.15670,arxiv.org,https://arxiv.org/abs/2103.15670,https://scholar.google.com/scholar?cites=15056564293298660220&as_sdt=2005&sciodt=2007&hl=en,86.0,2022-07-12 11:49:54,,,,,,,,,49,49.0,10.0,5.0,1.0,"… the certified robustness of the models using randomized smoothing, where the robustness is … In Proceedings of the AAAI conference on artificial intelligence, volume 34, pp. 8018–8025, …",https://arxiv.org/pdf/2103.15670,https://scholar.google.com/scholar?q=related:fKPln3Op89AJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['transformer network']
Yes,"weight perturbation, robustness generalization",Formalizing Generalization and Adversarial Robustness of Neural Networks to Weight Perturbations,3.0,"Yu-Lin Tsai, Chia-Yi Hsu, Chia-Mu Yu, Pin-Yu Chen",2021.0,,,,,87.0,2022-07-13 09:22:57,,,,,,,,,3,3.0,1.0,4.0,1.0,"Studying the sensitivity of weight perturbation in neural networks and its impacts on model performance, including generalization and robustness, is an active research topic due to its implications on a wide range of machine learning tasks such as model compression, generalization gap assessment, and adversarial attacks. In this paper, we provide the first integral study and analysis for feed-forward neural networks in terms of the robustness in pairwise class margin and its generalization behavior under weight perturbation. We further design a new theory-driven loss function for training generalizable and robust neural networks against weight perturbations. Empirical experiments are conducted to validate our theoretical analysis. Our results offer fundamental insights for characterizing the generalization and robustness of neural networks against weight perturbations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generalization', 'perturbations']"
No,"NLP, knowledge",Towards Robust Text Classification with Semantics-Aware Recurrent Neural Architecture,14.0,"Blaž Škrlj, Jan Kralj, N. Lavrac, Senja Pollak",2019.0,,,,,87.0,2022-07-13 10:11:25,,10.3390/MAKE1020034,,,,,,,14,"0,2131944444",4.0,4.0,3.0,"Deep neural networks are becoming ubiquitous in text mining and natural language processing, but semantic resources, such as taxonomies and ontologies, are yet to be fully exploited in a deep learning setting. This paper presents an efficient semantic text mining approach, which converts semantic information related to a given set of documents into a set of novel features that are used for learning. The proposed Semantics-aware Recurrent deep Neural Architecture (SRNA) enables the system to learn simultaneously from the semantic vectors and from the raw text documents. We test the effectiveness of the approach on three text classification tasks: news topic categorization, sentiment analysis and gender profiling. The experiments show that the proposed approach outperforms the approach without semantic knowledge, with highest accuracy gain (up to 10%) achieved on short document fragments.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['knowledge', 'nlp']"
Yes,"Pruning, Robust, Neural Network, Framework",Can pruning improve certified robustness of neural networks?,0.0,"Zhangheng Li, Tianlong Chen, Linyi Li, Bo Li, Zhangyang Wang",2022.0,,,,,88.0,2022-07-13 09:28:50,,10.48550/arXiv.2206.07311,,,,,,,0,0.0,0.0,5.0,1.0,"—With the rapid development of deep learning, the sizes of neural networks become larger and larger so that the training and inference often overwhelm the hardware resources. Given the fact that neural networks are often over-parameterized, one effective way to reduce such computational overhead is neural network pruning, by removing redundant parameters from trained neural networks. It has been recently observed that pruning can not only reduce computational overhead but also can improve empirical robustness of deep neural networks (NNs), potentially owing to removing spurious correlations while preserving the predictive accuracies. This paper for the ﬁrst time demonstrates that pruning can generally improve certiﬁed robustness for ReLU-based NNs under the complete veriﬁcation setting. Using the popular Branch-and-Bound (BaB) framework, we ﬁnd that pruning can enhance the estimated bound tightness of certiﬁed robustness veriﬁcation, by alleviating linear relaxation and sub-domain split problems. We empirically verify our ﬁndings with off-the-shelf pruning methods and further present a new stability-based pruning method tailored for reducing neuron instability, that outperforms existing pruning methods in enhancing certiﬁed robustness. Our experiments show that by appropriately pruning an NN, its certiﬁed accuracy can be boosted up to 8.2% under standard training, and up to 24.5% under adversarial training on the CIFAR10 dataset. We additionally observe the existence of certiﬁed lottery tickets that can match both standard and certiﬁed robust accuracies of the original dense models across different datasets. Our ﬁndings offer a new angle to study the intriguing interaction between sparsity and robustness, i.e. interpreting the interaction of sparsity and certiﬁed robustness via neuron stability. Codes are available at:",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['framework', 'neural network', 'pruning', 'robust']"
Yes,"Robustness vs accuracy, new evaluation method",Robustness and Accuracy Could Be Reconcilable by (Proper) Definition,2.0,"T Pang, M Lin, X Yang, J Zhu, S Yan",2022.0,arXiv preprint arXiv:2202.10103,arxiv.org,https://arxiv.org/abs/2202.10103,https://scholar.google.com/scholar?cites=12573058517676493723&as_sdt=2005&sciodt=2007&hl=en,88.0,2022-07-12 13:44:06,,,,,,,,,2,2.0,0.0,5.0,1.0,"… We then try to describe how the robustness-accuracy tradeoff stems from the previously defined robust error. Recall that in supervised learning, we have a joint data distribution pd(x, y), …",https://arxiv.org/pdf/2202.10103,https://scholar.google.com/scholar?q=related:m4tOXht6fK4J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'evaluation']"
Yes,"Adversarial, Training, Robust",Encoding Robustness to Image Style via Adversarial Feature Perturbations,3.0,"Manli Shu, Zuxuan Wu, Micah Goldblum, T. Goldstein",2020.0,,,,,89.0,2022-07-13 09:22:57,,,,,,,,,3,1.5,1.0,4.0,2.0,"Adversarial training is the industry standard for producing models that are robust to small adversarial perturbations. However, machine learning practitioners need models that are robust to other kinds of changes that occur naturally, such as changes in the style or illumination of input images. Such changes in input distribution have been effectively modeled as shifts in the mean and variance of deep image features. We adapt adversarial training by directly perturbing feature statistics, rather than image pixels, to produce models that are robust to various unseen distributional shifts. We explore the relationship between these perturbations and distributional shifts by visualizing adversarial features. Our proposed method, Adversarial Batch Normalization (AdvBN), is a single network layer that generates worst-case feature perturbations during training. By fine-tuning neural networks on adversarial feature distributions, we observe improved robustness of networks to various unseen distributional shifts, including style variations and image corruptions. In addition, we show that our proposed adversarial feature perturbation can be complementary to existing image space data augmentation methods, leading to improved performance. The source code and pre-trained models are released at https://github.com/azshue/AdvBN.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust', 'training']"
No,"Regularization, Generalization, Noise",Interpolation can hurt robust generalization even when there is no noise,3.0,"K Donhauser, A Tifrea, M Aerni…",2021.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2021/hash/c4f2c88e16a579900657c18726641c81-Abstract.html,https://scholar.google.com/scholar?cites=15775630453700777923&as_sdt=2005&sciodt=2007&hl=en,90.0,2022-07-13 15:31:15,,,,,,,,,3,3.0,1.0,4.0,1.0,"… robust risk of both linear regression and classification, and hence provide the first theoretical result on robust … Our results provide the first rigorous explanation of robust overfitting even in …",https://proceedings.neurips.cc/paper/2021/file/c4f2c88e16a579900657c18726641c81-Paper.pdf,https://scholar.google.com/scholar?q=related:w1erlFBM7toJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generalization', 'noise', 'regularization']"
Yes,"adversarial robustness, face recognition",Detecting and Mitigating Adversarial Perturbations for Robust Face Recognition,50.0,G. Goswami,2019.0,International Journal of Computer Vision,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85062828919&origin=inward,91.0,2022-07-12 16:28:25,Article,10.1007/s11263-019-01160-w,0920-5691,https://api.elsevier.com/content/abstract/scopus_id/85062828919,127.0,6.0,719.0,742.0,50,17.07,50.0,1.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'computer vision']"
No,"Adversarial, Robust, Neural Network ",Attack as defense: Characterizing adversarial examples using robustness,9.0,"Z Zhao, G Chen, J Wang, Y Yang, F Song…",2021.0,Proceedings of the 30th …,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3460319.3464822,https://scholar.google.com/scholar?cites=5485294770828616566&as_sdt=2005&sciodt=2007&hl=en,91.0,2022-07-12 11:56:54,,10.1145/3460319.3464822,,,,,,,9,9.0,2.0,6.0,1.0,"… To mitigate this threat, we compare with the works from both the artificial intelligence community (eg, BL1 and BL2) and the software engineering community (eg, BL3 and BL4). BL1 is …",https://dl.acm.org/doi/pdf/10.1145/3460319.3464822,https://scholar.google.com/scholar?q=related:diOMH4iuH0wJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'neural network', 'robust']"
No,"Neural Network, Perturbation, Robust",Input Validation for Neural Networks via Runtime Local Robustness Verification,4.0,"Jiangchao Liu, Liqian Chen, A. Miné, Ji Wang",2020.0,,,,,91.0,2022-07-13 09:26:34,,,,,,,,,4,2.0,1.0,4.0,2.0,"Local robustness verification can verify that a neural network is robust wrt. any perturbation to a specific input within a certain distance. We call this distance Robustness Radius. We observe that the robustness radii of correctly classified inputs are much larger than that of misclassified inputs which include adversarial examples, especially those from strong adversarial attacks. Another observation is that the robustness radii of correctly classified inputs often follow a normal distribution. Based on these two observations, we propose to validate inputs for neural networks via runtime local robustness verification. Experiments show that our approach can protect neural networks from adversarial examples and improve their accuracies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural network', 'perturbation', 'robust']"
No,"Adversarial. Robust, Classification",Achieving robustness in classification using optimal transport with hinge regularization,1.0,M. Serrurier,2021.0,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85108856299&origin=inward,92.0,2022-07-12 16:28:09,Conference Paper,10.1109/CVPR46437.2021.00057,1063-6919,https://api.elsevier.com/content/abstract/scopus_id/85108856299,,,505.0,514.0,1,1.0,1.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'classification']"
Yes,"Text Recognition, Transformer Network, Robust",Robust Scene Text Recognition with Automatic Rectification,0.0,"Baoguang Shi, Xinggang Wang, Pengyuan Lyu, Cong Yao, Xiang Bai, IEEE",2016.0,2016 Ieee Conference On Computer Vision And Pattern Recognition (Cvpr),,,,92.0,2022-07-15 11:51:22,Proceedings Paper,10.1109/CVPR.2016.452,1063-6919,,,,4168.0,4176.0,0,0.0,0.0,6.0,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'robust', 'transformer network']"
No,"Reinforcement Learning, Robust, Framework",A bayesian approach to robust reinforcement learning,25.0,"E Derman, D Mankowitz, T Mann…",2020.0,… in Artificial Intelligence,proceedings.mlr.press,http://proceedings.mlr.press/v115/derman20a.html,https://scholar.google.com/scholar?cites=4842016241508892309&as_sdt=2005&sciodt=2007&hl=en,92.0,2022-07-12 13:40:41,,,,,,,,,25,12.5,6.0,4.0,2.0,"… If H is fixed, this construction falls into the definition of a Bayesian confidence interval … In contrast, our proposed approach aims to adapt the level of robustness iteratively and online from …",http://proceedings.mlr.press/v115/derman20a/derman20a.pdf,https://scholar.google.com/scholar?q=related:lT7PFx5MMkMJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['framework', 'reinforcement learning', 'robust']"
Yes,"Image corruption, non-adversarial robustness",Robustness in Deep Learning for Computer Vision: Mind the gap?,3.0,"N Drenkow, N Sani, I Shpitser, M Unberath",2021.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2112.00639,https://scholar.google.com/scholar?cites=2844534409710022825&as_sdt=2005&sciodt=2007&hl=en,92.0,2022-07-13 09:30:22,,,,,,,,,3,3.0,1.0,4.0,1.0,"… robustness in that context. In short, beyond adversarial machine learning, what is the robustness … explanation and exploration under the neural sde framework. pages 279–287, UCLA, …",https://arxiv.org/pdf/2112.00639,https://scholar.google.com/scholar?q=related:qTAkgvnQeScJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'perturbations']"
Yes,"robustness, unknown unknowns, review",Steps Toward Robust Artificial Intelligence,81.0,Thomas G. Dietterich,2017.0,,,,,92.0,2022-07-13 09:19:02,,10.1609/aimag.v38i3.2756,,,,,,,81,16.2,81.0,1.0,5.0,"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['robust', 'survey', 'unknown unknowns']"
No,"explainability, malware",Towards explainability in machine learning for Malware detection,3.0,S Bose,2020.0,,search.proquest.com,https://search.proquest.com/openview/fe1a8b1269a3d0fdb0c38f3094bc31ce/1?pq-origsite=gscholar&cbl=18750&diss=y,https://scholar.google.com/scholar?cites=17788943027129114600&as_sdt=2005&sciodt=2007&hl=en,93.0,2022-07-14 09:50:02,,,,,,,,,3,1.5,3.0,1.0,2.0,"… , machine learning techniques have been widely used for a number of applications including malware detection. Most machine learning … This will be beneficial in building more robust …",https://diginole.lib.fsu.edu/islandora/object/fsu:776810/datastream/PDF/view,https://scholar.google.com/scholar?q=related:6F-x7W4F3_YJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22explainability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['cybersecurity', 'explainability']"
No,"Counterfactual explanations, privacy, perturbations",Robust Counterfactual Explanations for Privacy-Preserving SVM,1.0,"R Mochaourab, S Sinha, S Greenstein…",2021.0,… on Machine Learning …,diva-portal.org,https://www.diva-portal.org/smash/record.jsf?pid=diva2:1581005,https://scholar.google.com/scholar?cites=16352585168385814797&as_sdt=2005&sciodt=2007&hl=en,93.0,2022-07-14 09:57:23,,,,,,,,,1,1.0,0.0,4.0,1.0,"… Therefore, counterfactual explanations need to be made robust against such perturbations … in the SVM weights and formulate the robust counterfactual explanation problem. Then, we …",https://www.diva-portal.org/smash/get/diva2:1581005/FULLTEXT01.pdf,https://scholar.google.com/scholar?q=related:DTH1KJIN8OIJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['counterfactual', 'explainability', 'perturbations', 'privacy']"
Yes,"Datasets, benchmarks, data robustness ",Pervasive label errors in test sets destabilize machine learning benchmarks,145.0,"CG Northcutt, A Athalye, J Mueller",2021.0,arXiv preprint arXiv:2103.14749,arxiv.org,https://arxiv.org/abs/2103.14749,https://scholar.google.com/scholar?cites=14801847567784198244&as_sdt=2005&sciodt=2007&hl=en,93.0,2022-07-13 10:54:59,,,,,,,,,145,145.0,48.0,3.0,1.0,"… 48]; however, these approaches lack either robustness to class imbalance or theoretical support for … For robustness to class imbalance and theoretical support for exact uncertainty …",https://arxiv.org/pdf/2103.14749,https://scholar.google.com/scholar?q=related:ZMBBefC5as0J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22accountability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['benchmark', 'dataset', 'robust']"
Discussion,"explainability, anomaly detection",Toward Explainable Deep Neural Network Based Anomaly Detection,52.0,"Kasun Amarasinghe, K. Kenney, M. Manic",2018.0,,,,,93.0,2022-07-13 09:27:29,,10.1109/HSI.2018.8430788,,,,,,,52,13.0,17.0,3.0,4.0,"Anomaly detection in industrial processes is crucial for general process monitoring and process health assessment. Deep Neural Networks (DNNs) based anomaly detection has received increased attention in recent work. Albeit their high accuracy, the black-box nature of DNNs is a drawback in practical deployment. Especially in industrial anomaly detection systems, explanations of DNN detected anomalies are crucial. This paper presents a framework for DNN based anomaly detection which provides explanations of detected anomalies. The framework answers the following questions during online processing: 1) “why is it an anomaly?” and 2) “what is the confidence?” Further, the framework can be used offline to evaluate the “knowledge” of the trained DNN. The framework reduces the opaqueness of the DNN based anomaly detector and thus improves human operators' trust in the algorithm. This paper implements the first steps of the presented framework on the benchmark KDD-NSL dataset for Denial of Service (DoS) attack detection. Offline DNN explanations showed that the DNN was detecting DoS attacks based on features indicating destination of connection, frequency and amount of data transferred while showing an accuracy around 97%.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['anomaly detection', 'explainability']"
Yes,"robustness, causality, counterfactual explanations",Towards robust classification model by counterfactual and invariant data generation,1.0,C.H. Chang,2021.0,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85123107524&origin=inward,93.0,2022-07-12 16:32:32,Conference Paper,10.1109/CVPR46437.2021.01496,1063-6919,https://api.elsevier.com/content/abstract/scopus_id/85123107524,,,15207.0,15216.0,1,1.0,1.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['causality', 'counterfactual', 'explainability', 'robust']"
Yes,"Deep Learning, Discriminative Featues, Adversarial Attacks",IMPROVING ROBUSTNESS TO ADVERSARIAL EXAMPLES BY ENCOURAGING DISCRIMINATIVE FEATURES,0.0,"Chirag Agarwal, Anh Nguyen, Dan Schonfeld, IEEE",2019.0,2019 Ieee International Conference On Image Processing (Icip),,,,93.0,2022-07-13 13:26:12,Proceedings Paper,,1522-4880,,,,3801.0,3805.0,0,0.0,0.0,4.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'deep learning', 'features']"
Yes,"Adversarial Attacks, Generalization, Privacy","Robustness, privacy, and generalization of adversarial training",3.0,"F He, S Fu, B Wang, D Tao",2020.0,arXiv preprint arXiv:2012.13573,arxiv.org,https://arxiv.org/abs/2012.13573,https://scholar.google.com/scholar?cites=4559168910294412614&as_sdt=2005&sciodt=2007&hl=en,93.0,2022-07-12 11:56:54,,,,,,,,,3,1.5,1.0,4.0,2.0,"… privacy-robustness trade-off and generalization-robustness trade-off in adversarial training … We first define a notion, robustified intensity to measure the robustness of an adversarial …",https://arxiv.org/pdf/2012.13573,https://scholar.google.com/scholar?q=related:Rm0a6fdrRT8J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'generalization', 'privacy']"
Yes,"adversarial robustness, identification methods, explainability, neural networks",Detection defense against adversarial attacks with saliency map,4.0,D. Ye,2021.0,International Journal of Intelligent Systems,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85105311586&origin=inward,94.0,2022-07-12 16:24:02,Article,10.1002/int.22458,0884-8173,https://api.elsevier.com/content/abstract/scopus_id/85105311586,,,,,4,4.0,4.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'explainability', 'identification', 'neural networks']"
No,"robustness to noise labels, mitigation method, neural networks",On Robustness of Neural Architecture Search Under Label Noise,0.0,"Yi-Wei Chen, Qingquan Song, Xi Liu, P. S. Sastry, Xia Hu",2020.0,Frontiers In Big Data,,,,94.0,2022-07-13 13:26:12,Article,10.3389/fdata.2020.00002,,,3.0,,,,0,0.0,0.0,5.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['label noise', 'mitigation method', 'neural networks']"
Discussion,"Interpretability, decision trees",Optimizing for interpretability in deep neural networks with tree regularization,6.0,"M Wu, S Parbhoo, MC Hughes, V Roth…",2021.0,… of Artificial Intelligence …,jair.org,https://www.jair.org/index.php/jair/article/view/12558,https://scholar.google.com/scholar?cites=5872662375312268670&as_sdt=2005&sciodt=2007&hl=en,94.0,2022-07-12 13:46:44,,,,,,,,,6,6.0,1.0,5.0,1.0,"… interpretability from the very start ie add an “interpretability … deep models for interpretability remains largely nascent. In this … Tree Regularization To optimize for interpretability, one must …",https://www.jair.org/index.php/jair/article/download/12558/26715/,https://scholar.google.com/scholar?q=related:fh1KzlDjf1EJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22interpretability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['decision trees', 'interpretability']"
Yes,"data augmentation, robust",Exploiting the Potential of Datasets: A Data-Centric Approach for Model Robustness,0.0,"Y Zhong, L Wu, X Liu, J Jiang",2022.0,arXiv preprint arXiv:2203.05323,arxiv.org,https://arxiv.org/abs/2203.05323,,94.0,2022-07-13 14:46:28,,,,,,,,,0,0.0,0.0,4.0,1.0,… Robustness of DNNs to the perturbations on model inputs is of great concern in trustworthy AI. … The research community has made great efforts to improve model robustness. To defense …,https://arxiv.org/pdf/2203.05323,https://scholar.google.com/scholar?q=related:FnyXAWYClYYJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22trustworthy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['data augmentation', 'robust']"
Yes,"Adversarial, Robust",Boosting robustness certification of neural networks,49.0,G. Singh,2019.0,"7th International Conference on Learning Representations, ICLR 2019",,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85083952572&origin=inward,94.0,2022-07-12 16:28:25,Conference Paper,,,https://api.elsevier.com/content/abstract/scopus_id/85083952572,,,,,49,16.33,49.0,1.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
Yes,"Classification, Robust, Adversarial, Benchmark",Benchmarking Adversarial Robustness on Image Classification,0.0,"Yinpeng Dong, Qi-An Fu, Xiao Yang, Tianyu Pang, Hang Su, Zihao Xiao, Jun Zhu, IEEE",2020.0,2020 Ieee/Cvf Conference On Computer Vision And Pattern Recognition (Cvpr),,,,95.0,2022-07-13 15:27:05,Proceedings Paper,10.1109/CVPR42600.2020.00040,1063-6919,,,,318.0,328.0,0,0.0,0.0,8.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'benchmark', 'classification', 'robust']"
Yes,"federated learning, adversarial robustness, mitigation method",Differentially private self-normalizing neural networks for adversarial robustness in federated learning,0.0,"O Ibitoye, MO Shafiq, A Matrawy",2022.0,Computers & Security,Elsevier,https://www.sciencedirect.com/science/article/pii/S016740482200030X?casa_token=BaWsP3bDQ3oAAAAA:6DL1hYOZ6LSrgSAdEVCiE02dvfsbjOSKrhVWHzkvkWteujSG3m-E16EtJ3Dvz7_0oUKATpsOSu0,,95.0,2022-07-13 10:30:58,HTML,,,,,,,,0,0.0,0.0,3.0,1.0,"… training improves the robustness to adversarial samples in Machine Learning, it is not as … The Differentially Private noise layer establishes stability bounds to determine how the output …",https://www.sciencedirect.com/science/article/pii/S016740482200030X?casa_token=BaWsP3bDQ3oAAAAA:6DL1hYOZ6LSrgSAdEVCiE02dvfsbjOSKrhVWHzkvkWteujSG3m-E16EtJ3Dvz7_0oUKATpsOSu0,https://scholar.google.com/scholar?q=related:9r9uGDLkFHwJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22stability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'federated learning', 'mitigation method']"
No,"Extreme Learning Machine, Robust, Pruning",A Robust and Optimally Pruned Extreme Learning Machine,1.0,"A. Freire, A. R. Neto",2016.0,,,,,95.0,2022-07-13 09:23:56,,10.1007/978-3-319-53480-0_9,,,,,,,1,0.17,1.0,2.0,6.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['extreme learning machine', 'pruning', 'robust']"
No,"Reinforcement Learning, Robust, Metric","Review of Metrics to Measure the Stability, Robustness and Resilience of Reinforcement Learning",0.0,LL Pullum,2022.0,arXiv preprint arXiv:2203.12048,arxiv.org,https://arxiv.org/abs/2203.12048,,96.0,2022-07-12 13:42:35,,,,,,,,,0,0.0,0.0,1.0,1.0,"… We believe that this is the first comprehensive review of stability, robustness and resilience specifically geared towards RL. The remainder of the paper is organized as follows. Section 2 …",https://arxiv.org/pdf/2203.12048,https://scholar.google.com/scholar?q=related:0rVbleDWwsAJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22resilience%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['metric', 'reinforcement learning', 'robust']"
Yes,"Robust, Classification, Convolutional Neural Network",A noise robust convolutional neural network for image classification,16.0,"M Momeny, AM Latif, MA Sarram, R Sheikhpour…",2021.0,Results in …,Elsevier,https://www.sciencedirect.com/science/article/pii/S2590123021000268,https://scholar.google.com/scholar?cites=13463517809281790056&as_sdt=2005&sciodt=2007&hl=en,96.0,2022-07-14 09:04:21,HTML,,,,,,,,16,16.0,3.0,5.0,1.0,"… The images corrupted by impulse noise, missing image samples, packet loss in image … of the proposed CNN to noise. To robust the proposed CNN to noise, a noise map layer and an …",https://www.sciencedirect.com/science/article/pii/S2590123021000268,https://scholar.google.com/scholar?q=related:aPBiYR0G2LoJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'convolutional neural network', 'robust']"
Yes,"Robust, Adversarial",Deep Repulsive Prototypes for Adversarial Robustness,1.0,"A. Serban, E. Poll, Joost Visser",2021.0,,,,,96.0,2022-07-13 09:22:57,,,,,,,,,1,1.0,0.0,3.0,1.0,"While many defences against adversarial examples have been proposed, finding robust machine learning models is still an open problem. The most compelling defence to date is adversarial training and consists of complementing the training data set with adversarial examples. Yet adversarial training severely impacts training time and depends on finding representative adversarial samples. In this paper we propose to train models on output spaces with large class separation in order to gain robustness without adversarial training. We introduce a method to partition the output space into class prototypes with large separation and train models to preserve it. Experimental results shows that models trained with these prototypes – which we call deep repulsive prototypes – gain robustness competitive with adversarial training, while also preserving more accuracy on natural samples. Moreover, the models are more resilient to large perturbation sizes. For example, we obtained over 50% robustness for CIFAR-10, with 92% accuracy on natural samples and over 20% robustness for CIFAR-100, with 71% accuracy on natural samples without adversarial training. For both data sets, the models preserved robustness against large perturbations better than adversarially trained models.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
Yes,"adversarial robustness, efficiency, CNN",Towards Robust Compressed Convolutional Neural Networks,11.0,"A. Wijayanto, Jun Jin Choong, Kaushalya Madhawa, T. Murata",2019.0,,,,,97.0,2022-07-13 10:08:07,,10.1109/BIGCOMP.2019.8679132,,,,,,,11,"0,1715277778",3.0,4.0,3.0,"Recent studies on robustness of Convolutional Neural Network (CNN) shows that CNNs are highly vulnerable towards adversarial attacks. Meanwhile, smaller sized CNN models with no significant accuracy loss are being introduced to mobile devices. However, only the accuracy on standard datasets is reported along with such research. The wide deployment of smaller models on millions of mobile devices stresses importance of their robustness. In this research, we study how robust such models are with respect to state-of-the-art compression techniques such as quantization. Our contributions include: (1) insights to achieve smaller models and robust models (2) a compression framework which is adversarial-aware. In the former, we discovered that compressed models are naturally more robust than compact models. This provides an incentive to perform compression rather than designing compact models. Additionally, the latter provides benefits of increased accuracy and higher compression rate, up to 90×.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'convolutional neural network', 'efficiency']"
No,certified robustness,Training Robust Neural Networks Via Smooth Activation Functions,0.0,"M Tan, C Xie, G Boqing, QV Le",2021.0,"US Patent App. 17/337,812",Google Patents,https://patents.google.com/patent/US20210383237A1/en,,97.0,2022-07-14 12:10:31,,,,,,,,,0,0.0,0.0,4.0,1.0,… of robust neural network models by using smooth activation functions. Systems and methods according to the present disclosure may generate and/or train neural network models with …,https://patentimages.storage.googleapis.com/0d/de/39/9cd5b1271848a9/US20210383237A1.pdf,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['robust']
Yes,"Distillation, Training, Adversarial Attacks",Revisiting Adversarial Robustness Distillation: Robust Soft Labels Make Student Better,7.0,"Bojia Zi, Shihao Zhao, Xingjun Ma, Yu-Gang Jiang",2021.0,,,,,98.0,2022-07-13 09:30:15,,10.1109/ICCV48922.2021.01613,,,,,,,7,7.0,2.0,4.0,1.0,"Adversarial training is one effective approach for training robust deep neural networks against adversarial attacks. While being able to bring reliable robustness, adversarial training (AT) methods in general favor high capacity models, i.e., the larger the model the better the robustness. This tends to limit their effectiveness on small models, which are more preferable in scenarios where storage or computing resources are very limited (e.g., mobile devices). In this paper, we leverage the concept of knowledge distillation to improve the robustness of small models by distilling from adversarially trained large models. We first revisit several state-of-the-art AT methods from a distillation perspective and identify one common technique that can lead to improved robustness: the use of robust soft labels – predictions of a robust model. Following this observation, we propose a novel adversarial robustness distillation method called Robust Soft Label Adversarial Distillation (RSLAD) to train robust small student models. RSLAD fully exploits the robust soft labels produced by a robust (adversarially-trained) large teacher model to guide the student’s learning on both natural and adversarial examples in all loss terms. We empirically demonstrate the effectiveness of our RSLAD approach over existing adversarial training and distillation methods in improving the robustness of small models against state-of-the-art attacks including the AutoAttack. We also provide a set of understandings on our RSLAD and the importance of robust soft labels for adversarial robustness distillation. Code: https://github.com/zibojia/RSLAD.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'distillation', 'training']"
Yes,"Out-of-distribution, computer vision, adversarial training",On the Sensitivity of Adversarial Robustness to Input Data Distributions.,35.0,"GW Ding, KYC Lui, X Jin, L Wang, R Huang",2019.0,ICLR (Poster),openaccess.thecvf.com,http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Ding_On_the_Sensitivity_of_Adversarial_Robustness_to_Input_Data_Distributions_CVPRW_2019_paper.pdf,https://scholar.google.com/scholar?cites=925121948530123203&as_sdt=2005&sciodt=2007&hl=en,98.0,2022-07-13 13:05:38,PDF,,,,,,,,35,12.07,7.0,5.0,3.0,"… an intriguing phenomenon that adversarial robustness, unlike … cause a significantly different robustness for the adversarially … task, we perform standard neural network training and test ac…",http://openaccess.thecvf.com/content_CVPRW_2019/papers/Uncertainty%20and%20Robustness%20in%20Deep%20Visual%20Learning/Ding_On_the_Sensitivity_of_Adversarial_Robustness_to_Input_Data_Distributions_CVPRW_2019_paper.pdf,https://scholar.google.com/scholar?q=related:wyEOvIKx1gwJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial training', 'computer vision', 'out-of-distribution data']"
No,"adversarial robustness, mitigation method, interpretability",On Saliency Maps and Adversarial Robustness,0.0,"Puneet Mangla, Vedant Singh, Vineeth N. Balasubramanian",2021.0,"Machine Learning And Knowledge Discovery In Databases, Ecml Pkdd 2020, Pt Ii",,,,98.0,2022-07-13 15:42:09,Proceedings Paper,10.1007/978-3-030-67661-2_17,0302-9743,,12458.0,,272.0,288.0,0,0.0,0.0,3.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'interpretability', 'mitigation method']"
Yes,"fairness, adversarial robustness",To be Robust or to be Fair: Towards Fairness in Adversarial Training,29.0,"Han Xu, Xiaorui Liu, Yaxin Li, Jiliang Tang",2020.0,,,,,98.0,2022-07-13 09:24:12,,,,,,,,,29,14.5,7.0,4.0,2.0,"Adversarial training algorithms have been proven to be reliable to improve machine learning models' robustness against adversarial examples. However, we find that adversarial training algorithms tend to introduce severe disparity of accuracy and robustness between different groups of data. For instance, PGD adversarially trained ResNet18 model on CIFAR-10 has 93% clean accuracy and 67% PGD $l_\infty-8$ adversarial accuracy on the class ""automobile"" but only 59% and 17% on class ""cat"". This phenomenon happens in balanced datasets and does not exist in naturally trained models when only using clean samples. In this work, we theoretically show that this phenomenon can generally happen under adversarial training algorithms which minimize DNN models' robust errors. Motivated by these findings, we propose a Fair-Robust-Learning (FRL) framework to mitigate this unfairness problem when doing adversarial defenses and experimental results validate the effectiveness of FRL.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'fairness']"
No,"adversarial, training, robust",Helper-based adversarial training: Reducing excessive margin to achieve a better accuracy vs. robustness trade-off,13.0,"R Rade, SM Moosavi-Dezfooli",2021.0,… on Adversarial Machine Learning,openreview.net,https://openreview.net/forum?id=BuD2LmNaU3a,https://scholar.google.com/scholar?cites=4762295802787243992&as_sdt=2005&sciodt=2007&hl=en,99.0,2022-07-13 09:47:56,,,,,,,,,13,13.0,7.0,2.0,1.0,"… a superfluous increase in the margin along the initial adversarial directions as compared to the nominal increase required to attain robustness. Second, we provide evidence which …",https://openreview.net/pdf?id=BuD2LmNaU3a,https://scholar.google.com/scholar?q=related:2Kd3CM0SF0IJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust', 'training']"
No,robustness to distribution shifts,Distributionally Robust Bayesian Optimization,36.0,"Johannes Kirschner, Ilija Bogunovic, S. Jegelka, A. Krause",2020.0,,,,,99.0,2022-07-13 09:25:10,,,,,,,,,36,18.0,9.0,4.0,2.0,"Robustness to distributional shift is one of the key challenges of contemporary machine learning. Attaining such robustness is the goal of distributionally robust optimization, which seeks a solution to an optimization problem that is worst-case robust under a specified distributional shift of an uncontrolled covariate. In this paper, we study such a problem when the distributional shift is measured via the maximum mean discrepancy (MMD). For the setting of zeroth-order, noisy optimization, we present a novel distributionally robust Bayesian optimization algorithm (DRBO). Our algorithm provably obtains sub-linear robust regret in various settings that differ in how the uncertain covariate is observed. We demonstrate the robust performance of our method on both synthetic and real-world benchmarks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['distribution shift']
Yes,"Speech Recognition, Noise, Autoencoder",Multi-task autoencoder for noise-robust speech recognition,13.0,"H Zhang, C Liu, N Inoue…",2018.0,2018 IEEE International …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8461446/?casa_token=F_VBJIyu3cYAAAAA:0_NH3yNFhwVcTd-5y6WSTlXEeM8tGGDWNrbwJ1ZbY25BpqBd0ns4ZJPHZqXLnDkhBJaenYLhqQ,https://scholar.google.com/scholar?cites=13239037729015928699&as_sdt=2005&sciodt=2007&hl=en,100.0,2022-07-14 11:17:40,,,,,,,,,13,3.25,3.0,4.0,4.0,… Its input is the noisy-speech feature and the output is the noise feature. The training … noise features and noise features to learn the mapping function from noisy-speech features to noise …,https://ieeexplore.ieee.org/iel7/8450881/8461260/08461446.pdf?casa_token=zKWvDvyLe6YAAAAA:mSnUQaHSi3PmusvRJgr2wmAHGeNEq7NqHPApbskylIOJXQdhNdsMb4lcPtktLBanmaLNFTykNg,https://scholar.google.com/scholar?q=related:exf9zauCurcJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['autoencoder', 'noise', 'time series']"
No,"CNN, robustness to natural perturbations, mitigation method",Distortion robust image classification using deep convolutional neural network with discrete cosine transform,24.0,"MT Hossain, SW Teng, D Zhang…",2019.0,2019 IEEE International …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8803787/,https://scholar.google.com/scholar?cites=15931476292136609778&as_sdt=2005&sciodt=2007&hl=en,100.0,2022-07-14 13:30:21,,,,,,,,,24,8.0,6.0,4.0,3.0,"… In this work, we propose distortion robust DCT-Net, a Discrete Cosine Transform based module … Overall accuracy is the average of clean and distorted datasets. The best accuracy is …",https://arxiv.org/pdf/1811.05819,https://scholar.google.com/scholar?q=related:8idcdEf5F90J:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'mitigation method', 'perturbations']"
Yes,"human-in-the-loop, unknown unknowns",What Should You Know? A Human-In-the-Loop Approach to Unknown Unknowns Characterization in Image Recognition,2.0,"S Sharifi Noorian, S Qiu, U Gadiraju, J Yang…",2022.0,Proceedings of the …,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3485447.3512040,https://scholar.google.com/scholar?cites=10200287607645284691&as_sdt=2005&sciodt=2007&hl=en,100.0,2022-07-12 11:53:57,,10.1145/3485447.3512040,,,,,,,2,2.0,0.0,5.0,1.0,… human computation components for cost-efficient characterization and identification of unknown unknowns; We present the design of human computation … show the robustness of our …,https://dl.acm.org/doi/fullHtml/10.1145/3485447.3512040,https://scholar.google.com/scholar?q=related:UymEJImzjo0J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22human+computation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['hci', 'unknown unknowns']"
Yes,"Adversarial, Robust, Genralization",Adversarial Vertex Mixup: Toward Better Adversarially Robust Generalization,0.0,"Saehyung Lee, Hyungyu Lee, Sungroh Yoon, IEEE",2020.0,2020 Ieee/Cvf Conference On Computer Vision And Pattern Recognition (Cvpr),,,,100.0,2022-07-13 15:57:12,Proceedings Paper,10.1109/CVPR42600.2020.00035,1063-6919,,,,269.0,278.0,0,0.0,0.0,4.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'generalization', 'robust']"
No,"Uncertanty, Robust, Reinforcement Learning",Distributional robustness and regularization in statistical learning,119.0,"R Gao, X Chen, AJ Kleywegt",2017.0,arXiv preprint arXiv:1712.06050,asset-pdf.scinapse.io,https://asset-pdf.scinapse.io/prod/2781132081/2781132081.pdf,https://scholar.google.com/scholar?cites=8399804995882546287&as_sdt=2005&sciodt=2007&hl=en,100.0,2022-07-13 09:41:18,PDF,,,,,,,,119,23.8,40.0,3.0,5.0,"… A central question in statistical learning is to design algorithms that not only perform well on … robustness, and apply the regularization scheme to other machine learning problems. …",https://asset-pdf.scinapse.io/prod/2781132081/2781132081.pdf,https://scholar.google.com/scholar?q=related:b3QhwiMaknQJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['reinforcement learning', 'robust', 'uncertainty']"
Yes,"Robust, Adversarial",Certified adversarial robustness with additive noise,179.0,"B Li, C Chen, W Wang, L Carin",2019.0,Advances in neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2019/hash/335cd1b90bfa4ee70b39d08a4ae0cf2d-Abstract.html,https://scholar.google.com/scholar?cites=15944556675714796056&as_sdt=2005&sciodt=2007&hl=en,101.0,2022-07-12 11:56:54,,,,,,,,,179,60.07.00,45.0,4.0,3.0,"… guarantees have been studied intensively, yet most fail to obtain non-trivial robustness … adversarial examples. We establish a connection between robustness against adversarial …",https://proceedings.neurips.cc/paper/2019/file/335cd1b90bfa4ee70b39d08a4ae0cf2d-Paper.pdf,https://scholar.google.com/scholar?q=related:GEoQldFxRt0J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
Yes,"Robust Features, Optimization, Classification",Robust classification,63.0,"D Bertsimas, J Dunn, C Pawlowski…",2019.0,INFORMS Journal on …,pubsonline.informs.org,https://pubsonline.informs.org/doi/abs/10.1287/ijoo.2018.0001,https://scholar.google.com/scholar?cites=14341683299693310524&as_sdt=2005&sciodt=2007&hl=en,101.0,2022-07-14 10:58:18,,10.1287/ijoo.2018.0001,,,,,,,63,21.0,16.0,4.0,3.0,"… Machine Learning Repository. Furthermore, we identify characteristics of classification problems for which robust methods lead to significant accuracy … for machine-learning practitioners …",https://pubsonline.informs.org/doi/pdf/10.1287/ijoo.2018.0001,https://scholar.google.com/scholar?q=related:PDJfv_nkB8cJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'features', 'optimization']"
Yes,"Recurrent Neural Network, Framework, Robust",Cert-RNN: Towards Certifying the Robustness of Recurrent Neural Networks,2.0,"Tianyu Du, S. Ji, Lujia Shen, Yao Zhang, Jinfeng Li, Jie Shi, Chengfang Fang, Jianwei Yin, R. Beyah, Ting Wang",2021.0,,,,,102.0,2022-07-13 09:28:50,,10.1145/3460120.3484538,,,,,,,2,2.0,0.0,10.0,1.0,"Certifiable robustness, the functionality of verifying whether the given region surrounding a data point admits any adversarial example, provides guaranteed security for neural networks deployed in adversarial environments. A plethora of work has been proposed to certify the robustness of feed-forward networks, e.g., FCNs and CNNs. Yet, most existing methods cannot be directly applied to recurrent neural networks (RNNs), due to their sequential inputs and unique operations. In this paper, we present Cert-RNN, a general framework for certifying the robustness of RNNs. Specifically, through detailed analysis for the intrinsic property of the unique function in different ranges, we exhaustively discuss different cases for the exact formula of bounding planes, based on which we design several precise and efficient abstract transformers for the unique calculations in RNNs. Cert-RNN significantly outperforms the state-of-the-art methods (e.g., POPQORN) in terms of (i) effectiveness -- it provides much tighter robustness bounds, and (ii) efficiency -- it scales to much more complex models. Through extensive evaluation, we validate Cert-RNN's superior performance across various network architectures (e.g., vanilla RNN and LSTM) and applications (e.g., image classification, sentiment analysis, toxic comment detection, and malicious URL detection). For instance, for the RNN-2-32 model on the MNIST sequence dataset, the robustness bound certified by Cert-RNN is on average 1.86 times larger than that by POPQORN. Besides certifying the robustness of given RNNs, Cert-RNN also enables a range of practical applications including evaluating the provable effectiveness for various defenses (i.e., the defense with a larger robustness region is considered to be more robust), improving the robustness of RNNs (i.e., incorporating Cert-RNN with verified robust training) and identifying sensitive words (i.e., the word with the smallest certified robustness bound is considered to be the most sensitive word in a sentence), which helps build more robust and interpretable deep learning systems. We will open-source Cert-RNN for facilitating the DNN security research.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['framework', 'recurrent neural network', 'robust']"
Yes,"Deep Neural Network, classifier, adversarial, robust",Improve Adversarial Robustness via Weight Penalization on Classification Layer,1.0,"Cong Xu, Dan Li, Min Yang",2020.0,,,,,102.0,2022-07-13 09:26:01,,,,,,,,,1,0.5,0.0,3.0,2.0,"It is well-known that deep neural networks are vulnerable to adversarial attacks. Recent studies show that well-designed classification parts can lead to better robustness. However, there is still much space for improvement along this line. In this paper, we first prove that, from a geometric point of view, the robustness of a neural network is equivalent to some angular margin condition of the classifier weights. We then explain why ReLU type function is not a good choice for activation under this framework. These findings reveal the limitations of the existing approaches and lead us to develop a novel light-weight-penalized defensive method, which is simple and has a good scalability. Empirical results on multiple benchmark datasets demonstrate that our method can effectively improve the robustness of the network without requiring too much additional computation, while maintaining a high classification precision for clean data.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'classifier', 'deep learning', 'robust']"
Yes,"Robust, Benchmark",Advbox: a toolbox to generate adversarial examples that fool neural networks,29.0,"Dou Goodman, Xin Hao, Yang Wang, Yuesheng Wu, Junfeng Xiong, Huan Zhang",2020.0,,,,,102.0,2022-07-13 09:25:10,,,,,,,,,29,14.5,5.0,6.0,2.0,"In recent years, neural networks have been extensively deployed for computer vision tasks, particularly visual classification problems, where new algorithms reported to achieve or even surpass the human performance. Recent studies have shown that they are all vulnerable to the attack of adversarial examples. Small and often imperceptible perturbations to the input images are sufficient to fool the most powerful neural networks. \emph{Advbox} is a toolbox to generate adversarial examples that fool neural networks in PaddlePaddle, PyTorch, Caffe2, MxNet, Keras, TensorFlow and it can benchmark the robustness of machine learning models. Compared to previous work, our platform supports black box attacks on Machine-Learning-as-a-service, as well as more attack scenarios, such as Face Recognition Attack, Stealth T-shirt, and DeepFake Face Detect. The code is licensed under the Apache 2.0 and is openly available at this https URL. Advbox now supports Python 3.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['benchmark', 'robust']"
Yes,"Accuracy, Robust, Training, Graph Network",Certifiable robustness to graph perturbations,74.0,"A Bojchevski, S Günnemann",2019.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2019/hash/e2f374c3418c50bc30d67d5f7454a5b4-Abstract.html,https://scholar.google.com/scholar?cites=1934279975175671629&as_sdt=2005&sciodt=2007&hl=en,102.0,2022-07-13 08:56:29,,,,,,,,,74,25.07.00,37.0,2.0,3.0,Page 1. Certifiable Robustness to Graph Perturbations … Abstract Despite the exploding interest in graph neural networks there has been little effort to verify and improve their robustness …,https://proceedings.neurips.cc/paper/2019/file/e2f374c3418c50bc30d67d5f7454a5b4-Paper.pdf,https://scholar.google.com/scholar?q=related:TU9_oGPx1xoJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'graph neural network', 'robust', 'training']"
Yes,"Computer Vision, Data Augmentation, Robust",A fourier perspective on model robustness in computer vision,242.0,"D Yin, R Gontijo Lopes, J Shlens…",2019.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2019/hash/b05b57f6add810d3b7490866d74c0053-Abstract.html,https://scholar.google.com/scholar?cites=15027153125578915817&as_sdt=2005&sciodt=2007&hl=en,102.0,2022-07-12 11:49:54,,,,,,,,,242,81.07.00,61.0,4.0,3.0,"… approach for improving robustness, however robustness gains are typically not … robustness to corruptions that are concentrated in the high frequency domain while reducing robustness …",https://proceedings.neurips.cc/paper/2019/file/b05b57f6add810d3b7490866d74c0053-Paper.pdf,https://scholar.google.com/scholar?q=related:6T-IgyYsi9AJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'data augmentation', 'robust']"
No,"Adversarial Robustness, Loss Function, Smoothing",Improving adversarial robustness through progressive hardening,24.0,"C Sitawarin, S Chakraborty, D Wagner",2020.0,,,,https://scholar.google.com/scholar?cites=13614254380543856371&as_sdt=2005&sciodt=2007&hl=en,102.0,2022-07-13 09:41:18,CITATION,,,,,,,,24,12.0,8.0,3.0,2.0,,,https://scholar.google.com/scholar?q=related:804fSjqM77wJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'loss function', 'smoothing']"
Yes,"Interpretability, Face Recognition, Modular Neural Network",Interpretable and Robust Face Verification,0.0,"Preetam Prabhu Srikar Dammu, S. Chalamala, A. Singh, B. Yegnanarayana",2021.0,,,,,102.0,2022-07-13 10:08:47,,,,,,,,,0,0.0,0.0,4.0,1.0,"Advances in deep learning have been instrumental in enhancing the performance of face verification systems. Despite their ability to attain high accuracy, most of these systems fail to provide interpretations of their decisions. With the increased demands in making deep learning models more interpretable, numerous post-hoc methods have been proposed to probe the workings of these systems. Yet, the quest for face verification systems that inherently provide interpretations still remains largely unexplored. Additionally, most of the existing face recognition models are highly susceptible to adversarial attacks. In this work, we propose a face verification system which addresses the issue of interpretability by employing modular neural networks. In this, representations for each individual facial parts such as nose, mouth, eyes etc. are learned separately. We also show that our method is significantly more resistant to adversarial attacks, thereby addressing another crucial weakness concerning deep learning models.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'interpretability', 'neural network']"
No,"Generalization, Robust, Convolutional Neural Network",A robust modulation classification method using convolutional neural networks,36.0,"Siyang Zhou, Zhendong Yin, Zhilu Wu, Yunfei Chen, Nan Zhao, Zhutian Yang",2019.0,,,,,102.0,2022-07-13 09:40:19,,10.1186/s13634-019-0616-6,,,,,,,36,12.0,6.0,6.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'generalization', 'robust']"
Yes,"Text, embedding, robustness",TextShield: Robust Text Classification Based on Multimodal Embedding and Neural Machine Translation,19.0,"Jinfeng Li, Tianyu Du, S. Ji, Rong Zhang, Quan Lu, Min Yang, Ting Wang",2020.0,,,,,103.0,2022-07-13 09:38:44,,,,,,,,,19,9.5,3.0,7.0,2.0,"Text-based toxic content detection is an important tool for reducing harmful interactions in online social media environments. Yet, its underlying mechanism, deep learning-based text classification (DLTC), is inherently vulnerable to maliciously crafted adversarial texts. To mitigate such vulnerabilities, intensive research has been conducted on strengthening English-based DLTC models. However, the existing defenses are not effective for Chinese-based DLTC models, due to the unique sparseness, diversity, and variation of the Chinese language. In this paper, we bridge this striking gap by presenting TEXTSHIELD, a new adversarial defense framework specifically designed for Chinese-based DLTC models. TEXTSHIELD differs from previous work in several key aspects: (i) generic – it applies to any Chinese-based DLTC models without requiring re-training; (ii) robust – it significantly reduces the attack success rate even under the setting of adaptive attacks; and (iii) accurate – it has little impact on the performance of DLTC models over legitimate inputs. Extensive evaluations show that it outperforms both existing methods and the industry-leading platforms. Future work will explore its applicability in broader practical tasks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['embedding', 'nlp', 'robust']"
Yes,"Certified Robustness, Adversarial Attacks, Neural Network",Robustness Certificates Against Adversarial Examples for ReLU Networks,15.0,"Sahil Singla, S. Feizi",2019.0,,,,,103.0,2022-07-13 09:26:34,,,,,,,,,15,5.0,8.0,2.0,3.0,"While neural networks have achieved high performance in different learning tasks, their accuracy drops significantly in the presence of small adversarial perturbations to inputs. Defenses based on regularization and adversarial training are often followed by new attacks to defeat them. In this paper, we propose attack-agnostic robustness certificates for a multi-label classification problem using a deep ReLU network. Although computing the exact distance of a given input sample to the classification decision boundary requires solving a non-convex optimization, we characterize two lower bounds for such distances, namely the simplex certificate and the decision boundary certificate. These robustness certificates leverage the piece-wise linear structure of ReLU networks and use the fact that in a polyhedron around a given sample, the prediction function is linear. In particular, the proposed simplex certificate has a closed-form, is differentiable and is an order of magnitude faster to compute than the existing methods even for deep networks. In addition to theoretical bounds, we provide numerical results for our certificates over MNIST and compare them with some existing upper bounds.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'neural network', 'robust']"
No,"Federated Learning, Adversarial, Robust",Cronus: Robust and Heterogeneous Collaborative Learning with Black-Box Knowledge Transfer,51.0,"Hong Chang, Virat Shejwalkar, R. Shokri, A. Houmansadr",2019.0,,,,,103.0,2022-07-13 10:06:51,,,,,,,,,51,17.0,13.0,4.0,3.0,"Collaborative (federated) learning enables multiple parties to train a model without sharing their private data, but through repeated sharing of the parameters of their local models. Despite its advantages, this approach has many known privacy and security weaknesses and performance overhead, in addition to being limited only to models with homogeneous architectures. Shared parameters leak a significant amount of information about the local (and supposedly private) datasets. Besides, federated learning is severely vulnerable to poisoning attacks, where some participants can adversarially influence the aggregate parameters. Large models, with high dimensional parameter vectors, are in particular highly susceptible to privacy and security attacks: curse of dimensionality in federated learning. We argue that sharing parameters is the most naive way of information exchange in collaborative learning, as they open all the internal state of the model to inference attacks, and maximize the model's malleability by stealthy poisoning attacks. We propose Cronus, a robust collaborative machine learning framework. The simple yet effective idea behind designing Cronus is to control, unify, and significantly reduce the dimensions of the exchanged information between parties, through robust knowledge transfer between their black-box local models. We evaluate all existing federated learning algorithms against poisoning attacks, and we show that Cronus is the only secure method, due to its tight robustness guarantee. Treating local models as black-box, reduces the information leakage through models, and enables us using existing privacy-preserving algorithms that mitigate the risk of information leakage through the model's output (predictions). Cronus also has a significantly lower sample complexity, compared to federated learning, which does not bind its security to the number of participants.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'federated learning', 'robust']"
Yes,"Recommendation System, Robust, Adversarial",Adversarial training towards robust multimedia recommender system,84.0,"J Tang, X Du, X He, F Yuan, Q Tian…",2019.0,IEEE Transactions on …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8618394/?casa_token=9sIRER_eTrEAAAAA:M8vWjLopZ5eB-oqjwrFyGi2nMA3WLg88bJ7tymZ78ityvuMXQ8H9VbHc4DoZN656d9ncJYb4Kw,https://scholar.google.com/scholar?cites=9908345026635623913&as_sdt=2005&sciodt=2007&hl=en,104.0,2022-07-14 10:13:06,,,,,,,,,84,28.0,14.0,6.0,3.0,"… Adversarial Multimedia Recommendation (AMR), which can lead to a more robust multimedia recommender model by using adversarial … the robustness of machine learning models [24], …",https://ieeexplore.ieee.org/iel7/69/4358933/08618394.pdf?casa_token=FgVlgQ-M-pIAAAAA:9M_iZZnAxgTBKKxnMPJ4GYS1qhFZtFSaYFPfQObjjnbYCWEF25j_mh1hqAn29CZjU31L3pKvvg,https://scholar.google.com/scholar?q=related:6e2fGE6DgYkJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'recommender systems', 'robust']"
Yes,"Adversarial learning, object detection",Robust and accurate object detection via adversarial learning,19.0,"X Chen, C Xie, M Tan, L Zhang…",2021.0,Proceedings of the …,openaccess.thecvf.com,http://openaccess.thecvf.com/content/CVPR2021/html/Chen_Robust_and_Accurate_Object_Detection_via_Adversarial_Learning_CVPR_2021_paper.html,https://scholar.google.com/scholar?cites=8271637616216749842&as_sdt=2005&sciodt=2007&hl=en,105.0,2022-07-13 17:19:08,,,,,,,,,19,19.0,4.0,5.0,1.0,"… We also test the detectors’ robustness to natural corruptions on the COCO-C dataset [27], including 15 types of corruption each with 5 severity levels. Finally, we apply the detectors to …",http://openaccess.thecvf.com/content/CVPR2021/papers/Chen_Robust_and_Accurate_Object_Detection_via_Adversarial_Learning_CVPR_2021_paper.pdf,https://scholar.google.com/scholar?q=related:ErdbQZbCynIJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'computer vision']"
Yes,"GNN, fairness, fair representation, stable representation",Towards a unified framework for fair and stable graph representation learning,32.0,"C Agarwal, H Lakkaraju…",2021.0,… in Artificial Intelligence,proceedings.mlr.press,https://proceedings.mlr.press/v161/agarwal21b,https://scholar.google.com/scholar?cites=12725550151796970032&as_sdt=2005&sciodt=2007&hl=en,105.0,2022-07-12 13:41:36,,,,,,,,,32,32.0,11.0,3.0,1.0,"… tween counterfactual fairness and stability. While stability accounts for robustness wrt small … , counterfactual fairness accounts for robustness wrt modifications of the sensitive attribute. …",https://proceedings.mlr.press/v161/agarwal21b/agarwal21b.pdf,https://scholar.google.com/scholar?q=related:MNJGsXA8mrAJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22stability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['fairness', 'graph neural network', 'stability']"
No,"Classification, Robust",A Robust Classifier Ensemble for Improving the Performance of Classification,1.0,"H Parvin, S Parvin",2012.0,… Conference on Artificial Intelligence,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/6387214/,https://scholar.google.com/scholar?cites=1821751531127174491&as_sdt=2005&sciodt=2007&hl=en,105.0,2022-07-14 09:16:55,,,,,,,,,1,0.1,1.0,2.0,10.0,"Usage of recognition systems has found many applications in almost all fields. Generally in design of multiple classifier systems, the more diverse the results of the classifiers, the more …",,https://scholar.google.com/scholar?q=related:W_XbkV0pSBkJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22performance%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'robust']"
Yes,"Robust, Adversarial Training, Convolutional Neural Network",Investigation of Adversarial Robust Training for Establishing Interpretable CNN-based Numerical Observers,0.0,S. Sengupta,2022.0,Progress in Biomedical Optics and Imaging - Proceedings of SPIE,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85131892540&origin=inward,106.0,2022-07-12 16:28:40,Conference Paper,10.1117/12.2613220,1605-7422,https://api.elsevier.com/content/abstract/scopus_id/85131892540,12035.0,,,,0,0.0,0.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial training', 'convolutional neural network', 'robust']"
Yes,"Robust, Generalize, Natural Language Inference",Behavior Analysis of NLI Models: Uncovering the Influence of Three Factors on Robustness,0.0,"VI Sanchez Carmona, J Mitchell, S Riedel",2018.0,arXiv e-prints,ui.adsabs.harvard.edu,https://ui.adsabs.harvard.edu/abs/2018arXiv180504212S/abstract,,107.0,2022-07-13 08:53:52,,,,,,,,,0,0.0,0.0,3.0,4.0,"… this single evaluation metric to examine robustness to semantically … In particular, while strong performance is possible on … and Language; Computer Science - Artificial Intelligence. E-Print …",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generalization', 'nlp', 'robust']"
Yes,"Graph Neural Network, Robust, Embedding",Iterative deep graph learning for graph neural networks: Better and robust node embeddings,35.0,Y. Chen,2020.0,Advances in Neural Information Processing Systems,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85108315659&origin=inward,108.0,2022-07-12 16:34:58,Conference Paper,,1049-5258,https://api.elsevier.com/content/abstract/scopus_id/85108315659,2020.0,,,,35,17.5,35.0,1.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['embedding', 'graph neural network', 'robust']"
No,adversarial robustness,Training Efficiency and Robustness in Deep Learning,0.0,F Faghri,2021.0,arXiv preprint arXiv:2112.01423,arxiv.org,https://arxiv.org/abs/2112.01423,,109.0,2022-07-12 11:49:54,,,,,,,,,0,0.0,0.0,1.0,1.0,"… machine learning and artificial intelligence, achieving … improve the training efficiency and robustness of deep learning models. … Finally, we study adversarial robustness in deep learning …",https://arxiv.org/pdf/2112.01423,https://scholar.google.com/scholar?q=related:0v1Mw6szoSkJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['adversarial robustness']
No,"Uncertainty, Deep Learning, Out-of-Distribution Data",Improving Robustness of Deep Neural Networks for Aerial Navigation by Incorporating Input Uncertainty,1.0,"F Arnez, H Espinoza, A Radermacher…",2021.0,… Conference on Computer …,Springer,https://link.springer.com/chapter/10.1007/978-3-030-83906-2_17,https://scholar.google.com/scholar?cites=11187852938631128269&as_sdt=2005&sciodt=2007&hl=en,109.0,2022-07-13 14:02:43,,10.1007/978-3-030-83906-2_17,,,,,,,1,1.0,0.0,4.0,1.0,… their inability to represent confidence in their predictions. These … ) uncertainties to represent the confidence in the outputs. … neural network for control in an autonomous navigation task. …,https://arxiv.org/pdf/2110.13729,https://scholar.google.com/scholar?q=related:zSRrzvg8Q5sJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'out-of-distribution data', 'uncertainty']"
Yes,"adversarial, robustness, assessment",Hierarchical verification for adversarial robustness,1.0,"CH Lim, R Urtasun, E Yumer",2020.0,… on Machine Learning,proceedings.mlr.press,https://proceedings.mlr.press/v119/lim20b.html,https://scholar.google.com/scholar?cites=15022888517785921517&as_sdt=2005&sciodt=2007&hl=en,109.0,2022-07-13 09:47:56,,,,,,,,,1,0.5,0.0,3.0,2.0,"… We introduce a new framework for the exact pointwise lp robustness verification problem … input space, and one can verify the lp robustness around a point by checking all the activation …",http://proceedings.mlr.press/v119/lim20b/lim20b.pdf,https://scholar.google.com/scholar?q=related:7afsIoMFfNAJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'evaluation', 'robust']"
No,"Adversarial, Robust, Training",Adversarially Robust Neural Networks via Optimal Control: Bridging Robustness with Lyapunov Stability,0.0,"Z Chen, H Su",2019.0,,openreview.net,https://openreview.net/forum?id=BklVA2NYvH,,110.0,2022-07-13 10:30:58,,,,,,,,,0,0.0,0.0,2.0,3.0,"… In this paper, we bridge adversarial robustness of neural nets with Lyapunov stability of … periments show that our method effectively improves deep model’s adversarial robustness. …",https://openreview.net/pdf?id=BklVA2NYvH,https://scholar.google.com/scholar?q=related:Guc2ObYZUxMJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22stability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust', 'training']"
Yes,"Data Augmentation, Robust, Adversarial",Are Labels Required for Improving Adversarial Robustness?,195.0,"Jonathan Uesato, Jean-Baptiste Alayrac, Po-Sen Huang, Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli",2019.0,,,,,110.0,2022-07-13 09:22:57,,,,,,,,,195,65.0,33.0,6.0,3.0,"Recent work has uncovered the interesting (and somewhat surprising) finding that training models to be invariant to adversarial perturbations requires substantially larger datasets than those required for standard classification. This result is a key hurdle in the deployment of robust machine learning models in many real world applications where labeled data is expensive. Our main insight is that unlabeled data can be a competitive alternative to labeled data for training adversarially robust models. Theoretically, we show that in a simple statistical setting, the sample complexity for learning an adversarially robust model from unlabeled data matches the fully supervised case up to constant factors. On standard datasets like CIFAR-10, a simple Unsupervised Adversarial Training (UAT) approach using unlabeled data improves robust accuracy by 21.7% over using 4K supervised examples alone, and captures over 95% of the improvement from the same number of labeled examples. Finally, we report an improvement of 4% over the previous state-of-the-art on CIFAR-10 against the strongest known attack by using additional unlabeled data from the uncurated 80 Million Tiny Images dataset. This demonstrates that our finding extends as well to the more realistic case where unlabeled data is also uncurated, therefore opening a new avenue for improving adversarial training.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'data augmentation', 'robust']"
Yes,"Visual, Robust",Certifying geometric robustness of neural networks,71.0,"M Balunovic, M Baader, G Singh…",2019.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2019/hash/f7fa6aca028e7ff4ef62d75ed025fe76-Abstract.html,https://scholar.google.com/scholar?cites=9475017515216465786&as_sdt=2005&sciodt=2007&hl=en,111.0,2022-07-13 11:20:18,,,,,,,,,71,24.07.00,18.0,4.0,3.0,"… robustness to changes in pixel intensity (eg, [6, 7, 8]), only the recent work of [9] proposed a method to certify robustness to … To the best of our knowledge, DEEPG is currently the state-of-…",https://proceedings.neurips.cc/paper/2019/file/f7fa6aca028e7ff4ef62d75ed025fe76-Paper.pdf,https://scholar.google.com/scholar?q=related:eu8UszoGfoMJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['robust', 'visual']"
No,robustness to distribution shifts,Toward learning human-aligned cross-domain robust models by countering misaligned features,4.0,"H Wang, Z Huang, H Zhang, E Xing",2021.0,arXiv preprint arXiv:2111.03740,arxiv.org,https://arxiv.org/abs/2111.03740,https://scholar.google.com/scholar?cites=12439667863377323696&as_sdt=2005&sciodt=2007&hl=en,111.0,2022-07-14 09:57:23,,,,,,,,,4,4.0,1.0,4.0,1.0,"… Machine learning has demonstrated remarkable prediction accuracy over iid data, but the … linked to many previous methods in robust machine learning literature. We also compared the …",https://arxiv.org/pdf/2111.03740,https://scholar.google.com/scholar?q=related:sKZyeAOUoqwJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['distribution shift']
Yes,"Generalization, Robust, Convolutional Neural Network",A scale robust convolutional neural network for automatic building extraction from aerial and satellite imagery,83.0,"S Ji, S Wei, M Lu",2019.0,International journal of remote sensing,Taylor & Francis,https://www.tandfonline.com/doi/abs/10.1080/01431161.2018.1528024,https://scholar.google.com/scholar?cites=4884056930463774325&as_sdt=2005&sciodt=2007&hl=en,111.0,2022-07-14 13:30:21,,10.1080/01431161.2018.1528024,,,,,,,83,28.07.00,28.0,3.0,3.0,"… In this study, we develop a scale robust CNN structure to improve the segmentation accuracy of building data from high-resolution aerial and satellite images. Based on a fully …",,https://scholar.google.com/scholar?q=related:dfYTw-anx0MJ:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'generalization', 'robust']"
Discussion,"Robust, Explanation",Evaluations and Methods for Explanation through Robustness Analysis,21.0,"Cheng-Yu Hsieh, Chih-Kuan Yeh, Xuanqing Liu, Pradeep Ravikumar, Seungyeon Kim, Sanjiv Kumar, Cho-Jui Hsieh",2019.0,,,,,111.0,2022-07-13 09:22:24,,,,,,,,,21,7.0,3.0,7.0,3.0,"Among multiple ways of interpreting a machine learning model, measuring the importance of a set of features tied to a prediction is probably one of the most intuitive ways to explain a model. In this paper, we establish the link between a set of features to a prediction with a new evaluation criterion, robustness analysis, which measures the minimum distortion distance of adversarial perturbation. By measuring the tolerance level for an adversarial attack, we can extract a set of features that provides the most robust support for a prediction, and also can extract a set of features that contrasts the current prediction to a target class by setting a targeted adversarial attack. By applying this methodology to various prediction tasks across multiple domains, we observe the derived explanations are indeed capturing the significant feature set qualitatively and quantitatively.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'robust']"
Yes,"robust, adversarial, benchmark",Human Uncertainty Makes Classification More Robust,108.0,"Joshua C. Peterson, R. Battleday, T. Griffiths, Olga Russakovsky",2019.0,,,,,112.0,2022-07-13 10:11:42,,10.1109/ICCV.2019.00971,,,,,,,108,36.0,27.0,4.0,3.0,"The classification performance of deep neural networks has begun to asymptote at near-perfect levels. However, their ability to generalize outside the training set and their robustness to adversarial attacks have not. In this paper, we make progress on this problem by training with full label distributions that reflect human perceptual uncertainty. We first present a new benchmark dataset which we call CIFAR10H, containing a full distribution of human labels for each image of the CIFAR10 test set. We then show that, while contemporary classifiers fail to exhibit human-like uncertainty on their own, explicit training on our dataset closes this gap, supports improved generalization to increasingly out-of-training-distribution test datasets, and confers robustness to adversarial attacks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'benchmark', 'robust']"
Yes,"Robust, Support Vector Machine, Label Noise",Improved Sparsity of Support Vector Machine with Robustness Towards Label Noise Based on Rescaled α -Hinge Loss with Non-smooth Regularizer,1.0,"M Singla, D Ghosh, KK Shukla",2020.0,Neural Processing Letters,Springer,https://link.springer.com/article/10.1007/s11063-020-10346-0,https://scholar.google.com/scholar?cites=1342182912716465041&as_sdt=2005&sciodt=2007&hl=en,112.0,2022-07-13 11:05:20,,10.1007/s11063-020-10346-0,,,,,,,1,0.5,0.0,3.0,2.0,… obtain the robustness is popular in robust machine learning. … not only improved the robustness but also improved the … robust to outliers and also added robustness to it. SVM was also …,https://scholar.google.com/scholar?output=instlink&q=info:kWMQeD9koBIJ:scholar.google.com/&hl=en&as_sdt=2007&scillfp=15025315493160115231&oi=lle,https://scholar.google.com/scholar?q=related:kWMQeD9koBIJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['label noise', 'robust', 'support vector machine']"
Discussion,"Evaluation of ML, interpretability, trust",Quantifying interpretability and trust in machine learning systems,80.0,"P Schmidt, F Biessmann",2019.0,arXiv preprint arXiv:1901.08558,arxiv.org,https://arxiv.org/abs/1901.08558,https://scholar.google.com/scholar?cites=8900590612697221730&as_sdt=2005&sciodt=2007&hl=en,113.0,2022-07-13 10:52:14,,,,,,,,,80,27.07.00,40.0,2.0,3.0,"… Moreover our approach relates directly to interpretability and not to computational efficiency, as in (Lundberg and Lee 2017), or to robustness of the model under perturbations as in (…",https://arxiv.org/pdf/1901.08558,https://scholar.google.com/scholar?q=related:Yor-QAFAhXsJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22interpretability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'interpretability', 'trustworthy']"
Yes,"theoretical framework, trustworthy",Designing trustworthy AI: A human-machine teaming framework to guide development,14.0,CJ Smith,2019.0,arXiv preprint arXiv:1910.03515,arxiv.org,https://arxiv.org/abs/1910.03515,https://scholar.google.com/scholar?cites=14226673766617249223&as_sdt=2005&sciodt=2007&hl=en,113.0,2022-07-14 09:14:15,,,,,,,,,14,5.07,14.0,1.0,3.0,"… Artificial intelligence (AI) holds great promise to empower us … Diverse teams are needed to build trustworthy artificial intel… Last, the system must be built in a robust, valid, and reliable …",https://arxiv.org/pdf/1910.03515,https://scholar.google.com/scholar?q=related:x_WWImpMb8UJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22trustworthy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['framework', 'trustworthy']"
Yes,"Robust, Adversarial, Accuracy, Stability",Adversarially Robust Generalization Just Requires More Unlabeled Data,89.0,"Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, J. Hopcroft, Liwei Wang",2019.0,,,,,113.0,2022-07-13 09:27:51,,,,,,,,,89,30.07.00,13.0,7.0,3.0,"Neural network robustness has recently been highlighted by the existence of adversarial examples. Many previous works show that the learned networks do not perform well on perturbed test data, and significantly more labeled data is required to achieve adversarially robust generalization. In this paper, we theoretically and empirically show that with just more unlabeled data, we can learn a model with better adversarially robust generalization. The key insight of our results is based on a risk decomposition theorem, in which the expected robust risk is separated into two parts: the stability part which measures the prediction stability in the presence of perturbations, and the accuracy part which evaluates the standard classification accuracy. As the stability part does not depend on any label information, we can optimize this part using unlabeled data. We further prove that for a specific Gaussian mixture problem illustrated by [35], adversarially robust generalization can be almost as easy as the standard generalization in supervised learning if a sufficiently large amount of unlabeled data is provided. Inspired by the theoretical findings, we propose a new algorithm called PASS by leveraging unlabeled data during adversarial training. We show that in the transductive and semi-supervised settings, PASS achieves higher robust accuracy and defense success rate on the Cifar-10 task.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial', 'robust', 'stability']"
Yes,"Adversarial Robustness, Attention, Computer Vision",Improving Adversarial Robustness via Attention and Adversarial Logit Pairing,0.0,"X Li, D Goodman, J Liu, T Wei…",2021.0,… in Artificial Intelligence,ncbi.nlm.nih.gov,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8829878/,,114.0,2022-07-12 11:56:54,HTML,,,,,,,,0,0.0,0.0,5.0,1.0,… adversarial examples over adversarial training. We show that AT+ ALP can effectively increase the average activations of adversarial … features to improve the robustness of the model. …,https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8829878/,https://scholar.google.com/scholar?q=related:QONWqnh44K4J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'attention', 'computer vision']"
Yes,"Adversarial, Accuracy, Robust, Training",Adversarial concurrent training: Optimizing robustness and accuracy trade-off of deep neural networks,5.0,"E Arani, F Sarfraz, B Zonooz",2020.0,arXiv preprint arXiv:2008.07015,arxiv.org,https://arxiv.org/abs/2008.07015,https://scholar.google.com/scholar?cites=10984570156716186250&as_sdt=2005&sciodt=2007&hl=en,114.0,2022-07-13 10:35:21,,,,,,,,,5,2.5,2.0,3.0,2.0,"… robustness of models. However, there seems to be an inherent trade-off between optimizing the model for accuracy and robustness… standard accuracy and 44.29% robustness accuracy …",https://arxiv.org/pdf/2008.07015,https://scholar.google.com/scholar?q=related:itps2VsIcZgJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial', 'robust', 'training']"
Yes,"Generative Adversarial Network, Metric, Noise",MetricGAN+/-: Increasing Robustness of Noise Reduction on Unseen Data,0.0,"G Close, T Hain, S Goetze",2022.0,arXiv preprint arXiv:2203.12369,arxiv.org,https://arxiv.org/abs/2203.12369,,114.0,2022-07-13 11:05:20,,,,,,,,,0,0.0,0.0,3.0,1.0,… Eight noise files are sourced from the DEMAND [20] noise … and two others a babble noise and a speech-shaped noise. The … to generalise to unseen noise types and recording scenarios …,https://arxiv.org/pdf/2203.12369,https://scholar.google.com/scholar?q=related:N4R57_-MB3cJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generative adversarial networks', 'metric', 'noise']"
No,"Noise, Emotion Recognition, Robust",Improving noise robustness of speech emotion recognition system,10.0,Ł Juszkiewicz,2014.0,Intelligent Distributed Computing VII,Springer,https://link.springer.com/chapter/10.1007/978-3-319-01571-2_27,https://scholar.google.com/scholar?cites=12860437866912044385&as_sdt=2005&sciodt=2007&hl=en,115.0,2022-07-13 11:05:20,,10.1007/978-3-319-01571-2_27,,,,,,,10,1.25,10.0,1.0,8.0,… Weka 3 (Waikato Environment for Knowledge Analysis) is a popular suite of machine learning software written in Java. It contains a collection of visualization tools and algorithms for …,,https://scholar.google.com/scholar?q=related:YVVGORp0ebIJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'noise', 'robust']"
Yes,evaluation of explanations,Towards Robust Explanations for Deep Neural Networks,14.0,"Ann-Kathrin Dombrowski, Christopher J. Anders, K. Müller, P. Kessel",2020.0,,,,,115.0,2022-07-13 10:07:43,,10.1016/j.patcog.2021.108194,,,,,,,14,7.0,4.0,4.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'explainability']"
Yes,"deep neural network, robust, adversarial, generative adversarial network",Enhancing the Robustness of Deep Neural Networks by Boundary Conditional GAN,12.0,"Ke Sun, Zhanxing Zhu, Zhouchen Lin",2019.0,,,,,115.0,2022-07-13 09:26:34,,,,,,,,,12,4.0,4.0,3.0,3.0,"Deep neural networks have been widely deployed in various machine learning tasks. However, recent works have demonstrated that they are vulnerable to adversarial examples: carefully crafted small perturbations to cause misclassification by the network. In this work, we propose a novel defense mechanism called Boundary Conditional GAN to enhance the robustness of deep neural networks against adversarial examples. Boundary Conditional GAN, a modified version of Conditional GAN, can generate boundary samples with true labels near the decision boundary of a pre-trained classifier. These boundary samples are fed to the pre-trained classifier as data augmentation to make the decision boundary more robust. We empirically show that the model improved by our approach consistently defenses against various types of adversarial attacks successfully. Further quantitative investigations about the improvement of robustness and visualization of decision boundaries are also provided to justify the effectiveness of our strategy. This new defense mechanism that uses boundary samples to enhance the robustness of networks opens up a new way to defense adversarial attacks consistently.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'deep learning', 'generative adversarial networks', 'robust']"
Yes,"Spiking Neural Network, Perturbations, Robust",Robustness of classification ability of spiking neural networks,8.0,"J Yang, P Zhang, Y Liu",2015.0,Nonlinear Dynamics,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11071-015-2190-2&casa_token=vwgYyMt9StgAAAAA:UmuCK-UEbqCmmtfIcADCtXCxvGAzTuRVua04pAN_ItruoFonpwmEiayX-ctnvED-i_iphI2zkD423H2n6A,https://scholar.google.com/scholar?cites=10992732618057371623&as_sdt=2005&sciodt=2007&hl=en,115.0,2022-07-13 14:12:47,HTML,10.1007/s11071-015-2190-2,,,,,,,8,1.14,3.0,3.0,7.0,"… robustness of an artificial neural network is important for its application. In this paper, we focus on the robustness … the asymptotic stability of delay nonlinear cellular neural network. By …",https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11071-015-2190-2&casa_token=vwgYyMt9StgAAAAA:UmuCK-UEbqCmmtfIcADCtXCxvGAzTuRVua04pAN_ItruoFonpwmEiayX-ctnvED-i_iphI2zkD423H2n6A,https://scholar.google.com/scholar?q=related:588o-hIIjpgJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22stability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['perturbations', 'robust', 'spiking neural network']"
Yes,"Ensemble Classifier, Support Vector Machine, Adversarial Attacks",Gradient Correlation: Are Ensemble Classifiers More Robust Against Evasion Attacks in Practical Settings?,4.0,F. Zhang,2018.0,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85055819395&origin=inward,115.0,2022-07-12 16:30:36,Conference Paper,10.1007/978-3-030-02922-7_7,0302-9743,https://api.elsevier.com/content/abstract/scopus_id/85055819395,11233.0,,96.0,110.0,4,1.0,4.0,1.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'ensemble classifier', 'support vector machine']"
Yes,"adversarial robustness, feature robustness, computer vision, neural networks, mitigation method",Dropping pixels for adversarial robustness,6.0,"H Hosseini, S Kannan…",2019.0,Proceedings of the IEEE …,openaccess.thecvf.com,http://openaccess.thecvf.com/content_CVPRW_2019/html/CV-COPS/Hosseini_Dropping_Pixels_for_Adversarial_Robustness_CVPRW_2019_paper.html,https://scholar.google.com/scholar?cites=18031072871371986485&as_sdt=2005&sciodt=2007&hl=en,115.0,2022-07-13 09:30:22,,,,,,,,,6,2.0,2.0,3.0,3.0,"… to improve both accuracy and robustness. Random subsampling of … Nevertheless, we show that it provides robustness against … fier and E be the explanation function that maps inputs to …",http://openaccess.thecvf.com/content_CVPRW_2019/papers/CV-COPS/Hosseini_Dropping_Pixels_for_Adversarial_Robustness_CVPRW_2019_paper.pdf,https://scholar.google.com/scholar?q=related:NVZPaT49O_oJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'computer vision', 'features', 'mitigation method', 'neural networks']"
Yes,"Robust, Convolutional Neural Network",Combining Different V1 Brain Model Variants to Improve Robustness to Image Corruptions in CNNs,0.0,"A. Baidya, Joel Dapello, J. DiCarlo, Tiago Marques",2021.0,,,,,116.0,2022-07-13 09:27:29,,,,,,,,,0,0.0,0.0,4.0,1.0,"While some convolutional neural networks (CNNs) have surpassed human visual abilities in object classiﬁcation, they often struggle to recognize objects in images corrupted with different types of common noise patterns, highlighting a major limitation of this family of models. Recently, it has been shown that simulating a primary visual cortex (V1) at the front of CNNs leads to small improvements in robustness to these image perturbations. In this study, we start with the observation that different variants of the V1 model show gains for speciﬁc corruption types. We then build a new model using an ensembling technique, which combines multiple individual models with different V1 front-end variants. The model ensemble lever-ages the strengths of each individual model, leading to signiﬁcant improvements in robustness across all corruption categories and outperforming the base model by 38% on average. Finally, we show that using distillation it is possible to partially compress the knowledge in the ensemble model into a single model with a V1 front-end. While the ensembling and distillation techniques used here are hardly biologically-plausible, the results presented here demonstrate that by combining the speciﬁc strengths of different neuronal circuits in V1 it is possible to improve the robustness of CNNs for a wide range of perturbations.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'robust']"
Yes,"Smoothing, Classification, Robust",SmoothMix: Training Confidence-calibrated Smoothed Classifiers for Certified Robustness,6.0,"Jongheon Jeong, Sejun Park, Minkyu Kim, Heung-Chang Lee, Do-Guk Kim, Jinwoo Shin",2021.0,,,,,116.0,2022-07-13 09:27:43,,,,,,,,,6,6.0,1.0,6.0,1.0,"Randomized smoothing is currently a state-of-the-art method to construct a certifiably robust classifier from neural networks against l2-adversarial perturbations. Under the paradigm, the robustness of a classifier is aligned with the prediction confidence, i.e., the higher confidence from a smoothed classifier implies the better robustness. This motivates us to rethink the fundamental trade-off between accuracy and robustness in terms of calibrating confidences of a smoothed classifier. In this paper, we propose a simple training scheme, coined SmoothMix, to control the robustness of smoothed classifiers via self-mixup: it trains on convex combinations of samples along the direction of adversarial perturbation for each input. The proposed procedure effectively identifies over-confident, near off-class samples as a cause of limited robustness in case of smoothed classifiers, and offers an intuitive way to adaptively set a new decision boundary between these samples for better robustness. Our experimental results demonstrate that the proposed method can significantly improve the certified l2-robustness of smoothed classifiers compared to existing state-of-the-art robust training methods.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'robust', 'smoothing']"
Yes,"Support Vector Machines, Robust, Classification, Accuracy",Adding robustness to support vector machines against adversarial reverse engineering,33.0,"IM Alabdulmohsin, X Gao, X Zhang",2014.0,Proceedings of the 23rd ACM …,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/2661829.2662047,https://scholar.google.com/scholar?cites=7259684188712605294&as_sdt=2005&sciodt=2007&hl=en,116.0,2022-07-12 11:56:54,,10.1145/2661829.2662047,,,,,,,33,4.13,11.0,3.0,8.0,"… In Section 2, we review existing literature on adversarial learning, and discuss where the reverse … Using the accuracyvariance tradeoff curves, we show that robustness against reverse …",https://dl.acm.org/doi/pdf/10.1145/2661829.2662047,https://scholar.google.com/scholar?q=related:bha8YU6Uv2QJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'classification', 'robust', 'support vector machine']"
Yes,"Self-supervision, Label Noise, Jacobian",Investigating Why Contrastive Learning Benefits Robustness Against Label Noise,0.0,"Y Xue, K Whitecross, B Mirzasoleiman",2022.0,arXiv preprint arXiv:2201.12498,arxiv.org,https://arxiv.org/abs/2201.12498,,117.0,2022-07-13 11:05:20,,,,,,,,,0,0.0,0.0,3.0,1.0,"… of deep networks trained with contrastive learning against label noise. In … noise, or random label flipping. We prove that contrastive learning boosts robustness under extreme noise by …",https://arxiv.org/pdf/2201.12498,https://scholar.google.com/scholar?q=related:WnvJJtmmcf4J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['jacobian', 'label noise', 'self-supervision']"
Yes,"Training, Adversarial, Robust",Deep Defense: Training DNNs with Improved Adversarial Robustness,70.0,"Ziang Yan, Yiwen Guo, Changshui Zhang",2018.0,,,,,117.0,2022-07-13 09:26:34,,,,,,,,,70,17.5,23.0,3.0,4.0,"Despite the efficacy on a variety of computer vision tasks, deep neural networks (DNNs) are vulnerable to adversarial attacks, limiting their applications in security-critical systems. Recent works have shown the possibility of generating imperceptibly perturbed image inputs (a.k.a., adversarial examples) to fool well-trained DNN classifiers into making arbitrary predictions. To address this problem, we propose a training recipe named ""deep defense"". Our core idea is to integrate an adversarial perturbation-based regularizer into the classification objective, such that the obtained models learn to resist potential attacks, directly and precisely. The whole optimization problem is solved just like training a recursive network. Experimental results demonstrate that our method outperforms training with adversarial/Parseval regularizations by large margins on various datasets (including MNIST, CIFAR-10 and ImageNet) and different DNN architectures. Code and models for reproducing our results are available at this https URL",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust', 'training']"
Yes,"adverarial robustness, malware",Detection and robustness evaluation of android malware classifiers,0.0,"M. Anupama, P. Vinod, C. A. Visaggio, M. Arya, Josna Philomina, Rincy Raphael, Anson Pinhero, K. S. Ajith, P. Mathiyalagan",2021.0,,,,,117.0,2022-07-13 09:28:00,,10.1007/s11416-021-00390-2,,,,,,,0,0.0,0.0,9.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'cybersecurity']"
Yes,"Neural Network, Uncertainty",A t-Distribution Based Operator for Enhancing Out of Distribution Robustness of Neural Network Classifiers,1.0,"N Antonello, PN Garner",2020.0,IEEE Signal Processing Letters,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9115250/,https://scholar.google.com/scholar?cites=16743016879475944078&as_sdt=2005&sciodt=2007&hl=en,118.0,2022-07-13 10:25:08,,,,,,,,,1,0.5,1.0,2.0,2.0,"… Other techniques rely on learning confidence measures using … In this paper a novel method that produces reliable confidence … for benchmarking machine learning algorithms,” 2017, …",https://arxiv.org/pdf/2006.05389,https://scholar.google.com/scholar?q=related:jmqIwSQlW-gJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural network', 'uncertainty']"
Yes,"Anomaly Dection, Outliers, Autoencoder",Robust Anomaly Detection in Images Using Adversarial Autoencoders,18.0,L. Beggel,2020.0,Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics),,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85084806698&origin=inward,119.0,2022-07-12 16:32:47,Conference Paper,10.1007/978-3-030-46150-8_13,0302-9743,https://api.elsevier.com/content/abstract/scopus_id/85084806698,11906.0,,206.0,222.0,18,9.0,18.0,1.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['anomaly detection', 'autoencoder', 'outliers']"
Yes,"Active Learning, Out-of-Distribution Data, Accuracy",Mitigating Sampling Bias and Improving Robustness in Active Learning,1.0,"R Krishnan, A Sinha, N Ahuja, M Subedar…",2021.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2109.06321,https://scholar.google.com/scholar?cites=11524658959264879035&as_sdt=2005&sciodt=2007&hl=en,119.0,2022-07-13 10:25:08,,,,,,,,,1,1.0,0.0,5.0,1.0,"… between model confidence and accuracy as defined in Equation (6). It is computed after dividing the confidence range [0,1] … Journal of machine learning research, 2(Nov):45–66, 2001. …",https://arxiv.org/pdf/2109.06321,https://scholar.google.com/scholar?q=related:u5GBYEPQ758J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'active learning', 'out-of-distribution data']"
Yes,"Active Learning, Out-of-Distribution Data, Dataset",Robust adversarial active learning with a novel diversity constraint,1.0,"C Sun, H Sun, X Liu",2020.0,… Conference on Big Data (Big Data),ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9377910/,https://scholar.google.com/scholar?cites=13936031293972881777&as_sdt=2005&sciodt=2007&hl=en,119.0,2022-07-13 17:19:08,,,,,,,,,1,0.5,0.0,3.0,2.0,"… In this paper, we propose a robust adversarial active learning method that performs well on datasets with OoD samples. First, we incorporate recent advances in adversarial networks …",,https://scholar.google.com/scholar?q=related:cSH7J6C6ZsEJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['active learning', 'dataset', 'out-of-distribution data']"
Yes,"Robust, Convolutional Neural Network, Prediction",A Learning Convolutional Neural Network Approach for Network Robustness Prediction,0.0,"Y Lou, R Wu, J Li, L Wang, X Li, G Chen",2022.0,arXiv preprint arXiv:2203.10552,arxiv.org,https://arxiv.org/abs/2203.10552,,119.0,2022-07-13 14:54:34,,,,,,,,,0,0.0,0.0,6.0,1.0,"… robustness and controllability robustness are introduced in this section, where connectivity robustness … , namely LFR-CNN, is developed for network robustness performance prediction, …",https://arxiv.org/pdf/2203.10552,https://scholar.google.com/scholar?q=related:NirTvpDXZlkJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22performance%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'prediction', 'robust']"
Yes,"Robust, Textual",Combating adversarial misspellings with robust word recognition,179.0,"D Pruthi, B Dhingra, ZC Lipton",2019.0,arXiv preprint arXiv:1905.11268,arxiv.org,https://arxiv.org/abs/1905.11268,https://scholar.google.com/scholar?cites=11987384049356380745&as_sdt=2005&sciodt=2007&hl=en,120.0,2022-07-14 10:13:06,,,,,,,,,179,60.07.00,60.0,3.0,3.0,"… To combat adversarial spelling mistakes, we propose placing a word recognition model in front of the downstream classifier. Our word recognition models build upon the RNN semi…",https://arxiv.org/pdf/1905.11268,https://scholar.google.com/scholar?q=related:SRoS8Ee-W6YJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['nlp', 'robust']"
No,"Adversarial, Robust, Pruned",Adversarial robustness of pruned neural networks,21.0,"L Wang, GW Ding, R Huang, Y Cao, YC Lui",2018.0,,openreview.net,https://openreview.net/forum?id=SJGrAisIz,https://scholar.google.com/scholar?cites=7437030077828452256&as_sdt=2005&sciodt=2007&hl=en,120.0,2022-07-13 10:35:21,,,,,,,,,21,5.25,4.0,5.0,4.0,"… drops earlier, which means less compression rate is allowed if we want to maintain the original robustness other than just the original classification accuracy. We suspect the reason is …",https://openreview.net/pdf?id=SJGrAisIz,https://scholar.google.com/scholar?q=related:oE_c5HOjNWcJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'pruning', 'robust']"
Yes,"Robust, Adversarial",DeepSafe: A Data-Driven Approach for Assessing Robustness of Neural Networks,67.0,"D. Gopinath, Guy Katz, C. Pasareanu, C. Barrett",2018.0,,,,,121.0,2022-07-13 09:26:34,,10.1007/978-3-030-01090-4_1,,,,,,,67,17.15,17.0,4.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
Yes,"adversarial robustness, mitigation method",Developing a Robust Defensive System against Adversarial Examples Using Generative Adversarial Networks,4.0,"S Taheri, A Khormali, M Salem, JS Yuan",2020.0,Big Data and Cognitive …,mdpi.com,https://www.mdpi.com/723596,https://scholar.google.com/scholar?cites=13994062155703569957&as_sdt=2005&sciodt=2007&hl=en,122.0,2022-07-14 12:23:15,,,,,,,,,4,2.0,1.0,4.0,2.0,"… After attacking the neural network, we generate a plethora of adversarial examples in an … approach to generate new attacks to help strengthen the neural network is the best defense. …",https://www.mdpi.com/2504-2289/4/2/11/pdf,https://scholar.google.com/scholar?q=related:Jc5yC2PlNMIJ:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'mitigation method']"
Yes,"Neural Network, Regularization, Matrix Factorization",Neural Network Regularization via Robust Weight Factorization,5.0,"Jan Rudy, Weiguang Ding, Daniel Jiwoong Im, Graham W. Taylor",2014.0,,,,,123.0,2022-07-13 10:08:47,,,,,,,,,5,1.03,1.0,4.0,8.0,"Regularization is essential when training large neural networks. As deep neural networks can be mathematically interpreted as universal function approximators, they are effective at memorizing sampling noise in the training data. This results in poor generalization to unseen data. Therefore, it is no surprise that a new regularization technique, Dropout, was partially responsible for the now-ubiquitous winning entry to ImageNet 2012 by the University of Toronto. Currently, Dropout (and related methods such as DropConnect) are the most effective means of regularizing large neural networks. These amount to efficiently visiting a large number of related models at training time, while aggregating them to a single predictor at test time. The proposed FaMe model aims to apply a similar strategy, yet learns a factorization of each weight matrix such that the factors are robust to noise.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['matrix factorization', 'neural network', 'regularization']"
Yes,"Adversarial Attacks, Neural Network, Robust",Robustra: Training Provable Robust Neural Networks over Reference Adversarial Space,11.0,"L. Li, Zexuan Zhong, B. Li, Tao Xie",2019.0,,,,,123.0,2022-07-13 09:38:44,,10.24963/ijcai.2019/654,,,,,,,11,"0,1715277778",3.0,4.0,3.0,"Machine learning techniques, especially deep neural networks (DNNs), have been widely adopted in various applications. However, DNNs are recently found to be vulnerable against adversarial examples, i.e., maliciously perturbed inputs that can mislead the models to make arbitrary prediction errors. Empirical defenses have been studied, but many of them can be adaptively attacked again.  Provable defenses provide provable error bound of DNNs, while such bound so far is far from satisfaction.  To address this issue, in this paper, we present our approach named Robustra for effectively improving the provable error bound of DNNs.  We leverage the adversarial space of a reference model as the feasible region to solve the min-max game between the attackers and defenders.  We solve its dual problem by linearly approximating the attackers' best strategy and utilizing the monotonicity of the slack variables introduced by the reference model. The evaluation results show that our approach can provide significantly better provable adversarial error bounds on MNIST and CIFAR10 datasets, compared to the state-of-the-art results. In particular, bounded by L^infty, with epsilon = 0.1, on MNIST we reduce the error bound from 2.74% to 2.09%; with epsilon = 0.3, we reduce the error bound from 24.19% to 16.91%.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'neural network', 'robust']"
Yes,"Adversarial Attacks, Robust, Neural Architecture",On adversarial robustness: A neural architecture search perspective,2.0,"C Devaguptapu, D Agarwal, G Mittal…",2021.0,Proceedings of the …,openaccess.thecvf.com,https://openaccess.thecvf.com/content/ICCV2021W/AROW/html/Devaguptapu_On_Adversarial_Robustness_A_Neural_Architecture_Search_Perspective_ICCVW_2021_paper.html,https://scholar.google.com/scholar?cites=13481271496648129308&as_sdt=2005&sciodt=2007&hl=en,123.0,2022-07-13 12:24:11,,,,,,,,,2,2.0,1.0,4.0,1.0,… adversarial robustness purely from an architectural perspective. We show that the complex topology of neural network architectures can be leveraged to achieve robustness without …,https://openaccess.thecvf.com/content/ICCV2021W/AROW/papers/Devaguptapu_On_Adversarial_Robustness_A_Neural_Architecture_Search_Perspective_ICCVW_2021_paper.pdf,https://scholar.google.com/scholar?q=related:HHtP1v8YF7sJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'neural architecture', 'robust']"
Yes,"Robust, Adversarial, Generalization",Improve generalization and robustness of neural networks via weight scale shifting invariant regularizations,3.0,"Z Liu, Y Cui, AB Chan",2020.0,arXiv preprint arXiv:2008.02965,arxiv.org,https://arxiv.org/abs/2008.02965,https://scholar.google.com/scholar?cites=2733615116786483449&as_sdt=2005&sciodt=2007&hl=en,123.0,2022-07-13 12:03:09,,,,,,,,,3,1.5,1.0,3.0,2.0,… weight norms of a neural network with positively homogeneous … robustness. Our WEISSI regularizers mitigate this difficulty of … over weight decay in improving the adversarial robustness. …,https://arxiv.org/pdf/2008.02965,https://scholar.google.com/scholar?q=related:-UCyJHfA7yUJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'generalization', 'robust']"
Yes,"counterfactual explanations, manual perturbations",On the Robustness of Counterfactual Explanations to Adverse Perturbations,1.0,"M Virgolin, S Fracaros",2022.0,arXiv preprint arXiv:2201.09051,arxiv.org,https://arxiv.org/abs/2201.09051,https://scholar.google.com/scholar?cites=12301222125701520187&as_sdt=2005&sciodt=2007&hl=en,124.0,2022-07-13 09:30:22,,,,,,,,,1,1.0,1.0,2.0,1.0,"… 2.2 C-robustness We begin by focusing on the features that should be changed according to a given counterfactual explanation… black-box machine learning model, C-robustness can be …",https://arxiv.org/pdf/2201.09051,https://scholar.google.com/scholar?q=related:O1udQVm4tqoJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['counterfactual', 'explainability', 'perturbations']"
Yes,"Assessment, Robust, Adversarial, Deep Neural Network",Accelerating robustness verification of deep neural networks guided by target labels,5.0,"W Wan, Z Zhang, Y Zhu, M Zhang, F Song",2020.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2007.08520,https://scholar.google.com/scholar?cites=8861588213495875951&as_sdt=2005&sciodt=2007&hl=en,124.0,2022-07-13 14:46:28,,,,,,,,,5,2.5,1.0,5.0,2.0,… We first sort the labels as aforementioned (lines 1–2) and then verify the robustness of the given neural network against the labels one by one in J wrt the input x and a perturbation …,https://arxiv.org/pdf/2007.08520,https://scholar.google.com/scholar?q=related:b8k1i4ev-noJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22trustworthy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'deep learning', 'evaluation', 'robust']"
No,"Generative Adversarial Network, Robust, Deep Neural Network",A method for robustness optimization using generative adversarial networks,1.0,"S Bergmann, N Feldkamp, F Conrad…",2020.0,Proceedings of the 2020 …,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3384441.3395981,https://scholar.google.com/scholar?cites=13641460585943788230&as_sdt=2005&sciodt=2007&hl=en,124.0,2022-07-12 11:55:13,,10.1145/3384441.3395981,,,,,,,1,0.5,0.0,4.0,2.0,"… Machine learning, as a sub-discipline of artificial intelligence (AI), is currently a very popular … , the robustness of both methods is tested against the simplex lattice design experiment plan …",https://dl.acm.org/doi/pdf/10.1145/3384441.3395981,https://scholar.google.com/scholar?q=related:xuqrmiA0UL0J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'generative adversarial networks', 'robust']"
No,"theoretical framework, trustworthy","Designing trust in Artificial Intelligence: A comparative study among specifications, principles and levels of control",0.0,"F Galdon, A Hall, L Ferrarello",2020.0,International Conference on Human …,Springer,https://link.springer.com/chapter/10.1007/978-3-030-44267-5_14,,124.0,2022-07-12 11:56:04,,10.1007/978-3-030-44267-5_14,,,,,,,0,0.0,0.0,3.0,2.0,"… Questions around, how do we achieve fairness, robustness, explainability, accountability and value alignment through design, and how do we integrate them throughout the entire …",https://researchonline.rca.ac.uk/4382/1/Galdon2020_Chapter_DesigningTrustInArtificialInte.pdf,https://scholar.google.com/scholar?q=related:XJ6f7jdMa2IJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explainability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['framework', 'trustworthy']"
No,"NLP, mitigation method, adversarial robustness, semantics robustness",Disentangled contrastive learning for learning robust textual representations,4.0,"X Chen, X Xie, Z Bi, H Ye, S Deng, N Zhang…",2021.0,… on Artificial Intelligence,Springer,https://link.springer.com/chapter/10.1007/978-3-030-93049-3_18,https://scholar.google.com/scholar?cites=10280461870068694477&as_sdt=2005&sciodt=2007&hl=en,124.0,2022-07-13 15:31:15,,10.1007/978-3-030-93049-3_18,,,,,,,4,4.0,1.0,7.0,1.0,"… We investigate robust textual representation learning problems and introduce a … inputs such that they are robust to permutations. Therefore, a robust representation should be similar in …",https://arxiv.org/pdf/2104.04907,https://scholar.google.com/scholar?q=related:zX1kRJqJq44J:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'mitigation method', 'nlp', 'robust']"
No,"Computer Vision, Deep Learning, Adversarial Perturbations",Robustness of Rotation-Equivariant Networks to Adversarial Perturbations,22.0,"Beranger Dumont, Simona Maggio, Pablo Montalvo",2018.0,,,,,125.0,2022-07-13 09:26:34,,,,,,,,,22,5.5,7.0,3.0,4.0,"Deep neural networks have been shown to be vulnerable to adversarial examples: very small perturbations of the input having a dramatic impact on the predictions. A wealth of adversarial attacks and distance metrics to quantify the similarity between natural and adversarial images have been proposed, recently enlarging the scope of adversarial examples with geometric transformations beyond pixel-wise attacks. In this context, we investigate the robustness to adversarial attacks of new Convolutional Neural Network architectures providing equivariance to rotations. We found that rotation-equivariant networks are significantly less vulnerable to geometric-based attacks than regular networks on the MNIST, CIFAR-10, and ImageNet datasets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'computer vision', 'deep learning']"
Yes,"Fairness, adversarial attacks",Robustness may be at odds with fairness: An empirical study on class-wise accuracy,14.0,"P Benz, C Zhang, A Karjauv…",2021.0,NeurIPS 2020 Workshop …,proceedings.mlr.press,https://proceedings.mlr.press/v148/benz21a,https://scholar.google.com/scholar?cites=11563094360597196327&as_sdt=2005&sciodt=2007&hl=en,125.0,2022-07-12 13:44:06,,,,,,,,,14,14.0,4.0,4.0,1.0,… the model average robustness without class-wise … accuracy and robustness of adversarially trained models. We find that there exists inter-class discrepancy for accuracy and robustness …,http://proceedings.mlr.press/v148/benz21a/benz21a.pdf,https://scholar.google.com/scholar?q=related:J-63mw5deKAJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'fairness']"
Yes,"Adversarial, Robust, Convolutional Neural Network",A singular value perspective on model robustness,4.0,"M Jere, M Kumar, F Koushanfar",2020.0,arXiv preprint arXiv:2012.03516,arxiv.org,https://arxiv.org/abs/2012.03516,https://scholar.google.com/scholar?cites=18079972312635351038&as_sdt=2005&sciodt=2007&hl=en,125.0,2022-07-13 12:01:57,,,,,,,,,4,2.0,1.0,3.0,2.0,… the neural network. In the white box threat model all information about the neural network is … that image rank and rank-based robustness metrics might be better suited to capture image …,https://arxiv.org/pdf/2012.03516,https://scholar.google.com/scholar?q=related:_h-Hywb36PoJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22explainability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'convolutional neural network', 'robust']"
Yes,"Regularization, Robust, Generalization",An Empirical Evaluation on Robustness and Uncertainty of Regularization Methods,28.0,"Sanghyuk Chun, Seong Joon Oh, Sangdoo Yun, Dongyoon Han, Junsuk Choe, Y. Yoo",2020.0,,,,,125.0,2022-07-13 09:30:38,,,,,,,,,28,14.0,5.0,6.0,2.0,"Despite apparent human-level performances of deep neural networks (DNN), they behave fundamentally differently from humans. They easily change predictions when small corruptions such as blur and noise are applied on the input (lack of robustness), and they often produce confident predictions on out-of-distribution samples (improper uncertainty measure). While a number of researches have aimed to address those issues, proposed solutions are typically expensive and complicated (e.g. Bayesian inference and adversarial training). Meanwhile, many simple and cheap regularization methods have been developed to enhance the generalization of classifiers. Such regularization methods have largely been overlooked as baselines for addressing the robustness and uncertainty issues, as they are not specifically designed for that. In this paper, we provide extensive empirical evaluations on the robustness and uncertainty estimates of image classifiers (CIFAR-100 and ImageNet) trained with state-of-the-art regularization methods. Furthermore, experimental results show that certain regularization methods can serve as strong baseline methods for robustness and uncertainty estimation of DNNs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generalization', 'regularization', 'robust']"
Yes,"Visual Question Answering, Generalization, Adversarial Perturbations",Robust Visual Reasoning via Language Guided Neural Module Networks,9.0,"Arjun Reddy Akula, V. Jampani, Soravit Changpinyo, Song-Chun Zhu",2021.0,,,,,125.0,2022-07-13 09:27:16,,,,,,,,,9,9.0,2.0,4.0,1.0,"Neural module networks (NMN) are a popular approach for solving multi-modal tasks such as visual question answering (VQA) and visual referring expression recognition (REF). A key limitation in prior implementations of NMN is that the neural modules do not effectively capture the association between the visual input and the relevant neighbourhood context of the textual input. This limits their generalizability. For instance, NMN fail to understand new concepts such as “yellow sphere to the left"" even when it is a combination of known concepts from train data: “blue sphere"", “yellow cube"", and “metallic cube to the left"". In this paper, we address this limitation by introducing a language-guided adaptive convolution layer (LG-Conv) into NMN, in which the filter weights of convolutions are explicitly multiplied with a spatially varying language-guided kernel. Our model allows the neural module to adaptively co-attend over potential objects of interest from the visual and textual inputs. Extensive experiments on VQA and REF tasks demonstrate the effectiveness of our approach. Additionally, we propose a new challenging out-of-distribution test split for REF task, which we call C3Ref+, for explicitly evaluating the NMN’s ability to generalize well to adversarial perturbations and unseen combinations of known concepts. Experiments on C3Ref+ further demonstrate the generalization capabilities of our approach.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'generalization', 'visual question answering']"
No,"Robust. Accuracy, Uncertainty","Blurs Behave Like Ensembles: Spatial Smoothings to Improve Accuracy, Uncertainty, and Robustness",2.0,"Namuk Park, S. Kim",2021.0,,,,,126.0,2022-07-13 09:26:09,,,,,,,,,2,2.0,1.0,2.0,1.0,"Neural network ensembles, such as Bayesian neural networks (BNNs), have shown success in the areas of uncertainty estimation and robustness. However, a crucial challenge prohibits their use in practice. BNNs require a large number of predictions to produce reliable results, leading to a significant increase in computational cost. To alleviate this issue, we propose spatial smoothing, a method that spatially ensembles neighboring feature map points of convolutional neural networks. By simply adding a few blur layers to the models, we empirically show that spatial smoothing improves accuracy, uncertainty estimation, and robustness of BNNs across a whole range of ensemble sizes. In particular, BNNs incorporating spatial smoothing achieve high predictive performance merely with a handful of ensembles. Moreover, this method also can be applied to canonical deterministic neural networks to improve the performances. A number of evidences suggest that the improvements can be attributed to the stabilized feature maps and the smoothing of the loss landscape. In addition, we provide a fundamental explanation for prior works—namely, global average pooling, pre-activation, and ReLU6—by addressing them as special cases of spatial smoothing. These not only enhance accuracy, but also improve uncertainty estimation and robustness by making the loss landscape smoother in the same manner as spatial smoothing.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'uncertainty']"
Yes,"Self-supervision, Speech Recognition, Robust",Multi-Task Self-Supervised Learning for Robust Speech Recognition,139.0,"M. Ravanelli, Jianyuan Zhong, Santiago Pascual, P. Swietojanski, João Monteiro, J. Trmal, Yoshua Bengio",2020.0,,,,,126.0,2022-07-13 10:11:25,,10.1109/ICASSP40776.2020.9053569,,,,,,,139,69.5,20.0,7.0,2.0,"Despite the growing interest in unsupervised learning, extracting meaningful knowledge from unlabelled audio remains an open challenge. To take a step in this direction, we recently proposed a problem-agnostic speech encoder (PASE), that combines a convolutional encoder followed by multiple neural networks, called workers, tasked to solve self-supervised problems (i.e., ones that do not require manual annotations as ground truth). PASE was shown to capture relevant speech information, including speaker voice-print and phonemes. This paper proposes PASE+, an improved version of PASE for robust speech recognition in noisy and reverberant environments. To this end, we employ an online speech distortion module, that contaminates the input signals with a variety of random disturbances. We then propose a revised encoder that better learns short- and long-term speech dynamics with an efficient combination of recurrent and convolutional networks. Finally, we refine the set of workers used in self-supervision to encourage better cooperation.Results on TIMIT, DIRHA and CHiME-5 show that PASE+ significantly outperforms both the previous version of PASE as well as common acoustic features. Interestingly, PASE+ learns transferable representations suitable for highly mismatched acoustic conditions.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['robust', 'self-supervision', 'time series']"
No,"Robust, Explainability, Deep Learning",Robust Explainability: A tutorial on gradient-based attribution methods for deep neural networks,0.0,"IE Nielsen, D Dera, G Rasool…",2022.0,IEEE Signal …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9810053/?casa_token=2BWbi21-uikAAAAA:4WhmdfX7C1pOhI5LvKkoVar49xDI8KKq3UnWU9apYY5v9AIRX1xQCgfO8CHg2PtfyCJWnC336Q,,127.0,2022-07-13 09:27:33,,,,,,,,,0,0.0,0.0,4.0,1.0,… that should be examined before choosing an explainability method. We conclude with future … robustness and explainability. Introduction DL has transformed the field of machine learning …,https://ieeexplore.ieee.org/iel7/79/9810027/09810053.pdf?casa_token=_LXbk80vrZoAAAAA:hxkmepMWSzrRFkCQwWUypVmh9ujdv26ix4c5xYTodKHdIA3kHzbhoDjjY_l7-J2fMjo51k0ZXg,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'explainability', 'robust']"
Yes,"Fairness, Training, Trustworthiness",Sample selection for fair and robust training,8.0,"Y Roh, K Lee, S Whang, C Suh",2021.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2021/hash/07563a3fe3bbe7e3ba84431ad9d055af-Abstract.html,https://scholar.google.com/scholar?cites=953252445411110901&as_sdt=2005&sciodt=2007&hl=en,128.0,2022-07-13 11:13:23,,,,,,,,,8,8.0,2.0,4.0,1.0,"… Fairness and robustness are critical elements of Trustworthy … focus on fairness and robustness because they are closely … be indispensable in future machine learning systems, we should …",https://proceedings.neurips.cc/paper/2021/file/07563a3fe3bbe7e3ba84431ad9d055af-Paper.pdf,https://scholar.google.com/scholar?q=related:9W8AbgyiOg0J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22trustworthy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['fairness', 'training', 'trustworthy']"
Yes,"Deep Learning, Robust, Adversarial Attacks",MRobust: A Method for Robustness against Adversarial Attacks on Deep Neural Networks,1.0,"Yi-Ling Liu, A. Lomuscio",2020.0,,,,,128.0,2022-07-13 09:22:57,,10.1109/IJCNN48605.2020.9207354,,,,,,,1,0.5,1.0,2.0,2.0,"We present a novel black-box adversarial training algorithm to defend against state-of-the-art attack methods in machine learning. In order to search for an adversarial attack, the algorithm analyses small regions around the input that are likely to make significant contributions for the generation of adversarial samples. Unlike some of the literature in the area, the proposed method does not require access to the internal layers of the model and is therefore applicable to applications such as security. We report the experimental results obtained on models of different sizes built for the MNIST and CIFAR10 datasets. The results suggest that known attacks on the resulting models are less transferable than those models trained by state-of-the art attack algorithms.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'deep learning', 'robust']"
Yes,"Convolutional Neural Network, Emotion Recognition, Robust",HoloNet: towards robust emotion recognition in the wild,93.0,"A Yao, D Cai, P Hu, S Wang, L Sha…",2016.0,Proceedings of the 18th …,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/2993148.2997639,https://scholar.google.com/scholar?cites=6439166689073377382&as_sdt=2005&sciodt=2007&hl=en,128.0,2022-07-14 13:30:21,,10.1145/2993148.2997639,,,,,,,93,15.5,16.0,6.0,6.0,"… , a well-designed Convolutional Neural Network (CNN) … simple and shallow neural network architectures to address … our HoloNet can well enjoy accuracy gain from considerably …",,https://scholar.google.com/scholar?q=related:ZjC_z_yDXFkJ:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'convolutional neural network', 'robust']"
No,"Robust, Training, Label Noise","Machine Learning Robustness, Fairness, and their Convergence",4.0,"Jae-Gil Lee, Yuji Roh, Hwanjun Song, S. E. Whang",2021.0,,,,,129.0,2022-07-13 09:22:49,,10.1145/3447548.3470799,,,,,,,4,4.0,1.0,4.0,1.0,"Responsible AI becomes critical where robustness and fairness must be satisfied together. Traditionally, the two topics have been studied by different communities for different applications. Robust training is designed for noisy or poisoned data where image data is typically considered. In comparison, fair training primarily deals with biased data where structured data is typically considered. Nevertheless, robust training and fair training are fundamentally similar in considering that both of them aim at fixing the inherent flaws of real-world data. In this tutorial, we first cover state-of-the-art robust training techniques where most of the research is on combating various label noises. In particular, we cover label noise modeling, robust training approaches, and real-world noisy data sets. Then, proceeding to the related fairness literature, we discuss pre-processing, in-processing, and post-processing unfairness mitigation techniques, depending on whether the mitigation occurs before, during, or after the model training. Finally, we cover the recent trend emerged to combine robust and fair training in two flavors: the former is to make the fair training more robust (i.e., robust fair training), and the latter is to consider robustness and fairness as two equals to incorporate them into a holistic framework. This tutorial is indeed timely and novel because the convergence of the two topics is increasingly common, but yet to be addressed in tutorials. The tutors have extensive experience publishing papers in top-tier machine learning and data mining venues and developing machine learning platforms.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['label noise', 'robust', 'training']"
Yes,"Metric, Robust, Adversarial",Dsrna: Differentiable search of robust neural architectures,11.0,"R Hosseini, X Yang, P Xie",2021.0,… of the IEEE/CVF Conference on …,openaccess.thecvf.com,http://openaccess.thecvf.com/content/CVPR2021/html/Hosseini_DSRNA_Differentiable_Search_of_Robust_Neural_Architectures_CVPR_2021_paper.html,https://scholar.google.com/scholar?cites=10145051360811348198&as_sdt=2005&sciodt=2007&hl=en,129.0,2022-07-14 13:30:21,,,,,,,,,11,11.0,4.0,3.0,1.0,… One way to measure the robustness of a neural network is to use the … accuracy of our methods with state-of-the-art baselines under the attack-free setting. Table 2 shows the accuracy …,http://openaccess.thecvf.com/content/CVPR2021/papers/Hosseini_DSRNA_Differentiable_Search_of_Robust_Neural_Architectures_CVPR_2021_paper.pdf,https://scholar.google.com/scholar?q=related:5vBAPXZ2yowJ:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'metric', 'robust']"
Yes,"Adversarial Training, Convolutional Neural Network, Channels",Improving adversarial robustness via channel-wise activation suppressing,45.0,"Y Bai, Y Zeng, Y Jiang, ST Xia, X Ma…",2021.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2103.08307,https://scholar.google.com/scholar?cites=16315998776184141539&as_sdt=2005&sciodt=2007&hl=en,129.0,2022-07-13 13:05:38,,,,,,,,,45,45.0,8.0,6.0,1.0,… of adversarial robustness and adversarial training. We highlight two new characteristics of the channels of adversarial … We find that standard adversarial training improves robustness by …,https://arxiv.org/pdf/2103.08307,https://scholar.google.com/scholar?q=related:4_Yp6nASbuIJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial training', 'convolutional neural network']"
Yes,"Pruning, Adversarial Attacks, Computer Vision",Robustness-Aware Filter Pruning for Robust Neural Networks Against Adversarial Attacks,0.0,"H Lim, SD Roh, S Park…",2021.0,2021 IEEE 31st …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9596121/,,129.0,2022-07-13 14:59:54,,,,,,,,,0,0.0,0.0,4.0,1.0,"… However, we want to claim that filter pruning is a very effective way to enhance the robustness of a neural network. To the best of our knowledge, this is the first attempt to verify that the …",http://esoc.hanyang.ac.kr/publications/2021/Robustness_aware_Filter_Pruning_for_Robust_Neural_Networks_Against_Adversarial_Attacks.pdf,https://scholar.google.com/scholar?q=related:K8GIWuQA6QoJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'computer vision', 'pruning']"
Yes,"Design, Robust, Neural Network",AdvRush: Searching for Adversarially Robust Neural Architectures,4.0,"J. Mok, Byunggook Na, Hyeokjun Choe, Sungroh Yoon",2021.0,,,,,130.0,2022-07-13 09:30:38,,10.1109/iccv48922.2021.01210,,,,,,,4,4.0,1.0,4.0,1.0,"Deep neural networks continue to awe the world with their remarkable performance. Their predictions, however, are prone to be corrupted by adversarial examples that are imperceptible to humans. Current efforts to improve the robustness of neural networks against adversarial examples are focused on developing robust training methods, which update the weights of a neural network in a more robust direction. In this work, we take a step beyond training of the weight parameters and consider the problem of designing an adversarially robust neural architecture with high intrinsic robustness. We propose AdvRush, a novel adversarial robustness-aware neural architecture search algorithm, based upon a finding that independent of the training method, the intrinsic robustness of a neural network can be represented with the smoothness of its input loss landscape. Through a regularizer that favors a candidate architecture with a smoother input loss landscape, AdvRush successfully discovers an adversarially robust neural architecture. Along with a comprehensive theoretical motivation for AdvRush, we conduct an extensive amount of experiments to demonstrate the efficacy of AdvRush on various benchmark datasets. Notably, on CIFAR-10, AdvRush achieves 55.91% robust accuracy under FGSM attack after standard training and 50.04% robust accuracy under AutoAttack after 7-step PGD adversarial training.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['design', 'neural network', 'robust']"
Yes,"robustness, assessment",Global Robustness Evaluation of Deep Neural Networks with Provable Guarantees for L0 Norm,24.0,"Wenjie Ruan, Min Wu, Youcheng Sun, Xiaowei Huang, D. Kroening, M. Kwiatkowska",2018.0,,,,,130.0,2022-07-13 09:26:34,,,,,,,,,24,6.0,4.0,6.0,4.0,"Deployment of deep neural networks (DNNs) in safety- or security-critical systems requires provable guarantees on their correct behaviour. A common requirement is robustness to adversarial perturbations in a neighbourhood around an input. In this paper we focus on the $L_0$ norm and aim to compute, for a trained DNN and an input, the maximal radius of a safe norm ball around the input within which there are no adversarial examples. Then we define global robustness as an expectation of the maximal safe radius over a test data set. We first show that the problem is NP-hard, and then propose an approximate approach to iteratively compute lower and upper bounds on the network's robustness. The approach is \emph{anytime}, i.e., it returns intermediate bounds and robustness estimates that are gradually, but strictly, improved as the computation proceeds; \emph{tensor-based}, i.e., the computation is conducted over a set of inputs simultaneously, instead of one by one, to enable efficient GPU computation; and has \emph{provable guarantees}, i.e., both the bounds and the robustness estimates can converge to their optimal values. Finally, we demonstrate the utility of the proposed approach in practice to compute tight bounds by applying and adapting the anytime algorithm to a set of challenging problems, including global robustness evaluation, competitive $L_0$ attacks, test case generation for DNNs, and local robustness evaluation on large-scale ImageNet DNNs. We release the code of all case studies via GitHub.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'robust']"
No,"Evaluation, robustness",Testing the Robustness of a BiLSTM-based Structural Story Classifier,0.0,"A Hussain, SDP Nanduri…",2022.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2201.02733,,130.0,2022-07-13 11:05:20,,,,,,,,,0,0.0,0.0,3.0,1.0,"… fake news in the machine learning community. While several machine learning techniques for this … the impact of noise on these techniques’ performance, where noise constitutes news …",https://arxiv.org/pdf/2201.02733,https://scholar.google.com/scholar?q=related:4bvMQU_RNagJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'robust']"
Yes,"Lipschitz, Robust, Noise",Enhancing robustness verification for deep neural networks via symbolic propagation,5.0,"P Yang, J Li, J Liu, CC Huang, R Li, L Chen…",2021.0,Formal Aspects of …,Springer,https://link.springer.com/article/10.1007/s00165-021-00548-1,https://scholar.google.com/scholar?cites=10724147984334893052&as_sdt=2005&sciodt=2007&hl=en,130.0,2022-07-12 11:55:13,,10.1007/s00165-021-00548-1,,,,,,,5,5.0,1.0,7.0,1.0,"… SIGPLAN conference on programming language design and implementation, PLDI 2019, … artificial intelligence, AAAI 2019, The thirty-first innovative applications of artificial Intelligence …",https://livrepository.liverpool.ac.uk/3148181/1/FAOC21_dnn.pdf,https://scholar.google.com/scholar?q=related:_K8A1cXT05QJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['lipschitz', 'noise', 'robust']"
Yes,"Adversarial Attacks, Explainability, Deep Learning",Robust adversarial attack against explainable deep classification models based on adversarial images with different patch sizes and perturbation ratios,0.0,T.T.H. Le,2021.0,IEEE Access,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85117006996&origin=inward,131.0,2022-07-12 16:34:43,Article,10.1109/ACCESS.2021.3115764,2169-3536,https://api.elsevier.com/content/abstract/scopus_id/85117006996,9.0,,133049.0,133061.0,0,0.0,0.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'deep learning', 'explainability']"
No,"Robust, Deep Learning, Data Poisoning",Oriole: Thwarting Privacy against Trustworthy Deep Learning Models,3.0,"Liuqiao Chen, Hu Wang, Benjamin Zi Hao Zhao, Minhui Xue, Hai-feng Qian",2021.0,,,,,132.0,2022-07-13 09:29:26,,10.1007/978-3-030-90567-5_28,,,,,,,3,3.0,1.0,5.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['data poisoning', 'deep learning', 'robust']"
No,"Adversarial, Graph Neural Network, Perturbation, Survey","A Comprehensive Survey on Trustworthy Graph Neural Networks: Privacy, Robustness, Fairness, and Explainability",1.0,"E Dai, T Zhao, H Zhu, J Xu, Z Guo, H Liu, J Tang…",2022.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2204.08570,https://scholar.google.com/scholar?cites=15220919062935286572&as_sdt=2005&sciodt=2007&hl=en,132.0,2022-07-12 11:56:04,,,,,,,,,1,1.0,0.0,8.0,1.0,"… In this paper, we give a comprehensive survey of GNNs in the computational aspects of privacy, robustness, fairness, and explainability. For each aspect, we give the taxonomy of the …",https://arxiv.org/pdf/2204.08570,https://scholar.google.com/scholar?q=related:LLM6bT6RO9MJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explainability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'graph neural network', 'perturbation', 'survey']"
No,"Generative Adversarial Network, Robust, Noise",Distributional robustness with ipms and links to regularization and gans,10.0,H Husain,2020.0,Advances in Neural Information Processing …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2020/hash/8929c70f8d710e412d38da624b21c3c8-Abstract.html,https://scholar.google.com/scholar?cites=17030829115574787429&as_sdt=2005&sciodt=2007&hl=en,132.0,2022-07-13 11:20:18,,,,,,,,,10,5.0,10.0,1.0,2.0,"… This is, to the best of our knowledge, the first analysis of robustness for f-GANs with respect … robustness and machine learning at large. Unlike most DRO applications to machine learning…",https://proceedings.neurips.cc/paper/2020/file/8929c70f8d710e412d38da624b21c3c8-Paper.pdf,https://scholar.google.com/scholar?q=related:ZR038NioWewJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generative adversarial networks', 'noise', 'robust']"
No,robustness to forgetting (when learning new tasks),On robustness of generative representations against catastrophic forgetting,1.0,"W Masarczyk, K Deja, T Trzcinski",2021.0,International Conference on Neural …,Springer,https://link.springer.com/chapter/10.1007/978-3-030-92310-5_38,https://scholar.google.com/scholar?cites=5773076107393875254&as_sdt=2005&sciodt=2007&hl=en,133.0,2022-07-12 11:49:54,,10.1007/978-3-030-92310-5_38,,,,,,,1,1.0,0.0,3.0,1.0,"… Another approach [6] investigates the problem with Explainable Artificial Intelligence (XAI) tools, comparing the effects of catastrophic forgetting for different layers of CNN. …",https://arxiv.org/pdf/2109.01844,https://scholar.google.com/scholar?q=related:Nr2B8iEWHlAJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['robust']
No,"Convolutional Neural Network, Framework, Robust, Accuracy",A robust CNN model for handwritten digits recognition and classification,4.0,"S Ali, Z Sakhawat, T Mahmood…",2020.0,… on Advances in …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9213530/,https://scholar.google.com/scholar?cites=10878003948705578523&as_sdt=2005&sciodt=2007&hl=en,133.0,2022-07-14 12:10:31,,,,,,,,,4,2.0,1.0,4.0,2.0,"… of neural network with multiple layers specifically formulated to recognize visual patterns. This neural network … Now, it is the time to share our convolutional neural network model. To get …",https://www.researchgate.net/profile/Muhammad-Aslam-80/publication/344668321_A_robust_CNN_model_for_handwritten_digits_recognition_and_classification/links/5f90dab8458515b7cf93731b/A-robust-CNN-model-for-handwritten-digits-recognition-and-classification.pdf,https://scholar.google.com/scholar?q=related:G37kC_Vu9pYJ:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'convolutional neural network', 'framework', 'robust']"
Yes,"Open-world Learning, Robust, Adversarial, Out-of-Distribution",Analyzing the Robustness of Open-World Machine Learning,27.0,"Vikash Sehwag, A. Bhagoji, Liwei Song, Chawin Sitawarin, Daniel Cullina, M. Chiang, Prateek Mittal",2019.0,,,,,133.0,2022-07-13 09:22:49,,10.1145/3338501.3357372,,,,,,,27,9.0,4.0,7.0,3.0,"When deploying machine learning models in real-world applications, an open-world learning framework is needed to deal with both normal in-distribution inputs and undesired out-of-distribution (OOD) inputs. Open-world learning frameworks include OOD detectors that aim to discard input examples which are not from the same distribution as the training data of machine learning classifiers. However, our understanding of current OOD detectors is limited to the setting of benign OOD data, and an open question is whether they are robust in the presence of adversaries. In this paper, we present the first analysis of the robustness of open-world learning frameworks in the presence of adversaries by introducing and designing øodAdvExamples. Our experimental results show that current OOD detectors can be easily evaded by slightly perturbing benign OOD inputs, revealing a severe limitation of current open-world learning frameworks. Furthermore, we find that øodAdvExamples also pose a strong threat to adversarial training based defense methods in spite of their effectiveness against in-distribution adversarial attacks. To counteract these threats and ensure the trustworthy detection of OOD inputs, we outline a preliminary design for a robust open-world machine learning framework.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'open environment', 'out-of-distribution data', 'robust']"
Yes,"adversarial robustness, computer vision",Towards Fast and Robust Adversarial Training for Image Classification,1.0,"EC Chen, CR Lee",2020.0,… of the Asian Conference on Computer …,openaccess.thecvf.com,https://openaccess.thecvf.com/content/ACCV2020/html/Chen_Towards_Fast_and_Robust_Adversarial_Training_for_Image_Classification_ACCV_2020_paper.html,https://scholar.google.com/scholar?cites=12022191671647106179&as_sdt=2005&sciodt=2007&hl=en,135.0,2022-07-13 17:19:08,,,,,,,,,1,0.5,1.0,2.0,2.0,"… With the mechanism to check the robust accuracy during training process, we can terminate … % robust accuracy. If the enhanced FGSM is combined with TRADES, the robust accuracy of …",https://openaccess.thecvf.com/content/ACCV2020/papers/Chen_Towards_Fast_and_Robust_Adversarial_Training_for_Image_Classification_ACCV_2020_paper.pdf,https://scholar.google.com/scholar?q=related:g3QvCqFn16YJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'computer vision']"
Yes,"Outliers, Dataset, Deep Learning",Less is More: Culling the Training Set to Improve Robustness of Deep Neural Networks,12.0,"Yongshuai Liu, Jiyu Chen, Hao Chen",2018.0,,,,,136.0,2022-07-13 09:26:34,,10.1007/978-3-030-01554-1_6,,,,,,,12,3.0,4.0,3.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['dataset', 'deep learning', 'outliers']"
No,"Representation Learning, Robust, Adversarial Attacks",Learning Representations Robust to Group Shifts and Adversarial Examples,0.0,"MC Chiu, X Ma",2022.0,arXiv preprint arXiv:2202.09446,arxiv.org,https://arxiv.org/abs/2202.09446,,136.0,2022-07-13 17:19:08,,,,,,,,,0,0.0,0.0,2.0,1.0,"… Robust metrics. We evaluate on average accuracy and average adversarial accuracy to compare adversarial robustness. For distributional robustness, we use robust accuracy, which …",https://arxiv.org/pdf/2202.09446,https://scholar.google.com/scholar?q=related:ndCkjCUtRqUJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'representation learning', 'robust']"
Yes,"robustness, adversarial",Error Correcting Output Codes Improve Probability Estimation and Adversarial Robustness of Deep Neural Networks,35.0,"Gunjan Verma, A. Swami",2019.0,,,,,137.0,2022-07-13 09:22:57,,,,,,,,,35,12.07,18.0,2.0,3.0,"Modern machine learning systems are susceptible to adversarial examples; inputs which clearly preserve the characteristic semantics of a given class, but whose classification is (usually confidently) incorrect. Existing approaches to adversarial defense generally rely on modifying the input, e.g. quantization, or the learned model parameters, e.g. via adversarial training. However, recent research has shown that most such approaches succumb to adversarial examples when different norms or more sophisticated adaptive attacks are considered. In this paper, we propose a fundamentally different approach which instead changes the way the output is represented and decoded. This simple approach achieves state-of-the-art robustness to adversarial examples for L 2 and L ∞ based adversarial perturbations on MNIST and CIFAR10. In addition, even under strong white-box attacks, we find that our model often assigns adversarial examples a low probability; those with high probability are usually interpretable, i.e. perturbed towards the perceptual boundary between the original and adversarial class. Our approach has several advantages: it yields more meaningful probability estimates, is extremely fast during training and testing, requires essentially no architectural changes to existing discriminative learning pipelines, is wholly complementary to other defense approaches including adversarial training, and does not sacrifice benign test set performance",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
No,"Adversarial, Training, Robust",Generalized Real-World Super-Resolution through Adversarial Robustness,2.0,"A Castillo, M Escobar, JC Pérez…",2021.0,Proceedings of the …,openaccess.thecvf.com,https://openaccess.thecvf.com/content/ICCV2021W/AIM/html/Castillo_Generalized_Real-World_Super-Resolution_Through_Adversarial_Robustness_ICCVW_2021_paper.html,https://scholar.google.com/scholar?cites=1584004586455624240&as_sdt=2005&sciodt=2007&hl=en,137.0,2022-07-12 11:56:54,,,,,,,,,2,2.0,1.0,4.0,1.0,"… , and hence adversarial robustness can serve … adversarial robustness can be used as a useful prior for learning DNNs for real-world SR. In particular, we show that by using adversarial …",https://openaccess.thecvf.com/content/ICCV2021W/AIM/papers/Castillo_Generalized_Real-World_Super-Resolution_Through_Adversarial_Robustness_ICCVW_2021_paper.pdf,https://scholar.google.com/scholar?q=related:MOpGaseD-xUJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust', 'training']"
Yes,"Classification, Computer Vision, Distribution Shift",Measuring robustness to natural distribution shifts in image classification,168.0,"R Taori, A Dave, V Shankar, N Carlini…",2020.0,Advances in …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2020/hash/d8330f857a17c53d217014ee776bfd50-Abstract.html,https://scholar.google.com/scholar?cites=3019171535172049328&as_sdt=2005&sciodt=2007&hl=en,137.0,2022-07-13 10:02:07,,,,,,,,,168,84.0,34.0,5.0,2.0,"… In this paper, we investigate how robust current machine learning techniques are to distribution shift arising naturally from real image data without synthetic modifications. To this end, …",https://proceedings.neurips.cc/paper/2020/file/d8330f857a17c53d217014ee776bfd50-Paper.pdf,https://scholar.google.com/scholar?q=related:sJnm1YNA5ikJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22transparency%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'computer vision', 'distribution shift']"
Discussion,"Malware, Adversarial, Robust",A Deep Learning-based Fine-grained Hierarchical Learning Approach for Robust Malware Classification,3.0,"Ahmed A. Abusnaina, M. Abuhamad, Hisham Alasmary, Afsah Anwar, Rhongho Jang, Saeed Salem, Daehun Nyang, David A. Mohaisen",2020.0,,,,,137.0,2022-07-13 09:38:44,,10.1109/tdsc.2021.3097296,,,,,,,3,1.5,0.0,8.0,2.0,"The wide acceptance of Internet of Things (IoT) for both household and industrial applications is accompanied by several security concerns. A major security concern is their probable abuse by adversaries towards their malicious intent. Understanding and analyzing IoT malicious behaviors is crucial, especially with their rapid growth and adoption in wide-range of applications. However, recent studies have shown that machine learning-based approaches are susceptible to adversarial attacks by adding junk codes to the binaries, for example, with an intention to fool those machine learning or deep learning-based detection systems. Realizing the importance of addressing this challenge, this study proposes a malware detection system that is robust to adversarial attacks. To do so, examine the performance of the state-of-the-art methods against adversarial IoT software crafted using the graph embedding and augmentation techniques. In particular, we study the robustness of such methods against two black-box adversarial methods, GEA and SGEA, to generate Adversarial Examples (AEs) with reduced overhead, and keeping their practicality intact. Our comprehensive experimentation with GEA-based AEs show the relation between misclassification and the graph size of the injected sample. Upon optimization and with small perturbation, by use of SGEA, all the IoT malware samples are misclassified as benign. This highlights the vulnerability of current detection systems under adversarial settings. With the landscape of possible adversarial attacks, we then propose DL-FHMC, a fine-grained hierarchical learning approach for malware detection and classification, that is robust to AEs with a capability to detect 88.52% of the malicious AEs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'cybersecurity', 'robust']"
No,"Generative Adversarial Networks, Training, Framework",Robust generative adversarial network,4.0,"S Zhang, Z Qian, K Huang, J Xiao, Y He",2020.0,arXiv preprint arXiv:2004.13344,arxiv.org,https://arxiv.org/abs/2004.13344,https://scholar.google.com/scholar?cites=6431698108446932155&as_sdt=2005&sciodt=2007&hl=en,138.0,2022-07-13 17:19:08,,,,,,,,,4,2.0,1.0,5.0,2.0,"… Particularly, we design a robust optimization framework … We have proved that our robust method can obtain a tighter … indicate that our proposed robust framework can improve on …",https://arxiv.org/pdf/2004.13344,https://scholar.google.com/scholar?q=related:u_yHYFr7QVkJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['framework', 'generative adversarial networks', 'training']"
Yes,"Embedding, Adversarial Perturbations, Robust",Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications,28.0,"Pouya Pezeshkpour, Yifan Tian, Sameer Singh",2018.0,,,,,138.0,2022-07-13 09:22:57,,10.18653/v1/N19-1337,,,,,,,28,7.0,9.0,3.0,4.0,"Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on improving accuracy and overlook other aspects such as robustness and interpretability. In this paper, we propose adversarial modifications for link prediction models: identifying the fact to add into or remove from the knowledge graph that changes the prediction for a target fact after the model is retrained. Using these single modifications of the graph, we identify the most influential fact for a predicted link and evaluate the sensitivity of the model to the addition of fake facts. We introduce an efficient approach to estimate the effect of such modifications by approximating the change in the embeddings when the knowledge graph changes. To avoid the combinatorial search over all possible facts, we train a network to decode embeddings to their corresponding graph components, allowing the use of gradient-based optimization to identify the adversarial modification. We use these techniques to evaluate the robustness of link prediction models (by measuring sensitivity to additional facts), study interpretability through the facts most responsible for predictions (by identifying the most influential neighbors), and detect incorrect facts in the knowledge base.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'embedding', 'robust']"
Yes,"Deep Neural Network, Question Answering, Robust, Adversarial, Explainability",Counterfactual Variable Control for Robust and Interpretable Question Answering,4.0,"S. Yu, Yulei Niu, Shuohang Wang, Jing Jiang, Qianru Sun",2020.0,,,,,139.0,2022-07-13 10:07:35,,,,,,,,,4,2.0,1.0,5.0,2.0,"Deep neural network based question answering (QA) models are neither robust nor explainable in many cases. For example, a multiple-choice QA model, tested without any input of question, is surprisingly ""capable"" to predict the most of correct options. In this paper, we inspect such spurious ""capability"" of QA models using causal inference. We find the crux is the shortcut correlation, e.g., unrobust word alignment between passage and options learned by the models. We propose a novel approach called Counterfactual Variable Control (CVC) that explicitly mitigates any shortcut correlation and preserves the comprehensive reasoning for robust QA. Specifically, we leverage multi-branch architecture that allows us to disentangle robust and shortcut correlations in the training process of QA. We then conduct two novel CVC inference methods (on trained models) to capture the effect of comprehensive reasoning as the final prediction. For evaluation, we conduct extensive experiments using two BERT backbones on both multi-choice and span-extraction QA benchmarks. The results show that our CVC achieves high robustness against a variety of adversarial attacks in QA while maintaining good interpretation ability.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'deep learning', 'explainability', 'nlp', 'robust']"
Yes,"Smoothing, Adversarial Robustness, Deep Learning",Gradient Masking of Label Smoothing in Adversarial Robustness,2.0,"Hyungyu Lee, Ho Bae, Sungroh Yoon",2021.0,,,,,139.0,2022-07-13 09:28:50,,10.1109/ACCESS.2020.3048120,,,,,,,2,2.0,1.0,3.0,1.0,"Deep neural networks (DNNs) have achieved impressive results in several image classification tasks. However, these architectures are unstable for adversarial examples (AEs) such as inputs crafted by a hardly perceptible perturbation with the intent of causing neural networks to make errors. AEs must be considered to prevent accidents in areas such as unmanned car driving using visual object detection in Internet of Things (IoT) networks. Gaussian noise with label smoothing or logit squeezing can be used to increase the robustness against AEs in the training of DNNs. However, from a model interpretability aspect, Gaussian noise with label smoothing does not increase the adversarial robustness of the model. To resolve this problem, we tackle the AE instead of measuring the accuracy of the model against AEs. Considering that a robust model shows a small curvature of the loss surface, we propose a metric to measure the strength of the AEs and the robustness of the model. Furthermore, we introduce a method to verify the existence of the obfuscated gradients of the model based on the black-box attack sanity check method. The proposed method enables us to identify a gradient masking problem wherein the model does not provide useful gradients and exploits false defenses. We evaluate our technique against representative adversarially trained models using the CIFAR10, CIFAR100, SVHN, and Restricted ImageNet datasets. Our results show that the performance of some false defense models decreases by up to 32% compared to the previous evaluation metrics. Moreover, our metric reveals that traditional metrics used to measure the robustness of the model may produce false results.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'deep learning', 'smoothing']"
Yes,"Robust, Classification",A robust ensemble based approach to combine heterogeneous classifiers in the presence of class label noise,6.0,"S Khalid, S Arshad",2013.0,2013 Fifth International Conference on …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/6663179/,https://scholar.google.com/scholar?cites=9313732467420788231&as_sdt=2005&sciodt=2007&hl=en,139.0,2022-07-14 09:04:21,,,,,,,,,6,1.07,3.0,2.0,9.0,"… its competitors in the presence of class label noise, we have induced the artificial noise to class labels of 70% train dataset. To add L% class label noise to 70% train dataset TD, pick …",,https://scholar.google.com/scholar?q=related:B3ajNVUGQYEJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'robust']"
Yes,"Measure, Robust, Stability, Assessment",Beyond Robustness: Resilience Verification of Tree-Based Classifiers,0.0,"S Calzavara, L Cazzaro, C Lucchese…",2021.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2112.02705,,141.0,2022-07-12 13:42:35,,,,,,,,,0,0.0,0.0,4.0,1.0,"… Abstract—In this paper we criticize the robustness … of robustness, we introduce a new measure called resilience and we focus on its verification. In particular, we discuss how resilience …",https://arxiv.org/pdf/2112.02705,https://scholar.google.com/scholar?q=related:wsm1tJr5r2UJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22resilience%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'metric', 'robust', 'stability']"
Yes,"Explainability, Adversarial Robustness, Convolutional Neural Network",How explainable are adversarially-robust CNNs?,0.0,"Mehdi Nourelahi, Lars Kotthoff, Peijie Chen, Anh M Nguyen",2022.0,,,,,141.0,2022-07-13 10:07:35,,10.48550/arXiv.2205.13042,,,,,,,0,0.0,0.0,4.0,1.0,"Three important criteria of existing convolutional neural networks (CNNs) are (1) test-set accuracy; (2) out-of-distribution accuracy; and (3) explainability. While these criteria have been studied independently, their relationship is unknown. For example, do CNNs that have a stronger out-of-distribution performance have also stronger explainability? Furthermore, most prior feature-importance studies only evaluate methods on 2-3 common vanilla ImageNet-trained CNNs, leaving it unknown how these methods generalize to CNNs of other architectures and training algorithms. Here, we perform the ﬁrst, large-scale evaluation of the relations of the three criteria using 9 feature-importance methods and 12 ImageNet-trained CNNs that are of 3 training algorithms and 5 CNN architectures. We ﬁnd several important insights and recommendations for ML practitioners. First, adversarially robust CNNs have a higher explainability score on gradient-based attribution methods (but not CAM-based or perturbation-based methods). Second, AdvProp models, despite being highly accurate more than both vanilla and robust models alone, are not superior in explainability. Third, among 9 feature attribution methods tested, GradCAM and RISE are consistently the best methods. Fourth, Insertion and Deletion are biased towards vanilla and robust models respectively, due to their strong correlation with the conﬁdence score distributions of a CNN. Fifth, we did not ﬁnd a single CNN to be the best in all three criteria, which interestingly suggests that CNNs are harder to interpret as they become more accurate.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'convolutional neural network', 'explainability']"
Yes,"Robustness, Deep Neural Network",A fast test method for noise robustness of deep neural networks,1.0,"M Yasuda, H Sakata, S Cho, T Harada…",2017.0,Proc. of the 2017 …,ieice.org,https://www.ieice.org/nolta/symposium/archive/2017/articles/5042.pdf,https://scholar.google.com/scholar?cites=18101304724147928645&as_sdt=2005&sciodt=2007&hl=en,142.0,2022-07-13 11:05:20,PDF,,,,,,,,1,0.2,0.0,5.0,5.0,"… Abstract—In this paper, we propose a fast test method for the noise robustness of pattern recognition systems based on deep neural networks. The proposed method allows the noise …",https://www.ieice.org/nolta/symposium/archive/2017/articles/5042.pdf,https://scholar.google.com/scholar?q=related:Rf7v5b3ANPsJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'robust']"
Yes,"Assessment, Robust, Classifier",Bias Field Robustness Verification of Large Neural Image Classifiers,2.0,"P Henriksen, K Hammernik…",2021.0,… of the 32nd …,bmvc2021-virtualconference.com,https://www.bmvc2021-virtualconference.com/assets/papers/1291.pdf,https://scholar.google.com/scholar?cites=10851392655326366518&as_sdt=2005&sciodt=2007&hl=en,142.0,2022-07-13 13:50:13,PDF,,,,,,,,2,2.0,1.0,3.0,1.0,… We present a method for verifying the robustness of neural network-based image classifiers … transformations into neural network operations to exploit neural network formal verification …,https://www.bmvc2021-virtualconference.com/assets/papers/1291.pdf,https://scholar.google.com/scholar?q=related:NssCeyDkl5YJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22reasoning%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classifier', 'evaluation', 'robust']"
Yes,"Deep Neural Network, Adeversarial, Framework, Robust",Adversarially Robust Neural Architectures,20.0,"Minjing Dong, Yanxi Li, Yunhe Wang, Chang Xu",2020.0,,,,,144.0,2022-07-13 09:30:38,,,,,,,,,20,10.0,5.0,4.0,2.0,"Deep Neural Network (DNN) are vulnerable to adversarial attack. Existing methods are devoted to developing various robust training strategies or regularizations to update the weights of the neural network. But beyond the weights, the overall structure and information flow in the network are explicitly determined by the neural architecture, which remains unexplored. This paper thus aims to improve the adversarial robustness of the network from the architecture perspective with NAS framework. We explore the relationship among adversarial robustness, Lipschitz constant, and architecture parameters and show that an appropriate constraint on architecture parameters could reduce the Lipschitz constant to further improve the robustness. For NAS framework, all the architecture parameters are equally treated when the discrete architecture is sampled from supernet. However, the importance of architecture parameters could vary from operation to operation or connection to connection, which is not explored and might reduce the confidence of robust architecture sampling. Thus, we propose to sample architecture parameters from trainable multivariate log-normal distributions, with which the Lipschitz constant of entire network can be approximated using a univariate log-normal distribution with mean and variance related to architecture parameters. Compared with adversarially trained neural architectures searched by various NAS algorithms as well as efficient human-designed models, our algorithm empirically achieves the best performance among all the models under various attacks on different datasets.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'deep learning', 'framework', 'robust']"
Yes,"Deep Neural Network, Robust, Security, Reliability",A Robustness-Oriented Data Augmentation Method for DNN,0.0,"M Liu, W Hong, W Pan, C Feng",2021.0,"… Software Quality, Reliability …",ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9741895/?casa_token=Ou9n9uFuFdgAAAAA:UPu8GLcNtOuuFR1eEk_ur3_Mq8LUDLECINqszBI--Xqr3UteCD8iUAXGBOF25FJnzL1IgfuhJA,,145.0,2022-07-13 14:26:15,,,,,,,,,0,0.0,0.0,4.0,1.0,… neural network training methods based on adversarial examples generation can be used to improve the robustness of neural network… of the neural network model on the testing dataset. …,https://ieeexplore.ieee.org/iel7/9741747/9741863/09741895.pdf?casa_token=xXzQWkEu-JkAAAAA:NxJegjcJ_D9MKFdYKjyyct-ogZi-PXGOORl3orcViPV7xUNk-4N6rr0B8S7iqvKz72k9TdnDYA,https://scholar.google.com/scholar?q=related:SQmlUUy-NyUJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22reliability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['cybersecurity', 'deep learning', 'reliability', 'robust']"
No,"Adversarial Attacks, Attack Algorithms, Noise",Robustness of Adversarial Attacks in Sound Event Classification,15.0,"Subramanian, Emmanouil Benetos, M. Sandler, Events",2019.0,,,,,145.0,2022-07-13 09:22:57,,10.33682/SP9N-QK06,,,,,,,15,5.0,4.0,4.0,3.0,"An adversarial attack is a method to generate perturbations to the input of a machine learning model in order to make the output of the model incorrect. The perturbed inputs are known as adversarial examples. In this paper, we investigate the robustness of adversarial examples to simple input transformations such as mp3 compression, resampling, white noise and reverb in the task of sound event classification. By performing this analysis, we aim to provide insights on strengths and weaknesses in current adversarial attack algorithms as well as provide a baseline for defenses against adversarial attacks. Our work shows that adversarial attacks are not robust to simple input transformations. White noise is the most consistent method to defend against adversarial attacks with a success rate of 73.72% averaged across all models and attack algorithms.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'noise']"
Yes,"adversarial robustness, neural networks, mitigation method","Divide, Denoise, and Defend against Adversarial Attacks",29.0,"Seyed-Mohsen Moosavi-Dezfooli, A. Shrivastava, Oncel Tuzel",2018.0,,,,,145.0,2022-07-13 09:24:12,,,,,,,,,29,7.25,10.0,3.0,4.0,"Deep neural networks, although shown to be a successful class of machine learning algorithms, are known to be extremely unstable to adversarial perturbations. Improving the robustness of neural networks against these attacks is important, especially for security-critical applications. To defend against such attacks, we propose dividing the input image into multiple patches, denoising each patch independently, and reconstructing the image, without losing significant image content. This proposed defense mechanism is non-differentiable which makes it non-trivial for an adversary to apply gradient-based attacks. Moreover, we do not fine-tune the network with adversarial examples, making it more robust against unknown attacks. We present a thorough analysis of the tradeoff between accuracy and robustness against adversarial attacks. We evaluate our method under black-box, grey-box, and white-box settings. The proposed method outperforms the state-of-the-art by a significant margin on the ImageNet dataset under grey-box attacks while maintaining good accuracy on clean images. We also establish a strong baseline for a novel white-box attack.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'mitigation method', 'neural networks']"
Yes,"Robust, Deep neural Network, Adversarial, Classifier",Can attention masks improve adversarial robustness?,5.0,"P Vaishnavi, T Cong, K Eykholt, A Prakash…",2020.0,… Machine Learning …,Springer,https://link.springer.com/chapter/10.1007/978-3-030-62144-5_2,https://scholar.google.com/scholar?cites=10115110937856012506&as_sdt=2005&sciodt=2007&hl=en,145.0,2022-07-13 09:47:56,,10.1007/978-3-030-62144-5_2,,,,,,,5,2.5,1.0,5.0,2.0,"… that the important property for increasing robustness is the elimination of image background using … robustness. On the adversarially trained classifiers, we see an adversarial robustness …",https://arxiv.org/pdf/1911.11946,https://scholar.google.com/scholar?q=related:2nCTSs4XYIwJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'classifier', 'deep learning', 'robust']"
Yes,"Lipschitz, Certified Robustness, Classification",Improved deterministic l2 robustness on CIFAR-10 and CIFAR-100,3.0,"Sahil Singla, Surbhi Singla, S. Feizi",2021.0,,,,,145.0,2022-07-13 09:28:50,,,,,,,,,3,3.0,1.0,3.0,1.0,"Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the l2 norm is useful for provable adversarial robustness, interpretable gradients and stable training. While 1-Lipschitz CNNs can be designed by enforcing a 1-Lipschitz constraint on each layer, training such networks requires each layer to have an orthogonal Jacobian matrix (for all inputs) to prevent the gradients from vanishing during backpropagation. A layer with this property is said to be Gradient Norm Preserving (GNP). In this work, we introduce a procedure to certify the robustness of 1-Lipschitz CNNs by relaxing the orthogonalization of the last linear layer of the network that significantly advances the state of the art for both standard and provable robust accuracies on CIFAR-100 (gains of 4.80% and 4.71%, respectively). We further boost their robustness by introducing (i) a novel Gradient Norm preserving activation function called the Householder activation function (that includes every GroupSort activation) and (ii) a certificate regularization. On CIFAR-10, we achieve significant improvements over prior works in provable robust accuracy (5.81%) with only a minor drop in standard accuracy (−0.29%). Code for reproducing all experiments in the paper is available at https://github.com/singlasahil14/SOC.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'lipschitz', 'robust']"
Yes,"natural language processing, robust",Evaluating the robustness of neural language models to input perturbations,12.0,"M Moradi, M Samwald",2021.0,arXiv preprint arXiv:2108.12237,arxiv.org,https://arxiv.org/abs/2108.12237,https://scholar.google.com/scholar?cites=12438478572547042067&as_sdt=2005&sciodt=2007&hl=en,145.0,2022-07-12 11:49:54,,,,,,,,,12,12.0,6.0,2.0,1.0,… methods aim at evaluating the robustness of NLP systems to noisy … robustness of NLP systems on non-synthetic text. An important contribution of this work is to evaluate the robustness …,https://arxiv.org/pdf/2108.12237,https://scholar.google.com/scholar?q=related:E2v5HlxanqwJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['nlp', 'robust']"
Yes,"Adversarial Robustness, Deep Learning, Self-supervision",Improving adversarial robustness requires revisiting misclassified examples,253.0,"Y Wang, D Zou, J Yi, J Bailey, X Ma…",2019.0,… Conference on Learning …,openreview.net,https://openreview.net/forum?id=rklOg6EFwS,https://scholar.google.com/scholar?cites=7100431639219605362&as_sdt=2005&sciodt=2007&hl=en,146.0,2022-07-13 09:47:56,,,,,,,,,253,84.33.00,42.0,6.0,3.0,"… • Experimentally, we show that adversarial robustness can be significantly improved over the stateof-the-art, by a specific focus on misclassified examples. It also helps improve recently …",https://openreview.net/pdf?id=rklOg6EFwS,https://scholar.google.com/scholar?q=related:cg-hx_TMiWIJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'deep learning', 'self-supervision']"
Yes,"Training, Adversarial, Robust",Confidence-Calibrated Adversarial Training: Towards Robust Models Generalizing Beyond the Attack Used During Training,1.0,"D Stutz, M Hein, B Schiele",2019.0,,openreview.net,https://openreview.net/forum?id=SJgwf04KPr,https://scholar.google.com/scholar?cites=9055278695386851424&as_sdt=2005&sciodt=2007&hl=en,147.0,2022-07-14 12:55:58,,,,,,,,,1,0.33,0.0,3.0,3.0,"… 3 we present our confidence-thresholded robust test error and present experimental results … typically have lower confidence. Then, we intend to use test error (Err) and robust test error (…",https://openreview.net/pdf?id=SJgwf04KPr,https://scholar.google.com/scholar?q=related:YCiyHP_Pqn0J:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust', 'training']"
Yes,"data augmentation, robust",Fuzz testing based data augmentation to improve robustness of deep neural networks,46.0,"X Gao, RK Saha, MR Prasad…",2020.0,2020 IEEE/ACM 42nd …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9283925/?casa_token=1KSGlH22SDQAAAAA:FgAds8v4MzSKkEpaRfBiHs-FVWQhLVnvBxKdENLaJ1qe_Uue1F-vd3AcKPQDt3QkpE9cMklPFg,https://scholar.google.com/scholar?cites=5210704511978612364&as_sdt=2005&sciodt=2007&hl=en,148.0,2022-07-13 14:17:37,,,,,,,,,46,23.0,12.0,4.0,2.0,"… the robustness of DNN models, following Engstrom et al. [8], we compute robust accuracy of … [42] proposed an orthogonal approach to improve the robustness deep neural network mod…",https://ieeexplore.ieee.org/iel7/9283875/9283910/09283925.pdf?casa_token=JKg8UJJHNh8AAAAA:UF6nWl0lxFLoaazrExk2sM7vpo7h5pS7rwg4XC-vy-GPi5M5BNZzijlScXv0komZkPr28Vl5Wg,https://scholar.google.com/scholar?q=related:jKYCaiUkUEgJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['data augmentation', 'robust']"
Yes,"Robust, Classification, Adversarial",A general framework for defining and optimizing robustness,0.0,"Alessandro Tibo, M. Jaeger, K. Larsen",2020.0,,,,,148.0,2022-07-13 09:26:01,,,,,,,,,0,0.0,0.0,3.0,2.0,"Robustness of neural networks has recently attracted a great amount of interest. The many investigations in this area lack a precise common foundation of robustness concepts. Therefore, in this paper, we propose a rigorous and flexible framework for defining different types of robustness that also help to explain the interplay between adversarial robustness and generalization. The different robustness objectives directly lead to an adjustable family of loss functions. For two robustness concepts of particular interest we show effective ways to minimize the corresponding loss functions. One loss is designed to strengthen robustness against adversarial off-manifold attacks, and another to improve generalization under the given data distribution. Empirical results show that we can effectively train under different robustness objectives, obtaining higher robustness scores and better generalization, for the two examples respectively, compared to the state-of-the-art data augmentation and regularization techniques.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'classification', 'robust']"
Yes,"Robustness, accuracy, adversarial perturbations",Robustness may be at odds with accuracy,991.0,"D Tsipras, S Santurkar, L Engstrom, A Turner…",2018.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/1805.12152,https://scholar.google.com/scholar?cites=5850945088404252192&as_sdt=2005&sciodt=2007&hl=en,148.0,2022-07-12 13:35:23,,,,,,,,,991,248.15.00,198.0,5.0,4.0,… know that robustness comes at a … robustness and interpretability of deep neural networks by regularizing their input gradients. In Thirty-second AAAI conference on artificial intelligence…,"https://arxiv.org/pdf/1805.12152.pdf,",https://scholar.google.com/scholar?q=related:IOam6o67MlEJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22human+knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial perturbations', 'robust']"
Yes,"Regularization, Fine Tuning, Neural Network",Improved regularization and robustness for fine-tuning in neural networks,2.0,"D Li, H Zhang",2021.0,Advances in Neural Information Processing …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2021/hash/e4a93f0332b2519177ed55741ea4e5e7-Abstract.html,https://scholar.google.com/scholar?cites=14262652923694182167&as_sdt=2005&sciodt=2007&hl=en,148.0,2022-07-13 14:41:59,,,,,,,,,2,2.0,1.0,2.0,1.0,"… Moreover, we observe that the neural network has not yet … two different kinds of label noise to test the robustness of fine-… both independent random noise and correlated noise in our …",https://proceedings.neurips.cc/paper/2021/file/e4a93f0332b2519177ed55741ea4e5e7-Paper.pdf,https://scholar.google.com/scholar?q=related:F296b0Qf78UJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['fine tuning', 'neural network', 'regularization']"
Yes,"Robust, Video Saliency Detection",Improved Robust Video Saliency Detection Based on Long-Term Spatial-Temporal Information,41.0,"Chenglizhao Chen, Guotao Wang, Chong Peng, Xiaowei Zhang, Hong Qin",2020.0,,,,,149.0,2022-07-13 10:10:51,,10.1109/TIP.2019.2934350,,,,,,,41,20.5,8.0,5.0,2.0,"This paper proposes to utilize supervised deep convolutional neural networks to take full advantage of the long-term spatial-temporal information in order to improve the video saliency detection performance. The conventional methods, which use the temporally neighbored frames solely, could easily encounter transient failure cases when the spatial-temporal saliency clues are less-trustworthy for a long period. To tackle the aforementioned limitation, we plan to identify those beyond-scope frames with trustworthy long-term saliency clues first and then align it with the current problem domain for an improved video saliency detection.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'robust']"
No,"Adversarial Training, Robust, Projected Gradient Descent",Improved adversarial robustness by reducing open space risk via tent activations,14.0,"A Rozsa, TE Boult",2019.0,arXiv preprint arXiv:1908.02435,arxiv.org,https://arxiv.org/abs/1908.02435,https://scholar.google.com/scholar?cites=707164045192728392&as_sdt=2005&sciodt=2007&hl=en,149.0,2022-07-12 11:56:54,,,,,,,,,14,5.07,7.0,2.0,3.0,"… [6], we focus on measuring adversarial robustness quantitatively as we evaluate the robustness of classifiers by calculating accuracies on test samples that have been perturbed by …",https://arxiv.org/pdf/1908.02435,https://scholar.google.com/scholar?q=related:SA-kz_NZ0AkJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial training', 'projected gradient descent', 'robust']"
No,"Adversarial Training, Perturbations, Robust",On Norm-Agnostic Robustness of Adversarial Training,10.0,"Bai Li, Changyou Chen, Wenlin Wang, L. Carin",2019.0,,,,,150.0,2022-07-13 09:22:57,,,,,,,,,10,3.33,3.0,4.0,3.0,"Adversarial examples are carefully perturbed in-puts for fooling machine learning models. A well-acknowledged defense method against such examples is adversarial training, where adversarial examples are injected into training data to increase robustness. In this paper, we propose a new attack to unveil an undesired property of the state-of-the-art adversarial training, that is it fails to obtain robustness against perturbations in $\ell_2$ and $\ell_\infty$ norms simultaneously. We discuss a possible solution to this issue and its limitations as well.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial training', 'perturbations', 'robust']"
No,"Adversarial, Robust, Neural Network ",Attack-Guided Efficient Robustness Verification of ReLU Neural Networks,0.0,"Y Zhu, F Wang, W Wan, M Zhang",2021.0,2021 International Joint …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9534410/,,150.0,2022-07-13 14:46:28,,,,,,,,,0,0.0,0.0,4.0,1.0,"… In summary, this work stays in line with previous sequel of works neural network robustness … attack and verification techniques to improve the efficiency of neural network verification. …",https://faculty.ecnu.edu.cn/_upload/article/files/39/62/197880be44aba90d9d44ac6de8bb/95cecb61-ce1c-49c2-a38b-d8a3728fb5c8.pdf,https://scholar.google.com/scholar?q=related:UabTrA1b3IkJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22trustworthy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'neural network', 'robust']"
Yes,"Adversarial, Data Augmentation, Robust",Addressing Neural Network Robustness with Mixup and Targeted Labeling Adversarial Training,6.0,"Alfred Laugros, A. Caplier, Matthieu Ospici",2020.0,,,,,150.0,2022-07-13 09:19:22,,10.1007/978-3-030-68238-5_14,,,,,,,6,3.0,2.0,3.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'data augmentation', 'robust']"
Yes,"Generative Adversarial Network, Training, Robust",Lipizzaner: A System That Scales Robust Generative Adversarial Network Training,12.0,"Tom Schmiedlechner, Ignavier Ng, Abdullah Al-Dujaili, Erik Hemberg, Una-May O’Reilly",2018.0,,,,,150.0,2022-07-13 09:38:44,,,,,,,,,12,3.0,2.0,5.0,4.0,"GANs are difficult to train due to convergence pathologies such as mode and discriminator collapse. We introduce Lipizzaner, an open source software system that allows machine learning engineers to train GANs in a distributed and robust way. Lipizzaner distributes a competitive coevolutionary algorithm which, by virtue of dual, adapting, generator and discriminator populations, is robust to collapses. The algorithm is well suited to efficient distribution because it uses a spatial grid abstraction. Training is local to each cell and strong intermediate training results are exchanged among overlapping neighborhoods allowing high performing solutions to propagate and improve with more rounds of training. Experiments on common image datasets overcome critical collapses. Communication overhead scales linearly when increasing the number of compute instances and we observe that increasing scale leads to improved model performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generative adversarial networks', 'robust', 'training']"
Yes,"robust, adversarial, global",Globally-Robust Neural Networks,19.0,"Klas Leino, Zifan Wang, Matt Fredrikson",2021.0,,,,,151.0,2022-07-13 09:26:34,,,,,,,,,19,19.0,6.0,3.0,1.0,"The threat of adversarial examples has motivated work on training certifiably robust neural networks to facilitate efficient verification of local robustness at inference time. We formalize a notion of global robustness, which captures the operational properties of on-line local robustness certification while yielding a natural learning objective for robust training. We show that widely-used architectures can be easily adapted to this objective by incorporating efficient global Lipschitz bounds into the network, yielding certifiably-robust models by construction that achieve state-of-theart verifiable accuracy. Notably, this approach requires significantly less time and memory than recent certifiable training methods, and leads to negligible costs when certifying points on-line; for example, our evaluation shows that it is possible to train a large robust Tiny-Imagenet model in a matter of hours. Our models effectively leverage inexpensive global Lipschitz bounds for real-time certification, despite prior suggestions that tighter local bounds are needed for good performance; we posit this is possible because our models are specifically trained to achieve tighter global bounds. Namely, we prove that the maximum achievable verifiable accuracy for a given dataset is not improved by using a local bound.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'global', 'robust']"
Yes,"Machine Translation, Noise, Accuracy",Improving robustness of machine translation with synthetic noise,51.0,"V Vaibhav, S Singh, C Stewart, G Neubig",2019.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/1902.09508,https://scholar.google.com/scholar?cites=634408279737213129&as_sdt=2005&sciodt=2007&hl=en,152.0,2022-07-13 14:14:52,,,,,,,,,51,17.0,13.0,4.0,3.0,… This paper introduced two methods of improving the resilience of vanilla MT systems to noise occurring in internet and social media text: a method of emulating specific types of noise …,https://arxiv.org/pdf/1902.09508,https://scholar.google.com/scholar?q=related:yaBNHPjezQgJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22resilience%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'nlp', 'noise']"
Yes,"trust, interpretability","Trust in AI: Interpretability is not necessary or sufficient, while black-box interaction is necessary and sufficient",0.0,MW Shen,2022.0,arXiv preprint arXiv:2202.05302,arxiv.org,https://arxiv.org/abs/2202.05302,,152.0,2022-07-12 13:46:44,,,,,,,,,0,0.0,0.0,1.0,1.0,"… The problem of human trust in artificial intelligence is one of the most fundamental problems … focus on robustness testing and performance validation on held-out data, not interpretability […",https://arxiv.org/pdf/2202.05302,https://scholar.google.com/scholar?q=related:j_lWzknWAmgJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22interpretability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['interpretability', 'trustworthy']"
No,"robustness, efficiency, GAN",Toward Robust and Efficient Training of Generative Adversarial Networks with Bayesian Approximation,6.0,"Tao Li, Kaiming Fu, Minsoo Choi, Xudong Liu, Ying Chen",2018.0,,,,,152.0,2022-07-13 09:38:44,,,,,,,,,6,1.5,1.0,5.0,4.0,"Generative adversarial networks (GANs) promote recent successes of deep learning in fields such as computer vision and speech synthesis. However, training a GAN is notoriously tricky and unpredictable, and requires substantial efforts from both human and machines. In this paper, we introduce a novel Bayesian framework based on recent advances in deep network compression, with an attempt to mitigate the robustness issue of training GANs as well as preserving computing resources. Our novelties are twofold: (i) we leverage state-of-the-art compression techniques (e.g., hashing, pruning, vector quantization, and Huffman coding) in adversarial settings; and (ii) instead of shrinking deep nets afterwards, we adapt the network at the same time of training. The stability and efficiency of our approach are confirmed by experiments under various scenarios while the performance trade-offs are shown to be negligible. Approximation Theory and Machine Learning Conference, Purdue University, September 29-30, 2018.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['efficiency', 'generative adversarial networks', 'robust']"
Yes,"Training, Framework, Robust",Can cross entropy loss be robust to label noise?,35.0,"L Feng, S Shu, Z Lin, F Lv, L Li, B An",2021.0,… Conferences on Artificial Intelligence,ijcai.org,https://www.ijcai.org/proceedings/2020/0305.pdf,https://scholar.google.com/scholar?cites=7712422440254852105&as_sdt=2005&sciodt=2007&hl=en,153.0,2022-07-13 19:04:06,PDF,,,,,,,,35,35.0,6.0,6.0,1.0,"… between CCE and other loss functions, for robust learning with label noise. To answer this question, this paper proposes a general robust learning framework to train deep models in the …",https://www.ijcai.org/proceedings/2020/0305.pdf,https://scholar.google.com/scholar?q=related:CUwk5VgHCGsJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['framework', 'robust', 'training']"
Yes,"Knowledge, Robust, Accuracy",Adversarially Robust Distillation,56.0,"Micah Goldblum, Liam Fowl, S. Feizi, T. Goldstein",2019.0,,,,,153.0,2022-07-13 10:11:25,,10.1609/AAAI.V34I04.5816,,,,,,,56,19.07,14.0,4.0,3.0,"Knowledge distillation is effective for producing small, high-performance neural networks for classification, but these small networks are vulnerable to adversarial attacks. This paper studies how adversarial robustness transfers from teacher to student during knowledge distillation. We find that a large amount of robustness may be inherited by the student even when distilled on only clean images. Second, we introduce Adversarially Robust Distillation (ARD) for distilling robustness onto student networks. In addition to producing small models with high test accuracy like conventional distillation, ARD also passes the superior robustness of large networks onto the student. In our experiments, we find that ARD student models decisively outperform adversarially trained networks of identical architecture in terms of robust accuracy, surpassing state-of-the-art methods on standard robustness benchmarks. Finally, we adapt recent fast adversarial training methods to ARD for accelerated robust distillation.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'knowledge', 'robust']"
Yes,"recommender systems, training, robust",Fight Fire with Fire: Towards Robust Recommender Systems via Adversarial Poisoning Training,5.0,"C Wu, D Lian, Y Ge, Z Zhu, E Chen…",2021.0,Proceedings of the 44th …,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3404835.3462914,https://scholar.google.com/scholar?cites=17345846308711717813&as_sdt=2005&sciodt=2007&hl=en,154.0,2022-07-13 17:19:08,,10.1145/3404835.3462914,,,,,,,5,5.0,1.0,6.0,1.0,"… the upper bound of adversarial robustness against poisoning … of adversarial training’s positive effect on building a robust … We propose a novel robust training strategy, adversarial poi…",http://staff.ustc.edu.cn/~cheneh/paper_pdf/2021/Chenwang-Wu-SIGIR.pdf,https://scholar.google.com/scholar?q=related:tVtIx1DTuPAJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['recommender systems', 'robust', 'training']"
Yes,"Training, Robust, Noise",DualGraph: A graph-based method for reasoning about label noise,1.0,H.Y. Zhang,2021.0,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85120723170&origin=inward,154.0,2022-07-12 16:35:19,Conference Paper,10.1109/CVPR46437.2021.00953,1063-6919,https://api.elsevier.com/content/abstract/scopus_id/85120723170,,,9649.0,9658.0,1,1.0,1.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['noise', 'robust', 'training']"
No,"Domain Adpatation, Mixup, Deep Learning",Improving robustness of deep learning based knee mri segmentation: Mixup and adversarial domain adaptation,49.0,"E Panfilov, A Tiulpin, S Klein…",2019.0,Proceedings of the …,openaccess.thecvf.com,http://openaccess.thecvf.com/content_ICCVW_2019/html/VRMI/Panfilov_Improving_Robustness_of_Deep_Learning_Based_Knee_MRI_Segmentation_Mixup_ICCVW_2019_paper.html,https://scholar.google.com/scholar?cites=2283373958577116606&as_sdt=2005&sciodt=2007&hl=en,154.0,2022-07-12 11:56:54,,,,,,,,,49,16.33,12.0,4.0,3.0,… mixup and adversarial unsupervised domain adaptation (UDA) – to improve the robustness of DL-… We assessed the robustness of automatic segmentation by comparing mixup and UDA …,http://openaccess.thecvf.com/content_ICCVW_2019/papers/VRMI/Panfilov_Improving_Robustness_of_Deep_Learning_Based_Knee_MRI_Segmentation_Mixup_ICCVW_2019_paper.pdf,https://scholar.google.com/scholar?q=related:vtVdeoQssB8J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'domain adaptation', 'mixup']"
No,"Training, Stability, Robust, Accuracy","Are Deep Neural Networks"" Robust""?",0.0,P Meer,2020.0,arXiv preprint arXiv:2008.12650,arxiv.org,https://arxiv.org/abs/2008.12650,,155.0,2022-07-13 13:39:40,,,,,,,,,0,0.0,0.0,1.0,2.0,"… neural network” and “robust*” returns 31,200 results in the field of computer vision). This essay argues that the robustness … , he obtained solutions for two unknowns in all ten pairs. The …",https://arxiv.org/pdf/2008.12650,https://scholar.google.com/scholar?q=related:aOrD582SF7EJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22unknowns%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'robust', 'stability', 'training']"
Yes,"Fairness, Datasets",Retiring adult: New datasets for fair machine learning,43.0,"F Ding, M Hardt, J Miller…",2021.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2021/hash/32e54441e6382a7fbacbbbaf3c450059-Abstract.html,https://scholar.google.com/scholar?cites=4475275989640781366&as_sdt=2005&sciodt=2007&hl=en,155.0,2022-07-14 10:15:05,,,,,,,,,43,43.0,11.0,4.0,1.0,"… We highlight three robust observations: … benchmark datasets in machine learning emerged only in late 1980s [19]. Created in 1987, the UCI Machine Learning Repository contributed to …",https://proceedings.neurips.cc/paper/2021/file/32e54441e6382a7fbacbbbaf3c450059-Paper.pdf,https://scholar.google.com/scholar?q=related:Np5fMs1fGz4J:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22transparency%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['dataset', 'fairness']"
No,"Object Tracking, Robust, Open Environment",Fuzzy detection aided real-time and robust visual tracking under complex environments,104.0,"S Liu, S Wang, X Liu, CT Lin, Z Lv",2020.0,IEEE Transactions on …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9132673/,https://scholar.google.com/scholar?cites=11442482513864117434&as_sdt=2005&sciodt=2007&hl=en,156.0,2022-07-13 18:08:11,,,,,,,,,104,52.0,21.0,5.0,2.0,"Today, a new generation of artificial intelligence has brought several new research domains such as computer vision (CV). Thus, target tracking, the base of CV, has been a hotspot …",https://scholar.google.com/scholar?output=instlink&q=info:ukx9szjdy54J:scholar.google.com/&hl=en&as_sdt=2007&scillfp=8162521977049022674&oi=lle,https://scholar.google.com/scholar?q=related:ukx9szjdy54J:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22reasoning%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'open environment', 'robust']"
Yes,"Object Detection, Generalization, Neural Network",Generalization and robustness implications in object-centric learning,6.0,"A Dittadi, S Papa, M De Vita, B Schölkopf…",2021.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2107.00637,https://scholar.google.com/scholar?cites=9362373326387424526&as_sdt=2005&sciodt=2007&hl=en,156.0,2022-07-13 10:14:47,,,,,,,,,6,6.0,1.0,5.0,1.0,"… Similarly, in complex machine learning downstream tasks like physical modelling and … Clevr: A diagnostic dataset for compositional language and elementary visual reasoning. In …",https://arxiv.org/pdf/2107.00637,https://scholar.google.com/scholar?q=related:Di3qte_U7YEJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22reasoning%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'generalization', 'neural network']"
Yes,"Adversarial Robustness, Accuracy, Deep Learning",Metric learning for adversarial robustness,107.0,"C Mao, Z Zhong, J Yang…",2019.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2019/hash/c24cd76e1ce41366a4bbe8a49b02a028-Abstract.html,https://scholar.google.com/scholar?cites=12602705747887433697&as_sdt=2005&sciodt=2007&hl=en,157.0,2022-07-12 11:56:54,,,,,,,,,107,36.07.00,27.0,4.0,3.0,"… -known to be fragile to adversarial attacks. We conduct … robustness, but also detects previously unseen adversarial samples. Quantitative experiments show improvement of robustness …",https://proceedings.neurips.cc/paper/2019/file/c24cd76e1ce41366a4bbe8a49b02a028-Paper.pdf,https://scholar.google.com/scholar?q=related:4aeaDxvO5a4J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial robustness', 'deep learning']"
Yes,"Adversarial, Robust",(Certified!!) Adversarial Robustness for Free!,0.0,"N Carlini, F Tramer, JZ Kolter",2022.0,arXiv preprint arXiv:2206.10550,arxiv.org,https://arxiv.org/abs/2206.10550,,158.0,2022-07-13 09:47:56,,,,,,,,,0,0.0,0.0,3.0,1.0,"… predictions as is required for randomized smoothing to obtain a robustness certificate. … empirical robustness to adversarial examples, as evaluated by robustness under adversarial …",https://arxiv.org/pdf/2206.10550,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
No,"adversarial robustness, explainability",Towards Interpretability and Robustness of Machine Learning Models,0.0,J Chen,2019.0,,search.proquest.com,https://search.proquest.com/openview/b3152e2d82185a8648394fddcee9f783/1?pq-origsite=gscholar&cbl=18750&diss=y,,158.0,2022-07-12 13:46:44,BOOK,,,,,,,,0,0.0,0.0,1.0,3.0,… models and evaluating their adversarial robustness. Part I focuses … Part II focuses on the evaluation of adversarial robustness. … We investigate the robustness of various machine learning …,https://escholarship.org/content/qt2bj9c0br/qt2bj9c0br.pdf,https://scholar.google.com/scholar?q=related:epT3GdqRuCwJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22interpretability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'explainability']"
Yes,"Smoothing, Noise, Certified Robustness","Improved, Deterministic Smoothing for L_1 Certified Robustness",12.0,"AJ Levine, S Feizi",2021.0,International Conference on Machine …,proceedings.mlr.press,http://proceedings.mlr.press/v139/levine21a.html,https://scholar.google.com/scholar?cites=4413252390109069610&as_sdt=2005&sciodt=2007&hl=en,158.0,2022-07-13 08:56:29,,,,,,,,,12,12.0,6.0,2.0,1.0,"… To the best of our knowledge, this is the first work to pro- vide deterministic … On CIFAR-10 and ImageNet datasets, we provide substantially larger l1 robustness certificates com- pared to …",http://proceedings.mlr.press/v139/levine21a/levine21a.pdf,https://scholar.google.com/scholar?q=related:KkGYmKoFPz0J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['noise', 'robust', 'smoothing']"
Yes,"Neural ODE Networks, Classification, Adversarial Attacks",On the robustness to adversarial examples of neural ode image classifiers,9.0,"F Carrara, R Caldelli, F Falchi…",2019.0,2019 IEEE International …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9035109/,https://scholar.google.com/scholar?cites=17958906168619090062&as_sdt=2005&sciodt=2007&hl=en,158.0,2022-07-13 13:05:38,,,,,,,,,9,3.0,2.0,4.0,3.0,"… In this paper, we presented an analysis of the robustness to adversarial examples of ODE-Nets — a recently introduced neural network architecture with continuous hidden …",http://cloudone.isti.cnr.it/falchi/Draft/2019-WIFS-Preprint.pdf,https://scholar.google.com/scholar?q=related:jsAcjwLaOvkJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'classification', 'neural network']"
Yes,"Adversarial, Framework, Robust, Natural Language Processing",Combating Word-level Adversarial Text with Robust Adversarial Training,0.0,"Xia Du, Jie Yu, Shasha Li, Zibo Yi, Hai Liu, Jun Ma",2021.0,,,,,158.0,2022-07-13 10:11:42,,10.1109/IJCNN52387.2021.9533725,,,,,,,0,0.0,0.0,6.0,1.0,"NLP models perform well on many tasks, but they are also easy to be fooled by adversarial examples. A small perturbation can change the output of the deep neural network model. This kind of perturbation is hard to be perceived by humans, especially adversarial examples generated by word-level adversarial attack. Character-level adversarial attack can be defended by grammar detection and word recognition. The existing word-level textual adversarial attacks are based on synonym replacement, so adversarial texts usually have correct grammar and semantics. The defense of word-level adversarial attack is more challenging. In this paper, we propose a framework which is called Robust Adversarial Training (RAT) to defend against word-level adversarial attacks. RAT enhances the model by combining adversarial training and data perturbation during training. Our experiments on two datasets show that the model based on our framework can effectively defend against word-level adversarial attacks. Compared with the existing defense methods, the model trained under RAT has a higher defense success rate on 1000 adversarial examples. In addition, the accuracy of our model on the standard testing set is also better than the existing defense methods, and the accuracy is very close to or even higher than that of the standard model.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'framework', 'nlp', 'robust']"
No,"adversarial robustness, computer vision, GAN",ECG-ATK-GAN: Robustness against Adversarial Attacks on ECG using Conditional Generative Adversarial Networks,0.0,"K Fariha Hossain, S Amit Kamran, X Ma…",2021.0,arXiv e …,ui.adsabs.harvard.edu,https://ui.adsabs.harvard.edu/abs/2021arXiv211009983F/abstract,,159.0,2022-07-13 10:50:02,,,,,,,,,0,0.0,0.0,4.0,1.0,"… Thus, safety concerns arise as it becomes challenging to establish the system's reliability, given that clinical applications require high levels of trust. To mitigate this problem and make …",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'computer vision', 'generative adversarial networks']"
Yes,"Adversarial Attacks, Neural Network, Robust",Robustness of bayesian neural networks to gradient-based attacks,29.0,"G Carbone, M Wicker, L Laurenti…",2020.0,Advances in …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2020/hash/b3f61131b6eceeb2b14835fa648a48ff-Abstract.html,https://scholar.google.com/scholar?cites=10011308363254706917&as_sdt=2005&sciodt=2007&hl=en,159.0,2022-07-13 09:30:22,,,,,,,,,29,14.5,7.0,4.0,2.0,… a possible explanation for the observed robustness of BNNs … lead to greater robustness of machine learning algorithms in … Machine learning algorithms show remarkable performance …,https://proceedings.neurips.cc/paper/2020/file/b3f61131b6eceeb2b14835fa648a48ff-Paper.pdf,https://scholar.google.com/scholar?q=related:5bKxCOpP74oJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'neural network', 'robust']"
Yes,"Generalization, Dialogue, Robust",SGD-X: A Benchmark for Robust Generalization in Schema-Guided Dialogue Systems,1.0,"H Lee, R Gupta, A Rastogi, Y Cao, B Zhang…",2022.0,… on Artificial Intelligence,ojs.aaai.org,https://ojs.aaai.org/index.php/AAAI/article/view/21341,https://scholar.google.com/scholar?cites=12792351986743106203&as_sdt=2005&sciodt=2007&hl=en,159.0,2022-07-12 13:41:36,,,,,,,,,1,1.0,0.0,6.0,1.0,… We explore the robustness of dialogue systems to linguistic … data augmentation method to improve schema robustness. … metric that quantifies the stability of model predictions across …,https://ojs.aaai.org/index.php/AAAI/article/download/21341/21090,https://scholar.google.com/scholar?q=related:m9qV61qQh7EJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22stability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['generalization', 'nlp', 'robust']"
No,adversarial robustness,Toward Adversarial Robustness via Semi-supervised Robust Training,9.0,"Yiming Li, Baoyuan Wu, Yan Feng, Yanbo Fan, Yong Jiang, Zhifeng Li, Shutao Xia",2020.0,,,,,159.0,2022-07-13 09:29:15,,,,,,,,,9,4.5,1.0,7.0,2.0,"Adversarial examples have been shown to be the severe threat to deep neural networks (DNNs). One of the most effective adversarial defense methods is adversarial training (AT) through minimizing the adversarial risk $R_{adv}$, which encourages both the benign example $x$ and its adversarially perturbed neighborhoods within the $\ell_{p}$-ball to be predicted as the ground-truth label. In this work, we propose a novel defense method, the robust training (RT), by jointly minimizing two separated risks ($R_{stand}$ and $R_{rob}$), which is with respect to the benign example and its neighborhoods respectively. The motivation is to explicitly and jointly enhance the accuracy and the adversarial robustness. We prove that $R_{adv}$ is upper-bounded by $R_{stand} + R_{rob}$, which implies that RT has similar effect as AT. Intuitively, minimizing the standard risk enforces the benign example to be correctly predicted, and the robust risk minimization encourages the predictions of the neighbor examples to be consistent with the prediction of the benign example. Besides, since $R_{rob}$ is independent of the ground-truth label, RT is naturally extended to the semi-supervised mode ($i.e.$, SRT), to further enhance the adversarial robustness. Moreover, we extend the $\ell_{p}$-bounded neighborhood to a general case, which covers different types of perturbations, such as the pixel-wise ($i.e.$, $x + \delta$) or the spatial perturbation ($i.e.$, $ AX + b$). Extensive experiments on benchmark datasets not only verify the superiority of the proposed SRT method to state-of-the-art methods for defensing pixel-wise or spatial perturbations separately, but also demonstrate its robustness to both perturbations simultaneously. The code for reproducing main results is available at \url{this https URL}.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['adversarial robustness']
Yes,"robustness, recurrent neural netweork, accuracy",How Robust are Deep Neural Networks?,15.0,"B. Sengupta, Karl J. Friston",2018.0,,,,,160.0,2022-07-13 09:24:12,,,,,,,,,15,4.15,8.0,2.0,4.0,"Convolutional and Recurrent, deep neural networks have been successful in machine learning systems for computer vision, reinforcement learning, and other allied fields. However, the robustness of such neural networks is seldom apprised, especially after high classification accuracy has been attained. In this paper, we evaluate the robustness of three recurrent neural networks to tiny perturbations, on three widely used datasets, to argue that high accuracy does not always mean a stable and a robust (to bounded perturbations, adversarial attacks, etc.) system. Especially, normalizing the spectrum of the discrete recurrent network to bound the spectrum (using power method, Rayleigh quotient, etc.) on a unit disk produces stable, albeit highly non-robust neural networks. Furthermore, using the $\epsilon$-pseudo-spectrum, we show that training of recurrent networks, say using gradient-based methods, often result in non-normal matrices that may or may not be diagonalizable. Therefore, the open problem lies in constructing methods that optimize not only for accuracy but also for the stability and the robustness of the underlying neural network, a criterion that is distinct from the other.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'recurrent neural network', 'robust']"
No,"adversarial robustness, privacy, federated learning",Understanding the Interplay between Privacy and Robustness in Federated Learning,1.0,"Y Han, Y Cao, M Yoshikawa",2021.0,arXiv preprint arXiv:2106.07033,arxiv.org,https://arxiv.org/abs/2106.07033,https://scholar.google.com/scholar?cites=10394492587768335042&as_sdt=2005&sciodt=2007&hl=en,160.0,2022-07-12 11:55:13,,,,,,,,,1,1.0,0.0,3.0,1.0,"… However, it is still not clear how LDP affects adversarial robustness in FL. To … robustness in FL. Clarifying the interplay is significant since this is the first step towards a principled design …",https://arxiv.org/pdf/2106.07033,https://scholar.google.com/scholar?q=related:wspcqu-nQJAJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'federated learning', 'privacy']"
No,robustness to natural perturbations,A Behavioral Approach to Robust Machine Learning,0.0,MPG Revay,2021.0,,ses.library.usyd.edu.au,https://ses.library.usyd.edu.au/handle/2123/27515,,161.0,2022-07-14 10:07:00,,,,,,,,,0,0.0,0.0,1.0,1.0,"… the difficulty of ensuring reliable and robust model behavior. Deep … Specifically, we apply methods from robust and nonlinear … robust model sets in data-driven nonlinear observer design …",https://ses.library.usyd.edu.au/bitstream/handle/2123/27515/Revay_Max_thesis.pdf?sequence=2,https://scholar.google.com/scholar?q=related:mVXclb1xJMsJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['perturbations']
No,"Noise, Neural Network, Regularization",Improving the robustness of threshold-based single hidden layer neural networks via regularization,3.0,"E Ragusa, C Gianoglio, R Zunino…",2020.0,… on Artificial Intelligence …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9073976/,https://scholar.google.com/scholar?cites=12433820024470431024&as_sdt=2005&sciodt=2007&hl=en,161.0,2022-07-12 11:55:13,,,,,,,,,3,1.5,1.0,4.0,2.0,"… Their robustness to perturbations at the node level, though, is an issue: a small perturbation … This paper shows that the robustness of this class of SLFNs can be improved by introducing …",https://ieeexplore.ieee.org/iel7/9066468/9072687/09073976.pdf,https://scholar.google.com/scholar?q=related:MOWyX2_NjawJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural network', 'noise', 'regularization']"
Yes,"Text Classification, Robust, Interpretability",Assessing Robustness of Text Classification through Maximal Safe Radius Computation,6.0,"Emanuele La Malfa, Min Wu, L. Laurenti, Benjie Wang, A. Hartshorn, M. Kwiatkowska",2020.0,,,,,162.0,2022-07-13 09:28:50,,10.18653/v1/2020.findings-emnlp.266,,,,,,,6,3.0,1.0,6.0,2.0,"Neural network NLP models are vulnerable to small modifications of the input that maintain the original meaning but result in a different prediction. In this paper, we focus on robustness of text classification against word substitutions, aiming to provide guarantees that the model prediction does not change if a word is replaced with a plausible alternative, such as a synonym. As a measure of robustness, we adopt the notion of the maximal safe radius for a given input text, which is the minimum distance in the embedding space to the decision boundary. Since computing the exact maximal safe radius is not feasible in practice, we instead approximate it by computing a lower and upper bound. For the upper bound computation, we employ Monte Carlo Tree Search in conjunction with syntactic filtering to analyse the effect of single and multiple word substitutions. The lower bound computation is achieved through an adaptation of the linear bounding techniques implemented in tools CNN-Cert and POPQORN, respectively for convolutional and recurrent network models. We evaluate the methods on sentiment analysis and news classification models for four datasets (IMDB, SST, AG News and NEWS) and a range of embeddings, and provide an analysis of robustness trends. We also apply our framework to interpretability analysis and compare it with LIME.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['interpretability', 'nlp', 'robust']"
Yes,"Deep Neural Network, Performance, Stability, Robust, Out-of-Distribution",An empirical study on robustness of DNNs with out-of-distribution awareness,2.0,"L Zhou, B Yu, D Berend, X Xie, X Li…",2020.0,2020 27th Asia …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9359272/,https://scholar.google.com/scholar?cites=7770340534352825304&as_sdt=2005&sciodt=2007&hl=en,162.0,2022-07-13 14:02:43,,,,,,,,,2,1.0,0.0,6.0,2.0,"… Abstract—The state-of-the-art deep neural network (DNN) … about the effectiveness and robustness of different techniques … images, with high prediction confidence. It is also found in our …",,https://scholar.google.com/scholar?q=related:2EvrBYzL1WsJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'out-of-distribution data', 'performance', 'robust', 'stability']"
No,"Certified Robustness, Data Poisoning",Intrinsic Certified Robustness of Bagging against Data Poisoning Attacks,38.0,"Jinyuan Jia, Xiaoyu Cao, N. Gong",2020.0,,,,,162.0,2022-07-13 09:24:04,,,,,,,,,38,19.0,13.0,3.0,2.0,"In a \emph{data poisoning attack}, an attacker modifies, deletes, and/or inserts some training examples to corrupt the learnt machine learning model. \emph{Bootstrap Aggregating (bagging)} is a well-known ensemble learning method, which trains multiple base models on random subsamples of a training dataset using a base learning algorithm and uses majority vote to predict labels of testing examples. We prove the intrinsic certified robustness of bagging against data poisoning attacks. Specifically, we show that bagging with an arbitrary base learning algorithm provably predicts the same label for a testing example when the number of modified, deleted, and/or inserted training examples is bounded by a threshold. Moreover, we show that our derived threshold is tight if no assumptions on the base learning algorithm are made. We evaluate our method on MNIST and CIFAR10. For instance, our method achieves a certified accuracy of $91.1\%$ on MNIST when arbitrarily modifying, deleting, and/or inserting 100 training examples.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['data poisoning', 'robust']"
Yes,"robust, adversarial, accuracy",Improved Adversarial Robustness by Reducing Open Space Risk via Tent Activations,12.0,"Andras Rozsa, T. Boult",2019.0,,,,,162.0,2022-07-13 09:22:41,,,,,,,,,12,4.0,6.0,2.0,3.0,"Adversarial examples contain small perturbations that can remain imperceptible to human observers but alter the behavior of even the best performing deep learning models and yield incorrect outputs. Since their discovery, adversarial examples have drawn significant attention in machine learning: researchers try to reveal the reasons for their existence and improve the robustness of machine learning models to adversarial perturbations. The state-of-the-art defense is the computationally expensive and very time consuming adversarial training via projected gradient descent (PGD). We hypothesize that adversarial attacks exploit the open space risk of classic monotonic activation functions. This paper introduces the tent activation function with bounded open space risk and shows that tents make deep learning models more robust to adversarial attacks. We demonstrate on the MNIST dataset that a classifier with tents yields an average accuracy of 91.8% against six white-box adversarial attacks, which is more than 15 percentage points above the state of the art. On the CIFAR-10 dataset, our approach improves the average accuracy against the six white-box adversarial attacks to 73.5% from 41.8% achieved by adversarial training via PGD.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial', 'robust']"
Yes,"Measure, Robust, Adversarial, Out-of-Distribution",Checking robustness of representations learned by deep neural networks,3.0,"K Szyc, T Walkowiak, H Maciejewski",2021.0,… Conference on Machine Learning …,Springer,https://link.springer.com/chapter/10.1007/978-3-030-86517-7_25,https://scholar.google.com/scholar?cites=1752577556058441796&as_sdt=2005&sciodt=2007&hl=en,163.0,2022-07-13 10:02:07,,10.1007/978-3-030-86517-7_25,,,,,,,3,3.0,1.0,3.0,1.0,"… To address this problem, we propose a new method and a measure called robustness score. The method allows indicating which classes are recognized by the deep model using non-…",https://2021.ecmlpkdd.org/wp-content/uploads/2021/07/sub_987.pdf,https://scholar.google.com/scholar?q=related:RIgvbgBoUhgJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22transparency%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'metric', 'out-of-distribution data', 'robust']"
Yes,"Adversarial Training, Loss, Siamese Network",Improving Deep Neural Network Robustness with Siamese Empowered Adversarial Training,0.0,"Y Zhu, Z Li, F Xu, S Zhong",2020.0,"… on Security, Privacy and Anonymity in …",Springer,https://link.springer.com/chapter/10.1007/978-3-030-68851-6_4,,164.0,2022-07-13 12:24:11,,10.1007/978-3-030-68851-6_4,,,,,,,0,0.0,0.0,4.0,2.0,"… more flexible by balancing the two design choices. And the inter… training design achieves the best model robustness in most … Second, we evaluate our design with extensive experiments …",,https://scholar.google.com/scholar?q=related:TDskB5U_4asJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial training', 'loss function', 'siamese network']"
No,"Adversarial Training, Classification, Robust",Improving robustness of jet tagging algorithms with adversarial training,1.0,"A Stein, X Coubez, S Mondal, A Novak…",2022.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2203.13890,https://scholar.google.com/scholar?cites=14904130541530839842&as_sdt=2005&sciodt=2007&hl=en,164.0,2022-07-13 13:05:38,,,,,,,,,1,1.0,0.0,5.0,1.0,"… All tests conducted with the nominal training confirm earlier findings that relate higher performance with higher susceptibility, now for a deep neural network that replicates a typical jet …",https://arxiv.org/pdf/2203.13890,https://scholar.google.com/scholar?q=related:Ivv-FcMb1s4J:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial training', 'classification', 'robust']"
Yes,"Robust, Design, Convolutional Neural Network",Empirical Evaluation on Robustness of Deep Convolutional Neural Networks Activation Functions Against Adversarial Perturbation,3.0,"Jiawei Su, Danilo Vasconcellos Vargas, K. Sakurai",2018.0,,,,,164.0,2022-07-13 09:27:43,,10.1109/CANDARW.2018.00049,,,,,,,3,1.15,1.0,3.0,4.0,"Recent research has shown that deep convolutional neural networks (DCNN) are vulnerable to several different types of attacks while the reasons of such vulnerability are still under investigation. For instance, the adversarial perturbations can conduct a slight change on a natural image to make the target DCNN make the wrong recognition, while the reasons that DCNN is sensitive to such small modification are divergent from one research to another. In this paper, we evaluate the robustness of two commonly used activation functions of DCNN, namely the sigmoid and ReLu, against the recently proposed low-dimensional one-pixel attack. We show that the choosing of activation functions can be an important factor that influences the robustness of DCNN. The results show that comparing with sigmoid, the ReLu non-linearity is more vulnerable which allows the low dimensional one-pixel attack exploit much higher success rate and confidence of launching the attack. The results give insights on designing new activation functions to enhance the security of DCNN.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'design', 'robust']"
No,"Adversarial, Robust, Classifier, Federated Learning",DiPSeN: Differentially Private Self-normalizing Neural Networks For Adversarial Robustness in Federated Learning,2.0,"O Ibitoye, MO Shafiq, A Matrawy",2021.0,arXiv preprint arXiv:2101.03218,arxiv.org,https://arxiv.org/abs/2101.03218,https://scholar.google.com/scholar?cites=4837034637102260888&as_sdt=2005&sciodt=2007&hl=en,165.0,2022-07-13 10:30:58,,,,,,,,,2,2.0,1.0,3.0,1.0,… in improving the adversarial robustness of neural networks in a … model that maintains the stability of the network during the … Adversarial Machine Learning A machine learning model …,https://arxiv.org/pdf/2101.03218,https://scholar.google.com/scholar?q=related:mKK97l-ZIEMJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22stability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'classifier', 'federated learning', 'robust']"
Yes,"Robust, Adversarial, Deep Neural Network",Deeply Supervised Discriminative Learning for Adversarial Defense,12.0,"Aamir Mustafa, S. Khan, Munawar Hayat, R. Goecke, Jianbing Shen, L. Shao",2020.0,,,,,166.0,2022-07-13 09:26:34,,10.1109/TPAMI.2020.2978474,,,,,,,12,6.0,2.0,6.0,2.0,"Deep neural networks can easily be fooled by an adversary with minuscule perturbations added to an input image. The existing defense techniques suffer greatly under white-box attack settings, where an adversary has full knowledge of the network and can iterate several times to find strong perturbations. We observe that the main reason for the existence of such vulnerabilities is the close proximity of different class samples in the learned feature space of deep models. This allows the model decisions to be completely changed by adding an imperceptible perturbation to the inputs. To counter this, we propose to class-wise disentangle the intermediate feature representations of deep networks, specifically forcing the features for each class to lie inside a convex polytope that is maximally separated from the polytopes of other classes. In this manner, the network is forced to learn distinct and distant decision regions for each class. We observe that this simple constraint on the features greatly enhances the robustness of learned models, even against the strongest white-box attacks, without degrading the classification performance on clean images. We report extensive evaluations in both black-box and white-box attack scenarios and show significant gains in comparison to state-of-the-art defenses.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'deep learning', 'robust']"
Yes,"Robust, Causality, Out-of-Distribution Data",Improving model robustness using causal knowledge,4.0,"T Kyono, M van der Schaar",2019.0,arXiv preprint arXiv:1911.12441,arxiv.org,https://arxiv.org/abs/1911.12441,https://scholar.google.com/scholar?cites=12212178965453373622&as_sdt=2005&sciodt=2007&hl=en,166.0,2022-07-12 13:35:23,,,,,,,,,4,1.33,2.0,2.0,3.0,… our model selection based on the preservation of the causal structure of the machine learning model predictions and input variables relative to our prior causal or human knowledge of …,https://arxiv.org/pdf/1911.12441,https://scholar.google.com/scholar?q=related:toBY8RBgeqkJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22human+knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['causality', 'out-of-distribution data', 'robust']"
Yes,"Robust, Adversarial, Accuracy",An orthogonal classifier for improving the adversarial robustness of neural networks,3.0,"C Xu, X Li, M Yang",2022.0,Information Sciences,Elsevier,https://www.sciencedirect.com/science/article/pii/S0020025522000627,https://scholar.google.com/scholar?cites=17423309891033919691&as_sdt=2005&sciodt=2007&hl=en,167.0,2022-07-12 11:56:54,HTML,,,,,,,,3,3.0,1.0,3.0,1.0,"… To further improve the robustness, we turn to adversarial samples and introduce a special … the balance between natural accuracy and adversarial robustness. According to the sensitivity …",https://www.sciencedirect.com/science/article/pii/S0020025522000627,https://scholar.google.com/scholar?q=related:y3CggggIzPEJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial', 'robust']"
Yes,"Deep Learning, Robust, Adversarial",Analysis on Adversarial Robustness of Deep Learning Model LeNet-5 Based on Data Perturbation,0.0,"Y Liu, M Lu, D Peng, J Wang, J Ai",2020.0,2020 7th International …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9331165/?casa_token=B-5UdbUWOhQAAAAA:pSwFLffE0Yl_8mYWz0lfqXsE5yeK1HlGibIRn7EnhdUg4yTDPfUGamsWhAvA9x5QcKJSSlkvig,,168.0,2022-07-13 14:26:15,,,,,,,,,0,0.0,0.0,5.0,2.0,"… robustness, remains as a problem worthy of attention. In the present study, a deep learning model based on the convolutional neural network … has made the reliability and security of …",https://ieeexplore.ieee.org/iel7/9329443/9331011/09331165.pdf?casa_token=BOLWr4OhnZYAAAAA:hEXnbhh1m41MnKxkEm-lzy6DaU9hJrTWx9sfpjdPgWQ6vDr2jHcz2D2Kzonzq_uyn4mRP9lspg,https://scholar.google.com/scholar?q=related:G2dl_N7pjQcJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22reliability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'deep learning', 'robust']"
No,"Error Rate, Speech Recognition, Deep Learning",Joint training of front-end and back-end deep neural networks for robust speech recognition,68.0,"Tian Gao, Jun Du, Lirong Dai, Chin-Hui Lee",2015.0,,,,,168.0,2022-07-13 10:11:25,,10.1109/ICASSP.2015.7178797,,,,,,,68,10.11,17.0,4.0,7.0,"Based on the recently proposed speech pre-processing front-end with deep neural networks (DNNs), we first investigate different feature mapping directly from noisy speech via DNN for robust speech recognition. Next, we propose to jointly train a single DNN for both feature mapping and acoustic modeling. In the end, we show that the word error rate (WER) of the jointly trained system could be significantly reduced by the fusion of multiple DNN pre-processing systems which implies that features obtained from different domains of the DNN-enhanced speech signals are strongly complementary. Testing on the Aurora4 noisy speech recognition task our best system with multi-condition training can achieves an average WER of 10.3%, yielding a relative reduction of 16.3% over our previous DNN pre-processing only system with a WER of 12.3%. To the best of our knowledge, this represents the best published result on the Aurora4 task without using any adaptation techniques.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'error rate', 'time series']"
Yes,"Machine Translation, Data Augmentation, Noise",Improving Neural Machine Translation Robustness via Data Augmentation,0.0,Z Li,2019.0,,zhenhaoli.net,https://zhenhaoli.net/files/master_thesis.pdf,,168.0,2022-07-13 14:36:56,PDF,,,,,,,,0,0.0,0.0,1.0,3.0,"… In the first experiment, we try with different model architectures, based on recurrent neural network (RNN), convolutional neural network (CNN) and Transformer. In the rest experiments, …",https://zhenhaoli.net/files/master_thesis.pdf,https://scholar.google.com/scholar?q=related:c91ZWvOJYR8J:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22accountability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['data augmentation', 'nlp', 'noise']"
Discussion,"Adversarial, Robust, Benchmark",Are Adversarial Robustness and Common Perturbation Robustness Independent Attributes ?,17.0,"Alfred Laugros, A. Caplier, Matthieu Ospici",2019.0,,,,,168.0,2022-07-13 09:19:22,,10.1109/ICCVW.2019.00134,,,,,,,17,6.07,6.0,3.0,3.0,"Neural Networks have been shown to be sensitive to common perturbations such as blur, Gaussian noise, rotations, etc. They are also vulnerable to some artificial malicious corruptions called adversarial examples. The adversarial examples study has recently become very popular and it sometimes even reduces the term ""adversarial robustness"" to the term ""robustness"". Yet, we do not know to what extent the adversarial robustness is related to the global robustness. Similarly, we do not know if a robustness to various common perturbations such as translations or contrast losses for instance, could help with adversarial corruptions. We intend to study the links between the robustnesses of neural networks to both perturbations. With our experiments, we provide one of the first benchmark designed to estimate the robustness of neural networks to common perturbations. We show that increasing the robustness to carefully selected common perturbations, can make neural networks more robust to unseen common perturbations. We also prove that adversarial robustness and robustness to common perturbations are independent. Our results make us believe that neural network robustness should be addressed in a broader sense.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'benchmark', 'robust']"
Yes,"Explainability, Adversarial Attacks, LIME",Fooling LIME and SHAP: Adversarial Attacks on Post hoc Explanation Methods,258.0,"Dylan Slack, Sophie Hilgard, Emily Jia, Sameer Singh, Himabindu Lakkaraju",2019.0,,,,,169.0,2022-07-13 09:22:33,,10.1145/3375627.3375830,,,,,,,258,86.0,52.0,5.0,3.0,"As machine learning black boxes are increasingly being deployed in domains such as healthcare and criminal justice, there is growing emphasis on building tools and techniques for explaining these black boxes in an interpretable manner. Such explanations are being leveraged by domain experts to diagnose systematic errors and underlying biases of black boxes. In this paper, we demonstrate that post hoc explanations techniques that rely on input perturbations, such as LIME and SHAP, are not reliable. Specifically, we propose a novel scaffolding technique that effectively hides the biases of any given classifier by allowing an adversarial entity to craft an arbitrary desired explanation. Our approach can be used to scaffold any biased classifier in such a way that its predictions on the input data distribution still remain biased, but the post hoc explanations of the scaffolded classifier look innocuous. Using extensive evaluation with multiple real world datasets (including COMPAS), we demonstrate how extremely biased (racist) classifiers crafted by our framework can easily fool popular explanation techniques such as LIME and SHAP into generating innocuous explanations which do not reflect the underlying biases.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'explainability', 'lime']"
Yes,"Adversarial, Spiking Neural Network, Robust, Resilience",A Comprehensive Analysis on Adversarial Robustness of Spiking Neural Networks,17.0,"Saima Sharmin, P. Panda, Syed Shakib Sarwar, Chankyu Lee, Wachirawit Ponghiran, K. Roy",2019.0,,,,,169.0,2022-07-13 09:19:22,,10.1109/IJCNN.2019.8851732,,,,,,,17,6.07,3.0,6.0,3.0,"In this era of machine learning models, their functionality is being threatened by adversarial attacks. In the face of this struggle for making artificial neural networks robust, finding a model, resilient to these attacks, is very important. In this work, we present, for the first time, a comprehensive analysis of the behavior of more bio-plausible networks, namely Spiking Neural Network (SNN) under state-of-the-art adversarial tests. We perform a comparative study of the accuracy degradation between conventional VGG-9 Artificial Neural Network (ANN) and equivalent spiking network with CIFAR-10 dataset in both whitebox and blackbox setting for different types of single-step and multi-step FGSM (Fast Gradient Sign Method) attacks. We demonstrate that SNNs tend to show more resiliency compared to ANN under blackbox attack scenario. Additionally, we find that SNN robustness is largely dependent on the corresponding training mechanism. We observe that SNNs trained by spike-based backpropagation are more adversarially robust than the ones obtained by ANN-to-SNN conversion rules in several whitebox and blackbox scenarios. Finally, we also propose a simple, yet, effective framework for crafting adversarial attacks from SNNs. Our results suggest that attacks crafted from SNNs following our proposed method are much stronger than those crafted from ANNs.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'resilience', 'robust', 'spiking neural network']"
Yes,"Robust, Jacobian, Adversarial Training",Jacobian Adversarially Regularized Networks for Robustness,40.0,"Alvin Chan, Yi Tay, Y. Ong, Jie Fu",2019.0,,,,,171.0,2022-07-13 09:28:50,,,,,,,,,40,13.33,10.0,4.0,3.0,"Adversarial examples are crafted with imperceptible perturbations with the intent to fool neural networks. Against such attacks, adversarial training and its variants stand as the strongest defense to date. Previous studies have pointed out that robust models that have undergone adversarial training tend to produce more salient and interpretable Jacobian matrices than their non-robust counterparts. A natural question is whether a model trained with an objective to produce salient Jacobian can result in better robustness. This paper answers this question with affirmative empirical results. We propose Jacobian Adversarially Regularized Networks (JARN) as a method to optimize the saliency of a classifier's Jacobian by adversarially regularizing the model's Jacobian to resemble natural training images. Image classifiers trained with JARN show improved robust accuracy compared to standard models on the MNIST, SVHN and CIFAR-10 datasets, uncovering a new angle to boost robustness without using adversarial training examples.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial training', 'jacobian', 'robust']"
Yes,"adversarial robustness, neural networks, robustness method",Towards Robust Deep Neural Networks with BANG,64.0,"Andras Rozsa, Manuel Günther, T. Boult",2016.0,,,,,172.0,2022-07-13 09:26:01,,10.1109/WACV.2018.00093,,,,,,,64,"0,4631944444",21.0,3.0,6.0,"Machine learning models, including state-of-the-art deep neural networks, are vulnerable to small perturbations that cause unexpected classification errors. This unexpected lack of robustness raises fundamental questions about their generalization properties and poses a serious concern for practical deployments. As such perturbations can remain imperceptible – the formed adversarial examples demonstrate an inherent inconsistency between vulnerable machine learning models and human perception – some prior work casts this problem as a security issue. Despite the significance of the discovered instabilities and ensuing research, their cause is not well understood and no effective method has been developed to address the problem. In this paper, we present a novel theory to explain why this unpleasant phenomenon exists in deep neural networks. Based on that theory, we introduce a simple, efficient, and effective training approach, Batch Adjusted Network Gradients (BANG), which significantly improves the robustness of machine learning models. While the BANG technique does not rely on any form of data augmentation or the utilization of adversarial images for training, the resultant classifiers are more resistant to adversarial perturbations while maintaining or even enhancing the overall classification performance.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'neural networks', 'robust']"
No,"Image Segmentation, Convolutional Neural Network, Robust",Orientation robust object detection in aerial images using deep convolutional neural network,218.0,"H Zhu, X Chen, W Dai, K Fu, Q Ye…",2015.0,2015 IEEE International …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/7351502/,https://scholar.google.com/scholar?cites=7585146723170615230&as_sdt=2005&sciodt=2007&hl=en,172.0,2022-07-14 14:18:52,,,,,,,,,218,31.14.00,36.0,6.0,7.0,"… For orientation robust DCNN feature extraction, we employ the … Other works demonstrated that the best performance is … FC7, are considered for rotation robust feature extraction. …",http://lamp.ucas.ac.cn/downloads/publication/ICIP2015_ZhuHaigang.pdf,https://scholar.google.com/scholar?q=related:vn_Nw8HaQ2kJ:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22performance%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'convolutional neural network', 'robust']"
No,"Deep Learning, Lipschitz Continuity, Robust",Safety and Robustness for Deep Learning with Provable Guarantees,1.0,M. Kwiatkowska,2019.0,,,,,173.0,2022-07-13 09:22:57,,10.1145/3338906.3342812,,,,,,,1,"0,02291666667",1.0,1.0,3.0,"Computing systems are becoming ever more complex, with decisions increasingly often based on deep learning components. A wide variety of applications are being developed, many of them safety-critical, such as self-driving cars and medical diagnosis. Since deep learning is unstable with respect to adversarial perturbations, there is a need for rigorous software development methodologies that encompass machine learning components. This lecture will describe progress with developing automated verification and testing techniques for deep neural networks to ensure safety and robustness of their decisions with respect to bounded input perturbations. The techniques exploit Lipschitz continuity of the networks and aim to approximate, for a given set of inputs, the reachable set of network outputs in terms of lower and upper bounds, in anytime manner, with provable guarantees. We develop novel algorithms based on feature-guided search, games, global optimisation and Bayesian methods, and evaluate them on state-of-the-art networks. The lecture will conclude with an overview of the challenges in this field.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'lipschitz', 'robust']"
Yes,"Robust, Accuracy, LSTM",Certified Robustness to Programmable Transformations in LSTMs,8.0,"Yuhao Zhang, Aws Albarghouthi, Loris D'antoni",2021.0,,,,,173.0,2022-07-13 09:26:34,,10.18653/v1/2021.emnlp-main.82,,,,,,,8,8.0,3.0,3.0,1.0,"Deep neural networks for natural language processing are fragile in the face of adversarial examples—small input perturbations, like synonym substitution or word duplication, which cause a neural network to change its prediction. We present an approach to certifying the robustness of LSTMs (and extensions of LSTMs) and training models that can be efficiently certified. Our approach can certify robustness to intractably large perturbation spaces defined programmatically in a language of string transformations. Our evaluation shows that (1) our approach can train models that are more robust to combinations of string transformations than those produced using existing techniques; (2) our approach can show high certification accuracy of the resulting models.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'recurrent neural network', 'robust']"
No,"Malware, Robust, Adversarial",Domain Constraints in Feature Space: Strengthening Robustness of Android Malware Detection against Realizable Adversarial Examples,0.0,"H Bostani, Z Liu, Z Zhao, V Moonsamy",2022.0,arXiv preprint arXiv:2205.15128,arxiv.org,https://arxiv.org/abs/2205.15128,,173.0,2022-07-13 11:20:18,,,,,,,,,0,0.0,0.0,4.0,1.0,"… Strengthening the robustness of machine learning-based malware detectors against … in the perfect knowledge setting, but here we also test it in a limited knowledge setting where the …",https://arxiv.org/pdf/2205.15128,https://scholar.google.com/scholar?q=related:RAGPjkCLSo8J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'cybersecurity', 'robust']"
Yes,"Adversarial, Robust, Classification, Accuracy, Generalization",Consistency Regularization for Certified Robustness of Smoothed Classifiers,25.0,"Jongheon Jeong, Jinwoo Shin",2020.0,,,,,174.0,2022-07-13 09:26:34,,,,,,,,,25,12.5,13.0,2.0,2.0,"A recent technique of randomized smoothing has shown that the worst-case (adversarial) $\ell_2$-robustness can be transformed into the average-case Gaussian-robustness by ""smoothing"" a classifier, i.e., by considering the averaged prediction over Gaussian noise. In this paradigm, one should rethink the notion of adversarial robustness in terms of generalization ability of a classifier under noisy observations. We found that the trade-off between accuracy and certified robustness of smoothed classifiers can be greatly controlled by simply regularizing the prediction consistency over noise. This relationship allows us to design a robust training objective without approximating a non-existing smoothed classifier, e.g., via soft smoothing. Our experiments under various deep neural network architectures and datasets demonstrate that the ""certified"" $\ell_2$-robustness can be dramatically improved with the proposed regularization, even achieving better or comparable results to the state-of-the-art approaches with significantly less training costs and hyperparameters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial', 'classification', 'generalization', 'robust']"
Yes,"Deep Neural Network, Robust, Adversarial",AdvGuard: Fortifying Deep Neural Networks against Optimized Adversarial Example Attack,6.0,H. Kwon,2020.0,IEEE Access,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85097940600&origin=inward,174.0,2022-07-12 16:33:17,Article,10.1109/ACCESS.2020.3042839,2169-3536,https://api.elsevier.com/content/abstract/scopus_id/85097940600,,,,,6,3.0,6.0,1.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'deep learning', 'robust']"
No,"Recurrent Neural Network, Stability, Robust",A Convex Parameterization of Robust Recurrent Neural Networks,7.0,M. Revay,2021.0,IEEE Control Systems Letters,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85096842800&origin=inward,174.0,2022-07-12 16:26:49,Article,10.1109/LCSYS.2020.3038221,2475-1456,https://api.elsevier.com/content/abstract/scopus_id/85096842800,5.0,4.0,1363.0,1368.0,7,7.0,7.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['recurrent neural network', 'robust', 'stability']"
Yes,"Adversarial, Robust, Out-of-Distribution",Certifiably Adversarially Robust Detection of Out-of-Distribution Data,22.0,"Julian Bitterwolf, Alexander Meinke, Matthias Hein",2020.0,,,,,174.0,2022-07-13 10:11:42,,,,,,,,,22,11.0,7.0,3.0,2.0,"Deep neural networks are known to be overconfident when applied to out-of-distribution (OOD) inputs which clearly do not belong to any class. This is a problem in safety-critical applications since a reliable assessment of the uncertainty of a classifier is a key property, allowing the system to trigger human intervention or to transfer into a safe state. In this paper, we aim for certifiable worst case guarantees for OOD detection by enforcing not only low confidence at the OOD point but also in an $l_\infty$-ball around it. For this purpose, we use interval bound propagation (IBP) to upper bound the maximal confidence in the $l_\infty$-ball and minimize this upper bound during training time. We show that non-trivial bounds on the confidence for OOD data generalizing beyond the OOD dataset seen at training time are possible. Moreover, in contrast to certified adversarial robustness which typically comes with significant loss in prediction performance, certified guarantees for worst case OOD detection are possible without much loss in accuracy.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'out-of-distribution data', 'robust']"
Yes,"Dataset, Out-of-Distribution Data, Robust",Pretraining boosts out-of-domain robustness for pose estimation,59.0,"A Mathis, T Biasi, S Schneider…",2021.0,Proceedings of the …,openaccess.thecvf.com,http://openaccess.thecvf.com/content/WACV2021/html/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.html,https://scholar.google.com/scholar?cites=18437109915355843291&as_sdt=2005&sciodt=2007&hl=en,174.0,2022-07-13 13:38:18,,,,,,,,,59,59.0,15.0,4.0,1.0,"… shift), we also investigate the robustness with respect to image … -domain robustness on different individuals, nor robustness to … Lines with alpha transparency represent corrupted images, …",http://openaccess.thecvf.com/content/WACV2021/papers/Mathis_Pretraining_Boosts_Out-of-Domain_Robustness_for_Pose_Estimation_WACV_2021_paper.pdf,https://scholar.google.com/scholar?q=related:2wqetcjF3f8J:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22transparency%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['dataset', 'out-of-distribution data', 'robust']"
No,"Training, Adversarial, Interpretability",Bridging adversarial robustness and gradient interpretability,29.0,"B Kim, J Seo, T Jeon",2019.0,arXiv preprint arXiv:1903.11626,arxiv.org,https://arxiv.org/abs/1903.11626,https://scholar.google.com/scholar?cites=16006700487228917183&as_sdt=2005&sciodt=2007&hl=en,175.0,2022-07-12 11:56:54,,,,,,,,,29,10.07,10.0,3.0,3.0,"… In this paper, we attempted to bridge this gap between adversarial robustness and gradient interpretability through a series of experiments that addresses the following questions:1 “why …",https://arxiv.org/pdf/1903.11626,https://scholar.google.com/scholar?q=related:vwVaN0k5I94J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'interpretability', 'training']"
Yes,"Noise, Classification, Deep Learning",Robustness of classifiers to uniform and Gaussian noise,35.0,"JY Franceschi, A Fawzi…",2018.0,… on Artificial Intelligence …,proceedings.mlr.press,http://proceedings.mlr.press/v84/franceschi18a.html,https://scholar.google.com/scholar?cites=9960908843152398325&as_sdt=2005&sciodt=2007&hl=en,175.0,2022-07-12 11:49:54,,,,,,,,,35,9.15,12.0,3.0,4.0,… We study the robustness of classifiers to various kinds of random … We characterize this robustness to random noise in terms of the … The predicted robustness is verified experimentally. …,http://proceedings.mlr.press/v84/franceschi18a/franceschi18a.pdf,https://scholar.google.com/scholar?q=related:9VclVNFBPIoJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'deep learning', 'noise']"
Yes,"Data Augmentation, Neural Network, Generalization",Realistic Adversarial Data Augmentation for MR Image Segmentation,31.0,"Chen Chen, C. Qin, Huaqi Qiu, C. Ouyang, Shuo Wang, Liang Chen, G. Tarroni, Wenjia Bai, D. Rueckert",2020.0,,,,,175.0,2022-07-13 09:26:34,,10.1007/978-3-030-59710-8_65,,,,,,,31,15.5,3.0,9.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['data augmentation', 'generalization', 'neural network']"
No,"Noise, Speech Recognition, Convolutional Neural Network",Noise robust speech recognition using recent developments in neural networks for computer vision,23.0,"T Yoshioka, K Ohnishi, F Fang…",2016.0,2016 IEEE International …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/7472775/,https://scholar.google.com/scholar?cites=16097134733060814495&as_sdt=2005&sciodt=2007&hl=en,176.0,2022-07-14 13:53:52,,,,,,,,,23,4.23,6.0,4.0,6.0,… There are three basic neural network architectures that have been used for acoustic … CNNs are known to be effective especially when speech features are corrupted by noise [3]. …,,https://scholar.google.com/scholar?q=related:nzax4cCCZN8J:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'noise', 'time series']"
No,"Algorithmic fairness, adversarial attack",Poisoning attacks on algorithmic fairness,40.0,"D Solans, B Biggio, C Castillo",2020.0,… Conference on Machine Learning and …,Springer,https://link.springer.com/chapter/10.1007/978-3-030-67658-2_10,https://scholar.google.com/scholar?cites=13249061336161543270&as_sdt=2005&sciodt=2007&hl=en,176.0,2022-07-14 11:14:51,,10.1007/978-3-030-67658-2_10,,,,,,,40,20.0,13.0,3.0,2.0,… Studying adversarial attacks on algorithmic fairness can help to make machine learning systems more robust. Additional type of models such neural nets and/or other data sets can be …,https://arxiv.org/pdf/2004.07401,https://scholar.google.com/scholar?q=related:ZmRFtRYf3rcJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22accountability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'fairness']"
Yes,"Deep Learning, Computer Vision, Robust",FoolChecker: A platform to evaluate the robustness of images against adversarial attacks,4.0,"L Hui, Z Bo, H Linquan, G Jiabao, L Yifan",2020.0,Neurocomputing,Elsevier,https://www.sciencedirect.com/science/article/pii/S0925231220309073,https://scholar.google.com/scholar?cites=10758449391523689897&as_sdt=2005&sciodt=2007&hl=en,176.0,2022-07-13 13:05:38,,,,,,,,,4,2.0,1.0,5.0,2.0,"… examples and the adversarial ones to quantify the robustness against adversarial attacks. Firstly, … [17] proposed a simple modification to standard neural network architectures and …",,https://scholar.google.com/scholar?q=related:qXXJi7iwTZUJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'deep learning', 'robust']"
Yes,"Audio Recognition, Noise, Adversarial Perturbations",Robustness of Neural Architectures for Audio Event Detection,0.0,"JB Li, S Qu, F Metze",2022.0,arXiv preprint arXiv:2205.03268,arxiv.org,https://arxiv.org/abs/2205.03268,,177.0,2022-07-13 14:41:59,,,,,,,,,0,0.0,0.0,3.0,1.0,"… robustness) when performing audio classification. Some recent work in the vision domain advocated for ViT’s robustness [8], while its robustness … difference of noise robustness between …",https://arxiv.org/pdf/2205.03268,https://scholar.google.com/scholar?q=related:GfeH2xkrrYwJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'noise', 'time series']"
Yes,"Convolutional Neural Network, Lipschitz, Robust",Householder Activations for Provable Robustness against Adversarial Attacks,4.0,"Sahil Singla, Surbhi Singla, S. Feizi",2021.0,,,,,177.0,2022-07-13 09:28:50,,,,,,,,,4,4.0,1.0,3.0,1.0,"Training convolutional neural networks (CNNs) with a strict Lipschitz constraint under the l2 norm is useful for provable adversarial robustness, interpretable gradients and stable training. While 1-Lipschitz CNNs can be designed by enforcing a 1-Lipschitz constraint on each layer, training such networks requires each layer to have an orthogonal Jacobian matrix (for all inputs) to prevent gradients from vanishing during backpropagation. A layer with this property is said to be Gradient Norm Preserving (GNP). To construct expressive GNP activation functions, we first prove that the Jacobian of any GNP piecewise linear function is only allowed to change via Householder (HH) transformations for the function to be continuous. Building on this result, we introduce a class of nonlinear GNP activations with learnable Householder transformations called Householder activations. A householder activation parameterized by the vector v outputs (I − 2vv )z for its input z if v z ≤ 0; otherwise it outputs z. Existing GNP activations such as MaxMin can be viewed as special cases of HH activations for certain settings of these transformations. Thus, networks with HH activations have higher expressive power than those with MaxMin activations. Although networks with HH activations have nontrivial provable robustness against adversarial attacks, we further boost their robustness by (i) introducing a certificate regularization and (ii) relaxing orthogonalization of the last layer of the network. Our experiments on CIFAR-10 and CIFAR-100 show that our regularized networks with HH activations lead to significant improvements in both the standard and provable robust accuracy over the prior works (gain of 3.65% and 4.46% on CIFAR-100 respectively).",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'lipschitz', 'robust']"
Yes,"Adversarial Attack, Deep Learning, Metric",Roby: Evaluating the robustness of a deep model by its decision boundaries,2.0,"J Chen, Z Wang, H Zheng, J Xiao, Z Ming",2020.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2012.10282,https://scholar.google.com/scholar?cites=182840497747837749&as_sdt=2005&sciodt=2007&hl=en,177.0,2022-07-12 11:49:54,,,,,,,,,2,1.0,0.0,5.0,2.0,… artificial intelligence researcher Christian Szegedy [7] calculated the global Lipschitz constant of each layer of the neural network and uses their product to explain the robustness …,https://arxiv.org/pdf/2012.10282,https://scholar.google.com/scholar?q=related:NcPQxnaUiQIJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'deep learning', 'metric']"
Yes,"Generative Adversarial Network, Data Augmentation, Robust, Noise",Data augmentation using generative adversarial networks for robust speech recognition,23.0,"Y Qian, H Hu, T Tan",2019.0,Speech Communication,Elsevier,https://www.sciencedirect.com/science/article/pii/S0167639319300044?casa_token=cOH0vqUXGJYAAAAA:fNw5cEN6yVmW8eb4zef893kfrQYrOUfPKdsQvw-oamSRMeRHqCkyLiKE3SX7l5vNAzhx5kB4SJQ,https://scholar.google.com/scholar?cites=13496096631379288340&as_sdt=2005&sciodt=2007&hl=en,178.0,2022-07-14 12:23:15,HTML,,,,,,,,23,8.07,8.0,3.0,3.0,"… For noise robust speech recognition, data mismatch between training … , in this work we utilize generative adversarial networks (GAN) for data … Very deep convolutional neural network …",https://www.sciencedirect.com/science/article/pii/S0167639319300044?casa_token=cOH0vqUXGJYAAAAA:fNw5cEN6yVmW8eb4zef893kfrQYrOUfPKdsQvw-oamSRMeRHqCkyLiKE3SX7l5vNAzhx5kB4SJQ,https://scholar.google.com/scholar?q=related:FGG1f2HES7sJ:scholar.google.com/&scioq=%22robust%22+%22neural+network%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['data augmentation', 'generative adversarial networks', 'noise', 'robust']"
No,"Robust, Learning",Impact-Learning: A Robust Machine Learning Algorithm,7.0,"Md. Kowsher, A. Tahabilder, S. Murad",2020.0,,,,,178.0,2022-07-13 09:23:05,,10.1145/3411174.3411185,,,,,,,7,3.5,2.0,3.0,2.0,"The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['learning', 'robust']"
Yes,"Adversarial, Robust, Classification",Achieving Generalizable Robustness of Deep Neural Networks by Stability Training,11.0,"Jan Laermann, W. Samek, Nils Strodthoff",2019.0,,,,,178.0,2022-07-13 09:27:51,,10.1007/978-3-030-33676-9_25,,,,,,,11,4.07,4.0,3.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'classification', 'robust']"
No,"Neural Network, Stability",Global Robust Stability Analysis for Hybrid BAM Neural Networks,1.0,"N. M. Thoiyab, P. Muruganantham, N. Gunasekaran",2021.0,,,,,178.0,2022-07-13 09:26:01,,10.1109/CMI50323.2021.9362980,,,,,,,1,1.0,0.0,3.0,1.0,"In this paper, we study some new sufficient criteria on global stability analysis for the hybrid bidirectional associative memory (BAM) neural networks with multiple time delays. The ultimate focus of this paper is to derive some new generalized sufficient criteria for the global asymptotic robust stability (GARS) of equilibrium point of the time-delayed BAM neural networks. The obtained sufficient conditions are always independent on the delay of system parameters of hybrid BAM neural networks. Finally, numerical example has been given to explain the effectiveness of our results in terms of network parameters.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural network', 'stability']"
No,"Explanations, XAI",Quantus: an explainable AI toolkit for responsible evaluation of neural network explanations,5.0,"A Hedström, L Weber, D Bareeva, F Motzkus…",2022.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2202.06861,https://scholar.google.com/scholar?cites=16173639951134967753&as_sdt=2005&sciodt=2007&hl=en,178.0,2022-07-12 13:52:29,,,,,,,,,5,5.0,1.0,5.0,1.0,"… is supposed to strengthen trust in artificial intelligence, it is necessary to systematically review … To increase transparency and reproducibility in the field, we therefore built Quantus — a …",https://arxiv.org/pdf/2202.06861,https://scholar.google.com/scholar?q=related:yUd3DNhPdOAJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22reproducibility%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['explainability']
Yes,"NLP, adversarial robustness, fairness",Does robustness improve fairness? approaching fairness with word substitution robustness methods for text classification,8.0,"Y Pruksachatkun, S Krishna, J Dhamala…",2021.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2106.10826,https://scholar.google.com/scholar?cites=1905293051771980137&as_sdt=2005&sciodt=2007&hl=en,178.0,2022-07-13 11:13:23,,,,,,,,,8,8.0,2.0,4.0,1.0,"… future explorations in using robustness methods to not only … and robustness, two important aspects of creating trustworthy … robustness of fair machine learning. ArXiv, abs/2006.08669. …",https://arxiv.org/pdf/2106.10826,https://scholar.google.com/scholar?q=related:aYnjl-_1cBoJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22trustworthy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'fairness', 'nlp']"
No,"Adversarial Attacks, Robust, Generalization",Generalizability vs. Robustness: Adversarial Examples for Medical Imaging,15.0,"Magdalini Paschali, Sailesh Conjeti, Fernando Navarro, Nassir Navab",2018.0,,,,,178.0,2022-07-13 09:22:57,,,,,,,,,15,4.15,4.0,4.0,4.0,"In this paper, for the first time, we propose an evaluation method for deep learning models that assesses the performance of a model not only in an unseen test scenario, but also in extreme cases of noise, outliers and ambiguous input data. To this end, we utilize adversarial examples, images that fool machine learning models, while looking imperceptibly different from original data, as a measure to evaluate the robustness of a variety of medical imaging models. Through extensive experiments on skin lesion classification and whole brain segmentation with state-of-the-art networks such as Inception and UNet, we show that models that achieve comparable performance regarding generalizability may have significant variations in their perception of the underlying data manifold, leading to an extensive performance gap in their robustness.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'generalization', 'robust']"
No,"Malware, Autoencoder, Adversarial Attacks",HashTran-DNN: A Framework for Enhancing Robustness of Deep Neural Networks against Adversarial Malware Samples,12.0,"Deqiang Li, Ramesh Baral, Tao Li, Han Wang, Qianmu Li, Shouhuai Xu",2018.0,,,,,179.0,2022-07-13 09:22:57,,,,,,,,,12,3.0,2.0,6.0,4.0,"Adversarial machine learning in the context of image processing and related applications has received a large amount of attention. However, adversarial machine learning, especially adversarial deep learning, in the context of malware detection has received much less attention despite its apparent importance. In this paper, we present a framework for enhancing the robustness of Deep Neural Networks (DNNs) against adversarial malware samples, dubbed Hashing Transformation Deep Neural Networks} (HashTran-DNN). The core idea is to use hash functions with a certain locality-preserving property to transform samples to enhance the robustness of DNNs in malware classification. The framework further uses a Denoising Auto-Encoder (DAE) regularizer to reconstruct the hash representations of samples, making the resulting DNN classifiers capable of attaining the locality information in the latent space. We experiment with two concrete instantiations of the HashTran-DNN framework to classify Android malware. Experimental results show that four known attacks can render standard DNNs useless in classifying Android malware, that known defenses can at most defend three of the four attacks, and that HashTran-DNN can effectively defend against all of the four attacks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'autoencoder', 'cybersecurity']"
Yes,"Speech Detection, Deep Neural Network, Robust",A Joint Training Framework for Robust Automatic Speech Recognition,98.0,"Zhong-Qiu Wang, Deliang Wang",2016.0,,,,,179.0,2022-07-13 09:29:07,,10.1109/TASLP.2016.2528171,,,,,,,98,16.33,49.0,2.0,6.0,"Robustness against noise and reverberation is critical for ASR systems deployed in real-world environments. In robust ASR, corrupted speech is normally enhanced using speech separation or enhancement algorithms before recognition. This paper presents a novel joint training framework for speech separation and recognition. The key idea is to concatenate a deep neural network (DNN) based speech separation frontend and a DNN-based acoustic model to build a larger neural network, and jointly adjust the weights in each module. This way, the separation frontend is able to provide enhanced speech desired by the acoustic model and the acoustic model can guide the separation frontend to produce more discriminative enhancement. In addition, we apply sequence training to the jointly trained DNN so that the linguistic information contained in the acoustic and language models can be back-propagated to influence the separation frontend at the training stage. To further improve the robustness, we add more noise- and reverberation-robust features for acoustic modeling. At the test stage, utterance-level unsupervised adaptation is performed to adapt the jointly trained network by learning a linear transformation of the input of the separation frontend. The resulting sequence-discriminative jointly-trained multistream system with run-time adaptation achieves 10.63% average word error rate (WER) on the test set of the reverberant and noisy CHiME-2 dataset (task-2), which represents the best performance on this dataset and a 22.75% error reduction over the best existing method.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'robust', 'time series']"
Yes,"Robust, Deep Neural Network",A causal view on robustness of neural networks,32.0,"C Zhang, K Zhang, Y Li",2020.0,Advances in Neural Information …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2020/hash/02ed812220b0705fabb868ddbf17ea20-Abstract.html,https://scholar.google.com/scholar?cites=16034502372142635207&as_sdt=2005&sciodt=2007&hl=en,179.0,2022-07-12 11:49:54,,,,,,,,,32,16.0,11.0,3.0,2.0,"… We present a causal view on the robustness of neural networks against input manipulations… , our proposed model shows superior robustness against unseen manipulations. As a by-…",https://proceedings.neurips.cc/paper/2020/file/02ed812220b0705fabb868ddbf17ea20-Paper.pdf,https://scholar.google.com/scholar?q=related:x1i4-fP-hd4J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'robust']"
Yes,adversarial robustness,Towards robust image classification using sequential attention models,21.0,D. Zoran,2020.0,Proceedings of the IEEE Computer Society Conference on Computer Vision and Pattern Recognition,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85093667689&origin=inward,179.0,2022-07-12 16:34:58,Conference Paper,10.1109/CVPR42600.2020.00950,1063-6919,https://api.elsevier.com/content/abstract/scopus_id/85093667689,,,9480.0,9489.0,21,10.5,21.0,1.0,2.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['adversarial robustness']
No,adversarial robustness,Towards Deploying Robust Machine Learning Systems,0.0,L Tong,2021.0,,search.proquest.com,https://search.proquest.com/openview/6e83665bc52e1cc4fe437f2643b286d6/1?pq-origsite=gscholar&cbl=18750&diss=y,,180.0,2022-07-14 11:40:34,,,,,,,,,0,0.0,0.0,1.0,1.0,"Abstract Machine learning (ML) has come to be widely used in a broad array of settings, including important security applications such as network intrusion, fraud, and malware detection…",,https://scholar.google.com/scholar?q=related:NdFqV2v4dZ8J:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['adversarial robustness']
No,"survey, human-in-the-loop, explainability",The Role of Human Knowledge in Explainable AI,0.0,"A Tocchetti, M Brambilla",2022.0,Data,mdpi.com,https://www.mdpi.com/1714208,,180.0,2022-07-13 09:37:32,,,,,,,,,0,0.0,0.0,2.0,1.0,"… and complexity of machine learning models have grown … the trustworthiness and robustness of AI systems. While [47… 24], etc.—through a human computation game named “Peek-a-boom…",https://www.mdpi.com/2306-5729/7/7/93/pdf?version=1657188498,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'hci', 'survey']"
Yes,"Adversarial, Quantization, Robust, Training",A Layer-wise Adversarial-aware Quantization Optimization for Improving Robustness,0.0,"C Song, R Ranjan, H Li",2021.0,arXiv preprint arXiv:2110.12308,arxiv.org,https://arxiv.org/abs/2110.12308,,180.0,2022-07-13 14:14:52,,,,,,,,,0,0.0,0.0,3.0,1.0,"… neural network robustness, our major contributions include: (1) We find that with careful quantization setting selection, the robustness … the models’ resilience to adversarial attacks. The …",https://arxiv.org/pdf/2110.12308,https://scholar.google.com/scholar?q=related:sPv3nRH1kW8J:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22resilience%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'quantization', 'robust', 'training']"
Yes,"Projected Gradient Descent, Training, Robust",Improving the affordability of robustness training for DNNs,11.0,"S Gupta, P Dube, A Verma",2020.0,… of the IEEE/CVF conference on …,openaccess.thecvf.com,http://openaccess.thecvf.com/content_CVPRW_2020/html/w47/Gupta_Improving_the_Affordability_of_Robustness_Training_for_DNNs_CVPRW_2020_paper.html,https://scholar.google.com/scholar?cites=5904264191928535633&as_sdt=2005&sciodt=2007&hl=en,180.0,2022-07-13 10:35:21,,,,,,,,,11,5.5,4.0,3.0,2.0,"… In fact, our robustness accuracy is mostly higher as compared to that of regular adversarial training due to better generalization on adversarial samples as discussed in Section 3. …",http://openaccess.thecvf.com/content_CVPRW_2020/papers/w47/Gupta_Improving_the_Affordability_of_Robustness_Training_for_DNNs_CVPRW_2020_paper.pdf,https://scholar.google.com/scholar?q=related:UYr1GwAp8FEJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['projected gradient descent', 'robust', 'training']"
Yes,"Classification, Noise, Adversarial Perturbations",Improving the robustness of neural networks using k-support norm based adversarial training,9.0,"SW Akhtar, S Rehman, M Akhtar, MA Khan…",2016.0,IEEE …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/7795200/,https://scholar.google.com/scholar?cites=16456628132239475842&as_sdt=2005&sciodt=2007&hl=en,181.0,2022-07-13 14:41:59,,,,,,,,,9,1.5,2.0,5.0,6.0,… an engineered noise model such as adversarial noise to improve robustness of neural network or we can achieve the same or better robustness using uniform random noise (URN)? To …,https://ieeexplore.ieee.org/iel7/6287639/6514899/07795200.pdf,https://scholar.google.com/scholar?q=related:goTVZhiwYeQJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22noise%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'classification', 'noise']"
Yes,"Data Poisoning, Clustering, Robust",Certified robustness of nearest neighbors against data poisoning attacks,13.0,"J Jia, X Cao, NZ Gong",2020.0,arXiv preprint arXiv:2012.03765,arxiv.org,https://arxiv.org/abs/2012.03765,https://scholar.google.com/scholar?cites=11022552774848914674&as_sdt=2005&sciodt=2007&hl=en,181.0,2022-07-13 10:35:21,,,,,,,,,13,6.5,4.0,3.0,2.0,"… to corrupt a machine learning classifier via … accuracy under no attacks (ie, CA(0)) and robustness against attacks. Therefore, we set their parameters such that they have similar accuracy …",https://arxiv.org/pdf/2012.03765,https://scholar.google.com/scholar?q=related:8qTdRln595gJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['clustering', 'data poisoning', 'robust']"
No,"Classification, Support Vector Machine, Robust",Multiclass Capped ℓp-Norm SVM for Robust Classifications,45.0,"F. Nie, Xiaoqian Wang, Heng Huang",2017.0,,,,,182.0,2022-07-13 10:05:55,,10.1609/aaai.v31i1.10948,,,,,,,45,9.0,15.0,3.0,5.0,"    Support vector machine (SVM) model is one of most successful machine learning methods and has been successfully applied to solve numerous real-world application. Because the SVM methods use the hinge loss or squared hinge loss functions for classifications, they usually outperform other classification approaches, e.g. the least square loss function based methods. However, like most supervised learning algorithms, they learn classifiers based on the labeled data in training set without specific strategy to deal with the noise data. In many real-world applications, we often have data outliers in train set, which could misguide the classifiers learning, such that the classification performance is suboptimal. To address this problem, we proposed a novel capped Lp-norm SVM classification model by utilizing the capped `p-norm based hinge loss in the objective which can deal with both light and heavy outliers. We utilize the new formulation to naturally build the multiclass capped Lp-norm SVM. More importantly, we derive a novel optimization algorithms to efficiently minimize the capped Lp-norm based objectives, and also rigorously prove the convergence of proposed algorithms. We present experimental results showing that employing the new capped Lp-norm SVM method can consistently improve the classification performance, especially in the cases when the data noise level increases.   ",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'robust', 'support vector machine']"
Yes,"Adversarial Attacks, Metric, Graph Neural Network",Robustness of Graph Neural Networks at Scale,8.0,"Simon Geisler, Tobias Schmidt, Hakan cSirin, D. Zugner, Aleksandar Bojchevski, Stephan Gunnemann",2021.0,,,,,182.0,2022-07-13 09:28:33,,,,,,,,,8,8.0,1.0,6.0,1.0,"Graph Neural Networks (GNNs) are increasingly important given their popularity and the diversity of applications. Yet, existing studies of their vulnerability to adversarial attacks rely on relatively small graphs. We address this gap and study how to attack and defend GNNs at scale. We propose two sparsity-aware first-order optimization attacks that maintain an efficient representation despite optimizing over a number of parameters which is quadratic in the number of nodes. We show that common surrogate losses are not well-suited for global attacks on GNNs. Our alternatives can double the attack strength. Moreover, to improve GNNs’ reliability we design a robust aggregation function, Soft Median, resulting in an effective defense at all scales. We evaluate our attacks and defense with standard GNNs on graphs more than 100 times larger compared to previous work. We even scale one order of magnitude further by extending our techniques to a scalable GNN.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'graph neural network', 'metric']"
No,"convolutional neural network, robust",Exploring CNN features in the context of adversarial robustness and human perception,0.0,E. Casamassima,2021.0,Proceedings of SPIE - The International Society for Optical Engineering,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85118247527&origin=inward,182.0,2022-07-12 16:26:44,Conference Paper,10.1117/12.2594363,0277-786X,https://api.elsevier.com/content/abstract/scopus_id/85118247527,11843.0,,,,0,0.0,0.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'robust']"
Yes,"robustness to changes of hyperparameters, framework",On Robustness: An Undervalued Dimension of Human Rationality.,11.0,"AS Nobandegani, K da Silva Castanheira…",2019.0,…,iccm-conference.neocities.org,https://iccm-conference.neocities.org/2019/proceedings/papers/ICCM2019_paper_21.pdf,https://scholar.google.com/scholar?cites=17307040301284572457&as_sdt=2005&sciodt=2007&hl=en,183.0,2022-07-13 08:53:52,PDF,,,,,,,,11,4.07,4.0,3.0,3.0,"… A generalized approach to portfolio optimization: Improving performance by constraining portfolio norms … Kitano, H. (2004). Biological robustness … Artificial Intelligence, 94(1-2), 57–77 …",https://iccm-conference.neocities.org/2019/proceedings/papers/ICCM2019_paper_21.pdf,https://scholar.google.com/scholar?q=related:Kb1vF3X1LvAJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22performance%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['framework', 'hyperparameters']"
Yes,"robust, adversarial, training, deep neural network",Fast Training of Deep Neural Networks Robust to Adversarial Perturbations,0.0,"Justin A. Goodwin, Olivia M. Brown, Victoria Helus",2020.0,,,,,183.0,2022-07-13 09:26:01,,10.1109/HPEC43674.2020.9286256,,,,,,,0,0.0,0.0,3.0,2.0,"Despite their promising performance, deep neural networks have shown sensitivities to perturbations of their inputs (e.g., adversarial examples) and their learned feature representations are often difficult to interpret, raising concerns about their true capability and trustworthiness. Recent work in adversarial training, a form of robust optimization in which the model is optimized against adversarial examples, demonstrates the ability to improve performance sensitivities to perturbations and yield feature representations that are more interpretable. Adversarial training, however, comes with an increased computational cost over that of standard (i.e., nonrobust) training, rendering it impractical for use in large-scale problems. Recent work suggests that a fast approximation to adversarial training shows promise for reducing training time and maintaining robustness in the presence of perturbations bounded by the infinity norm. In this work, we demonstrate that this approach extends to the Euclidean norm and preserves the human-aligned feature representations that are common for robust models. Additionally, we show that using a distributed training scheme can further reduce the time to train robust deep networks. Fast adversarial training is a promising approach that will provide increased security and explainability in machine learning applications for which robust optimization was previously thought to be impractical.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'deep learning', 'robust', 'training']"
Yes,Robustness evaluation framework,Robustbench: a standardized adversarial robustness benchmark,128.0,"F Croce, M Andriushchenko, V Sehwag…",2020.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2010.09670,https://scholar.google.com/scholar?cites=2257115641228924434&as_sdt=2005&sciodt=2007&hl=en,183.0,2022-07-12 11:56:54,,,,,,,,,128,64.0,32.0,4.0,2.0,… We argue that benchmarking adversarial robustness in a standardized way requires some restrictions on the type of considered models. The goal of these restrictions is to prevent …,https://arxiv.org/pdf/2010.09670,https://scholar.google.com/scholar?q=related:EpZtjLfiUh8J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['evaluation', 'framework']"
No,"Metric, Representation Learning, Causality",Robustly disentangled causal mechanisms: Validating deep representations for interventional robustness,80.0,"R Suter, D Miladinovic, B Schölkopf…",2019.0,… on Machine Learning,proceedings.mlr.press,http://proceedings.mlr.press/v97/suter19a.html,https://scholar.google.com/scholar?cites=10781334169107419114&as_sdt=2005&sciodt=2007&hl=en,183.0,2022-07-13 10:25:08,,,,,,,,,80,27.07.00,20.0,4.0,3.0,… In the column i⇤ we then plot the estimate of E[Zl|gi⇤ ] together with its confidence bound in order to visualize the informativeness of Zl about Gi⇤ . For example the upper row in plot 4 …,http://proceedings.mlr.press/v97/suter19a/suter19a.pdf,https://scholar.google.com/scholar?q=related:6s9tCk7-npUJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['causality', 'metric', 'representation learning']"
Yes,"Representation Learning, Dropout, Convolutional Neural Network",Informative Dropout for Robust Representation Learning: A Shape-bias Perspective,32.0,"Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, Jingdong Wang",2020.0,,,,,184.0,2022-07-13 09:40:40,,,,,,,,,32,16.0,5.0,6.0,2.0,"Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at this https URL.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['convolutional neural network', 'dropout', 'representation learning']"
Yes,"Object Detection, Adversarial Perturbations, Convolutional Neural Network",Robust Physical Adversarial Attack on Faster R-CNN Object Detector,223.0,"Shang-Tse Chen, Cory Cornelius, Jason Martin, Duen Horng Chau",2018.0,,,,,185.0,2022-07-13 09:26:34,,10.1007/978-3-030-10925-7_4,,,,,,,223,56.15.00,56.0,4.0,4.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'computer vision', 'convolutional neural network']"
Yes,"Adversarial, Robust, Generalization",Implicit Euler Skip Connections: Enhancing Adversarial Robustness via Numerical Stability,12.0,"Mingjie Li, Lingshen He, Zhouchen Lin",2020.0,,,,,185.0,2022-07-13 09:27:51,,,,,,,,,12,6.0,4.0,3.0,2.0,"Moosavi-Dezfooli et al., 2016; Szegedy et al., 2013), i.e., Deep neural networks have achieved great success in various areas, but recent works have found that neural networks are vulnerable to adversarial attacks, which leads to a hot topic nowadays. Although many approaches have been proposed to enhance the robustness of neural networks, few of them explored robust architectures for neural networks. On this account, we try to address such an issue from the perspective of dynamic system in this work. By viewing ResNet as an explicit Euler discretization of an ordinary differential equation (ODE), for the frst time, we fnd that the adversarial robustness of ResNet is connected to the numerical stability of the corresponding dynamic system, i.e., more stable numerical schemes may correspond to more robust deep networks. Furthermore, inspired by the implicit Euler method for solving numerical ODE problems, we propose Implicit Euler skip connections (IE-Skips) by modifying the original skip connection in ResNet or its variants. Then we theoretically prove its advantages under the adversarial attack and the experimental results show that our ResNet with IE-Skips can largely improve the robustness and the generalization ability under adversarial attacks when compared with the vanilla ResNet of the same parameter size.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'generalization', 'robust']"
No,"robustness to distribution shifts, domain shifts, machine translation, NLP",Domain robustness in neural machine translation,51.0,"M Müller, A Rios, R Sennrich",2019.0,arXiv preprint arXiv:1911.03109,arxiv.org,https://arxiv.org/abs/1911.03109,https://scholar.google.com/scholar?cites=9040239527995912262&as_sdt=2005&sciodt=2007&hl=en,187.0,2022-07-13 14:59:54,,,,,,,,,51,17.0,17.0,3.0,3.0,"… We consider domain robustness a desirable property of NLP systems, along with other … robustness. Defensive distillation exploits knowledge distillation to fend off adversarial attacks. …",https://arxiv.org/pdf/1911.03109,https://scholar.google.com/scholar?q=related:RjCkFvRhdX0J:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['distribution shift', 'nlp']"
No ,"Object Detection, Robust, Support Vector Machine",Improving object classification robustness in RGB-D using adaptive SVMs,4.0,"JR Nuricumbo, H Ali, ZC Márton…",2016.0,Multimedia Tools and …,Springer,https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11042-015-2612-7&casa_token=nFqLX__xRzsAAAAA:QMLuPvvWAZ3F_2cckb7jr7czWOV-bobXZY5w0qx0G6LFGB_U2LLehwmZn1iEUnzL3OFpqcFGUYSysueP2g,https://scholar.google.com/scholar?cites=33538781075553894&as_sdt=2005&sciodt=2007&hl=en,187.0,2022-07-13 11:38:03,HTML,10.1007/s11042-015-2612-7,,,,,,,4,1.07,1.0,4.0,6.0,"… Our approach first trains a classifier on RGB-D data from a source domain by using feature extractors and a supervised machine learning algorithm. Then, the classifier adaptation …",https://idp.springer.com/authorize/casa?redirect_uri=https://link.springer.com/article/10.1007/s11042-015-2612-7&casa_token=nFqLX__xRzsAAAAA:QMLuPvvWAZ3F_2cckb7jr7czWOV-bobXZY5w0qx0G6LFGB_U2LLehwmZn1iEUnzL3OFpqcFGUYSysueP2g,https://scholar.google.com/scholar?q=related:Zj41_FcndwAJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22human+interpretation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'robust', 'support vector machine']"
Yes,"robust, adversarial",Impact of Spatial Frequency Based Constraints on Adversarial Robustness,0.0,R. Bernhard,2021.0,Proceedings of the International Joint Conference on Neural Networks,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85116414682&origin=inward,187.0,2022-07-12 16:29:20,Conference Paper,10.1109/IJCNN52387.2021.9534307,,https://api.elsevier.com/content/abstract/scopus_id/85116414682,2021.0,,,,0,0.0,0.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
No,"semi-supervised learning, robustness to noise labels, mitigation method",Noise-robust semi-supervised learning by large-scale sparse coding,19.0,"Z Lu, X Gao, L Wang, JR Wen, S Huang",2015.0,… on Artificial Intelligence,aaai.org,https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewPaper/9366,https://scholar.google.com/scholar?cites=3013400039281747305&as_sdt=2005&sciodt=2007&hl=en,188.0,2022-07-13 15:31:15,,,,,,,,,19,3.11,4.0,5.0,7.0,"… In this paper, we focus on developing a novel noiserobust semi-supervised learning algorithm to deal with the challenging problem of semi-supervised learning with noisy initial labels. …",https://www.aaai.org/ocs/index.php/AAAI/AAAI15/paper/viewPDFInterstitial/9366/9948,https://scholar.google.com/scholar?q=related:aWWJu16_0SkJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['label noise', 'mitigation method', 'semi-supervised learning']"
Yes,"Classification, Label Uncertainty, Intrinsic Robustness",Incorporating Label Uncertainty in Understanding Adversarial Robustness,1.0,"X Zhang, D Evans",2021.0,arXiv preprint arXiv:2107.03250,arxiv.org,https://arxiv.org/abs/2107.03250,https://scholar.google.com/scholar?cites=6165783556697967589&as_sdt=2005&sciodt=2007&hl=en,188.0,2022-07-13 09:47:56,,,,,,,,,1,1.0,1.0,2.0,1.0,… A fundamental question in adversarial machine learning is whether a robust classifier exists for a given task. A line of research has made progress towards this goal by studying …,https://arxiv.org/pdf/2107.03250,https://scholar.google.com/scholar?q=related:5UvtInpDkVUJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['classification', 'robust', 'uncertainty']"
Yes,"trustworthy, metric",Trustworthy explainability acceptance: A new metric to measure the trustworthiness of interpretable AI medical diagnostic systems,5.0,"D Kaur, S Uslu, A Durresi, S Badve…",2021.0,Conference on Complex …,Springer,https://link.springer.com/chapter/10.1007/978-3-030-79725-6_4,https://scholar.google.com/scholar?cites=1075970497145752793&as_sdt=2005&sciodt=2007&hl=en,188.0,2022-07-13 15:27:39,,10.1007/978-3-030-79725-6_4,,,,,,,5,5.0,1.0,5.0,1.0,"We propose, Trustworthy Explainability Acceptance metric to evaluate explainable AI systems using expert-in-the-loop. Our metric calculates acceptance by quantifying the distance …",https://cs.iupui.edu/~adurresi/papers/W_Explainability_Metrics_Final.pdf,https://scholar.google.com/scholar?q=related:2UwsZ3ed7g4J:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22explainability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['metric', 'trustworthy']"
No,"Perturbations, Adversarial Attacks, Robust",On 1/n neural representation and robustness,7.0,"J Nassar, P Sokol, SY Chung…",2020.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2020/hash/44bf89b63173d40fb39f9842e308b3f9-Abstract.html,https://scholar.google.com/scholar?cites=14612770369819484609&as_sdt=2005&sciodt=2007&hl=en,188.0,2022-07-13 09:30:22,,,,,,,,,7,3.5,2.0,4.0,2.0,"… shared by neuroscience and machine learning. It is therefore … both their generalization, and robustness to perturbations. In this work… explanation for why batch normalization reduces the …",https://proceedings.neurips.cc/paper/2020/file/44bf89b63173d40fb39f9842e308b3f9-Paper.pdf,https://scholar.google.com/scholar?q=related:waFxxTr9ysoJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'perturbations', 'robust']"
Yes,"Training, Regularization, Robust",A Consistency Regularization for Certified Robust Neural Networks,0.0,"M Xu, T Zhang, Z Li, D Zhang",2021.0,… Conference on Artificial Intelligence,Springer,https://link.springer.com/chapter/10.1007/978-3-030-93049-3_3,,189.0,2022-07-14 08:45:17,,10.1007/978-3-030-93049-3_3,,,,,,,0,0.0,0.0,4.0,1.0,"… of robust constraint and original accuracy … accuracy as well as certified robustness of the network. However, for misclassified examples, the constraints on robustness and accuracy are …",,https://scholar.google.com/scholar?q=related:470AnYgltUIJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['regularization', 'robust', 'training']"
Yes,"adversarial robustness, mitigation method",Distributionally Robust Deep Learning as a Generalization of Adversarial Training,33.0,M. Staib,2017.0,,,,,189.0,2022-07-13 09:22:57,,,,,,,,,33,6.6,33.0,1.0,5.0,"Machine learning models are vulnerable to adversarial attacks at test time: a correctly classified test example can be slightly perturbed to cause a misclassification. Training models that are robust to these attacks, and theoretical understanding of such defenses are active research areas. Adversarial Training (AT) via robust optimization is a promising approach, where the model is trained against an adversary acting on the training set, but it is less clear how to reason about perturbations on the unseen test set. Distributionally Robust Optimization (DRO) with Wasserstein distance is an interesting theoretical tool for understanding robustness and generalization, but it has been limited algorithmically to simple models. We link DRO and AT both theoretically and algorithmically: AT is a special case of DRO, and in general DRO yields a stronger adversary. We also give an algorithm for DRO for neural networks that is no more expensive than AT.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'mitigation method']"
No,"federated learning, robustness to distribution shifts",Towards causal federated learning for enhanced robustness and privacy,6.0,"S Francis, I Tenison, I Rish",2021.0,arXiv preprint arXiv:2104.06557,arxiv.org,https://arxiv.org/abs/2104.06557,https://scholar.google.com/scholar?cites=1121113831662884253&as_sdt=2005&sciodt=2007&hl=en,190.0,2022-07-13 10:25:08,,,,,,,,,6,6.0,2.0,3.0,1.0,Federated Learning is an emerging privacy-preserving distributed machine learning approach to building a shared model by performing distributed training locally on participating …,https://arxiv.org/pdf/2104.06557,https://scholar.google.com/scholar?q=related:nRULCRf_jg8J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['distribution shift', 'federated learning']"
No,"Robust, Metric",An Explainable Adversarial Robustness Metric for Deep Learning Neural Networks,2.0,"C Agarwal, B Dong, D Schonfeld, A Hoogs",2018.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/1806.01477,https://scholar.google.com/scholar?cites=17874415709515414219&as_sdt=2005&sciodt=2007&hl=en,191.0,2022-07-13 09:30:22,,,,,,,,,2,0.5,1.0,4.0,4.0,… on the proposed robustness metric score by providing an extensive explanation to understand it’s role … metric that measures the resiliency of machine learning algorithms to adversarial …,https://arxiv.org/pdf/1806.01477,https://scholar.google.com/scholar?q=related:y_pTlWKuDvgJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['metric', 'robust']"
Yes,"Noise, Self-supervision, Semantic Segmentation",Improved Noise and Attack Robustness for Semantic Segmentation by Using Multi-Task Training with Self-Supervised Depth Estimation,17.0,"Marvin Klingner, Andreas Bär, T. Fingscheidt",2020.0,,,,,191.0,2022-07-13 09:26:34,,10.1109/CVPRW50498.2020.00168,,,,,,,17,8.5,6.0,3.0,2.0,"While current approaches for neural network training often aim at improving performance, less focus is put on training methods aiming at robustness towards varying noise conditions or directed attacks by adversarial examples. In this paper, we propose to improve robustness by a multi-task training, which extends supervised semantic segmentation by a self-supervised monocular depth estimation on unlabeled videos. This additional task is only performed during training to improve the semantic segmentation model’s robustness at test time under several input perturbations. Moreover, we even find that our joint training approach also improves the performance of the model on the original (supervised) semantic segmentation task. Our evaluation exhibits a particular novelty in that it allows to mutually compare the effect of input noises and adversarial attacks on the robustness of the semantic segmentation. We show the effectiveness of our method on the Cityscapes dataset, where our multi-task training approach consistently outperforms the single-task semantic segmentation baseline in terms of both robustness vs. noise and in terms of adversarial attacks, without the need for depth labels in training.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'noise', 'self-supervision']"
Yes,"Graph Neural Network, Perturbations, Robust",Graph Structure Learning for Robust Graph Neural Networks,154.0,"Wei Jin, Yao Ma, Xiaorui Liu, Xianfeng Tang, Suhang Wang, Jiliang Tang",2020.0,,,,,191.0,2022-07-13 09:29:15,,10.1145/3394486.3403049,,,,,,,154,77.0,26.0,6.0,2.0,"Graph Neural Networks (GNNs) are powerful tools in representation learning for graphs. However, recent studies show that GNNs are vulnerable to carefully-crafted perturbations, called adversarial attacks. Adversarial attacks can easily fool GNNs in making predictions for downstream tasks. The vulnerability to adversarial attacks has raised increasing concerns for applying GNNs in safety-critical applications. Therefore, developing robust algorithms to defend adversarial attacks is of great significance. A natural idea to defend adversarial attacks is to clean the perturbed graph. It is evident that real-world graphs share some intrinsic properties. For example, many real-world graphs are low-rank and sparse, and the features of two adjacent nodes tend to be similar. In fact, we find that adversarial attacks are likely to violate these graph properties. Therefore, in this paper, we explore these properties to defend adversarial attacks on graphs. In particular, we propose a general framework Pro-GNN, which can jointly learn a structural graph and a robust graph neural network model from the perturbed graph guided by these properties. Extensive experiments on real-world graphs demonstrate that the proposed framework achieves significantly better performance compared with the state-of-the-art defense methods, even when the graph is heavily perturbed. We release the implementation of Pro-GNN to our DeepRobust repository for adversarial attacks and defenses. The specific experimental settings to reproduce our results can be found in https://github.com/ChandlerBang/Pro-GNN.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['graph neural network', 'perturbations', 'robust']"
No,"Computer Vision, Reconstruction, Perturbations",Improving Robustness of Deep-Learning-Based Image Reconstruction,17.0,"Ankit Raj, Y. Bresler, Bo Li",2020.0,,,,,192.0,2022-07-13 09:26:34,,,,,,,,,17,8.5,6.0,3.0,2.0,"Deep-learning-based methods for different applications have been shown vulnerable to adversarial examples. These examples make deployment of such models in safety-critical tasks questionable. Use of deep neural networks as inverse problem solvers has generated much excitement for medical imaging including CT and MRI, but recently a similar vulnerability has also been demonstrated for these tasks. We show that for such inverse problem solvers, one should analyze and study the effect of adversaries in the measurement-space, instead of the signal-space as in previous work. In this paper, we propose to modify the training strategy of end-to-end deep-learning-based inverse problem solvers to improve robustness. We introduce an auxiliary network to generate adversarial examples, which is used in a min-max formulation to build robust image reconstruction networks. Theoretically, we show for a linear reconstruction scheme the min-max formulation results in a singular-value(s) filter regularized solution, which suppresses the effect of adversarial examples occurring because of ill-conditioning in the measurement matrix. We find that a linear network using the proposed min-max learning scheme indeed converges to the same solution. In addition, for non-linear Compressed Sensing (CS) reconstruction using deep networks, we show significant improvement in robustness using the proposed approach over other methods. We complement the theory by experiments for CS on two different datasets and evaluate the effect of increasing perturbations on trained networks. We find the behavior for ill-conditioned and well-conditioned measurement matrices to be qualitatively different.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'perturbations', 'reconstruction']"
Yes,"Deep Neural Network, Robust, Adversarial, Generalization",Distillation as a Defense to Adversarial Perturbations Against Deep Neural Networks,2131.0,"Nicolas Papernot, P. Mcdaniel, Xi Wu, S. Jha, A. Swami",2015.0,,,,,192.0,2022-07-13 09:22:57,,10.1109/SP.2016.41,,,,,,,2131,304.43.00,426.0,5.0,7.0,"Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content filters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95% to less than 0.5% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also find that distillation increases the average minimum number of features that need to be modified to create adversarial samples by about 800% on one of the DNNs we tested.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'deep learning', 'generalization', 'robust']"
No,"Attention, Autoencoder, Speech Recognition",Jointly Adversarial Enhancement Training for Robust End-to-End Speech Recognition.,16.0,"B Liu, S Nie, S Liang, W Liu, M Yu, L Chen, S Peng…",2019.0,Interspeech,isca-speech.org,https://www.isca-speech.org/archive_v0/Interspeech_2019/pdfs/1242.pdf,https://scholar.google.com/scholar?cites=370093622846303787&as_sdt=2005&sciodt=2007&hl=en,192.0,2022-07-13 17:19:08,,,,,,,,,16,5.33,2.0,8.0,3.0,"… In this paper, we propose a jointly adversarial enhancement training to boost robustness of … and adversarial loss, the compositional scheme is expected to learn more robust …",,https://scholar.google.com/scholar?q=related:Ky5kazDWIgUJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['attention', 'autoencoder', 'time series']"
Yes,"Visual Question Answering, Explanations, Perturbations",Robust explanations for visual question answering,15.0,"B Patro, S Patel, V Namboodiri",2020.0,Proceedings of the IEEE …,openaccess.thecvf.com,http://openaccess.thecvf.com/content_WACV_2020/html/Patro_Robust_Explanations_for_Visual_Question_Answering_WACV_2020_paper.html,https://scholar.google.com/scholar?cites=13315968884160177520&as_sdt=2005&sciodt=2007&hl=en,192.0,2022-07-13 15:31:15,,,,,,,,,15,7.5,5.0,3.0,2.0,"… In this paper, we propose a method to obtain robust explanations for visual question answering(VQA) that correlate well with the answers. Our model explains the answers obtained …",https://openaccess.thecvf.com/content_WACV_2020/papers/Patro_Robust_Explanations_for_Visual_Question_Answering_WACV_2020_paper.pdf,https://scholar.google.com/scholar?q=related:cKWFLCbTy7gJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'perturbations', 'visual question answering']"
Yes,"adversarial robustness, fairness",Towards Fair and Robust Classification,0.0,"H Sun, K Wu, T Wang, WH Wang",2022.0,2022 IEEE 7th European …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9797363/?casa_token=pOGyLAihl9wAAAAA:oWY8PrP_yhABjZRFr2-1iVnlw_auoMq90xaRyjdkew3K8TVyaN7_iXlkPgjUUCC__ROCnab8OQ,,192.0,2022-07-14 11:28:06,,,,,,,,,0,0.0,0.0,4.0,1.0,… of research in trustworthy ML. Two important issues of trustworthy ML are fairness and … Robust machine learning. A considerably large amounts of research on adversarial ML and …,https://ieeexplore.ieee.org/iel7/9797336/9797337/09797363.pdf?casa_token=CnCPM23e43sAAAAA:XQL383__fFzjvOaq3Icx3d-dV4kfewasrOhp7RnBluHjfGmRa2ohs1yxN0-PhOo4mTDbZYn-YQ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'fairness']"
Yes,"backdoor, robust",Exposing Backdoors in Robust Machine Learning Models,15.0,"Ezekiel O. Soremekun, Sakshi Udeshi, Sudipta Chattopadhyay, A. Zeller",2020.0,,,,,193.0,2022-07-13 09:23:05,,,,,,,,,15,7.5,4.0,4.0,2.0,"The introduction of robust optimisation has pushed the state-of-the-art in defending against adversarial attacks. However, the behaviour of such optimisation has not been studied in the light of a fundamentally different class of attacks called backdoors. In this paper, we demonstrate that adversarially robust models are susceptible to backdoor attacks. Subsequently, we observe that backdoors are reflected in the feature representation of such models. Then, this observation is leveraged to detect backdoor-infected models via a detection technique called AEGIS. Specifically, AEGIS uses feature clustering to effectively detect backdoor-infected robust Deep Neural Networks (DNNs).  In our evaluation of major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS effectively detects robust DNNs infected with backdoors. Overall, AEGIS has 97% (70/72) detection accuracy and 0.3% (2/648) false positive rate, for all configurations. Our investigation reveals that salient features of adversarially robust DNNs break the stealthy nature of backdoor attacks.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['cybersecurity', 'robust']"
No,"adversarial robustness, effieciency, mitigation method, neural networks",On pruning adversarially robust neural networks,13.0,"V Sehwag, S Wang, P Mittal, S Jana",2020.0,,,,https://scholar.google.com/scholar?cites=15550021240436828053&as_sdt=2005&sciodt=2007&hl=en,194.0,2022-07-14 10:58:18,CITATION,,,,,,,,13,6.5,3.0,4.0,2.0,,,https://scholar.google.com/scholar?q=related:lScEPu7FzNcJ:scholar.google.com/&scioq=%22robust%22+%22machine+learning%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'efficiency', 'mitigation method', 'neural networks']"
Yes,"Robust, Adversarial",Evaluating the robustness of bayesian neural networks against different types of attacks,3.0,"Y Pang, S Cheng, J Hu, Y Liu",2021.0,arXiv preprint arXiv:2106.09223,arxiv.org,https://arxiv.org/abs/2106.09223,https://scholar.google.com/scholar?cites=2795260331369990544&as_sdt=2005&sciodt=2007&hl=en,194.0,2022-07-13 11:20:18,,,,,,,,,3,3.0,1.0,4.0,1.0,… of adversary’s knowledge. White-box attacks typically acquire the full knowledge of the model… ongoing malicious activities toward the deploy machine learning systems. This leads to the …,https://arxiv.org/pdf/2106.09223,https://scholar.google.com/scholar?q=related:kIkMInbCyiYJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust']"
Yes,"Adversarial Perturbations, Deep Learning, Robust",On Detecting Adversarial Perturbations,700.0,"J. H. Metzen, Tim Genewein, Volker Fischer, B. Bischoff",2017.0,,,,,194.0,2022-07-13 09:38:44,,,,,,,,,700,140.0,175.0,4.0,5.0,"Machine learning and deep learning in particular has advanced tremendously on perceptual tasks in recent years. However, it remains vulnerable against adversarial perturbations of the input that have been crafted specifically to fool the system while being quasi-imperceptible to a human. In this work, we propose to augment deep neural networks with a small ""detector"" subnetwork which is trained on the binary classification task of distinguishing genuine data from data containing adversarial perturbations. Our method is orthogonal to prior work on addressing adversarial perturbations, which has mostly focused on making the classification network itself more robust. We show empirically that adversarial perturbations can be detected surprisingly well even though they are quasi-imperceptible to humans. Moreover, while the detectors have been trained to detect only a specific adversary, they generalize to similar and weaker adversaries. In addition, we propose an adversarial attack that fools both the classifier and the detector and a novel training procedure for the detector that counteracts this attack.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'deep learning', 'robust']"
No,"Robust, Active Learning, Classification, Accuracy",A Novel Active Learning Algorithm for Robust Image Classification,0.0,"X Xiong, M Fan, C Yu, Z Hong",2020.0,IEEE Access,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/8963974/,,194.0,2022-07-14 08:45:17,,,,,,,,,0,0.0,0.0,4.0,2.0,"… Especially, it can show higher classification accuracy under the condition that only a few … Improving the accuracy of image classification is one of the most important and interesting …",https://ieeexplore.ieee.org/iel7/6287639/8948470/08963974.pdf,https://scholar.google.com/scholar?q=related:g0Y0muaC1zQJ:scholar.google.com/&scioq=%22robust%22+%22artificial+intelligence%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'active learning', 'classification', 'robust']"
Yes,"deep neural network, robust, adversarial",Exploring Robustness of Neural Networks through Graph Measures,0.0,"A Waqas, G Rasool, H Farooq…",2021.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2106.15850,,194.0,2022-07-13 09:41:18,,,,,,,,,0,0.0,0.0,4.0,1.0,"… In the machine learning realm, graph structures (ie, neurons and connections) of ANNs … the robustness of various ANNs to adversarial attacks. To this end, we (1) explore the design …",https://arxiv.org/pdf/2106.15850,https://scholar.google.com/scholar?q=related:Vk8xMmvtkDYJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22design%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'deep learning', 'robust']"
No,"Adversarial Attacks, Computer Vision, Deep Learning",Robustness of deep convolutional neural networks for image recognition,28.0,"M Uličný, J Lundström, S Byttner",2016.0,International Symposium on Intelligent …,Springer,https://link.springer.com/chapter/10.1007/978-3-319-30447-2_2,https://scholar.google.com/scholar?cites=3031984762654367347&as_sdt=2005&sciodt=2007&hl=en,195.0,2022-07-12 11:49:54,,10.1007/978-3-319-30447-2_2,,,,,,,28,5.07,9.0,3.0,6.0,… In this work we define robustness as the ability to correctly classify similar inputs. … robustness methods and by comparing their results. The article provides investigation of robustness not …,https://www.researchgate.net/profile/Stefan-Byttner/publication/308735425_Robustness_of_Deep_Convolutional_Neural_Networks_for_Image_Recognition/links/5db156eea6fdccc99d939571/Robustness-of-Deep-Convolutional-Neural-Networks-for-Image-Recognition.pdf,https://scholar.google.com/scholar?q=related:cx5p1hPGEyoJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'computer vision', 'deep learning']"
No,"Neural Network, Training, Robust",Adaptive Retraining for Neural Network Robustness in Classification,0.0,"R Yao, C Huang, Z Hu, K Pei",2021.0,2021 International Joint …,ieeexplore.ieee.org,https://ieeexplore.ieee.org/abstract/document/9534294/?casa_token=mcVY6J6PAHYAAAAA:DXtjtqsV7Phg3zSCI1PvmthLUpSDLkaMwfzMyE4899ds3cgwUfNN6IZzZesRRvA36pxgeU1k_w,,196.0,2022-07-13 09:30:22,,,,,,,,,0,0.0,0.0,4.0,1.0,… robustness … robustness optimization primarily focus on increasing the minimum adversarial perturbation of individual datum while neglecting the purpose of the target machine learning …,https://ieeexplore.ieee.org/iel7/9533266/9533267/09534294.pdf?casa_token=cZFNLVYVkVwAAAAA:qMhX4cbfMYWbal1n-bNO4OxxwM2R6b5RPpftyMHKvlLWhhf_3vYy1pZSg6z4-lI1GJpst3WOlQ,https://scholar.google.com/scholar?q=related:O4UnA2d0saQJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22explanation%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural network', 'robust', 'training']"
Yes,"Convolutional Neural Network, Neural Architecture, Adversarial Attacks",Neural architecture dilation for adversarial robustness,2.0,"Y Li, Z Yang, Y Wang, C Xu",2021.0,Advances in Neural …,proceedings.neurips.cc,https://proceedings.neurips.cc/paper/2021/hash/f7664060cc52bc6f3d620bcedc94a4b6-Abstract.html,https://scholar.google.com/scholar?cites=9843129568378931723&as_sdt=2005&sciodt=2007&hl=en,196.0,2022-07-12 11:56:54,,,,,,,,,2,2.0,1.0,4.0,1.0,"… adversarial training, there is a trade-off between standard accuracy and adversarial robustness… , this paper aims to improve the adversarial robustness of the backbone CNNs that have …",https://proceedings.neurips.cc/paper/2021/file/f7664060cc52bc6f3d620bcedc94a4b6-Paper.pdf,https://scholar.google.com/scholar?q=related:C8pe-TDSmYgJ:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial attacks', 'convolutional neural network', 'neural architecture']"
Yes,"Adversarial Robustness, Ensemble Classifier, Training",Improving adversarial robustness of ensembles with diversity training,68.0,"S Kariyappa, MK Qureshi",2019.0,arXiv preprint arXiv:1901.09981,arxiv.org,https://arxiv.org/abs/1901.09981,https://scholar.google.com/scholar?cites=5717604442314465721&as_sdt=2005&sciodt=2007&hl=en,197.0,2022-07-13 09:47:56,,,,,,,,,68,23.07,34.0,2.0,3.0,… Our key insight is that an adversarial example is less likely to fool multiple models in the … We show that our method significantly improves the adversarial robustness of ensembles …,https://arxiv.org/pdf/1901.09981,https://scholar.google.com/scholar?q=related:uWFZdvICWU8J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22adversarial%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'ensemble classifier', 'training']"
No,"neural networks, unknown unknowns, adversarial robustness",Towards Robust Artificial Intelligence Systems,2.0,Sunny Raj,2020.0,,,,,197.0,2022-07-13 09:19:02,,,,,,,,,2,1.0,2.0,1.0,2.0,"Adoption of deep neural networks (DNNs) into safety-critical and high-assurance systems has been hindered by the inability of DNNs to handle adversarial and out-of-distribution input. State-ofthe-art DNNs misclassify adversarial input and give high confidence output for out-of-distribution input. We attempt to solve this problem by employing two approaches, first, by detecting adversarial input and, second, by developing a confidence metric that can indicate when a DNN system has reached its limits and is not performing to the desired specifications. The effectiveness of our method at detecting adversarial input is demonstrated against the popular DeepFool adversarial image generation method. On a benchmark of 50,000 randomly chosen ImageNet adversarial images generated for CaffeNet and GoogLeNet DNNs, our method can recover the correct label with 95.76% and 97.43% accuracy, respectively. The proposed attribution-based confidence (ABC) metric utilizes attributions used to explain DNN output to characterize whether an output corresponding to an input to the DNN can be trusted. The attribution based approach removes the need to store training or test data or to train an ensemble of models to obtain confidence scores. Hence, the ABC metric can be used when only the trained DNN is available during inference. We test the effectiveness of the ABC metric against both adversarial and out-of-distribution input. We experimental demonstrate that the ABC metric is high for ImageNet input and low for adversarial input generated by FGSM, PGD, DeepFool, CW, and adversarial patch methods. For a DNN trained on MNIST images, ABC metric is high for in-distribution MNIST input and low for out-of-distribution Fashion-MNIST and notMNIST input.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'neural networks', 'unknown unknowns']"
Yes,"Certified Robustness, Quantization, Adversarial Perturbations",Integer-arithmetic-only Certified Robustness for Quantized Neural Networks,0.0,"H Lin, J Lou, L Xiong…",2021.0,Proceedings of the IEEE …,openaccess.thecvf.com,http://openaccess.thecvf.com/content/ICCV2021/html/Lin_Integer-Arithmetic-Only_Certified_Robustness_for_Quantized_Neural_Networks_ICCV_2021_paper.html,,198.0,2022-07-13 10:25:08,,,,,,,,,0,0.0,0.0,4.0,1.0,… machine learning and security communities. A line of work on tackling adversarial examples is certified robustness … make wrong predictions with high confidence when a sample is …,https://openaccess.thecvf.com/content/ICCV2021/papers/Lin_Integer-Arithmetic-Only_Certified_Robustness_for_Quantized_Neural_Networks_ICCV_2021_paper.pdf,https://scholar.google.com/scholar?q=related:RLJLcPkfMVQJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22confidence%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial perturbations', 'quantization', 'robust']"
No,"Recurrent Neural Network, Performance, Robust",Balance Between Performance and Robustness of Recurrent Neural Networks Brought by Brain-Inspired Constraints on Initial Structure,0.0,"Y Ikeda, T Fusauchi, T Samura",2021.0,International Conference on Neural …,Springer,https://link.springer.com/chapter/10.1007/978-3-030-92270-2_15,,198.0,2022-07-13 08:53:52,,10.1007/978-3-030-92270-2_15,,,,,,,0,0.0,0.0,3.0,1.0,"… a balance between performance and robustness of RNNs at a … T., Fusauchi, T.: Improvement on performance of recurrent … International Conference on Artificial Intelligence and Statistics …",,https://scholar.google.com/scholar?q=related:UiW_adZ_CC8J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22performance%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['performance', 'recurrent neural network', 'robust']"
Yes,"Counterfactual explanations, adversarial robustness",On the connections between counterfactual explanations and adversarial examples,4.0,"M Pawelczyk, S Joshi, C Agarwal, S Upadhyay…",2021.0,arXiv e …,ui.adsabs.harvard.edu,https://ui.adsabs.harvard.edu/abs/2021arXiv210609992P/abstract,https://scholar.google.com/scholar?cites=5723580864007092326&as_sdt=2005&sciodt=2007&hl=en,198.0,2022-07-12 11:56:04,,,,,,,,,4,4.0,1.0,5.0,1.0,… Counterfactual explanations and adversarial examples have emerged as critical research areas for addressing the explainability and robustness goals of machine learning (ML). While …,,https://scholar.google.com/scholar?q=related:Zrhdkng-bk8J:scholar.google.com/&scioq=%22robustness%22+%22artificial+intelligence%22+%22explainability%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial robustness', 'counterfactual', 'explainability']"
No,"Convolutional Neural Network, Object Tracking, Robust",Improved Hierarchical Convolutional Features for Robust Visual Object Tracking,0.0,J. Sun,2021.0,Complexity,,,https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85100898259&origin=inward,198.0,2022-07-12 16:29:20,Article,10.1155/2021/6690237,1076-2787,https://api.elsevier.com/content/abstract/scopus_id/85100898259,2021.0,,,,0,0.0,0.0,1.0,1.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['computer vision', 'convolutional neural network', 'robust']"
Yes,"malware, evasion, robust",Enhancing Robustness of Neural Networks through Fourier Stabilization,0.0,"N Raviv, A Kelley, M Guo…",2021.0,… on Machine Learning,proceedings.mlr.press,http://proceedings.mlr.press/v139/raviv21a.html,,199.0,2022-07-13 14:17:37,,,,,,,,,0,0.0,0.0,4.0,1.0,… a neural network by selecting a subset of neurons to stabilize that best trades off robustness and accuracy. … Our goal is to maximize robustness subject to a constraint that accuracy is no …,http://proceedings.mlr.press/v139/raviv21a/raviv21a.pdf,https://scholar.google.com/scholar?q=related:IXi3T-bHmUUJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['cybersecurity', 'evasion', 'robust']"
No,"Certified Robustness, Smoothing, Neural Network",Insta-RS: Instance-wise Randomized Smoothing for Improved Robustness and Accuracy,0.0,"C Chen, K Kong, P Yu, J Luque, T Goldstein…",2021.0,arXiv preprint arXiv …,arxiv.org,https://arxiv.org/abs/2103.04436,,199.0,2022-07-13 10:35:21,,,,,,,,,0,0.0,0.0,6.0,1.0,… that boosts the certified robustness of the smoothed model. … model that boosts the certified robustness of the instance-wise … well as the clean data accuracy compared to existing state-of-…,,https://scholar.google.com/scholar?q=related:7hQyB29JndwJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22accuracy%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['neural network', 'robust', 'smoothing']"
Yes,robustness to natural perturbations,A Causal View on Robustness of Neural Networks,32.0,"C. Zhang, Kun Zhang, Yingzhen Li",2020.0,,,,,199.0,2022-07-13 09:26:25,,,,,,,,,32,16.0,11.0,3.0,2.0,"We present a causal view on the robustness of neural networks against input manipulations, which applies not only to traditional classification tasks but also to general measurement data. Based on this view, we design a deep causal manipulation augmented model (deep CAMA) which explicitly models possible manipulations on certain causes leading to changes in the observed effect. We further develop data augmentation and test-time fine-tuning methods to improve deep CAMA's robustness. When compared with discriminative deep neural networks, our proposed model shows superior robustness against unseen manipulations. As a by-product, our model achieves disentangled representation which separates the representation of manipulations from those of other latent causes.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,['perturbations']
No,"Robust, Adversarial, Training",A Mutual Information Regularization for Adversarial Training,1.0,"M Atsague, O Fakorede, J Tian",2021.0,… on Machine Learning,proceedings.mlr.press,https://proceedings.mlr.press/v157/atsague21a,https://scholar.google.com/scholar?cites=8234370430743684029&as_sdt=2005&sciodt=2007&hl=en,199.0,2022-07-13 10:18:24,,,,,,,,,1,1.0,0.0,3.0,1.0,"… -AT, for improving the robustness of deep neural networks … robustness against common attacks and achieves a better trade-off between the natural accuracy and adversarial robustness…",https://proceedings.mlr.press/v157/atsague21a/atsague21a.pdf,https://scholar.google.com/scholar?q=related:vSN1eUdcRnIJ:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22human+knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['adversarial', 'robust', 'training']"
Yes,"explainability, survey",Towards Explainable Artificial Intelligence,196.0,"W. Samek, K. Müller",2019.0,,,,,199.0,2022-07-13 09:19:02,,10.1007/978-3-030-28954-6_1,,,,,,,196,"2,73125",98.0,2.0,3.0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['explainability', 'survey']"
No,"Distillation, Label Noise, Generalization",Improving generalization and robustness with noisy collaboration in knowledge distillation,4.0,"E Arani, F Sarfraz, B Zonooz",2019.0,arXiv preprint arXiv:1910.05057,researchgate.net,https://www.researchgate.net/profile/Elahe-Arani/publication/336510685_Improving_Generalization_and_Robustness_with_Noisy_Collaboration_in_Knowledge_Distillation/links/5f04f89a299bf188160a290c/Improving-Generalization-and-Robustness-with-Noisy-Collaboration-in-Knowledge-Distillation.pdf,https://scholar.google.com/scholar?cites=17104764233117078197&as_sdt=2005&sciodt=2007&hl=en,200.0,2022-07-13 11:20:18,PDF,,,,,,,,4,1.33,1.0,3.0,3.0,"… We report the worst robustness accuracy for five random initialization runs. Finally, we test the robustness of our models to … The journal of machine learning research, 15(1):1929–1958. …",https://www.researchgate.net/profile/Elahe-Arani/publication/336510685_Improving_Generalization_and_Robustness_with_Noisy_Collaboration_in_Knowledge_Distillation/links/5f04f89a299bf188160a290c/Improving-Generalization-and-Robustness-with-Noisy-Collaboration-in-Knowledge-Distillation.pdf,https://scholar.google.com/scholar?q=related:tRLA8XFUYO0J:scholar.google.com/&scioq=%22robustness%22+%22machine+learning%22+%22knowledge%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['distillation', 'generalization', 'label noise']"
Discussion,"Adversarial Perturbations, Training, Accuracy",Precise Statistical Analysis of Classification Accuracies for Adversarial Training,17.0,"Adel Javanmard, M. Soltanolkotabi",2020.0,,,,,200.0,2022-07-13 09:40:19,,,,,,,,,17,8.5,9.0,2.0,2.0,"Despite the wide empirical success of modern machine learning algorithms and models in a multitude of applications, they are known to be highly susceptible to seemingly small indiscernible perturbations to the input data known as adversarial attacks. A variety of recent adversarial training procedures have been proposed to remedy this issue. Despite the success of such procedures at increasing accuracy on adversarially perturbed inputs or robust accuracy, these techniques often reduce accuracy on natural unperturbed inputs or standard accuracy. Complicating matters further the effect and trend of adversarial training procedures on standard and robust accuracy is rather counter intuitive and radically dependent on a variety of factors including the perceived form of the perturbation during training, size/quality of data, model overparameterization, etc. In this paper we focus on binary classification problems where the data is generated according to the mixture of two Gaussians with general anisotropic covariance matrices and derive a precise characterization of the standard and robust accuracy for a class of minimax adversarially trained models. We consider a general norm-based adversarial model, where the adversary can add perturbations of bounded $\ell_p$ norm to each input data, for an arbitrary $p\ge 1$. Our comprehensive analysis allows us to theoretically explain several intriguing empirical phenomena and provide a precise understanding of the role of different problem parameters on standard and robust accuracies.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['accuracy', 'adversarial perturbations', 'training']"
No,"Reliability, Explainability, Deep Learning",Smint: Toward interpretable and robust model sharing for deep neural networks,3.0,"H Wu, C Wang, R Nock, W Wang, J Yin, K Lu…",2020.0,ACM Transactions on …,dl.acm.org,https://dl.acm.org/doi/abs/10.1145/3381833,https://scholar.google.com/scholar?cites=908049134289085316&as_sdt=2005&sciodt=2007&hl=en,200.0,2022-07-13 13:42:15,,10.1145/3381833,,,,,,,3,1.5,0.0,7.0,2.0,"… model, particularly a deep neural network via prediction APIs, … , such as saliency maps and interpretable input masks, into … data to make these models interpretable. The human pilot …",https://www.researchgate.net/profile/Huijun-Wu/publication/341159066_SMINT_Toward_Interpretable_and_Robust_Model_Sharing_for_Deep_Neural_Networks/links/5f7d189d92851c14bcb37265/SMINT-Toward-Interpretable-and-Robust-Model-Sharing-for-Deep-Neural-Networks.pdf,https://scholar.google.com/scholar?q=related:hOPmZ-AJmgwJ:scholar.google.com/&scioq=%22robustness%22+%22neural+network%22+%22interpretable%22&hl=en&as_sdt=2007,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"['deep learning', 'explainability', 'reliability']"
,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Number of Yes,379,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Number of No,163,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Number of Discussion,20,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
, ,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,