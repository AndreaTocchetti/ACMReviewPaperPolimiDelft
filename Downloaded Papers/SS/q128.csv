Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
22,"F. Emmert‐Streib, M. Dehmer","A Machine Learning Perspective on Personalized Medicine: An Automized, Comprehensive Knowledge Base with Ontology for Pattern Recognition",2018,"","","","",1,"2022-07-13 10:07:05","","10.3390/MAKE1010009","","",,,,,22,5.50,11,2,4,"Personalized or precision medicine is a new paradigm that holds great promise for individualized patient diagnosis, treatment, and care. However, personalized medicine has only been described on an informal level rather than through rigorous practical guidelines and statistical protocols that would allow its robust practical realization for implementation in day-to-day clinical practice. In this paper, we discuss three key factors, which we consider dimensions that effect the experimental design for personalized medicine: (I) phenotype categories; (II) population size; and (III) statistical analysis. This formalization allows us to define personalized medicine from a machine learning perspective, as an automized, comprehensive knowledge base with an ontology that performs pattern recognition of patient profiles.","",""
127,"Lei Zhang, D. Zhang","Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation",2016,"","","","",2,"2022-07-13 10:07:05","","10.1109/TIP.2016.2598679","","",,,,,127,21.17,64,2,6,"We address the problem of visual knowledge adaptation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine (ELM)-based cross-domain network learning framework, that is called ELM-based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the ℓ2,1-norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross-domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as the base classifiers. The network output weights cannot only be analytically determined, but also transferrable. In addition, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semisupervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition demonstrate that our EDA methods outperform the existing cross-domain learning methods.","",""
13,"Ali A. Abdallah, S. Saab, Z. Kassas","A machine learning approach for localization in cellular environments",2018,"","","","",3,"2022-07-13 10:07:05","","10.1109/PLANS.2018.8373508","","",,,,,13,3.25,4,3,4,"A machine learning approach is developed for localization based on received signal strength (RSS) from cellular towers. The proposed approach only assumes knowledge of RSS fingerprints of the environment, and does not require knowledge of the cellular base transceiver station (BTS) locations, nor uses any RSS mathematical model. The proposed localization scheme integrates a weighted K-nearest neighbor (WKNN) and a multilayer neural network. The integration takes advantage of the robust clustering ability of WKNN and implements a neural network that could estimate the position within each cluster. Experimental results are presented to demonstrate the proposed approach in two urban environments and one rural environment, achieving a mean distance localization error of 5.9 m and 5.1 m in the urban environments and 8.7 m in the rural environment. This constitutes an improvement of 41%, 45%, and 16%, respectively, over the WKNN-only algorithm.","",""
2,"Lei Zhang, David Zhang","Robust Visual Knowledge Transfer via EDA",2015,"","","","",4,"2022-07-13 10:07:05","","","","",,,,,2,0.29,1,2,7,"We address the problem of visual knowledge adaptation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine based cross-domain network learning framework, that is called Extreme Learning Machine (ELM) based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the l_(2,1)-norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as base classifiers. The network output weights cannot only be analytically determined, but also transferrable. Additionally, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semi-supervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition, demonstrate that our EDA methods outperform existing cross-domain learning methods.","",""
1,"M. Feroz, C. Jada, R. K. Kumar, H. Yenala, V. Kumar","Map based representation of navigation information for robust machines learning",2015,"","","","",5,"2022-07-13 10:07:05","","10.1109/SPACES.2015.7058224","","",,,,,1,0.14,0,5,7,"Much research is being carried out in autonomous driving of vehicles under various disciplines but very few autonomous navigating vehicles are developed till date. This paper presents a novel technique, with a practical example, for the representation of navigational information under real-time considerations. Machine learning algorithms can easily be trained with the data sets developed from the representation and to testify this, an Artificial Neural Network is trained with a represented data set. This technique is independent of driving capabilities of vehicle and provides simple directional instructions for navigation. Using this method, an autonomous driving vehicle can be made to learn to navigate between locations in a known region. Moreover, the usage of ANN makes it completely adaptive and any changes or modifications in the trained region can easily be updated into the knowledge base.","",""
1,"Moulay Rachid Douiri, M. Cherkaoui","Learning fuzzy controller and extended Kalman filter for sensorless induction motor robust against resistance variation",2012,"","","","",6,"2022-07-13 10:07:05","","10.1007/S11460-012-0206-Y","","",,,,,1,0.10,1,2,10,"","",""
14,"Quang-Phuoc Nguyen, Anh-Dung Vo, Joon-Choul Shin, Cheolyoung Ock","Effect of Word Sense Disambiguation on Neural Machine Translation: A Case Study in Korean",2018,"","","","",7,"2022-07-13 10:07:05","","10.1109/ACCESS.2018.2851281","","",,,,,14,3.50,4,4,4,"With the advent of robust deep learning, neural machine translation (NMT) has achieved great progress and recently become the dominant paradigm in machine translation (MT). However, it is still confronted with the challenge of word ambiguities that force NMT to choose among several translation candidates that represent different senses of an input word. This research presents a case study using Korean word sense disambiguation (WSD) to improve NMT performance. First, we constructed a Korean lexical semantic network (LSN) as a large-scale lexical semantic knowledge base. Then, based on the Korean LSN, we built a Korean WSD preprocessor that can annotate the correct sense of Korean words in the training corpus. Finally, we conducted a series of translation experiments using Korean-English, Korean-French, Korean-Spanish, and Korean-Japanese language pairs. The experimental results show that our Korean WSD system can significantly improve the translation quality of NMT in terms of the BLEU, TER, and DLRATIO metrics. On average, it improved the precision by 2.94 BLEU points and improved translation error prevention by 4.04 TER points and 4.51 DLRATIO points for all the language pairs.","",""
2,"Eko Mulyani, I. Hidayah, S. Fauziati","Dropout Prediction Optimization through SMOTE and Ensemble Learning",2019,"","","","",8,"2022-07-13 10:07:05","","10.1109/ISRITI48646.2019.9034673","","",,,,,2,0.67,1,3,3,"Dropout rates on Massive Open Online Courses (MOOCs) are still very high. One effort to reduce the number of dropouts is by interfering at-risk students. An accurate and high consistency prediction system is needed. Machine learning method is the most popular in handling this case. The focus of machine learning that ensures the ability of models in generalizing knowledge makes overfitting a critical issue in supervised learning. Thus, a single classifier may fail to classify correctly. Besides, the inherent class imbalance between dropout (majority class) and non-dropout (minority) makes it difficult to build robust predictions. To overcome this problem, an ensemble learning (EL) is combined with synthetic minority over-sampling technique (SMOTE). The SMOTE-Ensemble Learning (SEL) is done by majority voting from three base machine learning methods: Logistic Regression (LR), K-Nearest Neighbor (KNN) and Random Forest (RF). The result on the KDDCUP2015 dataset shows that this combination able to improve prediction performance with an average improvement of the harmonic mean of precision and recall (F1-score): 7.74% compared to previous work.","",""
6,"P. Langley","Challenges for the Application of Machine Learning",1997,"","","","",9,"2022-07-13 10:07:05","","","","",,,,,6,0.24,6,1,25,"By most accounts, the applied branch of machine learning has been a clear success. Induction techniques have aided the development of elded systems in science and industry, on a range of tasks, including me-searchers in the area can feel genuinely proud that their algorithms have proven so robust and developers deserve major credit for identifying promising applications and seeing them through to completion. The basic development story should by now be quite The developer works with a domain expert or user to understand some problem, and then reformulates the problem into one that can be addressed by well-established methods for supervised learning. He then determines a set of likely features to describe the training cases, and devises an approach to collecting and preparing those data. Once these are available, he runs some induction method over the data. The developer (and possibly the expert) then evaluate the resulting knowledge base along dimensions of interest, such as accuracy, understandability, and consistency with existing domain knowledge. If the result seems acceptable, they attempt to deploy the learned knowledge in the eld. 1 Applied work in machine learning diiers from academic learning research in its acknowledgement of this development process. Most research papers on learning continue to emphasize reenements in the induction technique and ignore the steps that must occur 1 Of course, this process is not linear but iterative. Problems at any step can lead the developer to backtrack to an earlier stage and revisit decisions made there. before and after its invocation. In contrast, applied efforts recognize the importance of problem formulation, representation engineering, data collection and preparation , inspection of the learned knowledge, and the elding process itself. Within the applications community , there is an emerging consensus that these steps play a role at least as important as the induction stage itself. Indeed, there is even a common belief that, once they are handled, the particular induction method one uses has little eeect on the outcome. 2 Automating the Overall Process Clearly, the applied induction community could continue along these lines and be quite successful. The discipline could use its established approach to develop more elded applications and train a cadre of problem and representation engineers to become expert at the overall process. Over time, this new generation of developers would come to replace the knowledge engineers currently charged with creating knowledge-based systems. But this is a limited vision, and …","",""
2,"Dongjin Choi, Myunggwon Hwang, Byeongkyu Ko, Sicheon You, Pankoo Kim","Low Ambiguity First Algorithm: A New Approach to Knowledge-Based Word Sense Disambiguation",2015,"","","","",10,"2022-07-13 10:07:05","","10.1007/978-3-319-20895-4_52","","",,,,,2,0.29,0,5,7,"","",""
4,"B. Bahrak, J. Park","Security of Spectrum Learning in Cognitive Radios",2013,"","","","",11,"2022-07-13 10:07:05","","","","",,,,,4,0.44,2,2,9,"Due to delay and energy constraints, a cognitive radio may not be able to perform spectrum sensing in all available channels. Therefore, a sensing policy is needed to decide which channels to sense. The channel selection problem is the problem of designing such a sensing policy to maximize throughput while avoiding interference to primary users. The channel selection problem can be formulated as a reinforcement learning problem. Channel selection schemes that employ reinforcement machine learning algorithms are vulnerable to belief manipulation attacks that contaminate the knowledge base of the learning algorithms. In this paper, we analyze the security of channel selection algorithms that are based on reinforcement learning and propose mitigation techniques that make these algorithms more robust against belief manipulation attacks.","",""
14,"R. M. M. Vallim, D. Goldberg, Xavier Llorà, Thyago Duque, A. Carvalho","A new approach for multi-label classification based on default hierarchies and organizational learning",2008,"","","","",12,"2022-07-13 10:07:05","","10.1145/1388969.1389015","","",,,,,14,1.00,3,5,14,"Learning Classifier Systems (LCSs) are a class of expert systems that use a knowledge base of decision rules and a genetic algorithm (GA) [9] as a discovery mechanism. The set of decision rules allows the LCS to represent and learn control strategies, while the robust search ability of the GA allows it to search for new rules based on the performance of existing rules. LCS were first designed to solve machine learning problems, especially classification problems. Classification problems are problems where instances of a data set belong to a set of classes, and the system needs to infer, based on past experience, the correct class (or classes) of new, previously unseen, instances. However, the features of LCSs are also very useful for solving reinforcement learning problems, a class of problems where the system should learn to operate in the environment based only on performance feedback. This paper considers LCSs as an approach to classification problems, more specifically a more complex kind of classification called multi-label classification. This paper analyses the default hierarchy formation theory presented by [14] as a way of favoring the hierarchical arrangement of rules, and also the organizational learning theory [17] for adjusting the degree of individual and collective behaviors. We suggest a new method, combining both organizational learning and default hierarchy formation, for solving multi-label problems. The preliminary results with a simple multi-label problem show the potential of this method. Final discussion presents the conclusions and directions for further research.","",""
21,"L. Saitta, M. Botta, F. Neri","Multistrategy learning and theory revision",1993,"","","","",13,"2022-07-13 10:07:05","","10.1007/BF00993075","","",,,,,21,0.72,7,3,29,"","",""
3,"K. Ramamurthi, C. Hough","A hybrid approach for robust diagnostics of cutting tools",1994,"","","","",14,"2022-07-13 10:07:05","","10.1109/21.278996","","",,,,,3,0.11,2,2,28,"A new multisensor based hybrid technique has been developed for robust diagnosis of cutting tools. The technique combines the concepts of pattern classification and real-time knowledge based systems (RTKBS) and draws upon their strengths; learning facility in the case of pattern classification and a higher level of reasoning in the case of RTKBS. It eliminates some of their major drawbacks: false alarms or delayed/lack of diagnosis in case of pattern classification and tedious knowledge base generation in case of RTKBS. It utilizes a dynamic distance classifier, developed upon a new separability criterion and a new definition of robust diagnosis for achieving these benefits. The promise of this technique has been proven concretely through an on-line diagnosis of drill wear. Its suitability for practical implementation is substantiated by the use of practical, inexpensive, machine-mounted sensors and low-cost delivery systems. >","",""
8,"L. Saitta, M. Botta, F. Neri","Multistrategy Learning and Theory Revision",2004,"","","","",15,"2022-07-13 10:07:05","","10.1023/A:1022605302519","","",,,,,8,0.44,3,3,18,"","",""
7,"Björn Pelzer, Ingo Glöckner","Combining Theorem Proving with Natural Language Processing",2008,"","","","",16,"2022-07-13 10:07:05","","","","",,,,,7,0.50,4,2,14,"The LogAnswer system is an application of automated reasoning to the field of open domain question answering, which aims at finding answers to natural language questions regarding arbitrary topics. In our system we have integrated an automated theorem prover in a framework of natural language processing tools to allow for deductive reasoning over an extensive knowledge base derived from textual sources. For this purpose we had to intertwine two opposing approaches: on the one hand formal logic with its precision but brittleness, and on the other hand, machine learning applied to shallow linguistic features, which are robust but less precise. In the paper we present implementation details and discuss obstacles and their proposed solutions.","",""
1,"M. Staron, Helena Odenstedt Herg'es, S. Naredi, L. Block, Ali El-Merhi, Richard Vithal, M. Elam","Robust Machine Learning in Critical Care — Software Engineering and Medical Perspectives",2021,"","","","",17,"2022-07-13 10:07:05","","10.1109/WAIN52551.2021.00016","","",,,,,1,1.00,0,7,1,"Using machine learning in clinical practice poses hard requirements on explainability, reliability, replicability and robustness of these systems. Therefore, developing reliable software for monitoring critically ill patients requires close collaboration between physicians and software engineers. However, these two different disciplines need to find own research perspectives in order to contribute to both the medical and the software engineering domain. In this paper, we address the problem of how to establish a collaboration where software engineering and medicine meets to design robust machine learning systems to be used in patient care. We describe how we designed software systems for monitoring patients under carotid endarterectomy, in particular focusing on the process of knowledge building in the research team. Our results show what to consider when setting up such a collaboration, how it develops over time and what kind of systems can be constructed based on it. We conclude that the main challenge is to find a good research team, where different competences are committed to a common goal.","",""
7,"Jonathan Fürst, Mauricio Fadel Argerich, Bin Cheng, E. Kovacs","Towards Knowledge Infusion for Robust and Transferable Machine Learning in IoT",2020,"","","","",18,"2022-07-13 10:07:05","","","","",,,,,7,3.50,2,4,2,"Machine learning (ML) applications in Internet of Things (IoT) scenarios face the issue that supervision signals, such as labeled data, are scarce and expensive to obtain. For example, it often requires a human to manually label events in a data stream by observing the same events in the real world. In addition, the performance of trained models usually depends on a specific context: (1) location, (2) time and (3) data quality. This context is not static in reality, making it hard to achieve robust and transferable machine learning for IoT systems in practice. In this paper, we address these challenges with an envisioned method that we name Knowledge Infusion. First, we present two past case studies in which we combined external knowledge with traditional data-driven machine learning in IoT scenarios to ease the supervision effort: (1) a weak-supervision approach for the IoT domain to auto-generate labels based on external knowledge (e.g., domain knowledge) encoded in simple labeling functions. Our evaluation for transport mode classification achieves a micro-F1 score of 80.2%, with only seven labeling functions, on par with a fully supervised model that relies on hand-labeled data. (2) We introduce guiding functions to Reinforcement Learning (RL) to guide the agents' decisions and experience. In initial experiments, our guided reinforcement learning achieves more than three times higher reward in the beginning of its training than an agent with no external knowledge. We use the lessons learned from these experiences to develop our vision of knowledge infusion. In knowledge infusion, we aim to automate the inclusion of knowledge from existing knowledge bases and domain experts to combine it with traditional data-driven machine learning techniques during setup/training phase, but also during the execution phase.","",""
17,"Zhenfeng Lei, Yuandong Sun, Y. Nanehkaran, Shuangyuan Yang, Md. Saiful Islam, Huiqing Lei, De-fu Zhang","A novel data-driven robust framework based on machine learning and knowledge graph for disease classification",2020,"","","","",19,"2022-07-13 10:07:05","","10.1016/j.future.2019.08.030","","",,,,,17,8.50,2,7,2,"","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",20,"2022-07-13 10:07:05","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
3,"Suk Joon Hong, B. Bennett","Tackling Domain-Specific Winograd Schemas with Knowledge-Based Reasoning and Machine Learning",2020,"","","","",21,"2022-07-13 10:07:05","","10.4230/OASIcs.LDK.2021.41","","",,,,,3,1.50,2,2,2,"The Winograd Schema Challenge (WSC) is a common-sense reasoning task that requires background knowledge. In this paper, we contribute to tackling WSC in four ways. Firstly, we suggest a keyword method to define a restricted domain where distinctive high-level semantic patterns can be found. A thanking domain was defined by key-words, and the data set in this domain is used in our experiments. Secondly, we develop a high-level knowledge-based reasoning method using semantic roles which is based on the method of Sharma [2019]. Thirdly, we propose an ensemble method to combine knowledge-based reasoning and machine learning which shows the best performance in our experiments. As a machine learning method, we used Bidirectional Encoder Representations from Transformers (BERT) [Kocijan et al., 2019]. Lastly, in terms of evaluation, we suggest a ""robust"" accuracy measurement by modifying that of Trichelair et al. [2018]. As with their switching method, we evaluate a model by considering its performance on trivial variants of each sentence in the test set.","",""
2,"Nezihe Merve Gurel, Xiangyu Qi, Luka Rimanic, Ce Zhang, Bo Li","Knowledge Enhanced Machine Learning Pipeline against Diverse Adversarial Attacks",2021,"","","","",22,"2022-07-13 10:07:05","","","","",,,,,2,2.00,0,5,1,"Despite the great successes achieved by deep neural networks (DNNs), recent studies show that they are vulnerable against adversarial examples, which aim to mislead DNNs by adding small adversarial perturbations. Several defenses have been proposed against such attacks, while many of them have been adaptively attacked. In this work, we aim to enhance the ML robustness from a different perspective by leveraging domain knowledge: We propose a Knowledge Enhanced Machine Learning Pipeline (KEMLP) to integrate domain knowledge (i.e., logic relationships among different predictions) into a probabilistic graphical model via first-order logic rules. In particular, we develop KEMLP by integrating a diverse set of weak auxiliary models based on their logical relationships to the main DNN model that performs the target task. Theoretically, we provide convergence results and prove that, under mild conditions, the prediction of KEMLP is more robust than that of the main DNN model. Empirically, we take road sign recognition as an example and leverage the relationships between road signs and their shapes and contents as domain knowledge. We show that compared with adversarial training and other baselines, KEMLP achieves higher robustness against physical attacks, Lp bounded attacks, unforeseen attacks, and natural corruptions under both whitebox and blackbox settings, while still maintaining high clean accuracy. Equal contribution ETH Zurich, Zurich, Switzerland Zhejiang University, China (work done during remote internship at UIUC) University of Illinois at Urbana-Champaign, Illinois, USA. Correspondence to: Nezihe Merve Gürel <nezihe.guerel@inf.ethz.ch>, Xiangyu Qi <unispac@zju.edu.cn>, Ce Zhang <ce.zhang@inf.ethz.ch>, Bo Li <lbo@illinois.edu>. Proceedings of the 38 th International Conference on Machine Learning, PMLR 139, 2021. Copyright 2021 by the author(s).","",""
23,"Sina Shaham, Ming Ding, Bo Liu, Shuping Dang, Zihuai Lin, Jun Li","Privacy Preserving Location Data Publishing: A Machine Learning Approach",2019,"","","","",23,"2022-07-13 10:07:05","","10.1109/tkde.2020.2964658","","",,,,,23,7.67,4,6,3,"Publishing datasets plays an essential role in open data research and promoting transparency of government agencies. However, such data publication might reveal users’ private information. One of the most sensitive sources of data is spatiotemporal trajectory datasets. Unfortunately, merely removing unique identifiers cannot preserve the privacy of users. Adversaries may know parts of the trajectories or be able to link the published dataset to other sources for the purpose of user identification. Therefore, it is crucial to apply privacy preserving techniques before the publication of spatiotemporal trajectory datasets. In this paper, we propose a robust framework for the anonymization of spatiotemporal trajectory datasets termed as machine learning based anonymization (MLA). By introducing a new formulation of the problem, we are able to apply machine learning algorithms for clustering the trajectories and propose to use <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=""shaham-ieq1-2964658.gif""/></alternatives></inline-formula>-means algorithm for this purpose. A variation of <inline-formula><tex-math notation=""LaTeX"">$k$</tex-math><alternatives><mml:math><mml:mi>k</mml:mi></mml:math><inline-graphic xlink:href=""shaham-ieq2-2964658.gif""/></alternatives></inline-formula>-means algorithm is also proposed to preserve the privacy in overly sensitive datasets. Moreover, we improve the alignment process by considering multiple sequence alignment as part of the MLA. The framework and all the proposed algorithms are applied to T-Drive, Geolife, and Gowalla location datasets. The experimental results indicate a significantly higher utility of datasets by anonymization based on MLA framework.","",""
21,"A. Naimi, Alan Mishler, Edward H. Kennedy","Challenges in Obtaining Valid Causal Effect Estimates with Machine Learning Algorithms.",2017,"","","","",24,"2022-07-13 10:07:05","","10.1093/aje/kwab201","","",,,,,21,4.20,7,3,5,"Unlike parametric regression, machine learning (ML) methods do not generally require precise knowledge of the true data generating mechanisms. As such, numerous authors have advocated for ML methods to estimate causal effects. Unfortunately, ML algorithmscan perform worse than parametric regression. We demonstrate the performance of ML-based single- and double-robust estimators. We use 100 Monte Carlo samples with sample sizes of 200, 1200, and 5000 to investigate bias and confidence interval coverage under several scenarios. In a simple confounding scenario, confounders were related to the treatment and the outcome via parametric models. In a complex confounding scenario, the simple confounders were transformed to induce complicated nonlinear relationships. In the simple scenario, when ML algorithms were used, double-robust estimators were superior to single-robust estimators. In the complex scenario, single-robust estimators with ML algorithms were at least as biased as estimators using misspecified parametric models. Double-robust estimators were less biased, but coverage was well below nominal. The use of sample splitting, inclusion of confounder interactions, reliance on a richly specified ML algorithm, and use of doubly robust estimators was the only explored approach that yielded negligible bias and nominal coverage. Our results suggest that ML based singly robust methods should be avoided.","",""
16,"Yonghan Jung, Jin Tian, E. Bareinboim","Estimating Identifiable Causal Effects through Double Machine Learning",2021,"","","","",25,"2022-07-13 10:07:05","","","","",,,,,16,16.00,5,3,1,"Identifying causal effects from observational data is a perva- sive challenge found throughout the empirical sciences. Very general methods have been developed to decide the identiﬁ- ability of a causal quantity from a combination of observational data and causal knowledge about the underlying sys- tem. In practice, however, there are still challenges to estimating identiﬁable causal functionals from ﬁnite samples. Re- cently, a method known as double/debiased machine learning (DML) (Chernozhukov et al. 2018) has been proposed to learn parameters leveraging modern machine learning techniques, which is both robust to model misspeciﬁcation and bias-reducing. Still, DML has only been used for causal estimation in settings when the back-door condition (also known as conditional ignorability) holds. In this paper, we develop a new, general class of estimators for any identiﬁable causal functionals that exhibit DML properties, which we name DML-ID. In particular, we introduce a complete identiﬁca- tion algorithm that returns an inﬂuence function (IF) for any identiﬁable causal functional. We then construct the DML es- timator based on the derived IF. We show that DML-ID estimators hold the key properties of debiasedness and doubly robustness. Simulation results corroborate with the theory.","",""
15,"Ali Hürriyetoǧlu, Erdem Yörük, Osman Mutlu, Fırat Duruşan, Çağrı Yoltar, Deniz Yüret, Burak Gürel","Cross-Context News Corpus for Protest Event-Related Knowledge Base Construction",2021,"","","","",26,"2022-07-13 10:07:05","","10.1162/dint_a_00092","","",,,,,15,15.00,2,7,1,"Abstract We describe a gold standard corpus of protest events that comprise various local and international English language sources from various countries. The corpus contains document-, sentence-, and token-level annotations. This corpus facilitates creating machine learning models that automatically classify news articles and extract protest event-related information, constructing knowledge bases that enable comparative social and political science studies. For each news source, the annotation starts with random samples of news articles and continues with samples drawn using active learning. Each batch of samples is annotated by two social and political scientists, adjudicated by an annotation supervisor, and improved by identifying annotation errors semi-automatically. We found that the corpus possesses the variety and quality that are necessary to develop and benchmark text classification and event extraction systems in a cross-context setting, contributing to the generalizability and robustness of automated text processing systems. This corpus and the reported results will establish a common foundation in automated protest event collection studies, which is currently lacking in the literature.","",""
0,"H. Anh, Cao Van Kien","Robust extreme learning machine neural approach for uncertain nonlinear hyper‐chaotic system identification",2021,"","","","",27,"2022-07-13 10:07:05","","10.1002/rnc.5756","","",,,,,0,0.00,0,2,1,"This paper proposes a novel nonlinearly parameterized advanced single‐hidden layer neural extreme learning machine (ASHLN‐ELM) model in which the hidden and output weighting values are simultaneously updated using adaptively robust rules that are implemented based on Lyapunov stability principle. The proposed scheme guarantees the fast convergence speed of the state‐estimation residual errors bounded to null regarding to the influence of time‐varied disturbances. Additionally, proposed method needs no any knowledge related to desired weighting values or required approximating error. Typical uncertain hyper‐chaotic benchmark systems are used as to verify the new ASHLN‐ELM approach and to demonstrate the efficiency and the robustness of proposed method.","",""
18,"S. Fleming, A. Goodbody","A Machine Learning Metasystem for Robust Probabilistic Nonlinear Regression-Based Forecasting of Seasonal Water Availability in the US West",2019,"","","","",28,"2022-07-13 10:07:05","","10.1109/ACCESS.2019.2936989","","",,,,,18,6.00,9,2,3,"Hydroelectric power generation, water supplies for municipal, agricultural, manufacturing, and service industry uses including technology-sector requirements, dam safety, flood control, recreational uses, and ecological and legal constraints, all place simultaneous, competing demands on the heavily stressed water management infrastructure of the mostly arid American West. Optimally managing these resources depends on predicting water availability. We built a probabilistic nonlinear regression water supply forecast (WSF) technique for the US Department of Agriculture, which runs the largest stand-alone WSF system in the US West. Design criteria included improved accuracy over the existing system; uncertainty estimates that seamlessly handle complex (heteroscedastic, non-Gaussian) prediction errors; integration of physical hydrometeorological process knowledge and domain-specific expert experience; ability to accommodate nonlinearity, model selection uncertainty and equifinality, and predictor multicollinearity and high dimensionality; and relatively easy, low-cost implementation. Some methods satisfied some of these requirements but none met all, leading us to develop a novel, interdisciplinary, and pragmatic prediction metasystem through a carefully considered synthesis of well-established, off-the-shelf components and approaches, spanning supervised and unsupervised machine learning, nonparametric statistical modeling, ensemble learning, and evolutionary optimization, focusing on maintaining but radically updating the principal components regression framework widely used for WSF. Testing this integrated multi-method prediction engine demonstrated its value for river forecasting; USDA adoption is a landmark for transitioning machine learning from research into practice in this field. Its ability to handle all the foregoing design criteria and requirements, which are not unique to WSF, suggests potential for extension to complex probabilistic prediction problems in other fields.","",""
15,"Jungryul Seo, T. Laine, Kyung-ah Sohn","An Exploration of Machine Learning Methods for Robust Boredom Classification Using EEG and GSR Data",2019,"","","","",29,"2022-07-13 10:07:05","","10.3390/s19204561","","",,,,,15,5.00,5,3,3,"In recent years, affective computing has been actively researched to provide a higher level of emotion-awareness. Numerous studies have been conducted to detect the user’s emotions from physiological data. Among a myriad of target emotions, boredom, in particular, has been suggested to cause not only medical issues but also challenges in various facets of daily life. However, to the best of our knowledge, no previous studies have used electroencephalography (EEG) and galvanic skin response (GSR) together for boredom classification, although these data have potential features for emotion classification. To investigate the combined effect of these features on boredom classification, we collected EEG and GSR data from 28 participants using off-the-shelf sensors. During data acquisition, we used a set of stimuli comprising a video clip designed to elicit boredom and two other video clips of entertaining content. The collected samples were labeled based on the participants’ questionnaire-based testimonies on experienced boredom levels. Using the collected data, we initially trained 30 models with 19 machine learning algorithms and selected the top three candidate classifiers. After tuning the hyperparameters, we validated the final models through 1000 iterations of 10-fold cross validation to increase the robustness of the test results. Our results indicated that a Multilayer Perceptron model performed the best with a mean accuracy of 79.98% (AUC: 0.781). It also revealed the correlation between boredom and the combined features of EEG and GSR. These results can be useful for building accurate affective computing systems and understanding the physiological properties of boredom.","",""
10,"Yifan Cui, E. Tchetgen","Selective machine learning of doubly robust functionals.",2019,"","","","",30,"2022-07-13 10:07:05","","","","",,,,,10,3.33,5,2,3,"While model selection is a well-studied topic in parametric and nonparametric regression or density estimation, model selection of possibly high-dimensional nuisance parameters in semiparametric problems is far less developed. In this paper, we propose a selective machine learning framework for making inferences about a finite-dimensional functional defined on a semiparametric model, when the latter admits a doubly robust estimating function. We introduce two model selection criteria for bias reduction of functional of interest, each based on a novel definition of pseudo-risk for the functional that embodies this double robustness property and thus may be used to select the candidate model that is nearest to fulfilling this property even when all models are wrong. We establish an oracle property for a multi-fold cross-validation version of the new model selection criteria which states that our empirical criteria perform nearly as well as an oracle with a priori knowledge of the pseudo-risk for each candidate model. We also describe a smooth approximation to the selection criteria which allows for valid post-selection inference. Finally, we apply the approach to model selection of a semiparametric estimator of average treatment effect given an ensemble of candidate machine learners to account for confounding in an observational study.","",""
2,"Lipeng Zhu, D. J. Hill, Chao Lu","Semi-Supervised Ensemble Learning Framework for Accelerating Power System Transient Stability Knowledge Base Generation",2022,"","","","",31,"2022-07-13 10:07:05","","10.1109/tpwrs.2021.3117402","","",,,,,2,2.00,1,3,1,"Machine learning approaches have exhibited high potential in power system transient stability assessment (TSA), yet their initial preparation stages of stability knowledge base generation (SKBG) based on time-domain simulations often undergo high computational costs. In fact, how to ease the heavy computational burden of SKBG without sacrificing the reliability is still a significant challenge. To address this problem, this paper develops a semi-supervised ensemble learning (SSEL) framework for reliable SKBG acceleration, without additional need for computing hardware upgrading. In particular, it performs detailed simulations for a minority of cases and fast simulations for the majority ones to reduce the total computation time. Considering the absence of stability status information of those fast simulated cases, an SSEL scheme is systematically designed to reliably label their stability status. Before implementing SSEL, two concise feature descriptors are first introduced to efficiently extract transient features from multiplex system trajectories. Then, all the cases are characterized in a unified feature space, whereby a series of semi-supervised support vector machines are trained in randomly formed subspaces. Afterward, these single machines are systematically combined to derive an enhanced SSEL model, which is able to make reliable and robust labeling decisions. Further, a backtrace strategy is carefully devised for SSEL, so as to maintain the high reliability of SKBG. Test results on the IEEE 39-bus system and the realistic GD Power Grid in South China illustrate the excellent performances of the proposed framework on SKBG acceleration.","",""
5,"Furkan M. Torun, S. V. Winter, Sophia Doll, Felix M. Riese, A. Vorobyev, Johannes B. Mueller‐Reif, Philipp E. Geyer, Maximilian T. Strauss","Transparent exploration of machine learning for biomarker discovery from proteomics and omics data",2021,"","","","",32,"2022-07-13 10:07:05","","10.1101/2021.03.05.434053","","",,,,,5,5.00,1,8,1,"Biomarkers are of central importance for assessing the health state and to guide medical interventions and their efficacy, but they are lacking for most diseases. Mass spectrometry (MS)-based proteomics is a powerful technology for biomarker discovery, but requires sophisticated bioinformatics to identify robust patterns. Machine learning (ML) has become indispensable for this purpose, however, it is sometimes applied in an opaque manner, generally requires expert knowledge and complex and expensive software. To enable easy access to ML for biomarker discovery without any programming or bioinformatic skills, we developed ‘OmicLearn’ (https://OmicLearn.com), an open-source web-based ML tool using the latest advances in the Python ML ecosystem. We host a web server for the exploration of the researcher’s results that can readily be cloned for internal use. Output tables from proteomics experiments are easily uploaded to the central or a local webserver. OmicLearn enables rapid exploration of the suitability of various ML algorithms for the experimental datasets. It fosters open science via transparent assessment of state-of-the-art algorithms in a standardized format for proteomics and other omics sciences. Graphical Abstract Highlights OmicLearn is an open-source platform allows researchers to apply machine learning (ML) for biomarker discovery The ready-to-use structure of OmicLearn enables accessing state-of-the-art ML algorithms without requiring any prior bioinformatics knowledge OmicLearn’s web-based interface provides an easy-to-follow platform for classification and gaining insights into the dataset Several algorithms and methods for preprocessing, feature selection, classification and cross-validation of omics datasets are integrated All results, settings and method text can be exported in publication-ready formats","",""
4,"Alexandra Renouard, A. Maggi, M. Grunberg, C. Doubre, C. Hibert","Toward False Event Detection and Quarry Blast versus Earthquake Discrimination in an Operational Setting Using Semiautomated Machine Learning",2021,"","","","",33,"2022-07-13 10:07:05","","10.1785/0220200305","","",,,,,4,4.00,1,5,1,"  Small-magnitude earthquakes shed light on the spatial and magnitude distribution of natural seismicity, as well as its rate and occurrence, especially in stable continental regions where natural seismicity remains difficult to explain under slow strain-rate conditions. However, capturing them in catalogs is strongly hindered by signal-to-noise ratio issues, resulting in high rates of false and man-made events also being detected. Accurate and robust discrimination of these events is critical for optimally detecting small earthquakes. This requires uncovering recurrent salient features that can rapidly distinguish first false events from real events, then earthquakes from man-made events (mainly quarry blasts), despite high signal variability and noise content. In this study, we combined the complementary strengths of human and interpretable rule-based machine-learning algorithms for solving this classification problem. We used human expert knowledge to co-create two reliable machine-learning classifiers through human-assisted selection of classification features and review of events with uncertain classifier predictions. The two classifiers are integrated into the SeisComP3 operational monitoring system. The first one discards false events from the set of events obtained with a low short-term average/long-term average threshold; the second one labels the remaining events as either earthquakes or quarry blasts. When run in an operational setting, the first classifier correctly detected more than 99% of false events and just over 93% of earthquakes; the second classifier correctly labeled 95% of quarry blasts and 96% of earthquakes. After a manual review of the second classifier low-confidence outputs, the final catalog contained fewer than 2% of misclassified events. These results confirm that machine learning strengthens the quality of earthquake catalogs and that the performance of machine-learning classifiers can be improved through human expertise. Our study promotes a broader implication of hybrid intelligence monitoring within seismological observatories.","",""
37,"Efstathios D. Gennatas, J. Friedman, L. Ungar, R. Pirracchio, Eric Eaton, L. Reichman, Y. Interian, C. Simone, A. Auerbach, E. Delgado, M. J. Laan, T. Solberg, G. Valdes","Expert-augmented machine learning",2019,"","","","",34,"2022-07-13 10:07:05","","10.1073/pnas.1906831117","","",,,,,37,12.33,4,13,3,"Significance Machine learning is increasingly used across fields to derive insights from data, which further our understanding of the world and help us anticipate the future. The performance of predictive modeling is dependent on the amount and quality of available data. In practice, we rely on human experts to perform certain tasks and on machine learning for others. However, the optimal learning strategy may involve combining the complementary strengths of humans and machines. We present expert-augmented machine learning, an automated way to automatically extract problem-specific human expert knowledge and integrate it with machine learning to build robust, dependable, and data-efficient predictive models. Machine learning is proving invaluable across disciplines. However, its success is often limited by the quality and quantity of available data, while its adoption is limited by the level of trust afforded by given models. Human vs. machine performance is commonly compared empirically to decide whether a certain task should be performed by a computer or an expert. In reality, the optimal learning strategy may involve combining the complementary strengths of humans and machines. Here, we present expert-augmented machine learning (EAML), an automated method that guides the extraction of expert knowledge and its integration into machine-learned models. We used a large dataset of intensive-care patient data to derive 126 decision rules that predict hospital mortality. Using an online platform, we asked 15 clinicians to assess the relative risk of the subpopulation defined by each rule compared to the total sample. We compared the clinician-assessed risk to the empirical risk and found that, while clinicians agreed with the data in most cases, there were notable exceptions where they overestimated or underestimated the true risk. Studying the rules with greatest disagreement, we identified problems with the training data, including one miscoded variable and one hidden confounder. Filtering the rules based on the extent of disagreement between clinician-assessed risk and empirical risk, we improved performance on out-of-sample data and were able to train with less data. EAML provides a platform for automated creation of problem-specific priors, which help build robust and dependable machine-learning models in critical applications.","",""
3,"Dongsheng Xiao, Brandon Jonathan Forys, M. Vanni, T. Murphy","MesoNet allows automated scaling and segmentation of mouse mesoscale cortical maps using machine learning",2021,"","","","",35,"2022-07-13 10:07:05","","10.1038/s41467-021-26255-2","","",,,,,3,3.00,1,4,1,"","",""
3,"Fei Xu, Fanzhou Kong, Hong Peng, S. Dong, Weiyu Gao, Guangtao Zhang","Combing machine learning and elemental profiling for geographical authentication of Chinese Geographical Indication (GI) rice",2021,"","","","",36,"2022-07-13 10:07:05","","10.1038/s41538-021-00100-8","","",,,,,3,3.00,1,6,1,"","",""
2,"B. Ouyang, Yu Song, Yuhai Li, Feishu Wu, Huizi Yu, Yongzhe Wang, Zhanyuan Yin, X. L. Luo, G. Sant, M. Bauchy","Using machine learning to predict concrete’s strength: learning from small datasets",2021,"","","","",37,"2022-07-13 10:07:05","","10.1088/2631-8695/abe344","","",,,,,2,2.00,0,10,1,"Despite previous efforts to map the proportioning of a concrete to its strength, a robust knowledge-based model enabling accurate strength predictions is still lacking. As an alternative to physical or chemical-based models, data-driven machine learning methods offer a promising pathway to address this problem. Although machine learning can infer the complex, non-linear, non-additive relationship between concrete mixture proportions and strength, large datasets are needed to robustly train such models. This is a concern as reliable concrete strength data is rather limited, especially for realistic industrial concretes. Here, based on the analysis of a fairly large dataset (>10,000 observations) of measured compressive strengths from industrial concretes, we compare the ability of three selected machine learning algorithms (polynomial regression, artificial neural network, random forest) to reliably predict concrete strength as a function of the size of the training dataset. In addition, by adopting stratified sampling, we investigate the influence of the representativeness of the training datapoints on the learning capability of the models considered herein. Based on these results, we discuss the nature of the competition between how accurate a given model can eventually be (when trained on a large dataset) and how much data is actually required to train this model.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",38,"2022-07-13 10:07:05","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
19,"Yannick Suter, U. Knecht, Mariana Alão, W. Valenzuela, E. Hewer, P. Schucht, R. Wiest, M. Reyes","Radiomics for glioblastoma survival analysis in pre-operative MRI: exploring feature robustness, class boundaries, and machine learning techniques",2020,"","","","",39,"2022-07-13 10:07:05","","10.1186/s40644-020-00329-8","","",,,,,19,9.50,2,8,2,"","",""
3,"Wei Huang, Jing Zhao, Guokuan Yu, P. Wong","Intelligent Vibration Control for Semiactive Suspension Systems Without Prior Knowledge of Dynamical Nonlinear Damper Behaviors Based on Improved Extreme Learning Machine",2020,"","","","",40,"2022-07-13 10:07:05","","10.1109/tmech.2020.3031840","","",,,,,3,1.50,1,4,2,"Aiming at enhancing the vehicle comfort and handling performances, this article concerns with the development of the vibration control method for the semiactive suspension systems installed with electrohydraulic dampers. To reject the external disturbances induced by the irregular road profile, sliding mode was intensively investigated for vibration control owing to its robust feature of insensitivity to the parametric uncertainty and external disturbances. However, the unknown nonlinearity of damper behaviors leads to model mismatch to cause the high-frequency switching of sliding-mode controllers. Consequently, the severe chattering phenomenon produces. Although saturated function is available to alleviate the chattering problem of sliding-mode control, there is always a tradeoff problem between tracking accuracy and chattering suppression. To solve this problem, this study provides a new intelligent robust control method for simultaneous improvements of tracking accuracy and chattering suppression. Given the computational efficiency, an improved extreme learning machine (ELM) is proposed to intelligently approximate and compensate the unmodeled dynamics with unknown nonlinearity to restrain the chattering problem, where a new adaptive learning law is designed in the premise of Lyapunov stability. To validate the effectiveness and efficiency of the proposed ELM-based robust control, a quarter-car test rig was set up for the hardware-in-the-loop test. Experimental results show that the proposed controller outperforms the sliding-mode controller with saturated function in depressing the sprung mass acceleration and tire deflection, showing its significance in both control performance enhancement and chattering elimination.","",""
87,"G. Peng, M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, Savador Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Multiscale modeling meets machine learning: What can we learn?",2019,"","","","",41,"2022-07-13 10:07:05","","10.1007/s11831-020-09405-5","","",,,,,87,29.00,9,12,3,"","",""
12,"H. Sufriyana, Yu-Wei Wu, E. C. Su","Prediction of Preeclampsia and Intrauterine Growth Restriction: Development of Machine Learning Models on a Prospective Cohort",2020,"","","","",42,"2022-07-13 10:07:05","","10.2196/15411","","",,,,,12,6.00,4,3,2,"Background Preeclampsia and intrauterine growth restriction are placental dysfunction–related disorders (PDDs) that require a referral decision be made within a certain time period. An appropriate prediction model should be developed for these diseases. However, previous models did not demonstrate robust performances and/or they were developed from datasets with highly imbalanced classes. Objective In this study, we developed a predictive model of PDDs by machine learning that uses features at 24-37 weeks’ gestation, including maternal characteristics, uterine artery (UtA) Doppler measures, soluble fms-like tyrosine kinase receptor-1 (sFlt-1), and placental growth factor (PlGF). Methods A public dataset was taken from a prospective cohort study that included pregnant women with PDDs (66/95, 69%) and a control group (29/95, 31%). Preliminary selection of features was based on a statistical analysis using SAS 9.4 (SAS Institute). We used Weka (Waikato Environment for Knowledge Analysis) 3.8.3 (The University of Waikato, Hamilton, NZ) to automatically select the best model using its optimization algorithm. We also manually selected the best of 23 white-box models. Models, including those from recent studies, were also compared by interval estimation of evaluation metrics. We used the Matthew correlation coefficient (MCC) as the main metric. It is not overoptimistic to evaluate the performance of a prediction model developed from a dataset with a class imbalance. Repeated 10-fold cross-validation was applied. Results The classification via regression model was chosen as the best model. Our model had a robust MCC (.93, 95% CI .87-1.00, vs .64, 95% CI .57-.71) and specificity (100%, 95% CI 100-100, vs 90%, 95% CI 90-90) compared to each metric of the best models from recent studies. The sensitivity of this model was not inferior (95%, 95% CI 91-100, vs 100%, 95% CI 92-100). The area under the receiver operating characteristic curve was also competitive (0.970, 95% CI 0.966-0.974, vs 0.987, 95% CI 0.980-0.994). Features in the best model were maternal weight, BMI, pulsatility index of the UtA, sFlt-1, and PlGF. The most important feature was the sFlt-1/PlGF ratio. This model used an M5P algorithm consisting of a decision tree and four linear models with different thresholds. Our study was also better than the best ones among recent studies in terms of the class balance and the size of the case class (66/95, 69%, vs 27/239, 11.3%). Conclusions Our model had a robust predictive performance. It was also developed to deal with the problem of a class imbalance. In the context of clinical management, this model may improve maternal mortality and neonatal morbidity and reduce health care costs.","",""
25,"H. Yoon, Jae-Hoon Sim, M. Han","Analytic continuation via domain knowledge free machine learning",2018,"","","","",43,"2022-07-13 10:07:05","","10.1103/PhysRevB.98.245101","","",,,,,25,6.25,8,3,4,"We present a machine-learning approach to a long-standing issue in quantum many-body physics, namely, analytic continuation. This notorious ill-conditioned problem of obtaining spectral function from an imaginary time Green's function has been a focus of new method developments for past decades. Here we demonstrate the usefulness of modern machine-learning techniques including convolutional neural networks and the variants of a stochastic gradient descent optimizer. The machine-learning continuation kernel is successfully realized without any ``domain knowledge,'' which means that any physical ``prior'' is not utilized in the kernel construction and the neural networks ``learn'' the knowledge solely from ``training.'' The outstanding performance is achieved for both insulating and metallic band structure. Our machine-learning-based approach not only provides the more accurate spectrum than the conventional methods in terms of peak positions and heights, but is also more robust against the noise which is the required key feature for any continuation technique to be successful. Furthermore, its computation speed is ${10}^{4}\text{--}{10}^{5}$ times faster than the maximum entropy method.","",""
7,"Jina Suh, S. Ghorashi, Gonzalo A. Ramos, N. Chen, S. Drucker, J. Verwey, P. Simard","AnchorViz: Facilitating Semantic Data Exploration and Concept Discovery for Interactive Machine Learning",2019,"","","","",44,"2022-07-13 10:07:05","","10.1145/3241379","","",,,,,7,2.33,1,7,3,"When building a classifier in interactive machine learning (iML), human knowledge about the target class can be a powerful reference to make the classifier robust to unseen items. The main challenge lies in finding unlabeled items that can either help discover or refine concepts for which the current classifier has no corresponding features (i.e., it has feature blindness). Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare concepts that are hard to recall. This article presents AnchorViz, an interactive visualization that facilitates the discovery of prediction errors and previously unseen concepts through human-driven semantic data exploration. By creating example-based or dictionary-based anchors representing concepts, users create a topology that (a) spreads data based on their similarity to the concepts and (b) surfaces the prediction and label inconsistencies between data points that are semantically related. Once such inconsistencies and errors are discovered, users can encode the new information as labels or features and interact with the retrained classifier to validate their actions in an iterative loop. We evaluated AnchorViz through two user studies. Our results show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods. Furthermore, during the beginning stages of a training task, an iML tool with AnchorViz can help users build classifiers comparable to the ones built with the same tool with uncertainty sampling and keyword search, but with fewer labels and more generalizable features. We discuss exploration strategies observed during the two studies and how AnchorViz supports discovering, labeling, and refining of concepts through a sensemaking loop.","",""
6,"Syed Javeed Pasha, E. Mohamed","Novel Feature Reduction (NFR) Model With Machine Learning and Data Mining Algorithms for Effective Disease Risk Prediction",2020,"","","","",45,"2022-07-13 10:07:05","","10.1109/ACCESS.2020.3028714","","",,,,,6,3.00,3,2,2,"Presently, the application of machine learning (ML) and data mining (DM) techniques have a vital role in healthcare systems and wisely convert all obtainable data into beneficial knowledge. It is proven from the literature works that a chance of 12% error remains in the diagnosis of the diseases by the medical practitioners. Moreover, for effective disease risk prediction in medical analysis, more emphasis is accorded to the area under the curve (AUC) with accuracy as an evaluation metric. However, the role of the AUC has not been previously characterized notably. In this research article, a novel feature reduction (NFR) model that is aligned with the ML and DM algorithms is proposed to reduce the error rate and further improve the performance. The proposed NFR model comprises of two approaches and uses the AUC in addition to the accuracy to achieve a robust and effective disease risk prediction. The first approach is based on a heuristic process evaluating performance by reducing features with respect to the improvement in the AUC besides the accuracy as evaluation metrics, working to obtain the best subset of highly contributing features in the prediction. The second approach evaluates the accuracy and AUC of all individual features and forms the subsets with the highest accuracies, AUCs, and least difference between them, which are combined in various combinations to achieve the best-reduced set of highly relevant features. For this purpose, the benchmarked public heart datasets of the ML repository of the University of California, Irvine (UCI) are tested; the results are promising. The highest accuracy and AUC achieved with the proposed NFR model are 95.52% and 99.20% with 41.67% feature reduction, respectively. The accuracy is 4.22% higher than recent existing research with a significant improvement of 25% in the performance of the running time of the algorithm.","",""
6,"Wienand A. Omta, Roy G. van Heesbeen, I. Shen, Jacob de Nobel, D. Robers, Lieke M. van der Velden, R. Medema, A. Siebes, A. Feelders, S. Brinkkemper, J. Klumperman, M. Spruit, Matthieu J. S. Brinkhuis, D. Egan","Combining Supervised and Unsupervised Machine Learning Methods for Phenotypic Functional Genomics Screening",2020,"","","","",46,"2022-07-13 10:07:05","","10.1177/2472555220919345","","",,,,,6,3.00,1,14,2,"There has been an increase in the use of machine learning and artificial intelligence (AI) for the analysis of image-based cellular screens. The accuracy of these analyses, however, is greatly dependent on the quality of the training sets used for building the machine learning models. We propose that unsupervised exploratory methods should first be applied to the data set to gain a better insight into the quality of the data. This improves the selection and labeling of data for creating training sets before the application of machine learning. We demonstrate this using a high-content genome-wide small interfering RNA screen. We perform an unsupervised exploratory data analysis to facilitate the identification of four robust phenotypes, which we subsequently use as a training set for building a high-quality random forest machine learning model to differentiate four phenotypes with an accuracy of 91.1% and a kappa of 0.85. Our approach enhanced our ability to extract new knowledge from the screen when compared with the use of unsupervised methods alone.","",""
1,"Anay Raj","Malaria Disease Diagnosis using Machine Learning Techniques",2020,"","","","",47,"2022-07-13 10:07:05","","","","",,,,,1,0.50,1,1,2,"Malaria is a major infectious disease of humans, with roughly 200 million cases worldwide and more than 400,000 deaths per year. Malaria could be prevented, controlled, and cured more effectively if a more accurate and efficient diagnostic method was available. The standard diagnostic method for malaria is the microscopic examination of blood smears for infected erythrocytes by qualified microscopists. However, this method is inefficient and the quality of the diagnosis depends on the experience and knowledge of the microscopists. This study proposes a new and robust machine learning model based on a Convolutional neural network (CNN) to automatically classify single cells in thin blood smears on standard microscope slides as either infected or uninfected. This will help in the faster diagnosis of malaria and save valuable time for beginning the treatment.","",""
5,"L. Heikkinen, M. Äijälä, K. Daellenbach, Gang Chen, O. Garmash, D. Aliaga, F. Graeffe, M. Räty, K. Luoma, P. Aalto, M. Kulmala, T. Petäjä, D. Worsnop, M. Ehn","Eight years of sub-micrometre organic aerosol composition data from the boreal forest characterized using a machine-learning approach",2020,"","","","",48,"2022-07-13 10:07:05","","10.5194/acp-2020-868","","",,,,,5,2.50,1,14,2,"Abstract. The Station for Measuring Ecosystem Atmosphere Relations (SMEAR) II is a unique station in the world due to the wide range of long-term measurements tracking the Earth-atmosphere interface. In this study, we characterize the composition of organic aerosol (OA) at SMEAR II by quantifying its driving constituents. We utilize a multi-year data set of OA mass spectra measured in situ with an Aerosol Chemical Speciation Monitor (ACSM) at the station. To our knowledge, this mass spectral time series is the longest of its kind published to date, and its detailed analysis required development of a new methodology. To this purpose, we developed an efficient and robust data analysis framework utilizing machine learning tools. These included unsupervised feature extraction and classification stages to manage and process the large amounts of data. The extensive chemometric analysis was conducted with a combination of Positive Matrix Factorization (PMF), rolling window analysis, bootstrapping, K-Means clustering, data weighting and diagnostics based algorithmic choice-making, among others. This combination of statistical tools provided a data driven analysis methodology to achieve robust solutions with minimal subjectivity. Following the extensive statistical analyses, we were able to divide the 2012–2019 SMEAR II OA data (mass concentration interquartile range (IQR): 0.7, 1.3, 2.6 µg m−3) to three sub-categories: low-volatility oxygenated OA (LV-OOA), semi-volatile oxygenated OA (SV-OOA), and primary OA (POA). LV-OOA was the most dominant OA type (organic mass fraction IQR: 49, 62, and 73 %). The seasonal cycle of LV-OOA was bimodal, with peaks both in summer and in February. We associated the wintertime LV-OOA with anthropogenic sources and assumed biogenic influence in LV-OOA formation in summer. Through a brief trajectory analysis, we estimated summertime natural LV-OOA formation of tens of ng m−3 h−1 over the boreal forest. SV-OOA was the second highest contributor to OA mass (organic mass fraction IQR: 19, 31, and 43 %). Due to SV-OOA’s clear peak in summer, we estimate biogenic processes as the main drivers in its formation. Unlike for LV-OOA, the highest SV-OOA concentrations were detected in stable summertime nocturnal surface layers. However, also the nearby sawmills likely played a significant role in SV-OOA production as also exemplified by previous studies at SMEAR II. POA, taken as a mix of two different OA types reported previously, hydrocarbon-like OA (HOA) and biomass burning OA (BBOA), made up a minimal OA mass fraction (IQR: 2, 6, and 13 %). Both POA organic mass fraction and mass concentration peaked in winter. Its appearance at SMEAR II was linked to strong southerly winds. The high wind speeds probably enabled the POA transport to SMEAR II from faraway sources in a relatively fresh state. In case of slower wind speeds, POA likely evaporated or aged into oxidized organic aerosol before detection. The POA organic mass fraction was significantly lower than reported by aerosol mass spectrometer (AMS) measurements two to four years prior to the ACSM measurements. While the co-located long-term measurements of black carbon supported the hypothesis of higher POA loadings prior to year 2012, it is also possible that ACSM was less efficiently capturing short term (POA) pollution plumes. Despite the length of the ACSM data set, we did not focus on quantifying long-term trends of POA (nor other components) due to the high sensitivity of OA composition to meteorological anomalies, the occurrence of which is likely not normally distributed over the eight year measurement period. We hope that our successfully applied methodology encourages also other researchers possessing several-year-long time series of similar data to tackle the data analysis via similar semi- or unsupervised machine learning approaches. This way aerosol chemometric analysis procedures would be further developed into yet more streamlined and autonomous directions. ","",""
1,"B. Ouyang, Yuhai Li, Yu Song, Feishu Wu, Huizi Yu, Yongzhe Wang, M. Bauchy, G. Sant","Learning from Sparse Datasets: Predicting Concrete's Strength by Machine Learning",2020,"","","","",49,"2022-07-13 10:07:05","","","","",,,,,1,0.50,0,8,2,"Despite enormous efforts over the last decades to establish the relationship between concrete proportioning and strength, a robust knowledge-based model for accurate concrete strength predictions is still lacking. As an alternative to physical or chemical-based models, data-driven machine learning (ML) methods offer a new solution to this problem. Although this approach is promising for handling the complex, non-linear, non-additive relationship between concrete mixture proportions and strength, a major limitation of ML lies in the fact that large datasets are needed for model training. This is a concern as reliable, consistent strength data is rather limited, especially for realistic industrial concretes. Here, based on the analysis of a large dataset (>10,000 observations) of measured compressive strengths from industrially-produced concretes, we compare the ability of select ML algorithms to ""learn"" how to reliably predict concrete strength as a function of the size of the dataset. Based on these results, we discuss the competition between how accurate a given model can eventually be (when trained on a large dataset) and how much data is actually required to train this model.","",""
12,"N. Khoa, M. M. Alamdari, T. Rakotoarivelo, Ali Anaissi, Yang Wang","Structural Health Monitoring Using Machine Learning Techniques and Domain Knowledge Based Features",2018,"","","","",50,"2022-07-13 10:07:05","","10.1007/978-3-319-90403-0_20","","",,,,,12,3.00,2,5,4,"","",""
8,"Ali Hürriyetoǧlu, Erdem Yörük, Deniz Yuret, Osman Mutlu, Çağrı Yoltar, Fırat Duruşan, Burak Gürel","Cross-context News Corpus for Protest Events related Knowledge Base Construction",2020,"","","","",51,"2022-07-13 10:07:05","","10.24432/C5D59R","","",,,,,8,4.00,1,7,2,"We describe a gold standard corpus of protest events that comprise of various local and international sources from various countries in English. The corpus contains document, sentence, and token level annotations. This corpus facilitates creating machine learning models that automatically classify news articles and extract protest event related information, constructing databases which enable comparative social and political science studies. For each news source, the annotation starts on random samples of news articles and continues with samples that are drawn using active learning. Each batch of samples was annotated by two social and political scientists, adjudicated by an annotation supervisor, and was improved by identifying annotation errors semi-automatically. We found that the corpus has the variety and quality to develop and benchmark text classification and event extraction systems in a cross-context setting, which contributes to generalizability and robustness of automated text processing systems. This corpus and the reported results will set the currently lacking common ground in automated protest event collection studies.","",""
4,"Shuteng Niu, Jian Wang, Yongxin Liu, H. Song","Transfer Learning based Data-Efficient Machine Learning Enabled Classification",2020,"","","","",52,"2022-07-13 10:07:05","","10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00108","","",,,,,4,2.00,1,4,2,"Recently, waste sorting has become more and more important in our daily life. It plays an essential role in the big picture of waste recycling, reducing environmental pollution significantly. Deep learning (DL) methods have been dominating the field of image classification and have been successfully applied to waste sorting tasks to achieve state-of-art performance. However, most traditional DL methods require a massive amount of annotated data for the training phase. Unfortunately, there is only one small data set for waste sorting, TrashNet created by Standford. In addition, manually collecting and labeling a massive data-set can be too costly. To address this issue, we decided to implement transfer learning (TL) techniques to construct a robust model based on a fairly small set of training data by transferring knowledge from existing deep networks, such as AlexNet, Resnet, and DensNet. As an innovation, we propose a novel domain loss function, Dual Dynamic Domain Distance (4D), to produce a more accurate domain distance measurement. There are three contributions to this paper. First, our model has achieved the best performance on the TrashNet data. Secondly, it is the first time that TL has been used for waste sorting. Finally, the proposed novel 4D domain loss has improved the performance of TL for this task. In this paper, we implemented two types of transfer learning methods, DDC, DeepCoral, to TrashNet data-set. Moreover, the DeepCoral-Resnet50 model yields the best performance of 96% test accuracy. More importantly, this work can be easily generalized to other image classification tasks.","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",53,"2022-07-13 10:07:05","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
10,"Yifan Cui, E. Tchetgen","Bias-aware model selection for machine learning of doubly robust functionals",2019,"","","","",54,"2022-07-13 10:07:05","","","","",,,,,10,3.33,5,2,3,"While model selection is a well-studied topic in parametric and nonparametric regression or density estimation, model selection of possibly high dimensional nuisance parameters in semiparametric problems is far less developed. In this paper, we propose a new model selection framework for making inferences about a finite dimensional functional defined on a semiparametric model, when the latter admits a doubly robust estimating function. The class of such doubly robust functionals is quite large, including many missing data and causal inference problems. Under double robustness, the estimated functional should incur no bias if either of two nuisance parameters is evaluated at the truth while the other spans a large collection of candidate models. We introduce two model selection criteria for bias reduction of functional of interest, each based on a novel definition of pseudo-risk for the functional that embodies this double robustness property and thus may be used to select the candidate model that is nearest to fulfilling this property even when all models are wrong. Both selection criteria have a bias awareness property that selection of one nuisance parameter can be made to compensate for excessive bias due to poor learning of the other nuisance parameter. We establish an oracle property for a multi-fold cross-validation version of the new model selection criteria which states that our empirical criteria perform nearly as well as an oracle with a priori knowledge of the pseudo-risk for each candidate model. We also describe a smooth approximation to the selection criteria which allows for valid post-selection inference. Finally, we apply the approach to perform model selection of a semiparametric estimator of average treatment effect given an ensemble of candidate machine learning methods to account for confounding in a study of right heart catheterization in the intensive care unit of critically ill patients.","",""
2,"Jiafei Zhao, Rongkun Jiang, Xuetian Wang, Hongmin Gao","Robust CFAR Detection for Multiple Targets in K-Distributed Sea Clutter Based on Machine Learning",2019,"","","","",55,"2022-07-13 10:07:05","","10.3390/sym11121482","","",,,,,2,0.67,1,4,3,"For K-distributed sea clutter, a constant false alarm rate (CFAR) is crucial as a desired property for automatic target detection in an unknown and non-stationary background. In multiple-target scenarios, the target masking effect reduces the detection performance of CFAR detectors evidently. A machine learning based processor, associating the artificial neural network (ANN) and a clustering algorithm of density-based spatial clustering of applications with noise (DBSCAN), namely, DBSCAN-CFAR, is proposed herein to address this issue. ANN is trained with a symmetrical structure to estimate the shape parameter of background clutter, whereas DBSCAN is devoted to excluding interference targets and sea spikes as outliers in the leading and lagging windows that are symmetrical about the cell under test (CUT). Simulation results verified that the ANN-based method provides the optimal parameter estimation results in the range of 0.1 to 30, which facilitates the control of actual false alarm probability. The effectiveness and robustness of DBSCAN-CFAR are also confirmed by the comparisons of conventional CFAR processors in different clutter conditions, comprised of varying target numbers, shape parameters, and false alarm probabilities. Although the proposed ANN-based DBSCAN-CFAR processor incurs more elapsed time, it achieves superior CFAR performance without a prior knowledge on the number and distribution of interference targets.","",""
1,"Youhao Hu, Hai Wang, Z. Cao, Z. Man, Ming Yu, Zhaowu Ping","Robust fast nonsingular terminal sliding mode control strategy for electronic throttle based on extreme learning machine",2019,"","","","",56,"2022-07-13 10:07:05","","10.23919/ChiCC.2019.8866147","","",,,,,1,0.33,0,6,3,"This paper proposes an extreme-learning-machine-based robust fast nonsingular terminal sliding mode control (FNTSMC) strategy for an electronic throttle (ET) system. Distinguished from the conventional implementations of sliding mode control (SMC), the prior knowledge of disturbance bound is not required but estimated by the novel neural networks titled as extreme learning machine (ELM) which features in the fast learning rate and excellent generalization. The unique of the proposed control strategy lies on that both the sliding variable and system state enjoy a finite-time convergence without the information of predetermined bound of system nonlinearities and disturbances. The comparative simulations are conducted to verify the effectiveness and robustness of the proposed control strategy.","",""
52,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",57,"2022-07-13 10:07:05","","10.1145/3219617.3219655","","",,,,,52,10.40,17,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q łe m for an arbitrarily small but fixed constant ε>0. The parameter estimate converges in O(łog N) rounds with an estimation error on the order of max √dq/N, ~√d/N , which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q . The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log 3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
4,"Esther Heid, W. Green","Machine Learning of Reaction Properties via Learned Representations of the Condensed Graph of Reaction",2021,"","","","",58,"2022-07-13 10:07:05","","10.1021/acs.jcim.1c00975","","",,,,,4,4.00,2,2,1,"The estimation of chemical reaction properties such as activation energies, rates, or yields is a central topic of computational chemistry. In contrast to molecular properties, where machine learning approaches such as graph convolutional neural networks (GCNNs) have excelled for a wide variety of tasks, no general and transferable adaptations of GCNNs for reactions have been developed yet. We therefore combined a popular cheminformatics reaction representation, the so-called condensed graph of reaction (CGR), with a recent GCNN architecture to arrive at a versatile, robust, and compact deep learning model. The CGR is a superposition of the reactant and product graphs of a chemical reaction and thus an ideal input for graph-based machine learning approaches. The model learns to create a data-driven, task-dependent reaction embedding that does not rely on expert knowledge, similar to current molecular GCNNs. Our approach outperforms current state-of-the-art models in accuracy, is applicable even to imbalanced reactions, and possesses excellent predictive capabilities for diverse target properties, such as activation energies, reaction enthalpies, rate constants, yields, or reaction classes. We furthermore curated a large set of atom-mapped reactions along with their target properties, which can serve as benchmark data sets for future work. All data sets and the developed reaction GCNN model are available online, free of charge, and open source.","",""
20,"Shichao Pei, Lu Yu, Guoxian Yu, Xiangliang Zhang","REA: Robust Cross-lingual Entity Alignment Between Knowledge Graphs",2020,"","","","",59,"2022-07-13 10:07:05","","10.1145/3394486.3403268","","",,,,,20,10.00,5,4,2,"Cross-lingual entity alignment aims at associating semantically similar entities in knowledge graphs with different languages. It has been an essential research problem for knowledge integration and knowledge graph connection, and been studied with supervised or semi-supervised machine learning methods with the assumption of clean labeled data. However, labels from human annotations often include errors, which can largely affect the alignment results. We thus aim to formulate and explore the robust entity alignment problem, which is non-trivial, due to the deficiency of noisy labels. Our proposed method named REA (Robust Entity Alignment) consists of two components: noise detection and noise-aware entity alignment. The noise detection is designed by following the adversarial training principle. The noise-aware entity alignment is devised by leveraging graph neural network based knowledge graph encoder as the core. In order to mutually boost the performance of the two components, we propose a unified reinforced training strategy to combine them. To evaluate our REA method, we conduct extensive experiments on several real-world datasets. The experimental results demonstrate the effectiveness of our proposed method and also show that our model consistently outperforms the state-of-the-art methods with significant improvement on alignment accuracy in the noise-involved scenario.","",""
658,"Wieland Brendel, Jonas Rauber, M. Bethge","Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",2017,"","","","",60,"2022-07-13 10:07:05","","","","",,,,,658,131.60,219,3,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .","",""
1,"Alina Oprea","Machine Learning Integrity and Privacy in Adversarial Environments",2021,"","","","",61,"2022-07-13 10:07:05","","10.1145/3450569.3462164","","",,,,,1,1.00,1,1,1,"Machine learning is increasingly being used for automated decisions in applications such as health care, finance, autonomous vehicles, and personalized recommendations. These critical applications require strong guarantees on both the integrity of the machine learning models and the privacy of the user data used to train these models. The area of adversarial machine learning studies the effect of adversarial attacks against machine learning models and aims to design robust defense algorithms. The main challenges in this space are the development of realistic adversarial models that consider the specifics of real-world applications, and the design of machine learning algorithms resilient against a wide range of threats. In this talk, we describe our work on creating a taxonomy of poisoning attacks against machine learning systems at training time. In light of recent software supply chain vulnerabilities revealed by the SolarWinds attack, the supply chain of machine learning development needs to be protected. First, we introduce our optimization approach to create poisoning availability attacks against linear regression and discuss robust defenses based on techniques from robust statistics [1]. Then, we discuss how an attacker with minimal knowledge of a machine learning classifier can inject backdoor poisoning attacks, by leveraging techniques from machine learning explainability [4]. We demonstrate these methods on several malware classifiers and show the challenges of designing robust defenses to protect against these attacks. We also define a new attack model called subpopulation poisoning, that requires a small set of poisoning points to impact the accuracy of the model on a targeted subpopulation [2]. We evaluate our poisoning attacks on multiple data modalities, including image, text, and tabular data. Finally, we highlight a surprising connection between machine learning integrity and privacy attacks, and show how poisoning attacks can be used for auditing the privacy of machine learning algorithms such as differentially private stochastic gradient descent [3].","",""
9,"Kexin Pei, Jonas Guan, David Williams-King, Junfeng Yang, S. Jana","XDA: Accurate, Robust Disassembly with Transfer Learning",2020,"","","","",62,"2022-07-13 10:07:05","","10.14722/NDSS.2021.23112","","",,,,,9,4.50,2,5,2,"Accurate and robust disassembly of stripped binaries is challenging. The root of the difficulty is that high-level structures, such as instruction and function boundaries, are absent in stripped binaries and must be recovered based on incomplete information. Current disassembly approaches rely on heuristics or simple pattern matching to approximate the recovery, but these methods are often inaccurate and brittle, especially across different compiler optimizations.  We present XDA, a transfer-learning-based disassembly framework that learns different contextual dependencies present in machine code and transfers this knowledge for accurate and robust disassembly. We design a self-supervised learning task motivated by masked Language Modeling to learn interactions among byte sequences in binaries. The outputs from this task are byte embeddings that encode sophisticated contextual dependencies between input binaries' byte tokens, which can then be finetuned for downstream disassembly tasks.  We evaluate XDA's performance on two disassembly tasks, recovering function boundaries and assembly instructions, on a collection of 3,121 binaries taken from SPEC CPU2017, SPEC CPU2006, and the BAP corpus. The binaries are compiled by GCC, ICC, and MSVC on x86/x64 Windows and Linux platforms over 4 optimization levels. XDA achieves 99.0% and 99.7% F1 score at recovering function boundaries and instructions, respectively, surpassing the previous state-of-the-art on both tasks. It also maintains speed on par with the fastest ML-based approach and is up to 38x faster than hand-written disassemblers like IDA Pro.","",""
19,"T. Le, M. Penna, D. Winkler, I. Yarovsky","Quantitative design rules for protein-resistant surface coatings using machine learning",2019,"","","","",63,"2022-07-13 10:07:05","","10.1038/s41598-018-36597-5","","",,,,,19,6.33,5,4,3,"","",""
18,"P. Fusar-Poli, Dominic Stringer, Alice M. S. Durieux, G. Rutigliano, I. Bonoldi, A. De Micheli, D. Ståhl","Clinical-learning versus machine-learning for transdiagnostic prediction of psychosis onset in individuals at-risk",2019,"","","","",64,"2022-07-13 10:07:05","","10.1038/s41398-019-0600-9","","",,,,,18,6.00,3,7,3,"","",""
7,"M. A. Ganaie, M. Tanveer, P. Suganthan","Regularized robust fuzzy least squares twin support vector machine for class imbalance learning",2020,"","","","",65,"2022-07-13 10:07:05","","10.1109/IJCNN48605.2020.9207724","","",,,,,7,3.50,2,3,2,"Twin support vector machines (TWSVM) have been successfully applied to the classification problems. TWSVM is computationally efficient model of support vector machines (SVM). However, in real world classification problems issues of class imbalance and noise provide great challenges. Due to this, models lead to the inaccurate classification either due to higher tendency towards the majority class or due to the presence of noise. We provide an improved version of robust fuzzy least squares twin support vector machine (RFLSTSVM) known as regularized robust fuzzy least squares twin support vector machine (RRFLSTSVM) to handle the imbalance problem. The advantage of RRFLSTSVM over RFLSTSVM is that the proposed RRFLSTSVM implements the structural risk minimization principle by the introduction of regularization term in the primal formulation of the objective functions. This modification leads to the improved classification as it embodies the marrow of statistical learning theory. The proposed RRFLSTSVM doesn’t require any extra assumption as the matrices resulting in the dual are positive definite. However, RFLSTSVM is based on the assumption that the inverse of the matrices resulting in the dual always exist as the matrices are positive semi-definite. To subsidize the effects of class imbalance and noise, the data samples are assigned weights via fuzzy membership function. The fuzzy membership function incorporates the imbalance ratio knowledge and assigns appropriate weights to the data samples. Unlike TWSVM which solves a pair of quadratic programming problem (QPP), the proposed RRFLSTSVM method solves a pair of system of linear equations and hence is computationally efficient. Experimental and statistical analysis show the efficacy of the proposed RRFLSTSVM method.","",""
24,"Saikat Das, Ph.D., Ahmed M. Mahfouz, D. Venugopal, S. Shiva","DDoS Intrusion Detection Through Machine Learning Ensemble",2019,"","","","",66,"2022-07-13 10:07:05","","10.1109/QRS-C.2019.00090","","",,,,,24,8.00,6,4,3,"Distributed Denial of Service (DDoS) attacks have been the prominent attacks over the last decade. A Network Intrusion Detection System (NIDS) should seamlessly configure to fight against these attackers' new approaches and patterns of DDoS attack. In this paper, we propose a NIDS which can detect existing as well as new types of DDoS attacks. The key feature of our NIDS is that it combines different classifiers using ensemble models, with the idea that each classifier can target specific aspects/types of intrusions, and in doing so provides a more robust defense mechanism against new intrusions. Further, we perform a detailed analysis of DDoS attacks, and based on this domain-knowledge verify the reduced feature set [27, 28] to significantly improve accuracy. We experiment with and analyze NSL-KDD dataset with reduced feature set and our proposed NIDS can detect 99.1% of DDoS attacks successfully. We compare our results with other existing approaches. Our NIDS approach has the learning capability to keep up with new and emerging DDoS attack patterns.","",""
1,"Yuntian Chen, Dongxiao Zhang","Integration of knowledge and data in machine learning",2022,"","","","",67,"2022-07-13 10:07:05","","","","",,,,,1,1.00,1,2,1,"Scientiﬁc research’s mandate is to comprehend and explore the world, as well as to improve it based on experience and knowledge. Knowledge embedding and knowledge discovery are two signif-icant methods of integrating knowledge and data. Through knowledge embedding, the barriers between knowledge and data can be eliminated, and machine learning models with physical common sense can be established. Meanwhile, humans’ understanding of the world is always limited, and knowledge discovery takes advantage of machine learning to extract new knowledge from observations. Knowledge discovery can not only assist researchers to better grasp the nature of physics, but it can also support them in conducting knowledge embedding research. A closed loop of knowledge generation and usage are formed by combining knowledge embedding with knowledge discovery, which can improve the robustness and accuracy of models and uncover previously unknown scientiﬁc principles. This study summarizes and ana-lyzes extant literature, as well as identiﬁes research gaps and future opportunities.","",""
2,"Christian Fiedler, C. Scherer, S. Trimpe","Learning-enhanced robust controller synthesis with rigorous statistical and control-theoretic guarantees",2021,"","","","",68,"2022-07-13 10:07:05","","10.1109/CDC45484.2021.9682836","","",,,,,2,2.00,1,3,1,"The combination of machine learning with control offers many opportunities, in particular for robust control. However, due to strong safety and reliability requirements in many real-world applications, providing rigorous statistical and control-theoretic guarantees is of utmost importance, yet difficult to achieve for learning-based control schemes. We present a general framework for learning-enhanced robust control that allows for systematic integration of prior engineering knowledge, is fully compatible with modern robust control and still comes with rigorous and practically meaningful guarantees. Building on the established Linear Fractional Representation and Integral Quadratic Constraints framework, we integrate Gaussian Process Regression as a learning component and state-of-the-art robust controller synthesis. In a concrete robust control example, our approach is demonstrated to yield improved performance with more data, while guarantees are maintained throughout.","",""
3,"D. Kurrant, M. Omer, Nasim Abdollahi, P. Mojabi, E. Fear, J. Lovetri","Evaluating Performance of Microwave Image Reconstruction Algorithms: Extracting Tissue Types with Segmentation Using Machine Learning",2021,"","","","",69,"2022-07-13 10:07:05","","10.3390/jimaging7010005","","",,,,,3,3.00,1,6,1,"Evaluating the quality of reconstructed images requires consistent approaches to extracting information and applying metrics. Partitioning medical images into tissue types permits the quantitative assessment of regions that contain a specific tissue. The assessment facilitates the evaluation of an imaging algorithm in terms of its ability to reconstruct the properties of various tissue types and identify anomalies. Microwave tomography is an imaging modality that is model-based and reconstructs an approximation of the actual internal spatial distribution of the dielectric properties of a breast over a reconstruction model consisting of discrete elements. The breast tissue types are characterized by their dielectric properties, so the complex permittivity profile that is reconstructed may be used to distinguish different tissue types. This manuscript presents a robust and flexible medical image segmentation technique to partition microwave breast images into tissue types in order to facilitate the evaluation of image quality. The approach combines an unsupervised machine learning method with statistical techniques. The key advantage for using the algorithm over other approaches, such as a threshold-based segmentation method, is that it supports this quantitative analysis without prior assumptions such as knowledge of the expected dielectric property values that characterize each tissue type. Moreover, it can be used for scenarios where there is a scarcity of data available for supervised learning. Microwave images are formed by solving an inverse scattering problem that is severely ill-posed, which has a significant impact on image quality. A number of strategies have been developed to alleviate the ill-posedness of the inverse scattering problem. The degree of success of each strategy varies, leading to reconstructions that have a wide range of image quality. A requirement for the segmentation technique is the ability to partition tissue types over a range of image qualities, which is demonstrated in the first part of the paper. The segmentation of images into regions of interest corresponding to various tissue types leads to the decomposition of the breast interior into disjoint tissue masks. An array of region and distance-based metrics are applied to compare masks extracted from reconstructed images and ground truth models. The quantitative results reveal the accuracy with which the geometric and dielectric properties are reconstructed. The incorporation of the segmentation that results in a framework that effectively furnishes the quantitative assessment of regions that contain a specific tissue is also demonstrated. The algorithm is applied to reconstructed microwave images derived from breasts with various densities and tissue distributions to demonstrate the flexibility of the algorithm and that it is not data-specific. The potential for using the algorithm to assist in diagnosis is exhibited with a tumor tracking example. This example also establishes the usefulness of the approach in evaluating the performance of the reconstruction algorithm in terms of its sensitivity and specificity to malignant tissue and its ability to accurately reconstruct malignant tissue.","",""
8,"Dongqi Han, Zhiliang Wang, Ying Zhong, Wenqi Chen, Jiahai Yang, Shuqiang Lu, Xingang Shi, Xia Yin","Evaluating and Improving Adversarial Robustness of Machine Learning-Based Network Intrusion Detectors",2020,"","","","",70,"2022-07-13 10:07:05","","10.1109/JSAC.2021.3087242","","",,,,,8,4.00,1,8,2,"Machine learning (ML), especially deep learning (DL) techniques have been increasingly used in anomaly-based network intrusion detection systems (NIDS). However, ML/DL has shown to be extremely vulnerable to adversarial attacks, especially in such security-sensitive systems. Many adversarial attacks have been proposed to evaluate the robustness of ML-based NIDSs. Unfortunately, existing attacks mostly focused on feature-space and/or white-box attacks, which make impractical assumptions in real-world scenarios, leaving the study on practical gray/black-box attacks largely unexplored. To bridge this gap, we conduct the first systematic study of the gray/black-box traffic-space adversarial attacks to evaluate the robustness of ML-based NIDSs. Our work outperforms previous ones in the following aspects: (i) practical —the proposed attack can automatically mutate original traffic with extremely limited knowledge and affordable overhead while preserving its functionality; (ii) generic —the proposed attack is effective for evaluating the robustness of various NIDSs using diverse ML/DL models and non-payload-based features; (iii) explainable —we propose an explanation method for the fragile robustness of ML-based NIDSs. Based on this, we also propose a defense scheme against adversarial attacks to improve system robustness. We extensively evaluate the robustness of various NIDSs using diverse feature sets and ML/DL models. Experimental results show our attack is effective (e.g., >97% evasion rate in half cases for Kitsune, a state-of-the-art NIDS) with affordable execution cost and the proposed defense method can effectively mitigate such attacks (evasion rate is reduced by >50% in most cases).","",""
8,"Sakshi Udeshi, Sudipta Chattopadhyay","Grammar Based Directed Testing of Machine Learning Systems",2019,"","","","",71,"2022-07-13 10:07:05","","10.1109/tse.2019.2953066","","",,,,,8,2.67,4,2,3,"The massive progress of machine learning has seen its application over a variety of domains in the past decade. But how do we develop a systematic, scalable and modular strategy to validate machine-learning systems? We present, to the best of our knowledge, the first approach, which provides a systematic test framework for machine-learning systems that accepts grammar-based inputs. Our Ogma approach automatically discovers erroneous behaviours in classifiers and leverages these erroneous behaviours to improve the respective models. Ogma leverages inherent robustness properties present in any well trained machine-learning model to direct test generation and thus, implementing a scalable test generation methodology. To evaluate our Ogma approach, we have tested it on three real world natural language processing (NLP) classifiers. We have found thousands of erroneous behaviours in these systems. We also compare Ogma with a random test generation approach and observe that Ogma is more effective than such random test generation by up to 489 percent.","",""
10,"E. Adabor, G. Acquaah-Mensah","Machine learning approaches to decipher hormone and HER2 receptor status phenotypes in breast cancer",2019,"","","","",72,"2022-07-13 10:07:05","","10.1093/bib/bbx138","","",,,,,10,3.33,5,2,3,"Breast cancer prognosis and administration of therapies are aided by knowledge of hormonal and HER2 receptor status. Breast cancer lacking estrogen receptors, progesterone receptors and HER2 receptors are difficult to treat. Regarding large data repositories such as The Cancer Genome Atlas, available wet-lab methods for establishing the presence of these receptors do not always conclusively cover all available samples. To this end, we introduce median-supplement methods to identify hormonal and HER2 receptor status phenotypes of breast cancer patients using gene expression profiles. In these approaches, supplementary instances based on median patient gene expression are introduced to balance a training set from which we build simple models to identify the receptor expression status of patients. In addition, for the purpose of benchmarking, we examine major machine learning approaches that are also applicable to the problem of finding receptor status in breast cancer. We show that our methods are robust and have high sensitivity with extremely low false-positive rates compared with the well-established methods. A successful application of these methods will permit the simultaneous study of large collections of samples of breast cancer patients as well as save time and cost while standardizing interpretation of outcomes of such studies.","",""
14,"G. Rehm, Jinyoung Han, B. Kuhn, J. Delplanque, N. Anderson, Jason Y. Adams, C. Chuah","Creation of a Robust and Generalizable Machine Learning Classifier for Patient Ventilator Asynchrony.",2018,"","","","",73,"2022-07-13 10:07:05","","10.3414/ME17-02-0012","","",,,,,14,3.50,2,7,4,"BACKGROUND As healthcare increasingly digitizes, streaming waveform data is being made available from an variety of sources, but there still remains a paucity of performant clinical decision support systems. For example, in the intensive care unit (ICU) existing automated alarm systems typically rely on simple thresholding that result in frequent false positives. Recurrent false positive alerts create distrust of alarm mechanisms that can be directly detrimental to patient health. To improve patient care in the ICU, we need alert systems that are both pervasive, and accurate so as to be informative and trusted by providers.   OBJECTIVE We aimed to develop a machine learning-based classifier to detect abnormal waveform events using the use case of mechanical ventilation waveform analysis, and the detection of harmful forms of ventilation delivery to patients. We specifically focused on detecting injurious subtypes of patient-ventilator asynchrony (PVA).   METHODS Using a dataset of breaths recorded from 35 different patients, we used machine learning to create computational models to automatically detect, and classify two types of injurious PVA, double trigger asynchrony (DTA), breath stacking asynchrony (BSA). We examined the use of synthetic minority over-sampling technique (SMOTE) to overcome class imbalance problems, varied methods for feature selection, and use of ensemble methods to optimize the performance of our model.   RESULTS We created an ensemble classifier that is able to accurately detect DTA at a sensitivity/specificity of 0.960/0.975, BSA at sensitivity/specificity of 0.944/0.987, and non-PVA events at sensitivity/specificity of .967/.980.   CONCLUSIONS Our results suggest that it is possible to create a high-performing machine learning-based model for detecting PVA in mechanical ventilator waveform data in spite of both intra-patient, and inter-patient variability in waveform patterns, and the presence of clinical artifacts like cough and suction procedures. Our work highlights the importance of addressing class imbalance in clinical data sets, and the combined use of statistical methods and expert knowledge in feature selection.","",""
46,"F. Yuan, S. A. Zargar, Qiuyi Chen, Shaohan Wang","Machine learning for structural health monitoring: challenges and opportunities",2020,"","","","",74,"2022-07-13 10:07:05","","10.1117/12.2561610","","",,,,,46,23.00,12,4,2,"A physics-based approach to structural health monitoring (SHM) has practical shortcomings which restrict its suitability to simple structures under well controlled environments. With the advances in information and sensing technology (sensors and sensor networks), it has become feasible to monitor large/diverse number of parameters in complex real-world structures either continuously or intermittently by employing large in-situ (wireless) sensor networks. The availability of this historical data has engendered a lot of interest in a data-driven approach as a natural and more viable option for realizing the goal of SHM in such structures. However, the lack of sensor data corresponding to different damage scenarios continues to remain a challenge. Most of the supervised machine-learning/deep-learning techniques, when trained using this inherently limited data, lack robustness and generalizability. Physics-informed learning, which involves the integration of domain knowledge into the learning process, is presented here as a potential remedy to this challenge. As a step towards the goal of automated damage detection (mathematically an inverse problem), preliminary results are presented from dynamic modelling of beam structures using physics-informed artificial neural networks. Forward and inverse problems involving partial differential equations are solved and comparisons reveal a clear superiority of physics-informed approach over one that is purely datadriven vis-à-vis overfitting/generalization. Other ways of incorporating domain knowledge into the machine learning pipeline are then presented through case-studies on various aspects of NDI/SHM (visual inspection, impact diagnosis). Lastly, as the final attribute of an optimal SHM approach, a sensing paradigm for non-contact full-field measurements for damage diagnosis is presented.","",""
3,"F. Lécué, B. Abeloos, Jonathan Anctil, Manuel Bergeron, Damien Dalla-Rosa, Simon Corbeil-Letourneau, Florian Martet, Tanguy Pommellet, L. Salvan, Simon Veilleux, M. Ziaeefard","Thales XAI Platform: Adaptable Explanation of Machine Learning Systems - A Knowledge Graphs Perspective",2019,"","","","",75,"2022-07-13 10:07:05","","","","",,,,,3,1.00,0,11,3,"Explanation in Machine Learning systems has been identified to be the main asset to have for large scale deployment of Artificial Intelligence (AI) in critical systems. Explanations could be example-, features-, semantics-based or even counterfactual to potentially action on an AI system; they could be represented in many different ways e.g., textual, graphical, or visual. All representations serve different means, purpose and operators. We built the first-of-its-kind XAI (eXplainable AI) platform for critical systems i.e., Thales XAI Platform which aims at serving explanations through various forms. This paper emphasizes on the semantics-based explanations for Machine Learning systems. 1 Explainable AI in Critical Systems Motivation: The current hype of Artificial Intelligence (AI) mostly refers to the success of Machine Learning (ML) and its sub-domain of deep learning. However industries operating with critical systems are either highly regulated, or require high level of certification and robustness. Therefore, such industry constraints do limit the adoption of non deterministic and ML systems. Answers to the question of explainability will be intrinsically connected to the adoption of AI in industry at scale. Indeed explanation, which could be used for debugging intelligent systems or deciding to follow a recommendation in real-time, will increase acceptance and (business) user trust. Explainable AI (XAI) is now referring to the core backup for industry to apply AI in products at scale, particularly for industries operating with critical systems. Focus: Thales XAI Platform is designed to provide explanation for a ML task (classification, regression, object detection, segmentation). Although Thales XAI Platform does provide different levels of explanation e.g., example-based, featuresbased, counterfactual using textual and visual representations, we emphasis only on the semantics-based explanation through knowledge graphs. ? Copyright c © 2019 for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). Critical Applications: From adapting a plane trajectory, stopping a train, refitting a boat to reconfiguring a satellite, all are examples of critical situations where explanation is a must-to-have to follow an AI system decision. 2 Why Knowledge Graphs for Explainable AI? State-of-the-Art Limitations: Most approaches limits explanation of ML systems to features involved in the data and model, or at best to examples, prototypes or counterfactuals. Explanation should go beyond correlation (features importance) and numerical similarity (local explanation). Opportunity: By expanding and linking initial (training, validation and test) data with entities in knowledge graphs, (i) context is encoded, (ii) connections and relations are exposed, and (iii) inference and causation are natively supported. Knowledge graphs are used for encoding better representation of data, structuring a ML model in a more interpretable way, and adopt a semantic similarity for local (instance-based) and global (model-based) explanation. 3 Thales XAI Platform: A Knowledge Graph Perspective (Semantic) Perspective: The platform is combining ML and reasoning functionalities to expose a human-like rational as explanation when (i) recognizing an object (in a raw image) of any class in a knowledge graph, (ii) predicting a link in a knowledge graph. Thales XAI Platform is using state-of-the-art Semantic Web tools for enriching input, output (class) data with DBpedia (4, 233, 000 resources) and domain-specific knowledge graphs, usually enterprise knowledge graphs. This is a crucial step for contextualizing training, validation, test data. Explainable ML Classifications: Starting from raw images, as unstructured data, but with class labels augmented with a domain knowledge graph, Thales XAI Platform relies on existing neural network architectures to build the most appropriate models. All confidence scores of output classes on any input image are updated based on the semantic description of the output classes. For instance, an input classified as a car will have a higher overall confidence score in case some properties of car in the knowledge graph are retrieved e.g., having wheels, being on a road. In addition the platform is embedding naturally explanation i.e., properties of the objects retrieved in both the raw data and knowledge graph. Explainable Relational Learning: Starting from relational data, structured as graph, and augmented with a domain knowledge graph, Thales XAI Platform relies on existing knowledge graph embeddings frameworks to build the most appropriate models. Explanation of any link prediction is retrieved by identifying representative hotspots in the knowledge graph i.e., connected parts of the graphs that negatively impact prediction accuracy when removed.","",""
9,"F. Maes, D. Robben, D. Vandermeulen, P. Suetens","The Role of Medical Image Computing and Machine Learning in Healthcare",2019,"","","","",76,"2022-07-13 10:07:05","","10.1007/978-3-319-94878-2_2","","",,,,,9,3.00,2,4,3,"","",""
3,"H. Vardhan, P. Völgyesi, J. Sztipanovits","Machine learning assisted propeller design",2021,"","","","",77,"2022-07-13 10:07:05","","10.1145/3450267.3452001","","",,,,,3,3.00,1,3,1,"Propellers are one of the most widely used propulsive devices for generating thrust from rotational engine motion both in marine vehicles and subsonic air-crafts. Due to their simplicity, robustness and high efficiency, propellers remained the mainstream design choice over the last hundred years. On the other hand, finding the optimal application-specific geometry is still challenging. This work in progress report describes application of modern and rapidly developing Machine Learning (ML) techniques to gain novel designs. We rely on a rich set of preexisting parametric design patterns and accumulated engineering knowledge supplemented by high-fidelity simulation models to formulate the design process as a supervised learning problem. The aim of our work is to develop and evaluate machine learning models for the parametric design of propellers based on application-specific constraints. While the application of ML techniques in optimal propeller design is at a very nascent level, we believe that our early results are promising with a potentially significant impact on the overall design process. The ML-assisted design flow allows for a more automated design space exploration process with less dependency on human intuition and engineering guidance.","",""
3,"Luanxiao Zhao, Caifeng Zou, Yuanyuan Chen, Wenlong Shen, Yirong Wang, Huaizhen Chen, J. Geng","Fluid and lithofacies prediction based on integration of well-log data and seismic inversion: A machine-learning approach",2021,"","","","",78,"2022-07-13 10:07:05","","10.1190/GEO2020-0521.1","","",,,,,3,3.00,0,7,1,"Seismic prediction of fluid and lithofacies distribution is of great interest to reservoir characterization, geologic model building, and flow unit delineation. Inferring fluids and lithofacies from seismic data under the framework of machine learning is commonly subject to issues of limited features, imbalanced data sets, and spatial constraints. As a consequence, an extreme gradient boosting-based workflow, which takes feature engineering, data balancing, and spatial constraints into account, is proposed to predict the fluid and lithofacies distribution by integrating well-log and seismic data. The constructed feature set based on simple mathematical operations and domain knowledge outperforms the benchmark group consisting of conventional elastic attributes of P-impedance and [Formula: see text] ratio. A radial basis function characterizing the weights of training samples according to the distances from the available wells to the target region is developed to impose spatial constraints on the model training process, significantly improving the prediction accuracy and reliability of gas sandstone. The strategy combining the synthetic minority oversampling technique and spatial constraints further increases the F1 score of gas sandstone and also benefits the overall prediction performance of all of the facies. The application of the combined strategy on prestack seismic inversion results generates a more geologically reasonable spatial distribution of fluids, thus verifying the robustness and effectiveness of our workflow.","",""
3,"N. Yousefpour, Z. Medina-Cetina, F. G. Hernandez-Martinez, A. Al-Tabbaa","Stiffness and Strength of Stabilized Organic Soils—Part II/II: Parametric Analysis and Modeling with Machine Learning",2021,"","","","",79,"2022-07-13 10:07:05","","10.3390/GEOSCIENCES11050218","","",,,,,3,3.00,1,4,1,"Predicting the range of achievable strength and stiffness from stabilized soil mixtures is critical for engineering design and construction, especially for organic soils, which are often considered “unsuitable” due to their high compressibility and the lack of knowledge about their mechanical behavior after stabilization. This study investigates the mechanical behavior of stabilized organic soils using machine learning (ML) methods. ML algorithms were developed and trained using a database from a comprehensive experimental study (see Part I), including more than one thousand unconfined compression tests on organic clay samples stabilized by wet soil mixing (WSM) technique. Three different ML methods were adopted and compared, including two artificial neural networks (ANN) and a linear regression method. ANN models proved reliable in the prediction of the stiffness and strength of stabilized organic soils, significantly outperforming linear regression models. Binder type, mixing ratio, soil organic and water content, sample size, aging, temperature, relative humidity, and carbonation were the control variables (input parameters) incorporated into the ML models. The impacts of these factors were evaluated through rigorous ANN-based parametric analyses. Additionally, the nonlinear relations of stiffness and strength with these parameters were developed, and their optimum ranges were identified through the ANN models. Overall, the robust ML approach presented in this paper can significantly improve the mixture design for organic soil stabilization and minimize the experimental cost for implementing WSM in engineering projects.","",""
3,"Tingting Sun, Yuting Chen, Yuhao Wen, Zefeng Zhu, Minghui Li","PremPLI: a machine learning model for predicting the effects of missense mutations on protein-ligand interactions",2021,"","","","",80,"2022-07-13 10:07:05","","10.1038/s42003-021-02826-3","","",,,,,3,3.00,1,5,1,"","",""
2,"Somayah Albaradei, Mahmut Uludag, Maha A. Thafar, T. Gojobori, M. Essack, Xin Gao","Predicting Bone Metastasis Using Gene Expression-Based Machine Learning Models",2021,"","","","",81,"2022-07-13 10:07:05","","10.3389/fgene.2021.771092","","",,,,,2,2.00,0,6,1,"Bone is the most common site of distant metastasis from malignant tumors, with the highest prevalence observed in breast and prostate cancers. Such bone metastases (BM) cause many painful skeletal-related events, such as severe bone pain, pathological fractures, spinal cord compression, and hypercalcemia, with adverse effects on life quality. Many bone-targeting agents developed based on the current understanding of BM onset’s molecular mechanisms dull these adverse effects. However, only a few studies investigated potential predictors of high risk for developing BM, despite such knowledge being critical for early interventions to prevent or delay BM. This work proposes a computational network-based pipeline that incorporates a ML/DL component to predict BM development. Based on the proposed pipeline we constructed several machine learning models. The deep neural network (DNN) model exhibited the highest prediction accuracy (AUC of 92.11%) using the top 34 featured genes ranked by betweenness centrality scores. We further used an entirely separate, “external” TCGA dataset to evaluate the robustness of this DNN model and achieved sensitivity of 85%, specificity of 80%, positive predictive value of 78.10%, negative predictive value of 80%, and AUC of 85.78%. The result shows the models’ way of learning allowed it to zoom in on the featured genes that provide the added benefit of the model displaying generic capabilities, that is, to predict BM for samples from different primary sites. Furthermore, existing experimental evidence provides confidence that about 50% of the 34 hub genes have BM-related functionality, which suggests that these common genetic markers provide vital insight about BM drivers. These findings may prompt the transformation of such a method into an artificial intelligence (AI) diagnostic tool and direct us towards mechanisms that underlie metastasis to bone events.","",""
1,"Minh-Thang Nguyen, Sungoh Kwon","Machine Learning–Based Mobility Robustness Optimization Under Dynamic Cellular Networks",2021,"","","","",82,"2022-07-13 10:07:05","","10.1109/ACCESS.2021.3083554","","",,,,,1,1.00,1,2,1,"In this paper, we propose a machine learning−based mobility robustness optimization algorithm to optimize handover parameters for seamless mobility under dynamic small-cell networks. Small cells can be arbitrarily deployed, portable, and turned on and off to fulfill wireless traffic demands or energy efficiency. As a result, the small-cell network topology dynamically varies challenging network optimization, especially handover optimization. Previous studies have only considered dynamics due to user mobility in a specific static network topology. To optimize handovers under dynamic network topologies, together with user mobility, we propose an algorithm consisting of two steps: topology adaptation and mobility adaptation. To adapt to a dynamic topology, the algorithm obtains prior knowledge, which presents a belief distribution of the optimal handover parameters, for the current network topology as coarse optimization. In the second step, the algorithm fine-tunes the handover parameters to adapt to user mobility based on reinforcement learning, which utilizes the knowledge obtained during the first step. Under a dynamic small-cell network, we showed that the proposed algorithm reduced adaptation time to 4.17% of the time needed by a comparative machine–based algorithm. Furthermore, the proposed algorithm improved the user satisfaction rate to 416.7% compared to the previous work.","",""
21,"Aaron M. Smith, J. Walsh, John J Long, Craig B Davis, Peter V. Henstock, M. Hodge, M. Maciejewski, X. Mu, Stephen Ra, Shanrong Zhao, D. Ziemek, Charles K. Fisher","Standard machine learning approaches outperform deep representation learning on phenotype prediction from transcriptomics data",2020,"","","","",83,"2022-07-13 10:07:05","","10.1186/s12859-020-3427-8","","",,,,,21,10.50,2,12,2,"","",""
6,"Kyriacos Yiannacou, V. Sariola","Controlled Manipulation and Active Sorting of Particles Inside Microfluidic Chips Using Bulk Acoustic Waves and Machine Learning",2021,"","","","",84,"2022-07-13 10:07:05","","10.1021/acs.langmuir.1c00063","","",,,,,6,6.00,3,2,1,"Manipulation of cells, droplets, and particles via ultrasound within microfluidic chips is a rapidly growing field, with applications in cell and particle sorting, blood fractionation, droplet transport, and enrichment of rare or cancerous cells, among others. However, current methods with a single ultrasonic transducer offer limited control of the position of single particles. In this paper, we demonstrate closed-loop two-dimensional manipulation of particles inside closed-channel microfluidic chips, by controlling the frequency of a single ultrasound transducer, based on machine-vision-measured positions of the particles. For the control task, we propose using algorithms derived from the family of multi-armed bandit algorithms. We show that these algorithms can achieve controlled manipulation with no prior information on the acoustic field shapes. The method learns as it goes: there is no need to restart the experiment at any point. Starting with no knowledge of the field shapes, the algorithms can (eventually) move a particle from one position inside the chamber to another. This makes the method very robust to changes in chip and particle properties. We demonstrate that the method can be used to manipulate a single particle, three particles simultaneously, and also a single particle in the presence of a bubble in the chip. Finally, we demonstrate the practical applications of this method in active sorting of particles, by guiding each particle to exit the chip through one of three different outlets at will. Because the method requires no model or calibration, the work paves the way toward the acoustic manipulation of microparticles inside unstructured environments.","",""
4,"Hyochang Ahn, Han-Jin Cho","Research of multi-object detection and tracking using machine learning based on knowledge for video surveillance system",2019,"","","","",85,"2022-07-13 10:07:05","","10.1007/s00779-019-01296-z","","",,,,,4,1.33,2,2,3,"","",""
8,"C. Rea, K. Montes, A. Pau, R. Granetz, O. Sauter","Progress Toward Interpretable Machine Learning–Based Disruption Predictors Across Tokamaks",2020,"","","","",86,"2022-07-13 10:07:05","","10.1080/15361055.2020.1798589","","",,,,,8,4.00,2,5,2,"Abstract In this paper we lay the groundwork for a robust cross-device comparison of data-driven disruption prediction algorithms on DIII-D and JET tokamaks. In order to consistently carry on a comparative analysis, we define physics-based indicators of disruption precursors based on temperature, density, and radiation profiles that are currently not used in many other machine learning predictors for DIII-D data. These profile-based indicators are shown to well-describe impurity accumulation events in both DIII-D and JET discharges that eventually disrupt. The univariate analysis of the features used as input signals in the data-driven algorithms applied on the data of both tokamaks statistically highlights the differences in the dominant disruption precursors. JET with its ITER-like wall is more prone to impurity accumulation events, while DIII-D is more subject to edge-cooling mechanisms that destabilize dangerous magnetohydrodynamic modes. Even though the analyzed data sets are characterized by such intrinsic differences, we show through a few examples that the inclusion of physics-based disruption markers in data-driven algorithms is a promising path toward the realization of a uniform framework to predict and interpret disruptive scenarios across different tokamaks. As long as the destabilizing precursors are diagnosed in a device-independent way, the knowledge that data-driven algorithms learn on one device can be re-used to explain a disruptive behavior on another device.","",""
21,"Christopher Culley, S. Vijayakumar, Guido Zampieri, C. Angione","A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth",2020,"","","","",87,"2022-07-13 10:07:05","","10.1073/pnas.2002959117","","",,,,,21,10.50,5,4,2,"Significance Linking genotype and phenotype is a fundamental problem in biology, key to several biomedical and biotechnological applications. Cell growth is a central phenotypic trait, resulting from interactions between environment, gene regulation, and metabolism, yet its functional bases are still not completely understood. We propose and test a machine-learning approach that integrates large-scale gene expression profiles and mechanistic metabolic models, for characterizing cell growth and understanding its driving mechanisms in Saccharomyces cerevisiae. At its core, a custom-built multimodal learning method merges experimentally generated and model-generated data. We show that our approach can leverage the advantages of both machine learning and metabolic modeling, revealing unknown interactions between biological domains, incorporating mechanistic knowledge, and therefore overcoming black-box limitations of conventional data-driven approaches. Metabolic modeling and machine learning are key components in the emerging next generation of systems and synthetic biology tools, targeting the genotype–phenotype–environment relationship. Rather than being used in isolation, it is becoming clear that their value is maximized when they are combined. However, the potential of integrating these two frameworks for omic data augmentation and integration is largely unexplored. We propose, rigorously assess, and compare machine-learning–based data integration techniques, combining gene expression profiles with computationally generated metabolic flux data to predict yeast cell growth. To this end, we create strain-specific metabolic models for 1,143 Saccharomyces cerevisiae mutants and we test 27 machine-learning methods, incorporating state-of-the-art feature selection and multiview learning approaches. We propose a multiview neural network using fluxomic and transcriptomic data, showing that the former increases the predictive accuracy of the latter and reveals functional patterns that are not directly deducible from gene expression alone. We test the proposed neural network on a further 86 strains generated in a different experiment, therefore verifying its robustness to an additional independent dataset. Finally, we show that introducing mechanistic flux features improves the predictions also for knockout strains whose genes were not modeled in the metabolic reconstruction. Our results thus demonstrate that fusing experimental cues with in silico models, based on known biochemistry, can contribute with disjoint information toward biologically informed and interpretable machine learning. Overall, this study provides tools for understanding and manipulating complex phenotypes, increasing both the prediction accuracy and the extent of discernible mechanistic biological insights.","",""
550,"F. Hutter, Lars Kotthoff, J. Vanschoren","Automated Machine Learning: Methods, Systems, Challenges",2019,"","","","",88,"2022-07-13 10:07:05","","10.1007/978-3-030-05318-5","","",,,,,550,183.33,183,3,3,"","",""
14,"Zi Zhang, Hong Pan, Xingyu Wang, Zhibin Lin","Machine Learning-Enriched Lamb Wave Approaches for Automated Damage Detection",2020,"","","","",89,"2022-07-13 10:07:05","","10.3390/s20061790","","",,,,,14,7.00,4,4,2,"Lamb wave approaches have been accepted as efficiently non-destructive evaluations in structural health monitoring for identifying damage in different states. Despite significant efforts in signal process of Lamb waves, physics-based prediction is still a big challenge due to complexity nature of the Lamb wave when it propagates, scatters and disperses. Machine learning in recent years has created transformative opportunities for accelerating knowledge discovery and accurately disseminating information where conventional Lamb wave approaches cannot work. Therefore, the learning framework was proposed with a workflow from dataset generation, to sensitive feature extraction, to prediction model for lamb-wave-based damage detection. A total of 17 damage states in terms of different damage type, sizes and orientations were designed to train the feature extraction and sensitive feature selection. A machine learning method, support vector machine (SVM), was employed for the learning model. A grid searching (GS) technique was adopted to optimize the parameters of the SVM model. The results show that the machine learning-enriched Lamb wave-based damage detection method is an efficient and accuracy wave to identify the damage severity and orientation. Results demonstrated that different features generated from different domains had certain levels of sensitivity to damage, while the feature selection method revealed that time-frequency features and wavelet coefficients exhibited the highest damage-sensitivity. These features were also much more robust to noise. With increase of noise, the accuracy of the classification dramatically dropped.","",""
3,"U. Erdogdu, Mehmet Tan, R. Alhajj, Faruk Polat, J. Rokne, D. Demetrick","Integrating machine learning techniques into robust data enrichment approach and its application to gene expression data",2013,"","","","",90,"2022-07-13 10:07:05","","10.1504/IJDMB.2013.056090","","",,,,,3,0.33,1,6,9,"The availability of enough samples for effective analysis and knowledge discovery has been a challenge in the research community, especially in the area of gene expression data analysis. Thus, the approaches being developed for data analysis have mostly suffered from the lack of enough data to train and test the constructed models. We argue that the process of sample generation could be successfully automated by employing some sophisticated machine learning techniques. An automated sample generation framework could successfully complement the actual sample generation from real cases. This argument is validated in this paper by describing a framework that integrates multiple models (perspectives) for sample generation. We illustrate its applicability for producing new gene expression data samples, a highly demanding area that has not received attention. The three perspectives employed in the process are based on models that are not closely related. The independence eliminates the bias of having the produced approach covering only certain characteristics of the domain and leading to samples skewed towards one direction. The first model is based on the Probabilistic Boolean Network (PBN) representation of the gene regulatory network underlying the given gene expression data. The second model integrates Hierarchical Markov Model (HIMM) and the third model employs a genetic algorithm in the process. Each model learns as much as possible characteristics of the domain being analysed and tries to incorporate the learned characteristics in generating new samples. In other words, the models base their analysis on domain knowledge implicitly present in the data itself. The developed framework has been extensively tested by checking how the new samples complement the original samples. The produced results are very promising in showing the effectiveness, usefulness and applicability of the proposed multi-model framework.","",""
11,"M. Elgendi, C. Menon","Machine Learning Ranks ECG as an Optimal Wearable Biosignal for Assessing Driving Stress",2020,"","","","",91,"2022-07-13 10:07:05","","10.1109/ACCESS.2020.2974933","","",,,,,11,5.50,6,2,2,"The demand for wearable devices that can detect anxiety and stress when driving is increasing. Recent studies have attempted to use multiple biosignals to detect driving stress. However, collecting multiple biosignals can be complex and is associated with numerous challenges. Determining the optimal biosignal for assessing driving stress can save lives. To the best of our knowledge, no study has investigated both longitudinal and transitional stress assessment using supervised and unsupervised ML techniques. Thus, this study hypothesizes that the optimal signal for assessing driving stress will consistently detect stress using supervised and unsupervised machine learning (ML) techniques. Two different approaches were used to assess driving stress: longitudinal (a combined repeated measurement of the same biosignals over three driving states) and transitional (switching from state to state such as city to highway driving). The longitudinal analysis did not involve a feature extraction phase while the transitional analysis involved a feature extraction phase. The longitudinal analysis consists of a novel interaction ensemble (INTENSE) that aggregates three unsupervised ML approaches: interaction principal component analysis, connectivity-based clustering, and K-means clustering. INTENSE was developed to uncover new knowledge by revealing the strongest correlation between the biosignal and driving stress marker. These three MLs each have their well-known and distinctive geometrical basis. Thus, the aggregation of their result would provide a more robust examination of the simultaneous non-causal associations between six biosignals: electrocardiogram (ECG), electromyogram, hand galvanic skin resistance, foot galvanic skin resistance, heart rate, respiration, and the driving stress marker. INTENSE indicates that ECG is highly correlated with the driving stress marker. The supervised ML algorithms confirmed that ECG is the most informative biosignal for detecting driving stress, with an overall accuracy of 75.02%.","",""
5,"M. Kaden, Katrin Sophie Bohnsack, Mirko Weber, Mateusz Kudla, Kaja Gutowska, J. Blazewicz, T. Villmann","Analysis of SARS-CoV-2 RNA-Sequences by Interpretable Machine Learning Models",2020,"","","","",92,"2022-07-13 10:07:05","","10.1101/2020.05.15.097741","","",,,,,5,2.50,1,7,2,"We present an approach to investigate SARS-CoV-2 virus sequences based on alignment-free methods for RNA sequence comparison. In particular, we verify a given clustering result for the GISAID data set, which was obtained analyzing the molecular differences in coronavirus populations by phylogenetic trees. For this purpose, we use alignment-free dissimilarity measures for sequences and combine them with learning vector quantization classifiers for virus type discriminant analysis and classification. Those vector quantizers belong to the class of interpretable machine learning methods, which, on the one hand side provide additional knowledge about the classification decisions like discriminant feature correlations, and on the other hand can be equipped with a reject option. This option gives the model the property of self controlled evidence if applied to new data, i.e. the models refuses to make a classification decision, if the model evidence for the presented data is not given. After training such a classifier for the GISAID data set, we apply the obtained classifier model to another but unlabeled SARS-CoV-2 virus data set. On the one hand side, this allows us to assign new sequences to already known virus types and, on the other hand, the rejected sequences allow speculations about new virus types with respect to nucleotide base mutations in the viral sequences. Author summary The currently emerging global disease COVID-19 caused by novel SARS-CoV-2 viruses requires all scientific effort to investigate the development of the viral epidemy, the properties of the virus and its types. Investigations of the virus sequence are of special interest. Frequently, those are based on mathematical/statistical analysis. However, machine learning methods represent a promising alternative, if one focuses on interpretable models, i.e. those that do not act as black-boxes. Doing so, we apply variants of Learning Vector Quantizers to analyze the SARS-CoV-2 sequences. We encoded the sequences and compared them in their numerical representations to avoid the computationally costly comparison based on sequence alignments. Our resulting model is interpretable, robust, efficient, and has a self-controlling mechanism regarding the applicability to data. This framework was applied to two data sets concerning SARS-CoV-2. We were able to verify previously published virus type findings for one of the data sets by training our model to accurately identify the virus type of sequences. For sequences without virus type information (second data set), our trained model can predict them. Thereby, we observe a new scattered spreading of the sequences in the data space which probably is caused by mutations in the viral sequences.","",""
3,"Kai Liang Tan, Anuj Sharma, S. Sarkar","Robust Deep Reinforcement Learning for Traffic Signal Control",2020,"","","","",93,"2022-07-13 10:07:05","","10.1007/s42421-020-00029-6","","",,,,,3,1.50,1,3,2,"","",""
1,"Zheren Ma, E. Davani, Xiaodan Ma, Hanna Lee, I. Arslan, Xiang Zhai, H. Darabi, D. Castineira","Finding a Trend Out of Chaos, A Machine Learning Approach for Well Spacing Optimization",2020,"","","","",94,"2022-07-13 10:07:05","","10.2118/201698-ms","","",,,,,1,0.50,0,8,2,"  Data-driven decisions powered by machine-learning methods are increasing in popularity when it comes to optimizing field development in unconventional reservoirs. However, since well performance is impacted by many factors (e.g., geological characteristics, completion design, well design, etc.), the challenge is uncovering trends from all the noise.  By leveraging basin-level knowledge captured by big data sculpting, integrating private and public data with the use of uncertainty quantification, Augmented AI (a combination of expert-based knowledge and advanced AI frameworks) can provide quick and science-based answers for well spacing and fracking optimization and assess the full potential of an asset in unconventional reservoirs.  Augmented AI is artificial intelligence powered by engineering wisdom. The Augmented AI workflow starts with data sculpting, which includes information retrieval, data cleaning and standardization, and finally a smart, deep and systematic data QC. Feature engineering generates all the relevant parameters going into the machine learning model—over 50 features have been generated for this work and categorized. The final step is to perform model tuning and ensemble, evaluating the model robustness, generating model explanation and uncertainty quantification. Augmented AI adopts an iterative machine learning modeling approach. This approach combines new and innovative engineering and G&G workflows with data-driven models so that a deep understanding of the field behavior can be developed. Loops from feature selection to model tuning are used until good model results are achieved. The loop is automated using Bayesians optimization. All machine learning models have different strengths and weaknesses for prediction. Instead of manually determining which machine learning model to use, this approach uses an adaptive ensemble machine learning approach that is a stacking algorithm that combines multiple regression models via a second level machine learning model. It smartly aggregates opinions from different models with reduced variance and better robustness.  Augmented AI has been applied in unconventional reservoirs with great results. A case study in Midland Basin is presented in this paper. Domain-induced feature engineering was performed to obtain important features for predicting well performance, and initial feature selection was conducted using feature correlation analysis. A trusted and explainable ML model was built and enhanced with uncertainty quantification. After running several sensitivity analyses, Augmented AI optimized the attributes of interest, then vetted the outcome, generating a report and visualizing the results.  In addition, further information about the direct impact of well spacing on EUR was deconvoluted from other parameters using an ML explanation technique for Wolfcamp Formation in Permian Basin and subsequently well spacing optimization was presented for the case study in Midland Basin.  An innovative model was created using Augmented AI to optimize well spacing, leveraging big data sculpting, domain and physics-induced feature engineering, and machine learning. The learning was transferred from the basin model to the specific region of interest. Augmented AI provides efficient and systematic private data organization, an explainable machine learning model, robust production forecast with quantified uncertainty and well spacing and frac parameters optimization.  Augmented AI models are already built for major basins such as Midland and Delaware basins. The learning and knowledge of the model can be transferred to any region in a basin and can be refined using more accurate private data. This allows conclusions to be drawn even with a limited number of wells.","",""
2,"Jiyuan Tu, Weidong Liu, Xiaojun Mao","Byzantine-robust distributed sparse learning for M-estimation",2021,"","","","",95,"2022-07-13 10:07:05","","10.1007/S10994-021-06001-X","","",,,,,2,2.00,1,3,1,"","",""
2,"Wentao Wei, Xuhui Hu, Hua Liu, Ming Zhou, Yan Song","Towards Integration of Domain Knowledge-Guided Feature Engineering and Deep Feature Learning in Surface Electromyography-Based Hand Movement Recognition",2021,"","","","",96,"2022-07-13 10:07:05","","10.1155/2021/4454648","","",,,,,2,2.00,0,5,1,"As a machine-learning-driven decision-making problem, the surface electromyography (sEMG)-based hand movement recognition is one of the key issues in robust control of noninvasive neural interfaces such as myoelectric prosthesis and rehabilitation robot. Despite the recent success in sEMG-based hand movement recognition using end-to-end deep feature learning technologies based on deep learning models, the performance of today's sEMG-based hand movement recognition system is still limited by the noisy, random, and nonstationary nature of sEMG signals and researchers have come up with a number of methods that improve sEMG-based hand movement via feature engineering. Aiming at achieving higher sEMG-based hand movement recognition accuracies while enabling a trade-off between performance and computational complexity, this study proposed a progressive fusion network (PFNet) framework, which improves sEMG-based hand movement recognition via integration of domain knowledge-guided feature engineering and deep feature learning. In particular, it learns high-level feature representations from raw sEMG signals and engineered time-frequency domain features via a feature learning network and a domain knowledge network, respectively, and then employs a 3-stage progressive fusion strategy to progressively fuse the two networks together and obtain the final decisions. Extensive experiments were conducted on five sEMG datasets to evaluate our proposed PFNet, and the experimental results showed that the proposed PFNet could achieve the average hand movement recognition accuracies of 87.8%, 85.4%, 68.3%, 71.7%, and 90.3% on the five datasets, respectively, which outperformed those achieved by the state of the arts.","",""
1,"Rahul Mishra, Hari Prabhat Gupta, Tanima Dutta","A Network Resource Aware Federated Learning Approach using Knowledge Distillation",2021,"","","","",97,"2022-07-13 10:07:05","","10.1109/INFOCOMWKSHPS51825.2021.9484597","","",,,,,1,1.00,0,3,1,"Federated Learning (FL) has gained unprecedented growth in the past few years by facilitating data privacy. This poster proposes a network resource aware federated learning approach that utilizes the concept of knowledge distillation to train a machine learning model by using local data samples. The approach creates different groups based on the bandwidth between clients and server and iteratively applies FL to each group by compressing the model using knowledge distillation. The approach reduces the bandwidth requirement and generates a more robust model trained on the data of all clients without revealing privacy.","",""
4,"Ahmed A. Abusnaina, M. Abuhamad, Hisham Alasmary, Afsah Anwar, Rhongho Jang, Saeed Salem, Daehun Nyang, David A. Mohaisen","DL-FHMC: Deep Learning-based Fine-grained Hierarchical Learning Approach for Robust Malware Classiﬁcation",2021,"","","","",98,"2022-07-13 10:07:05","","","","",,,,,4,4.00,1,8,1,"—The acceptance of the Internet of Things (IoT) for both household and industrial applications is accompanied by the rapid growth of IoT malware. With the increase of their attack surface, analyzing, understanding, and detecting IoT malicious behavior are crucial. Traditionally, machine and deep learning-based approaches are used for malware detection and behavioral understanding. However, recent research has shown the susceptibility of those approaches to adversarial attacks by introducing noise to the feature space. In this work, we introduce DL-FHMC, a ﬁne-grained hierarchical learning approach for robust IoT malware detection. DL-FHMC utilizes Control Flow Graph (CFG)-based behavioral patterns for adversarial IoT malicious software detection. In particular, we extract a comprehensive list of behavioral patterns from a large dataset of malicious IoT binaries, represented by the shared execution ﬂows, and use them as a modality for malicious behavior detection. Leveraging machine learning and subgraph isomorphism matching algorithms, DL-FHMC provides a state-of-the-art performance in detecting malware samples and adversarial examples (AEs). We start this work by examining the performance of the current CFG-based IoT malware detection systems against adversarial IoT software crafted using Graph Embedding and Augmentation (GEA) techniques. As a result, we show the adversarial capabilities in generating practical functionality-preserving AEs with reduced overhead, highlighting caveats in the state of the current detection systems under adversarial settings. We then introduce suspicious behavior detector, a component that incorporates comprehensive behavioral patterns extracted from three popular IoT malicious families, Gafgyt, Mirai, and Tsunami, and detects AEs with high accuracy, up to 100% under different attack conﬁgurations. The suspicious behavior detector operates as a standalone module that can operate alongside other malware detection methods and does not assume prior knowledge of the adversarial attacks nor their conﬁgurations.","",""
28,"Lili Su, Jiaming Xu","Securing Distributed Machine Learning in High Dimensions",2018,"","","","",99,"2022-07-13 10:07:05","","","","",,,,,28,7.00,14,2,4,"We consider securing a distributed machine learning system wherein the data is kept confidential by its providers who are recruited as workers to help the learner to train a $d$--dimensional model. In each communication round, up to $q$ out of the $m$ workers suffer Byzantine faults; faulty workers are assumed to have complete knowledge of the system and can collude to behave arbitrarily adversarially against the learner. We assume that each worker keeps a local sample of size $n$. (Thus, the total number of data points is $N=nm$.) Of particular interest is the high-dimensional regime $d \gg n$.  We propose a secured variant of the classical gradient descent method which can tolerate up to a constant fraction of Byzantine workers. We show that the estimation error of the iterates converges to an estimation error $O(\sqrt{q/N} + \sqrt{d/N})$ in $O(\log N)$ rounds. The core of our method is a robust gradient aggregator based on the iterative filtering algorithm proposed by Steinhardt et al. \cite{Steinhardt18} for robust mean estimation. We establish a uniform concentration of the sample covariance matrix of gradients, and show that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function. As a by-product, we develop a new concentration inequality for sample covariance matrices of sub-exponential distributions, which might be of independent interest.","",""
89,"Huichen Lihuichen","DECISION-BASED ADVERSARIAL ATTACKS: RELIABLE ATTACKS AGAINST BLACK-BOX MACHINE LEARNING MODELS",2017,"","","","",100,"2022-07-13 10:07:05","","","","",,,,,89,17.80,89,1,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradientor score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available at XXXXXX. Gradient-based Model M Untargeted Flip to any label Targeted Flip to target label FGSM, DeepFool L-BFGS-B, Houdini, JSMA, Carlini & Wagner, Iterative Gradient Descent Score-based Detailed Model Prediction Y (e.g. probabilities or logits) ZOO Local Search Decision-based Final Model Prediction Ymax (e.g. max class label) this work (Boundary Attack) Transfer-based Training Data T","",""
23,"Muxin Gu, M. Buckley","Semi-supervised machine learning for automated species identification by collagen peptide mass fingerprinting",2018,"","","","",101,"2022-07-13 10:07:05","","10.1186/s12859-018-2221-3","","",,,,,23,5.75,12,2,4,"","",""
11,"Ghassan Alnwaimi, Talha Zahir, S. Vahid, K. Moessner","Machine Learning Based Knowledge Acquisition on Spectrum Usage for LTE Femtocells",2013,"","","","",102,"2022-07-13 10:07:05","","10.1109/VTCFall.2013.6692276","","",,,,,11,1.22,3,4,9,"The decentralised and ad hoc nature of femtocell deployments calls for distributed learning strategies to mitigate interference. We propose a distributed spectrum awareness scheme for femtocell networks, based on combined payoff and strategy reinforcement learning (RL) models. We present two different learning strategies, based on modifications to the Bush Mosteller (BM) RL and the Roth-Erev RL algorithms. The simulation results show the convergence behaviour of the learning strategies under a dynamic robust game. As compared to the Bush Mosteller (BM) RL, our modified BM (MBM) converges smoothly to a stable satisfactory solution. Moreover, the MBM significantly reduces the interference collision cost during the learning process. Both the MBM and the modified Roth-Erev (MRE) algorithms are stochastic-based learning strategies which require less computation than the gradient follower (GF) learning strategy and have the capability to escape from suboptimal solution.","",""
9,"Kevser Ovaz Akpinar, Ibrahim Ozcelik","Analysis of Machine Learning Methods in EtherCAT-Based Anomaly Detection",2019,"","","","",103,"2022-07-13 10:07:05","","10.1109/ACCESS.2019.2960497","","",,,,,9,3.00,5,2,3,"Today, the use of Ethernet-based protocols in industrial control systems (ICS) communications has led to the emergence of attacks based on information technology (IT) on supervisory control and data acquisition systems. In addition, the familiarity of Ethernet and TCP/IP protocols and the diversity and success of attacks on them raises security risks and cyber threats for ICS. This issue is compounded by the absence of encryption, authorization, and authentication mechanisms due to the development of industrial communications protocols only for performance purposes. Recent zero-day attacks, such as Triton, Stuxnet, Havex, Dragonfly, and Blackenergy, as well as the Ukraine cyber-attack, are possible because of the vulnerabilities of the systems; these attacksare carried by the protocols used in communication between PLC and I/O units or HMI and engineering stations. It is evident that there is a need for robust solutions that detect and prevent protocol-based cyber threats. In this paper, machine learning methods are evaluated for anomaly detection, particularly for EtherCAT-based ICS. To the best of the author’s knowledge, there has been no research focusing on machine learning algorithms for anomaly detection of EtherCAT. Before testing anomaly detection, an EtherCAT-based water level control system testbed was developed. Then, a total of 16 events were generated in four categories and applied on the testbed. The dataset created was used for anomaly detection. The results showed that the k-nearest neighbors (k-NN) and support vector machine with genetic algorithm (SVM GA) models perform best among the 18 techniques applied. In addition to detecting anomalies, the methods are able to flag the attack types better than other techniques and are applicable in EtherCAT networks. Also, the dataset and events can be used for further studies since it is difficult to obtain data for ICS due to its critical infrastructure and continuous real-time operation.","",""
3,"Wenxing Chen, Shuyang Dai, B. Zheng, Hao Lin","An Efficient Evaluation Method for Automobile Shells Design Based on Semi-supervised Machine Learning Strategy",2022,"","","","",104,"2022-07-13 10:07:05","","10.1088/1742-6596/2171/1/012026","","",,,,,3,3.00,1,4,1,"Automobile is one of the important modes of transportation for human travel in today’s society. Batch production in various countries in the world has also promoted the transformation of production concepts. At present, the development of the automobile industry is developing towards the trend of intelligence, personalizat-ion and sharing. Car appearance in a variety of ways, not every design is reasonable. Therefore, the main purpose of this article is to establish a scientific evaluation standard in order to large-scale test the quality of a variety of car shells design. The scientific nature is mainly reflected in combination the fluid-solid coupling knowledge and machine learning in this article, which can analyze the force of different shells in the flow field, and put out the cloud map information such as the stress, pressure and velocity of the shell. At last, analyze the best test samples and store them in the database, and then using semi-supervised heuristic algorithm to perform the sample training, the ultimate goal is to make the evaluation system more robust. The trained model can correctly evaluate each personalized car shape and give a reasonable score, which is convenient for car manufacturers to make best decision with personalized demand and scientific production.","",""
2,"F. Garzoli, D. Croce, M. Nardini, F. Ciambra, Roberto Basili","Robust Requirements Analysis in Complex Systems through Machine Learning",2012,"","","","",105,"2022-07-13 10:07:05","","10.1007/978-3-642-45260-4_4","","",,,,,2,0.20,0,5,10,"","",""
35,"Feng Ren, Chenglei Wang, Hui Tang","Active control of vortex-induced vibration of a circular cylinder using machine learning",2019,"","","","",106,"2022-07-13 10:07:05","","10.1063/1.5115258","","",,,,,35,11.67,12,3,3,"We demonstrate the use of high-fidelity computational fluid dynamics simulations in machine-learning based active flow control. More specifically, for the first time, we adopt the genetic programming (GP) to select explicit control laws, in a data-driven and unsupervised manner, for the suppression of vortex-induced vibration (VIV) of a circular cylinder in a low-Reynolds-number flow (Re = 100), using blowing/suction at fixed locations. A cost function that balances both VIV suppression and energy consumption for the control is carefully chosen according to the knowledge obtained from pure blowing/suction open-loop controls. By implementing reasonable constraints to VIV amplitude and actuation strength during the GP evolution, the GP-selected best ten control laws all point to suction-type actuation. The best control law suggests that the suction strength should be nonzero when the cylinder is at its equilibrium position and should increase nonlinearly with the cylinder’s transverse displacement. Applying this control law suppresses 94.2% of the VIV amplitude and achieves 21.4% better overall performance than the best open-loop controls. Furthermore, it is found that the GP-selected control law is robust, being effective in flows ranging from Re = 100 to 400. On the contrary, although the P-control can achieve similar performance as the GP-selected control at Re = 100, it deteriorates in higher Reynolds number flows. Although for demonstration purpose the chosen control problem is relatively simple, the training experience and insights obtained from this study can shed some light on future GP-based control of more complicated problems.We demonstrate the use of high-fidelity computational fluid dynamics simulations in machine-learning based active flow control. More specifically, for the first time, we adopt the genetic programming (GP) to select explicit control laws, in a data-driven and unsupervised manner, for the suppression of vortex-induced vibration (VIV) of a circular cylinder in a low-Reynolds-number flow (Re = 100), using blowing/suction at fixed locations. A cost function that balances both VIV suppression and energy consumption for the control is carefully chosen according to the knowledge obtained from pure blowing/suction open-loop controls. By implementing reasonable constraints to VIV amplitude and actuation strength during the GP evolution, the GP-selected best ten control laws all point to suction-type actuation. The best control law suggests that the suction strength should be nonzero when the cylinder is at its equilibrium position and should increase nonlinearly with the cylinder’s transverse displacement. Applying...","",""
67,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings",2017,"","","","",107,"2022-07-13 10:07:05","","10.1145/3154503","","",,,,,67,13.40,22,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q ≤ for an arbitrarily small but fixed constant ε > 0. The parameter estimate converges in O(log N) rounds with an estimation error on the order of max{√dq/N, √d/N, which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q. The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
3,"E. Tsukerman","Machine Learning for Cybersecurity Cookbook",2019,"","","","",108,"2022-07-13 10:07:05","","","","",,,,,3,1.00,3,1,3,"Learn how to apply modern AI to create powerful cybersecurity solutions for malware, pentesting, social engineering, data privacy, and intrusion detection Key Features Manage data of varying complexity to protect your system using the Python ecosystem  Apply ML to pentesting, malware, data privacy, intrusion detection system(IDS) and social engineering  Automate your daily workflow by addressing various security challenges using the recipes covered in the book Book Description Organizations today face a major threat in terms of cybersecurity, from malicious URLs to credential reuse, and having robust security systems can make all the difference. With this book, you'll learn how to use Python libraries such as TensorFlow and scikit-learn to implement the latest artificial intelligence (AI) techniques and handle challenges faced by cybersecurity researchers.  You'll begin by exploring various machine learning (ML) techniques and tips for setting up a secure lab environment. Next, you'll implement key ML algorithms such as clustering, gradient boosting, random forest, and XGBoost. The book will guide you through constructing classifiers and features for malware, which you'll train and test on real samples. As you progress, you'll build self-learning, reliant systems to handle cybersecurity tasks such as identifying malicious URLs, spam email detection, intrusion detection, network protection, and tracking user and process behavior. Later, you'll apply generative adversarial networks (GANs) and autoencoders to advanced security tasks. Finally, you'll delve into secure and private AI to protect the privacy rights of consumers using your ML models.   By the end of this book, you'll have the skills you need to tackle real-world problems faced in the cybersecurity domain using a recipe-based approach.   What you will learn Learn how to build malware classifiers to detect suspicious activities  Apply ML to generate custom malware to pentest your security  Use ML algorithms with complex datasets to implement cybersecurity concepts  Create neural networks to identify fake videos and images  Secure your organization from one of the most popular threats – insider threats  Defend against zero-day threats by constructing an anomaly detection system  Detect web vulnerabilities effectively by combining Metasploit and ML  Understand how to train a model without exposing the training data Who this book is for This book is for cybersecurity professionals and security researchers who are looking to implement the latest machine learning techniques to boost computer security, and gain insights into securing an organization using red and blue team ML. This recipe-based book will also be useful for data scientists and machine learning developers who want to experiment with smart techniques in the cybersecurity domain. Working knowledge of Python programming and familiarity with cybersecurity fundamentals will help you get the most out of this book.","",""
22,"Mohamed Maher, S. Sakr","SmartML: A Meta Learning-Based Framework for Automated Selection and Hyperparameter Tuning for Machine Learning Algorithms",2019,"","","","",109,"2022-07-13 10:07:05","","10.5441/002/edbt.2019.54","","",,,,,22,7.33,11,2,3,"Due to the increasing success of machine learning techniques, nowadays, thay have been widely utilized in almost every domain such as financial applications, marketing, recommender systems and user behavior analytics, just to name a few. In practice, the machine learning model creation process is a highly iterative exploratory process. In particular, an effective machine learning modeling process requires solid knowledge and understanding of the different types of machine learning algorithms. In addition, all machine learning algorithms require user-defined inputs to achieve a balance between accuracy and generalizability. This task is referred to as Hyperparameter Tuning . Thus, in practice, data scientists work hard to find the best model or algorithm that meets the specifications of their prob-lem. Such iterative and explorative nature of the modeling process is commonly tedious and time-consuming. We demonstrate SmartML , a meta learning-based framework for automated selection and hyperparameter tuning for machine learning algorithms. Being meta learning-based, the framework is able to simulate the role of the machine learning expert. In particular, the framework is equipped with a continuously updated knowledge base that stores information about the meta-features of all processed datasets along with the associated performance of the different classifiers and their tuned parameters. Thus, for any new dataset, SmartML automatically extracts its meta features and searches its knowledge base for the best performing algorithm to start its optimization process. In addition, SmartML makes use of the new runs to continuously en-rich its knowledge base to improve its performance and robustness for future runs. We will show how our approach outperforms the-state-of-the-art techniques in the domain of automated machine learning frameworks.","",""
20,"Di Wu, Binxing Fang, Junnan Wang, Qixu Liu, Xiang Cui","Evading Machine Learning Botnet Detection Models via Deep Reinforcement Learning",2019,"","","","",110,"2022-07-13 10:07:05","","10.1109/ICC.2019.8761337","","",,,,,20,6.67,4,5,3,"Botnets are one of predominant threats to Internet security. To date, machine learning technology has wide application in botnet detection because that it is able to summarize the features of existing attacks and generalize to never-before-seen botnet families. However, recent works in adversarial machine learning have shown that attackers are able to bypass the detection model by constructing specific samples, which due to many algorithms are vulnerable to almost imperceptible perturbations of their inputs. According to the degree of adversaries' knowledge about the model, adversarial attacks can be classified into several groups, such as gradient- and score-based attacks. In this paper, we propose a more general framework based on deep reinforcement learning (DRL), which effectively generates adversarial traffic flows to deceive the detection model by automatically adding perturbations to samples. Throughout the process, the target detector will be regarded as a black box and more close to realistic attack circumstance. A reinforcement learning agent is equipped for updating the adversarial samples by combining the feedback from the target model (i.e. benign or malicious) and the sequence of actions, which is able to change the temporal and spatial features of the traffic flows while maintaining the original functionality and executability. The experiment results show that the evasion rates of adversarial botnet flows are significantly improved. Furthermore, with the perspective of defense, this research can help the detection model spot its defect and thus enhance the robustness.","",""
5,"D. Mislis, S. Pyrzas, K. Alsubai","TSARDI: a Machine Learning data rejection algorithm for transiting exoplanet light curves",2018,"","","","",111,"2022-07-13 10:07:05","","10.1093/mnras/sty2361","","",,,,,5,1.25,2,3,4,"We present TSARDI, an efficient rejection algorithm designed to improve the transit detection efficiency in data collected by large scale surveys. TSARDI is based on the Machine Learning clustering algorithm DBSCAN, and its purpose is to serve as a robust and adaptable filter aiming to identify unwanted noise points left over from data detrending processes. TSARDI is an unsupervised method, which can treat each light curve individually; there is no need of previous knowledge of any other field light curves. We conduct a simulated transit search by injecting planets on real data obtained by the QES project and show that TSARDI leads to an overall transit detection efficiency increase of $\sim$11\%, compared to results obtained from the same sample, but using a standard sigma-clip algorithm. For the brighter end of our sample (host star magnitude < 12), TSARDI achieves a detection efficiency of $\sim$80\% of injected planets. While our algorithm has been developed primarily for the field of exoplanets, it is easily adaptable and extendable for use in any time series.","",""
16,"Mo Zhou, Yoshimi Fukuoka, Ken Goldberg, E. Vittinghoff, Anil Aswani","Applying machine learning to predict future adherence to physical activity programs",2019,"","","","",112,"2022-07-13 10:07:05","","10.1186/s12911-019-0890-0","","",,,,,16,5.33,3,5,3,"","",""
2,"L. Yang, Yuncheng Dong, Jiafu Zhuang, Jun Yu Li","A Recognition Algorithm for Workpieces Based on the Machine Learning",2018,"","","","",113,"2022-07-13 10:07:05","","10.1109/ISCID.2018.10185","","",,,,,2,0.50,1,4,4,"In order to learn and grasp the predetermined workpieces for robot actively, a recognition algorithm based on machine learning is proposed. Compared with traditional algorithms, we replenish a MTSM (multi threshold space model) for getting clearer workpiece shapes. To automatically and compactly learn workpieces knowledge, both shape and gradient features are designed to express the specific object by aid of contour mask, meanwhile, the compound descriptors are fed into a SVM classifier and they are trained jointly to minimize a classification loss. Finally, we adopt density estimation to acquire the grasping point of the workpieces from MTSM. Experimental result of workpieces grasping demonstrates the effectiveness and stability in complex environment, and the proposed algorithm is robust to rotation, scaling and deformation of shapes.","",""
2,"V. Chernozhukov, W. Newey, Victor Quintas-Martinez, Vasilis Syrgkanis","RieszNet and ForestRiesz: Automatic Debiased Machine Learning with Neural Nets and Random Forests",2021,"","","","",114,"2022-07-13 10:07:05","","","","",,,,,2,2.00,1,4,1,"Many causal and policy effects of interest are deﬁned by linear functionals of high-dimensional or non-parametric regression functions. √ n consistent and asymptotically normal estimation of the object of interest requires debiasing to reduce the effects of regularization and/or model selection on the object of interest. Debiasing is typically achieved by adding a correction term to the plug-in estimator of the functional, which leads to properties such as semi-parametric efﬁ-ciency, double robustness, and Neyman orthogo-nality. We implement an automatic debiasing pro-cedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. Our method only relies on black-box evaluation oracle access to the linear functional and does not require knowledge of its analytic form. We propose a multitasking Neural Net debiasing method with stochastic gradient descent minimization of a combined Riesz representer and regression loss, while sharing representation layers for the two functions. We also propose a Random Forest method which learns a locally linear representation of the Riesz function. Even though our method applies to arbitrary functionals, we experimentally ﬁnd that it performs well compared to the state of art neural net based algorithm of Shi et al. (2019) for the case of the average treatment effect functional. We also evaluate our method on the problem of estimating average marginal effects with continuous treat-ments, using semi-synthetic data of gasoline price changes on gasoline demand. Code available at","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",115,"2022-07-13 10:07:05","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
11,"J. Halotel, V. Demyanov, A. Gardiner","Value of Geologically Derived Features in Machine Learning Facies Classification",2019,"","","","",116,"2022-07-13 10:07:05","","10.1007/s11004-019-09838-0","","",,,,,11,3.67,4,3,3,"","",""
32,"K. Javed","A robust and reliable data-driven prognostics approach based on Extreme Learning Machine and Fuzzy Clustering",2014,"","","","",117,"2022-07-13 10:07:05","","","","",,,,,32,4.00,32,1,8,"Prognostics and Health Management (PHM) aims at extending the life cycle of engineerin gassets, while reducing exploitation and maintenance costs. For this reason,prognostics is considered as a key process with future capabilities. Indeed, accurateestimates of the Remaining Useful Life (RUL) of an equipment enable defining furtherplan of actions to increase safety, minimize downtime, ensure mission completion andefficient production.Recent advances show that data-driven approaches (mainly based on machine learningmethods) are increasingly applied for fault prognostics. They can be seen as black-boxmodels that learn system behavior directly from Condition Monitoring (CM) data, usethat knowledge to infer its current state and predict future progression of failure. However,approximating the behavior of critical machinery is a challenging task that canresult in poor prognostics. As for understanding, some issues of data-driven prognosticsmodeling are highlighted as follows. 1) How to effectively process raw monitoringdata to obtain suitable features that clearly reflect evolution of degradation? 2) Howto discriminate degradation states and define failure criteria (that can vary from caseto case)? 3) How to be sure that learned-models will be robust enough to show steadyperformance over uncertain inputs that deviate from learned experiences, and to bereliable enough to encounter unknown data (i.e., operating conditions, engineering variations,etc.)? 4) How to achieve ease of application under industrial constraints andrequirements? Such issues constitute the problems addressed in this thesis and have ledto develop a novel approach beyond conventional methods of data-driven prognostics.","",""
8,"A. Faisst, A. Prakash, P. Capak, Bomee Lee","How to Find Variable Active Galactic Nuclei with Machine Learning",2019,"","","","",118,"2022-07-13 10:07:05","","10.3847/2041-8213/ab3581","","",,,,,8,2.67,2,4,3,"Machine-learning (ML) algorithms will play a crucial role in studying the large datasets delivered by new facilities over the next decade and beyond. Here, we investigate the capabilities and limits of such methods in finding galaxies with brightness-variable active galactic nuclei (AGN). Specifically, we focus on an unsupervised method based on self-organizing maps (SOM) that we apply to a set of nonparametric variability estimators. This technique allows us to maintain domain knowledge and systematics control while using all the advantages of ML. Using simulated light curves that match the noise properties of observations, we verify the potential of this algorithm in identifying variable light curves. We then apply our method to a sample of ~8300 WISE color-selected AGN candidates in Stripe 82, in which we have identified variable light curves by visual inspection. We find that with ML we can identify these variable classified AGN with a purity of 86% and a completeness of 66%, a performance that is comparable to that of more commonly used supervised deep-learning neural networks. The advantage of the SOM framework is that it enables not only a robust identification of variable light curves in a given dataset, but it is also a tool to investigate correlations between physical parameters in multi-dimensional space - such as the link between AGN variability and the properties of their host galaxies. Finally, we note that our method can be applied to any time-sampled light curve (e.g., supernovae, exoplanets, pulsars, and other transient events).","",""
48,"H. Aghakhani, Fabio Gritti, Francesco Mecca, Martina Lindorfer, Stefano Ortolani, D. Balzarotti, Giovanni Vigna, C. Kruegel","When Malware is Packin' Heat; Limits of Machine Learning Classifiers Based on Static Analysis Features",2020,"","","","",119,"2022-07-13 10:07:05","","10.14722/ndss.2020.24310","","",,,,,48,24.00,6,8,2,"Machine learning techniques are widely used in addition to signatures and heuristics to increase the detection rate of anti-malware software, as they automate the creation of detection models, making it possible to handle an ever-increasing number of new malware samples. In order to foil the analysis of anti-malware systems and evade detection, malware uses packing and other forms of obfuscation. However, few realize that benign applications use packing and obfuscation as well, to protect intellectual property and prevent license abuse. In this paper, we study how machine learning based on static analysis features operates on packed samples. Malware researchers have often assumed that packing would prevent machine learning techniques from building effective classifiers. However, both industry and academia have published results that show that machine-learning-based classifiers can achieve good detection rates, leading many experts to think that classifiers are simply detecting the fact that a sample is packed, as packing is more prevalent in malicious samples. We show that, different from what is commonly assumed, packers do preserve some information when packing programs that is “useful” for malware classification. However, this information does not necessarily capture the sample’s behavior. We demonstrate that the signals extracted from packed executables are not rich enough for machine-learning-based models to (1) generalize their knowledge to operate on unseen packers, and (2) be robust against adversarial examples. We also show that a naı̈ve application of machine learning techniques results in a substantial number of false positives, which, in turn, might have resulted in incorrect labeling of ground-truth data used in past work.","",""
16,"Xiaokai Liu, Cheng-lin Zhao, Pengbiao Wang, Yang Zhang, Tian-le Yang","Blind modulation classification algorithm based on machine learning for spatially correlated MIMO system",2017,"","","","",120,"2022-07-13 10:07:05","","10.1049/iet-com.2015.1222","","",,,,,16,3.20,3,5,5,"Spatial correlation is a decisive factor for pragmatic multiple-input multiple-output (MIMO) system, simultaneously bringing about some problems in the received signal modulation identification respect. In this study, the authors focus on blind digital modulation identification in the spatially correlated MIMO system and deliver a robust signal recognition algorithm based on extreme learning machine (ELM) and higher order statistical features for MIMO signal identification without a priori knowledge of the channel and signal parameters. The superiority of ELM lies in random selections of hidden nodes and ascertains output weights analytically, which result in lower computational complexity. Theoretically, this algorithm has a tendency to supply excellent generalisation performance at staggering learning rate. Further, the simulation results indicate that the ELM could reap a perfectly acceptable recognition performance and thus provides a solid ground structure for tackling MIMO modulation challenges in low signal-to-noise ratio.","",""
32,"Baifeng Shi, Dinghuai Zhang, Qi Dai, Zhanxing Zhu, Yadong Mu, Jingdong Wang","Informative Dropout for Robust Representation Learning: A Shape-bias Perspective",2020,"","","","",121,"2022-07-13 10:07:05","","","","",,,,,32,16.00,5,6,2,"Convolutional Neural Networks (CNNs) are known to rely more on local texture rather than global shape when making decisions. Recent work also indicates a close relationship between CNN's texture-bias and its robustness against distribution shift, adversarial perturbation, random corruption, etc. In this work, we attempt at improving various kinds of robustness universally by alleviating CNN's texture bias. With inspiration from the human visual system, we propose a light-weight model-agnostic method, namely Informative Dropout (InfoDrop), to improve interpretability and reduce texture bias. Specifically, we discriminate texture from shape based on local self-information in an image, and adopt a Dropout-like algorithm to decorrelate the model output from the local texture. Through extensive experiments, we observe enhanced robustness under various scenarios (domain generalization, few-shot classification, image corruption, and adversarial perturbation). To the best of our knowledge, this work is one of the earliest attempts to improve different kinds of robustness in a unified model, shedding new light on the relationship between shape-bias and robustness, also on new approaches to trustworthy machine learning algorithms. Code is available at this https URL.","",""
3,"Yusuke Kawamoto","Towards Logical Specification of Statistical Machine Learning",2019,"","","","",122,"2022-07-13 10:07:05","","10.1007/978-3-030-30446-1_16","","",,,,,3,1.00,3,1,3,"","",""
5,"J. M. Alves, L. Honório, Miriam A. M. Capretz","ML4IoT: A Framework to Orchestrate Machine Learning Workflows on Internet of Things Data",2019,"","","","",123,"2022-07-13 10:07:05","","10.1109/ACCESS.2019.2948160","","",,,,,5,1.67,2,3,3,"Internet of Things (IoT) applications generate vast amounts of real-time data. Temporal analysis of these data series to discover behavioural patterns may lead to qualified knowledge affecting a broad range of industries. Hence, the use of machine learning (ML) algorithms over IoT data has the potential to improve safety, economy, and performance in critical processes. However, creating ML workflows at scale is a challenging task that depends upon both production and specialized skills. Such tasks require investigation, understanding, selection, and implementation of specific ML workflows, which often lead to bottlenecks, production issues, and code management complexity and even then may not have a final desirable outcome. This paper proposes the Machine Learning Framework for IoT data (ML4IoT), which is designed to orchestrate ML workflows, particularly on large volumes of data series. The ML4IoT framework enables the implementation of several types of ML models, each one with a different workflow. These models can be easily configured and used through a simple pipeline. ML4IoT has been designed to use container-based components to enable training and deployment of various ML models in parallel. The results obtained suggest that the proposed framework can manage real-world IoT heterogeneous data by providing elasticity, robustness, and performance.","",""
6,"Roberto Medico, D. Spina, D. Vande Ginste, D. Deschrijver, T. Dhaene","Machine-Learning-Based Error Detection and Design Optimization in Signal Integrity Applications",2019,"","","","",124,"2022-07-13 10:07:05","","10.1109/TCPMT.2019.2916902","","",,,,,6,2.00,1,5,3,"Evaluating the robustness of integrated circuits (ICs) against noise and disturbances is of crucial importance in signal integrity (SI) applications. In this paper, the addressed challenge is to build a software-based framework allowing for automated detection of failures and fast simulation-based evaluation of designs. In particular, these tasks are here addressed using anomaly detection (AD), a branch of machine learning (ML) techniques focused on identifying erroneous or deviant data. In the proposed framework, the ML model only requires the time-domain waveforms and no additional knowledge about the circuit nor about the errors to be identified. Specifically, a two-step approach to detect anomalous behaviors in output waveforms of digital ICs is proposed, comprising a first phase where the ML models are trained to learn relevant features describing the data and a second one where those features are used to identify anomalies with unsupervised or semisupervised AD techniques. Two relevant application examples validate the performance and flexibility of the proposed method.","",""
4,"Niko Murrell, Ryan Bradley, N. Bajaj, Julie Whitney, G. Chiu","A Method for Sensor Reduction in a Supervised Machine Learning Classification System",2019,"","","","",125,"2022-07-13 10:07:05","","10.1109/TMECH.2018.2881889","","",,,,,4,1.33,1,5,3,"Smart devices employing interconnected sensors for feedback and control are being rapidly adopted. Many useful applications for these devices are in markets that demand cost-conscious solutions. Traditional machine-learning-based control systems often rely on multiple measurements from many sensors to achieve performance targets. An alternative method is presented that leverages a time-series output produced by a single sensor. By using domain expert knowledge, the time-series output is discretized into finite intervals that correspond to the physical events occurring in the system. Statistical measures are taken across these intervals to serve as the features to the machine learning system. Additional features that decouple key physical metrics are identified, improving the performance of the system. This novel approach requires a more modest dataset and does not compromise performance. The resulting development effort is significantly more cost-effective than traditional sensor classification systems, not only due to the reduced sensor count, but also due to a significantly simplified and more robust algorithm development and testing step. Results are presented with the case study of a media-type classification system within a printing system, which was deployed to the field as a commercial product.","",""
4,"E. Glaab, Armin Rauschenberger, R. Banzi, C. Gerardi, Paula Garcia, J. Demotes","Biomarker discovery studies for patient stratification using machine learning analysis of omics data: a scoping review",2021,"","","","",126,"2022-07-13 10:07:05","","10.1136/bmjopen-2021-053674","","",,,,,4,4.00,1,6,1,"Objective To review biomarker discovery studies using omics data for patient stratification which led to clinically validated FDA-cleared tests or laboratory developed tests, in order to identify common characteristics and derive recommendations for future biomarker projects. Design Scoping review. Methods We searched PubMed, EMBASE and Web of Science to obtain a comprehensive list of articles from the biomedical literature published between January 2000 and July 2021, describing clinically validated biomarker signatures for patient stratification, derived using statistical learning approaches. All documents were screened to retain only peer-reviewed research articles, review articles or opinion articles, covering supervised and unsupervised machine learning applications for omics-based patient stratification. Two reviewers independently confirmed the eligibility. Disagreements were solved by consensus. We focused the final analysis on omics-based biomarkers which achieved the highest level of validation, that is, clinical approval of the developed molecular signature as a laboratory developed test or FDA approved tests. Results Overall, 352 articles fulfilled the eligibility criteria. The analysis of validated biomarker signatures identified multiple common methodological and practical features that may explain the successful test development and guide future biomarker projects. These include study design choices to ensure sufficient statistical power for model building and external testing, suitable combinations of non-targeted and targeted measurement technologies, the integration of prior biological knowledge, strict filtering and inclusion/exclusion criteria, and the adequacy of statistical and machine learning methods for discovery and validation. Conclusions While most clinically validated biomarker models derived from omics data have been developed for personalised oncology, first applications for non-cancer diseases show the potential of multivariate omics biomarker design for other complex disorders. Distinctive characteristics of prior success stories, such as early filtering and robust discovery approaches, continuous improvements in assay design and experimental measurement technology, and rigorous multicohort validation approaches, enable the derivation of specific recommendations for future studies.","",""
1,"E. Kuiper, Efthymios Constantinides, S. Vries, Robert F. Marinescu-Muster, F. Metzner","A Framework of Unsupervised Machine Learning Algorithms for User Profiling",2019,"","","","",127,"2022-07-13 10:07:05","","","","",,,,,1,0.33,0,5,3,"Organizations often have difficulties to extract knowledge from data and selecting appropriate Machine Learning algorithms in order to develop accurate Behavioural Profiles or user segments. Moreover, marketing departments often lack a fundamental understanding on data-driven segmentation methodologies. This paper aims to develop a framework outlining Unsupervised Machine Learning algorithms for the purpose of User Profiling with respect to important data properties. A systematic literature review was conducted on the most prominent Unsupervised Machine Learning algorithms and their requirements regarding the characteristics of the dataset. A framework is proposed outlining various Unsupervised Machine Learning algorithms for User Profiling. It provides two-stage clustering strategies for categorical, numerical, and mixed types of data with respect to the data size and data dimensionality. The first stage consists of an hierarchical or model-based clustering algorithm to determine the number of clusters. In the second stage, a non-hierarchical clustering algorithm is applied for cluster refinement. The framework can support researchers and practitioners to determine which Unsupervised Machine Learning algorithms are appropriate for developing robust behavioural profiles or data-driven user segments.","",""
376,"Rui Zhao, Ruqiang Yan, Jinjiang Wang, K. Mao","Learning to Monitor Machine Health with Convolutional Bi-Directional LSTM Networks",2017,"","","","",128,"2022-07-13 10:07:05","","10.3390/s17020273","","",,,,,376,75.20,94,4,5,"In modern manufacturing systems and industries, more and more research efforts have been made in developing effective machine health monitoring systems. Among various machine health monitoring approaches, data-driven methods are gaining in popularity due to the development of advanced sensing and data analytic techniques. However, considering the noise, varying length and irregular sampling behind sensory data, this kind of sequential data cannot be fed into classification and regression models directly. Therefore, previous work focuses on feature extraction/fusion methods requiring expensive human labor and high quality expert knowledge. With the development of deep learning methods in the last few years, which redefine representation learning from raw data, a deep neural network structure named Convolutional Bi-directional Long Short-Term Memory networks (CBLSTM) has been designed here to address raw sensory data. CBLSTM firstly uses CNN to extract local features that are robust and informative from the sequential input. Then, bi-directional LSTM is introduced to encode temporal information. Long Short-Term Memory networks (LSTMs) are able to capture long-term dependencies and model sequential data, and the bi-directional structure enables the capture of past and future contexts. Stacked, fully-connected layers and the linear regression layer are built on top of bi-directional LSTMs to predict the target value. Here, a real-life tool wear test is introduced, and our proposed CBLSTM is able to predict the actual tool wear based on raw sensory data. The experimental results have shown that our model is able to outperform several state-of-the-art baseline methods.","",""
3,"A. Appice, P. Rodrigues, V. S. Costa, Carlos Soares, João Gama, A. Jorge","Machine Learning and Knowledge Discovery in Databases",2015,"","","","",129,"2022-07-13 10:07:05","","10.1007/978-3-319-23525-7","","",,,,,3,0.43,1,6,7,"","",""
3,"Nicolas Känzig, Roland Meier, L. Gambazzi, Vincent Lenders, L. Vanbever","Machine Learninģ-based Detection of C&C Channels with a Focus on the Locked Shields Cyber Defense Exercise",2019,"","","","",130,"2022-07-13 10:07:05","","10.23919/CYCON.2019.8756814","","",,,,,3,1.00,1,5,3,"The diversity of applications and devices in enterprise networks combined with large traffic volumes make it inherently challenging to quickly identify malicious traffic. When incidents occur, emergency response teams often lose precious time in reverse-engineering the network topology and configuration before they can focus on malicious activities and digital forensics. In this paper, we present a system that quickly and reliably identifies Command and Control (C&C) channels without prior network knowledge. The key idea is to train a classifier using network traffic from attacks that happened in the past and use it to identify C&C connections in the current traffic of other networks. Specifically, we leverage the fact that - while benign traffic differs - malicious traffic bears similarities across networks (e.g., devices participating in a botnet act in a similar manner irrespective of their location). To ensure performance and scalability, we use a random forest classifier based on a set of computationally-efficient features tailored to the detection of C&C traffic. In order to prevent attackers from outwitting our classifier, we tune the model parameters to maximize robustness. We measure high resilience against possible attacks - e.g., attempts to camouflaging C&C flows as benign traffic - and packet loss during the inference. We have implemented our approach and we show its practicality on a real use case: Locked Shields, the world's largest cyber defense exercise. In Locked Shields, defenders have limited resources to protect a large, heterogeneous network against unknown attacks. Using recorded datasets (from 2017 and 2018) from a participating team, we show that our classifier is able to identify C&C channels with 99% precision and over 90% recall in near real time and with realistic resource requirements. If the team had used our system in 2018, it would have discovered 10 out of 12 C&C servers in the first hours of the exercise.","",""
2,"Hadi Saki, Nabeel Khan, M. Martini, Moustafa M. Nasralla","Machine Learning Based Frame Classification for Videos Transmitted over Mobile Networks",2019,"","","","",131,"2022-07-13 10:07:05","","10.1109/CAMAD.2019.8858448","","",,,,,2,0.67,1,4,3,"In wireless systems, content-aware MAC layer scheduling strategies contribute to supporting an adequate Quality of Service (QoS) and Quality of Experience (QoE) for the users. Content related information that can be used in such strategies include information about the video frame type (Intra-Predicted, Inter-Predicted or Backward-Predicted). Frame type identification and prediction in the MAC layer scheduler (as well as in other network locations) is hence a critical part of content-aware resource management and traffic engineering in video streaming. It is important to note that, when encryption is adopted at the upper layers, packet inspection is not possible. To address this issue, we propose an adaptive clustering and prediction algorithm. The algorithm uses unsupervised clustering combined with an adaptive classification approach to automatically group packets into frame clusters. Unlike conventional video traffic classifiers, our approach requires neither a-priori knowledge of video frames pattern nor a training set of frames. Instead, this approach automatically classifies packets into different frame types by employing simple statistical features. The proposed method continuously learns from the upcoming traffic thus the proper frame class labels can be easily discovered. Furthermore, the proposed method uses an autoregressive integrated moving average (ARIMA) algorithm to predict the upcoming traffic frame type. A large number of experiments has been carried out using data from several video samples. Results show the robustness and the effectiveness of our classification method, which is capable of achieving a detection rate of up to 97.3% for I frames and overall 2 identification accuracy of 91.3% (for all I, P and B frames).","",""
5,"M. Usama, Muhammad Asim, Junaid Qadir, Ala Al-Fuqaha, M. Imran","Adversarial Machine Learning Attack on Modulation Classification",2019,"","","","",132,"2022-07-13 10:07:05","","10.1109/UCET.2019.8881843","","",,,,,5,1.67,1,5,3,"Modulation classification is an important component of cognitive self-driving networks. Recently many ML-based modulation classification methods have been proposed. We have evaluated the robustness of 9 ML-based modulation classifiers against the powerful Carlini & Wagner (C-W) attack and showed that the current ML-based modulation classifiers do not provide any deterrence against adversarial ML examples. To the best of our knowledge, we are the first to report the results of the application of the C-W attack for creating adversarial examples against various ML models for modulation classification.","",""
5,"Tom Z. Jiahao, M. Hsieh, E. Forgoston","Knowledge-based learning of nonlinear dynamics and chaos",2020,"","","","",133,"2022-07-13 10:07:05","","10.1063/5.0065617","","",,,,,5,2.50,2,3,2,"Extracting predictive models from nonlinear systems is a central task in scientific machine learning. One key problem is the reconciliation between modern data-driven approaches and first principles. Despite rapid advances in machine learning techniques, embedding domain knowledge into datadriven models remains a challenge. In this work, we present a universal learning framework for extracting predictive models from nonlinear systems based on observations. Our framework can readily incorporate first principle knowledge because it naturally models nonlinear systems as continuous-time systems. This both improves the extracted models’ extrapolation power and reduces the amount of data needed for training. In addition, our framework has the advantages of robustness to observational noise and applicability to irregularly sampled data. We demonstrate the effectiveness of our scheme by learning predictive models for a wide variety of systems including a stiff Van der Pol oscillator, the Lorenz system, and the Kuramoto-Sivashinsky equation. For the Lorenz system, different types of domain knowledge are incorporated to demonstrate the strength of knowledge embedding in data-driven system identification.","",""
12,"F. López Seguí, Ricardo Ander Egg Aguilar, Gabriel de Maeztu, A. García‐Altés, Francesc García Cuyàs, Sandra Walsh, Marta Sagarra Castro, J. Vidal-Alaball","Teleconsultations between Patients and Healthcare Professionals in Primary Care in Catalonia: The Evaluation of Text Classification Algorithms Using Supervised Machine Learning",2020,"","","","",134,"2022-07-13 10:07:05","","10.3390/ijerph17031093","","",,,,,12,6.00,2,8,2,"Background: The primary care service in Catalonia has operated an asynchronous teleconsulting service between GPs and patients since 2015 (eConsulta), which has generated some 500,000 messages. New developments in big data analysis tools, particularly those involving natural language, can be used to accurately and systematically evaluate the impact of the service. Objective: The study was intended to assess the predictive potential of eConsulta messages through different combinations of vector representation of text and machine learning algorithms and to evaluate their performance. Methodology: Twenty machine learning algorithms (based on five types of algorithms and four text representation techniques) were trained using a sample of 3559 messages (169,102 words) corresponding to 2268 teleconsultations (1.57 messages per teleconsultation) in order to predict the three variables of interest (avoiding the need for a face-to-face visit, increased demand and type of use of the teleconsultation). The performance of the various combinations was measured in terms of precision, sensitivity, F-value and the ROC curve. Results: The best-trained algorithms are generally effective, proving themselves to be more robust when approximating the two binary variables “avoiding the need of a face-to-face visit” and “increased demand” (precision = 0.98 and 0.97, respectively) rather than the variable “type of query” (precision = 0.48). Conclusion: To the best of our knowledge, this study is the first to investigate a machine learning strategy for text classification using primary care teleconsultation datasets. The study illustrates the possible capacities of text analysis using artificial intelligence. The development of a robust text classification tool could be feasible by validating it with more data, making it potentially more useful for decision support for health professionals.","",""
6,"Heng Li, ShiYao Zhou, Wei Yuan, Xiapu Luo, Cuiying Gao, Shuiyan Chen","Robust Android Malware Detection against Adversarial Example Attacks",2021,"","","","",135,"2022-07-13 10:07:05","","10.1145/3442381.3450044","","",,,,,6,6.00,1,6,1,"Adversarial examples pose severe threats to Android malware detection because they can render the machine learning based detection systems useless. How to effectively detect Android malware under various adversarial example attacks becomes an essential but very challenging issue. Existing adversarial example defense mechanisms usually rely heavily on the instances or the knowledge of adversarial examples, and thus their usability and effectiveness are significantly limited because they often cannot resist the unseen-type adversarial examples. In this paper, we propose a novel robust Android malware detection approach that can resist adversarial examples without requiring their instances or knowledge by jointly investigating malware detection and adversarial example defenses. More precisely, our approach employs a new VAE (variational autoencoder) and an MLP (multi-layer perceptron) to detect malware, and combines their detection outcomes to make the final decision. In particular, we share a feature extraction network between the VAE and the MLP to reduce model complexity and design a new loss function to disentangle the features of different classes, hence improving detection performance. Extensive experiments confirm our model’s advantage in accuracy and robustness. Our method outperforms 11 state-of-the-art robust Android malware detection models when resisting 7 kinds of adversarial example attacks.","",""
10,"J. Watson, C. Holmes","Machine learning analysis plans for randomised controlled trials: detecting treatment effect heterogeneity with strict control of type I error",2020,"","","","",136,"2022-07-13 10:07:05","","10.1186/s13063-020-4076-y","","",,,,,10,5.00,5,2,2,"","",""
4,"Neha Shah, D. Mohan, J. Bashingwa, O. Ummer, A. Chakraborty, A. Lefevre","Using Machine Learning to Optimize the Quality of Survey Data: Protocol for a Use Case in India",2020,"","","","",137,"2022-07-13 10:07:05","","10.2196/17619","","",,,,,4,2.00,1,6,2,"Background Data quality is vital for ensuring the accuracy, reliability, and validity of survey findings. Strategies for ensuring survey data quality have traditionally used quality assurance procedures. Data analytics is an increasingly vital part of survey quality assurance, particularly in light of the increasing use of tablets and other electronic tools, which enable rapid, if not real-time, data access. Routine data analytics are most often concerned with outlier analyses that monitor a series of data quality indicators, including response rates, missing data, and reliability of coefficients for test-retest interviews. Machine learning is emerging as a possible tool for enhancing real-time data monitoring by identifying trends in the data collection, which could compromise quality. Objective This study aimed to describe methods for the quality assessment of a household survey using both traditional methods as well as machine learning analytics. Methods In the Kilkari impact evaluation’s end-line survey amongst postpartum women (n=5095) in Madhya Pradesh, India, we plan to use both traditional and machine learning–based quality assurance procedures to improve the quality of survey data captured on maternal and child health knowledge, care-seeking, and practices. The quality assurance strategy aims to identify biases and other impediments to data quality and includes seven main components: (1) tool development, (2) enumerator recruitment and training, (3) field coordination, (4) field monitoring, (5) data analytics, (6) feedback loops for decision making, and (7) outcomes assessment. Analyses will include basic descriptive and outlier analyses using machine learning algorithms, which will involve creating features from time-stamps, “don’t know” rates, and skip rates. We will also obtain labeled data from self-filled surveys, and build models using k-folds cross-validation on a training data set using both supervised and unsupervised learning algorithms. Based on these models, results will be fed back to the field through various feedback loops. Results Data collection began in late October 2019 and will span through March 2020. We expect to submit quality assurance results by August 2020. Conclusions Machine learning is underutilized as a tool to improve survey data quality in low resource settings. Study findings are anticipated to improve the overall quality of Kilkari survey data and, in turn, enhance the robustness of the impact evaluation. More broadly, the proposed quality assurance approach has implications for data capture applications used for special surveys as well as in the routine collection of health information by health workers. International Registered Report Identifier (IRRID) DERR1-10.2196/17619","",""
4,"A. Morera, J. Martínez de Aragón, J. Bonet, Jingjing Liang, S. de-Miguel","Performance of statistical and machine learning-based methods for predicting biogeographical patterns of fungal productivity in forest ecosystems",2020,"","","","",138,"2022-07-13 10:07:05","","10.21203/rs.3.rs-122045/v1","","",,,,,4,2.00,1,5,2,"Background The prediction of biogeographical patterns from a large number of driving factors with complex interactions, correlations and non-linear dependences require advanced analytical methods and modeling tools. This study compares different statistical and machine learning-based models for predicting fungal productivity biogeographical patterns as a case study for the thorough assessment of the performance of alternative modeling approaches to provide accurate and ecologically-consistent predictions. Methods We evaluated and compared the performance of two statistical modeling techniques, namely, generalized linear mixed models and geographically weighted regression, and four techniques based on different machine learning algorithms, namely, random forest, extreme gradient boosting, support vector machine and artificial neural network to predict fungal productivity. Model evaluation was conducted using a systematic methodology combining random, spatial and environmental blocking together with the assessment of the ecological consistency of spatially-explicit model predictions according to scientific knowledge. Results Fungal productivity predictions were sensitive to the modeling approach and the number of predictors used. Moreover, the importance assigned to different predictors varied between machine learning modeling approaches. Decision tree-based models increased prediction accuracy by more than 10% compared to other machine learning approaches, and by more than 20% compared to statistical models, and resulted in higher ecological consistence of the predicted biogeographical patterns of fungal productivity. Conclusions Decision tree-based models were the best approach for prediction both in sampling-like environments as well as in extrapolation beyond the spatial and climatic range of the modeling data. In this study, we show that proper variable selection is crucial to create robust models for extrapolation in biophysically differentiated areas. This allows for reducing the dimensions of the ecosystem space described by the predictors of the models, resulting in higher similarity between the modeling data and the environmental conditions over the whole study area. When dealing with spatial-temporal data in the analysis of biogeographical patterns, environmental blocking is postulated as a highly informative technique to be used in cross-validation to assess the prediction error over larger scales.","",""
5,"Konstantinos Konstantinidis, A. Ramamoorthy","ByzShield: An Efficient and Robust System for Distributed Training",2020,"","","","",139,"2022-07-13 10:07:05","","","","",,,,,5,2.50,3,2,2,"Training of large scale models on distributed clusters is a critical component of the machine learning pipeline. However, this training can easily be made to fail if some workers behave in an adversarial (Byzantine) fashion whereby they return arbitrary results to the parameter server (PS). A plethora of existing papers consider a variety of attack models and propose robust aggregation and/or computational redundancy to alleviate the effects of these attacks. In this work we consider an omniscient attack model where the adversary has full knowledge about the gradient computation assignments of the workers and can choose to attack (up to) any q out of n worker nodes to induce maximal damage. Our redundancy-based method ByzShield leverages the properties of bipartite expander graphs for the assignment of tasks to workers; this helps to effectively mitigate the effect of the Byzantine behavior. Specifically, we demonstrate an upper bound on the worst case fraction of corrupted gradients based on the eigenvalues of our constructions which are based on mutually orthogonal Latin squares and Ramanujan graphs. Our numerical experiments indicate over a 36% reduction on average in the fraction of corrupted gradients compared to the state of the art. Likewise, our experiments on training followed by image classification on the CIFAR-10 dataset show that ByzShield has on average a 20% advantage in accuracy under the most sophisticated attacks. ByzShield also tolerates a much larger fraction of adversarial nodes compared to prior work.","",""
5,"Y. Dehbi, André Henn, G. Gröger, Viktor Stroh, L. Plümer","Robust and fast reconstruction of complex roofs with active sampling from 3D point clouds",2020,"","","","",140,"2022-07-13 10:07:05","","10.1111/tgis.12659","","",,,,,5,2.50,1,5,2,"This article proposes a novel method for the 3D reconstruction of LoD2 buildings from LiDAR data. We propose an active sampling strategy which applies a cascade of filters focusing on promising samples at an early stage, thus avoiding the pitfalls of RANSAC‐based approaches. Filters are based on prior knowledge represented by (nonparametric) density distributions. In our approach samples are pairs of surflets—3D points together with normal vectors derived from a plane approximation of their neighborhood. Surflet pairs provide parameters for model candidates such as azimuth, inclination and ridge height, as well as parameters estimating internal precision and consistency. This provides a ranking of roof model candidates and leads to a small number of promising hypotheses. Building footprints are derived in a preprocessing step using machine learning methods, in particular support vector machines.","",""
3,"Agnese Chiatti, E. Motta, E. Daga, G. Bardaro","Fit to Measure: Reasoning about Sizes for Robust Object Recognition",2020,"","","","",141,"2022-07-13 10:07:05","","","","",,,,,3,1.50,1,4,2,"Service robots can help with many of our daily tasks, especially in those cases where it is inconvenient or unsafe for us to intervene: e.g., under extreme weather conditions or when social distance needs to be maintained. However, before we can successfully delegate complex tasks to robots, we need to enhance their ability to make sense of dynamic, real world environments. In this context, the first prerequisite to improving the Visual Intelligence of a robot is building robust and reliable object recognition systems. While object recognition solutions are traditionally based on Machine Learning methods, augmenting them with knowledge based reasoners has been shown to improve their performance. In particular, based on our prior work on identifying the epistemic requirements of Visual Intelligence, we hypothesise that knowledge of the typical size of objects could significantly improve the accuracy of an object recognition system. To verify this hypothesis, in this paper we present an approach to integrating knowledge about object sizes in a ML based architecture. Our experiments in a real world robotic scenario show that this combined approach ensures a significant performance increase over state of the art Machine Learning methods.","",""
66,"S. Deeb, S. Tyanova, M. Hummel, M. Schmidt-Supprian, Jüergen Cox, M. Mann","Machine Learning-based Classification of Diffuse Large B-cell Lymphoma Patients by Their Protein Expression Profiles",2015,"","","","",142,"2022-07-13 10:07:05","","10.1074/mcp.M115.050245","","",,,,,66,9.43,11,6,7,"Characterization of tumors at the molecular level has improved our knowledge of cancer causation and progression. Proteomic analysis of their signaling pathways promises to enhance our understanding of cancer aberrations at the functional level, but this requires accurate and robust tools. Here, we develop a state of the art quantitative mass spectrometric pipeline to characterize formalin-fixed paraffin-embedded tissues of patients with closely related subtypes of diffuse large B-cell lymphoma. We combined a super-SILAC approach with label-free quantification (hybrid LFQ) to address situations where the protein is absent in the super-SILAC standard but present in the patient samples. Shotgun proteomic analysis on a quadrupole Orbitrap quantified almost 9,000 tumor proteins in 20 patients. The quantitative accuracy of our approach allowed the segregation of diffuse large B-cell lymphoma patients according to their cell of origin using both their global protein expression patterns and the 55-protein signature obtained previously from patient-derived cell lines (Deeb, S. J., D'Souza, R. C., Cox, J., Schmidt-Supprian, M., and Mann, M. (2012) Mol. Cell. Proteomics 11, 77–89). Expression levels of individual segregation-driving proteins as well as categories such as extracellular matrix proteins behaved consistently with known trends between the subtypes. We used machine learning (support vector machines) to extract candidate proteins with the highest segregating power. A panel of four proteins (PALD1, MME, TNFAIP8, and TBC1D4) is predicted to classify patients with low error rates. Highly ranked proteins from the support vector analysis revealed differential expression of core signaling molecules between the subtypes, elucidating aspects of their pathobiology.","",""
13,"D. Hagos, P. Engelstad, A. Yazidi, Ø. Kure","General TCP State Inference Model From Passive Measurements Using Machine Learning Techniques",2018,"","","","",143,"2022-07-13 10:07:05","","10.1109/ACCESS.2018.2833107","","",,,,,13,3.25,3,4,4,"Many applications in the Internet use the reliable end-to-end Transmission Control Protocol (TCP) as a transport protocol due to practical considerations. There are many different TCP variants widely in use, and each variant uses a specific congestion control algorithm to avoid congestion, while also attempting to share the underlying network capacity equally among the competing users. This paper shows how an intermediate node (e.g., a network operator) can identify the transmission state of the TCP client associated with a TCP flow by passively monitoring the TCP traffic. Here, we present a robust, scalable and generic machine learning-based method which may be of interest for network operators that experimentally infers Congestion Window (cwnd) and the underlying variant of loss-based TCP algorithms within a flow from passive traffic measurements collected at an intermediate node. The method can also be extended to predict other TCP transmission states of the client. We believe that our study also has a potential benefit and opportunity for researchers and scientists in the networking community from both academia and industry who want to assess the characteristics of TCP transmission states related to network congestion. We validate the robustness and scalability approach of our prediction model through a large number of controlled experiments. It turns out, surprisingly enough, that the learned prediction model performs reasonably well by leveraging knowledge from the emulated network when it is applied on a real-life scenario setting. Thus, our prediction model is general bearing similarity to the concept of transfer learning in the machine learning community. The accuracy of our experimental results both in an emulated network, realistic and combined scenario settings and across multiple TCP congestion control variants demonstrate that our model is reasonably effective and has considerable potential.","",""
2,"D. Eschweiler, J. Stegmaier","Robust 3D Cell Segmentation: Extending the View of Cellpose",2021,"","","","",144,"2022-07-13 10:07:05","","","","",,,,,2,2.00,1,2,1,"Increasing data set sizes of 3D microscopy imaging experiments demand for an automation of segmentation processes to be able to extract meaningful biomedical information. Due to the shortage of annotated 3D image data that can be used for machine learning-based approaches, 3D segmentation approaches are required to be robust and to generalize well to unseen data. The Cellpose approach proposed by Stringer et al. [1] proved to be such a generalist approach for cell instance segmentation tasks. In this paper, we extend the Cellpose approach to improve segmentation accuracy on 3D image data and we further show how the formulation of the gradient maps can be simplified while still being robust and reaching similar segmentation accuracy. The code is publicly available and was integrated into two established open-source applications that allow using the 3D extension of Cellpose without any programming knowledge.","",""
2,"H. Lam, Yibo Zeng","Complexity-Free Generalization via Distributionally Robust Optimization",2021,"","","","",145,"2022-07-13 10:07:05","","","","",,,,,2,2.00,1,2,1,"Established approaches to obtain generalization bounds in data-driven optimization and machine learning mostly build on solutions from empirical risk minimization (ERM), which depend crucially on the functional complexity of the hypothesis class. In this paper, we present an alternate route to obtain these bounds on the solution from distributionally robust optimization (DRO), a recent data-driven optimization framework based on worst-case analysis and the notion of ambiguity set to capture statistical uncertainty. In contrast to the hypothesis class complexity in ERM, our DRO bounds depend on the ambiguity set geometry and its compatibility with the true loss function. Notably, when using maximum mean discrepancy as a DRO distance metric, our analysis implies, to the best of our knowledge, the first generalization bound in the literature that depends solely on the true loss function, entirely free of any complexity measures or bounds on the hypothesis class.","",""
2,"T. Kyono, M. van der Schaar","Exploiting Causal Structure for Robust Model Selection in Unsupervised Domain Adaptation",2021,"","","","",146,"2022-07-13 10:07:05","","10.1109/tai.2021.3101185","","",,,,,2,2.00,1,2,1,"In many real-world settings, such as healthcare, machine learning models are trained and validated on one labeled domain and tested or deployed on another, where feature distributions differ, i.e., there is covariate shift. When annotations are costly or prohibitive, an unsupervised domain adaptation (UDA) regime can be leveraged requiring only unlabeled samples in the target domain. Existing UDA methods are unable to factor in a model’s predictive loss based on predictions in the target domain and, therefore, suboptimally leverage density ratios of only the input covariates in each domain. In this article, we propose a model selection method for leveraging model predictions on a target domain without labels by exploiting the domain invariance of causal structure. We assume or learn a causal graph from the source domain and select models that produce predicted distributions in the target domain that have the highest likelihood of fitting our causal graph. We thoroughly analyze our method under oracle knowledge using synthetic data. We then show on several real-world datasets, including several COVID-19 examples, that our method is able to improve on the state-of-the-art UDA algorithms for model selection.","",""
32,"Himabindu Lakkaraju, Nino Arsov, Osbert Bastani","Robust and Stable Black Box Explanations",2020,"","","","",147,"2022-07-13 10:07:05","","","","",,,,,32,16.00,11,3,2,"As machine learning black boxes are increasingly being deployed in real-world applications, there has been a growing interest in developing post hoc explanations that summarize the behaviors of these black boxes. However, existing algorithms for generating such explanations have been shown to lack stability and robustness to distribution shifts. We propose a novel framework for generating robust and stable explanations of black box models based on adversarial training. Our framework optimizes a minimax objective that aims to construct the highest fidelity explanation with respect to the worst-case over a set of adversarial perturbations. We instantiate this algorithm for explanations in the form of linear models and decision sets by devising the required optimization procedures. To the best of our knowledge, this work makes the first attempt at generating post hoc explanations that are robust to a general class of adversarial perturbations that are of practical interest. Experimental evaluation with real-world and synthetic datasets demonstrates that our approach substantially improves robustness of explanations without sacrificing their fidelity on the original data distribution.","",""
24,"Youhao Hu, Hai Wang, Z. Cao, Jinchuan Zheng, Zhaowu Ping, Long Chen, Xiaozheng Jin","Extreme-learning-machine-based FNTSM control strategy for electronic throttle",2020,"","","","",148,"2022-07-13 10:07:05","","10.1007/s00521-019-04446-9","","",,,,,24,12.00,3,7,2,"","",""
11,"Gonzalo Marín, P. Casas, G. Capdehourat","Deep in the Dark - Deep Learning-Based Malware Traffic Detection Without Expert Knowledge",2019,"","","","",149,"2022-07-13 10:07:05","","10.1109/SPW.2019.00019","","",,,,,11,3.67,4,3,3,"With the ever-growing occurrence of networking attacks, robust network security systems are essential to prevent and mitigate their harming effects. In recent years, machine learning-based systems have gain popularity for network security applications, usually considering the application of shallow models, where a set of expert handcrafted features are needed to pre-process the data before training. The main problem with this approach is that handcrafted features can fail to perform well given different kinds of scenarios and problems. Deep Learning models can solve this kind of issues using their ability to learn feature representations from input raw or basic, non-processed data. In this paper we explore the power of deep learning models on the specific problem of detection and classification of malware network traffic, using different representations for the input data. As a major advantage as compared to the state of the art, we consider raw measurements coming directly from the stream of monitored bytes as the input to the proposed models, and evaluate different raw-traffic feature representations, including packet and flow-level ones. Our results suggest that deep learning models can better capture the underlying statistics of malicious traffic as compared to classical, shallow-like models, even while operating in the dark, i.e., without any sort of expert handcrafted inputs.","",""
3,"Sahely Bhadra","Learning Robust Support Vector Machine Classifiers With Uncertain Observations",2012,"","","","",150,"2022-07-13 10:07:05","","","","",,,,,3,0.30,3,1,10,"The central theme of the thesis is to study linear and non linear SVM formulations in the presence of uncertain observations. The main contribution of this thesis is to derive robust classifiers from partial knowledge of the underlying uncertainty. In the case of linear classification, a new bounding scheme based on Bernstein inequality has been proposed, which models interval-valued uncertainty in a less conservative fashion and hence is expected to generalize better than the existing methods. Next, potential of partial information such as bounds on second order moments along with support information has been explored. Bounds on second order moments make the resulting classifiers robust to moment estimation errors. Uncertainty in the dataset will lead to uncertainty in the kernel matrices. A novel distribution free large deviation inequality has been proposed which handles uncertainty in kernels through co-positive programming in a chance constraint setting. Although such formulations are NP hard, under several cases of interest the problem reduces to a convex program. However, the independence assumption mentioned above, is restrictive and may not always define a valid uncertain kernel. To alleviate this problem an affine set based alternative is proposed and using a robust optimization framework the resultant problem is posed as a minimax problem. In both the cases of Chance Constraint Program or Robust Optimization (for non-linear SVM), mirror descent algorithm (MDA) like procedures have been applied.","",""
54,"M. S. Hossain Lipu, M. Hannan, A. Hussain, M. Saad, A. Ayob, M. Uddin","Extreme Learning Machine Model for State-of-Charge Estimation of Lithium-Ion Battery Using Gravitational Search Algorithm",2019,"","","","",151,"2022-07-13 10:07:05","","10.1109/TIA.2019.2902532","","",,,,,54,18.00,9,6,3,"This paper develops a state-of-charge (SOC) estimation model for a lithium-ion battery using an improved extreme learning machine (ELM) algorithm. ELM is suitable for an SOC estimation since the ELM algorithm has fast estimation speed, good generalization performance, and high accuracy. However, the performance of ELM is highly dependent on training accuracy and the number of neurons in a hidden layer. Hence, a gravitational search algorithm (GSA) is applied to improve the ELM computational intelligence by searching for the optimal value hidden layer neurons. The optimal ELM-based GSA model does not require internal battery knowledge and mathematical model for an SOC estimation. The model robustness is validated at different temperatures using different electric vehicle drive cycles. The performance of the ELM-GSA model is verified with two popular neural network methods: back-propagation neural network (BPNN) and radial basis function neural network (RBFNN). The results are evaluated using different error rates and computation costs. The results demonstrate that the ELM-based GSA model offers a higher accuracy and lower SOC error rate than those of BPNN-based GSA and RBFNN-based GSA models. Furthermore, a detailed comparative study between the proposed model and existing SOC strategies is conducted, which also demonstrates the superiority of the proposed model.","",""
13,"Nastasiya F. Grinberg, R. King","An evaluation of machine-learning for predicting phenotype: studies in yeast, rice, and wheat",2017,"","","","",152,"2022-07-13 10:07:05","","10.1007/s10994-019-05848-5","","",,,,,13,2.60,7,2,5,"","",""
18,"Rui Gao","Finite-Sample Guarantees for Wasserstein Distributionally Robust Optimization: Breaking the Curse of Dimensionality",2020,"","","","",153,"2022-07-13 10:07:05","","","","",,,,,18,9.00,18,1,2,"Wasserstein distributionally robust optimization (DRO) aims to find robust and generalizable solutions by hedging against data perturbations in Wasserstein distance. Despite its recent empirical success in operations research and machine learning, existing performance guarantees for generic loss functions are either overly conservative due to the curse of dimensionality, or plausible only in large sample asymptotics. In this paper, we develop a non-asymptotic framework for analyzing the out-of-sample performance for Wasserstein robust learning and the generalization bound for its related Lipschitz and gradient regularization problems. To the best of our knowledge, this gives the first finite-sample guarantee for generic Wasserstein DRO problems without suffering from the curse of dimensionality. Our results highlight the bias-variation trade-off intrinsic in the Wasserstein DRO, which automatically balances between the empirical mean of the loss and the variation of the loss, measured by the Lipschitz norm or the gradient norm of the loss. Our analysis is based on two novel methodological developments which are of independent interest: 1) a new concentration inequality characterizing the decay rate of large deviation probabilities by the variation of the loss and, 2) a localized Rademacher complexity theory based on the variation of the loss.","",""
381,"Maximilian Nickel, Volker Tresp, H. Kriegel","Factorizing YAGO: scalable machine learning for linked data",2012,"","","","",154,"2022-07-13 10:07:05","","10.1145/2187836.2187874","","",,,,,381,38.10,127,3,10,"Vast amounts of structured information have been published in the Semantic Web's Linked Open Data (LOD) cloud and their size is still growing rapidly. Yet, access to this information via reasoning and querying is sometimes difficult, due to LOD's size, partial data inconsistencies and inherent noisiness. Machine Learning offers an alternative approach to exploiting LOD's data with the advantages that Machine Learning algorithms are typically robust to both noise and data inconsistencies and are able to efficiently utilize non-deterministic dependencies in the data. From a Machine Learning point of view, LOD is challenging due to its relational nature and its scale. Here, we present an efficient approach to relational learning on LOD data, based on the factorization of a sparse tensor that scales to data consisting of millions of entities, hundreds of relations and billions of known facts. Furthermore, we show how ontological knowledge can be incorporated in the factorization to improve learning results and how computation can be distributed across multiple nodes. We demonstrate that our approach is able to factorize the YAGO~2 core ontology and globally predict statements for this large knowledge base using a single dual-core desktop computer. Furthermore, we show experimentally that our approach achieves good results in several relational learning tasks that are relevant to Linked Data. Once a factorization has been computed, our model is able to predict efficiently, and without any additional training, the likelihood of any of the 4.3 ⋅ 1014 possible triples in the YAGO~2 core ontology.","",""
4,"Donglin Jiang, Chen Shan, Zhihui Zhang","Federated Learning Algorithm Based on Knowledge Distillation",2020,"","","","",155,"2022-07-13 10:07:05","","10.1109/ICAICE51518.2020.00038","","",,,,,4,2.00,1,3,2,"Federated learning is a new scheme of distributed machine learning, which enables a large number of edge computing devices to jointly learn a shared model without private data sharing. Federated learning allows nodes to synchronize only the locally trained models instead of their own private data, which provides a guarantee for privacy and security. However, due to the challenges of heterogeneity in federated learning, which are: (1) heterogeneous model architecture among devices; (2) statistical heterogeneity in real federated dataset, which do not obey independent-identical-distribution, resulting in poor performance of traditional federated learning algorithms. To solve the problems above, this paper proposes FedDistill, a new distributed training method based on knowledge distillation. By introducing personalized model on each device, the personalized model aims to improve the local performance even in a situation that global model fails to adapt to the local dataset, thereby improving the ability and robustness of the global model. The improvement of the performance of local device benefits from the effect of knowledge distillation, which can guide the improvement of global model by knowledge transfer between heterogeneous networks. Experiments show that FedDistill can significantly improve the accuracy of classification tasks and meet the needs of heterogeneous users.","",""
4,"M. Nilashi, O. Ibrahim, H. Ahmadi, L. Shahmoradi, Sarminah Samad, Karamollah Bagherifard","A Recommendation Agent for Health Products Recommendation Using Dimensionality Reduction and Prediction Machine Learning Techniques",2018,"","","","",156,"2022-07-13 10:07:05","","","","",,,,,4,1.00,1,6,4,"In the current business practice, recommender agents are widely used in e-commerce domain to actively recommend the right items to online users. Traditional Collaborative Filtering (CF) recommender systems are developed based on single ratings which are used to match similar users based on their past ratings. Although these types of recommender systems have been successfully implemented in healthcare context, however the use of multi-criteria CF for health product recommendation has been rarely explored. The aim of this paper is to propose a new recommendation method based on multi-criteria CF to enhance the predictive accuracy of recommender systems in healthcare domain using clustering, dimensionality reduction and prediction machine learning methods. To do so, we develop a knowledge-based system to predict the users’ overall assessment value of health products using Mamdani’s fuzzy inference technique. Accordingly, we used Classification and Regression Trees (CART) to discover the fuzzy rules to be used in the fuzzy rule-based technique. To improve the recommendation accuracy of proposed multi-criteria CF, we apply a clustering technique and ensembles of fuzzy rule-based prediction models. We also use a robust dimensionality reduction technique, Higher Order Singular Value Decomposition (HOSVD), to find the similar users and products in each cluster to solve the sparsity issue.  We test the accuracy of recommendation method on two health products datasets with three criteria, Product Brand, Product Price and Product Quality, crawled from the online health products stores. Our experiments confirm that the proposed method can be a promising and effective intelligent system for healthcare products recommendation.","",""
12,"A. Menon, James A. Thompson-Colón, N. Washburn","Hierarchical Machine Learning Model for Mechanical Property Predictions of Polyurethane Elastomers From Small Datasets",2019,"","","","",157,"2022-07-13 10:07:05","","10.3389/fmats.2019.00087","","",,,,,12,4.00,4,3,3,"Polyurethanes are a broad class of material that finds application in coatings, foams, and solid elastomers. The urethane chemistry allows a diversity of monomers to be used, and prediction of mechanical properties, which are determined by complex interplay between monomer chemistry and chain architecture, is an unresolved challenge. Urethanes are based on aromatic or cyclic isocyanates and linear or branched polyols, and polymerization results in linear chains for bifunctional monomers or branched chains for multifunctional monomers. Strong intermolecular interactions between aromatic groups result in the formation of hard-segment domains that generate physical crosslinks between disorganized rubbery domains and anchor the material microstructure, contributing to resistance to deformation. Here, a general hierarchical machine learning (HML) model for predicting the stress-at-break, strain-at-break, and Tan δ for thermoplastic and thermoset polyurethanes is presented. The algorithm was trained on a library of 18 polymers with different diisocyanates, bifunctional or trifunctional polyols, and NCO:OH index. HML reduces data requirements through robust embedding of domain knowledge and surrogate data in a middle layer that bridges input variables (composition) and output responses (mechanical properties). In this work, the middle layer included information on overall polymer composition, predictions of chain architecture derived from Monte Carlo simulations of polymerization, information on interchain interactions from empirically derived molecular potentials and shifts in infrared (IR) spectroscopy absorbances. The HML predictions are shown to be more accurate than those from a Random Forest model directly relating composition and properties, suggesting that embedding domain knowledge provides significant advantages in predicting the properties of complex material systems based on small datasets.","",""
38,"R. Xu, Li Li, QuanQiu Wang","dRiskKB: a large-scale disease-disease risk relationship knowledge base constructed from biomedical text",2014,"","","","",158,"2022-07-13 10:07:05","","10.1186/1471-2105-15-105","","",,,,,38,4.75,13,3,8,"","",""
1,"Xiangdong Xing, Yongli Zhao, Yajie Li, Jie Zhang","Knowledge-Based Collective Self-learning for Alarm Prediction in Real Multi-Domain Autonomous Optical Networks",2020,"","","","",159,"2022-07-13 10:07:05","","10.1109/DRCN48652.2020.1570611066","","",,,,,1,0.50,0,4,2,"In this paper, a collective self-learning method based on knowledge sharing is proposed to predict alarms in multi-domain autonomous optical networks. The well-considered architecture is rendered, together with various alternatives for combining machine learning (ML) knowledge. The proposed method has been tested in the commercial large-scale multidomain network with 274 nodes and 487 links. Experimental results show that it can achieve high accuracy for alarm prediction. In addition, it can achieve similar performance with much better flexibility than a collective scheme based on training data sharing as well as more superior accuracy and robustness than an individual ML model.","",""
9,"M. Gunarathna, K. Sakai, Tamotsu Nakandakari, K. Momii, M.K.N. Kumari","Machine Learning Approaches to Develop Pedotransfer Functions for Tropical Sri Lankan Soils",2019,"","","","",160,"2022-07-13 10:07:05","","10.3390/w11091940","","",,,,,9,3.00,2,5,3,"Poor data availability on soil hydraulic properties in tropical regions hampers many studies, including crop and environmental modeling. The high cost and effort of measurement and the increasing demand for such data have driven researchers to search for alternative approaches. Pedotransfer functions (PTFs) are predictive functions used to estimate soil properties by easily measurable soil parameters. PTFs are popular in temperate regions, but few attempts have been made to develop PTFs in tropical regions. Regression approaches are widely used to develop PTFs worldwide, and recently a few attempts were made using machine learning methods. PTFs for tropical Sri Lankan soils have already been developed using classical multiple linear regression approaches. However, no attempts were made to use machine learning approaches. This study aimed to determine the applicability of machine learning algorithms in developing PTFs for tropical Sri Lankan soils. We tested three machine learning algorithms (artificial neural networks (ANN), k-nearest neighbor (KNN), and random forest (RF)) with three different input combination (sand, silt, and clay (SSC) percentages; SSC and bulk density (BD); SSC, BD, and organic carbon (OC)) to estimate volumetric water content (VWC) at −10 kPa, −33 kPa (representing field capacity (FC); however, most studies in Sri Lanka use −33 kPa as the FC) and −1500 kPa (representing the permanent wilting point (PWP)) of Sri Lankan soils. This analysis used the open-source data mining software in the Waikato Environment for Knowledge Analysis. Using a wrapper approach and best-first search method, we selected the most appropriate inputs to develop PTFs using different machine learning algorithms and input levels. We developed PTFs to estimate FC and PWP and compared them with the previously reported PTFs for tropical Sri Lankan soils. We found that RF was the best algorithm to develop PTFs for tropical Sri Lankan soils. We tried to further the development of PTFs by adding volumetric water content at −10 kPa as an input variable because it is quite an easily measurable parameter compared to the other targeted VWCs. With the addition of VWC at −10 kPa, all machine learning algorithms boosted the performance. However, RF was the best. We studied the functionality of finetuned PTFs and found that they can estimate the available water content of Sri Lankan soils as well as measurements-based calculations. We identified RF as a robust alternative to linear regression methods in developing PTFs to estimate field capacity and the permanent wilting point of tropical Sri Lankan soils. With those findings, we recommended that PTFs be developed using the RF algorithm in the related software to make up for the data gaps present in tropical regions.","",""
12,"K. Jablonka, D. Ongari, S. M. Moosavi, B. Smit","Using collective knowledge to assign oxidation states of metal cations in metal–organic frameworks",2021,"","","","",161,"2022-07-13 10:07:05","","10.1038/s41557-021-00717-y","","",,,,,12,12.00,3,4,1,"","",""
5,"D. Slaughter, Yun Zhang","Hyperspectral Vision-Based Machine Learning for Robust Plant Recognition in Autonomous Weed Control.",2011,"","","","",162,"2022-07-13 10:07:05","","","","",,,,,5,0.45,3,2,11,"While herbicide application and mechanical cultivation remain the primary means for weed control in agricultural production, the only solution to-date for weed control within close proximity of crop plants in the seedline is hand hoeing. To reduce manual labor cost and minimize herbicide usage for organic farming, this research developed an intelligent robotic system for automated weed detection and control within the seedline. The system utilized visible and near infrared (NIR) reflectance-based features in hyperspectral images of plant foliage for real-time species recognition. This technique is less computationally intensive than the shape- and texture-based machine vision methods. For real-time, in-field applications, this technique is superior to traditional shape-based machine vision, as it does not require singulation of individual plants, and is robust to visual occlusion and less susceptible to leaf morphological variation or damage.  A principle challenge of reflectance-based species recognition has been that the optical properties of plant foliage are functions of external growing conditions, and are greatly impacted by the variability in natural environment of agricultural fields. Prior studies of using visible and NIR spectroscopy for plant recognition were restricted to reflectance spectra measured on plants grown in a single season with exposure to a single environmental condition. This work, for the first time, demonstrated the potential of hyperspectral imaging technology for plant species identification under varying external factors of growing temperature, soil moisture and sunlight intensity, as well as over three multiple seasons of natural field environment. This work also developed adaptive learning techniques that mitigated environmental effects and provided solutions to robust plant recognition across the variation in the studied single conditions and seasons. Finally, the machine vision system was coupled with a thermal micro-dosing application system and validated under outdoor conditions for real-time automated weed control with heated food-grade oil. This research provided a complete solution to automated weed control in row crops using machine vision reflectance-based plant recognition.  The technical results of this research are summarized as below: Multispectral Bayesian classifiers were developed for distinguishing tomatoes among black nightshade and pigweed. The effects of variation in the three single environmental factors of temperature, soil moisture, and solar irradiance, on spectroscopy-based plant recognition demonstrated that (i) the optimum performance, ranging from 88.2% to 95.3%, occurred when the models were applied in same environmental conditions as represented in training; (ii) increasing the deviation of the validation conditions from the calibration conditions degraded the performance to 62.5–81.7%; (iii) environmental stress made the plant species more distinguishable and slightly improved the overall accuracy by 1.3–6.9% for same condition applications; (iv) the Bayesian classifiers optimized for the normal conditions demonstrated more robust plant recognition over environmental variations.  An environmentally-adaptive machine learning algorithm was developed for automatic site-specific recalibration of a Bayesian classifier in a dynamic environment. Validation performance of the classifier demonstrated that site-specific recalibration can be implemented by establishing the models exclusively with a fraction of new data (approximately 30 to 80 plants) without adverse impact due to ignoring the old data originally used to train the model. This method alleviated the bias produced by single-condition calibration and, overall, it improved the classification rates to 90.4–94.5% across variation in the three studied environmental factors.  Global calibration was another approach investigated to improve robustness of the Bayesian classifiers to varying environmental conditions of the three studied factors. The overall classification rates of global classifiers ranged from 90.0% to 93.0% and the performance stability was also improved. Global calibration was recommended as it was able to provide robust classification performances across variation in the three studied environmental factors and was comparable to the optimal results obtained when the single-condition models were cross-validated on their training conditions.  However, global calibration was not superior when used to discriminate tomatoes among weeds over three seasons (2005, 2006 and 2008), in which the plants were grown in a natural agricultural field environment. To improve the seasonal stability of plant recognition, a multiclassifier system was developed by integrating expert knowledge from historical data that most closely matched the new field environment. This method improved the performance of the global model by 10.5% (from 85.0% to over 95.5%) and provided an innovative direction for achieving robust plant recognition over the variation in field environment of multiple seasons.  In an outdoor test of the complete weed control system, the hyperspectral vision system correctly mapped, by species, 91.0% of plant canopy and the thermal micro-dosing system then delivered preheated (160oC) food-grade oil exclusively to the targeted weed foliage. Fifteen days post application, the system successfully controlled 95.8% of black nightshade and 93.8% of pigweed; while only 2.4% of tomato was damaged to the extent of non-viability due to inadvertent spray.","",""
11,"Nami Ashizawa, Naoto Yanai, Jason Paul Cruz, Shingo Okamura","Eth2Vec: Learning Contract-Wide Code Representations for Vulnerability Detection on Ethereum Smart Contracts",2021,"","","","",163,"2022-07-13 10:07:05","","10.1145/3457337.3457841","","",,,,,11,11.00,3,4,1,"Ethereum smart contracts are programs that run on the Ethereum blockchain, and many smart contract vulnerabilities have been discovered in the past decade. Many security analysis tools have been created to detect such vulnerabilities, but their performance decreases drastically when codes to be analyzed are being rewritten. In this paper, we propose Eth2Vec, a machine-learning-based static analysis tool for vulnerability detection in smart contracts. It is also robust against code rewrites, i.e., it can detect vulnerabilities even in rewritten codes. Existing machine-learning-based static analysis tools for vulnerability detection need features, which analysts create manually, as inputs. In contrast, Eth2Vec automatically learns features of vulnerable Ethereum Virtual Machine (EVM) bytecodes with tacit knowledge through a neural network for natural language processing. Therefore, Eth2Vec can detect vulnerabilities in smart contracts by comparing the code similarity between target EVM bytecodes and the EVM bytecodes it already learned. We conducted experiments with existing open databases, such as Etherscan, and our results show that Eth2Vec outperforms a recent model based on support vector machine in terms of well-known metrics, i.e., precision, recall, and F1-score.","",""
10,"Junyu Gao, Tianzhu Zhang, Changsheng Xu","Learning to Model Relationships for Zero-Shot Video Classification",2020,"","","","",164,"2022-07-13 10:07:05","","10.1109/tpami.2020.2985708","","",,,,,10,5.00,3,3,2,"With the explosive growth of video categories, zero-shot learning (ZSL) in video classification has become a promising research direction in pattern analysis and machine learning. Based on some auxiliary information such as word embeddings and attributes, the key to a robust ZSL method is to transfer the learned knowledge from seen classes to unseen classes, which requires relationship modeling between these concepts (e.g., categories and attributes). However, most existing approaches ignore to model the explicit relationships in an end-to-end manner, resulting in low effectiveness of knowledge transfer. To tackle this problem, we reconsider the video ZSL task as a task-driven message passing process to jointly enjoy several merits including alleviated heterogeneity gap, low domain shift, and robust temporal modeling. Specifically, we propose a prototype-sample GNN (PS-GNN) consisting of a prototype branch and a sample branch to directly and adaptively model all the relationships between category-attribute, category-category, and attribute-attribute. The prototype branch aims to learn robust representations of video categories, which takes as input a set of word-embedding vectors corresponding to the concepts. The sample branch is designed to generate features of a video sample by leveraging its object semantics. With the co-adaption and cooperation between both branches, a unified and robust ZSL framework is achieved. Extensive experiments strongly evidence that PS-GNN obtains favorable performance on five popular video benchmarks consistently.","",""
2,"Marzieh Jalal Abadi, Luca Luceri, M. Hassan, C. Chou, M. Nicoli","A Cooperative Machine Learning Approach for Pedestrian Navigation in Indoor IoT",2019,"","","","",165,"2022-07-13 10:07:05","","10.3390/s19214609","","",,,,,2,0.67,0,5,3,"This paper presents a system based on pedestrian dead reckoning (PDR) for localization of networked mobile users, which relies only on sensors embedded in the devices and device- to-device connectivity. The user trajectory is reconstructed by measuring step by step the user displacements. Though step length can be estimated rather accurately, heading evaluation is extremely problematic in indoor environments. Magnetometer is typically used, however measurements are strongly perturbed. To improve the location accuracy, this paper proposes a novel cooperative system to estimate the direction of motion based on a machine learning approach for perturbation detection and filtering, combined with a consensus algorithm for performance augmentation by cooperative data fusion at multiple devices. A first algorithm filters out perturbed magnetometer measurements based on a-priori information on the Earth’s magnetic field. A second algorithm aggregates groups of users walking in the same direction, while a third one combines the measurements of the aggregated users in a distributed way to extract a more accurate heading estimate. To the best of our knowledge, this is the first approach that combines machine learning with consensus algorithms for cooperative PDR. Compared to other methods in the literature, the method has the advantage of being infrastructure-free, fully distributed and robust to sensor failures thanks to the pre-filtering of perturbed measurements. Extensive indoor experiments show that the heading error is highly reduced by the proposed approach thus leading to noticeable enhancements in localization performance.","",""
4,"R. LeMoyne, T. Mastroianni, D. Whiting, N. Tomycz","Preliminary Network Centric Therapy for Machine Learning Classification of Deep Brain Stimulation Status for the Treatment of Parkinson’s Disease with a Conformal Wearable and Wireless Inertial Sensor",2019,"","","","",166,"2022-07-13 10:07:05","","10.4236/apd.2019.84007","","",,,,,4,1.33,1,4,3,"The concept of Network Centric Therapy represents an  amalgamation of wearable and wireless inertial sensor systems and machine  learning with access to a Cloud computing environment. The advent of Network  Centric Therapy is highly relevant to the treatment of Parkinson’s disease  through deep brain stimulation. Originally wearable and wireless systems for  quantifying Parkinson’s disease involved the use a smartphone to quantify hand  tremor. Although originally novel, the smartphone has notable issues as a  wearable application for quantifying movement disorder tremor. The smartphone  has evolved in a pathway that has made the smartphone progressively more  cumbersome to mount about the dorsum of the hand. Furthermore, the smartphone  utilizes an inertial sensor package that is not certified for medical analysis,  and the trial data access a provisional Cloud computing environment through an  email account. These concerns are resolved with the recent development of a  conformal wearable and wireless inertial sensor system. This conformal wearable  and wireless system mounts to the hand with the profile of a bandage by  adhesive and accesses a secure Cloud computing environment through a segmented  wireless connectivity strategy involving a smartphone and tablet. Additionally,  the conformal wearable and wireless system is certified by the FDA of the United  States of America for ascertaining medical grade inertial sensor data. These  characteristics make the conformal wearable and wireless system uniquely suited  for the quantification of Parkinson’s disease treatment through deep brain  stimulation. Preliminary evaluation of the conformal wearable and wireless  system is demonstrated through the differentiation of deep brain stimulation  set to “On” and “Off” status. Based on the robustness of the acceleration  signal, this signal was selected to quantify hand tremor for the prescribed  deep brain stimulation settings. Machine learning classification using the  Waikato Environment for Knowledge Analysis (WEKA) was applied using the  multilayer perceptron neural network. The multilayer perceptron neural network  achieved considerable classification accuracy for distinguishing between the  deep brain stimulation system set to “On” and “Off” status through the  quantified acceleration signal data obtained by this recently developed  conformal wearable and wireless system. The research achievement  establishes a progressive pathway to the future objective of achieving deep  brain stimulation capabilities that promote closed-loop acquisition of  configuration parameters that are uniquely optimized to the individual through  extrinsic means of a highly conformal wearable and wireless inertial sensor  system and machine learning with access to Cloud computing resources.","",""
4,"A. Ramirez, J. Iriarte","Event Recognition on Time Series Frac Data Using Machine Learning",2019,"","","","",167,"2022-07-13 10:07:05","","10.2118/195317-MS","","",,,,,4,1.33,2,2,3,"  Hydraulic fracturing pumping data is recorded and mapped in the field at one-second intervals. The designation of the stage start and end time is very important because these boundaries govern summary calculations, such as pressure, rate, and concentrations. Manual selection of staging flags is often very time consuming and prone to inaccuracies due to the lack of uniform selection and interpretation methods across the industry. The purpose of this study is to demonstrate the automation process to identify accurate and consistent stage start and end times in a high-frequency treating plot using machine learning algorithms.  This study is based on the analysis of metered high-frequency treatment data coupled with supervised machine learning algorithms. The pumping dataset includes treating pressure, slurry rate, and clean volume for 179 stages, for a total of 1,530,445 rows of data per variable. Sixty-six percent of the data were used to train the model, eight percent were used to validate the model, and the remaining twenty-six percent were used to test it. Subject matter expertise, taking into account user-defined start/end time flags, was used to train the algorithm.  Pumping data behaves very differently than traditional time-series data such as weather, stock prices, or population growth. The features examined are not affected by time but by physical events, so the correlation or dependency between features can affect accurate pattern recognition. To allow the algorithm to run leaner, the dataset was pre-processed using loss functions, smoothing techniques, and the rate of change of the main data channels. To understand how data may impair the predictions and to evaluate different model performances, we tested two classification algorithms: logistic regression and support vector machine.  Classification techniques were used to generate an accurate suggestion of where the pumping of a hydraulic fracturing stage starts and ends in a high-frequency treating plot. Results show that flag predictions have a training and validation accuracy of approximately 90 percent using logistic regression algorithms. The predicted flags were within 10 seconds of the manual selected flag. A limitation of this method is that it requires periodic retraining with new field data to improve the prediction robustness and to maintain high accuracy.  Accurate start and end time selections make it not only viable to process large volumes of fracture treatment data but also reduce the time spent reviewing field data for quality control. Petroleum engineers need to continue their focus on optimizing their systems with the greatest possible efficiency. Leveraging common analytical methods combined with the large, structured datasets that are readily available provide impressive results without extensive programming knowledge.","",""
1,"Cassio Polpo de Campos, Alessandro Antonucci","Imprecision in Machine Learning and AI",2015,"","","","",168,"2022-07-13 10:07:05","","","","",,,,,1,0.14,1,2,7,"IN this note we consider five different relevant problems in AI and machine learning. We argue that possible solutions to such problems might be achieved by replacing the probability distributions in the systems with sets of them. Such a robust approach is based on the so-called impreciseprobabilistic framework. The proposed solutions provide a persuasive justification of the imprecise framework. The problems we consider are: • proper treatment of missing data, • reliable classification, • sensitivity analysis, • feature selection, • elicitation of qualitative expert knowledge. Before reporting a separate discussion for each problem, let us briefly resume the general ideas characterising imprecise-probabilistic methods.","",""
19,"A. Menon, Chetali Gupta, K. Perkins, B. DeCost, Nikita Budwal, Renee T. Rios, Kunpeng Zhang, B. Póczos, N. Washburn","Elucidating multi-physics interactions in suspensions for the design of polymeric dispersants: a hierarchical machine learning approach",2017,"","","","",169,"2022-07-13 10:07:05","","10.1039/C7ME00027H","","",,,,,19,3.80,2,9,5,"A computational method for understanding and optimizing the properties of complex physical systems is presented using polymeric dispersants as an example. Concentrated suspensions are formulated with dispersants to tune rheological parameters, such as yield stress or viscosity, but their competing effects on solution and particle variables have made it impossible to design them based on our knowledge of the interplay of chemistry and function. Here, physical and statistical modeling are integrated into a hierarchical framework of machine learning that provides insight into sparse experimental datasets. A library of 10 polymers having similar molecular weight but incorporating different functional groups commonly found in aqueous dispersants was used as a training set in magnesium oxide slurries. The compositions of these polymers were the experimental variables that determined the complex system responses, but the method leverages knowledge of the constituent “single-physics” interactions that underlie the suspension properties. Integration of domain knowledge is shown to allow robust predictions based on orders of magnitude fewer samples in the training set compared with purely statistical methods that directly correlate dispersant chemistry with changes in rheological properties. Minimization of the resulting function for slurry yield stress resulted in the prediction of a novel dispersant that was synthesized and shown to impart similar reductions as a leading commercial dispersant but with a significantly different composition and molecular architecture.","",""
14,"Zainab Abaid, M. Kâafar, Sanjay Jha","Quantifying the impact of adversarial evasion attacks on machine learning based android malware classifiers",2017,"","","","",170,"2022-07-13 10:07:05","","10.1109/NCA.2017.8171381","","",,,,,14,2.80,5,3,5,"With the proliferation of Android-based devices, malicious apps have increasingly found their way to user devices. Many solutions for Android malware detection rely on machine learning; although effective, these are vulnerable to attacks from adversaries who wish to subvert these algorithms and allow malicious apps to evade detection. In this work, we present a statistical analysis of the impact of adversarial evasion attacks on various linear and non-linear classifiers, using a recently proposed Android malware classifier as a case study. We systematically explore the complete space of possible attacks varying in the adversary's knowledge about the classifier; our results show that it is possible to subvert linear classifiers (Support Vector Machines and Logistic Regression) by perturbing only a few features of malicious apps, with more knowledgeable adversaries degrading the classifier's detection rate from 100% to 0% and a completely blind adversary able to lower it to 12%. We show non-linear classifiers (Random Forest and Neural Network) to be more resilient to these attacks. We conclude our study with recommendations for designing classifiers to be more robust to the attacks presented in our work.","",""
15,"A. Kaur, Kamaldeep Kaur","An Empirical Study of Robustness and Stability of Machine Learning Classifiers in Software Defect Prediction",2014,"","","","",171,"2022-07-13 10:07:05","","10.1007/978-3-319-11218-3_35","","",,,,,15,1.88,8,2,8,"","",""
3,"Josef Laimer, P. Lackner","MHCII3D—Robust Structure Based Prediction of MHC II Binding Peptides",2020,"","","","",172,"2022-07-13 10:07:05","","10.3390/ijms22010012","","",,,,,3,1.50,2,2,2,"Knowledge of MHC II binding peptides is highly desired in immunological research, particularly in the context of cancer, autoimmune diseases, or allergies. The most successful prediction methods are based on machine learning methods trained on sequences of experimentally characterized binding peptides. Here, we describe a complementary approach called MHCII3D, which is based on structural scaffolds of MHC II-peptide complexes and statistical scoring functions (SSFs). The MHC II alleles reported in the Immuno Polymorphism Database are processed in a dedicated 3D-modeling pipeline providing a set of scaffold complexes for each distinct allotype sequence. Antigen protein sequences are threaded through the scaffolds and evaluated by optimized SSFs. We compared the predictive power of MHCII3D with different sequence-based machine learning methods. The Pearson correlation to experimentally determine IC50 values for MHC II Automated Server Benchmarks data sets from IEDB (Immune Epitope Database) is 0.42, which is in the competitor methods range. We show that MHCII3D is quite robust in leaving one molecule out tests and is therefore not prone to overfitting. Finally, we provide evidence that MHCII3D can complement the current sequence-based methods and help to identify problematic entries in IEDB. Scaffolds and MHCII3D executables can be freely downloaded from our web pages.","",""
2,"Rui Gao","Finite-Sample Guarantees for Wasserstein Distributionally Robust Optimization: Breaking the Curse of Dimensionality",2020,"","","","",173,"2022-07-13 10:07:05","","","","",,,,,2,1.00,2,1,2,"Wasserstein distributionally robust optimization (DRO) aims to find robust and generalizable solutions by hedging against data perturbations in Wasserstein distance. Despite its recent empirical success in operations research and machine learning, existing performance guarantees for generic loss functions are either overly conservative due to the curse of dimensionality, or plausible only in large sample asymptotics. In this paper, we develop a non-asymptotic framework for analyzing the out-of-sample performance for Wasserstein robust learning and the generalization bound for its related Lipschitz and gradient regularization problems. To the best of our knowledge, this gives the first finite-sample guarantee for generic Wasserstein DRO problems without suffering from the curse of dimensionality. Our results highlight the bias-variation trade-off intrinsic in the Wasserstein DRO, which balances between the empirical mean of the loss and the variation of the loss, measured by the Lipschitz norm or the gradient norm of the loss. Our analysis is based on two novel methodological developments that are of independent interest: 1) a new concentration inequality controlling the decay rate of large deviation probabilities by the variation of the loss and, 2) a localized Rademacher complexity theory based on the variation of the loss.","",""
9,"Syed Ashiqur Rahman, Peter Giacobbi, L. Pyles, C. Mullett, Gianfranco Doretto, D. Adjeroh","Deep learning for biological age estimation",2020,"","","","",174,"2022-07-13 10:07:05","","10.1093/bib/bbaa021","","",,,,,9,4.50,2,6,2,"Modern machine learning techniques (such as deep learning) offer immense opportunities in the field of human biological aging research. Aging is a complex process, experienced by all living organisms. While traditional machine learning and data mining approaches are still popular in aging research, they typically need feature engineering or feature extraction for robust performance. Explicit feature engineering represents a major challenge, as it requires significant domain knowledge. The latest advances in deep learning provide a paradigm shift in eliciting meaningful knowledge from complex data without performing explicit feature engineering. In this article, we review the recent literature on applying deep learning in biological age estimation. We consider the current data modalities that have been used to study aging and the deep learning architectures that have been applied. We identify four broad classes of measures to quantify the performance of algorithms for biological age estimation and based on these evaluate the current approaches. The paper concludes with a brief discussion on possible future directions in biological aging research using deep learning. This study has significant potentials for improving our understanding of the health status of individuals, for instance, based on their physical activities, blood samples and body shapes. Thus, the results of the study could have implications in different health care settings, from palliative care to public health.","",""
27,"Nicholas Shawen, L. Lonini, C. Mummidisetty, Ilona Shparii, Mark V. Albert, Konrad Paul Kording, A. Jayaraman","Fall Detection in Individuals With Lower Limb Amputations Using Mobile Phones: Machine Learning Enhances Robustness for Real-World Applications",2017,"","","","",175,"2022-07-13 10:07:05","","10.2196/mhealth.8201","","",,,,,27,5.40,4,7,5,"Background Automatically detecting falls with mobile phones provides an opportunity for rapid response to injuries and better knowledge of what precipitated the fall and its consequences. This is beneficial for populations that are prone to falling, such as people with lower limb amputations. Prior studies have focused on fall detection in able-bodied individuals using data from a laboratory setting. Such approaches may provide a limited ability to detect falls in amputees and in real-world scenarios. Objective The aim was to develop a classifier that uses data from able-bodied individuals to detect falls in individuals with a lower limb amputation, while they freely carry the mobile phone in different locations and during free-living. Methods We obtained 861 simulated indoor and outdoor falls from 10 young control (non-amputee) individuals and 6 individuals with a lower limb amputation. In addition, we recorded a broad database of activities of daily living, including data from three participants’ free-living routines. Sensor readings (accelerometer and gyroscope) from a mobile phone were recorded as participants freely carried it in three common locations—on the waist, in a pocket, and in the hand. A set of 40 features were computed from the sensors data and four classifiers were trained and combined through stacking to detect falls. We compared the performance of two population-specific models, trained and tested on either able-bodied or amputee participants, with that of a model trained on able-bodied participants and tested on amputees. A simple threshold-based classifier was used to benchmark our machine-learning classifier. Results The accuracy of fall detection in amputees for a model trained on control individuals (sensitivity: mean 0.989, 1.96*standard error of the mean [SEM] 0.017; specificity: mean 0.968, SEM 0.025) was not statistically different (P=.69) from that of a model trained on the amputee population (sensitivity: mean 0.984, SEM 0.016; specificity: mean 0.965, SEM 0.022). Detection of falls in control individuals yielded similar results (sensitivity: mean 0.979, SEM 0.022; specificity: mean 0.991, SEM 0.012). A mean 2.2 (SD 1.7) false alarms per day were obtained when evaluating the model (vs mean 122.1, SD 166.1 based on thresholds) on data recorded as participants carried the phone during their daily routine for two or more days. Machine-learning classifiers outperformed the threshold-based one (P<.001). Conclusions A mobile phone-based fall detection model can use data from non-amputee individuals to detect falls in individuals walking with a prosthesis. We successfully detected falls when the mobile phone was carried across multiple locations and without a predetermined orientation. Furthermore, the number of false alarms yielded by the model over a longer period of time was reasonably low. This moves the application of mobile phone-based fall detection systems closer to a real-world use case scenario.","",""
66,"David J. Miller, Zhen Xiang, G. Kesidis","Adversarial Learning Targeting Deep Neural Network Classification: A Comprehensive Review of Defenses Against Attacks",2020,"","","","",176,"2022-07-13 10:07:05","","10.1109/JPROC.2020.2970615","","",,,,,66,33.00,22,3,2,"With wide deployment of machine learning (ML)-based systems for a variety of applications including medical, military, automotive, genomic, multimedia, and social networking, there is great potential for damage from adversarial learning (AL) attacks. In this article, we provide a contemporary survey of AL, focused particularly on defenses against attacks on deep neural network classifiers. After introducing relevant terminology and the goals and range of possible knowledge of both attackers and defenders, we survey recent work on test-time evasion (TTE), data poisoning (DP), backdoor DP, and reverse engineering (RE) attacks and particularly defenses against the same. In so doing, we distinguish robust classification from anomaly detection (AD), unsupervised from supervised, and statistical hypothesis-based defenses from ones that do not have an explicit null (no attack) hypothesis. We also consider several scenarios for detecting backdoors. We provide a technical assessment for reviewed works, including identifying any issues/limitations, required hyperparameters, needed computational complexity, as well as the performance measures evaluated and the obtained quality. We then delve deeper, providing novel insights that challenge conventional AL wisdom and that target unresolved issues, including: robust classification versus AD as a defense strategy; the belief that attack success increases with attack strength, which ignores susceptibility to AD; small perturbations for TTE attacks: a fallacy or a requirement; validity of the universal assumption that a TTE attacker knows the ground-truth class for the example to be attacked; black, gray, or white-box attacks as the standard for defense evaluation; and susceptibility of query-based RE to an AD defense. We also discuss attacks on the privacy of training data. We then present benchmark comparisons of several defenses against TTE, RE, and backdoor DP attacks on images. The article concludes with a discussion of continuing research directions, including the supreme challenge of detecting attacks whose goal is not to alter classification decisions, but rather simply to embed, without detection, “fake news” or other false content.","",""
2,"Shubham Sharma, A. Gee, D. Paydarfar, J. Ghosh","FaiR-N: Fair and Robust Neural Networks for Structured Data",2020,"","","","",177,"2022-07-13 10:07:05","","10.1145/3461702.3462559","","",,,,,2,1.00,1,4,2,"Fairness and robustness in machine learning are crucial when individuals are subject to automated decisions made by models in high-stake domains. To promote ethical artificial intelligence, fairness metrics that rely on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias. However, fairness measures that rely on comparing the ability to achieve recourse have been relatively unexplored. In this paper, we present a novel formulation for training neural networks that considers the distance of data observations to the decision boundary such that the new objective: (1) reduces the disparity in the average ability of recourse between individuals in each protected group, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that models trained with this new objective are more fair and adversarially robust neural networks, with similar accuracies, when compared to models without it. We also investigate a trade-off between the recourse-based fairness and robustness objectives. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across protected groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse disparity across groups are considered to train fairer neural networks.","",""
1,"Zhe Xu","Robust Inference and Verification of Temporal Logic Classifier-in-the-loop Systems",2020,"","","","",178,"2022-07-13 10:07:05","","","","",,,,,1,0.50,1,1,2,"Autonomous systems embedded with machine learning modules often rely on deep neural networks for classifying different objects of interest in the environment or different actions or strategies to take for the system. Due to the non-linearity and high-dimensionality of deep neural networks, the interpretability of the autonomous systems is compromised. Besides, the machine learning methods in autonomous systems are mostly data-intensive and lack commonsense knowledge and reasoning that are natural to humans. In this paper, we propose the framework of temporal logic classifier-in-the-loop systems. The temporal logic classifiers can output different actions to take for an autonomous system based on the environment, such that the behavior of the autonomous system can satisfy a given temporal logic specification. Our approach is robust and provably-correct, as we can prove that the behavior of the autonomous system can satisfy a given temporal logic specification in the presence of (bounded) disturbances.","",""
9,"Qingxue Zhang, Dian Zhou, Xuan Zeng","Machine Learning-Empowered Biometric Methods for Biomedicine Applications",2017,"","","","",179,"2022-07-13 10:07:05","","10.3934/MEDSCI.2017.3.274","","",,,,,9,1.80,3,3,5,"Nowadays, pervasive computing technologies are paving a promising way for advanced smart health applications. However, a key impediment faced by wide deployment of these assistive smart devices, is the increasing privacy and security issue, such as how to protect access to sensitive patient data in the health record. Focusing on this challenge, biometrics are attracting intense attention in terms of effective user identification to enable confidential health applications. In this paper, we take special interest in two bio-potential-based biometric modalities, electrocardiogram (ECG) and electroencephalogram (EEG), considering that they are both unique to individuals, and more reliable than token (identity card) and knowledge-based (username/password) methods. After extracting effective features in multiple domains from ECG/EEG signals, several advanced machine learning algorithms are introduced to perform the user identification task, including Neural Network, K-nearest Neighbor, Bagging, Random Forest and AdaBoost. Experimental results on two public ECG and EEG datasets show that ECG is a more robust biometric modality compared to EEG, leveraging a higher signal to noise ratio and also more distinguishable morphological patterns. Among different machine learning classifiers, the random forest greatly outperforms the others and owns an identification rate as high as 98%. This study is expected to demonstrate that properly selected biometric empowered by an effective machine learner owns a great potential, to enable confidential biomedicine applications in the era of smart digital health.","",""
57,"Sawsan Abdulrahman, Hanine Tout, Hakima Ould-Slimane, A. Mourad, C. Talhi, M. Guizani","A Survey on Federated Learning: The Journey From Centralized to Distributed On-Site Learning and Beyond",2021,"","","","",180,"2022-07-13 10:07:05","","10.1109/JIOT.2020.3030072","","",,,,,57,57.00,10,6,1,"Driven by privacy concerns and the visions of deep learning, the last four years have witnessed a paradigm shift in the applicability mechanism of machine learning (ML). An emerging model, called federated learning (FL), is rising above both centralized systems and on-site analysis, to be a new fashioned design for ML implementation. It is a privacy-preserving decentralized approach, which keeps raw data on devices and involves local ML training while eliminating data communication overhead. A federation of the learned and shared models is then performed on a central server to aggregate and share the built knowledge among participants. This article starts by examining and comparing different ML-based deployment architectures, followed by in-depth and in-breadth investigation on FL. Compared to the existing reviews in the field, we provide in this survey a new classification of FL topics and research fields based on thorough analysis of the main technical challenges and current related work. In this context, we elaborate comprehensive taxonomies covering various challenging aspects, contributions, and trends in the literature, including core system models and designs, application areas, privacy and security, and resource management. Furthermore, we discuss important challenges and open research directions toward more robust FL systems.","",""
46,"Stefano Ortona, V. V. Meduri, Paolo Papotti","Robust Discovery of Positive and Negative Rules in Knowledge Bases",2018,"","","","",181,"2022-07-13 10:07:05","","10.1109/ICDE.2018.00108","","",,,,,46,11.50,15,3,4,"We present RUDIK, a system for the discovery of declarative rules over knowledge-bases (KBs). RUDIK discovers rules that express positive relationships between entities, such as ""if two persons have the same parent, they are siblings"", and negative rules, i.e., patterns that identify contradictions in the data, such as ""if two persons are married, one cannot be the child of the other"". While the former class infers new facts in the KB, the latter class is crucial for other tasks, such as detecting erroneous triples in data cleaning, or the creation of negative examples to bootstrap learning algorithms. The system is designed to: (i) enlarge the expressive power of the rule language to obtain complex rules and wide coverage of the facts in the KB, (ii) discover approximate rules (soft constraints) to be robust to errors and incompleteness in the KB, (iii) use disk-based algorithms, effectively enabling rule mining in commodity machines. In contrast with traditional ranking of all rules based on a measure of support, we propose an approach to identify the subset of useful rules to be exposed to the user. We model the mining process as an incremental graph exploration problem and prove that our search strategy has guarantees on the optimality of the results. We have conducted extensive experiments using real-world KBs to show that RUDIK outperforms previous proposals in terms of efficiency and that it discovers more effective rules for the application at hand.","",""
6,"C. Brecher, M. Obdenbusch, Melanie Buchsbaum","Optimized state estimation by application of machine learning",2017,"","","","",182,"2022-07-13 10:07:05","","10.1007/s11740-017-0724-9","","",,,,,6,1.20,2,3,5,"","",""
17,"R. Dutta, Ahsan Morshed","Performance Evaluation of South Esk Hydrological Sensor Web: Unsupervised Machine Learning and Semantic Linked Data Approach",2013,"","","","",183,"2022-07-13 10:07:05","","10.1109/JSEN.2013.2264666","","",,,,,17,1.89,9,2,9,"Technological progress has lead the sensor network domain to an era where environmental and agricultural domain applications are completely dependent on hydrological sensor networks. Data from the sensor networks are being used for knowledge management and critical decision support system. The quality of data can, however, vary widely. Existing automated quality assurance approach based on simple threshold rulebase could potentially miss serious errors requiring robust and complex domain knowledge to identify. This paper proposes a linked data concept, unsupervised pattern recognition, and semantic ontologies based dynamic framework to assess the reliability of hydrological sensor network and evaluate the performance of the sensor network. Newly designed framework is used successfully to evaluate the South Esk hydrological sensor web in Tasmania, indicating that domain ontology based linked data approach could be a very useful methodology for quality assurance of the complex data.","",""
0,"Zhuang Liu, Kaiyu Huang, Ziyu Gao, Degen Huang","Knowledge-Aware LSTM for Machine Comprehension",2019,"","","","",184,"2022-07-13 10:07:05","","10.1109/ISKE47853.2019.9170351","","",,,,,0,0.00,0,4,3,"Machine Comprehension (MC) of text is the problem to answer a query based on a given document. Although MC has been very popular recently, it still have some serious weaknesses which rely only on query-to-document interaction or its learning is just heavily dependent on the training data. To take advantage of external knowledge to improve neural networks for MC, we propose a novel knowledge enhanced recurrent neural model, called knowledge-aware LSTM (k-LSTM), an extension to basic LSTM cells, designed to exploit external knowledge bases (KBs) to improve neural networks for MC task. To incorporate KBs with contextual information effectively from the currently text, k-LSTM employs an compositional attention mechanism to adaptively decide whether to attend to KBs and which information from external knowledge is useful. Furthermore, we present our knowledge enhanced neural network, called Knowledge-guided DIM Reader (K-DIM Reader), which is a novel knowledge-aware compositional attention neural network architecture, employing the k-LSTM in our framework. By stringing external background knowledge together and imposing compositional attention interaction that regulate their interaction, K-DIM Reader effectively learns to perform reading comprehension processes that are directly inferred from the data in an end-to-end approach. We show our proposed models strength, robustness and interpretability on the challenging MC datasets, achieving significant improvements on SQuAD dataset [1] and obtaining new state-of-the-art results on both Cloze-style datasets, CBTest [2] and CNN news [3]. In particular, we further extend 6 popular end-to-end neural MC models using k-LSTM incorporating knowledge into models for improving MC, and evaluate their performance on both well-known MC datasets. We demonstrate that neural model with external knowledge improves performance on MC task.","",""
81,"A. Urieli","Robust French syntax analysis: reconciling statistical methods and linguistic knowledge in the Talismane toolkit. (Analyse syntaxique robuste du français : concilier méthodes statistiques et connaissances linguistiques dans l'outil Talismane)",2013,"","","","",185,"2022-07-13 10:07:05","","","","",,,,,81,9.00,81,1,9,"In this thesis we explore robust statistical syntax analysis for French. Our main concern is to explore methods whereby the linguist can inject linguistic knowledge and/or resources into the robust statistical engine in order to improve results for specific phenomena. We first explore the dependency annotation schema for French, concentrating on certain phenomena. Next, we look into the various algorithms capable of producing this annotation, and in particular on the transition-based parsing algorithm used in the rest of this thesis. After exploring supervised machine learning algorithms for NLP classification problems, we present the Talismane toolkit for syntax analysis, built within the framework of this thesis, including four statistical modules - sentence boundary detection, tokenisation, pos-tagging and parsing - as well as the various linguistic resources used for the baseline model, including corpora, lexicons and feature sets. Our first experiments attempt various machine learning configurations in order to identify the best baseline. We then look into improvements made possible by beam search and beam propagation. Finally, we present a series of experiments aimed at correcting errors related to specific linguistic phenomena, using targeted features. One our innovation is the introduction of rules that can impose or prohibit certain decisions locally, thus bypassing the statistical model. We explore the usage of rules for errors that the features are unable to correct. Finally, we look into the enhancement of targeted features by large scale linguistic resources, and in particular a semi-supervised approach using a distributional semantic resource.","",""
55,"Zan Gao, Yinming Li, S. Wan","Exploring Deep Learning for View-Based 3D Model Retrieval",2020,"","","","",186,"2022-07-13 10:07:05","","10.1145/3377876","","",,,,,55,27.50,18,3,2,"In recent years, view-based 3D model retrieval has become one of the research focuses in the field of computer vision and machine learning. In fact, the 3D model retrieval algorithm consists of feature extraction and similarity measurement, and the robust features play a decisive role in the similarity measurement. Although deep learning has achieved comprehensive success in the field of computer vision, deep learning features are used for 3D model retrieval only in a small number of works. To the best of our knowledge, there is no benchmark to evaluate these deep learning features. To tackle this problem, in this work we systematically evaluate the performance of deep learning features in view-based 3D model retrieval on four popular datasets (ETH, NTU60, PSB, and MVRED) by different kinds of similarity measure methods. In detail, the performance of hand-crafted features and deep learning features are compared, and then the robustness of deep learning features is assessed. Finally, the difference between single-view deep learning features and multi-view deep learning features is also evaluated. By quantitatively analyzing the performances on different datasets, it is clear that these deep learning features can consistently outperform all of the hand-crafted features, and they are also more robust than the hand-crafted features when different degrees of noise are added into the image. The exploration of latent relationships among different views in multi-view deep learning network architectures shows that the performance of multi-view deep learning outperforms that of single-view deep learning features with low computational complexity.","",""
3,"Sijie Yang, Fei Zhu, Xinghong Ling, QUAN LIU, Peiyao Zhao","Intelligent Health Care: Applications of Deep Learning in Computational Medicine",2021,"","","","",187,"2022-07-13 10:07:05","","10.3389/fgene.2021.607471","","",,,,,3,3.00,1,5,1,"With the progress of medical technology, biomedical field ushered in the era of big data, based on which and driven by artificial intelligence technology, computational medicine has emerged. People need to extract the effective information contained in these big biomedical data to promote the development of precision medicine. Traditionally, the machine learning methods are used to dig out biomedical data to find the features from data, which generally rely on feature engineering and domain knowledge of experts, requiring tremendous time and human resources. Different from traditional approaches, deep learning, as a cutting-edge machine learning branch, can automatically learn complex and robust feature from raw data without the need for feature engineering. The applications of deep learning in medical image, electronic health record, genomics, and drug development are studied, where the suggestion is that deep learning has obvious advantage in making full use of biomedical data and improving medical health level. Deep learning plays an increasingly important role in the field of medical health and has a broad prospect of application. However, the problems and challenges of deep learning in computational medical health still exist, including insufficient data, interpretability, data privacy, and heterogeneity. Analysis and discussion on these problems provide a reference to improve the application of deep learning in medical health.","",""
30,"Xishan Zhang, Yang Yang, Yongdong Zhang, Huanbo Luan, Jintao Li, Hanwang Zhang, Tat-Seng Chua","Enhancing Video Event Recognition Using Automatically Constructed Semantic-Visual Knowledge Base",2015,"","","","",188,"2022-07-13 10:07:05","","10.1109/TMM.2015.2449660","","",,,,,30,4.29,4,7,7,"The task of recognizing events from video has attracted a lot of attention in recent years. However, due to the complex nature of user-defined events, the use of purely audio- visual content analysis without domain knowledge has been found to be grossly inadequate. In this paper, we propose to construct a semantic-visual knowledge base to encode the rich event-centric concepts and their relationships from the well- established lexical databases, including FrameNet, as well as the concept-specific visual knowledge from ImageNet. Based on this semantic-visual knowledge bases, we design an effective system for video event recognition. Specifically, in order to narrow the semantic gap between the high-level complex events and low-level visual representations, we utilize the event-centric semantic concepts encoded in the knowledge base as the intermediate-level event representation, which offers both human-perceivable and machine-interpretable semantic clues for event recognition. In addition, in order to leverage the abundant ImageNet images, we propose a robust transfer learning model to learn the noise- resistant concept classifiers for videos. Extensive experiments on various real-world video datasets demonstrate the superiority of our proposed system as compared to the state-of-the-art approaches.","",""
4,"A. Toubman, J. Roessingh, P. Spronck, A. Plaat, H. J. V. Herik","Improving Air-to-Air Combat Behavior through Transparent Machine Learning",2014,"","","","",189,"2022-07-13 10:07:05","","","","",,,,,4,0.50,1,5,8,"Training simulations, especially those for tactical training, require properly behaving computer generated forces (CGFs) in the opponent role for an effective training experience. Traditionally, the behavior of such CGFs is controlled through scripts. There are two main problems with the use of scripts for controlling the behavior of CGFs: (1) building an effective script requires expert knowledge, which is costly, and (2) costs further increase with the number of ‘learning events’ in a scenario (e.g. a new opponent tactic). Machine learning techniques may offer a solution to these two problems, by automatically generating, evaluating and improving CGF behavior. In this paper we describe an application of the dynamic scripting technique to the generation of CGF behavior for training simulations. Dynamic scripting is a machine learning technique that searches for effective scripts by combining rules from a rule base with predefined behavior rules. Although dynamic scripting was initially developed for artificial intelligence (AI) in commercial video games, its computational and functional qualities are also desirable in military training simulations. Among other qualities, dynamic scripting generates behavior in a transparent manner. Also, dynamic scripting’s learning method is robust: a minimum level of effectiveness is guaranteed through the use of domain knowledge in the initial rule base. In our research, we investigate the application of dynamic scripting for generating behaviors of multiple cooperating aircraft in air-to-air combat. Coordination in multi-agent systems remains a non-trivial problem. We enabled explicit team coordination through communication between team members. This coordination method was tested in an air combat simulation experiment, and compared against a baseline that consisted of a similar dynamic scripting setup, without explicit coordination. In terms of combat performance, the team using the explicit team coordination was 20% more effective than the baseline. Finally, the paper will discuss the application of dynamic scripting in a practical setting.","",""
2,"Shuyin Xia, Xinyu Bai, Guoyin Wang, Deyu Meng, Xinbo Gao, Zizhong Chen, Elisabeth Giem","An Efficient and Accurate Rough Set for Feature Selection, Classification and Knowledge Representation",2021,"","","","",190,"2022-07-13 10:07:05","","","","",,,,,2,2.00,0,7,1,"FEATURE eature, selection based on rough set theory belongs to filter method. Rough set theory was proposed by Polish scientist Pawlak in 1982 [1]. It is an effective mathematical tool to proess uncertain, inconsistent and incomplete data, which has been widely applied in machine learning, data mining, decision support systems and other application fields [2]–[5]. However, the efficiency of rough set is not efficient and accurate enough. Few studies can broadly improve the accuracy of rough sets. These shortcomings restrict the application ability of rough sets. The main contributions of this paper are as follows: (1)We first find ineffectiveness of rough set because of overfiting, especially in a type of noise attribute. To deal with this problem, we propose a completely new measurement to make rough set robust to the type of noise attribute and have better generalizability. This enhances the accuracy of rough set considerably. (2) We proposed the concept of ”rough concept tree” for knowledge representation and classification. This makes rough sets a very rare method that can effectively implement feature selection, data classification and knowledge representation inherently at the same time, not rely on any other classifiers or methods.","",""
2,"Virginia Bordignon, Stefan Vlaski, V. Matta, A. Sayed","Network Classifiers Based on Social Learning",2020,"","","","",191,"2022-07-13 10:07:05","","10.1109/ICASSP39728.2021.9414126","","",,,,,2,1.00,1,4,2,"This work proposes a new way of combining independently trained classifiers over space and time. Combination over space means that the outputs of spatially distributed classifiers are aggregated. Combination over time means that the classifiers respond to streaming data during testing and continue to improve their performance even during this phase. By doing so, the proposed architecture is able to improve prediction performance over time with unlabeled data. Inspired by social learning algorithms, which require prior knowledge of the observations distribution, we propose a Social Machine Learning (SML) paradigm that is able to exploit the imperfect models generated during the learning phase. We show that this strategy results in consistent learning with high probability, and it yields a robust structure against poorly trained classifiers. Simulations with an ensemble of feedforward neural networks are provided to illustrate the theoretical results.","",""
2,"K. Yan, Adam P. Harrison","Interpretable Medical Image Classification with Self-Supervised Anatomical Embedding and Prior Knowledge",2021,"","","","",192,"2022-07-13 10:07:05","","","","",,,,,2,2.00,1,2,1,"In medical image analysis tasks, it is important to make machine learning models focus on correct anatomical locations, so as to improve interpretability and robustness of the model. We adopt a latest algorithm called self-supervised anatomical embedding (SAM) to locate point of interest (POI) on computed tomography (CT) scans. SAM can detect arbitrary POI with only one labeled sample needed. Then, we can extract targeted features from the POIs to train a simple prediction model guided by clinical prior knowledge. This approach mimics the practice of human radiologists, thus is interpretable, controllable, and robust. We illustrate our approach on the application of CT contrast phase classification and it outperforms an existing deep learning based method trained on the whole image.","",""
56,"Haotian Lin, Erping Long, Xiaohu Ding, Hongxing Diao, Zicong Chen, Runzhong Liu, Jialing Huang, Jingheng Cai, Shuangjuan Xu, Xiayin Zhang, Dongni Wang, Kexin Chen, Tongyong Yu, Dongxuan Wu, Xutu Zhao, Zhenzhen Liu, Xiaohang Wu, Yuzhen Jiang, X. Yang, Dongmei Cui, Wenyan Liu, Yingfeng Zheng, L. Luo, Haibo Wang, Chi-Chao Chan, I. Morgan, M. He, Yizhi Liu","Prediction of myopia development among Chinese school-aged children using refraction data from electronic medical records: A retrospective, multicentre machine learning study",2018,"","","","",193,"2022-07-13 10:07:05","","10.1371/journal.pmed.1002674","","",,,,,56,14.00,6,28,4,"Background Electronic medical records provide large-scale real-world clinical data for use in developing clinical decision systems. However, sophisticated methodology and analytical skills are required to handle the large-scale datasets necessary for the optimisation of prediction accuracy. Myopia is a common cause of vision loss. Current approaches to control myopia progression are effective but have significant side effects. Therefore, identifying those at greatest risk who should undergo targeted therapy is of great clinical importance. The objective of this study was to apply big data and machine learning technology to develop an algorithm that can predict the onset of high myopia, at specific future time points, among Chinese school-aged children. Methods and findings Real-world clinical refraction data were derived from electronic medical record systems in 8 ophthalmic centres from January 1, 2005, to December 30, 2015. The variables of age, spherical equivalent (SE), and annual progression rate were used to develop an algorithm to predict SE and onset of high myopia (SE ≤ −6.0 dioptres) up to 10 years in the future. Random forest machine learning was used for algorithm training and validation. Electronic medical records from the Zhongshan Ophthalmic Centre (a major tertiary ophthalmic centre in China) were used as the training set. Ten-fold cross-validation and out-of-bag (OOB) methods were applied for internal validation. The remaining 7 independent datasets were used for external validation. Two population-based datasets, which had no participant overlap with the ophthalmic-centre-based datasets, were used for multi-resource validation testing. The main outcomes and measures were the area under the curve (AUC) values for predicting the onset of high myopia over 10 years and the presence of high myopia at 18 years of age. In total, 687,063 multiple visit records (≥3 records) of 129,242 individuals in the ophthalmic-centre-based electronic medical record databases and 17,113 follow-up records of 3,215 participants in population-based cohorts were included in the analysis. Our algorithm accurately predicted the presence of high myopia in internal validation (the AUC ranged from 0.903 to 0.986 for 3 years, 0.875 to 0.901 for 5 years, and 0.852 to 0.888 for 8 years), external validation (the AUC ranged from 0.874 to 0.976 for 3 years, 0.847 to 0.921 for 5 years, and 0.802 to 0.886 for 8 years), and multi-resource testing (the AUC ranged from 0.752 to 0.869 for 4 years). With respect to the prediction of high myopia development by 18 years of age, as a surrogate of high myopia in adulthood, the algorithm provided clinically acceptable accuracy over 3 years (the AUC ranged from 0.940 to 0.985), 5 years (the AUC ranged from 0.856 to 0.901), and even 8 years (the AUC ranged from 0.801 to 0.837). Meanwhile, our algorithm achieved clinically acceptable prediction of the actual refraction values at future time points, which is supported by the regressive performance and calibration curves. Although the algorithm achieved balanced and robust performance, concerns about the compromised quality of real-world clinical data and over-fitting issues should be cautiously considered. Conclusions To our knowledge, this study, for the first time, used large-scale data collected from electronic health records to demonstrate the contribution of big data and machine learning approaches to improved prediction of myopia prognosis in Chinese school-aged children. This work provides evidence for transforming clinical practice, health policy-making, and precise individualised interventions regarding the practical control of school-aged myopia.","",""
1,"Abdel-Badeeh M. Salem","Machine Learning Applications in Cancer Informatics",2013,"","","","",194,"2022-07-13 10:07:05","","10.1007/978-3-319-00029-9_1","","",,,,,1,0.11,1,1,9,"","",""
1,"Andrea Rossi, D. Firmani, P. Merialdo","Knowledge Graph Embeddings or Bias Graph Embeddings? A Study of Bias in Link Prediction Models",2021,"","","","",195,"2022-07-13 10:07:05","","","","",,,,,1,1.00,0,3,1,"Link Prediction aims at tackling Knowledge Graph incompleteness by inferring new facts based on the existing, already known ones. Nowadays most Link Prediction systems rely on Machine Learning and Deep Learning approaches; this results in inherent opaque models in which assessing the robustness to data biases is not trivial. We define 3 specific types of Sample Selection Bias and estimate their presence in the 5 best-established Link Prediction datasets. We then verify how these biases affect the behaviour of 9 systems representative for every major family of Link Prediction models. We find that these models do indeed learn and incorporate each of the presented biases, with a heavily negative effect on their behaviour. We thus advocate for the creation of novel more robust datasets and of more effective evaluation practices.","",""
1,"A. Ennouni, Noura Ouled Sihamman, M. A. Sabri, A. Aarab","Analysis and Classification of Plant Diseases Based on Deep Learning",2021,"","","","",196,"2022-07-13 10:07:05","","10.1007/978-3-030-73882-2_12","","",,,,,1,1.00,0,4,1,"","",""
6,"Weilin Wu, Jianyong Duan, R. Lu, F. Gao, Yuquan Chen","Embedded machine learning systems for robust spoken language parsing",2005,"","","","",197,"2022-07-13 10:07:05","","10.1109/NLPKE.2005.1598729","","",,,,,6,0.35,1,5,17,"In processing ill-formed spontaneous spoken utterance, many state-of-the-art robust parsers achieve robustness by allowing skipping of words and rule symbols. The parser's ability to skip words and rule symbols, however, results in a much bigger search space and greatly increases the parse ambiguity. Previous approaches resolved these issues through manually labeling the types of rule symbols, or by utilizing heuristic scores or statistical probabilities. However, these approaches have certain drawbacks. This paper proposes to exploit embedded machine learning techniques to help with pruning and disambiguation in robust parsers. An embedded machine learning system is integrated with the heuristic score and the strategy of basing the types of rule symbols upon their correspondence to the domain model. This integration can considerably relieve the reliance of robust parser development on linguistic expert handcrafting. Our experiments show that this integration offers stronger capability in ambiguity resolution, thereby enabling the robust parser to achieve better parsing accuracy.","",""
27,"Shichang Du, J. Lv, L. Xi","A robust approach for root causes identification in machining processes using hybrid learning algorithm and engineering knowledge",2012,"","","","",198,"2022-07-13 10:07:05","","10.1007/s10845-010-0498-9","","",,,,,27,2.70,9,3,10,"","",""
18,"Prachi Agrawal, T. Ganesh, A. W. Mohamed","A novel binary gaining-sharing knowledge-based optimization algorithm for feature selection",2020,"","","","",199,"2022-07-13 10:07:05","","10.1007/s00521-020-05375-8","","",,,,,18,9.00,6,3,2,"","",""
22,"Pasquale Antonante, Vasileios Tzoumas, Heng Yang, L. Carlone","Outlier-Robust Estimation: Hardness, Minimally Tuned Algorithms, and Applications",2020,"","","","",200,"2022-07-13 10:07:05","","10.1109/tro.2021.3094984","","",,,,,22,11.00,6,4,2,"Nonlinear estimation in robotics and vision is typically plagued with outliers due to wrong data association or incorrect detections from signal processing and machine learning methods. This article introduces two unifying formulations for outlier-robust estimation, <italic>generalized maximum consensus</italic> (<inline-formula><tex-math notation=""LaTeX"">$\text{G}$</tex-math></inline-formula>-<inline-formula><tex-math notation=""LaTeX"">$\text{MC}$</tex-math></inline-formula>) and <italic>generalized truncated least squares</italic> (<inline-formula><tex-math notation=""LaTeX"">$\text{G-TLS}$</tex-math></inline-formula>), and investigates fundamental limits, practical algorithms, and applications. Our first contribution is a proof that outlier-robust estimation is <italic>inapproximable:</italic> In the worst case, it is impossible to (even approximately) find the set of outliers, even with slower-than-polynomial-time algorithms (particularly, algorithms running in <italic>quasi-polynomial</italic> time). As a second contribution, we review and extend two general-purpose algorithms. The first, <italic>adaptive trimming</italic> (<inline-formula><tex-math notation=""LaTeX"">$\text{ADAPT}$</tex-math></inline-formula>), is combinatorial and is suitable for <inline-formula><tex-math notation=""LaTeX"">$\text{G}$</tex-math></inline-formula>-<inline-formula><tex-math notation=""LaTeX"">$\text{MC}$</tex-math></inline-formula>; the second, <italic>graduated nonconvexity</italic> (<inline-formula><tex-math notation=""LaTeX"">$\text{GNC}$</tex-math></inline-formula>), is based on homotopy methods and is suitable for <inline-formula><tex-math notation=""LaTeX"">$\text{G-TLS}$</tex-math></inline-formula>. We extend <inline-formula><tex-math notation=""LaTeX"">$\text{ADAPT}$</tex-math></inline-formula> and <inline-formula><tex-math notation=""LaTeX"">$\text{GNC}$</tex-math></inline-formula> to the case where the user does not have prior knowledge of the inlier-noise statistics (or the statistics may vary over time) and is unable to guess a reasonable threshold to separate inliers from outliers (as the one commonly used in RANdom SAmple Consensus <inline-formula><tex-math notation=""LaTeX"">$(\text{RANSAC})$</tex-math></inline-formula>. We propose the first <italic>minimally tuned</italic> algorithms for outlier rejection, which dynamically decide how to separate inliers from outliers. Our third contribution is an evaluation of the proposed algorithms on robot perception problems: mesh registration, image-based object detection (<italic>shape alignment</italic>), and pose graph optimization. <inline-formula><tex-math notation=""LaTeX"">$\text{ADAPT}$</tex-math></inline-formula> and <inline-formula><tex-math notation=""LaTeX"">$\text{GNC}$</tex-math></inline-formula> execute in real time, are deterministic, outperform <inline-formula><tex-math notation=""LaTeX"">$\text{RANSAC}$</tex-math></inline-formula>, and are robust up to 80–90% outliers. Their minimally tuned versions also compare favorably with the state of the art, even though they do not rely on a noise bound for the inliers.","",""
