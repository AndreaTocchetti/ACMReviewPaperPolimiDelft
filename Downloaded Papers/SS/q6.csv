Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
51,"Shubham Sharma, Jette Henderson, Joydeep Ghosh","CERTIFAI: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models",2019,"","","","",1,"2022-07-13 09:19:31","","10.1145/3375627.3375812","","",,,,,51,17.00,17,3,3,"As artificial intelligence plays an increasingly important role in our society, there are ethical and moral obligations for both businesses and researchers to ensure that their machine learning models are designed, deployed, and maintained responsibly. These models need to be rigorously audited for fairness, robustness, transparency, and interpretability. A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming. In this paper, we introduce a unified and model-agnostic approach to address these issues: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models (CERTIFAI). Unlike previous methods in this domain, CERTIFAI is a general tool that can be applied to any black-box model and any type of input data. Given a model and an input instance, CERTIFAI uses a custom genetic algorithm to generate counterfactuals: instances close to the input that change the prediction of the model. We demonstrate how these counterfactuals can be used to examine issues of robustness, interpretability, transparency, and fairness. Additionally, we introduce CERScore, the first black-box model robustness score that performs comparably to methods that have access to model internals.","",""
1,"Michael Tsang, James Enouen, Yan Liu","Interpretable Artificial Intelligence through the Lens of Feature Interaction",2021,"","","","",2,"2022-07-13 09:19:31","","","","",,,,,1,1.00,0,3,1,"Deep learning, alongside other modern machine learning techniques, has become the state of the art solution for a diverse range of real-world tasks. These include a variety of sensitive applications such as healthcare, finance, autonomous driving, criminal justice, and others which all pose significant concerns for fairness, robustness, safety, and trustworthiness. Despite these applications to critical tasks, deep networks are infamously referred to as black-box models because of their total lack of transparency in decision-making. If we are able to gain insight into how a model is coming to its conclusions, we are able to more clearly assess the trustworthiness and validity of its decisions. Consequently, an abundance of ongoing research is attempting to address model interpretability as the key problem to resolving these issues. There are many methods which are currently used to provide explanations of complex model predictions. LIME (Ribeiro et al., 2016) fits a local linear model around a data point, showing which features positively and negatively influence the prediction results. Despite the overall model being nonlinear, the local model gets an interpretable picture of how the model looks at small scales around the data point. Extensions of this method use other interpretable models like small decision tress. Shapley Values and SHAP follows a similar idea to assign a score to each feature, using a gametheoretic formulation which treats each feature as a player causing the final prediction (Lundberg and Lee, 2017). Its more rigorous formulation yields guarantees of its explanations summing up to the prediction score, but practically it usually must be estimated because of its high computational cost. Shuffle-based feature importance permutes the data of each feature to ascertain its importance in the final prediction in comparison to its normal prediction (Fisher et al., 2018). IG uses the fundamental theorem of calculus to provide additive explanations of a prediction (Sundararajan et al., 2017). This method is very popular in computer vision where its computational efficiency and saliency are prized, even though other work has exposed some of its shortcomings in providing an interpretation (Adebayo et al., 2018). Other methods are specifically designed for computer vision like TCAV (Kim et al., 2018) which finds a ‘concept direction’ corresponding to a large sample of concept images from the user. Surprisingly, all of these most popular interpretability methods share the same one limitation. None of these methods consider the shared importance of groups of two or more features; they only look at the effects had by each of the features individually. A feature interaction between two variables broadly describes a situation where both of the features/ variables are simultaneously important for a model’s prediction. In text applications for sentiment, ”not good” is a very simple example of two words strongly interacting with one another to create a negative sentiment. In modern-day applications, neural networks are usually hailed as amazing function approximators exactly because of their incredible ability to automatically uncover these kinds of complex relationships between the variables of the dataset. In many ways, however,","",""
0,"Simon Paul Bimczok, Elizabeth Godynyuk, Joris Pierey, Malin Siv Roppel, Mirjam Lisa Scholz","How are excellence and trust for using artificial intelligence ensured? Evaluation of its current use in EU healthcare",2021,"","","","",3,"2022-07-13 09:19:31","","10.11576/SEEJPH-4685","","",,,,,0,0.00,0,5,1,"Context: Artificial intelligence (AI) could be a key driver in different healthcare dossiers, ranging from preventive to diagnostic and treatment purposes. The establishment of the Artificial Intelligence High-Level Expert Group in the European Commission, as well as their White Paper, show first attempts of creating policies in the domain of artificial intelligence in the EU. Despite these policy approaches, there is a need for a coherent regulatory framework that enables the efficient use of AI in the field of health. The aim of this policy brief is to evaluate current legislative gaps in terms of the introduction of AI in healthcare, focusing on the domains of Data Protection, Liability & Transparency, as well as Robustness & Accuracy.  Policy Options: This policy brief identified a high degree of eHealth infrastructure fragmentation on member state level and limited action towards a structured and coherent framework for AI in healthcare, under the domains of Data Protection, Liability & Transparency, and Robustness & Accuracy.  Recommendations: A unified approach at EU-level, based on proposed recommendations and merged into the form of a Directive, is advised. The development of the Health-AI-Directive will bring progress and improvement to legal certainty in the European AI-landscape. The introduction of the Health-AI-Directive is recommended to ensure trust and excellence in the use of AI in healthcare.     Acknowledgments: The authors of this policy brief would like to thank all our tutors, lecturers and professors of the M.Sc. Governance and Leadership in European Public Health, with special thanks to Kasia Czabanowska and Rok Hržic, for enabling and encouraging us in the creation of this policy brief.  Authors’ contributions: All authors contributed equally to this work     Conflict of interest: None declared     Source of funding: None declared","",""
0,"","Patients set to benefit from new guidelines on artificial intelligence health solutions",2020,"","","","",4,"2022-07-13 09:19:31","","","","",,,,,0,0.00,0,0,2,"The use of these international guidelines will enable patients, health care professionals and policy-makers to be more confident on whether an AI intervention is safe and effective. This is a key step towards trustworthy AI in health. Development of new reporting guidelines which expand on the current SPIRIT 2013 and CONSORT 2010 reporting frameworks will boost transparency and robustness for clinical trials evaluating AI health solutions.","",""
5,"David Abele, Sara D’Onofrio","Artificial Intelligence – The Big Picture",2020,"","","","",5,"2022-07-13 09:19:31","","10.1007/978-3-658-27941-7_2","","",,,,,5,2.50,3,2,2,"","",""
3,"G. Irwin","Artificial intelligence approaches to model-based control",1998,"","","","",6,"2022-07-13 09:19:31","","10.1049/IC:19981030","","",,,,,3,0.13,3,1,24,"While of undoubted value for nonlinear identification and control of dynamic systems, neural networks have a number of limitations for practical applications. Thus, in online training, due consideration must be given to the necessity for regularisation with noisy data and to the choice of network architecture. More fundamentally, the nontransparent black-box nature of neural models make it difficult to include a priori system information, and to interpret the final structure meaningfully in terms of physical process characteristics. Neural approaches also fail to exploit the significant body of theoretical results available for conventional model-based control, making it difficult to analyse the closed-loop behaviour in terms of stability and robustness. The aim of this paper is to describe a nonlinear modelling architecture, called the local model network (LMN), which introduces transparency while offering distinct advantages for nonlinear model-based control. Simulation results for a pH neutralisation process are used to illustrate the performance benefits of LMNs for nonlinear dynamic matrix control (DMC) and for nonlinear internal model control (IMC). (6 pages)","",""
4,"Bo Li, Peng Qi, Bo Liu, Shuai Di, Jingen Liu, Jiquan Pei, Jinfeng Yi, Bowen Zhou","Trustworthy AI: From Principles to Practices",2021,"","","","",7,"2022-07-13 09:19:31","","","","",,,,,4,4.00,1,8,1,"The rapid development of Artificial Intelligence (AI) technology has enabled the deployment of various systems based on it. However, many current AI systems are found vulnerable to imperceptible attacks, biased against underrepresented groups, lacking in user privacy protection. These shortcomings degrade user experience and erode people’s trust in all AI systems. In this review, we provide AI practitioners with a comprehensive guide for building trustworthy AI systems. We first introduce the theoretical framework of important aspects of AI trustworthiness, including robustness, generalization, explainability, transparency, reproducibility, fairness, privacy preservation, and accountability. To unify currently available but fragmented approaches toward trustworthy AI, we organize them in a systematic approach that considers the entire lifecycle of AI systems, ranging from data acquisition to model development, to system development and deployment, finally to continuous monitoring and governance. In this framework, we offer concrete action items for practitioners and societal stakeholders (e.g., researchers, engineers, and regulators) to improve AI trustworthiness. Finally, we identify key opportunities and challenges for the future development of trustworthy AI systems, where we identify the need for a paradigm shift toward comprehensively trustworthy AI systems.","",""
2,"V. W. Anelli, Alejandro Bellog'in, T. D. Noia, F. Donini, Vincenzo Paparella, Claudio Pomo","Adherence and Constancy in LIME-RS Explanations for Recommendation (Long paper)",2021,"","","","",8,"2022-07-13 09:19:31","","","","",,,,,2,2.00,0,6,1,"Explainable Recommendation has attracted a lot of attention due to a renewed interest in explainable artificial intelligence. In particular, post-hoc approaches have proved to be the most easily applicable ones to increasingly complex recommendation models, which are then treated as black boxes. The most recent literature has shown that for post-hoc explanations based on local surrogate models, there are problems related to the robustness of the approach itself. This consideration becomes even more relevant in human-related tasks like recommendation. The explanation also has the arduous task of enhancing increasingly relevant aspects of user experience such as transparency or trustworthiness. This paper aims to show how the characteristics of a classical post-hoc model based on surrogates is strongly model-dependent and does not prove to be accountable for the explanations generated.","",""
1,"G. Vouros","Explainable Deep Reinforcement Learning: State of the Art and Challenges",2022,"","","","",9,"2022-07-13 09:19:31","","10.1145/3527448","","",,,,,1,1.00,1,1,1,"Interpretability, explainability and transparency are key issues to introducing Artificial Intelligence methods in many critical domains: This is important due to ethical concerns and trust issues strongly connected to reliability, robustness, auditability and fairness, and has important consequences towards keeping the human in the loop in high levels of automation, especially in critical cases for decision making, where both (human and the machine) play important roles. While the research community has given much attention to explainability of closed (or black) prediction boxes, there are tremendous needs for explainability of closed-box methods that support agents to act autonomously in the real world. Reinforcement learning methods, and especially their deep versions, are such closed-box methods. In this article we aim to provide a review of state of the art methods for explainable deep reinforcement learning methods, taking also into account the needs of human operators - i.e., of those that take the actual and critical decisions in solving real-world problems. We provide a formal specification of the deep reinforcement learning explainability problems, and we identify the necessary components of a general explainable reinforcement learning framework. Based on these, we provide a comprehensive review of state of the art methods, categorizing them in classes according to the paradigm they follow, the interpretable models they use, and the surface representation of explanations provided. The article concludes identifying open questions and important challenges.","",""
1,"Deepnshu Singh, Jyotinder Kaur Chaddah","A Study On Application Of Blockchain Technology To Control Counterfeit Drugs, Enhance Data Privacy and Improve Distribution In Online Pharmacy",2021,"","","","",10,"2022-07-13 09:19:31","","10.24083/apjhm.v16i3.1013","","",,,,,1,1.00,1,2,1,"Technology has always emerged to embolden the existing working process. On the one hand, it encourages transparency, accessibility and robustness in the system and on the other hand, it begets mitigation of the risks and allows us to detect, evaluate and eliminate vulnerability in the system. The most prominent technologies in todays’ world like Artificial Intelligence, Virtual/Augmented Reality, Automation, Cloud Computing are thriving to solve society’s problems and ensure the expedition in the process from its previous generation. Blockchain technology is no exception in providing the solution to eliminate the counterfeit markets across the globe and building trust among parties to do business without the fear of indulging or facing any unscrupulous business. Blockchain technology can ensure data privacy while improving supply chain transparency and reducing fraud. It can provide all stakeholders within a certain supply chain with access to the same information, potentially reducing errors. A decentralised application can pave the way to promulgate online pharmaceutical business where health information and each stakeholders’ data is not compromised. Against this backdrop the research conducted throws light on the challenges in the online medical drug and devices’ distribution and proposes a solution, an architectural design for blockchain technology in an online pharmaceutical platform to mitigate the counterfeit market while bringing efficiency to the ecosystem.","",""
0,"Amar Ali N. Khan, N. Aouf","Encoding A Mathematically Faithful DeepVIO Solution",2021,"","","","",11,"2022-07-13 09:19:31","","10.23919/ICCAS52745.2021.9649964","","",,,,,0,0.00,0,2,1,"Within the last decade visual odometry (VO) has been continually accumulating the research interests of the computer vision community. The accent of artificial intelligence (AI) is in real-time rerouting the interest of researchers from the traditional feature point methods, to AI based solutions - primarily those based on deep learning (DL), which in turn has forced the VO literature to become increasingly opaque. In an attempt to strike a balance between the understandability/robustness of new model and the ever-increasing temptation of exceedingly sophisticated black box models, this paper produced a highly sophisticated DL based End-to-End VO solution which religiously encodes classical mathematical VO solutions. This paper has developed a method to encode a mathematically proven traditional VO solution into a DL based solution with high transparency and achieved results inline with those of the comparable solution on the KITTI leaderboard.","",""
0,"G. El-khawaga, Mervat Abu-Elkheir, M. Reichert","XAI in the Context of Predictive Process Monitoring: An Empirical Analysis Framework",2022,"","","","",12,"2022-07-13 09:19:31","","10.3390/a15060199","","",,,,,0,0.00,0,3,1,"Predictive Process Monitoring (PPM) has been integrated into process mining use cases as a value-adding task. PPM provides useful predictions on the future of the running business processes with respect to different perspectives, such as the upcoming activities to be executed next, the final execution outcome, and performance indicators. In the context of PPM, Machine Learning (ML) techniques are widely employed. In order to gain trust of stakeholders regarding the reliability of PPM predictions, eXplainable Artificial Intelligence (XAI) methods have been increasingly used to compensate for the lack of transparency of most of predictive models. Multiple XAI methods exist providing explanations for almost all types of ML models. However, for the same data, as well as, under the same preprocessing settings or same ML models, generated explanations often vary significantly. Corresponding variations might jeopardize the consistency and robustness of the explanations and, subsequently, the utility of the corresponding model and pipeline settings. This paper introduces a framework that enables the analysis of the impact PPM-related settings and ML-model-related choices may have on the characteristics and expressiveness of the generated explanations. Our framework provides a means to examine explanations generated either for the whole reasoning process of an ML model, or for the predictions made on the future of a certain business process instance. Using well-defined experiments with different settings, we uncover how choices made through a PPM workflow affect and can be reflected through explanations. This framework further provides the means to compare how different characteristics of explainability methods can shape the resulting explanations and reflect on the underlying model reasoning process.","",""
0,"A. Aksjonov, V. Kyrki","A Safety-Critical Decision Making and Control Framework Combining Machine Learning and Rule-based Algorithms",2022,"","","","",13,"2022-07-13 09:19:31","","","","",,,,,0,0.00,0,2,1,"While artificial-intelligence-based methods suffer from lack of transparency, rule-based methods dominate in safety-critical systems. Yet, the latter cannot compete with the first ones in robustness to multiple requirements, for instance, simultaneously addressing safety, comfort, and efficiency. Hence, to benefit from both methods they must be joined in a single system. This paper proposes a decision making and control framework, which profits from advantages of both the ruleand machine-learning-based techniques while compensating for their disadvantages. The proposed method embodies two controllers operating in parallel, called Safety and Learned. A rule-based switching logic selects one of the actions transmitted from both controllers. The Safety controller is prioritized every time, when the Learned one does not meet the safety constraint, and also directly participates in the safe Learned controller training. Decision making and control in autonomous driving is chosen as the system case study, where an autonomous vehicle learns a multi-task policy to safely cross an unprotected intersection. Multiple requirements (i.e., safety, efficiency, and comfort) are set for vehicle operation. A numerical simulation is performed for the proposed framework validation, where its ability to satisfy the requirements and robustness to changing environment is successfully demonstrated.","",""
111,"M. Shahin, M. Jaksa, H. Maier","Recent Advances and Future Challenges for Artificial Neural Systems in Geotechnical Engineering Applications",2009,"","","","",14,"2022-07-13 09:19:31","","10.1155/2009/308239","","",,,,,111,8.54,37,3,13,"Artificial neural networks (ANNs) are a form of artificial intelligence that has proved to provide a high level of competency in solving many complex engineering problems that are beyond the computational capability of classicalmathematics and traditional procedures. In particular, ANNs have been applied successfully to almost all aspects of geotechnical engineering problems. Despite the increasing number and diversity of ANN applications in geotechnical engineering, the contents of reported applications indicate that the progress in ANN development and procedures is marginal and not moving forward since the mid-1990s. This paper presents a brief overview of ANN applications in geotechnical engineering, briefly provides an overview of the operation of ANN modeling, investigates the current research directions of ANNs in geotechnical engineering, and discusses some ANN modeling issues that need further attention in the future, including model robustness; transparency and knowledge extraction; extrapolation; uncertainty.","",""
5,"P. Santhanam","Quality Management of Machine Learning Systems",2020,"","","","",15,"2022-07-13 09:19:31","","10.1007/978-3-030-62144-5_1","","",,,,,5,2.50,5,1,2,"","",""
0,"J. Filipe, Ashish Ghosh, R. Prates, O. Shehory, E. Farchi, Guy Barash","Engineering Dependable and Secure Machine Learning Systems: Third International Workshop, EDSMLS 2020, New York City, NY, USA, February 7, 2020, Revised Selected Papers",2020,"","","","",16,"2022-07-13 09:19:31","","10.1007/978-3-030-62144-5","","",,,,,0,0.00,0,6,2,"","",""
0,"Javier Parra Domínguez, Pedro Roseiro","Blockchain: a brief review of Agri-Food Supply Chain Solutions and Opportunities",2020,"","","","",17,"2022-07-13 09:19:31","","10.14201/ADCAIJ20209495106","","",,,,,0,0.00,0,2,2,"This article aims at presenting Blockchain and Distributed Ledger Technologies from business perspective (although providing adequate technology context) and, especially, highlighting concrete implementations in Agri-Food Supply Chain, bringing security, transparency and robustness to solutions, and enabling the creation of added value through the provisioning of information to consumers which allow them to understand the origin, the transformation and the transportation of agri-food goods. It also brings some examples of European Programmes and Projects that are supporting innovative solutions to reach the market.","",""
32,"M. Jaksa, H. Maier, M. Shahin","Future challenges for artificial neural network modelling in geotechnical engineering",2008,"","","","",18,"2022-07-13 09:19:31","","","","",,,,,32,2.29,11,3,14,"Artificial neural networks (ANNs) are a form of artificial intelligence and, since the mid-1990s, ANNbased models have been successfully applied to virtually every problem in geotechnical engineering. This paper briefly examines the areas of geotechnical engineering to which ANNs have been applied, provides a brief overview of the operation of ANN models, and highlights and discusses four important issues which require further attention in the future. These are model robustness, transparency and knowledge extraction, extrapolation, and uncertainty. For ANN models to be more effective and useful in the future, it is essential that further work be undertaken in these four areas, particularly in the context of geotechnical engineering.","",""
1,"Geoffrey Rockwell, Emily Black, Evan Selinger, Antonio Davola, Elana Seide, K. Gulson","From Shortcut to Sleight of Hand: Why the Checklist Approach in the EU Guidelines Does Not Work",2019,"","","","",19,"2022-07-13 09:19:31","","","","",,,,,1,0.33,0,6,3,"Author(s): Rockwell, Geoffrey; Black, Emily; Selinger, Evan; Davola, Antonio; Seide, Elana; Gulson, Kalervo | Abstract: In April 2019, the High-Level Expert Group on Artificial Intelligence (AI) nominated by the EU Commission presented “Ethics Guidelines for Trustworthy Artificial Intelligence,” followed in June 2019 by a second “Policy and investment recommendations” Document.The Guidelines establish three characteristics (lawful, ethical, and robust) and seven key requirements (Human agency and oversight; Technical Robustness and safety; Privacy and data governance; Transparency; Diversity, non-discrimination and fairness; Societal and environmental well-being; and Accountability) that the development of AI should follow.The Guidelines are of utmost significance for the international debate over the regulation of AI. Firstly, they aspire to set a universal standard of care for the development of AI in the future. Secondly, they have been developed within a group of experts nominated by a regulatory body, and therefore will shape the normative approach in the EU regulation of AI and in its interaction with foreign countries. As the GDPR has shown, the effect of this normative activity goes way past the European Union territory.One of the most debated aspects of the Guidelines was the need to find an objective methodology to evaluate conformity with the key requirements. For this purpose, the Expert Group drafted an “assessment checklist” in the last part of the document: the list is supposed to be incorporated into existing practices, as a way for technology developers to consider relevant ethical issues and create more “trustworthy” AI. Our group undertook a critical assessment of the proposed tool from a multidisciplinary perspective, to assess its implications and limitations for global AI development.","",""
0,"Joshua A. Kroll","RFI : Developing a Federal AI Standards Engagement Plan",2019,"","","","",20,"2022-07-13 09:19:31","","","","",,,,,0,0.00,0,1,3,"I wish to submit the attached article, ""Data Science Data Governance"" for the NIST RFI on Artificial Intelligence Standards (Docket Number: 190312229-9229-01). While it was published in IEEE Security and Privacy, I hold an independent copyright and so can submit it for public posting here. The article, which describes high level approaches to data governance and software system governance I've encountered during my research on the governance of software systems, speaks most closely to question (8) on ""Technical standards and guidance that are needed to establish and advance trustworthy aspects (e.g., accuracy, transparency, security, privacy, and robustness) of AI technologies.""","",""
6,"Shifa Zhang, Anne Kim, Dianbo Liu, Sandeep C. Nuckchadyy, Lauren Huangy, Aditya Masurkary, Jingwei Zhangy, Lawrence Pratheek Karnatiz, Laura Martínez, T. Hardjono, Manolis Kellis, Zhizhuo Zhang","Genie: A Secure, Transparent Sharing and Services Platform for Genetic and Health Data",2018,"","","","",21,"2022-07-13 09:19:31","","","","",,,,,6,1.50,1,12,4,"Artificial Intelligence (AI) incorporating genetic and medical information have been applied in disease risk prediction, unveiling disease mechanism, and advancing therapeutics. However, AI training relies on highly sensitive and private data which significantly limit their applications and robustness evaluation. Moreover, the data access management after sharing across organization heavily relies on legal restriction, and there is no guarantee in preventing data leaking after sharing. Here, we present Genie, a secure AI platform which allows AI models to be trained on medical data securely. The platform combines the security of Intel Software Guarded eXtensions (SGX), transparency of blockchain technology, and verifiability of open algorithms and source codes. Genie shares insights of genetic and medical data without exposing anyone's raw data. All data is instantly encrypted upon upload and contributed to the models that the user chooses. The usage of the model and the value generated from the genetic and health data will be tracked via a blockchain, giving the data transparent and immutable ownership.","",""
0,"Peer-Olaf Siebers","Hongmei He (intelligent System Lab, University of Bristol): Soft Computing Approaches under the Framework of Hierarchical Decision Making or Classification System",2010,"","","","",22,"2022-07-13 09:19:31","","","","",,,,,0,0.00,0,1,12,"Hongmei He (Intelligent System Lab, University of Bristol): Soft Computing Approaches Under the Framework of Hierarchical Decision Making or Classification System With the development of AI, we can see there are a surprising number of the brain functions of the human Intelligent System (IS) are quite similar to those of an artificial IS, since most artificial ISs are modelled through naturally emulating human intelligence. A wide variety of approaches have been utilised in the functional design of artificial ISs. For example, fuzzy logic for robustness, decision trees for the transparency of reasoning, machine learning for knowledge learning, semantics for understandability, probabilistic reasoning, and neural computing, etc.","",""
28,"Karl Fine Licht, Jenny Fine Licht","Artificial intelligence, transparency, and public decision-making",2020,"","","","",23,"2022-07-13 09:19:31","","10.1007/s00146-020-00960-w","","",,,,,28,14.00,14,2,2,"","",""
24,"Karl de Fine Licht, Jenny de Fine Licht","Artificial intelligence, transparency, and public decision-making",2020,"","","","",24,"2022-07-13 09:19:31","","10.1007/s00146-020-00960-w","","",,,,,24,12.00,12,2,2,"","",""
3,"","Artificial intelligence, transparency, and public decision-making: Why explanations are key when trying to produce perceived legitimacy",2020,"","","","",25,"2022-07-13 09:19:31","","","","",,,,,3,1.50,0,0,2,"The increasing use of Artificial Intelligence (AI) for making decisions in public affairs has sparked a lively debate on the benefits and potential harms of self-learning technologies, ranging from the hopes of fully informed and objectively taken decisions to fear for the destruction of mankind. To prevent the negative outcomes and to achieve accountable systems, many have argued that we need to open up the “black box” of AI decision-making and make it more transparent. Whereas this debate has primarily focused on how transparency can secure high-quality, fair, and reliable decisions, far less attention has been devoted to the role of transparency when it comes to how the general public come to perceive AI decision-making as legitimate and worthy of acceptance. Since relying on coercion is not only normatively problematic but also costly and highly inefficient, perceived legitimacy is fundamental to the democratic system. This paper discusses how transparency in and about AI decision-making can affect the public’s perception of the legitimacy of decisions and decision-makers and produce a framework for analyzing these questions. We argue that a limited form of transparency that focuses on providing justifications for decisions has the potential to provide sufficient ground for perceived legitimacy without producing the harms full transparency would bring.","",""
13,"Karl de Fine Licht, Jenny de Fine Licht","Artificial intelligence, transparency, and public decision-making",2020,"","","","",26,"2022-07-13 09:19:31","","10.1007/S00146-020-00960-W","","",,,,,13,6.50,7,2,2,"","",""
13,"Joel Walmsley","Artificial intelligence and the value of transparency",2020,"","","","",27,"2022-07-13 09:19:31","","10.1007/s00146-020-01066-z","","",,,,,13,6.50,13,1,2,"","",""
13,"R. Daneshjou, Mary P Smith, Mary D Sun, V. Rotemberg, James Zou","Lack of Transparency and Potential Bias in Artificial Intelligence Data Sets and Algorithms: A Scoping Review.",2021,"","","","",28,"2022-07-13 09:19:31","","10.1001/jamadermatol.2021.3129","","",,,,,13,13.00,3,5,1,"Importance Clinical artificial intelligence (AI) algorithms have the potential to improve clinical care, but fair, generalizable algorithms depend on the clinical data on which they are trained and tested.   Objective To assess whether data sets used for training diagnostic AI algorithms addressing skin disease are adequately described and to identify potential sources of bias in these data sets.   Data Sources In this scoping review, PubMed was used to search for peer-reviewed research articles published between January 1, 2015, and November 1, 2020, with the following paired search terms: deep learning and dermatology, artificial intelligence and dermatology, deep learning and dermatologist, and artificial intelligence and dermatologist.   Study Selection Studies that developed or tested an existing deep learning algorithm for triage, diagnosis, or monitoring using clinical or dermoscopic images of skin disease were selected, and the articles were independently reviewed by 2 investigators to verify that they met selection criteria.   Consensus Process Data set audit criteria were determined by consensus of all authors after reviewing existing literature to highlight data set transparency and sources of bias.   Results A total of 70 unique studies were included. Among these studies, 1 065 291 images were used to develop or test AI algorithms, of which only 257 372 (24.2%) were publicly available. Only 14 studies (20.0%) included descriptions of patient ethnicity or race in at least 1 data set used. Only 7 studies (10.0%) included any information about skin tone in at least 1 data set used. Thirty-six of the 56 studies developing new AI algorithms for cutaneous malignant neoplasms (64.3%) met the gold standard criteria for disease labeling. Public data sets were cited more often than private data sets, suggesting that public data sets contribute more to new development and benchmarks.   Conclusions and Relevance This scoping review identified 3 issues in data sets that are used to develop and test clinical AI algorithms for skin disease that should be addressed before clinical translation: (1) sparsity of data set characterization and lack of transparency, (2) nonstandard and unverified disease labels, and (3) inability to fully assess patient diversity used for algorithm development and testing.","",""
129,"S. Vollmer, B. Mateen, G. Bohner, F. Király, R. Ghani, P. Jónsson, Sarah Cumbers, Adrian Jonas, K. McAllister, P. Myles, David Grainger, M. Birse, Richard Branson, K. Moons, G. Collins, J. Ioannidis, C. Holmes, H. Hemingway","Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness",2020,"","","","",29,"2022-07-13 09:19:31","","10.1136/bmj.l6927","","",,,,,129,64.50,13,18,2,"Machine learning, artificial intelligence, and other modern statistical methods are providing new opportunities to operationalise previously untapped and rapidly growing sources of data for patient benefit. Despite much promising research currently being undertaken, particularly in imaging, the literature as a whole lacks transparency, clear reporting to facilitate replicability, exploration for potential ethical concerns, and clear demonstrations of effectiveness. Among the many reasons why these problems exist, one of the most important (for which we provide a preliminary solution here) is the current lack of best practice guidance specific to machine learning and artificial intelligence. However, we believe that interdisciplinary groups pursuing research and impact projects involving machine learning and artificial intelligence for health would benefit from explicitly addressing a series of questions concerning transparency, reproducibility, ethics, and effectiveness (TREE). The 20 critical questions proposed here provide a framework for research groups to inform the design, conduct, and reporting; for editors and peer reviewers to evaluate contributions to the literature; and for patients, clinicians and policy makers to critically appraise where new findings may deliver patient benefit.","",""
35,"S. Larsson, F. Heintz","Transparency in artificial intelligence",2020,"","","","",30,"2022-07-13 09:19:31","","10.14763/2020.2.1469","","",,,,,35,17.50,18,2,2,"This conceptual paper addresses the issues of transparency as linked to artificial intelligence (AI) from socio-legal and computer scientific perspectives. Firstly, we discuss the conceptual distinction between transparency in AI and algorithmic transparency, and argue for the wider concept ‘in AI’, as a partly contested albeit useful notion in relation to transparency. Secondly, we show that transparency as a general concept is multifaceted, and of widespread theoretical use in multiple disciplines over time, particularly since the 1990s. Still, it has had a resurgence in contemporary notions of AI governance, such as in the multitude of recently published ethics guidelines on AI. Thirdly, we discuss and show the relevance of the fact that transparency expresses a conceptual metaphor of more general significance, linked to knowing, bringing positive connotations that may have normative effects to regulatory debates. Finally, we draw a possible categorisation of aspects related to transparency in AI, or what we interchangeably call AI transparency, and argue for the need of developing a multidisciplinary understanding, in order to contribute to the governance of AI as applied on markets and in society. (Less)","",""
40,"Philipp Schmidt, F. Biessmann, Timm Teubner","Transparency and trust in artificial intelligence systems",2020,"","","","",31,"2022-07-13 09:19:31","","10.1080/12460125.2020.1819094","","",,,,,40,20.00,13,3,2,"ABSTRACT Assistive technology featuring artificial intelligence (AI) to support human decision-making has become ubiquitous. Assistive AI achieves accuracy comparable to or even surpassing that of human experts. However, often the adoption of assistive AI systems is limited by a lack of trust of humans into an AI’s prediction. This is why the AI research community has been focusing on rendering AI decisions more transparent by providing explanations of an AIs decision. To what extent these explanations really help to foster trust into an AI system remains an open question. In this paper, we report the results of a behavioural experiment in which subjects were able to draw on the support of an ML-based decision support tool for text classification. We experimentally varied the information subjects received and show that transparency can actually have a negative impact on trust. We discuss implications for decision makers employing assistive AI technology.","",""
4,"I. Barclay, Harrison Taylor, A. Preece, Ian Taylor, Ian Taylor, D. Verma, Geeth de Mel","A framework for fostering transparency in shared artificial intelligence models by increasing visibility of contributions",2020,"","","","",32,"2022-07-13 09:19:31","","10.1002/cpe.6129","","",,,,,4,2.00,1,7,2,"Increased adoption of artificial intelligence (AI) systems into scientific workflows will result in an increasing technical debt as the distance between the data scientists and engineers who develop AI system components and scientists, researchers and other users grows. This could quickly become problematic, particularly where guidance or regulations change and once‐acceptable best practice becomes outdated, or where data sources are later discredited as biased or inaccurate. This paper presents a novel method for deriving a quantifiable metric capable of ranking the overall transparency of the process pipelines used to generate AI systems, such that users, auditors and other stakeholders can gain confidence that they will be able to validate and trust the data sources and contributors in the AI systems that they rely on. The methodology for calculating the metric, and the type of criteria that could be used to make judgements on the visibility of contributions to systems are evaluated through models published at ModelHub and PyTorch Hub, popular archives for sharing science resources, and is found to be helpful in driving consideration of the contributions made to generating AI systems and approaches toward effective documentation and improving transparency in machine learning assets shared within scientific communities.","",""
3,"Simone Stumpf, Lorenzo Strappelli, Subeida Ahmed, Yuri Nakao, A. Naseer, Giulia Del Gamba, D. Regoli","Design Methods for Artificial Intelligence Fairness and Transparency",2021,"","","","",33,"2022-07-13 09:19:31","","","","",,,,,3,3.00,0,7,1,"Fairness and transparency in artificial intelligence (AI) continue to become more prevalent as topics for research, design and development. General principles and guidelines for designing ethical and responsible AI systems have been proposed, yet there is a lack of design methods for these kinds of systems. In this paper, we present CoFAIR, a novel method to design user interfaces for exploring fairness, consisting of series of co-design workshops, and wider evaluation. This method can be readily applied in practice by researchers, designers and developers to create responsible and ethical AI systems.","",""
1,"Ana Lucic, Maurits Bleeker, Sami Jullien, Samarth Bhargav, M. de Rijke","Teaching Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence through the Lens of Reproducibility",2021,"","","","",34,"2022-07-13 09:19:31","","","","",,,,,1,1.00,0,5,1,"In this work we explain the setup for a technical, graduatelevel course on Fairness, Accountability, Confidentiality and Transparency in Artificial Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI concepts through the lens of reproducibility. The focal point of the course is a group project based on reproducing existing FACT-AI algorithms from top AI conferences, and writing a report about their experiences. In the first iteration of the course, we created an open source repository with the code implementations from the group projects. In the second iteration, we encouraged students to submit their group projects to the Machine Learning Reproducibility Challenge, which resulted in 9 reports from our course being accepted to the challenge. We reflect on our experience teaching the course over two academic years, where one year coincided with a global pandemic, and propose guidelines for teaching FACTAI through reproducibility in graduate-level AI programs. We hope this can be a useful resource for instructors to set up similar courses at their universities in the future.","",""
29,"H. Felzmann, E. Fosch-Villaronga, C. Lutz, A. Tamó-Larrieux","Towards Transparency by Design for Artificial Intelligence",2020,"","","","",35,"2022-07-13 09:19:31","","10.1007/s11948-020-00276-4","","",,,,,29,14.50,7,4,2,"","",""
52,"Hamon Ronan, Junklewitz Henrik, S. Ignacio","Robustness and Explainability of Artificial Intelligence",2020,"","","","",36,"2022-07-13 09:19:31","","10.2760/57493","","",,,,,52,26.00,17,3,2,"","",""
109,"B. Haibe-Kains, George Adam, A. Hosny, F. Khodakarami, L. Waldron, Bo Wang, C. McIntosh, A. Goldenberg, A. Kundaje, C. Greene, Tamara Broderick, M. M. Hoffman, J. Leek, K. Korthauer, W. Huber, A. Brazma, Joelle Pineau, R. Tibshirani, T. Hastie, J. Ioannidis, John Quackenbush, H. Aerts","Transparency and reproducibility in artificial intelligence.",2020,"","","","",37,"2022-07-13 09:19:31","","10.1038/s41586-020-2766-y","","",,,,,109,54.50,11,22,2,"","",""
18,"M. Alshamsi, S. Salloum, M. Alshurideh, S. Abdallah","Artificial Intelligence and Blockchain for Transparency in Governance",2020,"","","","",38,"2022-07-13 09:19:31","","10.1007/978-3-030-51920-9_11","","",,,,,18,9.00,5,4,2,"","",""
18,"B. Haibe-Kains, George Adam, A. Hosny, F. Khodakarami, Maqc Society Board, L. Waldron, Bo Wang, C. McIntosh, A. Kundaje, C. Greene, M. M. Hoffman, J. Leek, W. Huber, A. Brazma, Joelle Pineau, R. Tibshirani, T. Hastie, J. Ioannidis, John Quackenbush, H. Aerts","The importance of transparency and reproducibility in artificial intelligence research",2020,"","","","",39,"2022-07-13 09:19:31","","","","",,,,,18,9.00,2,20,2,"In their study, McKinney et al. showed the high potential of artificial intelligence for breast cancer screening. However, the lack of detailed methods and computer code undermines its scientific value. We identify obstacles hindering transparent and reproducible AI research as faced by McKinney et al and provide solutions with implications for the broader field.","",""
13,"B. Koçak, O. Kaya, Çağrı Erdim, Ece Ates Kus, O. Kilickesmez","Artificial Intelligence in Renal Mass Characterization: A Systematic Review of Methodologic Items Related to Modeling, Performance Evaluation, Clinical Utility, and Transparency.",2020,"","","","",40,"2022-07-13 09:19:31","","10.2214/AJR.20.22847","","",,,,,13,6.50,3,5,2,"OBJECTIVE. The objective of our study was to systematically review the literature about the application of artificial intelligence (AI) to renal mass characterization with a focus on the methodologic quality items. MATERIALS AND METHODS. A systematic literature search was conducted using PubMed to identify original research studies about the application of AI to renal mass characterization. Besides baseline study characteristics, a total of 15 methodologic quality items were extracted and evaluated on the basis of the following four main categories: modeling, performance evaluation, clinical utility, and transparency items. The qualitative synthesis was presented using descriptive statistics with an accompanying narrative. RESULTS. Thirty studies were included in this systematic review. Overall, the methodologic quality items were mostly favorable for modeling (63%) and performance evaluation (63%). Even so, the studies (57%) more frequently constructed their work on nonrobust features. Furthermore, only a few studies (10%) had a generalizability assessment with independent or external validation. The studies were mostly unsuccessful in terms of clinical utility evaluation (89%) and transparency (97%) items. For clinical utility, the interesting findings were lack of comparisons with both radiologists' evaluation (87%) and traditional models (70%) in most of the studies. For transparency, most studies (97%) did not share their data with the public. CONCLUSION. To bring AI-based renal mass characterization from research to practice, future studies need to improve modeling and performance evaluation strategies and pay attention to clinical utility and transparency issues.","",""
25,"Stephen Cory Robinson","Trust, transparency, and openness: How inclusion of cultural values shapes Nordic national public policy strategies for artificial intelligence (AI)",2020,"","","","",41,"2022-07-13 09:19:31","","10.1016/j.techsoc.2020.101421","","",,,,,25,12.50,25,1,2,"","",""
0,"Małgorzata Duda-Śmiałek","Transparency of artificial intelligence systems as an expression of the right to be informed",2021,"","","","",42,"2022-07-13 09:19:31","","10.2307/j.ctv282jgff.18","","",,,,,0,0.00,0,1,1,"","",""
7,"Tom van Nuenen, Xavier Ferrer, J. Such, M. Coté","Transparency for Whom? Assessing Discriminatory Artificial Intelligence",2020,"","","","",43,"2022-07-13 09:19:31","","10.1109/MC.2020.3002181","","",,,,,7,3.50,2,4,2,"Artificial intelligence decision making can cause discriminatory harm to many vulnerable groups. Redress is often suggested through increased transparency of these systems. But for what group are we implementing it? This article seeks to identify what transparency means for technical, legislative, and public realities and stakeholders.","",""
5,"Sara B. Jordan, Samantha L. Fenn, Benjamin B. Shannon","Transparency as Threat at the Intersection of Artificial Intelligence and Cyberbiosecurity",2020,"","","","",44,"2022-07-13 09:19:31","","10.1109/MC.2020.2995578","","",,,,,5,2.50,2,3,2,"Have separate actions designed to meet ethical norms of transparency inadvertently led to a situation of greater possible harm from biological warfare attacks? Efforts to make artificial intelligence and data from biological sciences more publicly available have raised novel concerns about national security.","",""
4,"Tingting Wu, Yunwei Dong, Zhiwei Dong, Aziz Singa, Xiong Chen, Yu Zhang","Testing Artificial Intelligence System Towards Safety and Robustness: State of the Art",2020,"","","","",45,"2022-07-13 09:19:31","","","","",,,,,4,2.00,1,6,2,"With the increasing development of machine learning, conventional embedded systems cannot meet the requirement of current academic researches and industrial applications. Artificial Intelligence System (AIS) based on machine learning has been widely used in various safety-critical systems, such as machine vision, autonomous vehicles, collision avoidance system. Different from conventional embedded systems, AIS generates and updates control strategies through learning algorithms which make the control behaviors nondeterministic and bring about the test oracle problem in AIS testing procedure. There have been various testing approaches for AIS to guarantee the safety and robustness. However, few researches explain how to conduct AIS testing with a complete workflow systematically. This paper provides a comprehensive survey of existing testing techniques to detect the erroneous behaviors of AIS, and sums up the involved key steps and testing components in terms of test coverage criterion, test data generation, testing approach and common dataset. This literature review aims at organizing a standardized workflow and leading to a practicable insight and research trend towards AIS testing.","",""
353,"Erico Tjoa, Cuntai Guan","A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI",2019,"","","","",46,"2022-07-13 09:19:31","","10.1109/TNNLS.2020.3027314","","",,,,,353,117.67,177,2,3,"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.","",""
16,"","Machine learning and artificial intelligence research for patient benefit: 20 critical questions on transparency, replicability, ethics, and effectiveness",2020,"","","","",47,"2022-07-13 09:19:31","","10.1136/bmj.m1312","","",,,,,16,8.00,0,0,2,"","",""
9,"S. M. McKinney, A. Karthikesalingam, Daniel Tse, Christopher J. Kelly, Yun Liu, G. Corrado, S. Shetty","Reply to: Transparency and reproducibility in artificial intelligence.",2020,"","","","",48,"2022-07-13 09:19:31","","10.1038/s41586-020-2767-x","","",,,,,9,4.50,1,7,2,"","",""
86,"H. Felzmann, E. F. Villaronga, C. Lutz, A. Tamó-Larrieux","Transparency you can trust: Transparency requirements for artificial intelligence between legal norms and contextual concerns",2019,"","","","",49,"2022-07-13 09:19:31","","10.1177/2053951719860542","","",,,,,86,28.67,22,4,3,"Transparency is now a fundamental principle for data processing under the General Data Protection Regulation. We explore what this requirement entails for artificial intelligence and automated decision-making systems. We address the topic of transparency in artificial intelligence by integrating legal, social, and ethical aspects. We first investigate the ratio legis of the transparency requirement in the General Data Protection Regulation and its ethical underpinnings, showing its focus on the provision of information and explanation. We then discuss the pitfalls with respect to this requirement by focusing on the significance of contextual and performative factors in the implementation of transparency. We show that human–computer interaction and human-robot interaction literature do not provide clear results with respect to the benefits of transparency for users of artificial intelligence technologies due to the impact of a wide range of contextual factors, including performative aspects. We conclude by integrating the information- and explanation-based approach to transparency with the critical contextual approach, proposing that transparency as required by the General Data Protection Regulation in itself may be insufficient to achieve the positive goals associated with transparency. Instead, we propose to understand transparency relationally, where information provision is conceptualized as communication between technology providers and users, and where assessments of trustworthiness based on contextual factors mediate the value of transparency communications. This relational concept of transparency points to future research directions for the study of transparency in artificial intelligence systems and should be taken into account in policymaking.","",""
14,"A. Zaji, H. Bonakdari","Robustness lake water level prediction using the search heuristic-based artificial intelligence methods",2019,"","","","",50,"2022-07-13 09:19:31","","10.1080/09715010.2018.1424568","","",,,,,14,4.67,7,2,3,"Abstract Lakes have a crucial role in the industrial, agricultural, environment, and drinking water fields. Accurate prediction of lake levels is one of the most important parameters in the reservoir management and lakeshore structure designing. The goal of the present study is to examine the robustness of two different Genetic Algorithm-based regression methods namely the Genetic Algorithm Artificial neural network (GAA) and the Genetic Programming (GP) by considering their performance in predicting the non-observed lakes. To do that, data collected from the four-year daily measurements of the Chahnimeh#1 lake in Eastern Iran were used for developing the GAA and GP models and after that, the performance of the considered models are examined to predict the lake water levels of an adjacent lake namely Chahnimeh#4 as the non-observed information. The results showed that both model has the ability to simulate adjacent lakes using the considered lake water levels for the training procedure. In addition, another goal is to develop simple, practical formulation for predicting the lake water level, So that, using the GP method, as the superior model, three different formulations are proposed in order to predict the one, three, and five days ahead lake water level, respectively.","",""
755,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xisheng Fang, Shiqin Zhang, J. Xia, Jun Xia","Artificial Intelligence Distinguishes COVID-19 from Community Acquired Pneumonia on Chest CT",2020,"","","","",51,"2022-07-13 09:19:31","","10.1148/radiol.2020200905","","",,,,,755,377.50,76,18,2,"Background Coronavirus disease has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performances. Materials and Methods In this retrospective and multi-center study, a deep learning model, COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT exams for the detection of COVID-19. Community acquired pneumonia (CAP) and other non-pneumonia CT exams were included to test the robustness of the model. The datasets were collected from 6 hospitals between August 2016 and February 2020. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity and specificity. Results The collected dataset consisted of 4356 chest CT exams from 3,322 patients. The average age is 49±15 years and there were slightly more male patients than female (1838 vs 1484; p-value=0.29). The per-exam sensitivity and specificity for detecting COVID-19 in the independent test set was 114 of 127 (90% [95% CI: 83%, 94%]) and 294 of 307 (96% [95% CI: 93%, 98%]), respectively, with an AUC of 0.96 (p-value<0.001). The per-exam sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175) and 92% (239 of 259), respectively, with an AUC of 0.95 (95% CI: 0.93, 0.97). Conclusions A deep learning model can accurately detect COVID-19 and differentiate it from community acquired pneumonia and other lung diseases.","",""
0,"Yanyong Du","On the Transparency of Artificial Intelligence System",2022,"","","","",52,"2022-07-13 09:19:31","","10.32629/jai.v5i1.486","","",,,,,0,0.00,0,1,1,"In order to improve the effectiveness of the management of artificial intelligence system, there is a growing demand for improving the transparency of artificial intelligence system from all parts of society. Improving the transparency of artificial intelligence system is conducive to relevant personnel better assuming their responsibilities and protecting the public’s right to know. Therefore, the principle of transparency appears most frequently in all kinds of ethical principles and ethical guidelines of artificial intelligence, but there are some differences in the definition of its connotation by different subjects. The transparency of artificial intelligence system is reflected in many aspects like algorithm interpretation, data transparency and function transparency. We need to fully understand the limit of artificial intelligence transparency from the perspective of the characteristics of intelligence, the current situation of artificial intelligence technology and the feasibility of technical governance. For the construction path of artificial intelligence system transparency, there are many ways, such as technical approach, ethical and legal regulation and cultural approach.","",""
4,"Kacper Sokol","Fairness, Accountability and Transparency in Artificial Intelligence: A Case Study of Logical Predictive Models",2019,"","","","",53,"2022-07-13 09:19:31","","10.1145/3306618.3314316","","",,,,,4,1.33,4,1,3,"Machine learning -- the part of artificial intelligence aimed at eliciting knowledge from data and automated decision making without explicit instructions -- is making great strides, with new algorithms being invented every day. These algorithms find myriads of applications, but their ubiquity often comes at the expense of limited interpretability, hidden biases and unexpected vulnerabilities. Whenever one of these factors is a priority, the learning algorithm of choice is often a method considered to be inherently interpretable, e.g. logical models such as decision trees. In my research I challenge this assumption and highlight (quite common) cases when the assumed interpretability fails to deliver. To restore interpretability of logical machine learning models (decision trees and their ensembles in particular) I propose to explain them with class-contrastive counterfactual statements, which are a very common type of explanation in human interactions, well-grounded in social science research. To evaluate transparency of such models I collate explainability desiderata that can be used to systematically assess and compare such methods as an addition to user studies. Given contrastive explanations, I investigate their influence on the model's security, in particular gaming and stealing the model. Finally, I evaluate model fairness, where I am interested in choosing the most fair model among all the models with equal performance.","",""
7,"Shlomit Yanisky-Ravid Sean K. Hallisey","“Equality and Privacy by Design”: A New Model of Artificial Intelligence Data Transparency via Auditing, Certification, and Safe Harbor Regimes",2019,"","","","",54,"2022-07-13 09:19:31","","","","",,,,,7,2.33,7,1,3,"Artificial Intelligence and Machine Learning (AI) are often described as technological breakthroughs that will completely transform our society and economy. AI systems have been implemented everywhere, from medicine, transportation, finance, art, to legal and social spheres, and even in weapons development. In many sectors, AI systems have already started making decisions previously made by humans. Promising as AI systems may be, they also pose urgent challenges to our everyday life. While much attention has concerned AI’s legal implications, the literature suffers from a lack of solutions that account for both legal and engineering practices and constraints. This leaves technology firms without * Professor Shlomit Yanisky-Ravid, Ph.D., Fordham Law School, Visiting Professor; Fordham Law Center on Law and Information Policy (CLIP), Head of AI-IP and Blockchain Project; Yale Law School, Information Society Project (ISP), Fellow; Ono Law School, Israel, Senior Faculty, the Shalom Comparative Legal Research Institute, OAC, Founder and Academic Director. Sean K. Hallisey, Fordham Law, CLIP, AI-IP Project, Fellow. We gratefully dedicate this Article to Joel Reidenberg, the founder and the head of Fordham Law Center of Law and Information Policy (CLIP), for his initiative, support, and encouragement, all of which tremendously contributed to the writing of this Article and the development of its ideas. We would also like to thank all the Fellows at the Fordham CLIP IP-AI and Blockchain Project, Yale Law, ISP, as well as to the students of the course “Intellectual Property and the Challenges of Advanced Technology: AI and Blockchain,” for their wonderful discussions, insights and comments. Finally, we thank Dean Matthew Diller, Fordham Law School, for promoting and stressing the challenges of advanced technology, data privacy, and intellectual property, and Linda Sugin, Associate Dean for Academic Affairs at Fordham Law, for her support. 2019] FORDHAM URB. L.J. 429 guidelines and increases the risk of societal harm. It also means that policymakers and judges operate without a regulatory regime to turn to when addressing these novel and unpredictable outcomes. This Article tries to fill the void by focusing on data rather than on the software and programmers. It suggests a new model that stems from a recognition of the significant role that the data plays in the development and functioning of AI systems. Data is the most important aspect of teaching AI systems to operate. AI algorithms begin with a massive preexisting dataset, which data providers use to train the system. But the data that AI systems “swallow” can be illegal, discriminatory, altered, unreliable, or simply incomplete. Thus, the more data fed to the AI systems, the higher the likelihood that they could produce biased, discriminatory decisions and violate privacy rights. The Article discusses how discrimination can arise, even inadvertently, from the operation of “trusted” and “objective” AI systems. To address this problem, this Article proposes a new AI Data Transparency Model that focuses on disclosure of data rather than, as some scholars argue, focusing on the initial software program and programmers. The Model includes an auditing regime and a certification program, run either by a governmental body or, in the absence of such entity, by private institutions. This Model will encourage the industry to take proactive steps to ensure and publicize that datasets are trustworthy. The suggested Model includes a safe harbor, which incentivizes firms to implement transparency recommendations even without massive regulatory oversight. From an engineering point of view, the Model recognizes data providers and big data as the most important components in the process of creating, training and operating AI systems. Even more importantly, the Model is technologically feasible because data can be easily absorbed and kept by a technological tool. Further, this Model is also practically feasible because it follows already existing legal frameworks of data transparency, such as the ones being implemented by the FDA and the SEC. Improving transparency in data systems would result in less harmful AI systems, better protect societal rights and norms, and produce improved outcomes in this emerging field, especially for minority communities that often lack resources or representation to challenge AI systems. Increased transparency of the data used while developing, training or operating AI systems would mitigate and reduce these harms. Additionally, to better identify the risks of faulty data, industry players must conduct critical evaluations and audits of the data used to train AI systems; one way to incentivize this is a 430 FORDHAM URB. L.J. [Vol. XLVI certification system to publicize good-faith efforts to reduce the possibility of discriminatory outcomes and privacy violations in AI systems. This Article strives to incentivize the creation of new standards, which the industry could implement from the genesis of AI systems to mitigate the possibility of harm, rather than post-hoc assignments of liability.","",""
1,"Jeanna Neefe Matthews","Patterns and Antipatterns, Principles, and Pitfalls: Accountability and Transparency in Artificial Intelligence",2019,"","","","",55,"2022-07-13 09:19:31","","","","",,,,,1,0.33,1,1,3,"Copyright © 2020, Association for the Advancement of Artificial Intelligence. All rights reserved. ISSN 0738-4602 82 AI MAGAZINE Increasingly, decisions that significantly impact the lives of individuals (such as decisions about hiring, housing, insurance, loans, criminal justice, or medical treatment) are being made in a partnership between human decisionmakers and artificial intelligence (AI) systems. As builders of AI systems, we know how easy it is for errors to occur. We also know how difficult it can be to push the boundaries and adapt a system developed in one context into another. As developers of AI, we know how our systems learn from people and from the past, assimilating latent biases. Understanding all of this, who better than us to insist that the systems we build support investigation and iterative improvement, so that others are empowered to help counter the limitations of AI while benefiting from its strengths?  This article discusses a set of principles for accountability and transparency in AI as well as a set of antipatterns or harmful trends too often seen in deployed systems. It provides concrete suggestions for what can be done to shift the balance away from these antipatterns and toward more positive ones. Patterns and Antipatterns, Principles, and Pitfalls: Accountability and Transparency in Artificial Intelligence","",""
11,"T. Wischmeyer","Artificial Intelligence and Transparency: Opening the Black Box",2019,"","","","",56,"2022-07-13 09:19:31","","10.1007/978-3-030-32361-5_4","","",,,,,11,3.67,11,1,3,"","",""
16,"Vikram Puri, A. Kataria, Vishal Sharma","Artificial intelligence‐powered decentralized framework for Internet of Things in Healthcare 4.0",2021,"","","","",57,"2022-07-13 09:19:31","","10.1002/ETT.4245","","",,,,,16,16.00,5,3,1,"Correspondence to: Vishal Sharma, School of Electronics, Electrical Engineering and Computer Science (EEECS), Queen’s University Belfast, Belfast, UK. Email:vishal_sharma2012@hotmail.com Abstract Remote patient monitoring and data management have gained much popularity in recent years because of their enhanced access to low-cost healthcare services. A cloud-based healthcare system provides numerous solutions for collecting patient data and offers on-demand well-managed reports to patients and healthcare providers. However, it equally suffers from single-point failure, security, privacy, and non-transparency issues with the data, impacting the continuity of the system. To resolve such concerns, this article proposes an artificial intelligence (AI)-enabled decentralized healthcare framework that accesses and authenticates Internet of Things (IoT) devices and create trust and transparency in patient healthcare records (PHR). The mechanism is based on the AI-enabled smart contracts and the conceptualization of the public blockchain network. Alongside this, the framework identifies the malicious IoT nodes in the system. The experimental analyses are performed on the real-time test environment, and significant improvements are suggested in terms of device energy consumption, data request time, throughput, average latency, and transaction fee.","",""
1,"Ana Lucic, Maurits Bleeker, Sami Jullien, Samarth Bhargav, M. de Rijke","Reproducibility as a Mechanism for Teaching Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence",2021,"","","","",58,"2022-07-13 09:19:31","","10.1609/aaai.v36i11.21558","","",,,,,1,1.00,0,5,1,"In this work, we explain the setup for a technical, graduate-level course on Fairness, Accountability, Confidentiality, and Transparency in Artificial Intelligence (FACT-AI) at the University of Amsterdam, which teaches FACT-AI concepts through the lens of reproducibility.  The focal point of the course is a group project based on reproducing existing FACT-AI algorithms from top AI conferences and writing a corresponding report.  In the first iteration of the course, we created an open source repository with the code implementations from the group projects.  In the second iteration, we encouraged students to submit their group projects to the Machine Learning Reproducibility Challenge, resulting in 9 reports from our course being accepted for publication in the ReScience journal.  We reflect on our experience teaching the course over two years, where one year coincided with a global pandemic, and propose guidelines for teaching FACT-AI through reproducibility in graduate-level AI study programs.  We hope this can be a useful resource for instructors who want to set up similar courses in the future.","",""
19,"Ruhhee Tabbussum, A. Q. Dar","Performance evaluation of artificial intelligence paradigms—artificial neural networks, fuzzy logic, and adaptive neuro-fuzzy inference system for flood prediction",2021,"","","","",59,"2022-07-13 09:19:31","","10.1007/s11356-021-12410-1","","",,,,,19,19.00,10,2,1,"","",""
0,"Beilei Wang, Jie Jing, Xiaochun Huang, Cheng Hua, Qin Qin, Y. Jia, Zhiyong Wang, Lei Jiang, Bai Gao, Les J. Wu, Xianfei Zeng, Fubo Wang, Chuanbin Mao, Shanrong Liu","Establishment of a Knowledge‐and‐Data‐Driven Artificial Intelligence System with Robustness and Interpretability in Laboratory Medicine",2022,"","","","",60,"2022-07-13 09:19:31","","10.1002/aisy.202100204","","",,,,,0,0.00,0,14,1,"Laboratory medicine plays an important role in clinical diagnosis. However, no laboratory‐based artificial intelligence (AI) diagnostic system has been applied in current clinical practice due to the lack of robustness and interpretability. Although many attempts have been made, it is still difficult for doctors to adopt the existing machine learning (ML) patterns in interpreting laboratory (lab) big data. Here, a knowledge‐and‐data‐driven laboratory diagnostic system is developed, termed AI‐based Lab tEst tO diagNosis (AI LEON), by integrating an innovative knowledge graph analysis framework and “mixed XGboost and Genetic Algorithm (MiXG)” technique to simulate the doctor's laboratory‐based diagnosis. To establish AI LEON, we included 89 116 949 laboratory data and 10 423 581 diagnosis data points from 730 113 participants. Among them, 686 626 participants were recruited for training and validating purposes with the remaining for testing purposes. AI LEON automatically identified and analyzed 2071 lab indexes, resulting in multiple disease recommendations that involved 441 common diseases in ten organ systems. AI LEON exhibited outstanding transparency and interpretability in three universal clinical application scenarios and outperformed human physicians in interpreting lab reports. AI LEON is an advanced intelligent system that enables a comprehensive interpretation of lab big data, which substantially improves the clinical diagnosis.","",""
0,"Y. Hayashi","Toward the transparency of deep learning in radiological imaging: beyond quantitative to qualitative artificial intelligence",2019,"","","","",61,"2022-07-13 09:19:31","","10.21037/jmai.2019.09.0","","",,,,,0,0.00,0,1,3,"In the near future, nearly every type of clinician, from paramedics to certificated medical specialists, will be expected to utilize artificial intelligence (AI) technology, and deep learning (DL) in particular (1). In terms of exceeding human ability, DL has been the backbone of computer science. DL mostly involves automated feature extraction using deep neural networks (DNNs), which can aid in the classification and discrimination of medical images, including mammograms, skin lesions, pathological slides, radiological images, and retinal fundus photographs.","",""
1793,"E. Topol","High-performance medicine: the convergence of human and artificial intelligence",2019,"","","","",62,"2022-07-13 09:19:31","","10.1038/s41591-018-0300-7","","",,,,,1793,597.67,1793,1,3,"","",""
14,"S. Modgil, R. Singh, C. Hannibal","Artificial intelligence for supply chain resilience: learning from Covid-19",2021,"","","","",63,"2022-07-13 09:19:31","","10.1108/ijlm-02-2021-0094","","",,,,,14,14.00,5,3,1,"PurposeMany supply chains have faced disruption during Covid-19. Artificial intelligence (AI) is one mechanism that can be used to improve supply chain resilience by developing business continuity capabilities. This study examines how firms employ AI and consider the opportunities for AI to enhance supply chain resilience by developing visibility, risk, sourcing and distribution capabilities.Design/methodology/approachThe authors have gathered rich data by conducting semistructured interviews with 35 experts from the e-commerce supply chain. The authors have adopted a systematic approach of coding using open, axial and selective methods to map and identify the themes that represent the critical elements of AI-enabled supply chain resilience.FindingsThe results of the study highlight the emergence of five critical areas where AI can contribute to enhanced supply chain resilience; (1) transparency, (2) ensuring last-mile delivery, (3) offering personalized solutions to both upstream and downstream supply chain stakeholders, (4) minimizing the impact of disruption and (5) facilitating an agile procurement strategy.Research limitations/implicationsThe study offers interesting implications for bridging the theory–practice gap by drawing on contemporary empirical data to demonstrate how enhancing dynamic capabilities via AI technologies further strengthens supply chain resilience. The study also offers suggestions for utilizing the findings and proposes a framework to strengthen supply chain resilience through AI.Originality/valueThe study presents the dynamic capabilities for supply chain resilience through the employment of AI. AI can contribute to readying supply chains to reduce their risk of disruption through enhanced resilience.","",""
14,"A. Rosenfeld","Better Metrics for Evaluating Explainable Artificial Intelligence",2021,"","","","",64,"2022-07-13 09:19:31","","10.5555/3463952.3463962","","",,,,,14,14.00,14,1,1,"This paper presents objective metrics for how explainable artificial intelligence (XAI) can be quantified. Through an overview of current trends, we show that many explanations are generated post-hoc and independent of the agent’s logical process, which in turn creates explanations with limited meaning as they lack transparency and fidelity. While user studies are a known basis for evaluating XAI, studies that do not consider objective metrics for evaluating XAI may have limited meaning and may suffer from confirmation bias, particularly if they use low fidelity explanations unnecessarily. To avoid this issue, this paper suggests a paradigm shift in evaluating XAI that focuses on metrics that quantify the explanation itself and its appropriateness given the XAI goal. We suggest four such metrics based on performance differences, D, between the explanation’s logic and the agent’s actual performance, the number of rules, R, outputted by the explanation, the number of features, F , used to generate that explanation, and the stability, S, of the explanation. We believe that user studies that focus on these metrics in their evaluations are inherently more valid and should be integrated in future XAI research.","",""
411,"R. Vinuesa, Hossein Azizpour, Iolanda Leite, Madeline Balaam, Virginia Dignum, S. Domisch, Anna Felländer, S. Langhans, Max Tegmark, F. F. Nerini","The role of artificial intelligence in achieving the Sustainable Development Goals",2019,"","","","",65,"2022-07-13 09:19:31","","10.1038/s41467-019-14108-y","","",,,,,411,137.00,41,10,3,"","",""
822,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xi Fang, Shiqin Zhang, J. Xia, Jun Xia","Using Artificial Intelligence to Detect COVID-19 and Community-acquired Pneumonia Based on Pulmonary CT: Evaluation of the Diagnostic Accuracy",2020,"","","","",66,"2022-07-13 09:19:31","","10.1148/RADIOL.2020200905","","",,,,,822,411.00,82,18,2,"Background Coronavirus disease 2019 (COVID-19) has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performance. Materials and Methods In this retrospective and multicenter study, a deep learning model, the COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT scans for the detection of COVID-19. CT scans of community-acquired pneumonia (CAP) and other non-pneumonia abnormalities were included to test the robustness of the model. The datasets were collected from six hospitals between August 2016 and February 2020. Diagnostic performance was assessed with the area under the receiver operating characteristic curve, sensitivity, and specificity. Results The collected dataset consisted of 4352 chest CT scans from 3322 patients. The average patient age (±standard deviation) was 49 years ± 15, and there were slightly more men than women (1838 vs 1484, respectively; P = .29). The per-scan sensitivity and specificity for detecting COVID-19 in the independent test set was 90% (95% confidence interval [CI]: 83%, 94%; 114 of 127 scans) and 96% (95% CI: 93%, 98%; 294 of 307 scans), respectively, with an area under the receiver operating characteristic curve of 0.96 (P < .001). The per-scan sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175 scans) and 92% (239 of 259 scans), respectively, with an area under the receiver operating characteristic curve of 0.95 (95% CI: 0.93, 0.97). Conclusion A deep learning model can accurately detect coronavirus 2019 and differentiate it from community-acquired pneumonia and other lung conditions. © RSNA, 2020 Online supplemental material is available for this article.","",""
195,"Jessica Fjeld, Nele Achten, Hannah Hilligoss, Ádám Nagy, Madhulika Srikumar","Principled Artificial Intelligence: Mapping Consensus in Ethical and Rights-Based Approaches to Principles for AI",2020,"","","","",67,"2022-07-13 09:19:31","","10.2139/ssrn.3518482","","",,,,,195,97.50,39,5,2,"The rapid spread of artificial intelligence (AI) systems has precipitated a rise in ethical and human rights-based frameworks intended to guide the development and use of these technologies. Despite the proliferation of these ""AI principles,"" there has been little scholarly focus on understanding these efforts either individually or as contextualized within an expanding universe of principles with discernible trends.    To that end, this white paper and its associated data visualization compare the contents of thirty-six prominent AI principles documents side-by-side. This effort uncovered a growing consensus around eight key thematic trends: privacy, accountability, safety and security, transparency and explainability, fairness and non-discrimination, human control of technology, professional responsibility, and promotion of human values. Underlying this “normative core,” our analysis examined the forty-seven individual principles that make up the themes, detailing notable similarities and differences in interpretation found across the documents. In sharing these observations, it is our hope that policymakers, advocates, scholars, and others working to maximize the benefits and minimize the harms of AI will be better positioned to build on existing efforts and to push the fractured, global conversation on the future of AI toward consensus.","",""
159,"Xiaoxuan Liu, S. C. Rivera, D. Moher, M. Calvert, A. Denniston, H. Ashrafian, A. Beam, A. Chan, G. Collins, A. Darzi, J. Deeks, M. Elzarrad, Cyrus Espinoza, Andre Esteva, L. Faes, L. Ferrante di Ruffano, J. Fletcher, R. Golub, H. Harvey, C. Haug, Christopher Holmes, Adrian Jonas, P. Keane, Christopher J. Kelly, Aaron Y. Lee, Cecilia S Lee, Elaine Manna, J. Matcham, Melissa D. McCradden, Joao Monteiro, C. Mulrow, L. Oakden-Rayner, D. Paltoo, M. Panico, G. Price, Samuel d. Rowley, Richard Savage, Rupa Sarkar, S. Vollmer, C. Yau","Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI Extension",2020,"","","","",68,"2022-07-13 09:19:31","","10.1136/bmj.m3164","","",,,,,159,79.50,16,40,2,"Abstract The CONSORT 2010 (Consolidated Standards of Reporting Trials) statement provides minimum guidelines for reporting randomised trials. Its widespread use has been instrumental in ensuring transparency when evaluating new interventions. More recently, there has been a growing recognition that interventions involving artificial intelligence (AI) need to undergo rigorous, prospective evaluation to demonstrate impact on health outcomes. The CONSORT-AI extension is a new reporting guideline for clinical trials evaluating interventions with an AI component. It was developed in parallel with its companion statement for clinical trial protocols: SPIRIT-AI. Both guidelines were developed through a staged consensus process, involving a literature review and expert consultation to generate 29 candidate items, which were assessed by an international multi-stakeholder group in a two-stage Delphi survey (103 stakeholders), agreed on in a two-day consensus meeting (31 stakeholders) and refined through a checklist pilot (34 participants). The CONSORT-AI extension includes 14 new items, which were considered sufficiently important for AI interventions, that they should be routinely reported in addition to the core CONSORT 2010 items. CONSORT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention is integrated, the handling of inputs and outputs of the AI intervention, the human-AI interaction and providing analysis of error cases. CONSORT-AI will help promote transparency and completeness in reporting clinical trials for AI interventions. It will assist editors and peer-reviewers, as well as the general readership, to understand, interpret and critically appraise the quality of clinical trial design and risk of bias in the reported outcomes.","",""
12,"H. Ibrahim, Xiaoxuan Liu, S. C. Rivera, D. Moher, A. Chan, M. Sydes, M. Calvert, A. Denniston","Reporting guidelines for clinical trials of artificial intelligence interventions: the SPIRIT-AI and CONSORT-AI guidelines",2021,"","","","",69,"2022-07-13 09:19:31","","10.1186/s13063-020-04951-6","","",,,,,12,12.00,2,8,1,"","",""
10,"R. Berk","Artificial Intelligence, Predictive Policing, and Risk Assessment for Law Enforcement",2021,"","","","",70,"2022-07-13 09:19:31","","10.1146/annurev-criminol-051520-012342","","",,,,,10,10.00,10,1,1,"There are widespread concerns about the use of artificial intelligence in law enforcement. Predictive policing and risk assessment are salient examples. Worries include the accuracy of forecasts that guide both activities, the prospect of bias, and an apparent lack of operational transparency. Nearly breathless media coverage of artificial intelligence helps shape the narrative. In this review, we address these issues by first unpacking depictions of artificial intelligence. Its use in predictive policing to forecast crimes in time and space is largely an exercise in spatial statistics that in principle can make policing more effective and more surgical. Its use in criminal justice risk assessment to forecast who will commit crimes is largely an exercise in adaptive, nonparametric regression. It can in principle allow law enforcement agencies to better provide for public safety with the least restrictive means necessary, which can mean far less use of incarceration. None of this is mysterious. Nevertheless, concerns about accuracy, fairness, and transparency are real, and there are tradeoffs between them for which there can be no technical fix. You can't have it all. Solutions will be found through political and legislative processes achieving an acceptable balance between competing priorities.","",""
10,"Shun Zhang, Muye Li, Mengnan Jian, Yajun Zhao, Feifei Gao","AIRIS: Artificial intelligence enhanced signal processing in reconfigurable intelligent surface communications",2021,"","","","",71,"2022-07-13 09:19:31","","10.23919/JCC.2021.07.013","","",,,,,10,10.00,2,5,1,"Reconfigurable intelligent surface (RIS) is an emerging meta-surface that can provide additional communications links through reflecting the signals, and has been recognized as a strong candidate of 6G mobile communications systems. Meanwhile, it has been recently admitted that implementing artificial intelligence (AI) into RIS communications will extensively benefit the reconfiguration capacity and enhance the robustness to complicated transmission environments. Besides the conventional model-driven approaches, AI can also deal with the existing signal processing problems in a data-driven manner via digging the inherent characteristic from the real data. Hence, AI is particularly suitable for the signal processing problems over RIS networks under unideal scenarios like modeling mismatching, insufficient resource, hardware impairment, as well as dynamical transmissions. As one of the earliest survey papers, we will introduce the merging of AI and RIS, called AIRIS, over various signal processing topics, including environmental sensing, channel acquisition, beam-forming design, and resource scheduling, etc. We will also discuss the challenges of AIRIS and present some interesting future directions.","",""
7,"S. Yanisky-Ravid, Sean Hallisey","‘Equality and Privacy by Design’: Ensuring Artificial Intelligence (AI) Is Properly Trained & Fed: A New Model of AI Data Transparency & Certification As Safe Harbor Procedures",2018,"","","","",72,"2022-07-13 09:19:31","","10.2139/SSRN.3278490","","",,,,,7,1.75,4,2,4,"Artificial Intelligence systems (“AI”) are often described as a technological breakthrough that will completely transform our society and economy. AI systems have been implemented in all facets of the economy, from medicine to transportation, finance, art, legal, social, and weapons; making decisions previously determined by humans. While this article recognizes that AI systems promise benefits, it also identifies urgent challenges to our everyday life. Just as the technology has become prolific, so has the literature concerning its legal implications. However, the literature suffers from a lack of solutions that address the legal and engineering perspectives. This leaves technology firms without guidelines and increases the risk of societal harm. Policymakers, including judges, operate without a regulatory regime to turn to when addressing these novel and unpredictable outcomes. This article tries to fill the void by focusing on the use of data by these systems, rather than on the software and software programmers. It suggests a new Model that stems from a recognition of the significant role that the data plays in the development and functioning of AI systems.    One of the most important phases of teaching AI systems to operate starts with a preexisting massive dataset that the data providers use to train the system. The data providers are programmers, trainers; the stakeholders who enable access to data or the systems’ users. In this article, we analyze and discuss the threats the use of data by AI systems pose in terms of producing discriminatory outcomes as well as violations of privacy.    The data can be illegal, discriminatory, manufacture, unreliable, or simply incomplete. The more data that AI systems “swallow,” the likelihood increases that AI systems could produce biased, discriminatory decisions and/or violate privacy. The article discusses how discrimination can arise, even inadvertently, from the operation of “trusted” and ""objective"" AI systems. The article addresses, on the one hand, the hurdles and challenges behind the use of big data by AI systems, and on the other, suggests a possible, new solution.    We propose a new AI data transparency Model that focuses on disclosure of the data being used by AI systems, when necessary. To perfect the Model we recommend an auditing regime and a certification program, either by governmental body or, in the absence of such entity, by private institutes. This Model will encourage the industry to take steps, proactively, to ensure that the dataset is trustworthy and then, to publicly exhibit the quality of the data (that their AI systems rely on). By receiving and publicizing a quality “stamp” the firms will fairly build their competitive reputation and will strengthen the public control of the systems.    We envision that the implementation of this Model will help firms and individuals become educated about the potential issues concerning AI, discrimination and the continued weakening of societal expectations of privacy. In this sense, the AI data transparency Model operates as a safe harbor mechanism that incentivizes the industry, from the first steps of developing and training AI systems, to the actual operation of the AI systems, to implement effective standards, that we coin Equality and Privacy by Design.    The suggested AI Transparency Model functions as a safe harbor, even without massive regulatory steps. From an engineering point of view, not only does the model recognize the data providers and the big data as the most important components in the process of creating, training and operating AI systems, but the AI Data Transparency Model is also technologically feasible as data can be easily absorbed and kept by a technological tool. This Model is feasible from a practical perspective, as it follows already existing legal frameworks of data transparency, such as the ones being implemented by the FDA and SEC.    We argue that improving transparency in data systems should result in less harmful AI systems, better protect societal rights and norms, and produce improved outcomes in this emerging field, specifically for minority communities, who often lack resources or representation to combat the use of AI systems. We assert that improvements in transparency regarding the data used while developing, training or operating AI systems could mitigate and reduce these harms. We recommend critical evaluations and audits of data used to train AI systems to identify such risks, and propose a certification system whereby AI systems can publicize good faith efforts to reduce the possibility of discriminatory outcomes and privacy violations. We do not purport to solve the riddle of every possible negative outcome created by AI systems; instead, we are trying to incentivize the creation of new standards that the industry could implement, from day one of developing AI systems that addresses the possibility of harm, rather than post-hoc assignments of liability.","",""
90,"R. Shafin, Lingjia Liu, V. Chandrasekhar, Hao Chen, J. Reed, Jianzhong Zhang","Artificial Intelligence-Enabled Cellular Networks: A Critical Path to Beyond-5G and 6G",2019,"","","","",73,"2022-07-13 09:19:31","","10.1109/MWC.001.1900323","","",,,,,90,30.00,15,6,3,"Mobile network operators (MNOs) are in the process of overlaying their conventional macro cellular networks with shorter range cells such as outdoor pico cells. The resultant increase in network complexity creates substantial overhead in terms of operating expenses, time, and labor for their planning and management. Artificial intelligence (AI) offers the potential for MNOs to operate their networks in a more organic and cost-efficient manner. We argue that deploying AI in fifth generation (5G) and beyond will require surmounting significant technical barriers in terms of robustness, performance, and complexity. We outline future research directions, identify top five challenges, and present a possible roadmap to realize the vision of AI-enabled cellular networks for Beyond- 5G and sixth generation (6G) networks.","",""
61,"S. Gerke, T. Minssen, Glenn Cohen","Ethical and legal challenges of artificial intelligence-driven healthcare",2020,"","","","",74,"2022-07-13 09:19:31","","10.2139/ssrn.3570129","","",,,,,61,30.50,20,3,2,"  Abstract    This chapter will map the ethical and legal challenges posed by artificial intelligence (AI) in healthcare and suggest directions for resolving them. Section 1 will briefly clarify what AI is and Section 2 will give an idea of the trends and strategies in the United States (US) and Europe, thereby tailoring the discussion to the ethical and legal debate of AI-driven healthcare. This will be followed in Section 3 by a discussion of four primary ethical challenges, namely, (1) informed consent to use, (2) safety and transparency, (3) algorithmic fairness and biases, and (4) data privacy. Section 4 will then analyze five legal challenges in the US and Europe: (1) safety and effectiveness, (2) liability, (3) data protection and privacy, (4) cybersecurity, and (5) intellectual property law. Finally, Section 5 will summarize the major conclusions and especially emphasize the importance of building an AI-driven healthcare system that is successful and promotes trust and the motto Health AIs for All of Us.   ","",""
132,"Xiaoxuan Liu, Samantha Cruz Rivera, D. Moher, M. Calvert, A. Denniston","Reporting guidelines for clinical trial reports for interventions involving artificial intelligence: the CONSORT-AI extension",2020,"","","","",75,"2022-07-13 09:19:31","","10.1038/s41591-020-1034-x","","",,,,,132,66.00,26,5,2,"","",""
8,"S. Atakishiyev, H. Babiker, Nawshad Farruque, R. Goebel1, M-Y. Kima, M. H. Motallebi, J. Rabelo, T. Syed, Osmar R Zaiane","A multi-component framework for the analysis and design of explainable artificial intelligence",2020,"","","","",76,"2022-07-13 09:19:31","","10.3390/make3040045","","",,,,,8,4.00,1,9,2,"The rapid growth of research in explainable artificial intelligence (XAI) follows on two substantial developments. First, the enormous application success of modern machine learning methods, especially deep and reinforcement learning, which have created high expectations for industrial, commercial and social value. Second, the emergence of concern for creating trusted AI systems, including the creation of regulatory principles to ensure transparency and trust of AI systems.These two threads have created a kind of ""perfect storm"" of research activity, all eager to create and deliver it any set of tools and techniques to address the XAI demand. As some surveys of current XAI suggest, there is yet to appear a principled framework that respects the literature of explainability in the history of science, and which provides a basis for the development of a framework for transparent XAI. Here we intend to provide a strategic inventory of XAI requirements, demonstrate their connection to a history of XAI ideas, and synthesize those ideas into a simple framework to calibrate five successive levels of XAI.","",""
8,"M. Dora, Ashwani Kumar, S. Mangla, Abhay Pant, M. Kamal","Critical success factors influencing artificial intelligence adoption in food supply chains",2021,"","","","",77,"2022-07-13 09:19:31","","10.1080/00207543.2021.1959665","","",,,,,8,8.00,2,5,1,"The adoption of Artificial Intelligence (AI) in the food supply chains (FSC) can address unique challenges of food safety, quality and wastage by improving transparency and traceability. However, t...","",""
8,"Akshat Pandey, Aylin Caliskan","Disparate Impact of Artificial Intelligence Bias in Ridehailing Economy's Price Discrimination Algorithms",2020,"","","","",78,"2022-07-13 09:19:31","","10.1145/3461702.3462561","","",,,,,8,4.00,4,2,2,"Ridehailing applications that collect mobility data from individuals to inform smart city planning predict each trip's fare pricing with automated algorithms that rely on artificial intelligence (AI). This type of AI algorithm, namely a price discrimination algorithm, is widely used in the industry's black box systems for dynamic individualized pricing. Lacking transparency, studying such AI systems for fairness and disparate impact has not been possible without access to data used in generating the outcomes of price discrimination algorithms. Recently, in an effort to enhance transparency in city planning, the city of Chicago regulation mandated that transportation providers publish anonymized data on ridehailing. As a result, we present the first large-scale measurement of the disparate impact of price discrimination algorithms used by ridehailing applications. The application of random effects models from the meta-analysis literature combines the city-level effects of AI bias on fare pricing from census tract attributes, aggregated from the American Community Survey. An analysis of 100 million ridehailing samples from the city of Chicago indicates a significant disparate impact in fare pricing of neighborhoods due to AI bias learned from ridehailing utilization patterns associated with demographic attributes. Neighborhoods with larger non-white populations, higher poverty levels, younger residents, and high education levels are significantly associated with higher fare prices, with combined effect sizes, measured in Cohen's d, of -0.32, -0.28, 0.69, and 0.24 for each demographic, respectively. Further, our methods hold promise for identifying and addressing the sources of disparate impact in AI algorithms learning from datasets that contain U.S. geolocations.","",""
46,"I. Poel","Embedding Values in Artificial Intelligence (AI) Systems",2020,"","","","",79,"2022-07-13 09:19:31","","10.1007/s11023-020-09537-4","","",,,,,46,23.00,46,1,2,"","",""
34,"T. H. Aldhyani, M. Al-Yaari, Hasan Alkahtani, Mashael S. Maashi","Water Quality Prediction Using Artificial Intelligence Algorithms",2020,"","","","",80,"2022-07-13 09:19:31","","10.1155/2020/6659314","","",,,,,34,17.00,9,4,2,"During the last years, water quality has been threatened by various pollutants. Therefore, modeling and predicting water quality have become very important in controlling water pollution. In this work, advanced artificial intelligence (AI) algorithms are developed to predict water quality index (WQI) and water quality classification (WQC). For the WQI prediction, artificial neural network models, namely nonlinear autoregressive neural network (NARNET) and long short-term memory (LSTM) deep learning algorithm, have been developed. In addition, three machine learning algorithms, namely, support vector machine (SVM), K-nearest neighbor (K-NN), and Naive Bayes, have been used for the WQC forecasting. The used dataset has 7 significant parameters, and the developed models were evaluated based on some statistical parameters. The results revealed that the proposed models can accurately predict WQI and classify the water quality according to superior robustness. Prediction results demonstrated that the NARNET model performed slightly better than the LSTM for the prediction of the WQI values and the SVM algorithm has achieved the highest accuracy (97.01%) for the WQC prediction. Furthermore, the NARNET and LSTM models have achieved similar accuracy for the testing phase with a slight difference in the regression coefficient (RNARNET = 96.17% and RLSTM = 94.21%). This kind of promising research can contribute significantly to water management.","",""
81,"M. Coeckelbergh","Artificial Intelligence, Responsibility Attribution, and a Relational Justification of Explainability",2019,"","","","",81,"2022-07-13 09:19:31","","10.1007/s11948-019-00146-8","","",,,,,81,27.00,81,1,3,"","",""
4,"A. Gaggioli","Bringing More Transparency to Artificial Intelligence",2017,"","","","",82,"2022-07-13 09:19:31","","10.1089/cyber.2016.29060.csi","","",,,,,4,0.80,4,1,5,"The development of artificial intelligence (AI) has taken giant steps during the last decade to the point that for many experts, including the world-renowned atrophysics Stephen Hawking and hi-tech entrepreneur Elon Musk, AI could even destroy civilization by overtaking humans. However, on the other hand, AI may bring about huge benefits for humankind, some of which may be still beyond our imagination today. Thus, the scientific community is faced with the challenge of how we can develop powerful AI systems that support civilization while at the same time preventing the potential side effects of an uncontrolled AI evolution. To address these challenges, in late September 2016, tech giants Google, Facebook, Microsoft, Amazon, and IBM launched a ‘‘Partnership on Artificial Intelligence To Benefit People and Society.’’ The new alliance has been established ‘‘to study and formulate best practices on AI technologies, to advance the public’s understanding of AI, and to serve as an open platform for discussion and engagement about AI and its influences on people and society’’ (https://www .partnershiponai.org/). As claimed in the mission statement, a specific goal of the initiative is to help improving public awareness of what is happening in the AI field, where a number of players are shaping the future of intelligent services. Also, the Partnership aims to create more inclusive discussion by extending participation from AI specialists to activists and experts in other disciplines, such as psychology, philosophy, economics, finance, sociology, public policy, and law, to discuss and provide guidance on emerging issues related to the impact of AI on society. The Partnership has the potential to create a greater multidisciplinary understanding of the opportunities and challenges associated with potential breakthroughs in this field. Yet, some key players, such as Apple and Elon Musk’s OpenAI—a nonprofit AI research project (https://www .openai.com/blog/)—have not yet joined the club. While the goals of the Partnership have been set, the strategy that the alliance intends to put in place to attain these objectives is still unclear. Thus, it is too early to understand how the association will concretely address the challenges that need to be addressed with the public, such as how AI can be used safely to support military activities, or how to deal with the legal responsibilities for any damage caused by AI to humans.","",""
4,"Irene-Angelica Chounta, Emanuele Bardone, Aet Raudsep, M. Pedaste","Exploring Teachers’ Perceptions of Artificial Intelligence as a Tool to Support their Practice in Estonian K-12 Education",2021,"","","","",83,"2022-07-13 09:19:31","","10.1007/S40593-021-00243-5","","",,,,,4,4.00,1,4,1,"","",""
5,"F. Hussain, R. Hussain, E. Hossain","Explainable Artificial Intelligence (XAI): An Engineering Perspective",2021,"","","","",84,"2022-07-13 09:19:31","","","","",,,,,5,5.00,2,3,1,"The remarkable advancements in Deep Learning (DL) algorithms have fueled enthusiasm for using Artificial Intelligence (AI) technologies in almost every domain; however, the opaqueness of these algorithms put a question mark on their applications in safety-critical systems. In this regard, the ‘explainability’ dimension is not only essential to both explain the inner workings of black-box algorithms, but it also adds accountability and transparency dimensions that are of prime importance for regulators, consumers, and service providers. eXplainable Artificial Intelligence (XAI) is the set of techniques and methods to convert the so-called black-box AI algorithms to white-box algorithms, where the results achieved by these algorithms and the variables, parameters, and steps taken by the algorithm to reach the obtained results, are transparent and explainable. To complement the existing literature on XAI, in this paper, we take an ‘engineering’ approach to illustrate the concepts of XAI. We discuss the stakeholders in XAI and describe the mathematical contours of XAI from engineering perspective. Then we take the autonomous car as a use-case and discuss the applications of XAI for its different components such as object detection, perception, control, action decision, and so on. This work is an exploratory study to identify new avenues of research in the field of XAI.","",""
61,"N. Mirchi, Vincent Bissonnette, R. Yilmaz, N. Ledwos, A. Winkler-Schwartz, R. Del Maestro","The Virtual Operative Assistant: An explainable artificial intelligence tool for simulation-based training in surgery and medicine",2020,"","","","",85,"2022-07-13 09:19:31","","10.1371/journal.pone.0229596","","",,,,,61,30.50,10,6,2,"Simulation-based training is increasingly being used for assessment and training of psychomotor skills involved in medicine. The application of artificial intelligence and machine learning technologies has provided new methodologies to utilize large amounts of data for educational purposes. A significant criticism of the use of artificial intelligence in education has been a lack of transparency in the algorithms’ decision-making processes. This study aims to 1) introduce a new framework using explainable artificial intelligence for simulation-based training in surgery, and 2) validate the framework by creating the Virtual Operative Assistant, an automated educational feedback platform. Twenty-eight skilled participants (14 staff neurosurgeons, 4 fellows, 10 PGY 4–6 residents) and 22 novice participants (10 PGY 1–3 residents, 12 medical students) took part in this study. Participants performed a virtual reality subpial brain tumor resection task on the NeuroVR simulator using a simulated ultrasonic aspirator and bipolar. Metrics of performance were developed, and leave-one-out cross validation was employed to train and validate a support vector machine in Matlab. The classifier was combined with a unique educational system to build the Virtual Operative Assistant which provides users with automated feedback on their metric performance with regards to expert proficiency performance benchmarks. The Virtual Operative Assistant successfully classified skilled and novice participants using 4 metrics with an accuracy, specificity and sensitivity of 92, 82 and 100%, respectively. A 2-step feedback system was developed to provide participants with an immediate visual representation of their standing related to expert proficiency performance benchmarks. The educational system outlined establishes a basis for the potential role of integrating artificial intelligence and virtual reality simulation into surgical educational teaching. The potential of linking expertise classification, objective feedback based on proficiency benchmarks, and instructor input creates a novel educational tool by integrating these three components into a formative educational paradigm.","",""
5,"Xiaochen Zhang, Dayu Yang","Research on Music Assisted Teaching System Based on Artificial Intelligence Technology",2021,"","","","",86,"2022-07-13 09:19:31","","10.1088/1742-6596/1852/2/022032","","",,,,,5,5.00,3,2,1,"With the advent of the information age, computer technology has been greatly developed, especially the development of Artificial Intelligence(AI). And with the passage of time, AI began to involve various fields, music education is no exception. In this paper, after a detailed understanding of some research results of AI on music assisted instruction system, we mainly analyze the students’ video, audio and other related information, and save it in the database. This paper first introduces the evaluation process by using AI technology. In fact, it is necessary to find out the relationship between the influencing factors and evaluation of music assisted teaching system. Neural network(NN) is actually a model proposed by simulating the way people think in the brain. It has no strict requirements for data distribution. In terms of nonlinear data processing method, robustness and dynamics, it is very suitable to be used as a model for evaluating music assisted instruction system. Then each factor is taken as the input parameter of the NN. According to the evaluation index of music teaching, a special modeling system is designed. With the help of technical personnel, we obtained the sample data of music performance and completed the neural training. The experimental results show that the development of AI technology has broken the original situation of traditional teaching, especially the application of music system and intelligent music software based on AI in music teaching.","",""
35,"S. Lo Piano","Ethical principles in machine learning and artificial intelligence: cases from the field and possible ways forward",2020,"","","","",87,"2022-07-13 09:19:31","","10.1057/s41599-020-0501-9","","",,,,,35,17.50,35,1,2,"","",""
6,"M. Loi, M. Spielkamp","Towards Accountability in the Use of Artificial Intelligence for Public Administrations",2021,"","","","",88,"2022-07-13 09:19:31","","10.1145/3461702.3462631","","",,,,,6,6.00,3,2,1,"We argue that the phenomena of distributed responsibility, induced acceptance, and acceptance through ignorance constitute instances of imperfect delegation when tasks are delegated to computationally-driven systems. Imperfect delegation challenges human accountability. We hold that both direct public accountability via public transparency and indirect public accountability via transparency to auditors in public organizations can be both instrumentally ethically valuable and required as a matter of deontology from the principle of democratic self-government. We analyze the regulatory content of 16 guideline documents about the use of AI in the public sector, by mapping their requirements to those of our philosophical account of accountability, and conclude that while some guidelines refer processes that amount to auditing, it seems that the debate would benefit from more clarity about the nature of the entitlement of auditors and the goals of auditing, also in order to develop ethically meaningful standards with respect to which different forms of auditing can be evaluated and compared.","",""
3,"Irene A. Niet, R. Est, F. Veraart","Governing AI in Electricity Systems: Reflections on the EU Artificial Intelligence Bill",2021,"","","","",89,"2022-07-13 09:19:31","","10.3389/frai.2021.690237","","",,,,,3,3.00,1,3,1,"The Proposal for an Artificial Intelligence Act, published by the European Commission in April 2021, marks a major step in the governance of artificial intelligence (AI). This paper examines the significance of this Act for the electricity sector, specifically investigating to what extent the current European Union Bill addresses the societal and governance challenges posed by the use of AI that affects the tasks of system operators. For this we identify various options for the use of AI by system operators, as well as associated risks. AI has the potential to facilitate grid management, flexibility asset management and electricity market activities. Associated risks include lack of transparency, decline of human autonomy, cybersecurity, market dominance, and price manipulation on the electricity market. We determine to what extent the current bill pays attention to these identified risks and how the European Union intends to govern these risks. The proposed AI Act addresses well the issue of transparency and clarifying responsibilities, but pays too little attention to risks related to human autonomy, cybersecurity, market dominance and price manipulation. We make some governance suggestions to address those gaps.","",""
3,"Amy Papadopoulos, J. Salinas, Cindy Crump","Computational modeling approaches to characterize risk and achieve safe, effective, and trusted designs in the development of artificial intelligence and autonomous closed-loop medical systems",2021,"","","","",90,"2022-07-13 09:19:31","","10.1117/12.2586101","","",,,,,3,3.00,1,3,1,"While software using artificial intelligence and machine learning (AI/ML) is pervasive in many areas of society today, the use of these technologies to diagnose and treat medical conditions is limited due to a number of challenges associated with the trustworthiness of the results. This may include the inability to fully explain how an algorithm works inherent to the black-box nature of the system. Additionally, AI/ML may create a potential for bias and artifacts that cannot be validated due to the same limitations. In a medical application, the lack of transparency in how the system operates may lead to a loss of trust by users. Bayesian approaches that use computational modeling to quantify the level of uncertainty in a given result may provide a path towards improved confidence and use. In this paper, evidence from studies in a range of medical applications is presented and discussed, showing how Bayesian approaches can help to foster trust. A retrospective study using a publicly available dataset explored the feasibility of creating predictive models for early intervention in a Type 1 diabetes population. Creating the perfect model was not the goal of the exercise, rather the study aimed to demonstrate how Bayesian methods could be used to identify areas of uncertainty during model development. Feature selection was based on analytical assessment of various patterns found in the data. Models were trained, validated, and tested, generating uncertainty estimates. A two-feature Gaussian Naïve Bayes (GNB) model, using the previous five minutes and ten minutes of blood glucose values, showed similar results for predictive accuracy as a threefeature model that included average change over the preceding 30 minutes. The two-feature model was selected because it allowed for a more easily understood visualization of uncertainty. The 2-feature GNB achieved an AUC = .94. The model showed good sensitivity for exceeding the < 180 mg/dl limit, obtaining threshold prediction = 89.8% and normal range prediction = 90.8%. The sensitivity was lower for the < 70 mg/dl limit, attaining a sensitivity = 77.5%. Posterior probabilities showed differing levels of uncertainty in the prediction of high and low out-of-range conditions. The model demonstrated the feasibility of providing robust parameter estimates. Bayesian machine learning approaches to model uncertainty may improve the transparency, explainability, and applicability of AI/ML in medical treatment, realizing the promise to improve patient safety and outcomes.","",""
32,"D. Bates, A. Auerbach, Peter F. Schulam, A. Wright, S. Saria","Reporting and Implementing Interventions Involving Machine Learning and Artificial Intelligence",2020,"","","","",91,"2022-07-13 09:19:31","","10.7326/M19-0872","","",,,,,32,16.00,6,5,2,"Increasingly, interventions aimed at improving care are likely to use such technologies as machine learning and artificial intelligence. However, health care has been relatively late to adopt them. This article provides clinical examples in which machine learning and artificial intelligence are already in use in health care and appear to deliver benefit. Three key bottlenecks toward increasing the pace of diffusion and adoption are methodological issues in evaluation of artificial intelligence-based interventions, reporting standards to enable assessment of model performance, and issues that need to be addressed for an institution to adopt these interventions. Methodological best practices will include external validation, ideally at a different site; use of proactive learning algorithms to correct for site-specific biases and increase robustness as algorithms are deployed across multiple sites; addressing subgroup performance; and communicating to providers the uncertainty of predictions. Regarding reporting, especially important issues are the extent to which implementing standardized approaches for introducing clinical decision support has been followed, describing the data sources, reporting on data assumptions, and addressing biases. Although most health care organizations in the United States have adopted electronic health records, they may be ill prepared to adopt machine learning and artificial intelligence. Several steps can enable this: preparing data, developing tools to get suggestions to clinicians in useful ways, and getting clinicians engaged in the process. Open challenges and the role of regulation in this area are briefly discussed. Although these techniques have enormous potential to improve care and personalize recommendations for individuals, the hype regarding them is tremendous. Organizations will need to approach this domain carefully with knowledgeable partners to obtain the hoped-for benefits and avoid failures.","",""
1,"R. Montenegro, Elva Corona, D. Badillo-Perez, Angel Mandujano, L. Vazquez, D. Cruz, Miguel P. Xochicale","AIR4Children: Artificial Intelligence and Robotics for Children",2021,"","","","",92,"2022-07-13 09:19:31","","","","",,,,,1,1.00,0,7,1,"We introduce AIR4Children, Artificial Intelligence for Children, as a way to (a) tackle aspects for inclusion, accessibility, transparency, equity, fairness and participation and (b) to create affordable childcentred materials in AI and Robotics (AIR). We present current challenges and opportunities for a child-centred approaches for AIR. Similarly, we touch on open-sourced software and hardware technologies to make a more inclusive, affordable and fair participation of children in areas of AIR. Then, we describe the avenues that AIR4Children can take with the development of open-sourced software and hardware based on our initial pilots and experiences. Similarly, we propose to follow the philosophy of Montessori education to help children to not only develop computational thinking but also to internalise new concepts and learning skills through activities of movement and repetition. Finally, we conclude with the opportunities of our work and mainly we pose the future work of putting in practice what is proposed here to evaluate the potential impact on AIR to children, instructors, parents and their community. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. Woodstock ’18, 8 March 2021, Online © 2018 Association for Computing Machinery. ACM ISBN 978-1-4503-XXXX-X/18/06. . . $15.00 https://doi.org/10.1145/1122445.1122456 CCS CONCEPTS • Human-centered computing → Empirical studies in HCI; Accessibility systems and tools; • Applied computing → Interactive learning environments; • Social and professional topics → Children; • Computingmethodologies→ Cognitive robotics.","",""
1,"Ke Zhang, Peidong Xu, Tianlu Gao, Jun Zhang","A Trustworthy Framework of Artificial Intelligence for Power Grid Dispatching Systems",2021,"","","","",93,"2022-07-13 09:19:31","","10.1109/DTPI52967.2021.9540198","","",,,,,1,1.00,0,4,1,"With the widespread application of artificial intelligence (AI) technologies in power systems, the properties of lack of reliability and transparency for AI technologies have revealed gradually. Here, how to build a trustworthy-AI framework based on the power system is the focus. Due to the multidimensional and heterogeneous information of power grid data, the heterogeneous graph attention network (HGAT) model of power grid dispatching is established, and the corresponding explainer (HGAT-Explainer) for the model of power equipment faults is proposed to provide more favorable support for the trustworthy-AI systems.","",""
0,"Feng Xiaohua, Conrad Marc, E. Elias, Hussein Khalid","Artificial Intelligence and Blockchain for Future Cyber Security Application",2021,"","","","",94,"2022-07-13 09:19:31","","10.1109/DASC-PICom-CBDCom-CyberSciTech52372.2021.00133","","",,,,,0,0.00,0,4,1,"AI (Artificial intelligence) application on Big Data had been developed fast. AI cyber security defense for the facing threats were required. Blockchain technology was invented in 2008 with BTC (Bit coin. This technology could be benefited alongside the custom of Blockchain, AI, Big Data and so on. There were a rapid progress in the advancement of Blockchain. This subject had recently become a discussion topic in the ICT (Information and Communications Technology) world. In this paper, AI security is discussed from the initial stage. Suggestion: In this paper, we discussed the impact of AI security from the initial stage and its impact and benefits to IT engineers, ICT students and CS (Computer Sciences) academic researchers, using a case study of medical records with personal recognizable identification privacy information that needs strict access control security. We considered its need for trustworthy cyber security, anti-fake, anti-alteration and transaction accounting transparency reputation to be applied to the NHS (National Health Service). Lastly, the paper provided some necessarily analysis. Blockchain technology had trustworthy cyber security, anti-fake, anti-alteration and transaction accounting transparency reputation to be considered to be applied to NHS (National Health Service). This short paper provided some analysis necessarily.","",""
0,"Hui Li, Zhenjing Pang, Yanjun Li","Research on the Realistic Dilemma and Optimized Path of Education Governance Modernization from the Perspective of Artificial Intelligence",2021,"","","","",95,"2022-07-13 09:19:31","","10.1109/ICAIE53562.2021.00009","","",,,,,0,0.00,0,3,1,"Supported by Artificial Intelligence centered on heuristic search, machine learning, and expert systems, modern education governance presents the typical characteristics of governance process transparency, subject diversification and system intelligence. However, from the perspective of Artificial Intelligence, education governance modernization is not only stuck in the “structural” absence of multiple governance subjects, but also trapped in the “initiative” shortage of vertical one-way organizational structure and the “data” block of modern education governance paradigm. Thus, the application of Artificial Intelligence in intelligently mining data and information, autonomously adapting to complex educational situations, and systematically optimizing governance decisionmaking behavior is limited. This paper explores the optimized path of education governance from three dimensions of concept cultivation, subject training and technology innovation, rationally discusses the “interactive” application of Artificial Intelligence and education governance modernization, so as to promote the deep integration of artificial intelligence and education governance, and improve education governance modernization as well as talent cultivation in the new era.","",""
0,"M. Aggarwal, Christian Gingras, R. Deber","Artificial Intelligence in Healthcare from a Policy Perspective",2021,"","","","",96,"2022-07-13 09:19:31","","10.1007/978-3-030-67303-1_5","","",,,,,0,0.00,0,3,1,"","",""
0,"Caijin Ling, Ting Zeng, Yang Su","Research on Intelligent Supervision and Application System of Food Traceability Based on Blockchain and Artificial intelligence",2021,"","","","",97,"2022-07-13 09:19:31","","10.1109/ICIBA52610.2021.9688295","","",,,,,0,0.00,0,3,1,"The lack of transparency in the production and circulation of commodities and the lack of corresponding supervision have led to endless problems such as food safety, counterfeit and shoddy products, loss and damage of commodities, and damage to the rights and interests of consumers. The traditional centralized database traceability monitoring system has serious problems of data trust, data fragmentation, difficulty in accountability, and low enthusiasm of merchants. In order to solve the traditional system problems, it is proposed to build an intelligent supervision system model for food traceability based on blockchain and artificial intelligence. Blockchain technology can effectively make up and improve the shortcomings of the existing commodity traceability technology, and achieve full process control and real-time storage. Forensic forensics, increase transparency, prevent counterfeiting, and increase consumer trust; AI uses industry-sharing data to perform big data analysis to guide companies in business decisions; at the same time, in order to increase user stickiness and increase the ecological environment of the platform, the article proposes to increase Blockchain and artificial intelligence and application ecology form a more practical and complete integrated system model of traceability, supervision and application. Finally, using FISCO BCOS as a blockchain platform development platform, the validity of the model is verified, and it can provide a certain reference for food traceability companies, software R&D companies, and government regulatory agencies.","",""
30,"S. L. Piano","Ethical principles in machine learning and artificial intelligence: cases from the field and possible ways forward",2020,"","","","",98,"2022-07-13 09:19:31","","10.1057/S41599-020-0501-9","","",,,,,30,15.00,30,1,2,"","",""
0,"Jie Wang, Xiangyuan Zheng, Qingdong He","Artificial Intelligence Applied to Extreme Value Prediction of Non-Gaussian Processes with Bandwidth Effect and Non-monotonicity",2021,"","","","",99,"2022-07-13 09:19:31","","10.1109/ICAICA52286.2021.9498204","","",,,,,0,0.00,0,3,1,"Extreme value prediction of a short-term non-Gaussian random process like ocean waves has been a tough issue for decades. In the 1990’s Winterstein proposed a cubic Hermite transformation using skewness and kurtosis, which has been widely applied in many areas for its accuracy and robustness. However, this approach is valid for monotonic transformation and narrow-banded processes. When the bandwidth of a random process is wide, no reasonable methods are available for acquiring the extreme value. This paper therefore applies the artificial neural network and genetic algorithm to do the extreme value prediction, without seeking rigorous mathematical derivations. Not only skewness and kurtosis are used, the spectral moments up to 4th-order reflecting bandwidth effects are also adopted. The results of many random case studies show that the artificial intelligence method is more accurate than the Hermite method in most of situations, especially for non-monotonic transformations. Besides, the artificial intelligence method has a wider application range.","",""
30,"Xinqin Liao, Wei‐ming Song, X. Zhang, Chaoqun Yan, Tianliang Li, Hongliang Ren, Cunzhi Liu, Yongtian Wang, Yuanjin Zheng","A bioinspired analogous nerve towards artificial intelligence",2020,"","","","",100,"2022-07-13 09:19:31","","10.1038/s41467-019-14214-x","","",,,,,30,15.00,3,9,2,"","",""
30,"Xinqin Liao, Wei‐ming Song, X. Zhang, Chaoqun Yan, Tianliang Li, Hongliang Ren, Cunzhi Liu, Yongtian Wang, Yuanjin Zheng","A bioinspired analogous nerve towards artificial intelligence",2020,"","","","",101,"2022-07-13 09:19:31","","10.1038/s41467-019-14214-x","","",,,,,30,15.00,3,9,2,"","",""
24,"Paul Henman","Improving public services using artificial intelligence: possibilities, pitfalls, governance",2020,"","","","",102,"2022-07-13 09:19:31","","10.1080/23276665.2020.1816188","","",,,,,24,12.00,24,1,2,"Artificial intelligence arising from the use of machine learning is rapidly being developed and deployed by governments to enhance operations, public services, and compliance and security activities. This article reviews how artificial intelligence is being used in public sector for automated decision making, for chatbots to provide information and advice, and for public safety and security. It then outlines four public administration challenges to deploying artificial intelligence in public administration: accuracy, bias and discrimination; legality, due process and administrative justice; responsibility, accountability, transparency and explainability; and power, compliance and control. The article outlines technological and governance innovations that are being developed to address these challenges.","",""
25,"C. Goldstein, R. Berry, D. Kent, D. Kristo, A. Seixas, S. Redline, M. Westover","Artificial intelligence in sleep medicine: Background and implications for clinicians.",2020,"","","","",103,"2022-07-13 09:19:31","","10.5664/jcsm.8388","","",,,,,25,12.50,4,7,2,"None Polysomnography (PSG) remains the cornerstone of objective testing in sleep medicine and results in massive amounts of electrophysiological data, which is well-suited for analysis with artificial intelligence (AI)-based tools. Combined with other sources of health data, AI is expected to provide new insights to inform the clinical care of sleep disorders and advance our understanding of the integral role sleep plays in human health. Additionally, AI has the potential to streamline day-to-day operations and therefore optimize direct patient care by the sleep disorders team. However, clinicians, scientists, and other stakeholders must develop best practices to integrate this rapidly evolving technology into our daily work while maintaining the highest degree of quality and transparency in health care and research. Ultimately, when harnessed appropriately in conjunction with human expertise, AI will improve the practice of sleep medicine and further sleep science for the health and well-being of our patients.","",""
5,"Vanessa Laurim, Selin Arpaci, Barbara Prommegger, H. Krcmar","Computer, Whom Should I Hire? - Acceptance Criteria for Artificial Intelligence in the Recruitment Process",2021,"","","","",104,"2022-07-13 09:19:31","","10.24251/HICSS.2021.668","","",,,,,5,5.00,1,4,1,"In the war for talents, the need for appropriate tools to fill open positions with the right talents is becoming increasingly important for employers. AI-based technologies simplify recruiters’ daily work and increase the efficiency of the recruitment process by replacing time-consuming approaches. However, little is known about the reactions of stakeholders to AI-based recruiting. Thus, this paper aims to identify personal and contextual factors that influence the acceptance of AI-based technologies in the recruitment process. Based on the interviews with recruiters, managers, and applicants involved in the recruitment process, we present that transparency, complementary features of the AI tools, and a sense of control play key roles in the acceptance of AI-based technology when used for recruiting. The findings contribute to research on the adoption of AI in the recruitment process and provide recommendations on the use of AI technologies when hiring talents.","",""
1,"K. Vogel, Gwendolynne Reid, Christopher Kampe, Paul Jones","The impact of AI on intelligence analysis: tackling issues of collaboration, algorithmic transparency, accountability, and management",2021,"","","","",105,"2022-07-13 09:19:31","","10.1080/02684527.2021.1946952","","",,,,,1,1.00,0,4,1,"In January 2019, the U.S. Office of the Director of National Intelligence (ODNI) released a new strategy on the use of artificial intelligence (AI) technologies in U.S. intelligence. The report called for incorporating AI and automation technologies into intelligence work ‘to amplify the effectiveness of our workforce . . . advance mission capability and enhance the IC’s [Intelligence Community’s] ability to provide data interpretation to decision makers’. The ODNI noted it was evaluating and monitoring how these technologies might also have ‘vulnerabilities in development and adoption’. The report stated it was critical to address issues of ‘AI assurance, transparency, and reliability . . . to . . . understand how AI algorithms may fail’, and noted the importance of developing AI systems that ‘can demonstrate the underlying rationale behind decisions and responses to both users and overseers’. Finally, it emphasized the importance of monitoring ‘implementation and user feedback’ in a future AI-enabled workforce. This imagined future is not only to come; it is being realized now. Within the past few years, probably one of the most visibly controversial IC projects involving AI and intelligence analysis was Project Maven, a 2017 Department of Defense–driven intelligence project that used advances in big data, machine learning, and deep learning to extract objects of interest from drone-derived imagery, saving intelligence analysts hours of tedious imagery processing time. Project Maven partnered with Google to use some of the tech giant’s AI technology, but the project was ultimately cancelled in 2018 because of Google employee protests against the use of company algorithms for military targeting. Beyond Project Maven, a number of less controversial R&D programs ARE underway that aim to augment intelligence analysts’ capabilities. For example, the Defense Advanced Research Project Agency (DARPA)’s Explainable (XAI) Program is creating a set of new machine-learning techniques to ‘enable human users to understand, appropriately trust, and effectively manage the emerging generation of artificially intelligent partners’. The National Security Agency’s Laboratory for Analytic Sciences is developing big-data and AI-enabled technological platforms to bring more AI-enabled systems to analysts’ desktops. To date, these are individual proof-ofconcept technologies that may be applied and integrated into a variety of potential intelligence tools, such as: Journaling, a productivity device for intelligence analysts that enables them to understand their own individual work flows; OpenKE, which automates the collection and analysis of open-source information; BEAST, a platform of natural language processing and other extraction services that can scrape and process data from various sources for anticipatory intelligence analysis; and CyberMonkey, which allows a large number of analytic tools to be executed proactively in-browser instead of having to utilize tools sequentially or separately. All of these technologies aim ‘to assist the analyst without the analyst explicitly telling the machine everything it needs to do’. The Intelligence Advanced Research Project Activity (IARPA)’s Multimodal","",""
30,"L. Longo, R. Goebel, F. Lécué, Peter Kieseberg, Andreas Holzinger","Explainable Artificial Intelligence: Concepts, Applications, Research Challenges and Visions",2020,"","","","",106,"2022-07-13 09:19:31","","10.1007/978-3-030-57321-8_1","","",,,,,30,15.00,6,5,2,"","",""
22,"G. Francolini, I. Desideri, G. Stocchi, V. Salvestrini, L. Ciccone, P. Garlatti, M. Loi, L. Livi","Artificial Intelligence in radiotherapy: state of the art and future directions",2020,"","","","",107,"2022-07-13 09:19:31","","10.1007/s12032-020-01374-w","","",,,,,22,11.00,3,8,2,"","",""
1549,"Amina Adadi, M. Berrada","Peeking Inside the Black-Box: A Survey on Explainable Artificial Intelligence (XAI)",2018,"","","","",108,"2022-07-13 09:19:31","","10.1109/ACCESS.2018.2870052","","",,,,,1549,387.25,775,2,4,"At the dawn of the fourth industrial revolution, we are witnessing a fast and widespread adoption of artificial intelligence (AI) in our daily life, which contributes to accelerating the shift towards a more algorithmic society. However, even with such unprecedented advancements, a key impediment to the use of AI-based systems is that they often lack transparency. Indeed, the black-box nature of these systems allows powerful predictions, but it cannot be directly explained. This issue has triggered a new debate on explainable AI (XAI). A research field holds substantial promise for improving trust and transparency of AI-based systems. It is recognized as the sine qua non for AI to continue making steady progress without disruption. This survey provides an entry point for interested researchers and practitioners to learn key aspects of the young and rapidly growing body of research related to XAI. Through the lens of the literature, we review the existing approaches regarding the topic, discuss trends surrounding its sphere, and present major research trajectories.","",""
21,"Chuan Zhang, Yeong-Luh Ueng, Christoph Studer, A. Burg","Artificial Intelligence for 5G and Beyond 5G: Implementations, Algorithms, and Optimizations",2020,"","","","",109,"2022-07-13 09:19:31","","10.1109/JETCAS.2020.3000103","","",,,,,21,10.50,5,4,2,"The communication industry is rapidly advancing towards 5G and beyond 5G (B5G) wireless technologies in order to fulfill the ever-growing needs for higher data rates and improved quality-of-service (QoS). Emerging applications require wireless connectivity with tremendously increased data rates, substantially reduced latency, and growing support for a large number of devices. These requirements pose new challenges that can no longer be efficiently addressed by conventional approaches. Artificial intelligence (AI) is considered as one of the most promising solutions to improve the performance and robustness of 5G and B5G systems, fueled by the massive amount of data generated in 5G and B5G networks and the availability of powerful data processing fabrics. As a consequence, a plethora of research on AI-based communication technologies has emerged recently, promising higher data rates and improved QoS with affordable implementation overhead. In this overview paper, we summarize the state-of-the-art of AI-based 5G and B5G techniques on the algorithm, implementation, and optimization levels. We shed light on the advantages and limitations of AI-based solutions, and we provide a summary of emerging techniques and open research problems.","",""
19,"M. Kuzlu, Umit Cali, Vinayak Sharma, Özgür Güler","Gaining Insight Into Solar Photovoltaic Power Generation Forecasting Utilizing Explainable Artificial Intelligence Tools",2020,"","","","",110,"2022-07-13 09:19:31","","10.1109/ACCESS.2020.3031477","","",,,,,19,9.50,5,4,2,"Over the last two decades, Artificial Intelligence (AI) approaches have been applied to various applications of the smart grid, such as demand response, predictive maintenance, and load forecasting. However, AI is still considered to be a “black-box” due to its lack of explainability and transparency, especially for something like solar photovoltaic (PV) forecasts that involves many parameters. Explainable Artificial Intelligence (XAI) has become an emerging research field in the smart grid domain since it addresses this gap and helps understand why the AI system made a forecast decision. This article presents several use cases of solar PV energy forecasting using XAI tools, such as LIME, SHAP, and ELI5, which can contribute to adopting XAI tools for smart grid applications. Understanding the inner workings of a prediction model based on AI can give insights into the application field. Such insight can provide improvements to the solar PV forecasting models and point out relevant parameters.","",""
16,"B. Koçak, Ece Ates Kus, O. Kilickesmez","How to read and review papers on machine learning and artificial intelligence in radiology: a survival guide to key methodological concepts",2020,"","","","",111,"2022-07-13 09:19:31","","10.1007/s00330-020-07324-4","","",,,,,16,8.00,5,3,2,"","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",112,"2022-07-13 09:19:31","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
14,"Gaolei Li, K. Ota, M. Dong, Jun Wu, Jianhua Li","DeSVig: Decentralized Swift Vigilance Against Adversarial Attacks in Industrial Artificial Intelligence Systems",2020,"","","","",113,"2022-07-13 09:19:31","","10.1109/TII.2019.2951766","","",,,,,14,7.00,3,5,2,"Individually reinforcing the robustness of a single deep learning model only gives limited security guarantees especially when facing adversarial examples. In this article, we propose DeSVig, a decentralized swift vigilance framework to identify adversarial attacks in an industrial artificial intelligence systems (IAISs), which enables IAISs to correct the mistake in a few seconds. The DeSVig is highly decentralized, which improves the effectiveness of recognizing abnormal inputs. We try to overcome the challenges on ultralow latency caused by dynamics in industries using peculiarly designated mobile edge computing and generative adversarial networks. The most important advantage of our work is that it can significantly reduce the failure risks of being deceived by adversarial examples, which is critical for safety-prioritized and delay-sensitive environments. In our experiments, adversarial examples of industrial electronic components are generated by several classical attacking models. Experimental results demonstrate that the DeSVig is more robust, efficient, and scalable than some state-of-art defenses.","",""
13,"Yuanbin Wang, P. Zheng, Tao Peng, Huayong Yang, J. Zou","Smart additive manufacturing: Current artificial intelligence-enabled methods and future perspectives",2020,"","","","",114,"2022-07-13 09:19:31","","10.1007/s11431-020-1581-2","","",,,,,13,6.50,3,5,2,"","",""
12,"A. Lal, Yuliya Pinevich, O. Gajic, V. Herasevich, B. Pickering","Artificial intelligence and computer simulation models in critical illness",2020,"","","","",115,"2022-07-13 09:19:31","","10.5492/wjccm.v9.i2.13","","",,,,,12,6.00,2,5,2,"Widespread implementation of electronic health records has led to the increased use of artificial intelligence (AI) and computer modeling in clinical medicine. The early recognition and treatment of critical illness are central to good outcomes but are made difficult by, among other things, the complexity of the environment and the often non-specific nature of the clinical presentation. Increasingly, AI applications are being proposed as decision supports for busy or distracted clinicians, to address this challenge. Data driven “associative” AI models are built from retrospective data registries with missing data and imprecise timing. Associative AI models lack transparency, often ignore causal mechanisms, and, while potentially useful in improved prognostication, have thus far had limited clinical applicability. To be clinically useful, AI tools need to provide bedside clinicians with actionable knowledge. Explicitly addressing causal mechanisms not only increases validity and replicability of the model, but also adds transparency and helps gain trust from the bedside clinicians for real world use of AI models in teaching and patient care.","",""
10,"J. E. Taylor, Graham W. Taylor","Artificial cognition: How experimental psychology can help generate explainable artificial intelligence.",2020,"","","","",116,"2022-07-13 09:19:31","","10.3758/s13423-020-01825-5","","",,,,,10,5.00,5,2,2,"","",""
10,"Virginia Dignum","Responsibility and Artificial Intelligence",2020,"","","","",117,"2022-07-13 09:19:31","","10.1093/oxfordhb/9780190067397.013.12","","",,,,,10,5.00,10,1,2,"This chapter explores the concept of responsibility in artificial intelligence (AI). Being fundamentally tools, AI systems are fully under the control and responsibility of their owners or users. However, their potential autonomy and capability to learn require that design considers accountability, responsibility, and transparency principles in an explicit and systematic manner. The main concern of Responsible AI is thus the identification of the relative responsibility of all actors involved in the design, development, deployment, and use of AI systems. Firstly, society must be prepared to take responsibility for AI impact. Secondly, Responsible AI implies the need for mechanisms that enable AI systems to act according to ethics and human values. Lastly, Responsible AI is about participation. It is necessary to understand how different people work with and live with AI technologies across cultures in order to develop frameworks for responsible AI.","",""
9,"M. Gorris, S. Hoogenboom, M. Wallace, J. V. van Hooft","Artificial intelligence for the management of pancreatic diseases",2020,"","","","",118,"2022-07-13 09:19:31","","10.1111/den.13875","","",,,,,9,4.50,2,4,2,"Novel artificial intelligence techniques are emerging in all fields of healthcare, including gastroenterology. The aim of this review is to give an overview of artificial intelligence applications in the management of pancreatic diseases. We performed a systematic literature search in PubMed and Medline up to May 2020 to identify relevant articles. Our results showed that the development of machine‐learning based applications is rapidly evolving in the management of pancreatic diseases, guiding precision medicine in clinical, endoscopic and radiologic settings. Before implementation into clinical practice, further research should focus on the external validation of novel techniques, clarifying the accuracy and robustness of these models.","",""
7,"R. Kusters, D. Misevic, H. Berry, Antoine Cully, Y. Le Cunff, Loic Dandoy, N. Díaz-Rodríguez, Marion Ficher, Jonathan Grizou, Alice Othmani, Themis Palpanas, M. Komorowski, P. Loiseau, Clément Moulin Frier, Santino Nanini, D. Quercia, M. Sebag, Françoise Soulié Fogelman, Sofiane Taleb, Liubov Tupikina, Vaibhav Sahu, J. Vie, Fatima Wehbi","Interdisciplinary Research in Artificial Intelligence: Challenges and Opportunities",2020,"","","","",119,"2022-07-13 09:19:31","","10.3389/fdata.2020.577974","","",,,,,7,3.50,1,23,2,"The use of artificial intelligence (AI) in a variety of research fields is speeding up multiple digital revolutions, from shifting paradigms in healthcare, precision medicine and wearable sensing, to public services and education offered to the masses around the world, to future cities made optimally efficient by autonomous driving. When a revolution happens, the consequences are not obvious straight away, and to date, there is no uniformly adapted framework to guide AI research to ensure a sustainable societal transition. To answer this need, here we analyze three key challenges to interdisciplinary AI research, and deliver three broad conclusions: 1) future development of AI should not only impact other scientific domains but should also take inspiration and benefit from other fields of science, 2) AI research must be accompanied by decision explainability, dataset bias transparency as well as development of evaluation methodologies and creation of regulatory agencies to ensure responsibility, and 3) AI education should receive more attention, efforts and innovation from the educational and scientific communities. Our analysis is of interest not only to AI practitioners but also to other researchers and the general public as it offers ways to guide the emerging collaborations and interactions toward the most fruitful outcomes.","",""
7,"Nariman Ammar, Arash Shaban-Nejad","Explainable Artificial Intelligence Recommendation System by Leveraging the Semantics of Adverse Childhood Experiences: Proof-of-Concept Prototype Development",2020,"","","","",120,"2022-07-13 09:19:31","","10.2196/18752","","",,,,,7,3.50,4,2,2,"Background The study of adverse childhood experiences and their consequences has emerged over the past 20 years. Although the conclusions from these studies are available, the same is not true of the data. Accordingly, it is a complex problem to build a training set and develop machine-learning models from these studies. Classic machine learning and artificial intelligence techniques cannot provide a full scientific understanding of the inner workings of the underlying models. This raises credibility issues due to the lack of transparency and generalizability. Explainable artificial intelligence is an emerging approach for promoting credibility, accountability, and trust in mission-critical areas such as medicine by combining machine-learning approaches with explanatory techniques that explicitly show what the decision criteria are and why (or how) they have been established. Hence, thinking about how machine learning could benefit from knowledge graphs that combine “common sense” knowledge as well as semantic reasoning and causality models is a potential solution to this problem. Objective In this study, we aimed to leverage explainable artificial intelligence, and propose a proof-of-concept prototype for a knowledge-driven evidence-based recommendation system to improve mental health surveillance. Methods We used concepts from an ontology that we have developed to build and train a question-answering agent using the Google DialogFlow engine. In addition to the question-answering agent, the initial prototype includes knowledge graph generation and recommendation components that leverage third-party graph technology. Results To showcase the framework functionalities, we here present a prototype design and demonstrate the main features through four use case scenarios motivated by an initiative currently implemented at a children’s hospital in Memphis, Tennessee. Ongoing development of the prototype requires implementing an optimization algorithm of the recommendations, incorporating a privacy layer through a personal health library, and conducting a clinical trial to assess both usability and usefulness of the implementation. Conclusions This semantic-driven explainable artificial intelligence prototype can enhance health care practitioners’ ability to provide explanations for the decisions they make.","",""
8,"Baloko Makala, Tonci Bakovic","Artificial Intelligence in the Power Sector",2020,"","","","",121,"2022-07-13 09:19:31","","10.1596/34303","","",,,,,8,4.00,4,2,2,"The energy sector worldwide faces growing challenges related to rising demand, efficiency, changing supply and demand patterns, and a lack of analytics needed for optimal management. These challenges are more acute in emerging market nations. Efficiency issues are particularly problematic, as the prevalence of informal connections to the power grid means a large amount of power is neither measured nor billed, resulting in losses as well as greater CO2 emissions, as consumers have little incentive to rationally use energy they don’t pay for. The power sector in developed nations has already begun to use artificial intelligence and related technologies that allow for communication between smart grids, smart meters, and Internet of Things devices. These technologies can help improve power management, efficiency, and transparency, and increase the use of renewable energy sources.","",""
585,"J. He, Sally L. Baxter, Jie Xu, Jiming Xu, Xingtao Zhou, Kang Zhang","The practical implementation of artificial intelligence technologies in medicine",2019,"","","","",122,"2022-07-13 09:19:31","","10.1038/s41591-018-0307-0","","",,,,,585,195.00,98,6,3,"","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",123,"2022-07-13 09:19:31","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
423,"Andreas Holzinger, G. Langs, H. Denk, K. Zatloukal, Heimo Müller","Causability and explainability of artificial intelligence in medicine",2019,"","","","",124,"2022-07-13 09:19:31","","10.1002/widm.1312","","",,,,,423,141.00,85,5,3,"Explainable artificial intelligence (AI) is attracting much interest in medicine. Technically, the problem of explainability is as old as AI itself and classic AI represented comprehensible retraceable approaches. However, their weakness was in dealing with uncertainties of the real world. Through the introduction of probabilistic learning, applications became increasingly successful, but increasingly opaque. Explainable AI deals with the implementation of transparency and traceability of statistical black‐box machine learning methods, particularly deep learning (DL). We argue that there is a need to go beyond explainable AI. To reach a level of explainable medicine we need causability. In the same way that usability encompasses measurements for the quality of use, causability encompasses measurements for the quality of explanations. In this article, we provide some necessary definitions to discriminate between explainability and causability as well as a use‐case of DL interpretation and of human explanation in histopathology. The main contribution of this article is the notion of causability, which is differentiated from explainability in that causability is a property of a person, while explainability is a property of a system","",""
5,"Christian Meske, Enrico Bunde","Using Explainable Artificial Intelligence to Increase Trust in Computer Vision",2020,"","","","",125,"2022-07-13 09:19:31","","","","",,,,,5,2.50,3,2,2,"Computer Vision, and hence Artificial Intelligence-based extraction of information from images, has increasingly received attention over the last years, for instance in medical diagnostics. While the algorithms' complexity is a reason for their increased performance, it also leads to the ""black box"" problem, consequently decreasing trust towards AI. In this regard, ""Explainable Artificial Intelligence"" (XAI) allows to open that black box and to improve the degree of AI transparency. In this paper, we first discuss the theoretical impact of explainability on trust towards AI, followed by showcasing how the usage of XAI in a health-related setting can look like. More specifically, we show how XAI can be applied to understand why Computer Vision, based on deep learning, did or did not detect a disease (malaria) on image data (thin blood smear slide images). Furthermore, we investigate, how XAI can be used to compare the detection strategy of two different deep learning models often used for Computer Vision: Convolutional Neural Network and Multi-Layer Perceptron. Our empirical results show that i) the AI sometimes used questionable or irrelevant data features of an image to detect malaria (even if correctly predicted), and ii) that there may be significant discrepancies in how different deep learning models explain the same prediction. Our theoretical discussion highlights that XAI can support trust in Computer Vision systems, and AI systems in general, especially through an increased understandability and predictability.","",""
2,"Weiheng Li, Xiangmin Fan, He Zhu, Jingzheng Wu, Dongxing Teng","Research on the Influencing Factors of User Trust Based on Artificial Intelligence Self Diagnosis System",2020,"","","","",126,"2022-07-13 09:19:31","","10.1145/3393527.3393561","","",,,,,2,1.00,0,5,2,"Artificial intelligence technology has gradually become an important auxiliary means for people to obtain medical consultation. However, many researches reflect users' doubts about the feedback results of AI system. Therefore, this paper focuses on the design of the user-friendly intelligent self diagnosis system, using interdisciplinary research methods. Firstly, based on the psychological questionnaire and interview, this paper studies the public's cognition of the intelligent self diagnosis system, and discusses how to effectively improve the user's recognition and trust of the intelligent self diagnosis system. It is found that the credibility of the intelligent self diagnosis system can be improved by further improving the transparency of the system. For example, users can be provided with four different types of information, including system reasoning, system reliability, information source and personalized information. On the basis of interviews, 48 volunteers were invited to carry out a comparative experiment in groups to evaluate the impact of medical interpretation of intelligent self diagnosis system on patients' perception and trust under different transparency and accuracy models. We found that the improvement of system reliability information (i.e. accuracy model and confidence score) composed of accuracy model and confidence score strengthens patients' cognition of AI system and improves the user trust of the system; while the detailed display of system reasoning and logic can improve the comprehensibility of medical advice output of the system, but does not substantially improve the user trust. At the end of this paper, some suggestions are put forward to design an interpretable and reliable intelligent self diagnosis system.","",""
2,"Carlos E. Jimenez-Gomez, Jesús Cano Carrillo, F. Falcone","Artificial Intelligence in Government",2020,"","","","",127,"2022-07-13 09:19:31","","10.1109/MC.2020.3010043","","",,,,,2,1.00,1,3,2,"The articles in this special section focus on government applications that use artificial intelligence (AI). The repercussions of artificial intelligence (AI) in government are broad and significant. The characteristics of these technologies will have an impact on almost everything in public organizations, from governance or the multidimensional perspective of interoperability, to the organizational or social implications linked to concepts like public value, transparency, or accountability. This special issue seeks to shed light on foundations and key elements to be taken into account for AI adoption by public organizations. Governments are the primary enablers of technology and market stimulators and regulators of general activities in our society. Governments have always sought the common good and, therefore, the advancement of public and collective interests. This is key to understanding, as a first step, why the principles of public-sector organizations do not always match those of the private sector. Public and private perspectives are very different, whether they be management, strategy, or policy.","",""
2,"Richard, M. Surya, Avenia Clarissa Wibowo","Converging Artificial Intelligence and Blockchain Technology using Oracle Contract in Ethereum Blockchain Platform",2020,"","","","",128,"2022-07-13 09:19:31","","10.1109/ICIC50835.2020.9288611","","",,,,,2,1.00,1,3,2,"Artificial Intelligence (AI) and blockchain are two emerging concepts that explored exponentially in recent years. In software development, artificial Intelligence offers a sophisticated enhancement on the programming logic, while blockchain technology provides transparency and relief the obligation to trust in a central party. Both are two different technologies but considered a disruptive technology in recent years. Artificial Intelligence faces several issues related to the validity of data, algorithms, and policies, while blockchain is called so “friendly” with transparency, trust, and immutability. Both technologies are still in the early stages of their development age. Always, the opportunity for both technologies to converge in some ways would be an exciting area to be explored. This research elaborates on the technical possibilities of application development on the blockchain platform that communicate with AI to do prediction task. Oracle contract allows the data inside Ethereum blockchain to interact with the external data source.","",""
2,"M. Bashayreh, F. Sibai, Amer Tabbara","Artificial intelligence and legal liability: towards an international approach of proportional liability based on risk sharing",2021,"","","","",129,"2022-07-13 09:19:31","","10.1080/13600834.2020.1856025","","",,,,,2,2.00,1,3,1,"ABSTRACT This paper critically examines the allocation of liability when autonomous artificial intelligence (AI) systems cause accidents. Problems of applying existing principles of legal liability in AI environment are addressed. This paper argues that the sharing of risk as a basis for proportionate liability should be a basis for a new liability regime to govern future autonomous machines. It is argued that this approach favors the reality of parties’ consent to taking the risk of unpredictable AI behavior over the technicality of existing principles of legal liability. The suggested approach also encourages transparency and responsible decisions of developers and owners of AI systems. A flowchart to clarify possible outcomes of applying the suggested approach is provided. The paper also discusses the need for harmonization of national laws and international cooperation regarding AI incidents crossing national borders to ensure predictability of legal rules governing the liability ensuing from AI applications.","",""
54,"G. Collins, P. Dhiman, Constanza L. Andaur Navarro, Jie Ma, L. Hooft, J. Reitsma, P. Logullo, Andrew Beam, Lily Peng, B. van Calster, M. van Smeden, R. Riley, K. Moons","Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence",2021,"","","","",130,"2022-07-13 09:19:31","","10.1136/bmjopen-2020-048008","","",,,,,54,54.00,5,13,1,"Introduction The Transparent Reporting of a multivariable prediction model of Individual Prognosis Or Diagnosis (TRIPOD) statement and the Prediction model Risk Of Bias ASsessment Tool (PROBAST) were both published to improve the reporting and critical appraisal of prediction model studies for diagnosis and prognosis. This paper describes the processes and methods that will be used to develop an extension to the TRIPOD statement (TRIPOD-artificial intelligence, AI) and the PROBAST (PROBAST-AI) tool for prediction model studies that applied machine learning techniques. Methods and analysis TRIPOD-AI and PROBAST-AI will be developed following published guidance from the EQUATOR Network, and will comprise five stages. Stage 1 will comprise two systematic reviews (across all medical fields and specifically in oncology) to examine the quality of reporting in published machine-learning-based prediction model studies. In stage 2, we will consult a diverse group of key stakeholders using a Delphi process to identify items to be considered for inclusion in TRIPOD-AI and PROBAST-AI. Stage 3 will be virtual consensus meetings to consolidate and prioritise key items to be included in TRIPOD-AI and PROBAST-AI. Stage 4 will involve developing the TRIPOD-AI checklist and the PROBAST-AI tool, and writing the accompanying explanation and elaboration papers. In the final stage, stage 5, we will disseminate TRIPOD-AI and PROBAST-AI via journals, conferences, blogs, websites (including TRIPOD, PROBAST and EQUATOR Network) and social media. TRIPOD-AI will provide researchers working on prediction model studies based on machine learning with a reporting guideline that can help them report key details that readers need to evaluate the study quality and interpret its findings, potentially reducing research waste. We anticipate PROBAST-AI will help researchers, clinicians, systematic reviewers and policymakers critically appraise the design, conduct and analysis of machine learning based prediction model studies, with a robust standardised tool for bias evaluation. Ethics and dissemination Ethical approval has been granted by the Central University Research Ethics Committee, University of Oxford on 10-December-2020 (R73034/RE001). Findings from this study will be disseminated through peer-review publications. PROSPERO registration number CRD42019140361 and CRD42019161764.","",""
0,"A. Paic","Policies for Artificial Intelligence in Science and Innovation",2020,"","","","",131,"2022-07-13 09:19:31","","10.22323/1.372.0045","","",,,,,0,0.00,0,1,2,"This contribution synthesizes the discussions of the special session on policies for Artificial Intelligence in Science and Innovation, organized by the OECD’s Directorate for Science, Technology and Innovation. The session was opened by Dr Judith Arrieta, Minister of the Foreign Service at the Chief of Staff’s Office of the Secretary of Foreign Affairs of Mexico, and the two panels included speakers from governments, industry and civil society from European countries, USA,Canada China and Australia. Participants discussed the disruptive nature of AI and the formidable challenges it poses. Most of the discussion focused under the umbrella title of ethics, but they span very different issues of human-centered values, fairness, transparency, explainability, and many more. Other challenges include employment, education, SME policy, enabling environment, access to data and computing technology. Responses by governments were also discussed with a particular focus on national strategies, whose main pillars are oriented toward knowledge creation through AI research, knowledge diffusion through linkages to the private sector, development of human capital which will underpin the development of the sector, and a strong values, ethical and regulatory framework to create the conditions for the development of trustworthy AI.  In a world of finite resources, discussants concluded that one cannot apply very stringent requirements to all AI decisions, and there is clearly a need to require more transparency, explainability and robustness from systems which have the greatest impact on human lives. Therefore an approach based on algorithmic impact assessment seems reasonable. Such an approach needs to be further developed and standardized.","",""
286,"Anna Jobin, M. Ienca, E. Vayena","Artificial Intelligence: the global landscape of ethics guidelines",2019,"","","","",132,"2022-07-13 09:19:31","","10.1038/s42256-019-0088-2","","",,,,,286,95.33,95,3,3,"","",""
11,"Aditya Kuppa, N. Le-Khac","Black Box Attacks on Explainable Artificial Intelligence(XAI) methods in Cyber Security",2020,"","","","",133,"2022-07-13 09:19:31","","10.1109/IJCNN48605.2020.9206780","","",,,,,11,5.50,6,2,2,"Cybersecurity community is slowly leveraging Machine Learning (ML) to combat ever evolving threats. One of the biggest drivers for successful adoption of these models is how well domain experts and users are able to understand and trust their functionality. As these black-box models are being employed to make important predictions, the demand for transparency and explainability is increasing from the stakeholders.Explanations supporting the output of ML models are crucial in cyber security, where experts require far more information from the model than a simple binary output for their analysis. Recent approaches in the literature have focused on three different areas: (a) creating and improving explainability methods which help users better understand the internal workings of ML models and their outputs; (b) attacks on interpreters in white box setting; (c) defining the exact properties and metrics of the explanations generated by models. However, they have not covered, the security properties and threat models relevant to cybersecurity domain, and attacks on explainable models in black box settings.In this paper, we bridge this gap by proposing a taxonomy for Explainable Artificial Intelligence (XAI) methods, covering various security properties and threat models relevant to cyber security domain. We design a novel black box attack for analyzing the consistency, correctness and confidence security properties of gradient based XAI methods. We validate our proposed system on 3 security-relevant data-sets and models, and demonstrate that the method achieves attacker’s goal of misleading both the classifier and explanation report and, only explainability method without affecting the classifier output. Our evaluation of the proposed approach shows promising results and can help in designing secure and robust XAI methods.","",""
21,"Adrien Bécue, Isabel Praça, J. Gama","Artificial intelligence, cyber-threats and Industry 4.0: challenges and opportunities",2021,"","","","",134,"2022-07-13 09:19:31","","10.1007/S10462-020-09942-2","","",,,,,21,21.00,7,3,1,"","",""
26,"P. Radanliev, D. D. Roure, M. V. Kleek, Omar Santos, U. Ani","Artificial intelligence in cyber physical systems",2019,"","","","",135,"2022-07-13 09:19:31","","10.1007/s00146-020-01049-0","","",,,,,26,8.67,5,5,3,"","",""
15,"S. Ebrahimian, Fatemeh Homayounieh, M. Rockenbach, Preetham Putha, T. Raj, I. Dayan, B. Bizzo, Varun Buch, Dufan Wu, Kyungsang Kim, Quanzheng Li, S. Digumarthy, M. Kalra","Artificial intelligence matches subjective severity assessment of pneumonia for prediction of patient outcome and need for mechanical ventilation: a cohort study",2021,"","","","",136,"2022-07-13 09:19:31","","10.1038/s41598-020-79470-0","","",,,,,15,15.00,2,13,1,"","",""
16,"A. Amritphale, Ranojoy Chatterjee, Suvo Chatterjee, N. Amritphale, Ali Rahnavard, G. Awan, B. Omar, G. Fonarow","Predictors of 30-Day Unplanned Readmission After Carotid Artery Stenting Using Artificial Intelligence",2021,"","","","",137,"2022-07-13 09:19:31","","10.1007/s12325-021-01709-7","","",,,,,16,16.00,2,8,1,"","",""
196,"W. Samek, K. Müller","Towards Explainable Artificial Intelligence",2019,"","","","",138,"2022-07-13 09:19:31","","10.1007/978-3-030-28954-6_1","","",,,,,196,65.33,98,2,3,"","",""
109,"Shilin Qiu, Qihe Liu, Shijie Zhou, Chunjiang Wu","Review of Artificial Intelligence Adversarial Attack and Defense Technologies",2019,"","","","",139,"2022-07-13 09:19:31","","10.3390/APP9050909","","",,,,,109,36.33,27,4,3,"In recent years, artificial intelligence technologies have been widely used in computer vision, natural language processing, automatic driving, and other fields. However, artificial intelligence systems are vulnerable to adversarial attacks, which limit the applications of artificial intelligence (AI) technologies in key security fields. Therefore, improving the robustness of AI systems against adversarial attacks has played an increasingly important role in the further development of AI. This paper aims to comprehensively summarize the latest research progress on adversarial attack and defense technologies in deep learning. According to the target model’s different stages where the adversarial attack occurred, this paper expounds the adversarial attack methods in the training stage and testing stage respectively. Then, we sort out the applications of adversarial attack technologies in computer vision, natural language processing, cyberspace security, and the physical world. Finally, we describe the existing adversarial defense methods respectively in three main categories, i.e., modifying data, modifying models and using auxiliary tools.","",""
93,"Mark O. Riedl","Human-Centered Artificial Intelligence and Machine Learning",2019,"","","","",140,"2022-07-13 09:19:31","","10.1002/HBE2.117","","",,,,,93,31.00,93,1,3,"Humans are increasingly coming into contact with artificial intelligence and machine learning systems. Human-centered artificial intelligence is a perspective on AI and ML that algorithms must be designed with awareness that they are part of a larger system consisting of humans. We lay forth an argument that human-centered artificial intelligence can be broken down into two aspects: (1) AI systems that understand humans from a sociocultural perspective, and (2) AI systems that help humans understand them. We further argue that issues of social responsibility such as fairness, accountability, interpretability, and transparency.","",""
17,"Yi-Shan Lin, Wen-Chuan Lee, Z. B. Celik","What Do You See?: Evaluation of Explainable Artificial Intelligence (XAI) Interpretability through Neural Backdoors",2020,"","","","",141,"2022-07-13 09:19:31","","10.1145/3447548.3467213","","",,,,,17,8.50,6,3,2,"EXplainable AI (XAI) methods have been proposed to interpret how a deep neural network predicts inputs through model saliency explanations that highlight the input parts deemed important to arrive at a decision for a specific target. However, it remains challenging to quantify the correctness of their interpretability as current evaluation approaches either require subjective input from humans or incur high computation cost with automated evaluation. In this paper, we propose backdoor trigger patterns--hidden malicious functionalities that cause misclassification--to automate the evaluation of saliency explanations. Our key observation is that triggers provide ground truth for inputs to evaluate whether the regions identified by an XAI method are truly relevant to its output. Since backdoor triggers are the most important features that cause deliberate misclassification, a robust XAI method should reveal their presence at inference time. We introduce three complementary metrics for the systematic evaluation of explanations that an XAI method generates. We evaluate seven state-of-the-art model-free and model-specific post-hoc methods through 36 models trojaned with specifically crafted triggers using color, shape, texture, location, and size. We found six methods that use local explanation and feature relevance fail to completely highlight trigger regions, and only a model-free approach can uncover the entire trigger region. We made our code available at https://github.com/yslin013/evalxai.","",""
15,"J. Janet, Chenru Duan, A. Nandy, Fang Liu, H. Kulik","Navigating Transition-Metal Chemical Space: Artificial Intelligence for First-Principles Design.",2021,"","","","",142,"2022-07-13 09:19:31","","10.1021/acs.accounts.0c00686","","",,,,,15,15.00,3,5,1,"ConspectusThe variability of chemical bonding in open-shell transition-metal complexes not only motivates their study as functional materials and catalysts but also challenges conventional computational modeling tools. Here, tailoring ligand chemistry can alter preferred spin or oxidation states as well as electronic structure properties and reactivity, creating vast regions of chemical space to explore when designing new materials atom by atom. Although first-principles density functional theory (DFT) remains the workhorse of computational chemistry in mechanism deduction and property prediction, it is of limited use here. DFT is both far too computationally costly for widespread exploration of transition-metal chemical space and also prone to inaccuracies that limit its predictive performance for localized d electrons in transition-metal complexes. These challenges starkly contrast with the well-trodden regions of small-organic-molecule chemical space, where the analytical forms of molecular mechanics force fields and semiempirical theories have for decades accelerated the discovery of new molecules, accurate DFT functional performance has been demonstrated, and gold-standard methods from correlated wavefunction theory can predict experimental results to chemical accuracy.The combined promise of transition-metal chemical space exploration and lack of established tools has mandated a distinct approach. In this Account, we outline the path we charted in exploration of transition-metal chemical space starting from the first machine learning (ML) models (i.e., artificial neural network and kernel ridge regression) and representations for the prediction of open-shell transition-metal complex properties. The distinct importance of the immediate coordination environment of the metal center as well as the lack of low-level methods to accurately predict structural properties in this coordination environment first motivated and then benefited from these ML models and representations. Once developed, the recipe for prediction of geometric, spin state, and redox potential properties was straightforwardly extended to a diverse range of other properties, including in catalysis, computational ""feasibility"", and the gas separation properties of periodic metal-organic frameworks. Interpretation of selected features most important for model prediction revealed new ways to encapsulate design rules and confirmed that models were robustly mapping essential structure-property relationships. Encountering the special challenge of ensuring that good model performance could generalize to new discovery targets motivated investigation of how to best carry out model uncertainty quantification. Distance-based approaches, whether in model latent space or in carefully engineered feature space, provided intuitive measures of the domain of applicability. With all of these pieces together, ML can be harnessed as an engine to tackle the large-scale exploration of transition-metal chemical space needed to satisfy multiple objectives using efficient global optimization methods. In practical terms, bringing these artificial intelligence tools to bear on the problems of transition-metal chemical space exploration has resulted in ML-model assessments of large, multimillion compound spaces in minutes and validated new design leads in weeks instead of decades.","",""
77,"K. Paranjape, M. Schinkel, R. N. Nannan Panday, J. Car, P. Nanayakkara","Introducing Artificial Intelligence Training in Medical Education",2019,"","","","",143,"2022-07-13 09:19:31","","10.2196/16048","","",,,,,77,25.67,15,5,3,"Health care is evolving and with it the need to reform medical education. As the practice of medicine enters the age of artificial intelligence (AI), the use of data to improve clinical decision making will grow, pushing the need for skillful medicine-machine interaction. As the rate of medical knowledge grows, technologies such as AI are needed to enable health care professionals to effectively use this knowledge to practice medicine. Medical professionals need to be adequately trained in this new technology, its advantages to improve cost, quality, and access to health care, and its shortfalls such as transparency and liability. AI needs to be seamlessly integrated across different aspects of the curriculum. In this paper, we have addressed the state of medical education at present and have recommended a framework on how to evolve the medical education curriculum to include AI.","",""
67,"Xiaoxuan Liu, S. C. Rivera, L. Faes, L. F. D. Ruffano, C. Yau, P. Keane, H. Ashrafian, A. Darzi, S. Vollmer, J. Deeks, L. Bachmann, Christopher Holmes, A. Chan, D. Moher, M. Calvert, A. Denniston","Reporting guidelines for clinical trials evaluating artificial intelligence interventions are needed",2019,"","","","",144,"2022-07-13 09:19:31","","10.1038/s41591-019-0603-3","","",,,,,67,22.33,7,16,3,"","",""
51,"Miriam C. Buiten","Towards Intelligent Regulation of Artificial Intelligence",2019,"","","","",145,"2022-07-13 09:19:31","","10.1017/err.2019.8","","",,,,,51,17.00,51,1,3,"Artificial intelligence (AI) is becoming a part of our daily lives at a fast pace, offering myriad benefits for society. At the same time, there is concern about the unpredictability and uncontrollability of AI. In response, legislators and scholars call for more transparency and explainability of AI. This article considers what it would mean to require transparency of AI. It advocates looking beyond the opaque concept of AI, focusing on the concrete risks and biases of its underlying technology: machine-learning algorithms. The article discusses the biases that algorithms may produce through the input data, the testing of the algorithm and the decision model. Any transparency requirement for algorithms should result in explanations of these biases that are both understandable for the prospective recipients, and technically feasible for producers. Before asking how much transparency the law should require from algorithms, we should therefore consider if the explanation that programmers could offer is useful in specific legal contexts.","",""
48,"M. Cukurova, C. Kent, R. Luckin","Artificial intelligence and multimodal data in the service of human decision-making: A case study in debate tutoring",2019,"","","","",146,"2022-07-13 09:19:31","","10.1111/BJET.12829","","",,,,,48,16.00,16,3,3,"The question: ""What is an appropriate role for AI?"" is the subject of much discussion and interest. Arguments about whether AI should be a human replacing technology or a human assisting technology frequently take centre stage. Education is no exception when it comes to questions about the role that AI should play, and as with many other professional areas, the exact role of AI in education is not easy to predict. Here, we argue that one potential role for AI in education is to provide opportunities for human intelligence augmentation, with AI supporting us in decision‐making processes, rather than replacing us through automation. To provide empirical evidence to support our argument, we present a case study in the context of debate tutoring, in which we use prediction and classification models to increase the transparency of the intuitive decision‐making processes of expert tutors for advanced reflections and feedback. Furthermore, we compare the accuracy of unimodal and multimodal classification models of expert human tutors' decisions about the social and emotional aspects of tutoring while evaluating trainees. Our results show that multimodal data leads to more accurate classification models in the context we studied. [ABSTRACT FROM AUTHOR]","",""
94,"F. Pedró, Miguel Subosa, A. Rivas, Paula Valverde","Artificial intelligence in education : challenges and opportunities for sustainable development",2019,"","","","",147,"2022-07-13 09:19:31","","","","",,,,,94,31.33,24,4,3,".........................................................................................................................................................................4 Executive Summary .......................................................................................................................................................5 Introduction .................................................................................................................................................................................... 7 Section I: Leveraging AI towards improving learning and equity ......................................................................................11 (1) AI to promote personalisation and better learning outcomes ...................................................................................................12 (2) Data analytics in Education Management Information Systems (EMIS) and the evolution to Learning Management Systems (LMS) ......................................................................................................15 Section II: Preparing learners to thrive in the future with AI ..............................................................................................17 (1) A new curriculum for a digital and AI-powered world ......................................................................................................................18 (2) Strengthening AI capacities through post-basic education and training ................................................................................. 22 Section III: AI in education challenges and policy implications ............................................................................25 First challenge: a comprehensive public policy on AI for sustainable development ....................................................................26 Second challenge: Ensuring inclusion and equity in AI in education .................................................................................................28 Third challenge: Preparing teachers for AI-powered education and preparing AI to understand education ......................28 Fourth challenge: Developing quality and inclusive data systems ....................................................................................................30 Fifth challenge: making research on AI in education significant .........................................................................................................31 Sixth challenge: ethics and transparency in data collection, use and dissemination ...................................................................32 CONCLUSIONS .............................................................................................................................................................34 ANNEX ..........................................................................................................................................................................37 AI definition and related concepts .................................................................................................................................................................37 REFERENCES .................................................................................................................................................................................41","",""
37,"M. Beil, Ingo Proft, Daniel van Heerden, S. Sviri, P. V. van Heerden","Ethical considerations about artificial intelligence for prognostication in intensive care",2019,"","","","",148,"2022-07-13 09:19:31","","10.1186/s40635-019-0286-6","","",,,,,37,12.33,7,5,3,"","",""
35,"Sonia K. Katyal","Private Accountability in an Age of Artificial Intelligence",2019,"","","","",149,"2022-07-13 09:19:31","","10.1017/9781108680844.004","","",,,,,35,11.67,35,1,3,"Author(s): Katyal, SK | Abstract: © 2019 American Statistical Association. All Rights Reserved. In this Article, I explore the impending conflict between the protection of civil rights and artificial intelligence (AI). While both areas of law have amassed rich and well-developed areas of scholarly work and doctrinal support, a growing body of scholars are interrogating the intersection between them. This Article argues that the issues surrounding algorithmic accountability demonstrate a deeper, more structural tension within a new generation of disputes regarding law and technology. As I argue, the true promise of AI does not lie in the information we reveal to one another, but rather in the questions it raises about the interaction of technology, property, and civil rights. For this reason, I argue that we are looking in the wrong place if we look only to the state to address issues of algorithmic accountability. Instead, given the state's reluctance to address the issue, we must turn to other ways to ensure more transparency and accountability that stem from private industry, rather than public regulation. The issue of algorithmic bias represents a crucial new world of civil rights concerns, one that is distinct in nature from the ones that preceded it. Since we are in a world where the activities of private corporations, rather than the state, are raising concerns about privacy, due process, and discrimination, we must focus on the role of private corporations in addressing the issue. Towards this end, I discuss a variety of tools to help eliminate the opacity of AI, including codes of conduct, impact statements, and whistleblower protection, which I argue carries the potential to encourage greater endogeneity in civil rights enforcement. Ultimately, by examining the relationship between private industry and civil rights, we can perhaps develop a new generation of forms of accountability in the process.","",""
31,"T. Ertekin, Qian Sun","Artificial Intelligence Applications in Reservoir Engineering: A Status Check",2019,"","","","",150,"2022-07-13 09:19:31","","10.3390/EN12152897","","",,,,,31,10.33,16,2,3,"This article provides a comprehensive review of the state-of-art in the area of artificial intelligence applications to solve reservoir engineering problems. Research works including proxy model development, artificial-intelligence-assisted history-matching, project design, and optimization, etc. are presented to demonstrate the robustness of the intelligence systems. The successes of the developments prove the advantages of the AI approaches in terms of high computational efficacy and strong learning capabilities. Thus, the implementation of intelligence models enables reservoir engineers to accomplish many challenging and time-intensive works more effectively. However, it is not yet astute to completely replace the conventional reservoir engineering models with intelligent systems, since the defects of the technology cannot be ignored. The trend of research and industrial practices of reservoir engineering area would be establishing a hand-shaking protocol between the conventional modeling and the intelligent systems. Taking advantages of both methods, more robust solutions could be obtained with significantly less computational overheads.","",""
42,"Sherin M. Mathews","Explainable Artificial Intelligence Applications in NLP, Biomedical, and Malware Classification: A Literature Review",2019,"","","","",151,"2022-07-13 09:19:31","","10.1007/978-3-030-22868-2_90","","",,,,,42,14.00,42,1,3,"","",""
157,"S. C. Rivera, Xiaoxuan Liu, A. Chan, A. Denniston, M. Calvert, H. Ashrafian, A. Beam, G. Collins, A. Darzi, J. Deeks, M. Elzarrad, Cyrus Espinoza, Andre Esteva, L. Faes, L. Ferrante di Ruffano, J. Fletcher, R. Golub, H. Harvey, C. Haug, Christopher Holmes, Adrian Jonas, P. Keane, Christopher J. Kelly, Aaron Y. Lee, Cecilia S Lee, Elaine Manna, J. Matcham, Melissa D. McCradden, D. Moher, Joao Monteiro, C. Mulrow, L. Oakden-Rayner, D. Paltoo, M. Panico, G. Price, Samuel d. Rowley, Richard Savage, Rupa Sarkar, S. Vollmer, C. Yau","Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI Extension",2020,"","","","",152,"2022-07-13 09:19:31","","10.1136/bmj.m3210","","",,,,,157,78.50,16,40,2,"Abstract The SPIRIT 2013 (The Standard Protocol Items: Recommendations for Interventional Trials) statement aims to improve the completeness of clinical trial protocol reporting, by providing evidence-based recommendations for the minimum set of items to be addressed. This guidance has been instrumental in promoting transparent evaluation of new interventions. More recently, there is a growing recognition that interventions involving artificial intelligence need to undergo rigorous, prospective evaluation to demonstrate their impact on health outcomes. The SPIRIT-AI extension is a new reporting guideline for clinical trials protocols evaluating interventions with an AI component. It was developed in parallel with its companion statement for trial reports: CONSORT-AI. Both guidelines were developed using a staged consensus process, involving a literature review and expert consultation to generate 26 candidate items, which were consulted on by an international multi-stakeholder group in a 2-stage Delphi survey (103 stakeholders), agreed on in a consensus meeting (31 stakeholders) and refined through a checklist pilot (34 participants). The SPIRIT-AI extension includes 15 new items, which were considered sufficiently important for clinical trial protocols of AI interventions. These new items should be routinely reported in addition to the core SPIRIT 2013 items. SPIRIT-AI recommends that investigators provide clear descriptions of the AI intervention, including instructions and skills required for use, the setting in which the AI intervention will be integrated, considerations around the handling of input and output data, the human-AI interaction and analysis of error cases. SPIRIT-AI will help promote transparency and completeness for clinical trial protocols for AI interventions. Its use will assist editors and peer-reviewers, as well as the general readership, to understand, interpret and critically appraise the design and risk of bias for a planned clinical trial.","",""
25,"S. Petersen, M. Abdulkareem, T. Leiner","Artificial Intelligence Will Transform Cardiac Imaging—Opportunities and Challenges",2019,"","","","",153,"2022-07-13 09:19:31","","10.3389/fcvm.2019.00133","","",,,,,25,8.33,8,3,3,"Artificial intelligence (AI) using machine learning techniques will change healthcare as we know it. While healthcare AI applications are currently trailing behind popular AI applications, such as personalized web-based advertising, the pace of research and deployment is picking up and about to become disruptive. Overcoming challenges such as patient and public support, transparency over the legal basis for healthcare data use, privacy preservation, technical challenges related to accessing large-scale data from healthcare systems not designed for Big Data analysis, and deployment of AI in routine clinical practice will be crucial. Cardiac imaging and imaging of other body parts is likely to be at the frontier for the development of applications as pattern recognition and machine learning are a significant strength of AI with practical links to image processing. Many opportunities in cardiac imaging exist where AI will impact patients, medical staff, hospitals, commissioners and thus, the entire healthcare system. This perspective article will outline our vision for AI in cardiac imaging with examples of potential applications, challenges and some lessons learnt in recent years.","",""
28,"M. Komorowski","Artificial intelligence in intensive care: are we there yet?",2019,"","","","",154,"2022-07-13 09:19:31","","10.1007/s00134-019-05662-6","","",,,,,28,9.33,28,1,3,"","",""
26,"E. Racine, W. Boehlen, M. Sample","Healthcare uses of artificial intelligence: Challenges and opportunities for growth",2019,"","","","",155,"2022-07-13 09:19:31","","10.1177/0840470419843831","","",,,,,26,8.67,9,3,3,"Forms of Artificial Intelligence (AI), like deep learning algorithms and neural networks, are being intensely explored for novel healthcare applications in areas such as imaging and diagnoses, risk analysis, lifestyle management and monitoring, health information management, and virtual health assistance. Expected benefits in these areas are wide-ranging and include increased speed in imaging, greater insight into predictive screening, and decreased healthcare costs and inefficiency. However, AI-based clinical tools also create a host of situations wherein commonly-held values and ethical principles may be challenged. In this short column, we highlight three potentially problematic aspects of AI use in healthcare: (1) dynamic information and consent, (2) transparency and ownership, and (3) privacy and discrimination. We discuss their impact on patient/client, clinician, and health institution values and suggest ways to tackle this impact. We propose that AI-related ethical challenges may represent an opportunity for growth in organizations.","",""
29,"Melanie Mitchell","Artificial Intelligence Hits the Barrier of Meaning",2019,"","","","",156,"2022-07-13 09:19:31","","10.3390/info10020051","","",,,,,29,9.67,29,1,3,"Today’s AI systems sorely lack the essence of human intelligence: Understanding the situations we experience, being able to grasp their meaning. The lack of humanlike understanding in machines is underscored by recent studies demonstrating lack of robustness of state-of-the-art deep-learning systems. Deeper networks and larger datasets alone are not likely to unlock AI’s “barrier of meaning”; instead the field will need to embrace its original roots as an interdisciplinary science of intelligence.","",""
6,"Dina M. El-Sherif, Mohamed Abouzid, Mohamed Tarek Elzarif, Alhassan Ali Ahmed, Ashwag Albakri, Mohammed M. Alshehri","Telehealth and Artificial Intelligence Insights into Healthcare during the COVID-19 Pandemic",2022,"","","","",157,"2022-07-13 09:19:31","","10.3390/healthcare10020385","","",,,,,6,6.00,1,6,1,"Soon after the coronavirus disease 2019 pandemic was proclaimed, digital health services were widely adopted to respond to this public health emergency, including comprehensive monitoring technologies, telehealth, creative diagnostic, and therapeutic decision-making methods. The World Health Organization suggested that artificial intelligence might be a valuable way of dealing with the crisis. Artificial intelligence is an essential technology of the fourth industrial revolution that is a critical nonmedical intervention for overcoming the present global health crisis, developing next-generation pandemic preparation, and regaining resilience. While artificial intelligence has much potential, it raises fundamental privacy, transparency, and safety concerns. This study seeks to address these issues and looks forward to an intelligent healthcare future based on best practices and lessons learned by employing telehealth and artificial intelligence during the COVID-19 pandemic.","",""
24,"Anastassia Lauterbach","Artificial intelligence and policy: quo vadis?",2019,"","","","",158,"2022-07-13 09:19:31","","10.1108/DPRG-09-2018-0054","","",,,,,24,8.00,24,1,3," Purpose This paper aims to inform policymakers about key artificial intelligence (AI) technologies, risks and trends in national AI strategies. It suggests a framework of social governance to ensure emergence of safe and beneficial AI.   Design/methodology/approach The paper is based on approximately 100 interviews with researchers, executives of traditional companies and startups and policymakers in seven countries. The interviews were carried out in January-August 2017.   Findings Policymakers still need to develop an informed, scientifically grounded and forward-looking view on what societies and businesses might expect from AI. There is lack of transparency on what key AI risks are and what might be regulatory approaches to handle them. There is no collaborative framework in place involving all important actors to decide on AI technology design principles and governance. Today's technology decisions will have long-term consequences on lives of billions of people and competitiveness of millions of businesses.   Research limitations/implications The research did not include a lot of insights from the emerging markets.   Practical implications Policymakers will understand the scope of most important AI concepts, risks and national strategies.   Social implications AI is progressing at a very fast rate, changing industries, businesses and approaches how companies learn, generate business insights, design products and communicate with their employees and customers. It has a big societal impact, as – if not designed with care – it can scale human bias, increase cybersecurity risk and lead to negative shifts in employment. Like no other invention, it can tighten control by the few over the many, spread false information and propaganda and therewith shape the perception of people, communities and enterprises.   Originality/value This paper is a compendium on the most important concepts of AI, bringing clarity into discussions around AI risks and the ways to mitigate them. The breadth of topics is valuable to policymakers, students, practitioners, general executives and board directors alike. ","",""
14,"C. Marsden, Trisha Meyer","Regulating disinformation with artificial intelligence:effects of disinformation initiatives on freedom of expression and media pluralism",2019,"","","","",159,"2022-07-13 09:19:31","","10.2861/003689","","",,,,,14,4.67,7,2,3,"This study examines the consequences of the increasingly prevalent use of artificial intelligence (AI) disinformation initiatives upon freedom of expression, pluralism and the functioning of a democratic polity.  The study examines the trade-offs in using automated technology to limit the spread of disinformation online. It presents options (from self-regulatory to legislative) to regulate automated content recognition (ACR) technologies in this context. Special attention is paid to the opportunities for the European Union as a whole to take the lead in setting the framework for designing these technologies in a way that enhances accountability and transparency and respects free speech. The present project reviews some of the key academic and policy ideas on technology and disinformation and highlights their relevance to European policy.  Chapter 1 introduces the background to the study and presents the definitions used. Chapter 2 scopes the policy boundaries of disinformation from economic, societal and technological perspectives, focusing on the media context, behavioural economics and technological regulation. Chapter 3 maps and evaluates existing regulatory and technological responses to disinformation. In Chapter 4, policy options are presented, paying particular attention to interactions between technological solutions, freedom of expression and media pluralism.","",""
0,"Renata Guizzardi, Jennifer Horkoff, A. Perini, A. Susi","Preface: 3rd Workshop on Requirements Engineering for Artificial Intelligence (RE4AI)",2022,"","","","",160,"2022-07-13 09:19:31","","","","",,,,,0,0.00,0,4,1,"Artificial Intelligence (AI) is embedded in software systems used in everyday life, such as cars, household appliances, wearable devices, healthcare chatbots, as well as in a variety of software applications that support data-driven decisions, e.g. business intelligence services for insurance companies. For several years, AI researchers have manifested their worries and recommendations for the responsible use of data, employment of discrimination-free algorithms, alignment of AI-based systems and technologies with human values and transparency. Awareness for the need of approaches for “Responsible AI” has rapidly increased and motivated attention by normative and standardisation organisations (e.g. EU Ethics Guidelines for Trustworthy AI1, and the IEEE Global Initiative on Ethics of Autonomous and Intelligent Systems2), software technology big players, and diverse research communities, including Software Engineering and Requirements Engineering research communities. The Requirements Engineering for Artificial Intelligence (RE4AI) workshop aims to provide a forum for discussing how Requirements Engineering methods, techniques and tools may be used to support the development of Artificial Intelligence systems that are lawful, ethical and robust. The main goals of the RE4AI workshop are as follows: raising awareness in the RE community about the importance of RE in realizing Trustworthy AI systems; bringing in the same room people from AI and RE industry and academia to discuss pressing issues, such as how RE can contribute to prevent AI systems to fail or to go rogue; setting up the basis for collaboratively producing a report on the challenges, candidate solution paths, and research","",""
0,"Jurgita Černevičienė, Audrius Kabašinskas","Review of Multi-Criteria Decision-Making Methods in Finance Using Explainable Artificial Intelligence",2022,"","","","",161,"2022-07-13 09:19:31","","10.3389/frai.2022.827584","","",,,,,0,0.00,0,2,1,"The influence of Artificial Intelligence is growing, as is the need to make it as explainable as possible. Explainability is one of the main obstacles that AI faces today on the way to more practical implementation. In practise, companies need to use models that balance interpretability and accuracy to make more effective decisions, especially in the field of finance. The main advantages of the multi-criteria decision-making principle (MCDM) in financial decision-making are the ability to structure complex evaluation tasks that allow for well-founded financial decisions, the application of quantitative and qualitative criteria in the analysis process, the possibility of transparency of evaluation and the introduction of improved, universal and practical academic methods to the financial decision-making process. This article presents a review and classification of multi-criteria decision-making methods that help to achieve the goal of forthcoming research: to create artificial intelligence-based methods that are explainable, transparent, and interpretable for most investment decision-makers.","",""
14,"M. Najafzadeh, G. Oliveto","More reliable predictions of clear-water scour depth at pile groups by robust artificial intelligence techniques while preserving physical consistency",2021,"","","","",162,"2022-07-13 09:19:31","","10.1007/s00500-020-05567-3","","",,,,,14,14.00,7,2,1,"","",""
11,"Luke Stark, Jevan Hutson","Physiognomic Artificial Intelligence",2021,"","","","",163,"2022-07-13 09:19:31","","10.2139/ssrn.3927300","","",,,,,11,11.00,6,2,1,"The reanimation of the pseudosciences of physiognomy and phrenology at scale through computer vision and machine learning is a matter of urgent concern. This Article, which contributes to critical data studies, consumer protection law, biometric privacy law, and anti-discrimination law, endeavors to conceptualize and problematize physiognomic artificial intelligence (AI) and offer policy recommendations for state and federal lawmakers to forestall its proliferation.    Physiognomic AI, we contend, is the practice of using computer software and related systems to infer or create hierarchies of an individual’s body composition, protected class status, perceived character, capabilities, and future social outcomes based on their physical or behavioral characteristics. Physiognomic and phrenological logics are intrinsic to the technical mechanism of computer vision applied to humans. In this Article, we observe how computer vision is a central vector for physiognomic AI technologies, unpacking how computer vision reanimates physiognomy in conception, form, and practice and the dangers this trend presents for civil liberties.    This Article thus argues for legislative action to forestall and roll back the proliferation of physiognomic AI. To that end, we consider a potential menu of safeguards and limitations to significantly limit the deployment of physiognomic AI systems, which we hope can be used to strengthen local, state, and federal legislation. We foreground our policy discussion by proposing the abolition of physiognomic AI. From there, we posit regimes of U.S. consumer protection law, biometric privacy law, and civil rights law as vehicles for rejecting physiognomy’s digital renaissance in artificial intelligence. Specifically, we argue that physiognomic AI should be categorically rejected as oppressive and unjust. Second, we argue that lawmakers should declare physiognomic AI to be unfair and deceptive per se. Third, we argue that lawmakers should enact or expand biometric privacy laws to prohibit physiognomic AI. Fourth, we argue that lawmakers should prohibit physiognomic AI in places of public accommodation. We also observe the paucity of procedural and managerial regimes of fairness, accountability, and transparency in addressing physiognomic AI and attend to potential counterarguments in support of physiognomic AI.","",""
0,"Pan Wang, Yangyang Zhong, Zhenan Yao","Modeling and Estimation of CO2 Emissions in China Based on Artificial Intelligence",2022,"","","","",164,"2022-07-13 09:19:31","","10.1155/2022/6822467","","",,,,,0,0.00,0,3,1,"Since China’s reform and opening up, the social economy has achieved rapid development, followed by a sharp increase in carbon dioxide (CO2) emissions. Therefore, at the 75th United Nations General Assembly, China proposed to achieve carbon peaking by 2030 and carbon neutrality by 2060. The research work on advance forecasting of CO2 emissions is essential to achieve the above-mentioned carbon peaking and carbon neutrality goals in China. In order to achieve accurate prediction of CO2 emissions, this study establishes a hybrid intelligent algorithm model suitable for CO2 emissions prediction based on China’s CO2 emissions and related socioeconomic indicator data from 1971 to 2017. The hyperparameters of Least Squares Support Vector Regression (LSSVR) are optimized by the Adaptive Artificial Bee Colony (AABC) algorithm to build a high-performance hybrid intelligence model. The research results show that the hybrid intelligent algorithm model designed in this paper has stronger robustness and accuracy with relative error almost within ±5% in the advance prediction of CO2 emissions. The modeling scheme proposed in this study can not only provide strong support for the Chinese government and industry departments to formulate policies related to the carbon peaking and carbon neutrality goals, but also can be extended to the research of other socioeconomic-related issues.","",""
10,"Zihao Chen, Long Hu, Baoting Zhang, Aiping Lu, Yaofeng Wang, Yuanyuan Yu, Ge Zhang","Artificial Intelligence in Aptamer–Target Binding Prediction",2021,"","","","",165,"2022-07-13 09:19:31","","10.3390/ijms22073605","","",,,,,10,10.00,1,7,1,"Aptamers are short single-stranded DNA, RNA, or synthetic Xeno nucleic acids (XNA) molecules that can interact with corresponding targets with high affinity. Owing to their unique features, including low cost of production, easy chemical modification, high thermal stability, reproducibility, as well as low levels of immunogenicity and toxicity, aptamers can be used as an alternative to antibodies in diagnostics and therapeutics. Systematic evolution of ligands by exponential enrichment (SELEX), an experimental approach for aptamer screening, allows the selection and identification of in vitro aptamers with high affinity and specificity. However, the SELEX process is time consuming and characterization of the representative aptamer candidates from SELEX is rather laborious. Artificial intelligence (AI) could help to rapidly identify the potential aptamer candidates from a vast number of sequences. This review discusses the advancements of AI pipelines/methods, including structure-based and machine/deep learning-based methods, for predicting the binding ability of aptamers to targets. Structure-based methods are the most used in computer-aided drug design. For this part, we review the secondary and tertiary structure prediction methods for aptamers, molecular docking, as well as molecular dynamic simulation methods for aptamer–target binding. We also performed analysis to compare the accuracy of different secondary and tertiary structure prediction methods for aptamers. On the other hand, advanced machine-/deep-learning models have witnessed successes in predicting the binding abilities between targets and ligands in drug discovery and thus potentially offer a robust and accurate approach to predict the binding between aptamers and targets. The research utilizing machine-/deep-learning techniques for prediction of aptamer–target binding is limited currently. Therefore, perspectives for models, algorithms, and implementation strategies of machine/deep learning-based methods are discussed. This review could facilitate the development and application of high-throughput and less laborious in silico methods in aptamer selection and characterization.","",""
10,"T. Penzkofer, A. Padhani, B. Turkbey, M. Haider, H. Huisman, J. Walz, G. Salomon, I. Schoots, J. Richenberg, G. Villeirs, V. Panebianco, O. Rouvière, V. Løgager, J. Barentsz","ESUR/ESUI position paper: developing artificial intelligence for precision diagnosis of prostate cancer using magnetic resonance imaging",2021,"","","","",166,"2022-07-13 09:19:31","","10.1007/s00330-021-08021-6","","",,,,,10,10.00,1,14,1,"","",""
13,"Wolfgang Hoffmann-Riem","Artificial Intelligence as a Challenge for Law and Regulation",2019,"","","","",167,"2022-07-13 09:19:31","","10.1007/978-3-030-32361-5_1","","",,,,,13,4.33,13,1,3,"","",""
395,"Filip Karlo Dosilovic, Mario Brčič, N. Hlupic","Explainable artificial intelligence: A survey",2018,"","","","",168,"2022-07-13 09:19:31","","10.23919/MIPRO.2018.8400040","","",,,,,395,98.75,132,3,4,"In the last decade, with availability of large datasets and more computing power, machine learning systems have achieved (super)human performance in a wide variety of tasks. Examples of this rapid development can be seen in image recognition, speech analysis, strategic game planning and many more. The problem with many state-of-the-art models is a lack of transparency and interpretability. The lack of thereof is a major drawback in many applications, e.g. healthcare and finance, where rationale for model's decision is a requirement for trust. In the light of these issues, explainable artificial intelligence (XAI) has become an area of interest in research community. This paper summarizes recent developments in XAI in supervised learning, starts a discussion on its connection with artificial general intelligence, and gives proposals for further research directions.","",""
329,"M. Nagendran, Yang Chen, C. Lovejoy, A. Gordon, M. Komorowski, H. Harvey, E. Topol, J. Ioannidis, G. Collins, M. Maruthappu","Artificial intelligence versus clinicians: systematic review of design, reporting standards, and claims of deep learning studies",2020,"","","","",169,"2022-07-13 09:19:31","","10.1136/bmj.m689","","",,,,,329,164.50,33,10,2,"Abstract Objective To systematically examine the design, reporting standards, risk of bias, and claims of studies comparing the performance of diagnostic deep learning algorithms for medical imaging with that of expert clinicians. Design Systematic review. Data sources Medline, Embase, Cochrane Central Register of Controlled Trials, and the World Health Organization trial registry from 2010 to June 2019. Eligibility criteria for selecting studies Randomised trial registrations and non-randomised studies comparing the performance of a deep learning algorithm in medical imaging with a contemporary group of one or more expert clinicians. Medical imaging has seen a growing interest in deep learning research. The main distinguishing feature of convolutional neural networks (CNNs) in deep learning is that when CNNs are fed with raw data, they develop their own representations needed for pattern recognition. The algorithm learns for itself the features of an image that are important for classification rather than being told by humans which features to use. The selected studies aimed to use medical imaging for predicting absolute risk of existing disease or classification into diagnostic groups (eg, disease or non-disease). For example, raw chest radiographs tagged with a label such as pneumothorax or no pneumothorax and the CNN learning which pixel patterns suggest pneumothorax. Review methods Adherence to reporting standards was assessed by using CONSORT (consolidated standards of reporting trials) for randomised studies and TRIPOD (transparent reporting of a multivariable prediction model for individual prognosis or diagnosis) for non-randomised studies. Risk of bias was assessed by using the Cochrane risk of bias tool for randomised studies and PROBAST (prediction model risk of bias assessment tool) for non-randomised studies. Results Only 10 records were found for deep learning randomised clinical trials, two of which have been published (with low risk of bias, except for lack of blinding, and high adherence to reporting standards) and eight are ongoing. Of 81 non-randomised clinical trials identified, only nine were prospective and just six were tested in a real world clinical setting. The median number of experts in the comparator group was only four (interquartile range 2-9). Full access to all datasets and code was severely limited (unavailable in 95% and 93% of studies, respectively). The overall risk of bias was high in 58 of 81 studies and adherence to reporting standards was suboptimal (<50% adherence for 12 of 29 TRIPOD items). 61 of 81 studies stated in their abstract that performance of artificial intelligence was at least comparable to (or better than) that of clinicians. Only 31 of 81 studies (38%) stated that further prospective studies or trials were required. Conclusions Few prospective deep learning studies and randomised trials exist in medical imaging. Most non-randomised trials are not prospective, are at high risk of bias, and deviate from existing reporting standards. Data and code availability are lacking in most studies, and human comparator groups are often small. Future studies should diminish risk of bias, enhance real world clinical relevance, improve reporting and transparency, and appropriately temper conclusions. Study registration PROSPERO CRD42019123605.","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",170,"2022-07-13 09:19:31","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
10,"S. Larsson","The Socio-Legal Relevance of Artificial Intelligence",2019,"","","","",171,"2022-07-13 09:19:31","","10.3917/DRS1.103.0573","","",,,,,10,3.33,10,1,3,"This article draws on socio-legal theory in relation to growing concerns over fairness, accountability and transparency of societally applied artificial intelligence (AI) and machine learning. The purpose is to contribute to a broad socio-legal orientation by describing legal and normative challenges posed by applied AI. To do so, the article first analyzes a set of problematic cases, e.g., image recognition based on gender-biased databases. It then presents seven aspects of transparency that may complement notions of explainable AI (XAI) within AI-research undertaken by computer scientists. The article finally discusses the normative mirroring effect of using human values and societal structures as training data for learning technologies; it concludes by arguing for the need for a multidisciplinary approach in AI research, development, and governance.","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",172,"2022-07-13 09:19:31","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
63,"M. VerMilyea, J. Hall, S. Diakiw, A. Johnston, T. Nguyen, D. Perugini, A. Miller, A. Picou, A. P. Murphy, M. Perugini","Development of an artificial intelligence-based assessment model for prediction of embryo viability using static images captured by optical light microscopy during IVF",2020,"","","","",173,"2022-07-13 09:19:31","","10.1093/humrep/deaa013","","",,,,,63,31.50,6,10,2,"Abstract STUDY QUESTION Can an artificial intelligence (AI)-based model predict human embryo viability using images captured by optical light microscopy? SUMMARY ANSWER We have combined computer vision image processing methods and deep learning techniques to create the non-invasive Life Whisperer AI model for robust prediction of embryo viability, as measured by clinical pregnancy outcome, using single static images of Day 5 blastocysts obtained from standard optical light microscope systems. WHAT IS KNOWN ALREADY Embryo selection following IVF is a critical factor in determining the success of ensuing pregnancy. Traditional morphokinetic grading by trained embryologists can be subjective and variable, and other complementary techniques, such as time-lapse imaging, require costly equipment and have not reliably demonstrated predictive ability for the endpoint of clinical pregnancy. AI methods are being investigated as a promising means for improving embryo selection and predicting implantation and pregnancy outcomes. STUDY DESIGN, SIZE, DURATION These studies involved analysis of retrospectively collected data including standard optical light microscope images and clinical outcomes of 8886 embryos from 11 different IVF clinics, across three different countries, between 2011 and 2018. PARTICIPANTS/MATERIALS, SETTING, METHODS The AI-based model was trained using static two-dimensional optical light microscope images with known clinical pregnancy outcome as measured by fetal heartbeat to provide a confidence score for prediction of pregnancy. Predictive accuracy was determined by evaluating sensitivity, specificity and overall weighted accuracy, and was visualized using histograms of the distributions of predictions. Comparison to embryologists’ predictive accuracy was performed using a binary classification approach and a 5-band ranking comparison. MAIN RESULTS AND THE ROLE OF CHANCE The Life Whisperer AI model showed a sensitivity of 70.1% for viable embryos while maintaining a specificity of 60.5% for non-viable embryos across three independent blind test sets from different clinics. The weighted overall accuracy in each blind test set was >63%, with a combined accuracy of 64.3% across both viable and non-viable embryos, demonstrating model robustness and generalizability beyond the result expected from chance. Distributions of predictions showed clear separation of correctly and incorrectly classified embryos. Binary comparison of viable/non-viable embryo classification demonstrated an improvement of 24.7% over embryologists’ accuracy (P = 0.047, n = 2, Student’s t test), and 5-band ranking comparison demonstrated an improvement of 42.0% over embryologists (P = 0.028, n = 2, Student’s t test). LIMITATIONS, REASONS FOR CAUTION The AI model developed here is limited to analysis of Day 5 embryos; therefore, further evaluation or modification of the model is needed to incorporate information from different time points. The endpoint described is clinical pregnancy as measured by fetal heartbeat, and this does not indicate the probability of live birth. The current investigation was performed with retrospectively collected data, and hence it will be of importance to collect data prospectively to assess real-world use of the AI model. WIDER IMPLICATIONS OF THE FINDINGS These studies demonstrated an improved predictive ability for evaluation of embryo viability when compared with embryologists’ traditional morphokinetic grading methods. The superior accuracy of the Life Whisperer AI model could lead to improved pregnancy success rates in IVF when used in a clinical setting. It could also potentially assist in standardization of embryo selection methods across multiple clinical environments, while eliminating the need for complex time-lapse imaging equipment. Finally, the cloud-based software application used to apply the Life Whisperer AI model in clinical practice makes it broadly applicable and globally scalable to IVF clinics worldwide. STUDY FUNDING/COMPETING INTEREST(S) Life Whisperer Diagnostics, Pty Ltd is a wholly owned subsidiary of the parent company, Presagen Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation and Startup Fund (RCSF). ‘In kind’ support and embryology expertise to guide algorithm development were provided by Ovation Fertility. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. Presagen has filed a provisional patent for the technology described in this manuscript (52985P pending). A.P.M. owns stock in Life Whisperer, and S.M.D., A.J., T.N. and A.P.M. are employees of Life Whisperer.","",""
8,"Fabian Horst, D. Slijepcevic, S. Lapuschkin, Anna-Maria Raberger, M. Zeppelzauer, W. Samek, C. Breiteneder, W. Schöllhorn, B. Horsak","On the Understanding and Interpretation of Machine Learning Predictions in Clinical Gait Analysis Using Explainable Artificial Intelligence",2019,"","","","",174,"2022-07-13 09:19:31","","","","",,,,,8,2.67,1,9,3,"Systems incorporating Artificial Intelligence (AI) and machine learning (ML) techniques are increasingly used to guide decision-making in the healthcare sector. While AI-based systems provide powerful and promising results with regard to their classification and prediction accuracy (e.g., in differentiating between different disorders in human gait), most share a central limitation, namely their black-box character. Understanding which features classification models learn, whether they are meaningful and consequently whether their decisions are trustworthy is difficult and often impossible to comprehend. This severely hampers their applicability as decisionsupport systems in clinical practice. There is a strong need for AI-based systems to provide transparency and justification of predictions, which are necessary also for ethical and legal compliance. As a consequence, in recent years the field of explainable AI (XAI) has gained increasing importance. XAI focuses on the development of methods that enhance transparency and interpretability of complex ML models, such as Deep (Convolutional) Neural Networks. The primary aim of this article is to investigate whether XAI methods can enhance transparency, explainability and interpretability of predictions in automated clinical gait classification. We utilize a dataset comprising bilateral three-dimensional ground reaction force measurements from 132 patients with different lower-body gait disorders and 62 healthy controls. In our experiments, 1 ar X iv :1 91 2. 07 73 7v 1 [ cs .L G ] 1 6 D ec 2 01 9 Horst and Slijepcevic et al. Explainable AI in Clinical Gait Analysis we included several gait classification tasks, employed a representative set of classification methods, and a well-established XAI method – Layer-wise Relevance Propagation (LRP) – to explain decisions at the signal (input) level. The classification results are analyzed, compared and interpreted in terms of classification accuracy and relevance of input values for specific decisions. The decomposed input relevance information are evaluated from a statistical (using Statistical Parameter Mapping) and clinical (by an expert) viewpoint. There are three dimensions in our comparison: (i) different classification tasks, (ii) different classification methods, and (iii) data normalization. The presented approach exemplifies how XAI can be used to understand and interpret state-of-the-art ML models trained for gait classification tasks, and shows that the features that are considered relevant for machine learning models can be attributed to meaningful and clinically relevant biomechanical gait characteristics.","",""
7,"S. Larsson, Mikael Anneroth, Anna Felländer, L. Felländer-Tsai, F. Heintz, Rebecka Cedering Ångström","Sustainable AI: An inventory of the state of knowledge of ethical, social, and legal challenges related to artificial intelligence",2019,"","","","",175,"2022-07-13 09:19:31","","","","",,,,,7,2.33,1,6,3,"This report is an inventory of the state of knowledge of ethical, social, and legal challenges related to artificial intelligence conducted within the Swedish Vinnova-funded project “Hallbar AI – AI Ethics and Sustainability”, led by Anna Fellander. Based on a review and mapping of reports and studies, a quantitative and bibliometric analysis, and in-depth analyses of the healt- care sector, the telecom sector, and digital platforms, the report proposes three recommendations. Sustainable AI requires: 1. a broad focus on AI governance and regulation issues, 2. promoting multi-disciplinary collaboration, and 3. building trust in AI applications and applied machine-learning, which is a matter of key importance and requires further study of the relationship between transparency and accountability. (Less)","",""
9,"B. N. Manjunatha Reddy, S. K. Pramada, T. Roshni","Monthly surface runoff prediction using artificial intelligence: A study from a tropical climate river basin",2021,"","","","",176,"2022-07-13 09:19:31","","10.1007/s12040-020-01508-8","","",,,,,9,9.00,3,3,1,"","",""
9,"Nathalie A. Smuha, Emma Ahmed-Rengers, Adam Harkens, Wenlong Li, J. Maclaren, Riccardo Piselli, K. Yeung","How the EU Can Achieve Legally Trustworthy AI: A Response to the European Commission’s Proposal for an Artificial Intelligence Act",2021,"","","","",177,"2022-07-13 09:19:31","","10.2139/ssrn.3899991","","",,,,,9,9.00,1,7,1,"This document contains the response to the European Commission’s Proposal for an Artificial Intelligence Act from members of the Legal, Ethical & Accountable Digital Society (LEADS) Lab at the University of Birmingham. The Proposal seeks to give expression to the concept of ‘Lawful AI.’ This concept was mentioned, but not developed in the Commission’s High-Level Expert Group on AI’s Ethics Guidelines for Trustworthy AI (2019), which instead confined its discussion to the concepts of ‘Ethical’ and ‘Robust’ AI. After a brief introduction (Chapter 1), we set out the many aspects of the Proposal which we welcome, and stress our wholehearted support for its aim to protect fundamental rights (Chapter 2). Subsequently, we develop the concept of ‘Legally Trustworthy AI,’ arguing that it should be grounded in respect for three pillars on which contemporary liberal democratic societies are founded, namely: fundamental rights, the rule of law, and democracy (Chapter 3). Drawing on this conceptual framework, we first argue that the Proposal fails to reflect fundamental rights as claims with enhanced moral and legal status, which subjects any rights interventions to a demanding regime of scrutiny and must satisfy tests of necessity and proportionality. Moreover, the Proposal does not always accurately recognise the wrongs and harms associated with different kinds of AI systems and appropriately allocates responsibility for them. Second, the Proposal does not provide an effective framework for the enforcement of legal rights and duties, and does not ensure legal certainty and consistency, which are essential for the rule of law. Third, the Proposal neglects to ensure meaningful transparency, accountability, and rights of public participation, thereby failing to reflect adequate protection for democracy (Chapter 4). Based on these shortcomings in respecting and promoting the three pillars of Legally Trustworthy AI, we provide detailed recommendations for the Proposal’s revision (Chapter 5).","",""
9,"Junfeng Peng, Kaiqiang Zou, Mi Zhou, Yi Teng, Xiongyong Zhu, Feifei Zhang, Jun Xu","An Explainable Artificial Intelligence Framework for the Deterioration Risk Prediction of Hepatitis Patients",2021,"","","","",178,"2022-07-13 09:19:31","","10.1007/s10916-021-01736-5","","",,,,,9,9.00,1,7,1,"","",""
43,"Dan Liu, Fei Liu, Xiao-yan Xie, Liya Su, Ming Liu, Xiaohua Xie, M. Kuang, Guangliang Huang, Yuqi Wang, Hui Zhou, Kun Wang, Manxia Lin, Jie Tian","Accurate prediction of responses to transarterial chemoembolization for patients with hepatocellular carcinoma by using artificial intelligence in contrast-enhanced ultrasound",2020,"","","","",179,"2022-07-13 09:19:31","","10.1007/s00330-019-06553-6","","",,,,,43,21.50,4,13,2,"","",""
4,"Yeferson Torres Berru, V. F. López Batista, P. Torres-Carrión, Maria Gabriela Jimenez","Artificial Intelligence Techniques to Detect and Prevent Corruption in Procurement: A Systematic Literature Review",2019,"","","","",180,"2022-07-13 09:19:31","","10.1007/978-3-030-42520-3_21","","",,,,,4,1.33,1,4,3,"","",""
3,"Shuiping Luo","Research on the Change of Educational Management in the Era of Artificial Intelligence",2019,"","","","",181,"2022-07-13 09:19:31","","10.1109/ICICTA49267.2019.00101","","",,,,,3,1.00,3,1,3,"With the development and progress of artificial intelligence, artificial intelligence has become a hot topic of discussion in the current society. At present, people from all walks of life have different opinions on artificial intelligence technology. However, it is worth noting that in the future, artificial intelligence technology will inevitably enter all walks of life and greatly improve people's quality of life. In addition, artificial intelligence simulates human intelligence, which is in line with the education industry. With the continuous development of artificial intelligence, it has gradually combined with educational management, which makes educational management more forward-looking, and gradually promotes the development of educational management towards the direction of data, transparency and rationalization. However, with the gradual integration of artificial intelligence and education management, a series of problems have arisen, such as violation of privacy and security, violation of ethics and so on, which have brought enormous challenges to education management. In other words, there are advantages and disadvantages in integrating artificial intelligence technology into education management. Therefore, the author believes that education managers should adopt scientific and efficient measures, such as clearing up the working boundary of human and artificial intelligence in educational management activities, improving the relevant legal system, strengthening the relevant legal functions, and educating subjects should do their best and make appropriate use of artificial intelligence technology. Only in this way can we give full play to the respective advantages of artificial intelligence technology and education management, and further promote the development of education management towards modernization and intellectualization.","",""
3,"Philipp Lüthi, Thibault Gagnaux, Marcel Gygli","Distributed Ledger for Provenance Tracking of Artificial Intelligence Assets",2019,"","","","",182,"2022-07-13 09:19:31","","10.1007/978-3-030-42504-3_26","","",,,,,3,1.00,1,3,3,"","",""
3,"Saad Mahamood","Explainable Artificial Intelligence and its potential within Industry",2019,"","","","",183,"2022-07-13 09:19:31","","10.18653/v1/W19-8401","","",,,,,3,1.00,3,1,3,"The age of Big Data has enabled the creation of artificial intelligence solutions that has allowed systems to better respond to their users requests and needs. Applications such as recommender systems, automated content generation systems, etc. are increasingly leveraging such large amounts of data to make better informed decisions about how to tailor their output appropriately. However, the opaqueness of these AI systems in how they derive their decisions or outputs has led to an increasing call for transparency with increasing concerns for the potential of bias to occur in areas such as finance and criminal law. The culmination of these calls have lead to tentative legislative steps. For example, the ""Right to explanation"" as part of the recently enacted European Union’s General Data Protection Regulation. Natural Language Generation (NLG) has been used in successfully in many data-to-text applications allowing users to gain insights from their data sets. Whilst NLG technology has a strong role to play in generating explanations for AI models there still remains inherit challenges in developing and deploying text generation systems within a commercial context. In this talk I will explore the role and potential that Natural Language Explainable AI can have within trivago and the wider industry. trivago is a leading accommodation meta-search engine that enables users find the right hotel or apartment at the right price. In particular, this talk will describe the work we have done to apply natural language solutions within trivago and the challenges of applying AI solutions from a commercial perspective. Finally, this talk will also explore the potential applications of where explainable AI approaches could be used within trivago.","",""
4,"Debraj Sen, R. Chakrabarti, S. Chatterjee, D. Grewal, K. Manrai","Artificial intelligence and the radiologist: the future in the Armed Forces Medical Services",2019,"","","","",184,"2022-07-13 09:19:31","","10.1136/jramc-2018-001055","","",,,,,4,1.33,1,5,3,"Artificial intelligence (AI) involves computational networks (neural networks) that simulate human intelligence. The incorporation of AI in radiology will help in dealing with the tedious, repetitive, time-consuming job of detecting relevant findings in diagnostic imaging and segmenting the detected images into smaller data. It would also help in identifying details that are oblivious to the human eye. AI will have an immense impact in populations with deficiency of radiologists and in screening programmes. By correlating imaging data from millions of patients and their clinico-demographic-therapy-morbidity-mortality profiles, AI could lead to identification of new imaging biomarkers. This would change therapy and direct new research. However, issues of standardisation, transparency, ethics, regulations, training, accreditation and safety are the challenges ahead. The Armed Forces Medical Services has widely dispersed units, medical echelons and roles ranging from small field units to large static tertiary care centres. They can incorporate AI-enabled radiological services to subserve small remotely located hospitals and detachments without posted radiologists and ease the load of radiologists in larger hospitals. Early widespread incorporation of information technology and enabled services in our hospitals, adequate funding, regular upgradation of software and hardware, dedicated trained manpower to manage the information technology services and train staff, and cyber security are issues that need to be addressed.","",""
37,"T. Babina, A. Fedyk, A. He, James Hodson","Artificial Intelligence, Firm Growth, and Industry Concentration",2020,"","","","",185,"2022-07-13 09:19:31","","10.2139/ssrn.3651052","","",,,,,37,18.50,9,4,2,"Which firms invest in artificial intelligence (AI) technologies, and how do these investments affect individual firms and industries? We provide a comprehensive picture of the use of AI technologies and their impact among US firms over the last decade, using a unique combination of job postings and individual-level employment profiles. We introduce a novel measure of investments in AI technologies based on human capital and document that larger firms with higher sales, markups, and cash holdings tend to invest more in AI. Firms that invest in AI experience faster growth in both sales and employment, which translates into analogous growth at the industry level. The positive effects are concentrated among the ex ante largest firms, leading to a positive correlation between AI investments and an increase in industry concentration. However, the increase in concentration is not accompanied by either increased markups or increased productivity. Instead, firms tend to expand into new product and geographic markets. Our results are robust to instrumenting firm-level AI investments with foreign industry-level AI investments and with local variation in industry-level AI investments, and to controlling for investments in general information technology and robotics. We also document consistent patterns across measures of AI using firms' demand for AI talent (job postings) and actual AI talent (resumes). Overall, our findings support the view that new technologies, such as AI, increase the scale of the most productive firms and contribute to the rise of superstar firms.","",""
8,"C. Christou, G. Tsoulfas","Challenges and opportunities in the application of artificial intelligence in gastroenterology and hepatology",2021,"","","","",186,"2022-07-13 09:19:31","","10.3748/wjg.v27.i37.6191","","",,,,,8,8.00,4,2,1,"Artificial intelligence (AI) is an umbrella term used to describe a cluster of interrelated fields. Machine learning (ML) refers to a model that learns from past data to predict future data. Medicine and particularly gastroenterology and hepatology, are data-rich fields with extensive data repositories, and therefore fruitful ground for AI/ML-based software applications. In this study, we comprehensively review the current applications of AI/ML-based models in these fields and the opportunities that arise from their application. Specifically, we refer to the applications of AI/ML-based models in prevention, diagnosis, management, and prognosis of gastrointestinal bleeding, inflammatory bowel diseases, gastrointestinal premalignant and malignant lesions, other nonmalignant gastrointestinal lesions and diseases, hepatitis B and C infection, chronic liver diseases, hepatocellular carcinoma, cholangiocarcinoma, and primary sclerosing cholangitis. At the same time, we identify the major challenges that restrain the widespread use of these models in healthcare in an effort to explore ways to overcome them. Notably, we elaborate on the concerns regarding intrinsic biases, data protection, cybersecurity, intellectual property, liability, ethical challenges, and transparency. Even at a slower pace than anticipated, AI is infiltrating the healthcare industry. AI in healthcare will become a reality, and every physician will have to engage with it by necessity.","",""
8,"G. Currie, K. Hawk","Ethical and Legal Challenges of Artificial Intelligence in Nuclear Medicine.",2020,"","","","",187,"2022-07-13 09:19:31","","10.1053/j.semnuclmed.2020.08.001","","",,,,,8,4.00,4,2,2,"Artificial intelligence (AI) in nuclear medicine has gained significant traction and promises to be a disruptive, but innovative, technology. Recent developments in artificial neural networks, machine learning, and deep learning have ignited debate with respect to ethical and legal challenges associated with the use of AI in healthcare and medicine. While AI in nuclear medicine has the potential to improve workflow and productivity, and enhance clinical and research capabilities, there remains a professional responsibility to the profession and to patients: ethical, social, and legal. Enthusiasm to embrace new technology should not displace responsibilities for the ethical, social, and legal application of technology. This is especially true in relation to data usage, the algorithms applied, and how algorithms are used in practice. Governance of software and algorithms used for detection (segmentation) and/or diagnosis (classification) of disease using medical images requires rigorous evidence-based regulation. A number of frameworks have been developed for ethical application of AI generally in society and in radiology. For nuclear medicine, consideration needs to be given to beneficence, nonmaleficence, fairness and justice, safety, reliability, data security, privacy and confidentiality, mitigation of bias, transparency, explainability, and autonomy. AI is merely a tool, how it is utilised is a human choice. There is potential for AI applications to enhance clinical and research practice in nuclear medicine and concurrently produce deeper, more meaningful interactions between the physicians and the patient. Nonetheless ethical, legal, and social challenges demand careful attention and formulation of standards/guidelines for nuclear medicine.","",""
8,"Linbo Liu, Mingcheng Bi, Yunhua Wang, Junfeng Liu, Xiwen Jiang, Zhongbin Xu, Xingcai Zhang","Artificial intelligence-powered microfluidics for nanomedicine and materials synthesis.",2021,"","","","",188,"2022-07-13 09:19:31","","10.1039/d1nr06195j","","",,,,,8,8.00,1,7,1,"Artificial intelligence (AI) is an emerging technology with great potential, and its robust calculation and analysis capabilities are unmatched by traditional calculation tools. With the promotion of deep learning and open-source platforms, the threshold of AI has also become lower. Combining artificial intelligence with traditional fields to create new fields of high research and application value has become a trend. AI has been involved in many disciplines, such as medicine, materials, energy, and economics. The development of AI requires the support of many kinds of data, and microfluidic systems can often mine object data on a large scale to support AI. Due to the excellent synergy between the two technologies, excellent research results have emerged in many fields. In this review, we briefly review AI and microfluidics and introduce some applications of their combination, mainly in nanomedicine and material synthesis. Finally, we discuss the development trend of the combination of the two technologies.","",""
101,"Samantha Cruz Rivera, Xiaoxuan Liu, A. Chan, A. Denniston, M. Calvert","Guidelines for clinical trial protocols for interventions involving artificial intelligence: the SPIRIT-AI extension",2020,"","","","",189,"2022-07-13 09:19:31","","10.1038/s41591-020-1037-7","","",,,,,101,50.50,20,5,2,"","",""
103,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dentistry: Chances and Challenges",2020,"","","","",190,"2022-07-13 09:19:31","","10.1177/0022034520915714","","",,,,,103,51.50,34,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
6,"Sarah Friedrich, Stefan Groß, I. König, S. Engelhardt, M. Bahls, J. Heinz, C. Huber, L. Kaderali, M. Kelm, A. Leha, Jasmin Rühl, J. Schaller, Clemens Scherer, M. Vollmer, T. Seidler, T. Friede","Applications of artificial intelligence/machine learning approaches in cardiovascular medicine: a systematic review with recommendations",2021,"","","","",191,"2022-07-13 09:19:31","","10.1093/EHJDH/ZTAB054","","",,,,,6,6.00,1,16,1,"      Artificial intelligence (AI) and machine learning (ML) promise vast advances in medicine. The current state of AI/ML applications in cardiovascular medicine is largely unknown. This systematic review aims to close this gap and provides recommendations for future applications.        Pubmed and EMBASE were searched for applied publications using AI/ML approaches in cardiovascular medicine without limitations regarding study design or study population. The PRISMA statement was followed in this review. A total of 215 studies were identified and included in the final analysis. The majority (87%) of methods applied belong to the context of supervised learning. Within this group, tree-based methods were most commonly used, followed by network and regression analyses as well as boosting approaches. Concerning the areas of application, the most common disease context was coronary artery disease followed by heart failure and heart rhythm disorders. Often, different input types such as electronic health records and images were combined in one AI/ML application. Only a minority of publications investigated reproducibility and generalizability or provided a clinical trial registration.        A major finding is that methodology may overlap even with similar data. Since we observed marked variation in quality, reporting of the evaluation and transparency of data and methods urgently need to be improved. ","",""
0,"S. Sadeghi, M. Amiri, Farzaneh Mansoori Mooseloo","Artificial Intelligence and Its Application in Optimization under Uncertainty",2021,"","","","",192,"2022-07-13 09:19:31","","10.5772/intechopen.98628","","",,,,,0,0.00,0,3,1,"Nowadays, the increase in data acquisition and availability and complexity around optimization make it imperative to jointly use artificial intelligence (AI) and optimization for devising data-driven and intelligent decision support systems (DSS). A DSS can be successful if large amounts of interactive data proceed fast and robustly and extract useful information and knowledge to help decision-making. In this context, the data-driven approach has gained prominence due to its provision of insights for decision-making and easy implementation. The data-driven approach can discover various database patterns without relying on prior knowledge while also handling flexible objectives and multiple scenarios. This chapter reviews recent advances in data-driven optimization, highlighting the promise of data-driven optimization that integrates mathematical programming and machine learning (ML) for decision-making under uncertainty and identifies potential research opportunities. This chapter provides guidelines and implications for researchers, managers, and practitioners in operations research who want to advance their decision-making capabilities under uncertainty concerning data-driven optimization. Then, a comprehensive review and classification of the relevant publications on the data-driven stochastic program, data-driven robust optimization, and data-driven chance-constrained are presented. This chapter also identifies fertile avenues for future research that focus on deep-data-driven optimization, deep data-driven models, as well as online learning-based data-driven optimization. Perspectives on reinforcement learning (RL)-based data-driven optimization and deep RL for solving NP-hard problems are discussed. We investigate the application of data-driven optimization in different case studies to demonstrate improvements in operational performance over conventional optimization methodology. Finally, some managerial implications and some future directions are provided.","",""
11,"D. Almeida, Konstantin Shmarko, Elizabeth Lomas","The ethics of facial recognition technologies, surveillance, and accountability in an age of artificial intelligence: a comparative analysis of US, EU, and UK regulatory frameworks",2021,"","","","",193,"2022-07-13 09:19:31","","10.1007/s43681-021-00077-w","","",,,,,11,11.00,4,3,1,"","",""
7,"F. Arama, Slimane Laribi, T. Ghaitaoui","A Control Method using Artificial Intelligence in Wind Energy Conversion System",2019,"","","","",194,"2022-07-13 09:19:31","","10.46657/ajresd.2019.1.1.6","","",,,,,7,2.33,2,3,3,"This work presents a field-oriented control (FOC) of active and reactive power applied on Doubly Fed Induction Machine (DFIM) integrated in wind energy conversion system (WECS). The main objective of this work is to compare the performances of energy produced by the use of two types of controllers ( PI regulator and the neural network regulator (NN)) in order to control the wind power conversion system to compare their precision & robustness against the wind fluctuation and the impact on the quality of produced energy. A field oriented control of DEFIG stator is also presented to control the active and reactive power. To show the efficiency of the performances and the robustness of the two control methods those were analyzed and compared by simulation using Matlab/Simulink software. The results described the favoured method.","",""
4,"A. Walz, Kay Firth-Butterfield","Implementing ethics into artificial intelligence: a contribution, from a legal perspective, to the development of an AI governance regime",2019,"","","","",195,"2022-07-13 09:19:31","","","","",,,,,4,1.33,2,2,3,"The increasing use of AI and autonomous systems will have revolutionary impacts on society. Despite many benefits, AI and autonomous systems involve considerable risks that need to be managed. Minimizing these risks will emphasize the respective benefits while at the same time protecting the ethical values defined by fundamental rights and basic constitutional principles, thereby preserving a human centric society. This Article advocates for the need to conduct in-depth risk-benefit-assessments with regard to the use of AI and autonomous systems. This Article points out major concerns in relation to AI and autonomous systems such as likely job losses, causation of damages, lack of transparency, increasing loss of humanity in social relationships, loss of privacy and personal autonomy, potential information biases and the error proneness, and susceptibility to manipulation of AI and autonomous systems. This critical analysis aims to raise awareness on the side of policy-makers to sufficiently address these concerns and design an appropriate AI governance regime with a focus on the preservation of a human-centric society. Raising awareness for eventual risks and concerns should, however, not be misunderstood as an anti-innovative approach. Rather, it is necessary to consider risks and concerns adequately and sufficiently in order to make sure that new technologies such as AI and autonomous systems are constructed and operate in a way which is acceptable for individual users and society as a whole. To this end, this article develops a graded governance model for the implementation of ethical concerns in AI systems reflecting the often-misjudged fact that, actually, there is a variety of policy-making instruments which policy-makers can make use of. In particular, ethical concerns do not only need to be addressed by legislation or international conventions. Depending on the ethical concern at hand, alternative regulatory measures such as technical standardization or certification may even be preferable. To illustrate the practical impact of this graded governance model for the implementation of ethical concerns in AI systems, two concrete global approaches are presented herein, in addition, which regulators, † District Attorney, Public Prosecutor’s Office Traunstein, former Senior Research Fellow, Max Planck Institute for Innovation and Competition, Munich †† Head of Artificial Intelligence and Machine Learning, World Economic Forum, San Francisco","",""
50,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello","Artificial Intelligence, Algorithmic Pricing, and Collusion",2020,"","","","",196,"2022-07-13 09:19:31","","10.1257/AER.20190623","","",,,,,50,25.00,13,4,2,"Increasingly, pricing algorithms are supplanting human decision making in real marketplaces. To inform the competition policy debate on the possible consequences of this development, we experiment with pricing algorithms powered by Artificial Intelligence (AI) in controlled environments (computer simulations), studying the interaction among a number of Q-learning algorithms in a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. In this setting the algorithms consistently learn to charge supra-competitive prices, without communicating with one another. The high prices are sustained by classical collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.","",""
43,"M. González-Rivero, Oscar Beijbom, A. Rodriguez-Ramirez, D. Bryant, A. Ganase, Y. González-Marrero, A. Herrera-Reveles, E. Kennedy, Catherine J. S. Kim, S. Lopez-Marcano, Kathryn Markey, B. Neal, K. Osborne, C. Reyes-Nivia, E. Sampayo, Kristin Stolberg, Abbie Taylor, J. Vercelloni, Mathew Wyatt, O. Hoegh‐Guldberg","Monitoring of Coral Reefs Using Artificial Intelligence: A Feasible and Cost-Effective Approach",2020,"","","","",197,"2022-07-13 09:19:31","","10.3390/rs12030489","","",,,,,43,21.50,4,20,2,"Ecosystem monitoring is central to effective management, where rapid reporting is essential to provide timely advice. While digital imagery has greatly improved the speed of underwater data collection for monitoring benthic communities, image analysis remains a bottleneck in reporting observations. In recent years, a rapid evolution of artificial intelligence in image recognition has been evident in its broad applications in modern society, offering new opportunities for increasing the capabilities of coral reef monitoring. Here, we evaluated the performance of Deep Learning Convolutional Neural Networks for automated image analysis, using a global coral reef monitoring dataset. The study demonstrates the advantages of automated image analysis for coral reef monitoring in terms of error and repeatability of benthic abundance estimations, as well as cost and benefit. We found unbiased and high agreement between expert and automated observations (97%). Repeated surveys and comparisons against existing monitoring programs also show that automated estimation of benthic composition is equally robust in detecting change and ensuring the continuity of existing monitoring data. Using this automated approach, data analysis and reporting can be accelerated by at least 200x and at a fraction of the cost (1%). Combining commonly used underwater imagery in monitoring with automated image annotation can dramatically improve how we measure and monitor coral reefs worldwide, particularly in terms of allocating limited resources, rapid reporting and data integration within and across management areas.","",""
38,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases.",2020,"","","","",198,"2022-07-13 09:19:31","","10.1038/s41746-020-0229-3","","",,,,,38,19.00,6,6,2,"","",""
45,"Avishek Choudhury, Onur Asan","Role of Artificial Intelligence in Patient Safety Outcomes: Systematic Literature Review",2020,"","","","",199,"2022-07-13 09:19:31","","10.2196/18599","","",,,,,45,22.50,23,2,2,"Background Artificial intelligence (AI) provides opportunities to identify the health risks of patients and thus influence patient safety outcomes. Objective The purpose of this systematic literature review was to identify and analyze quantitative studies utilizing or integrating AI to address and report clinical-level patient safety outcomes. Methods We restricted our search to the PubMed, PubMed Central, and Web of Science databases to retrieve research articles published in English between January 2009 and August 2019. We focused on quantitative studies that reported positive, negative, or intermediate changes in patient safety outcomes using AI apps, specifically those based on machine-learning algorithms and natural language processing. Quantitative studies reporting only AI performance but not its influence on patient safety outcomes were excluded from further review. Results We identified 53 eligible studies, which were summarized concerning their patient safety subcategories, the most frequently used AI, and reported performance metrics. Recognized safety subcategories were clinical alarms (n=9; mainly based on decision tree models), clinical reports (n=21; based on support vector machine models), and drug safety (n=23; mainly based on decision tree models). Analysis of these 53 studies also identified two essential findings: (1) the lack of a standardized benchmark and (2) heterogeneity in AI reporting. Conclusions This systematic review indicates that AI-enabled decision support systems, when implemented correctly, can aid in enhancing patient safety by improving error detection, patient stratification, and drug management. Future work is still needed for robust validation of these systems in prospective and real-world clinical environments to understand how well AI can predict safety outcomes in health care settings.","",""
37,"Z. Yaseen, Z. H. Ali, Sinan Q. Salih, N. Al‐Ansari","Prediction of Risk Delay in Construction Projects Using a Hybrid Artificial Intelligence Model",2020,"","","","",200,"2022-07-13 09:19:31","","10.3390/su12041514","","",,,,,37,18.50,9,4,2,"Project delays are the major problems tackled by the construction sector owing to the associated complexity and uncertainty in the construction activities. Artificial Intelligence (AI) models have evidenced their capacity to solve dynamic, uncertain and complex tasks. The aim of this current study is to develop a hybrid artificial intelligence model called integrative Random Forest classifier with Genetic Algorithm optimization (RF-GA) for delay problem prediction. At first, related sources and factors of delay problems are identified. A questionnaire is adopted to quantify the impact of delay sources on project performance. The developed hybrid model is trained using the collected data of the previous construction projects. The proposed RF-GA is validated against the classical version of an RF model using statistical performance measure indices. The achieved results of the developed hybrid RF-GA model revealed a good resultant performance in terms of accuracy, kappa and classification error. Based on the measured accuracy, kappa and classification error, RF-GA attained 91.67%, 87% and 8.33%, respectively. Overall, the proposed methodology indicated a robust and reliable technique for project delay prediction that is contributing to the construction project management monitoring and sustainability.","",""
