Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
1,"W. Waegeman, B. Baets, L. Boullart","Integrating Expert Knowledge into Kernel-based Preference Models",2008,"","","","",1,"2022-07-13 09:25:27","","","","",,,,,1,0.07,0,3,14,"In multi-criteria decision making (MCDM) and fuzzy modeling, preference models are typically constructed by interacting with the human decision maker (DM). When the DM experiences difficulties to specify precise for all parameters of the model, inference and elicitation procedures can assist him/her to find a satisfactory model and to assess unlabelled examples. In a related but more statistical way, machine learning algorithms can also infer preference models with similar setups and purposes, but here less interaction with the DM is integrated. We present a hybrid approach that combines the best of both worlds. It consists of a general kernel-based framework for constructing and inferring preference models, in which expert knowledge can be included. Additive models, for which interpretability is preserved, and utility models can be considered as special cases. Besides generality, important benefits of this approach are its robustness to noise and good scalability. We show in detail how this framework can be utilized to aggregate single-criterion outranking relations, resulting in a flexible class of preference models for which domain knowledge can be specified by a DM.","",""
1,"Dan Kerrigan, J. Hullman, E. Bertini","A Survey of Domain Knowledge Elicitation in Applied Machine Learning",2021,"","","","",2,"2022-07-13 09:25:27","","10.3390/mti5120073","","",,,,,1,1.00,0,3,1,"Eliciting knowledge from domain experts can play an important role throughout the machine learning process, from correctly specifying the task to evaluating model results. However, knowledge elicitation is also fraught with challenges. In this work, we consider why and how machine learning researchers elicit knowledge from experts in the model development process. We develop a taxonomy to characterize elicitation approaches according to the elicitation goal, elicitation target, elicitation process, and use of elicited knowledge. We analyze the elicitation trends observed in 28 papers with this taxonomy and identify opportunities for adding rigor to these elicitation approaches. We suggest future directions for research in elicitation for machine learning by highlighting avenues for further exploration and drawing on what we can learn from elicitation research in other fields.","",""
8,"Dongqi Han, Zhiliang Wang, Ying Zhong, Wenqi Chen, Jiahai Yang, Shuqiang Lu, Xingang Shi, Xia Yin","Evaluating and Improving Adversarial Robustness of Machine Learning-Based Network Intrusion Detectors",2020,"","","","",3,"2022-07-13 09:25:27","","10.1109/JSAC.2021.3087242","","",,,,,8,4.00,1,8,2,"Machine learning (ML), especially deep learning (DL) techniques have been increasingly used in anomaly-based network intrusion detection systems (NIDS). However, ML/DL has shown to be extremely vulnerable to adversarial attacks, especially in such security-sensitive systems. Many adversarial attacks have been proposed to evaluate the robustness of ML-based NIDSs. Unfortunately, existing attacks mostly focused on feature-space and/or white-box attacks, which make impractical assumptions in real-world scenarios, leaving the study on practical gray/black-box attacks largely unexplored. To bridge this gap, we conduct the first systematic study of the gray/black-box traffic-space adversarial attacks to evaluate the robustness of ML-based NIDSs. Our work outperforms previous ones in the following aspects: (i) practical —the proposed attack can automatically mutate original traffic with extremely limited knowledge and affordable overhead while preserving its functionality; (ii) generic —the proposed attack is effective for evaluating the robustness of various NIDSs using diverse ML/DL models and non-payload-based features; (iii) explainable —we propose an explanation method for the fragile robustness of ML-based NIDSs. Based on this, we also propose a defense scheme against adversarial attacks to improve system robustness. We extensively evaluate the robustness of various NIDSs using diverse feature sets and ML/DL models. Experimental results show our attack is effective (e.g., >97% evasion rate in half cases for Kitsune, a state-of-the-art NIDS) with affordable execution cost and the proposed defense method can effectively mitigate such attacks (evasion rate is reduced by >50% in most cases).","",""
1,"Minh-Thang Nguyen, Sungoh Kwon","Machine Learning–Based Mobility Robustness Optimization Under Dynamic Cellular Networks",2021,"","","","",4,"2022-07-13 09:25:27","","10.1109/ACCESS.2021.3083554","","",,,,,1,1.00,1,2,1,"In this paper, we propose a machine learning−based mobility robustness optimization algorithm to optimize handover parameters for seamless mobility under dynamic small-cell networks. Small cells can be arbitrarily deployed, portable, and turned on and off to fulfill wireless traffic demands or energy efficiency. As a result, the small-cell network topology dynamically varies challenging network optimization, especially handover optimization. Previous studies have only considered dynamics due to user mobility in a specific static network topology. To optimize handovers under dynamic network topologies, together with user mobility, we propose an algorithm consisting of two steps: topology adaptation and mobility adaptation. To adapt to a dynamic topology, the algorithm obtains prior knowledge, which presents a belief distribution of the optimal handover parameters, for the current network topology as coarse optimization. In the second step, the algorithm fine-tunes the handover parameters to adapt to user mobility based on reinforcement learning, which utilizes the knowledge obtained during the first step. Under a dynamic small-cell network, we showed that the proposed algorithm reduced adaptation time to 4.17% of the time needed by a comparative machine–based algorithm. Furthermore, the proposed algorithm improved the user satisfaction rate to 416.7% compared to the previous work.","",""
19,"Yannick Suter, U. Knecht, Mariana Alão, W. Valenzuela, E. Hewer, P. Schucht, R. Wiest, M. Reyes","Radiomics for glioblastoma survival analysis in pre-operative MRI: exploring feature robustness, class boundaries, and machine learning techniques",2020,"","","","",5,"2022-07-13 09:25:27","","10.1186/s40644-020-00329-8","","",,,,,19,9.50,2,8,2,"","",""
2,"Lu Yin, Vlado Menkovski, Mykola Pechenizkiy","Knowledge Elicitation using Deep Metric Learning and Psychometric Testing",2020,"","","","",6,"2022-07-13 09:25:27","","10.1007/978-3-030-67661-2_10","","",,,,,2,1.00,1,3,2,"","",""
1,"Lu Yin","Beyond Labels: Knowledge Elicitation using Deep Metric Learning and Psychometric Testing",2020,"","","","",7,"2022-07-13 09:25:27","","","","",,,,,1,0.50,1,1,2,"Knowledge present in a domain is well expressed as relationships between corresponding concepts. For example, in zoology, animal species form complex hierarchies; in genomics, the different (parts of) molecules are organized in groups and subgroups based on their functions; plants, molecules, and astronomical objects all form complex taxonomies. Nevertheless, when applying supervised machine learning (ML) in such domains, we commonly reduce the complex and rich knowledge to a fixed set of labels. This oversimplifies and limits the potential impact that the ML solution can deliver. The main reason for such a reductionist approach is the difficulty in eliciting the domain knowledge from the experts. Developing a label structure with sufficient fidelity and providing comprehensive multilabel annotation can be exceedingly labor-intensive in many real-world applications. Here, we provide a method for efficient hierarchical knowledge elicitation (HKE) from experts working with highdimensional data such as images or videos. Our method is based on psychometric testing and active deep metric learning. The developed models embed the high-dimensional data in a metric space where distances are semantically meaningful, and the data can be organized in a hierarchical structure.","",""
5,"Carl Wilhjelm, Awad A. Younis","A Threat Analysis Methodology for Security Requirements Elicitation in Machine Learning Based Systems",2020,"","","","",8,"2022-07-13 09:25:27","","10.1109/QRS-C51114.2020.00078","","",,,,,5,2.50,3,2,2,"Machine learning (ML) models are now a key component for many applications. However, machine learning based systems (MLBSs), those systems that incorporate them, have proven vulnerable to various new attacks as a result. Currently, there exists no systematic process for eliciting security requirements for MLBSs that incorporates the identification of adversarial machine learning (AML) threats with those of a traditional non-MLBS. In this research study, we explore the applicability of traditional threat modeling and existing attack libraries in addressing MLBS security in the requirements phase. Using an example MLBS, we examined the applicability of 1) DFD and STRIDE in enumerating AML threats; 2) Microsoft SDL AI/ML Bug Bar in ranking the impact of the identified threats; and 3) the Microsoft AML attack library in eliciting threat mitigations to MLBSs. Such a method has the potential to assist team members, even with only domain specific knowledge, to collaboratively mitigate MLBS threats.","",""
5,"Zhuolin Yang, Zhikuan Zhao, Hengzhi Pei, Boxin Wang, Bojan Karlas, Ji Liu, Heng Guo, Bo Li, Ce Zhang","End-to-end Robustness for Sensing-Reasoning Machine Learning Pipelines",2020,"","","","",9,"2022-07-13 09:25:27","","","","",,,,,5,2.50,1,9,2,"As machine learning (ML) being applied to many mission-critical scenarios, certifying ML model robustness becomes increasingly important. Many previous works focuses on the robustness of independent ML and ensemble models, and can only certify a very small magnitude of the adversarial perturbation. In this paper, we take a different viewpoint and improve learning robustness by going beyond independent ML and ensemble models. We aim at promoting the generic Sensing-Reasoning machine learning pipeline which contains both the sensing (e.g. deep neural networks) and reasoning (e.g. Markov logic networks (MLN)) components enriched with domain knowledge. Can domain knowledge help improve learning robustness? Can we formally certify the end-to-end robustness of such an ML pipeline?  We first theoretically analyze the computational complexity of checking the provable robustness in the reasoning component. We then derive the provable robustness bound for several concrete reasoning components. We show that for reasoning components such as MLN and a specific family of Bayesian networks it is possible to certify the robustness of the whole pipeline even with a large magnitude of perturbation which cannot be certified by existing work. Finally, we conduct extensive real-world experiments on large scale datasets to evaluate the certified robustness for Sensing-Reasoning ML pipelines.","",""
62,"Markus Borg, Cristofer Englund, K. Wnuk, B. Durán, Christoffer Levandowski, Shenjian Gao, Yanwen Tan, Henrik Kaijser, Henrik Lönn, J. Törnqvist","Safely Entering the Deep: A Review of Verification and Validation for Machine Learning and a Challenge Elicitation in the Automotive Industry",2018,"","","","",10,"2022-07-13 09:25:27","","10.2991/JASE.D.190131.001","","",,,,,62,15.50,6,10,4,"Deep Neural Networks (DNN) will emerge as a cornerstone in automotive software engineering. However, developing systems with DNNs introduces novel challenges for safety assessments. This paper reviews the state-of-the-art in verification and validation of safety-critical systems that rely on machine learning. Furthermore, we report from a workshop series on DNNs for perception with automotive experts in Sweden, confirming that ISO 26262 largely contravenes the nature of DNNs. We recommend aerospace-to-automotive knowledge transfer and systems-based safety approaches, e.g., safety cage architectures and simulated system test cases.","",""
6,"P. Paokanta","β-Thalassemia Knowledge Elicitation Using Data Engineering: PCA, Pearson's Chi Square and Machine Learning",2012,"","","","",11,"2022-07-13 09:25:27","","10.7763/IJCTE.2012.V4.561","","",,,,,6,0.60,6,1,10,"Data Engineering is one of the Knowledge Elicitation and Analysis methods, among serveral techniques; Feature Selection methods play an important role for these processes which are the processes in data mining technique esspecially classification tasks. The filtering process is an important pre-treatment for every classification process. Not only decreasing the computational time and cost, but selecting an appropriate variable is increasing the classification accuracy also. In this paper, the Thalassemia knowledge was elicited using Data engineering techniques (PCA, Pearson's Chi square and Machine Learning). This knowledge presented in form of the comparison of classification performance of machine learning techniques between using Principal Components Analysis (PCA) and Pearson's Chi square for screening the genotypes of β-Thalassemia patients. According to using PCA, the classification results show that the Multi-Layer Perceptron (MLP) is the best algorithm, providing that the percentage of accuracy reaches 86.61, K- Nearest Neighbors (KNN), NaiveBayes, Bayesian Networks (BNs) and Multinomial Logistic Regression with the percentage of accuracy 85.83, 85.04, 85.04 and 82.68. On the other hand, these results were compared to the Pearson's Chi Square and presented that…. In the future, we will search for the other feature selection techniques in order to improve the classification performance such as the hybrid method, filtering mathod etc.","",""
1,"Hatim M. Elhassan Ibrahim, Nazir Ahmad, M. Rehman, Iqrar Ahmad, Rizwan Khan","Implementing and Automating Elicitation Technique Selection using Machine Learning",2019,"","","","",12,"2022-07-13 09:25:27","","10.1109/ICCIKE47802.2019.9004398","","",,,,,1,0.33,0,5,3,"Technique selection is one of the frequent issues in the requirement elicitation process; this issue has a major impact on the final requirement report output. The inappropriate selection of the techniques could lead to improper requirements and thus increase the risk of failure for the intended project. This paper addresses the technique selection issue encountered during the requirements elicitation stage, through a proposed a machine learning model to transfer the experts’ knowledge of elicitation technique selection of the less experienced. Based on the system analysts, stakeholders and technique properties as such systems and automate the technique selection process to provide the best optimization technique nomination, for the elicitation case complexity characteristics.","",""
5,"Ibrahim Yazici, Ömer Faruk Beyca, O. F. Gurcan, Halil Zaim, D. Delen, S. Zaim","A comparative analysis of machine learning techniques and fuzzy analytic hierarchy process to determine the tacit knowledge criteria",2020,"","","","",13,"2022-07-13 09:25:27","","10.1007/s10479-020-03697-3","","",,,,,5,2.50,1,6,2,"","",""
8,"Sakshi Udeshi, Sudipta Chattopadhyay","Grammar Based Directed Testing of Machine Learning Systems",2019,"","","","",14,"2022-07-13 09:25:27","","10.1109/tse.2019.2953066","","",,,,,8,2.67,4,2,3,"The massive progress of machine learning has seen its application over a variety of domains in the past decade. But how do we develop a systematic, scalable and modular strategy to validate machine-learning systems? We present, to the best of our knowledge, the first approach, which provides a systematic test framework for machine-learning systems that accepts grammar-based inputs. Our Ogma approach automatically discovers erroneous behaviours in classifiers and leverages these erroneous behaviours to improve the respective models. Ogma leverages inherent robustness properties present in any well trained machine-learning model to direct test generation and thus, implementing a scalable test generation methodology. To evaluate our Ogma approach, we have tested it on three real world natural language processing (NLP) classifiers. We have found thousands of erroneous behaviours in these systems. We also compare Ogma with a random test generation approach and observe that Ogma is more effective than such random test generation by up to 489 percent.","",""
29,"Nishat Koti, Mahak Pancholi, A. Patra, A. Suresh","SWIFT: Super-fast and Robust Privacy-Preserving Machine Learning",2020,"","","","",15,"2022-07-13 09:25:27","","","","",,,,,29,14.50,7,4,2,"Performing ML computation on private data while maintaining data privacy aka Privacy-preserving Machine Learning (PPML) is an emergent field of research. Recently, PPML has seen a visible shift towards the adoption of Secure Outsourced Computation (SOC) paradigm, due to the heavy computation that it entails. In the SOC paradigm, computation is outsourced to a set of powerful and specially equipped servers that provide service on a pay-per-use basis. In this work, we propose SWIFT, a robust PPML framework for a range of ML algorithms in SOC setting, that guarantees output delivery to the users irrespective of any adversarial behaviour. Robustness, a highly desirable feature, evokes user participation without the fear of denial of service.  At the heart of our framework lies a highly-efficient, maliciously-secure, three-party computation (3PC) over rings that provides guaranteed output delivery (GOD) in the honest-majority setting. To the best of our knowledge, SWIFT is the first robust and efficient PPML framework in the 3PC setting. SWIFT is as fast as the best-known 3PC framework BLAZE (Patra et al. NDSS'20) which only achieves fairness. Fairness ensures either all or none receive the output, whereas GOD ensures guaranteed output delivery no matter what. We extend our 3PC framework for four parties (4PC). In this regime, SWIFT is as fast as the best known fair 4PC framework Trident (Chaudhari et al. NDSS'20) and twice faster than the best-known robust 4PC framework FLASH (Byali et al. PETS'20).  We demonstrate the practical relevance of our framework by benchmarking two important applications-- i) ML algorithms: Logistic Regression and Neural Network, and ii) Biometric matching, both over a 64-bit ring in WAN setting. Our readings reflect our claims as above.","",""
6,"F. Sufi, M. Alsulami","Knowledge Discovery of Global Landslides Using Automated Machine Learning Algorithms",2021,"","","","",16,"2022-07-13 09:25:27","","10.1109/ACCESS.2021.3115043","","",,,,,6,6.00,3,2,1,"Understanding the complex dynamics of global landslides is essential for disaster planners to make timely and effective decisions that save lives and reduce the economic impacts on society. Using NASA’s inventory of global landslide data, we developed a new machine learning (ML)–based system for town planners, disaster recovery strategists, and landslide researchers. Our system revealed hidden knowledge about a range of complex scenarios created from five landslide feature attributes. Users of our system can select from a list of $1.295\times {10}^{64}$ possible global landslide scenarios to discover valuable knowledge and predictions about the selected scenario in an interactive manner. Three ML algorithms—anomaly detection, decomposition analysis, and automated regression analysis—are used to elicit detailed knowledge about 25 scenarios selected from 14,532 global landslide records covering 12,220 injuries and 63,573 fatalities across 157 countries. Anomaly detection, logistic regression, and decomposition analysis performed well for all scenarios under study, with the area under the curve averaging 0.951, 0.911, and 0.896, respectively. Moreover, the prediction accuracy of linear regression had a mean absolute percentage error of 0.255. To the best of our knowledge, our scenario-based ML knowledge discovery system is the first of its kind to provide a comprehensive understanding of global landslide data.","",""
11,"S. Tripathi, David Muhr, Manuel Brunner, F. Emmert‐Streib, H. Jodlbauer, M. Dehmer","Ensuring the Robustness and Reliability of Data-Driven Knowledge Discovery Models in Production and Manufacturing",2020,"","","","",17,"2022-07-13 09:25:27","","10.3389/frai.2021.576892","","",,,,,11,5.50,2,6,2,"The Cross-Industry Standard Process for Data Mining (CRISP-DM) is a widely accepted framework in production and manufacturing. This data-driven knowledge discovery framework provides an orderly partition of the often complex data mining processes to ensure a practical implementation of data analytics and machine learning models. However, the practical application of robust industry-specific data-driven knowledge discovery models faces multiple data- and model development-related issues. These issues need to be carefully addressed by allowing a flexible, customized and industry-specific knowledge discovery framework. For this reason, extensions of CRISP-DM are needed. In this paper, we provide a detailed review of CRISP-DM and summarize extensions of this model into a novel framework we call Generalized Cross-Industry Standard Process for Data Science (GCRISP-DS). This framework is designed to allow dynamic interactions between different phases to adequately address data- and model-related issues for achieving robustness. Furthermore, it emphasizes also the need for a detailed business understanding and the interdependencies with the developed models and data quality for fulfilling higher business objectives. Overall, such a customizable GCRISP-DS framework provides an enhancement for model improvements and reusability by minimizing robustness-issues.","",""
161,"A. Bhagoji, Daniel Cullina, Chawin Sitawarin, Prateek Mittal","Enhancing robustness of machine learning systems via data transformations",2017,"","","","",18,"2022-07-13 09:25:27","","10.1109/CISS.2018.8362326","","",,,,,161,32.20,40,4,5,"We propose the use of data transformations as a defense against evasion attacks on ML classifiers. We present and investigate strategies for incorporating a variety of data transformations including dimensionality reduction via Principal Component Analysis to enhance the resilience of machine learning, targeting both the classification and the training phase. We empirically evaluate and demonstrate the feasibility of linear transformations of data as a defense mechanism against evasion attacks using multiple real-world datasets. Our key findings are that the defense is (i) effective against the best known evasion attacks from the literature, resulting in a two-fold increase in the resources required by a white-box adversary with knowledge of the defense for a successful attack, (ii) applicable across a range of ML classifiers, including Support Vector Machines and Deep Neural Networks, and (iii) generalizable to multiple application domains, including image classification and human activity classification.","",""
38,"M. Yoosefzadeh-Najafabadi, H. Earl, D. Tulpan, J. Sulik, M. Eskandari","Application of Machine Learning Algorithms in Plant Breeding: Predicting Yield From Hyperspectral Reflectance in Soybean",2021,"","","","",19,"2022-07-13 09:25:27","","10.3389/fpls.2020.624273","","",,,,,38,38.00,8,5,1,"Recent substantial advances in high-throughput field phenotyping have provided plant breeders with affordable and efficient tools for evaluating a large number of genotypes for important agronomic traits at early growth stages. Nevertheless, the implementation of large datasets generated by high-throughput phenotyping tools such as hyperspectral reflectance in cultivar development programs is still challenging due to the essential need for intensive knowledge in computational and statistical analyses. In this study, the robustness of three common machine learning (ML) algorithms, multilayer perceptron (MLP), support vector machine (SVM), and random forest (RF), were evaluated for predicting soybean (Glycine max) seed yield using hyperspectral reflectance. For this aim, the hyperspectral reflectance data for the whole spectra ranged from 395 to 1005 nm, which were collected at the R4 and R5 growth stages on 250 soybean genotypes grown in four environments. The recursive feature elimination (RFE) approach was performed to reduce the dimensionality of the hyperspectral reflectance data and select variables with the largest importance values. The results indicated that R5 is more informative stage for measuring hyperspectral reflectance to predict seed yields. The 395 nm reflectance band was also identified as the high ranked band in predicting the soybean seed yield. By considering either full or selected variables as the input variables, the ML algorithms were evaluated individually and combined-version using the ensemble–stacking (E–S) method to predict the soybean yield. The RF algorithm had the highest performance with a value of 84% yield classification accuracy among all the individual tested algorithms. Therefore, by selecting RF as the metaClassifier for E–S method, the prediction accuracy increased to 0.93, using all variables, and 0.87, using selected variables showing the success of using E–S as one of the ensemble techniques. This study demonstrated that soybean breeders could implement E–S algorithm using either the full or selected spectra reflectance to select the high-yielding soybean genotypes, among a large number of genotypes, at early growth stages.","",""
46,"F. Yuan, S. A. Zargar, Qiuyi Chen, Shaohan Wang","Machine learning for structural health monitoring: challenges and opportunities",2020,"","","","",20,"2022-07-13 09:25:27","","10.1117/12.2561610","","",,,,,46,23.00,12,4,2,"A physics-based approach to structural health monitoring (SHM) has practical shortcomings which restrict its suitability to simple structures under well controlled environments. With the advances in information and sensing technology (sensors and sensor networks), it has become feasible to monitor large/diverse number of parameters in complex real-world structures either continuously or intermittently by employing large in-situ (wireless) sensor networks. The availability of this historical data has engendered a lot of interest in a data-driven approach as a natural and more viable option for realizing the goal of SHM in such structures. However, the lack of sensor data corresponding to different damage scenarios continues to remain a challenge. Most of the supervised machine-learning/deep-learning techniques, when trained using this inherently limited data, lack robustness and generalizability. Physics-informed learning, which involves the integration of domain knowledge into the learning process, is presented here as a potential remedy to this challenge. As a step towards the goal of automated damage detection (mathematically an inverse problem), preliminary results are presented from dynamic modelling of beam structures using physics-informed artificial neural networks. Forward and inverse problems involving partial differential equations are solved and comparisons reveal a clear superiority of physics-informed approach over one that is purely datadriven vis-à-vis overfitting/generalization. Other ways of incorporating domain knowledge into the machine learning pipeline are then presented through case-studies on various aspects of NDI/SHM (visual inspection, impact diagnosis). Lastly, as the final attribute of an optimal SHM approach, a sensing paradigm for non-contact full-field measurements for damage diagnosis is presented.","",""
3,"H. Vardhan, P. Völgyesi, J. Sztipanovits","Machine learning assisted propeller design",2021,"","","","",21,"2022-07-13 09:25:27","","10.1145/3450267.3452001","","",,,,,3,3.00,1,3,1,"Propellers are one of the most widely used propulsive devices for generating thrust from rotational engine motion both in marine vehicles and subsonic air-crafts. Due to their simplicity, robustness and high efficiency, propellers remained the mainstream design choice over the last hundred years. On the other hand, finding the optimal application-specific geometry is still challenging. This work in progress report describes application of modern and rapidly developing Machine Learning (ML) techniques to gain novel designs. We rely on a rich set of preexisting parametric design patterns and accumulated engineering knowledge supplemented by high-fidelity simulation models to formulate the design process as a supervised learning problem. The aim of our work is to develop and evaluate machine learning models for the parametric design of propellers based on application-specific constraints. While the application of ML techniques in optimal propeller design is at a very nascent level, we believe that our early results are promising with a potentially significant impact on the overall design process. The ML-assisted design flow allows for a more automated design space exploration process with less dependency on human intuition and engineering guidance.","",""
1,"M. Staron, Helena Odenstedt Herg'es, S. Naredi, L. Block, Ali El-Merhi, Richard Vithal, M. Elam","Robust Machine Learning in Critical Care — Software Engineering and Medical Perspectives",2021,"","","","",22,"2022-07-13 09:25:27","","10.1109/WAIN52551.2021.00016","","",,,,,1,1.00,0,7,1,"Using machine learning in clinical practice poses hard requirements on explainability, reliability, replicability and robustness of these systems. Therefore, developing reliable software for monitoring critically ill patients requires close collaboration between physicians and software engineers. However, these two different disciplines need to find own research perspectives in order to contribute to both the medical and the software engineering domain. In this paper, we address the problem of how to establish a collaboration where software engineering and medicine meets to design robust machine learning systems to be used in patient care. We describe how we designed software systems for monitoring patients under carotid endarterectomy, in particular focusing on the process of knowledge building in the research team. Our results show what to consider when setting up such a collaboration, how it develops over time and what kind of systems can be constructed based on it. We conclude that the main challenge is to find a good research team, where different competences are committed to a common goal.","",""
5,"Katharina Beckh, Sebastian Muller, Matthias Jakobs, Vanessa Toborek, Hanxiao Tan, Raphael Fischer, Pascal Welke, Sebastian Houben, Laura von Rueden","Explainable Machine Learning with Prior Knowledge: An Overview",2021,"","","","",23,"2022-07-13 09:25:27","","","","",,,,,5,5.00,1,9,1,"This survey presents an overview of integrating prior knowledge into machine learning systems in order to improve explainability. The complexity of machine learning models has elicited research to make them more explainable. However, most explainability methods cannot provide insight beyond the given data, requiring additional information about the context. We propose to harness prior knowledge to improve upon the explanation capabilities of machine learning models. In this paper, we present a categorization of current research into three main categories which either integrate knowledge into the machine learning pipeline, into the explainability method or derive knowledge from explanations. To classify the papers, we build upon the existing taxonomy of informed machine learning and extend it from the perspective of explainability. We conclude with open challenges and research directions.","",""
3,"Suk Joon Hong, B. Bennett","Tackling Domain-Specific Winograd Schemas with Knowledge-Based Reasoning and Machine Learning",2020,"","","","",24,"2022-07-13 09:25:27","","10.4230/OASIcs.LDK.2021.41","","",,,,,3,1.50,2,2,2,"The Winograd Schema Challenge (WSC) is a common-sense reasoning task that requires background knowledge. In this paper, we contribute to tackling WSC in four ways. Firstly, we suggest a keyword method to define a restricted domain where distinctive high-level semantic patterns can be found. A thanking domain was defined by key-words, and the data set in this domain is used in our experiments. Secondly, we develop a high-level knowledge-based reasoning method using semantic roles which is based on the method of Sharma [2019]. Thirdly, we propose an ensemble method to combine knowledge-based reasoning and machine learning which shows the best performance in our experiments. As a machine learning method, we used Bidirectional Encoder Representations from Transformers (BERT) [Kocijan et al., 2019]. Lastly, in terms of evaluation, we suggest a ""robust"" accuracy measurement by modifying that of Trichelair et al. [2018]. As with their switching method, we evaluate a model by considering its performance on trivial variants of each sentence in the test set.","",""
20,"Vida Groznik, Matej Guid, A. Sadikov, M. Mozina, D. Georgiev, Veronika Kragelj, S. Ribaric, Z. Pirtošek, I. Bratko","Elicitation of neurological knowledge with argument-based machine learning",2013,"","","","",25,"2022-07-13 09:25:27","","10.1016/j.artmed.2012.08.003","","",,,,,20,2.22,2,9,9,"","",""
10,"M. Campi, S. Garatti","Scenario optimization with relaxation: a new tool for design and application to machine learning problems",2020,"","","","",26,"2022-07-13 09:25:27","","10.1109/CDC42340.2020.9303914","","",,,,,10,5.00,5,2,2,"Scenario optimization is by now a well established technique to perform designs in the presence of uncertainty. It relies on domain knowledge integrated with first-hand information that comes from data and generates solutions that are also accompanied by precise statements of reliability. In this paper, following recent developments in [22], we venture beyond the traditional set-up of scenario optimization by analyzing the concept of constraints relaxation. By a solid theoretical underpinning, this new paradigm furnishes fundamental tools to perform designs that meet a proper compromise between robustness and performance. After suitably expanding the scope of constraints relaxation as proposed in [22], we focus on various classical Support Vector methods in machine learning – including SVM (Support Vector Machine), SVR (Support Vector Regression) and SVDD (Support Vector Data Description) – and derive new results that attest the ability of these methods to generalize.","",""
2,"Yang Lou, Yaodong He, Lin Wang, K. Tsang, Guanrong Chen","Knowledge-Based Prediction of Network Controllability Robustness",2020,"","","","",27,"2022-07-13 09:25:27","","10.1109/TNNLS.2021.3071367","","",,,,,2,1.00,0,5,2,"Network controllability robustness (CR) reflects how well a networked system can maintain its controllability against destructive attacks. Its measure is quantified by a sequence of values that record the remaining controllability of the network after a sequence of node-removal or edge-removal attacks. Traditionally, the CR is determined by attack simulations, which is computationally time-consuming or even infeasible. In this article, an improved method for predicting the network CR is developed based on machine learning using a group of convolutional neural networks (CNNs). In this scheme, a number of training data generated by simulations are used to train the group of CNNs for classification and prediction, respectively. Extensive experimental studies are carried out, which demonstrate that 1) the proposed method predicts more precisely than the classical single-CNN predictor; 2) the proposed CNN-based predictor provides a better predictive measure than the traditional spectral measures and network heterogeneity.","",""
10,"Yang Liu, Jiaheng Wei","Incentives for Federated Learning: a Hypothesis Elicitation Approach",2020,"","","","",28,"2022-07-13 09:25:27","","","","",,,,,10,5.00,5,2,2,"Federated learning provides a promising paradigm for collecting machine learning models from distributed data sources without compromising users' data privacy. The success of a credible federated learning system builds on the assumption that the decentralized and self-interested users will be willing to participate to contribute their local models in a trustworthy way. However, without proper incentives, users might simply opt out the contribution cycle, or will be mis-incentivized to contribute spam/false information. This paper introduces solutions to incentivize truthful reporting of a local, user-side machine learning model for federated learning. Our results build on the literature of information elicitation, but focus on the questions of eliciting hypothesis (rather than eliciting human predictions). We provide a scoring rule based framework that incentivizes truthful reporting of local hypotheses at a Bayesian Nash Equilibrium. We study the market implementation, accuracy as well as robustness properties of our proposed solution too. We verify the effectiveness of our methods using MNIST and CIFAR-10 datasets. Particularly we show that by reporting low-quality hypotheses, users will receive decreasing scores (rewards, or payments).","",""
87,"G. Peng, M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, Savador Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Multiscale modeling meets machine learning: What can we learn?",2019,"","","","",29,"2022-07-13 09:25:27","","10.1007/s11831-020-09405-5","","",,,,,87,29.00,9,12,3,"","",""
75,"K. Kashinath, M. Mustafa, A. Albert, J.-L. Wu, C. Jiang, S. Esmaeilzadeh, K. Azizzadenesheli, R. Wang, A. Chattopadhyay, A. Singh, A. Manepalli, D. Chirila, R. Yu, R. Walters, B. White, H. Xiao, H. Tchelepi, P. Marcus, A. Anandkumar, P. Hassanzadeh, Prabhat","Physics-informed machine learning: case studies for weather and climate modelling",2021,"","","","",30,"2022-07-13 09:25:27","","10.1098/rsta.2020.0093","","",,,,,75,75.00,8,21,1,"Machine learning (ML) provides novel and powerful ways of accurately and efficiently recognizing complex patterns, emulating nonlinear dynamics, and predicting the spatio-temporal evolution of weather and climate processes. Off-the-shelf ML models, however, do not necessarily obey the fundamental governing laws of physical systems, nor do they generalize well to scenarios on which they have not been trained. We survey systematic approaches to incorporating physics and domain knowledge into ML models and distill these approaches into broad categories. Through 10 case studies, we show how these approaches have been used successfully for emulating, downscaling, and forecasting weather and climate processes. The accomplishments of these studies include greater physical consistency, reduced training time, improved data efficiency, and better generalization. Finally, we synthesize the lessons learned and identify scientific, diagnostic, computational, and resource challenges for developing truly robust and reliable physics-informed ML models for weather and climate processes. This article is part of the theme issue ‘Machine learning for weather and climate modelling’.","",""
665,"Weihua Hu, Matthias Fey, M. Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, J. Leskovec","Open Graph Benchmark: Datasets for Machine Learning on Graphs",2020,"","","","",31,"2022-07-13 09:25:27","","","","",,,,,665,332.50,83,8,2,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .","",""
9,"Phauk Sokkhey, T. Okazaki","Hybrid Machine Learning Algorithms for Predicting Academic Performance",2020,"","","","",32,"2022-07-13 09:25:27","","10.14569/ijacsa.2020.0110104","","",,,,,9,4.50,5,2,2,"The large volume of data and its complexity in educational institutions require the sakes from informative technologies. In order to facilitate this task, many researchers have focused on using machine learning to extract knowledge from the education database to support students and instructors in getting better performance. In prediction models, the challenging task is to choose the effective techniques which could produce satisfying predictive accuracy. Hence, in this work, we introduced a hybrid approach of principal component analysis (PCA) as conjunction with four machines learning (ML) algorithms: random forest (RF), C5.0 of decision tree (DT), and naive Bayes (NB) of Bayes network and support vector machine (SVM), to improve the performances of classification by solving the misclassification problem. Three datasets were used to confirm the robustness of the proposed models. Through the given datasets, we evaluated the classification accuracy and root mean square error (RSME) as evaluation metrics of the proposed models. In this classification problem, 10-fold cross-validation was proposed to evaluate the predictive performance. The proposed hybrid models produced very prediction results which shown itself as the optimal prediction and classification algorithms.","",""
21,"A. Naimi, Alan Mishler, Edward H. Kennedy","Challenges in Obtaining Valid Causal Effect Estimates with Machine Learning Algorithms.",2017,"","","","",33,"2022-07-13 09:25:27","","10.1093/aje/kwab201","","",,,,,21,4.20,7,3,5,"Unlike parametric regression, machine learning (ML) methods do not generally require precise knowledge of the true data generating mechanisms. As such, numerous authors have advocated for ML methods to estimate causal effects. Unfortunately, ML algorithmscan perform worse than parametric regression. We demonstrate the performance of ML-based single- and double-robust estimators. We use 100 Monte Carlo samples with sample sizes of 200, 1200, and 5000 to investigate bias and confidence interval coverage under several scenarios. In a simple confounding scenario, confounders were related to the treatment and the outcome via parametric models. In a complex confounding scenario, the simple confounders were transformed to induce complicated nonlinear relationships. In the simple scenario, when ML algorithms were used, double-robust estimators were superior to single-robust estimators. In the complex scenario, single-robust estimators with ML algorithms were at least as biased as estimators using misspecified parametric models. Double-robust estimators were less biased, but coverage was well below nominal. The use of sample splitting, inclusion of confounder interactions, reliance on a richly specified ML algorithm, and use of doubly robust estimators was the only explored approach that yielded negligible bias and nominal coverage. Our results suggest that ML based singly robust methods should be avoided.","",""
16,"Yonghan Jung, Jin Tian, E. Bareinboim","Estimating Identifiable Causal Effects through Double Machine Learning",2021,"","","","",34,"2022-07-13 09:25:27","","","","",,,,,16,16.00,5,3,1,"Identifying causal effects from observational data is a perva- sive challenge found throughout the empirical sciences. Very general methods have been developed to decide the identiﬁ- ability of a causal quantity from a combination of observational data and causal knowledge about the underlying sys- tem. In practice, however, there are still challenges to estimating identiﬁable causal functionals from ﬁnite samples. Re- cently, a method known as double/debiased machine learning (DML) (Chernozhukov et al. 2018) has been proposed to learn parameters leveraging modern machine learning techniques, which is both robust to model misspeciﬁcation and bias-reducing. Still, DML has only been used for causal estimation in settings when the back-door condition (also known as conditional ignorability) holds. In this paper, we develop a new, general class of estimators for any identiﬁable causal functionals that exhibit DML properties, which we name DML-ID. In particular, we introduce a complete identiﬁca- tion algorithm that returns an inﬂuence function (IF) for any identiﬁable causal functional. We then construct the DML es- timator based on the derived IF. We show that DML-ID estimators hold the key properties of debiasedness and doubly robustness. Simulation results corroborate with the theory.","",""
7,"Jonathan Fürst, Mauricio Fadel Argerich, Bin Cheng, E. Kovacs","Towards Knowledge Infusion for Robust and Transferable Machine Learning in IoT",2020,"","","","",35,"2022-07-13 09:25:27","","","","",,,,,7,3.50,2,4,2,"Machine learning (ML) applications in Internet of Things (IoT) scenarios face the issue that supervision signals, such as labeled data, are scarce and expensive to obtain. For example, it often requires a human to manually label events in a data stream by observing the same events in the real world. In addition, the performance of trained models usually depends on a specific context: (1) location, (2) time and (3) data quality. This context is not static in reality, making it hard to achieve robust and transferable machine learning for IoT systems in practice. In this paper, we address these challenges with an envisioned method that we name Knowledge Infusion. First, we present two past case studies in which we combined external knowledge with traditional data-driven machine learning in IoT scenarios to ease the supervision effort: (1) a weak-supervision approach for the IoT domain to auto-generate labels based on external knowledge (e.g., domain knowledge) encoded in simple labeling functions. Our evaluation for transport mode classification achieves a micro-F1 score of 80.2%, with only seven labeling functions, on par with a fully supervised model that relies on hand-labeled data. (2) We introduce guiding functions to Reinforcement Learning (RL) to guide the agents' decisions and experience. In initial experiments, our guided reinforcement learning achieves more than three times higher reward in the beginning of its training than an agent with no external knowledge. We use the lessons learned from these experiences to develop our vision of knowledge infusion. In knowledge infusion, we aim to automate the inclusion of knowledge from existing knowledge bases and domain experts to combine it with traditional data-driven machine learning techniques during setup/training phase, but also during the execution phase.","",""
12,"Yingxu Wang, Omar A. Zatarain","A Novel Machine Learning Algorithm for Cognitive Concept Elicitation by Cognitive Robots",2017,"","","","",36,"2022-07-13 09:25:27","","10.4018/IJCINI.2017070103","","",,,,,12,2.40,6,2,5,"Cognitive knowledge learning (CKL) is a fundamental methodology for cognitive robots and machine learning. Traditional technologies for machine learning deal with object identification, cluster classification, pattern recognition, functional regression and behavior acquisition. A new category of CKL is presented in this paper embodied by the Algorithm of Cognitive Concept Elicitation (ACCE). Formal concepts are autonomously generated based on collective intension (attributes) and extension (objects) elicited from informal descriptions in dictionaries. A system of formal concept generation by cognitive robots is implemented based on the ACCE algorithm. Experiments on machine learning for knowledge acquisition reveal that a cognitive robot is able to learn synergized concepts in human knowledge in order to build its own knowledge base. The machine–generated knowledge base demonstrates that the ACCE algorithm can outperform human knowledge expressions in terms of relevance, accuracy, quantification and cohesiveness.","",""
27,"Nicholas Shawen, L. Lonini, C. Mummidisetty, Ilona Shparii, Mark V. Albert, Konrad Paul Kording, A. Jayaraman","Fall Detection in Individuals With Lower Limb Amputations Using Mobile Phones: Machine Learning Enhances Robustness for Real-World Applications",2017,"","","","",37,"2022-07-13 09:25:27","","10.2196/mhealth.8201","","",,,,,27,5.40,4,7,5,"Background Automatically detecting falls with mobile phones provides an opportunity for rapid response to injuries and better knowledge of what precipitated the fall and its consequences. This is beneficial for populations that are prone to falling, such as people with lower limb amputations. Prior studies have focused on fall detection in able-bodied individuals using data from a laboratory setting. Such approaches may provide a limited ability to detect falls in amputees and in real-world scenarios. Objective The aim was to develop a classifier that uses data from able-bodied individuals to detect falls in individuals with a lower limb amputation, while they freely carry the mobile phone in different locations and during free-living. Methods We obtained 861 simulated indoor and outdoor falls from 10 young control (non-amputee) individuals and 6 individuals with a lower limb amputation. In addition, we recorded a broad database of activities of daily living, including data from three participants’ free-living routines. Sensor readings (accelerometer and gyroscope) from a mobile phone were recorded as participants freely carried it in three common locations—on the waist, in a pocket, and in the hand. A set of 40 features were computed from the sensors data and four classifiers were trained and combined through stacking to detect falls. We compared the performance of two population-specific models, trained and tested on either able-bodied or amputee participants, with that of a model trained on able-bodied participants and tested on amputees. A simple threshold-based classifier was used to benchmark our machine-learning classifier. Results The accuracy of fall detection in amputees for a model trained on control individuals (sensitivity: mean 0.989, 1.96*standard error of the mean [SEM] 0.017; specificity: mean 0.968, SEM 0.025) was not statistically different (P=.69) from that of a model trained on the amputee population (sensitivity: mean 0.984, SEM 0.016; specificity: mean 0.965, SEM 0.022). Detection of falls in control individuals yielded similar results (sensitivity: mean 0.979, SEM 0.022; specificity: mean 0.991, SEM 0.012). A mean 2.2 (SD 1.7) false alarms per day were obtained when evaluating the model (vs mean 122.1, SD 166.1 based on thresholds) on data recorded as participants carried the phone during their daily routine for two or more days. Machine-learning classifiers outperformed the threshold-based one (P<.001). Conclusions A mobile phone-based fall detection model can use data from non-amputee individuals to detect falls in individuals walking with a prosthesis. We successfully detected falls when the mobile phone was carried across multiple locations and without a predetermined orientation. Furthermore, the number of false alarms yielded by the model over a longer period of time was reasonably low. This moves the application of mobile phone-based fall detection systems closer to a real-world use case scenario.","",""
0,"T. Nguyen","Layered Approximation Approach to Knowledge Elicitation in Machine Learning",2010,"","","","",38,"2022-07-13 09:25:27","","10.1007/978-3-642-13529-3_48","","",,,,,0,0.00,0,1,12,"","",""
14,"M. Vollmer, B. Glampson, T. Mellan, Swapnil Mishra, L. Mercuri, Ceire Costello, R. Klaber, G. Cooke, S. Flaxman, S. Bhatt","A unified machine learning approach to time series forecasting applied to demand at emergency departments",2020,"","","","",39,"2022-07-13 09:25:27","","10.1186/s12873-020-00395-y","","",,,,,14,7.00,1,10,2,"","",""
14,"M. Levine, A. Stuart","A Framework for Machine Learning of Model Error in Dynamical Systems",2021,"","","","",40,"2022-07-13 09:25:27","","","","",,,,,14,14.00,7,2,1,"The development of data-informed predictive models for dynamical systems is of widespread interest in many disciplines. We present a unifying framework for blending mechanistic and machine-learning approaches to identify dynamical systems from noisily and partially observed data. We compare pure data-driven learning with hybrid models which incorporate imperfect domain knowledge, referring to the discrepancy between an assumed truth model and the imperfect mechanistic model as model error. Our formulation is agnostic to the chosen machine learning model, is presented in both continuousand discrete-time settings, and is compatible both with model errors that exhibit substantial memory and errors that are memoryless. First, we study memoryless linear (w.r.t. parametric-dependence) model error from a learning theory perspective, defining excess risk and generalization error. For ergodic continuous-time systems, we prove that both excess risk and generalization error are bounded above by terms that diminish with the square-root of T , the time-interval over which training data is specified. Secondly, we study scenarios that benefit from modeling with memory, proving universal approximation theorems for two classes of continuous-time recurrent neural networks (RNNs): both can learn memory-dependent model error, assuming that it is governed by a finite-dimensional hidden variable and that, together, the observed and hidden variables form a continuous-time Markovian system. In addition, we connect one class of RNNs to reservoir computing, thereby relating learning of memory-dependent error to recent work on supervised learning between Banach spaces using random features. Numerical results are presented (Lorenz ’63, Lorenz ’96 Multiscale systems) to compare purely data-driven and hybrid approaches, finding hybrid methods less data-hungry and more parametrically efficient. We also find that, while a continuous-time framing allows for robustness to irregular sampling and desirable domain-interpretability, a discrete-time framing can provide similar or better predictive performance, especially when data are undersampled and the vector field defining the true dynamics cannot be identified. Finally, we demonstrate numerically how data assimilation can be leveraged to learn hidden dynamics from noisy, partially-observed data, and illustrate challenges in representing memory by this approach, and in the training of such models. Received by the editors July 13, 2021. 2020 Mathematics Subject Classification. Primary 68T30, 37A30, 37M10; Secondary 37M25,","",""
1920,"A. Kurakin, Ian J. Goodfellow, Samy Bengio","Adversarial Machine Learning at Scale",2016,"","","","",41,"2022-07-13 09:25:27","","","","",,,,,1920,320.00,640,3,6,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.","",""
33,"J. Schneider, J. Handali","Personalized Explanation for Machine Learning: a Conceptualization",2019,"","","","",42,"2022-07-13 09:25:27","","","","",,,,,33,11.00,17,2,3,"Explanation in machine learning and related fields such as artificial intelligence aims at making machine learning models and their decisions understandable to humans. Existing work suggests that personalizing explanations might help to improve understandability. In this work, we derive a conceptualization of personalized explanation by defining and structuring the problem based on prior work on machine learning explanation, personalization (in machine learning) and concepts and techniques from other domains such as privacy and knowledge elicitation. We perform a categorization of explainee data used in the process of personalization as well as describing means to collect this data. We also identify three key explanation properties that are amendable to personalization: complexity, decision information and presentation. We also enhance existing work on explanation by introducing additional desiderata and measures to quantify the quality of personalized explanations.","",""
22,"F. Emmert‐Streib, M. Dehmer","A Machine Learning Perspective on Personalized Medicine: An Automized, Comprehensive Knowledge Base with Ontology for Pattern Recognition",2018,"","","","",43,"2022-07-13 09:25:27","","10.3390/MAKE1010009","","",,,,,22,5.50,11,2,4,"Personalized or precision medicine is a new paradigm that holds great promise for individualized patient diagnosis, treatment, and care. However, personalized medicine has only been described on an informal level rather than through rigorous practical guidelines and statistical protocols that would allow its robust practical realization for implementation in day-to-day clinical practice. In this paper, we discuss three key factors, which we consider dimensions that effect the experimental design for personalized medicine: (I) phenotype categories; (II) population size; and (III) statistical analysis. This formalization allows us to define personalized medicine from a machine learning perspective, as an automized, comprehensive knowledge base with an ontology that performs pattern recognition of patient profiles.","",""
10,"Runzhe Zhan, Xuebo Liu, Derek F. Wong, Lidia S. Chao","Meta-Curriculum Learning for Domain Adaptation in Neural Machine Translation",2021,"","","","",44,"2022-07-13 09:25:27","","","","",,,,,10,10.00,3,4,1,"Meta-learning has been sufficiently validated to be beneficial for low-resource neural machine translation (NMT). However, we find that meta-trained NMT fails to improve the translation performance of the domain unseen at the metatraining stage. In this paper, we aim to alleviate this issue by proposing a novel meta-curriculum learning for domain adaptation in NMT. During meta-training, the NMT first learns the similar curricula from each domain to avoid falling into a bad local optimum early, and finally learns the curricula of individualities to improve the model robustness for learning domain-specific knowledge. Experimental results on 10 different low-resource domains show that meta-curriculum learning can improve the translation performance of both familiar and unfamiliar domains. All the codes and data are freely available at https://github.com/NLP2CT/ Meta-Curriculum.","",""
2,"V. Chernozhukov, W. Newey, Victor Quintas-Martinez, Vasilis Syrgkanis","RieszNet and ForestRiesz: Automatic Debiased Machine Learning with Neural Nets and Random Forests",2021,"","","","",45,"2022-07-13 09:25:27","","","","",,,,,2,2.00,1,4,1,"Many causal and policy effects of interest are deﬁned by linear functionals of high-dimensional or non-parametric regression functions. √ n consistent and asymptotically normal estimation of the object of interest requires debiasing to reduce the effects of regularization and/or model selection on the object of interest. Debiasing is typically achieved by adding a correction term to the plug-in estimator of the functional, which leads to properties such as semi-parametric efﬁ-ciency, double robustness, and Neyman orthogo-nality. We implement an automatic debiasing pro-cedure based on automatically learning the Riesz representation of the linear functional using Neural Nets and Random Forests. Our method only relies on black-box evaluation oracle access to the linear functional and does not require knowledge of its analytic form. We propose a multitasking Neural Net debiasing method with stochastic gradient descent minimization of a combined Riesz representer and regression loss, while sharing representation layers for the two functions. We also propose a Random Forest method which learns a locally linear representation of the Riesz function. Even though our method applies to arbitrary functionals, we experimentally ﬁnd that it performs well compared to the state of art neural net based algorithm of Shi et al. (2019) for the case of the average treatment effect functional. We also evaluate our method on the problem of estimating average marginal effects with continuous treat-ments, using semi-synthetic data of gasoline price changes on gasoline demand. Code available at","",""
12,"J. Schneider, J. Handali","Personalized explanation in machine learning",2019,"","","","",46,"2022-07-13 09:25:27","","","","",,,,,12,4.00,6,2,3,"Explanation in machine learning and related fields such as artificial intelligence aims at making machine learning models and their decisions understandable to humans. Existing work suggests that personalizing explanations might help to improve understandability. In this work, we derive a conceptualization of personalized explanation by defining and structuring the problem based on prior work on machine learning explanation, personalization (in machine learning) and concepts and techniques from other domains such as privacy and knowledge elicitation. We perform a categorization of explainee information used in the process of personalization as well as describing means to collect this information. We also identify three key explanation properties that are amendable to personalization: complexity, decision information and presentation. We also enhance existing work on explanation by introducing additional desiderata and measures to quantify the quality of personalized explanations.","",""
6,"C. Yeomans, R. Shail, S. Grebby, V. Nykänen, M. Middleton, P. Lusty","A machine learning approach to tungsten prospectivity modelling using knowledge-driven feature extraction and model confidence",2019,"","","","",47,"2022-07-13 09:25:27","","10.31223/osf.io/9fet8","","",,,,,6,2.00,1,6,3,"Abstract Novel mineral prospectivity modelling presented here applies knowledge-driven feature extraction to a data-driven machine learning approach for tungsten mineralisation. The method emphasises the importance of appropriate model evaluation and develops a new Confidence Metric to generate spatially refined and robust exploration targets. The data-driven Random Forest™ algorithm is employed to model tungsten mineralisation in SW England using a range of geological, geochemical and geophysical evidence layers which include a depth to granite evidence layer. Two models are presented, one using standardised input variables and a second that implements fuzzy set theory as part of an augmented feature extraction step. The use of fuzzy data transformations mean feature extraction can incorporate some user-knowledge about the mineralisation into the model. The typically subjective approach is guided using the Receiver Operating Characteristics (ROC) curve tool where transformed data are compared to known training samples. The modelling is conducted using 34 known true positive samples with 10 sets of randomly generated true negative samples to test the random effect on the model. The two models have similar accuracy but show different spatial distributions when identifying highly prospective targets. Areal analysis shows that the fuzzy-transformed model is a better discriminator and highlights three areas of high prospectivity that were not previously known. The Confidence Metric, derived from model variance, is employed to further evaluate the models. The new metric is useful for refining exploration targets and highlighting the most robust areas for follow-up investigation. The fuzzy-transformed model is shown to contain larger areas of high model confidence compared to the model using standardised variables. Finally, legacy mining data, from drilling reports and mine descriptions, is used to further validate the fuzzy-transformed model and gauge the depth of potential deposits. Descriptions of mineralisation corroborate that the targets generated in these models could be undercover at depths of less than 300 ​m. In summary, the modelling workflow presented herein provides a novel integration of knowledge-driven feature extraction with data-driven machine learning modelling, while the newly derived Confidence Metric generates reliable mineral exploration targets.","",""
75,"M. Prosperi, Yi Guo, M. Sperrin, J. Koopman, Jae Min, Xing He, S. Rich, Mo Wang, I. Buchan, J. Bian","Causal inference and counterfactual prediction in machine learning for actionable healthcare",2020,"","","","",48,"2022-07-13 09:25:27","","10.1038/s42256-020-0197-y","","",,,,,75,37.50,8,10,2,"","",""
10,"Yifan Cui, E. Tchetgen","Selective machine learning of doubly robust functionals.",2019,"","","","",49,"2022-07-13 09:25:27","","","","",,,,,10,3.33,5,2,3,"While model selection is a well-studied topic in parametric and nonparametric regression or density estimation, model selection of possibly high-dimensional nuisance parameters in semiparametric problems is far less developed. In this paper, we propose a selective machine learning framework for making inferences about a finite-dimensional functional defined on a semiparametric model, when the latter admits a doubly robust estimating function. We introduce two model selection criteria for bias reduction of functional of interest, each based on a novel definition of pseudo-risk for the functional that embodies this double robustness property and thus may be used to select the candidate model that is nearest to fulfilling this property even when all models are wrong. We establish an oracle property for a multi-fold cross-validation version of the new model selection criteria which states that our empirical criteria perform nearly as well as an oracle with a priori knowledge of the pseudo-risk for each candidate model. We also describe a smooth approximation to the selection criteria which allows for valid post-selection inference. Finally, we apply the approach to model selection of a semiparametric estimator of average treatment effect given an ensemble of candidate machine learners to account for confounding in an observational study.","",""
37,"Efstathios D. Gennatas, J. Friedman, L. Ungar, R. Pirracchio, Eric Eaton, L. Reichman, Y. Interian, C. Simone, A. Auerbach, E. Delgado, M. J. Laan, T. Solberg, G. Valdes","Expert-augmented machine learning",2019,"","","","",50,"2022-07-13 09:25:27","","10.1073/pnas.1906831117","","",,,,,37,12.33,4,13,3,"Significance Machine learning is increasingly used across fields to derive insights from data, which further our understanding of the world and help us anticipate the future. The performance of predictive modeling is dependent on the amount and quality of available data. In practice, we rely on human experts to perform certain tasks and on machine learning for others. However, the optimal learning strategy may involve combining the complementary strengths of humans and machines. We present expert-augmented machine learning, an automated way to automatically extract problem-specific human expert knowledge and integrate it with machine learning to build robust, dependable, and data-efficient predictive models. Machine learning is proving invaluable across disciplines. However, its success is often limited by the quality and quantity of available data, while its adoption is limited by the level of trust afforded by given models. Human vs. machine performance is commonly compared empirically to decide whether a certain task should be performed by a computer or an expert. In reality, the optimal learning strategy may involve combining the complementary strengths of humans and machines. Here, we present expert-augmented machine learning (EAML), an automated method that guides the extraction of expert knowledge and its integration into machine-learned models. We used a large dataset of intensive-care patient data to derive 126 decision rules that predict hospital mortality. Using an online platform, we asked 15 clinicians to assess the relative risk of the subpopulation defined by each rule compared to the total sample. We compared the clinician-assessed risk to the empirical risk and found that, while clinicians agreed with the data in most cases, there were notable exceptions where they overestimated or underestimated the true risk. Studying the rules with greatest disagreement, we identified problems with the training data, including one miscoded variable and one hidden confounder. Filtering the rules based on the extent of disagreement between clinician-assessed risk and empirical risk, we improved performance on out-of-sample data and were able to train with less data. EAML provides a platform for automated creation of problem-specific priors, which help build robust and dependable machine-learning models in critical applications.","",""
4,"Alexandra Renouard, A. Maggi, M. Grunberg, C. Doubre, C. Hibert","Toward False Event Detection and Quarry Blast versus Earthquake Discrimination in an Operational Setting Using Semiautomated Machine Learning",2021,"","","","",51,"2022-07-13 09:25:27","","10.1785/0220200305","","",,,,,4,4.00,1,5,1,"  Small-magnitude earthquakes shed light on the spatial and magnitude distribution of natural seismicity, as well as its rate and occurrence, especially in stable continental regions where natural seismicity remains difficult to explain under slow strain-rate conditions. However, capturing them in catalogs is strongly hindered by signal-to-noise ratio issues, resulting in high rates of false and man-made events also being detected. Accurate and robust discrimination of these events is critical for optimally detecting small earthquakes. This requires uncovering recurrent salient features that can rapidly distinguish first false events from real events, then earthquakes from man-made events (mainly quarry blasts), despite high signal variability and noise content. In this study, we combined the complementary strengths of human and interpretable rule-based machine-learning algorithms for solving this classification problem. We used human expert knowledge to co-create two reliable machine-learning classifiers through human-assisted selection of classification features and review of events with uncertain classifier predictions. The two classifiers are integrated into the SeisComP3 operational monitoring system. The first one discards false events from the set of events obtained with a low short-term average/long-term average threshold; the second one labels the remaining events as either earthquakes or quarry blasts. When run in an operational setting, the first classifier correctly detected more than 99% of false events and just over 93% of earthquakes; the second classifier correctly labeled 95% of quarry blasts and 96% of earthquakes. After a manual review of the second classifier low-confidence outputs, the final catalog contained fewer than 2% of misclassified events. These results confirm that machine learning strengthens the quality of earthquake catalogs and that the performance of machine-learning classifiers can be improved through human expertise. Our study promotes a broader implication of hybrid intelligence monitoring within seismological observatories.","",""
5,"Furkan M. Torun, S. V. Winter, Sophia Doll, Felix M. Riese, A. Vorobyev, Johannes B. Mueller‐Reif, Philipp E. Geyer, Maximilian T. Strauss","Transparent exploration of machine learning for biomarker discovery from proteomics and omics data",2021,"","","","",52,"2022-07-13 09:25:27","","10.1101/2021.03.05.434053","","",,,,,5,5.00,1,8,1,"Biomarkers are of central importance for assessing the health state and to guide medical interventions and their efficacy, but they are lacking for most diseases. Mass spectrometry (MS)-based proteomics is a powerful technology for biomarker discovery, but requires sophisticated bioinformatics to identify robust patterns. Machine learning (ML) has become indispensable for this purpose, however, it is sometimes applied in an opaque manner, generally requires expert knowledge and complex and expensive software. To enable easy access to ML for biomarker discovery without any programming or bioinformatic skills, we developed ‘OmicLearn’ (https://OmicLearn.com), an open-source web-based ML tool using the latest advances in the Python ML ecosystem. We host a web server for the exploration of the researcher’s results that can readily be cloned for internal use. Output tables from proteomics experiments are easily uploaded to the central or a local webserver. OmicLearn enables rapid exploration of the suitability of various ML algorithms for the experimental datasets. It fosters open science via transparent assessment of state-of-the-art algorithms in a standardized format for proteomics and other omics sciences. Graphical Abstract Highlights OmicLearn is an open-source platform allows researchers to apply machine learning (ML) for biomarker discovery The ready-to-use structure of OmicLearn enables accessing state-of-the-art ML algorithms without requiring any prior bioinformatics knowledge OmicLearn’s web-based interface provides an easy-to-follow platform for classification and gaining insights into the dataset Several algorithms and methods for preprocessing, feature selection, classification and cross-validation of omics datasets are integrated All results, settings and method text can be exported in publication-ready formats","",""
3,"N. Yousefpour, Z. Medina-Cetina, F. G. Hernandez-Martinez, A. Al-Tabbaa","Stiffness and Strength of Stabilized Organic Soils—Part II/II: Parametric Analysis and Modeling with Machine Learning",2021,"","","","",53,"2022-07-13 09:25:27","","10.3390/GEOSCIENCES11050218","","",,,,,3,3.00,1,4,1,"Predicting the range of achievable strength and stiffness from stabilized soil mixtures is critical for engineering design and construction, especially for organic soils, which are often considered “unsuitable” due to their high compressibility and the lack of knowledge about their mechanical behavior after stabilization. This study investigates the mechanical behavior of stabilized organic soils using machine learning (ML) methods. ML algorithms were developed and trained using a database from a comprehensive experimental study (see Part I), including more than one thousand unconfined compression tests on organic clay samples stabilized by wet soil mixing (WSM) technique. Three different ML methods were adopted and compared, including two artificial neural networks (ANN) and a linear regression method. ANN models proved reliable in the prediction of the stiffness and strength of stabilized organic soils, significantly outperforming linear regression models. Binder type, mixing ratio, soil organic and water content, sample size, aging, temperature, relative humidity, and carbonation were the control variables (input parameters) incorporated into the ML models. The impacts of these factors were evaluated through rigorous ANN-based parametric analyses. Additionally, the nonlinear relations of stiffness and strength with these parameters were developed, and their optimum ranges were identified through the ANN models. Overall, the robust ML approach presented in this paper can significantly improve the mixture design for organic soil stabilization and minimize the experimental cost for implementing WSM in engineering projects.","",""
3,"A. Horst, Erand Smakaj, E. Natali, Deniz Tosoni, Lmar Babrak, P. Meier, Enkelejda Miho","Machine Learning Detects Anti-DENV Signatures in Antibody Repertoire Sequences",2021,"","","","",54,"2022-07-13 09:25:27","","10.3389/frai.2021.715462","","",,,,,3,3.00,0,7,1,"Dengue infection is a global threat. As of today, there is no universal dengue fever treatment or vaccines unreservedly recommended by the World Health Organization. The investigation of the specific immune response to dengue virus would support antibody discovery as therapeutics for passive immunization and vaccine design. High-throughput sequencing enables the identification of the multitude of antibodies elicited in response to dengue infection at the sequence level. Artificial intelligence can mine the complex data generated and has the potential to uncover patterns in entire antibody repertoires and detect signatures distinctive of single virus-binding antibodies. However, these machine learning have not been harnessed to determine the immune response to dengue virus. In order to enable the application of machine learning, we have benchmarked existing methods for encoding biological and chemical knowledge as inputs and have investigated novel encoding techniques. We have applied different machine learning methods such as neural networks, random forests, and support vector machines and have investigated the parameter space to determine best performing algorithms for the detection and prediction of antibody patterns at the repertoire and antibody sequence levels in dengue-infected individuals. Our results show that immune response signatures to dengue are detectable both at the antibody repertoire and at the antibody sequence levels. By combining machine learning with phylogenies and network analysis, we generated novel sequences that present dengue-binding specific signatures. These results might aid further antibody discovery and support vaccine design.","",""
3,"Dongsheng Xiao, Brandon Jonathan Forys, M. Vanni, T. Murphy","MesoNet allows automated scaling and segmentation of mouse mesoscale cortical maps using machine learning",2021,"","","","",55,"2022-07-13 09:25:27","","10.1038/s41467-021-26255-2","","",,,,,3,3.00,1,4,1,"","",""
3,"Tingting Sun, Yuting Chen, Yuhao Wen, Zefeng Zhu, Minghui Li","PremPLI: a machine learning model for predicting the effects of missense mutations on protein-ligand interactions",2021,"","","","",56,"2022-07-13 09:25:27","","10.1038/s42003-021-02826-3","","",,,,,3,3.00,1,5,1,"","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",57,"2022-07-13 09:25:27","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
2,"A. Fernández-Fontelo, Pascal J. Kieslich, Felix Henninger, F. Kreuter, S. Greven","Predicting Question Difficulty in Web Surveys: A Machine Learning Approach Based on Mouse Movement Features",2021,"","","","",58,"2022-07-13 09:25:27","","10.1177/08944393211032950","","",,,,,2,2.00,0,5,1,"Survey research aims to collect robust and reliable data from respondents. However, despite researchers’ efforts in designing questionnaires, survey instruments may be imperfect, and question structure not as clear as could be, thus creating a burden for respondents. If it were possible to detect such problems, this knowledge could be used to predict problems in a questionnaire during pretesting, inform real-time interventions through responsive questionnaire design, or to indicate and correct measurement error after the fact. Previous research has used paradata, specifically response times, to detect difficulties and help improve user experience and data quality. Today, richer data sources are available, for example, movements respondents make with their mouse, as an additional detailed indicator for the respondent–survey interaction. This article uses machine learning techniques to explore the predictive value of mouse-tracking data regarding a question’s difficulty. We use data from a survey on respondents’ employment history and demographic information, in which we experimentally manipulate the difficulty of several questions. Using measures derived from mouse movements, we predict whether respondents have answered the easy or difficult version of a question, using and comparing several state-of-the-art supervised learning methods. We have also developed a personalization method that adjusts for respondents’ baseline mouse behavior and evaluate its performance. For all three manipulated survey questions, we find that including the full set of mouse movement measures and accounting for individual differences in these measures improve prediction performance over response-time-only models.","",""
3,"Yusuke Kawamoto","Towards Logical Specification of Statistical Machine Learning",2019,"","","","",59,"2022-07-13 09:25:27","","10.1007/978-3-030-30446-1_16","","",,,,,3,1.00,3,1,3,"","",""
0,"Johannes Schneider","FOR MACHINE LEARNING : A CONCEPTUALIZATION",2019,"","","","",60,"2022-07-13 09:25:27","","","","",,,,,0,0.00,0,1,3,"Explanation in machine learning and related fields such as artificial intelligence aims at making machine learning models and their decisions understandable to humans. Existing work suggests that personalizing explanations might help to improve understandability. In this work, we derive a conceptualization of personalized explanation by defining and structuring the problem based on prior work on machine learning explanation, personalization (in machine learning) and concepts and techniques from other domains such as privacy and knowledge elicitation. We perform a categorization of explainee data used in the process of personalization as well as describing means to collect this data. We also identify three key explanation properties that are amendable to personalization: complexity, decision information and presentation. We also enhance existing work on explanation by introducing additional desiderata and measures to quantify the quality of personalized explanations.","",""
5,"M. Usama, Muhammad Asim, Junaid Qadir, Ala Al-Fuqaha, M. Imran","Adversarial Machine Learning Attack on Modulation Classification",2019,"","","","",61,"2022-07-13 09:25:27","","10.1109/UCET.2019.8881843","","",,,,,5,1.67,1,5,3,"Modulation classification is an important component of cognitive self-driving networks. Recently many ML-based modulation classification methods have been proposed. We have evaluated the robustness of 9 ML-based modulation classifiers against the powerful Carlini & Wagner (C-W) attack and showed that the current ML-based modulation classifiers do not provide any deterrence against adversarial ML examples. To the best of our knowledge, we are the first to report the results of the application of the C-W attack for creating adversarial examples against various ML models for modulation classification.","",""
15,"A. Kaur, Kamaldeep Kaur","An Empirical Study of Robustness and Stability of Machine Learning Classifiers in Software Defect Prediction",2014,"","","","",62,"2022-07-13 09:25:27","","10.1007/978-3-319-11218-3_35","","",,,,,15,1.88,8,2,8,"","",""
21,"Christopher Culley, S. Vijayakumar, Guido Zampieri, C. Angione","A mechanism-aware and multiomic machine-learning pipeline characterizes yeast cell growth",2020,"","","","",63,"2022-07-13 09:25:27","","10.1073/pnas.2002959117","","",,,,,21,10.50,5,4,2,"Significance Linking genotype and phenotype is a fundamental problem in biology, key to several biomedical and biotechnological applications. Cell growth is a central phenotypic trait, resulting from interactions between environment, gene regulation, and metabolism, yet its functional bases are still not completely understood. We propose and test a machine-learning approach that integrates large-scale gene expression profiles and mechanistic metabolic models, for characterizing cell growth and understanding its driving mechanisms in Saccharomyces cerevisiae. At its core, a custom-built multimodal learning method merges experimentally generated and model-generated data. We show that our approach can leverage the advantages of both machine learning and metabolic modeling, revealing unknown interactions between biological domains, incorporating mechanistic knowledge, and therefore overcoming black-box limitations of conventional data-driven approaches. Metabolic modeling and machine learning are key components in the emerging next generation of systems and synthetic biology tools, targeting the genotype–phenotype–environment relationship. Rather than being used in isolation, it is becoming clear that their value is maximized when they are combined. However, the potential of integrating these two frameworks for omic data augmentation and integration is largely unexplored. We propose, rigorously assess, and compare machine-learning–based data integration techniques, combining gene expression profiles with computationally generated metabolic flux data to predict yeast cell growth. To this end, we create strain-specific metabolic models for 1,143 Saccharomyces cerevisiae mutants and we test 27 machine-learning methods, incorporating state-of-the-art feature selection and multiview learning approaches. We propose a multiview neural network using fluxomic and transcriptomic data, showing that the former increases the predictive accuracy of the latter and reveals functional patterns that are not directly deducible from gene expression alone. We test the proposed neural network on a further 86 strains generated in a different experiment, therefore verifying its robustness to an additional independent dataset. Finally, we show that introducing mechanistic flux features improves the predictions also for knockout strains whose genes were not modeled in the metabolic reconstruction. Our results thus demonstrate that fusing experimental cues with in silico models, based on known biochemistry, can contribute with disjoint information toward biologically informed and interpretable machine learning. Overall, this study provides tools for understanding and manipulating complex phenotypes, increasing both the prediction accuracy and the extent of discernible mechanistic biological insights.","",""
2,"Juan Gao, Chunfang Li, Zhen-Guo Liu, Lian-Zhong Liu","Elicitation of machine learning to human learning from iterative error correcting",2013,"","","","",64,"2022-07-13 09:25:27","","10.1109/ICMLC.2013.6890473","","",,,,,2,0.22,1,4,9,"Numerous high performance machine learning algorithms are designed based on human learning, while human learning can also acquire elicitation from machine learning to investigate highly efficient learning process. This paper presents two iteratively error correcting based probabilistic neural networks (PNN) for connecting human learning and machine learning. C-PNN, G-PNN and G-PNN have been used to delete redundancy samples in our learning software based on question bank. In detail, we propose a recommendation approach of learning samples which selects samples according to density of knowledge points through calculating data field of knowledge points covered by problems. The approach also deletes redundant problems in order to deal with the question-sea tactical and remedy the defects of random selecting usually used in human learning.","",""
242,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",65,"2022-07-13 09:25:27","","10.1145/3308809.3308857","","",,,,,242,48.40,81,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server andm working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of them working machines suffer Byzantine faults - a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.","",""
14,"Zi Zhang, Hong Pan, Xingyu Wang, Zhibin Lin","Machine Learning-Enriched Lamb Wave Approaches for Automated Damage Detection",2020,"","","","",66,"2022-07-13 09:25:27","","10.3390/s20061790","","",,,,,14,7.00,4,4,2,"Lamb wave approaches have been accepted as efficiently non-destructive evaluations in structural health monitoring for identifying damage in different states. Despite significant efforts in signal process of Lamb waves, physics-based prediction is still a big challenge due to complexity nature of the Lamb wave when it propagates, scatters and disperses. Machine learning in recent years has created transformative opportunities for accelerating knowledge discovery and accurately disseminating information where conventional Lamb wave approaches cannot work. Therefore, the learning framework was proposed with a workflow from dataset generation, to sensitive feature extraction, to prediction model for lamb-wave-based damage detection. A total of 17 damage states in terms of different damage type, sizes and orientations were designed to train the feature extraction and sensitive feature selection. A machine learning method, support vector machine (SVM), was employed for the learning model. A grid searching (GS) technique was adopted to optimize the parameters of the SVM model. The results show that the machine learning-enriched Lamb wave-based damage detection method is an efficient and accuracy wave to identify the damage severity and orientation. Results demonstrated that different features generated from different domains had certain levels of sensitivity to damage, while the feature selection method revealed that time-frequency features and wavelet coefficients exhibited the highest damage-sensitivity. These features were also much more robust to noise. With increase of noise, the accuracy of the classification dramatically dropped.","",""
25,"H. Yoon, Jae-Hoon Sim, M. Han","Analytic continuation via domain knowledge free machine learning",2018,"","","","",67,"2022-07-13 09:25:27","","10.1103/PhysRevB.98.245101","","",,,,,25,6.25,8,3,4,"We present a machine-learning approach to a long-standing issue in quantum many-body physics, namely, analytic continuation. This notorious ill-conditioned problem of obtaining spectral function from an imaginary time Green's function has been a focus of new method developments for past decades. Here we demonstrate the usefulness of modern machine-learning techniques including convolutional neural networks and the variants of a stochastic gradient descent optimizer. The machine-learning continuation kernel is successfully realized without any ``domain knowledge,'' which means that any physical ``prior'' is not utilized in the kernel construction and the neural networks ``learn'' the knowledge solely from ``training.'' The outstanding performance is achieved for both insulating and metallic band structure. Our machine-learning-based approach not only provides the more accurate spectrum than the conventional methods in terms of peak positions and heights, but is also more robust against the noise which is the required key feature for any continuation technique to be successful. Furthermore, its computation speed is ${10}^{4}\text{--}{10}^{5}$ times faster than the maximum entropy method.","",""
12,"H. Sufriyana, Yu-Wei Wu, E. C. Su","Prediction of Preeclampsia and Intrauterine Growth Restriction: Development of Machine Learning Models on a Prospective Cohort",2020,"","","","",68,"2022-07-13 09:25:27","","10.2196/15411","","",,,,,12,6.00,4,3,2,"Background Preeclampsia and intrauterine growth restriction are placental dysfunction–related disorders (PDDs) that require a referral decision be made within a certain time period. An appropriate prediction model should be developed for these diseases. However, previous models did not demonstrate robust performances and/or they were developed from datasets with highly imbalanced classes. Objective In this study, we developed a predictive model of PDDs by machine learning that uses features at 24-37 weeks’ gestation, including maternal characteristics, uterine artery (UtA) Doppler measures, soluble fms-like tyrosine kinase receptor-1 (sFlt-1), and placental growth factor (PlGF). Methods A public dataset was taken from a prospective cohort study that included pregnant women with PDDs (66/95, 69%) and a control group (29/95, 31%). Preliminary selection of features was based on a statistical analysis using SAS 9.4 (SAS Institute). We used Weka (Waikato Environment for Knowledge Analysis) 3.8.3 (The University of Waikato, Hamilton, NZ) to automatically select the best model using its optimization algorithm. We also manually selected the best of 23 white-box models. Models, including those from recent studies, were also compared by interval estimation of evaluation metrics. We used the Matthew correlation coefficient (MCC) as the main metric. It is not overoptimistic to evaluate the performance of a prediction model developed from a dataset with a class imbalance. Repeated 10-fold cross-validation was applied. Results The classification via regression model was chosen as the best model. Our model had a robust MCC (.93, 95% CI .87-1.00, vs .64, 95% CI .57-.71) and specificity (100%, 95% CI 100-100, vs 90%, 95% CI 90-90) compared to each metric of the best models from recent studies. The sensitivity of this model was not inferior (95%, 95% CI 91-100, vs 100%, 95% CI 92-100). The area under the receiver operating characteristic curve was also competitive (0.970, 95% CI 0.966-0.974, vs 0.987, 95% CI 0.980-0.994). Features in the best model were maternal weight, BMI, pulsatility index of the UtA, sFlt-1, and PlGF. The most important feature was the sFlt-1/PlGF ratio. This model used an M5P algorithm consisting of a decision tree and four linear models with different thresholds. Our study was also better than the best ones among recent studies in terms of the class balance and the size of the case class (66/95, 69%, vs 27/239, 11.3%). Conclusions Our model had a robust predictive performance. It was also developed to deal with the problem of a class imbalance. In the context of clinical management, this model may improve maternal mortality and neonatal morbidity and reduce health care costs.","",""
16,"Pedram Daee, T. Peltola, Aki Vehtari, Samuel Kaski","User Modelling for Avoiding Overfitting in Interactive Knowledge Elicitation for Prediction",2017,"","","","",69,"2022-07-13 09:25:27","","10.1145/3172944.3172989","","",,,,,16,3.20,4,4,5,"In human-in-the-loop machine learning, the user provides information beyond that in the training data. Many algorithms and user interfaces have been designed to optimize and facilitate this human--machine interaction; however, fewer studies have addressed the potential defects the designs can cause. Effective interaction often requires exposing the user to the training data or its statistics. The design of the system is then critical, as this can lead to double use of data and overfitting, if the user reinforces noisy patterns in the data. We propose a user modelling methodology, by assuming simple rational behaviour, to correct the problem. We show, in a user study with 48 participants, that the method improves predictive performance in a sparse linear regression sentiment analysis task, where graded user knowledge on feature relevance is elicited. We believe that the key idea of inferring user knowledge with probabilistic user models has general applicability in guarding against overfitting and improving interactive machine learning.","",""
11,"M. Elgendi, C. Menon","Machine Learning Ranks ECG as an Optimal Wearable Biosignal for Assessing Driving Stress",2020,"","","","",70,"2022-07-13 09:25:27","","10.1109/ACCESS.2020.2974933","","",,,,,11,5.50,6,2,2,"The demand for wearable devices that can detect anxiety and stress when driving is increasing. Recent studies have attempted to use multiple biosignals to detect driving stress. However, collecting multiple biosignals can be complex and is associated with numerous challenges. Determining the optimal biosignal for assessing driving stress can save lives. To the best of our knowledge, no study has investigated both longitudinal and transitional stress assessment using supervised and unsupervised ML techniques. Thus, this study hypothesizes that the optimal signal for assessing driving stress will consistently detect stress using supervised and unsupervised machine learning (ML) techniques. Two different approaches were used to assess driving stress: longitudinal (a combined repeated measurement of the same biosignals over three driving states) and transitional (switching from state to state such as city to highway driving). The longitudinal analysis did not involve a feature extraction phase while the transitional analysis involved a feature extraction phase. The longitudinal analysis consists of a novel interaction ensemble (INTENSE) that aggregates three unsupervised ML approaches: interaction principal component analysis, connectivity-based clustering, and K-means clustering. INTENSE was developed to uncover new knowledge by revealing the strongest correlation between the biosignal and driving stress marker. These three MLs each have their well-known and distinctive geometrical basis. Thus, the aggregation of their result would provide a more robust examination of the simultaneous non-causal associations between six biosignals: electrocardiogram (ECG), electromyogram, hand galvanic skin resistance, foot galvanic skin resistance, heart rate, respiration, and the driving stress marker. INTENSE indicates that ECG is highly correlated with the driving stress marker. The supervised ML algorithms confirmed that ECG is the most informative biosignal for detecting driving stress, with an overall accuracy of 75.02%.","",""
7,"Jina Suh, S. Ghorashi, Gonzalo A. Ramos, N. Chen, S. Drucker, J. Verwey, P. Simard","AnchorViz: Facilitating Semantic Data Exploration and Concept Discovery for Interactive Machine Learning",2019,"","","","",71,"2022-07-13 09:25:27","","10.1145/3241379","","",,,,,7,2.33,1,7,3,"When building a classifier in interactive machine learning (iML), human knowledge about the target class can be a powerful reference to make the classifier robust to unseen items. The main challenge lies in finding unlabeled items that can either help discover or refine concepts for which the current classifier has no corresponding features (i.e., it has feature blindness). Yet it is unrealistic to ask humans to come up with an exhaustive list of items, especially for rare concepts that are hard to recall. This article presents AnchorViz, an interactive visualization that facilitates the discovery of prediction errors and previously unseen concepts through human-driven semantic data exploration. By creating example-based or dictionary-based anchors representing concepts, users create a topology that (a) spreads data based on their similarity to the concepts and (b) surfaces the prediction and label inconsistencies between data points that are semantically related. Once such inconsistencies and errors are discovered, users can encode the new information as labels or features and interact with the retrained classifier to validate their actions in an iterative loop. We evaluated AnchorViz through two user studies. Our results show that AnchorViz helps users discover more prediction errors than stratified random and uncertainty sampling methods. Furthermore, during the beginning stages of a training task, an iML tool with AnchorViz can help users build classifiers comparable to the ones built with the same tool with uncertainty sampling and keyword search, but with fewer labels and more generalizable features. We discuss exploration strategies observed during the two studies and how AnchorViz supports discovering, labeling, and refining of concepts through a sensemaking loop.","",""
7,"Md. Kowsher, A. Tahabilder, S. Murad","Impact-Learning: A Robust Machine Learning Algorithm",2020,"","","","",72,"2022-07-13 09:25:27","","10.1145/3411174.3411185","","",,,,,7,3.50,2,3,2,"The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth.","",""
7,"D. Jacob","Cross-Fitting and Averaging for Machine Learning Estimation of Heterogeneous Treatment Effects",2020,"","","","",73,"2022-07-13 09:25:27","","","","",,,,,7,3.50,7,1,2,"We investigate the finite sample performance of sample splitting, cross-fitting and averaging for the estimation of the conditional average treatment effect. Recently proposed methods, so-called meta-learners, make use of machine learning to estimate different nuisance functions and hence allow for fewer restrictions on the underlying structure of the data. To limit a potential overfitting bias that may result when using machine learning methods, cross-fitting estimators have been proposed. This includes the splitting of the data in different folds to reduce bias and averaging over folds to restore efficiency. To the best of our knowledge, it is not yet clear how exactly the data should be split and averaged. We employ a Monte Carlo study with different data generation processes and consider twelve different estimators that vary in sample-splitting, cross-fitting and averaging procedures. We investigate the performance of each estimator independently on four different meta-learners: the doubly-robust-learner, R-learner, T-learner and X-learner. We find that the performance of all meta-learners heavily depends on the procedure of splitting and averaging. The best performance in terms of mean squared error (MSE) among the sample split estimators can be achieved when applying cross-fitting plus taking the median over multiple different sample-splitting iterations. Some meta-learners exhibit a high variance when the lasso is included in the ML methods. Excluding the lasso decreases the variance and leads to robust and at least competitive results.","",""
12,"N. Khoa, M. M. Alamdari, T. Rakotoarivelo, Ali Anaissi, Yang Wang","Structural Health Monitoring Using Machine Learning Techniques and Domain Knowledge Based Features",2018,"","","","",74,"2022-07-13 09:25:27","","10.1007/978-3-319-90403-0_20","","",,,,,12,3.00,2,5,4,"","",""
6,"Syed Javeed Pasha, E. Mohamed","Novel Feature Reduction (NFR) Model With Machine Learning and Data Mining Algorithms for Effective Disease Risk Prediction",2020,"","","","",75,"2022-07-13 09:25:27","","10.1109/ACCESS.2020.3028714","","",,,,,6,3.00,3,2,2,"Presently, the application of machine learning (ML) and data mining (DM) techniques have a vital role in healthcare systems and wisely convert all obtainable data into beneficial knowledge. It is proven from the literature works that a chance of 12% error remains in the diagnosis of the diseases by the medical practitioners. Moreover, for effective disease risk prediction in medical analysis, more emphasis is accorded to the area under the curve (AUC) with accuracy as an evaluation metric. However, the role of the AUC has not been previously characterized notably. In this research article, a novel feature reduction (NFR) model that is aligned with the ML and DM algorithms is proposed to reduce the error rate and further improve the performance. The proposed NFR model comprises of two approaches and uses the AUC in addition to the accuracy to achieve a robust and effective disease risk prediction. The first approach is based on a heuristic process evaluating performance by reducing features with respect to the improvement in the AUC besides the accuracy as evaluation metrics, working to obtain the best subset of highly contributing features in the prediction. The second approach evaluates the accuracy and AUC of all individual features and forms the subsets with the highest accuracies, AUCs, and least difference between them, which are combined in various combinations to achieve the best-reduced set of highly relevant features. For this purpose, the benchmarked public heart datasets of the ML repository of the University of California, Irvine (UCI) are tested; the results are promising. The highest accuracy and AUC achieved with the proposed NFR model are 95.52% and 99.20% with 41.67% feature reduction, respectively. The accuracy is 4.22% higher than recent existing research with a significant improvement of 25% in the performance of the running time of the algorithm.","",""
5,"Tom Z. Jiahao, M. Hsieh, E. Forgoston","Knowledge-based learning of nonlinear dynamics and chaos",2020,"","","","",76,"2022-07-13 09:25:27","","10.1063/5.0065617","","",,,,,5,2.50,2,3,2,"Extracting predictive models from nonlinear systems is a central task in scientific machine learning. One key problem is the reconciliation between modern data-driven approaches and first principles. Despite rapid advances in machine learning techniques, embedding domain knowledge into datadriven models remains a challenge. In this work, we present a universal learning framework for extracting predictive models from nonlinear systems based on observations. Our framework can readily incorporate first principle knowledge because it naturally models nonlinear systems as continuous-time systems. This both improves the extracted models’ extrapolation power and reduces the amount of data needed for training. In addition, our framework has the advantages of robustness to observational noise and applicability to irregularly sampled data. We demonstrate the effectiveness of our scheme by learning predictive models for a wide variety of systems including a stiff Van der Pol oscillator, the Lorenz system, and the Kuramoto-Sivashinsky equation. For the Lorenz system, different types of domain knowledge are incorporated to demonstrate the strength of knowledge embedding in data-driven system identification.","",""
3,"J. Correia, Juliana Alves Pereira, Rafael Maiani de Mello, Alessandro F. Garcia, B. Neto, Márcio Ribeiro, Rohit Gheyi, M. Kalinowski, Renato Cerqueira, Willy Tiengo","Brazilian Data Scientists: Revealing their Challenges and Practices on Machine Learning Model Development",2020,"","","","",77,"2022-07-13 09:25:27","","10.1145/3439961.3439971","","",,,,,3,1.50,0,10,2,"Data scientists often develop machine learning models to solve a variety of problems in the industry and academy. To build these models, these professionals usually perform activities that are also performed in the traditional software development lifecycle, such as eliciting and implementing requirements. One might argue that data scientists could rely on the engineering of traditional software development to build machine learning models. However, machine learning development presents certain characteristics, which may raise challenges that lead to the need for adopting new practices. The literature lacks in characterizing this knowledge from the perspective of the data scientists. In this paper, we characterize challenges and practices addressing the engineering of machine learning models that deserve attention from the research community. To this end, we performed a qualitative study with eight data scientists across five different companies having different levels of experience in developing machine learning models. Our findings suggest that: (i) data processing and feature engineering are the most challenging stages in the development of machine learning models; (ii) it is essential synergy between data scientists and domain experts in most of stages; and (iii) the development of machine learning models lacks the support of a well-engineered process.","",""
5,"Dan Jiang, Weihua Lin, N. Raghavan","A Novel Framework for Semiconductor Manufacturing Final Test Yield Classification Using Machine Learning Techniques",2020,"","","","",78,"2022-07-13 09:25:27","","10.1109/ACCESS.2020.3034680","","",,,,,5,2.50,2,3,2,"Advanced data analysis tools and techniques are important for semiconductor companies to gain competitive advantage. In particular, yield prediction tools, which fully utilize production data, help to improve operational efficiency and reduce production costs. This paper introduces a novel and scalable framework for semiconductor manufacturing Final Test (FT) yield prediction leveraging machine learning techniques. This framework is able to predict FT yield at wafer fabrication stage, so that FT low yield problems can be caught at an earlier production stage compared to past studies. Our work presents a robust solution to automatically handle both numerical and categorical production related data without prior knowledge of the low yield root cause. Gaussian Mixture Models, One Hot Encoder and Label Encoder techniques are adopted for data pre-processing. To improve model performance for both binary and multi-class classification, model selection and model ensemble using the F1-macro method is demonstrated. The framework has been applied to three mass production products with different wafer technologies and manufacturing flows. All of them achieved high F1-macro test score indicative of the robustness of our framework.","",""
1,"Anay Raj","Malaria Disease Diagnosis using Machine Learning Techniques",2020,"","","","",79,"2022-07-13 09:25:27","","","","",,,,,1,0.50,1,1,2,"Malaria is a major infectious disease of humans, with roughly 200 million cases worldwide and more than 400,000 deaths per year. Malaria could be prevented, controlled, and cured more effectively if a more accurate and efficient diagnostic method was available. The standard diagnostic method for malaria is the microscopic examination of blood smears for infected erythrocytes by qualified microscopists. However, this method is inefficient and the quality of the diagnosis depends on the experience and knowledge of the microscopists. This study proposes a new and robust machine learning model based on a Convolutional neural network (CNN) to automatically classify single cells in thin blood smears on standard microscope slides as either infected or uninfected. This will help in the faster diagnosis of malaria and save valuable time for beginning the treatment.","",""
40,"G. Choudhury, David F. Lynch, Gaurav Thakur, Simon Tse","Two use cases of machine learning for SDN-enabled ip/optical networks: traffic matrix prediction and optical path performance prediction [Invited]",2018,"","","","",80,"2022-07-13 09:25:27","","10.1364/JOCN.10.000D52","","",,,,,40,10.00,10,4,4,"We describe two applications ofmachine learning in the context of internet protocol (IP)/Optical networks. The first one allows agilemanagement of resources in a core IP/Optical network by using machine learning for shorttermand long-term prediction of traffic flows. It also allows joint global optimization of IP and optical layers using colorless/ directionless (CD) reconfigurable optical add-drop multiplexers (ROADMs). Multilayer coordination allows for significant cost savings, flexible new services to meet dynamic capacity needs, and improved robustness by being able to proactively adapt to new traffic patterns and network conditions. The second application is important as we migrate our networks to Open ROADM networks to allow physical routing without the need for detailed knowledge of optical parameters. We discuss a proof-of-concept study, where detailed performance data for established wavelengths in an existing ROADM network is used for machine learning to predict the optical performance of each wavelength. Both applications can be efficiently implemented by using a software-defined network controller.","",""
79,"Taesik Na, J. Ko, S. Mukhopadhyay","Cascade Adversarial Machine Learning Regularized with a Unified Embedding",2017,"","","","",81,"2022-07-13 09:25:27","","","","",,,,,79,15.80,26,3,5,"Injecting adversarial examples during training, known as adversarial training, can improve robustness against one-step attacks, but not for unknown iterative attacks. To address this challenge, we first show iteratively generated adversarial images easily transfer between networks trained with the same strategy. Inspired by this observation, we propose cascade adversarial training, which transfers the knowledge of the end results of adversarial training. We train a network from scratch by injecting iteratively generated adversarial images crafted from already defended networks in addition to one-step adversarial images from the network being trained. We also propose to utilize embedding space for both classification and low-level (pixel-level) similarity learning to ignore unknown pixel level perturbation. During training, we inject adversarial images without replacing their corresponding clean images and penalize the distance between the two embeddings (clean and adversarial). Experimental results show that cascade adversarial training together with our proposed low-level similarity learning efficiently enhances the robustness against iterative attacks, but at the expense of decreased robustness against one-step attacks. We show that combining those two techniques can also improve robustness under the worst case black box attack scenario.","",""
3,"N. Major, N. Shadbolt","CNN: Integrating Knowledge Elicitation With a Machine Learning Technique",1992,"","","","",82,"2022-07-13 09:25:27","","","","",,,,,3,0.10,2,2,30,"","",""
4,"Shuteng Niu, Jian Wang, Yongxin Liu, H. Song","Transfer Learning based Data-Efficient Machine Learning Enabled Classification",2020,"","","","",83,"2022-07-13 09:25:27","","10.1109/DASC-PICom-CBDCom-CyberSciTech49142.2020.00108","","",,,,,4,2.00,1,4,2,"Recently, waste sorting has become more and more important in our daily life. It plays an essential role in the big picture of waste recycling, reducing environmental pollution significantly. Deep learning (DL) methods have been dominating the field of image classification and have been successfully applied to waste sorting tasks to achieve state-of-art performance. However, most traditional DL methods require a massive amount of annotated data for the training phase. Unfortunately, there is only one small data set for waste sorting, TrashNet created by Standford. In addition, manually collecting and labeling a massive data-set can be too costly. To address this issue, we decided to implement transfer learning (TL) techniques to construct a robust model based on a fairly small set of training data by transferring knowledge from existing deep networks, such as AlexNet, Resnet, and DensNet. As an innovation, we propose a novel domain loss function, Dual Dynamic Domain Distance (4D), to produce a more accurate domain distance measurement. There are three contributions to this paper. First, our model has achieved the best performance on the TrashNet data. Secondly, it is the first time that TL has been used for waste sorting. Finally, the proposed novel 4D domain loss has improved the performance of TL for this task. In this paper, we implemented two types of transfer learning methods, DDC, DeepCoral, to TrashNet data-set. Moreover, the DeepCoral-Resnet50 model yields the best performance of 96% test accuracy. More importantly, this work can be easily generalized to other image classification tasks.","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",84,"2022-07-13 09:25:27","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
41,"Xiang Lu, M. Hasanipanah, Kathirvel Brindhadevi, H. Bakhshandeh Amnieh, Seyedamirhesam Khalafi","ORELM: A Novel Machine Learning Approach for Prediction of Flyrock in Mine Blasting",2019,"","","","",85,"2022-07-13 09:25:27","","10.1007/s11053-019-09532-2","","",,,,,41,13.67,8,5,3,"","",""
71,"T. Cohen, M. Freytsis, B. Ostdiek","(Machine) learning to do more with less",2017,"","","","",86,"2022-07-13 09:25:27","","10.1007/JHEP02(2018)034","","",,,,,71,14.20,24,3,5,"","",""
33,"Jana Sperschneider","Machine learning in plant-pathogen interactions: empowering biological predictions from field scale to genome scale.",2020,"","","","",87,"2022-07-13 09:25:27","","10.1111/nph.15771","","",,,,,33,16.50,33,1,2,"Contents Summary I. A primer on machine learning: what is it and what are the common pitfalls? II. Machine learning applications in plant-pathogen interactions III. Conclusion Acknowledgements References SUMMARY: Machine learning (ML) encompasses statistical methods that learn to identify patterns in complex datasets. Here, I review application areas in plant-pathogen interactions that have recently benefited from ML, such as disease monitoring, the discovery of gene regulatory networks, genomic selection for disease resistance and prediction of pathogen effectors. However, achieving robust performance from ML is not trivial and requires knowledge of both the methodology and the biology. I discuss common pitfalls and challenges in using ML approaches. Finally, I highlight future opportunities for ML as a tool for dissecting plant-pathogen interactions using high-throughput data, for example, through integration of diverse data sources and the analysis with higher resolution, such as from individual cells or on elaborate spatial and temporal scales.","",""
35,"Feng Ren, Chenglei Wang, Hui Tang","Active control of vortex-induced vibration of a circular cylinder using machine learning",2019,"","","","",88,"2022-07-13 09:25:27","","10.1063/1.5115258","","",,,,,35,11.67,12,3,3,"We demonstrate the use of high-fidelity computational fluid dynamics simulations in machine-learning based active flow control. More specifically, for the first time, we adopt the genetic programming (GP) to select explicit control laws, in a data-driven and unsupervised manner, for the suppression of vortex-induced vibration (VIV) of a circular cylinder in a low-Reynolds-number flow (Re = 100), using blowing/suction at fixed locations. A cost function that balances both VIV suppression and energy consumption for the control is carefully chosen according to the knowledge obtained from pure blowing/suction open-loop controls. By implementing reasonable constraints to VIV amplitude and actuation strength during the GP evolution, the GP-selected best ten control laws all point to suction-type actuation. The best control law suggests that the suction strength should be nonzero when the cylinder is at its equilibrium position and should increase nonlinearly with the cylinder’s transverse displacement. Applying this control law suppresses 94.2% of the VIV amplitude and achieves 21.4% better overall performance than the best open-loop controls. Furthermore, it is found that the GP-selected control law is robust, being effective in flows ranging from Re = 100 to 400. On the contrary, although the P-control can achieve similar performance as the GP-selected control at Re = 100, it deteriorates in higher Reynolds number flows. Although for demonstration purpose the chosen control problem is relatively simple, the training experience and insights obtained from this study can shed some light on future GP-based control of more complicated problems.We demonstrate the use of high-fidelity computational fluid dynamics simulations in machine-learning based active flow control. More specifically, for the first time, we adopt the genetic programming (GP) to select explicit control laws, in a data-driven and unsupervised manner, for the suppression of vortex-induced vibration (VIV) of a circular cylinder in a low-Reynolds-number flow (Re = 100), using blowing/suction at fixed locations. A cost function that balances both VIV suppression and energy consumption for the control is carefully chosen according to the knowledge obtained from pure blowing/suction open-loop controls. By implementing reasonable constraints to VIV amplitude and actuation strength during the GP evolution, the GP-selected best ten control laws all point to suction-type actuation. The best control law suggests that the suction strength should be nonzero when the cylinder is at its equilibrium position and should increase nonlinearly with the cylinder’s transverse displacement. Applying...","",""
54,"M. S. Hossain Lipu, M. Hannan, A. Hussain, M. Saad, A. Ayob, M. Uddin","Extreme Learning Machine Model for State-of-Charge Estimation of Lithium-Ion Battery Using Gravitational Search Algorithm",2019,"","","","",89,"2022-07-13 09:25:27","","10.1109/TIA.2019.2902532","","",,,,,54,18.00,9,6,3,"This paper develops a state-of-charge (SOC) estimation model for a lithium-ion battery using an improved extreme learning machine (ELM) algorithm. ELM is suitable for an SOC estimation since the ELM algorithm has fast estimation speed, good generalization performance, and high accuracy. However, the performance of ELM is highly dependent on training accuracy and the number of neurons in a hidden layer. Hence, a gravitational search algorithm (GSA) is applied to improve the ELM computational intelligence by searching for the optimal value hidden layer neurons. The optimal ELM-based GSA model does not require internal battery knowledge and mathematical model for an SOC estimation. The model robustness is validated at different temperatures using different electric vehicle drive cycles. The performance of the ELM-GSA model is verified with two popular neural network methods: back-propagation neural network (BPNN) and radial basis function neural network (RBFNN). The results are evaluated using different error rates and computation costs. The results demonstrate that the ELM-based GSA model offers a higher accuracy and lower SOC error rate than those of BPNN-based GSA and RBFNN-based GSA models. Furthermore, a detailed comparative study between the proposed model and existing SOC strategies is conducted, which also demonstrates the superiority of the proposed model.","",""
52,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",90,"2022-07-13 09:25:27","","10.1145/3219617.3219655","","",,,,,52,10.40,17,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q łe m for an arbitrarily small but fixed constant ε>0. The parameter estimate converges in O(łog N) rounds with an estimation error on the order of max √dq/N, ~√d/N , which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q . The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log 3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
22,"Mohamed Maher, S. Sakr","SmartML: A Meta Learning-Based Framework for Automated Selection and Hyperparameter Tuning for Machine Learning Algorithms",2019,"","","","",91,"2022-07-13 09:25:27","","10.5441/002/edbt.2019.54","","",,,,,22,7.33,11,2,3,"Due to the increasing success of machine learning techniques, nowadays, thay have been widely utilized in almost every domain such as financial applications, marketing, recommender systems and user behavior analytics, just to name a few. In practice, the machine learning model creation process is a highly iterative exploratory process. In particular, an effective machine learning modeling process requires solid knowledge and understanding of the different types of machine learning algorithms. In addition, all machine learning algorithms require user-defined inputs to achieve a balance between accuracy and generalizability. This task is referred to as Hyperparameter Tuning . Thus, in practice, data scientists work hard to find the best model or algorithm that meets the specifications of their prob-lem. Such iterative and explorative nature of the modeling process is commonly tedious and time-consuming. We demonstrate SmartML , a meta learning-based framework for automated selection and hyperparameter tuning for machine learning algorithms. Being meta learning-based, the framework is able to simulate the role of the machine learning expert. In particular, the framework is equipped with a continuously updated knowledge base that stores information about the meta-features of all processed datasets along with the associated performance of the different classifiers and their tuned parameters. Thus, for any new dataset, SmartML automatically extracts its meta features and searches its knowledge base for the best performing algorithm to start its optimization process. In addition, SmartML makes use of the new runs to continuously en-rich its knowledge base to improve its performance and robustness for future runs. We will show how our approach outperforms the-state-of-the-art techniques in the domain of automated machine learning frameworks.","",""
20,"Di Wu, Binxing Fang, Junnan Wang, Qixu Liu, Xiang Cui","Evading Machine Learning Botnet Detection Models via Deep Reinforcement Learning",2019,"","","","",92,"2022-07-13 09:25:27","","10.1109/ICC.2019.8761337","","",,,,,20,6.67,4,5,3,"Botnets are one of predominant threats to Internet security. To date, machine learning technology has wide application in botnet detection because that it is able to summarize the features of existing attacks and generalize to never-before-seen botnet families. However, recent works in adversarial machine learning have shown that attackers are able to bypass the detection model by constructing specific samples, which due to many algorithms are vulnerable to almost imperceptible perturbations of their inputs. According to the degree of adversaries' knowledge about the model, adversarial attacks can be classified into several groups, such as gradient- and score-based attacks. In this paper, we propose a more general framework based on deep reinforcement learning (DRL), which effectively generates adversarial traffic flows to deceive the detection model by automatically adding perturbations to samples. Throughout the process, the target detector will be regarded as a black box and more close to realistic attack circumstance. A reinforcement learning agent is equipped for updating the adversarial samples by combining the feedback from the target model (i.e. benign or malicious) and the sequence of actions, which is able to change the temporal and spatial features of the traffic flows while maintaining the original functionality and executability. The experiment results show that the evasion rates of adversarial botnet flows are significantly improved. Furthermore, with the perspective of defense, this research can help the detection model spot its defect and thus enhance the robustness.","",""
19,"T. Le, M. Penna, D. Winkler, I. Yarovsky","Quantitative design rules for protein-resistant surface coatings using machine learning",2019,"","","","",93,"2022-07-13 09:25:27","","10.1038/s41598-018-36597-5","","",,,,,19,6.33,5,4,3,"","",""
18,"P. Fusar-Poli, Dominic Stringer, Alice M. S. Durieux, G. Rutigliano, I. Bonoldi, A. De Micheli, D. Ståhl","Clinical-learning versus machine-learning for transdiagnostic prediction of psychosis onset in individuals at-risk",2019,"","","","",94,"2022-07-13 09:25:27","","10.1038/s41398-019-0600-9","","",,,,,18,6.00,3,7,3,"","",""
16,"Mo Zhou, Yoshimi Fukuoka, Ken Goldberg, E. Vittinghoff, Anil Aswani","Applying machine learning to predict future adherence to physical activity programs",2019,"","","","",95,"2022-07-13 09:25:27","","10.1186/s12911-019-0890-0","","",,,,,16,5.33,3,5,3,"","",""
3,"M. Mozina","Arguments in Interactive Machine Learning",2018,"","","","",96,"2022-07-13 09:25:27","","","","",,,,,3,0.75,3,1,4,"In most applications of machine learning, domain experts provide domain specic knowledge. From previous experience it is known that domain experts are unable to provide all relevant knowledge in advance, but need to see some results of machine learning rst. Interactive machine learning, where experts and machine learning algorithm improve the model in turns, seems to solve this problem. In this position paper, we propose to use arguments in interaction between machine learning and experts. Since using and understanding arguments is a practical skill that humans learn in everyday life, we believe that arguments will help experts to better understand the models, facilitate easier elicitation of new knowledge from experts, and can be intuitively integrated in machine learning. We describe an argument-based dialogue, which is based on a series of steps such as questions and arguments, that can help obtain from a domain expert exactly that knowledge which is missing in the current model.","",""
13,"Karolis Liulys","Machine Learning Application in Predictive Maintenance",2019,"","","","",97,"2022-07-13 09:25:27","","10.1109/ESTREAM.2019.8732146","","",,,,,13,4.33,13,1,3,"Industrial organizations worldwide cannot ignore Industry 4.0 and its impact to their businesses. The biggest struggle is to find the way how to adopt all the possibilities for each plants unique use cases. In those situations where it is hard to find unified solutions internet is playing major part. Inseparable part of Industry 4.0 is Internet of Things (IoT) paradigm, where it is possible to connect all devices into united system. While robust Distributed Control Systems (DCS) are preferred for their safety, Industrial IoT (IIoT) allows next level prospects: big data performance analyzation, control patterns identification and predictive preventative maintenance by using machine learning algorithms. The study shows how implementing open-source software enables engineers to develop predictive maintenance applications with basic programming knowledge. These type of applications can be widely used in industrial field to inform about possible equipment malfunction helping reduce possible damages.","",""
0,"Rafael M. Frongillo","Machine Learning and Microeconomics Elicitation and Crowdsourcing",2015,"","","","",98,"2022-07-13 09:25:27","","","","",,,,,0,0.00,0,1,7,"The focus of my research is on theoretical problems at the interface between machine learning and microeconomics. This interface is broad, spanning domains such as crowdsourcing and prediction markets, to finance and algorithmic game theory. I am particularly interested in applying machine learning theory to economic domains, such as using coordinate descent to understand market behavior, and conversely using economic models to develop more robust and realistic techniques for machine learning.","",""
32,"Qigang Li, Keyan Zhao, C. Bustamante, Xin Ma, W. Wong","Xrare: a machine learning method jointly modeling phenotypes and genetic evidence for rare disease diagnosis",2019,"","","","",99,"2022-07-13 09:25:27","","10.1038/s41436-019-0439-8","","",,,,,32,10.67,6,5,3,"","",""
24,"Saikat Das, Ph.D., Ahmed M. Mahfouz, D. Venugopal, S. Shiva","DDoS Intrusion Detection Through Machine Learning Ensemble",2019,"","","","",100,"2022-07-13 09:25:27","","10.1109/QRS-C.2019.00090","","",,,,,24,8.00,6,4,3,"Distributed Denial of Service (DDoS) attacks have been the prominent attacks over the last decade. A Network Intrusion Detection System (NIDS) should seamlessly configure to fight against these attackers' new approaches and patterns of DDoS attack. In this paper, we propose a NIDS which can detect existing as well as new types of DDoS attacks. The key feature of our NIDS is that it combines different classifiers using ensemble models, with the idea that each classifier can target specific aspects/types of intrusions, and in doing so provides a more robust defense mechanism against new intrusions. Further, we perform a detailed analysis of DDoS attacks, and based on this domain-knowledge verify the reduced feature set [27, 28] to significantly improve accuracy. We experiment with and analyze NSL-KDD dataset with reduced feature set and our proposed NIDS can detect 99.1% of DDoS attacks successfully. We compare our results with other existing approaches. Our NIDS approach has the learning capability to keep up with new and emerging DDoS attack patterns.","",""
12,"Atik Mahabub","A robust voting approach for diabetes prediction using traditional machine learning techniques",2019,"","","","",101,"2022-07-13 09:25:27","","10.1007/s42452-019-1759-7","","",,,,,12,4.00,12,1,3,"","",""
657,"Wieland Brendel, Jonas Rauber, M. Bethge","Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",2017,"","","","",102,"2022-07-13 09:25:27","","","","",,,,,657,131.40,219,3,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .","",""
11,"J. Halotel, V. Demyanov, A. Gardiner","Value of Geologically Derived Features in Machine Learning Facies Classification",2019,"","","","",103,"2022-07-13 09:25:27","","10.1007/s11004-019-09838-0","","",,,,,11,3.67,4,3,3,"","",""
10,"Yifan Cui, E. Tchetgen","Bias-aware model selection for machine learning of doubly robust functionals",2019,"","","","",104,"2022-07-13 09:25:27","","","","",,,,,10,3.33,5,2,3,"While model selection is a well-studied topic in parametric and nonparametric regression or density estimation, model selection of possibly high dimensional nuisance parameters in semiparametric problems is far less developed. In this paper, we propose a new model selection framework for making inferences about a finite dimensional functional defined on a semiparametric model, when the latter admits a doubly robust estimating function. The class of such doubly robust functionals is quite large, including many missing data and causal inference problems. Under double robustness, the estimated functional should incur no bias if either of two nuisance parameters is evaluated at the truth while the other spans a large collection of candidate models. We introduce two model selection criteria for bias reduction of functional of interest, each based on a novel definition of pseudo-risk for the functional that embodies this double robustness property and thus may be used to select the candidate model that is nearest to fulfilling this property even when all models are wrong. Both selection criteria have a bias awareness property that selection of one nuisance parameter can be made to compensate for excessive bias due to poor learning of the other nuisance parameter. We establish an oracle property for a multi-fold cross-validation version of the new model selection criteria which states that our empirical criteria perform nearly as well as an oracle with a priori knowledge of the pseudo-risk for each candidate model. We also describe a smooth approximation to the selection criteria which allows for valid post-selection inference. Finally, we apply the approach to perform model selection of a semiparametric estimator of average treatment effect given an ensemble of candidate machine learning methods to account for confounding in a study of right heart catheterization in the intensive care unit of critically ill patients.","",""
48,"H. Aghakhani, Fabio Gritti, Francesco Mecca, Martina Lindorfer, Stefano Ortolani, D. Balzarotti, Giovanni Vigna, C. Kruegel","When Malware is Packin' Heat; Limits of Machine Learning Classifiers Based on Static Analysis Features",2020,"","","","",105,"2022-07-13 09:25:27","","10.14722/ndss.2020.24310","","",,,,,48,24.00,6,8,2,"Machine learning techniques are widely used in addition to signatures and heuristics to increase the detection rate of anti-malware software, as they automate the creation of detection models, making it possible to handle an ever-increasing number of new malware samples. In order to foil the analysis of anti-malware systems and evade detection, malware uses packing and other forms of obfuscation. However, few realize that benign applications use packing and obfuscation as well, to protect intellectual property and prevent license abuse. In this paper, we study how machine learning based on static analysis features operates on packed samples. Malware researchers have often assumed that packing would prevent machine learning techniques from building effective classifiers. However, both industry and academia have published results that show that machine-learning-based classifiers can achieve good detection rates, leading many experts to think that classifiers are simply detecting the fact that a sample is packed, as packing is more prevalent in malicious samples. We show that, different from what is commonly assumed, packers do preserve some information when packing programs that is “useful” for malware classification. However, this information does not necessarily capture the sample’s behavior. We demonstrate that the signals extracted from packed executables are not rich enough for machine-learning-based models to (1) generalize their knowledge to operate on unseen packers, and (2) be robust against adversarial examples. We also show that a naı̈ve application of machine learning techniques results in a substantial number of false positives, which, in turn, might have resulted in incorrect labeling of ground-truth data used in past work.","",""
10,"E. Adabor, G. Acquaah-Mensah","Machine learning approaches to decipher hormone and HER2 receptor status phenotypes in breast cancer",2019,"","","","",106,"2022-07-13 09:25:27","","10.1093/bib/bbx138","","",,,,,10,3.33,5,2,3,"Breast cancer prognosis and administration of therapies are aided by knowledge of hormonal and HER2 receptor status. Breast cancer lacking estrogen receptors, progesterone receptors and HER2 receptors are difficult to treat. Regarding large data repositories such as The Cancer Genome Atlas, available wet-lab methods for establishing the presence of these receptors do not always conclusively cover all available samples. To this end, we introduce median-supplement methods to identify hormonal and HER2 receptor status phenotypes of breast cancer patients using gene expression profiles. In these approaches, supplementary instances based on median patient gene expression are introduced to balance a training set from which we build simple models to identify the receptor expression status of patients. In addition, for the purpose of benchmarking, we examine major machine learning approaches that are also applicable to the problem of finding receptor status in breast cancer. We show that our methods are robust and have high sensitivity with extremely low false-positive rates compared with the well-established methods. A successful application of these methods will permit the simultaneous study of large collections of samples of breast cancer patients as well as save time and cost while standardizing interpretation of outcomes of such studies.","",""
18,"S. Fleming, A. Goodbody","A Machine Learning Metasystem for Robust Probabilistic Nonlinear Regression-Based Forecasting of Seasonal Water Availability in the US West",2019,"","","","",107,"2022-07-13 09:25:27","","10.1109/ACCESS.2019.2936989","","",,,,,18,6.00,9,2,3,"Hydroelectric power generation, water supplies for municipal, agricultural, manufacturing, and service industry uses including technology-sector requirements, dam safety, flood control, recreational uses, and ecological and legal constraints, all place simultaneous, competing demands on the heavily stressed water management infrastructure of the mostly arid American West. Optimally managing these resources depends on predicting water availability. We built a probabilistic nonlinear regression water supply forecast (WSF) technique for the US Department of Agriculture, which runs the largest stand-alone WSF system in the US West. Design criteria included improved accuracy over the existing system; uncertainty estimates that seamlessly handle complex (heteroscedastic, non-Gaussian) prediction errors; integration of physical hydrometeorological process knowledge and domain-specific expert experience; ability to accommodate nonlinearity, model selection uncertainty and equifinality, and predictor multicollinearity and high dimensionality; and relatively easy, low-cost implementation. Some methods satisfied some of these requirements but none met all, leading us to develop a novel, interdisciplinary, and pragmatic prediction metasystem through a carefully considered synthesis of well-established, off-the-shelf components and approaches, spanning supervised and unsupervised machine learning, nonparametric statistical modeling, ensemble learning, and evolutionary optimization, focusing on maintaining but radically updating the principal components regression framework widely used for WSF. Testing this integrated multi-method prediction engine demonstrated its value for river forecasting; USDA adoption is a landmark for transitioning machine learning from research into practice in this field. Its ability to handle all the foregoing design criteria and requirements, which are not unique to WSF, suggests potential for extension to complex probabilistic prediction problems in other fields.","",""
15,"Jungryul Seo, T. Laine, Kyung-ah Sohn","An Exploration of Machine Learning Methods for Robust Boredom Classification Using EEG and GSR Data",2019,"","","","",108,"2022-07-13 09:25:27","","10.3390/s19204561","","",,,,,15,5.00,5,3,3,"In recent years, affective computing has been actively researched to provide a higher level of emotion-awareness. Numerous studies have been conducted to detect the user’s emotions from physiological data. Among a myriad of target emotions, boredom, in particular, has been suggested to cause not only medical issues but also challenges in various facets of daily life. However, to the best of our knowledge, no previous studies have used electroencephalography (EEG) and galvanic skin response (GSR) together for boredom classification, although these data have potential features for emotion classification. To investigate the combined effect of these features on boredom classification, we collected EEG and GSR data from 28 participants using off-the-shelf sensors. During data acquisition, we used a set of stimuli comprising a video clip designed to elicit boredom and two other video clips of entertaining content. The collected samples were labeled based on the participants’ questionnaire-based testimonies on experienced boredom levels. Using the collected data, we initially trained 30 models with 19 machine learning algorithms and selected the top three candidate classifiers. After tuning the hyperparameters, we validated the final models through 1000 iterations of 10-fold cross validation to increase the robustness of the test results. Our results indicated that a Multilayer Perceptron model performed the best with a mean accuracy of 79.98% (AUC: 0.781). It also revealed the correlation between boredom and the combined features of EEG and GSR. These results can be useful for building accurate affective computing systems and understanding the physiological properties of boredom.","",""
4,"E. Glaab, Armin Rauschenberger, R. Banzi, C. Gerardi, Paula Garcia, J. Demotes","Biomarker discovery studies for patient stratification using machine learning analysis of omics data: a scoping review",2021,"","","","",109,"2022-07-13 09:25:27","","10.1136/bmjopen-2021-053674","","",,,,,4,4.00,1,6,1,"Objective To review biomarker discovery studies using omics data for patient stratification which led to clinically validated FDA-cleared tests or laboratory developed tests, in order to identify common characteristics and derive recommendations for future biomarker projects. Design Scoping review. Methods We searched PubMed, EMBASE and Web of Science to obtain a comprehensive list of articles from the biomedical literature published between January 2000 and July 2021, describing clinically validated biomarker signatures for patient stratification, derived using statistical learning approaches. All documents were screened to retain only peer-reviewed research articles, review articles or opinion articles, covering supervised and unsupervised machine learning applications for omics-based patient stratification. Two reviewers independently confirmed the eligibility. Disagreements were solved by consensus. We focused the final analysis on omics-based biomarkers which achieved the highest level of validation, that is, clinical approval of the developed molecular signature as a laboratory developed test or FDA approved tests. Results Overall, 352 articles fulfilled the eligibility criteria. The analysis of validated biomarker signatures identified multiple common methodological and practical features that may explain the successful test development and guide future biomarker projects. These include study design choices to ensure sufficient statistical power for model building and external testing, suitable combinations of non-targeted and targeted measurement technologies, the integration of prior biological knowledge, strict filtering and inclusion/exclusion criteria, and the adequacy of statistical and machine learning methods for discovery and validation. Conclusions While most clinically validated biomarker models derived from omics data have been developed for personalised oncology, first applications for non-cancer diseases show the potential of multivariate omics biomarker design for other complex disorders. Distinctive characteristics of prior success stories, such as early filtering and robust discovery approaches, continuous improvements in assay design and experimental measurement technology, and rigorous multicohort validation approaches, enable the derivation of specific recommendations for future studies.","",""
8,"Mazaher Kianpour, Shao-Fang Wen","Timing Attacks on Machine Learning: State of the Art",2019,"","","","",110,"2022-07-13 09:25:27","","10.1007/978-3-030-29516-5_10","","",,,,,8,2.67,4,2,3,"","",""
1,"Alessio Ragno, A. Baldisserotto, L. Antonini, M. Sabatino, F. Sapienza, E. Baldini, Raissa Buzzi, S. Vertuani, S. Manfredini","Machine Learning Data Augmentation as a Tool to Enhance Quantitative Composition–Activity Relationships of Complex Mixtures. A New Application to Dissect the Role of Main Chemical Components in Bioactive Essential Oils",2021,"","","","",111,"2022-07-13 09:25:27","","10.3390/molecules26206279","","",,,,,1,1.00,0,9,1,"Scientific investigation on essential oils composition and the related biological profile are continuously growing. Nevertheless, only a few studies have been performed on the relationships between chemical composition and biological data. Herein, the investigation of 61 assayed essential oils is reported focusing on their inhibition activity against Microsporum spp. including development of machine learning models with the aim of highlining the possible chemical components mainly related to the inhibitory potency. The application of machine learning and deep learning techniques for predictive and descriptive purposes have been applied successfully to many fields. Quantitative composition–activity relationships machine learning-based models were developed for the 61 essential oils tested as Microsporum spp. growth modulators. The models were built with in-house python scripts implementing data augmentation with the purpose of having a smoother flow between essential oils’ chemical compositions and biological data. High statistical coefficient values (Accuracy, Matthews correlation coefficient and F1 score) were obtained and model inspection permitted to detect possible specific roles related to some components of essential oils’ constituents. Robust machine learning models are far more useful tools to reveal data augmentation in comparison with raw data derived models. To the best of the authors knowledge this is the first report using data augmentation to highlight the role of complex mixture components, in particular a first application of these data will be for the development of ingredients in the dermo-cosmetic field investigating microbial species considering the urge for the use of natural preserving and acting antimicrobial agents.","",""
3,"Sergio Jiménez Celorrio, F. Fernández, D. Borrajo","Machine Learning of Plan Robustness Knowledge About Instances",2005,"","","","",112,"2022-07-13 09:25:27","","10.1007/11564096_60","","",,,,,3,0.18,1,3,17,"","",""
3,"A. Derville, G. Gey, J. Baderot, S. Martínez, G. Bernard, J. Foucher","Using machine learning technology to accelerate the development of plasma etching processes",2019,"","","","",113,"2022-07-13 09:25:27","","10.1117/12.2514705","","",,,,,3,1.00,1,6,3,"The latest advances in Machine Learning (ML) produce results with unprecedented accuracy, and could signal a new era in the smart manufacturing field. We propose a framework designed to work alongside experts: learning from them and optimizing their knowledge. This framework must be considered as a tool to assist the experts in their daily work. The user creates a measurement recipe which includes an example of the feature as well as the measurements placed by the process engineer. Grouping the measurement recipes of the same object in an entity collection allows the user to train a machine learning recipe which includes a deformation model to handle variations in structure and contrast. The new images are analyzed following the machine learning pipeline which includes the detection of features, repositioning, measurement, quality evaluation and finally the results of measurement are given to the user. We discuss the pipeline and we focus on the metrics to validate the machine learning recipe, providing quantitative results for stability and robustness to variations.","",""
4,"Niko Murrell, Ryan Bradley, N. Bajaj, Julie Whitney, G. Chiu","A Method for Sensor Reduction in a Supervised Machine Learning Classification System",2019,"","","","",114,"2022-07-13 09:25:27","","10.1109/TMECH.2018.2881889","","",,,,,4,1.33,1,5,3,"Smart devices employing interconnected sensors for feedback and control are being rapidly adopted. Many useful applications for these devices are in markets that demand cost-conscious solutions. Traditional machine-learning-based control systems often rely on multiple measurements from many sensors to achieve performance targets. An alternative method is presented that leverages a time-series output produced by a single sensor. By using domain expert knowledge, the time-series output is discretized into finite intervals that correspond to the physical events occurring in the system. Statistical measures are taken across these intervals to serve as the features to the machine learning system. Additional features that decouple key physical metrics are identified, improving the performance of the system. This novel approach requires a more modest dataset and does not compromise performance. The resulting development effort is significantly more cost-effective than traditional sensor classification systems, not only due to the reduced sensor count, but also due to a significantly simplified and more robust algorithm development and testing step. Results are presented with the case study of a media-type classification system within a printing system, which was deployed to the field as a commercial product.","",""
1,"E. Kuiper, Efthymios Constantinides, S. Vries, Robert F. Marinescu-Muster, F. Metzner","A Framework of Unsupervised Machine Learning Algorithms for User Profiling",2019,"","","","",115,"2022-07-13 09:25:27","","","","",,,,,1,0.33,0,5,3,"Organizations often have difficulties to extract knowledge from data and selecting appropriate Machine Learning algorithms in order to develop accurate Behavioural Profiles or user segments. Moreover, marketing departments often lack a fundamental understanding on data-driven segmentation methodologies. This paper aims to develop a framework outlining Unsupervised Machine Learning algorithms for the purpose of User Profiling with respect to important data properties. A systematic literature review was conducted on the most prominent Unsupervised Machine Learning algorithms and their requirements regarding the characteristics of the dataset. A framework is proposed outlining various Unsupervised Machine Learning algorithms for User Profiling. It provides two-stage clustering strategies for categorical, numerical, and mixed types of data with respect to the data size and data dimensionality. The first stage consists of an hierarchical or model-based clustering algorithm to determine the number of clusters. In the second stage, a non-hierarchical clustering algorithm is applied for cluster refinement. The framework can support researchers and practitioners to determine which Unsupervised Machine Learning algorithms are appropriate for developing robust behavioural profiles or data-driven user segments.","",""
25,"L. Micallef, Iiris Sundin, P. Marttinen, Muhammad Ammad-ud-din, T. Peltola, Marta Soare, G. Jacucci, Samuel Kaski","Interactive Elicitation of Knowledge on Feature Relevance Improves Predictions in Small Data Sets",2016,"","","","",116,"2022-07-13 09:25:27","","10.1145/3025171.3025181","","",,,,,25,4.17,3,8,6,"Providing accurate predictions is challenging for machine learning algorithms when the number of features is larger than the number of samples in the data. Prior knowledge can improve machine learning models by indicating relevant variables and parameter values. Yet, this prior knowledge is often tacit and only available from domain experts. We present a novel approach that uses interactive visualization to elicit the tacit prior knowledge and uses it to improve the accuracy of prediction models. The main component of our approach is a user model that models the domain expert's knowledge of the relevance of different features for a prediction task. In particular, based on the expert's earlier input, the user model guides the selection of the features on which to elicit user's knowledge next. The results of a controlled user study show that the user model significantly improves prior knowledge elicitation and prediction accuracy, when predicting the relative citation counts of scientific documents in a specific domain.","",""
37,"Tianwei Yu, Dean P. Jones","Improving peak detection in high-resolution LC/MS metabolomics data using preexisting knowledge and machine learning approach",2014,"","","","",117,"2022-07-13 09:25:27","","10.1093/bioinformatics/btu430","","",,,,,37,4.63,19,2,8,"MOTIVATION Peak detection is a key step in the preprocessing of untargeted metabolomics data generated from high-resolution liquid chromatography-mass spectrometry (LC/MS). The common practice is to use filters with predetermined parameters to select peaks in the LC/MS profile. This rigid approach can cause suboptimal performance when the choice of peak model and parameters do not suit the data characteristics.   RESULTS Here we present a method that learns directly from various data features of the extracted ion chromatograms (EICs) to differentiate between true peak regions from noise regions in the LC/MS profile. It utilizes the knowledge of known metabolites, as well as robust machine learning approaches. Unlike currently available methods, this new approach does not assume a parametric peak shape model and allows maximum flexibility. We demonstrate the superiority of the new approach using real data. Because matching to known metabolites entails uncertainties and cannot be considered a gold standard, we also developed a probabilistic receiver-operating characteristic (pROC) approach that can incorporate uncertainties.   AVAILABILITY AND IMPLEMENTATION The new peak detection approach is implemented as part of the apLCMS package available at http://web1.sph.emory.edu/apLCMS/ CONTACT: tyu8@emory.edu   SUPPLEMENTARY INFORMATION Supplementary data are available at Bioinformatics online.","",""
20,"D. Grana, L. Azevedo, Mingliang Liu","A comparison of deep machine learning and Monte Carlo methods for facies classification from seismic data",2020,"","","","",118,"2022-07-13 09:25:27","","10.1190/geo2019-0405.1","","",,,,,20,10.00,7,3,2,"Among the large variety of mathematical and computational methods for estimating reservoir properties such as facies and petrophysical variables from geophysical data, deep machine-learning algorithms have gained significant popularity for their ability to obtain accurate solutions for geophysical inverse problems in which the physical models are partially unknown. Solutions of classification and inversion problems are generally not unique, and uncertainty quantification studies are required to quantify the uncertainty in the model predictions and determine the precision of the results. Probabilistic methods, such as Monte Carlo approaches, provide a reliable approach for capturing the variability of the set of possible models that match the measured data. Here, we focused on the classification of facies from seismic data and benchmarked the performance of three different algorithms: recurrent neural network, Monte Carlo acceptance/rejection sampling, and Markov chain Monte Carlo. We tested and validated these approaches at the well locations by comparing classification predictions to the reference facies profile. The accuracy of the classification results is defined as the mismatch between the predictions and the log facies profile. Our study found that when the training data set of the neural network is large enough and the prior information about the transition probabilities of the facies in the Monte Carlo approach is not informative, machine-learning methods lead to more accurate solutions; however, the uncertainty of the solution might be underestimated. When some prior knowledge of the facies model is available, for example, from nearby wells, Monte Carlo methods provide solutions with similar accuracy to the neural network and allow a more robust quantification of the uncertainty, of the solution.","",""
39,"Krishnamurthy Dvijotham, Jamie Hayes, Borja Balle, J. Z. Kolter, Chongli Qin, A. György, Kai Y. Xiao, Sven Gowal, Pushmeet Kohli","A Framework for robustness Certification of Smoothed Classifiers using F-Divergences",2020,"","","","",119,"2022-07-13 09:25:27","","","","",,,,,39,19.50,4,9,2,"Formal verification techniques that compute provable guarantees on properties of machine learning models, like robustness to norm-bounded adversarial perturbations, have yielded impressive results. Although most techniques developed so far requires knowledge of the architecture of the machine learning model and remains hard to scale to complex prediction pipelines, the method of randomized smoothing has been shown to overcome many of these obstacles. By requiring only black-box access to the underlying model, randomized smoothing scales to large architectures and is agnostic to the internals of the network. However, past work on randomized smoothing has focused on restricted classes of smoothing measures or perturbations (like Gaussian or discrete) and has only been able to prove robustness with respect to simple norm bounds. In this paper we introduce a general framework for proving robustness properties of smoothed machine learning models in the black-box setting. Specifically, we extend randomized smoothing procedures to handle arbitrary smoothing measures and prove robustness of the smoothed classifier by using $f$-divergences. Our methodology achieves state-of-the-art}certified robustness on MNIST, CIFAR-10 and ImageNet and also audio classification task, Librispeech, with respect to several classes of adversarial perturbations.","",""
8,"Benjamin Sliwa, Cedrik Schüler, Manuel Patchou, C. Wietfeld","PARRoT: Predictive Ad-hoc Routing Fueled by Reinforcement Learning and Trajectory Knowledge",2020,"","","","",120,"2022-07-13 09:25:27","","10.1109/VTC2021-Spring51267.2021.9448959","","",,,,,8,4.00,2,4,2,"Swarms of collaborating Unmanned Aerial Vehicles (UAVs) that utilize ad-hoc networking technologies for coordinating their actions offer the potential to catalyze emerging research fields such as autonomous exploration of disaster areas, demand-driven network provisioning, and near field packet delivery in Intelligent Transportation Systems (ITSs). As these mobile robotic networks are characterized by high grades of relative mobility, existing routing protocols often fail to adopt their decision making to the implied network topology dynamics. For addressing these challenges, we present Predictive Ad-hoc Routing fueled by Reinforcement learning and Trajectory knowledge (PARRoT) as a novel machine learning-enabled routing protocol which exploits mobility control information for integrating knowledge about the future motion of the mobile agents into the routing process. The performance of the proposed routing approach is evaluated using comprehensive network simulation. In comparison to established routing protocols, PARRoT achieves a massively higher robustness and a significantly lower end-to-end latency.","",""
8,"Hal S. Greenwald, Carsten K. Oertel","Future Directions in Machine Learning",2017,"","","","",121,"2022-07-13 09:25:27","","10.3389/frobt.2016.00079","","",,,,,8,1.60,4,2,5,"Current machine learning algorithms identify statistical regularities in complex data sets and are regularly used across a range of application domains, but they lack the robustness and generalizability associated with human learning. If machine learning techniques could enable computers to learn from fewer examples, transfer knowledge between tasks, and adapt to changing contexts and environments, the results would have very broad scientific and societal impacts. Increased processing and memory resources have enabled larger, more capable learning models, but there is growing recognition that even greater computing resources would not be sufficient to yield algorithms capable of learning from a few examples and generalizing beyond initial training sets. This paper presents perspectives on feature selection, representation schemes and interpretability, transfer learning, continuous learning, and learning and adaptation in time-varying contexts and environments, five key areas that are essential for advancing machine learning capabilities. Appropriate learning tasks that require these capabilities can demonstrate the strengths of novel machine learning approaches that could address these challenges.","",""
29,"Violeta Mirchevska, M. Luštrek, M. Gams","Combining domain knowledge and machine learning for robust fall detection",2014,"","","","",122,"2022-07-13 09:25:27","","10.1111/exsy.12019","","",,,,,29,3.63,10,3,8,"This paper presents a method for combining domain knowledge and machine learning (CDKML) for classifier generation and online adaptation. The method exploits advantages in domain knowledge and machine learning as complementary information sources. Whereas machine learning may discover patterns in interest domains that are too subtle for humans to detect, domain knowledge may contain information on a domain not present in the available domain dataset. CDKML has three steps. First, prior domain knowledge is enriched with relevant patterns obtained by machine learning to create an initial classifier. Second, genetic algorithms refine the classifier. Third, the classifier is adapted online on the basis of user feedback using the Markov decision process. CDKML was applied in fall detection. Tests showed that the classifiers developed by CDKML have better performance than machine‐learning classifiers generated on a training dataset that does not adequately represent all real‐life cases of the learned concept. The accuracy of the initial classifier was 10 percentage points higher than the best machine‐learning classifier and the refinement added 3 percentage points. The online adaptation improved the accuracy of the refined classifier by an additional 15 percentage points.","",""
0,"P. Keogh","Eliciting Knowledge Bases with Defeasible Reasoning: A Comparative Analysis with Machine Learning",2015,"","","","",123,"2022-07-13 09:25:27","","","","",,,,,0,0.00,0,1,7,"Faculty Name DIT School of Computing MSc in Computing (Advanced Software Development) Eliciting Knowledge Bases with Defeasible Reasoning: A Comparative Analysis with Machine Learning by Peter KEOGH This thesis compares the ability of an implementation of Defeasible Reasoning (via Argumentation Theory) to model a construct (mental workload) with Machine Learning. In order to perform this comparison a defeasible reasoning system was designed and implemented in software. This software was used to elicit a knowledge base from an expert in an experiment which was then compared with machine learning. The central findings of this thesis were that the knowledge based approach was better at predicting an objective performance measure, time, than machine learning. However, machine learning was better equiped to identify another object measure task membership. The knowledge base of the expert had a high concurrent validity with objective performance measures and a high convergent validity with existing measures of mental workload.","",""
2,"A. Nikitin, S. Kaski","Decision Rule Elicitation for Domain Adaptation",2021,"","","","",124,"2022-07-13 09:25:27","","10.1145/3397481.3450682","","",,,,,2,2.00,1,2,1,"Human-in-the-loop machine learning is widely used in artificial intelligence (AI) to elicit labels for data points from experts or to provide feedback on how close the predicted results are to the target. This simplifies away all the details of the decision-making process of the expert. In this work, we allow the experts to additionally produce decision rules describing their decision-making; the rules are expected to be imperfect but to give additional information. In particular, the rules can extend to new distributions, and hence enable significantly improving performance for cases where the training and testing distributions differ, such as in domain adaptation. We apply the proposed method to lifelong learning and domain adaptation problems and discuss applications in other branches of AI, such as knowledge acquisition problems in expert systems. In simulated and real-user studies, we show that decision rule elicitation improves domain adaptation of the algorithm and helps to propagate expert’s knowledge to the AI model.","",""
3,"D. Kurrant, M. Omer, Nasim Abdollahi, P. Mojabi, E. Fear, J. Lovetri","Evaluating Performance of Microwave Image Reconstruction Algorithms: Extracting Tissue Types with Segmentation Using Machine Learning",2021,"","","","",125,"2022-07-13 09:25:27","","10.3390/jimaging7010005","","",,,,,3,3.00,1,6,1,"Evaluating the quality of reconstructed images requires consistent approaches to extracting information and applying metrics. Partitioning medical images into tissue types permits the quantitative assessment of regions that contain a specific tissue. The assessment facilitates the evaluation of an imaging algorithm in terms of its ability to reconstruct the properties of various tissue types and identify anomalies. Microwave tomography is an imaging modality that is model-based and reconstructs an approximation of the actual internal spatial distribution of the dielectric properties of a breast over a reconstruction model consisting of discrete elements. The breast tissue types are characterized by their dielectric properties, so the complex permittivity profile that is reconstructed may be used to distinguish different tissue types. This manuscript presents a robust and flexible medical image segmentation technique to partition microwave breast images into tissue types in order to facilitate the evaluation of image quality. The approach combines an unsupervised machine learning method with statistical techniques. The key advantage for using the algorithm over other approaches, such as a threshold-based segmentation method, is that it supports this quantitative analysis without prior assumptions such as knowledge of the expected dielectric property values that characterize each tissue type. Moreover, it can be used for scenarios where there is a scarcity of data available for supervised learning. Microwave images are formed by solving an inverse scattering problem that is severely ill-posed, which has a significant impact on image quality. A number of strategies have been developed to alleviate the ill-posedness of the inverse scattering problem. The degree of success of each strategy varies, leading to reconstructions that have a wide range of image quality. A requirement for the segmentation technique is the ability to partition tissue types over a range of image qualities, which is demonstrated in the first part of the paper. The segmentation of images into regions of interest corresponding to various tissue types leads to the decomposition of the breast interior into disjoint tissue masks. An array of region and distance-based metrics are applied to compare masks extracted from reconstructed images and ground truth models. The quantitative results reveal the accuracy with which the geometric and dielectric properties are reconstructed. The incorporation of the segmentation that results in a framework that effectively furnishes the quantitative assessment of regions that contain a specific tissue is also demonstrated. The algorithm is applied to reconstructed microwave images derived from breasts with various densities and tissue distributions to demonstrate the flexibility of the algorithm and that it is not data-specific. The potential for using the algorithm to assist in diagnosis is exhibited with a tumor tracking example. This example also establishes the usefulness of the approach in evaluating the performance of the reconstruction algorithm in terms of its sensitivity and specificity to malignant tissue and its ability to accurately reconstruct malignant tissue.","",""
13,"Koosha Sadeghi, A. Banerjee, S. Gupta","A System-Driven Taxonomy of Attacks and Defenses in Adversarial Machine Learning",2020,"","","","",126,"2022-07-13 09:25:27","","10.1109/TETCI.2020.2968933","","",,,,,13,6.50,4,3,2,"Machine Learning (ML) algorithms, specifically supervised learning, are widely used in modern real-world applications, which utilize Computational Intelligence (CI) as their core technology, such as autonomous vehicles, assistive robots, and biometric systems. Attacks that cause misclassifications or mispredictions can lead to erroneous decisions resulting in unreliable operations. Designing robust ML with the ability to provide reliable results in the presence of such attacks has become a top priority in the field of adversarial machine learning. An essential characteristic for rapid development of robust ML is an arms race between attack and defense strategists. However, an important prerequisite for the arms race is access to a well-defined system model so that experiments can be repeated by independent researchers. This article proposes a fine-grained system-driven taxonomy to specify ML applications and adversarial system models in an unambiguous manner such that independent researchers can replicate experiments and escalate the arms race to develop more evolved and robust ML applications. The article provides taxonomies for: 1) the dataset, 2) the ML architecture, 3) the adversary's knowledge, capability, and goal, 4) adversary's strategy, and 5) the defense response. In addition, the relationships among these models and taxonomies are analyzed by proposing an adversarial machine learning cycle. The provided models and taxonomies are merged to form a comprehensive system-driven taxonomy, which represents the arms race between the ML applications and adversaries in recent years. The taxonomies encode best practices in the field and help evaluate and compare the contributions of research works and reveals gaps in the field.","",""
2,"Kar Fye Alvin Lee, W. Gan, Georgios Christopoulos","Biomarker-Informed Machine Learning Model of Cognitive Fatigue from a Heart Rate Response Perspective",2021,"","","","",127,"2022-07-13 09:25:27","","10.3390/s21113843","","",,,,,2,2.00,1,3,1,"Cognitive fatigue is a psychological state characterised by feelings of tiredness and impaired cognitive functioning arising from high cognitive demands. This paper examines the recent research progress on the assessment of cognitive fatigue and provides informed recommendations for future research. Traditionally, cognitive fatigue is introspectively assessed through self-report or objectively inferred from a decline in behavioural performance. However, more recently, researchers have attempted to explore the biological underpinnings of cognitive fatigue to understand and measure this phenomenon. In particular, there is evidence indicating that the imbalance between sympathetic and parasympathetic nervous activity appears to be a physiological correlate of cognitive fatigue. This imbalance has been indexed through various heart rate variability indices that have also been proposed as putative biomarkers of cognitive fatigue. Moreover, in contrast to traditional inferential methods, there is also a growing research interest in using data-driven approaches to assessing cognitive fatigue. The ubiquity of wearables with the capability to collect large amounts of physiological data appears to be a major facilitator in the growth of data-driven research in this area. Preliminary findings indicate that such large datasets can be used to accurately predict cognitive fatigue through various machine learning approaches. Overall, the potential of combining domain-specific knowledge gained from biomarker research with machine learning approaches should be further explored to build more robust predictive models of cognitive fatigue.","",""
12,"F. López Seguí, Ricardo Ander Egg Aguilar, Gabriel de Maeztu, A. García‐Altés, Francesc García Cuyàs, Sandra Walsh, Marta Sagarra Castro, J. Vidal-Alaball","Teleconsultations between Patients and Healthcare Professionals in Primary Care in Catalonia: The Evaluation of Text Classification Algorithms Using Supervised Machine Learning",2020,"","","","",128,"2022-07-13 09:25:27","","10.3390/ijerph17031093","","",,,,,12,6.00,2,8,2,"Background: The primary care service in Catalonia has operated an asynchronous teleconsulting service between GPs and patients since 2015 (eConsulta), which has generated some 500,000 messages. New developments in big data analysis tools, particularly those involving natural language, can be used to accurately and systematically evaluate the impact of the service. Objective: The study was intended to assess the predictive potential of eConsulta messages through different combinations of vector representation of text and machine learning algorithms and to evaluate their performance. Methodology: Twenty machine learning algorithms (based on five types of algorithms and four text representation techniques) were trained using a sample of 3559 messages (169,102 words) corresponding to 2268 teleconsultations (1.57 messages per teleconsultation) in order to predict the three variables of interest (avoiding the need for a face-to-face visit, increased demand and type of use of the teleconsultation). The performance of the various combinations was measured in terms of precision, sensitivity, F-value and the ROC curve. Results: The best-trained algorithms are generally effective, proving themselves to be more robust when approximating the two binary variables “avoiding the need of a face-to-face visit” and “increased demand” (precision = 0.98 and 0.97, respectively) rather than the variable “type of query” (precision = 0.48). Conclusion: To the best of our knowledge, this study is the first to investigate a machine learning strategy for text classification using primary care teleconsultation datasets. The study illustrates the possible capacities of text analysis using artificial intelligence. The development of a robust text classification tool could be feasible by validating it with more data, making it potentially more useful for decision support for health professionals.","",""
8,"C. Rea, K. Montes, A. Pau, R. Granetz, O. Sauter","Progress Toward Interpretable Machine Learning–Based Disruption Predictors Across Tokamaks",2020,"","","","",129,"2022-07-13 09:25:27","","10.1080/15361055.2020.1798589","","",,,,,8,4.00,2,5,2,"Abstract In this paper we lay the groundwork for a robust cross-device comparison of data-driven disruption prediction algorithms on DIII-D and JET tokamaks. In order to consistently carry on a comparative analysis, we define physics-based indicators of disruption precursors based on temperature, density, and radiation profiles that are currently not used in many other machine learning predictors for DIII-D data. These profile-based indicators are shown to well-describe impurity accumulation events in both DIII-D and JET discharges that eventually disrupt. The univariate analysis of the features used as input signals in the data-driven algorithms applied on the data of both tokamaks statistically highlights the differences in the dominant disruption precursors. JET with its ITER-like wall is more prone to impurity accumulation events, while DIII-D is more subject to edge-cooling mechanisms that destabilize dangerous magnetohydrodynamic modes. Even though the analyzed data sets are characterized by such intrinsic differences, we show through a few examples that the inclusion of physics-based disruption markers in data-driven algorithms is a promising path toward the realization of a uniform framework to predict and interpret disruptive scenarios across different tokamaks. As long as the destabilizing precursors are diagnosed in a device-independent way, the knowledge that data-driven algorithms learn on one device can be re-used to explain a disruptive behavior on another device.","",""
33,"Megha Srivastava, Tatsunori B. Hashimoto, Percy Liang","Robustness to Spurious Correlations via Human Annotations",2020,"","","","",130,"2022-07-13 09:25:27","","","","",,,,,33,16.50,11,3,2,"The reliability of machine learning systems critically assumes that the associations between features and labels remain similar between training and test distributions. However, unmeasured variables, such as confounders, break this assumption---useful correlations between features and labels at training time can become useless or even harmful at test time. For example, high obesity is generally predictive for heart disease, but this relation may not hold for smokers who generally have lower rates of obesity and higher rates of heart disease. We present a framework for making models robust to spurious correlations by leveraging humans' common sense knowledge of causality. Specifically, we use human annotation to augment each training example with a potential unmeasured variable (i.e. an underweight patient with heart disease may be a smoker), reducing the problem to a covariate shift problem. We then introduce a new distributionally robust optimization objective over unmeasured variables (UV-DRO) to control the worst-case loss over possible test-time shifts. Empirically, we show improvements of 5-10% on a digit recognition task confounded by rotation, and 1.5-5% on the task of analyzing NYPD Police Stops confounded by location.","",""
4,"Neha Shah, D. Mohan, J. Bashingwa, O. Ummer, A. Chakraborty, A. Lefevre","Using Machine Learning to Optimize the Quality of Survey Data: Protocol for a Use Case in India",2020,"","","","",131,"2022-07-13 09:25:27","","10.2196/17619","","",,,,,4,2.00,1,6,2,"Background Data quality is vital for ensuring the accuracy, reliability, and validity of survey findings. Strategies for ensuring survey data quality have traditionally used quality assurance procedures. Data analytics is an increasingly vital part of survey quality assurance, particularly in light of the increasing use of tablets and other electronic tools, which enable rapid, if not real-time, data access. Routine data analytics are most often concerned with outlier analyses that monitor a series of data quality indicators, including response rates, missing data, and reliability of coefficients for test-retest interviews. Machine learning is emerging as a possible tool for enhancing real-time data monitoring by identifying trends in the data collection, which could compromise quality. Objective This study aimed to describe methods for the quality assessment of a household survey using both traditional methods as well as machine learning analytics. Methods In the Kilkari impact evaluation’s end-line survey amongst postpartum women (n=5095) in Madhya Pradesh, India, we plan to use both traditional and machine learning–based quality assurance procedures to improve the quality of survey data captured on maternal and child health knowledge, care-seeking, and practices. The quality assurance strategy aims to identify biases and other impediments to data quality and includes seven main components: (1) tool development, (2) enumerator recruitment and training, (3) field coordination, (4) field monitoring, (5) data analytics, (6) feedback loops for decision making, and (7) outcomes assessment. Analyses will include basic descriptive and outlier analyses using machine learning algorithms, which will involve creating features from time-stamps, “don’t know” rates, and skip rates. We will also obtain labeled data from self-filled surveys, and build models using k-folds cross-validation on a training data set using both supervised and unsupervised learning algorithms. Based on these models, results will be fed back to the field through various feedback loops. Results Data collection began in late October 2019 and will span through March 2020. We expect to submit quality assurance results by August 2020. Conclusions Machine learning is underutilized as a tool to improve survey data quality in low resource settings. Study findings are anticipated to improve the overall quality of Kilkari survey data and, in turn, enhance the robustness of the impact evaluation. More broadly, the proposed quality assurance approach has implications for data capture applications used for special surveys as well as in the routine collection of health information by health workers. International Registered Report Identifier (IRRID) DERR1-10.2196/17619","",""
4,"A. Morera, J. Martínez de Aragón, J. Bonet, Jingjing Liang, S. de-Miguel","Performance of statistical and machine learning-based methods for predicting biogeographical patterns of fungal productivity in forest ecosystems",2020,"","","","",132,"2022-07-13 09:25:27","","10.21203/rs.3.rs-122045/v1","","",,,,,4,2.00,1,5,2,"Background The prediction of biogeographical patterns from a large number of driving factors with complex interactions, correlations and non-linear dependences require advanced analytical methods and modeling tools. This study compares different statistical and machine learning-based models for predicting fungal productivity biogeographical patterns as a case study for the thorough assessment of the performance of alternative modeling approaches to provide accurate and ecologically-consistent predictions. Methods We evaluated and compared the performance of two statistical modeling techniques, namely, generalized linear mixed models and geographically weighted regression, and four techniques based on different machine learning algorithms, namely, random forest, extreme gradient boosting, support vector machine and artificial neural network to predict fungal productivity. Model evaluation was conducted using a systematic methodology combining random, spatial and environmental blocking together with the assessment of the ecological consistency of spatially-explicit model predictions according to scientific knowledge. Results Fungal productivity predictions were sensitive to the modeling approach and the number of predictors used. Moreover, the importance assigned to different predictors varied between machine learning modeling approaches. Decision tree-based models increased prediction accuracy by more than 10% compared to other machine learning approaches, and by more than 20% compared to statistical models, and resulted in higher ecological consistence of the predicted biogeographical patterns of fungal productivity. Conclusions Decision tree-based models were the best approach for prediction both in sampling-like environments as well as in extrapolation beyond the spatial and climatic range of the modeling data. In this study, we show that proper variable selection is crucial to create robust models for extrapolation in biophysically differentiated areas. This allows for reducing the dimensions of the ecosystem space described by the predictors of the models, resulting in higher similarity between the modeling data and the environmental conditions over the whole study area. When dealing with spatial-temporal data in the analysis of biogeographical patterns, environmental blocking is postulated as a highly informative technique to be used in cross-validation to assess the prediction error over larger scales.","",""
85,"Neoklis Polyzotis, Sudip Roy, S. E. Whang, Martin A. Zinkevich","Data Lifecycle Challenges in Production Machine Learning",2018,"","","","",133,"2022-07-13 09:25:27","","10.1145/3299887.3299891","","",,,,,85,21.25,21,4,4,"Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to explore how different constraints are imposed on the solutions depending on where in the lifecycle of a model the problems are encountered and who encounters them.","",""
21,"Aaron M. Smith, J. Walsh, John J Long, Craig B Davis, Peter V. Henstock, M. Hodge, M. Maciejewski, X. Mu, Stephen Ra, Shanrong Zhao, D. Ziemek, Charles K. Fisher","Standard machine learning approaches outperform deep representation learning on phenotype prediction from transcriptomics data",2020,"","","","",134,"2022-07-13 09:25:27","","10.1186/s12859-020-3427-8","","",,,,,21,10.50,2,12,2,"","",""
13,"Zhifang Liang, Ci Zhang, Hao Sun, An Song, Tao Liu","Improving the Robustness of Prediction Model by Transfer Learning for Interference Suppression of Electronic Nose",2018,"","","","",135,"2022-07-13 09:25:27","","10.1109/JSEN.2017.2778012","","",,,,,13,3.25,3,5,4,"This paper gives a solution to solve the interference problem of electronic nose (e-nose), which is ill-posed due to the uncertainty and unpredictability of its instable behavior. Traditional methods for interference suppression are component correction frameworks, which are laborious or little efficient. With interference (especially background interference and sensor drift), the distribution of test data obtained in practical application is different from that of the previous training data. From the viewpoint of machine learning, a novel domain correction and adaptive extreme learning machines (DC-AELM) framework with transferring capability is proposed to solve the serious interference problem in e-nose. The framework consists of two parts: 1) DC, which makes the distributions of two domains close and 2) AELM, which realizes the knowledge transfer at the decision level and makes the robustness of the prediction model improved. This method is motivated by the idea of transfer learning, especially from the perspective of domain correction and decision-making, to realize the knowledge transfer for interference suppression. A background interference data set obtained by our designed e-nose and a public benchmark sensor drift data set are used to verify the effectiveness of the proposed DC-AELM method.","",""
15,"Linyi Li, Maurice Weber, Xiaojun Xu, Luka Rimanic, B. Kailkhura, Tao Xie, Ce Zhang, Bo Li","TSS: Transformation-Specific Smoothing for Robustness Certification",2020,"","","","",136,"2022-07-13 09:25:27","","10.1145/3460120.3485258","","",,,,,15,7.50,2,8,2,"As machine learning (ML) systems become pervasive, safeguarding their security is critical. However, recently it has been demonstrated that motivated adversaries are able to mislead ML systems by perturbing test data using semantic transformations. While there exists a rich body of research providing provable robustness guarantees for ML models against Lp bounded adversarial perturbations, guarantees against semantic perturbations remain largely underexplored. In this paper, we provide TSS-a unified framework for certifying ML robustness against general adversarial semantic transformations. First, depending on the properties of each transformation, we divide common transformations into two categories, namely resolvable (e.g., Gaussian blur) and differentially resolvable (e.g., rotation) transformations. For the former, we propose transformation-specific randomized smoothing strategies and obtain strong robustness certification. The latter category covers transformations that involve interpolation errors, and we propose a novel approach based on stratified sampling to certify the robustness. Our framework TSS leverages these certification strategies and combines with consistency-enhanced training to provide rigorous certification of robustness. We conduct extensive experiments on over ten types of challenging semantic transformations and show that TSS significantly outperforms the state of the art. Moreover, to the best of our knowledge, TSS is the first approach that achieves nontrivial certified robustness on the large-scale ImageNet dataset. For instance, our framework achieves 30.4% certified robust accuracy against rotation attack (within ±30°) on ImageNet. Moreover, to consider a broader range of transformations, we show TSS is also robust against adaptive attacks and unforeseen image corruptions such as CIFAR-10-C and ImageNet-C.","",""
15,"V. Braverman, Avinatan Hassidim, Y. Matias, Mariano Schain, Sandeep Silwal, Samson Zhou","Adversarial Robustness of Streaming Algorithms through Importance Sampling",2021,"","","","",137,"2022-07-13 09:25:27","","","","",,,,,15,15.00,3,6,1,"In the adversarial streaming model, an adversary gives an algorithm a sequence of adaptively chosen updates as a data stream and the goal of the algorithm is to compute or approximate some predetermined function for every preﬁx of the adversarial stream. However, the adversary may generate future updates based on previous outputs of the algorithm and in particular, the adversary may gradually learn the random bits internally used by an algorithm to manipulate dependencies in the input. This is especially problematic as many important problems in the streaming model require randomized algorithms, as they are known to not admit any deterministic algorithms that use sublinear space. In this paper, we introduce adversarially robust streaming algorithms for central machine learning and algorithmic tasks, such as regression and clustering, as well as their more general counterparts, subspace embedding, low-rank approximation, and coreset construction. Our results are based on a simple, but powerful, observation that many importance sampling-based algorithms give rise to adversarial robustness in contrast to sketching based algorithms, which are very preva-lent in the streaming literature but suffer from adversarial attacks. In addition, we show that the well-known merge and reduce paradigm used for corset construction in streaming is adversarially robust. To the best of our knowledge, these are the ﬁrst adversarially robust results for these problems yet require no new algorithmic implementations. Finally, we empirically conﬁrm the robustness of our algorithms on various adversarial attacks and demonstrate that by contrast, some common existing algorithms are not robust.","",""
133,"A. Perini, A. Susi, P. Avesani","A Machine Learning Approach to Software Requirements Prioritization",2013,"","","","",138,"2022-07-13 09:25:27","","10.1109/TSE.2012.52","","",,,,,133,14.78,44,3,9,"Deciding which, among a set of requirements, are to be considered first and in which order is a strategic process in software development. This task is commonly referred to as requirements prioritization. This paper describes a requirements prioritization method called Case-Based Ranking (CBRank), which combines project's stakeholders preferences with requirements ordering approximations computed through machine learning techniques, bringing promising advantages. First, the human effort to input preference information can be reduced, while preserving the accuracy of the final ranking estimates. Second, domain knowledge encoded as partial order relations defined over the requirement attributes can be exploited, thus supporting an adaptive elicitation process. The techniques CBRank rests on and the associated prioritization process are detailed. Empirical evaluations of properties of CBRank are performed on simulated data and compared with a state-of-the-art prioritization method, providing evidence of the method ability to support the management of the tradeoff between elicitation effort and ranking accuracy and to exploit domain knowledge. A case study on a real software project complements these experimental measurements. Finally, a positioning of CBRank with respect to state-of-the-art requirements prioritization methods is proposed, together with a discussion of benefits and limits of the method.","",""
28,"Lili Su, Jiaming Xu","Securing Distributed Machine Learning in High Dimensions",2018,"","","","",139,"2022-07-13 09:25:27","","","","",,,,,28,7.00,14,2,4,"We consider securing a distributed machine learning system wherein the data is kept confidential by its providers who are recruited as workers to help the learner to train a $d$--dimensional model. In each communication round, up to $q$ out of the $m$ workers suffer Byzantine faults; faulty workers are assumed to have complete knowledge of the system and can collude to behave arbitrarily adversarially against the learner. We assume that each worker keeps a local sample of size $n$. (Thus, the total number of data points is $N=nm$.) Of particular interest is the high-dimensional regime $d \gg n$.  We propose a secured variant of the classical gradient descent method which can tolerate up to a constant fraction of Byzantine workers. We show that the estimation error of the iterates converges to an estimation error $O(\sqrt{q/N} + \sqrt{d/N})$ in $O(\log N)$ rounds. The core of our method is a robust gradient aggregator based on the iterative filtering algorithm proposed by Steinhardt et al. \cite{Steinhardt18} for robust mean estimation. We establish a uniform concentration of the sample covariance matrix of gradients, and show that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function. As a by-product, we develop a new concentration inequality for sample covariance matrices of sub-exponential distributions, which might be of independent interest.","",""
11,"Ghassan Alnwaimi, Talha Zahir, S. Vahid, K. Moessner","Machine Learning Based Knowledge Acquisition on Spectrum Usage for LTE Femtocells",2013,"","","","",140,"2022-07-13 09:25:27","","10.1109/VTCFall.2013.6692276","","",,,,,11,1.22,3,4,9,"The decentralised and ad hoc nature of femtocell deployments calls for distributed learning strategies to mitigate interference. We propose a distributed spectrum awareness scheme for femtocell networks, based on combined payoff and strategy reinforcement learning (RL) models. We present two different learning strategies, based on modifications to the Bush Mosteller (BM) RL and the Roth-Erev RL algorithms. The simulation results show the convergence behaviour of the learning strategies under a dynamic robust game. As compared to the Bush Mosteller (BM) RL, our modified BM (MBM) converges smoothly to a stable satisfactory solution. Moreover, the MBM significantly reduces the interference collision cost during the learning process. Both the MBM and the modified Roth-Erev (MRE) algorithms are stochastic-based learning strategies which require less computation than the gradient follower (GF) learning strategy and have the capability to escape from suboptimal solution.","",""
89,"Huichen Lihuichen","DECISION-BASED ADVERSARIAL ATTACKS: RELIABLE ATTACKS AGAINST BLACK-BOX MACHINE LEARNING MODELS",2017,"","","","",141,"2022-07-13 09:25:27","","","","",,,,,89,17.80,89,1,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradientor score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available at XXXXXX. Gradient-based Model M Untargeted Flip to any label Targeted Flip to target label FGSM, DeepFool L-BFGS-B, Houdini, JSMA, Carlini & Wagner, Iterative Gradient Descent Score-based Detailed Model Prediction Y (e.g. probabilities or logits) ZOO Local Search Decision-based Final Model Prediction Ymax (e.g. max class label) this work (Boundary Attack) Transfer-based Training Data T","",""
64,"Alexander H S Harris, A. Kuo, Yingjie Weng, A. Trickey, Thomas R Bowe, N. Giori","Can Machine Learning Methods Produce Accurate and Easy-to-use Prediction Models of 30-day Complications and Mortality After Knee or Hip Arthroplasty?",2019,"","","","",142,"2022-07-13 09:25:27","","10.1097/CORR.0000000000000601","","",,,,,64,21.33,11,6,3,"Background Existing universal and procedure-specific surgical risk prediction models of death and major complications after elective total joint arthroplasty (TJA) have limitations including poor transparency, poor to modest accuracy, and insufficient validation to establish performance across diverse settings. Thus, the need remains for accurate and validated prediction models for use in preoperative management, informed consent, shared decision-making, and risk adjustment for reimbursement. Questions/purposes The purpose of this study was to use machine learning methods and large national databases to develop and validate (both internally and externally) parsimonious risk-prediction models for mortality and complications after TJA. Methods Preoperative demographic and clinical variables from all 107,792 nonemergent primary THAs and TKAs in the 2013 to 2014 American College of Surgeons-National Surgical Quality Improvement Program (ACS-NSQIP) were evaluated as predictors of 30-day death and major complications. The NSQIP database was chosen for its high-quality data on important outcomes and rich characterization of preoperative demographic and clinical predictors for demographically and geographically diverse patients. Least absolute shrinkage and selection operator (LASSO) regression, a type of machine learning that optimizes accuracy and parsimony, was used for model development. Tenfold validation was used to produce C-statistics, a measure of how well models discriminate patients who experience an outcome from those who do not. External validation, which evaluates the generalizability of the models to new data sources and patient groups, was accomplished using data from the Veterans Affairs Surgical Quality Improvement Program (VASQIP). Models previously developed from VASQIP data were also externally validated using NSQIP data to examine the generalizability of their performance with a different group of patients outside the VASQIP context. Results The models, developed using LASSO regression with diverse clinical (for example, American Society of Anesthesiologists classification, comorbidities) and demographic (for example, age, gender) inputs, had good accuracy in terms of discriminating the likelihood a patient would experience, within 30 days of arthroplasty, a renal complication (C-statistic, 0.78; 95% confidence interval [CI], 0.76-0.80), death (0.73; 95% CI, 0.70-0.76), or a cardiac complication (0.73; 95% CI, 0.71-0.75) from one who would not. By contrast, the models demonstrated poor accuracy for venous thromboembolism (C-statistic, 0.61; 95% CI, 0.60-0.62) and any complication (C-statistic, 0.64; 95% CI, 0.63-0.65). External validation of the NSQIP- derived models using VASQIP data found them to be robust in terms of predictions about mortality and cardiac complications, but not for predicting renal complications. Models previously developed with VASQIP data had poor accuracy when externally validated with NSQIP data, suggesting they should not be used outside the context of the Veterans Health Administration. Conclusions Moderately accurate predictive models of 30-day mortality and cardiac complications after elective primary TJA were developed as well as internally and externally validated. To our knowledge, these are the most accurate and rigorously validated TJA-specific prediction models currently available (http://med.stanford.edu/s-spire/Resources/clinical-tools-.html). Methods to improve these models, including the addition of nonstandard inputs such as natural language processing of preoperative clinical progress notes or radiographs, should be pursued as should the development and validation of models to predict longer term improvements in pain and function. Level of Evidence Level III, diagnostic study.","",""
23,"Muxin Gu, M. Buckley","Semi-supervised machine learning for automated species identification by collagen peptide mass fingerprinting",2018,"","","","",143,"2022-07-13 09:25:27","","10.1186/s12859-018-2221-3","","",,,,,23,5.75,12,2,4,"","",""
15,"S. Leighton, R. Krishnadas, K. Chung, A. Blair, Susie Brown, S. Clark, K. Sowerbutts, M. Schwannauer, J. Cavanagh, A. Gumley","Predicting one-year outcome in first episode psychosis using machine learning",2018,"","","","",144,"2022-07-13 09:25:27","","10.1101/390096","","",,,,,15,3.75,2,10,4,"Lay Summary Evidence before this study Our knowledge of factors which predict outcome in first episode psychosis (FEP) is incomplete. Poor premorbid adjustment, history of developmental disorder, symptom severity at baseline and duration of untreated psychosis are the most replicated predictors of poor clinical, functional, cognitive, and biological outcomes. Yet, such group level differences are not always replicated in individuals, nor can observational results be clearly equated with causation. Advanced machine learning techniques have potential to revolutionise medicine by looking at causation and the prediction of individual patient outcome. Within psychiatry, Koutsouleris et al employed machine learning to predict 4- and 52-week functional outcome in FEP to a 75% and 73.8% test-fold balanced accuracy on repeated nested internal cross-validation. The authors suggest that before employing a machine learning model “into real-world care, replication is needed in external first episode samples”. Added value of this study We believe our study to be the first externally validated evidence, in a temporally and geographically independent cohort, for predictive modelling in FEP at an individual patient level. Our results demonstrate the ability to predict both symptom remission and functioning (in employment, education or training (EET)) at one-year. The performance of our EET model was particularly robust, with an ability to accurately predict the one-year EET outcome in more than 85% of patients. Regularised regression results in sparse models which are uniquely interpretable and identify meaningful predictors of recovery including specific individual PANSS items, and social support. This builds on existing studies of group-level differences and the elegant work of Koutsouleris et al. Implications of all the available evidence We have demonstrated the externally validated ability to accurately predict one-year symptomatic and functional status in individual patients with FEP. External validation in a plausibly related temporally and geographically distinct population assesses model transportability to an untested situation rather than simply reproducibility alone. We propose that our results represent important and exciting progress in unlocking the potential of predictive modelling in psychiatric illness. The next step prior to implementation into routine clinical practice would be to establish whether, by the accurate identification of individuals who will have poor outcomes, we can meaningful intervene to improve their prognosis. Abstract Background Early illness course correlates with long-term outcome in psychosis. Accurate prediction could allow more focused intervention. Earlier intervention corresponds to significantly better symptomatic and functional outcomes. We use routinely collected baseline demographic and clinical characteristics to predict employment, education or training (EET) status, and symptom remission in patients with first episode psychosis (FEP) at 1 year. Methods 83 FEP patients were recruited from National Health Service (NHS) Glasgow between 2011 and 2014 to a 24-month prospective cohort study with regular assessment of demographic and psychometric measures. An external independent cohort of 79 FEP patients were recruited from NHS Glasgow and Edinburgh during a 12-month study between 2006 and 2009. Elastic net regularised logistic regression models were built to predict binary EET status, period and point remission outcomes at 1 year on 83 Glasgow patients (training dataset). Models were externally validated on an independent dataset of 79 patients from Glasgow and Edinburgh (validation dataset). Only baseline predictors shared across both cohorts were made available for model training and validation. Outcomes After excluding participants with missing outcomes, models were built on the training dataset for EET status, period and point remission outcomes and externally validated on the validation dataset. Models predicted EET status, period and point remission with ROC area under curve (AUC) performances of 0.876 (95%CI: 0.864, 0.887), 0.630 (95%CI: 0.612, 0.647) and 0.652 (95%CI: 0.635, 0.670) respectively. Positive predictors of EET included baseline EET and living with spouse/children. Negative predictors included higher PANSS suspiciousness, hostility and delusions scores. Positive predictors for symptom remission included living with spouse/children, and affective symptoms on the Positive and Negative Syndrome Scale (PANSS). Negative predictors of remission included passive social withdrawal symptoms on PANSS. Interpretation Using advanced statistical machine learning techniques, we provide the first externally validated evidence for the ability to predict 1-year EET status and symptom remission in FEP patients. Funding The authors acknowledge financial support from NHS Research Scotland, the Chief Scientist Office, the Wellcome Trust, and the Scottish Mental Health Research Network.","",""
16,"E. Swann, B. Sun, D. Cleland, A. Barnard","Representing molecular and materials data for unsupervised machine learning",2018,"","","","",145,"2022-07-13 09:25:27","","10.1080/08927022.2018.1450982","","",,,,,16,4.00,4,4,4,"Abstract Statistical analysis and machine learning can help us understand and predict the collective properties and performance of ensembles of molecules and nanostructures, while accounting for all the complexity and diversity of real world specimens. Combining data-driven techniques with robust and reliable simulation methods can provide insights that cannot be made any other way. However, not all statistical and machine learning methods are right for all occasions; testing, validation and perhaps some trial and error are needed. Domain knowledge alone is not sufficient to choose the right algorithms. Data representation methods that are best suited to machine learning are not necessarily scientifically intuitive. The best descriptors are not always the structural features or physiochemical properties that we are aiming to control, and the way our data is distributed can be as important as what it contains. In this review, we discuss the differences, advantages and disadvantages of some of the common data representation, reduction and classification methods applicable to molecular and materials modelling. Focussing on unsupervised methods, we highlight features of these algorithms that determine their suitability and can inform choices of which learning method to use and how to effectively prepare data. A case study is also provided to demonstrate how testing can be undertaken, and how methods can be combined.","",""
127,"Lei Zhang, D. Zhang","Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation",2016,"","","","",146,"2022-07-13 09:25:27","","10.1109/TIP.2016.2598679","","",,,,,127,21.17,64,2,6,"We address the problem of visual knowledge adaptation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine (ELM)-based cross-domain network learning framework, that is called ELM-based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the ℓ2,1-norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross-domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as the base classifiers. The network output weights cannot only be analytically determined, but also transferrable. In addition, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semisupervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition demonstrate that our EDA methods outperform the existing cross-domain learning methods.","",""
2,"H. L. Fernández, M. Reboiro-Jato, José A. Pérez Rodríguez, F. Fdez-Riverola, D. Glez-Peña","Implementing effective machine learning-based workflows for the analysis of mass spectrometry data",2016,"","","","",147,"2022-07-13 09:25:27","","10.5584/JIOMICS.V6I1.196","","",,,,,2,0.33,0,5,6,"Mass spectrometry using matrix assisted laser desorption ionization coupled to time of flight analyzers (MALDI-TOF MS) has become very popular during the last decade due to its high speed, sensitivity and robustness for accurately detecting proteins and peptides. This allows quickly analyzing large sets of samples in one single batch and doing high-throughput proteomics. In this scenario, bioinformatics methods and computational tools play a key role in MALDI-TOF MS data analysis, as they are able to correctly handle the large amount of raw data generated with the goal of discovering new knowledge and extracting useful conclusions. A typical MALDI-TOF MS data analysis workflow consists of three main stages: data acquisition, preprocessing and analysis. Although the most popular use of this technology is to identify proteins through their peptides, analyses that make use of artificial intelligence (AI), machine learning (ML), and statistical methods are of particular interest to conduct biomarker discovery, automatic diagnosis, and knowledge discovery. In this introductory work, the potential of these techniques are explored and novel solutions based on the application of AI, ML, and statistical methods are reviewed. In addition, an integrated software platform that supports full MALDI-TOF MS data analysis is presented with the goal of facilitating the work of proteomics researchers without advanced bioinformatics skills.","",""
13,"Ali A. Abdallah, S. Saab, Z. Kassas","A machine learning approach for localization in cellular environments",2018,"","","","",148,"2022-07-13 09:25:27","","10.1109/PLANS.2018.8373508","","",,,,,13,3.25,4,3,4,"A machine learning approach is developed for localization based on received signal strength (RSS) from cellular towers. The proposed approach only assumes knowledge of RSS fingerprints of the environment, and does not require knowledge of the cellular base transceiver station (BTS) locations, nor uses any RSS mathematical model. The proposed localization scheme integrates a weighted K-nearest neighbor (WKNN) and a multilayer neural network. The integration takes advantage of the robust clustering ability of WKNN and implements a neural network that could estimate the position within each cluster. Experimental results are presented to demonstrate the proposed approach in two urban environments and one rural environment, achieving a mean distance localization error of 5.9 m and 5.1 m in the urban environments and 8.7 m in the rural environment. This constitutes an improvement of 41%, 45%, and 16%, respectively, over the WKNN-only algorithm.","",""
13,"Nastasiya F. Grinberg, R. King","An evaluation of machine-learning for predicting phenotype: studies in yeast, rice, and wheat",2017,"","","","",149,"2022-07-13 09:25:27","","10.1007/s10994-019-05848-5","","",,,,,13,2.60,7,2,5,"","",""
0,"Ziyi Yang, Zhaofeng Ye, Yijia Xiao, Chang-Yu Hsieh","SPLDExtraTrees: Robust machine learning approach for predicting kinase inhibitor resistance",2021,"","","","",150,"2022-07-13 09:25:27","","10.1093/bib/bbac050","","",,,,,0,0.00,0,4,1,"Drug resistance is a major threat to the global health and a significant concern throughout the clinical treatment of diseases and drug development. The mutation in proteins that is related to drug binding is a common cause for adaptive drug resistance. Therefore, quantitative estimations of how mutations would affect the interaction between a drug and the target protein would be of vital significance for the drug development and the clinical practice. Computational methods that rely on molecular dynamics simulations, Rosetta protocols, as well as machine learning methods have been proven to be capable of predicting ligand affinity changes upon protein mutation. However, the severely limited sample size and heavy noise induced overfitting and generalization issues have impeded wide adoption of machine learning for studying drug resistance. In this paper, we propose a robust machine learning method, termed SPLDExtraTrees, which can accurately predict ligand binding affinity changes upon protein mutation and identify resistance-causing mutations. Especially, the proposed method ranks training data following a specific scheme that starts with easy-to-learn samples and gradually incorporates harder and diverse samples into the training, and then iterates between sample weight recalculations and model updates. In addition, we calculate additional physics-based structural features to provide the machine learning model with the valuable domain knowledge on proteins for these data-limited predictive tasks. The experiments substantiate the capability of the proposed method for predicting kinase inhibitor resistance under three scenarios and achieve predictive accuracy comparable with that of molecular dynamics and Rosetta methods with much less computational costs.","",""
13,"Cong T. Nguyen, Nguyen Van Huynh, Nam H. Chu, Y. Saputra, D. Hoang, Diep N. Nguyen, Quoc-Viet Pham, D. Niyato, E. Dutkiewicz, W. Hwang","Transfer Learning for Future Wireless Networks: A Comprehensive Survey",2021,"","","","",151,"2022-07-13 09:25:27","","","","",,,,,13,13.00,1,10,1,"With outstanding features, Machine Learning (ML) has become the backbone of numerous applications in wireless networks. However, the conventional ML approaches face many challenges in practical implementation, such as the lack of labeled data, the constantly changing wireless environments, the long training process, and the limited capacity of wireless devices. These challenges, if not addressed, can impede the effectiveness and applicability of ML in wireless networks. To address these problems, Transfer Learning (TL) has recently emerged to be a promising solution. The core idea of TL is to leverage and synthesize distilled knowledge from similar tasks as well as from valuable experiences accumulated from the past to facilitate the learning of new problems. Doing so, TL techniques can reduce the dependence on labeled data, improve the learning speed, and enhance the ML methods’ robustness to different wireless environments. This article aims to provide a comprehensive survey on applications of TL in wireless networks. Particularly, we first provide an overview of TL including formal definitions, classification, and various types of TL techniques. We then discuss diverse TL approaches proposed to address emerging issues in wireless networks. The issues include spectrum management, localization, signal recognition, security, human activity recognition and caching, which are all important to nextgeneration networks such as 5G and beyond. Finally, we highlight important challenges, open issues, and future research directions of TL in future wireless networks.","",""
1,"Rober Boshra","Automated Machine Learning Framework for EEG/ERP Analysis: Viable Improvement on Traditional Approaches?",2016,"","","","",152,"2022-07-13 09:25:27","","","","",,,,,1,0.17,1,1,6,"Event Related Potential (ERP) measures derived from the electroencephalogram (EEG) have been widely used in research on language, cognition, and pathology. The high dimensionality (time x channel x condition) of a typical EEG/ERP dataset makes it a timeconsuming prospect to properly analyze, explore, and validate knowledge without a particular restricted hypothesis. This study proposes an automated empirical greedy approach to the analysis process to datamine an EEG dataset for the location, robustness, and latency of ERPs, if any, present in a given dataset. We utilize Support Vector Machines (SVM), a well established machine learning model, on top of a preprocessing pipeline that focuses on detecting differences across experimental conditions. A hybrid of monte-carlo bootstrapping, cross-validation, and permutation tests is used to ensure the reproducibility of results. This framework serves to reduce researcher bias, time spent during analysis, and provide statistically sound results that are agnostic to dataset specifications including the ERPs in question. This method has been tested and validated on three different datasets with different ERPs (N100, Mismatch Negativity (MMN), N2b, Phonological Mapping Negativity (PMN), and P300). Results show statistically significant, above-chance level identification of all ERPs in their respective experimental conditions, latency, and location.","",""
13,"Suqing Zheng, Wenping Chang, Wen-ping Xu, Yong Xu, Fu Lin","e-Sweet: A Machine-Learning Based Platform for the Prediction of Sweetener and Its Relative Sweetness",2019,"","","","",153,"2022-07-13 09:25:27","","10.3389/fchem.2019.00035","","",,,,,13,4.33,3,5,3,"Artificial sweeteners (AS) can elicit the strong sweet sensation with the low or zero calorie, and are widely used to replace the nutritive sugar in the food and beverage industry. However, the safety issue of current AS is still controversial. Thus, it is imperative to develop more safe and potent AS. Due to the costly and laborious experimental-screening of AS, in-silico sweetener/sweetness prediction could provide a good avenue to identify the potential sweetener candidates before experiment. In this work, we curate the largest dataset of 530 sweeteners and 850 non-sweeteners, and collect the second largest dataset of 352 sweeteners with the relative sweetness (RS) from the literature. In light of these experimental datasets, we adopt five machine-learning methods and conformational-independent molecular fingerprints to derive the classification and regression models for the prediction of sweetener and its RS, respectively via the consensus strategy. Our best classification model achieves the 95% confidence intervals for the accuracy (0.91 ± 0.01), precision (0.90 ± 0.01), specificity (0.94 ± 0.01), sensitivity (0.86 ± 0.01), F1-score (0.88 ± 0.01), and NER (Non-error Rate: 0.90 ± 0.01) on the test set, which outperforms the model (NER = 0.85) of Rojas et al. in terms of NER, and our best regression model gives the 95% confidence intervals for the R2(test set) and ΔR2 [referring to |R2(test set)- R2(cross-validation)|] of 0.77 ± 0.01 and 0.03 ± 0.01, respectively, which is also better than the other works based on the conformation-independent 2D descriptors (e.g., 2D Dragon) according to R2(test set) and ΔR2. Our models are obtained by averaging over nineteen data-splitting schemes, and fully comply with the guidelines of Organization for Economic Cooperation and Development (OECD), which are not completely followed by the previous relevant works that are all on the basis of only one random data-splitting scheme for the cross-validation set and test set. Finally, we develop a user-friendly platform “e-Sweet” for the automatic prediction of sweetener and its corresponding RS. To our best knowledge, it is a first and free platform that can enable the experimental food scientists to exploit the current machine-learning methods to boost the discovery of more AS with the low or zero calorie content.","",""
13,"Omar A. Zatarain, Yingxu Wang","Experiments on the supervised learning algorithm for formal concept elicitation by cognitive robots",2016,"","","","",154,"2022-07-13 09:25:27","","10.1109/ICCI-CC.2016.7862015","","",,,,,13,2.17,7,2,6,"Concept elicitation is a fundamental methodology for knowledge extraction and representation in cognitive robot learning. Traditional machine learning technologies deal with object identification, cluster classification, functional regression, and behavior acquisition. This paper presents a supervised machine knowledge learning methodology for concept elicitation from sample dictionaries in natural languages. Formal concepts are autonomously generated based on collective intention of attributes and collective extension of objects elicited from informal definitions in dictionaries. A system of formal concept generation for a cognitive robot is implemented by the Algorithm of Machine Concept Elicitation (AMCE) in MATLAB. Experiments on machine learning for creating a set of twenty formal concepts reveal that the cognitive robot is able to learn synergized concepts in human knowledge in order to build its own cognitive knowledge base. The results of machine-generated concepts demonstrate that the AMCE algorithm can over perform human knowledge expressions in dictionaries in terms of relevance, accuracy, quantitativeness, and cohesiveness.","",""
3,"A. Heinlein, A. Klawonn, M. Lanser, J. Weber","Machine Learning in Adaptive FETI-DP – A Comparison of Smart and Random Training Data",2018,"","","","",155,"2022-07-13 09:25:27","","10.1007/978-3-030-56750-7_24","","",,,,,3,0.75,1,4,4,"","",""
10,"Sharif Amit Kamran, A. Tavakkoli, S. Zuckerbrod","Improving Robustness Using Joint Attention Network for Detecting Retinal Degeneration From Optical Coherence Tomography Images",2020,"","","","",156,"2022-07-13 09:25:27","","10.1109/ICIP40778.2020.9190742","","",,,,,10,5.00,3,3,2,"Noisy data and the similarity in the ocular appearances caused by different ophthalmic pathologies pose significant challenges for an automated expert system to accurately detect retinal diseases. In addition, the lack of knowledge transferability and the need for unreasonably large datasets limit clinical application of current machine learning systems. To increase robustness, a better understanding of how the retinal subspace deformations lead to various levels of disease severity needs to be utilized for prioritizing disease-specific model details. In this paper we propose the use of disease-specific feature representation as a novel architecture comprised of two joint networks - one for supervised encoding of disease model and the other for producing attention maps in an unsupervised manner to retain disease specific spatial information. Our experimental results on publicly available datasets show the proposed joint-network significantly improves the accuracy and robustness of state-of-the-art retinal disease classification networks on unseen datasets.","",""
10,"Tom Seymoens, F. Ongenae, An Jacobs, S. Verstichel, A. Ackaert","A Methodology to Involve Domain Experts and Machine Learning Techniques in the Design of Human-Centered Algorithms",2018,"","","","",157,"2022-07-13 09:25:27","","10.1007/978-3-030-05297-3_14","","",,,,,10,2.50,2,5,4,"","",""
10,"J. Collins, K. Howe, B. Nachman","CWoLa Hunting: Extending the Bump Hunt with Machine Learning",2018,"","","","",158,"2022-07-13 09:25:27","","","","",,,,,10,2.50,3,3,4,"The oldest and most robust technique to search for new particles is to look for `bumps' in invariant mass spectra over smoothly falling backgrounds. This is a powerful technique, but only uses one-dimensional information. One can restrict the phase space to enhance a potential signal, but such tagging techniques require a signal hypothesis and training a classifier in simulation and applying it on data. We present a new method for using all of the available information (with machine learning) without using any prior knowledge about potential signals. Given the lack of new physics signals at the Large Hadron Collider (LHC), such model independent approaches are critical for ensuring full coverage to fully exploit the rich datasets from the LHC experiments. In addition to illustrating how the new method works in simple test cases, we demonstrate the power of the extended bump hunt on a realistic all-hadronic resonance search in a channel that would not be covered with existing techniques.","",""
67,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings",2017,"","","","",159,"2022-07-13 09:25:27","","10.1145/3154503","","",,,,,67,13.40,22,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q ≤ for an arbitrarily small but fixed constant ε > 0. The parameter estimate converges in O(log N) rounds with an estimation error on the order of max{√dq/N, √d/N, which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q. The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
15,"Stien Heremans, J. Van Orshoven","Machine learning methods for sub-pixel land-cover classification in the spatially heterogeneous region of Flanders (Belgium): a multi-criteria comparison",2015,"","","","",160,"2022-07-13 09:25:27","","10.1080/01431161.2015.1054047","","",,,,,15,2.14,8,2,7,"Until now, few research has addressed the use of machine learning methods for classification at the sub-pixel level. To close this knowledge gap, in this article, six machine learning methods were compared for the specific task of sub-pixel land-cover extraction in the spatially heterogeneous region of Flanders (Belgium). In addition to the classification accuracy at the pixel and the municipality level, three evaluation criteria reflecting the methods’ ease-of-use were added to the comparison: the time needed for training, the number of meta-parameters, and the minimum training set size. Robustness to changing training data was also included as the sixth evaluation criterion. Based on their scores for these six criteria, the machine learning methods were ranked according to three multi-criteria ranking scenarios. These ranking scenarios correspond to different decision-making scenarios that differ in their weighting of the criteria. In general, no overall winner could be designated: no method performs best for all evaluation scenarios. However, when both time available for preprocessing and the magnitude of the training data set are unconstrained, Support Vector Machines (SVMs) clearly outperform the other methods.","",""
12,"A. Menon, James A. Thompson-Colón, N. Washburn","Hierarchical Machine Learning Model for Mechanical Property Predictions of Polyurethane Elastomers From Small Datasets",2019,"","","","",161,"2022-07-13 09:25:27","","10.3389/fmats.2019.00087","","",,,,,12,4.00,4,3,3,"Polyurethanes are a broad class of material that finds application in coatings, foams, and solid elastomers. The urethane chemistry allows a diversity of monomers to be used, and prediction of mechanical properties, which are determined by complex interplay between monomer chemistry and chain architecture, is an unresolved challenge. Urethanes are based on aromatic or cyclic isocyanates and linear or branched polyols, and polymerization results in linear chains for bifunctional monomers or branched chains for multifunctional monomers. Strong intermolecular interactions between aromatic groups result in the formation of hard-segment domains that generate physical crosslinks between disorganized rubbery domains and anchor the material microstructure, contributing to resistance to deformation. Here, a general hierarchical machine learning (HML) model for predicting the stress-at-break, strain-at-break, and Tan δ for thermoplastic and thermoset polyurethanes is presented. The algorithm was trained on a library of 18 polymers with different diisocyanates, bifunctional or trifunctional polyols, and NCO:OH index. HML reduces data requirements through robust embedding of domain knowledge and surrogate data in a middle layer that bridges input variables (composition) and output responses (mechanical properties). In this work, the middle layer included information on overall polymer composition, predictions of chain architecture derived from Monte Carlo simulations of polymerization, information on interchain interactions from empirically derived molecular potentials and shifts in infrared (IR) spectroscopy absorbances. The HML predictions are shown to be more accurate than those from a Random Forest model directly relating composition and properties, suggesting that embedding domain knowledge provides significant advantages in predicting the properties of complex material systems based on small datasets.","",""
524,"S. Raschka","Python Machine Learning",2015,"","","","",162,"2022-07-13 09:25:27","","","","",,,,,524,74.86,524,1,7,"Unlock deeper insights into Machine Leaning with this vital guide to cutting-edge predictive analyticsAbout This BookLeverage Python's most powerful open-source libraries for deep learning, data wrangling, and data visualizationLearn effective strategies and best practices to improve and optimize machine learning systems and algorithmsAsk and answer tough questions of your data with robust statistical models, built for a range of datasetsWho This Book Is ForIf you want to find out how to use Python to start answering critical questions of your data, pick up Python Machine Learning whether you want to get started from scratch or want to extend your data science knowledge, this is an essential and unmissable resource.What You Will LearnExplore how to use different machine learning models to ask different questions of your dataLearn how to build neural networks using Keras and TheanoFind out how to write clean and elegant Python code that will optimize the strength of your algorithmsDiscover how to embed your machine learning model in a web application for increased accessibilityPredict continuous target outcomes using regression analysisUncover hidden patterns and structures in data with clusteringOrganize data using effective pre-processing techniquesGet to grips with sentiment analysis to delve deeper into textual and social media dataIn DetailMachine learning and predictive analytics are transforming the way businesses and other organizations operate. Being able to understand trends and patterns in complex data is critical to success, becoming one of the key strategies for unlocking growth in a challenging contemporary marketplace. Python can help you deliver key insights into your data its unique capabilities as a language let you build sophisticated algorithms and statistical models that can reveal new perspectives and answer key questions that are vital for success.Python Machine Learning gives you access to the world of predictive analytics and demonstrates why Python is one of the world's leading data science languages. If you want to ask better questions of data, or need to improve and extend the capabilities of your machine learning systems, this practical data science book is invaluable. Covering a wide range of powerful Python libraries, including scikit-learn, Theano, and Keras, and featuring guidance and tips on everything from sentiment analysis to neural networks, you'll soon be able to answer some of the most important questions facing you and your organization.Style and approachPython Machine Learning connects the fundamental theoretical principles behind machine learning to their practical application in a way that focuses you on asking and answering the right questions. It walks you through the key elements of Python and its powerful machine learning libraries, while demonstrating how to get to grips with a range of statistical models.","",""
6,"M. Zambetti, R. Sala, D. Russo, G. Pezzotta, R. Pinto","A patent review on machine learning techniques and applications: Depicting main players, relations and technology landscapes",2018,"","","","",163,"2022-07-13 09:25:27","","","","",,,,,6,1.50,1,5,4,"The increasing availability of data, promised by the 4th industrial revolution wave, is challenging companies and organizations in diverse industry sectors to extract useful and actionable information. To this end, a vast array of data management strategies and new analytical methods is becoming available to the large audience of researchers and practitioners. Although traditional statistical approaches are still applicable for different purposes, artificial intelligence techniques, particularly machine learning algorithms, are increasingly being explored and adopted to approach data analysis. Artificial intelligence becomes a necessary ingredient for technology progress. The machine learning domain, in particular, has been extensively investigated by academics, who mainly focused on algorithms and suitable applications, and it is also permeating business reality at an unprecedented rate. Against this background, instead of eliciting knowledge from academics, the proposed research adopts a patent review and analysis approach, with the specific purpose of understanding the ongoing industrial effort on the subject, and new as well as expected trends on machine learning technologies and applications. The paper analyses technological development in various industries by defining patents trend over the years and investigating the different areas of applications according to the Cooperative Patent Classification (CPC), a patent classification system jointly developed by the European and US patent authorities. Patent applicants are also investigated in order to highlight active and competitive players in the domain, as well as collaboration between different companies. Furthermore, the paper includes a patent citation network analysis, which is useful to show critical technologies developed, and to understand applicants’ behaviours, such as influences or infringement trials. Overall, the paper provides an original and “literature-complementary” outlook on the machine learning landscape, giving an understanding on industrial R&D effort in this context, delineating trends related to technology diffusion and innovation from an industrial perspective.","",""
4,"Henggang Cui, G. Ganger, Phillip B. Gibbons","MLtuner: System Support for Automatic Machine Learning Tuning",2018,"","","","",164,"2022-07-13 09:25:27","","","","",,,,,4,1.00,1,3,4,"MLtuner automatically tunes settings for training tunables (such as the learning rate, the momentum, the mini-batch size, and the data staleness bound) that have a significant impact on large-scale machine learning (ML) performance. Traditionally, these tunables are set manually, which is unsurprisingly error-prone and difficult to do without extensive domain knowledge. MLtuner uses efficient snapshotting, branching, and optimization-guided online trial-and-error to find good initial settings as well as to re-tune settings during execution. Experiments show that MLtuner can robustly find and re-tune tunable settings for a variety of ML applications, including image classification (for 3 models and 2 datasets), video classification, and matrix factorization. Compared to state-of-the-art ML auto-tuning approaches, MLtuner is more robust for large problems and over an order of magnitude faster.","",""
5,"Y. Malhotra","AI, Machine Learning & Deep Learning Risk Management & Controls: Beyond Deep Learning and Generative Adversarial Networks: Model Risk Management in AI, Machine Learning & Deep Learning",2018,"","","","",165,"2022-07-13 09:25:27","","10.2139/SSRN.3193693","","",,,,,5,1.25,5,1,4,"The current paper proposes how model risk management in operationalizing machine learning for algorithm deployment can be applied in national C4I and Cyber projects such as Project Maven. It builds upon recent leadership of global Management and Leadership industry executives for AI and Machine Learning Executive Education for MIT Sloan School of Management and the MIT Computer Science and Artificial Intelligence Laboratory (CSAIL) and invited presentations at Princeton University. After building understanding about why model risk management is most crucial to robust AI, Machine Learning, Deep Learning, and, Neural Networks deployment, it introduces a Knowledge Management Framework for Model Risk Management to advance beyond ‘AI Automation’ to ‘AI Augmentation.’","",""
5,"D. Mislis, S. Pyrzas, K. Alsubai","TSARDI: a Machine Learning data rejection algorithm for transiting exoplanet light curves",2018,"","","","",166,"2022-07-13 09:25:27","","10.1093/mnras/sty2361","","",,,,,5,1.25,2,3,4,"We present TSARDI, an efficient rejection algorithm designed to improve the transit detection efficiency in data collected by large scale surveys. TSARDI is based on the Machine Learning clustering algorithm DBSCAN, and its purpose is to serve as a robust and adaptable filter aiming to identify unwanted noise points left over from data detrending processes. TSARDI is an unsupervised method, which can treat each light curve individually; there is no need of previous knowledge of any other field light curves. We conduct a simulated transit search by injecting planets on real data obtained by the QES project and show that TSARDI leads to an overall transit detection efficiency increase of $\sim$11\%, compared to results obtained from the same sample, but using a standard sigma-clip algorithm. For the brighter end of our sample (host star magnitude < 12), TSARDI achieves a detection efficiency of $\sim$80\% of injected planets. While our algorithm has been developed primarily for the field of exoplanets, it is easily adaptable and extendable for use in any time series.","",""
2,"L. Yang, Yuncheng Dong, Jiafu Zhuang, Jun Yu Li","A Recognition Algorithm for Workpieces Based on the Machine Learning",2018,"","","","",167,"2022-07-13 09:25:27","","10.1109/ISCID.2018.10185","","",,,,,2,0.50,1,4,4,"In order to learn and grasp the predetermined workpieces for robot actively, a recognition algorithm based on machine learning is proposed. Compared with traditional algorithms, we replenish a MTSM (multi threshold space model) for getting clearer workpiece shapes. To automatically and compactly learn workpieces knowledge, both shape and gradient features are designed to express the specific object by aid of contour mask, meanwhile, the compound descriptors are fed into a SVM classifier and they are trained jointly to minimize a classification loss. Finally, we adopt density estimation to acquire the grasping point of the workpieces from MTSM. Experimental result of workpieces grasping demonstrates the effectiveness and stability in complex environment, and the proposed algorithm is robust to rotation, scaling and deformation of shapes.","",""
9,"Kevser Ovaz Akpinar, Ibrahim Ozcelik","Analysis of Machine Learning Methods in EtherCAT-Based Anomaly Detection",2019,"","","","",168,"2022-07-13 09:25:27","","10.1109/ACCESS.2019.2960497","","",,,,,9,3.00,5,2,3,"Today, the use of Ethernet-based protocols in industrial control systems (ICS) communications has led to the emergence of attacks based on information technology (IT) on supervisory control and data acquisition systems. In addition, the familiarity of Ethernet and TCP/IP protocols and the diversity and success of attacks on them raises security risks and cyber threats for ICS. This issue is compounded by the absence of encryption, authorization, and authentication mechanisms due to the development of industrial communications protocols only for performance purposes. Recent zero-day attacks, such as Triton, Stuxnet, Havex, Dragonfly, and Blackenergy, as well as the Ukraine cyber-attack, are possible because of the vulnerabilities of the systems; these attacksare carried by the protocols used in communication between PLC and I/O units or HMI and engineering stations. It is evident that there is a need for robust solutions that detect and prevent protocol-based cyber threats. In this paper, machine learning methods are evaluated for anomaly detection, particularly for EtherCAT-based ICS. To the best of the author’s knowledge, there has been no research focusing on machine learning algorithms for anomaly detection of EtherCAT. Before testing anomaly detection, an EtherCAT-based water level control system testbed was developed. Then, a total of 16 events were generated in four categories and applied on the testbed. The dataset created was used for anomaly detection. The results showed that the k-nearest neighbors (k-NN) and support vector machine with genetic algorithm (SVM GA) models perform best among the 18 techniques applied. In addition to detecting anomalies, the methods are able to flag the attack types better than other techniques and are applicable in EtherCAT networks. Also, the dataset and events can be used for further studies since it is difficult to obtain data for ICS due to its critical infrastructure and continuous real-time operation.","",""
6,"Yingxu Wang, Omar A. Zatarain, M. Valipour","Formal description of a supervised learning algorithm for concept elicitation by cognitive robots",2016,"","","","",169,"2022-07-13 09:25:27","","10.1109/ICCI-CC.2016.7862014","","",,,,,6,1.00,2,3,6,"Concept elicitation is centric for machine knowledge extraction and representation in cognitive robot learning. This paper presents a supervised methodology for machine concept elicitation from informal counterparts described in natural languages. The collective opinions of a given concept in ten selected dictionaries are quantitatively analyzed and formally represented according to the attribute-object-relation (OAR) pattern of formal concepts. The concept elicitation methodology for machine learning is aimed to deal with complex problems inherited in informal concepts of natural languages such as diversity, redundancy, ambiguity, inexplicit semantics, inconsistent attributes/objects, mixed synonyms, and fuzzy hyper-/hypo-concept relations. The system of formal concept elicitation is implemented by an algorithms in MATLAB for formal concept extraction and representation. Experiments on supervised machine learning for creating twenty primitive concepts reveal that a cognitive robot is able to learn synergized concepts in human knowledge in order to build its own cognitive knowledge base.","",""
4,"R. LeMoyne, T. Mastroianni, D. Whiting, N. Tomycz","Preliminary Network Centric Therapy for Machine Learning Classification of Deep Brain Stimulation Status for the Treatment of Parkinson’s Disease with a Conformal Wearable and Wireless Inertial Sensor",2019,"","","","",170,"2022-07-13 09:25:27","","10.4236/apd.2019.84007","","",,,,,4,1.33,1,4,3,"The concept of Network Centric Therapy represents an  amalgamation of wearable and wireless inertial sensor systems and machine  learning with access to a Cloud computing environment. The advent of Network  Centric Therapy is highly relevant to the treatment of Parkinson’s disease  through deep brain stimulation. Originally wearable and wireless systems for  quantifying Parkinson’s disease involved the use a smartphone to quantify hand  tremor. Although originally novel, the smartphone has notable issues as a  wearable application for quantifying movement disorder tremor. The smartphone  has evolved in a pathway that has made the smartphone progressively more  cumbersome to mount about the dorsum of the hand. Furthermore, the smartphone  utilizes an inertial sensor package that is not certified for medical analysis,  and the trial data access a provisional Cloud computing environment through an  email account. These concerns are resolved with the recent development of a  conformal wearable and wireless inertial sensor system. This conformal wearable  and wireless system mounts to the hand with the profile of a bandage by  adhesive and accesses a secure Cloud computing environment through a segmented  wireless connectivity strategy involving a smartphone and tablet. Additionally,  the conformal wearable and wireless system is certified by the FDA of the United  States of America for ascertaining medical grade inertial sensor data. These  characteristics make the conformal wearable and wireless system uniquely suited  for the quantification of Parkinson’s disease treatment through deep brain  stimulation. Preliminary evaluation of the conformal wearable and wireless  system is demonstrated through the differentiation of deep brain stimulation  set to “On” and “Off” status. Based on the robustness of the acceleration  signal, this signal was selected to quantify hand tremor for the prescribed  deep brain stimulation settings. Machine learning classification using the  Waikato Environment for Knowledge Analysis (WEKA) was applied using the  multilayer perceptron neural network. The multilayer perceptron neural network  achieved considerable classification accuracy for distinguishing between the  deep brain stimulation system set to “On” and “Off” status through the  quantified acceleration signal data obtained by this recently developed  conformal wearable and wireless system. The research achievement  establishes a progressive pathway to the future objective of achieving deep  brain stimulation capabilities that promote closed-loop acquisition of  configuration parameters that are uniquely optimized to the individual through  extrinsic means of a highly conformal wearable and wireless inertial sensor  system and machine learning with access to Cloud computing resources.","",""
2,"Marzieh Jalal Abadi, Luca Luceri, M. Hassan, C. Chou, M. Nicoli","A Cooperative Machine Learning Approach for Pedestrian Navigation in Indoor IoT",2019,"","","","",171,"2022-07-13 09:25:27","","10.3390/s19214609","","",,,,,2,0.67,0,5,3,"This paper presents a system based on pedestrian dead reckoning (PDR) for localization of networked mobile users, which relies only on sensors embedded in the devices and device- to-device connectivity. The user trajectory is reconstructed by measuring step by step the user displacements. Though step length can be estimated rather accurately, heading evaluation is extremely problematic in indoor environments. Magnetometer is typically used, however measurements are strongly perturbed. To improve the location accuracy, this paper proposes a novel cooperative system to estimate the direction of motion based on a machine learning approach for perturbation detection and filtering, combined with a consensus algorithm for performance augmentation by cooperative data fusion at multiple devices. A first algorithm filters out perturbed magnetometer measurements based on a-priori information on the Earth’s magnetic field. A second algorithm aggregates groups of users walking in the same direction, while a third one combines the measurements of the aggregated users in a distributed way to extract a more accurate heading estimate. To the best of our knowledge, this is the first approach that combines machine learning with consensus algorithms for cooperative PDR. Compared to other methods in the literature, the method has the advantage of being infrastructure-free, fully distributed and robust to sensor failures thanks to the pre-filtering of perturbed measurements. Extensive indoor experiments show that the heading error is highly reduced by the proposed approach thus leading to noticeable enhancements in localization performance.","",""
0,"B. Boguslawski, Matthieu Boujonnier, Loryne Bissuel-Beauvais, Fahd Saghir","Edge Analytics at the Wellhead: Designing Robust Machine Learning Models for Artificial Lift Failure Detection",2018,"","","","",172,"2022-07-13 09:25:27","","10.2118/192886-MS","","",,,,,0,0.00,0,4,4,"  This paper outlines the challenges and constraints related to deployment of Machine Learning solutions for rod pump abnormal states recognition and diagnosis at the wellhead. Those abnormal states may lead to a failure or to non-optimized production. Particular focus is on two main aspects: 1) Develop a robust Machine Learning model & IIoT architecture to predict rod pump failure directly at the wellhead, 2) Ensure high level of pump failure prediction through Machine Learning to ensure operator confidence.  To the best of our knowledge, this is the first-of-its-kind IIoT Edge Analytics solution which provides operators with the capability of automated Dynagraph Card recognition directly at the wellhead via Machine Learning models. This solution also addresses end-user requirements in terms of confidentiality and communication infrastructure.","",""
1,"Haozhe Zhang","Topics in functional data analysis and machine learning predictive inference",2019,"","","","",173,"2022-07-13 09:25:27","","","","",,,,,1,0.33,1,1,3,"This dissertation is composed of three research projects focused on functional data analysis and machine learning predictive inference. The first project deals with the covariance estimation, principal component analysis, and prediction of spatially correlated functional data. We develop a general framework and fully nonparametric estimation methods for spatial functional data collected under a geostatistics setting, where locations are sampled from a spatial point process and a random function is discretely observed at each location and contaminated with a functional nugget effect and measurement errors. Unified asymptotic convergence rates are developed for the proposed estimators that are applicable to both sparse and dense functional data. Simulation studies and analyses of two real-estate datasets show that our proposed approach outperforms other state-of-the-art approaches. In the second project, we present a novel application of functional modeling to plant phenotypic data derived from crowdscourced images annotated by Amazon Mechanical Turk (MTurk) workers. The goal of this study is to estimate the effect of genotype and its interaction with environment on plant growth while adjusting for measurement errors from crowdsourcing image analysis. We assume plant height measurements as discrete observations of growth curves contaminated with MTurk worker random effects and heteroscedastic measurement errors. A reduced-rank functional model, along with a robust and shape-constrained estimation approach, is developed for growth curves and derivatives that depend on replicates, genotypes, and environmental conditions. As byprodxvii ucts, the proposed model leads to a new method for assessing the quality of MTurk worker data and an index for measuring the sensitivity to drought for various genotypes. In the third project, we propose a new approach to constructing random forest prediction intervals that utilizes the empirical distribution of out-of-bag prediction errors, and provides theory that guarantees asymptotic coverage for the proposed intervals. We perform extensive numerical experiments along with analysis of 60 real datasets to compare the finite-sample properties of the proposed intervals with two state-of-the-art approaches: quantile regression forests and split conformal intervals. The results demonstrate the advantages, reliability and efficiency of the proposed approach. 1 CHAPTER 1. GENERAL INTRODUCTION 1.1 Spatially Correlated Functional Data Statistical methodology and theory for analysis of indepedent functional data have been well developed and studied in the past decades (Ramsay and Silverman, 2005; Yao et al., 2005; Ferraty and Vieu, 2006). However, it is often unrealistic to assume independence in many real applications, especially when the functional data are collected over space or time (Hörmann and Kokoszka, 2010). Therefore, It is reasonable to expect that the functional data observed at one location may be naturally correlated with the observations in the neighboring area to some extent. The violation of independence assumption has motivated recent research on dependent functional data, including multi-level functional data (Crainiceanu et al., 2009; Xu et al., 2018a), functional time series (Aue et al., 2015; Paparoditis, 2018), and spatially dependent functional data (Baladandayuthapani et al., 2008; Zhou et al., 2010; Staicu et al., 2010; Gromenko et al., 2012; Zhang et al., 2016a,b; Liu et al., 2017). Most existing papers on spatially dependent functional data focused on modeling and methodology developments; and those with theoretical justifications usually considered the ideal situation where the trajectories of functional data are fully observed. In practice, functional data are often observed on discrete time points and the measurements are contaminated with errors. Based on the number of observations on each curve, functional data are traditionally classified as sparse functional data (Yao et al., 2005) and dense functional data (Hall et al., 2006). For independent functional data, it is known that the convergence rates for various functional estimators (such as the mean, covariance and 2 principal components) are different under different sampling schemes. There is also a grey zone between sparse and dense functional data where the convergence rate of a functional estimator is between nonparametric and parametric rates. Many recent research efforts focused on developing unified estimation and inference strategies for all types of functional data (Li and Hsing, 2010; Zhang and Wang, 2016; Wang et al., 2018). No such results yet exist for spatially dependent functional data. In addition, functional nugget effects have not been studied in the literature. In Chapter 2, motivated by two real-estate datasets, we propose a general framework and estimation methods for spatially dependent functional data collected under a geostatistics setting, where locations are sampled from a spatial point process and a random function is observed at each location. We assume that the functional response is the sum of a temporal process that is spatially correlated with neighboring functions and a location-specific random process which characterizes the local variations and is independent from neighbors. The location-specific random process is also interpreted as the “nugget” effect following classic geostatistics literature (Cressie, 1993). Observations on each function are made on discrete time points and contaminated with measurement errors. Under the assumption of spatial stationarity and isotropy, we propose a tensor product spline estimator for the spatio-temporal covariance function. If a coregionalization covariance structure (Banerjee et al., 2003; Gelfand et al., 2004) is further assumed, we propose a new functional principal component analysis method that borrows information from neighboring functions. Byproducts of our approach also include nonparametric estimators for the spatial covariance functions of the principal component scores. The proposed method also generates nonparametric estimators for the spatial covariance functions, which can be used for functional kriging. Under an increasing domain asymptotic framework (Guan et al., 2004; Li and Guan, 2014), we develop unified asymptotic convergence rates for the proposed estimators that are applicable to both sparse and dense 3 functional data and allow the number of observations per curve to be of any rate relative to the number of functions. 1.2 Functional Modeling of Crowdsourced Growth Data In the literature, functional data analysis (Ramsay and Silverman, 2005) has been extensively applied to growth studies which give rise to longitidual data measured for experimental units or subjects over time (Diggle et al., 2002; Fitzmaurice et al., 2012). As examples of recent relevant work, Dai et al. (2017) proposed a new estimation approach to estimating derivatives with an application to Tammar Wallaby growth data, and Xu et al. (2018b) analyzed the empirical dynamics of plant growth by the functional ANOVA method. In these studies, functional data modeling has shown its advantages in modeling growth curves which are latent, smooth, and very often obscured by measurement errors and contaminated observations. Crowdsourcing is an effective technique for data collection popularly used in many scientific areas. For example, Zhou et al. (2018) explored the use of crowdsoucring to segment corn tassels from images taken in the crop field; Can et al. (2017) discussed the promising application of crowdscouring in wildlife research and conservation; In Griffith et al. (2017), a new expert-crowdsourced knowledgebase was applied in the clinical interpretation of variants in cancer; Fritz et al. (2017) describes a global dataset of crowdsourced land cover and land use reference data. Due to its low-cost, efficiency, and overall high-quality advantages, the advent of crowdsourcing techniques has created intriguing new opportunities for improving upon classical methods of data collection and annotation (Lease, 2011). However, this approach also introduces challenging problems for data analysis, such as quantifying and adjusting the unccertainty from crowdsourcing procedures, evaluating data quality, detecting outliers, or handling disagreements among mul4 tiple measurements on the same unit (Ruiz et al., 2019). All these problems, together with wide availability of crowdsourced data, encourage researchers to develop new solutions that are statistically and scientifically sound and practical. To name a few, recent methodological developments in analyzing crowdsourced data include Raykar et al. (2010), Ruiz et al. (2016), and Giuffrida et al. (2018). To our knowledge, our work presented in Chapter 3 is the first study that analyzes crowdsourced growth data, motivated by a maize plant growth study conducted by a group of plant scientists, engineers, and statisticians. The goal of this study is to identify maize genotypes that are most sensitive or resistant to water stress in the context of the entire growth development. The maize growth data were derived from high-throughput phenotyping technology and crowdsourcing image analysis. During the growing season, maize plants of various genotypes were imaged by hundreds of cameras. Amazon Mechanical Turk (MTurk) workers were hired to manually mark plant bodies on these images, from which plant heights were obtained. We propose a novel functional data model and a robust shape-constrained estimation procedure for plant height measurements. Advantages of our proposed approaches are demonstrated by real data analysis in Section 3.6 and synthetic experiments in Section 3.7. 1.3 Prediction Intervals for Random Forests Diagnostics, interpretation, and uncertainty quantification of machine learning algorithms have received increasing attention recently. Predictive inference (Lei et al., 2018; Shen et al., 2018), as a ","",""
87,"Nicholas Wagner, J. Rondinelli","Theory-Guided Machine Learning in Materials Science",2016,"","","","",174,"2022-07-13 09:25:27","","10.3389/fmats.2016.00028","","",,,,,87,14.50,44,2,6,"Materials scientists are increasingly adopting the use of machine learning tools to discover hidden trends in data and make predictions. Applying concepts from data science without foreknowledge of their limitations and the unique qualities of materials data, however, could lead to errant conclusions. The differences that exist between various kinds of experimental and calculated data require careful choices of data processing and machine learning methods. Here, we outline potential pitfalls involved in using machine learning without robust protocols. We address some problems of overfitting to training data using decision trees as an example, rational descriptor selection in the field of perovskites, and preserving physical interpretability in the application of dimensionality reducing techniques such as principal component analysis. We show how proceeding without the guidance of domain knowledge can lead to both quantitatively and qualitatively incorrect predictive models.","",""
1,"Cassio Polpo de Campos, Alessandro Antonucci","Imprecision in Machine Learning and AI",2015,"","","","",175,"2022-07-13 09:25:27","","","","",,,,,1,0.14,1,2,7,"IN this note we consider five different relevant problems in AI and machine learning. We argue that possible solutions to such problems might be achieved by replacing the probability distributions in the systems with sets of them. Such a robust approach is based on the so-called impreciseprobabilistic framework. The proposed solutions provide a persuasive justification of the imprecise framework. The problems we consider are: • proper treatment of missing data, • reliable classification, • sensitivity analysis, • feature selection, • elicitation of qualitative expert knowledge. Before reporting a separate discussion for each problem, let us briefly resume the general ideas characterising imprecise-probabilistic methods.","",""
19,"A. Menon, Chetali Gupta, K. Perkins, B. DeCost, Nikita Budwal, Renee T. Rios, Kunpeng Zhang, B. Póczos, N. Washburn","Elucidating multi-physics interactions in suspensions for the design of polymeric dispersants: a hierarchical machine learning approach",2017,"","","","",176,"2022-07-13 09:25:27","","10.1039/C7ME00027H","","",,,,,19,3.80,2,9,5,"A computational method for understanding and optimizing the properties of complex physical systems is presented using polymeric dispersants as an example. Concentrated suspensions are formulated with dispersants to tune rheological parameters, such as yield stress or viscosity, but their competing effects on solution and particle variables have made it impossible to design them based on our knowledge of the interplay of chemistry and function. Here, physical and statistical modeling are integrated into a hierarchical framework of machine learning that provides insight into sparse experimental datasets. A library of 10 polymers having similar molecular weight but incorporating different functional groups commonly found in aqueous dispersants was used as a training set in magnesium oxide slurries. The compositions of these polymers were the experimental variables that determined the complex system responses, but the method leverages knowledge of the constituent “single-physics” interactions that underlie the suspension properties. Integration of domain knowledge is shown to allow robust predictions based on orders of magnitude fewer samples in the training set compared with purely statistical methods that directly correlate dispersant chemistry with changes in rheological properties. Minimization of the resulting function for slurry yield stress resulted in the prediction of a novel dispersant that was synthesized and shown to impart similar reductions as a leading commercial dispersant but with a significantly different composition and molecular architecture.","",""
17,"V. Mir, evská, M. Luštrek, M. Gams","Combining machine learning and expert knowledge for classifying human posture",2009,"","","","",177,"2022-07-13 09:25:27","","","","",,,,,17,1.31,4,4,13,"This paper presents a rule engine for classifying h uman posture according to information about the location f body parts. The rule engine was developed by enrich i g decision trees with expert knowledge. Results show 5 percentage points improvement in accuracy compared to support vector machines and a significant 11 percentage points compared to decision trees. The incorporation of expert knowledge overcomes the problem of classifier over-fitting observed with classifiers induced with machine learning. Better robustness of the posture classification rule engin e is expected in real-life tests in comparison to classi fiers induced with machine learning.","",""
14,"Zainab Abaid, M. Kâafar, Sanjay Jha","Quantifying the impact of adversarial evasion attacks on machine learning based android malware classifiers",2017,"","","","",178,"2022-07-13 09:25:27","","10.1109/NCA.2017.8171381","","",,,,,14,2.80,5,3,5,"With the proliferation of Android-based devices, malicious apps have increasingly found their way to user devices. Many solutions for Android malware detection rely on machine learning; although effective, these are vulnerable to attacks from adversaries who wish to subvert these algorithms and allow malicious apps to evade detection. In this work, we present a statistical analysis of the impact of adversarial evasion attacks on various linear and non-linear classifiers, using a recently proposed Android malware classifier as a case study. We systematically explore the complete space of possible attacks varying in the adversary's knowledge about the classifier; our results show that it is possible to subvert linear classifiers (Support Vector Machines and Logistic Regression) by perturbing only a few features of malicious apps, with more knowledgeable adversaries degrading the classifier's detection rate from 100% to 0% and a completely blind adversary able to lower it to 12%. We show non-linear classifiers (Random Forest and Neural Network) to be more resilient to these attacks. We conclude our study with recommendations for designing classifiers to be more robust to the attacks presented in our work.","",""
35,"Emir Muñoz, V. Nováček, P. Vandenbussche","Facilitating prediction of adverse drug reactions by using knowledge graphs and multi‐label learning models",2019,"","","","",179,"2022-07-13 09:25:27","","10.1093/bib/bbx099","","",,,,,35,11.67,12,3,3,"Abstract Timely identification of adverse drug reactions (ADRs) is highly important in the domains of public health and pharmacology. Early discovery of potential ADRs can limit their effect on patient lives and also make drug development pipelines more robust and efficient. Reliable in silico prediction of ADRs can be helpful in this context, and thus, it has been intensely studied. Recent works achieved promising results using machine learning. The presented work focuses on machine learning methods that use drug profiles for making predictions and use features from multiple data sources. We argue that despite promising results, existing works have limitations, especially regarding flexibility in experimenting with different data sets and/or predictive models. We suggest to address these limitations by generalization of the key principles used by the state of the art. Namely, we explore effects of: (1) using knowledge graphs—machine‐readable interlinked representations of biomedical knowledge—as a convenient uniform representation of heterogeneous data; and (2) casting ADR prediction as a multi‐label ranking problem. We present a specific way of using knowledge graphs to generate different feature sets and demonstrate favourable performance of selected off‐the‐shelf multi‐label learning models in comparison with existing works. Our experiments suggest better suitability of certain multi‐label learning methods for applications where ranking is preferred. The presented approach can be easily extended to other feature sources or machine learning methods, making it flexible for experiments tuned toward specific requirements of end users. Our work also provides a clearly defined and reproducible baseline for any future related experiments.","",""
16,"Xiaokai Liu, Cheng-lin Zhao, Pengbiao Wang, Yang Zhang, Tian-le Yang","Blind modulation classification algorithm based on machine learning for spatially correlated MIMO system",2017,"","","","",180,"2022-07-13 09:25:27","","10.1049/iet-com.2015.1222","","",,,,,16,3.20,3,5,5,"Spatial correlation is a decisive factor for pragmatic multiple-input multiple-output (MIMO) system, simultaneously bringing about some problems in the received signal modulation identification respect. In this study, the authors focus on blind digital modulation identification in the spatially correlated MIMO system and deliver a robust signal recognition algorithm based on extreme learning machine (ELM) and higher order statistical features for MIMO signal identification without a priori knowledge of the channel and signal parameters. The superiority of ELM lies in random selections of hidden nodes and ascertains output weights analytically, which result in lower computational complexity. Theoretically, this algorithm has a tendency to supply excellent generalisation performance at staggering learning rate. Further, the simulation results indicate that the ELM could reap a perfectly acceptable recognition performance and thus provides a solid ground structure for tackling MIMO modulation challenges in low signal-to-noise ratio.","",""
8,"Tong Li","Identifying Security Requirements Based on Linguistic Analysis and Machine Learning",2017,"","","","",181,"2022-07-13 09:25:27","","10.1109/APSEC.2017.45","","",,,,,8,1.60,8,1,5,"Eliciting security requirements in early stage of system development has been widely recognized as an efficient way for minimizing security cost and avoiding recurring security problems. However, in many projects, security requirements are not explicitly specified but rather mixed with other requirements, requiring precise and fast identification of such security requirements. Although several probability-based approaches have been proposed to tackle this problem, they are either imprecise or domain-dependent. In this paper, we propose a tool-supported method to efficiently identify security requirements, which combines linguistic analysis with machine learning techniques. In particular, we apply a systematic approach to identify linguistic features of security requirements based on existing security requirements ontologies and linguistic knowledge. We automatically extract such features from textual requirements, which are then used to train security requirements classifiers using typical machine learning techniques. We have implemented a prototype tool to support our approach, and have systematically evaluated our approach based on three realistic requirements specifications. The evaluation results show that our approach has promising potential to train classifiers that can classify requirements specifications from different application domains.","",""
9,"Qingxue Zhang, Dian Zhou, Xuan Zeng","Machine Learning-Empowered Biometric Methods for Biomedicine Applications",2017,"","","","",182,"2022-07-13 09:25:27","","10.3934/MEDSCI.2017.3.274","","",,,,,9,1.80,3,3,5,"Nowadays, pervasive computing technologies are paving a promising way for advanced smart health applications. However, a key impediment faced by wide deployment of these assistive smart devices, is the increasing privacy and security issue, such as how to protect access to sensitive patient data in the health record. Focusing on this challenge, biometrics are attracting intense attention in terms of effective user identification to enable confidential health applications. In this paper, we take special interest in two bio-potential-based biometric modalities, electrocardiogram (ECG) and electroencephalogram (EEG), considering that they are both unique to individuals, and more reliable than token (identity card) and knowledge-based (username/password) methods. After extracting effective features in multiple domains from ECG/EEG signals, several advanced machine learning algorithms are introduced to perform the user identification task, including Neural Network, K-nearest Neighbor, Bagging, Random Forest and AdaBoost. Experimental results on two public ECG and EEG datasets show that ECG is a more robust biometric modality compared to EEG, leveraging a higher signal to noise ratio and also more distinguishable morphological patterns. Among different machine learning classifiers, the random forest greatly outperforms the others and owns an identification rate as high as 98%. This study is expected to demonstrate that properly selected biometric empowered by an effective machine learner owns a great potential, to enable confidential biomedicine applications in the era of smart digital health.","",""
6,"C. Brecher, M. Obdenbusch, Melanie Buchsbaum","Optimized state estimation by application of machine learning",2017,"","","","",183,"2022-07-13 09:25:27","","10.1007/s11740-017-0724-9","","",,,,,6,1.20,2,3,5,"","",""
2,"D. Urda, Rafael Marcos Luque Baena, L. Franco, J. M. Jerez, N. Sánchez-Maroño","Machine learning models to search relevant genetic signatures in clinical context",2017,"","","","",184,"2022-07-13 09:25:27","","10.1109/IJCNN.2017.7966049","","",,,,,2,0.40,0,5,5,"Clinicians are interested in the estimation of robust and relevant genetic signatures from gene sequencing data. Many machine learning approaches have been proposed trying to address well-known issues of this complex task (feature or gene selection, classification or model selection, and prediction assessment). Addressing this problem often requires a deep knowledge of these methods and some of them demand high computational resources that may not be affordable. In this paper, an exhaustive study that includes different types of feature selection methods and classifiers is presented, providing clinicians an useful insight of the most suitable methods for this purpose. Predictions assessment is performed using a bootstrap cross-validation strategy as an honest validation scheme. The results of this study for six benchmark datasets show that filter or embedded methods are preferred, in general, to wrapper methods according to their better statistical significant results, in terms of accuracy, and lower demand for computational resources.","",""
376,"Rui Zhao, Ruqiang Yan, Jinjiang Wang, K. Mao","Learning to Monitor Machine Health with Convolutional Bi-Directional LSTM Networks",2017,"","","","",185,"2022-07-13 09:25:27","","10.3390/s17020273","","",,,,,376,75.20,94,4,5,"In modern manufacturing systems and industries, more and more research efforts have been made in developing effective machine health monitoring systems. Among various machine health monitoring approaches, data-driven methods are gaining in popularity due to the development of advanced sensing and data analytic techniques. However, considering the noise, varying length and irregular sampling behind sensory data, this kind of sequential data cannot be fed into classification and regression models directly. Therefore, previous work focuses on feature extraction/fusion methods requiring expensive human labor and high quality expert knowledge. With the development of deep learning methods in the last few years, which redefine representation learning from raw data, a deep neural network structure named Convolutional Bi-directional Long Short-Term Memory networks (CBLSTM) has been designed here to address raw sensory data. CBLSTM firstly uses CNN to extract local features that are robust and informative from the sequential input. Then, bi-directional LSTM is introduced to encode temporal information. Long Short-Term Memory networks (LSTMs) are able to capture long-term dependencies and model sequential data, and the bi-directional structure enables the capture of past and future contexts. Stacked, fully-connected layers and the linear regression layer are built on top of bi-directional LSTMs to predict the target value. Here, a real-life tool wear test is introduced, and our proposed CBLSTM is able to predict the actual tool wear based on raw sensory data. The experimental results have shown that our model is able to outperform several state-of-the-art baseline methods.","",""
55,"Zan Gao, Yinming Li, S. Wan","Exploring Deep Learning for View-Based 3D Model Retrieval",2020,"","","","",186,"2022-07-13 09:25:27","","10.1145/3377876","","",,,,,55,27.50,18,3,2,"In recent years, view-based 3D model retrieval has become one of the research focuses in the field of computer vision and machine learning. In fact, the 3D model retrieval algorithm consists of feature extraction and similarity measurement, and the robust features play a decisive role in the similarity measurement. Although deep learning has achieved comprehensive success in the field of computer vision, deep learning features are used for 3D model retrieval only in a small number of works. To the best of our knowledge, there is no benchmark to evaluate these deep learning features. To tackle this problem, in this work we systematically evaluate the performance of deep learning features in view-based 3D model retrieval on four popular datasets (ETH, NTU60, PSB, and MVRED) by different kinds of similarity measure methods. In detail, the performance of hand-crafted features and deep learning features are compared, and then the robustness of deep learning features is assessed. Finally, the difference between single-view deep learning features and multi-view deep learning features is also evaluated. By quantitatively analyzing the performances on different datasets, it is clear that these deep learning features can consistently outperform all of the hand-crafted features, and they are also more robust than the hand-crafted features when different degrees of noise are added into the image. The exploration of latent relationships among different views in multi-view deep learning network architectures shows that the performance of multi-view deep learning outperforms that of single-view deep learning features with low computational complexity.","",""
0,"Hsiao-Chi Li, Chang-Yu Cheng, Chia Chou, Chien-Chang Hsu, Meng-Lin Chang, Y. Chiu, J. Chai","Multi-Class Brain Age Discrimination Using Machine Learning Algorithm",2019,"","","","",187,"2022-07-13 09:25:27","","10.1109/ICMLC48188.2019.8949317","","",,,,,0,0.00,0,7,3,"Resting-state functional connectivity analyses have revealed a significant effect on the inter-regional interactions in brain. The brain age prediction based on resting-state functional magnetic resonance imaging has been proved as biomarkers to characterize the typical brain development and neuropsychiatric disorders. The brain age prediction model based on functional connectivity measurements derived from resting-state functional magnetic resonance imaging has received a lots of interest in recent years due to its great success in age prediction. However, some of the recent studies rely on experienced neuroscientist experts to select appropriate connectivity features in order to build a robust model for prediction while the others just selected the features based on trial-and-error test. Besides, the subjects used in this studies omitted some subjects that can be divided into two groups with less similarity which may confused the prediction model. In this study, we proposed a multi-class age categories discrimination method with the connectivity features selected via K-means clustering with no prior knowledge provided. The experimental results show that with K-means selected features the proposed model better discriminate multi-class age categories.","",""
57,"Sawsan Abdulrahman, Hanine Tout, Hakima Ould-Slimane, A. Mourad, C. Talhi, M. Guizani","A Survey on Federated Learning: The Journey From Centralized to Distributed On-Site Learning and Beyond",2021,"","","","",188,"2022-07-13 09:25:27","","10.1109/JIOT.2020.3030072","","",,,,,57,57.00,10,6,1,"Driven by privacy concerns and the visions of deep learning, the last four years have witnessed a paradigm shift in the applicability mechanism of machine learning (ML). An emerging model, called federated learning (FL), is rising above both centralized systems and on-site analysis, to be a new fashioned design for ML implementation. It is a privacy-preserving decentralized approach, which keeps raw data on devices and involves local ML training while eliminating data communication overhead. A federation of the learned and shared models is then performed on a central server to aggregate and share the built knowledge among participants. This article starts by examining and comparing different ML-based deployment architectures, followed by in-depth and in-breadth investigation on FL. Compared to the existing reviews in the field, we provide in this survey a new classification of FL topics and research fields based on thorough analysis of the main technical challenges and current related work. In this context, we elaborate comprehensive taxonomies covering various challenging aspects, contributions, and trends in the literature, including core system models and designs, application areas, privacy and security, and resource management. Furthermore, we discuss important challenges and open research directions toward more robust FL systems.","",""
30,"M. Mozina, Matej Guid, J. Krivec, A. Sadikov, I. Bratko","Fighting Knowledge Acquisition Bottleneck with Argument Based Machine Learning",2008,"","","","",189,"2022-07-13 09:25:27","","10.3233/978-1-58603-891-5-234","","",,,,,30,2.14,6,5,14,"Knowledge elicitation is known to be a difficult task and thus a major bottleneck in building a knowledge base. Machine learning has long ago been proposed as a way to alleviate this problem. Machine learning usually helps the domain expert to uncover some of the more tacit concepts. However, the learned concepts are often hard to understand and hard to extend. A common view is that a combination of a domain expert and machine learning would yield the best results. Recently, argument based machine learning (ABML) has been introduced as a combination of argumentation and machine learning. Through argumentation, ABML enables the expert to articulate his knowledge easily and in a very natural way. ABML was shown to significantly improve the comprehensibility and accuracy of the learned concepts. This makes ABML a most natural tool for constructing a knowledge base. The present paper shows how this is accomplished through a case study of building a knowledge base of an expert system used in a chess tutoring application.","",""
0,"Zhuang Liu, Kaiyu Huang, Ziyu Gao, Degen Huang","Knowledge-Aware LSTM for Machine Comprehension",2019,"","","","",190,"2022-07-13 09:25:27","","10.1109/ISKE47853.2019.9170351","","",,,,,0,0.00,0,4,3,"Machine Comprehension (MC) of text is the problem to answer a query based on a given document. Although MC has been very popular recently, it still have some serious weaknesses which rely only on query-to-document interaction or its learning is just heavily dependent on the training data. To take advantage of external knowledge to improve neural networks for MC, we propose a novel knowledge enhanced recurrent neural model, called knowledge-aware LSTM (k-LSTM), an extension to basic LSTM cells, designed to exploit external knowledge bases (KBs) to improve neural networks for MC task. To incorporate KBs with contextual information effectively from the currently text, k-LSTM employs an compositional attention mechanism to adaptively decide whether to attend to KBs and which information from external knowledge is useful. Furthermore, we present our knowledge enhanced neural network, called Knowledge-guided DIM Reader (K-DIM Reader), which is a novel knowledge-aware compositional attention neural network architecture, employing the k-LSTM in our framework. By stringing external background knowledge together and imposing compositional attention interaction that regulate their interaction, K-DIM Reader effectively learns to perform reading comprehension processes that are directly inferred from the data in an end-to-end approach. We show our proposed models strength, robustness and interpretability on the challenging MC datasets, achieving significant improvements on SQuAD dataset [1] and obtaining new state-of-the-art results on both Cloze-style datasets, CBTest [2] and CNN news [3]. In particular, we further extend 6 popular end-to-end neural MC models using k-LSTM incorporating knowledge into models for improving MC, and evaluate their performance on both well-known MC datasets. We demonstrate that neural model with external knowledge improves performance on MC task.","",""
28,"Pouya Pezeshkpour, Yifan Tian, Sameer Singh","Investigating Robustness and Interpretability of Link Prediction via Adversarial Modifications",2018,"","","","",191,"2022-07-13 09:25:27","","10.18653/v1/N19-1337","","",,,,,28,7.00,9,3,4,"Representing entities and relations in an embedding space is a well-studied approach for machine learning on relational data. Existing approaches, however, primarily focus on improving accuracy and overlook other aspects such as robustness and interpretability. In this paper, we propose adversarial modifications for link prediction models: identifying the fact to add into or remove from the knowledge graph that changes the prediction for a target fact after the model is retrained. Using these single modifications of the graph, we identify the most influential fact for a predicted link and evaluate the sensitivity of the model to the addition of fake facts. We introduce an efficient approach to estimate the effect of such modifications by approximating the change in the embeddings when the knowledge graph changes. To avoid the combinatorial search over all possible facts, we train a network to decode embeddings to their corresponding graph components, allowing the use of gradient-based optimization to identify the adversarial modification. We use these techniques to evaluate the robustness of link prediction models (by measuring sensitivity to additional facts), study interpretability through the facts most responsible for predictions (by identifying the most influential neighbors), and detect incorrect facts in the knowledge base.","",""
56,"Haotian Lin, Erping Long, Xiaohu Ding, Hongxing Diao, Zicong Chen, Runzhong Liu, Jialing Huang, Jingheng Cai, Shuangjuan Xu, Xiayin Zhang, Dongni Wang, Kexin Chen, Tongyong Yu, Dongxuan Wu, Xutu Zhao, Zhenzhen Liu, Xiaohang Wu, Yuzhen Jiang, X. Yang, Dongmei Cui, Wenyan Liu, Yingfeng Zheng, L. Luo, Haibo Wang, Chi-Chao Chan, I. Morgan, M. He, Yizhi Liu","Prediction of myopia development among Chinese school-aged children using refraction data from electronic medical records: A retrospective, multicentre machine learning study",2018,"","","","",192,"2022-07-13 09:25:27","","10.1371/journal.pmed.1002674","","",,,,,56,14.00,6,28,4,"Background Electronic medical records provide large-scale real-world clinical data for use in developing clinical decision systems. However, sophisticated methodology and analytical skills are required to handle the large-scale datasets necessary for the optimisation of prediction accuracy. Myopia is a common cause of vision loss. Current approaches to control myopia progression are effective but have significant side effects. Therefore, identifying those at greatest risk who should undergo targeted therapy is of great clinical importance. The objective of this study was to apply big data and machine learning technology to develop an algorithm that can predict the onset of high myopia, at specific future time points, among Chinese school-aged children. Methods and findings Real-world clinical refraction data were derived from electronic medical record systems in 8 ophthalmic centres from January 1, 2005, to December 30, 2015. The variables of age, spherical equivalent (SE), and annual progression rate were used to develop an algorithm to predict SE and onset of high myopia (SE ≤ −6.0 dioptres) up to 10 years in the future. Random forest machine learning was used for algorithm training and validation. Electronic medical records from the Zhongshan Ophthalmic Centre (a major tertiary ophthalmic centre in China) were used as the training set. Ten-fold cross-validation and out-of-bag (OOB) methods were applied for internal validation. The remaining 7 independent datasets were used for external validation. Two population-based datasets, which had no participant overlap with the ophthalmic-centre-based datasets, were used for multi-resource validation testing. The main outcomes and measures were the area under the curve (AUC) values for predicting the onset of high myopia over 10 years and the presence of high myopia at 18 years of age. In total, 687,063 multiple visit records (≥3 records) of 129,242 individuals in the ophthalmic-centre-based electronic medical record databases and 17,113 follow-up records of 3,215 participants in population-based cohorts were included in the analysis. Our algorithm accurately predicted the presence of high myopia in internal validation (the AUC ranged from 0.903 to 0.986 for 3 years, 0.875 to 0.901 for 5 years, and 0.852 to 0.888 for 8 years), external validation (the AUC ranged from 0.874 to 0.976 for 3 years, 0.847 to 0.921 for 5 years, and 0.802 to 0.886 for 8 years), and multi-resource testing (the AUC ranged from 0.752 to 0.869 for 4 years). With respect to the prediction of high myopia development by 18 years of age, as a surrogate of high myopia in adulthood, the algorithm provided clinically acceptable accuracy over 3 years (the AUC ranged from 0.940 to 0.985), 5 years (the AUC ranged from 0.856 to 0.901), and even 8 years (the AUC ranged from 0.801 to 0.837). Meanwhile, our algorithm achieved clinically acceptable prediction of the actual refraction values at future time points, which is supported by the regressive performance and calibration curves. Although the algorithm achieved balanced and robust performance, concerns about the compromised quality of real-world clinical data and over-fitting issues should be cautiously considered. Conclusions To our knowledge, this study, for the first time, used large-scale data collected from electronic health records to demonstrate the contribution of big data and machine learning approaches to improved prediction of myopia prognosis in Chinese school-aged children. This work provides evidence for transforming clinical practice, health policy-making, and precise individualised interventions regarding the practical control of school-aged myopia.","",""
38,"S. Raschka, R. S. Olson","Python machine learning : unlock deeper insights into machine learning with this vital guide to cutting-edge predictive analytics",2015,"","","","",193,"2022-07-13 09:25:27","","","","",,,,,38,5.43,19,2,7,"Unlock deeper insights into Machine Leaning with this vital guide to cutting-edge predictive analytics About This Book * Leverage Python's most powerful open-source libraries for deep learning, data wrangling, and data visualization * Learn effective strategies and best practices to improve and optimize machine learning systems and algorithms * Ask and answer tough questions of your data with robust statistical models, built for a range of datasets Who This Book Is For If you want to find out how to use Python to start answering critical questions of your data, pick up Python Machine Learning whether you want to get started from scratch or want to extend your data science knowledge, this is an essential and unmissable resource. What You Will Learn * Explore how to use different machine learning models to ask different questions of your data * Learn how to build neural networks using Keras and Theano * Find out how to write clean and elegant Python code that will optimize the strength of your algorithms * Discover how to embed your machine learning model in a web application for increased accessibility * Predict continuous target outcomes using regression analysis * Uncover hidden patterns and structures in data with clustering * Organize data using effective pre-processing techniques * Get to grips with sentiment analysis to delve deeper into textual and social media data In Detail Machine learning and predictive analytics are transforming the way businesses and other organizations operate. Being able to understand trends and patterns in complex data is critical to success, becoming one of the key strategies for unlocking growth in a challenging contemporary marketplace. Python can help you deliver key insights into your data its unique capabilities as a language let you build sophisticated algorithms and statistical models that can reveal new perspectives and answer key questions that are vital for success. Python Machine Learning gives you access to the world of predictive analytics and demonstrates why Python is one of the world's leading data science languages. If you want to ask better questions of data, or need to improve and extend the capabilities of your machine learning systems, this practical data science book is invaluable. Covering a wide range of powerful Python libraries, including scikit-learn, Theano, and Keras, and featuring guidance and tips on everything from sentiment analysis to neural networks, you'll soon be able to answer some of the most important questions facing you and your organization. Style and approach Python Machine Learning connects the fundamental theoretical principles behind machine learning to their practical application in a way that focuses you on asking and answering the right questions. It walks you through the key elements of Python and its powerful machine learning libraries, while demonstrating how to get to grips with a range of statistical models.","",""
19,"A. Aravkin, Damek Davis","A SMART Stochastic Algorithm for Nonconvex Optimization with Applications to Robust Machine Learning",2016,"","","","",194,"2022-07-13 09:25:27","","","","",,,,,19,3.17,10,2,6,"In this paper, we show how to transform any optimization problem that arises from fitting a machine learning model into one that (1) detects and removes contaminated data from the training set while (2) simultaneously fitting the trimmed model on the uncontaminated data that remains. To solve the resulting nonconvex optimization problem, we introduce a fast stochastic proximal-gradient algorithm that incorporates prior knowledge through nonsmooth regularization. For datasets of size $n$, our approach requires $O(n^{2/3}/\varepsilon)$ gradient evaluations to reach $\varepsilon$-accuracy and, when a certain error bound holds, the complexity improves to $O(\kappa n^{2/3}\log(1/\varepsilon))$. These rates are $n^{1/3}$ times better than those achieved by typical, full gradient methods.","",""
11,"Namwoo Kang, Yi Ren, F. Feinberg, P. Papalambros","Form + Function: Optimizing Aesthetic Product Design via Adaptive, Geometrized Preference Elicitation",2019,"","","","",195,"2022-07-13 09:25:27","","","","",,,,,11,3.67,3,4,3,"Visual design is critical to product success, and the subject of intensive marketing research effort. Yet visual elements, due to their holistic and interactive nature, do not lend themselves well to optimization using extant decompositional methods for preference elicitation. Here we present a systematic methodology to incorporate interactive, 3D-rendered product configurations into a conjoint-like framework. The method relies on rapid, scalable machine learning algorithms to adaptively update product designs along with standard information-oriented product attributes. At its heart is a parametric account of a product's geometry, along with a novel, adaptive ""bi-level"" query task that can estimate individuals' visual design form preferences and their trade-offs against such traditional elements as price and product features. We illustrate the method's performance through extensive simulations and robustness checks, a formal proof of the bi-level query methodology's domain of superiority, and a field test for the design of a mid-priced sedan, using real-time 3D rendering for an online panel. Results indicate not only substantially enhanced predictive accuracy, but two quantities beyond the reach of standard conjoint methods: trade-offs between form and function overall, and willingness-to-pay for specific design elements. Moreover -- and most critically for applications -- the method provides ""optimal"" visual designs for both individuals and model-derived or analyst-supplied consumer groupings, as well as their sensitivities to form and functional elements.","",""
29,"T. Ishikawa, A. Hayashi, Shingo Nagamatsu, Y. Kyutoku, I. Dan, T. Wada, K. Oku, Y. Saeki, T. Uto, T. Tanabata, S. Isobe, N. Kochi","CLASSIFICATION OF STRAWBERRY FRUIT SHAPE BY MACHINE LEARNING",2018,"","","","",196,"2022-07-13 09:25:27","","10.5194/ISPRS-ARCHIVES-XLII-2-463-2018","","",,,,,29,7.25,3,12,4,"Abstract. Shape is one of the most important traits of agricultural products due to its relationships with the quality, quantity, and value of the products. For strawberries, the nine types of fruit shape were defined and classified by humans based on the sampler patterns of the nine types. In this study, we tested the classification of strawberry shapes by machine learning in order to increase the accuracy of the classification, and we introduce the concept of computerization into this field. Four types of descriptors were extracted from the digital images of strawberries: (1) the Measured Values (MVs) including the length of the contour line, the area, the fruit length and width, and the fruit width/length ratio; (2) the Ellipse Similarity Index (ESI); (3) Elliptic Fourier Descriptors (EFDs), and (4) Chain Code Subtraction (CCS). We used these descriptors for the classification test along with the random forest approach, and eight of the nine shape types were classified with combinations of MVs + CCS + EFDs. CCS is a descriptor that adds human knowledge to the chain codes, and it showed higher robustness in classification than the other descriptors. Our results suggest machine learning's high ability to classify fruit shapes accurately. We will attempt to increase the classification accuracy and apply the machine learning methods to other plant species. ","",""
61,"A. Tsoukalas, T. Albertson, I. Tagkopoulos","From Data to Optimal Decision Making: A Data-Driven, Probabilistic Machine Learning Approach to Decision Support for Patients With Sepsis",2015,"","","","",197,"2022-07-13 09:25:27","","10.2196/medinform.3445","","",,,,,61,8.71,20,3,7,"Background A tantalizing question in medical informatics is how to construct knowledge from heterogeneous datasets, and as an extension, inform clinical decisions. The emergence of large-scale data integration in electronic health records (EHR) presents tremendous opportunities. However, our ability to efficiently extract informed decision support is limited due to the complexity of the clinical states and decision process, missing data and lack of analytical tools to advice based on statistical relationships. Objective Development and assessment of a data-driven method that infers the probability distribution of the current state of patients with sepsis, likely trajectories, optimal actions related to antibiotic administration, prediction of mortality and length-of-stay. Methods We present a data-driven, probabilistic framework for clinical decision support in sepsis-related cases. We first define states, actions, observations and rewards based on clinical practice, expert knowledge and data representations in an EHR dataset of 1492 patients. We then use Partially Observable Markov Decision Process (POMDP) model to derive the optimal policy based on individual patient trajectories and we evaluate the performance of the model-derived policies in a separate test set. Policy decisions were focused on the type of antibiotic combinations to administer. Multi-class and discriminative classifiers were used to predict mortality and length of stay. Results Data-derived antibiotic administration policies led to a favorable patient outcome in 49% of the cases, versus 37% when the alternative policies were followed (P=1.3e-13). Sensitivity analysis on the model parameters and missing data argue for a highly robust decision support tool that withstands parameter variation and data uncertainty. When the optimal policy was followed, 387 patients (25.9%) have 90% of their transitions to better states and 503 patients (33.7%) patients had 90% of their transitions to worse states (P=4.0e-06), while in the non-policy cases, these numbers are 192 (12.9%) and 764 (51.2%) patients (P=4.6e-117), respectively. Furthermore, the percentage of transitions within a trajectory that lead to a better or better/same state are significantly higher by following the policy than for non-policy cases (605 vs 344 patients, P=8.6e-25). Mortality was predicted with an AUC of 0.7 and 0.82 accuracy in the general case and similar performance was obtained for the inference of the length-of-stay (AUC of 0.69 to 0.73 with accuracies from 0.69 to 0.82). Conclusions A data-driven model was able to suggest favorable actions, predict mortality and length of stay with high accuracy. This work provides a solid basis for a scalable probabilistic clinical decision support framework for sepsis treatment that can be expanded to other clinically relevant states and actions, as well as a data-driven model that can be adopted in other clinical areas with sufficient training data.","",""
66,"S. Deeb, S. Tyanova, M. Hummel, M. Schmidt-Supprian, Jüergen Cox, M. Mann","Machine Learning-based Classification of Diffuse Large B-cell Lymphoma Patients by Their Protein Expression Profiles",2015,"","","","",198,"2022-07-13 09:25:27","","10.1074/mcp.M115.050245","","",,,,,66,9.43,11,6,7,"Characterization of tumors at the molecular level has improved our knowledge of cancer causation and progression. Proteomic analysis of their signaling pathways promises to enhance our understanding of cancer aberrations at the functional level, but this requires accurate and robust tools. Here, we develop a state of the art quantitative mass spectrometric pipeline to characterize formalin-fixed paraffin-embedded tissues of patients with closely related subtypes of diffuse large B-cell lymphoma. We combined a super-SILAC approach with label-free quantification (hybrid LFQ) to address situations where the protein is absent in the super-SILAC standard but present in the patient samples. Shotgun proteomic analysis on a quadrupole Orbitrap quantified almost 9,000 tumor proteins in 20 patients. The quantitative accuracy of our approach allowed the segregation of diffuse large B-cell lymphoma patients according to their cell of origin using both their global protein expression patterns and the 55-protein signature obtained previously from patient-derived cell lines (Deeb, S. J., D'Souza, R. C., Cox, J., Schmidt-Supprian, M., and Mann, M. (2012) Mol. Cell. Proteomics 11, 77–89). Expression levels of individual segregation-driving proteins as well as categories such as extracellular matrix proteins behaved consistently with known trends between the subtypes. We used machine learning (support vector machines) to extract candidate proteins with the highest segregating power. A panel of four proteins (PALD1, MME, TNFAIP8, and TBC1D4) is predicted to classify patients with low error rates. Highly ranked proteins from the support vector analysis revealed differential expression of core signaling molecules between the subtypes, elucidating aspects of their pathobiology.","",""
14,"G. Rehm, Jinyoung Han, B. Kuhn, J. Delplanque, N. Anderson, Jason Y. Adams, C. Chuah","Creation of a Robust and Generalizable Machine Learning Classifier for Patient Ventilator Asynchrony.",2018,"","","","",199,"2022-07-13 09:25:27","","10.3414/ME17-02-0012","","",,,,,14,3.50,2,7,4,"BACKGROUND As healthcare increasingly digitizes, streaming waveform data is being made available from an variety of sources, but there still remains a paucity of performant clinical decision support systems. For example, in the intensive care unit (ICU) existing automated alarm systems typically rely on simple thresholding that result in frequent false positives. Recurrent false positive alerts create distrust of alarm mechanisms that can be directly detrimental to patient health. To improve patient care in the ICU, we need alert systems that are both pervasive, and accurate so as to be informative and trusted by providers.   OBJECTIVE We aimed to develop a machine learning-based classifier to detect abnormal waveform events using the use case of mechanical ventilation waveform analysis, and the detection of harmful forms of ventilation delivery to patients. We specifically focused on detecting injurious subtypes of patient-ventilator asynchrony (PVA).   METHODS Using a dataset of breaths recorded from 35 different patients, we used machine learning to create computational models to automatically detect, and classify two types of injurious PVA, double trigger asynchrony (DTA), breath stacking asynchrony (BSA). We examined the use of synthetic minority over-sampling technique (SMOTE) to overcome class imbalance problems, varied methods for feature selection, and use of ensemble methods to optimize the performance of our model.   RESULTS We created an ensemble classifier that is able to accurately detect DTA at a sensitivity/specificity of 0.960/0.975, BSA at sensitivity/specificity of 0.944/0.987, and non-PVA events at sensitivity/specificity of .967/.980.   CONCLUSIONS Our results suggest that it is possible to create a high-performing machine learning-based model for detecting PVA in mechanical ventilator waveform data in spite of both intra-patient, and inter-patient variability in waveform patterns, and the presence of clinical artifacts like cough and suction procedures. Our work highlights the importance of addressing class imbalance in clinical data sets, and the combined use of statistical methods and expert knowledge in feature selection.","",""
16,"F. Faghri, Sayed Hadi Hashemi, H. Leonard, S. Scholz, R. Campbell, M. Nalls, A. Singleton","Predicting onset, progression, and clinical subtypes of Parkinson disease using machine learning",2018,"","","","",200,"2022-07-13 09:25:27","","10.1101/338913","","",,,,,16,4.00,2,7,4,"Background The clinical manifestations of Parkinson disease are characterized by heterogeneity in age at onset, disease duration, rate of progression, and constellation of motor versus nonmotor features. Due to these variable presentations, counseling of patients about their individual risks and prognosis is limited. There is an unmet need for predictive tests that facilitate early detection and characterization of distinct disease subtypes as well as improved, individualized predictions of the disease course. The emergence of machine learning to detect hidden patterns in complex, multi-dimensional datasets provides unparalleled opportunities to address this critical need. Methods and Findings We used unsupervised and supervised machine learning approaches for subtype identification and prediction. We used machine learning methods on comprehensive, longitudinal clinical data from the Parkinson Disease Progression Marker Initiative (PPMI) (n=328 cases) to identify patient subtypes and to predict disease progression. The resulting models were validated in an independent, clinically well-characterized cohort from the Parkinson Disease Biomarker Program (PDBP) (n=112 cases). Our analysis distinguished three distinct disease subtypes with highly predictable progression rates, corresponding to slow, moderate and fast disease progressors. We achieved highly accurate projections of disease progression four years after initial diagnosis with an average Area Under the Curve of 0.93 (95% CI: 0.96 ± 0.01 for PDvec1, 0.87 ± 0.03 for PDvec2, and 0.96 ± 0.02 for PDvec3). We have demonstrated robust replication of these findings in the independent validation cohort. Conclusions These data-driven results enable clinicians to deconstruct the heterogeneity within their patient cohorts. This knowledge could have immediate implications for clinical trials by improving the detection of significant clinical outcomes that might have been masked by cohort heterogeneity. We anticipate that machine learning models will improve patient counseling, clinical trial design, allocation of healthcare resources and ultimately individualized clinical care.","",""
