Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
19,"Thomas G. Dietterich","Robust artificial intelligence and robust human organizations",2018,"","","","",1,"2022-07-13 09:34:35","","10.1007/s11704-018-8900-4","","",,,,,19,4.75,19,1,4,"","",""
7,"Vinicius M. Alves, S. Auerbach, N. Kleinstreuer, J. Rooney, E. Muratov, I. Rusyn, A. Tropsha, Charles Schmitt","Curated Data In — Trustworthy In Silico Models Out: The Impact of Data Quality on the Reliability of Artificial Intelligence Models as Alternatives to Animal Testing",2021,"","","","",2,"2022-07-13 09:34:35","","10.1177/02611929211029635","","",,,,,7,7.00,1,8,1,"New Approach Methodologies (NAMs) that employ artificial intelligence (AI) for predicting adverse effects of chemicals have generated optimistic expectations as alternatives to animal testing. However, the major underappreciated challenge in developing robust and predictive AI models is the impact of the quality of the input data on the model accuracy. Indeed, poor data reproducibility and quality have been frequently cited as factors contributing to the crisis in biomedical research, as well as similar shortcomings in the fields of toxicology and chemistry. In this article, we review the most recent efforts to improve confidence in the robustness of toxicological data and investigate the impact that data curation has on the confidence in model predictions. We also present two case studies demonstrating the effect of data curation on the performance of AI models for predicting skin sensitisation and skin irritation. We show that, whereas models generated with uncurated data had a 7–24% higher correct classification rate (CCR), the perceived performance was, in fact, inflated owing to the high number of duplicates in the training set. We assert that data curation is a critical step in building computational models, to help ensure that reliable predictions of chemical toxicity are achieved through use of the models.","",""
5,"Soheyl Khalilpourazari, S. Khalilpourazary, A. O. Çiftçioğlu, G. Weber","Designing energy-efficient high-precision multi-pass turning processes via robust optimization and artificial intelligence",2020,"","","","",3,"2022-07-13 09:34:35","","10.1007/s10845-020-01648-0","","",,,,,5,2.50,1,4,2,"","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",4,"2022-07-13 09:34:35","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
9,"B. N. Manjunatha Reddy, S. K. Pramada, T. Roshni","Monthly surface runoff prediction using artificial intelligence: A study from a tropical climate river basin",2021,"","","","",5,"2022-07-13 09:34:35","","10.1007/s12040-020-01508-8","","",,,,,9,9.00,3,3,1,"","",""
4,"Jianghua Wu, Changling Liu, Xiaoqing Liu, Wei Sun, Linfeng Li, Nannan Gao, Yajun Zhang, Xin Yang, Junjie Zhang, Hai-Yue Wang, Xinying Liu, Xiaozheng Huang, Yanhui Zhang, Runfen Cheng, K. Chi, Luning Mao, Lixin Zhou, D. Lin, S. Ling","Artificial intelligence-assisted system for precision diagnosis of PD-L1 expression in non-small cell lung cancer",2021,"","","","",6,"2022-07-13 09:34:35","","10.1038/s41379-021-00904-9","","",,,,,4,4.00,0,19,1,"","",""
27,"Omar Alshorman, Muhammad Irfan, N. Saad, D. Zhen, Noman Haider, A. Głowacz, Ahmad M. Alshorman","A Review of Artificial Intelligence Methods for Condition Monitoring and Fault Diagnosis of Rolling Element Bearings for Induction Motor",2020,"","","","",7,"2022-07-13 09:34:35","","10.1155/2020/8843759","","",,,,,27,13.50,4,7,2,"The fault detection and diagnosis (FDD) along with condition monitoring (CM) and of rotating machinery (RM) have critical importance for early diagnosis to prevent severe damage of infrastructure in industrial environments. Importantly, valuable industrial equipment needs continuous monitoring to enhance the safety, reliability, and availability and to decrease the cost of maintenance of modern industrial systems and applications. However, induction motor (IM) has been extensively used in several industrial processes because it is cheap, reliable, and robust. Rolling bearings are considered to be the main component of IM. Undoubtedly, any failure of this basic component can lead to a serious breakdown of IM and for whole industrial system. Thus, many current methods based on different techniques are employed as a fault prognosis and diagnosis of rolling elements bearing of IM. Moreover, these techniques include signal/image processing, intelligent diagnostics, data fusion, data mining, and expert systems for time and frequency as well as time-frequency domains. Artificial intelligence (AI) techniques have proven their significance in every field of digital technology. Industrial machines, automation, and processes are the net frontiers of AI adaptation. There are quite developed literatures that have been approaching the issues using signals and data processing techniques. However, the key contribution of this work is to present an extensive review of CM and FDD of the IM, especially for rolling elements bearings, based on artificial intelligent (AI) methods. This study highlights the advantages and performance limitations of each method. Finally, challenges and future trends are also highlighted.","",""
2,"Mohammed Falah Allawi, I. R. Hussain, Majid Ibrahim Salman, A. El-Shafie","Monthly inflow forecasting utilizing advanced artificial intelligence methods: a case study of Haditha Dam in Iraq",2021,"","","","",8,"2022-07-13 09:34:35","","10.1007/s00477-021-02052-7","","",,,,,2,2.00,1,4,1,"","",""
0,"Monika Singh","Integrating Artificial Intelligence and 5G in the Era of Next-Generation Computing",2021,"","","","",9,"2022-07-13 09:34:35","","10.1109/ICCMST54943.2021.00017","","",,,,,0,0.00,0,1,1,"The introduction of the 4G/LTE (Long Term Evolution) mobile network has addressed the main obstacle of higher capacity, allowing for the construction of true broadband mobile Internet. This was mainly made possible by a flexible network design and robust physical layer. However, services such as augmented reality (AR), virtual reality (VR), and others that require more bandwidth have been developed in new ways. Furthermore, new services like Internet-of-Vehicles and vehicle communications are putting a substantial demand on mobile networks for enhanced reliability and near-zero-latency performance (IoV). 5G has overcome some of these obstacles by employing a new radio interface based on massive multiple input multiple output (MIMO). However, network operators must consider an advanced level of intelligence in their networks to learn the operational environment and users' behaviours and needs in depth. In order to establish a proactive and efficient self-updatable network, it is also essential to foresee their evolution. We explore the importance of artificial intelligence and machine learning in 5G in order to develop a cost-effective and adaptive next-generation mobile network in this study. In addition, a comparison between the 5G and 4G networks has been done to showcase the capabilities of 5G network over 4G. Further, some practical use cases of AI/ML are also discussed. Although the countries such as China and US have already started working with some of the applications as discussed but in India, 5G technology will take some time to spread its wings in all real-time applications as discussed. This paper can help researchers to start integrating AI and Machine learning with 5G technology.","",""
1,"Natalia F. Espinoza-Sepulveda, J. Sinha","Robust vibration-based faults diagnosis machine learning model for rotating machines to enhance plant reliability",2021,"","","","",10,"2022-07-13 09:34:35","","10.21595/mrcm.2021.22110","","",,,,,1,1.00,1,2,1,"Plant availability and reliability can be improved through a robust condition monitoring and fault diagnosis model to predict the current status (healthy or faulty) of any machines and critical assets. The model can then predict the exact fault for the faulty asset so that remedial maintenance can be carried out in a planned plant outage. Nowadays, the artificial intelligence (AI)-based machine learning (ML) model seems to be current trend to meet these requirements. Hence, the paper is also proposing such vibration-based faults diagnosis ML model through an experimental rotating rig. Here, the 2-Steps approach is used with the ML model to easy the industrial operation and maintenance process. The Step-1 provides the information about the asset health status such as healthy or faulty. The Step-2 then identifies the exact nature of fault to aid the decision making for the fault rectification and maintenance activities to avoid the risk of failure and enhance the reliability.","",""
2,"Maurice Weber, A. Anand, Alba Cervera-Lierta, Jakob S. Kottmann, T. Kyaw, Bo Li, A. Aspuru‐Guzik, Ce Zhang, Zhikuan Zhao","Toward Reliability in the NISQ Era: Robust Interval Guarantee for Quantum Measurements on Approximate States",2021,"","","","",11,"2022-07-13 09:34:35","","","","",,,,,2,2.00,0,9,1,"Maurice Weber,1 Abhinav Anand,2 Alba Cervera-Lierta,2, 3 Jakob S. Kottmann,2, 3 Thi Ha Kyaw,2, 3 Bo Li,4 Alán Aspuru-Guzik,2, 3, 5, 6, ∗ Ce Zhang,1, † and Zhikuan Zhao1, ‡ Department of Computer Science, ETH Zürich, Universitätstrasse 6, 8092 Zürich, Switzerland Chemical Physics Theory Group, Department of Chemistry, University of Toronto, Canada. Department of Computer Science, University of Toronto, Canada. Department of Computer Science, University of Illinois, Urbana, Illinois 61801, USA Vector Institute for Artificial Intelligence, Toronto, Canada. Canadian Institute for Advanced Research (CIFAR) Lebovic Fellow, Toronto, Canada","",""
4,"I. M. Salte, A. Oestvik, E. Smistad, D. Melichova, T. M. Nguyen, H. Brunvand, T. Edvardsen, L. Loevstakken, B. Grenne","545 Deep learning/artificial intelligence for automatic measurement of global longitudinal strain by echocardiography",2020,"","","","",12,"2022-07-13 09:34:35","","10.1093/ehjci/jez319.279","","",,,,,4,2.00,0,9,2,"      The Norwegian Health Association, South-Eastern Norway regional health Authority and the national program for clinical therapy research (KLINBEFORSK).        Global longitudinal strain (GLS) by echocardiography has incremental prognostic value in patients with acute myocardial infarction and heart failure compared to left ventricular (LV) ejection fraction and provides more reproducible measurements of LV function. Recent advances in machine learning for image analysis now open the possibility for robust fully automated tracing of the LV and measurement of global longitudinal strain (GLS), without any operator input. This could make real-time GLS possible and remove inter-reader variability, thus resulting in saved time and improved test-retest reliability. The aim of the present study was to investigate how measurements by this novel automatic method compares to conventional speckle tracking analyses of GLS.        100 transthoracic echocardiographic examinations were included from a clinical database of patients with acute myocardial infarction or de-novo heart failure. Examinations were included consecutively and regardless of image quality. Simpson biplane LV ejection fraction ranged from 7 to 70%. Images of three standard apical planes from each examination were analysed using our novel and fully automated GLS method based on deep learning technology. The automated GLS measurements were compared to conventional speckle tracking GLS measurements of the same acquisitions using vendor specific format and software (EchoPAC, GE Healthcare), performed by a single experienced observer.        GLS was -11.6 ± 4.5% and -12.8 ± 5.0% for the deep learning method and the conventional method, respectively. Bland-Altman analysis showed a bias of -0.7 ± 1,9% and 95% limits of agreement of -4,6 to 3.1. No clear value dependent bias was found by visual inspection (Figure A). Feasibility for measurement of GLS was 93% for the deep learning based method and 99% for the conventional method. The limits of agreement found in our study is comparable to findings in the intervendor comparison study by the EASCVI/ASE/Industry Task force to standardize deformation imaging.        This novel deep learning based method succeeds without any operator input to automatically identify and classify the three apical standard views, trace the myocardium, perform motion estimation and measure global longitudinal strain. This could further facilitate the clinical use of GLS as an important tool for enhancing clinical decision-making.  Abstract 545 Figure. ","",""
2,"S. Atashrouz, M. Rahmani, Zahra Balzadeh, B. Nasernejad","Mathematical modeling of ethylene polymerization over advanced multisite catalysts: an artificial intelligence approach",2020,"","","","",13,"2022-07-13 09:34:35","","10.1007/s42452-020-2096-6","","",,,,,2,1.00,1,4,2,"","",""
6,"S. Jaekel, Bastian Scholz","Utilizing Artificial Intelligence to achieve a robust architecture for future robotic spacecraft",2015,"","","","",14,"2022-07-13 09:34:35","","10.1109/AERO.2015.7119180","","",,,,,6,0.86,3,2,7,"This paper presents a novel failure-tolerant architecture for future robotic spacecraft. It is based on the Time and Space Partitioning (TSP) principle as well as a combination of Artificial Intelligence (AI) and traditional concepts for system failure detection, isolation and recovery (FDIR). Contrary to classic payload that is separated from the platform, robotic devices attached onto a satellite become an integral part of the spacecraft itself. Hence, the robot needs to be integrated into the overall satellite FDIR concept in order to prevent fatal damage upon hardware or software failure. In addition, complex dexterous manipulators as required for onorbit servicing (OOS) tasks may reach unexpected failure states, where classic FDIR methods reach the edge of their capabilities with respect to successfully detecting and resolving them. Combining, and partly replacing traditional methods with flexible AI approaches aims to yield a control environment that features increased robustness, safety and reliability for space robots. The developed architecture is based on a modular on-board operational framework that features deterministic partition scheduling, an OS abstraction layer and a middleware for standardized inter-component and external communication. The supervisor (SUV) concept is utilized for exception and health management as well as deterministic system control and error management. In addition, a Kohonen self-organizing map (SOM) approach was implemented yielding a real-time robot sensor confidence analysis and failure detection. The SOM features nonsupervized training given a typical set of defined world states. By compiling a set of reviewable three-dimensional maps, alternative strategies in case of a failure can be found, increasing operational robustness. As demonstrator, a satellite simulator was set up featuring a client satellite that is to be captured by a servicing satellite with a 7-DoF dexterous manipulator. The avionics and robot control were integrated on an embedded, space-qualified Airbus e.Cube on-board computer. The experiments showed that the integration of SOM for robot failure detection positively complemented the capabilities of traditional FDIR methods.","",""
1,"A. Palin, I. Jacob","Review on Fog Based Spectrum Sensing for Artificial Intelligence",2018,"","","","",15,"2022-07-13 09:34:35","","10.32628/CSEIT183816","","",,,,,1,0.25,1,2,4,"Wireless Mesh Network (MWN) could be divided into proactive routing, reactive routing and hybrid routing, which must satisfy the requirements related to scalability, reliability, flexibility, throughput, load balancing, congestion control and efficiency. DMN (Directional Mesh Network) become more adaptive to the local environments and robust to spectrum changes. The existing computing units in the mesh network systems are Fog nodes, the DMN architecture is more economic and efficient since it doesn’t require architecture- level changes from existing systems. The cluster head (CH) manages a group of nodes such that the network has the hierarchical structure for the channel access, routing and bandwidth allocation. The feature extraction and situational awareness is conducted, each Fog node sends the information regarding the current situation to the cluster head in the contextual format. A Markov logic network (MLN) based reasoning engine is utilized for the final routing table updating regarding the system uncertainty and complexity.","",""
1,"V. Indragandhi, A. L","Artificial Intelligence Based Speed Control of SRM for Hybrid Electric Vehicles",2018,"","","","",16,"2022-07-13 09:34:35","","10.1109/ICPESYS.2018.8626982","","",,,,,1,0.25,1,2,4,"A Switched Reluctance Motor (SRM) is a robust electrical motor which have features that qualifies to be used in electric vehicle and aerospace application due to its simple construction. SRM drive has acquired an expanding enthusiasm in hybrid electric vehicle applications because of its high efficiency, reliability, robust structure and reduced rotor losses. However, current and torque ripples are major drawbacks in this type of motor for electric car application. In order to reduce the ripple contents for controlling the speed of the SRM drive, various control techniques are designed using simulation software. The target of this work is to look at the operation of Fuzzy logic, Adaptive-Neuro Fuzzy Inference Strategy (ANFIS) and Direct Torque Controller (DTC) to highlight performance of effective controller. The result of applying DTC technique to a SRM drive gives the high robustness and better performance than a conventional fuzzy logic and ANFIS design technique. Simulation work is carried out using Simulink environment.","",""
45,"M. Ahmadi","Developing a Robust Surrogate Model of Chemical Flooding Based on the Artificial Neural Network for Enhanced Oil Recovery Implications",2015,"","","","",17,"2022-07-13 09:34:35","","10.1155/2015/706897","","",,,,,45,6.43,45,1,7,"Application of chemical flooding in petroleum reservoirs turns into hot topic of the recent researches. Development strategies of the aforementioned technique are more robust and precise when we consider both economical points of view (net present value, NPV) and technical points of view (recovery factor, RF). In current study many attempts have been made to propose predictive model for estimation of efficiency of chemical flooding in oil reservoirs. To gain this end, a couple of swarm intelligence and artificial neural network (ANN) is employed. Also, lucrative and high precise chemical flooding data banks reported in previous attentions are utilized to test and validate proposed intelligent model. According to the mean square error (MSE), correlation coefficient, and average absolute relative deviation, the suggested swarm approach has acceptable reliability, integrity and robustness. Thus, the proposed intelligent model can be considered as an alternative model to predict the efficiency of chemical flooding in oil reservoir when the required experimental data are not available or accessible.","",""
8,"B. Dumnic, B. Popadic, D. Milicevic, V. Katić, D. Oros","Artificial Intelligence Based Vector Control of Induction Generator Without Speed Sensor for Use in Wind Energy Conversion System",2015,"","","","",18,"2022-07-13 09:34:35","","10.1234/IJRER.V5I1.2030.G6498","","",,,,,8,1.14,2,5,7,"Indirect field oriented control (IFOC) of squirrel-cage induction generator (SCIG) with full capacity power converter used in wind energy conversion system (WECS) is presented in this paper. In order to improve WECS reliability robust IFOC algorithm using artificial intelligence (AI) for speed estimation was developed. Estimated speed is used for realization of maximum power tracking algorithm (MPPT). Practical testing and validation of considered estimation techniques is performed using advance laboratory prototype of WECS. Extensive experimentation is conducted in order to verify efficiency and reliability of proposed speed-sensorless control technique under realistic WECS operating conditions.","",""
0,"","Estimation of project completion time using a model of combined artificial intelligence methods",2015,"","","","",19,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,0,7,"Estimate at Completion (EAC) for forecasting time of projects is considered as one of the important issues in project management. One of the main features of different projects is their dynamic procedure. This article provides a method based on ensemble of learning machines to estimate the time of projects. It also considers the main essential EVM features during the project progress. In this model, each learning machine conducts the project procedure dynamically. Then, an Ensemble is used to estimate the time of project based on knearest neighbor (KNN) algorithm. These learning machines are interacted with each other in the form of Selection policy in final results estimation. The main features of this method include higher reliability, generality, and noise robust features in comparison with individual models. Controlled system reliability, using stronger learning machines in Ensemble, and also the capability of this model to manage available weak estimator in Ensemble are another major features of it. Finally, a software code providing the connectivity to MSP has been created.","",""
6,"David Martín-Gutiérrez, Gustavo Hernández-Peñaloza, Alberto Belmonte Hernández, Alicia Lozano-Diez, Federico Álvarez","A Deep Learning Approach for Robust Detection of Bots in Twitter Using Transformers",2021,"","","","",20,"2022-07-13 09:34:35","","10.1109/ACCESS.2021.3068659","","",,,,,6,6.00,1,5,1,"During the last decades, the volume of multimedia content posted in social networks has grown exponentially and such information is immediately propagated and consumed by a significant number of users. In this scenario, the disruption of fake news providers and bot accounts for spreading propaganda information as well as sensitive content throughout the network has fostered applied research to automatically measure the reliability of social networks accounts via Artificial Intelligence (AI). In this paper, we present a multilingual approach for addressing the bot identification task in Twitter via Deep learning (DL) approaches to support end-users when checking the credibility of a certain Twitter account. To do so, several experiments were conducted using state-of-the-art Multilingual Language Models to generate an encoding of the text-based features of the user account that are later on concatenated with the rest of the metadata to build a potential input vector on top of a Dense Network denoted as Bot-DenseNet. Consequently, this paper assesses the language constraint from previous studies where the encoding of the user account only considered either the metadata information or the metadata information together with some basic semantic text features. Moreover, the Bot-DenseNet produces a low-dimensional representation of the user account which can be used for any application within the Information Retrieval (IR) framework.","",""
11,"S. Tripathi, David Muhr, Manuel Brunner, F. Emmert‐Streib, H. Jodlbauer, M. Dehmer","Ensuring the Robustness and Reliability of Data-Driven Knowledge Discovery Models in Production and Manufacturing",2020,"","","","",21,"2022-07-13 09:34:35","","10.3389/frai.2021.576892","","",,,,,11,5.50,2,6,2,"The Cross-Industry Standard Process for Data Mining (CRISP-DM) is a widely accepted framework in production and manufacturing. This data-driven knowledge discovery framework provides an orderly partition of the often complex data mining processes to ensure a practical implementation of data analytics and machine learning models. However, the practical application of robust industry-specific data-driven knowledge discovery models faces multiple data- and model development-related issues. These issues need to be carefully addressed by allowing a flexible, customized and industry-specific knowledge discovery framework. For this reason, extensions of CRISP-DM are needed. In this paper, we provide a detailed review of CRISP-DM and summarize extensions of this model into a novel framework we call Generalized Cross-Industry Standard Process for Data Science (GCRISP-DS). This framework is designed to allow dynamic interactions between different phases to adequately address data- and model-related issues for achieving robustness. Furthermore, it emphasizes also the need for a detailed business understanding and the interdependencies with the developed models and data quality for fulfilling higher business objectives. Overall, such a customizable GCRISP-DS framework provides an enhancement for model improvements and reusability by minimizing robustness-issues.","",""
1,"Sehar Saleem, Rehan Ahmad Khan Sherwani, Muhammad Amin, Maryam Khalid, N. Ali","Development of New Robust Optimal Score Function for the Weibull Distributed Error Term in Multilevel Models",2021,"","","","",22,"2022-07-13 09:34:35","","10.1155/2021/1953546","","",,,,,1,1.00,0,5,1,"A popular robust estimation technique for linear models is the rank-based method as an alternative to the ordinary least square (OLS) and restricted maximum likelihood (REML) in the presence of extreme observations. This method is applied in machine reliability analysis and quantum engineering, especially in artificial intelligence and optimization problems where outliers are commonly observed. This technique is also extended for the multilevel model, where the shape of error distribution contributes a significant role in more efficient estimation. In this study, we proposed the Weibull score function for the Weibull distributed error terms in the multilevel model. The efficiency of the proposed score function is compared with the existing Wilcoxon score function and the traditional method REML via Monte Carlo simulations after adding simulated extreme observations. For small values of shape parameter in Weibull distribution of error term showing the presence of outliers, the Weibull score function was found to be efficient as compared to the Wilcoxon and REML methods. However, for a large value of shape parameter, Wilcoxon score appeared either equally efficient than the Weibull score function. REML is observed least precise in all situations. These findings are verified through a real application on test scores data, with a small value of shape parameter, and the Weibull score function turned out the most efficient.","",""
4,"Ali Barzkar, M. Najafzadeh, F. Homaei","Evaluation of drought events in various climatic conditions using data-driven models and a reliability-based probabilistic model",2021,"","","","",23,"2022-07-13 09:34:35","","10.1007/s11069-021-05019-7","","",,,,,4,4.00,1,3,1,"","",""
19,"G. Zhong, Mengfei Zi, Chuanlai Ren, Q. Xiao, Mingkai Tang, Liyu Wei, F. An, S. Xie, Jinbin Wang, X. Zhong, Mingqiang Huang, Jiangyu Li","Flexible electronic synapse enabled by ferroelectric field effect transistor for robust neuromorphic computing",2020,"","","","",24,"2022-07-13 09:34:35","","10.1063/5.0013638","","",,,,,19,9.50,2,12,2,"Neuromorphic computing has the potential to accelerate high performance parallel and low power in-memory computation, artificial intelligence, and adaptive learning. Despite emulating the basic functions of biological synapses well, the existing artificial electronic synaptic devices have yet to match the softness, robustness, and ultralow power consumption of the brain. Here, we demonstrate an all-inorganic flexible artificial synapse enabled by a ferroelectric field effect transistor based on mica. The device not only exhibits excellent electrical pulse modulated conductance updating for synaptic functions but also shows remarkable mechanical flexibility and high temperature reliability, making robust neuromorphic computation possible under external disturbances such as stress and heating. Based on its linear, repeatable, and stable long-term plasticity, we simulate an artificial neural network for the Modified National Institute of Standards and Technology handwritten digit recognition with an accuracy of 94.4%. This work provides a promising way to enable flexible, low-power, robust, and highly efficient neuromorphic computation that mimics the brain.","",""
0,"A. Ruospo, D. Piumatti, A. Floridia, Ernesto Sánchez","A Suitability Analysis of Software Based Testing Strategies for the On-line Testing of Artificial Neural Networks Applications in Embedded Devices",2021,"","","","",25,"2022-07-13 09:34:35","","10.1109/IOLTS52814.2021.9486704","","",,,,,0,0.00,0,4,1,"Electronic devices based on artificial intelligence solutions are pervading our everyday life. Nowadays, human decision processes are supported by real-time data gathered from intelligent systems. Artificial Neural Networks (ANNs) are one of the most used deep learning predictive models due to their outstanding computational capabilities. However, assessing their reliability is still an open issue faced by both the academic and industrial worlds, especially when ANNs are deployed on safety-critical systems, such as self-driving cars in the automotive world. In these systems, a strategy for identifying hardware faults is required by industry standards (e.g., ISO26262 for automotive, and DO254 for avionics). Among the existing in-field test strategies, the periodic scheduling of on-line Software Test Library (STL) is a wide strategy adopted; STL allows to reach an acceptable fault coverage without the need for additional hardware. However, when dealing with ANN-based applications, the execution of on-line tests interleaving the ANN inferences may jeopardise the strive for performance maximization. The paper presents a comprehensive analysis of six possible scenarios concerning the execution of on-line self-test programs in embedded devices running ANN-based applications. In the proposed scenarios, the impact of the STL execution on the ANN performance is analyzed; in particular, the execution times of an inference and the Fault Detection Time (FDT) of the STL are discussed and compared. Experimental analyses are provided by relying on: an open-source RISC-V platform running two different convolutional neural networks; a STL for RISC-V cores with a maximum achievable fault coverage of 90%.","",""
1,"Naznin Akter, Masudur R. Siddiquee, M. Shur, N. Pala","AI-Powered Terahertz VLSI Testing Technology for Ensuring Hardware Security and Reliability",2021,"","","","",26,"2022-07-13 09:34:35","","10.1109/ACCESS.2021.3075429","","",,,,,1,1.00,0,4,1,"Hardware cybersecurity has become a key issue, especially for very large integrated circuits. If counterfeit, forged, or defective ICs present a significant threat to system reliability and security. The growing complexity of digital and mixed-signal systems makes it increasingly challenging yet vital to develop robust methods to assess and confirm the reliability and authenticity of ICs. We introduce a new terahertz testing method for non-destructive and unobtrusive identification of counterfeit, damaged, forged or defective ICs by measuring their response to incident terahertz and sub-terahertz radiation at the circuit pins and analyzing the response using artificial intelligence (AI). These responses create unique signatures for ICs. We generated 2D images by measuring the response on a selected pin of a radio frequency IC (RFIC) scanned by a focused terahertz radiation. By applying the data augmentation processes, we created a secure image data set to train the convolutional neural network (CNN) model. An unsecured image data set representing altered or damaged ICs was generated by modifying the original image data. The trained models identified secure devices with a ~94% accuracy.","",""
8,"Jae-Y. Lee, Wonpil Yu","Robust self-localization of ground vehicles using artificial landmark",2014,"","","","",27,"2022-07-13 09:34:35","","10.1109/URAI.2014.7057439","","",,,,,8,1.00,4,2,8,"Stable and accuracy localization has crucial importance on the success of autonomous navigation of robotic ground vehicles. In this paper, we present a landmark-based sensor fusion localization method that utilizes artificial landmarks, a cheap GPS sensor, and wheel odometry. We describe overall architecture of the proposed localization system and technical details of landmark-based localization algorithm. The developed landmark-based localization system gives centimeters localization accuracy and real time performance more than 100 Hz. The experimental results on extensive real video sequences confirms the effectiveness and reliability of the method, showing successful detection of most landmarks with almost no false detection.","",""
1,"P. Lin, Y. Chou, Yung Ting, Shian-Shing Shyu, Chang-Kuo Chen","A robust system reliability analysis using partitioning and parallel processing of Markov chain",2014,"","","","",28,"2022-07-13 09:34:35","","10.1017/S0890060414000493","","",,,,,1,0.13,0,5,8,"Abstract This paper presents a robust reliability analysis method for systems of multimodular redundant (MMR) controllers using the method of partitioning and parallel processing of a Markov chain (PPMC). A Markov chain is formulated to represent the N distinct states of the MMR controllers. Such a Markov chain has N2 directed edges, and each edge corresponds to a transition probability between a pair of start and end states. Because N can be easily increased substantially, the system reliability analysis may require large computational resources, such as the central processing unit usage and memory occupation. By the PPMC, a Markov chain's transition probability matrix can be partitioned and reordered, such that the system reliability can be evaluated through only the diagonal submatrices of the transition probability matrix. In addition, calculations regarding the submatrices are independent of each other and thus can be conducted in parallel to assure the efficiency. The simulation results show that, compared with the sequential method applied to an intact Markov chain, the proposed PPMC can improve the performance and produce allowable accuracy for the reliability analysis on large-scale systems of MMR controllers.","",""
0,"J. Vahldiek, K. Bressem, S. Niehues, L. Adams, L. Spiller, M. Protopopov, V. Rios Rodriguez, B. Muche, J. Rademacher, H. Haibel, M. Torğutalp, F. Proft, D. Poddubnyy","OP0254 AN ARTIFICIAL NEURAL NETWORK FOR THE DETECTION OF DEFINITE RADIOGRAPHIC SACROILIITIS WITH HIGH SPECIFICITY IN THE DIAGNOSTIC SETTING",2021,"","","","",29,"2022-07-13 09:34:35","","10.1136/ANNRHEUMDIS-2021-EULAR.3512","","",,,,,0,0.00,0,13,1,"Radiographs of the sacroiliac joints are commonly used as the first imaging method for the diagnosis of axial spondyloarthritis (axSpA), but the reliability of sacroiliitis detection is usually low. Recently we developed a deep convolutional neural network (CNN) that can detect radiographic sacroiliitis with expert-like accuracy in patients with axSpA, i.e., classification as radiographic or non-radiographic [1]. There is frequent criticism that many artificial intelligence algorithms for diagnostic analysis of medical images lack robust validation in real-world clinical applications.The aim of this study was to evaluate the performance of our deep CNN in detecting definite radiographic sacroiliitis in the diagnostic setting.In this study, we included a total of 361 patients with chronic back pain who presented to a rheumatologist in a specialized SpA center with a suspicion of axSpA within the OptiRef project [2]. All patients received a structured rheumatologic diagnostic work-up that resulted in the final diagnosis of axial SpA/no axial SpA. Radiographs of sacroiliac joints were evaluated by a rheumatologist and radiologist according to the modified New York criteria; the consensus judgement of the presence of definite radiographic sacroiliitis (>=grade 2 bilaterally or >=grade 3 unilaterally) was used as a reference. The predictions of the deep CNN’s inference (with a balanced cutoff of 0.724 for the predictions of the model that was derived from the training and validation steps [1]) on all available pelvic radiographs was compared to this reference judgement.Pelvic radiographs of 340 patients (110 with axSpA including 61 patient with radiographic and 49 with non-radiographic axSpA, and 230 without SpA) were available for the CNN evaluation. The deep CNN achieved a sensitivity of 79% for the diagnosis of radiographic axSpA. The specificity of radiographic sacroiliitis detection was 94% (Table 1). The area under the receiver operating characteristics curve for the prediction of the presence of definite radiographic sacroiliitis was 88%. Figure 1 shows an exemplary class activation map of our CNN.Table 1.Convolutional neural network predictions of the presence of radiographic sacroiliitis in patients with suspected axSpA according to the final diagnosis by rheumatologist in OptiRef (N=340).Clinical diagnosisCNN’s prediction on the presence of definite radiographic sacroiliitisPresentNot presentRadiographic axial SpA48/61 (78.7%)13/61 (21.3%)Non-radiographic axial SpA4/49 (8.2%)45/49 (91.8%)No SpA (other diagnosis)14/230 (6.1%)216/230 (93.9%)The artificial neural network showed good generalizability and a high specificity with acceptable sensitivity in the detection of radiographic sacroiliitis when applied in the diagnostic setting of patients with chronic back pain and suspicion of axSpA. This algorithm can therefore be used to aid the detection of radiographic sacroiliitis as a part of the diagnostic approach.[1]Bressem KK, et al. medRxiv. 2020:2020.05.19.20105304.[2]Proft F, et al. Semin Arthritis Rheum. 2020;50:1015-1021.The OptiRef study was supported by a research grant from Novartis.None declared","",""
0,"Minah Lee, Xueyuan She, Biswadeep Chakraborty, Saurabh Dash, B. Mudassar, S. Mukhopadhyay","Reliable Edge Intelligence in Unreliable Environment",2021,"","","","",30,"2022-07-13 09:34:35","","10.23919/DATE51398.2021.9474097","","",,,,,0,0.00,0,6,1,"A key challenge for deployment of artificial intelligence (AI) in real-time safety-critical systems at the edge is to ensure reliable performance even in unreliable environments. This paper will present a broad perspective on how to design AI platforms to achieve this unique goal. First, we will present examples of AI architecture and algorithm that can assist in improving robustness against input perturbations. Next, we will discuss examples of how to make AI platforms robust against hardware induced noise and variation. Finally, we will discuss the concept of using lightweight networks as reliability estimators to generate early warning of potential task failures.","",""
0,"Qiao Qi, Xiaoming Chen","Robust Design of Federated Learning for Edge-Intelligent Networks",2022,"","","","",31,"2022-07-13 09:34:35","","10.48550/arXiv.2205.06955","","",,,,,0,0.00,0,2,1,"—Mass data trafﬁcs, low-latency wireless services and advanced artiﬁcial intelligence (AI) technologies have driven the emergence of a new paradigm for wireless networks, namely edge-intelligent networks, which are more efﬁcient and ﬂexible than traditional cloud-intelligent networks. Considering users’ privacy, model sharing-based federated learning (FL) that mi- grates model parameters but not private data from edge devices to a central cloud is particularly attractive for edge-intelligent networks. Due to multiple rounds of iterative updating of high-dimensional model parameters between base station (BS) and edge devices, the communication reliability is a critical issue of FL for edge-intelligent networks. We reveal the impacts of the errors generated during model broadcast and model aggregation via wireless channels caused by channel fading, interference and noise on the accuracy of FL, especially when there exists channel uncertainty. To alleviate the impacts, we propose a robust FL algorithm for edge-intelligent networks with channel uncertainty, which is formulated as a worst-case optimization problem with joint device selection and transceiver design. Finally, simulation results validate the robustness and effectiveness of the proposed algorithm.","",""
0,"Antony Fan, Joddv Wang, Vladimir Aptekar","Advanced Circuit Verification for Robust Design",2020,"","","","",32,"2022-07-13 09:34:35","","10.1109/ICSICT49897.2020.9278330","","",,,,,0,0.00,0,3,2,"The growth in Artificial Intelligence, advanced 5G wireless network, cloud computing, safety-critical automotive electronics and mainstream adoption of FinFET technologies have resulted in significant increase in IC design robustness challenges. IC designers must contend with advanced circuit complexity with stringent requirements on performance, low power, design robustness for safety, and reliability such as electro-migration and device-aging. For safety-critical applications such as Autonomous Driving, ADAS, and Connected Car, IC designers are now looking to adopt rigorous and systematic methodologies to meet functional safety standard. In this paper, we will discuss the models used on circuit reliability verification, and application of these models to assess IC reliability and variability for targeted requirements with latest Synopsys' AMS solution for robust design.","",""
0,"Omar El-Kadi, A. El-Shazly, K. Nassar","Robust In-Plane Structures Oscillation Monitoring by Terrestrial Photogrammetry",2020,"","","","",33,"2022-07-13 09:34:35","","10.3390/s20082223","","",,,,,0,0.00,0,3,2,"Oscillation monitoring commonly requires complex setups integrating various types of sensors associated with intensive computations to achieve an adequate rate of observations and accuracy. This research presents a simple, cost-effective approach that allows two-dimensional oscillation monitoring by terrestrial photogrammetry using non-metric cameras. Tedious camera calibration procedures are eliminated by using a grid target that allows geometric correction to be performed to the frame’s region of interest at which oscillations are monitored. Region-based convolutional neural networks (Faster R-CNN) techniques are adopted to minimize the light exposure limitations, commonly constraining applications of terrestrial photogrammetry. The proposed monitoring procedure is tested at outdoor conditions to check its reliability and accuracy and examining the effect of using Faster R-CNN on monitoring results. The proposed artificial intelligence (AI) aided oscillation monitoring allowed sub-millimeter accuracy monitoring with observation rates up to 60 frames per second and gained the benefit of high optical zoom offered by market available bridge cameras to monitor oscillation of targets 100 m apart with high accuracy.","",""
0,"N. Rabhi, M. Guedri, N. Bouhaddi, R. Chaléat","The Coupling of Robust Metamodel and Heuristic Methods in Reliability Based Design Optimization",2012,"","","","",34,"2022-07-13 09:34:35","","10.4203/ccp.99.206","","",,,,,0,0.00,0,4,10,"The context of this paper is the establishment of help tools to aid in the decisions for robust design of mechanical structures in an uncertain environment. The problem considered is how to determine an optimized structure with regard to the security level criteria. This problem is called reliability optimization; that means the optimization is with regard to the constraint reliability. With a falling probability level, the engineer can decide if the structure has sufficient reliability. This paper presents a coupling between the methods of reliability approximation (FORM, SORM) and a set of modern techniques from the domain of the artificial intelligence which are characterized by their importance in the process of collection and analysis of the uncertainties. Among these techniques, the particular swarm, the genetic algorithms and the ant colony techniques should be mentioned. In this paper, a coupling of the reliability method and dynamics with metamodels (condensed models) to guide reliability optimization of such systems is proposed. Two numerical examples are presented to illustrate the performance of the proposed approach.","",""
24,"Omar Costilla-Reyes, R. Vera-Rodríguez, Patricia J. Scully, K. Ozanyan","Analysis of Spatio-Temporal Representations for Robust Footstep Recognition with Deep Residual Neural Networks",2019,"","","","",35,"2022-07-13 09:34:35","","10.1109/TPAMI.2018.2799847","","",,,,,24,8.00,6,4,3,"Human footsteps can provide a unique behavioural pattern for robust biometric systems. We propose spatio-temporal footstep representations from floor-only sensor data in advanced computational models for automatic biometric verification. Our models deliver an artificial intelligence capable of effectively differentiating the fine-grained variability of footsteps between legitimate users (clients) and impostor users of the biometric system. The methodology is validated in the largest to date footstep database, containing nearly 20,000 footstep signals from more than 120 users. The database is organized by considering a large cohort of impostors and a small set of clients to verify the reliability of biometric systems. We provide experimental results in 3 critical data-driven security scenarios, according to the amount of footstep data made available for model training: at airports security checkpoints (smallest training set), workspace environments (medium training set) and home environments (largest training set). We report state-of-the-art footstep recognition rates with an optimal equal false acceptance and false rejection rate (equal error rate) of 0.7 percent an improvement ratio of 371 percent compared to previous state-of-the-art. We perform a feature analysis of deep residual neural networks showing effective clustering of client's footstep data and to provide insights of the feature learning process.","",""
0,"Johannes Palmer, Aaron Schartner, A. Danilov, Vincent Tse","Concerted, Computing-Intense Novel MFL Approach Ensuring Reliability and Reducing the Need for Dig Verification",2020,"","","","",36,"2022-07-13 09:34:35","","10.1115/IPC2020-9361","","",,,,,0,0.00,0,4,2,"  Magnetic Flux Leakage (MFL) is a robust technology with high data coverage. Decades of continuous sizing improvement allowed for industry-accepted sizing reliability. The continuous optimization of sizing processes ensures accurate results in categorizing metal loss features. However, the identified selection of critical anomalies is not always optimal; sometimes anomalies are dug up too early or unnecessarily, this can be caused by the feature type in the field (true metal loss shape) being incorrectly identified which affects sizing and tolerance. In addition, there is the possibility for incorrectly identifying feature types causing false under-calls.  Today, complex empirical formulas together with multifaceted lookup tables fed by pull tests, synthetic data, dig verifications, machine learning, artificial intelligence and last but not least human expertise translate MFL signals into metal loss assessments with high levels of success. Nevertheless, two important principal elements are limiting the possible MFL sizing optimization. One is the empirical character of the signal interpretation. The other is the implicitly induced data and result simplification.  The reason to go this principal route for many years is simple: it is methodologically impossible to calculate the metal source geometry directly from the signals. In addition, the pure number of possible relevant geometries is so large that simplification is necessary and inevitable. Moreover, the second methodological reason is the ambiguity of the signal, which defines the target of metal loss sizing as the most probable solution. However, even under the best conditions, the most probable one is not necessarily the correct one.  This paper describes a novel, fundamentally different approach as a basic alternative to the common MFL-analysis approach described above. A calculation process is presented, which overcomes the empirical nature of traditional approaches by using a result optimization method that relies on intense computing and avoids any simplification. Additionally, the strategy to overcome MFL ambiguity will be shown. Together with the operator, detailed blind-test examples demonstrate the enormous level of detail, repeatability and accuracy of this groundbreaking technological method with the potential to reduce tool tolerance, increase sizing accuracy, increase growth rate accuracy, and help optimize the dig program to target critical features with greater confidence.","",""
0,"Tek Raj Chhetri, S. Srirama","Towards AI for cloud services reliability using combined metrics",2020,"","","","",37,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,2,2,"With the emergence of cloud computing and the Quality of Services (QoS), Compute Power, Performance, and Scalability it offers, the paradigm of computing has shifted towards the cloud. Due to attractiveness cloud offers, today, more and more businesses, research, and individuals are adopting cloud services. Even with the maturity of the cloud, reliability is still a concern. The reason being the constant occurrence of failure causes financial loss as well as a negative impact on its users as it directly affects QoS. Further, the scale and heterogeneity make it more prone to failure, highlighting the necessity for a robust solution to maintain the attractiveness and prevent financial loss. By predicting failure before it could happen, we can improve the reliability. Artificial Intelligence, now, has made significant progress, finding itself a place in all possible areas. In our study we present artificial intelligence with a combined metrics approach to improve the failure prediction. An experiment conducted with data recorded from more than 100 cloud servers shows significant improvement in failure prediction with high prediction accuracy, precision, and recall compared to state of the art studies.","",""
7,"G. Platt, Xin-She Yang, A. Neto","Computational Intelligence, Optimization and Inverse Problems with Applications in Engineering",2019,"","","","",38,"2022-07-13 09:34:35","","10.1007/978-3-319-96433-1","","",,,,,7,2.33,2,3,3,"","",""
0,"Jia Song, Kai Zhao, Huiyan Weng, Xu Wang, Yang Liu","Research on Reliable Path Planning of Manipulator Based on Artificial Beacons",2019,"","","","",39,"2022-07-13 09:34:35","","10.1109/DASC/PiCom/CBDCom/CyberSciTech.2019.00137","","",,,,,0,0.00,0,5,3,"In order to solve the problem that manipulators can only move in a fixed trajectory in common industrial production at present, we propose a monocular visual positioning method based on artificial beacons, combining visual position technology with the movement of manipulators to complete the reliable path planning of manipulators. When it comes to practicability, the traditional point pair-matching method is equipped with robust factors, to increase the reliability. We adopt a linear predict algorithm and ROI (region of interest) in the solving of PnP (Perspective-n-Point) to ensure the real-time performance and improve the robustness. Finally, the reliability is verified by experiments, which provides theoretical basis and practical reference for the path planning of manipulator based on artificial beacons.","",""
0,"M. Tariquzzaman, J. Y. Kim, S. Na","Robust audio-visual speaker identification using a modified score-based reliability in modality integration",2009,"","","","",40,"2022-07-13 09:34:35","","10.1145/1643823.1643896","","",,,,,0,0.00,0,3,13,"Identity recognition in real environment with reliable mode is a key issue in human computer interaction (HCI). In this paper, we present a robust speaker identification system considering score based optimal reliability measure of different modalities. We propose an extension of the modified convection function's optimizing parameter to account optimal reliability simultaneously via audio and lip information based reliability measure in bimodal speaker identification system for robust speaker identification. For degradation of visual signals, we have applied JPEG compression to test images. In addition, for creating mismatch in between enrollment and test session, acoustic Babble noises and artificial illumination have been added to test audio and visual signals, respectively. Local PCA has been used to both modalities for reducing the dimension of feature vector. We have applied a swarm intelligence algorithm i.e., particle swarm optimization for optimizing the modified convection function's optimizing parameters. The overall speaker identification experiments are performed using VidTimit DB. Experimental results show that our proposed optimal reliability measures have effectively enhanced the identification accuracy of 7.73% in comparison with the best classifier system in the integration system and maintains the modality reliability statistics in term of its performance thus verified the consistency of the proposed extension.","",""
90,"Eric Wu, Kevin Wu, R. Daneshjou, David Ouyang, Daniel E. Ho, James Zou","How medical AI devices are evaluated: limitations and recommendations from an analysis of FDA approvals.",2021,"","","","",41,"2022-07-13 09:34:35","","10.1038/s41591-021-01312-x","","",,,,,90,90.00,15,6,1,"","",""
2,"C. Parmar","Machine learning applications for Radiomics : towards robust non-invasive predictors in clinical oncology",2017,"","","","",42,"2022-07-13 09:34:35","","","","",,,,,2,0.40,2,1,5,"In this big-data era, like every other field, healthcare is also turning towards artificial intelligence (AI) and machine-learning (ML). In this thesis, state-of-the-art machine-learning methods were investigated for radiomic analyses. An unbiased evaluation of these advanced computational methods in terms of their accuracy and reliability is presented. Identification of optimal machine-learning methods for radiomic applications is a crucial step towards stable and clinically relevant radiomic biomarkers, providing a non-invasive way of quantifying and monitoring tumor-phenotypic characteristics in clinical practice. With ever increasing patient specific data, this work could stimulate further research towards brining AI and precision medicine in routine clinical oncology.","",""
29,"J. G. M. S. Decanini, Mauro Tonelli-Neto, C. R. Minussi","Robust fault diagnosis in power distribution systems based on fuzzy ARTMAP neural network-aided evidence theory",2012,"","","","",43,"2022-07-13 09:34:35","","10.1049/IET-GTD.2012.0028","","",,,,,29,2.90,10,3,10,"The present study proposes a methodology for the automatic diagnosis of short-circuit faults in distribution systems using modern techniques for signal analysis and artificial intelligence. This support tool for decision making accelerates the restoration process, providing greater security, reliability and profitability to utilities. The fault detection procedure is performed using statistical and direct analyses of the current waveforms in the wavelet domain. Current and voltage signal features are extracted using discrete wavelet transform, multi-resolution analysis and energy concept. These behavioural indices correspond to the input vectors of three parallel sets of fuzzy ARTMAP neural networks. The network outcomes are integrated by the Dempster-Shafer theory, giving quantitative information about the diagnosis and its reliability. Tests were carried out using a practical distribution feeder from a Brazilian electric utility, and the results show that the method is efficient with a high level of confidence.","",""
4,"S. A. Borz","Development of a Modality-Invariant Multi-Layer Perceptron to Predict Operational Events in Motor-Manual Willow Felling Operations",2021,"","","","",44,"2022-07-13 09:34:35","","10.3390/F12040406","","",,,,,4,4.00,4,1,1,"Motor-manual operations are commonly implemented in the traditional and short rotation forestry. Deep knowledge of their performance is needed for various strategic, tactical and operational decisions that rely on large amounts of data. To overcome the limitations of traditional analytical methods, Artificial Intelligence (AI) has been lately used to deal with various types of signals and problems to be solved. However, the reliability of AI models depends largely on the quality of the signals and on the sensing modalities used. Multimodal sensing was found to be suitable in developing AI models able to learn time and location-related data dependencies. For many reasons, such as the uncertainty of preserving the sensing location and the inter- and intra-variability of operational conditions and work behavior, the approach is particularly useful for monitoring motor-manual operations. The main aim of this study was to check if the use of acceleration data sensed at two locations on a brush cutter could provide a robust AI model characterized by invariance to data sensing location. As such, a Multi-Layer Perceptron (MLP) with backpropagation was developed and used to learn and classify operational events from bimodally-collected acceleration data. The data needed for training and testing was collected in the central part of Romania. Data collection modalities were treated by fusion in the training dataset, then four single-modality testing datasets were used to check the performance of the model on a binary classification problem. Fine tuning of the regularization parameters (α term) has led to acceptable testing and generalization errors of the model measured as the binary cross-entropy (log loss). Irrespective of the hyperparameters’ tunning strategy, the classification accuracy (CA) was found to be very high, in many cases approaching 100%. However, the best models were those characterized by α set at 0.0001 and 0.1, for which the CA in the test datasets ranged from 99.1% to 99.9% and from 99.5% to 99.9%, respectively. Hence, data fusion in the training set was found to be a good strategy to build a robust model, able to deal with data collected by single modalities. As such, the developed MLP model not only removes the problem of sensor placement in such applications, but also automatically classifies the events in the time domain, enabling the integration of data collection, handling and analysis in a simple less resource-demanding workflow, and making it a feasible alternative to the traditional approach to the problem.","",""
4,"Qamar Navid, Ahmed Hassan, A. Fardoun, R. Ramzan, A. Alraeesi","Fault Diagnostic Methodologies for Utility-Scale Photovoltaic Power Plants: A State of the Art Review",2021,"","","","",45,"2022-07-13 09:34:35","","10.3390/SU13041629","","",,,,,4,4.00,1,5,1,"The worldwide electricity supply network has recently experienced a huge rate of solar photovoltaic penetration. Grid-connected photovoltaic (PV) systems range from smaller custom built-in arrays to larger utility power plants. When the size and share of PV systems in the energy mix increases, the operational complexity and reliability of grid stability also increase. The growing concern about PV plants compared to traditional power plants is the dispersed existence of PV plants with millions of generators (PV panels) spread over kilometers, which increases the possibility of faults occurring and associated risk. As a result, a robust fault diagnosis and mitigation framework remain a key component of PV plants. Various fault monitoring and diagnostic systems are currently being used, defined by calculation of electrical parameters, extracted electrical parameters, artificial intelligence, and thermography. This article explores existing PV fault diagnostic systems in a detailed way and addresses their possible merits and demerits.","",""
2,"Z. Magnuska, B. Theek, Milita Darguzyte, M. Palmowski, E. Stickeler, V. Schulz, F. Kiessling","Influence of the Computer-Aided Decision Support System Design on Ultrasound-Based Breast Cancer Classification",2022,"","","","",46,"2022-07-13 09:34:35","","10.3390/cancers14020277","","",,,,,2,2.00,0,7,1,"Simple Summary The implementation of artificial intelligence in the computer-aided decision (CAD) support systems holds great promise for future cancer diagnosis. It is crucial to build these algorithms in a structured manner to ensure reproducibility and reliability. In this context, we used a dataset of breast ultrasound (US) images with 252 breast cancer and 253 benign cases to refine the CAD image analysis workflow. Various dataset preparations (i.e., pre-processing, and spatial augmentation) and machine learning algorithms were tested to establish the framework with the best performance in the detection and classification of breast lesions in US images. The efficacy of the proposed workflows was evaluated regarding accuracy, precision, specificity, and sensitivity. Abstract Automation of medical data analysis is an important topic in modern cancer diagnostics, aiming at robust and reproducible workflows. Therefore, we used a dataset of breast US images (252 malignant and 253 benign cases) to realize and compare different strategies for CAD support in lesion detection and classification. Eight different datasets (including pre-processed and spatially augmented images) were prepared, and machine learning algorithms (i.e., Viola–Jones; YOLOv3) were trained for lesion detection. The radiomics signature (RS) was derived from detection boxes and compared with RS derived from manually obtained segments. Finally, the classification model was established and evaluated concerning accuracy, sensitivity, specificity, and area under the Receiver Operating Characteristic curve. After training on a dataset including logarithmic derivatives of US images, we found that YOLOv3 obtains better results in breast lesion detection (IoU: 0.544 ± 0.081; LE: 0.171 ± 0.009) than the Viola–Jones framework (IoU: 0.399 ± 0.054; LE: 0.096 ± 0.016). Interestingly, our findings show that the classification model trained with RS derived from detection boxes and the model based on the RS derived from a gold standard manual segmentation are comparable (p-value = 0.071). Thus, deriving radiomics signatures from the detection box is a promising technique for building a breast lesion classification model, and may reduce the need for the lesion segmentation step in the future design of CAD systems.","",""
20,"I. Cortés-Ciriano, A. Bender","Concepts and Applications of Conformal Prediction in Computational Drug Discovery",2019,"","","","",47,"2022-07-13 09:34:35","","10.1039/9781788016841-00063","","",,,,,20,6.67,10,2,3,"Estimating the reliability of individual predictions is key to increase the adoption of computational models and artificial intelligence in preclinical drug discovery, as well as to foster its application to guide decision making in clinical settings. Among the large number of algorithms developed over the last decades to compute prediction errors, Conformal Prediction (CP) has gained increasing attention in the computational drug discovery community. A major reason for its recent popularity is the ease of interpretation of the computed prediction errors in both classification and regression tasks. For instance, at a confidence level of 90% the true value will be within the predicted confidence intervals in at least 90% of the cases. This so called validity of conformal predictors is guaranteed by the robust mathematical foundation underlying CP. The versatility of CP relies on its minimal computational footprint, as it can be easily coupled to any machine learning algorithm at little computational cost. In this review, we summarize underlying concepts and practical applications of CP with a particular focus on virtual screening and activity modelling, and list open source implementations of relevant software. Finally, we describe the current limitations in the field, and provide a perspective on future opportunities for CP in preclinical and clinical drug discovery.","",""
16,"Sathian Dananjayan, G. M. Raj","5G in healthcare: how fast will be the transformation?",2020,"","","","",48,"2022-07-13 09:34:35","","10.1007/s11845-020-02329-w","","",,,,,16,8.00,8,2,2,"","",""
0,"E. Blasch, Haoran Li, Zhihao Ma, Yang Weng","The Powerful Use of AI in the Energy Sector: Intelligent Forecasting",2021,"","","","",49,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,4,1,"Artificial Intelligence (AI) techniques continue to broaden across governmental and public sectors, such as power and energy which serve as critical infrastructures for most societal operations. However, due to the requirements of reliability, accountability, and explainability, it is risky to directly apply AI-based methods to power systems because society cannot afford cascading failures and large-scale blackouts, which easily cost billions of dollars. To meet society requirements, this paper proposes a methodology to develop, deploy, and evaluate AI systems in the energy sector by: (1) understanding the power system measurements with physics, (2) designing AI algorithms to forecast the need, (3) developing robust and accountable AI methods, and (4) creating reliable measures to evaluate the performance of the AI model. The goal is to provide a high level of confidence to energy utility users. For illustration purposes, the paper uses power system event forecasting (PEF) as an example, which carefully analyzes synchrophasor patterns measured by the Phasor Measurement Units (PMUs). Such a physical understanding leads to a data-driven framework that reduces the dimensionality with physics and forecasts the event with high credibility. Specifically, for dimensionality reduction, machine learning arranges physical information from different dimensions, resulting inefficient information extraction. For event forecasting, the supervised learning model fuses the results of different models to increase the confidence. Finally, comprehensive experiments demonstrate the high accuracy, efficiency, and reliability as compared to other state-of-the-art machine learning methods.","",""
0,"Yogesh Kulkarni, Z. SayfHussain, K. Ramamritham, Nivethitha Somu","EnsembleNTLDetect: An Intelligent Framework for Electricity Theft Detection in Smart Grid",2021,"","","","",50,"2022-07-13 09:34:35","","10.1109/ICDMW53433.2021.00070","","",,,,,0,0.00,0,4,1,"Artificial intelligence-based techniques applied to the electricity consumption data generated from the smart grid prove to be an effective solution in reducing Non Technical Loses (NTLs), thereby ensures safety, reliability, and security of the smart energy systems. However, imbalanced data, consecutive missing values, large training times, and complex architectures hinder the real time application of electricity theft detection models. In this paper, we present EnsembleNTLDetect, a robust and scalable electricity theft detection framework that employs a set of efficient data pre-processing techniques and machine learning models to accurately detect electricity theft by analysing consumers’ electricity consumption patterns. This framework utilises an enhanced Dynamic Time Warping Based Imputation (eDTWBI) algorithm to impute missing values in the time series data and leverages the Near-miss undersampling technique to generate balanced data.Further, stacked autoencoder is introduced for dimensionality reduction and to improve training efficiency. A Conditional Generative Adversarial Network (CTGAN) is used to augment the dataset to ensure robust training and a soft voting ensemble classifier is designed to detect the consumers with aberrant consumption patterns. Furthermore, experiments were conducted on the real-time electricity consumption data provided by the State Grid Corporation of China (SGCC) to validate the reliability and efficiency of EnsembleNTLDetect over the state-of-the-art electricity theft detection models in terms of various quality metrics.","",""
0,"G. Jacob, F. Raddatz","Data fusion for the efficient NDT of challenging aerospace structures: a review",2022,"","","","",51,"2022-07-13 09:34:35","","10.1117/12.2612357","","",,,,,0,0.00,0,2,1,"High performance multifunctional structural components and other system components are evolving for applications in the aerospace industry. The efficient operation and reliability of these structures must be ensured by suitable means for inspection and maintenance. However, inspection on complex structural elements via traditional non-destructive testing (NDT) methods presents challenges for the accurate detection and characterization of flaws. The combination of NDT methods offers considerable advantages over existing NDT technologies and ensures not only accuracy in the inspection, but also another perspective on flaws which otherwise would not have been identified. Large sets of data are generated through these inspections and require robust data fusion technologies for visualisation and interpretation. Emerging technologies such as artificial intelligence, internet of things and automation direct towards the new paradigms NDT 4.0 and digital twin. The measurement data for assessing the health and faulty conditions of the structure using integrated sensors and NDT methods can be represented in a digital replication of the structure called the digital twin. Technologies such as drones, augmented reality and remote NDT can help to improve the efficiency of inspections. Therefore, this review represents current technologies and concepts for NDT 4.0 and the digital twin concepts which are suitable to save time, optimize processes and maintenance costs.","",""
0,"Gasmi Safa, Djebbar Akila, Merouani Hayet Farida","A Survey on Hybrid Case-Based Reasoning and Deep Learning Systems for Medical Data Classification",2022,"","","","",52,"2022-07-13 09:34:35","","10.4018/978-1-7998-9016-4.ch006","","",,,,,0,0.00,0,3,1,"Several artificial intelligence approaches, particularly case-based reasoning (CBR), which is analogous to the context of human reasoning for problem resolution, have demonstrated their efficiency and reliability in the medical field. In recent years, deep learning represents the latest iteration of an advance in artificial intelligence technologies in medicine to aid in data classification, diagnosis of new diseases, and complex decision-making. Although these two independent approaches have good results in the medical field, the latter is still a complex field. This chapter reviews the available literature on CBR systems, deep learning systems, and CBR deep learning systems in medicine. The methods used and results obtained are discussed, and key findings are highlighted. Further, in the light of this review, some directions for future research are given. This chapter presents the proposed approach, which helps to make the retrieval phase of the CBR cycle more reliable and robust.","",""
0,"P. Narkhede, Animesh Mishra, K. Hamshita, Ankit Kumar Shubham, Aditya Chauhan","Inertial Sensors and GPS Fusion Using LSTM for Position Estimation of Aerial Vehicle",2022,"","","","",53,"2022-07-13 09:34:35","","10.1109/ICSSIT53264.2022.9716276","","",,,,,0,0.00,0,5,1,"Accurate position estimation is an application of high demand in unmanned aerial vehicles. Accelerometer, Gyroscope, Magnetometer, and Global Positioning System are some of the extensively used sensing units while estimating position. Kalman Filter is one of the widely used sensor fusion mechanisms for this purpose. With the applicability of Artificial Intelligence-based techniques in various navigation applications, this paper bestows a novel procedure for positioning an aerial vehicle in a 3D environment using the Long-Short Term Memory architecture. The LSTM network has feedback connections, making it stand different from the other typical feed-forward Neural Networks. The proposed framework's reliability and effectiveness have been exhibited on the robust dataset collected from the MATLAB example. This proposed framework provides a notable improvement in the results compared with the classic method. An AI module for real time operations can provide an effective action in estimating the position of UAVs using the LSTM network approach.","",""
0,"Junjie Peng, Xin Chen, Ming Li, Yan-hong Zhang","Torque anomaly detection of nuclear power electric valve actuator based on DAE-WDSVVD",2022,"","","","",54,"2022-07-13 09:34:35","","10.1088/1742-6596/2187/1/012048","","",,,,,0,0.00,0,4,1,"The abnormal detection of nuclear power electric valve actuator components can effectively improve its operation safety and reliability. With the rise of artificial intelligence technology, data-driven fault diagnosis methods have become more and more popular. However, in practical application, there are few or almost no fault data of valve actuator. For the problem of anomaly detection of actuator components of valve in the scenario of only normal data, anomaly detection method based on the fusion of deep autoencoder (DAE) and weighted deep weighted support vector data description (WDSVVD) is proposed. It uses normal data to train the depth self-encoder, and the reconstruction error of the depth self-encoder to train the support vector data description. Compared with the traditional anomaly detection method, it significantly improves the anomaly detection accuracy and can realize more sensitive and robust component anomaly detection.","",""
0,"Tao Guan, Wang Shanku, Momina Rauf, Shahzeb Adil, M. F. Iqbal, M. Tariq, I. Azim, A. Ng","Evolutionary Algorithm-Based Modeling of Split Tensile Strength of Foundry Sand-Based Concrete",2022,"","","","",55,"2022-07-13 09:34:35","","10.3390/su14063274","","",,,,,0,0.00,0,8,1,"Foundry sand (FS) is produced as a waste material by metal casting foundries. It is being utilized as an alternative to fine aggregates for developing sustainable concrete. In this paper, an artificial intelligence technique, i.e., gene expression programming (GEP) has been implemented to empirically formulate prediction models for split tensile strength (ST) of concrete containing FS. For this purpose, an extensive experimental database has been collated from the literature and split up into training, validation, and testing sets for modeling purposes. ST is modeled as a function of water-to-cement ratio, percentage of FS, and FS-to-cement content ratio. The reliability of the proposed expression is validated by conducting several statistical and parametric analyses. The modeling results depicted that the prediction model is robust and accurate with a high generalization capability. The availability of reliable formulation to predict strength properties can promote the utilization of foundry industry waste in the construction sector, promoting green construction and saving time and cost incurred during experimental testing.","",""
13,"Konstantinos N. Blazakis, T. Kapetanakis, G. Stavrakakis","Effective Electricity Theft Detection in Power Distribution Grids Using an Adaptive Neuro Fuzzy Inference System",2020,"","","","",56,"2022-07-13 09:34:35","","10.3390/en13123110","","",,,,,13,6.50,4,3,2,"Electric power grids are a crucial infrastructure for the proper operation of any country and must be preserved from various threats. Detection of illegal electricity power consumption is a crucial issue for distribution system operators (DSOs). Minimizing non-technical losses is a challenging task for the smooth operation of electrical power system in order to increase electricity provider’s and nation’s revenue and to enhance the reliability of electrical power grid. The widespread popularity of smart meters enables a large volume of electricity consumption data to be collected and new artificial intelligence technologies could be applied to take advantage of these data to solve the problem of power theft more efficiently. In this study, a robust artificial intelligence algorithm adaptive neuro fuzzy inference system (ANFIS)—with many applications in many various areas—is presented in brief and applied to achieve more effective detection of electric power theft. To the best of our knowledge, there are no studies yet that involve the application of ANFIS for the detection of power theft. The proposed technique is shown that if applied properly it could achieve very high success rates in various cases of fraudulent activities originating from unauthorized energy usage.","",""
10,"A. Naseri, M. Jamei, I. Ahmadianfar, M. Behbahani","Nanofluids thermal conductivity prediction applying a novel hybrid data-driven model validated using Monte Carlo-based sensitivity analysis",2020,"","","","",57,"2022-07-13 09:34:35","","10.1007/s00366-020-01163-z","","",,,,,10,5.00,3,4,2,"","",""
2,"I. Panapakidis, G. C. Christoforidis, G. Papagiannis","Hybrid computational intelligence model for Short-Term bus load forecasting",2015,"","","","",58,"2022-07-13 09:34:35","","10.1109/EEEIC.2015.7165487","","",,,,,2,0.29,1,3,7,"Distribution Generation (DG) technologies correspond to a technical field of increased interest since their application aid on system security and reliability. In order to bring forth the fully potential of DG, a robust forecasting tool is especially designed for small sized loads at the various buses. Bus load exhibit low correlation compared to the total system`s load; the presence of outlier loads is more regular and the load pattern presents high degree of stochasticity. Thus a load forecasting model designed for the system`s load is likely to show poor performance. This work proposes a hybrid bus load forecasting tool. The hybridization refers to the combined use of a clustering process with a feed-forward Artificial Neural Network (ANN). The proposed model is tested at four buses within the Greek interconnected system and simulation results highlight the efficiency of the model.","",""
5,"Sarah A. El-Sayed, Theofilos Spyrou, Antonios Pavlidis, Engin Afacan, L. Camuñas-Mesa, B. Linares-Barranco, H. Stratigopoulos","Spiking Neuron Hardware-Level Fault Modeling",2020,"","","","",59,"2022-07-13 09:34:35","","10.1109/iolts50870.2020.9159745","","",,,,,5,2.50,1,7,2,"The deployment of Artificial Intelligence (AI) hardware accelerators in a variety of applications, including safety-critical ones, requires assessing their inherent reliability to hardware-level faults and developing cost-effective fault tolerance techniques. This entails performing large-scale fault simulation experiments. However, transistor-level fault simulation is prohibitive and fault simulation should be carried out at a higher abstraction level. In this work, we focus on spiking neural networks (SNNs), and we follow a bottom-up approach starting from transistor-level simulations for developing a neuron behavioral-level fault model that can be readily employed for performing behavioral-level fault simulation of deep SNNs.","",""
3,"M. Anis, S. Taghipour, Chi-Guhn Lee","Optimal RUL Estimation: A State-of-Art Digital Twin Application",2020,"","","","",60,"2022-07-13 09:34:35","","10.1109/RAMS48030.2020.9153669","","",,,,,3,1.50,1,3,2,"A real world Industrial IoT set up has paved way for simultaneous monitoring of several sensors at their unique sampling rates. This has realized the need for artificial intelligence tools for robust data processing. However, the large size of input data requires real time monitoring and synchronization for online analysis. As the star concept behind the Industry 4.0 wave, a digital twin is a virtual, multi-scale and probabilistic simulation to mirror the performance of its physical counterpart and serve the product lifecycle in a virtual space. Evidently, a digital twin can proactively identify potential issues with its corresponding real twin. Thus, it is best suited for enabling a physics-based and data-driven model fusion to estimate the remaining useful life (RUL) of the components. Traditional RUL prediction approaches have assumed either an exponential or linear degradation trend with a fixed curve shape to build a Health Index (HI) model. Such an assumption may not be useful for multi-sensor systems or cases where sensor data is available intermittently. A common constraint in the industry is irregular sensor data collection. The resulting asynchronous time series of the sporadic data needs to be an accurate representation of the component’s HI when constructing a degradation model. In this paper, we extend the Long-Short Term Memory (LSTM) Recurrent Neural Network (RNN) technique to generate RUL prediction within a digital twin framework as a means of synchronization with changing operational states. More specifically, we first use LSTM encoder-decoder (LSTM-ED) to train a multilayered neural network and reconstruct the sensor data time series corresponding to a healthy state. The resulting reconstruction error can be used to capture patterns in input data time series and estimate HI of training and testing sets. Using a time lag to record similarity between the HI curves, a weighted average of the final RUL estimation is obtained. The described empirical approach is evaluated on publicly available engine degradation dataset with run-to-failure information. Results indicate a high RUL estimation accuracy with greater error reduction rate. This demonstrates wide applicability of the discussed methodology to various industries where event data is scarce for the application of only data-driven techniques.","",""
2,"S. Bhat, I. B. Sofi, Chong-Yung Chi","Edge Computing and Its Convergence With Blockchain in 5G and Beyond: Security, Challenges, and Opportunities",2020,"","","","",61,"2022-07-13 09:34:35","","10.1109/ACCESS.2020.3037108","","",,,,,2,1.00,1,3,2,"The internet is progressing towards a new technology archetype grounded on smart systems, heavily relying on artificial intelligence (AI), machine learning (ML), blockchain platforms, edge computing, and the internet of things (IoT). The merging of IoT, edge computing, and blockchain will be the most important factor of empowering new automatic service and commercial models with various desirable properties, such as self-verifying, self-executing, immutability, data reliability, and confidentiality provided by the advancement in blockchain smart contracts and containers. Motivated by the potential paradigm shift and the security features brought by blockchain from the traditional centralized model to a more robust and resilient decentralized model, this tutorial article proposes a multi-tier integrated blockchain and edge computing architecture for 5G and beyond for solving some security issues faced by resource-constrained edge devices. We begin with a comprehensive overview of different edge computing paradigms and their research challenges. Next, we present the classification of security threats and current defense mechanisms. Then, we present an overview of blockchain and its potential solutions to the main security issues in edge computing. Furthermore, we present the classification of facilitating developers of different architectures to select an appropriate platform for particular applications and offer insights for potential research directions. Finally, we provide key convergence features of the blockchain and edge computing, followed by some conclusions.","",""
0,"Shivani Jadhav, S. Joshi, Sabine Wehnert, S. Polley, Erasmo Purificato, E. W. D. Luca, A. Nürnberger","SEA: Summary Evaluation of Academic Publications with Unsupervised Methods",2020,"","","","",62,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,7,2,"We live in an information paradox where the problem is often that we are drowning in information while starving for knowledge. Artificial Intelligence can address this problem using summarization techniques, such as extractive summarization, which can save time and effort. While there has been a plethora of research on generating summaries, we focus on the specific case where the problem is aggravated by the absence of ground truth. Also, there can be multiple candidates for a suitable summary when the text contains paraphrasing sentences and descriptions on different abstraction levels. Hence, datasets with only one correct ground truth may only paint a fraction of the whole picture. The lack of robust evaluation methods challenges the reliability of the current summarization techniques. In this paper, we discuss three alternative unsupervised methods as an alternative for supervised summary evaluation. First, we propose the “Relative Clustering Comparison Score”, consisting of three individual scores for cluster evaluation (Adjusted Rand Index, Mutual Information, and Completeness). Second, a further method assesses the ranking the sentences obtain based on cosine similarity, and it is called the “Relative Ranking Comparison Score”. Third, we employ a semantic similarity metric called BERTScore. We mostly observe a correlation of our regarded unsupervised metrics with human judgement and unveil challenges we face while working with academic paper summarization datasets.","",""
0,"X. Yao, Lingxi Kong, M. Pecht","Date of publication xxxx 00, 0000, date of current version xxxx 00, 0000",2020,"","","","",63,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,3,2,"Hardware cybersecurity has become a key issue, especially for very large integrated circuits. If counterfeit, forged, or defective ICs present a significant threat to system reliability and security. The growing complexity of digital and mixed-signal systems makes it increasingly challenging yet vital to develop robust methods to assess and confirm the reliability and authenticity of ICs. We introduce a new terahertz testing method for non-destructive and unobtrusive identification of counterfeit, damaged, forged or defective ICs by measuring their response to incident terahertz and sub-terahertz radiation at the circuit pins and analyzing the response using artificial intelligence (AI). These responses create unique signatures for ICs. We generated 2D images by measuring the response on a selected pin of a radio frequency IC (RFIC) scanned by a focused terahertz radiation. By applying the data augmentation processes, we created a secure image data set to train the convolutional neural network (CNN) model. An unsecured image data set representing altered or damaged ICs was generated by modifying the original image data. The trained models identified secure devices with a ~94% accuracy. INDEX TERMS Terahertz, hardware cybersecurity, reliability, authentication, deep learning, convolution neural network, artificial intelligence.","",""
0,"Bo Pu","Design of 2.5D Interposer in High Bandwidth Memory and Through Silicon Via for High Speed Signal",2020,"","","","",64,"2022-07-13 09:34:35","","10.36227/techrxiv.12950261","","",,,,,0,0.00,0,1,2,"The 2.5D interposer becomes a crucial solution to realize grand bandwidth of HBM for the increasing data requirement of high performance computing (HPC) and Artificial Intelligence (AI) applications. To overcome high speed switching bottleneck caused by the large resistive and capacitive characteristics of interposer, design methods to achieve an optimized performance in a limited routing area are proposed. Unlike the conventional single through silicon via (TSV), considering the reliability, multiple TSV are used as the robust 3D interconnects for each signal path. An equivalent model to accurately describe the electrical characteristics of the multiple TSVs, and a configuration pattern strategy of TSV to mitigate crosstalk are also proposed.","",""
34,"Petrônio L. Braga, Adriano Oliveira, S. Meira","Software Effort Estimation Using Machine Learning Techniques with Robust Confidence Intervals",2007,"","","","",65,"2022-07-13 09:34:35","","10.1109/HIS.2007.56","","",,,,,34,2.27,11,3,15,"The precision and reliability of the estimation of the effort of software projects is very important for the competitiveness of software companies. Good estimates play a very important role in the management of software projects. Most methods proposed for effort estimation, including methods based on machine learning, provide only an estimate of the effort for a novel project. In this paper we introduce a method based on machine learning which gives the estimation of the effort together with a confidence interval for it. In our method, we propose to employ robust confidence intervals, which do not depend on the form of probability distribution of the errors in the training set. We report on a number of experiments using two datasets aimed to compare machine learning techniques for software effort estimation and to show that robust confidence intervals for the effort estimation can be successfully built.","",""
118,"Marco Zaffalon, Marcus Hutter","Robust Feature Selection by Mutual Information Distributions",2002,"","","","",66,"2022-07-13 09:34:35","","","","",,,,,118,5.90,59,2,20,"Mutual information is widely used in artificial intelligence, in a descriptive way, to measure the stochastic dependence of discrete random variables. In order to address questions such as the reliability of the empirical value, one must consider sample-to-population inferential approaches. This paper deals with the distribution of mutual information, as obtained in a Bayesian framework by a second-order Dirichlet prior distribution. The exact analytical expression for the mean and an analytical approximation of the variance are reported. Asymptotic approximations of the distribution are proposed. The results are applied to the problem of selecting features for incremental learning and classification of the naive Bayes classifier. A fast, newly defined method is shown to outperform the traditional approach based on empirical mutual information on a number of real data sets. Finally, a theoretical development is reported that allows one to efficiently extend the above methods to incomplete samples in an easy and effective way.","",""
8,"Miriyev","A Focus on Soft Actuation",2019,"","","","",67,"2022-07-13 09:34:35","","10.3390/act8040074","","",,,,,8,2.67,8,1,3,"The present editorial paper analyzes the hundred recent research works on soft actuation to understand the current main research focus in the light of the grand challenges in the field. Two characteristic paper types were obtained: one focuses on soft actuator design, manufacturing and demonstration, while another includes in addition the development of functional materials. Although vast majority of the works showcased soft actuation, evaluation of its robustness by multi-cyclic actuation was reported in less than 50% of the works, while only 10% described successful actuation for more than 1000 cycles. It is suggested that broadening the research focus to include investigation of mechanisms underlying the degradation of soft functional material performance in real cyclic actuation conditions, along with application of artificial intelligence methods for prediction of muscle behavior, may allow overcoming the reliability issues and developing robust soft-material actuators. The outcomes of the present work might be applicable to the entire soft robotics domain.","",""
2,"Chuanming Zhang, ZuHe Wang, Yang Wang, Lingzhi Sun","Applying artificial neural network to build engineering project bid evaluation system",2011,"","","","",68,"2022-07-13 09:34:35","","10.1109/AIMSEC.2011.6010456","","",,,,,2,0.18,1,4,11,"In the process of bidding for construction project, the winning bid is often determined by experts who assess bids in their marks, which have a strong subjective element. So there is the possibility to control who will be the winning bidder. This paper introduces the multi-layer feed forward back propagation (BP) neural network which belong to the artificial neural network (ANN) system to design a three-layer BP neural network system, which can automatic assess bids by computer. BP neural network with parallel processing, nonlinear approximating, good robust and adaptability can ensure bids evaluation activities fair and objective. Finally, the article uses a great deal of data for test and evaluation simulation to verify this system reliability and validity.","",""
0,"A. Richardson, D. Cheneler","Self-Monitoring, Self-Healing Biomorphic Sensor Technology",2019,"","","","",69,"2022-07-13 09:34:35","","10.1109/IOLTS.2019.8854453","","",,,,,0,0.00,0,2,3,"The deployment of autonomous sensors within electronic systems for both existing and emerging markets requires an increase in the reliability, security and dependability of the associated data generated. The availability of intelligent sensors that can self-adapt and ultimately self-heal would be a key step towards this objective. This paper presents ideas associated with the utilisation of sensor self-test principles and software algorithms able to generate sensor prognostics and drive adaptation, compensation and self-healing functions. Major initiatives supported both within Europe and further afield to migrate processing power to the “Edge”, deploy 5G technologies and integrate Artificial Intelligence across the system hierarchy provide technological platforms to deliver many of these concepts. An example associated with simple printed electrodes targeting corrosion detection and potentially the detection of hydrogen is presented in the context of a step towards full biomorphic capability.","",""
0,"B. Ayhan, C. Kwan","Practical Issues in Contingency Planning for UAVs with Engine Failures",2019,"","","","",70,"2022-07-13 09:34:35","","10.1145/3387168.3387202","","",,,,,0,0.00,0,2,3,"Unmanned Air Vehicles (UAV), also known as Unmanned Air Systems (UAS), are gaining more attention in recent years. Some potential commercial applications with UAVs may include small cargo transport, search and rescue operations, drought and pest monitoring, etc. It is well-known that UAVs are less reliable as compared to manned aircraft. This is probably one of the consequential reasons that Federal Aviation Administration (FAA) is hesitant to open up the national airspace (NAS) and imposes tight restrictions to UAVs. Reliability of UAVs can be improved using engines and equipment with high quality and fault diagnostic algorithms using machine learning and artificial intelligence techniques, and robust and fault tolerant controllers. Despite the above measures, engine and equipment malfunctions may still appear in various applications. In this paper, we summarize some recent research results by us with respect to engine failures encountered in UAVs. Due to engine failure, there is limited hanging time and the mishap UAV needs to land preferably in an unpopulated area. In particular, we explicitly address some practical issues related to engine failures.","",""
3,"S. A. B. Cruz, Alexandre Monteiro, Rafael Santos","Increasing process reliability in a geospatial web services composition",2009,"","","","",71,"2022-07-13 09:34:35","","10.1109/GEOINFORMATICS.2009.5293393","","",,,,,3,0.23,1,3,13,"This work proposes a method to specify the data quality requirements for the composition of geospatial web services at a semantic level, by using a rule-based approach. Those rules allow the semantic annotation of spatial data, and coupled with the conditional planning method, it can model more precisely the unpredictability of the compliance with the data quality requirements during the composition run-time. The mechanisms that automate the web services composition using artificial intelligence planning can incorporate those rules. The service composition produced by this method are more robust, thus improving process reliability when working with a composition of chained geospatial web services.","",""
14,"M. Newman","Theoretical Understanding in Science",2015,"","","","",72,"2022-07-13 09:34:35","","10.1093/bjps/axv041","","",,,,,14,2.00,14,1,7,"In this article I develop a model of theoretical understanding in science. This is a philosophical theory that specifies the conditions that are both necessary and sufficient for a scientist to satisfy the construction ‘S understands theory T ’. I first consider how this construction is preferable to others, then build a model of the requisite conditions on the basis of examples from elementary physics. I then show how this model of theoretical understanding can be made philosophically robust and provide a more sophisticated account than we see from models of a similar kind developed by those working in the psychology of physics and artificial intelligence. 1. Introduction2. The Explicandum/Analysandum3. Analysis of ‘S understands T’4. The Inferential Model 4.1. Which problems are we talking about?4.2. Does the solution have to be true?4.3. Does each cycle have to be correct for S to understand T?4.4. What is problem-solving reliability?4.5. Does every specific inference of a cycle have to be correct?4.6. Does each inference have to use a principle that is part of the theory?5. Theoretical Understanding and Conceptual Expertise: Empirical Considerations6. Conclusion: Inferential Model of Scientific Understanding and Our Traditional Problems Introduction The Explicandum/Analysandum Analysis of ‘S understands T’ The Inferential Model 4.1. Which problems are we talking about?4.2. Does the solution have to be true?4.3. Does each cycle have to be correct for S to understand T?4.4. What is problem-solving reliability?4.5. Does every specific inference of a cycle have to be correct?4.6. Does each inference have to use a principle that is part of the theory? Which problems are we talking about? Does the solution have to be true? Does each cycle have to be correct for S to understand T? What is problem-solving reliability? Does every specific inference of a cycle have to be correct? Does each inference have to use a principle that is part of the theory? Theoretical Understanding and Conceptual Expertise: Empirical Considerations Conclusion: Inferential Model of Scientific Understanding and Our Traditional Problems","",""
4,"M. Haque, Mohammad Noor Bin Shaheed, Seungdeog Choi","Deep Learning Based Micro-Grid Fault Detection and Classification in Future Smart Vehicle",2018,"","","","",73,"2022-07-13 09:34:35","","10.1109/ITEC.2018.8450201","","",,,,,4,1.00,1,3,4,"Microgrid in electric vehicles (EV) require high reliability as any malfunctioning may result in loss of human live. To ensure robust protection for micro-grid, intelligent continuous monitoring and fault classification mechanism is required along with existing traditional protection scheme. Industrial internet of things (IIOT) makes low-cost continuous health monitoring data from EV micro-grid available. Manually processing of these bulk data for fault classification is cost and time consuming, and impractical. Neural network (NN) and other artificial intelligence classification systems require time-consuming feature engineering. In this digest, a Deep Neural Network (NN) framework, Convolution Neural Network (CNN) are applied for automatic and rapid processing of bulk health monitoring data for high precision micro-grid fault classification. The proposed classification method learns from data autonomously and extracts features by identifying anomalies in converter and inverter output voltage, and DC-link capacitor voltage amplitude, phase angle and harmonic components. This CNN based fault classification method shows better performance compared to conventional artificial intelligence based fault classification systems. The proposed classification system is verified under different operating conditions.","",""
11,"Marco Zaffalon, Marcus Hutter","Robust inference of trees",2005,"","","","",74,"2022-07-13 09:34:35","","10.1007/s10472-005-9007-9","","",,,,,11,0.65,6,2,17,"","",""
16,"S. Rawat, Ahmed Patel, J. Celestino, André L. M. dos Santos","A dominance based rough set classification system for fault diagnosis in electrical smart grid environments",2016,"","","","",75,"2022-07-13 09:34:35","","10.1007/s10462-016-9468-8","","",,,,,16,2.67,4,4,6,"","",""
20,"Valquiria R. C. Martinho, Clodoaldo Nunes, C. R. Minussi","Prediction of school dropout risk group using Neural Network",2013,"","","","",76,"2022-07-13 09:34:35","","","","",,,,,20,2.22,7,3,9,"Dropping out of school is one of the most complex and crucial problems in education, causing social, economic, political, academic and financial losses. In order to contribute to solve the situation, this paper presents the potentials of an intelligent, robust and innovative system, developed for the prediction of risk groups of student dropout, using a Fuzzy-ARTMAP Neural Network, one of the techniques of artificial intelligence, with possibility of continued learning. This study was conducted under the Federal Institute of Education, Science and Technology of Mato Grosso, with students of the Colleges of Technology in Automation and Industrial Control, Control Works, Internet Systems, Computer Networks and Executive Secretary. The results showed that the proposed system is satisfactory, with global accuracy superior to 76% and significant degree of reliability, making possible the early identification, even in the first term of the course, the group of students likely to drop out.","",""
12,"Tang-Hsien Chang, Yi-Ru Chen","Driver fatigue surveillance via eye detection",2014,"","","","",77,"2022-07-13 09:34:35","","10.1109/ITSC.2014.6957718","","",,,,,12,1.50,6,2,8,"In this study, driver alertness and fatigue-related surveillance were measured by image processing techniques for detecting the driver's face and eyes in a frame. The image was acquired using an infrared-only camera that transforms human pupil into a distinct white circle; hence, the eyes are extracted more easily than those taken from a regular camera. The proposed model recorded eye closure measures, which are proven for the validation of fatigue. A multi-stage eye tracking process was also applied for ensuring robust, real-time eye movement. Meanwhile, a proposed warning module based on a back-propagation neural network employed as an artificial intelligence was used to train the program for adapting each individual. Finally, the proposed module attained a 97% success rate with high reliability at low cost.","",""
3,"Marcus Hutter, Marco Zaffalon","Distribution of mutual information for robust feature selection",2002,"","","","",78,"2022-07-13 09:34:35","","","","",,,,,3,0.15,2,2,20,"Mutual information is widely used in artificial intelligence, in a descriptive way, to measure the stochastic dependence of discrete random variables. In order to address questions such as the reliability of the empirical value, one must consider sample-to-population inferential approaches. This paper deals with the distribution of mutual information, as obtained in a Bayesian framework by using second-order Dirichlet prior distributions. We derive reliable and quickly computable analytical approximations for the distribution of mutual information. We concentrate on the mean, variance, skewness, and kurtosis. For the mean we also provide an exact expression. The results are applied to the problem of selecting features for incremental learning and classification of the naive Bayes classifier. A fast, newly defined filter is shown to outperform the traditional approach based on empirical mutual information on a number of real data sets. A theoretical development allows the above methods to be extended to incomplete samples in an easy and effective way. Further experiments on incomplete data sets support the extension of the proposed filter to the case of missing data.","",""
4,"L. Portinale, A. Bobbio, S. Montani","From AI to Dependability : using Bayesian Networks for Reliability Modeling and Analysis",2004,"","","","",79,"2022-07-13 09:34:35","","","","",,,,,4,0.22,1,3,18,"Bayesian Networks (BN) provide a robust probabilistic method of reasoning under uncertainty. They have been successfully proposed in the field of Artificial Intelligence (AI) as the most flexible formalism for reasoning under uncertain knowledge (Neapolitan 1990; Pearl 1989; Jensen 2001). Their success stands from several factors: • the graphical representation of the knowledge to reason with; in particular the graphical representation of the set of dependencies among the modeled variables, through the notion of d-separation (Pearl 1989); • the restricted number of probabilities to be specified with respect to a complete joint probability model; • the possibility of performing different kinds of inferences such as prediction (i.e. to infer information about effects starting from causes), abduction or diagnosis (i.e. to infer information about causes starting from effects) and inter-causal reasoning (i.e. to infer information about one cause given information about the effect and another cause); • the possibility of "" learning "" the model from a database of observations. For these reasons, they have been successfully applied in a variety of real-world tasks (Heckermann and Wellman 1995). However, they have received little attention in the area of dependability and reliability analysis. A few exceptions are the work by Almond exploiting a special kind of graphical models for modeling the reliability of a system (Almond 1992), the approach in (Torres-Toledano and Sucar 1998; Solano-Soto and Sucar 2001) where reliability block diagrams are converted into Bayesian Networks for the analysis and the recent work by Langseth (Langseth 2002). The present talk is aimed at exploring the capabilities of the BN formalism in the modeling and analysis of dependable systems. Starting from the work described in (Portinale and Bobbio 1999; Bobbio, Portinale, Minichino, and Ciancamerla 2001), we compare BN with one of the most popular techniques for dependability analysis of large, safety critical systems, namely Fault Trees Analysis (FTA). The talk shows that any Fault Tree (FT) can be directly mapped into a BN and that basic inference techniques on the latter may be used to obtain classical parameters computed from the former (i.e. reliability of the Top Event or of any subsystem , criticality of components, etc). The advantage is that, by using BN, some additional power can be obtained, both at the modeling and at the analysis level. At the modeling level, several restrictive assumptions implicit in the FT methodology can be removed and various kinds of dependencies among components can …","",""
131,"R. Stengel","Intelligent failure-tolerant control",1990,"","","","",80,"2022-07-13 09:34:35","","10.1109/ISIC.1990.128511","","",,,,,131,4.09,131,1,32,"An overview of failure-tolerant control is presented, beginning with robust control, progressing through parallel and analytical redundancy, and ending with rule-based systems and artificial neural networks. By design or implementation, failure-tolerant control systems are 'intelligent' systems. All failure-tolerant systems require some degree of robustness to protect against catastrophic failure; failure tolerance often can be improved by adaptivity in decision making and control, as well as by redundancy in measurement and actuation. Reliability, maintainability, and survivability can be enhanced by failure tolerance, although each objective poses different goals for control system design. Artificial intelligence concepts are helpful for integrating and codifying failure-tolerant control systems, not as alternatives but as adjuncts to conventional design methods.<<ETX>>","",""
4,"Si-Hun Sung, S. Chien, Mun-Gab Kim, Joon-Nyun Kim","Adaptive window algorithm with four-direction sizing factors for robust correlation-based tracking",1997,"","","","",81,"2022-07-13 09:34:35","","10.1109/TAI.1997.632258","","",,,,,4,0.16,1,4,25,"The authors propose the adaptive window algorithm with four-direction sizing factors (AWA-FSF) for robust correlation-based tracking in complex cluttered environments. When the size and shape of a moving object have been changed, the correlator often accumulates walk-off error. The success of correlation-based tracking largely depends on choosing a suitable window size and position and thus transferring the proper reference image to the next frame. Since the AWA-FSF is capable of adjusting a reference image size more rapidly and properly, one can minimize the influence of complex background and clutter. In addition, one can finely tune the center point of the reference image repeatedly after the main tracking process. Thus one has increased stability and reliability of correlation-based image tracking. They tested the performance of the AWA-FSF using 45 real image sequences made of over 3400 images.","",""
8,"Yue Yang, J. Wen, Xiaofei Chen","Improvements on particle swarm optimization algorithm for velocity calibration in microseismic monitoring",2015,"","","","",82,"2022-07-13 09:34:35","","10.1007/S11589-015-0127-Y","","",,,,,8,1.14,3,3,7,"","",""
62,"C. Lo, Y. Wong, A. Rad","Intelligent system for process supervision and fault diagnosis in dynamic physical systems",2006,"","","","",83,"2022-07-13 09:34:35","","10.1109/TIE.2006.870707","","",,,,,62,3.88,21,3,16,"In recent years, the increasing complexity of process plants and other engineered systems has extended the scope of interest in control engineering, which was previously focused on the development of controllers for specified performance criteria such as stability and precision. Modern industrial systems require a higher demand of system reliability, safety, and low-cost operation, which in turn call for sophisticated and elegant fault-detection and isolation algorithms. This paper develops an intelligent supervisory coordinator (ISC) for process supervision and fault diagnosis in dynamic physical systems. A qualitative bond graph modeling scheme, integrating artificial-intelligence techniques with control engineering, is used to construct the knowledge base of the ISC. A supervisor provided by the ISC utilizes the knowledge in the knowledge base to classify various system behaviors, coordinates different control tasks (e.g., fault diagnosis), and communicates system states to human operators. The ISC provides a robust semiautonomous system to assist human operators in managing dynamic physical systems. The proposed ISC has been successfully applied to supervise a laboratory-scale servo-tank liquid process rig.","",""
10,"D. Galar, U. Kumar, Yuan Fuqing","RUL prediction using moving trajectories between SVM hyper planes",2012,"","","","",84,"2022-07-13 09:34:35","","10.1109/RAMS.2012.6175481","","",,,,,10,1.00,3,3,10,"With increasing amounts of data being generated by businesses and researchers, there is a need for fast, accurate and robust algorithms for data analysis. Improvements in database's technology, computing performance and artificial intelligence have contributed to the development of intelligent data analysis. The primary aim of data mining is knowledge discovery, i.e. patterns in the data that lead to better understanding of the data generating process and to useful predictions. The knowledge that becomes available through data mining enables an asset owner to make important decisions about life cycle costs in advance. In maintenance field, CMMS (Computer maintenance management system) and CM (Condition Monitoring) are the most popular software available in the industries. Since first one stores all historical data, maintenance actions, events and ma nufacturer recommendations, second one collects and stores all critical physical parameters (vibration, temperature.) to be monitored in a regular time basis. However, converting these data into useful information is a challenge. The degradation process of a system may be affected by many unknown factors, such as unidentified fault modes, unmeasured operational conditions, engineering variance, environmental conditions, etc. These unknown factors not only complicate the degradation behaviors of the system, but also make it difficult to collect quality data. Due to lack of knowledge and incomplete measurements, certain important con text information (e.g. fault modes, operational conditions) of the collected data will be missing. Therefore, historical data of the system with a large variety of degradation patterns will be mixed together. With such data, learning a global model for Remaining Useful Life (RUL) prediction becomes extremely hard since the end user does not have enough and good-quality data to model properly the system. This has led us to look for advanced RUL prediction techniques beyond the traditional RUL prediction models. The degradation process for many engineering systems, especially mechanical systems, is irreversible unless the condition is recovered by effective maintenance actions. The irreversible degradation process does not necessarily imply that the observed features will exhibit a monotonic progression pattern during degradation. Such progression pattern is sometimes hard to model using parametric methods. Considering a degradation process involving no or limited maintenance, the process may compose of a sequence of irreversible stages (either discrete or continuous) from new to be worn out, which can be implicitly expressed by the trajectory of the measured condition data or features. Therefore, the RUL of the system can be estimated if its future degradation trend can be projected from those historical instances. In this paper, a novel RUL prediction method inspired by feature maps and SVM classifiers is proposed. The historical instances of a system with life-time condition data are used to create a classification by SVM hyper planes. For a test instance of the same system, whose RUL is going to be estimated, degradation speed is evaluated by computing the minimal distance defined based on the degradation trajectories, i.e. the approach of the system to the hyper plane that segregates good and bad condition data at a different time horizon. Therefore, the final RUL of a specific component can be estimated, and global RUL information can then be obtained by aggregating the multiple RUL estimations using a density estimation method. Proposed model develops an effective RUL prediction method that addresses multiple challenges in complex system prognostics, where many parameters are unknown. Similarities between degradation trajectories can be checked in order to enrich existing methodologies in prognostic's applications. Existing CM data for bearings will be used to verify the model.","",""
12,"Gang Zheng, R. Craven","Multiclass support vector machines for power system disturbances classification based on wide-area frequency measurements",2011,"","","","",85,"2022-07-13 09:34:35","","10.1109/SECON.2011.5752908","","",,,,,12,1.09,6,2,11,"The intelligent, robust and fast multi-class classification of power system disturbances is very important to improve control algorithms for ensuring power system security and reliability, an essential function for smart grid infrastructure. Moreover, in a future power system mostly consisting of distributed generators and renewable energy resources on which the disturbance has more impact, the analysis of disturbances by classifying and categorizing real-time frequency data is rather critical. Fortunately, wide area frequency data from a nation-wide frequency monitoring network (FNET) provides a means by which disturbances can be detected. However, so far none of strategies reported to date has good performance at classifying the disturbances although many of them are used currently in on-line analysis. The complex and irregular pattern characteristics of each kind of disturbance are the main reason. Artificial intelligence methods could be one of the solutions, but the large number of input values and an insufficient number of training examples has slowed the reduction of artificial intelligence methods to practice. Therefore, a mathematical model of common disturbances is proposed to generate a training database for artificial intelligence method and feature extraction by computing the wavelet coefficients, parameterizing the results and computer generating the data. This paper uses a multi-class support vector machine model to be trained on the extracted features to discern the otherwise hard-to-classify disturbances pattern and upon testing, yields good performance.","",""
39,"Hassan Baghgar Bostan Abad, A. Y. Varjani, Taheri Asghar","Using Fuzzy Controller in Induction Motor Speed Control with Constant Flux",2007,"","","","",86,"2022-07-13 09:34:35","","","","",,,,,39,2.60,13,3,15,"Abstract — Variable speed drives are growing and varying. Drives expanse depend on progress in different part of science like power system, microelectronic, control methods, and so on. Artificial intelligent contains hard computation and soft computation. Artificial intelligent has found high application in most nonlinear systems same as motors drive. Because it has intelligence like human but there are no sentimental against human like angriness and.... Artificial intelligent is used for various points like approximation, control, and monitoring. Because artificial intelligent techniques can use as controller for any system without requirement to system mathematical model, it has been used in electrical drive control. With this manner, efficiency and reliability of drives increase and volume, weight and cost of them decrease. Keywords — Artificial intelligent, electrical motor, intelligent drive and control, I. I NTRODUCTION ECAUSE induction motors require low maintenance and are robust [8], have many applications in industry [9]. Along with industry progress, it indicates requirement to progresses drive with high performance [8]. DC motors are control abler than AC motors but they require much cost. In addition, in equal power, DC motors have higher volume and weight. Main variations in semiconductors, converters topology, analyze technique and simulation of electrical machines drive and newer control technique have had role in this progress [1]. Usually classical control is used in motors drive [8,10,11]. Design and implementation of Conventional controls have difficulties that nameable: a) It is basis on mathematical accurate model of system that usual it is not known [12,13]. b) Drives are nonlinear systems and Classical control performance with this system decrease [9,13]. c) Variation of machine parameters (especially in vector control [12]) by load disturbance [9], motor saturation [9,13] or thermal variations [13] do not cause expectation performance [9,13]. d) Classical linear control shows high performance only for one unique act point [13]. e) With choose improperly coefficient, classical control cannot receive acceptable result and suitable choose for constant coefficient in especial application condition with set point varying, necessarily is not optimum [14].","",""
5,"Hamed Monkaresi, M. Hussain, R. Calvo","A Dynamic Approach for Detecting Naturalistic Affective States from Facial Videos during HCI",2012,"","","","",87,"2022-07-13 09:34:35","","10.1007/978-3-642-35101-3_15","","",,,,,5,0.50,2,3,10,"","",""
0,"Chunxiang Wang, Dongfang Xu, Yongqing Wang","Novel Svdd-Based Algorithm For Moving Object Detecting And Tracking Under Dynamic Scenes",2016,"","","","",88,"2022-07-13 09:34:35","","10.21307/IJSSIS-2017-911","","",,,,,0,0.00,0,3,6,"Abstract Object detecting and tracking is an important technique used in diverse applications of machine vision, and has made great progress with the prevalence of artificial intelligence technology, among which the detecting and tracking moving object under dynamic scenes is more challenging for high requirements on real-time performance and reliability. Essentially analyzing, object detecting and tracking need to classify the objects and background into two different categories according to different features, where the detecting and tracking drift caused by noisy background can be effectively handled by robust maximum margin classifier, such as one-class SVM. But the time and space complexities of traditional one-class SVM methods tend to be high, which limits its wide applications to various fields. Inspired by the idea proposed by Support Vector Data Description (SVDD), in this paper we present a novel SVDD-based algorithm to efficiently deal with detecting and tracking moving object under dynamic scenes. The experimental results on synthetic, benchmark data and real-world videos demonstrate the competitive performances of the proposed method","",""
0,"A. Thakur., S. Wadhwani, A. Wadhwani","A Review On Induction Motor Fault Diagnostic Techniques",2016,"","","","",89,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,3,6,"Induction motors plays a vital role in almost all the industrial drive systems because of their simple, efficient and robust nature offering high degree of reliability. These machines face various stresses during operating conditions which may lead to different types of faults. Hence condition monitoring and maintenance becomes necessary in order to avoid unexpected failures. Different fault monitoring techniques for induction motors can be broadly categorized as model based techniques, signal processing techniques, and artificial intelligence based soft computing techniques. The traditional model based diagnostic techniques provides a good detection of the fault in the machine but nowadays artificial intelligence techniques have been introduced to overcome the existing inaccuracy. Soft computing techniques enable better analysis of a faulty system even if models are inaccurate. Besides giving improved performance these techniques are easy to extend and modify. This paper provides a comprehensive study of conventional and innovative techniques. © 2016 Elixir All rights reserved. Elixir Elec. Engg. 93C (2016) 39829-39833 Electrical Engineering Available online at www.elixirpublishers.com (Elixir International Journal) Alka Thakur et al./ Elixir Elec. Engg. 93C (2016) 39829-39833 39830 energy caused by a structural deformation in a solid structure under mechanical or thermal stresses. Generation and propagation of cracks, growth of twins, etc. associated with plastic deformation are usually among the key sources of Acoustic emission [9]. Sound pressure is the effective pressure or root mean square (RMS) pressure. The effect of sound pressure generated by bearings has been researched by various scientists for fault identification. It has been observed that Sound pressure level in their frequency domain is the effective method for the fault identification in machine. Further sound intensity is defined as the time-averaged rate of flow of sound energy through unit area. Sound intensity measurement is more effective than sound pressure measurement technique for the diagnosis of bearing related faults and failures [10]. Hence Acoustic measurement proves to be important condition monitoring technique through non-destructive testing. 4.2 Vibration Vibration monitoring is one of the oldest techniques used for the detection of mechanical faults like misalignment, unbalance, defective bearings, cracked rotor bars etc. It can be used to monitor the above faults both in their time and frequency domains. Time domain analysis emphasis mainly on statistical parameters of vibration signals like standard deviation, skewness, kurtosis etc. The simplest approach in the time domain is to measure the root-mean-square (RMS) value and crest factor. It has been observed that this method has been applied for the detection of localized defects with limited success [11]. Frequency domain analysis emphasis mainly on fourier transform (FT), specially fast fourier transform (FFT) methods to transform the time domain signals into their frequency domain signals, and is finally analyzed in their vibration and power spectra where both low and high frequency ranges of the vibration spectrum are used for determine the condition of the bearing. Vibration analysis is widely accepted technique to detect faults of a machine since it is non-destructive and reliable that allows continuous monitoring without stopping the machine [12]. 4.3 Noise Noise of electrical machinery is generated by the vibration of machine parts. The generated vibration signal is then recorded which is then allowed to data acquisition system and finally analyzed in the form of fault. The sounds produced by a machine as a result from the dynamics of its components and by regularly monitoring these sounds the occurrence of such changes can be used to diagnose the condition of the machine and the probable onset of failure and faults [13]. One of the most important parameter is the sensitivity of frequency. 5 .Different type of faults in induction motor Whenever a machine undergoes stress conditions during the operations, it deviates from its normal behavior and leads to faults. These faults can be classified as electrical, mechanical and miscellaneous faults and are discussed below as; 5.1 Electrical Faults According to a survey by EPRI in 1982 it has been observed that, about 41% of all induction motor failures are caused by bearing faults, 37% by stator faults, 10% by rotor faults, and 12% by miscellaneous faults [14]. 5.1.1 Rotor faults Broken rotor bars can be either partly or fully broken. This brokenness can be due to frequency, sudden start at rated voltage, due to high temperature, vibration or mechanical stress which effects motor health very severely. Rotor faults can be categorized as broken rotor bars, shorted rotor field windings and broken rotor end rings [15]. 5.1.2Stator faults It includes faults related to stator core and stator winding. Most stator faults can be attributed to various stressful operating conditions. Further stator winding faults can be classified into four types such as turn-to-turn, coil-to-coil, open circuit, phase-to-phase and coil-to-ground [16]. 5.2 Mechanical faults This type of faults occurs in the mechanical components of the machines such as bearings housings and end covers. These are discussed below; 5.2.1. Bearing faults Bearing faults are categorized as inner faults and outer faults by their location. Contamination of lubricant, loss of lubricant, over-loading, and excess heating are the most common causes of bearing faults. Bearing defects may be classified as local or distributed [17]. The distributed defects include surface roughness, mismatched waviness and off-size rolling elements. A localized defect consists of flakes, pits and cracks on the rolling surfaces. Another problem caused by the bearing fault results in improper installation caused due to the imbalanced alignment of the bearing onto the shaft. This produces false indentation of the raceways and damages the motor physically. 5.2.2. Air gap eccentricity faults Basically these types of faults comprise of two types namely Static eccentricity and Dynamic eccentricity. The Static eccentricity is the generation of uneven stator-rotor air gap caused due to the presence of improper adjusted air gap for plain bearings. The rotor moves from its original position at the center of the stator and rotates around its own center in static eccentricity. Dynamic eccentricity is the generation of variable stator–rotor air gap due to the wear out of bearing housings and end covers due to which rotor stars rubbing with the stator. The rotor shifts from its normal position, but rotates around the center of the stator in dynamic eccentricity. 5.3 Miscellaneous faults It includes faults in the supporting devices which assemble the accessories of the drive system. These are classified mainly into two types: 5.3.1 Gear box faults Due to excessive load on the gearboxes, occurrence of fluctuations on the gearbox may cause vibrational defects in the motor. The main components in gear boxes vibrational spectrum are the tooth-meshing frequencies and its harmonics, along with sideband structures due to modulation effects. These sideband structures can be used as important diagnostic indications for gear fault detection. Hence, to perform a reliable and precise diagnosis of a rotor winding for motors connected to a gearbox, the effects of gearbox components in the spectrum need to be identified and scrutinized. 5.3.2 Cracked or bent shaft These faults may be as a result of rubbing between the rotor and stator, resulting in a serious damage to the stator core and windings. 6. Diagnostic techniques for fault detection Basically there are three types of techniques namely model based, signal processing and soft computing. In one way or the other these techniques are helpful in the detection of faults in electrical machines. These techniques are classified as; Alka Thakur et al./ Elixir Elec. Engg. 93C (2016) 39829-39833 39831 6.1Model based techniques Model based diagnostic techniques may be defined as an asymmetrical induction motor whose model is used to predict failure fault signatures. The difference between measured and simulated signatures is used for fault detection [18, 19]. 6.2 Signal processing based techniques Signal based diagnosis relies on advances in digital technology. It looks for known fault signatures in quantities sampled from the actual machine. The signatures are then monitored by suitable signal processors. Signal processing can be used to enhance signal to noise ratio and to normalize data to isolate the fault from other phenomenon and decrease sensitivity to operating conditions. These are then classified into following types: 6.2.1 Fourier transform The Fourier transform of the function f(x) is the function F (ω), where:","",""
2,"T. Kapetanakis, I. Vardiambasis, G. Liodakis, M. Ioannidou, A. Maras","Smart Antenna Design Using Neural Networks",2013,"","","","",90,"2022-07-13 09:34:35","","","","",,,,,2,0.22,0,5,9,"Optimizing antenna arrays to approximate desired far field radiation patterns is of exceptional interest in smart antenna technology. This paper shows how to apply artificial intelligence, in the form of neural networks, to achieve specific beam-forming with linear antenna arrays. Multilayer feed-forward neural networks are used to maximize multiple main beams’ radiation of a linear antenna array. In particular, a triple beam radiation pattern is presented in order to demonstrate the effectiveness and the reliability of the proposed approach. The results show that multilayer feed-forward neural networks are robust and can solve complex antenna problems.","",""
1,"L. Jaeger, S. Segonds, C. Bès","Methodology Based on Multiagent for Solving Multidisciplinary Optimization Problem Under Uncertainty",2015,"","","","",91,"2022-07-13 09:34:35","","10.2514/1.I010241","","",,,,,1,0.14,0,3,7,"T HERE are a large number ofmethodologies available inmultidisciplinary design optimization (MDO) for deterministic environments [1–3]. However, to design systems that are both robust and reliable, uncertainties regarding models and parameters have to be catered for to ensure success of the project in terms of timeliness, cost, and performance. This is crucial, especially during the preliminary design phasewhere there are still many uncertainties and decisions have to be compatible with the more detailed analysis of future development phases. To cope with this problem, reliability-based and robust design methods [4] have been developed to assist designers in the decision-making process. A number of relatively time-consuming techniques to solve optimization problems under uncertainty have been devised [5–8]. The main drawback with these methods is the computational burden associated with evaluation of robustness and reliability during design. A powerful methodology stemming from artificial intelligence, the autoadaptive multiagent system (AMAS), has recently been developed to solve complexmultidisciplinary optimization problems, especially in the aeronautical field [9–11]. Due to both its local and parallel approach, the AMAS method allows complex deterministic optimization problems to be solved rapidly, even in multidisciplinary contexts such as aircraft design where thermal engineering, structural, and fluid mechanics issues all interact. However, the said AMASmethodwas developed to respond to a deterministic environment only. The aim of the present Note is to extend theAMASmethodwith its multiagent framework to copewithMDO under conditions of uncertainty, providing a new methodology so it can be effectively used in an uncertain environment. To do so, a strategy to integrate and propagate uncertainties in themultiagent framework is first developed. Then, to optimize computing time,multiagent propagation is combined with a sequential optimization process coupled with the use of AMAS. This Note is organized as follows. Section II gives the approach used to solve a deterministic multidisciplinary optimization problem with a multiagent system. For didactic purposes, this approach is presented through an extremely simple mathematical application. Section III then goes on to describe the proposedmethodology to solveMDOproblems in an uncertain environment. This section is broken down into two subsections. The first describes the integration and propagation of uncertainties in the multiagent framework, whereas the second looks at the interactions between uncertainty propagation, the sequential optimization process, and the AMAS method. More precisely, at each step of the sequential optimization process, the AMAS method is effectively used to solve a deterministic optimization problem. From the given AMAS deterministic optimum, a reliability analysis is than conducted. Violated constraints are shiftedwith the introduction of adaptive safety coefficients as computed from multiagent uncertainty propagation. These coefficients are applied to define a new deterministic optimization problem to be solved by the AMAS over the following cycle. Iterations stop once the safety coefficients have been brought down to a low enough level. Finally, in Sec. IV, this newmethodology is applied to a preliminary aircraft design test case. Results show substantial improvements in terms of robustness and efficiency comparedwith the standard double-loop optimization process used to solve classicalMDOproblems. Conclusions and perspectives are presented in Sec. V.","",""
0,"Yue, Yang, Jian, Wen, Xiaofei, Chen","Improvements on particle for velocity calibration in swarm optimization algorithm microseismic monitoring",2015,"","","","",92,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,6,7,"In this paper,we apply particle swarm optimization(PSO),an artificial intelligence technique,to velocity calibration in microseismic monitoring.We ran simulations with four 1-D layered velocity models and three different initial model ranges.The results using the basic PSO algorithm were reliable and accurate for simple models,but unsuccessful for complex models.We propose the staged shrinkage strategy(SSS) for the PSO algorithm.The SSS-PSO algorithm produced robust inversion results and had a fast convergence rate.We investigated the effects of PSO’s velocity clamping factor in terms of the algorithm reliability and computational efficiency.The velocity clamping factor had little impact on the reliability and efficiency of basic PSO,whereas it had a large effect on the efficiency of SSS-PSO.Reassuringly,SSS-PSO exhibits marginal reliability fluctuations,which suggests that it can be confidently implemented.","",""
0,"M. Power","CONTROLLING THE TORQUE OF THREE PHASE INDUCTION MOTOR WITH GENETIC ALGORITHM",2015,"","","","",93,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,1,7,"The induction motor, known for its robustness, relatively low cost, reliability and efficiency, is the vital part in many research works. The advancement in power semiconductor devices, digital data processing and control has led to great improvements in torque response control of AC motors. Direct Torque control principle has been used for Induction Motor (IM) drives with fast dynamics. DTC has been widely recognized for its fast and robust torque and flux control. Novel approach of the Genetic algorithm scheme for direct torque control (DTC) of an Induction Motor (IM) AC drive is the recent area of research. To improve the performance of conventional DTC, artificial intelligence like neural networks, fuzzy(1)(2) are implemented. Though DTC has high dynamic performance, it has few undesirable contents like high ripple in torque, output current and deviations in switching frequency of the inverter. The Z-source converter employs a unique impedance network to couple the converter main circuit to the power source. This provided unique features that cannot be obtained in the traditional voltage-source or current-source converters where a inductor and capacitor are used, respectively.","",""
23,"Lingyun Zhu, Baoming Wu, Changxiu Cao","[Introduction to medical data mining].",2003,"","","","",94,"2022-07-13 09:34:35","","","","",,,,,23,1.21,8,3,19,"Modern medicine generates a great deal of information stored in the medical database. Extracting useful knowledge and providing scientific decision-making for the diagnosis and treatment of disease from the database increasingly becomes necessary. Data mining in medicine can deal with this problem. It can also improve the management level of hospital information and promote the development of telemedicine and community medicine. Because the medical information is characteristic of redundancy, multi-attribution, incompletion and closely related with time, medical data mining differs from other one. In this paper we have discussed the key techniques of medical data mining involving pretreatment of medical data, fusion of different pattern and resource, fast and robust mining algorithms and reliability of mining results. The methods and applications of medical data mining based on computation intelligence such as artificial neural network, fuzzy system, evolutionary algorithms, rough set, and support vector machine have been introduced. The features and problems in data mining are summarized in the last section.","",""
21,"Y. Takizawa, Y. Hattori, J. Itoh, Akira Fukumoto","An intelligent man-machine system for future nuclear power plants",1994,"","","","",95,"2022-07-13 09:34:35","","10.13182/NT94-A34999","","",,,,,21,0.75,5,4,28,"The objective of the development of an intelligent man-machine system for future nuclear power plants is enhancement of operational reliability by applying recent advances in cognitive science, artificial intelligence, and computer technologies. To realize this objective, the intelligent man-machine system, aiming to support a knowledge-based decision making process in an operator's supervisory plant control tasks, consists of three main functions, i.e., a cognitive model-based advisor, a robust automatic sequence controller, and an ecological interface. These three functions have been integrated into a console-type nuclear power plant monitoring and control system as a validation test bed. The validation tests in which experienced operator crews participated were carried out in 1991 and 1992. The test results show the usefulness of the support functions and the validity of the system design approach.","",""
0,"A. Malatras, F. Peng, B. Hirsbrunner","Exploiting bio-inspired approaches for the monitoring of pervasive environments",2011,"","","","",96,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,3,11,"BioMPE (Bio-inspired Monitoring of Pervasive Environments) is a research project undertaken by the Pervasive and Artificial Intelligence research group at the Department of Informatics, University of Fribourg in Switzerland. The main goal of BioMPE is to build an intelligent and context-aware pervasive computing middleware to address the issues of reliability and dynamic adaptation of P2P overlays that are utilized to conduct monitoring over heterogeneous and ubiquitous networks. Drawing inspiration from nature and in particular from the behavior of ant colonies and swarm intelligence, we employ related selforganization techniques to construct reliable and robust P2P overlays. High-level policy-based management guides the operation and the possible need for adaptation of the monitoring task, in accordance to context information collected from the pervasive environment. In this respect, the key output of the project is the set of protocols, algorithms and mechanisms to enable the provision of reliable P2P overlay network monitoring, adaptation of which is guided by a pervasive computing middleware to reflect varying requirements.","",""
5,"F. Anifowose","Hybrid AI Models for the Characterization of Oil and Gas Reservoirs: Concept, Design and Implementation",2009,"","","","",97,"2022-07-13 09:34:35","","","","",,,,,5,0.38,5,1,13,"Oil and Gas remain the most exploited source of energy in the world today and have been predicted that they will continue to be available for exploitation in many decades to come. Hence, there is the need to develop accurate and robust predictive models for their effective and efficient exploration, exploitation and management to ensure consistent availability. Various Artificial Intelligence techniques have been used but with dire needs for improvement. Recently, hybrid schemes have been reported to offer better performance and reliability. The capabilities of these schemes have not been well utilized in Oil and Gas. This book explains how these schemes have been utilized in the prediction of porosity and permeability, two important indicators of oil and gas reserves, based on the hybridization of Type-2 Fuzzy Logic, Support Vector Machines and Functional Networks, using real-world well logs. The results are very promising. This book will be of great benefit to researchers and practitioners in the application of AI techniques in oil and gas as well as in Data Mining and Machine Learning.","",""
2,"R. Andoga, L. Főző, L. Madarász","Use of anytime control algorithms in the area of small turbojet engines",2008,"","","","",98,"2022-07-13 09:34:35","","10.1109/ICCCYB.2008.4721374","","",,,,,2,0.14,1,3,14,"The article deals with basics of modeling of our small experimental turbojet engine MPM 20 and the following design of a complex situational control system with elements of anytime control algorithms bound with a developed analytical model of the engine. By use of advanced algorithms using methods of artificial intelligence in a robust framework to achieve the aims of reliability and high efficiency, we are trying demonstrate their usability and proficiency in the area of turbojet engines. All the tests and data come from measurements on a real object of our experimental small turbojet engine and are therefore a good step to be used on normal sized engines.","",""
0,"J. Povazan, R. Andoga, L. Főző, J. Judičák","ANYTIME CONTROL ALGORITHM DESIGN FOR MPM 20 ENGINE",2008,"","","","",99,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,4,14,"The article deals with basics of modeling of our small experimental turbojet engine MPM 20 and furthermore design of a complex situational control system with elements of anytime control algorithms bound with a developed analytical model of the engine. By use of advanced algorithms using methods of artificial intelligence in a robust framework to achieve the aims of reliability and high efficiency, we are trying demonstrate their usability and proficiency in the area of turbojet engines. All the tests and data come from measurements on a real object of our experimental small turbojet engine and are therefore a good step to be used on normal sized engines.","",""
5,"S. Sundararajan","A cooperative distributed problem-solving framework",1990,"","","","",100,"2022-07-13 09:34:35","","","","",,,,,5,0.16,5,1,32,"This dissertation describes a framework for cooperative distributed problems solving (CDPS) and how this framework can be used to solve the class of problems comprised of semi-independent sub-problems. The CDPS framework provides speedup over traditional sequential problem solving methods, increased reliability and flexibility, reduced static consistency checking, graceful degradation owing to the increased spectral nature of knowledge, and efficient knowledge acquisition. In addition, the CDPS framework facilitates the coupling of disparate knowledge sources by means of a robust communication protocol, this facilitates the coupling of artificial intelligence (AI) techniques with more traditional problem solving techniques such as decision theory, operations research, etc. The CDPS framework also promotes the preservation of multiple perspectives and as a result multiple solutions. The dissertation describes the verification of the CDPS framework via the successful implementations of two prototypes, namely, the Computer Systems Manager and the Supplier-side Risk Analyzer. The dissertation also describes design experiments carried out on the CDPS framework that serve as useful guidelines for future developers of such systems. The design experiments involve comparisons with a Sequential Model and a Distributed Model in order to get a real feel for the performance of the CDPS framework.","",""
5,"Michael L. Baird, Raul A. Brauner, H. K. Hu, Terry K. Herms","Extending The Limits Of Pattern Inspection Using Machine Vision",1985,"","","","",101,"2022-07-13 09:34:35","","10.1117/12.947757","","",,,,,5,0.14,1,4,37,"Current state-of-the-art instruments for automatic visual inspection of masks, reticles, or wafers, are targeted to inspect patterns with line widths as small as 1.5 micron (gm) and defects as small as 0.5 micron. New near term targets for production inspection machines will require inspection of 1.0 micron line width patterns and 0.3 micron defects. The types of inspection machines which will adequately address these new targets will require very robust image analysis algorithms. Some current instrumentation will predictably fail in the sub-0.3 micron to sub-0.5 micron defect size range due to fundamental limitations of traditional image acquisition and processing methods (e.g., pixel to pixel comparison modes). Two studies were carried out to see how defect detection might be improved using a new approach exploiting adaptive signal processing, artificial intelligence methodologies, and CAD directed model matching. The first study investigated the current practice/ limits of defect detection; measuring the reliability with which defects could be detected, measured, and correlated with a CAD data base. We conclude that defects greater or equal to 0.5 micron can be consistently detected at a high (>90Z confidence level); with defects as small as 0.3 - 0.4 micron being detected routinely. In the second study, we explored the realistic potential of defect detectability of the new approach. In this experiment we report very encouraging experimental data extending to line widths of 0.5 micron, and defects below 0.2 micron.","",""
0,"Jiwu Wang, H. Kimura, M. Sugisaka","Study on the vision-based indoor navigation of an Alife robot",2003,"","","","",102,"2022-07-13 09:34:35","","10.1109/IECON.2003.1280323","","",,,,,0,0.00,0,3,19,"It is a prerequisite for a mobile robot to realize steady and reliable navigation. But its reliability is often weakened by an unavoidable slip and some incorrigible drift errors of sensors, especially after a long distance navigation. Although perceptual landmarks were solutions to such problems, it is easy to miss landmarks at some specific spots when the robot moves at different speeds, especially at higher speeds. And if the landmarks were put in any intervals, or if the illumination conditions were not good, it is more difficult to find them. In order to detect and extract the artificial landmarks robustly under different illuminating conditions, some low level but robust image processing technique was implemented. The moving speed was controlled with the visual servoing control method. When the robot suddenly can not find some specific landmarks at some specific spots because of the higher speed or rapid turn of the road, it will search for the landmarks based on its intelligence, the inertia of the previous motion, and find the landmark in the shortest time. These methods were verified by the reliable vision-based indoor navigation of an Alife mobile robot.","",""
0,"E. Wegman, Donald T. Gantz, John J. H. Miller","Computing Science and Statistics: Proceedings of the Symposium on the Interface: Computationally Intensive Methods in Statistics (20th) Held in Fairfax, Virginia on April 20-23, 1988",1989,"","","","",103,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,3,33,"Abstract : Contents: Computationally Intensive Statistical Methods; Statistical Graphics; Computational Aspects of Simulated Annealing; Parallel Computing; Density and Function Estimation; Software Tools for Statistics; Artificial Intelligence, Expert Systems, and Statistics; Numerical Methods; Statistical Methods; Computational Discrete Mathematics; Simulation; Robust and Nonparametric Methods; Time Series Analysis; Reliability and Life Distributions; Applications; Biostatistical Methods; Image Processing.","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",104,"2022-07-13 09:34:35","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
10,"M. Alomar, M. Hameed, N. Al‐Ansari, M. Alsaadi","Data-Driven Model for the Prediction of Total Dissolved Gas: Robust Artificial Intelligence Approach",2020,"","","","",105,"2022-07-13 09:34:35","","10.1155/2020/6618842","","",,,,,10,5.00,3,4,2,"Saturated total dissolved gas (TDG) is recently considered as a serious issue in the environmental engineering field since it stands behind the reasons for increasing the mortality rates of fish and aquatic organisms. The accurate and more reliable prediction of TDG has a very significant role in preserving the diversity of aquatic organisms and reducing the phenomenon of fish deaths. Herein, two machine learning approaches called support vector regression (SVR) and extreme learning machine (ELM) have been applied to predict the saturated TDG% at USGS 14150000 and USGS 14181500 stations which are located in the USA. For the USGS 14150000 station, the recorded samples from 13 October 2016 to 14 March 2019 (75%) were used for training set, and the rest from 15 March 2019 to 13 October 2019 (25%) were used for testing requirements. Similarly, for USGS 14181500 station, the hourly data samples which covered the period from 9 June 2017 till 11 March 2019 were used for calibrating the models and from 12 March 2019 until 9 October 2019 were used for testing the predictive models. Eight input combinations based on different parameters have been established as well as nine statistical performance measures have been used for evaluating the accuracy of adopted models, for instance, not limited, correlation of determination (        R      2        ), mean absolute relative error (MAE), and uncertainty at 95% (        U      95        ). The obtained results of the study for both stations revealed that the ELM managed efficiently to estimate the TDG in comparison to SVR technique. For USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.986 (0.986), MAE of 0.316 (0.441), and         U      95        of 3.592 (3.869). Lastly, for USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.991 (0.991), MAE of 0.338 (0.396), and         U      95        of 0.832 (0.837). In addition, ELM’s training process computational time is stated to be much shorter than that of SVM. The results also showed that the temperature parameter was the most significant variable that influenced TDG relative to the other parameters. Overall, the proposed model (ELM) proved to be an appropriate and efficient computer-assisted technology for saturated TDG modeling that will contribute to the basic knowledge of environmental considerations.","",""
120,"Hoang Nguyen, X. Bui","Predicting Blast-Induced Air Overpressure: A Robust Artificial Intelligence System Based on Artificial Neural Networks and Random Forest",2018,"","","","",106,"2022-07-13 09:34:35","","10.1007/s11053-018-9424-1","","",,,,,120,30.00,60,2,4,"","",""
0,"Bushra Rasheed, M. Usama, Asmara Safdar","Robust Artificial Intelligence Approach to Stabilize and Control Propeller Driven Hybrid UGV",2022,"","","","",107,"2022-07-13 09:34:35","","10.1109/ICAI55435.2022.9773375","","",,,,,0,0.00,0,3,1,"Hybrid Unmanned Ground Vehicle (HUGV) can drive on any terrain including walls and fly as well, using the multi directional thrust force of propellers. In the era of industrial revolution, hybrid UGVs need to be autonomous with intelligent decision making capabilities. During wall climbing of hybrid UGVs, stability is essential and depends on real time feedback from multiple sensors. To increase stability and control, it is proposed that PID control loops should be replaced by AI based algorithms that reduce the decision time and mathematical complexity. For autonomous movement in any terrain using the proposed model, intelligent UGVs can map and localize simultaneously.They can make intelligent decisions about mode of movement i.e. driving on ground or wall, steering on ground or wall, flying and maneuvering by using real time sensor readings. Integration of the proposed AI models with HUGV can be applied to many areas which are hard for humans to access, for instance; inspection of large structures, bio & nuclear hazard environments, planetary exploration & magnetic fields detection.","",""
81,"Thomas G. Dietterich","Steps Toward Robust Artificial Intelligence",2017,"","","","",108,"2022-07-13 09:34:35","","10.1609/aimag.v38i3.2756","","",,,,,81,16.20,81,1,5,"Recent advances in artificial intelligence are encouraging governments and corporations to deploy AI in high-stakes settings including driving cars autonomously, managing the power grid, trading on stock exchanges, and controlling autonomous weapons systems. Such applications require AI methods to be robust to both the known unknowns (those uncertain aspects of the world about which the computer can reason explicitly) and the unknown unknowns (those aspects of the world that are not captured by the system’s models). This article discusses recent progress in AI and then describes eight ideas related to robustness that are being pursued within the AI research community. While these ideas are a start, we need to devote more attention to the challenges of dealing with the known and unknown unknowns. These issues are fascinating, because they touch on the fundamental question of how finite systems can survive and thrive in a complex and dangerous world","",""
50,"M. Ryan","In AI We Trust: Ethics, Artificial Intelligence, and Reliability",2020,"","","","",109,"2022-07-13 09:34:35","","10.1007/s11948-020-00228-y","","",,,,,50,25.00,50,1,2,"","",""
0,"B. Herman, W. Sirichokchatchawan, C. Nantasenamat, S. Pongpanich","Artificial intelligence in overcoming rifampicin resistant-screening challenges in Indonesia: a qualitative study on the user experience of CUHAS-ROBUST",2021,"","","","",110,"2022-07-13 09:34:35","","10.1108/jhr-11-2020-0535","","",,,,,0,0.00,0,4,1,"PurposeThe Chulalongkorn-Hasanuddin Rifampicin-Resistant Tuberculosis Screening Tool (CUHAS-ROBUST) is an artificial intelligence–based (AI–based) application for rifampicin-resistant tuberculosis (RR-TB) screening. This study aims to elaborate on the drug-resistant TB (DR-TB) problem and the impact of CUHAS-ROBUST implementation on RR-TB screening.Design/methodology/approachA qualitative approach with content analysis was performed from September 2020 to October 2020. Medical staff from the primary care center were invited online for application trials and in-depth video call interviews. Transcripts were derived as a data source. An inductive thematic data saturation technique was conducted. Descriptive data of participants, user experience and the impact on the health service were summarizedFindingsA total of 33 participants were selected from eight major islands in Indonesia. The findings show that DR-TB is a new threat, and its diagnosis faces obstacles particularly prolonged waiting time and inevitable delayed treatment. Despite overcoming the RR-TB screening problems with fast prediction, the dubious screening performance, and the reliability of data collection for input parameters were the main concerns of CUHAS-ROBUST. Nevertheless, this application increases the confidence in decision-making, promotes medical procedure compliance, active surveillance and enhancing a low-cost screening approach.Originality/valueThe CUHAS-ROBUST achieved its purpose as a tool for clinical decision-making in RR-TB screening. Moreover, this study demonstrates AI roles in enhancing health-care quality and boost public health efforts against tuberculosis.","",""
0,"Xiuzhuo Wei, Huinan Zhao, Lianlian Liu","Reliability Analysis of Intelligent Big Data Hybrid Information System Based on Artificial Intelligence",2021,"","","","",111,"2022-07-13 09:34:35","","10.1109/PHM-Nanjing52125.2021.9612825","","",,,,,0,0.00,0,3,1,"In the intelligent big data hybrid information system, the index parameters of the binary semantic interval are too few, which leads to large idle parameters in reliability analysis and occupying more system resources. In response to the above problems, intelligent big data mixed information is analyzed based on the reliability of the artificial intelligence system. Use artificial intelligence technology to sort and process information operation tasks, construct homomorphic mapping information relationships, define weight parameters generated by index semantics, control the idleness of indicators generated by reliability algorithms, construct reliability analysis algorithms, and finally complete the analysis process. Prepare the known software and hardware environment of the intelligent big data hybrid information system, and apply the reliability analysis method based on state information, the reliability analysis method based on cloud computing, and the designed reliability analysis method for experiments. The results show that the designed reliability analysis method actually occupies less operating resources of the information system, and the processing time is short. High reliability analysis accuracy.","",""
353,"Erico Tjoa, Cuntai Guan","A Survey on Explainable Artificial Intelligence (XAI): Toward Medical XAI",2019,"","","","",112,"2022-07-13 09:34:35","","10.1109/TNNLS.2020.3027314","","",,,,,353,117.67,177,2,3,"Recently, artificial intelligence and machine learning in general have demonstrated remarkable performances in many tasks, from image processing to natural language processing, especially with the advent of deep learning (DL). Along with research progress, they have encroached upon many different fields and disciplines. Some of them require high level of accountability and thus transparency, for example, the medical sector. Explanations for machine decisions and predictions are thus needed to justify their reliability. This requires greater interpretability, which often means we need to understand the mechanism underlying the algorithms. Unfortunately, the blackbox nature of the DL is still unresolved, and many machine decisions are still poorly understood. We provide a review on interpretabilities suggested by different research works and categorize them. The different categories show different dimensions in interpretability research, from approaches that provide “obviously” interpretable information to the studies of complex patterns. By applying the same categorization to interpretability in medical research, it is hoped that: 1) clinicians and practitioners can subsequently approach these methods with caution; 2) insight into interpretability will be born with more considerations for medical practices; and 3) initiatives to push forward data-based, mathematically grounded, and technically grounded medical education are encouraged.","",""
51,"T. Dragičević, P. Wheeler, F. Blaabjerg","Artificial Intelligence Aided Automated Design for Reliability of Power Electronic Systems",2019,"","","","",113,"2022-07-13 09:34:35","","10.1109/TPEL.2018.2883947","","",,,,,51,17.00,17,3,3,"This paper proposes a new methodology for automated design of power electronic systems realized through the use of artificial intelligence. Existing approaches do not consider the system's reliability as a performance metric or are limited to reliability evaluation for a certain fixed set of design parameters. The method proposed in this paper establishes a functional relationship between design parameters and reliability metrics, and uses them as the basis for optimal design. The first step in this new framework is to create a nonparametric surrogate model of the power converter that can quickly map the variables characterizing the operating conditions (e.g., ambient temperature and irradiation) and design parameters (e.g., switching frequency and dc link voltage) into variables characterizing the thermal stress of a converter (e.g., mean temperature and temperature variation of its devices). This step can be carried out by training a dedicated artificial neural network (ANN) either on experimental or simulation data. The resulting network is named as $\text{ANN}_{1}$ and can be deployed as an accurate surrogate converter model. This model can then be used to quickly map the yearly mission profile into a thermal stress profile of any selected device for a large set of design parameter values. The resulting data is then used to train $\text{ANN}_{2}$, which becomes an overall system representation that explicitly maps the design parameters into a yearly lifetime consumption. To verify the proposed methodology, $\text{ANN}_{2}$ is deployed in conjunction with the standard converter design tools on an exemplary grid-connected PV converter case study. This study showed how to find the optimal balance between the reliability and output filter size in the system with respect to several design constraints. This paper is also accompanied by a comprehensive dataset that was used for training the ANNs.","",""
44,"A. Goli, H. Zare, R. Tavakkoli-Moghaddam, A. Sadeghieh","Hybrid artificial intelligence and robust optimization for a multi-objective product portfolio problem Case study: The dairy products industry",2019,"","","","",114,"2022-07-13 09:34:35","","10.1016/j.cie.2019.106090","","",,,,,44,14.67,11,4,3,"","",""
16,"A. Amritphale, Ranojoy Chatterjee, Suvo Chatterjee, N. Amritphale, Ali Rahnavard, G. Awan, B. Omar, G. Fonarow","Predictors of 30-Day Unplanned Readmission After Carotid Artery Stenting Using Artificial Intelligence",2021,"","","","",115,"2022-07-13 09:34:35","","10.1007/s12325-021-01709-7","","",,,,,16,16.00,2,8,1,"","",""
24,"David Mhlanga","Artificial Intelligence in the Industry 4.0, and Its Impact on Poverty, Innovation, Infrastructure Development, and the Sustainable Development Goals: Lessons from Emerging Economies?",2021,"","","","",116,"2022-07-13 09:34:35","","10.3390/SU13115788","","",,,,,24,24.00,24,1,1,"Artificial intelligence in the fourth industrial revolution is beginning to live up to its promises of delivering real value necessitated by the availability of relevant data, computational ability, and algorithms. Therefore, this study sought to investigate the influence of artificial intelligence on the attainment of Sustainable Development Goals with a direct focus on poverty reduction, goal one, industry, innovation, and infrastructure development goal 9, in emerging economies. Using content analysis, the result pointed to the fact that artificial intelligence has a strong influence on the attainment of Sustainable Development Goals particularly on poverty reduction, improvement of the certainty and reliability of infrastructure like transport making economic growth and development possible in emerging economies. The results revealed that Artificial intelligence is making poverty reduction possible through improving the collection of poverty-related data through poverty maps, revolutionizing agriculture education and the finance sector through financial inclusion. The study also discovered that AI is also assisting a lot in education, and the financial sector allowing the previously excluded individuals to be able to participate in the mainstream economy. Therefore, it is important that governments in emerging economies need to invest more in the use of AI and increase the research related to it so that the Sustainable Development Goals (SDGs) related to innovation, infrastructure development, poverty reduction are attained.","",""
18,"Xinyu Liu, Mingzhe Chen, Yuanwei Liu, Yue Chen, S. Cui, L. Hanzo","Artificial Intelligence Aided Next-Generation Networks Relying on UAVs",2020,"","","","",117,"2022-07-13 09:34:35","","10.1109/MWC.001.2000174","","",,,,,18,9.00,3,6,2,"In this article, we propose artificial intelligence (AI) enabled unmanned aerial vehicle (UAV) aided wireless networks (UAWN) for overcoming the challenges imposed by the random fluctuation of wireless channels, blocking and user mobility effects. In UAWN, multiple UAVs are employed as aerial base stations, which are capable of promptly adapting to the randomly fluctuating environment by collecting information about the users' position and tele-traffic demands, learning from the environment and acting upon the satisfaction level feedback received from the users. Moreover, AI enables the interaction among a swarm of UAVs for cooperative optimization of the system. As a benefit of the AI framework, several challenges of conventional UAWN may be circumvented, leading to enhanced network performance, improved reliability and agile adaptivity. As a further benefit, dynamic trajectory design and resource allocation are demonstrated. Finally, potential research challenges and opportunities are discussed.","",""
17,"Rajesh Gupta, Aparna Kumari, S. Tanwar","Fusion of blockchain and artificial intelligence for secure drone networking underlying 5G communications",2020,"","","","",118,"2022-07-13 09:34:35","","10.1002/ett.4176","","",,,,,17,8.50,6,3,2,"Nowadays, the exponential increase in the usage of drones in various realms of societal and military applications necessitates advancements and stability in drone communication. Drones have proven their potential in providing real‐time cost‐efficient solutions for several applications like healthcare, smart grid surveillance, smart city monitoring, and border surveillance. Though it has many security and privacy issues, researchers across the globe have given numerous solutions to protect drone communication from cyber‐attacks. Most of these solutions were based on cryptographic techniques and are highly compute extensive. There exist few blockchain‐based solutions, which suffer from high transaction storage costs with communication reliability, latency, and bandwidth issues. Motivated by these facts, in this paper, we present a comprehensive survey to secure drone communication and propose a blockchain‐based secure and intelligent drone communication architecture underlying 5G communication network and artificial intelligence (AI) techniques. The proposed architecture uses an InterPlanetary File System (IPFS) as a platform for data storage, which ensures improved network performance, communication security and privacy, and reduces transaction storage cost. Further, it facilitates efficient drone communication in providing dynamic, flexible, and on‐the‐fly decisions competencies through 5G and AI technologies. Then, we incorporate a healthcare‐based case study using the proposed architecture. At last, future research challenges and directions are emphasized for improvement in this research area.","",""
13,"Hooman Farzaneh, Ladan Malehmirchegini, A. Bejan, Taofeek Afolabi, Alphonce Mulumba, Precious P. Daka","Artificial Intelligence Evolution in Smart Buildings for Energy Efficiency",2021,"","","","",119,"2022-07-13 09:34:35","","10.3390/APP11020763","","",,,,,13,13.00,2,6,1,"The emerging concept of smart buildings, which requires the incorporation of sensors and big data (BD) and utilizes artificial intelligence (AI), promises to usher in a new age of urban energy efficiency. By using AI technologies in smart buildings, energy consumption can be reduced through better control, improved reliability, and automation. This paper is an in-depth review of recent studies on the application of artificial intelligence (AI) technologies in smart buildings through the concept of a building management system (BMS) and demand response programs (DRPs). In addition to elaborating on the principles and applications of the AI-based modeling approaches widely used in building energy use prediction, an evaluation framework is introduced and used for assessing the recent research conducted in this field and across the major AI domains, including energy, comfort, design, and maintenance. Finally, the paper includes a discussion on the open challenges and future directions of research on the application of AI in smart buildings.","",""
0,"Yuan Cao, Yongmao Wang","Reliability analysis of computer adaptive system based on artificial intelligence recognition model",2022,"","","","",120,"2022-07-13 09:34:35","","10.1007/s12652-021-03615-w","","",,,,,0,0.00,0,2,1,"","",""
10,"T. Penzkofer, A. Padhani, B. Turkbey, M. Haider, H. Huisman, J. Walz, G. Salomon, I. Schoots, J. Richenberg, G. Villeirs, V. Panebianco, O. Rouvière, V. Løgager, J. Barentsz","ESUR/ESUI position paper: developing artificial intelligence for precision diagnosis of prostate cancer using magnetic resonance imaging",2021,"","","","",121,"2022-07-13 09:34:35","","10.1007/s00330-021-08021-6","","",,,,,10,10.00,1,14,1,"","",""
2,"B. Nair, Yakov Diskin, V. Asari","Multi-modal low cost mobile indoor surveillance system on the Robust Artificial Intelligence-based Defense Electro Robot (RAIDER)",2012,"","","","",122,"2022-07-13 09:34:35","","10.1117/12.930353","","",,,,,2,0.20,1,3,10,"We present an autonomous system capable of performing security check routines. The surveillance machine, the Clearpath Husky robotic platform, is equipped with three IP cameras with different orientations for the surveillance tasks of face recognition, human activity recognition, autonomous navigation and 3D reconstruction of its environment. Combining the computer vision algorithms onto a robotic machine has given birth to the Robust Artificial Intelligencebased Defense Electro-Robot (RAIDER). The end purpose of the RAIDER is to conduct a patrolling routine on a single floor of a building several times a day. As the RAIDER travels down the corridors off-line algorithms use two of the RAIDER's side mounted cameras to perform a 3D reconstruction from monocular vision technique that updates a 3D model to the most current state of the indoor environment. Using frames from the front mounted camera, positioned at the human eye level, the system performs face recognition with real time training of unknown subjects. Human activity recognition algorithm will also be implemented in which each detected person is assigned to a set of action classes picked to classify ordinary and harmful student activities in a hallway setting.The system is designed to detect changes and irregularities within an environment as well as familiarize with regular faces and actions to distinguish potentially dangerous behavior. In this paper, we present the various algorithms and their modifications which when implemented on the RAIDER serves the purpose of indoor surveillance.","",""
462,"Stuart J. Russell, Dan Dewey, Max Tegmark","Research Priorities for Robust and Beneficial Artificial Intelligence",2015,"","","","",123,"2022-07-13 09:34:35","","10.1609/aimag.v36i4.2577","","",,,,,462,66.00,154,3,7,"Success in the quest for artificial intelligence has the potential to bring unprecedented benefits to humanity, and it is therefore worthwhile to investigate how to maximize these benefits while avoiding potential pitfalls. This article gives numerous examples (which should by no means be construed as an exhaustive list) of such worthwhile research aimed at ensuring that AI remains robust and beneficial.","",""
9,"Saeed Askary, Nasser Abu-Ghazaleh, Yasean A. Tahat","Artificial Intelligence and Reliability of Accounting Information",2018,"","","","",124,"2022-07-13 09:34:35","","10.1007/978-3-030-02131-3_28","","",,,,,9,2.25,3,3,4,"","",""
10,"A. C. Horta, A. Silva, C. Sargo, V. M. Gonçalves, T. C. Zangirolami, Roberto Campos Giordano","Robust artificial intelligence tool for automatic start-up of the supplementary medium feeding in recombinant E. coli cultivations",2011,"","","","",125,"2022-07-13 09:34:35","","10.1007/s00449-011-0540-0","","",,,,,10,0.91,2,6,11,"","",""
8,"Linbo Liu, Mingcheng Bi, Yunhua Wang, Junfeng Liu, Xiwen Jiang, Zhongbin Xu, Xingcai Zhang","Artificial intelligence-powered microfluidics for nanomedicine and materials synthesis.",2021,"","","","",126,"2022-07-13 09:34:35","","10.1039/d1nr06195j","","",,,,,8,8.00,1,7,1,"Artificial intelligence (AI) is an emerging technology with great potential, and its robust calculation and analysis capabilities are unmatched by traditional calculation tools. With the promotion of deep learning and open-source platforms, the threshold of AI has also become lower. Combining artificial intelligence with traditional fields to create new fields of high research and application value has become a trend. AI has been involved in many disciplines, such as medicine, materials, energy, and economics. The development of AI requires the support of many kinds of data, and microfluidic systems can often mine object data on a large scale to support AI. Due to the excellent synergy between the two technologies, excellent research results have emerged in many fields. In this review, we briefly review AI and microfluidics and introduce some applications of their combination, mainly in nanomedicine and material synthesis. Finally, we discuss the development trend of the combination of the two technologies.","",""
51,"C. Nelson, L. Perez-Chada, A. Creadore, S. Li, K. Lo, Priya Manjaly, Ashley B Pournamdari, Elizabeth Tkachenko, J. Barbieri, J. Ko, Alka V. Menon, R. Hartman, A. Mostaghimi","Patient Perspectives on the Use of Artificial Intelligence for Skin Cancer Screening: A Qualitative Study.",2020,"","","","",127,"2022-07-13 09:34:35","","10.1001/jamadermatol.2019.5014","","",,,,,51,25.50,5,13,2,"Importance The use of artificial intelligence (AI) is expanding throughout the field of medicine. In dermatology, researchers are evaluating the potential for direct-to-patient and clinician decision-support AI tools to classify skin lesions. Although AI is poised to change how patients engage in health care, patient perspectives remain poorly understood.   Objective To explore how patients conceptualize AI and perceive the use of AI for skin cancer screening.   Design, Setting, and Participants A qualitative study using a grounded theory approach to semistructured interview analysis was conducted in general dermatology clinics at the Brigham and Women's Hospital and melanoma clinics at the Dana-Farber Cancer Institute. Forty-eight patients were enrolled. Each interview was independently coded by 2 researchers with interrater reliability measurement; reconciled codes were used to assess code frequency. The study was conducted from May 6 to July 8, 2019.   Main Outcomes and Measures Artificial intelligence concept, perceived benefits and risks of AI, strengths and weaknesses of AI, AI implementation, response to conflict between human and AI clinical decision-making, and recommendation for or against AI.   Results Of 48 patients enrolled, 26 participants (54%) were women; mean (SD) age was 53.3 (21.7) years. Sixteen patients (33%) had a history of melanoma, 16 patients (33%) had a history of nonmelanoma skin cancer only, and 16 patients (33%) had no history of skin cancer. Twenty-four patients were interviewed about a direct-to-patient AI tool and 24 patients were interviewed about a clinician decision-support AI tool. Interrater reliability ratings for the 2 coding teams were κ = 0.94 and κ = 0.89. Patients primarily conceptualized AI in terms of cognition. Increased diagnostic speed (29 participants [60%]) and health care access (29 [60%]) were the most commonly perceived benefits of AI for skin cancer screening; increased patient anxiety was the most commonly perceived risk (19 [40%]). Patients perceived both more accurate diagnosis (33 [69%]) and less accurate diagnosis (41 [85%]) to be the greatest strength and weakness of AI, respectively. The dominant theme that emerged was the importance of symbiosis between humans and AI (45 [94%]). Seeking biopsy was the most common response to conflict between human and AI clinical decision-making (32 [67%]). Overall, 36 patients (75%) would recommend AI to family members and friends.   Conclusions and Relevance In this qualitative study, patients appeared to be receptive to the use of AI for skin cancer screening if implemented in a manner that preserves the integrity of the human physician-patient relationship.","",""
50,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello","Artificial Intelligence, Algorithmic Pricing, and Collusion",2020,"","","","",128,"2022-07-13 09:34:35","","10.1257/AER.20190623","","",,,,,50,25.00,13,4,2,"Increasingly, pricing algorithms are supplanting human decision making in real marketplaces. To inform the competition policy debate on the possible consequences of this development, we experiment with pricing algorithms powered by Artificial Intelligence (AI) in controlled environments (computer simulations), studying the interaction among a number of Q-learning algorithms in a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. In this setting the algorithms consistently learn to charge supra-competitive prices, without communicating with one another. The high prices are sustained by classical collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.","",""
7,"Efrén Pérez Santín, Raquel Rodríguez Solana, María de las Nieves González García, M. D. García Suárez, Gerardo David Blanco Díaz, María Dolores Cima Cabal, J. Moreno Rojas, J. I. López Sánchez","Toxicity prediction based on artificial intelligence: A multidisciplinary overview",2021,"","","","",129,"2022-07-13 09:34:35","","10.1002/wcms.1516","","",,,,,7,7.00,1,8,1,"The use and production of chemical compounds are subjected to strong legislative pressure. Chemical toxicity and adverse effects derived from exposure to chemicals are key regulatory aspects for a multitude of industries, such as chemical, pharmaceutical, or food, due to direct harm to humans, animals, plants, or the environment. Simultaneously, there are growing demands on the authorities to replace traditional in vivo toxicity tests carried out on laboratory animals (e.g., European Union REACH/3R principles, Tox21 and ToxCast by the U.S. government, etc.) with in silica computational models. This is not only for ethical aspects, but also because of its greater economic and time efficiency, as well as more recently because of their superior reliability and robustness than in vivo tests, mainly since the entry into the scene of artificial intelligence (AI)‐based models, promoting and setting the necessary requirements that these new in silico methodologies must meet. This review offers a multidisciplinary overview of the state of the art in the application of AI‐based methodologies for the fulfillment of regulatory‐related toxicological issues.","",""
7,"Thaísa Pinheiro Silva, Mariana Mendonça Hughes, L. S. Menezes, M. D. de Melo, W. M. Takeshita, Paulo Henrique Luiz de Freitas","Artificial intelligence-based cephalometric landmark annotation and measurements according to Arnett's analysis: can we trust a bot to do that?",2021,"","","","",130,"2022-07-13 09:34:35","","10.1259/dmfr.20200548","","",,,,,7,7.00,1,6,1,"OBJECTIVE To assess the reliability of CEFBOT, an artificial intelligence (AI)-based cephalometry software, for cephalometric landmark annotation and linear and angular measurement according to Arnett's analysis.   METHODS Thirty lateral cephalometric radiographs acquired with a Carestream CS 9000 3D unit (Carestream Health Inc., Rochester/NY) were used in this study. The 66 landmarks and the ten selected linear and angular measurements of Arnett's analysis were identified on each radiograph by a trained human examiner (control) and by CEFBOT (RadioMemory Ltd., Belo Horizonte, Brazil). For both methods, landmark annotations and measurements were duplicated with an interval of 15 days between measurements and the intraclass correlation coefficient (ICC) was calculated to determine reliability. The numerical values obtained with the two methods were compared by a t-test for independent variables.   RESULTS CEFBOT was able to perform all but one of the ten measurements. ICC values > 0.94 were found for the remaining eight measurements, while the Frankfurt horizontal plane - true horizontal line (THL) angular measurement showed the lowest reproducibility (human, ICC = 0.876; CEFBOT, ICC = 0.768). Measurements performed by the human examiner and by CEFBOT were not statistically different.   CONCLUSION Within the limitations of our methodology, we concluded that the AI contained in the CEFBOT software can be considered a promising tool for enhancing the capacities of human Radiologists.","",""
1,"Ya-ting Chan, Yi Fu, Lu Yu, Feng Wu, Ho‐Wei Wang, Ting‐Han Lin, S. Chan, Ming‐Chung Wu, J. Wang","Compacted Self‐Assembly Graphene with Hydrogen Plasma Surface Modification for Robust Artificial Electronic Synapses of Gadolinium Oxide Memristors",2020,"","","","",131,"2022-07-13 09:34:35","","10.1002/admi.202000860","","",,,,,1,0.50,0,9,2,"The rise of artificial intelligence and Internet of Things has led to an increase in the demand for a large amount of data computing in recent years. To fulfill the requirements of neuromorphic engineering, a promising system composed of artificial neurons and synapses has to be developed. Here, a cheap and mass‐productive compacted self‐assembly (CSA) graphene with hydrogen (H2) plasma surface modification is used as the bottom electrode (BE) of gadolinium oxide (GdxOy) memristors to emulate the robust synapses in neuromorphic systems. As the plasma treatment time increases, the increased resistance ratio and reduced operating voltages of GdxOy memristors are obtained, which can be attributed to the removal of functional groups on graphene flakes and the enhancement in the redox reaction of CSA graphene during the resistive switching. The GdxOy memristors with a 10 min H2 plasma surface modified CSA graphene BE present outstanding reliabilities of data retention for more than 104 s and cycling operation up to 150 times. Additionally, superior bionic characteristics with more adjustable synaptic weight and more harmonious spike‐timing‐dependent plasticity (STDP) behaviors of GdxOy memristors with H2 plasma surface modified CSA graphene BEs are achieved, providing an opportunity for the applications in future neuromorphic computing systems.","",""
96,"Eduardo H. B. Maia, L. Assis, Tiago Alves de Oliveira, Alisson Marques da Silva, A. Taranto","Structure-Based Virtual Screening: From Classical to Artificial Intelligence",2020,"","","","",132,"2022-07-13 09:34:35","","10.3389/fchem.2020.00343","","",,,,,96,48.00,19,5,2,"The drug development process is a major challenge in the pharmaceutical industry since it takes a substantial amount of time and money to move through all the phases of developing of a new drug. One extensively used method to minimize the cost and time for the drug development process is computer-aided drug design (CADD). CADD allows better focusing on experiments, which can reduce the time and cost involved in researching new drugs. In this context, structure-based virtual screening (SBVS) is robust and useful and is one of the most promising in silico techniques for drug design. SBVS attempts to predict the best interaction mode between two molecules to form a stable complex, and it uses scoring functions to estimate the force of non-covalent interactions between a ligand and molecular target. Thus, scoring functions are the main reason for the success or failure of SBVS software. Many software programs are used to perform SBVS, and since they use different algorithms, it is possible to obtain different results from different software using the same input. In the last decade, a new technique of SBVS called consensus virtual screening (CVS) has been used in some studies to increase the accuracy of SBVS and to reduce the false positives obtained in these experiments. An indispensable condition to be able to utilize SBVS is the availability of a 3D structure of the target protein. Some virtual databases, such as the Protein Data Bank, have been created to store the 3D structures of molecules. However, sometimes it is not possible to experimentally obtain the 3D structure. In this situation, the homology modeling methodology allows the prediction of the 3D structure of a protein from its amino acid sequence. This review presents an overview of the challenges involved in the use of CADD to perform SBVS, the areas where CADD tools support SBVS, a comparison between the most commonly used tools, and the techniques currently used in an attempt to reduce the time and cost in the drug development process. Finally, the final considerations demonstrate the importance of using SBVS in the drug development process.","",""
5,"Ashraf Ahmed, S. Elkatatny, Hany Gamal, A. Abdulraheem","Artificial Intelligence Models for Real-Time Bulk Density Prediction of Vertical Complex Lithology Using the Drilling Parameters",2021,"","","","",133,"2022-07-13 09:34:35","","10.1007/S13369-021-05537-3","","",,,,,5,5.00,1,4,1,"","",""
44,"Chiara Longoni, Luca Cian","Artificial Intelligence in Utilitarian vs. Hedonic Contexts: The “Word-of-Machine” Effect",2020,"","","","",134,"2022-07-13 09:34:35","","10.1177/0022242920957347","","",,,,,44,22.00,22,2,2,"Rapid development and adoption of AI, machine learning, and natural language processing applications challenge managers and policy makers to harness these transformative technologies. In this context, the authors provide evidence of a novel “word-of-machine” effect, the phenomenon by which utilitarian/hedonic attribute trade-offs determine preference for, or resistance to, AI-based recommendations compared with traditional word of mouth, or human-based recommendations. The word-of-machine effect stems from a lay belief that AI recommenders are more competent than human recommenders in the utilitarian realm and less competent than human recommenders in the hedonic realm. As a consequence, importance or salience of utilitarian attributes determine preference for AI recommenders over human ones, and importance or salience of hedonic attributes determine resistance to AI recommenders over human ones (Studies 1–4). The word-of machine effect is robust to attribute complexity, number of options considered, and transaction costs. The word-of-machine effect reverses for utilitarian goals if a recommendation needs matching to a person’s unique preferences (Study 5) and is eliminated in the case of human–AI hybrid decision making (i.e., augmented rather than artificial intelligence; Study 6). An intervention based on the consider-the-opposite protocol attenuates the word-of-machine effect (Studies 7a–b).","",""
29,"Grayson W. Armstrong, A. Lorch","A(eye): A Review of Current Applications of Artificial Intelligence and Machine Learning in Ophthalmology",2019,"","","","",135,"2022-07-13 09:34:35","","10.1097/IIO.0000000000000298","","",,,,,29,9.67,15,2,3,"Artificial intelligence (AI) is a branch of computer science that aims to enable computers to perform human-like tasks. Although AI is a broad discipline, machine learning is a specific branch of AI that uses computer algorithms capable of “learning” through the simulation of human intelligence. Machine learning algorithms have been applied to the medical field since the 1970s,1 and since that time have proven useful in computerassisted diagnosis, screening, and prognostication of disease.2–7 Ophthalmology is uniquely capable of capitalizing on the promise of AI. Ophthalmologists, during routine clinical encounters, generate robust data sources capable of supporting machine learning algorithms including multimodal ophthalmic images and quantifiable metrics such as visual acuity (VA), intraocular pressure, and cup to disk ratio. To date, AI techniques have been applied to ophthalmology to screen for and diagnose diseases, such as diabetic retinopathy (DR), age-related macular degeneration (AMD), macular edema (ME), glaucoma, keratoconus, postlaserassisted in situ keratomileusis corneal ectasia, retinopathy of prematurity (ROP), and cataracts, as well as predict the prognosis of various ophthalmic diseases. Advances in ophthalmology-specific AI stand to increase patient access to clinical screening and diagnosis as well as decrease health care costs, especially when applied to high-risk populations, low-resource communities, or when combined with telemedicine initiatives. This review provides an introduction to AI andmachine learning, as well as an overview of current applications in the field of ophthalmology.","",""
0,"Xiaohong W. Gao, B. Braden","Artificial intelligence in endoscopy: The challenges and future directions",2021,"","","","",136,"2022-07-13 09:34:35","","10.37126/aige.v2.i4.117","","",,,,,0,0.00,0,2,1,"Artificial intelligence based approaches, in particular deep learning, have achieved state-of-the-art performance in medical fields with increasing number of software systems being approved by both Europe and United States. This paper reviews their applications to early detection of oesophageal cancers with a focus on their advantages and pitfalls. The paper concludes with future recommendations towards the development of a real-time, clinical implementable, interpretable and robust diagnosis support systems.","",""
45,"Tom Kamiel Magda Vercauteren, M. Unberath, N. Padoy, N. Navab","CAI4CAI: The Rise of Contextual Artificial Intelligence in Computer-Assisted Interventions",2019,"","","","",137,"2022-07-13 09:34:35","","10.1109/JPROC.2019.2946993","","",,,,,45,15.00,11,4,3,"Data-driven computational approaches have evolved to enable extraction of information from medical images with reliability, accuracy, and speed, which is already transforming their interpretation and exploitation in clinical practice. While similar benefits are longed for in the field of interventional imaging, this ambition is challenged by a much higher heterogeneity. Clinical workflows within interventional suites and operating theaters are extremely complex and typically rely on poorly integrated intraoperative devices, sensors, and support infrastructures. Taking stock of some of the most exciting developments in machine learning and artificial intelligence for computer-assisted interventions, we highlight the crucial need to take the context and human factors into account in order to address these challenges. Contextual artificial intelligence for computer-assisted intervention (CAI4CAI) arises as an emerging opportunity feeding into the broader field of surgical data science. Central challenges being addressed in CAI4CAI include how to integrate the ensemble of prior knowledge and instantaneous sensory information from experts, sensors, and actuators; how to create and communicate a faithful and actionable shared representation of the surgery among a mixed human–AI actor team; and how to design interventional systems and associated cognitive shared control schemes for online uncertainty-aware collaborative decision-making ultimately producing more precise and reliable interventions.","",""
39,"Aditya Chidepatil, Prabhleen Bindra, Devyani Kulkarni, Mustafa Qazi, Meghana Kshirsagar, K. Sankaran","From Trash to Cash: How Blockchain and Multi-Sensor-Driven Artificial Intelligence Can Transform Circular Economy of Plastic Waste?",2020,"","","","",138,"2022-07-13 09:34:35","","10.3390/admsci10020023","","",,,,,39,19.50,7,6,2,"Virgin polymers based on petrochemical feedstock are mainly preferred by most plastic goods manufacturers instead of recycled plastic feedstock. Major reason for this is the lack of reliable information about the quality, suitability, and availability of recycled plastics, which is partly due to lack of proper segregation techniques. In this paper, we present our ongoing efforts to segregate plastics based on its types and improve the reliability of information about recycled plastics using the first-of-its-kind blockchain smart contracts powered by multi-sensor data-fusion algorithms using artificial intelligence. We have demonstrated how different data-fusion modes can be employed to retrieve various physico-chemical parameters of plastic waste for accurate segregation. We have discussed how these smart tools help in efficiently segregating commingled plastics and can be reliably used in the circular economy of plastic. Using these tools, segregators, recyclers, and manufacturers can reliably share data, plan the supply chain, execute purchase orders, and hence, finally increase the use of recycled plastic feedstock.","",""
20,"L. McCoy, Sujay Nagaraj, F. Morgado, V. Harish, Sunit Das, L. Celi","What do medical students actually need to know about artificial intelligence?",2020,"","","","",139,"2022-07-13 09:34:35","","10.1038/s41746-020-0294-7","","",,,,,20,10.00,3,6,2,"","",""
33,"S. Ali, B. Choi","State-of-the-Art Artificial Intelligence Techniques for Distributed Smart Grids: A Review",2020,"","","","",140,"2022-07-13 09:34:35","","10.3390/electronics9061030","","",,,,,33,16.50,17,2,2,"The power system worldwide is going through a revolutionary transformation due to the integration with various distributed components, including advanced metering infrastructure, communication infrastructure, distributed energy resources, and electric vehicles, to improve the reliability, energy efficiency, management, and security of the future power system. These components are becoming more tightly integrated with IoT. They are expected to generate a vast amount of data to support various applications in the smart grid, such as distributed energy management, generation forecasting, grid health monitoring, fault detection, home energy management, etc. With these new components and information, artificial intelligence techniques can be applied to automate and further improve the performance of the smart grid. In this paper, we provide a comprehensive review of the state-of-the-art artificial intelligence techniques to support various applications in a distributed smart grid. In particular, we discuss how artificial techniques are applied to support the integration of renewable energy resources, the integration of energy storage systems, demand response, management of the grid and home energy, and security. As the smart grid involves various actors, such as energy produces, markets, and consumers, we also discuss how artificial intelligence and market liberalization can potentially help to increase the overall social welfare of the grid. Finally, we provide further research challenges for large-scale integration and orchestration of automated distributed devices to realize a truly smart grid.","",""
28,"H. Alami, L. Rivard, P. Lehoux, S. Hoffman, Stephanie B. M. Cadeddu, Mathilde Savoldelli, M. A. Samri, M. A. Ag Ahmed, R. Fleet, J. Fortin","Artificial intelligence in health care: laying the Foundation for Responsible, sustainable, and inclusive innovation in low- and middle-income countries",2020,"","","","",141,"2022-07-13 09:34:35","","10.1186/s12992-020-00584-1","","",,,,,28,14.00,3,10,2,"","",""
25,"D. Schiff","Out of the laboratory and into the classroom: the future of artificial intelligence in education",2020,"","","","",142,"2022-07-13 09:34:35","","10.1007/s00146-020-01033-8","","",,,,,25,12.50,25,1,2,"","",""
1,"Ke Zhang, Peidong Xu, Tianlu Gao, Jun Zhang","A Trustworthy Framework of Artificial Intelligence for Power Grid Dispatching Systems",2021,"","","","",143,"2022-07-13 09:34:35","","10.1109/DTPI52967.2021.9540198","","",,,,,1,1.00,0,4,1,"With the widespread application of artificial intelligence (AI) technologies in power systems, the properties of lack of reliability and transparency for AI technologies have revealed gradually. Here, how to build a trustworthy-AI framework based on the power system is the focus. Due to the multidimensional and heterogeneous information of power grid data, the heterogeneous graph attention network (HGAT) model of power grid dispatching is established, and the corresponding explainer (HGAT-Explainer) for the model of power equipment faults is proposed to provide more favorable support for the trustworthy-AI systems.","",""
1,"Astha Jain, R. Krishnan, Ashwini Rogye, S. Natarajan","Use of offline artificial intelligence in a smartphone-based fundus camera for community screening of diabetic retinopathy",2021,"","","","",144,"2022-07-13 09:34:35","","10.4103/ijo.IJO_3808_20","","",,,,,1,1.00,0,4,1,"Purpose: The aim of the study was to analyse the reliability of an offline artificial intelligence (AI) algorithm for community screening of diabetic retinopathy. Methods: A total of 1378 patients with diabetes visiting public dispensaries under the administration of the Municipal Corporation of Greater Mumbai between August 2018 and September 2019 were enrolled for the study. Fundus images were captured by non-specialist operators using a smartphone-based camera covering the posterior pole, including the disc and macula, and the nasal and temporal fields. The offline AI algorithm on the smartphone marked the images as referable diabetic retinopathy (RDR) or non-RDR, which were then compared against the grading by two vitreoretinal surgeons to derive upon the sensitivity and specificity of the algorithm. Results: Out of 1378 patients, gradable fundus images were obtained and analysed for 1294 patients. The sensitivity and specificity of diagnosing RDR were 100% (95% CI: 94.72–100.00%) and 89.55% (95% CI: 87.76–91.16%), respectively; the same values for any diabetic retinopathy (DR) were 89.13% (95% CI: 82.71–93.79%) and 94.43% (95% CI: 91.89–94.74%), respectively, with no false-negative results. Conclusion: The robustness of the offline AI algorithm was established in this study making it a reliable tool for community-based DR screening.","",""
2,"P. W. Grimm, Maura R. Grossman, G. Cormack","Artificial Intelligence as Evidence",2021,"","","","",145,"2022-07-13 09:34:35","","","","",,,,,2,2.00,1,3,1,"This article explores issues that govern the admissibility of Artificial Intelligence (“AI”) applications in civil and criminal cases, from the perspective of a federal trial judge and two computer scientists, one of whom also is an experienced attorney. It provides a detailed yet intelligible discussion of what AI is and how it works, a history of its development, and a description of the wide variety of functions that it is designed to accomplish, stressing that AI applications are ubiquitous, both in the private and public sectors. Applications today include: health care, education, employment-related decision-making, finance, law enforcement, and the legal profession. The article underscores the importance of determining the validity of an AI application (i.e., how accurately the AI measures, classifies, or predicts what it is designed to), as well as its reliability (i.e., the consistency with which the AI produces accurate results when applied to the same or substantially similar circumstances), in deciding whether it should be admitted into evidence in civil and criminal cases. The article further discusses factors that can affect the validity and reliability of AI evidence, including bias of various types, “function creep,” lack of transparency and explainability, and the sufficiency of the objective testing of AI applications before they are released for public use. The article next provides an in-depth discussion of the evidentiary principles that govern whether AI evidence should be admitted in court cases, a topic which, at present, is not the subject of comprehensive analysis in decisional law. The focus of this discussion is on providing a step-by-step analysis of the most important issues, and the factors that affect decisions on whether to admit AI evidence. Finally, the article concludes with a discussion of practical suggestions intended to assist lawyers and judges as they are called upon to introduce, object to, or decide on whether to admit AI evidence. 1 Hon. Paul W. Grimm is a United States District Judge for the District of Maryland, and an adjunct professor at both the University of Maryland Carey School of Law and the University of Baltimore School of Law. Maura R. Grossman, J.D., Ph.D., is a Research Professor, and Gordon V. Cormack, Ph.D., is a Professor, in the David R. Cheriton School of Computer Science at the University of Waterloo. Professor Grossman is also an affiliate faculty member at the Vector Institute for Artificial Intelligence. Her work is funded, in part, by the National Sciences and Engineering Council of Canada (“NESERC”). The opinions expressed in this article are the authors’ own, and do not necessarily reflect the views of the institutions or organizations with which they are affiliated. NORTHWESTERN JOURNAL OF TECHNOLOGY AND INTELLECTUAL PROPERTY 10 INTRODUCTION .............................................................................................................. 10 I. WHAT IS “ARTIFICIAL INTELLIGENCE”? .................................................................... 14 II. WHY AI HAS COME TO THE FOREFRONT TODAY ...................................................... 17 III. THE AI TECHNOLOGY LANDSCAPE .......................................................................... 24 IV. USES OF AI IN BUSINESS AND LAW TODAY .............................................................. 32 V. ISSUES RAISED BY THE USE OF AI IN BUSINESS AND LAW TODAY ............................ 41 A. Bias ............................................................................................................... 42 B. Lack of Robust Testing for Validity and Reliability ....................................... 48 C. Failure to Monitor for Function Creep ......................................................... 51 D. Failure to Ensure Data Privacy and Data Protection .................................. 53 E. Lack of Transparency and Explainabilty ....................................................... 60 F. Lack of Accountability ................................................................................... 65 G. Lack of Resilience ......................................................................................... 72 VI. ESTABLISHING VALIDITY AND RELIABILITY ........................................................... 79 A. Testimony, Expert Testimony, or Technology? .............................................. 79 B. Benchmarks and Goodhart’s Law ................................................................. 82 VII. EVIDENTIARY PRINCIPLES THAT SHOULD BE CONSIDERED IN EVALUATING THE ADMISSIBILITY OF AI EVIDENCE IN CIVIL AND CRIMINAL TRIALS .................... 84 A. Adequacy of the Federal Rules of Evidence in Addressing the Admissibility of AI Evidence ......................................................................... 84 B. Relevance ...................................................................................................... 86 C. Authentication of AI Evidence ....................................................................... 90 D. Usefulness of the Daubert Factors in Determining Whether to Admit AI Evidence ....................................................................................................... 95 E. Practice Pointers for Lawyers and Judges .................................................... 97 CONCLUSION ............................................................................................................... 105","",""
0,"Jianxin Cheng, Haoming Luo, Wenyi Lin, Guopeng Hu","Pros and Cons of Artificial Intelligence—Lessons From E-Government Services in the COVID-19 Pandemic",2021,"","","","",146,"2022-07-13 09:34:35","","10.1109/ICAIE53562.2021.00042","","",,,,,0,0.00,0,4,1,"How to understand the role and impact of information technology and artificial intelligence has triggered a big debate. To explore the pros and cons of artificial intelligence and its applications, this article takes the face mask distribution programs in the COVID-19 pandemic as research objects, conducting a multi-case comparative study of three cities in China. By manual coding of a total of 4560 We Chat official account messages, and by analyzing information related to the distribution process, it was found that: (1) On the demand side, the task complexity, the demand diversity, and the unstructured decision-making process in the public health emergency have exposed some limitations of AI in data collecting and unstructured problem-solving. (2) On the supply side, the procedural and substantive rules designed, together with the reliability of an AI system, will shape the performance of the AI service channel. (3) Though AI and other new technologies are advancing drastically in the pandemic, there is still much room for improvement whether by the optimization of AI systems, or by political control and social participation, and by the supplement of alternative channels such as the community service delivery.","",""
0,"R. Mohanasundaram","Editorial: Special Section on Human-Centered Artificial Intelligence with Big Data Applications",2021,"","","","",147,"2022-07-13 09:34:35","","10.1520/JTE20219999","","",,,,,0,0.00,0,1,1,"In recent years, human-centered artificial intelligence (AI) has become the most promising research domain in both industrial and academic areas worldwide. AI is the next step on the journey from big data to full automation. Human needs are the motivation behind improvements in computing paradigms. In the aforementioned areas, system-generated information such as smart devices, sensors, agents, and meters—as well as human-generated information such as texts, photos, and videos—lead to a tremendous amount of data while new levels of security, performance, and reliability are required. This Special Section aims to highlight the unique areas of human-centered AI with big data applications and various innovations in multidiscipline areas, while also presenting technical evidence and its countermeasure. This Special Section aims to identify the emerging artificial intelligence with big data in all human-centered (HC) related areas. It consists of up-to-date, state-of-the-art research contributions with novel designs and developments of intelligent application, perception, and security methods in human-centered AI, to enhance the reliability and feasibility of HC in real-world applications. The first three papers by Zhang et al., Huang and Liu, and Qing et al. deal with performance and effect analysis of China's financial venture capital development, multimedia-assisted children’s tennis skills, and agglomeration in the middle reach of the Yangtze River. Shree et al. propose a new fusion-based agricultural synthetic aperture radar (SAR) image despeckling by using anisotropic diffusion and discrete wavelet transform method. SAR images have applications in various fields. Speckle noise, which has the characteristic of multiplicative noise, degrades the image quality of SAR images, which causes information loss. This study proposes a speckle noise reduction algorithm while using the speckle reducing anisotropic diffusion filter, discrete wavelet transform to remove speckle noise. The papers by Ren and Cui, Zhang, and Zhang and Li concentrate on the use of multimedia technology in college English reading teaching, gymnastics teaching, and musical drama teaching. They show that multimedia technology has a positive influence on college education, as it promotes scientific, advanced, and vivid development of college physical education. However, there are still problems in the application of multimedia technology in college physical education; for example, the problem in the links between multimedia teaching and traditional teaching and in the great influence of courseware on teaching effects. So it is necessary to accelerate multimedia technology development, strengthen the application of multimedia technology in college education, achieve proper cooperation between traditional and multimedia teaching, and enrich multimedia courseware and its effect. Yao et al. review the general application of multimedia technology in teaching innovation. Li et al. propose a design and implementation of multimedia technology-assisted English vocabulary teaching courseware for industrial engineering majors. Zhang et al. deal with the development and experimental research of multimedia cai courseware for hurdle running. Jena et al. focus on the thermo-mechanical characterization of rice husk filled carbon-reinforced hybrid polymer composites. Rice husk (RH) is a natural sheath that forms around rice grains during their growth. As a type of natural fiber obtained from agro-industrial waste, RH can be used as filler in composites materials in various polymer matrices. Wu addresses the asymmetric impact of inflation in financial development. This study analyzes the asymmetric effects of financial development on economic growth using a model augmented with inflation and asymmetries to inform model specification. The appropriate policies that favor low inflation and reduced expansion of feasibly reformed financial institutions, capital accumulation, and increased resource mobilization should be instituted if real growth is to positively happen. Ouyang et al., Priyadharshini et al., Krithika and Subramani, Gomathi et al., and Thangavel et al. deal with industrial development, such as the study on damage tests based on structure and operating parameters of wire ropes used by conveyors in orchards; development of intelligent smart metering system through remote monitoring and control under robust conditions; neural network-based drive cycle analysis for parallel hybrid electric vehicle; design fabrication and performance analysis of intelligent mesoscale capacitive accelerometer for vibration measurement; and dynamic modeling and control analysis of industrial electro-mechanical servo positioning system using machine learning technique. Pratheep et al. focuses on the genetic algorithm–based robust controller for an inverted pendulum using model order reduction. This paper considered proportional-integral optimized with a genetic algorithm controller on the inverted pendulum for the control of the angle position. The obtained results show that the GA-based PID controller confirms the enhanced performance indexes by holding minimum settling time and peak overshoot on comparing with the conventional PID controller. Tao et al. propose the existence of k-people stable alliance in n-player cooperative games. This paper considers the existence of a stable k-cooperative alliance with a nonempty core in an n-person cooperative game on the premise that the Nash negotiation solution is the distribution criterion. Also, this article provides sufficient conditions for the benefits of all players in a k-man alliance to lie in its internal sub alliance.","",""
0,"Ashwaq N. Hassan, S. Al-Chlaihawi, Ahlam R. Khekan","Artificial intelligence techniques over the fifth generation mobile networks",2021,"","","","",148,"2022-07-13 09:34:35","","","","",,,,,0,0.00,0,3,1,"Received Jul 15, 2021 Revised Aug 30, 2021 Accepted Sep 2, 2021 A well fifth generation (5G) mobile networks have been a common phrase in recent years. So, 2025 the number of devices based on 5G will reach about 100 billion, about 2.5 billion users are expected to consume more than a gigabyte (GB) of data per month. 5G will play important roles in new areas, from smart cities and mobile augmented reality, and 4,000 pixels (4K) video streaming. Bandwidth higher than the fourth generation (4G), more reliability and less latency are some of the features that distinguish this generation from previous generations. These features are impressive to a mobile network, but will pose serious challenges for operators and communications companies and will lead to complexity. Managing this network, preventing errors and minimizing latency are some of the challenges that 5G of mobile networks will bring. So, the use of artificial intelligence and machine learning is a good way to solve these challenges. In this paper, we will review the artificial intelligence techniques used in communications networks. Creating a robust and efficient communications network using artificial intelligence techniques is an incentive for future research. The importance of this issue is such that sixth generation (6G) of cellular communications. So, much emphasis on the use of artificial intelligence.","",""
0,"Huihui Liang, Chao Tang, Hai Wang, Bo Pang, Linlin Yuan, He Cai","Research on the Security Situation Awareness Method of Ubiquitous Power Internet of Things Network Based on Artificial Intelligence",2021,"","","","",149,"2022-07-13 09:34:35","","10.1109/AINIT54228.2021.00029","","",,,,,0,0.00,0,6,1,"In order to accurately determine the security situation awareness of the power system IoT network, this paper proposes an artificial intelligence-based security situation awareness[1] method, which abstracts the perception problem so that it can be applied to actual scenarios. Input the records of the actual field monitoring equipment as the data source to the classifier to obtain the perceptual results, and then use the processed results as the training input of the RBF neural network[2] to find out the mapping relationship with the network situation value, thereby quantifying the system’s performance Security posture. Finally, the KDD Cup99 data set[3] can be compared with the attack data of the power information network to verify the reliability of the proposed method in the network security situation analysis.","",""
0,"Q. Lu, Ling Wei, Wenwen He, Keke Zhang, Jinrui Wang, Yinglei Zhang, X. Rong, Zhennan Zhao, Lei Cai, Xixi He, Jun Wu, Dayong Ding, Yi Lu, Xiangjia Zhu","Lens Opacities Classification System III–based artificial intelligence program for automatic cataract grading",2021,"","","","",150,"2022-07-13 09:34:35","","10.1097/j.jcrs.0000000000000790","","",,,,,0,0.00,0,14,1,"An artificial intelligence-based and LOCS III-based automatic grading system of nuclear and cortical cataract was established, showing satisfactory grading reliability and referral capability. Purpose: To establish and validate an artificial intelligence (AI)-assisted automatic cataract grading program based on the Lens Opacities Classification System III (LOCS III). Setting: Eye and Ear, Nose, and Throat Hospital, Fudan University, Shanghai, China. Design: AI training. Methods: Advanced deep-learning algorithms, including Faster R-CNN and ResNet, were applied to the localization and analysis of the region of interest. An internal dataset from the EENT Hospital of Fudan University and an external dataset from the Pujiang Eye Study were used for AI training, validation, and testing. The datasets were automatically labeled on the AI platform regarding the capture mode and cataract grading based on the LOCS III. Results: The AI program showed reliable capture mode recognition, grading, and referral capability for nuclear and cortical cataract grading. In the internal and external datasets, 99.4% and 100% of automatic nuclear grading, respectively, had an absolute prediction error of ≤1.0, with a satisfactory referral capability (area under the curve [AUC]: 0.983 for the internal dataset; 0.977 for the external dataset); 75.0% (internal dataset) and 93.5% (external dataset) of the automatic cortical grades had an absolute prediction error of ≤1.0, with AUCs of 0.855 and 0.795 for referral, respectively. Good consistency was observed between automatic and manual grading when both nuclear and cortical cataracts were evaluated. However, automatic grading of posterior subcapsular cataracts was impractical. Conclusions: The AI program proposed in this study showed robust grading and diagnostic performance for both nuclear and cortical cataracts, based on LOCS III.","",""
0,"Rajole Meghana Bhausaheb","Speed Control of SRM for Hybrid Electric Vehicle Using Artificial Intelligence",2021,"","","","",151,"2022-07-13 09:34:35","","10.1109/ICCCNT51525.2021.9579857","","",,,,,0,0.00,0,1,1,"AI for switched reluctance motor (SRM) drive with integration of front end circuit is appealing for electric vehicle. As SRM carries the highlights like simple construction, high reliability, high fault tolerance capability and low production cost. However, the high torque ripples, running vibrations and acoustic noise are the major drawbacks in SRM. In proposed theory of an Artificial Intelligence for SRM drive overcome this drawback and flip it into advantages like high torque range, low torque ripple and vibration free response with flexible speed control. This paper represents an operating theory of AI for SRM drive. A hybrid ANN Controller is proposed in order to control the speed of the SRM motor. This paper mainly focuses on the comparison of the hybrid ANN controller with conventional PID and to prove the proposed controller provides the best performance and high robustness compared to a conventional PID controller alone. MATLAB/ Simulink are used to simulate.","",""
26,"Young-Hoon Cho, J. Kwon, Kyung-Hee Kim, J. Medina-Inojosa, K. Jeon, S. Cho, Soo Youn Lee, Jinsik Park, B. Oh","Artificial intelligence algorithm for detecting myocardial infarction using six-lead electrocardiography",2020,"","","","",152,"2022-07-13 09:34:35","","10.1038/s41598-020-77599-6","","",,,,,26,13.00,3,9,2,"","",""
36,"Jens Christian Bjerring, Jacob Busch","Artificial Intelligence and Patient-Centered Decision-Making",2020,"","","","",153,"2022-07-13 09:34:35","","10.1007/s13347-019-00391-6","","",,,,,36,18.00,18,2,2,"","",""
31,"M. Hasan, M. Ahmed, A. Hashim, M. Razzaque, S. Islam, B. Pandey","A Novel Artificial Intelligence Based Timing Synchronization Scheme for Smart Grid Applications",2020,"","","","",154,"2022-07-13 09:34:35","","10.1007/s11277-020-07408-w","","",,,,,31,15.50,5,6,2,"","",""
2,"Sharmila S. Gaikwad","Study on Artificial Intelligence in Healthcare",2021,"","","","",155,"2022-07-13 09:34:35","","10.1109/ICACCS51430.2021.9441741","","",,,,,2,2.00,2,1,1,"We have seen during this pandemic, India, there are many dangers to the healthcare system which calls for the need to encourage hospitals o switch an electronic healthcare information system from paper-based medical records to an electronic medical related data that offers e-healthcare information and integrates well with computer-based judgment making systems. With the growing interest in AI centered robust information technology (IT) infrastructure is critical in addressing such national issues as the need to improve the protection and quality of health care, as well as the rising health care costs in the health sector. Network technology and communication are presently noticeable improvements in information technology and to a great extent affect the medical services. The necessities of both patients as well as suppliers of healthcare/Medicare facilities shall be addressed. An AI enabled hospital or medical system is essential, challenging, and achievable. This paper focuses on the Medical, hospital related uses of IT, such as better clinical decision-making, imaging and history taking, reduced duplication of diagnostics, better reliability of medication, increased utilization of health facilities and preventive health initiatives.","",""
0,"Lucas Mendes Lima, Victor Calebe Cavalcante, Mariana Guimarães de Sousa, Cláudio Afonso Fleury, D. Oliveira, Eduardo Noronha de Andrade Freitas","Artificial Intelligence in Support of Welfare Monitoring of Dairy Cattle: A Systematic Literature Review",2021,"","","","",156,"2022-07-13 09:34:35","","10.1109/CSCI54926.2021.00324","","",,,,,0,0.00,0,6,1,"Context: Although agribusiness corresponded to more than 20% of Brazil’s Gross Domestic Product (GDP), most livestock is under manual control and manual monitoring. Additionally, alternative technologies are either uncomfortable and stressful, or expensive. Now, despite the great scientific advances in the area, there is still a pressing need for an automated robust, inexpensive and (sub)optimal technology to monitor animal behavior in a cost-effective, contact-less and stress-free fashion. Overall, this niche can leverage the benefits of Deep Learning schemes.Objective: This review aims to provide a systematic overview of most current projects in the area of comfort monitoring dairy cattle, as well as their corresponding image recognition-based techniques and technologies.Methods: First, a systematic review planning was carried out, and objectives, research questions, search strings, among others, were defined. Subsequently,a broad survey was conducted to extract, analyze and compile the data, to generate a easy-to-read visual source of information (tables and graphics).Results: Information was extracted from the reviewed papers. Among this data collected from the papers are techniques utilized, target behaviors, cow bodyparts identified in visual computational, besides their paper source font, the publication date, and localization. For example, the papers present are mostly recent. China has had a larger number of relevant papers in the area. The back was the body region most analyzed by the papers and the behaviors most analyzed were body condition score, lameness, cow’s body position and feeding/drinking behavior. Among the methods used is RCNN Inception V3 with the best accuracy for cow’s back region.Conclusion: The aim of this work is to present some of the papers that are being carried out in the area of dairy cow behavior monitoring, using techniques of Artifical Intelligence. It is expected that the information collected and presented in the present systematic review paper contribute to the future researches and projects of the area and the application of new techniques.","",""
0,"Keeley A. Crockett, Edwin Colyer, A. Latham","The Ethical Landscape of Data and Artificial Intelligence: Citizen Perspectives",2021,"","","","",157,"2022-07-13 09:34:35","","10.1109/SSCI50451.2021.9660153","","",,,,,0,0.00,0,3,1,"Globally, there is growing acknowledgement that those involved in the development and deployment of AI products and services should act responsibly and conduct their work within robust ethical frameworks. Many of the ethical guidelines now published highlight a requirement for citizens to have greater voice and involvement in this process and to hold actors to account regarding compliance and the impacts of their AI innovations. For citizens to participate in co-creation activities they need to be representative of the diverse communities of society and have an appropriate level of understanding of basic AI concepts. This paper presents the preliminary results of a longitudinal survey designed to capture citizen perspectives of the ethical landscape of data and AI. Forty participants were asked to participate in a survey and results were analyzed based on gender, age range and educational attainment. Results have shown that participant perception of AI, trust, bias and fairness is different but related to specific AI applications, and the context in which is applied. Citizens also are also very receptive to undertaking free courses/workshops on a wide range of AI concepts, ranging from family workshops to work-based training.","",""
0,"Shuchita Gupta, Ben A. Amaba, M. McMahon, Kunal Gupta","The Evolution of Artificial Intelligence in the Automotive Industry",2021,"","","","",158,"2022-07-13 09:34:35","","10.1109/rams48097.2021.9605795","","",,,,,0,0.00,0,4,1,"For many companies and institutions, the supply chain model for reliability & maintainability (R&M) of systems can either act as their competitive advantage or weakest link. Big data and analytics play an instrumental role within the supply chain, especially for companies using R&M principles, where these practices were born and matured. The automotive industry has led many initiatives to increase the adoption of such supply chain architectures within their vehicle ecosystems allowing big data and analytics to play the driving role to insure the performance of the vehicles. The automotive industry continues to use these practices to deal with disruptions within autonomous driving, self-healing efficient batteries, robotics, insurance risk assessments, and exceptional customer experiences. Whether it be automotive manufacturers, dealers, drivers, or insurance companies, R&M have had positive results. Today, however, another technology is surfacing fast, changing the face of R&M standard practices and applications. Artificial Intelligence (AI) is impacting and changing the entire automotive industry ecosystem. While advancements in chipsets, edge technology, 5th generation mobile network (5G), Internet of Things (IoT) and cloud have been acting as enablers, data and AI is at the heart of R&M future advances. As per VynZ Research, Global Artificial Intelligence (AI) Market for Automotive and Transportation Industry is Set to Reach USD 45.1 billion by 2024, Observing a CAGR of 17.7% during 2019–2024 [1].In this paper, we will share the impact of AI & trust on the auto industries R&M programs, broken down across four pillars: in-vehicle experience, connected vehicles, auto manufacturing, and autonomous vehicle with examples, and use-cases.","",""
43,"Stuart J. Russell, Thomas G. Dietterich, Eric Horvitz, B. Selman, F. Rossi, D. Hassabis, S. Legg, Mustafa Suleyman, D. George, D. Phoenix","Letter to the Editor: Research Priorities for Robust and Beneficial Artificial Intelligence: An Open Letter",2015,"","","","",159,"2022-07-13 09:34:35","","10.1609/aimag.v36i4.2621","","",,,,,43,6.14,4,10,7,"Artificial intelligence (AI) research has explored a variety of problems and approaches since its inception, but for the last 20 years or so has been focused on the problems surrounding the construction of intelligent agents — systems that perceive and act in some environment. In this context, ""intelligence"" is related to statistical and economic notions of rationality — colloquially, the ability to make good decisions, plans, or inferences. The adoption of probabilistic and decision-theoretic representations and statistical learning methods has led to a large degree of integration and cross-fertilization among AI, machine learning, statistics, control theory, neuroscience, and other fields. The establishment of shared theoretical frameworks, combined with the availability of data and processing power, has yielded remarkable successes in various component tasks such as speech recognition, image classification, autonomous vehicles, machine translation, legged locomotion, and question-answering systems. As capabilities in these areas and others cross the threshold from laboratory research to economically valuable technologies, a virtuous cycle takes hold whereby even small improvements in performance are worth large sums of money, prompting greater investments in research. There is now a broad consensus that AI research is progressing steadily, and that its impact on society is likely to increase. The potential benefits are huge, since everything that civilization has to offer is a product of human intelligence; we cannot predict what we might achieve when this intelligence is magnified by the tools AI may provide, but the eradication of disease and poverty are not unfathomable. Because of the great potential of AI, it is important to research how to reap its benefits while avoiding potential pitfalls. The progress in AI research makes it timely to focus research not only on making AI more capable, but also on maximizing the societal benefit of AI. Such considerations motivated the AAAI 2008–09 Presidential Panel on Long-Term AI Futures and other projects on AI impacts, and constitute a significant expansion of the field of AI itself, which up to now has focused largely on techniques that are neutral with respect to purpose. We recommend expanded research aimed at ensuring that increasingly capable AI systems are robust and beneficial: our AI systems must do what we want them to do. The attached research priorities document [see page X] gives many examples of such research directions that can help maximize the societal benefit of AI. This research is by necessity interdisciplinary, because it involves both society and AI. It ranges from economics, law and philosophy to computer security, formal methods and, of course, various branches of AI itself. In summary, we believe that research on how to make AI systems robust and beneficial is both important and timely, and that there are concrete research directions that can be pursued today.","",""
15,"S. Athey, K. Bryan, J. Gans","The Allocation of Decision Authority to Human and Artificial Intelligence",2020,"","","","",160,"2022-07-13 09:34:35","","10.2139/ssrn.3517287","","",,,,,15,7.50,5,3,2,"The allocation of decision authority by a principal to either a human agent or an artificial intelligence (AI) is examined. The principal trades off an AI's more aligned choice with the need to motivate the human agent to expend effort in learning choice payoffs. When agent effort is desired, it is shown that the principal is more likely to give that agent decision authority, reduce investment in AI reliability, and adopt an AI that may be biased. Organizational design considerations are likely to have an impact on how AIs are trained.","",""
18,"Ahmed Gowida, Salaheldin Elkatatny, Saad F. K. Al-Afnan, A. Abdulraheem","New Computational Artificial Intelligence Models for Generating Synthetic Formation Bulk Density Logs While Drilling",2020,"","","","",161,"2022-07-13 09:34:35","","10.3390/su12020686","","",,,,,18,9.00,5,4,2,"Synthetic well log generation using artificial intelligence tools is a robust solution for situations in which logging data are not available or are partially lost. Formation bulk density (RHOB) logging data greatly assist in identifying downhole formations. These data are measured in the field while drilling by using a density log tool in the form of either a logging while drilling (LWD) technique or (more often) by wireline logging after the formations are drilled. This is due to operational limitations during the drilling process. Therefore, the objective of this study was to develop a predictive tool for estimating RHOB while drilling using an adaptive network-based fuzzy interference system (ANFIS), functional network (FN), and support vector machine (SVM). The proposed model uses the mechanical drilling constraints as feeding input parameters, and the conventional RHOB log data as an output parameter. These mechanical drilling parameters are usually measured while drilling, and their responses vary with different formations. A dataset of 2400 actual datapoints, obtained from a horizontal well in the Middle East, were used to build the proposed models. The obtained dataset was divided into a 70/30 ratio for model training and testing, respectively. The optimized ANFIS-based model outperformed the FN- and SVM-based models with a correlation coefficient (R) of 0.93, and average absolute percentage error (AAPE) of 0.81% between the predicted and measured RHOB values. These results demonstrate the reliability of the developed ANFIS model for predicting RHOB while drilling, based on the mechanical drilling parameters. Subsequently, the ANFIS-based model was validated using unseen data from another well within the same field. The validation process yielded an AAPE of 0.97% between the predicted and actual RHOB values, which confirmed the robustness of the developed model as an effective predictive tool for RHOB.","",""
0,"Ashwaq N. Hassan, S. Al-Chlaihawi, Ahlam R. Khekan","Artificial intelligence techniques over the fifth generation (5G) mobile networks: a review",2021,"","","","",162,"2022-07-13 09:34:35","","10.11591/IJEECS.V24.I1.PP%P","","",,,,,0,0.00,0,3,1,"A well Fifth generation (5G) mobile networks have been a common phrase in recent years. We have all heard this phrase and know its importance. By 2025, the number of devices based on the fifth generation of mobile networks will reach about 100 billion devices. By then, about 2.5 billion users are expected to consume more than a gigabyte of streamed data per month. 5G will play important roles in a variety of new areas, from smart homes and cars to smart cities, virtual reality and mobile augmented reality, and 4K video streaming. Bandwidth much higher than the fourth generation, more reliability and less latency are some of the features that distinguish this generation of mobile networks from previous generations.  Clearly, at first glance, these features may seem very impressive and useful to a mobile network, but these features will pose serious challenges for operators and communications companies. All of these features will lead to considerable complexity. Managing this network, preventing errors, and minimizing latency are some of the challenges that the 5th generation of mobile networks will bring. Therefore, the use of artificial intelligence and machine learning is a good way to solve these challenges. in other say, in such a situation, proper management of the 5G network must be done using powerful tools such as artificial intelligence. Various researches in this field are currently being carried out. Research that enables automated management and servicing and reduces human error as much as possible. In this paper, we will review the artificial intelligence techniques used in communications networks. Creating a robust and efficient communications network using artificial intelligence techniques is a great incentive for future research. The importance of this issue is such that the sixth generation (6G) of cellular communications; There is a lot of emphasis on the use of artificial intelligence.","",""
0,"Ashwaq N. Hassan, S. Al-Chlaihawi, Ahlam R. Khekan","Artificial intelligence techniques over the fifth generation mobile networks: a review",2021,"","","","",163,"2022-07-13 09:34:35","","10.11591/ijeecs.v24.i1.pp317-328","","",,,,,0,0.00,0,3,1,"A well Fifth generation (5G) mobile networks have been a common phrase in recent years. We have all heard this phrase and know its importance. By 2025, the number of devices based on the fifth generation of mobile networks will reach about 100 billion devices. By then, about 2.5 billion users are expected to consume more than a gigabyte of streamed data per month. 5G will play important roles in a variety of new areas, from smart homes and cars to smart cities, virtual reality and mobile augmented reality, and 4K video streaming. Bandwidth much higher than the fourth generation, more reliability and less latency are some of the features that distinguish this generation of mobile networks from previous generations.  Clearly, at first glance, these features may seem very impressive and useful to a mobile network, but these features will pose serious challenges for operators and communications companies. All of these features will lead to considerable complexity. Managing this network, preventing errors, and minimizing latency are some of the challenges that the 5th generation of mobile networks will bring. Therefore, the use of artificial intelligence and machine learning is a good way to solve these challenges. in other say, in such a situation, proper management of the 5G network must be done using powerful tools such as artificial intelligence. Various researches in this field are currently being carried out. Research that enables automated management and servicing and reduces human error as much as possible. In this paper, we will review the artificial intelligence techniques used in communications networks. Creating a robust and efficient communications network using artificial intelligence techniques is a great incentive for future research. The importance of this issue is such that the sixth generation (6G) of cellular communications; There is a lot of emphasis on the use of artificial intelligence.","",""
0,"S. Farsoni, S. Simani","Validation of Fault Diagnosis Techniques Based on Artificial Intelligence Tools for a Wind Turbine Benchmark",2021,"","","","",164,"2022-07-13 09:34:35","","10.1109/SysTol52990.2021.9595291","","",,,,,0,0.00,0,2,1,"The fault diagnosis of wind turbines includes extremely challenging aspects that motivate the research issues considered in this paper. In particular, this work studies fault diagnosis solutions that are considered in a viable way and used as advanced techniques for condition monitoring of dynamic processes. To this end, the work proposes the design of fault diagnosis techniques that exploits the estimation of the fault by means of data–driven approaches. To this end, the fuzzy and neural network structures are integrated with auto–regressive with exogenous input regressors, thus making them able to approximate unknown nonlinear dynamic functions with arbitrary degree of accuracy. The capabilities of fault diagnosis schemes are validated by using a simulator of a wind turbine system. Moreover, at this stage the benchmark is also useful to analyse the robustness and the reliability characteristics of the developed tools in the presence of model–reality mismatch and modelling error effects featured by the wind turbine simulator. On the other hand, a hardware–in–the–loop tool is finally implemented for testing the performance of the developed fault diagnosis strategies in a more realistic environment.","",""
1,"A. Ruospo, Ernesto Sánchez","On the Reliability Assessment of Artificial Neural Networks Running on AI-Oriented MPSoCs",2021,"","","","",165,"2022-07-13 09:34:35","","10.3390/APP11146455","","",,,,,1,1.00,1,2,1,"Nowadays, the usage of electronic devices running artificial neural networks (ANNs)-based applications is spreading in our everyday life. Due to their outstanding computational capabilities, ANNs have become appealing solutions for safety-critical systems as well. Frequently, they are considered intrinsically robust and fault tolerant for being brain-inspired and redundant computing models. However, when ANNs are deployed on resource-constrained hardware devices, single physical faults may compromise the activity of multiple neurons. Therefore, it is crucial to assess the reliability of the entire neural computing system, including both the software and the hardware components. This article systematically addresses reliability concerns for ANNs running on multiprocessor system-on-a-chips (MPSoCs). It presents a methodology to assign resilience scores to individual neurons and, based on that, schedule the workload of an ANN on the target MPSoC so that critical neurons are neatly distributed among the available processing elements. This reliability-oriented methodology exploits an integer linear programming solver to find the optimal solution. Experimental results are given for three different convolutional neural networks trained on MNIST, SVHN, and CIFAR-10. We carried out a comprehensive assessment on an open-source artificial intelligence-based RISC-V MPSoC. The results show the reliability improvements of the proposed methodology against the traditional scheduling.","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",166,"2022-07-13 09:34:35","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
16,"B. Koçak, Ece Ates Kus, O. Kilickesmez","How to read and review papers on machine learning and artificial intelligence in radiology: a survival guide to key methodological concepts",2020,"","","","",167,"2022-07-13 09:34:35","","10.1007/s00330-020-07324-4","","",,,,,16,8.00,5,3,2,"","",""
14,"G. Coskuner, Majeed S Jassim, M. Zontul, Seda Karateke","Application of artificial intelligence neural network modeling to predict the generation of domestic, commercial and construction wastes",2020,"","","","",168,"2022-07-13 09:34:35","","10.1177/0734242X20935181","","",,,,,14,7.00,4,4,2,"Reliable prediction of municipal solid waste (MSW) generation rates is a significant element of planning and implementation of sustainable solid waste management strategies. In this study, the multi-layer perceptron artificial neural network (MLP-ANN) is applied to verify the prediction of annual generation rates of domestic, commercial and construction and demolition (C&D) wastes from the year 1997 to 2016 in Askar Landfill site in the Kingdom of Bahrain. The proposed robust predictive models incorporated selected explanatory variables to reflect the influence of social, demographical, economic, geographical and touristic factors upon waste generation rates (WGRs). The Mean Squared Error (MSE) and coefficient of determination (R2) are used as performance indicators to evaluate effectiveness of the developed models. MLP-ANN models exhibited strong accuracy in predictions with high R2 and low MSE values. The R2 values for domestic, commercial and C&D wastes are 0.95, 0.99 and 0.91, respectively. Our results show that the developed MLP-ANN models are effective for the prediction of WGRs from different sources and could be considered as a cost-effective approach for planning integrated MSW management systems.","",""
11,"K. Mudgal, Neelanjan Das","The ethical adoption of artificial intelligence in radiology",2019,"","","","",169,"2022-07-13 09:34:35","","10.1259/bjro.20190020","","",,,,,11,3.67,6,2,3,"Artificial intelligence (AI) is rapidly transforming healthcare—with radiology at the pioneering forefront. To be trustfully adopted, AI needs to be lawful, ethical and robust. This article covers the different aspects of a safe and sustainable deployment of AI in radiology during: training, integration and regulation. For training, data must be appropriately valued, and deals with AI companies must be centralized. Companies must clearly define anonymization and consent, and patients must be well-informed about their data usage. Data fed into algorithms must be made AI-ready by refining, purification, digitization and centralization. Finally, data must represent various demographics. AI needs to be safely integrated with radiologists-in-the-loop: guiding forming concepts of AI solutions and supervising training and feedback. To be well-regulated, AI systems must be approved by a health authority and agreements must be made upon liability for errors, roles of supervised and unsupervised AI and fair workforce distribution (between AI and radiologists), with a renewal of policy at regular intervals. Any errors made must have a root-cause analysis, with outcomes fedback to companies to close the loop—thus enabling a dynamic best prediction system. In the distant future, AI may act autonomously with little human supervision. Ethical training and integration can ensure a ""transparent"" technology that will allow insight: helping us reflect on our current understanding of imaging interpretation and fill knowledge gaps, eventually moulding radiological practice. This article proposes recommendations for ethical practise that can guide a nationalized framework to build a sustainable and transparent system.","",""
0,"Yusen Xie, Ting Sun, Xinglong Cui, Shuixin Deng, Lei Deng, Baohua Chen","Fast-robust book information extraction system for automated intelligence library",2021,"","","","",170,"2022-07-13 09:34:35","","10.1109/AIID51893.2021.9456499","","",,,,,0,0.00,0,6,1,"At present, in the large-scale book management scene, book sorting, daily maintenance and book retrieval are very common, but the book information is complicated and the efficiency of relying on manual management is extremely poor. Although there have been many self-service book systems based on optics or vision, they are mostly based on traditional computer vision algorithms such as boundary extraction. Due to the fact that there are more artificial experience thresholds, some shortcomings such as low detection accuracy, poor robustness, and inability to systematically deploy on a large scale, which lack of insufficient intelligence. Therefore, we proposed a book information extraction algorithm based on object detection and optical character recognition (OCR) that is suitable for multiple book information recognition, multiple book image angles and multiple book postures. It can be applied to scenes such as book sorting, bookshelf management and book retrieval. The system we designed includes the classification of book covers and back covers, the classification of books upright and inverted, the detection of book pages side and spine side, the recognition of book pricing. In terms of accuracy, the classification accuracy of the front cover and the back cover is 99.9%, the upright classification accuracy of book front covers is 98.8%, the back cover reaches 99.9%, the accuracy of book price recognition get 94.5%, and the book spine/page side detection mAP reaches 99.6%; in terms of detection speed, Yolov5 detection model was improved and the statistical-based pre-pruning strategy was adopted, support by our algorithm the system reaches 2.09 FPS in book price recognition, which improves the detection speed to meet actual needs.","",""
14,"Linda W. Lee, Amir Dabirian, Ian Paul McCarthy, Jan H. Kietzmann","Making sense of text: artificial intelligence-enabled content analysis",2020,"","","","",171,"2022-07-13 09:34:35","","10.1108/ejm-02-2019-0219","","",,,,,14,7.00,4,4,2,"Purpose: The purpose of this paper is to introduce, apply and compare how artificial intelligence (AI), and specifically the IBM Watson system, can be used for content analysis in marketing research relative to manual and computer-aided (non-AI) approaches to content analysis.    Design/methodology/approach: To illustrate the use of AI-enabled content analysis, this paper examines the text of leadership speeches, content related to organizational brand. The process and results of using AI are compared to manual and computer-aided approaches by using three performance factors for content analysis: reliability, validity and efficiency.    Findings: Relative to manual and computer-aided approaches, AI-enabled content analysis provides clear advantages with high reliability, high validity and moderate efficiency.    Research limitations/implications: This paper offers three contributions. First, it highlights the continued importance of the content analysis research method, particularly with the explosive growth of natural language-based user-generated content. Second, it provides a road map of how to use AI-enabled content analysis. Third, it applies and compares AI-enabled content analysis to manual and computer-aided, using leadership speeches.    Practical implications: For each of the three approaches, nine steps are outlined and described to allow for replicability of this study. The advantages and disadvantages of using AI for content analysis are discussed. Together these are intended to motivate and guide researchers to apply and develop AI-enabled content analysis for research in marketing and other disciplines.    Originality/value: To the best of the authors' knowledge, this paper is among the first to introduce, apply and compare how AI can be used for content analysis.","",""
8,"I. Wiafe, F. N. Koranteng, Emmanuel Nyarko Obeng, Nana Assyne, Abigail Wiafe, S. Gulliver","Artificial Intelligence for Cybersecurity: A Systematic Mapping of Literature",2020,"","","","",172,"2022-07-13 09:34:35","","10.1109/ACCESS.2020.3013145","","",,,,,8,4.00,1,6,2,"Due to the ever-increasing complexities in cybercrimes, there is the need for cybersecurity methods to be more robust and intelligent. This will make defense mechanisms to be capable of making real-time decisions that can effectively respond to sophisticated attacks. To support this, both researchers and practitioners need to be familiar with current methods of ensuring cybersecurity (CyberSec). In particular, the use of artificial intelligence for combating cybercrimes. However, there is lack of summaries on artificial intelligent methods for combating cybercrimes. To address this knowledge gap, this study sampled 131 articles from two main scholarly databases (ACM digital library and IEEE Xplore). Using a systematic mapping, the articles were analyzed using quantitative and qualitative methods. It was observed that artificial intelligent methods have made remarkable contributions to combating cybercrimes with significant improvement in intrusion detection systems. It was also observed that there is a reduction in computational complexity, model training times and false alarms. However, there is a significant skewness within the domain. Most studies have focused on intrusion detection and prevention systems, and the most dominant technique used was support vector machines. The findings also revealed that majority of the studies were published in two journal outlets. It is therefore suggested that to enhance research in artificial intelligence for CyberSec, researchers need to adopt newer techniques and also publish in other related outlets.","",""
5,"Cathy O'Neil, H. Gunn","Near-Term Artificial Intelligence and the Ethical Matrix",2020,"","","","",173,"2022-07-13 09:34:35","","10.1093/oso/9780190905033.003.0009","","",,,,,5,2.50,3,2,2,"This chapter takes up the issue of near-term artificial intelligence, or the algorithms that are already in place in a variety of public and private sectors, guiding decisions from advertising and to credit ratings to sentencing in the justice system. There is a pressing need to recognize and evaluate the ways that structural racism, sexism, classism, and ableism may be embedded in and amplified by these systems. The chapter proposes a framework for ethical analysis that can be used to facilitate more robust ethical reflection in AI development and implementation. It presents an ethical matrix that incorporates the language of data science as a tool that data scientists can build themselves in order to integrate ethical analysis into the design process, addressing the need for immediate analysis and accountability over the design and deployment of near-term AI.","",""
0,"","Congestion Control in Wireless Sensor Network using Artificial Intelligence Techniques",2020,"","","","",174,"2022-07-13 09:34:35","","10.35940/ijitee.e2796.039520","","",,,,,0,0.00,0,0,2,"Now-a-days, wireless sensor network has many issues and challenges like energy-efficient, congestion control, delay, scalability, reliability, robustness, etc. Communication between the wireless sensor nodes requires minimum response delay and congestion. It also requires disclosure to be energy efficient. Many congestion control protocols are using to control the congestion and improve the energy-efficient in that particular problem. Then the WSN protocol is classified as the protocol based, wired, wireless, frequency-based, and it will give the solution to that problem efficiently. Then the artificial intelligence techniques are used in a wireless sensor network to control the congestion in the systems. However, the primary fact is that the sensor node runs out of energy quickly, and traffic (congestion) has issues in many congestion control protocols. Here, congestion control is detects by hierarchical, distribution, energy-efficient in the way of algorithm in a WSN.This paper Present a Survey on Congestion Control in wireless sensor network using artificial intelligence Techniques.","",""
496,"Christopher J. Kelly, A. Karthikesalingam, Mustafa Suleyman, Greg Corrado, Dominic King","Key challenges for delivering clinical impact with artificial intelligence",2019,"","","","",175,"2022-07-13 09:34:35","","10.1186/s12916-019-1426-2","","",,,,,496,165.33,99,5,3,"","",""
9,"A. Kevat, Anaath Kalirajah, R. Roseby","Artificial intelligence accuracy in detecting pathological breath sounds in children using digital stethoscopes",2020,"","","","",176,"2022-07-13 09:34:35","","10.1186/s12931-020-01523-9","","",,,,,9,4.50,3,3,2,"","",""
0,"Diana Mercado Sierra, Argenis Alvarez Rojas, Victor Salazar Araque","Low Salinity Water Injection Optimization in the Namorado Field Using Compositional Simulation and Artificial Intelligence",2020,"","","","",177,"2022-07-13 09:34:35","","10.2118/198995-ms","","",,,,,0,0.00,0,3,2,"  This work aims to optimize the development plan involving Low Salinity Water Injection – LSWI for the Namorado Field (located in the Campos basin in Brazil). Multiple mechanisms have been proposed in the literature to represent LSWI, mechanisms that we are going to discuss in this paper. Additionally, it will be studied the impact to evaluate multiple geostatistical realizations at the same time with multiple development scenarios (Robust Optimization) to select the optimal Net Present Value (NPV) of the project considering the reservoir uncertainties.  This methodology combines compositional simulation and a mathematical optimization tool that uses artificial intelligence to maximize the NPV of the project under evaluation. Considering that LSWI alters the initial chemical equilibrium in the reservoir and induces changes in the system, a geochemical model is included with aqueous reactions, dissolution / precipitation mineral reaction and ion exchange. The optimization project was carried out in three stages, first a multiple assisted history match, followed by a probabilistic forecast and finally, a robust optimization.  As a result, an optimum operational strategy of LSWI was obtained in which the impact of changing the type, location and number of producer and injector wells as well as the volume of injected water (High Salinity Water Injection (HSWI) vs. LSWI) are considered. On average, the optimal scenario for LSWI obtained a recovery factor of 41%, compared to 34% if only conventional water is injected. The selection and optimization of the recovery strategy considered multiple geological realizations and multiple history matched models in the field development plan, this considers the uncertainty associated with the porosity, permeability and net to gross distribution, with the objective of increasing the accuracy and reliability of the results obtained for the recovery factor and the NPV. For the optimal LSWI case and considering five different selected geological scenarios (P10 to P90), the recovery factor obtained was between 37.89% (P10) and 42.21% (P90) and the project NPV between 807 MMUSD (P10) and 3315 MMUSD (P90).  This adapted methodology allowed to obtain the optimal development plan considering the uncertainty associated in the reservoir, using multiple geostatistical realizations and multiple history matched models at the same time (usually just one history match is considered).","",""
0,"Katanosh Morovat, B. Panda","A Survey of Artificial Intelligence in Cybersecurity",2020,"","","","",178,"2022-07-13 09:34:35","","10.1109/CSCI51800.2020.00026","","",,,,,0,0.00,0,2,2,"During the last decades, not only the number of cyberattacks have increased significantly, they have also become more sophisticated. Hence designing a cyber-resilient approach is of paramount importance. Traditional security methods are not adequate to prevent data breaches in case of cyberattacks. Cybercriminals have learned how to use new techniques and robust tools to hack, attack, and breach data. Fortunately, Artificial Intelligence (AI) technologies have been introduced into cyberspace to construct smart models for defending systems from attacks. Since AI technologies can rapidly evolve to address complex situations, they can be used as fundamental tools in the field of cybersecurity. Al-based techniques can provide efficient and powerful cyber defense tools to recognize malware attacks, network intrusions, phishing and spam emails, and data breaches, to name a few, and to alert security incidents when they occur. In this paper, we review the impact of AI in cybersecurity and summarize existing research in terms of benefits of AI in cybersecurity.","",""
6,"N. Gahungu, Robert Trueick, S. Bhat, P. Sengupta, G. Dwivedi","Current Challenges and Recent Updates in Artificial Intelligence and Echocardiography",2020,"","","","",179,"2022-07-13 09:34:35","","10.1007/s12410-020-9529-x","","",,,,,6,3.00,1,5,2,"","",""
54,"G. Collins, P. Dhiman, Constanza L. Andaur Navarro, Jie Ma, L. Hooft, J. Reitsma, P. Logullo, Andrew Beam, Lily Peng, B. van Calster, M. van Smeden, R. Riley, K. Moons","Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence",2021,"","","","",180,"2022-07-13 09:34:35","","10.1136/bmjopen-2020-048008","","",,,,,54,54.00,5,13,1,"Introduction The Transparent Reporting of a multivariable prediction model of Individual Prognosis Or Diagnosis (TRIPOD) statement and the Prediction model Risk Of Bias ASsessment Tool (PROBAST) were both published to improve the reporting and critical appraisal of prediction model studies for diagnosis and prognosis. This paper describes the processes and methods that will be used to develop an extension to the TRIPOD statement (TRIPOD-artificial intelligence, AI) and the PROBAST (PROBAST-AI) tool for prediction model studies that applied machine learning techniques. Methods and analysis TRIPOD-AI and PROBAST-AI will be developed following published guidance from the EQUATOR Network, and will comprise five stages. Stage 1 will comprise two systematic reviews (across all medical fields and specifically in oncology) to examine the quality of reporting in published machine-learning-based prediction model studies. In stage 2, we will consult a diverse group of key stakeholders using a Delphi process to identify items to be considered for inclusion in TRIPOD-AI and PROBAST-AI. Stage 3 will be virtual consensus meetings to consolidate and prioritise key items to be included in TRIPOD-AI and PROBAST-AI. Stage 4 will involve developing the TRIPOD-AI checklist and the PROBAST-AI tool, and writing the accompanying explanation and elaboration papers. In the final stage, stage 5, we will disseminate TRIPOD-AI and PROBAST-AI via journals, conferences, blogs, websites (including TRIPOD, PROBAST and EQUATOR Network) and social media. TRIPOD-AI will provide researchers working on prediction model studies based on machine learning with a reporting guideline that can help them report key details that readers need to evaluate the study quality and interpret its findings, potentially reducing research waste. We anticipate PROBAST-AI will help researchers, clinicians, systematic reviewers and policymakers critically appraise the design, conduct and analysis of machine learning based prediction model studies, with a robust standardised tool for bias evaluation. Ethics and dissemination Ethical approval has been granted by the Central University Research Ethics Committee, University of Oxford on 10-December-2020 (R73034/RE001). Findings from this study will be disseminated through peer-review publications. PROSPERO registration number CRD42019140361 and CRD42019161764.","",""
20,"Hong Zhang, Hoang Nguyen, X. Bui, B. Pradhan, P. Asteris, R. Costache, J. Aryal","A generalized artificial intelligence model for estimating the friction angle of clays in evaluating slope stability using a deep neural network and Harris Hawks optimization algorithm",2021,"","","","",181,"2022-07-13 09:34:35","","10.1007/S00366-020-01272-9","","",,,,,20,20.00,3,7,1,"","",""
21,"Adrien Bécue, Isabel Praça, J. Gama","Artificial intelligence, cyber-threats and Industry 4.0: challenges and opportunities",2021,"","","","",182,"2022-07-13 09:34:35","","10.1007/S10462-020-09942-2","","",,,,,21,21.00,7,3,1,"","",""
15,"S. Ebrahimian, Fatemeh Homayounieh, M. Rockenbach, Preetham Putha, T. Raj, I. Dayan, B. Bizzo, Varun Buch, Dufan Wu, Kyungsang Kim, Quanzheng Li, S. Digumarthy, M. Kalra","Artificial intelligence matches subjective severity assessment of pneumonia for prediction of patient outcome and need for mechanical ventilation: a cohort study",2021,"","","","",183,"2022-07-13 09:34:35","","10.1038/s41598-020-79470-0","","",,,,,15,15.00,2,13,1,"","",""
19,"Ruhhee Tabbussum, A. Q. Dar","Performance evaluation of artificial intelligence paradigms—artificial neural networks, fuzzy logic, and adaptive neuro-fuzzy inference system for flood prediction",2021,"","","","",184,"2022-07-13 09:34:35","","10.1007/s11356-021-12410-1","","",,,,,19,19.00,10,2,1,"","",""
1,"M. H. Hajialia, M. Mosavi, K. Shahanaghi","A New Decision Support System at Estimation of Project Completion Time Considering the Combination of Artificial Intelligence Methods based on Earn Value Management Framework",2020,"","","","",185,"2022-07-13 09:34:35","","","","",,,,,1,0.50,0,3,2,"One of the important issues in project management is estimation of projects completion time. This paper proposes a model based on ensemble learning using certain features of projects in Earn Value Management (EVM) to estimate project completion time. Proper simulation of the dynamic nature of the project, higher reliability in comparison with individual methods, better robustness against the presence of a weak estimator, and appropriate control of the type and number of the existing regressions in ensemble are the important features of the proposed model. The proposed method is evaluated based on two datasets, which are created by three real projects, and promising results are obtained as compared to the other well-known estimators.","",""
0,"Naser El Naily, S. Saad, S. Abeid, H. Saleh","Improved Over-Current Coordination Using Artificial Intelligence In Benghazi MV-Distribution Network Case Study",2020,"","","","",186,"2022-07-13 09:34:35","","10.1145/3410352.3410809","","",,,,,0,0.00,0,4,2,"Sustaining both of the security and the reliability of modern distribution networks requires high performance of the protection system, the coordination of the protective Over Current Relays (OCRs) optimally plays a key role in the protection of the Distribution Network (DN) in terms of its operations and isolating the affected parts during faults. Many factors force operators to optimally coordinate OCRs such as increasing daily activities of consumers and interconnected systems. In the DN, calculations of the OCRs breakpoint and orderly enumeration of multiple loops are the most important requirements for the coordination of OCR scheme. Consequently, avoiding blackouts and electricity interruption of some feeders from the grid. In this paper, a detailed improved OCR coordination by a mean of an intelligent technique using Benghazi MV-Distribution Network as a case study. The Water Cycle Algorithm (WCA) has been applied as one of the optimization techniques for the Benghazi MV-distribution Network. For validation purposes, Particle Swarm Optimization (PSO) has been implemented to check the applicability of the proposed WCA. A coded WCA and PSO are used in MATLAB to attain the OCR settings, then Benghazi MV-Distribution Network has been utilized in ETAP package to evaluate the robustness of the proposed approach. The proposed approach exhibit improved relay coordination.","",""
755,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xisheng Fang, Shiqin Zhang, J. Xia, Jun Xia","Artificial Intelligence Distinguishes COVID-19 from Community Acquired Pneumonia on Chest CT",2020,"","","","",187,"2022-07-13 09:34:35","","10.1148/radiol.2020200905","","",,,,,755,377.50,76,18,2,"Background Coronavirus disease has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performances. Materials and Methods In this retrospective and multi-center study, a deep learning model, COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT exams for the detection of COVID-19. Community acquired pneumonia (CAP) and other non-pneumonia CT exams were included to test the robustness of the model. The datasets were collected from 6 hospitals between August 2016 and February 2020. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity and specificity. Results The collected dataset consisted of 4356 chest CT exams from 3,322 patients. The average age is 49±15 years and there were slightly more male patients than female (1838 vs 1484; p-value=0.29). The per-exam sensitivity and specificity for detecting COVID-19 in the independent test set was 114 of 127 (90% [95% CI: 83%, 94%]) and 294 of 307 (96% [95% CI: 93%, 98%]), respectively, with an AUC of 0.96 (p-value<0.001). The per-exam sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175) and 92% (239 of 259), respectively, with an AUC of 0.95 (95% CI: 0.93, 0.97). Conclusions A deep learning model can accurately detect COVID-19 and differentiate it from community acquired pneumonia and other lung diseases.","",""
99,"R. Colling, Helen Pitman, K. Oien, N. Rajpoot, P. Macklin, D. Snead, Tony Sackville, C. Verrill","Artificial intelligence in digital pathology: a roadmap to routine use in clinical practice",2019,"","","","",188,"2022-07-13 09:34:35","","10.1002/path.5310","","",,,,,99,33.00,12,8,3,"The use of artificial intelligence will transform clinical practice over the next decade and the early impact of this will likely be the integration of image analysis and machine learning into routine histopathology. In the UK and around the world, a digital revolution is transforming the reporting practice of diagnostic histopathology and this has sparked a proliferation of image analysis software tools. While this is an exciting development that could discover novel predictive clinical information and potentially address international pathology workforce shortages, there is a clear need for a robust and evidence‐based framework in which to develop these new tools in a collaborative manner that meets regulatory approval. With these issues in mind, the NCRI Cellular Molecular Pathology (CM‐Path) initiative and the British In Vitro Diagnostics Association (BIVDA) have set out a roadmap to help academia, industry, and clinicians develop new software tools to the point of approved clinical use. © 2019 Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.","",""
85,"A. Grzybowski, Piotr Brona, Gilbert Lim, P. Ruamviboonsuk, G. Tan, M. Abràmoff, D. Ting","Artificial intelligence for diabetic retinopathy screening: a review",2019,"","","","",189,"2022-07-13 09:34:35","","10.1038/s41433-019-0566-0","","",,,,,85,28.33,12,7,3,"","",""
73,"Valentina Bellemo, Gilbert Lim, T. Rim, G. Tan, C. Cheung, S. Sadda, M. He, A. Tufail, M. Lee, W. Hsu, D. Ting","Artificial Intelligence Screening for Diabetic Retinopathy: the Real-World Emerging Application",2019,"","","","",190,"2022-07-13 09:34:35","","10.1007/s11892-019-1189-3","","",,,,,73,24.33,7,11,3,"","",""
67,"Yonghui Shang, Hoang Nguyen, X. Bui, Quang-Hieu Tran, H. Moayedi","A Novel Artificial Intelligence Approach to Predict Blast-Induced Ground Vibration in Open-Pit Mines Based on the Firefly Algorithm and Artificial Neural Network",2019,"","","","",191,"2022-07-13 09:34:35","","10.1007/s11053-019-09503-7","","",,,,,67,22.33,13,5,3,"","",""
51,"Xiaohang Wu, Yelin Huang, Zhenzhen Liu, Weiyi Lai, Erping Long, Kai Zhang, Jiewei Jiang, Duoru Lin, Kexin Chen, Tongyong Yu, Dongxuan Wu, Cong Li, Yanyi Chen, Minjie Zou, Chuan Chen, Yi Zhu, Chong Guo, Xiayin Zhang, Ruixin Wang, Yahan Yang, Yifan Xiang, Lijian Chen, Congxin Liu, J. Xiong, Z. Ge, Ding-ding Wang, Guihua Xu, Shao-lin Du, Chi Xiao, Jianghao Wu, Ke Zhu, Dan-yao Nie, Fan Xu, Jian Lv, Weirong Chen, Yizhi Liu, Haotian Lin","Universal artificial intelligence platform for collaborative management of cataracts",2019,"","","","",192,"2022-07-13 09:34:35","","10.1136/bjophthalmol-2019-314729","","",,,,,51,17.00,5,37,3,"Purpose To establish and validate a universal artificial intelligence (AI) platform for collaborative management of cataracts involving multilevel clinical scenarios and explored an AI-based medical referral pattern to improve collaborative efficiency and resource coverage. Methods The training and validation datasets were derived from the Chinese Medical Alliance for Artificial Intelligence, covering multilevel healthcare facilities and capture modes. The datasets were labelled using a three-step strategy: (1) capture mode recognition; (2) cataract diagnosis as a normal lens, cataract or a postoperative eye and (3) detection of referable cataracts with respect to aetiology and severity. Moreover, we integrated the cataract AI agent with a real-world multilevel referral pattern involving self-monitoring at home, primary healthcare and specialised hospital services. Results The universal AI platform and multilevel collaborative pattern showed robust diagnostic performance in three-step tasks: (1) capture mode recognition (area under the curve (AUC) 99.28%–99.71%), (2) cataract diagnosis (normal lens, cataract or postoperative eye with AUCs of 99.82%, 99.96% and 99.93% for mydriatic-slit lamp mode and AUCs >99% for other capture modes) and (3) detection of referable cataracts (AUCs >91% in all tests). In the real-world tertiary referral pattern, the agent suggested 30.3% of people be ‘referred’, substantially increasing the ophthalmologist-to-population service ratio by 10.2-fold compared with the traditional pattern. Conclusions The universal AI platform and multilevel collaborative pattern showed robust diagnostic performance and effective service for cataracts. The context of our AI-based medical referral pattern will be extended to other common disease conditions and resource-intensive situations.","",""
13,"Valentin Kuleto, Milena P. Ilić, Mihail-Alexandru Dumangiu, Marko Ranković, Oliva M. D. Martins, D. Păun, Larisa Mihoreanu","Exploring Opportunities and Challenges of Artificial Intelligence and Machine Learning in Higher Education Institutions",2021,"","","","",193,"2022-07-13 09:34:35","","10.3390/su131810424","","",,,,,13,13.00,2,7,1,"The way people travel, organise their time, and acquire information has changed due to information technologies. Artificial intelligence (AI) and machine learning (ML) are mechanisms that evolved from data management and developing processes. Incorporating these mechanisms into business is a trend many different industries, including education, have identified as game-changers. As a result, education platforms and applications are more closely aligned with learners’ needs and knowledge, making the educational process more efficient. Therefore, AI and ML have great potential in e-learning and higher education institutions (HEI). Thus, the article aims to determine its potential and use areas in higher education based on secondary research and document analysis (literature review), content analysis, and primary research (survey). As referent points for this research, multiple academic, scientific, and commercial sources were used to obtain a broader picture of the research subject. Furthermore, the survey was implemented among students in the Republic of Serbia, with 103 respondents to generate data and information on how much knowledge of AI and ML is held by the student population, mainly to understand both opportunities and challenges involved in AI and ML in HEI. The study addresses critical issues, like common knowledge and stance of research bases regarding AI and ML in HEI; best practices regarding usage of AI and ML in HEI; students’ knowledge of AI and ML; and students’ attitudes regarding AI and ML opportunities and challenges in HEI. In statistical considerations, aiming to evaluate if the indicators were considered reflexive and, in this case, belong to the same theoretical dimension, the Correlation Matrix was presented, followed by the Composite Reliability. Finally, the results were evaluated by regression analysis. The results indicated that AI and ML are essential technologies that enhance learning, primarily through students’ skills, collaborative learning in HEI, and an accessible research environment.","",""
4,"Magdalini Tyrtaiou, Andonis Papaleonidas, A. Elenas, L. Iliadis","Accomplished Reliability Level for Seismic Structural Damage Prediction Using Artificial Neural Networks",2020,"","","","",194,"2022-07-13 09:34:35","","10.1007/978-3-030-48791-1_6","","",,,,,4,2.00,1,4,2,"","",""
32,"Jun-Ho Huh, Yeong-Seok Seo","Understanding Edge Computing: Engineering Evolution With Artificial Intelligence",2019,"","","","",195,"2022-07-13 09:34:35","","10.1109/ACCESS.2019.2945338","","",,,,,32,10.67,16,2,3,"The key to the explosion of the Internet of Things and the ability to collect, analyze, and provide big data in the cloud is edge computing, which is a new computing paradigm in which data is processed from edges. Edge Computing has been attracting attention as one of the top 10 strategic technology trends in the past two years and has innovative potential. It provides shorter response times, lower bandwidth costs, and more robust data safety and privacy protection than cloud computing. In particular, artificial intelligence technologies are rapidly incorporating edge computing. In this paper, we introduce the concepts, backgrounds, and pros and cons of edge computing, explain how it operates and its structure hierarchically with artificial intelligence concepts, list examples of its applications in various fields, and finally suggest some improvements and discuss the challenges of its application in three representative technological fields. We intend to clarify various analyses and opinions regarding edge computing and artificial intelligence.","",""
32,"Matt Taddy","The Technological Elements of Artificial Intelligence",2018,"","","","",196,"2022-07-13 09:34:35","","10.3386/W24301","","",,,,,32,8.00,32,1,4,"We have seen in the past decade a sharp increase in the extent that companies use data to optimize their businesses. Variously called the `Big Data' or `Data Science' revolution, this has been characterized by massive amounts of data, including unstructured and nontraditional data like text and images, and the use of fast and flexible Machine Learning (ML) algorithms in analysis. With recent improvements in Deep Neural Networks (DNNs) and related methods, application of high-performance ML algorithms has become more automatic and robust to different data scenarios. That has led to the rapid rise of an Artificial Intelligence (AI) that works by combining many ML algorithms together – each targeting a straightforward prediction task – to solve complex problems. We will define a framework for thinking about the ingredients of this new ML-driven AI. Having an understanding of the pieces that make up these systems and how they fit together is important for those who will be building businesses around this technology. Those studying the economics of AI can use these definitions to remove ambiguity from the conversation on AI's projected productivity impacts and data requirements. Finally, this framework should help clarify the role for AI in the practice of modern business analytics and economic measurement.","",""
57,"C. Connor","Artificial Intelligence and Machine Learning in Anesthesiology.",2019,"","","","",197,"2022-07-13 09:34:35","","10.1097/ALN.0000000000002694","","",,,,,57,19.00,57,1,3,"Commercial applications of artificial intelligence and machine learning have made remarkable progress recently, particularly in areas such as image recognition, natural speech processing, language translation, textual analysis, and self-learning. Progress had historically languished in these areas, such that these skills had come to seem ineffably bound to intelligence. However, these commercial advances have performed best at single-task applications in which imperfect outputs and occasional frank errors can be tolerated.The practice of anesthesiology is different. It embodies a requirement for high reliability, and a pressured cycle of interpretation, physical action, and response rather than any single cognitive act. This review covers the basics of what is meant by artificial intelligence and machine learning for the practicing anesthesiologist, describing how decision-making behaviors can emerge from simple equations. Relevant clinical questions are introduced to illustrate how machine learning might help solve them-perhaps bringing anesthesiology into an era of machine-assisted discovery.","",""
41,"Y. Ongena, M. Haan, Derya Yakar, T. Kwee","Patients’ views on the implementation of artificial intelligence in radiology: development and validation of a standardized questionnaire",2019,"","","","",198,"2022-07-13 09:34:35","","10.1007/s00330-019-06486-0","","",,,,,41,13.67,10,4,3,"","",""
36,"S. Devalla, Zhang Liang, T. H. Pham, C. Boote, N. Strouthidis, Alexandre Hoang Thiery, M. Girard","Glaucoma management in the era of artificial intelligence",2019,"","","","",199,"2022-07-13 09:34:35","","10.1136/bjophthalmol-2019-315016","","",,,,,36,12.00,5,7,3,"Glaucoma is a result of irreversible damage to the retinal ganglion cells. While an early intervention could minimise the risk of vision loss in glaucoma, its asymptomatic nature makes it difficult to diagnose until a late stage. The diagnosis of glaucoma is a complicated and expensive effort that is heavily dependent on the experience and expertise of a clinician. The application of artificial intelligence (AI) algorithms in ophthalmology has improved our understanding of many retinal, macular, choroidal and corneal pathologies. With the advent of deep learning, a number of tools for the classification, segmentation and enhancement of ocular images have been developed. Over the years, several AI techniques have been proposed to help detect glaucoma by analysis of functional and/or structural evaluations of the eye. Moreover, the use of AI has also been explored to improve the reliability of ascribing disease prognosis. This review summarises the role of AI in the diagnosis and prognosis of glaucoma, discusses the advantages and challenges of using AI systems in clinics and predicts likely areas of future progress.","",""
22,"Rushikesh S. Joshi, Alexander F. Haddad, Darryl Lau, C. Ames","Artificial Intelligence for Adult Spinal Deformity",2019,"","","","",200,"2022-07-13 09:34:35","","10.14245/ns.1938414.207","","",,,,,22,7.33,6,4,3,"Adult spinal deformity (ASD) is a complex disease that significantly affects the lives of many patients. Surgical correction has proven to be effective in achieving improvement of spinopelvic parameters as well as improving quality of life (QoL) for these patients. However, given the relatively high complication risk associated with ASD correction, it is of paramount importance to develop robust prognostic tools for predicting risk profile and outcomes. Historically, statistical models such as linear and logistic regression models were used to identify preoperative factors associated with postoperative outcomes. While these tools were useful for looking at simple associations, they represent generalizations across large populations, with little applicability to individual patients. More recently, predictive analytics utilizing artificial intelligence (AI) through machine learning for comprehensive processing of large amounts of data have become available for surgeons to implement. The use of these computational techniques has given surgeons the ability to leverage far more accurate and individualized predictive tools to better inform individual patients regarding predicted outcomes after ASD correction surgery. Applications range from predicting QoL measures to predicting the risk of major complications, hospital readmission, and reoperation rates. In addition, AI has been used to create a novel classification system for ASD patients, which will help surgeons identify distinct patient subpopulations with unique risk-benefit profiles. Overall, these tools will help surgeons tailor their clinical practice to address patients’ individual needs and create an opportunity for personalized medicine within spine surgery.","",""
