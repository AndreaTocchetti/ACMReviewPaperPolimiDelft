Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
14,"Ndapandula Nakashole","Automatic extraction of facts, relations, and entities for web-scale knowledge base population",2012,"","","","",1,"2022-07-13 09:36:46","","10.22028/D291-26412","","",,,,,14,1.40,14,1,10,"Equipping machines with knowledge, through the construction of machinereadable knowledge bases, presents a key asset for semantic search, machine translation, question answering, and other formidable challenges in artificial intelligence. However, human knowledge predominantly resides in books and other natural language text forms. This means that knowledge bases must be extracted and synthesized from natural language text. When the source of text is the Web, extraction methods must cope with ambiguity, noise, scale, and updates. The goal of this dissertation is to develop knowledge base population methods that address the afore mentioned characteristics of Web text. The dissertation makes three contributions. The first contribution is a method for mining high-quality facts at scale, through distributed constraint reasoning and a pattern representation model that is robust against noisy patterns. The second contribution is a method for mining a large comprehensive collection of relation types beyond those commonly found in existing knowledge bases. The third contribution is a method for extracting facts from dynamic Web sources such as news articles and social media where one of the key challenges is the constant emergence of new entities. All methods have been evaluated through experiments involving Web-scale text collections.","",""
0,"Bharat Khandelwal","FAME-BERT: Stable Meta-learning for Robust Question-Answering",2022,"","","","",2,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,1,1,"With the increasing importance of conversational AI, search engines, and other interactive systems, Question Answering has become a critical task to advancing the state-of-the-art in Natural Language Understanding systems. An important problem is for these systems to generalize well to new domains with a small amount of training examples - understanding how to build such systems will represent a leap of knowledge in our quest for Artificial General Intelligence, and will allow us to create general-purpose software that adapts on an as-needed basis. To obtain good out-of-domain generalization performance with fewshot learning, we propose our model FAME-BERT ( F inetune- A ugment- M etalearn-E nsemble Distil BERT ). We recognize and underline the benefits of carefully crafted learning rates, data augmentation, and ensembling over our base approach of meta-learning a DistilBERT model. Highlights of our model’s performance include Rank 1 by EM on the validation leaderboard and Top 5 by EM on the test leaderboard. On both validation and test sets, our model outperforms the DistilBERT baseline by a significant margin. Finally, we discuss future directions of research that are likely to further boost model performance.","",""
16,"S. Ulyanov","Quantum Fast Algorithm Computational Intelligence PT I: SW / HW Smart Toolkit",2019,"","","","",3,"2022-07-13 09:36:46","","10.30564/AIA.V1I1.619","","",,,,,16,5.33,16,1,3,"A new approach to a circuit implementation design of quantum algorithm gates for quantum massive parallel fast computing implementation is presented. The main attention is focused on the development of design method of fast quantum algorithm operators as superposition, entanglement and interference which are in general time-consuming operations due to the number of products that have to be performed. SW & HW support sophisticated smart toolkit of supercomputing accelerator of quantum algorithm simulation is described. The method for performing Grover’s interference without product operations as Benchmark introduced. The background of developed information technology is the ""Quantum / Soft Computing Optimizer"" (QSCOptKBTM) software based on soft and quantum computational intelligence toolkit. Quantum genetic and quantum fuzzy inference algorithm gate design considered. The quantum information technology of imperfect knowledge base self-organization design of fuzzy robust controllers for the guaranteed achievement of intelligent autonomous robot the control goal in unpredicted control situations is described.","",""
79,"Stefan Zwicklbauer, C. Seifert, M. Granitzer","Robust and Collective Entity Disambiguation through Semantic Embeddings",2016,"","","","",4,"2022-07-13 09:36:46","","10.1145/2911451.2911535","","",,,,,79,13.17,26,3,6,"Entity disambiguation is the task of mapping ambiguous terms in natural-language text to its entities in a knowledge base. It finds its application in the extraction of structured data in RDF (Resource Description Framework) from textual documents, but equally so in facilitating artificial intelligence applications, such as Semantic Search, Reasoning and Question & Answering. We propose a new collective, graph-based disambiguation algorithm utilizing semantic entity and document embeddings for robust entity disambiguation. Robust thereby refers to the property of achieving better than state-of-the-art results over a wide range of very different data sets. Our approach is also able to abstain if no appropriate entity can be found for a specific surface form. Our evaluation shows, that our approach achieves significantly (>5%) better results than all other publicly available disambiguation algorithms on 7 of 9 datasets without data set specific tuning. Moreover, we discuss the influence of the quality of the knowledge base on the disambiguation accuracy and indicate that our algorithm achieves better results than non-publicly available state-of-the-art algorithms.","",""
11,"Du Zhang","Quantifying Knowledge Base Inconsistency via Fixpoint Semantics",2007,"","","","",5,"2022-07-13 09:36:46","","10.1007/978-3-540-87563-5_9","","",,,,,11,0.73,11,1,15,"","",""
0,"D. L. Hall","A comparative analysis of guided vs. query-based intelligent tutoring systems (its) using a class - entity - relationship - attribute (cera) knowledge base",1987,"","","","",6,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,1,35,"One of the greatest problems facing researchers in the subfield of Artificial Intelligence known as Intelligent Tutoring Systems (ITS) is the selection of a knowledge base designs that will facilitate the modification of the knowledge base. The Class-Entity-Relationship-Attribute (CERA), proposed by R. P. Brazile, holds certain promise as a more generic knowledge base design framework upon which can be built robust and efficient ITS.  This study has a twofold purpose. The first is to demonstrate that a CERA knowledge base can be constructed for an ITS on a subset of the domain of Cretaceous paleontology and function as the ""expert module"" of the ITS. The second is to test the validity of the ideas that students guided through a lesson learn more factual knowledge, while those who explore the knowledge base that underlies the lesson through query at their own pace will be able to formulate their own integrative knowledge from the knowledge gained in their explorations and spend more time on the system.  This study concludes that a CERA-based system can be constructed as an effective teaching tool. However, while an ITS-treatment provides for statistically significant gains in achievement test scores, the type of treatment seems not to matter as much as time spent on task. This would seem to indicate that a query-based system which allows the user to progress at their own pace would be a better type of system for the presentation of material due to the greater amount of on-line computer time exhibited by the users.","",""
4,"S. Ulyanov","Quantum Algorithm of Imperfect KB Self-organization Pt I: Smart Control-Information-Thermodynamic Bounds",2021,"","","","",7,"2022-07-13 09:36:46","","10.30564/aia.v3i2.3171","","",,,,,4,4.00,4,1,1,"The quantum self-organization algorithm model of wise knowledge base design for intelligent fuzzy controllers with required robust level considered. Background of the model is a new model of quantum inference based on quantum genetic algorithm. Quantum genetic algorithm applied on line for the quantum correlation’s type searching between unknown solutions in quantum superposition of imperfect knowledge bases of intelligent controllers designed on soft computing. Disturbance conditions of analytical information-thermodynamic trade-off interrelations between main control quality measures (as new design laws) discussed in Part I. The smart control design with guaranteed achievement of these tradeoff interrelations is main goal for quantum self-organization algorithm of imperfect KB. Sophisticated synergetic quantum information effect in Part I (autonomous robot in unpredicted control situations) and II (swarm robots with imperfect KB exchanging between “master - slaves”) introduced: a new robust smart controller on line designed from responses on unpredicted control situations of any imperfect KB applying quantum hidden information extracted from quantum correlation. Within the toolkit of classical intelligent control, the achievement of the similar synergetic information effect is impossible. Benchmarks of intelligent cognitive robotic control applications considered.","",""
0,"Stefan Zwicklbauer","Robust Entity Linking in Heterogeneous Domains",2017,"","","","",8,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,1,5,"Entity Linking is the task of mapping terms in arbitrary documents to entities in a knowledge base by identifying the correct semantic meaning. It is applied in the extraction of structured data in RDF (Resource Description Framework) from textual documents, but equally so in facilitating artificial intelligence applications, such as Semantic Search, Reasoning and Question and Answering. Most existing Entity Linking systems were optimized for specific domains (e.g., general domain, biomedical domain), knowledge base types (e.g., DBpedia, Wikipedia), or document structures (e.g., tables) and types (e.g., news articles, tweets). This led to very specialized systems that lack robustness and are only applicable for very specific tasks. In this regard, this work focuses on the research and development of a robust Entity Linking system in terms of domains, knowledge base types, and document structures and types.    To create a robust Entity Linking system, we first analyze the following three crucial components of an Entity Linking algorithm in terms of robustness criteria: (i) the underlying knowledge base, (ii) the entity relatedness measure, and (iii) the textual context matching technique. Based on the analyzed components, our scientific contributions are three-fold. First, we show that a federated approach leveraging knowledge from various knowledge base types can significantly improve robustness in Entity Linking systems. Second, we propose a new state-of-the-art, robust entity relatedness measure for topical coherence computation based on semantic entity embeddings. Third, we present the neural-network-based approach Doc2Vec as a textual context matching technique for robust Entity Linking.    Based on our previous findings and outcomes, our main contribution in this work is DoSeR (Disambiguation of Semantic Resources). DoSeR is a robust, knowledge-base-agnostic Entity Linking framework that extracts relevant entity information from multiple knowledge bases in a fully automatic way. The integrated algorithm represents a collective, graph-based approach that utilizes semantic entity and document embeddings for entity relatedness and textual context matching computation. Our evaluation shows, that DoSeR achieves state-of-the-art results over a wide range of different document structures (e.g., tables), document types (e.g., news documents) and domains (e.g., general domain, biomedical domain). In this context, DoSeR outperforms all other (publicly available) Entity Linking algorithms on most data sets.","",""
1,"Jan Strohschein, A. Fischbach, Andreas Bunte, Heide Faeskorn-Woyke, N. Moriz, T. Bartz-Beielstein","Cognitive Capabilities for the CAAI in Cyber-Physical Production Systems",2020,"","","","",9,"2022-07-13 09:36:46","","10.1007/s00170-021-07248-3","","",,,,,1,0.50,0,6,2,"","",""
0,"Jerry D. Smith","Robust knowledge bases (abstract)",1986,"","","","",10,"2022-07-13 09:36:46","","10.1145/324634.325098","","",,,,,0,0.00,0,1,36,"Knowledge representation (KR) has emerged as one of the most important and active fields of investigation within artificial intelligence (AI). It is now clear that significant advances in the application of AI principles to real world problems will require sophisticated knowledge representation schemes. A variety of knowledge structures and techniques for knowledge storage and retrieval have been proposed. Many of these representation techniques are practical only when the knowledge base can exist in primary storage and/or when it is reasonably static. However, many future applications of AI will require management of large, very dynamic knowledge bases. This paper takes as an example a sophisticated tutoring system, i.e., one of greater magnitude than those offered in [Sleeman82]. Such a tutoring system can be developed only at great expense; this implies a need to consider multi-user access, which raises questions about concurrency, recovery, networking, etc. Researchers in the database field have begun to address issues such as concurrency, integrity, recovery, and others; problems that will arise in the long-term development of robust knowledge systems. It will not be possible to handle these problems effectively by introducing memory-management features tailored to large primary storage areas [Balkovich85]. Instead, knowledge systems will have to make efficient use of secondary storage. There are other considerations. A very important one deals with procedure and data abstraction. In general, KR schemes used in many hl applications have not provided sufficient separation of procedure and data, nor has abstraction been promoted within each of these. Another consideration in developing large knowledge systems is storage and retrieval efficiency. Efficiency has been an issue with current systems that are heavily dependent on primary storage access; secondary storage-intensive systems will demand greater consideration of efficiency.","",""
62,"C. Lo, Y. Wong, A. Rad","Intelligent system for process supervision and fault diagnosis in dynamic physical systems",2006,"","","","",11,"2022-07-13 09:36:46","","10.1109/TIE.2006.870707","","",,,,,62,3.88,21,3,16,"In recent years, the increasing complexity of process plants and other engineered systems has extended the scope of interest in control engineering, which was previously focused on the development of controllers for specified performance criteria such as stability and precision. Modern industrial systems require a higher demand of system reliability, safety, and low-cost operation, which in turn call for sophisticated and elegant fault-detection and isolation algorithms. This paper develops an intelligent supervisory coordinator (ISC) for process supervision and fault diagnosis in dynamic physical systems. A qualitative bond graph modeling scheme, integrating artificial-intelligence techniques with control engineering, is used to construct the knowledge base of the ISC. A supervisor provided by the ISC utilizes the knowledge in the knowledge base to classify various system behaviors, coordinates different control tasks (e.g., fault diagnosis), and communicates system states to human operators. The ISC provides a robust semiautonomous system to assist human operators in managing dynamic physical systems. The proposed ISC has been successfully applied to supervise a laboratory-scale servo-tank liquid process rig.","",""
11,"V. Clément, M. Thonnat","Handling knowledge in image processing libraries to build automatic systems",1989,"","","","",12,"2022-07-13 09:36:46","","10.1109/MIV.1989.40547","","",,,,,11,0.33,6,2,33,"Automatic vision systems are needed to build industrial applications able to adapt themselves to their environment. To allow the development of such robust applications, the authors propose a software tool named OCAPI, which uses artificial intelligence techniques, to handle knowledge about components of software libraries including their goals and their proper usage. An example of a knowledge base for a pattern recognition problem is presented.<<ETX>>","",""
5,"Quanshi Zhang, Xuan Song, Xiaowei Shao, Huijing Zhao, R. Shibasaki","From RGB-D Images to RGB Images",2015,"","","","",13,"2022-07-13 09:36:46","","10.1145/2629701","","",,,,,5,0.71,1,5,7,"Mining object-level knowledge, that is, building a comprehensive category model base, from a large set of cluttered scenes presents a considerable challenge to the field of artificial intelligence. How to initiate model learning with the least human supervision (i.e., manual labeling) and how to encode the structural knowledge are two elements of this challenge, as they largely determine the scalability and applicability of any solution. In this article, we propose a model-learning method that starts from a single-labeled object for each category, and mines further model knowledge from a number of informally captured, cluttered scenes. However, in these scenes, target objects are relatively small and have large variations in texture, scale, and rotation. Thus, to reduce the model bias normally associated with less supervised learning methods, we use the robust 3D shape in RGB-D images to guide our model learning, then apply the properly trained category models to both object detection and recognition in more conventional RGB images. In addition to model training for their own categories, the knowledge extracted from the RGB-D images can also be transferred to guide model learning for a new category, in which only RGB images without depth information in the new category are provided for training. Preliminary testing shows that the proposed method performs as well as fully supervised learning methods.","",""
3,"G. Biswas, G. Lee","Knowledge reorganization. A rule model scheme for efficient reasoning",1994,"","","","",14,"2022-07-13 09:36:46","","10.1109/CAIA.1994.323659","","",,,,,3,0.11,2,2,28,"Discusses the application of conceptual clustering in restructuring large knowledge bases for the purpose of improving their complex problem solving efficiency. The rule base of PLAYMAKER, a system for characterizing hydrocarbon fields and plays, is restructured into a hierarchy of rule models using our conceptual clustering scheme, ITERATE. The rule models, used with a task-specific reasoning methodology, provide a more efficient, focused, and robust inferencing mechanism. A set of case studies that have been conducted demonstrate the improved performance of the reasoning system. PLAYMAKER is implemented on MIDST (Mixed Inferencing Dempster-Shafer Tool), a general-purpose knowledge-based system construction tool that incorporates reasoning mechanisms based on a task-specific architecture and belief functions.<<ETX>>","",""
45,"J. Hushon","Expert systems for environmental problems",1987,"","","","",15,"2022-07-13 09:36:46","","10.1021/ES00163A604","","",,,,,45,1.29,45,1,35,"Expert systems comprise a branch of artificial intelligence. Defined as man and machine systems with specialized, problem-solving expertise, expert systems rely on a data base of knowledge about a particular subject area, an understanding of the problems addressed within that subject area, and skill at solving these problems. Expert systems software was developed during the early 1970s and was initially applied to well-defined problem areas. Gradually this software has become more robust and has evolved into domain-independent software that can facilitate the construction of applications. These expert system software tools are referred to as shells. Currently a number of these shells are available on microcomputers, and some come on minicomputers as well. The following types of situations are ideal candidates for expert systems solutions: situations that occur often, situations that are complex, situations that require knowledge of experts (higher reasoning), situations in which uncertainty is involved, situations that are dynamic, and situations that demand consistent responses. Nonetheless, expert systems are starting to be used to recognize and manage environmental problems. In general, expert systems can be divided into a number of functional categories: planning, monitoring and control, instruction, interpretation, production, diagnosis and repair, and design. Table 1 shows the environmentalmore » systems, classified according to this scheme. Note that no environmental systems were found that were devoted to monitoring and control, instrumentation, or design.« less","",""
24,"T. Bardasz, I. Zeid","DEJAVU: Case-based reasoning for mechanical design",1993,"","","","",16,"2022-07-13 09:36:46","","10.1017/S0890060400000809","","",,,,,24,0.83,12,2,29,"The architecture and implementation of a mechanical designer's assistant shell called DEJAVU is presented. The architecture is based on an integration of design and CAD with some of the more well known concepts in case-based reasoning (CBR). DEJAVU provides a flexible and cognitively intuitive approach for acquiring and utilizing design knowledge. It is a domain independent mechanical design shell that can incrementally acquire design knowledge in the domain of the user. DEJAVU provides a design environment that can learn from the designer(s) until it can begin to perform design tasks autonomously or semi-autonomously. The main components of DEJAVU are a knowledge base of design plans, an evaluation module in the form of a design plan system, and a blackboard-based adaptation module. The existance of these components are derived from the utilization of a CBR architecture. DEJAVU is the first step in developing a robust designer's assistant shell for mechanical design problems. One of the major contributions of DEJAVU is the development of a clean architecture for the utilization of case-based reasoning in a mechanical designer's assistant shell. In addition, the components of the architecture have been developed, tailored or modified from a general CBR context into a more synergistic relationship with mechanical design.","",""
2,"Y. Cheng, H. Melhem","Application of Fuzzy Case-Based Reasoning to Bridge Management",2005,"","","","",17,"2022-07-13 09:36:46","","10.1061/40794(179)35","","",,,,,2,0.12,1,2,17,"Case-based reasoning (CBR), one of the artificial intelligence (AI) learning approaches, is drawing the attention of many researchers in Civil Engineering. However, due to vagueness and uncertainties in knowledge representation, retrieval, and inference of cases in CBR, -- especially when dealing with similarity assessment -, it is difficult to find the cases in a case base which are exactly the same as the query case. Therefore, fuzzy theory h as been incorporated into CBR, which promises more robust, flexible, and accurate models. In this research, fuzzy case -based reasoning (FCBR) has been used to develop a model for bridge management. This model can deal with multiple objectives, namely, pr edicting the future health condition of a bridge deck, and recommending the appropriate maintenance, rehabilitation and replacement (MR&R) actions. The FCBR model’s learning capabilities have been validated using the cross-validation method. The code is implemented using the programming language C++, and all the cases used for both training and testing are extracted from the electronic bridge database of the Kansas Department of Transportation. In this paper, recommending MR&R actions, the second function of the developed bridge management model, is focused on.","",""
4,"A. Kandel, M. Schneider","Fuzzy intelligent hybrid systems and their applications",1995,"","","","",18,"2022-07-13 09:36:46","","10.1109/FUZZY.1995.409996","","",,,,,4,0.15,2,2,27,"This paper addresses some of the issues involved in developing a technology that supports the implementation of an autonomous fuzzy hybrid intelligent systems. The technology is based on the premise that integrated solution architectures will be much more effective and highly flexible in their ability to successfully handle a broad base of applications with a wider scope of problem variations. Hybrid systems in artificial intelligence represent a new field of research that deals with the synergism of expert systems and neural network technologies. The integration of the computational paradigms of these two highly complementary knowledge representation techniques is imperative to the process of developing effective robust intelligent systems for a large number of important applications.<<ETX>>","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",19,"2022-07-13 09:36:46","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",20,"2022-07-13 09:36:46","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",21,"2022-07-13 09:36:46","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
0,"C. Hamel, Mona Hersi, S. Kelly, A. Tricco, S. Straus, G. Wells, B. Pham, B. Hutton","Guidance for using artificial intelligence for title and abstract screening while conducting knowledge syntheses",2021,"","","","",22,"2022-07-13 09:36:46","","10.1186/s12874-021-01451-2","","",,,,,0,0.00,0,8,1,"","",""
11,"A. Massaro, A. Calicchio, Vincenzo Maritati, A. Galiano, Vitangelo Birardi, L. Pellicani, Maria Gutierrez Millan, Barbara Dalla Tezza, Mauro Bianchi, Guido Vertua, Antonello Puggioni","A Case Study of Innovation of an Information Communication System and Upgrade of the Knowledge Base in Industry by ESB, Artificial Intelligence, and Big Data System Integration",2018,"","","","",23,"2022-07-13 09:36:46","","10.5121/IJAIA.2018.9503","","",,,,,11,2.75,1,11,4,"In this paper, a case study is analyzed. This case study is about an upgrade of an industry communication system developed by following Frascati research guidelines. The knowledge Base (KB) of the industry is gained by means of different tools that are able to provide data and information having different formats and structures into an unique bus system connected to a Big Data. The initial part of the research is focused on the implementation of strategic tools, which can able to upgrade the KB. The second part of the proposed study is related to the implementation of innovative algorithms based on a KNIME (Konstanz Information Miner) Gradient Boosted Trees workflow processing data of the communication system which travel into an Enterprise Service Bus (ESB) infrastructure. The goal of the paper is to prove that all the new KB collected into a Cassandra big data system could be processed through the ESB by predictive algorithms solving possible conflicts between hardware and software. The conflicts are due to the integration of different database technologies and data structures. In order to check the outputs of the Gradient Boosted Trees algorithm an experimental dataset suitable for machine learning testing has been tested. The test has been performed on a prototype network system modeling a part of the whole communication system. The paper shows how to validate industrial research by following a complete design and development of a whole communication system network improving business intelligence (BI).","",""
10,"M. Alomar, M. Hameed, N. Al‐Ansari, M. Alsaadi","Data-Driven Model for the Prediction of Total Dissolved Gas: Robust Artificial Intelligence Approach",2020,"","","","",24,"2022-07-13 09:36:46","","10.1155/2020/6618842","","",,,,,10,5.00,3,4,2,"Saturated total dissolved gas (TDG) is recently considered as a serious issue in the environmental engineering field since it stands behind the reasons for increasing the mortality rates of fish and aquatic organisms. The accurate and more reliable prediction of TDG has a very significant role in preserving the diversity of aquatic organisms and reducing the phenomenon of fish deaths. Herein, two machine learning approaches called support vector regression (SVR) and extreme learning machine (ELM) have been applied to predict the saturated TDG% at USGS 14150000 and USGS 14181500 stations which are located in the USA. For the USGS 14150000 station, the recorded samples from 13 October 2016 to 14 March 2019 (75%) were used for training set, and the rest from 15 March 2019 to 13 October 2019 (25%) were used for testing requirements. Similarly, for USGS 14181500 station, the hourly data samples which covered the period from 9 June 2017 till 11 March 2019 were used for calibrating the models and from 12 March 2019 until 9 October 2019 were used for testing the predictive models. Eight input combinations based on different parameters have been established as well as nine statistical performance measures have been used for evaluating the accuracy of adopted models, for instance, not limited, correlation of determination (        R      2        ), mean absolute relative error (MAE), and uncertainty at 95% (        U      95        ). The obtained results of the study for both stations revealed that the ELM managed efficiently to estimate the TDG in comparison to SVR technique. For USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.986 (0.986), MAE of 0.316 (0.441), and         U      95        of 3.592 (3.869). Lastly, for USGS 14181500 station, the statistical measures for ELM (SVR) were, respectively, reported as         R      2        of 0.991 (0.991), MAE of 0.338 (0.396), and         U      95        of 0.832 (0.837). In addition, ELM’s training process computational time is stated to be much shorter than that of SVM. The results also showed that the temperature parameter was the most significant variable that influenced TDG relative to the other parameters. Overall, the proposed model (ELM) proved to be an appropriate and efficient computer-assisted technology for saturated TDG modeling that will contribute to the basic knowledge of environmental considerations.","",""
1,"O. Jenkins, D. Lopresti, M. Mitchell","Next Wave Artificial Intelligence: Robust, Explainable, Adaptable, Ethical, and Accountable",2020,"","","","",25,"2022-07-13 09:36:46","","","","",,,,,1,0.50,0,3,2,"The history of AI has included several ""waves"" of ideas. The first wave, from the mid-1950s to the 1980s, focused on logic and symbolic hand-encoded representations of knowledge, the foundations of so-called ""expert systems"". The second wave, starting in the 1990s, focused on statistics and machine learning, in which, instead of hand-programming rules for behavior, programmers constructed ""statistical learning algorithms"" that could be trained on large datasets. In the most recent wave research in AI has largely focused on deep (i.e., many-layered) neural networks, which are loosely inspired by the brain and trained by ""deep learning"" methods. However, while deep neural networks have led to many successes and new capabilities in computer vision, speech recognition, language processing, game-playing, and robotics, their potential for broad application remains limited by several factors.  A concerning limitation is that even the most successful of today's AI systems suffer from brittleness-they can fail in unexpected ways when faced with situations that differ sufficiently from ones they have been trained on. This lack of robustness also appears in the vulnerability of AI systems to adversarial attacks, in which an adversary can subtly manipulate data in a way to guarantee a specific wrong answer or action from an AI system. AI systems also can absorb biases-based on gender, race, or other factors-from their training data and further magnify these biases in their subsequent decision-making. Taken together, these various limitations have prevented AI systems such as automatic medical diagnosis or autonomous vehicles from being sufficiently trustworthy for wide deployment. The massive proliferation of AI across society will require radically new ideas to yield technology that will not sacrifice our productivity, our quality of life, or our values.","",""
0,"Canan Tiftik","Investigation of Human Resources Dimension in Management and Organization Structure of the Effects of Artificial Intelligence",2021,"","","","",26,"2022-07-13 09:36:46","","10.21733/IBAD.833256","","",,,,,0,0.00,0,1,1,"In the competitive time, there has been a great deal of progress in the industry. It is one of the most serious obstacles to the industry in many industries that adopt contemporary technologies to manage continuous development and faster than ordinary jobs. Many of the scientists and researchers recommend using AI tools and digital technologies for industries. Machine language and artificial intelligence are used by many organizations in the human resources unit, where it undertakes an integrated task in recruiting, performance analysis, personnel selection, data collection for employees, providing real-time information and obtaining the right information. Artificial intelligence-based Human Resources (HR) applications have a solid potential to increase employee productivity and support HR experts to become knowledge and trained consultants that increase the success of the employee. HR applications authorized by artificial intelligence have the ability to analyze, predict, diagnose and seek and find more robust and capable resources.","",""
4,"Shubham Yadav, S. Ganesh, Debanjan Das, U. Venkanna, R. Mahapatra, A. Shrivastava, Prantar Chakrabarti, A. Talukder","Suśruta: Artificial Intelligence and Bayesian Knowledge Network in Health Care - Smartphone Apps for Diagnosis and Differentiation of Anemias with Higher Accuracy at Resource Constrained Point-of-Care Settings",2019,"","","","",27,"2022-07-13 09:36:46","","10.1007/978-3-030-37188-3_10","","",,,,,4,1.33,1,8,3,"","",""
1,"Pingping Sun, Lingang Gu","Fuzzy knowledge graph system for artificial intelligence-based smart education",2021,"","","","",28,"2022-07-13 09:36:46","","10.3233/JIFS-189332","","",,,,,1,1.00,1,2,1,"Fuzzy knowledge graph system is a semantic network that reveals the relationships between entities, and a tool or methodology that can formally describe things in the real world and their relationships. Smart education is an educational concept or model that uses advanced information technology to build a smart environment, integrates theory and practice to build an educational framework for information age, and provides paths to practice it. Artificial intelligence (AI) is a comprehensive discipline developed by the interpenetration of computer science, cybernetics, information theory, linguistics, neurophysiology and other disciplines, which is a direction for the development of information technology in the future. On the basis of summarizing and analyzing of previous research works, this paper expounded the research status and significance of AI technology, elaborated the development background, current status and future challenges of the construction and application of fuzzy knowledge graph system for smart education, introduced the methods and principles of data acquisition methods and digitalized apprenticeship, realized the process design, information extraction, entity recognition and relationship mining of smart education, constructed a systematic framework for fuzzy knowledge graph, and analyzed the high-quality resources sharing and personalized service of AI-assisted smart education, discussed automatic knowledge acquisition and fusion of fuzzy knowledge graph, performed co-occurrence relationship analysis, and finally conducted application case analysis. The results show that the smart education knowledge graph for AI-assisted smart education can integrate teaching experience and domain knowledge of discipline experts, enhance explainable and robust machine intelligence for AI-assisted smart education, and provide data-driven and knowledge-driven information processing methods; it can also discover the analysis hotspots and main content of research objects through clustering of high-frequency topic words, reveal the corresponding research structure in depth, and then systematically explore its research dimensions, subject background and theoretical basis.","",""
0,"A. Stanciu, A. M. Țîțu, Dorin Vasile Deac-Şuteu","Driving Digital Transformation Of Knowledge-Based Organizations Through Artificial Intelligence Enabled Data Centric, Consumption Based, As-A-Service Models",2021,"","","","",29,"2022-07-13 09:36:46","","10.1109/ECAI52376.2021.9515172","","",,,,,0,0.00,0,3,1,"A recent shift in the way people live, work and develop knowledge demands unlocking insights from personal, organizational, and market data. Starting with ingestion, processing, visualization, interactions through insights, cross-team efforts, and decision-making processes, artificial intelligence-based IT support systems are no longer an option in day-to-day activities but an imperative for reaching today’s organizations’ innovation, efficiency, and effectiveness goals. A consistent automation framework for the needed infrastructure, data, IT assets, and life cycle management is highly related to sustainability, security, compatibility, compliance, and legal regulations. The biggest challenge remains the continuous progress in both infrastructure and software advancements. This paper addresses the challenges of modern businesses’ digitalization, states, and demonstrates the solution through flexible, consumption-based information technology services. The purpose is to accelerate the transition from capital expenditures to operating expenses, thereby accelerating innovation and enhancing agility, security, compliance, flexibility, and cost control. The ultimate goal is to use IT as a platform, shifting from an operations realm to an innovation driver, thus creating new revenue streams.","",""
0,"Chengbing Tan, Qun Chen","Application of an artificial intelligence algorithm model of memory retrieval and roaming in sorting Chinese medicinal materials",2021,"","","","",30,"2022-07-13 09:36:46","","10.3233/jcm-215477","","",,,,,0,0.00,0,2,1,"In order to capture autobiographical memory, inspired by the development of human intelligence, a computational AM model for autobiographical memory is proposed in this paper, which is a three-layer network structure, in which the bottom layer encodes the event-specific knowledge comprising 5W1H, and provides retrieval clues to the middle layer, encodes the related events, and the top layer encodes the event set. According to the bottom-up memory search process, the corresponding events and event sets can be identified in the middle layer and the top layer respectively; At the same time, AM model can simulate human memory roaming through the process of rule-based memory retrieval. The computational AM model proposed in this paper not only has robust and flexible memory retrieval, but also has better response performance to noisy memory retrieval cues than the commonly used memory retrieval model based on keyword query method, and can also imitate the roaming phenomenon in memory.","",""
0,"Poona Bahrebar, Leon Denis, Maxim Bonnaerens, Kristof Coddens, J. Dambre, W. Favoreel, I. Khvastunov, A. Munteanu, Hung Nguyen-Duc, S. Schulte, D. Stroobandt, Ramses Valvekens, N. V. D. Broeck, Geert Verbruggen","cREAtIve: reconfigurable embedded artificial intelligence",2021,"","","","",31,"2022-07-13 09:36:46","","10.1145/3457388.3458857","","",,,,,0,0.00,0,14,1,"cREAtIve targets the development of novel highly-adaptable embedded deep learning solutions for automotive and traffic monitoring applications, including position sensor processing, scene interpretation based on LiDAR, and object detection and classification in thermal images for traffic camera systems. These applications share the need for deep learning solutions tailored for deployment on embedded devices with limited resources and featuring high adaptability and robustness to changing environmental conditions. cREAtIve develops knowledge, tools and methods that enable hardware-efficient, adaptable, and robust deep learning.","",""
0,"Khadijeh Karamzadeh, H. Moharrami","Survey of robust artificial intelligence classifier proper for various digital data",2015,"","","","",32,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,2,7,"Artificial intelligence or machine intelligence should be considered as the vast domain of junction of many knowledge, sciences and old and new technics. Today, classification of documents is adopted extensively in information recovery for organizing documents. In the method of document supervised classification some correct information about documents that previously have been classified are available for us and based on these information we classify these documents. Thus, we will examine methods such as: expert systems, artificial neural network, Genetic algorithm and fuzzy logics and so on. In this project we examine documents thematically and then using existing algorithms we predict a theme for a new document.","",""
21,"D. Ali, S. Frimpong","Artificial intelligence, machine learning and process automation: existing knowledge frontier and way forward for mining sector",2020,"","","","",33,"2022-07-13 09:36:46","","10.1007/s10462-020-09841-6","","",,,,,21,10.50,11,2,2,"","",""
9,"Kuansong Wang, Gang Yu, Chao Xu, Xiang-He Meng, Jian-hua Zhou, C. Zheng, Z. Deng, L. Shang, Ruijie Liu, S. Su, Xunjian Zhou, Qingling Li, Juanni Li, Jing Wang, K. Ma, J. Qi, Zhenmin Hu, P. Tang, Jeffrey Deng, X. Qiu, Bo Li, W. Shen, R. Quan, Juntao Yang, Lin Huang, Yao Xiao, Zhichun Yang, Zhongming Li, Shengchun Wang, Hongzheng Ren, C. Liang, Wei Guo, Yanchun Li, Heng Xiao, Yong-hong Gu, J. Yun, Dan Huang, Zhigang Song, Xiangshan Fan, Ling Chen, Xiaochu Yan, Zhi Li, Zhongjun Huang, Jufang Huang, Joseph Luttrell, Chaoyang Zhang, Weihua Zhou, Kun Zhang, C. Yi, Hui Shen","Accurate diagnosis of colorectal cancer based on histopathology images using artificial intelligence",2020,"","","","",34,"2022-07-13 09:36:46","","10.1186/s12916-021-01942-5","","",,,,,9,4.50,1,50,2,"","",""
39,"R. Confalonieri, Ludovik Çoba, Benedikt Wagner, Tarek R. Besold","A historical perspective of explainable Artificial Intelligence",2020,"","","","",35,"2022-07-13 09:36:46","","10.1002/widm.1391","","",,,,,39,19.50,10,4,2,"Explainability in Artificial Intelligence (AI) has been revived as a topic of active research by the need of conveying safety and trust to users in the “how” and “why” of automated decision‐making in different applications such as autonomous driving, medical diagnosis, or banking and finance. While explainability in AI has recently received significant attention, the origins of this line of work go back several decades to when AI systems were mainly developed as (knowledge‐based) expert systems. Since then, the definition, understanding, and implementation of explainability have been picked up in several lines of research work, namely, expert systems, machine learning, recommender systems, and in approaches to neural‐symbolic learning and reasoning, mostly happening during different periods of AI history. In this article, we present a historical perspective of Explainable Artificial Intelligence. We discuss how explainability was mainly conceived in the past, how it is understood in the present and, how it might be understood in the future. We conclude the article by proposing criteria for explanations that we believe will play a crucial role in the development of human‐understandable explainable systems.","",""
13,"Francesca Iandolo, F. Loia, Irene Fulco, Chiara Nespoli, F. Caputo","Combining Big Data and Artificial Intelligence for Managing Collective Knowledge in Unpredictable Environment—Insights from the Chinese Case in Facing COVID-19",2020,"","","","",36,"2022-07-13 09:36:46","","10.1007/s13132-020-00703-8","","",,,,,13,6.50,3,5,2,"","",""
1,"A. Zarzeczny, P. Babyn, S. Adams, Justin Longo","Artificial intelligence-based imaging analytics and lung cancer diagnostics: Considerations for health system leaders",2020,"","","","",37,"2022-07-13 09:36:46","","10.1177/0840470420975062","","",,,,,1,0.50,0,4,2,"Lung cancer is a leading cause of cancer death in Canada, and accurate, early diagnosis are critical to improving clinical outcomes. Artificial Intelligence (AI)-based imaging analytics are a promising healthcare innovation that aim to improve the accuracy and efficiency of lung cancer diagnosis. Maximizing their clinical potential while mitigating their risks and limitations will require focused leadership informed by interdisciplinary expertise and system-wide insight. We convened a knowledge exchange workshop with diverse Saskatchewan health system leaders and stakeholders to explore issues surrounding the use of AI in diagnostic imaging for lung cancer, including implementation opportunities, challenges, and priorities. This technology is anticipated to improve patient outcomes, reduce unnecessary healthcare spending, and increase knowledge. However, health system leaders must also address the needs for robust data, financial investment, effective communication and collaboration between healthcare sectors, privacy and data protections, and continued interdisciplinary research to achieve this technology’s potential benefits.","",""
8,"Sheikh Rabiul Islam, W. Eberle, S. Ghafoor, A. Siraj, Mike Rogers","Domain Knowledge Aided Explainable Artificial Intelligence for Intrusion Detection and Response",2019,"","","","",38,"2022-07-13 09:36:46","","","","",,,,,8,2.67,2,5,3,"Artificial Intelligence (AI) has become an integral part of modern-day security solutions for its ability to learn very complex functions and handling ""Big Data"". However, the lack of explainability and interpretability of successful AI models is a key stumbling block when trust in a model's prediction is critical. This leads to human intervention, which in turn results in a delayed response or decision. While there have been major advancements in the speed and performance of AI-based intrusion detection systems, the response is still at human speed when it comes to explaining and interpreting a specific prediction or decision. In this work, we infuse popular domain knowledge (i.e., CIA principles) in our model for better explainability and validate the approach on a network intrusion detection test case. Our experimental results suggest that the infusion of domain knowledge provides better explainability as well as a faster decision or response. In addition, the infused domain knowledge generalizes the model to work well with unknown attacks, as well as opens the path to adapt to a large stream of network traffic from numerous IoT devices.","",""
10,"S. Mukhopadhyay, Sumarga Kumar Sah Tyagi, N. Suryadevara, V. Piuri, F. Scotti, S. Zeadally","Artificial Intelligence-Based Sensors for Next Generation IoT Applications: A Review",2021,"","","","",39,"2022-07-13 09:36:46","","10.1109/JSEN.2021.3055618","","",,,,,10,10.00,2,6,1,"Sensors play a vital role in our daily lives and are an essential component for Internet of Things (IoT) based systems as they enable the IoT to collect data to take smart and intelligent decisions. Recent advances in IoT systems, applications, and technologies, including industrial Cyber-Physical Systems (CPSs), are being supported by a wide range of different types of sensors based on artificial intelligence (AI). These smart AI-based sensors are typically characterized by onboard intelligence and have the ability to communicate collaboratively or through the Internet. To achieve the high level of automation required in today’s smart IoT applications, sensors incorporated into nodes must be efficient, intelligent, context-aware, reliable, accurate, and connected. Such sensors must also be robust, safety- and privacy-aware for users interacting with them. Sensors leveraging advanced AI technologies, new capabilities have recently emerged which have the potential to detect, identify, and avoid performance degradation and discover new patterns. Along with knowledge from complex sensor datasets, they can promote product innovation, improve operation level, and open up novel business models. We review sensors, smart data processing, communication protocol, and artificial intelligence which will enable the deployment of AI-based sensors for next-generation IoT applications.","",""
0,"Cai Xin-lei, Qiu Rongfu, Cui Yanli, Xie Xinglang","Power Grid Auxiliary Control System Based on Big Data Application and Artificial Intelligence Decision",2020,"","","","",40,"2022-07-13 09:36:46","","10.1109/ICAICE51518.2020.00036","","",,,,,0,0.00,0,4,2,"This paper introduces artificial intelligence technology for the real-time dispatching business of the power grid based on big data applications and artificial intelligence thinking and decision-making. With security and error prevention as the criterion, automation and intelligence business system is builded. The system can assist control personnel to carry out daily business work. The system achieves the goals of reducing the load rate of regulatory personnel and improving the safety and convenience of power grid operations. It has the capabilities of safety and error prevention, and active power grid abnormal auxiliary decision-making based on expert library. Assisted driving and automatic driving is realized, which have the ability of ""high participation, command"". The system expand the knowledge base using machine learning. The system can proactively analyze operation events for auxiliary decision-making or autonomous disposal and improve dispatching business level and power grid security.","",""
0,"J. Blay, Jurgi Camblong, F. Sigaux","Artificial Intelligence Applied to Oncology",2020,"","","","",41,"2022-07-13 09:36:46","","10.1007/978-3-030-32161-1_24","","",,,,,0,0.00,0,3,2,"","",""
88,"Jeannette Paschen, Jan H. Kietzmann, T. Kietzmann","Artificial intelligence (AI) and its implications for market knowledge in B2B marketing",2019,"","","","",42,"2022-07-13 09:36:46","","10.1108/JBIM-10-2018-0295","","",,,,,88,29.33,29,3,3," Purpose The purpose of this paper is to explain the technological phenomenon artificial intelligence (AI) and how it can contribute to knowledge-based marketing in B2B. Specifically, this paper describes the foundational building blocks of any artificial intelligence system and their interrelationships. This paper also discusses the implications of the different building blocks with respect to market knowledge in B2B marketing and outlines avenues for future research.   Design/methodology/approach The paper is conceptual and proposes a framework to explicate the phenomenon AI and its building blocks. It further provides a structured discussion of how AI can contribute to different types of market knowledge critical for B2B marketing: customer knowledge, user knowledge and external market knowledge.   Findings The paper explains AI from an input–processes–output lens and explicates the six foundational building blocks of any AI system. It also discussed how the combination of the building blocks transforms data into information and knowledge.   Practical implications Aimed at general marketing executives, rather than AI specialists, this paper explains the phenomenon artificial intelligence, how it works and its relevance for the knowledge-based marketing in B2B firms. The paper highlights illustrative use cases to show how AI can impact B2B marketing functions.   Originality/value The study conceptualizes the technological phenomenon artificial intelligence from a knowledge management perspective and contributes to the literature on knowledge management in the era of big data. It addresses calls for more scholarly research on AI and B2B marketing. ","",""
3,"H. Mohammed, S. Ismail","Proposition of new computer artificial intelligence models for shear strength prediction of reinforced concrete beams",2021,"","","","",43,"2022-07-13 09:36:46","","10.1007/S00366-021-01400-Z","","",,,,,3,3.00,2,2,1,"","",""
0,"Chris Yang","Explainable Artificial Intelligence for Predictive Modeling in Healthcare",2022,"","","","",44,"2022-07-13 09:36:46","","10.1007/s41666-022-00114-1","","",,,,,0,0.00,0,1,1,"","",""
0,"D. Lange","Robustness of artificial intelligence in the face of novelty",2022,"","","","",45,"2022-07-13 09:36:46","","10.1117/12.2622912","","",,,,,0,0.00,0,1,1,"A critical factor in utilizing agents with Artificial Intelligence (AI) is their robustness to novelty. AI agents include models that are either engineered or trained. Engineered models include knowledge of those aspects of the environment that are known and considered important by the engineers. Learned models form embeddings of aspects of the environment based on connections made through the training data. In operation, however, a rich environment is likely to present challenges not seen in training sets or accounted for in engineered models. Worse still, adversarial environments are subject to change by opponents. A program at the Defense Advanced Research Project Agency (DARPA) seeks to develop the science necessary to develop and evaluate agents that are robust to novelty. This capability will be required, before AI has the role envisioned within mission critical environments.","",""
0,"Bukhoree Sahoh, Kanjana Haruehansapong, Mallika Kliangkhlao","Causal Artificial Intelligence for High-Stakes Decisions: The Design and Development of a Causal Machine Learning Model",2022,"","","","",46,"2022-07-13 09:36:46","","10.1109/access.2022.3155118","","",,,,,0,0.00,0,3,1,"A high-stakes decision requires deep thought to understand the complex factors that stop a situation from becoming worse. Such decisions are carried out under high pressure, with a lack of information, and in limited time. This research applies Causal Artificial Intelligence to high-stakes decisions, aiming to encode causal assumptions based on human-like intelligence, and thereby produce interpretable and argumentative knowledge. We develop a Causal Bayesian Networks model based on causal science using $d$ -separation and do-operations to discover the causal graph aligned with cognitive understanding. Causal odd ratios are used to measure the causal assumptions integrated with the real-world data to prove the proposed causal model compatibility. Causal effect relationships in the model are verified based on causal P-values and causal confident intervals and approved less than 1% by random chance. It shows that the causal model can encode cognitive understanding as precise, robust relationships. The concept of model design allows software agents to imitate human intelligence by inferring potential knowledge and be employed in high-stakes decision applications.","",""
0,"Xiaohong W. Gao, B. Braden","Artificial intelligence in endoscopy: The challenges and future directions",2021,"","","","",47,"2022-07-13 09:36:46","","10.37126/aige.v2.i4.117","","",,,,,0,0.00,0,2,1,"Artificial intelligence based approaches, in particular deep learning, have achieved state-of-the-art performance in medical fields with increasing number of software systems being approved by both Europe and United States. This paper reviews their applications to early detection of oesophageal cancers with a focus on their advantages and pitfalls. The paper concludes with future recommendations towards the development of a real-time, clinical implementable, interpretable and robust diagnosis support systems.","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",48,"2022-07-13 09:36:46","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
0,"Beilei Wang, Jie Jing, Xiaochun Huang, Cheng Hua, Qin Qin, Y. Jia, Zhiyong Wang, Lei Jiang, Bai Gao, Les J. Wu, Xianfei Zeng, Fubo Wang, Chuanbin Mao, Shanrong Liu","Establishment of a Knowledge‐and‐Data‐Driven Artificial Intelligence System with Robustness and Interpretability in Laboratory Medicine",2022,"","","","",49,"2022-07-13 09:36:46","","10.1002/aisy.202100204","","",,,,,0,0.00,0,14,1,"Laboratory medicine plays an important role in clinical diagnosis. However, no laboratory‐based artificial intelligence (AI) diagnostic system has been applied in current clinical practice due to the lack of robustness and interpretability. Although many attempts have been made, it is still difficult for doctors to adopt the existing machine learning (ML) patterns in interpreting laboratory (lab) big data. Here, a knowledge‐and‐data‐driven laboratory diagnostic system is developed, termed AI‐based Lab tEst tO diagNosis (AI LEON), by integrating an innovative knowledge graph analysis framework and “mixed XGboost and Genetic Algorithm (MiXG)” technique to simulate the doctor's laboratory‐based diagnosis. To establish AI LEON, we included 89 116 949 laboratory data and 10 423 581 diagnosis data points from 730 113 participants. Among them, 686 626 participants were recruited for training and validating purposes with the remaining for testing purposes. AI LEON automatically identified and analyzed 2071 lab indexes, resulting in multiple disease recommendations that involved 441 common diseases in ten organ systems. AI LEON exhibited outstanding transparency and interpretability in three universal clinical application scenarios and outperformed human physicians in interpreting lab reports. AI LEON is an advanced intelligent system that enables a comprehensive interpretation of lab big data, which substantially improves the clinical diagnosis.","",""
0,"Iván Manuel De la Vega Hernández, Angel Serrano Urdaneta, E. Carayannis","Global bibliometric mapping of the frontier of knowledge in the field of artificial intelligence for the period 1990–2019",2022,"","","","",50,"2022-07-13 09:36:46","","10.1007/s10462-022-10206-4","","",,,,,0,0.00,0,3,1,"","",""
7,"Redmond R. Shamshiri, Ibrahim A. Hameed, Kelly R. Thorp, Siva K. Balasundram, S. Shafian, Mohammad Fatemieh, M. Sultan, B. Mahns, S. Samiei","Greenhouse Automation Using Wireless Sensors and IoT Instruments Integrated with Artificial Intelligence",2021,"","","","",51,"2022-07-13 09:36:46","","10.5772/INTECHOPEN.97714","","",,,,,7,7.00,1,9,1,"Automation of greenhouse environment using simple timer-based actuators or by means of conventional control algorithms that require feedbacks from offline sensors for switching devices are not efficient solutions in large-scale modern greenhouses. Wireless instruments that are integrated with artificial intelligence (AI) algorithms and knowledge-based decision support systems have attracted growers’ attention due to their implementation flexibility, contribution to energy reduction, and yield predictability. Sustainable production of fruits and vegetables under greenhouse environments with reduced energy inputs entails proper integration of the existing climate control systems with IoT automation in order to incorporate real-time data transfer from multiple sensors into AI algorithms and crop growth models using cloud-based streaming systems. This chapter provides an overview of such an automation workflow in greenhouse environments by means of distributed wireless nodes that are custom-designed based on the powerful dual-core 32-bit microcontroller with LoRa modulation at 868 MHz. Sample results from commercial and research greenhouse experiments with the IoT hardware and software have been provided to show connection stability, robustness, and reliability. The presented setup allows deployment of AI on embedded hardware units such as CPUs and GPUs, or on cloud-based streaming systems that collect precise measurements from multiple sensors in different locations inside greenhouse environments.","",""
0,"S. Sadeghi, M. Amiri, Farzaneh Mansoori Mooseloo","Artificial Intelligence and Its Application in Optimization under Uncertainty",2021,"","","","",52,"2022-07-13 09:36:46","","10.5772/intechopen.98628","","",,,,,0,0.00,0,3,1,"Nowadays, the increase in data acquisition and availability and complexity around optimization make it imperative to jointly use artificial intelligence (AI) and optimization for devising data-driven and intelligent decision support systems (DSS). A DSS can be successful if large amounts of interactive data proceed fast and robustly and extract useful information and knowledge to help decision-making. In this context, the data-driven approach has gained prominence due to its provision of insights for decision-making and easy implementation. The data-driven approach can discover various database patterns without relying on prior knowledge while also handling flexible objectives and multiple scenarios. This chapter reviews recent advances in data-driven optimization, highlighting the promise of data-driven optimization that integrates mathematical programming and machine learning (ML) for decision-making under uncertainty and identifies potential research opportunities. This chapter provides guidelines and implications for researchers, managers, and practitioners in operations research who want to advance their decision-making capabilities under uncertainty concerning data-driven optimization. Then, a comprehensive review and classification of the relevant publications on the data-driven stochastic program, data-driven robust optimization, and data-driven chance-constrained are presented. This chapter also identifies fertile avenues for future research that focus on deep-data-driven optimization, deep data-driven models, as well as online learning-based data-driven optimization. Perspectives on reinforcement learning (RL)-based data-driven optimization and deep RL for solving NP-hard problems are discussed. We investigate the application of data-driven optimization in different case studies to demonstrate improvements in operational performance over conventional optimization methodology. Finally, some managerial implications and some future directions are provided.","",""
31,"I. Habli, T. Lawton, Zoe Porter","Artificial intelligence in health care: accountability and safety",2020,"","","","",53,"2022-07-13 09:36:46","","10.2471/BLT.19.237487","","",,,,,31,15.50,10,3,2,"Abstract The prospect of patient harm caused by the decisions made by an artificial intelligence-based clinical tool is something to which current practices of accountability and safety worldwide have not yet adjusted. We focus on two aspects of clinical artificial intelligence used for decision-making: moral accountability for harm to patients; and safety assurance to protect patients against such harm. Artificial intelligence-based tools are challenging the standard clinical practices of assigning blame and assuring safety. Human clinicians and safety engineers have weaker control over the decisions reached by artificial intelligence systems and less knowledge and understanding of precisely how the artificial intelligence systems reach their decisions. We illustrate this analysis by applying it to an example of an artificial intelligence-based system developed for use in the treatment of sepsis. The paper ends with practical suggestions for ways forward to mitigate these concerns. We argue for a need to include artificial intelligence developers and systems safety engineers in our assessments of moral accountability for patient harm. Meanwhile, none of the actors in the model robustly fulfil the traditional conditions of moral accountability for the decisions of an artificial intelligence system. We should therefore update our conceptions of moral accountability in this context. We also need to move from a static to a dynamic model of assurance, accepting that considerations of safety are not fully resolvable during the design of the artificial intelligence system before the system has been deployed.","",""
1,"Rahul Sadashiv Kharat, T. Devi","Artificial Intelligence in Environmental Management",2021,"","","","",54,"2022-07-13 09:36:46","","10.1201/9781003175865-3","","",,,,,1,1.00,1,2,1,"Artificial intelligence is the way of integrating technology and nature, to refine the understanding of living with nature as everything we find synthetic, artificial, l is not their own, but replicated from nature. Artificial intelligence is nothing but the inferences drawn from the real time situation by subjecting the fact related to them by a natural process as neural networks, genetic algorithm and several different optimization techniques, that the strategy, approach becomes specific to the issue solved by these training programs. Life is secured to none, it is predefined to none, it cannot be scheduled for the illusions of life, as time, space, knowledge, wish, and power but for the purpose, understanding that evolution, changes are unavoidable and which happens through openness to accept facts, analyze the need to get into the changes for progression. The nature as the supreme force has every way for simple living which we ignore since we think we are superior to nature. All the nature based artificial intelligence algorithms are now available as black box model and depending on the nature , trend and data available , any problem can be solved easily. We can simulate exactly similar situation if we understand the data and their occurrence in line with the nature. The less the volume of data, more accurate will be the prediction as the same do include outliers and represent immediate real time situations. The volume of data is not an issue, when we fix the underlying element as individual specific. When this is difficult in conventional statistical methods, considering the data with equal weights and individual specific is very much possible with artificial intelligence. It is possible to relate two independent variables for their influence","",""
1,"K. Panetta, Landry Kezebou, Victor Oludare, J. Intriligator, S. Agaian","Artificial Intelligence for Text-Based Vehicle Search, Recognition, and Continuous Localization in Traffic Videos",2021,"","","","",55,"2022-07-13 09:36:46","","10.3390/ai2040041","","",,,,,1,1.00,0,5,1,"The concept of searching and localizing vehicles from live traffic videos based on descriptive textual input has yet to be explored in the scholarly literature. Endowing Intelligent Transportation Systems (ITS) with such a capability could help solve crimes on roadways. One major impediment to the advancement of fine-grain vehicle recognition models is the lack of video testbench datasets with annotated ground truth data. Additionally, to the best of our knowledge, no metrics currently exist for evaluating the robustness and performance efficiency of a vehicle recognition model on live videos and even less so for vehicle search and localization models. In this paper, we address these challenges by proposing V-Localize, a novel artificial intelligence framework for vehicle search and continuous localization captured from live traffic videos based on input textual descriptions. An efficient hashgraph algorithm is introduced to compute valid target information from textual input. This work further introduces two novel datasets to advance AI research in these challenging areas. These datasets include (a) the most diverse and large-scale Vehicle Color Recognition (VCoR) dataset with 15 color classes—twice as many as the number of color classes in the largest existing such dataset—to facilitate finer-grain recognition with color information; and (b) a Vehicle Recognition in Video (VRiV) dataset, a first of its kind video testbench dataset for evaluating the performance of vehicle recognition models in live videos rather than still image data. The VRiV dataset will open new avenues for AI researchers to investigate innovative approaches that were previously intractable due to the lack of annotated traffic vehicle recognition video testbench dataset. Finally, to address the gap in the field, five novel metrics are introduced in this paper for adequately accessing the performance of vehicle recognition models in live videos. Ultimately, the proposed metrics could also prove intuitively effective at quantitative model evaluation in other video recognition applications. T One major advantage of the proposed vehicle search and continuous localization framework is that it could be integrated in ITS software solution to aid law enforcement, especially in critical cases such as of amber alerts or hit-and-run incidents.","",""
1,"Sonal Modak, D. Sehgal, J. Valadi","Applications of Artificial Intelligence and Machine Learning in Viral Biology",2019,"","","","",56,"2022-07-13 09:36:46","","10.1007/978-3-030-29022-1_1","","",,,,,1,0.33,0,3,3,"","",""
7,"A. Massaro, Palo Lisco, A. Lombardi, A. Galiano, Nicola Savino","A Case Study of Research Improvements in an Service Industry Upgrading the Knowledge Base of the Information System and the Process Management: Data Flow Automation, Association Rules and Data Mining",2019,"","","","",57,"2022-07-13 09:36:46","","10.5121/IJAIA.2019.10103","","",,,,,7,2.33,1,5,3,"In this paper is analyzed a case study of an upgrade of an industry communication system developed by following ‘Frascati’ research guidelines. The goal of the proposed model is to enhance the industry knowledge Base –KB- by acting directly on information communication system improvements and data system integration, enabling automated process and data processing. The paper follow all the steps performed during the project development: the preliminary data infrastructure design, the information infrastructure improvements, and data processing. Data processing is performed by a calculus engine embedding data mining association rules and Artificial Neural Network –ANN- predictive algorithms thus improving the research. The calculus engine has been implemented by a multiple variables model where the contract data are preliminary processed in order to define functions classifying the operation processes and activating automatically the service process management. The business intelligence –BI- operations are performed mainly by the calculus engine optimizing industry performances. The goal of the paper is to show how research and development –R&D- can be applied by gaining and optimizing the knowledge and processes of an Italian industry working in car services. The project has been developed with the collaboration of the industry ACI Global working in roadside assistance services. By means of a research project resources, the information technology –IT- infrastructure has been improved by new solutions of the communication system and of the data transfer. The proposed case of study provides a model and a guideline to follow in order to apply research in industry acting directly on data and information network.","",""
25,"Abbas Abbaszadeh Shahri, S. Larsson, Crister Renkel","Artificial intelligence models to generate visualized bedrock level: a case study in Sweden",2020,"","","","",58,"2022-07-13 09:36:46","","10.1007/s40808-020-00767-0","","",,,,,25,12.50,8,3,2,"","",""
0,"Yaxin Peng, S. Du, T. Zeng","Preface: Special Issue on Optimization Models and Algorithms in Artificial Intelligence",2019,"","","","",59,"2022-07-13 09:36:46","","10.1007/s40305-019-00278-5","","",,,,,0,0.00,0,3,3,"","",""
2,"M. Shahid, G. Abbas, Mohammad Rashid Hussain, M. Asad, U. Farooq, J. Gu, V. Balas, M. Uzair, A. Awan, T. Yazdan","Artificial Intelligence-Based Controller for DC-DC Flyback Converter",2019,"","","","",60,"2022-07-13 09:36:46","","10.3390/app9235108","","",,,,,2,0.67,0,10,3,"This paper presents an intelligent voltage controller designed on the basis of an adaptive neuro-fuzzy inference system (ANFIS) for a flyback converter (FC) working in continuous conduction mode (CCM). The union of fuzzy logic (FL) and adaptive neural networks (ANN) makes ANFIS more robust against model parameters’ uncertainties and perturbations in input voltage or load current. ANFIS inherits the advantages of structured knowledge representation from FL and learning capability from NN. Comparative analysis showed that the ANFIS controller offers not only the superior transient response characteristics, but also excellent steady-state characteristics compared to those of the FL controller (FLC) and proportional–integral–derivative (PID) controllers, thus validating its superiority over these traditional controllers. For this purpose, MATLAB/Simulink environment-based simulation results are presented for validation of the proposed converter compensated system under all operating conditions.","",""
1,"Latifa Mrisho, N. Mbilinyi, Mathias Ndalahwa, Amanda Ramcharan, Annalyse Kehs, Peter McCloskey, H. Murithi, David P. Hughes, J. Legg","Evaluating the accuracy of a smartphone-based artificial intelligence system, PlantVillage Nuru, in diagnosing of the viral diseases of cassava",2020,"","","","",61,"2022-07-13 09:36:46","","10.1101/2020.01.26.919449","","",,,,,1,0.50,0,9,2,"Premise of the study Nuru is an artificial intelligence system for diagnosis of plant diseases and pests developed as a public good by PlantVillage (Penn State University), FAO, IITA and CIMMYT. It provides a simple, inexpensive and robust means of conducting in-field diagnosis without requiring internet connection and provides real-time results and advice. The present work evaluates the effectiveness of Nuru as an in-field diagnostic tool by comparing the diagnosis capability of Nuru to that of cassava experts (researchers trained on cassava pests and diseases), agricultural extension agents and farmers. Methods The diagnosis capability of Nuru and that of the assessed individuals was determined by inspecting cassava plants in-field and by using the cassava symptom recognition assessment tool (CaSRAT) to score images of cassava leaves. Results Nuru’s accuracy for symptom recognition when using six leaves (74 - 88%, depending on the condition) was similar to that of experts, 1.5-times higher than agricultural extension agents and two-times higher than farmers. Discussion These findings suggests that Nuru can be an effective tool for in-field diagnosis of cassava diseases and has a potential of being a quick and cost-effective means of disseminating knowledge from researchers to agricultural extension agents and farmers.","",""
19,"Simone Castagno, Mohamed Khalifa","Perceptions of Artificial Intelligence Among Healthcare Staff: A Qualitative Survey Study",2020,"","","","",62,"2022-07-13 09:36:46","","10.3389/frai.2020.578983","","",,,,,19,9.50,10,2,2,"Objectives: The medical community is in agreement that artificial intelligence (AI) will have a radical impact on patient care in the near future. The purpose of this study is to assess the awareness of AI technologies among health professionals and to investigate their perceptions toward AI applications in medicine. Design: A web-based Google Forms survey was distributed via the Royal Free London NHS Foundation Trust e-newsletter. Setting: Only staff working at the NHS Foundation Trust received an invitation to complete the online questionnaire. Participants: 98 healthcare professionals out of 7,538 (response rate 1.3%; CI 95%; margin of error 9.64%) completed the survey, including medical doctors, nurses, therapists, managers, and others. Primary outcome: To investigate the prior knowledge of health professionals on the subject of AI as well as their attitudes and worries about its current and future applications. Results: 64% of respondents reported never coming across applications of AI in their work and 87% did not know the difference between machine learning and deep learning, although 50% knew at least one of the two terms. Furthermore, only 5% stated using speech recognition or transcription applications on a daily basis, while 63% never utilize them. 80% of participants believed there may be serious privacy issues associated with the use of AI and 40% considered AI to be potentially even more dangerous than nuclear weapons. However, 79% also believed AI could be useful or extremely useful in their field of work and only 10% were worried AI will replace them at their job. Conclusions: Despite agreeing on the usefulness of AI in the medical field, most health professionals lack a full understanding of the principles of AI and are worried about potential consequences of its widespread use in clinical practice. The cooperation of healthcare workers is crucial for the integration of AI into clinical practice and without it the NHS may miss out on an exceptionally rewarding opportunity. This highlights the need for better education and clear regulatory frameworks.","",""
0,"IM Ing. DI Dragos-Cristian Vasilescu, Michael Filzmoser, PhD, Diana Löffler, Andreas Theodorou, Johannes Grössl, Jan G. Michel, Vanessa Schäffner, Stefan Reining, Julia Alessandra Harzheim, Rebecca Davnall, Kilian Karger, Leonie Seng, Lukas Brand, A. Wykowska, D. Neumann, Walther Ch. Zimmerli, Scarlet Schaffrath, David J. Gunkel, Carmen Krämer, Astrid Marieke Rosenthal-von der Pütten, Benedikt Paul Göcke, Andreas Bischof, Arne Maibaum, Gábor L. Ambrus, Tobias Müller, B. P. Göcke","Artificial Intelligence",2019,"","","","",63,"2022-07-13 09:36:46","","10.1017/9781108555814.018","","",,,,,0,0.00,0,26,3,"The Collaborative Specialization in Artificial Intelligence (AI) provides thesis-based Master's students in Computer Science, Engineering, Mathematics and Statistics, and Bioinformatics with a diverse and comprehensive knowledge base in AI. Students wishing to undertake graduate studies at the Master's level with emphasis on artificial intelligence will be admitted by a participating department and will register in both the participating department and in the collaborative specialization.","",""
0,"E. Cambouropoulos, Maximos A. Kaliakatsos-Papakostas","Cognitive Musicology and Artificial Intelligence: Harmonic Analysis, Learning, and Generation",2021,"","","","",64,"2022-07-13 09:36:46","","10.1007/978-3-030-72116-9_10","","",,,,,0,0.00,0,2,1,"","",""
2,"Qiwei-Kong, Jing He, Peizhuang Wang","Factor space:a new idea for artificial intelligence based on causal reasoning",2020,"","","","",65,"2022-07-13 09:36:46","","10.1109/WIIAT50758.2020.00089","","",,,,,2,1.00,1,3,2,"In the rapid development of artificial intelligence in the last several years, machine learning is the mainstream method to realize artificial intelligence. What people usually call machine learning can be equivalent to statistical learning, which requires big data and powerful computing power; This is a machine learning trend driven by data, using algorithms to get a model with clear parameters, ignoring causal reasoning and focusing on statistical data; The machine learning method lacking logical causal reasoning will greatly hinder the advancement of artificial intelligence; How knowledge-driven causal reasoning provides new ideas for artificial intelligence is a question worth thinking about for scholars of artificial intelligence. Factor space theory that emphasizes causal reasoning will provide a new perspective and thinking for the development of artificial intelligence.","",""
5,"David Abele, Sara D’Onofrio","Artificial Intelligence – The Big Picture",2020,"","","","",66,"2022-07-13 09:36:46","","10.1007/978-3-658-27941-7_2","","",,,,,5,2.50,3,2,2,"","",""
0,"Wei Yan","IEEE Transactions on Artificial Intelligence",2020,"","","","",67,"2022-07-13 09:36:46","","10.1109/tfuzz.2020.2987029","","",,,,,0,0.00,0,1,2,"The IEEE Transactions on Artificial Intelligence (TAI) is a multidisciplinary journal publishing papers on theories and methodologies of Artificial Intelligence. Applications of Artificial Intelligence are also considered. Topics covered by IEEE TAI include, but not limited to, Agent-based Systems, Augmented Intelligence, Autonomic Computing, Constraint Systems, Explainable AI, Knowledge-Based Systems, Learning Theories, Planning, Reasoning, Search, Natural Language Processing, and Applications. Technical papers addressing contemporary topics in AI such as Ethics and Social Implications are welcomed.","",""
2,"Yuan Huang, Z. Cheng, Qianyu Zhou, Yuxing Xiang, Ruixiao Zhao","Data Mining Algorithm for Cloud Network Information Based on Artificial Intelligence Decision Mechanism",2020,"","","","",68,"2022-07-13 09:36:46","","10.1109/ACCESS.2020.2981632","","",,,,,2,1.00,0,5,2,"Due to the rapid development of information technology and network technology, there is a lot of data, but the phenomenon of lack of knowledge is becoming more and more serious. Data mining technology has developed vigorously in this environment, and it has shown more and more vitality. Based on Spark programming model, this paper designs the parallel extension of fuzzy c-means. In order to enhance the performance of fuzzy c-means parallel expansion, the improvement strategy of k-means during the initialization phase is borrowed, and k-means// is extended to fuzzy c-means to obtain better clustering performance. Combined with Spark’s programming model, this paper can obtain extended parallel fuzzy c-means algorithm. Several experiments on the data set of the algorithm proposed in this paper have shown good scalability and parallelism, effectively expanding fuzzy c-means clustering to distributed applications, greatly increasing the scale of the data processed by the algorithm. This improves the robustness of the algorithm and the adaptability of the algorithm to the shape and structure of the data, so that the parallel and scalable clustering algorithm can more effectively perform cluster analysis on big data. Three algorithms were simulated on MATLAB platform. We use simple data sets and complex two-dimensional data sets, and compare with the traditional fuzzy c-means algorithm and fuzzy c-means algorithm based on fuzzy entropy. Experiments show that the scalable parallel fuzzy c-means algorithm not only greatly improves the anti-noise performance, but also improves the convergence speed, and it can automatically determine the optimal number of clusters.","",""
1,"Shengtao Dong, J. Li, Haozong Zhao, Yu-Zhen Zheng, Yaoning Chen, Junxi Shen, Huan Yang, Jieyang Zhu","Risk Factor Analysis for Predicting the Onset of Rotator Cuff Calcific Tendinitis Based on Artificial Intelligence",2022,"","","","",69,"2022-07-13 09:36:46","","10.1155/2022/8978878","","",,,,,1,1.00,0,8,1,"Background Symptomatic rotator cuff calcific tendinitis (RCCT) is a common shoulder disorder, and approaches combined with artificial intelligence greatly facilitate the development of clinical practice. Current scarce knowledge of the onset suggests that clinicians may need to explore this disease thoroughly. Methods Clinical data were retrospectively collected from subjects diagnosed with RCCT at our institution within the period 2008 to 2020. A standardized questionnaire related to shoulder symptoms was completed in all cases, and standardized radiographs of both shoulders were extracted using a human-computer interactive electronic medical system (EMS) to clarify the clinical diagnosis of symptomatic RCCT. Based on the exclusion of asymptomatic subjects, risk factors in the baseline characteristics significantly associated with the onset of symptomatic RCCT were assessed via stepwise logistic regression analysis. Results Of the 1,967 consecutive subjects referred to our academic institution for shoulder discomfort, 237 were diagnosed with symptomatic RCCT (12.05%). The proportion of women and the prevalence of clinical comorbidities were significantly higher in the RCCT cohort than those in the non-RCCT cohort. Stepwise logistic regression analysis confirmed that female gender, hyperlipidemia, diabetes mellitus, and hypothyroidism were independent risk factors for the entire cohort. Stratified by gender, the study found a partial overlap of risk factors contributing to morbidity in men and women. Diagnosis of hyperlipidemia, diabetes mellitus, and hypothyroidism in male cases and diabetes mellitus in female cases were significantly associated with symptomatic RCCT. Conclusion Independent predictors of symptomatic RCCT are female, hyperlipidemia, diabetes mellitus, and hypothyroidism. Men diagnosed with hyperlipidemia, diabetes mellitus, and hypothyroidism are at high risk for symptomatic RCCT, while more medical attention is required for women with diabetes mellitus. Artificial intelligence offers pioneering innovations in the diagnosis and treatment of musculoskeletal disorders, and careful assessment through individualized risk stratification can help predict onset and targeted early stage treatment.","",""
1,"F. Voskens, J. Abbing, A. T. Ruys, J. Ruurda, I. Broeders","A nationwide survey on the perceptions of general surgeons on artificial intelligence",2022,"","","","",70,"2022-07-13 09:36:46","","10.20517/ais.2021.10","","",,,,,1,1.00,0,5,1,"Aim: Artificial intelligence (AI) has the potential to improve perioperative diagnosis and decision making. Despite promising study results, the majority of AI platforms in surgery currently remain in the research setting. Understanding the current knowledge and general attitude of surgeons toward AI applications in their surgical practice is essential and can contribute to the future development and uptake of AI in surgery. Methods: In March 2021, a web-based survey was conducted among members of the Dutch Association of Surgery. The survey measured opinions on the existing knowledge, expectations, and concerns on AI among surgical residents and surgeons. Results: A total of 313 respondents completed the survey. Overall, 85% of the respondents agreed that AI could be of value in the surgical field and 61% expected AI to improve their diagnostic ability. The outpatient clinic (35.8%) and operating room (39.6%) were stated as area of interest for the use of AI. Statistically, surgeons working in an academic hospital were more likely to be aware of the possibilities of AI (P = 0.01). The surgeons in this survey were not worried about job replacement, however they raised the greatest concerns on accountability issues (50.5%), loss of autonomy (46.6%), and risk of bias (43.5%). Conclusion: This survey demonstrates that the majority of the surgeons show a positive and open attitude towards AI. Although various ethical issues and concerns arise, the expectations regarding the implementation of future surgical AI applications are high.","",""
0,"Qun Luo, Jiliang Yang","The Artificial Intelligence and Neural Network in Teaching",2022,"","","","",71,"2022-07-13 09:36:46","","10.1155/2022/1778562","","",,,,,0,0.00,0,2,1,"This study aims to explore the application of artificial intelligence (AI) and network technology in teaching. By studying the AI-based smart classroom teaching mode and the advantages and disadvantages of network teaching using network technology and taking the mathematics classroom as an example, this study makes an intelligent analysis of the questioning link of classroom teachers in the teaching process. For the questions raised by teachers, the network classification models of convolutional neural network (CNN) and long short-term memory (LSTM) are used to classify the questions according to the content and types of questions and carry out experimental verification. The results show that the overall performance of the CNN model is better than that of the LSTM model in the classification results of the teacher's question content dimension. CNN has higher accuracy, and the classification accuracy of essential knowledge points reaches 86.3%. LSTM is only 79.2%, and CNN improves by 8.96%. In the classification results of teacher question types, CNN has higher accuracy. The classification accuracy of the prompt question is the highest, reaching 87.82%. LSTM is only 83.2%, and CNN improves by 4.95%. CNN performs better in teacher question classification results.","",""
4,"Qi Deng","Blockchain Economical Models, Delegated Proof of Economic Value and Delegated Adaptive Byzantine Fault Tolerance and their implementation in Artificial Intelligence BlockCloud",2019,"","","","",72,"2022-07-13 09:36:46","","10.3390/jrfm12040177","","",,,,,4,1.33,4,1,3,"The Artificial Intelligence BlockCloud (AIBC) is an artificial intelligence and blockchain technology based large-scale decentralized ecosystem that allows system-wide low-cost sharing of computing and storage resources. The AIBC consists of four layers: a fundamental layer, a resource layer, an application layer, and an ecosystem layer (the latter three are the collective “upper-layers”). The AIBC layers have distinguished responsibilities and thus performance and robustness requirements. The upper layers need to follow a set of economic policies strictly and run on a deterministic and robust protocol. While the fundamental layer needs to follow a protocol with high throughput without sacrificing robustness. As such, the AIBC implements a two-consensus scheme to enforce economic policies and achieve performance and robustness: Delegated Proof of Economic Value (DPoEV) incentive consensus on the upper layers, and Delegated Adaptive Byzantine Fault Tolerance (DABFT) distributed consensus on the fundamental layer. The DPoEV uses the knowledge map algorithm to accurately assess the economic value of digital assets. The DABFT uses deep learning techniques to predict and select the most suitable BFT algorithm in order to enforce the DPoEV, as well as to achieve the best balance of performance, robustness, and security. The DPoEV-DABFT dual-consensus architecture, by design, makes the AIBC attack-proof against risks such as double-spending, short-range and 51% attacks; it has a built-in dynamic sharding feature that allows scalability and eliminates the single-shard takeover. Our contribution is four-fold: that we develop a set of innovative economic models governing the monetary, trading and supply-demand policies in the AIBC; that we establish an upper-layer DPoEV incentive consensus algorithm that implements the economic policies; that we provide a fundamental layer DABFT distributed consensus algorithm that executes the DPoEV with adaptability; and that we prove the economic models can be effectively enforced by AIBC’s DPoEV-DABFT dual-consensus architecture.","",""
27,"N. S. Saravana Kumar","IMPLEMENTATION OF ARTIFICIAL INTELLIGENCE IN IMPARTING EDUCATION AND EVALUATING STUDENT PERFORMANCE",2019,"","","","",73,"2022-07-13 09:36:46","","10.36548/jaicn.2019.1.001","","",,,,,27,9.00,27,1,3,"Simulation of human intelligence process is made possible with the help of artificial intelligence. The learning, reasoning and self-correction properties are made possible in computer systems. Along with AI, other technologies are combined effectively in order to create remarkable applications. We apply the changing role of AI and its techniques in new educational paradigms to create a personalised teaching-learning environment. Features like recognition, pattern matching, decision making, reasoning, problem solving and so on are applied along with knowledge based system and supervised machine learning for a complete learning and assessment process.","",""
1,"Fang Zhang, Xiaochen Wang, J. Han, Jie Tang, Shiyin Wang","Fast Top-k Area Topics Extraction with Knowledge Base",2017,"","","","",74,"2022-07-13 09:36:46","","10.1109/DSC.2018.00016","","",,,,,1,0.20,0,5,5,"What are the most representative research topics in Artificial Intelligence (AI)? We formulate the problem as extracting top-k topics that can best represent a given area with the help of knowledge base. We theoretically prove that the problem is NP-hard and propose an optimization model, FastKATE, to address this problem by combining both explicit and latent representations for each topic. We leverage a large-scale knowledge base (Wikipedia) to generate topic embeddings using neural networks and use this kind of representations to help capture the representativeness of topics for given areas. We develop a fast heuristic algorithm to efficiently solve the problem with a provable error bound. We evaluate the proposed model on three real-world datasets. Experimental results demonstrate our model's effectiveness, robustness, real-timeness (return results in <1s), and its superiority over several alternative methods.","",""
26779,"Stuart J. Russell, Peter Norvig","Artificial Intelligence: A Modern Approach",1995,"","","","",75,"2022-07-13 09:36:46","","10.5860/choice.33-1577","","",,,,,26779,991.81,13390,2,27,"The long-anticipated revision of this #1 selling book offers the most comprehensive, state of the art introduction to the theory and practice of artificial intelligence for modern applications. Intelligent Agents. Solving Problems by Searching. Informed Search Methods. Game Playing. Agents that Reason Logically. First-order Logic. Building a Knowledge Base. Inference in First-Order Logic. Logical Reasoning Systems. Practical Planning. Planning and Acting. Uncertainty. Probabilistic Reasoning Systems. Making Simple Decisions. Making Complex Decisions. Learning from Observations. Learning with Neural Networks. Reinforcement Learning. Knowledge in Learning. Agents that Communicate. Practical Communication in English. Perception. Robotics. For computer professionals, linguists, and cognitive scientists interested in artificial intelligence.","",""
0,"Gang Li, Tongzhou Zhao","Approach of Intelligence Question-Answering System Based on Physical Fitness Knowledge Graph",2021,"","","","",76,"2022-07-13 09:36:46","","10.1109/RCAE53607.2021.9638824","","",,,,,0,0.00,0,2,1,"Artificial intelligence’s penetrating sports is a new development trend of modern sports. Physical Intelligence Question-Answering System (QAS) is a typical application of artificial intelligence in sports, which can quickly respond the physic fitness questions raised by people. This paper aims the construction method of physical fitness QAS based on knowledge graph. Firstly, the physical fitness knowledge graph is constructed based on the crawling data and expert knowledge. Secondly, several physical knowledge question templates are constructed. Thirdly, the Bayesian classifier is used to classify the questions and the Bidirectional Long Short-Term Memory (BiLSTM) combined with Conditional Random Fields (CRF) method is applied to extract contents from the input questions. Then a matching algorithm based on bidirectional slicing string and a statistical method are performed to implement the fuzzy query to enhance the accuracy and robustness of the QAS. The experiments show that the accuracy of physical fitness QAS can reach 93.5% when the question sentences are matched with the query templates, and 86.5% when not matched.","",""
427,"D. Ting, L. Pasquale, L. Peng, J. P. Campbell, Aaron Y. Lee, R. Raman, G. Tan, L. Schmetterer, P. Keane, T. Wong","Artificial intelligence and deep learning in ophthalmology",2018,"","","","",77,"2022-07-13 09:36:46","","10.1136/bjophthalmol-2018-313173","","",,,,,427,106.75,43,10,4,"Artificial intelligence (AI) based on deep learning (DL) has sparked tremendous global interest in recent years. DL has been widely adopted in image recognition, speech recognition and natural language processing, but is only beginning to impact on healthcare. In ophthalmology, DL has been applied to fundus photographs, optical coherence tomography and visual fields, achieving robust classification performance in the detection of diabetic retinopathy and retinopathy of prematurity, the glaucoma-like disc, macular oedema and age-related macular degeneration. DL in ocular imaging may be used in conjunction with telemedicine as a possible solution to screen, diagnose and monitor major eye diseases for patients in primary care and community settings. Nonetheless, there are also potential challenges with DL application in ophthalmology, including clinical and technical challenges, explainability of the algorithm results, medicolegal issues, and physician and patient acceptance of the AI ‘black-box’ algorithms. DL could potentially revolutionise how ophthalmology is practised in the future. This review provides a summary of the state-of-the-art DL systems described for ophthalmic applications, potential challenges in clinical deployment and the path forward.","",""
15,"Yun-he Pan","Special issue on artificial intelligence 2.0",2017,"","","","",78,"2022-07-13 09:36:46","","10.1631/FITEE.1710000","","",,,,,15,3.00,15,1,5,"With the ever-growing popularization of the Internet, universal existence of sensors, emergence of big data, development of e-commerce, rise of the information community, and interconnection and fusion of data and knowledge in human society, physical space, and cyberspace, the information environment surrounding artificial intelligence (AI) development has changed profoundly, leading to a new evolutionary stage: AI 2.0. The emergence of new technologies also promotes AI to a new stage (Pan, 2016). The next-generation AI, namely AI 2.0, is a more explainable, robust, open, and general AI with the following attractive merits: It effectively integrates data-driven machine learning approaches (bottom-up) with knowledge-guided methods (top-down). In addition, it can employ data with different modalities (e.g., visual, auditory, and natural language processing) to perform cross-media learning and inference. Furthermore, there will be a step from the pursuit of an intelligent machine to the hybridaugmented intelligence (i.e., high-level man-machine collaboration and fusion). AI 2.0 will also promote crowd-based intelligence and autonomous-intelligent systems. In the next decades, AI2.0 will probably achieve remarkable progress in aforementioned trends, and therefore significantly change our cities, products, services, economics, environments, even how we advance our society. This special issue aims at reporting recent re-thinking of AI 2.0 from aforementioned aspects as well as practical methodologies, efficient implementations, and applications of AI 2.0. The papers in this special issue can be categorized into two groups. The first group consists of six review papers and the second group five research papers. In the first group, Zhuang et al. (2017) reviewed recent emerging theoretical and technological advances of AI in big data settings. The authors concluded that integrating data-driven machine learning with human knowledge (common priors or implicit intuitions) can effectively lead to explainable, robust, and general AI. Li W et al. (2017) described the concepts of crowd intelligence, and explained its relationship to the existing related concepts, e.g., crowdsourcing and human computation. In addition, the authors introduced four categories of representative crowd intelligence platforms. Peng et al. (2017) presented approaches, advances, and future directions in cross-media analysis and reasoning. This paper covers cross-media representation, mining, reasoning, and cross-media knowledge evolution. Tian et al. (2017) reviewed the state-of-the-art research of the perception in terms of visual perception, auditory perception, and speech perception. It also covered perceptual information processing and learning engines. Zhang et al. (2017) introduced the trends in the development of intelligent unmanned autonomous systems. It covered unmanned vehicles, unmanned aerial vehicles, service robots, space robots, marine robots, and unmanned Editorial: Frontiers of Information Technology & Electronic Engineering www.zju.edu.cn/jzus; engineering.cae.cn; www.springerlink.com ISSN 2095-9184 (print); ISSN 2095-9230 (online) E-mail: jzus@zju.edu.cn","",""
4,"T. Schmid","Deconstructing the Final Frontier of Artificial Intelligence: Five Theses for a Constructivist Machine Learning",2019,"","","","",79,"2022-07-13 09:36:46","","","","",,,,,4,1.33,4,1,3,"Ambiguity and diversity in human cognition can be regarded a final frontier in developing equivalent systems of artificial intelligence. Despite astonishing accomplishments, modern machine learning algorithms are still hardly more than adaptive systems. Deep neural networks, for example, represent complexity through complex connectivity but are not able to allow for abstraction and differentiation of interpretable knowledge, i.e., for key mechanisms of human cognition. Like support vector machines, random forests and other statistically motivated algorithms, they do neither reflect nor yield structures and strategies of human thinking. Therefore, we suggest to realign the use of existing machine learning tools with respect to the philosophical paradigm of constructivism, which currently is the key concept in human learning and professional teaching. Based on the idea that learning units like classifiers can be considered models with limited validity, we formulate five principles to guide a constructivist machine learning. We describe how to define such models and model limitations, how to relate them and how relationships allow to abstract and differentiate models. To this end, we propose the use of meta data for classifiers and other models. Moreover, we argue that such meta data-based machine learning results in a knowledge base that is both created by the means of automation and interpretable for humans. Over the last decade, it has become widely accepted to address computational systems intelligent. Not only journalists, but also scientists have adapted this habit in their publications. In fact, many classical engineering tasks like monitoring or regulating have profited from the employment of machine learning (Abellan-Nebot and Romero Subirón 2010; Mohanraj, Jayaraj, and Muraleedharan 2012). The same holds true for pattern recognition, most prominently in automated image and video analysis (Zafeiriou, Zhang, and Zhang 2015; Yang et al. 2011). And even though ultimate challenges like the infamous Turing test are left unsatisfied (You 2015), some exceptional results in specialized tasks like playing the game of go (Silver et al. 2016) make current learning machines look intelligent on a human level. Copyright held by the author(s). In A. Martin, K. Hinkelmann, A. Gerber, D. Lenat, F. van Harmelen, P. Clark (Eds.), Proceedings of the AAAI 2019 Spring Symposium on Combining Machine Learning with Knowledge Engineering (AAAI-MAKE 2019). Stanford University, Palo Alto, California, USA, March 25-27, 2019. A final frontier for learning systems, however, is the variety of alternative cognitive functions observable in a diverse set of individuals or from ambiguous stimuli (Kornmeier and Bach 2012). While philosophy has acknowledged and embraced the subjectivity and limitations of human cognition during the last decades (Prawat and Floden 1994), current learning systems regard cognition a complex, yet technical task to be solved. In particular, established algorithms do neither provide convincing answers to the challenges provided by an ambiguous environment; nor do they offer concepts that explicitly allow for contradictory judgements comparable to differences in social perception. The main reason for this shortcoming is that so far both algorithms and researchers have failed to incorporate a constructivist point of view. Constructivism implies not only cognition to be a highly individual phenomenon, but also humans to take an active role in their perception of the world – and that there is no such thing as a human-independent reality (Reich 2009). Yet algorithms and applications aiming to predict things other than laws of nature are implicitly founded on exactly this outdated asumption. In the following, we introduce axioms that allow machine learning to follow constructivist principles. Key features of this approach are the use of modern tools from empirical sciences, model-oriented learning, the ability to handle ambiguity, the ability to integrate supervised and unsupervised learning into a unified framework, the ability to create an individual knowledge base and the ability to abstract, differentiate or discard learned knowledge automatically. 1. The key component of cognitive functionality is a model. Since the introduction of artificial neural networks as a theoretical concept (McCulloch and Pitts 1943), many mathematicians and computer scientists have considered neurons the key component of learning systems. In education and psychology, however, cognitive functions are often seen as certain skills or abilities acquired and exposed by an individual human and described in terms like the concept of competence, which, e.g., is widely used in the modern European education system (Méhaut and Winch 2012). Functionalistic psychology explains cognitive functions of humans by the concept of mental models (Rouse and Morris 1986). Initially, mental models have been used to understand motor control, e.g., of hand movements (Veldhuyzen and Stassen 1977). In a more general sense, however, mental models are described as “hypothetical constructs” (Wickens 2000) that can be ordered hierarchically (Rasmussen 1979) and allow a human to make predictions about his physical and social environment (Oatley 1985). It has also been postulated that such models cannot be of static nature but rather underlay continuous modifications (Oatley 1985). Philosophers, too, consider models an important tool in human knowledge acquisition (Klaus 1967, p. 412) or even the only tool, respectively (Stachowiak 1973, p. 56). While varying and concurring theoretical definitions exist, most model concepts assume an image, an origin of the image and a relationship between them. This definition is, e.g., matched by the idea of mathematical modeling as proposed by Heinrich Hertz and others (Hertz 1894; Hamilton 1982). With the rise of robotics and artificial intelligence, engineers have adapted and extended this idea by postulating the concept of a cybernetic model, which involves a generalized subject and an object of the model (Rose 2009). Cybernetics, however, did neither reflect time-related aspects nor issues involved with individual model subjects. This matter was adressed by Herbert Stachowiak, who was influenced by cybernetics when developing his General Model Theory (Hof 2018). He postulated any model to be limited to specific subjects, specific temporal ranges and specific purposes (Stachowiak 1973, p. 133). Limitations, to this end, are considered a matter of fact rather than a matter of definition. Thus, such models circumvent ambiguity by viewing an otherwise ambiguous model with unknown validity limits as a number of models of limited validity. 2. Learning constitutes from constructing, reconstructing or deconstructing models. Modern education is dominated by the ideas of constructivism and constructivist learning (Fox 2001). At its heart, this approach is based on the assumption that humans acquire knowledge and competences actively and individually through processes called construction, reconstruction and deconstruction (Duffy and Jonassen 1992). Construction is associated with creation, innovation and production and implies searching for variations, combinations or transfers of knowledge (Reich 2004, p. 145). Analogously, reconstruction is associated with application, repetition or imitation and implies searching for order, patterns or models (Reich 2004, p. 145). Deconstruction is in the context of constructivism associated with reconsideration, doubt and modification and implies searching for omissions, additions and defective parts of acquired knowledge (Reich 2004, p. 145). Learning algorithms have been used for half a century to transform sample data into models in a mathematical sense, that is: into generalized mathematical relationships between image and origin. The two major approaches or objectives, known as supervised and unsupervised learning, either do or do not require a given target parameter. Artificial neural networks and their relatives are among the most popular and prominent algorithms for learning with a given target parameter (Singh, Thakur, and Sharma 2016), but statistically motivated approaches like support vector machines (Cristianini and Shawe-Taylor 2000) or random forests (Breiman 2001) are also widely used for supervised learning; a specialized field of supervised learning is reinforcement learning, which is popular in robotics (Kober and Peters 2012) and adaptive control (Lewis, Vrabie, and Vamvoudakis 2012). For unsupervised learning, too, biologically inspired approaches like self-organizing maps (Kohonen 2001) as well as statistically motivated approaches like k-means (Jain 2010) are employed. To some extent, machine learning parallels modern education concepts. A construction process in the constructivist sense may be matched by an unsupervised learning, i.e., identifying clusterings or dimensionality reduction, and can, e.g., be implemented with self-organizing maps, kmeans, autoencoders or feature clustering (Schmid 2018). A reconstruction process in the constructivist sense may be matched by a supervised learning, i.e., classification or regression tasks, and can, e.g., be implemented with artificial neural networks or random forests (Schmid 2018). Few researchers, however, have discussed a constructivist approach to machine learning (Drescher 1989; Quartz 1993), and even less how to design a deconstruction process. While domainspecific applications with manual re-engineering options exist (Herbst and Karagiannis 2000), to the best of our knowledge, there is currently only one working implementation of an algorithmic deconstruction process (Schmid 2018). 3. Deconstructing models computationally requires model-based meta data. In order to automate and implement a deconstruction process, successfully learned models must be held available for comparison or re-training. More over, possible matchings with n","",""
6,"Yaron Einhorn, M. Einhorn, Adaia Kamshov, Oron Lev, A. Trabelsi, N. Paz-Yaacov, S. Gross","Gene-specific artificial intelligence-based variant classification engine: results of a time-capsule experiment",2019,"","","","",80,"2022-07-13 09:36:46","","10.21203/rs.2.11834/v1","","",,,,,6,2.00,1,7,3,"  Background: Interpretation of genetic variation remains an impediment to cost-effective application of genomics to medicine. An advanced artificial intelligence (AI)-based Variant Classification Engine (aiVCE), rooted in ACMG/AMP guidelines, employs data-driven methods to expedite gene-specific classification (franklin.genoox.com). In this blinded study, the aiVCE’s overall and rule-level performances were evaluated using ClinVar (v. 2018-10) variants with creation dates after 5/01/2017. By removing any prior knowledge of these variants from the aiVCE training data, they were treated as novel variants. Using a ‘Full’ dataset (75,801 variants with ≥1 star) and an ‘Increased-Certainty’ dataset (3,993 variants with ≥2 stars), the aiVCE classified variants as pathogenic (P), likely-pathogenic (LP), uncertain significance (VUS), likely-benign (LB), or benign (B). VUS with sufficient supporting data were subclassified as VUS-leaning benign or VUS-leaning pathogenic. aiVCE results were evaluated to determine concordance with final ClinVar classification and rule-level determinations. Results: The aiVCE demonstrated >97% concordance among Increased-Certainty variants. Concordance was >95% across variant effects (e.g., missense, null, splice region), and was >93.5% for the Full dataset. When assessing the aiVCE’s application of specific ACMG rules, significant differences were observed between ClinVar P/LP and B/LB variants rule-met proportions (all P<0.00001), thus supporting gene-specific rule selections. Evaluation of discordance between the aiVCE and ClinVar uncovered evidences that might have been unavailable to submitting laboratories, highlighting AI utility in variant classification. Conclusions: The aiVCE exhibited robust performance, despite lacking past evidence, in determining whether variants would be categorized as P/LP. Applying latest computational advances to existing guidelines may assist scientists and clinicians interpret variants with limited clinical information and greatly reduce analytical bottlenecks.","",""
2,"Chun-Hsi Huang, Xiangrong Wang","Financial Innovation Based on Artificial Intelligence Technologies",2019,"","","","",81,"2022-07-13 09:36:46","","10.1145/3349341.3349504","","",,,,,2,0.67,1,2,3,"Nowadays, the degree of the heated topic of artificial intelligence in the world reaches a new height. Due to the breakthrough of deep learning algorithm based on neural network, the level of artificial intelligence technologies has been enhanced significantly. The global financial industry is quietly changing under the catalysis of artificial intelligence. The frontier artificial intelligence technologies, such as the technology of expert system, machine learning and knowledge discovery in database are combed to explore the financial applications of artificial intelligence. Based on these key technologies, this paper proposed three applications of artificial intelligence in the financial field, including intelligent investment adviser, transaction forecast and financial regulation, discusses the key technologies of artificial intelligence and financial innovation products based on these technologies, such as the functions of the transaction prediction system based on artificial intelligence technologies include forecast analysis, index statistics, stock analysis and information retrieval, etc. The structures of the systems are drawn and the design principles are provided. Finally, to guard the safety of the applications of artificial intelligence, the paper gives the suggestions of enhancing identity authentication, introducing monitoring measures and limiting autonomy degree.","",""
3,"T. Kaur, Anirudra Diwakar, Kirandeep, Pranav Mirpuri, M. Tripathi, P. Chandra, T. Gandhi","Artificial Intelligence in Epilepsy",2021,"","","","",82,"2022-07-13 09:36:46","","10.4103/0028-3886.317233","","",,,,,3,3.00,0,7,1,"Background: The study of seizure patterns in electroencephalography (EEG) requires several years of intensive training. In addition, inadequate training and human error may lead to misinterpretation and incorrect diagnosis. Artificial intelligence (AI)-based automated seizure detection systems hold an exciting potential to create paradigms for proper diagnosis and interpretation. AI holds the promise to transform healthcare into a system where machines and humans can work together to provide an accurate, timely diagnosis, and treatment to the patients. Objective: This article presents a brief overview of research on the use of AI systems for pattern recognition in EEG for clinical diagnosis. Material and Methods: The article begins with the need for understanding nonstationary signals such as EEG and simplifying their complexity for accurate pattern recognition in medical diagnosis. It also explains the core concepts of AI, machine learning (ML), and deep learning (DL) methods. Results and Conclusions: In this present context of epilepsy diagnosis, AI may work in two ways; first by creating visual representations (e.g., color-coded paradigms), which allow persons with limited training to make a diagnosis. The second is by directly explaining a complete automated analysis, which of course requires more complex paradigms than the previous one. We also clarify that AI is not about replacing doctors and strongly emphasize the need for domain knowledge in building robust AI models that can work in real-time scenarios rendering good detection accuracy in a minimum amount of time.","",""
1,"H. López-Fernández","Application of data mining and artificial intelligence techniques to mass spectrometry data for knowledge discovery",2016,"","","","",83,"2022-07-13 09:36:46","","","","",,,,,1,0.17,1,1,6,"Mass spectrometry using matrix assisted laser desorption ionization coupled to time of flight analyzers (MALDI-TOF MS) has become popular during the last decade due to its high speed, sensitivity and robustness for detecting proteins and peptides. This allows quickly analyzing large sets of samples are in one single batch and doing high-throughput proteomics. In this scenario, bioinformatics methods and computational tools play a key role in MALDI-TOF data analysis, as they are able handle the large amounts of raw data generated in order to extract new knowledge and useful conclusions. A typical MALDI-TOF MS data analysis workflow has three main stages: data acquisition, preprocessing and analysis. Although the most popular use of this technology is to identify proteins through their peptides, analyses that make use of artificial intelligence (AI), machine learning (ML), and statistical methods can be also carried out in order to perform biomarker discovery, automatic diagnosis, and knowledge discovery. In this research work, this workflow is deeply explored and new solutions based on the application of AI, ML, and statistical methods are proposed. In addition, an integrated software platform that supports the full MALDI-TOF MS data analysis workflow that facilitate the work of proteomics researchers without advanced bioinformatics skills has been developed and released to the scientific community.","",""
41,"Guangnan Zhang, Z. H. Ali, M. Aldlemy, Mohamed H. Mussa, Sinan Q. Salih, M. Hameed, Z. Al-khafaji, Z. Yaseen","Reinforced concrete deep beam shear strength capacity modelling using an integrative bio-inspired algorithm with an artificial intelligence model",2020,"","","","",84,"2022-07-13 09:36:46","","10.1007/s00366-020-01137-1","","",,,,,41,20.50,5,8,2,"","",""
0,"H. L. Fernández","Application of data mining and artificial intelligence techniques to mass spectrometry data for knowledge discovery",2016,"","","","",85,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,1,6,"Mass spectrometry using matrix assisted laser desorption ionization coupled to time of flight analyzers (MALDI-TOF MS) has become popular during the last decade due to its high speed, sensitivity and robustness for detecting proteins and peptides. This allows quickly analyzing large sets of samples are in one single batch and doing high-throughput proteomics. In this scenario, bioinformatics methods and computational tools play a key role in MALDI-TOF data analysis, as they are able handle the large amounts of raw data generated in order to extract new knowledge and useful conclusions. A typical MALDI-TOF MS data analysis workflow has three main stages: data acquisition, preprocessing and analysis. Although the most popular use of this technology is to identify proteins through their peptides, analyses that make use of artificial intelligence (AI), machine learning (ML), and statistical methods can be also carried out in order to perform biomarker discovery, automatic diagnosis, and knowledge discovery. In this research work, this workflow is deeply explored and new solutions based on the application of AI, ML, and statistical methods are proposed. In addition, an integrated software platform that supports the full MALDI-TOF MS data analysis workflow that facilitate the work of proteomics researchers without advanced bioinformatics skills has been developed and released to the scientific community.","",""
0,"Qi Deng","Artificial Intelligence BlockCloud (AIBC) Technical Whitepaper",2018,"","","","",86,"2022-07-13 09:36:46","","10.2139/ssrn.3464239","","",,,,,0,0.00,0,1,4,"The AIBC is an Artificial Intelligence and blockchain technology based large-scale decentralized ecosystem that allows system-wide low-cost sharing of computing and storage resources. The AIBC consists of four layers: a fundamental layer, a resource layer, an application layer, and an ecosystem layer. The AIBC implements a two-consensus scheme to enforce upper-layer economic policies and achieve fundamental layer performance and robustness: the DPoEV incentive consensus on the application and resource layers, and the DABFT distributed consensus on the fundamental layer. The DABFT uses deep learning techniques to predict and select the most suitable BFT algorithm in order to achieve the best balance of performance, robustness, and security. The DPoEV uses the knowledge map algorithm to accurately assess the economic value of digital assets.","",""
0,"C. Carpenter","Augmented Artificial Intelligence Improves Data Analytics in Heavy-Oil Reservoirs",2019,"","","","",87,"2022-07-13 09:36:46","","10.2118/0519-0068-JPT","","",,,,,0,0.00,0,1,3,"This article, written by JPT Technology Editor Chris Carpenter, contains highlights of paper SPE 193650, “Augmented-Artificial-Intelligence Solutions for Heavy-Oil Reservoirs: Innovative Work Flows That Build From Smart Analytics, Machine Learning, and Expert-Based Systems,” by David Castineira, Xiang Zhai, and Hamed Darabi, Quantum Reservoir Impact Group, prepared for the 2018 SPE International Heavy Oil Conference and Exhibition, Kuwait City, Kuwait, 10–12 December. The paper has not been peer reviewed.  Recently, many heavy-oil fields have seen exponentially higher volumes of data made available as a result of omnipresent connectivity. Existing data platforms have focused traditionally on solving the problem of data storage and access. The more-complex problem of true knowledge discovery and systematic value creation from the massive amount of data is less frequently addressed. The authors of this paper propose a novel work flow for the problem of building intelligent data analytics in heavy-oil fields.  Introduction  Optimal reservoir management for heavy-oil reservoirs requires systematic solutions that combine both engineering ability and advanced analytics. The authors believe that this requirement is addressed by what they call augmented artificial intelligence (AAI), a process inspired by the intelligence-amplification concept in which machine learning and human expertise are combined to improve solutions derived by systems that learn without any type of input from engineers or geoscientists. Practical deployment of AAI will involve automated work flows that use solid technical expertise and proven processes to transform field data into more-effective reservoir-management solutions.  Even with rapid data-preprocessing solutions in place, developing an optimal reservoir-management framework for heavy-oil assets is inherently complex. Identifying key recovery obstacles (KROs) and field-development plans (FDPs) typically takes many months, involving a large team of experts and the construction of sophisticated full-field simulation models. The recommendation is that automated work flows and AAI solutions are combined to identify those KROs rapidly and prepare robust FDPs that increase production and optimize current operations.  Perhaps the less-intuitive step in developing systematic solutions for heavy-oil fields is the process of developing a quantitative reservoir diagnostic framework. This process must build from big-data analytics platforms and an array of analytical, numerical, and empirical models combined to deliver a catalog of KROs affecting field performance. To this end, the entire historical set of well, field, and reservoir data must be processed and input into this diagnostics platform. Once the KROs are understood, the next step is to translate the diagnostics into detailed action plans in the field that can generate production, reserves, or capital-efficiency improvements.  This paper aims to offer an alternative approach to traditional work flows that identify recovery obstacles and development opportunities in heavy-oil fields by labor-intensive solutions. In contrast, the authors propose a systematic framework that provides three key advantages:  Execution time is fast, and an initial opportunity inventory can be generated.  The user can choose from multiple algorithms and methods to customize the technology to unique field/reservoir complexities.  The core algorithms are data-driven, integrate multidisciplinary data sets, and leave little room for the biases of the user, which allows for a consistent and repeatable analysis.","",""
0,"Suk Lee, E. Ju, Suk Woo Choi, Hyung-ju Lee, Jang Bo Shim, Kyung Hwan Chang, Kwang Hyeon Kim, Chul Yong Kim","Prediction of Cancer Patient Outcomes Based on Artificial Intelligence",2018,"","","","",88,"2022-07-13 09:36:46","","10.5772/INTECHOPEN.81872","","",,,,,0,0.00,0,8,4,"Knowledge-based outcome predictions are common before radiotherapy. Because there are various treatment techniques, numerous factors must be considered in predicting cancer patient outcomes. As expectations surrounding personalized radiotherapy using complex data have increased, studies on outcome predictions using artificial intelligence have also increased. Representative artificial intelligence techniques used to predict the outcomes of cancer patients in the field of radiation oncology include collecting and processing big data, text mining of clinical literature, and machine learning for implementing prediction models. Here, methods of data preparation and model construction to predict rates of survival and toxicity using artificial intelligence are described.","",""
3,"Joyjit Mukherjee, Spandan Roy, I. Kar, Shubhabrata Mukherjee","Maneuvering control of planar snake robot: An adaptive robust approach with artificial time delay",2021,"","","","",89,"2022-07-13 09:36:46","","10.1002/rnc.5430","","",,,,,3,3.00,1,4,1,"This article proposes an adaptive‐robust maneuvering control framework for a planar snake robot under the influence of parameter uncertainties. The entire control objective of maneuvering control can be viewed as the simultaneous establishments of two goals: one to maintain a time‐varying body shape of the snake robot for consistent motion (called the outer layer) and the other dealing with the velocity and head‐angle tracking of the same (called the inner layer). Unknown variations in the ground friction coefficients have been considered to be the primary source of time‐varying uncertainties which affects the control performance in both the layers. Accordingly, an artificial time delay‐based adaptive‐robust control (ARC) framework, dual adaptive‐robust time‐delayed control (ARTDC), is proposed. The term dual signifies simultaneous application of ARTDC for the outer as well as the inner layer. ARTDC comprises of two segments: an artificial time delay‐based time‐delayed estimation (TDE) part and an ARC part. While TDE approximates the completely unknown friction forces, the ARC tackles the approximation error arising from the TDE. More importantly, compared with the existing ARC methodologies, the proposed ARTDC neither presumes the overall uncertainty to be upper bounded by a constant nor requires any prior knowledge of the bound of uncertainty to implement the controller. A Lyapunov function‐based method has been adopted for analyzing the stability of the closed‐loop system. Simulation studies affirm the improved performance of the ARTDC in contrast to the classical artificial delay‐based methodology.","",""
1,"James A. Crowder, John Carbone, Shelli Friess","Ontology-Based Knowledge Management for Artificial Intelligent Systems",2019,"","","","",90,"2022-07-13 09:36:46","","10.1007/978-3-030-17081-3_9","","",,,,,1,0.33,0,3,3,"","",""
0,"Fang Zhang, Xiaochen Wang, J. Han, Jie Tang, Shiyin Wang","Fast Top-$\boldsymbol{k}$ Area Topics Extraction with Knowledge Base",2017,"","","","",91,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,5,5,"What are the most popular research topics in Artificial Intelligence (AI)? We formulate the problem as extracting top-$k$ topics that can best represent a given area with the help of knowledge base. We theoretically prove that the problem is NP-hard and propose an optimization model, FastKATE, to address this problem by combining both explicit and latent representations for each topic. We leverage a large-scale knowledge base (Wikipedia) to generate topic embeddings using neural networks and use this kind of representations to help capture the representativeness of topics for given areas. We develop a fast heuristic algorithm to efficiently solve the problem with a provable error bound. We evaluate the proposed model on three real-world datasets. Experimental results demonstrate our model's effectiveness, robustness, real-timeness (return results in $<1$s), and its superiority over several alternative methods.","",""
1,"L. Bori, M. Valera, D. Gilboa, R. Maor, I. Kottel, J. Remohi, D. Seidman, M. Meseguer","O-084 Computer vision can distinguish between euploid and aneuploid embryos. A novel artificial intelligence (AI) approach to measure cell division activity associated with chromosomal status",2021,"","","","",92,"2022-07-13 09:36:46","","10.1093/humrep/deab125.014","","",,,,,1,1.00,0,8,1,"      Can we distinguish between top-grade euploid and aneuploid embryos by AI measurement of cell edges in time-lapse videos?        Aneuploid embryos can be distinguished from euploid embryos by AI determination of a longer time to blastulation and higher cell activity.        Continuous monitoring of the embryo development has brought out morphokinetic parameters that are used to predict pre-implantation genetic testing (PGT) results. Previous publications showed that euploid embryos reach blastulation earlier than non-euploid embryos. However, time-lapse data are currently under-utilized in making predictions about embryo chromosomal content. AI and computer vision could take advantage of the massive amount of data embedded in the images of embryo development. This is the first attempt to distinguish between euploid and aneuploid embryos by computer vision in an objective and indirect way based on the measurement of cell edges as a proxy for cell activity.        We performed a retrospective analysis of 1,314 time-lapse videos from embryos cultured to the blastocyst stage with PGT results. This single-center study involved two phases; a comparison of the start time of blastulation between euploid (n = 544) and aneuploid embryos (n = 797). In phase two, we designed a novel methodology to examine whether precise measurement of cell edges over time could reflect cell activity differences in blastulation.        We assumed that the delay in blastulation is reflected by higher cell activity that could be determined accurately for the first time using computer vision and machine learning to measure the length of the edges (from t2 to t8). We compared computer vision based measurements of cell edges, reflecting cell number and size, in videos of 231 top-grade euploid (n = 111) and aneuploid (n = 120) embryos.        The mean and standard deviation of blastulation start time was 100.1±6.8 h for euploid embryos and 101.8±8.2 h for aneuploid embryos (p < 0.001). Regarding the measurement of cell activity, a computer vision algorithm identified the edges and provided a certainty score for each edge, higher when the algorithm is more certain that this is a cell edge (as opposed to noise in the images). A threshold was set to distinguish cell edges from noise using this score. The following results for top-grade embryos are shown as the sum of the edge lengths (µm) average of 160 pictures per embryo (frames between t2 and t8). The total length of the cell edges increased from two cells (420±85 µm) to eight cells (861±237 µm), in line with the mitosis events. Both the average total edge measured (450±162 µm for euploid embryos and 489±215 µm for aneuploid embryos, p < 0.01) and the average total of the difference between consecutive frames (135±47 µm for euploid embryos and 153±64 µm for aneuploid embryos, p < 0.01) were higher for aneuploid embryos than for euploid embryos. A regression model to differentiate between the two classes achieved 73% sensitivity and 73% specificity on this dataset.        The main limitation of this study is the difficulty to correlate our findings to other measure of cell activity. A more robust AI function (using not only cell edges lengths) would be required for future analysis to measure the cell activity in cell division up to the blastocyst stage.        Our results show for the first time that an AI based system can precisely measure microscopic cell edges in the dividing embryo. Using this novel method, we could distinguish between euploid and aneuploid embryos. This non-invasive method could further enhance our knowledge of the developing embryo.        Not Applicable ","",""
0,"A. Paic","Policies for Artificial Intelligence in Science and Innovation",2020,"","","","",93,"2022-07-13 09:36:46","","10.22323/1.372.0045","","",,,,,0,0.00,0,1,2,"This contribution synthesizes the discussions of the special session on policies for Artificial Intelligence in Science and Innovation, organized by the OECD’s Directorate for Science, Technology and Innovation. The session was opened by Dr Judith Arrieta, Minister of the Foreign Service at the Chief of Staff’s Office of the Secretary of Foreign Affairs of Mexico, and the two panels included speakers from governments, industry and civil society from European countries, USA,Canada China and Australia. Participants discussed the disruptive nature of AI and the formidable challenges it poses. Most of the discussion focused under the umbrella title of ethics, but they span very different issues of human-centered values, fairness, transparency, explainability, and many more. Other challenges include employment, education, SME policy, enabling environment, access to data and computing technology. Responses by governments were also discussed with a particular focus on national strategies, whose main pillars are oriented toward knowledge creation through AI research, knowledge diffusion through linkages to the private sector, development of human capital which will underpin the development of the sector, and a strong values, ethical and regulatory framework to create the conditions for the development of trustworthy AI.  In a world of finite resources, discussants concluded that one cannot apply very stringent requirements to all AI decisions, and there is clearly a need to require more transparency, explainability and robustness from systems which have the greatest impact on human lives. Therefore an approach based on algorithmic impact assessment seems reasonable. Such an approach needs to be further developed and standardized.","",""
0,"Alakananda Mitra, S. Mohanty, P. Corcoran, E. Kougianos","EasyDeep: An IoT Friendly Robust Detection Method for GAN Generated Deepfake Images in Social Media",2021,"","","","",94,"2022-07-13 09:36:46","","10.1007/978-3-030-96466-5_14","","",,,,,0,0.00,0,4,1,"","",""
0,"S. K. Opoku","A Robust Mechanism for Categorizing Context-Aware Applications into Generations",2021,"","","","",95,"2022-07-13 09:36:46","","10.24018/ejece.2021.5.6.371","","",,,,,0,0.00,0,1,1,"The hunt to categorize context-aware applications has been a prevalent issue to developers of context-aware applications. The previous categorizations were based on the functions of the applications. These mechanisms yielded limited results since many applications could not be categorized. This paper categorizes applications into four generations based on developmental trends through a literature survey. The first generation applications focused on data acquisition and used hardware sensors. The second generation applications focused on knowledge acquisition and used software sensors, semantic language and ontology-based modelling languages. The third generation applications focused on intelligent reasoning and used mechanisms to handle information uncertainty. The fourth generation applications deprecate cumbersome ruleset implementations and focus on artificial intelligence whilst taking into consideration the effect of the dynamics of users’ background and preference on contextual information. The study demonstrated that when applications, methods or technologies can be categorized over some time, it is better to classify them into generations.","",""
128,"D. Bonderman","Artificial intelligence in cardiology",2017,"","","","",96,"2022-07-13 09:36:46","","10.1007/s00508-017-1275-y","","",,,,,128,25.60,128,1,5,"","",""
30,"S. Elkatatny, Zeeshan Tariq, M. Mahmoud, I. Mohamed, A. Abdulraheem","Development of New Mathematical Model for Compressional and Shear Sonic Times from Wireline Log Data Using Artificial Intelligence Neural Networks (White Box)",2018,"","","","",97,"2022-07-13 09:36:46","","10.1007/S13369-018-3094-5","","",,,,,30,7.50,6,5,4,"","",""
4,"Morteza Saberi, A. Azadeh, Z. Saberi, P. Pazhoheshfar","A knowledge management system based on artificial intelligence (AI) methods: A flexible fuzzy regression-analysis of variance algorithm for natural gas consumption estimation",2012,"","","","",98,"2022-07-13 09:36:46","","10.1109/InfRKM.2012.6205023","","",,,,,4,0.40,1,4,10,"A knowledge management (KM) system has a different schema that one of them has been studied in the present study. One of version of KM is based on artificial intelligence (AI) methods. KM based on AI has been investigated in the present work based on natural gas consumption estimation domain. Developing accurate and flexible model to natural gas consumption estimation is a strategic step in policy and decision-making process in energy sector. This paper provides a stage algorithm during that it gains optimal fuzzy regression model for studding natural gas consumption in sixteen countries according to data in years 1989 till 2007. Different countries have selected from Africa, America, Asia, Europe and Middle East based on high, middle and low GDP index and for every one of them nine fuzzy regression models have executed and the results and error of each model have calculated. Preprocess has been done on the initial data to gain better results which the min-max method has been used for this purpose. Two criterions have been used to determining suitable and appropriate fuzzy regression model in each country. Firstly, fuzzy regression models with MAPE value below 10 is deleted from the assessment and remained fuzzy regression models are compared with ANOVA. Upon logic that given algorithm sketches for some countries none of used models are proper while for other countries optimal model is gained in first or second filter of algorithm. To show the applicability and superiority of the proposed flexible Fuzzy regression model the data for oil consumption in Japan, Thailand, Bangladesh from Asia and Norway, Italy, Bulgaria from Europe and Qatar, Iran, Iraq from Middle East and The united state, Mexico, Bolivia from North America and Libya, Tunisia, Nigeria from Africa during 1989 to 2007 are used.","",""
8,"P. Mittal, Y. Singh","Development of Intelligent Transportation System for Improving Average Moving and Waiting time with Artificial Intelligence",2016,"","","","",99,"2022-07-13 09:36:46","","10.17485/IJST/2016/V9I3/84156","","",,,,,8,1.33,4,2,6,"Background: Real time traffic control is an important tool of Intelligent Transportation System (ITS). The development of system for controlling the urban traffic dynamically provides not only the safety for traffic, but also saves the time, money and provides polluted free environment. This paper describes the development of dynamic and robust traffic management system based on fuzzy logic approach. Method: Knowledge based system have been extensively adopted as approach for real time decision making system. Findings: As the conventional dynamic controllers were used sensors which are having certain limitations, so these limitations can be overcome by vision sensors i.e. camera. Also image and vision computing plays an important role in monitoring and measuring the traffic density on road. Problems were identified with the current traffic control system at the intersection on road and this necessitated the design and implementation of a new system to solve the congestion problems. Improvements: The performance of the proposed framework is evaluated with LabVIEW and MATLAB test bed. The results of extensive simulations using the proposed approach indicate that the system improves the average moving time and decrease the average waiting time than the controllers with conventional sensors.","",""
3,"E.V. Blagodarny, A.A Vedyakhin, A.M. Raygorodsky","Development of Educational Projects on the Basis of Technological Platforms with Artificial Intelligence: The Experience of MIPT on the Use of High Vox-Platform",2018,"","","","",100,"2022-07-13 09:36:46","","10.1109/IC-AIAI.2018.8674452","","",,,,,3,0.75,1,3,4,"The MIPT School of Applied Mathematics and Computer Science conducts research on artificial intelligence and develops education in this field in Russia. Modern science and technology are developing so quickly that a person needs to constantly learn and acquire new skills. Therefore, MIPT develops educational courses at the school, academic and corporate levels. Employers and scientific laboratories are more interested in the practical skills of employees in AI, than just theoretical knowledge. For this reason, the MIPT School of Applied Mathematics and Computer Science conducts and develop new practice-oriented educational courses. Moreover, the Laboratory of Innovation at MIPT creates the HighVox platform, which will allow MIPT students to gain experience in solving real problems from Russian companies during their studies. The platform creates the digital trace of each student: competences, scientific interests, courses taken, completed projects, soft skills, etc. Based on the digital trace of each participant, the platform automatically creates recommendations for projects and teams most suitable for the student. In the future, HighVox will become a place where technical specialists search for work, get an education (lifelong learning), communicate with colleagues on specialized topics and offer their ideas for startups. As part of the creation of this platform, the laboratory of innovation conducts research in two directions: a model of human competence and the formation of effective teams based on hard & soft skills using artificial intelligence.","",""
0,"","ACTIVITY REPORT Project-Team Models and Algorithms for Artiﬁcial Intelligence",2022,"","","","",101,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,0,1,"The expectation-maximization (EM) algorithm is a powerful computational technique for maximum likelihood estimation in incomplete data models. When the expectation step cannot be performed in closed form, a stochastic approximation of EM (SAEM) can be used. The convergence of the SAEM toward critical points of the observed likelihood has been proved and its numerical efﬁciency has been demonstrated. However, sampling from the posterior distribution may be intractable or have a high computational cost. Moreover, despite appealing features, the limit position of this algorithm can strongly depend on its starting one. To cope with this two issues, we propose in [11] new stochastic approximation version of the EM in which we do not sample from the exact distribution in the expectation phase of the procedure. We ﬁrst prove the convergence of this algorithm toward critical points of the observed likelihood. Then, we propose an instantiation of this general procedure to favor convergence toward global maxima. Experiments on synthetic and real data highlight the performance of this algorithm in comparison to the SAEM and the EM when feasible. of subject-speciﬁc weights characterizing partial membership across clusters. With this ﬂexibility come challenges in uniquely identifying, estimating, and interpreting the parameters. In [40], we propose a new class of Dimension-Grouped MMMs (Gro-M 3 s) for multivariate categorical data, which improve parsimony and interpretability. In Gro-M 3 s, observed variables are partitioned into groups such that the latent membership is constant for variables within a group but can differ across groups. Traditional latent class models are obtained when all variables are in one group, while traditional MMMs are obtained when each variable is in its own group. The new model corresponds to a novel decomposition of probability tensors. Theoretically, we derive transparent identiﬁability conditions for both the unknown grouping structure and model parameters in general settings. Methodologically, we propose a Bayesian approach for Dirichlet Gro-M3 s to inferring the variable grouping structure and estimating model parameters. Simulation results demonstrate good computational performance and empirically conﬁrm the identiﬁability results. We illustrate the new methodology through an application to a functional disability dataset. from this natural partition. In a Bayesian context, this is achieved by considering the Dirichlet cluster proportion prior parameter α as a regularisation term controlling the granularity of the clustering. This second step allows the exploration of the clustering at coarser scales and the ordering of the clusters an important output for the visual representations of the clustering results. The clustering results obtained with the proposed approach, on simulated as well as real settings, are compared with existing strategies and are shown to be particularly relevant. This work is implemented in the R package greed and Figure 2 illustrates the main idea of the method. In this applied work [19], we use the Fisher-EM algorithm for clustering for the unsupervised classiﬁcation of 702, 248 spectra of galaxies and quasars with resdshifts smaller than 0.25 that were retrieved from the Sloan Digital Sky Survey (SDSS) database, release 7. The spectra were ﬁrst corrected for the redshift, then wavelet-ﬁltered to reduce the noise, and ﬁnally binned to obtain about 1437 wavelengths per spectrum. Fisher-EM, an unsupervised clustering discriminative latent mixture model algorithm, was applied on these corrected spectra, considering the full set as well as several subsets of 100,000 and 300,000 spectra. The optimum number of classes given by a penalized likelihood criterion is 86 classes, the 37 most populated ones gathering 99% of the sample. These classes are established from a subset of 302144 spectra. Using several cross-validation techniques we ﬁnd that this classiﬁcation is in agreement with the results obtained on the other subsets with an average misclassiﬁcation error of about 15%. The large number of very small classes tends to increase this error rate. This is the ﬁrst time that an automatic, objective and robust unsupervised classiﬁcation is established on such a large amount of spectra of galaxies. The mean spectra of the classes can be used as templates for a large majority of galaxies in our Universe. Figure 7 illustrates the obtained results. Recurrent Neural Networks, Deep linguistic patterns the of a of of to the is this linguistic that becomes valuable for our descriptive approach through deep as it allows us to observe complex lexico-grammatical structures, that potentially associate several levels of text representation in the same structure. The convolutional model used until now must therefore be adapted to integrate this additional information in order to obtain an even ﬁner description of the textual salience of a corpus. the relevant features used by the CNN to perform the classiﬁcation task. We empirically demonstrate the efﬁciency of our approach on corpora from two different languages: English and French. On all datasets, wTDS automatically encodes complex linguistic objects based on co-occurrences and possibly on grammatical and syntax analysis. relationships between the concepts in the metadata by analyzing the contrast between the concepts similarities in the Joconde’s semantic model and other vocabularies and we tried to improve the model prediction scores based on the semantic relations. Our results show that cross-fertilization between symbolic AI and machine learning can indeed provide the tools to address the challenges of the museum curators work describing the artwork pieces and searching for the relevant images. that combines a geometric approach for decision rules with existing post hoc solutions for machine learning models to generate an intuitive feature ranking tailored to the end user. We show that established model-agnostic approaches produce poor results in this framework. Figure 13 illustrates this work. Algorithms involving Gaussian processes or determinantal point processes typically require computing the determinant of a kernel matrix. Frequently, the latter is computed from the Cholesky decomposition, an algorithm of cubic complexity in the size of the matrix. We show that, under mild assumptions, it is possible to estimate the determinant from only a sub-matrix, with probabilistic guarantee on the relative error. In [37], we present an augmentation of the Cholesky decomposition that stops under certain conditions before processing the whole matrix. Experiments demonstrate that this can save a considerable amount of time while having an overhead of less than 5% when not stopping early. More generally, we present a probabilistic stopping strategy for the approximation of a sum of known length where addends are revealed sequentially. We do not assume independence between addends, only that they are bounded from below and decrease in conditional expectation. of there is a signiﬁcant from combining and audio data in detecting active speakers. either of the modalities can potentially mislead audiovisual fusion by inducing unreliable or deceptive information. outlines active speaker detection as a multi-objective learning problem to leverage best of each modalities using a novel self-attention, uncertainty-based multimodal fusion scheme. Results obtained show that the proposed multi-objective learning architecture outperforms traditional approaches in improving both mAP and AUC scores. We further demonstrate that our fusion strategy surpasses, in active speaker detection, other modality fusion methods reported in various disciplines. We ﬁnally show that the proposed method signiﬁcantly improves the state-of-the-art on the AVA-ActiveSpeaker dataset. This paper explores the problem of summarizing professional soccer matches as automatically as possible using both the event-stream data collected from the ﬁeld and the content broadcasted on TV. We have designed an architecture, introducing ﬁrst (1) a Multiple Instance Learning method that takes into account the sequential dependency among events and then (2) a hierarchical multimodal attention layer that grasps the importance of each event in an action [31]. We evaluate our approach on matches from two professional European soccer leagues, showing its capability to identify the best actions for automatic summarization by comparing with real summaries made by human operators. Figure 18 illustrates the general schema of the approach. We a coherent framework for studying longitudinal manifold-valued data. We introduce a Bayesian mixed-effects model which allows estimating both a group-representative piecewise-geodesic creating clusters of similar sentences. The ideal practice is to obtain a cluster with only positive blocks and another with only negative ones. Comparing to the supervised approach (Bag of words + Logistic Regression Classiﬁer) with its f1-score as 0.8234 and f2-score as 0.8316, we found that both S-Bert [58] (with a f1-score of 0.6250 and f2-score of 0.6192) and BioBert [57] (f1-score as 0.7004 and f2 as 0.6955) can achieves relatively good results and latter even outperformed the former due to its domain speciﬁc knowledge. around 13 billion euros per year to European citizens [52]. In the ﬁeld of healthcare insurance, in France the compulsory scheme detected over 261.2 million euros of fraudulent services in 2018, mainly due to healthcare professionals and healthcare establishments [50]. In the United States, according to the FBI, medicare fraud costs insurance companies between 21 billion and 71 billion US dollars per year [55]. In a context where reducing management costs is a real issue for healthcare insurers, the ﬁght against fraud is a real expectation of the customers of professionals in the sector so that everyone receives a fair return for their contributions. This stud","",""
7,"G. Mazzini","A System of Governance for Artificial Intelligence through the Lens of Emerging Intersections between AI and EU Law",2019,"","","","",102,"2022-07-13 09:36:46","","","","",,,,,7,2.33,7,1,3,"The work provides an overview and a comment of the Communication on Artificial Intelligence (AI) adopted by the European Commission in April 2018. By offering a bird’s-eye view of those law and policy areas potentially relevant for or affected by AI, the AI Communication sets the stage for understanding how pervasively and extensively AI is likely to be mainstreamed in our economies and societies. Whether it is about safety of products, liability, consumer protection, personal data protection or the foundational values, principles and rights on which the European project is based on, AI is very rapidly cutting across domains. The work identifies and investigates some of the many intersections between AI and EU law. Two main “disrupting” trends emerge.    According to the first trend, AI seems to exercise some pressure on existing regulatory frameworks, such as in the areas of product safety, liability and consumer protection.    As regards product safety, the main concerns seem to revolve around the unpredictability risk of AI. While certain factual characteristics (possibly limitations) of AI as it functions today cannot and should not be denied, the policy debate on AI safety should focus on what potential risks brought about by AI (or rather by specific AI applications) can be considered as socially acceptable when weighed against potential benefits. Even though the challenges posed by AI may generate some pressure on the existing EU product safety frameworks, it seems that EU safety law as a broader normative field has at its disposal a varied set of regulatory tools and approaches that can be relevant sources of inspiration and reference for a discussion on the safety of AI-powered products.    In the field of product liability, although one should note that the Product Liability Directive (PLD) is not necessarily the only tool that can be invoked by victims in case of risks and damages linked to AI-powered products, there seem to be elements suggesting that AI (in general or with regard to certain of its product specific applications) may put under stress the continued suitability of the technology neutral design of the PLD - or at least some provisions thereof - to the extent that the PLD is expected to apply, in its current form, to both “smart” and “non-smart” products.    The protection of consumers in the context of profiling and targeting practices in the business-to-consumers transactions is an area where the General Data Protection Regulation (GDPR) is particularly relevant. To the extent GDPR rules effectively enhance the data subjects’ empowerment vis-a-vis traders and/or curtail the ability of traders to engage in manipulative and unfair practices, then there may be less need for a fine-tuning of dedicated consumer law instruments (such as the Unfair Commercial Practices Directive, the Consumer Rights Directive and the Directive on Unfair Terms in Consumer Contracts) in order to take account of the specificities of commercial transactions mediated by sophisticated algorithms. At the same time, consumer protection could be an interesting testing ground for the potential of AI to empower consumers and civil society in general: the very same tools, techniques and methods used by companies to pursue their commercial interests could also serve the purpose to re-balance the traditional asymmetry of information, power and knowledge impacting negatively on consumers.    Contrary to what happens in the legal domains mentioned above, a different “disrupting” trend emerges in the field of the protection of personal data. Here, the several intersections between AI and the GDPR can essentially be framed in terms of the law disrupting certain technological uses and applications of AI. Due to the fact that AI uses and applications in the context of commercial transactions, and, more generally, of the algorithm-mediated economic, social and political life of individuals extensively rely on and process personal data, the GDPR emerges as a key piece of legislation in the space. While the data protection authorities and the courts will certainly specify and fine-tune its principles and provisions as appropriate, the GDPR presents itself as a robust framework poised to capture and effectively curb at least those uses and applications of AI that appear most egregious and intolerable in light of the degree of legal protection for individual rights and freedoms that is currently expected by citizens in our European society.    The work argues that, even if each legal or policy area where AI surfaces is confronted with distinct normative questions that may not necessarily be relevant for other areas, a connecting tissue is needed. This should take the form of a system of AI governance or cabine de regie which should combine - on an ongoing basis - up-to-date scientific and technical knowledge, internal legal and policy expertise specific to each sector and the authority to impart policy direction and to arbitrate, across the board, between the societal opportunities and the societal concerns that underlie the composite interaction between AI and the law.","",""
1,"D. Handelman, Corban G. Rivera, R. St. Amant, Emma Holmes, Andrew R. Badger, Bryanna Y. Yeh","Adaptive human-robot teaming through integrated symbolic and subsymbolic artificial intelligence: preliminary results",2022,"","","","",103,"2022-07-13 09:36:46","","10.1117/12.2618686","","",,,,,1,1.00,0,6,1,"As the autonomy of intelligent systems continues to increase, the ability of humans to maintain control over machine behavior, work effectively in concert with them, and trust them, becomes paramount. Ideally, a machine’s plan of action would be accessible to and understandable by human team members, and machine behavior would be modifiable in real time, in the field, to accommodate unanticipated situations. The ability of machines to adapt to new situations quickly and reliably based on both human input and autonomous learning has the potential to enhance numerous human-machine teaming scenarios. Our research focuses on the question, “Can robots become competent and adaptive teammates by emulating human skill acquisition strategies?” In this paper we describe the Robotic Skill Acquisition (RSA) cognitive architecture and show preliminary results of teaming experiments involving a human wearing an augmented reality headset and a quadruped robot performing tasks related to reconnaissance. The goal is to combine instruction and discovery by integrating declarative symbolic AI and reflexive neural network learning to produce robust, explainable and trusted robot behavior, adjustable autonomy, and adaptive human-robot teaming. Humans and robots start with a playbook of modifiable hierarchical task descriptions that encode explicit task knowledge. Neural network based feedback error learning enables human-directed behavior shaping, and reinforcement learning enables discovery of novel subtask control strategies. It is anticipated that modifications to and transitions between symbolic and subsymbolic processing will enable highly adaptive behavior in support of enhanced situational awareness and operational effectiveness of human-robot teams.","",""
12,"S. Craw, A. Aamodt","Case Based Reasoning as a Model for Cognitive Artificial Intelligence",2018,"","","","",104,"2022-07-13 09:36:46","","10.1007/978-3-030-01081-2_5","","",,,,,12,3.00,6,2,4,"","",""
3,"Yongyue Wang, Chunhe Xia, Chengxiang Si, Beitong Yao, Tianbo Wang","Robust Reasoning Over Heterogeneous Textual Information for Fact Verification",2020,"","","","",105,"2022-07-13 09:36:46","","10.1109/ACCESS.2020.3019586","","",,,,,3,1.50,1,5,2,"Automatic fact verification (FV) based on artificial intelligence is considered as a promising approach which can be used to identify misinformation distributed on the web. Even though previous FV using deep learning have made great achievements in single dataset (e.g., FEVER), the trained systems are unlikely to be capable of extracting evidence from heterogeneous web-sources and validating claims in accordance with evidence found on the Internet. Nevertheless, the heterogeneity covers abundant semantic information, which will help FV system identify misinformation in a more accurate way. The current work is the first attempt to make the combination of knowledge graph (KG) and graph neural network (GNN) to enhance the robustness of FV systems for heterogeneous information. As a result, it can be generalized to multi-domain datasets after training on a sufficient single one. To make information update and aggregate well on the collaborative graph, the present study proposes a double graph attention network (DGAT) framework which recursively propagates the embeddings from a node’s neighbors to refine the node’s embedding as well as applies an attention mechanism to classify the importance of the neighbors. We train and evaluate our system on FEVER, a single and benchmark dataset for FV, and then re-evaluate our system on UKP Snopes Corpus, a new richly annotated corpus for FV tasks on the basis of heterogeneous web sources. According to experimental results, although DGAT has no excellent advantages in a single dataset, it shows outstanding performance in more realistic and multi-domain datasets. Moreover, the current study also provides a feasible method for deep learning to have the ability to infer heterogeneous information robustly.","",""
1,"Massoud Sokouti, B. Sokouti","Applying the Science of Systematic Review and Meta-Analysis to Retrospective Artificial Intelligence Based Studies: The importance of performance evaluation",2019,"","","","",106,"2022-07-13 09:36:46","","","","",,,,,1,0.33,1,2,3,"The rationale behind the meta-analysis goes back to the 17th century studies of astronomy which then Karl Pearson performed a study based on meta-analysis using the data for typhoid inoculation in 1904. After, William Cochran applied this type of analysis to medical researches by taking the advantage of multiple previous studies. For more information and details on the history, the readers are referred to. To emerge the important role of systematic and metaanalysis studies even in the area of artificial intelligence systems, it is an anticipated that more reliable results can be driven from previous research studies alongside a simple review of such studies from which most of them may be ignored or not included as a matter of their nonsystematical type of reviews. The meta-analysis technique uses various types of statistics tools and methodologies to commonly derive a predictive diagnostic or non-diagnostic performance result of their compared corresponding approaches on the target defined disorders using information included in different datasets of previous studies. Although, a meta-analysis study can be regarded as a review of previous studies, however, it thoroughly targets not only the achieving results of those studies but also determine the in-common and non-commonpatterns of those researches as well as biases of the performance results whether they have been inserted intentionally or unintentionally. The importance of meta-analysis has been vastly discussed in medical sciences and therefore, been conducted rigorously through various studies, mostly on clinical trial ones. However, this technique is one of those less valued tools imported in to biomedical engineering studies and hence, their related algorithms mostly on the performance of artificial intelligence approaches. One of those studies to mention is the one performed on classification algorithms for pattern recognition by So Young Sohn in 1999 based on some in-house implemented statistics tools without considering the meta-analysis software. Moreover, in 2015,Horn et al have conducted a systematic review on functional brain imaging studies on assessing the familiarity of artificial neural networks and discussed their pros and cons in terms of their experimental conflicting results based on a meta-analysis on 68 publishedarticles. In another recent study, the role of real-time biomedical systems has been evaluated by a meta-analysis approach on 134 real-times papers in terms of computational complexity, delay and speed up considering various types of algorithms and hardware implementation. Recently, two types of systematic review and analysis have been performed which shows the potential non-mature trends of this approach in artificial intelligence based researches.In the first one the authors studied the performance of different machine learning algorithms for heart disease diagnosis; however, the metaanalysis part was not performed due to the existence of heterogeneity in the final included studies through the PRISMA (Preferred reporting items for systematic reviews and meta-analyses) checklist. And in the second one, the performance of several DNA based encryption algorithms based according to the results obtained from previous publications has been proposed where, it has been found out that there were no improvements in the proposed algorithms and it has been suggested that a dataset of images should be available in order to test and evaluate the performance of methodologies. However, the methodologies should also be available for public use. Moreover, the analyses section can be carried out through a simple statistical student’s t test analysisor the metaanalysis procedure using available tools such as MetaDisc, MIX, and Meta-Analyst. While comparing the two environments (i.e., clinical and computational), there are in-common units for decision making in diagnosing symptoms which are human (brain system and some data) and computer (artificial intelligence systems and some data). This outstanding feature and the abovementioned examples makes the meta-analysis studies applicable to the researches performed based on artificial intelligence systems, too. This will open a new view on interactions between the results obtained from previous studies while considering their special algorithms, different datasets, and possible biases. One more thing to emphasize for the future research studies is on publicizing the datasets and the implemented algorithms in terms of web servers, Java, C++ and Matlab libraries or R packages to make the results re-generable using new datasets which make them more comparable with new designed methodologies to ease the metaanalysis robust studies. As, it is also clear, most of the webservers and datasets in the medical parts coupled with data derived from bioscience knowledge are publicly.","",""
2,"Preet Amol Singh, Neha Bajwa, S. Naman, A. Baldi","A Review on Robust Computational Approaches Based Identification and Authentication of Herbal Raw Drugs",2020,"","","","",107,"2022-07-13 09:36:46","","10.2174/1570180817666200304125520","","",,,,,2,1.00,1,4,2,"  Over the last decade, there has been a sudden rise in the demand for herbal as well as Information and Technology (IT) industry around the world. Identification of plant species has become useful and relevant to all the members of the society including farmers, traders, hikers, etc. Conventional authentication techniques such as morphological characterization, histological methods, and optical microscopy require multiple skills which are tedious, timeconsuming and difficult to learn for non-experts. This creates a hurdle for individuals interested in acquiring knowledge of species. Relying on rapid, economical and computerized approaches to identify and authenticate medicinal plants has become a recent development.    The purpose of this review is to summarize artificial intelligence-based technologies for wider dissemination of common plant-based knowledge such as identification and authentication to common people earlier limited to only experts.    A robust plant identification design enabling automated plant-organ and feature-based identification utilizing pattern recognition and image processing techniques resulting in image retrieval and recognition has been highlighted in this review for all the concerned stakeholders. Attempts have been made to compare conventional authentication methods with advanced computerized techniques to emphasize the advantages and future applications of an automated identification system in countering adulteration and providing fair trade opportunities to farmers.    Major findings suggested that microscopical features such as shape and size of calcium oxalate crystals, trichomes, scleriods, stone cells, fibers, etc. are the essential descriptors for identification and authentication of herbal raw drugs using computational approaches.    This computational design can be successfully employed to address quality issues of medicinal plants. Therefore, computational techniques proved as a milestone in the growth of agriculture and medicinal plant industries. ","",""
0,"M. Muzammul","Education System re-engineering with AI (artificial intelligence) for Quality Im-provements with proposed model",2019,"","","","",108,"2022-07-13 09:36:46","","10.14201/adcaij2019825160","","",,,,,0,0.00,0,1,3,"Re-engineering (RE) of existing educational institutions (EI) with adoption of latest technology trends (LTT) in form of artificial intelligence (AI) can be great effective in term of quality systems. Increase in student’s strength in class and terrorist attacks on EI urged us to introduce such approach that can assure education quality. Class monitoring with heavy strength always remain major issue for teacher during lecture delivery. In this paper, we implemented reengineering using artificial intelligence based two theories of 1) Multi-face recognition (MFR) system 2) Facial expression recognition (FER) system. Both of these theories supported by intelligent techniques as principal component analysis (PCA), discrete wavelet transform (DWT) and k-nearest neighbor (KNN). After implementation of these intelligent techniques student’s attentiveness will increase. Our developed system can detect expressions like happiness, repulsion, fear, anger, and confusion. Student’s attentiveness score will be displayed on screen. Teacher can interpret on the basis of attentiveness %age. System decision making can be helpful for class continuity or short break. This system is also an application of an expert system (ES) and knowledge base system (KBS) for educational quality assurance. A similar monitoring system was imposed in china with Hikvision Digital Technology. Predations results proved monitoring can be best way for education quality.","",""
2,"Shubham Sharma, A. Gee, D. Paydarfar, J. Ghosh","FaiR-N: Fair and Robust Neural Networks for Structured Data",2020,"","","","",109,"2022-07-13 09:36:46","","10.1145/3461702.3462559","","",,,,,2,1.00,1,4,2,"Fairness and robustness in machine learning are crucial when individuals are subject to automated decisions made by models in high-stake domains. To promote ethical artificial intelligence, fairness metrics that rely on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias. However, fairness measures that rely on comparing the ability to achieve recourse have been relatively unexplored. In this paper, we present a novel formulation for training neural networks that considers the distance of data observations to the decision boundary such that the new objective: (1) reduces the disparity in the average ability of recourse between individuals in each protected group, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that models trained with this new objective are more fair and adversarially robust neural networks, with similar accuracies, when compared to models without it. We also investigate a trade-off between the recourse-based fairness and robustness objectives. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across protected groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse disparity across groups are considered to train fairer neural networks.","",""
9,"G. D. Gasperis, I. Chiari, N. Florio","AIML Knowledge Base Construction from Text Corpora",2013,"","","","",110,"2022-07-13 09:36:46","","10.1007/978-3-642-29694-9_12","","",,,,,9,1.00,3,3,9,"","",""
1,"Zhaofeng Zhang, Yue Chen, Junshan Zhang","Distributionally Robust Learning Based on Dirichlet Process Prior in Edge Networks",2020,"","","","",111,"2022-07-13 09:36:46","","10.23919/JCIN.2020.9055108","","",,,,,1,0.50,0,3,2,"In order to meet the real-time performance requirements, intelligent decisions in Internet of things applications must take place right here right now at the network edge. Pushing the artificial intelligence frontier to achieve edge intelligence is nontrivial due to the constrained computing resources and limited training data at the network edge. To tackle these challenges, we develop a distributionally robust optimization (DRO)-based edge learning algorithm, where the uncertainty model is constructed to foster the synergy of cloud knowledge and local training. Specifically, the cloud transferred knowledge is in the form of a Dirichlet process prior distribution for the edge model parameters, and the edge device further constructs an uncertainty set centered around the empirical distribution of its local samples. The edge learning DRO problem, subject to these two distributional uncertainty constraints, is recast as a single-layer optimization problem using a duality approach. We then use an Expectation-Maximization algorithm-inspired method to derive a convex relaxation, based on which we devise algorithms to learn the edge model. Furthermore, we illustrate that the meta-learning fast adaptation procedure is equivalent to our proposed Dirichlet process prior-based approach. Finally, extensive experiments are implemented to showcase the performance gain over standard approaches using edge data only.","",""
1,"Chunheng Zhao, Yi Li, Matthew J. Wessner, Chinmay Rathod, P. Pisu","Support-Vector Machine Approach for Robust Fault Diagnosis of Electric Vehicle Permanent Magnet Synchronous Motor",2020,"","","","",112,"2022-07-13 09:36:46","","10.36001/PHMCONF.2020.V12I1.1291","","",,,,,1,0.50,0,5,2,"Permanent magnet synchronous motor (PMSM) is a leading technology for electric vehicles (EVs) and other high-performance industrial applications. These challenging applications demand robust fault diagnosis schemes, but conventional strategies based on models, system knowledge, and signal transformation have limitations that degrade the agility of diagnosing faults. These methods require extremely detailed design and consideration to remain robust against noise and disturbances in the actual application. Recent advancements in artificial intelligence and machine learning have proven to be promising next-generation solutions for fault diagnosis. In this paper, a support-vector machine (SVM) utilizing sparse representation is developed to perform sensor fault diagnosis of a PMSM. A simulation model of the pertinent PMSM drive system for automotive applications is used to generate a set of labelled training example sets that the SVM uses to determine margins between normal and faulty operating conditions. The PMSM model includes input as a torque reference profile and disturbance as a constant road grade, against both of which faults must be detectable. Even with limited training, the SVM classifier developed in this paper is capable of diagnosing faults with a high degree of accuracy, suggesting that such methods are feasible for the demanding fault diagnosis challenge in PMSM.","",""
53,"S. Sikchi, S. Sikchi, M. S. Ali","Artificial Intelligence in Medical Diagnosis",2012,"","","","",113,"2022-07-13 09:36:46","","","","",,,,,53,5.30,18,3,10,"The logical thinking of medical practitioner involves a lot of subjective decision making and its complexity makes traditional quantitative approaches of analysis inappropriate. The computer based diagnostic tools and knowledge base certainly helps for early diagnosis of diseases. The intelligent decision making systems can appropriately handle both the uncertainty and imprecision. This paper discusses about the application potential of artificial intelligence in medical diagnosis. The fuzzy expert system has been presented specific to liver disease diagnosis.","",""
0,"Zhaofeng Zhang, Yue Chen, Junshan Zhang","Distributionally Robust Edge Learning with Dirichlet Process Prior",2020,"","","","",114,"2022-07-13 09:36:46","","10.1109/ICDCS47774.2020.00016","","",,,,,0,0.00,0,3,2,"In order to meet the real-time performance requirements, intelligent decisions in many IoT applications must take place right here right now at the network edge. The conventional cloud-based learning approach would not be able to keep up with the demands in achieving edge intelligence in these applications. Nevertheless, pushing the artificial intelligence (AI) frontier to achieve edge intelligence is highly nontrivial due to the constrained computing resources and limited training data at the network edge. To tackle these challenges, we develop a distributionally robust optimization (DRO)-based edge learning algorithm, where the uncertainty model is constructed to foster the synergy of cloud knowledge transfer and local training. Specifically, the knowledge transferred from the cloud is in the form of a Dirichlet process prior distribution for the edge model parameters, and the edge device further constructs an uncertainty set centered around the empirical distribution of its local samples to capture the information of local data processing. The edge learning DRO problem, subject to the above two distributional uncertainty constraints, is then recast as an equivalent single-layer optimization problem using a duality approach. We then use an Expectation-Maximization (EM) algorithm-inspired method to derive a convex relaxation, based on which we devise algorithms to learn the edge model parameters. Finally, extensive experiments are implemented to showcase the performance gain over standard learning approaches using local edge data only.","",""
20,"FarzinPiltan, MarziehKamgari, SaeedZare, FatemehShahryarZadeh, M. Mansoorzadeh","Design Novel Model Reference Artificial Intelligence Based Methodology to Optimized Fuel Ratio in IC Engine",2013,"","","","",115,"2022-07-13 09:36:46","","10.5815/IJIEEB.2013.02.07","","",,,,,20,2.22,4,5,9,"In this research, model reference fuzzy based control is presented as robust controls for IC engine. The objective of the study is to design controls for IC engines without the knowledge of the boundary of uncertainties and dynamic information by using fuzzy model reference PD p lus mass of air while improve the robustness of the PD p lus mass of air control. A PD plus mass of air provides for eliminate the mass of air and ultimate accuracy in the presence of the bounded disturbance/uncertainties, although this methods also causes some oscillation. The fuzzy PD plus mass of air is proposed as a solution to the problems crated by unstability. Th is method has a good performance in presence of uncertainty.","",""
7,"Arwin Datumaya Wahyudi Sumari, A. S. Ahmad, Cognitive Artificial","The application of cognitive artificial intelligence within C4ISR framework for national resilience",2017,"","","","",116,"2022-07-13 09:36:46","","10.1109/ACDTJ.2017.8259600","","",,,,,7,1.40,2,3,5,"Cognitive Artificial Intelligence (CAI) is a new perspective in Artificial Intelligence (AI) which is aimed to emulate how human brain works in generating knowledge. Human becomes intelligent because of knowledge which grows over time in his brain. With comprehensive knowledge, he can understand the world (environment) and is able to make decision and or action on it. On the other hand, strategic decision which impacts to the continuance of having a nation and having state is a critical and crucial matter, and it should be done in precise and quick manner especially in the case of contingency and faced to mutiple-data multiple-decision-alternative problems. The most precise decision has to be based on the knowledge from extracted comprehensive information. In this paper we show you the application of CAI for National Security with Knowledge-Growing System (KGS) as the engine of decision making system. We apply the CAI to a framework called Cognitive Command, Control, Communications, Computers, Intelligence, Surveillance and Reconnaissance (C4ISR) with examples taken from a simulated of real-life case in the Defense-Security domain.","",""
2,"M. S. Yakoot, A. Ragab, O. Mahmoud","Multi-Class Taxonomy of Well Integrity Anomalies Applying Inductive Learning Algorithms: Analytical Approach for Artificial-Lift Wells",2021,"","","","",117,"2022-07-13 09:36:46","","10.2118/206129-ms","","",,,,,2,2.00,1,3,1,"  Well integrity has become a crucial field with increased focus and being published intensively in industry researches. It is important to maintain the integrity of the individual well to ensure that wells operate as expected for their designated life (or higher) with all risks kept as low as reasonably practicable, or as specified. Machine learning (ML) and artificial intelligence (AI) models are used intensively in oil and gas industry nowadays. ML concept is based on powerful algorithms and robust database. Developing an efficient classification model for well integrity (WI) anomalies is now feasible because of having enormous number of well failures and well barrier integrity tests, and analyses in the database.  Circa 9000 dataset points were collected from WI tests performed for 800 wells in Gulf of Suez, Egypt for almost 10 years. Moreover, those data have been quality-controlled and quality-assured by experienced engineers. The data contain different forms of WI failures. The contributing parameter set includes a total of 23 barrier elements.  Data were structured and fed into 11 different ML algorithms to build an automated systematic tool for calculating imposed risk category of any well. Comparison analysis for the deployed models was performed to infer the best predictive model that can be relied on. 11 models include both supervised and ensemble learning algorithms such as random forest, support vector machine (SVM), decision tree and scalable boosting techniques. Out of 11 models, the results showed that extreme gradient boosting (XGB), categorical boosting (CatBoost), and decision tree are the most reliable algorithms. Moreover, novel evaluation metrics for confusion matrix of each model have been introduced to overcome the problem of existing metrics which don't consider domain knowledge during model evaluation.  The innovated model will help to utilize company resources efficiently and dedicate personnel efforts to wells with the high-risk. As a result, progressive improvements on business, safety, environment, and performance of the business. This paper would be a milestone in the design and creation of the Well Integrity Database Management Program through the combination of integrity and ML.","",""
19,"T. Anandharajan, G. Hariharan, K. K. Vignajeth, R. Jijendiran, Kushmita","Weather Monitoring Using Artificial Intelligence",2016,"","","","",118,"2022-07-13 09:36:46","","10.1109/CINE.2016.26","","",,,,,19,3.17,4,5,6,"Weather forecasting is rather a statistical measure than a binary decision. We intend to develop an intelligent weather predicting module since this has become a necessary tool. This tool considers measures such as maximum temperature, minimum temperature and rainfall for a sampled period of days and are analyzed. An intelligent prediction based on the available data is accomplished using machine learning techniques. The analysis and prediction is based on linear regression which predicts the next day's weather with good accuracy. An accuracy of more than 90% is obtained, based on the data set. Recent studies have reflected that machine learning techniques achieved better performance than traditional statistical methods. Machine learning, a branch of artificial intelligence has been proved to be a robust method in predicting and analyzing a given data set. The module plays a vital role in agricultural, industrial and logistical fields where the weather forecast is an important criterion.","",""
2,"B. Dorr, Lucian Galescu, E. Golob, K. Venable, Y. Wilks","Companion-Based Ambient Robust Intelligence (CARING)",2015,"","","","",119,"2022-07-13 09:36:46","","","","",,,,,2,0.29,0,5,7,"We present a Companion-based Ambient Robust INtelliGence (CARING) system, for communication with, and support of, clients with Traumatic brain injury (TBI) or Amyotrophic Lateral Sclerosis (ALS). A central component of this system is an artificial companion, combined with a range of elements for ambient intelligence. The companion acts as a personalized intermediary for multi-party communication between the client, the environment (e.g. a Smart Home), caregivers and health professionals. CARING is based on tightly coupled systems drawing from natural language processing, speech recognition and adaptation, deep language understanding and constraintbased knowledge representation and reasoning. A major innovation of the system is its ability to adapt and accommodate different interfaces associated with different client capabilities and needs. The system will use, as a proxy, different interaction requirements of clients (e.g., Brain-Computer Interfaces) at different stages of ALS progression and with different types of TBI impairments. Ultimately, this technology is expected to improve the quality of life for clients through conversation with a computer.","",""
92,"A. Annoni, P. Benczúr, P. Bertoldi, Blagoj Delipetrev, Giuditta De Prato, C. Feijóo, Enrique Fernández-Macías, E. Gutiérrez, M. Portela, H. Junklewitz, M. L. Cobo, B. Martens, Susana Nascimento, S. Nativi, Alexandre Pólvora, Jose Ignacio Sanchez Martin, Songuel Tolan, I. Tuomi, Lucia Vesnić Alujević","Artificial Intelligence: A European Perspective",2018,"","","","",120,"2022-07-13 09:36:46","","10.2760/11251","","",,,,,92,23.00,9,19,4,"We are only at the beginning of a rapid period of transformation of our economy and society due to the convergence of many digital technologies. Artificial Intelligence (AI) is central to this change and offers major opportunities to improve our lives. The recent developments in AI are the result of increased processing power, improvements in algorithms and the exponential growth in the volume and variety of digital data. Many applications of AI have started entering into our every-day lives, from machine translations, to image recognition, and music generation, and are increasingly deployed in industry, government, and commerce. Connected and autonomous vehicles, and AI-supported medical diagnostics are areas of application that will soon be commonplace. There is strong global competition on AI among the US, China, and Europe. The US leads for now but China is catching up fast and aims to lead by 2030. For the EU, it is not so much a question of winning or losing a race but of finding the way of embracing the opportunities offered by AI in a way that is human-centred, ethical, secure, and true to our core values. The EU Member States and the European Commission are developing coordinated national and European strategies, recognising that only together we can succeed. We can build on our areas of strength including excellent research, leadership in some industrial sectors like automotive and robotics, a solid legal and regulatory framework, and very rich cultural diversity also at regional and sub-regional levels. It is generally recognised that AI can flourish only if supported by a robust computing infrastructure and good quality data: â€¢ With respect to computing, we identified a window of opportunity for Europe to invest in the emerging new paradigm of computing distributed towards the edges of the network, in addition to centralised facilities. This will support also the future deployment of 5G and the Internet of Things. â€¢ With respect to data, we argue in favour of learning from successful Internet companies, opening access to data and developing interactivity with the users rather than just broadcasting data. In this way, we can develop ecosystems of public administrations, firms, and civil society enriching the data to make it fit for AI applications responding to European needs. We should embrace the opportunities afforded by AI but not uncritically. The black box characteristics of most leading AI techniques make them opaque even to specialists. AI systems are currently limited to narrow and well-defined tasks, and their technologies inherit imperfections from their human creators, such as the well-recognised bias effect present in data. We should challenge the shortcomings of AI and work towards strong evaluation strategies, transparent and reliable systems, and good human-AI interactions. Ethical and secure-by-design algorithms are crucial to build trust in this disruptive technology, but we also need a broader engagement of civil society on the values to be embedded in AI and the directions for future development. This social engagement should be part of the effort to strengthen our resilience at all levels from local, to national and European, across institutions, industry and civil society. Developing local ecosystems of skills, computing, data, and applications can foster the engagement of local communities, respond to their needs, harness local creativity and knowledge, and build a human-centred, diverse, and socially driven AI. We still know very little about how AI will impact the way we think, make decisions, relate to each other, and how it will affect our jobs. This uncertainty can be a source of concern but is also a sign of opportunity. The future is not yet written. We can shape it based on our collective vision of what future we would like to have. But we need to act together and act fast.","",""
0,"A. J. Spiessbach","Task -specific knowledge by itself, however, is not sufficient to achieve robust machine perception in unrestricted, uncontrolled, or noncooperativ e environments. A higher -level",2017,"","","","",121,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,1,5,"Extending the recent successes demonstrated by artificial intelligence expert system technology to the broader domain of scene analysis necessitates a consequent broadening of the concepts and techniques used in current systems. Simple, single mechanisms must give way to multiple lines of reasoning and multiple levels of description. Meta -level reasoning, which is a recursive application of the basic expert system paradigm, is a promising approach to the problem of coping with the complexity inherent in highly variable, dynamic environments. This paper describes research directed towards incorporating meta level reasoning into context -based scene analysis systems. A multi -layered expert system architecture is outlined that is aimed at providing high -level strategies and dynamic planning capability to the basic image understanding process.","",""
3,"Agnese Chiatti, E. Motta, E. Daga, G. Bardaro","Fit to Measure: Reasoning about Sizes for Robust Object Recognition",2020,"","","","",122,"2022-07-13 09:36:46","","","","",,,,,3,1.50,1,4,2,"Service robots can help with many of our daily tasks, especially in those cases where it is inconvenient or unsafe for us to intervene: e.g., under extreme weather conditions or when social distance needs to be maintained. However, before we can successfully delegate complex tasks to robots, we need to enhance their ability to make sense of dynamic, real world environments. In this context, the first prerequisite to improving the Visual Intelligence of a robot is building robust and reliable object recognition systems. While object recognition solutions are traditionally based on Machine Learning methods, augmenting them with knowledge based reasoners has been shown to improve their performance. In particular, based on our prior work on identifying the epistemic requirements of Visual Intelligence, we hypothesise that knowledge of the typical size of objects could significantly improve the accuracy of an object recognition system. To verify this hypothesis, in this paper we present an approach to integrating knowledge about object sizes in a ML based architecture. Our experiments in a real world robotic scenario show that this combined approach ensures a significant performance increase over state of the art Machine Learning methods.","",""
22,"Xi Lin, Jun Wu, A. Bashir, Jianhua Li, Wu Yang, Jalil Piran","Blockchain-Based Incentive Energy-Knowledge Trading in IoT: Joint Power Transfer and AI Design",2020,"","","","",123,"2022-07-13 09:36:46","","10.1109/jiot.2020.3024246","","",,,,,22,11.00,4,6,2,"Recently, edge artificial intelligence techniques (e.g., federated edge learning) are emerged to unleash the potential of big data from Internet of Things (IoT). By learning knowledge on local devices, data privacy-preserving and quality of service (QoS) are guaranteed. Nevertheless, the dilemma between the limited on-device battery capacities and the high energy demands in learning is not resolved. When the on-device battery is exhausted, the edge learning process will have to be interrupted. In this paper, we propose a novel Wirelessly Powered Edge intelliGence (WPEG) framework, which aims to achieve a stable, robust, and sustainable edge intelligence by energy harvesting (EH) methods. Firstly, we build a permissioned edge blockchain to secure the peer-to-peer (P2P) energy and knowledge sharing in our framework. To maximize edge intelligence efficiency, we then investigate the wirelessly-powered multi-agent edge learning model and design the optimal edge learning strategy. Moreover, by constructing a two-stage Stackelberg game, the underlying energy-knowledge trading incentive mechanisms are also proposed with the optimal economic incentives and power transmission strategies. Finally, simulation results show that our incentive strategies could optimize the utilities of both parties compared with classic schemes, and our optimal learning design could realize the optimal learning efficiency.","",""
0,"A. Sarkar","QKSA: Quantum Knowledge Seeking Agent",2021,"","","","",124,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,1,1,"In this article we present the motivation and the core thesis towards the implementation of a Quantum Knowledge Seeking Agent (QKSA). QKSA is a general reinforcement learning agent that can be used to model classical and quantum dynamics. It merges ideas from universal artificial general intelligence, constructor theory and genetic programming to build a robust and general framework for testing the capabilities of the agent in a variety of environments. It takes the artificial life (or, animat) path to artificial general intelligence where a population of intelligent agents are instantiated to explore valid ways of modelling the perceptions. The multiplicity and survivability of the agents are defined by the fitness, with respect to the explainability and predictability, of a resource-bounded computational model of the environment. This general learning approach is then employed to model the physics of an environment based on subjective observer states of the agents. A specific case of quantum process tomography as a general modelling principle is presented. The various background ideas and a baseline formalism are discussed in this article which sets the groundwork for the implementations of the QKSA that are currently in active development.","",""
0,"A. Sarkar","J ul 2 02 1 QKSA : Quantum Knowledge Seeking Agent motivation , core thesis and baseline framework",2021,"","","","",125,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,1,1,"In this article we present the motivation and the core thesis towards the implementation of a Quantum Knowledge Seeking Agent (QKSA). QKSA is a general reinforcement learning agent that can be used to model classical and quantum dynamics. It merges ideas from universal artificial general intelligence, constructor theory and genetic programming to build a robust and general framework for testing the capabilities of the agent in a variety of environments. It takes the artificial life (or, animat) path to artificial general intelligence where a population of intelligent agents are instantiated to explore valid ways of modeling the perceptions. The multiplicity and survivability of the agents are defined by the fitness, with respect to the explainability and predictability, of a resource-bounded computational model of the environment. This general learning approach is then employed to model the physics of an environment based on subjective observer states of the agents. A specific case of quantum process tomography as a general modeling principle is presented. The various background ideas and a baseline formalism is discussed in this article which sets the groundwork for the implementations of the QKSA that are currently in active development. Section 2 presents a historic overview of the motivation behind this research In Section 3 we survey some general reinforcement learning models and bio-inspired computing techniques that forms a baseline for the design of the QKSA. Section 4 presents an overview of field of quantum artificial agents and the observer based operational theory that the QKSA aims to learn. In Section 5 and 6 we presents the salient features and a formal definition of our model. In Section 7 we present the task of quantum process tomography (QPT) as a general task to test our framework. In Section 8 we conclude the discussion with suggestive future directions for implementing the QKSA.","",""
0,"Ahmed Reda Ali, M. Jaya, E. A. Jones","Machine Learning Strategies for Accurate Log Prediction in Reservoir Characterization: Self-Calibrating Versus Domain-Knowledge",2021,"","","","",126,"2022-07-13 09:36:46","","10.2118/205602-ms","","",,,,,0,0.00,0,3,1,"  Petrophysical evaluation is a crucial task for reservoir characterization but it is often complicated, time-consuming and associated with uncertainties. Moreover, this job is subjective and ambiguous depending on the petrophysicist's experience. Utilizing the flourishing Artificial Intelligence (AI)/Machine Learning (ML) is a way to build an automating process with minimal human intervention, improving consistency and efficiency of well log prediction and interpretation. Nowadays, the argument is whether AI-ML should base on a statistically self-calibrating or knowledge-based prediction framework! In this study, we develop a petrophysically knowledge-based AI-ML workflow that upscale sparsely-sampled core porosity and permeability into continuous curves along the entire well interval.  AI-ML focuses on making predictions from analyzing data by learning and identifying patterns. The accuracy of the self-calibrating statistical models is heavily dependent on the volume of training data. The proposed AI-ML workflow uses raw well logs (gamma-ray, neutron and density) to predict porosity and permeability over the well interval using sparsely core data. The challenge in building the AI-ML model is the number of data points used for training showed an imbalance in the relative sampling of plugs, i.e. the number of core data (used as target variable) is less than 10%. Ensemble learning and stacking ML approaches are used to obtain maximum predictive performance of self-calibrating learning strategy.  Alternatively, a new petrophysical workflow is established to debrief the domain experience in the feature selection that is used as an important weight in the regression problem. This helps ML model to learn more accurately by discovering hidden relationships between independent and target variables. This workflow is the inference engine of the AI-ML model to extract relevant domain-knowledge within the system that leads to more accurate predictions.  The proposed knowledge-driven ML strategy achieved a prediction accuracy of R2 score = 87% (Correlation Coefficient (CC) of 96%). This is a significant improvement by R2 = 57% (CC = 62%) compared to the best performing self-calibrating ML models. The predicted properties are upscaled automatically to predict uncored intervals, improving data coverage and property population in reservoir models leading to the improvement of the model robustness. The high prediction accuracy demonstrates the potential of knowledge-driven AI-ML strategy in predicting rock properties under data sparsity and limitations and saving significant cost and time.  This paper describes an AI-ML workflow that predicts high-resolution continuous porosity and permeability logs from imbalanced and sparse core plug data. The method successfully incorporates new type petrophysical facies weight as a feature augmentation engine for ML domain-knowledge framework. The workflow consisted of petrophysical treatment of raw data includes log quality control, preconditioning, processing, features augmentation and labelling, followed by feature selection to impersonate domain experience.","",""
0,"Shrey Sukhadia, Aayushi Tyagi, V. Venkatraman, P. Mukherjee, Prathosh A.P., Mayur Divate, O. Gevaert, S. Nagaraj","Abstract 6341: ImaGene: A robust AI-based software platform for tumor radiogenomic evaluation and reporting",2022,"","","","",127,"2022-07-13 09:36:46","","10.1158/1538-7445.am2022-6341","","",,,,,0,0.00,0,8,1,"  The field of radiomics has undergone several advancements in approaches to uncovering hidden quantitative features from tumor imaging data for use in guiding clinical decision-making for cancer patients. Radiographic imaging techniques provide insight into the imaging features of tumor regions of interest (ROIs), while immunohistochemistry and sequencing techniques performed on biopsy samples yield omics data. These imaging and omics feature data can then be correlated and modeled using artificial intelligence (AI) techniques to highlight notable associations between tumor genotype and phenotype. Currently, however, the radiogenomics field lacks a unified and robust software platform capable of algorithmically analyzing imaging and omics features using modifiable parameters, detecting significant relationships among these features, and subjecting them to AI-based analysis. To address this gap, we developed ImaGene, a robust AI-based platform that uses omics and imaging features as inputs for different tumor types, performs statistical analyses of the correlations between these data types, and constructs AI models based upon significantly correlated features. It has several modifiable configuration parameters that provide users with complete control over their experiments. For each run, ImaGene produces comprehensive reports that can contribute to the construction of a novel radiogenomic knowledge base, in addition to enabling the deployment and sharing of AI models. To demonstrate the utility of ImaGene, we acquired imaging and omics datasets pertaining to Invasive Breast Cancer (IBC) and Head and Neck Squamous Cell Carcinoma (HNSCC) from public databases and analyzed them with this platform using specific parameters. In both cases, we uncovered significant associations between several imaging features and 11 genes: CRABP1, VRTN, SMTNL2, FABP1, HAND2, HAS1, C4BPA, FAM163A, DSG1, SMTNL2 and KCNJ16 for IBC, and 10 genes: CEACAM6, IGLL1, SERPINA1, NANOG, OCA2, PRLR, ACSM2B, CYP11B1, and VPREB1 for HNSCC. Overall, our software platform is capable of identifying, analyzing, and correlating important features from tumor scans, thereby providing researchers with a reliable and accurate tool for their radiogenomics experiments. We anticipate that ImaGene will become the gold standard for tumor analyses in the field of radiogenomics owing to its ease of use, flexibility, and reproducibility.  Citation Format: Shrey S. Sukhadia, Aayush Tyagi, Vivek Venkatraman, Pritam Mukherjee, Prathosh A.P., Mayur Divate, Olivier Gevaert, Shivashankar H. Nagaraj. ImaGene: A robust AI-based software platform for tumor radiogenomic evaluation and reporting [abstract]. In: Proceedings of the American Association for Cancer Research Annual Meeting 2022; 2022 Apr 8-13. Philadelphia (PA): AACR; Cancer Res 2022;82(12_Suppl):Abstract nr 6341.","",""
20,"A. Okon, S. Adewole, Emmanuel M. Uguma","Artificial neural network model for reservoir petrophysical properties: porosity, permeability and water saturation prediction",2020,"","","","",128,"2022-07-13 09:36:46","","10.1007/s40808-020-01012-4","","",,,,,20,10.00,7,3,2,"","",""
0,"Khalid Akbar, Y. Zou, Qasim Awais, M. J. A. Baig, M. Jamil","A Machine Learning-Based Robust State of Health (SOH) Prediction Model for Electric Vehicle Batteries",2022,"","","","",129,"2022-07-13 09:36:46","","10.3390/electronics11081216","","",,,,,0,0.00,0,5,1,"The car industry is entering a new age due to electric energy as a fuel in the contemporary era. Electric batteries are being more widely used in the automobile sector these days. As a result, the inner workings of these battery systems must be fully comprehended. There is currently no accurate model for predicting an electric car battery’s state of health (SOH). This study aims to use machine learning to develop a reliable SOH prediction model for batteries. A correct optimal method was also constructed to drive the modeling process in the right direction. Extensive simulations were performed to verify the accuracy of the suggested methodology. A state of health method for data processing was developed. The method involves a complex data-driven model combining Big Data, Artificial Intelligence (A.I.), and the Internet of Things (IoT) technologies. To establish the most effective technique for certifying the actual condition of real-life battery health, researchers compared the accuracy and performance of several states of health models. For improved understanding and prediction of the condition of health behavior, data-driven modeling has certain significant advantages over older methodologies. The methods used in this study can be seen as a revolutionary low-cost, high-accuracy, and dependable approach to understanding and analyzing the state of health of batteries. At first, an intelligent model was created using a data-driven modeling strategy. Secondly, the concurrent battery data are qualified using the data-driven model. The machine learning (ML) method creates a very accurate and dependable model for forecasting battery health in real-world scenarios. Third, the previously established ML model was used to develop a knowledge-based online service for battery health. This web service can be used to test battery health, monitor battery behavior, and perform a variety of other tasks. A variety of similar solutions for diverse systems can be derived using the same technique. The default efficiency of the ML algorithmic module, R-Squared (R2), and Mean Square Error (MSE) were also utilized as performance measures. The R2 as a standard is used to examine the effectiveness of a fit. The result is a value between 0 and 1, with 1 indicating a better model fit. MSE stands for mean squared error. A lower MSE number implies superior model performance, since it reflects how close the parameter estimates are to the actual values. The training set of the battery model had a score of 0.9999, whereas the testing set had a score of 0.9995. The R2 score was one, with an M.S.E. of 0.03. As a result of these three indicators, the data-driven ML model used in this study proved to be accurate.","",""
0,"S. Mukhopadhyay, S. Krishnan","Robust Identification of the QRS-Complexes in Electrocardiogram Signals Using Ramanujan Filter Bank-Based Periodicity Estimation Technique",2022,"","","","",130,"2022-07-13 09:36:46","","10.3389/frsip.2022.921973","","",,,,,0,0.00,0,2,1,"Plausibly, the first computerized and automated electrocardiogram (ECG) signal processing algorithm was published in the literature in 1961, and since then, the number of algorithms that have been developed to-date for the detection of the QRS-complexes in ECG signals is countless. Both the digital signal processing and artificial intelligence-based techniques have been tested rigorously in many applications to achieve a high accuracy of the detection of the QRS-complexes in ECG signals. However, since the ECG signals are quasi-periodic in nature, a periodicity analysis-based technique would be an apt approach for the detection its QRS-complexes. Ramanujan filter bank (RFB)-based periodicity estimation technique is used in this research for the identification of the QRS-complexes in ECG signals. An added advantage of the proposed algorithm is that, at the instant of detection of a QRS-complex the algorithm can efficiently indicate whether it is a normal or a premature ventricular contraction or an atrial premature contraction QRS-complex. First, the ECG signal is preprocessed using Butterworth low and highpass filters followed by amplitude normalization. The normalized signal is then passed through a set of Ramanujan filters. Filtered signals from all the filters in the bank are then summed up to obtain a holistic time-domain representation of the ECG signal. Next, a Gaussian-weighted moving average filter is used to smooth the time-period-estimation data. Finally, the QRS-complexes are detected from the smoothed data using a peak-detection-based technique, and the abnormal ones are identified using a period thresholding-based technique. Performance of the proposed algorithm is tested on nine ECG databases (totaling a duration of 48.91 days) and is found to be highly competent compared to that of the state-of-the-art algorithms. To the best of our knowledge, such an RFB-based QRS-complex detection algorithm is reported here for the first time. The proposed algorithm can be adapted for the detection of other ECG waves, and also for the processing of other biomedical signals which exhibit periodic or quasi-periodic nature.","",""
3,"Nuoa Lei","A robust modeling framework for energy analysis of data centers",2020,"","","","",131,"2022-07-13 09:36:46","","10.1145/3401335.3401648","","",,,,,3,1.50,3,1,2,"Global digitalization has given birth to the explosion of digital services in approximately every sector of contemporary life. Applications of artificial intelligence, blockchain technologies, and internet of things are promising to accelerate digitalization further. As a consequence, the number of data centers, which provide the services of data processing, storage, and communication services, is also increasing rapidly. Because data centers are energy-intensive with significant and growing electricity demand, an energy model of data centers with temporal, spatial, and predictive analysis capability is critical for guiding industry and governmental authorities for making technology investment decisions. However, current models fail to provide consistent and high dimensional energy analysis for data centers due to severe data gaps. This can be further attributed to the lack of the modeling capabilities for energy analysis of data center components including IT equipment and data center cooling and power provisioning infrastructure in current energy models. In this research, a technology-based modeling framework, in hybrid with a data-driven approach, is proposed to address the knowledge gaps in current data center energy models. The research aims to provide policy makers and data center energy analysts with comprehensive understanding of data center energy use and efficiency opportunities and a better understanding of macro-level data center energy demand and energy saving potentials, in addition to the technological barriers for adopting energy efficiency measures.","",""
150,"C. Yan, Liang Li, Chunjie Zhang, Bingtao Liu, Yongdong Zhang, Qionghai Dai","Cross-Modality Bridging and Knowledge Transferring for Image Understanding",2019,"","","","",132,"2022-07-13 09:36:46","","10.1109/TMM.2019.2903448","","",,,,,150,50.00,25,6,3,"The understanding of web images has been a hot research topic in both artificial intelligence and multimedia content analysis domains. The web images are composed of various complex foregrounds and backgrounds, which makes the design of an accurate and robust learning algorithm a challenging task. To solve the above significant problem, first, we learn a cross-modality bridging dictionary for the deep and complete understanding of a vast quantity of web images. The proposed algorithm leverages the visual features into the semantic concept probability distribution, which can construct a global semantic description for images while preserving the local geometric structure. To discover and model the occurrence patterns between intra- and inter-categories, multi-task learning is introduced for formulating the objective formulation with Capped-$\ell _{1}$ penalty, which can obtain the optimal solution with a higher probability and outperform the traditional convex function-based methods. Second, we propose a knowledge-based concept transferring algorithm to discover the underlying relations of different categories. This distribution probability transferring among categories can bring the more robust global feature representation, and enable the image semantic representation to generalize better as the scenario becomes larger. Experimental comparisons and performance discussion with classical methods on the ImageNet, Caltech-256, SUN397, and Scene15 datasets show the effectiveness of our proposed method at three traditional image understanding tasks.","",""
24,"Sebastian Bader, P. Hitzler, Steffen Hölldobler","The Integration of Connectionism and First-Order Knowledge Representation and Reasoning as a Challenge for Artificial Intelligence",2004,"","","","",133,"2022-07-13 09:36:46","","","","",,,,,24,1.33,8,3,18,"Intelligent systems based on first-order logic on the one hand, and on artificial neural networks (also called connectionist systems) on the other, differ substantially. It would be very desirable to combine the robust neural networking machinery with symbolic knowledge representation and reasoning paradigms like logic programming in such a way that the strengths of either paradigm will be retained. Current state-of-the-art research, however, fails by far to achieve this ultimate goal. As one of the main obstacles to be overcome we perceive the question how symbolic knowledge can be encoded by means of connectionist systems: Satisfactory answers to this will naturally lead the way to knowledge extraction algorithms and to integrated neural-symbolic systems.","",""
7,"Tatiana Tambouratzis, J. Giannatsis, A. Kyriazis, Panayiotis Siotropos","Applying the Computational Intelligence Paradigm to Nuclear Power Plant Operation",2020,"","","","",134,"2022-07-13 09:36:46","","10.4018/ijeoe.2020010102","","",,,,,7,3.50,2,4,2,"In the guise of artificial neural networks (ANNs), genetic/evolutionary computation algorithms (GAs/ECAs), fuzzy logic (FL) inference systems (FLIS) and their variants as well as combinations, the computational intelligence (CI) paradigm has been applied to nuclear energy (NE) since the late 1980s as a set of efficient and accurate, non-parametric, robust-to-noise as well as to-missing-information, non-invasive on-line tools for monitoring, predicting and overall controlling nuclear (power) plant (N(P)P) operation. Since then, the resulting CI-based implementations have afforded increasingly reliable as well as robust performance, demonstrating their potential as either stand-alone tools, or - whenever more advantageous - combined with each other as well as with traditional signal processing techniques. The present review is focused upon the application of CI methodologies to the - generally acknowledged as - key-issues of N(P)P operation, namely: control, diagnostics and fault detection, monitoring, N(P)P operations, proliferation and resistance applications, sensor and component reliability, spectroscopy, fusion supporting operations, as these have been reported in the relevant primary literature for the period 1990-2015. At one end, 1990 constitutes the beginning of the actual implementation of innovative, and – at the same time – robust as well as practical, directly implementable in H/W, CI-based solutions/tools which have proved to be significantly superior to the traditional as well as the artificial-intelligence-(AI)derived methodologies in terms of operation efficiency as well as robustness-to-noise and/or otherwise distorted/missing information. At the other end, 2015 marks a paradigm shift in terms of the emergent (and, swiftly, ubiquitous) use of deep neural networks (DNNs) over existing ANN architectures and FL problem representations, thus dovetailing the increasing requirements of the era of complex - as well as Big - Data and forever changing the means of ANN/neuro-fuzzy construction and application/performance. By exposing the prevalent CI-based tools for each key-issue of N(P)P operation, overall as well as over time for the given 1990-2015 period, the applicability and optimal use of CI tools to NE problems is revealed, thus providing the necessary know-how concerning crucial decisions that need to be made for the increasingly efficient as well as safe exploitation of NE.","",""
118,"Pengzhen Lu, Shengyong Chen, Yujun Zheng","Artificial Intelligence in Civil Engineering",2012,"","","","",135,"2022-07-13 09:36:46","","10.1155/2012/145974","","",,,,,118,11.80,39,3,10,"Artificial intelligence is a branch of computer science, involved in the research, design, and application of intelligent computer. Traditional methods for modeling and optimizing complex structure systems require huge amounts of computing resources, and artificial-intelligence-based solutions can often provide valuable alternatives for efficiently solving problems in the civil engineering. This paper summarizes recently developed methods and theories in the developing direction for applications of artificial intelligence in civil engineering, including evolutionary computation, neural networks, fuzzy systems, expert system, reasoning, classification, and learning, as well as others like chaos theory, cuckoo search, firefly algorithm, knowledge-based engineering, and simulated annealing. The main research trends are also pointed out in the end. The paper provides an overview of the advances of artificial intelligence applied in civil engineering.","",""
9,"R. Das, Ameya Godbole, S. Dhuliawala, M. Zaheer, A. McCallum","A Simple Approach to Case-Based Reasoning in Knowledge Bases",2020,"","","","",136,"2022-07-13 09:36:46","","10.24432/C52S3K","","",,,,,9,4.50,2,5,2,"We present a surprisingly simple yet accurate approach to reasoning in knowledge graphs (KGs) that requires \emph{no training}, and is reminiscent of case-based reasoning in classical artificial intelligence (AI).  Consider the task of finding a target entity given a source entity and a binary relation.  Our approach finds multiple \textit{graph path patterns} that connect similar source entities through the given relation, and looks for pattern matches starting from the query source.  Using our method, we obtain new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122.  We also demonstrate that our model is robust in low data settings, outperforming recently proposed meta-learning approaches.","",""
12,"Xinxin Jiang, Shirui Pan, Guodong Long, Fei Xiong, Jing Jiang, Chengqi Zhang","Cost-Sensitive Parallel Learning Framework for Insurance Intelligence Operation",2019,"","","","",137,"2022-07-13 09:36:46","","10.1109/TIE.2018.2873526","","",,,,,12,4.00,2,6,3,"Recent advancements in artificial intelligence are providing the insurance industry with new opportunities to create tailored solutions and services based on newfound knowledge of consumers, and the execution of enhanced operations and business functions. However, insurance data are heterogeneous, and imbalanced class distribution with low frequency and high dimensions, which presents four major challenges to machine learning in real-world business. Traditional machine learning algorithms can typically apply to standard data sets, which are normally homogeneous and balanced. In this paper, we focus on an efficient cost-sensitive parallel learning framework (CPLF) to enhance insurance operations with a deep learning approach that does not require preprocessing. Our approach comprises a novel, unified, end-to-end cost-sensitive parallel neural network that learns real-world heterogeneous data. A specifically designed cost-sensitive matrix then automatically generates a robust model for learning minority classifications, and the parameters of both the cost-sensitive matrix and the hybrid neural network are alternately but jointly optimized during training. We also study the CPLF-based architecture for a real-world insurance intelligence operation system, and demonstrate fraud detection and policy renewal experiments on this system. The results of comparative experiments on real-world insurance data sets reflecting actual business cases demonstrate the effectiveness of our design.","",""
7,"Han Xiao, Yidong Chen, X. Shi","Knowledge Graph Embedding Based on Multi-View Clustering Framework",2019,"","","","",138,"2022-07-13 09:36:46","","10.1109/TKDE.2019.2931548","","",,,,,7,2.33,2,3,3,"Knowledge representation is one of the critical problems in knowledge engineering and artificial intelligence, while knowledge embedding as a knowledge representation methodology indicates entities and relations in knowledge graph as low-dimensional, continuous vectors. In this way, knowledge graph is compatible with numerical machine learning models. Major knowledge embedding methods employ geometric translation to design score function, which is weak-semantic for natural language processing. To overcome this disadvantage, in this paper, we propose our model based on multi-view clustering framework, which could generate semantic representations of knowledge elements (i.e., entities/relations). With our semantic model, we also present an empowered solution to entity retrieval with entity description. Extensive experiments show that our model achieves substantial improvements against baselines on the task of knowledge graph completion, triple classification, entity classification, and entity retrieval.","",""
4,"M. Gates, Mukesh Ambani","Non-Parametric Reasoning on Knowledge Bases",2020,"","","","",139,"2022-07-13 09:36:46","","","","",,,,,4,2.00,2,2,2,"We present a surprisingly simple yet accurate approach to reasoning in knowledge graphs (KGs) that requires no training, and is reminiscent of case-based reasoning in classical artificial intelligence (AI). Consider the task of finding a target entity given a source entity and a binary relation. Our approach finds multiple graph path patterns that connect similar source entities through the given relation, and looks for pattern matches starting from the query source. Using our method, we obtain new state-of-the-art accuracy, outperforming all previous models, on NELL-995 and FB-122. We also demonstrate that our model is robust in low data settings, outperforming recently proposed meta-learning approaches.","",""
7,"Qianqian Song, Jing Su, Wei Zhang","scGCN: a Graph Convolutional Networks Algorithm for Knowledge Transfer in Single Cell Omics",2020,"","","","",140,"2022-07-13 09:36:46","","10.1101/2020.09.13.295535","","",,,,,7,3.50,2,3,2,"Single-cell omics represent the fastest-growing genomics data type in the literature and the public genomics repositories. Leveraging the growing repository of labeled datasets and transferring labels from existing datasets to newly generated datasets will empower the exploration of the single-cell omics. The current label transfer methods have limited performance, largely due to the intrinsic heterogeneity and extrinsic differences between datasets. Here, we present a robust graph-based artificial intelligence model, single-cell Graph Convolutional Network (scGCN), to achieve effective knowledge transfer across disparate datasets. Benchmarked with other label transfer methods on totally 30 single cell omics datasets, scGCN has consistently demonstrated superior accuracy on leveraging cells from different tissues, platforms, and species, as well as cells profiled at different molecular layers. scGCN is implemented as an integrated workflow as a python software, which is available at https://github.com/QSong-github/scGCN.","",""
21,"Tiago Oliveira, P. Novais, J. Neves","Development and implementation of clinical guidelines: An artificial intelligence perspective",2014,"","","","",141,"2022-07-13 09:36:46","","10.1007/s10462-013-9402-2","","",,,,,21,2.63,7,3,8,"","",""
2,"Matthew Sills, P. Ranade, Sudip Mittal","Cybersecurity Threat Intelligence Augmentation and Embedding Improvement - A Healthcare Usecase",2020,"","","","",142,"2022-07-13 09:36:46","","10.1109/ISI49825.2020.9280482","","",,,,,2,1.00,1,3,2,"The implementation of Internet of Things (IoT) devices in medical environments, has introduced a growing list of security vulnerabilities and threats. The lack of an extensible big data resource that captures medical device vulnerabilities limits the use of Artificial Intelligence (AI) based cyber defense systems in capturing, detecting, and preventing known and future attacks. We describe a system that generates a repository of Cyber Threat Intelligence (CTI) about various medical devices and their known vulnerabilities from sources such as manufacturer and ICS-CERT vulnerability alerts. We augment the intelligence repository with data sources such as Wikidata and public medical databases. The combined resources are integrated with threat intelligence in our Cybersecurity Knowledge Graph (CKG) from previous research. The augmented graph embeddings are useful in querying relevant information and can help in various AI assisted cybersecurity tasks. Given the integration of multiple resources, we found the augmented CKG produced higher quality graph representations. The augmented CKG produced a 31% increase in the Mean Average Precision (MAP) value, computed over an information retrieval task.","",""
2,"J. R. Silva, J. Silva, T. Vaquero","Formal Knowledge Engineering for Planning: Pre and Post-Design Analysis",2020,"","","","",143,"2022-07-13 09:36:46","","10.1007/978-3-030-38561-3_3","","",,,,,2,1.00,1,3,2,"","",""
2,"T. Schmid","Using Learning Algorithms to Create, Exploit and Maintain Knowledge Bases: Principles of Constructivist Machine Learning",2020,"","","","",144,"2022-07-13 09:36:46","","","","",,,,,2,1.00,2,1,2,"Recently, interest has grown in connecting modern machine learning approaches with traditional expert systems. This can mean, e.g, to identify patterns with neural networks and integrate them with knowledge graphs. While such combined systems offer a variety of advantages, few domainindependent approaches are known to make a hybrid artificial intelligence applicable without human interaction. To this end, we present the implementation of a constructivist machine learning framework (conML). This novel paradigm uses machine learning to manage a knowledge base and thereby allows for both raw data-based and symbolic information processing on the same internal knowledge representation. Based on axioms for a constructivist machine learning, we describe which operations are required to create, exploit and maintain a knowledge base and how these operations may be implemented with machine learning techniques. The major practical obstacle in this approach is to implement an automated deconstruction process that avoids ambiguity, handles continuous learning and allows knowledge abstraction. As we demonstrate, however, these obstacles can be overcome and constructivist machine learning can be put into practice. Combining machine learning and knowledge engineering is currently considered a potential game changing advancement in artificial intelligence. Neural networks and other machine learning techniques have proven strength in adapting to highly complex patterns and relationships, but are unable to represent existing knowledge explicitly and in an abstract fashion as expert systems can. Expert systems, on the other hand, operate on human-understandable knowledge representations but are highly domain-specific and, moreover, unable to process real-world data directly as machine learning can. Therefore, it is expected that joining both fields will produce a hybrid artificial intelligence that is “explainable, compliant and grounded in domain knowledge” (Martin et al. 2019). Such systems may, e.g., be able to identify patterns with neural networks and integrate them with knowledge graphs (Subasic, Yin, and Lin 2019). Copyright 2020 held by the author(s). In A. Martin, K. Hinkelmann, H.-G. Fill, A. Gerber, D. Lenat, R. Stolle, F. van Harmelen (Eds.), Proceedings of the AAAI 2020 Spring Symposium on Combining Machine Learning and Knowledge Engineering in Practice (AAAI-MAKE 2020). Stanford University, Palo Alto, California, USA, March 23-25, 2020. In fact, the idea of a hybrid artificial intelligence has been discussed for more than 30 years (Gallant 1988; Hendler 1989; Skeirik 1990; Levey 1991; Morik et al. 1993). So far, however, most research in this field focuses on specific knowledge or application domains like medical diagnosis (Hudson, Cohen, and Anderson 1991; Karabatak and Ince 2009; Herrmann 1995). This is to a large extent due to the fact that knowledge bases are typically created manually, which is a highly time-consuming task that requires detailled knowledge of the domain (Kidd 2012). No less timeconsuming are exploitation and maintenance of knowledge bases, which are typical follow-up phases within the life cycle of a knowledge base. While some progress has been made in employing algorithms for these tasks, several major challenges for an automated management of knowledge bases are still considered unresolved (Martinez-Gil 2015). Considering recent performance advancements in machine learning, manually managed knowledge bases obviously constitute a serious bottleneck in creating efficient hybrid systems. For truely automated systems, however, an implementable semantic interface between inductive machine learning and deductive expert systems is required. To this end, we have introduced a constructivist machine learning paradigm (Schmid 2019) based on the concept of learnable models and their storage in a knowledge base. While machine learning is currently dominated by neuro-inspired approaches, constructivist theories root in educational research (Fox 2001) and, so far, few actual implementations have been proposed for a constructivist machine learning (Drescher 1989; Quartz 1993). Central challenge for putting this into practice is the implementation of an automated deconstruction process, which to the best of our knowledge has only once been addressed successfully (Schmid 2018). Based on this paradigm, we designed a prototype for a constructivist machine learning that employs a meta databased knowledge base. Here, we present the underlying operationalizations and concepts required to put constructivist machine learning into practice. The rest of the paper is organized as follows: In section I, we lay out guidelines for automated knowledge base management. In section II, we define Stachowiak-like models as building blocks for knowledge representations. In section III, we introduce principles for constructivist machine learning processes. In section IV, we summarize our approach and point out future goals. Data Set or Stream Representation Knowledge Base select learn integrate","",""
1,"Chen Qiu, Guangyou Zhou, Z. Cai, Anders Søgaard","A Global–Local Attentive Relation Detection Model for Knowledge-Based Question Answering",2021,"","","","",145,"2022-07-13 09:36:46","","10.1109/TAI.2021.3068697","","",,,,,1,1.00,0,4,1,"Knowledge-based question answering (KBQA) is an essential but challenging task for artificial intelligence and natural language processing. A key challenge pertains to the design of effective algorithms for relation detection. Conventional methods model questions and candidate relations separately through the knowledge bases (KBs) without considering the rich word-level interactions between them. This approach may result in local optimal results. This article presents a global–local attentive relation detection model (GLAR) that utilizes the local module to learn the features of word-level interactions and employs the global module to acquire nonlinear relationships between questions and their candidate relations located in KBs. This article also reports on the application of an end-to-end retrieval-based KBQA system incorporating the proposed relation detection model. Experimental results obtained on two datasets demonstrated GLAR's remarkable performance in the relation detection task. Furthermore, the functioning of end-to-end KBQA systems was significantly improved through the relation detection model, whose results on both datasets outperformed even state-of-the-art methods. Impact Statement—Knowledge-based question answering (KBQA) aims at answering user questions posed over the knowledge bases (KBs). KBQA helps users access knowledge in the KBs more easily, and it works on two subtasks: entity mention detection and relation detection. While existing relation detection algorithms perform well on the global representation of questions and relations sequences, they ignore some local semantic information on interaction cases between them. The technology proposed in this article takes both global and local interactions into account. With superior improvement on two relation detection tasks and two KBQA end tasks, the technology provides more precise answers. It could be used in more applications, including intelligent customer service, intelligent finance, and others.","",""
5985,"David Silver, Julian Schrittwieser, K. Simonyan, Ioannis Antonoglou, Aja Huang, A. Guez, T. Hubert, Lucas baker, Matthew Lai, A. Bolton, Yutian Chen, T. Lillicrap, Fan Hui, L. Sifre, George van den Driessche, T. Graepel, D. Hassabis","Mastering the game of Go without human knowledge",2017,"","","","",146,"2022-07-13 09:36:46","","10.1038/nature24270","","",,,,,5985,1197.00,599,17,5,"","",""
2265,"M. Negnevitsky","Artificial Intelligence: A Guide to Intelligent Systems",2001,"","","","",147,"2022-07-13 09:36:46","","","","",,,,,2265,107.86,2265,1,21,"From the Publisher:  Virtually all the literature on artificial intelligence is expressed in the jargon of commuter science, crowded with complex matrix algebra and differential equations. Unlike many other books on computer intelligence, this one demonstrates that most ideas behind intelligent systems are simple and straightforward. The book has evolved from lectures given to students with little knowledge of calculus, and the reader needs no prerequisites associated with knowledge of any programming language. The methods used in the book have been extensively tested through several courses given by the author.    The book provides an introduction to the field of computer intelligence, covering    rule-based expert systems,  fuzzy expert systems,  frame-based expert systems,  artificail neural networks,  evolutionary computation,  hybrid intelligent systems,  knowledge engineering,  data mining.      In a university setting the book can be used as an introductory course within computer science, information systems or engineering departments. The book is also suitable as a self-study guide for non-computer science professionals, giving access to the state of the art in knowledge-based systems and computational intelligence. Everyone who faces challenging problems and cannot solve them using traditional approaches can benefit","",""
31,"Francesco Calimeri, Michael Fink, Stefano Germano, G. Ianni, Christoph Redl, Anton Wimmer","Angry-HEX: An Artificial Player for Angry Birds Based on Declarative Knowledge Bases",2016,"","","","",148,"2022-07-13 09:36:46","","10.1109/TCIAIG.2015.2509600","","",,,,,31,5.17,5,6,6,"This paper presents the Angry-HEX artificial intelligent agent that participated in the 2013 and 2014 Angry Birds Artificial Intelligence Competitions. The agent has been developed in the context of a joint project between the University of Calabria (UniCal) and the Vienna University of Technology (TU Vienna). The specific issues that arise when introducing artificial intelligence in a physics-based game are dealt with a combination of traditional imperative programming and declarative programming, used for modeling discrete knowledge about the game and the current situation. In particular, we make use of HEX programs, which are an extension of answer set programming (ASP) programs toward integration of external computation sources, such as 2-D physics simulation tools.","",""
1,"Divya, A. Jain, Gagandeep Singh","Classification of Big Data Through Artificial Intelligence",2015,"","","","",149,"2022-07-13 09:36:46","","","","",,,,,1,0.14,0,3,7,"By technology innovations, there has been a large increase within the utilization of Bigdata knowledge, joined of the foremost most well-liked styles of media thanks to its content richness, for several vital applications. To sustain Associate in Nursing current ascension of knowledge Bigdata, there's Associate in Nursing rising demand for a complicated content-based knowledge classification system. Thanks to the chop-chop increasing massive knowledge, abundant analysis effort has been dedicated to develop classification primarily based massive knowledge retrieval ways which may efficiently retrieve knowledge of interest. Considering the restricted man-power, it's abundant expected to develop retrieval ways that use options mechanically extracted from massive knowledge. Through Architecture-Algorithm co-design for Bigdata processing Applications, a scalable. Manycore processor consists of classification of heterogeneous cores with stream process capabilities, and zero-overhead inter-process communication through computer science with a hardware-software mechanism has been designed. This is often designed for achieving superior and low-power consumption, particularly thus on cut back access needed for Bigdata processing Applications. Keywords— classification , Bigdata , PBO(pollination based optimization ) , BBO(biogeography based optimization ) , Apriori. Divya et al, International Journal of Computer Science and Mobile Computing, Vol.4 Issue.8, August2015, pg. 17-25 © 2015, IJCSMC All Rights Reserved 18 INTRODUCTION Big data technologies are important in providing more accurate analysis, which may lead to more concrete decision-making resulting in greater operational efficiencies, cost reductions, and reduced risks for the business. To harness the power of big data, you would require an infrastructure that can manage and process huge volumes of structured and unstructured data in real time and can protect data privacy and security. There are various technologies in the market from different vendors including Amazon, IBM, Microsoft, etc., to handle big data. While looking into the technologies that handle big data, we examine the following two classes of technology: A. Operational Big Data This includes systems like Mongo DB that provide operational capabilities for real-time, interactive workloads where data is primarily captured and stored. NoSQL Big Data systems are designed to take advantage of new cloud computing architectures that have emerged over the past decade to allow massive computations to be run inexpensively and efficiently. This makes operational big data workloads much easier to manage, cheaper, and faster to implement. Some NoSQL systems can provide insights into patterns and trends based on realtime data with minimal coding and without the need for data scientists and additional infrastructure. B. Analytical Big Data This includes systems like Massively Parallel Processing (MPP) database systems and MapReduce that provide analytical capabilities for retrospective and complex analysis that may touch most or all of the data. MapReduce provides a new method of analyzing data that is complementary to the capabilities provided by SQL, and a system based on MapReduce that can be scaled up from single servers to thousands of high and low end machines. These two classes of technology are complementary and frequently deployed together. BENEFITS OF BIG DATA Big data is really critical to our life and its emerging as one of the most important technologies in modern world. Follow are just few benefits which are very much known to all of us: USING THE INFORMATION KEPT IN THE SOCIAL NETWORK LIKE FACEBOOK, THE MARKETING AGENCIES ARE LEARNING ABOUT THE RESPONSE FOR THEIR CAMPAIGNS, PROMOTIONS, AND OTHER ADVERTISING MEDIUMS. USING THE INFORMATION IN THE SOCIAL MEDIA LIKE PREFERENCES AND PRODUCT PERCEPTION OF THEIR CONSUMERS, PRODUCT COMPANIES AND RETAIL ORGANIZATIONS ARE PLANNING THEIR PRODUCTION. USING THE DATA REGARDING THE PREVIOUS MEDICAL HISTORY OF PATIENTS, HOSPITALS ARE PROVIDING BETTER AND QUICK SERVICE. REVIEW Behrouz et. al.[15] A combination of multiple classifiers leads to a significant improvement in classification performance. Furthermore, by learning an appropriate weighting of the features used via a genetic algorithm (GA), we further improve prediction accuracy. The GA is demonstrated to successfully improve the accuracy of combined classifier performance, about Divya et al, International Journal of Computer Science and Mobile Computing, Vol.4 Issue.8, August2015, pg. 17-25 © 2015, IJCSMC All Rights Reserved 19 10 To 12% when comparing to non-GA classifier. This method may be of considerable usefulness in identifying students at risk early, especially in very large classes, and allow the instructor to provide appropriate advising in a timely manner. Riccardo et al. [14] proposed cognitive, and behavioural aspects of distance students. Course Vis is presented in the paper, and several examples of pictorial representations generated by the tool. Luo et. al. [21] Efficient meaning for sampling of data, reduction of data also needed to develop. Newly develop mining technique and searching algorithms that are suitable for extracting more different or complex relationship between fields. Youssef M.ESSA et. al. [25] The proposed framework is developed by using mobile agent and MapReduce paradigm under Java Agent Development Framework (JADE). JADE is a promising middleware based on the agent paradigm because it supports generic services such as communication support, resource discovery, content delivery, data encoding and agents mobility. Indeed, there are seven reasons for using mobile agents as follows: (1) Reduce the network load, (2) Overcome network latency, (3) Encapsulate protocols, (4) Execute asynchronously and autonomously, (5) Adapt dynamically, (6) Naturally heterogeneous and robust, and","",""
0,"Zhijie Lu, Changzhu Zhang, Hao Zhang, Zhuping Wang, Chao Huang, Yuxiong Ji","Deep Reinforcement Learning Based Autonomous Racing Car Control With Priori Knowledge",2021,"","","","",150,"2022-07-13 09:36:46","","10.1109/cac53003.2021.9728289","","",,,,,0,0.00,0,6,1,"In the community of artificial intelligence, re-searchers have devoted much effort to the application of deep reinforcement learning algorithms for autonomous driving. Under deep reinforcement learning framework, it is important for the racing car agent to interact with its external environment to accumulate enough driving experience. However, the inter-action process is usually inefficient, risky and time-consuming. Furthermore, it is a common problem in relevant studies that brake policy is difficult to master. In this paper, we adopt some priori knowledge about vehicle dynamics to design the brake force and update it to the actor-critic network by soft-learning strategy. In addition, some effective strategies are developed to improve the training efficiency and control performance. The Open Racing Car Simulator(TORCS) is adopted to evaluate our algorithm. The simulation results show the effectiveness of our proposed algorithm with better learning efficiency, robustness and generalization performance.","",""
2,"L. Iliadis","Special issue of the 8th AIAI 2012 (Artificial Intelligence Applications and Innovations) international conference",2014,"","","","",151,"2022-07-13 09:36:46","","10.1007/s10462-013-9421-z","","",,,,,2,0.25,2,1,8,"","",""
5,"Runchi Zhang, Zhiyi Qiu","Optimizing hyper-parameters of neural networks with swarm intelligence: A novel framework for credit scoring",2020,"","","","",152,"2022-07-13 09:36:46","","10.1371/journal.pone.0234254","","",,,,,5,2.50,3,2,2,"Neural networks are widely used in automatic credit scoring systems with high accuracy and outstanding efficiency. However, in the absence of prior knowledge, it is difficult to determine the set of hyper-parameters, which makes its application limited in practice. This paper presents a novel framework of credit-scoring model based on neural networks trained by the optimal swarm intelligence (SI) algorithm. This framework incorporates three procedures. Step 1, pre-processing, including imputation, normalization, and re-ordering of the samples. Step 2, training, where SI algorithms optimize hyper-parameters of back-propagation artificial neural networks (BP-ANN) with the area under curve (AUC) as the evaluation function. Step 3, test, applying the optimized model in Step 2 to predict new samples. The results show that the framework proposed in this paper searches the hyper-parameter space efficiently and finds the optimal set of hyper parameters with appropriate time complexity, which enhances the fitting and generalization ability of BP-ANN. Compared with existing credit-scoring models, the model in this paper predicts with a higher accuracy. Additionally, the model enjoys a greater robustness, for the difference of performance between training and testing phases.","",""
1,"N. Howard, Naima Chouikhi, Ahsan Adeel, Katelyn Dial, Adam Howard, A. Hussain","BrainOS: A Novel Artificial Brain-Alike Automatic Machine Learning Framework",2020,"","","","",153,"2022-07-13 09:36:46","","10.3389/fncom.2020.00016","","",,,,,1,0.50,0,6,2,"Human intelligence is constituted by a multitude of cognitive functions activated either directly or indirectly by external stimuli of various kinds. Computational approaches to the cognitive sciences and to neuroscience are partly premised on the idea that computational simulations of such cognitive functions and brain operations suspected to correspond to them can help to further uncover knowledge about those functions and operations, specifically, how they might work together. These approaches are also partly premised on the idea that empirical neuroscience research, whether following on from such a simulation (as indeed simulation and empirical research are complementary) or otherwise, could help us build better artificially intelligent systems. This is based on the assumption that principles by which the brain seemingly operate, to the extent that it can be understood as computational, should at least be tested as principles for the operation of artificial systems. This paper explores some of the principles of the brain that seem to be responsible for its autonomous, problem-adaptive nature. The brain operating system (BrainOS) explicated here is an introduction to ongoing work aiming to create a robust, integrated model, combining the connectionist paradigm underlying neural networks and the symbolic paradigm underlying much else of AI. BrainOS is an automatic approach that selects the most appropriate model based on the (a) input at hand, (b) prior experience (a history of results of prior problem solving attempts), and (c) world knowledge (represented in the symbolic way and used as a means to explain its approach). It is able to accept diverse and mixed input data types, process histories and objectives, extract knowledge and infer a situational context. BrainOS is designed to be efficient through its ability to not only choose the most suitable learning model but to effectively calibrate it based on the task at hand.","",""
1,"Kiet Van Nguyen, Phong Nguyen-Thuan Do, Nhat Duy Nguyen, T. Huynh, A. Nguyen, N. Nguyen","XLMRQA: Open-Domain Question Answering on Vietnamese Wikipedia-based Textual Knowledge Source",2022,"","","","",154,"2022-07-13 09:36:46","","10.48550/arXiv.2204.07002","","",,,,,1,1.00,0,6,1,". Question answering (QA) is a natural language understanding task within the fields of information retrieval and information extraction that has attracted much attention from the computational linguistics and artificial intelligence research community in recent years because of the strong development of machine reading comprehension-based models. A reader-based QA system is a high-level search engine that can find correct answers to queries or questions in open-domain or domain-specific texts using machine reading comprehension (MRC) techniques. The majority of advancements in data resources and machine-learning approaches in the MRC and QA systems, on the other hand, especially in two resource-rich languages such as English and Chinese. A low-resource language like Vietnamese has witnessed a scarcity of research on QA systems. This paper presents XLMRQA, the first Vietnamese QA system using a supervised transformer-based reader on the Wikipedia-based textual knowledge source (using the UIT-ViQuAD corpus), outperforming the two robust QA systems using deep neural network models: DrQA and BERTserini with 24.46% and 6.28%, respectively. From the results obtained on the three systems, we analyze the influence of question types on the performance of the QA systems.","",""
0,"Y. D. Valle, N. Hampton","APPLICATION OF ARTIFICIAL INTELLIGENCE TO THE PROBLEM OF SELECTING THE APPROPRIATE DIAGNOSTIC FOR CABLE SYSTEMS",2011,"","","","",155,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,2,11,"Cable System Management requires an assessment of the health of the cables system. It is increasingly common for the assessment of aged cable systems to be made through the application of diagnostics measurements. There are a plethora of these techniques and embodiments; such that even an informed user has great difficulty making a rational choice on the most appropriate technique. To aid this decision making a Knowledge Based System has been developed that takes the knowledge of many diverse experts and delivers a robust framework by which rational, reproducible and transparent choices may be made. This paper discusses the development of the system and provides a number of illustrative case studies.","",""
22,"L. Valiant","Knowledge Infusion: In Pursuit of Robustness in Artificial Intelligence",2008,"","","","",156,"2022-07-13 09:36:46","","10.4230/LIPIcs.FSTTCS.2008.1770","","",,,,,22,1.57,22,1,14,"Endowing computers with the ability to apply commonsense knowledge with human- level performance is a primary challenge for computer science, comparable in importance to past great challenges in other fields of science such as the sequencing of the human genome. The right approach to this problem is still under debate. Here we shall discuss and attempt to justify one ap- proach, that of knowledge infusion. This approach is based on the view that the fundamental objective that needs to be achieved is robustness in the following sense: a framework is needed in which a computer system can represent pieces of knowledge about the world, each piece having some un- certainty, and the interactions among the pieces having even more uncertainty, such that the system can nevertheless reason from these pieces so that the uncertainties in its conclusions are at least controlled. In knowledge infusion rules are learned from the world in a principled way so that sub- sequent reasoning using these rules will also be principled, and subject only to errors that can be bounded in terms of the inverse of the effort invested in the learning process.","",""
4,"Glaucia C. Pereira","Genomics and Artificial Intelligence Working Together in Drug Discovery and Repositioning: The Advent of Adaptive Pharmacogenomics in Glioblastoma and Chronic Arterial Inflammation Therapies",2017,"","","","",157,"2022-07-13 09:36:46","","10.1007/978-3-319-53880-8_11","","",,,,,4,0.80,4,1,5,"","",""
2,"D. Grejner-Brzezinska, C. Toth, J. N. Markiel, S. Moafipoor, K. Czarnecka","Integration of Image-Based and Artificial Intelligence Algorithms: A Novel Approach to Personal Navigation",2012,"","","","",158,"2022-07-13 09:36:46","","10.1007/978-3-642-20338-1_120","","",,,,,2,0.20,0,5,10,"","",""
2,"Sankalp Khanna, A. Sattar, David Hansen","Advances in artificial intelligence research in health.",2012,"","","","",159,"2022-07-13 09:36:46","","10.4066/AMJ.2012.1352","","",,,,,2,0.20,1,3,10,"The business of health delivery is complex. Employing over 850,000 people, and delivering services to 21.3 million residents, the Australian health care system is currently strained to the maximum in dealing with increasing demand for services and an acute shortage of skilled professionals. The National e–Health Strategy drives a nationwide research agenda to provide the infrastructure and tools required to support the planning, management and delivery of health care services.    Deriving principles from the disciplines of computer science, mathematics, philosophy and physiology, and consisting of different fields, from machine vision to expert systems, the field of Artificial Intelligence (AI) deals with the creation of ""machines that can think"". Focused on traits of reasoning, knowledge representation, planning, learning, communication, perception and social intelligence, AI has been widely applied to augment the state of the art in Health Informatics.    This special issue reports on the latest developments in the field of AI motivated research in the health domain. The special issue arose from the inaugural Australian Workshop on Artificial Intelligence in Health (AIH 2011). It was held in conjunction with the 24th Australasian Joint Conference On Artificial Intelligence (AI2011), in Perth, Australia, in December 2011. The AIH 2011 workshop was a first of its kind, national initiative that aimed to bring together scholars and practitioners in the field of AI–driven health informatics to present and discuss their research, share their knowledge and experiences, define key research challenges and explore possible collaborations to advance e–Health development nationally and internationally. Therefore, the affiliation of AIH 2011 with AI2011 was both timely and mutually beneficial for the communities involved in these events.    AIH 2011 received 16 full paper submissions and each paper was reviewed by three program committee members. Six papers were accepted as full papers and five as short papers accompanied with posters.    The workshop brought together researchers from a variety of disciplines across various parts of the country and provided an excellent forum for discussion and exchange of ideas. In addition to presentation of papers, the workshop featured two keynote addresses, a poster session during lunch, and a panel discussion.    The first keynote address, “Using Artificial Intelligence to transform the management of Chronic Disease”, was delivered by Professor Michael Georgeff. In addition to presenting ongoing research efforts to apply AI techniques to better manage chronic disease, Professor Georgeff discussed the greatest innovations in healthcare from a medical practitioner’s viewpoint and focussed on how AI could be leveraged to transform healthcare.    The second keynote address, “Knowledge Acquisition Issues in Interpreting Laboratory Data”, was presented by Professor Paul Compton and posed a number of questions related to large–scale knowledge acquisition for decision– support systems in medicine, drawing lessons from experience in working with knowledge acquisition tools that support over 300 million diagnostics laboratory reports.    The workshop concluded with a panel discussion in which Professor Abdul Sattar and Professor Yogi Kanagasingam joined the keynote speakers to address the topic “AI for eHealth: 2012 and Beyond”. The discussion that ensued was very energetic and actively engaged audience interaction. Topics discussed ranged from the emerging contribution of AI over the past five decades, current issues with the marketing and uptake of AI–based solutions in mainstream healthcare, and the challenges for AI–based research and application in years to come. The panel and audience also acknowledged the key role a workshop like this would play in driving collaborative research efforts between AI and health informatics research communities.    All accepted papers were also invited to revise and submit their manuscripts for inclusion in this special issue of the Australasian Medical Journal (AMJ). Seven papers and a letter to the editor have been accepted for publication in the journal.    The first paper, by Bevan Koopman, Peter Bruza, Laurianne Sitbon and Michael Lawley, titled “Towards Semantic Search and Inference in Electronic Medical Records”, presents concept–based information retrieval for searching electronic medical records and demonstrates that the approach outperforms keyword–based search, working especially well for queries where the latter performs poorly. This paper was awarded the best paper prize at the workshop.    The second paper, by Kinzang Chhogyal, Abhaya Nayak, Rolf Schwitter and Abdul Sattar, titled “A Causal Model for Fluctuating Sugar Levels in Diabetes Patients”, investigates the use of fixed distance based belief revision to improve causal models. A simple scenario for fluctuating blood sugar levels in a diabetes patient is used to demonstrate the efficacy of this approach.    The third paper, by Alexander Krumpholz, David Hawking, Richard Jones, Tom Gedeon and Hugh Greville, titled “Automated Medical Literature Retrieval”, describes a system for the retrieval of relevant medical publications using queries generated automatically from data present in an electronic patient record. Integrated into an electronic record system, such a system would proactively support medical practitioners in the delivery of care.    The fourth paper, by Abeed Sarker, Diego Molla and Cecile Paris, titled “Extractive Summarisation of Medical Documents”, proposes a query focused approach for automatically summarising medical documents and helping medical practitioners find relevant information.    The fifth paper, by Diego Molla and Maria Elena Santiago– Martinez, titled “A Corpus for Evidence Based Summarisation”, presents a corpus of clinical questions and answers designed for training and testing automated text summarisers to support evidence–based medicine.    The sixth paper, by Di Xiao, Janardhan Vignarajan, Jane Lock, Shaun Frost, Mei–Ling Tay–Kearney and Yogi Kanagasingam, titled “Retinal Image Registration and Comparison for Clinical Decision Support”, proposes a set of accurate and robust retinal image registration solutions for longitudinal retinal image alignment and comparison    The seventh paper, by Amol Wagholikar, Maggie Fung and Colleen Nelson, titled “Improving Self–Care of Patients with Chronic Disease using Online Personal Health Record”, employs a case–based reasoning approach to self care for advanced prostate cancer patients in an online patient health record environment.    A letter to the editor, by Anthony Nguyen, Yue Kimi Sun, Laurianne Sitbon and Shlomo Geva, titled “Representation Of Assertions In Clinical Free Text Using SNOMED CT,” explores the use of the SNOMED CT terminology for representing medical concepts and their assertions in clinical free text. The authors demonstrate that that populating assertions as attribute values using SNOMED CT allows over 93% of assertions in their experimental dataset to be represented.    We hope that the breadth and diversity of the papers presented at the workshop and published in this special issue will foster further collaboration and AI driven research in health.    This workshop would not have been possible without the contributions of numerous fine people. First, we are greatly indebted to Professor Aditya Ghose, Professor Anthony Maeder, Professor Wayne Wobcke, Professor Mehmet Orgun, and Dr Yogesan (Yogi) Kanagasingam for their guidance and support. We would also like to thank the organising committee of the 24th Australasian Joint Conference on AI, the Institute of Integrated and Intelligent Systems, Griffith University for supporting the workshop, and the CSIRO Australian e–Health Research Centre for their support and sponsorship of travel scholarships and the best paper prize. Thanks are also due to Professor Moyez Jiwa and the AMJ for supporting the workshop and inviting accepted papers for inclusion into this special issue. Finally, we are indebted to the authors who responded to the invitation to submit their papers, and the reviewers who generously donated their time and expertise and provided very comprehensive reviews of the submitted papers.","",""
15,"Wei-Tsong Wang, Su-Ying Wu","Knowledge management based on information technology in response to COVID-19 crisis",2020,"","","","",160,"2022-07-13 09:36:46","","10.1080/14778238.2020.1860665","","",,,,,15,7.50,8,2,2,"ABSTRACT COVID-19’s rapid spread has caused a global pandemic. Consequently, it is imperative that healthcare organisations conduct crisis management (CM) to cope with this calamity. This study presents a set of operational guidelines for healthcare organisations to launch effective countermeasures against such crises by means of effective knowledge management (KM) practices. Additionally, information-technology (IT) applications can significantly improve organisations’ CM and KM capabilities by enhancing organisational responsiveness and flexibility. This study thus aims to articulate how the use of innovative IT-enabled mechanisms (e.g., non-contact monitoring devices, intelligent robots, and telemedicine) can reduce the risk of exposure and leverage an artificial intelligence-based epidemic intelligence dashboard to support appropriate decision-making by taking the operation of healthcare organisations in Taiwan during COVID-19 crisis as an example. The research results demonstrate the effectiveness of the employment of IT-enabled KM practices in CM settings in terms of preventing or minimising undesirable crisis consequences.","",""
1,"N. Zakaria, Rohayanti Hassan, M. R. Othman, Z. Zakaria, S. Kasim","A Review on Classification of the Urban Poverty Using the Artificial Intelligence Method",2017,"","","","",161,"2022-07-13 09:36:46","","10.18488/JOURNAL.2.2017.711.450.458","","",,,,,1,0.20,0,5,5,"Poverty and how it has been assessed and measured is a frequently discussed topic by policy makers and social developers. The identification process in poverty measurement is indeed essential towards acknowledging the poor in the population; hence this needs to be clarified. Malaysia measures poverty by means of poverty line, indicating the unidimensional and inflexible distribution of poor and non-poor especially in urban areas. Many researchers have used fuzzy logic to solve the problem of rigid poor/non-poor dichotomy. This current trend has been able to augment the gap between the rigid and inflexible classification of poor and non-poor. However, there are still several shortcomings that need attention. For instance, the classification of the poor in fuzzy logic that is based on the average income of households still does not cover on the different range of disadvantage on non-monetary items. Based on these trends, ANFIS is proposed to resolve on the highlighted issues. The winning features of ANFIS, which include on simplicity in implementation, understandable explanation facilities through fuzzy rules, and ease of incorporation of both linguistic and numeric knowledge for problem solving may help in producing better result in classification of the urban poor. Essentially, the neural network is proposed to complement the fuzzy system, hence overcoming the limitations of both fuzzy systems and neural networks. As such, ANFIS method is used in this study to better classify on the poor and non-poor compared to fuzzy rule-based system which is lacking in prediction error rate due to too many variables used. However, this method deteriorates from misclassified poverty indicators; hence this study proposed on ensemble ANFIS to produce more accurate and robust classification results. An ensemble model is usually employed to address the problems of over-fitting, high dimensionality or missing features in the training data. Generally, combining multiple classification models increases predictive performance compared to the use of an individual model alone. Therefore, based on these current trends, this study is aimed to do a review on classification of the urban poverty using the artificial intelligence method.","",""
8,"Jia-Jie Zhu, Wittawat Jitkrittum, M. Diehl, B. Schölkopf","Kernel Distributionally Robust Optimization: Generalized Duality Theorem and Stochastic Approximation",2021,"","","","",162,"2022-07-13 09:36:46","","","","",,,,,8,8.00,2,4,1,"We propose kernel distributionally robust optimization (Kernel DRO) using insights from the robust optimization theory and functional analysis. Our method uses reproducing kernel Hilbert spaces (RKHS) to construct a wide range of convex ambiguity sets, which can be generalized to sets based on integral probability metrics and finite-order moment bounds. This perspective unifies multiple existing robust and stochastic optimization methods. We prove a theorem that generalizes the classical duality in the mathematical problem of moments. Enabled by this theorem, we reformulate the maximization with respect to measures in DRO into the dual program that searches for RKHS functions. Using universal RKHSs, the theorem applies to a broad class of loss functions, lifting common limitations such as polynomial losses and knowledge of the Lipschitz constant. We then establish a connection between DRO and stochastic optimization with expectation constraints. Finally, we propose practical algorithms based on both batch convex solvers and stochastic Proceedings of the 24 International Conference on Artificial Intelligence and Statistics (AISTATS) 2021, San Diego, California, USA. PMLR: Volume 130. Copyright 2021 by the author(s). functional gradient, which apply to general optimization and machine learning tasks.","",""
43,"M. El-Melegy, Mohammed H. Essai, A. A. Ali","Robust Training of Artificial Feedforward Neural Networks",2009,"","","","",163,"2022-07-13 09:36:46","","10.1007/978-3-642-01082-8_9","","",,,,,43,3.31,14,3,13,"","",""
103,"I. Goldstein, S. Papert","Artificial Intelligence, Language, and the Study of Knowledge",1977,"","","","",164,"2022-07-13 09:36:46","","10.1207/s15516709cog0101_5","","",,,,,103,2.29,52,2,45,"This paper studies the relationship of Artificial Intelligence to the study of language and the representation of the underlying knowledge which supports the comprehension process. It develops the view that intelligence is based on the ability to use large amounts of diverse kinds of knowledge in procedural ways, rather than on the possession of a few general and uniform principles. The paper also provides a unifying thread to a variety of recent approaches to natural language comprehension. We conclude with a brief discussion of how Artificial Intelligence may have a radical impact on education if the principles which it utilizes to explore the representation and use of knowledge are made available to the student to use in his own learning experiences.","",""
82,"Zhongzhi Shi","Advanced Artificial Intelligence",2011,"","","","",165,"2022-07-13 09:36:46","","10.1142/7547","","",,,,,82,7.45,82,1,11,"Logic Foundation of Artificial Intelligence Constraint Reasoning Qualitative Reasoning Case-Based Reasoning Probabilistic Reasoning Inductive Learning Support Vector Machine Explanation-Based Learning Reinforcement Learning Rough Set Association Rules Knowledge Discovery Distributed Intelligence Evolutionary Computation.","",""
2,"Wei Tang, Chien-Ju Ho, Yang Liu","Linear Models are Robust Optimal Under Strategic Behavior",2021,"","","","",166,"2022-07-13 09:36:46","","","","",,,,,2,2.00,1,3,1,"There is an ubiquitous use of algorithms to inform decisions nowadays, from student evaluations, college admissions, to credit scoring. These decisions are made by applying a decision rule to individual’s observed features. Given the impacts of these decisions on individuals, decision makers are increasingly required to be transparent on their decision making to offer the “right to explanation.” Meanwhile, being transparent also invites potential manipulations, also known as gaming, that individuals can utilize the knowledge to strategically alter their features in order to receive a more beneficial decision. In this work, we study the problem of robust decision-making under strategic behavior. Prior works often assume that the decision maker has full knowledge of individuals’ cost structure for manipulations. We study the robust variant that relaxes this assumption: The decision maker does not have full knowledge but knows only a subset of the individuals’ available actions and associated costs. To approach this non-quantifiable uncertainty, we define robustness based on the worst-case guarantee of a decision, over all possible actions (including actions unknown to the decision maker) individuals might take. A decision rule is called robust optimal if its worst case performance is (weakly) better than that of all other decision rules. Our main contributions are two-fold. First, we provide a crisp characterization of the above robust optimality: For any decision rules under mild conditions that are robust optimal, there exists a linear decision rule that is equally robust opProceedings of the 24 International Conference on Artificial Intelligence and Statistics (AISTATS) 2021, San Diego, California, USA. PMLR: Volume 130. Copyright 2021 by the author(s). timal. Second, we explore the computational problem of searching for the robust optimal decision rule and demonstrate its connection to distributionally robust optimization. We believe our results promote the use of simple linear decisions with uncertain individual manipulations.","",""
67,"B. Chandrasekaran, Ashok K. Goel","From numbers to symbols to knowledge structures: artificial intelligence perspectives on the classification task",1988,"","","","",167,"2022-07-13 09:36:46","","10.1109/21.7491","","",,,,,67,1.97,34,2,34,"The general information-processing task of classification is considered and reviewed from the perspectives of the knowledge-based-reasoning, pattern-recognition, and connectionist paradigms in artificial intelligence, paying special attention to knowledge-based classificatory problem solving. The authors trace the evolution of the mechanisms for classification as the computational complexity of the problem increases, from numerical parameter-setting schemes, through those using intermediate abstractions and then relations between symbols, and finally to complex symbolic structures that explicitly incorporate domain knowledge. >","",""
11,"Gaolei Li, M. Dong, L. Yang, K. Ota, Jun Wu, Jianhua Li","Preserving Edge Knowledge Sharing Among IoT Services: A Blockchain-Based Approach",2020,"","","","",168,"2022-07-13 09:36:46","","10.1109/TETCI.2019.2952587","","",,,,,11,5.50,2,6,2,"Edge computational intelligence, integrating artificial intelligence (AI) and edge computing into Internet of Things (IoT), will generate many scattered knowledge. To enable auditable and delay-sensitive IoT services, these knowledge will be shared among decentralized intelligent network edges (DINEs), end users, and supervisors frequently. Blockchain has a promising ability to provide a traceable, privacy-preserving and tamper-resistant ledger for sharing edge knowledge. However, due to the complicated environments of network edges, knowledge sharing among DINEs still faces many challenges. Firstly, the resource limitation and mobility of DINEs impede the applicability of existing consensus tricks (e.g., Poof of Work, Proof of Stake, and Paxos) of blockchain. Secondly, the adversaries may eavesdrop the content of edge knowledge or entice the blockchain to forks using some attacking models (like man-in-the-middle attack, denial of services, etc.). In this article, an user-centric blockchain (UCB) framework is proposed for preserving edge knowledge sharing in IoT. Significant superiorities of UCB benefit from the proof of popularity (PoP) consensus mechanism, which is more energy-efficient and fast. Security analysis and experiments based on Raspberry Pi 3 Model B demonstrate its feasibility with low block generating delay and complexity.","",""
326,"Randall Davis, D. Lenat","Knowledge-based systems in artificial intelligence",1981,"","","","",169,"2022-07-13 09:36:46","","10.1016/s0736-5853(86)80076-4","","",,,,,326,7.95,163,2,41,"","",""
242,"A. Kitchen","Knowledge based systems in artificial intelligence",1985,"","","","",170,"2022-07-13 09:36:46","","10.1109/PROC.1985.13127","","",,,,,242,6.54,242,1,37,"","",""
0,"J. Fuchs","4th Workshop on Artificial Intelligence and Knowledge Based Systems for Space",1993,"","","","",171,"2022-07-13 09:36:46","","10.1017/S0269888900000217","","",,,,,0,0.00,0,1,29,"The Workshop on Artificial Intelligence and Knowledge-Based Systems for Space is a bi-annual event organized by the European Space Agency (ESA) at its Technology Centre (ESTEC) in Noordwijk, The Netherlands. It reflects the interest of the Agency in advanced technologies, including software. The 4th Workshop was held on May 17th-19th 1993, and was attended by over 100 industrial and research participants. The Workshop topics focussed on some interests of ESA, in particular: AI techniques in operational or quasi-operational applications; methodology, particularly Verification and Validation of KBSs; and Model-Based Reasoning, Knowledge Reuse and Planning/Scheduling.","",""
92,"Bo Göranzon, I. Josefson","Knowledge, Skill and Artificial Intelligence",1988,"","","","",172,"2022-07-13 09:36:46","","10.1007/978-1-4471-1632-5","","",,,,,92,2.71,46,2,34,"","",""
6,"Yuchuan Fu, Changle Li, F. Yu, T. Luan, Yao Zhang","An Autonomous Lane-Changing System With Knowledge Accumulation and Transfer Assisted by Vehicular Blockchain",2020,"","","","",173,"2022-07-13 09:36:46","","10.1109/JIOT.2020.2994975","","",,,,,6,3.00,1,5,2,"Inappropriate lane following and changing behaviors of connected and autonomous vehicles (CAVs) can result in accidents, such as rear-end collision and side collision. To remedy that, the use of deep reinforcement learning (DRL) for autonomous driving decisions is currently a widely used promising solution. In this case, the accuracy and effectiveness of such a machine learning (ML) model is quite essential for this artificial intelligence (AI)-enabled CAVs. This article proposes a blockchain-based collective learning (BCL) framework for autonomous lane-changing systems. Four key issues, namely, learning efficiency, data security, users’ privacy, as well as communication burden, are addressed by applying collective learning, vehicular blockchain, and knowledge transfer. First, we model the lane-changing problem as a DRL process and learn the autonomous lane-changing strategy through the deep deterministic policy gradient (DDPG) algorithm. Second, a single CAV involves a limited number of driving scenarios, and the independent learning method has the problem of inefficiency. Therefore, we propose a collective learning framework to utilize the “collective intelligence” shared by CAVs. Third, a vehicular blockchain is then applied to ensure the security and privacy of the user and data. In addition, the introduction of the blockchain can incentivize more users to participate in collective learning. Finally, in order to accelerate the learning process and achieve higher level performance while further reducing the communication burden, we use the corresponding knowledge extracted from the ML model such as human learning, as privileged information for sharing instead of directly sharing local ML models. Extensive simulation results validate the effectiveness and efficiency of our proposal in terms of learning efficiency, driving safety, as well as system security and robustness.","",""
0,"V. Lemaire, J. Lamirel, Pascal Cuxac","AIL Active and Incremental Learning August 27 , 2012 , Montpellier , France ECAI 2012 – 20 TH European Conference on Artificial Intelligence",2012,"","","","",174,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,3,10,"Most machine learning techniques assume, either explicitly or implicitly, that the data-generating process is stationary. This assumption guarantees that the model learnt during the initial training phase remains valid over time and that its performance is in line with our expectations. Unfortunately, this assumption does not truly hold in the real world representing, in many cases, a simplistic approximation of the reality. The talk will describe the Just-In-Time (JIT) approach that is a flexible tool implementing the detection/adaptation paradigm to cope with evolving processes. Solutions following this approach improve the knowledge about the model in stationary conditions by exploiting additional information coming from the field during the operational life. Differently, in nonstationary conditions, as soon as a change in the data-generating process is detected, the learnt model is discarded and a suitable one activated to keep the performance. As a valuable and challenging application of the proposed approach, JIT classifiers for concept drift will be detailed and discussed. Incremental Decision Tree based on order statistics Christophe Salperwyck1 and Vincent Lemaire2 Abstract. New application domains generate data which are not persistent anymore but volatile: network management, web profile modeling... These data arrive quickly, massively and are visible just once. Thus they necessarily have to be learnt according to their arrival orders. For classification problems online decision trees are known to perform well and are widely used on streaming data. In this paper, we propose a new decision tree method based on order statistics. The construction of an online tree usually needs summaries in the leaves. Our solution uses bounded error quantiles summaries. A robust and performing discretization or grouping method uses these summaries to provide, at the same time, a criterion to find the best split and better density estimations. This estimation is then used to build a naı̈ve Bayes classifier in the leaves to improve the prediction in the early learning stage. New application domains generate data which are not persistent anymore but volatile: network management, web profile modeling... These data arrive quickly, massively and are visible just once. Thus they necessarily have to be learnt according to their arrival orders. For classification problems online decision trees are known to perform well and are widely used on streaming data. In this paper, we propose a new decision tree method based on order statistics. The construction of an online tree usually needs summaries in the leaves. Our solution uses bounded error quantiles summaries. A robust and performing discretization or grouping method uses these summaries to provide, at the same time, a criterion to find the best split and better density estimations. This estimation is then used to build a naı̈ve Bayes classifier in the leaves to improve the prediction in the early learning stage.","",""
4,"Hongyu Li, D. Meng, Hong Wang, Xiaolin Li","Knowledge Federation: A Unified and Hierarchical Privacy-Preserving AI Framework",2020,"","","","",175,"2022-07-13 09:36:46","","10.1109/ICBK50248.2020.00022","","",,,,,4,2.00,1,4,2,"With strict protections and regulations of data privacy and security, conventional machine learning based on centralized datasets is confronted with significant challenges, making artificial intelligence (AI) impractical in many mission-critical and data-sensitive scenarios, such as finance, government, and health. In the meantime, tremendous datasets are scattered in isolated silos in various industries, organizations, different units of an organization, or different branches of an international organization. These valuable data resources are well underused. To advance AI theories and applications, we propose a comprehensive framework (called Knowledge Federation- KF) to address these challenges by enabling AI while preserving data privacy and ownership. Beyond the concepts of federated learning and secure multi-party computation, KF consists of four levels of federation: (1) information level, low-level statistics and computation of data, meeting the requirements of simple queries, searching and simplistic operators; (2) model level, supporting training, learning, and inference; (3) cognition level, enabling abstract feature representation at various levels of abstractions and contexts; (4) knowledge level, fusing knowledge discovery, representation, and reasoning. We further clarify the relationship and differentiation between knowledge federation and other related research areas. We have developed a reference implementation of KF, called iBond Platform, to offer a production-quality KF platform to enable industrial applications in finance, insurance, marketing, and government. The iBond platform will also help establish the KF community and a comprehensive ecosystem and usher in a novel paradigm shift towards secure, privacy-preserving and responsible AI. As far as we know, knowledge federation is the first hierarchical and unified framework for secure multi-party computing (statistics, queries, searching, and low-level operations) and learning (training, representation, discovery, inference, and reasoning).","",""
101,"J. Mira, José Ramón Álvarez-Sánchez","Artificial Intelligence and Knowledge Engineering Applications: A Bioinspired Approach: First International Work-Conference on the Interplay Between Natural and Artificial Computation, IWINAC 2005, Las Palmas, Canary Islands, Spain, June 15-18, 2005, Proceedings, Part II",2005,"","","","",176,"2022-07-13 09:36:46","","10.1007/b137296","","",,,,,101,5.94,51,2,17,"","",""
2,"M. Mistro, Y. Sheng, Y. Ge, C. Kelsey, J. Palta, Jing Cai, Qiuwen Wu, F. Yin, Q. Wu","Knowledge Models as Teaching Aid for Training Intensity Modulated Radiation Therapy Planning: A Lung Cancer Case Study",2020,"","","","",177,"2022-07-13 09:36:46","","10.3389/frai.2020.00066","","",,,,,2,1.00,0,9,2,"Purpose: Artificial intelligence (AI) employs knowledge models that often behave as a black-box to the majority of users and are not designed to improve the skill level of users. In this study, we aim to demonstrate the feasibility that AI can serve as an effective teaching aid to train individuals to develop optimal intensity modulated radiation therapy (IMRT) plans. Methods and Materials: The training program is composed of a host of training cases and a tutoring system that consists of a front-end visualization module powered by knowledge models and a scoring system. The current tutoring system includes a beam angle prediction model and a dose-volume histogram (DVH) prediction model. The scoring system consists of physician chosen criteria for clinical plan evaluation as well as specially designed criteria for learning guidance. The training program includes six lung/mediastinum IMRT patients: one benchmark case and five training cases. A plan for the benchmark case is completed by each trainee entirely independently pre- and post-training. Five training cases cover a wide spectrum of complexity from easy (2), intermediate (1) to hard (2). Five trainees completed the training program with the help of one trainer. Plans designed by the trainees were evaluated by both the scoring system and a radiation oncologist to quantify planning quality. Results: For the benchmark case, trainees scored an average of 21.6% of the total max points pre-training and improved to an average of 51.8% post-training. In comparison, the benchmark case's clinical plans score an average of 54.1% of the total max points. Two of the five trainees' post-training plans on the benchmark case were rated as comparable to the clinically delivered plans by the physician and all five were noticeably improved by the physician's standards. The total training time for each trainee ranged between 9 and 12 h. Conclusion: This first attempt at a knowledge model based training program brought unexperienced planners to a level close to experienced planners in fewer than 2 days. The proposed tutoring system can serve as an important component in an AI ecosystem that will enable clinical practitioners to effectively and confidently use KBP.","",""
2,"Joshua Ho, Chien-Min Wang","Explainable and Adaptable Augmentation in Knowledge Attention Network for Multi-Agent Deep Reinforcement Learning Systems",2020,"","","","",178,"2022-07-13 09:36:46","","10.1109/AIKE48582.2020.00031","","",,,,,2,1.00,1,2,2,"The scale of modem Artificial Intelligence systems has been growing and entering more research territories by incorporating Deep Learning (DL) and Deep Reinforcement Learning (DRL) methods. More specifically, multi-agent DRL methods have been widely applied to address the problems of high-dimensional computation, which interpret the conditions that real-world systems mainly encounter and the issues that require resolving. However, the current approaches of DL and DRL are often challenged for their untransparent and time-consuming modeling processes in their attempt to achieve a practical and applicable inference based on human-level perspective and acceptance. This paper presents an explainable and adaptable augmented knowledge attention network for multi-agent DRL systems, which uses game theory simulation to tackle the problem of non-stationarity at the beginning, while improving the learning exploration built upon the strategic ontology to achieve the learning convergence more efficiently for autonomous agents. We anticipate that our approach will facilitate future research studies and potential research inspections of emerging multi-agent DRL systems for increasingly complex and autonomous environments.","",""
0,"X. Jin, Jianmin Jiang, G. Min","Managing computer files via artificial intelligence approaches",2009,"","","","",179,"2022-07-13 09:36:46","","10.1007/s10462-009-9129-2","","",,,,,0,0.00,0,3,13,"","",""
0,"Gregory A. Luhan","Scaling Intelligence",2021,"","","","",180,"2022-07-13 09:36:46","","10.1080/24751448.2021.1967048","","",,,,,0,0.00,0,1,1,"T A D 5 : 2 Intelligence without action is inert. As the solicited contributions to this volume demonstrate, actionable intelligence relies on a common ground from which architecture and allied disciplines can leverage depths and breadths of knowledge to mobilize new technologies. The Op/Positions essays examine preexisting local knowledge in historical places, enhance discovery through systems-based workflows, and foster the transformational shift from invisible smartness to holistic, design trade-offs that produce more humane and cooperative cities. As Jyoti Hosagrahar notes, place-intelligence provides current generations with a scalable and reflective framework that values the past, promotes deeper foundations, and connects resilient community design and well-being to informed decision-making. Similarly, Azam Khan posits a systems-based approach for leveraging existing knowledge to solve increasingly complex problems holistically. The emergent metaheuristic tools expand architectural design ability, enhance discovery, and yield more energy-efficient and less wasteful buildings. Norbert Streitz advocates for resetting priorities at an urban scale and generating principles that simultaneously privilege the individual and the collective. The resulting types of affordances and ethical alignments could balance data harvesting with people’s need for interactive, communicative, and cooperative spaces and places. The Research Methodology contributions critically examine a site’s latent potential and propose challenging new ways for testing and improving the lived condition at all scales. Whether at the intimate scale of one human-robot interaction or applied to industry-level protocols or full-scale testing scenarios, real-world applied research design necessitates collecting and analyzing large data sets. Jim Tørresen examines predictive intelligent system design, comprising ethical sensor data collection, robot interaction, and human-centric artificial intelligence to anticipate and respond to elderly care needs. Integrating artificial intelligence and problem-solving best practices can interactively adapt to a user’s needs and draw upon years of industry-based construction knowledge. Lukas Kirner, Elisa Lublasser, and Sigrid Brell-Cokcan developed enhanced methods for elevating existing construction industry processes through interdisciplinary collaboration, robot-assisted interaction, laboratory experimentation, factoryto-field investigation, and full-scale testing. The jump from laboratory experiments to full-scale prototyping requires the refinement of previous data exchanges and information flows to produce generalizable results. Maintaining quantitative and qualitative data research design, controlled trials, and procedural rigor requires close monitoring and comparison of real-time data collections and digital simulations. In their Details+ contribution, Jonathan Heppner and Thomas Robinson deployed intelligent testing on an innovative post-tensioned, gravity-resistant, and lateral force-resistant rocking wall system at full scale and detail level. The lab-tested results generated valuable insights into damage-resistant construction methods, informed broader building practices, and demonstrated that their previously unproven assembly could prevent massive failure and save lives. Increasing the use of enhanced digital/computational methods brings renewed attention to gaining greater control over the software and tools used to generate and validate design decisions at all scales. Re/Views addresses these issues, examines interoperable software platforms, compares gaming engines, integrates sensors, and surveys current, emerging, and projected use of autonomous robots across the AEC industry. Karen Kensek presents strategies for improving workflows and overcoming software limitations through customizable add-in solutions for existing Building Information Modeling processes. Enhanced parametric interoperability and data functionality, streamlined procedures, and verified code-compliance bolster deliberative intelligence. Christopher Morse compares game engines that combine visualization, communication, and design in robust, adaptable, flexible, real-time, and interactive environments. Immersive, customizable, connective, and cloud-based integration inform architectural research and professional practice. Peter Kerr investigates scalable interactions with technology, examining affordances and benefits of sensor nodes connected via intelligent Building Management Systems (iBMS). Alvise Simondetti, Nicholas Bachand, Aifric Delahunty, James Griffith, and Julius Sustarevas examine the unfolding paradigm shift toward autonomous robotics, artificial intelligence, and machine learning as architecture moves beyond task-specific operations to inform scalable and sustainable design that augment and complement human capabilities. As shown by these authors, in its performative function, and when viewed through the prismatic lenses of technology, architecture, and design, scaling intelligence successfully narrows the gap between empirical observation, applied research, and professional practice. Scaling Intelligence","",""
23,"J. Prentzas","Artificial Intelligence Methods in Early Childhood Education",2013,"","","","",181,"2022-07-13 09:36:46","","10.1007/978-3-642-29694-9_8","","",,,,,23,2.56,23,1,9,"","",""
296,"F. Villa, K. Bagstad, B. Voigt, G. Johnson, R. Portela, M. Honzák, David Batker","A Methodology for Adaptable and Robust Ecosystem Services Assessment",2014,"","","","",182,"2022-07-13 09:36:46","","10.1371/journal.pone.0091001","","",,,,,296,37.00,42,7,8,"Ecosystem Services (ES) are an established conceptual framework for attributing value to the benefits that nature provides to humans. As the promise of robust ES-driven management is put to the test, shortcomings in our ability to accurately measure, map, and value ES have surfaced. On the research side, mainstream methods for ES assessment still fall short of addressing the complex, multi-scale biophysical and socioeconomic dynamics inherent in ES provision, flow, and use. On the practitioner side, application of methods remains onerous due to data and model parameterization requirements. Further, it is increasingly clear that the dominant “one model fits all” paradigm is often ill-suited to address the diversity of real-world management situations that exist across the broad spectrum of coupled human-natural systems. This article introduces an integrated ES modeling methodology, named ARIES (ARtificial Intelligence for Ecosystem Services), which aims to introduce improvements on these fronts. To improve conceptual detail and representation of ES dynamics, it adopts a uniform conceptualization of ES that gives equal emphasis to their production, flow and use by society, while keeping model complexity low enough to enable rapid and inexpensive assessment in many contexts and for multiple services. To improve fit to diverse application contexts, the methodology is assisted by model integration technologies that allow assembly of customized models from a growing model base. By using computer learning and reasoning, model structure may be specialized for each application context without requiring costly expertise. In this article we discuss the founding principles of ARIES - both its innovative aspects for ES science and as an example of a new strategy to support more accurate decision making in diverse application contexts.","",""
533,"R. Akerkar, P. Sajja","Knowledge Based Systems",2009,"","","","",183,"2022-07-13 09:36:46","","10.1007/978-3-319-17885-1_100645","","",,,,,533,41.00,267,2,13,"","",""
8,"R. Seidlová, J. Poživil, Jaromír Seidl","Marketing and business intelligence with help of ant colony algorithm",2019,"","","","",184,"2022-07-13 09:36:46","","10.1080/0965254X.2018.1430058","","",,,,,8,2.67,3,3,3,"Abstract Recently, there is increasing need of banks for targeting and acquiring new customers, for fraud detection in real time and for segmentation products through analysis of the customers. Doing it, they can serve their customers better, and can increase the effectiveness of the company. For this purpose, various data mining methods are used which enable extraction of interesting, nontrivial, implicit, previously unknown, and potentially useful patterns or knowledge from huge amounts of data. Traditional data mining methods include classification rule tasks, for their solution there are a number of methods. Among them can be mentioned, for example, Random forest algorithm or C4.5 algorithm. However, accuracy of these methods significantly reduces in the event that some data in databases is missing. These methods are always not optimal for very large databases. The aim of our work is to verify a possible solution of these problems by using the algorithm based on artificial ant colonies. This algorithm was successful in other areas. Therefore, we tested its applicability and accuracy in marketing and business intelligence and compared it with so far used methods. The experimental results showed that the presented algorithm is very effective, robust, and suitable for processing of very large files. It was also found that this algorithm overcomes the previously used algorithms in accuracy. Algorithm is easily implementable on different platforms and can be recommended for using in banking and business intelligence.","",""
199,"R. Luckin, Wayne Holmes","Intelligence Unleashed: An argument for AI in Education",2016,"","","","",185,"2022-07-13 09:36:46","","","","",,,,,199,33.17,100,2,6,"This paper on artificial intelligence in education (AIEd) has two aims. The first: to explain to a non-specialist, interested, reader what AIEd is: its goals, how it is built, and how it works. The second: to set out the argument for what AIEd can offer teaching and learning, both now and in the future, with an eye towards improving learning and life outcomes for all. Computer systems that are artificially intelligent interact with the world using capabilities (such as speech recognition) and intelligent behaviours (such as using available information to take the most sensible actions toward a stated goal) that we would think of as essentially human. At the heart of artificial intelligence in education is the scientific goal to make knowledge, which is often left implicit, computationally precise and explicit. In other words, in addition to being the engine behind much ‘smart’ ed tech, AIEd is also designed to be a powerful tool to open up what is sometimes called the ‘black box of learning,’ giving us more fine-grained understandings of how learning actually happens. Although some might find the concept of AIEd alienating, the algorithms and models that underpin ed tech powered by AIEd form the basis of an essentially human endeavor. Using AIEd, teachers will be able to offer learners educational experiences that are more personalised, flexible, inclusive and engaging. Crucially, we do not see a future in which AIEd replaces teachers. What we do see is a future in which the extraordinary expertise of teachers is better leveraged and augmented through the thoughtful deployment of well designed AIEd. We have available, right now, AIEd tools that could support student learning at a scale previously unimaginable by providing one-on-one tutoring to every student, in every subject. Existing technologies also have the capacity to provide intelligent support to learners working in a group, and to create authentic virtual learning environments where students have the right support, at the right time, to tackle real-life problems and puzzles. In the near future, we expect that teaching and learning will increasingly be supported by the thoughtful application of AIEd tools. For example, by lifelong learning companions powered by AI that can accompany and support individual learners throughout their studies - in and beyond school - and new forms of assessment that measure learning while it is taking place, shaping the learning experience in real time. If we are ultimately successful, we predict that AIEd will help us address some of the most intractable problems in education, including achievement gaps and teacher retention. AIEd will also help us respond to the most significant social challenge that AI has already brought - the steady replacement of jobs and occupations with clever algorithms and robots. It is our view that this provides a new innovation imperative in education, which can be expressed simply: as humans live and work alongside increasingly smart machines, our education systems will need to achieve at levels that none have managed to date. True progress will require the development of an AIEd infrastructure. This will not, however, be a single monolithic AIEd system. Instead, it will resemble the marketplace that has developed for smartphone apps: hundreds and then thousands of individual AIEd components, developed in collaboration with educators, conformed to uniform international data standards, and shared with researchers and developers worldwide. These standards will also enable system-level data collation and analysis that will help us to learn much more about learning itself – and how to improve it. Moving forward, we will need to pay close attention to three powerful forces as we map the future of artificial intelligence in education, namely pedagogy, technology, and system change. Paying attention to the pedagogy will mean that the design of new edtech should always start with what we know about learning. It also means that the system for funding this work must be simultaneously opened up and refocused, moving away from isolated pockets of R&D and toward collaborative enterprises that prioritise areas known to make a real difference to teaching and learning. Paying attention to the technology will mean creating smarter demand for commercial grade AIEd products that work. It also means the development of a robust, component-based AIEd infrastructure, similar to the smartphone app marketplace, where researchers and developers can access standardised components that have been developed in collaboration with educators. Paying attention to system change will mean involving teachers, students, and parents in co-designing new tools, so that AIEd will appropriately address the inherent “messiness” of real classroom, university, and workplace learning environments. It also means the development of data standards that promote the safe and ethical use of data. Said succinctly, we need intelligent technologies that embody what we know about great teaching and learning, embodied in enticing consumer grade products, which are then used effectively in real-life settings that combine the best of human and machine. We do not underestimate the new-thinking, inevitable wrong-turns, and effort required to realise these recommendations. However, if we are to properly unleash the intelligence of AIEd, we must do things differently - via new collaborations, sensible funding, and (always) a keen eye on the pedagogy. The potential prize is too great to act otherwise.","",""
0,"Khalid Rabeyee","Computing intelligence technique and multiresolution data processing for condition monitoring",2019,"","","","",186,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,1,3,"Condition monitoring (CM) of rotary machines has gained increasing importance and extensive research in recent years. Due to the rapid growth of data volume, automated data processing is necessary in order to deal with massive data efficiently to produce timely and accurate diagnostic results. Artificial intelligence (AI) and adaptive data processing approaches can be promising solutions to the challenge of large data volume. Unfortunately, the majority of AI-based techniques in CM have been developed for only the post-processing (classification) stage, whereas the critical tasks including feature extraction and selection are still manually processed, which often require considerable time and efforts but also yield a performance depending on prior knowledge and diagnostic expertise.    To achieve an automatic data processing, the research of this PhD project provides an integrated framework with two main approaches. Firstly, it focuses on extending AI techniques in all phases, including feature extraction by applying Componential Coding Neural Network (CCNN) which has been found to have unique properties of being trained through unsupervised learning, capable of dealing with raw datasets, translation invariance and high computational efficiency. These advantages of CCNN make it particularly suitable for automated analyzing of the vibration data arisen from typical machine components such as the rolling element bearings which exhibit periodic phenomena with high non-stationary and strong noise contamination. Then, once an anomaly is detected, a further analysis technique to identify the fault is proposed using a multiresolution data analysis approach based on Double-Density Discrete Wavelet Transform (DD-DWT) which was grounded on over-sampled filter banks with smooth tight frames. This makes it nearly shift-invariant which is important for extracting non-stationary periodical peaks. Also, in order to denoise and enhance the diagnostic features, a novel level-dependant adaptive thresholding method based on harmonic to signal ratio (HSR) is developed and implemented on the selected wavelet coefficients. This method has been developed to be a semi-automated (adaptive) approach to facilitate the process of fault diagnosis. The developed framework has been evaluated using both simulated and measured datasets from typical healthy and defective tapered roller bearings which are critical parts of all rotating machines. The results have demonstrated that the CCNN is a robust technique for early fault detection, and also showed that adaptive DD-DWT is a robust technique for diagnosing the faults induced to test bearings. The developed framework has achieved multi-objectives of high detection sensitivity, reliable diagnosis and minimized computing complexity.","",""
2,"M. Ghadi, L. Laouamer, Laurent Nana, A. Pascu","Rough Set Theory Based on Robust Image Watermarking",2018,"","","","",187,"2022-07-13 09:36:46","","10.1007/978-3-319-63754-9_28","","",,,,,2,0.50,1,4,4,"","",""
0,"C. Ezeofor, Onengiye M. Georgewill","Development of Knowledge Based Smart Home",2019,"","","","",188,"2022-07-13 09:36:46","","","","",,,,,0,0.00,0,2,3,"this paper presents the development of Knowledge Based Smart Home. Smart home came to existence the very moment Internet of Things (IoT) Technology was invented. Internet of Things (IoT) has been experimentally proven to work satisfactorily by connecting simple appliances to it and were successfully controlled remotely through the internet.This invention has led to automation of homes, offices, industries, robotics, artificial intelligence etc. and today, more robust systems are being developed. Two basic ways of controlling and monitoring home remotely had been implemented from research. The first is by using GSM phone to control home appliances via sms commands and the second is through web application platform via networked and internet based computers. This work covers both ways and integrated voice recognition as another way of controlling home appliances. In order to accommodate sms based, web based and voice based, knowledge based system is implemented.This system integrates various communication techonologies such as Bluetooth BLE, Wi-Fi, GSM and Voice Recognition for easy communication with smart devices like Android smartphones, tablets, PDA etc. and personal computers. The system is made up of security system, control system and communication system which houses various sensors, actuators, microcontrollers such as raspberry pi 3, ESp 32, STM32, ESP8266-01, LCDs, etc. The graphical user interface (GUI) for mobile and computer web applications is designed using Eclipse and brackets IDEs. Python, java, C++ languages are used to write control codes for both the GUI and the embedded system chips (microcontrollers).The system can also function as an intelligent personal assistant (IPA), thus answering query and performing actions via voice commands using natural language user interface. This is to assist people like elderly, sick and disabled with basic tasks to makes household decisions through stored information in the knowledge base of the system. The system is implemented with the aid of Artificial Intelligence for the effective and efficient management of the home. The complete system was tested successfully.","",""
11,"L. Lai, Chao-Chin Wu, Nien-Lin Hsueh, Liang-Tsung Huang, Shiow-Fen Hwang","An Artificial Intelligence Approach to Course Timetabling",2006,"","","","",189,"2022-07-13 09:36:46","","10.1142/S0218213008003868","","",,,,,11,0.69,2,5,16,"Course timetabling is a complex problem and cannot be dealt with using only a few general principles. Each actor (i.e. the administrator, the chairman, the instructor and the student) has his own objective, and these objectives are usually conflicting. The complicated relationships between time periods, classes, classrooms, and instructors make it difficult to attain a feasible solution. In this article, we propose an artificial intelligence approach that integrates expert systems and constraint programming to implement a course timetabling system. Expert systems are utilized to incorporate knowledge into the timetabling system and to provide the reasoning capability for knowledge deduction. The separation of the knowledge base, facts and the inference engine in expert systems provides greater flexibility to support changes. The constraint hierarchy is utilized to capture hard and soft constraints and to reason about constraints using constraint satisfaction and relaxation techniques. Moreover, object-oriented software engineering is applied to improve the development and maintenance of the course timetabling system. A course timetabling system in the Department of Computer Science and Information Engineering at National Changhua University of Education (NCUE) is used as an illustrate example for the proposed approach","",""
454,"P. Harmon, D. King","Expert systems: artificial intelligence in business",1985,"","","","",190,"2022-07-13 09:36:46","","10.2307/3105218","","",,,,,454,12.27,227,2,37,"Case Study: MYCIN: Varieties of Problem Solving Strategies The Anatomy of a Knowledge Base Anatomy of An Inference Engine MYCIN Reconsidered Languages and Tools for Knowledge Systems A Sampler of Knowledge Systems and Their Architectures How Knowledge Systems are Developed Near Futures: Knowledge Engineering in the Next Five Years Large Scale Knowledge Systems Near Futures: Intelligent Job Aids Not So Near Futures: Research Topics Likely to Bear Fruit in 5 Years or More Not So Near Futures: Intelligent Tutoring Systems Not So Near Futures: Planning and Preparing for the Knowledge Systems Revolution Appendixes","",""
624,"Wm To, T. Tryfonas, D. Farthing","Frontiers in Artificial Intelligence and Applications",2009,"","","","",191,"2022-07-13 09:36:46","","","","",,,,,624,48.00,208,3,13,"We trace the roots of ontology-drive information systems (ODIS) back to early work in artificial intelligence and software engineering. We examine the lofty goals of the Knowledge-Based Software Assistant project from the 80s, and pose some questions. Why didn't it work? What do we have today instead? What is on the horizon? We examine two critical ideas in software engineering: raising the level of abstraction, and the use of formal methods. We examine several other key technologies and show how they paved the way for today's ODIS. We identify two companies with surprising capabilities that are on the bleeding edge of today's ODIS, and are pointing the way to a bright future. In that future, application development will be opened up to the masses, who will require no computer science background. People will create models in visual environments and the models will be the applications, self-documenting and executing as they are being built. Neither humans nor computers will be writing application code. Most functionality will be created by reusing and combining pre-coded functionality. All application software will be ontology-driven.","",""
322,"J. Korbicz, J. M. Kóscielny, Z. Kowalczuk, W. Cholewa, Jozef Karbicz","Fault Diagnosis: Models, Artificial Intelligence, Applications",2004,"","","","",192,"2022-07-13 09:36:46","","","","",,,,,322,17.89,64,5,18,"1. Introduction.- 2. Models in the diagnostics of processes.- 3. Process diagnostics methodology.- 4. Methods of signal analysis.- 5. Control theory methods in designing diagnostic systems.- 6. Optimal detection observers based on eigenstructure assignment.- 7. Robust H?-optimal synthesis of FDI systems.- 8. Evolutionary methods in designing diagnostic systems.- 9. Artificial neural networks in fault diagnosis.- 10. Parametric and neural network Wiener and Hammerstein models in fault detection and isolation.- 11. Application of fuzzy logic to diagnostics.- 12. Observers and genetic programming in the identification and fault diagnosis of non-linear dynamic systems.- 13. Genetic algorithms in the multi-objective optimisation of fault detection observers.- 14. Pattern recognition approach to fault diagnostics.- 15. Expert systems in technical diagnostics.- 16. Selected methods of knowledge engineering in systems diagnosis.- 17. Methods of acqusition of diagnostic knowledge.- 18. State monitoring algorithms for complex dynamic systems.- 19. Diagnostics of industrial processes in decentralised structures.- 20. Detection and isolation of manoeuvres in adaptive tracking filtering based on multiple model switching.- 21. Detecting and locating leaks in transmission pipelines.- 22. Models in the diagnostics of processes.- 23. Diagnostic systems.","",""
247,"J. Minker","Logic-Based Artificial Intelligence",2000,"","","","",193,"2022-07-13 09:36:46","","10.1007/978-1-4615-1567-8","","",,,,,247,11.23,247,1,22,"","",""
228,"G. Brewka","Artificial intelligence - a modern approach by Stuart Russell and Peter Norvig, Prentice Hall. Series in Artificial Intelligence, Englewood Cliffs, NJ",1996,"","","","",194,"2022-07-13 09:36:46","","10.1017/S0269888900007724","","",,,,,228,8.77,228,1,26,"Benferhat, S, Dubois D and Prade, H, 1992. ""Representing default rules in possibilistic logic"" In: Proc. of the 3rd Inter. Conf. on Principles of knowledge Representation and Reasoning (KR'92), 673-684, Cambridge, MA, October 26-29. De Finetti, B, 1936. ""La logique de la probabilite"" Actes du Congres Inter, de Philosophic Scientifique, Paris. (Hermann et Cie Editions, 1936, IV1-IV9). Driankov, D, Hellendoorn, H and Reinfrank, M, 1995. An Introduction to Fuzzy Control, Springer-Verlag. Dubois, D and Prade, H, 1988. ""An introduction to possibilistic and fuzzy logics"" In: Non-Standard Logics for Automated Reasoning (P Smets, A Mamdani, D Dubois and H Prade, editors), 287-315, Academic Press. Dubois, D and Prade, H, 1994. ""Can we enforce full compositionality in uncertainty calculi?"" In: Proc. 12th US National Conf. On Artificial Intelligence (AAAI94), 149-154, Seattle, WA. Elkan, C, 1994. ""The paradoxical success of fuzzy logic"" IEEE Expert August, 3-8. Lehmann, D and Magidor. M, 1992. ""What does a conditional knowledge base entail?"" Artificial Intelligence 55 (1) 1-60. Maung, 1,1995. ""Two characterizations of a minimum-information principle in possibilistic reasoning"" Int. J. of Approximate Reasoning 12 133-156. Pearl, J, 1990. ""System Z: A natural ordering of defaults with tractable applications to default reasoning"" Proc. of the 2nd Conf. on Theoretical Aspects of Reasoning about Knowledge (TARK'90) 121-135, San Francisco, CA, Morgan Karfman. Shoham, Y, 1988. Reasoning about Change MIT Press. Smets, P, 1988. ""Belief functions"" In: Non-Standard Logics for Automated Reasoning (P Smets, A Mamdani, D Dubois and H Prade, editors), 253-286, Academic Press. Smets, P, 1990a. ""The combination of evidence in the transferable belief model"" IEEE Trans, on Pattern Anal. Mach. Intell. 12 447-458. Smets, P, 1990b. ""Constructing the pignistic probability function in a context of uncertainty"" Un certainty in Artificial Intelligence 5 (M Henrion et al., editors), 29-40, North-Holland. Smets, P, 1995. ""Quantifying beliefs by belief functions: An axiomatic justification"" In: Procoj the 13th Inter. Joint Conf. on Artificial Intelligence (IJACT93), 598-603, Chambey, France, August 28-September 3. Smets, P and Kennes, R, 1994. ""The transferable belief model"" Artificial Intelligence 66 191-234.","",""
0,"Qinyun Liu","Solution Generation through Hybrid Intelligence and Creativity based on Investment Portfolio",2018,"","","","",195,"2022-07-13 09:36:46","","10.23940/IJPE.18.07.P29.16411650","","",,,,,0,0.00,0,1,4,"Artificial Intelligence (AI) has been developed to be robust on computing. Learning can be achieved by connecting to heterogeneous data using AI algorithms, such as the Artificial Neural Network. Knowledge can be learned, and rules in the database can be discovered by machines through heuristic algorithms. However, creativity has not been achieved by computers like the human brain by using AI algorithms individually. This research serves to explore a method to achieve creative solution generation by utilizing a relationship between intelligence and creativity, assuming intelligence is the subset of creativity. Under this relationship, the computing can be fulfilled using AI algorithms. The theories of achieving creativity is the guidance of this method.","",""
110,"C. Kulikowski","Artificial intelligence methods and systems for medical consultation",1980,"","","","",196,"2022-07-13 09:36:46","","10.1109/TPAMI.1980.6592368","","",,,,,110,2.62,110,1,42,"The major AI problems that arise in designing a consultation program involve choices of knowledge representations, diagnostic interpretation strategies, and treatment planning strategies. The need to justify decisions and update the knowledge base in the light of new research findings places a premium on the modularity of a representation and the ease with which its reasoning procedures can be explained. In both diagnosis and treatment decisions, the relative advantages and disadvantages of different schemes for quantifying the uncertainty of inferences raises difficult issues of a formal logical nature, as well as many specific practical problems of system design. An important insight that has resulted from the design of several artificial intelligence systems is that robustness of performance in the presence of many uncertainty relationships can be achieved by eliciting from the expert a segmentation of knowledge that will also provide a rich network of deterministic relationships to interweave the space of hypotheses.","",""
342,"Anthony Kulis","Bio-Inspired Artificial Intelligence: Theories, Methods, and Technologies",2009,"","","","",197,"2022-07-13 09:36:46","","10.12694/SCPE.V10I4.623","","",,,,,342,26.31,342,1,13,"Bio-Inspired Artificial Intelligence: Theories, Methods, and Technologies by D. Floreano and C. Mattiussi This is a book that bridges biological systems and computer science. For digital-based researchers, having this book which details the biological components of natural life and seamlessly integrates that knowledge into our digital realm is an essential asset. Each chapter is systematically introduces the reader to a biological system while easing them into the its computational counterpart. There are seven chapters covering evolution, cellular, neural, developmental, immune, behavioral, and collective systems. Chapter 1 introduces the fundamental concept of computational evolution as related to biological systems. This chapter starts with the basic concepts of evolutionary theory and progresses, covering everything from fitness functions to analog circuits. The following chapter presents the next logical step upwards in biology, cellular structures and systems. Again introducing the basics of life and progressing towards cellular automata.  Chapter 3 covers Neural Networks by introducing the Biological Nervous System, then the Artificial Neural Network. The core concepts to Neural Networks are detailed in a systematic and common-sense manner, introducing unsupervised learning, supervised learning, and reinforce learning, then progressing onto neural hardware and hybrid systems. In Chapter 4, the authors detail developmental systems, explaining how nature utilizes the cellular structures to how engineers can mimic nature. This theme of progression from biological introduction to digital computation is reproduced as a single voice through out each chapter. The fundamentals of Bio-Inspired Artificial Intelligence are well demonstrated, allowing for a novice researcher in this area to develop the necessary skills and have a firm grasp on this topic. Once the reader has a solid grasp of the building blocks of life, the authors present chapters related to larger systems. Of particular interest to my research is the chapter on Immune Systems. This chapter provides a fundamental understanding of the Human Immune System, detailing the finer points of immunological cellular structures, while introducing a slightly more than generalized immune response concept. After a lengthy introduction of human immunology, we are introduced to the core of Artificial Immune Systems, the Negative Selection Algorithm and Clonal Selection Algorithm. Each one of these algorithms is covered enough so that the reader is capable of understanding each respective algorithms strengths and limitations. For new researchers to Artificial Immune Systems, days of reading journal articles is summarized in these sections, allowing for intelligent and efficient decision making in choosing your next step of research. Chapter 6 and 7 provides the audience with behavior systems and collective systems, respectively. The behavioral systems covered in this book relate to aspects of AI, robots, and some machine learning. Once behavior is understood, collective and cooperative systems are covered. Optimization techniques of particle swarms, ant colonies, and topics derived for robotics are detailed and well explained. While this is not a textbook, is does cover the fundamental concepts required to research Bio-Inspired Artificial Intelligence. For myself, the quality of this book can simply be noted by the publishers, MIT Press. Many of the best books I have encountered in my studies have been published by MIT, and here is another. Floreano and Mattiussi have not let me down in their quality, albeit I do have some complaints. First, while the topics cover a solid breadth, the depth on detailing the computation side is limited. I would like to have seen either more depth in each chapter or a broader look at each chapters algorithms, but the book falls somewhere in the middle. My current research involves Danger Signals and their relationship to preventing Epidemic Attacks, so I would have like to seen more detail about Polly Matzinger's Danger Theory rather than one short paragraph saying that it is not universally accepted. While Immunologists may debate Danger Theory, novel algorithms have been developed off of the concept of Danger Theory and deserve a place in this book. Yet to counter my own argument, the authors do finish off each chapter with a Suggested Readings section outlining a series of excellent supplement papers to the chapters topics that would eventually lead the reader to these novel topics. Overall, if you are interested in this field, buy this book. You can find it online at MIT Press for a discounted price. This book will make an excellent addition to any computer researchers library. Anthony Kulis, Department of Computer Science, Southern Illinois University","",""
0,"S. Ulyanov","Intelligent robust control of redun-dant smart robotic arm Pt II: Quantum computing KB optimizer",2020,"","","","",198,"2022-07-13 09:36:46","","10.30564/AIA.V2I2.1395","","",,,,,0,0.00,0,1,2,"In the first part of the article, two ways of fuzzy controller’s implementation showed. First way applied one controller for all links of the manipulator and showed the best performance. However, such an implementation is not possible in complex control objects, such as a planar redundant manipulator with seven degrees of freedom (DoF). The second way use of separated control when an independent fuzzy controller controls each link. The decomposition control due to a slight decrease in the quality of management has greatly simplified the processes of creating and placing knowledge bases. In this paper (Part II), the advantages and limitations of intelligent control systems based on soft computing technology described. To eliminate the mismatch of the work of separate independent fuzzy controllers, methods for self-organizing coordination control based on quantum computing technologies to create and design robust intelligent control systems for robotic manipulators with 3DOF and 7DOF described. Quantum fuzzy inference as quantum self-organization algorithm of imperfect KBs introduced. Quantum computational intelligence smart toolkit QCOptKBTMbased on quantum fuzzy inference applied. QCOptKBTM toolkit include quantum deep machine learning in on line. Successful engineering application of end-to-end quantum computing information technologies (as quantum sophisticated algorithms and quantum programming) in searching of solutions of algorithmic unsolved problems in classical dynamic intelligent control systems, artificial intelligence (AI) and intelligent cognitive robotics discussed. Quantum computing supremacy in efficient solution of intractable classical tasks as global robustness of redundant robotic manipulator in unpredicted control situations demonstrated. As result, the new synergetic self-organization information effect of robust KB design from responses of imperfect KBs (partial KB robustness cretead on toolkit SCOptKBTM in Pat I) fined.","",""
21,"Donald J. Gemaehlich","An overview of artificial intelligence",1984,"","","","",199,"2022-07-13 09:36:46","","","","",,,,,21,0.55,21,1,38,"The face of science and engineering has been changing with the recent growth in computer architecture. This growth is so important and robust that it is dramatically reshaping relationships among people and organizations and providing a foundation for understanding and learning of intelligent behavior in living and engineered systems. Is this growth beneficial to our society, these are such questions of the general public which are due to the lack of education concerning rapidly advancing technologies. This paper attempts to present an overview of Artificial Intelligence (AI). A generally accepted theory that ―machine will do and think like humans more in the future‖ is the concept behind AI. Brief literature of different aspects by which AI is achieved like expert system, knowledge based systems (knowledge engineering), neural networks, fuzzy logic, Neuro-fuzzy logic and fuzzy expert system, is included in order to have a clear understanding of AI. Along with this the different applications of AI, has been included in this paper. It is concluded that extensive ongoing research in the field of AI gives an idea that in near future a day will come when human beings and machines will merge into cyborgs or cybernetic organisms that are more capable and powerful than either. This idea is called transhumanism. KeywordsArtificial Intelligence; Expert System; Neural Network; Fuzzy Logic; Neuro-fuzzy logic.","",""
18,"P. Bock","The Emergence of Artificial Intelligence: Learning to Learn",1985,"","","","",200,"2022-07-13 09:36:46","","10.1609/AIMAG.V6I3.498","","",,,,,18,0.49,18,1,37,"The classical approach to the acquisition of knowledge and reason in artificial intelligence is to program the facts and rules into the machine. Unfortunately, the amount of time required to program the equivalent of human intelligence is prohibitively large. An alternative approach allows an automaton to learn to solve problems through iterative trial-and-error interaction with its environment, much as humans do. To solve a problem posed by the environment, the automaton generates a sequence or collection of responses based on its experience. The environment evaluates the effectiveness of this collection, and reports its evaluation to the automaton. The automaton modifies its strategy accordingly, and then generates a new collection of responses. This process is repeated until the automaton converges to the correct collection of responses. The principles underlying this paradigm, known as collective learning systems theory, are explained and applied to a simple game, demonstrating robust learning and dynamic adaptivity.","",""
