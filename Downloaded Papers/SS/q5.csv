Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
109,"Shilin Qiu, Qihe Liu, Shijie Zhou, Chunjiang Wu","Review of Artificial Intelligence Adversarial Attack and Defense Technologies",2019,"","","","",1,"2022-07-13 09:19:22","","10.3390/APP9050909","","",,,,,109,36.33,27,4,3,"In recent years, artificial intelligence technologies have been widely used in computer vision, natural language processing, automatic driving, and other fields. However, artificial intelligence systems are vulnerable to adversarial attacks, which limit the applications of artificial intelligence (AI) technologies in key security fields. Therefore, improving the robustness of AI systems against adversarial attacks has played an increasingly important role in the further development of AI. This paper aims to comprehensively summarize the latest research progress on adversarial attack and defense technologies in deep learning. According to the target model’s different stages where the adversarial attack occurred, this paper expounds the adversarial attack methods in the training stage and testing stage respectively. Then, we sort out the applications of adversarial attack technologies in computer vision, natural language processing, cyberspace security, and the physical world. Finally, we describe the existing adversarial defense methods respectively in three main categories, i.e., modifying data, modifying models and using auxiliary tools.","",""
14,"Gaolei Li, K. Ota, M. Dong, Jun Wu, Jianhua Li","DeSVig: Decentralized Swift Vigilance Against Adversarial Attacks in Industrial Artificial Intelligence Systems",2020,"","","","",2,"2022-07-13 09:19:22","","10.1109/TII.2019.2951766","","",,,,,14,7.00,3,5,2,"Individually reinforcing the robustness of a single deep learning model only gives limited security guarantees especially when facing adversarial examples. In this article, we propose DeSVig, a decentralized swift vigilance framework to identify adversarial attacks in an industrial artificial intelligence systems (IAISs), which enables IAISs to correct the mistake in a few seconds. The DeSVig is highly decentralized, which improves the effectiveness of recognizing abnormal inputs. We try to overcome the challenges on ultralow latency caused by dynamics in industries using peculiarly designated mobile edge computing and generative adversarial networks. The most important advantage of our work is that it can significantly reduce the failure risks of being deceived by adversarial examples, which is critical for safety-prioritized and delay-sensitive environments. In our experiments, adversarial examples of industrial electronic components are generated by several classical attacking models. Experimental results demonstrate that the DeSVig is more robust, efficient, and scalable than some state-of-art defenses.","",""
5,"Shen Wang, Zhuobiao Qiao","Robust Pervasive Detection for Adversarial Samples of Artificial Intelligence in IoT Environments",2019,"","","","",3,"2022-07-13 09:19:22","","10.1109/ACCESS.2019.2919695","","",,,,,5,1.67,3,2,3,"Nowadays, artificial intelligence technologies (e.g., deep neural networks) have been used widely in the Internet of Things (IoT) to provide smart services and sensing data processing. The evolving neural network even exceeds the human cognitive level. However, the accuracy of these structures depends to some extent on the accuracy of the training data. Some well-designed generated antagonistic disturbances are sufficient to deceive model when added to images. Such attacks cause the classifiers trained by the neural network to misidentify the object and thus completely fail. On the other hand, the various existing defensive methods that have been proposed suffer from two criticisms. The first thing that bears the brunt is unsatisfactory detection rate due to low robustness toward the adversarial sample. Second, the excessive dependence on the output of specific network structure layers hinders the emergence of universal schemes. In this paper, we propose the large margin cosine estimation (LMCE) detection scheme to overcome the above shortcomings, making the detection independent and universal. We illustrate the principle of our approach and demonstrate the significance and analysis of some important parameters. Moreover, we model various types of adversarial attacks and establish proposed defense mechanisms against them and evaluate our approach from different aspects. This method has been clearly validated on a range of standard datasets including MNIST, CIFAR-10, and SVHN. The assessment strongly reflects the robustness and pervasive of this approach in the face of various white and semi-white box attacks.","",""
0,"Yajie Wang, Yu-an Tan, T. Baker, Neeraj Kumar, Quanxin Zhang","Deep Fusion: Crafting Transferable Adversarial Examples and Improving Robustness of Industrial Artificial Intelligence of Things",2022,"","","","",4,"2022-07-13 09:19:22","","10.1109/tii.2022.3168874","","",,,,,0,0.00,0,5,1,"","",""
2,"Charles Rogers, John Bugg, C. Nyheim, Will Gebhardt, Brian Andris, Evan Heitman, C. Fleming","Adversarial Artificial Intelligence for Overhead Imagery Classification Models",2019,"","","","",5,"2022-07-13 09:19:22","","10.1109/SIEDS.2019.8735608","","",,,,,2,0.67,0,7,3,"In overhead object detection, computers are increasingly replacing humans at spotting and identifying specific items within images through the use of machine learning (ML). These ML programs must be both accurate and robust. Accuracy means the results must be trusted enough to substitute for the manual deduction process. Robustness is the magnitude to which the network can handle discrepancies within the images. One way to gauge the robustness is through the use of adversarial networks. Adversarial algorithms are trained using perturbations of the image to reduce the accuracy of an existing classification model. The greater degree of perturbations a model can withstand, the more robust it is. In this paper, comparisons of existing deep neural network models and the advancement of adversarial AI are explored. While there is some published research about AI and adversarial networks, very little discusses this particular utilization for overhead imagery. This paper focuses on overhead imagery, specifically that of ships. Using a public Kaggle dataset, we developed multiple models to detect ships in overhead imagery, specifically ResNet50, DenseNet201, and InceptionV3. The goal of the adversarial works is to manipulate an image so that its contents are misclassified. This paper focuses specifically on producing perturbations that can be recreated in the physical world. This serves to account for physical conditions, whether intentional or not, that could reduce accuracy within our network. While there are military applications for this specific research, the general findings can be applied to all AI overhead image classification topics. This work will explore both the vulnerabilities of existing classifier neural net models and the visualization of these vulnerabilities.","",""
0,"D. Lange","Robustness of artificial intelligence in the face of novelty",2022,"","","","",6,"2022-07-13 09:19:22","","10.1117/12.2622912","","",,,,,0,0.00,0,1,1,"A critical factor in utilizing agents with Artificial Intelligence (AI) is their robustness to novelty. AI agents include models that are either engineered or trained. Engineered models include knowledge of those aspects of the environment that are known and considered important by the engineers. Learned models form embeddings of aspects of the environment based on connections made through the training data. In operation, however, a rich environment is likely to present challenges not seen in training sets or accounted for in engineered models. Worse still, adversarial environments are subject to change by opponents. A program at the Defense Advanced Research Project Agency (DARPA) seeks to develop the science necessary to develop and evaluate agents that are robust to novelty. This capability will be required, before AI has the role envisioned within mission critical environments.","",""
6,"Dou Goodman, Xingjian Li, Jun Huan, Tao Wei","Improving Adversarial Robustness via Attention and Adversarial Logit Pairing",2019,"","","","",7,"2022-07-13 09:19:22","","10.3389/frai.2021.752831","","",,,,,6,2.00,2,4,3,"Though deep neural networks have achieved the state of the art performance in visual classification, recent studies have shown that they are all vulnerable to the attack of adversarial examples. In this paper, we develop improved techniques for defending against adversarial examples. First, we propose an enhanced defense technique denoted Attention and Adversarial Logit Pairing (AT + ALP), which encourages both attention map and logit for the pairs of examples to be similar. When being applied to clean examples and their adversarial counterparts, AT + ALP improves accuracy on adversarial examples over adversarial training. We show that AT + ALP can effectively increase the average activations of adversarial examples in the key area and demonstrate that it focuses on discriminate features to improve the robustness of the model. Finally, we conduct extensive experiments using a wide range of datasets and the experiment results show that our AT + ALP achieves the state of the art defense performance. For example, on 17 Flower Category Database, under strong 200-iteration Projected Gradient Descent (PGD) gray-box and black-box attacks where prior art has 34 and 39% accuracy, our method achieves 50 and 51%. Compared with previous work, our work is evaluated under highly challenging PGD attack: the maximum perturbation ϵ ∈ {0.25, 0.5} i.e. L ∞ ∈ {0.25, 0.5} with 10–200 attack iterations. To the best of our knowledge, such a strong attack has not been previously explored on a wide range of datasets.","",""
0,"Adaku Uchendu, Daniel Campoy, Christopher Menart, Alexandra Hildenbrandt","Robustness of Bayesian Neural Networks to White-Box Adversarial Attacks",2021,"","","","",8,"2022-07-13 09:19:22","","10.1109/AIKE52691.2021.00017","","",,,,,0,0.00,0,4,1,"Bayesian Neural Networks (BNNs), unlike Traditional Neural Networks (TNNs) are robust and adept at handling adversarial attacks by incorporating randomness. This randomness improves the estimation of uncertainty, a feature lacking in TNNs. Thus, we investigate the robustness of BNNs to white-box attacks using multiple Bayesian neural architectures. Furthermore, we create our BNN model, called BNN-DenseNet, by fusing Bayesian inference (i.e., variational Bayes) to the DenseNet architecture, and BDAV, by combining this intervention with adversarial training. Experiments are conducted on the CIFAR-10 and FGVC-Aircraft datasets. We attack our models with strong white-box attacks (l∞-FGSM, l∞-PGD, l2-PGD, EOT l∞-FGSM, and EOT l∞-PGD). In all experiments, at least one BNN outperforms traditional neural networks during adversarial attack scenarios. An adversarially-trained BNN outperforms its non-Bayesian, adversarially-trained counterpart in most experiments, and often by significant margins. These experimental results suggest that the dense nature of DenseNet provides robustness advantages that are further amplified by fusing Bayesian Inference with the architecture. Lastly, we investigate network calibration and find that BNNs do not make overconfident predictions, providing evidence that BNNs are also better at measuring uncertainty.","",""
1,"Wenzhao Xiang, Hang Su, Chang Liu, Yandong Guo, Shibao Zheng","Improving Robustness of Adversarial Attacks Using an Affine-Invariant Gradient Estimator",2021,"","","","",9,"2022-07-13 09:19:22","","","","",,,,,1,1.00,0,5,1,"As designers of artiﬁcial intelligence try to outwit hack-ers, both sides continue to hone in on AI’s inherent vulnera-bilities. Designed and trained from certain statistical distributions of data, AI’s deep neural networks (DNNs) remain vulnerable to deceptive inputs that violate a DNN’s statistical, predictive assumptions. Before being fed into a neural network, however, most existing adversarial examples cannot maintain malicious functionality when applied to an afﬁne transformation. For practical purposes, maintaining that malicious functionality serves as an important measure of the robustness of adversarial attacks. To help DNNs learn to defend themselves more thoroughly against attacks, we propose an afﬁne-invariant adversarial attack, which can consistently produce more robust adversarial examples over afﬁne transformations. For efﬁciency, we propose to dis-entangle current afﬁne-transformation strategies from the Euclidean geometry coordinate plane with its geometric translations, rotations and dilations; we reformulate the lat-ter two in polar coordinates. Afterwards, we construct an afﬁne-invariant gradient estimator by convolving the gradient at the original image with derived kernels, which can be integrated with any gradient-based attack methods. Extensive experiments on ImageNet, including some experiments under physical condition, demonstrate that our method can signiﬁcantly improve the afﬁne invariance of adversarial examples and, as a byproduct, improve the transferability of adversarial examples, compared with alternative state-of-the-art methods. 1","",""
1,"Sandhya Aneja, Nagender Aneja, Pg Emeroylariffion Abas, A. G. Naim","IAES International Journal of Artificial Intelligence (IJ-AI)",2021,"","","","",10,"2022-07-13 09:19:22","","","","",,,,,1,1.00,0,4,1,"Received Aug 22, 2021 Revised May 20, 2022 Accepted Jun 6, 2022 Despite substantial advances in network architecture performance, the susceptibility of adversarial attacks makes deep learning challenging to implement in safety-critical applications. This paper proposes a data-centric approach to addressing this problem. A nonlocal denoising method with different luminance values has been used to generate adversarial examples from the Modified National Institute of Standards and Technology database (MNIST) and Canadian Institute for Advanced Research (CIFAR-10) data sets. Under perturbation, the method provided absolute accuracy improvements of up to 9.3% in the MNIST data set and 13% in the CIFAR10 data set. Training using transformed images with higher luminance values increases the robustness of the classifier. We have shown that transfer learning is disadvantageous for adversarial machine learning. The results indicate that simple adversarial examples can improve resilience and make deep learning easier to apply in various applications.","",""
0,"Mehmet Melih Arıcı, A. Sen","Improving Robustness of Deep Learning Systems with Fast and Customizable Adversarial Data Generation",2021,"","","","",11,"2022-07-13 09:19:22","","10.1109/AITEST52744.2021.00017","","",,,,,0,0.00,0,2,1,"Deep Learning (DL) is the force behind the success of solving many complicated tasks in recent years. With the use of DL systems in safety-critical applications, it has become of great importance to make these systems robust against adversarial attacks. Adversarial data generation is an effective tool to make DL systems robust against such attacks, with the help of adversarial training. Recent studies focus on gradient-based adversarial attacks. Although they can successfully generate adversarial samples, high computation cost and lack of flexibility over input generation arise the need for an efficient and flexible adversarial attack methodology. In this paper, we present DeepCustom, a fast and customizable adversarial data generation framework towards bridging this gap. Convolutional autoencoders with custom loss functions, enable user-configurable data generation within a much shorter time compared to the state-of-the-art attack method called PGD. Experiments show that our technique produces adversarial samples faster than PGD and using these samples in adversarial training, allows comparable robustness against adversarial attacks.","",""
0,"Igor Kunjavskij","Cybersecurity in Machine Learning and Artificial Intelligence for Self Driving Vehicles A proposal for an unified and open source test framework",2021,"","","","",12,"2022-07-13 09:19:22","","","","",,,,,0,0.00,0,1,1,"This report summarizes the key methodologies applicable for attacks on Convolutional Neural Networks deployed in self driving vehicles. Furthemore, the need for an open source framework specifically designed to test such networks for robustness against these attacks is demonstrated. This need stems from the fact that solutions available so far only cover very generic scenarios of adversarial attacks. Hence these tools are not suited to specifically secure Convolutional Neural Networks deployed in in self driving vehicles. All in all the development of such a framework would benefit all parties that are involved in the market of autonomously driving vehicles.","",""
0,"Sanghyeon An, Min Jun Lee, Jungmin So","Improving Robustness Against Adversarial Example Attacks Using Non-Parametric Models on MNIST",2020,"","","","",13,"2022-07-13 09:19:22","","10.1109/ICAIIC48513.2020.9065264","","",,,,,0,0.00,0,3,2,"Deep learning research has been actively conducted, and neural networks including CNN have outstanding performance in computer vision. However, parametric models such as neural networks are known to be vulnerable to adversarial example attacks, making them inappropriate to employ when security becomes significant. Hence, non-parametric models are considered but there is a problem of having low accuracy. To solve the issue, we proposed a scheme where images are segmented into patch units for non-parametric models. Experimental results display that the proposed scheme improves both accuracy as well as robustness against adversarial example attacks.","",""
4,"Huy Phan, Yi Xie, Siyu Liao, Jie Chen, Bo Yuan","CAG: A Real-time Low-cost Enhanced-robustness High-transferability Content-aware Adversarial Attack Generator",2019,"","","","",14,"2022-07-13 09:19:22","","10.1609/AAAI.V34I04.5990","","",,,,,4,1.33,1,5,3,"Deep neural networks (DNNs) are vulnerable to adversarial attack despite their tremendous success in many artificial intelligence fields. Adversarial attack is a method that causes the intended misclassfication by adding imperceptible perturbations to legitimate inputs. To date, researchers have developed numerous types of adversarial attack methods. However, from the perspective of practical deployment, these methods suffer from several drawbacks such as long attack generating time, high memory cost, insufficient robustness and low transferability. To address the drawbacks, we propose a Content-aware Adversarial Attack Generator (CAG) to achieve real-time, low-cost, enhanced-robustness and high-transferability adversarial attack. First, as a type of generative model-based attack, CAG shows significant speedup (at least 500 times) in generating adversarial examples compared to the state-of-the-art attacks such as PGD and C&W. Furthermore, CAG only needs a single generative model to perform targeted attack to any targeted class. Because CAG encodes the label information into a trainable embedding layer, it differs from prior generative model-based adversarial attacks that use n different copies of generative models for n different targeted classes. As a result, CAG significantly reduces the required memory cost for generating adversarial examples. Moreover, CAG can generate adversarial perturbations that focus on the critical areas of input by integrating the class activation maps information in the training process, and hence improve the robustness of CAG attack against the state-of-art adversarial defenses. In addition, CAG exhibits high transferability across different DNN classifier models in black-box attack scenario by introducing random dropout in the process of generating perturbations. Extensive experiments on different datasets and DNN models have verified the real-time, low-cost, enhanced-robustness, and high-transferability benefits of CAG.","",""
2,"Chih-Ling Chang, Jui-Lung Hung, Chin-Wei Tien, Chia-Wei Tien, S. Kuo","Evaluating Robustness of AI Models against Adversarial Attacks",2020,"","","","",15,"2022-07-13 09:19:22","","10.1145/3385003.3410920","","",,,,,2,1.00,0,5,2,"Recently developed adversarial attacks on neural networks have become more aggressive and dangerous, because of which Artificial Intelligence (AI) models are no longer sufficiently robust against them. It is important to have a set of effective and reliable methods to detect malicious attacks to ensure the security of AI models. Such standardized methods can also serve as a reference for researchers to develop robust models and new kinds of attacks. This study proposes a method to assess the robustness of AI models. Six commonly used image classification CNN models were evaluated when subjected to 13 types of adversarial attacks. The robustness of the models is calculated unbiased and can be used as a reference for further improvement. It is distinguished from prior related works that our algorithm is attack-agnostic and is applicable to neural network model.","",""
2,"Tyler J. Shipp, Daniel Clouse, Michael J. De Lucia, Metin B. Ahiskali, Kai Steverson, Jonathan Mullin, Nathaniel D. Bastian","Advancing the Research and Development of Assured Artificial Intelligence and Machine Learning Capabilities",2020,"","","","",16,"2022-07-13 09:19:22","","","","",,,,,2,1.00,0,7,2,"Artificial intelligence (AI) and machine learning (ML) have become increasingly vital in the development of novel defense and intelligence capabilities across all domains of warfare. An adversarial AI (A2I) and adversarial ML (AML) attack seeks to deceive and manipulate AI/ML models. It is imperative that AI/ML models can defend against these attacks. A2I/AML defenses will help provide the necessary assurance of these advanced capabilities that use AI/ML models. The A2I Working Group (A2IWG) seeks to advance the research and development of assured AI/ML capabilities via new A2I/AML defenses by fostering a collaborative environment across the U.S. Department of Defense and U.S. Intelligence Community. The A2IWG aims to identify specific challenges that it can help solve or address more directly, with initial focus on three topics: AI Trusted Robustness, AI System Security, and AI/ML Architecture Vulnerabilities.","",""
2,"Alexandre Dey, Marc Velay, Jean-Philippe Fauvelle, Sylvain Navers","Adversarial vs behavioural-based defensive AI with joint, continual and active learning: automated evaluation of robustness to deception, poisoning and concept drift",2020,"","","","",17,"2022-07-13 09:19:22","","","","",,,,,2,1.00,1,4,2,"Recent advancements in Artificial Intelligence (AI) have brought new capabilities to behavioural analysis (UEBA) for cyber-security consisting in the detection of hostile action based on the unusual nature of events observed on the Information this http URL our previous work (presented at C\&ESAR 2018 and FIC 2019), we have associated deep neural networks auto-encoders for anomaly detection and graph-based events correlation to address major limitations in UEBA systems. This resulted in reduced false positive and false negative rates, improved alert explainability, while maintaining real-time performances and scalability. However, we did not address the natural evolution of behaviours through time, also known as concept drift. To maintain effective detection capabilities, an anomaly-based detection system must be continually trained, which opens a door to an adversary that can conduct the so-called ""frog-boiling"" attack by progressively distilling unnoticed attack traces inside the behavioural models until the complete attack is considered normal. In this paper, we present a solution to effectively mitigate this attack by improving the detection process and efficiently leveraging human expertise. We also present preliminary work on adversarial AI conducting deception attack, which, in term, will be used to help assess and improve the defense system. These defensive and offensive AI implement joint, continual and active learning, in a step that is necessary in assessing, validating and certifying AI-based defensive solutions.","",""
1,"Asmaa Ftaimi, T. Mazri","Evaluation and Analysis of Robustness of Adversarial Examples Attacks in Deep Neural Networks",2020,"","","","",18,"2022-07-13 09:19:22","","10.1109/ISAECT50560.2020.9523659","","",,,,,1,0.50,1,2,2,"Neural networks have revolutionized the field of artificial intelligence. They have given rise to several applications in various areas. However, it has been shown that they have flaws that could be leveraged by an attacker to perform an adversarial examples attack. Several studies have attempted to design defensive mechanisms that would ensure the security of neural networks. Nevertheless, these mitigation techniques remain insufficient to entirely address all the vulnerabilities that may reside in these architectures. In this article, we will extensively address the security of neural networks. We will begin with a study of the different approaches towards the security of neural networks. Then, we will detail significant sources of vulnerabilities in neural networks. Afterward, we will examine the theory of adversarial examples attack as well as the optimization problem related to them. Thereafter, we will conduct a comparative study of the most common mitigation techniques in the literature. Finally, we will propose a framework devoted to assessing adversarial examples robustness.","",""
0,"Qi Xuan, Yalu Shan, Jinhuan Wang, Zhongyuan Ruan, Guanrong Chen","Adversarial Attacks to Scale-Free Networks: Testing the Robustness of Physical Criteria",2020,"","","","",19,"2022-07-13 09:19:22","","","","",,,,,0,0.00,0,5,2,"Adversarial attacks have been alerting the artificial intelligence community recently, since many machine learning algorithms were found vulnerable to malicious attacks. This paper studies adversarial attacks to scale-free networks to test their robustness in terms of statistical measures. In addition to the well-known random link rewiring (RLR) attack, two heuristic attacks are formulated and simulated: degree-addition-based link rewiring (DALR) and degree-interval-based link rewiring (DILR). These three strategies are applied to attack a number of strong scale-free networks of various sizes generated from the Barabasi-Albert model. It is found that both DALR and DILR are more effective than RLR, in the sense that rewiring a smaller number of links can succeed in the same attack. However, DILR is as concealed as RLR in the sense that they both are constructed by introducing a relatively small number of changes on several typical structural properties such as average shortest path-length, average clustering coefficient, and average diagonal distance. The results of this paper suggest that to classify a network to be scale-free has to be very careful from the viewpoint of adversarial attack effects.","",""
0,"Kazim Ali","Towards Robustness of Convolutional Neural Network against Adversarial Examples",2020,"","","","",20,"2022-07-13 09:19:22","","","","",,,,,0,0.00,0,1,2,"Deep learning is at the heart of the current rise of artificial intelligence. In the field of Computer Vision, it has become the workhorse for applications ranging from self-driving cars to surveillance and security. A Convolutional Neural Network (ConvNet/CNN) is a Deep Learning algorithm which can take in an input image, assign importance (learnable weights and biases) to various objects in the image and be able to differentiate one from the other. The architecture of a ConvNet is analogous to that of the connectivity pattern of Neurons in the Human Brain and was inspired by the organization of the Visual Cortex. Convolutional Neural Networks have demonstrated phenomenal success (often beyond human capabilities) in solving complex problems, but recent studies show that they are vulnerable to adversarial attacks in the form of subtle perturbations to inputs that lead a model to predict incorrect outputs. For images, such perturbations are often too small to be perceptible, yet they completely fool the CNN models. Adversarial attacks pose a serious threat to the success of CNN in practice. In this paper, we have tried to reconstruct adversarial examples/patches/images which are created by physical attacks (Gaussian blur attack and Salt and Pepper Noise Attack), so that CNN again classified correctly these reconstructed adversarial or perturbed images.","",""
1,"Hanjie Chen, Yangfeng Ji","Adversarial Training for Improving Model Robustness? Look at Both Prediction and Interpretation",2022,"","","","",21,"2022-07-13 09:19:22","","10.48550/arXiv.2203.12709","","",,,,,1,1.00,1,2,1,"Neural language models show vulnerability to adversarial examples which are semantically similar to their original counterparts with a few words replaced by their synonyms. A common way to improve model robustness is adversarial training which follows two steps—collecting adversarial examples by attacking a target model, and fine-tuning the model on the augmented dataset with these adversarial examples. The objective of traditional adversarial training is to make a model produce the same correct predictions on an original/adversarial example pair. However, the consistency between model decision-makings on two similar texts is ignored. We argue that a robust model should behave consistently on original/adversarial example pairs, that is making the same predictions (what) based on the same reasons (how) which can be reflected by consistent interpretations. In this work, we propose a novel feature-level adversarial training method named FLAT. FLAT aims at improving model robustness in terms of both predictions and interpretations. FLAT incorporates variational word masks in neural networks to learn global word importance and play as a bottleneck teaching the model to make predictions based on important words. FLAT explicitly shoots at the vulnerability problem caused by the mismatch between model understandings on the replaced words and their synonyms in original/adversarial example pairs by regularizing the corresponding global word importance scores. Experiments show the effectiveness of FLAT in improving the robustness with respect to both predictions and interpretations of four neural network models (LSTM, CNN, BERT, and DeBERTa) to two adversarial attacks on four text classification tasks. The models trained via FLAT also show better robustness than baseline models on unforeseen adversarial examples across different attacks.","",""
0,"Xuanyu Zhang, Shi-You Xu, Jun Hu, Zhi-Yuan Xie","Optimized L2 Norm Loss for Adversarial Robustness",2022,"","","","",22,"2022-07-13 09:19:22","","10.1109/CCAI55564.2022.9807767","","",,,,,0,0.00,0,4,1,"Although adversarial training is the most common method to make models obtain better adversarial robustness, its drawback of leading to reduced accuracy has been plaguing the academic community. In recent years, many articles have pointed out that good Lipschitz continuity helps models obtain better robustness and standard accuracy, and argued that models that are both robust and accurate exist. However, many methods still perform less well with models even with the addition of Lipschitz continuity constraints. Therefore, we discuss the drawbacks of existing Lipschitz continuity metric in deep learning in terms of Lipschitz continuity, and propose a counteracting Lipschitz continuity metric that is more suitable for deep learning. We demonstrate theoretically and experimentally that Mixup can significantly enhance the local Lipschitz continuity of the model. Using this property, we generate a large number of mix confrontation samples using Target attack to fill the entire neighborhood space. Our method gives the model a smoother localization and significantly improves the adversarial robustness of the model beyond most existing adversarial training methods.","",""
0,"Bilel Tarchoun, Anouar Ben Khalifa, M. Mahjoub","Investigating the robustness of multi-view detection to current adversarial patch threats",2022,"","","","",23,"2022-07-13 09:19:22","","10.1109/ATSIP55956.2022.9805870","","",,,,,0,0.00,0,3,1,"As deep neural networks are increasingly integrated in our daily lives, the safety and reliability of their results has become of paramount importance. However, the vulnerability of these networks to adversarial attacks are an obstacle to wider adoption, especially in safety-critical applications: A malicious actor can manipulate the results of a deep neural network by adding a nearly imperceptible noise to the input. And adversarial patch attacks make real-life implementations of these threats easier. Therefore, studying these attacks has become a rapidly growing field of artificial intelligence research. One aspect of this research is studying the behavior of patch attacks in various scenarios to understand their inner workings and find novel method to secure deep neural networks. In this paper, we examine the effectiveness of existing adversarial patch attacks against a multi-view detector. To this aim, we propose an evaluation framework where an adversarial patch is trained against a single view of a multi-view dataset and transfer the patch to the other views of the dataset with the use of perspective geometric transforms. Our results confirm that current single-view adversarial patches struggle against multi-view detectors, especially when only few views are attacked. These observations suggest that multi-view detection methods may be a step forward towards reliable and safe AI.","",""
0,"Tianjun Mo, Qifan Hu","The analysis of artificial intelligence technology: based on neural network",2022,"","","","",24,"2022-07-13 09:19:22","","10.1117/12.2628491","","",,,,,0,0.00,0,2,1,"Contemporarily, deep Learning has achieved remarkable achievement in the field of artificial intelligence technology. Comparing with traditional machine learning methods, deep learning creates its model by constructing the neural network. This investigation reviews the neural network's development history and describes classical neural network methods, e.g., convolutional neural networks and recurrent neural networks. Besides, the shortcomings and limitations that the neural network is currently facing, including aspects in accuracy, stability, and robustness, are discussed. Meanwhile, the solutions towards these limitations are also mentioned, e.g., capsule network and adversarial attack. These results shed light for the future developments of Neural Network.","",""
0,"Narmin Ghaffari Laleh, D. Truhn, Gregory Patrick Veldhuizen, Tianyu Han, Marko van Treeck, R. D. Bülow, R. Langer, B. Dislich, P. Boor, V. Schulz, J. Kather","Adversarial attacks and adversarial robustness in computational pathology",2022,"","","","",25,"2022-07-13 09:19:22","","10.1101/2022.03.15.484515","","",,,,,0,0.00,0,11,1,"Artificial Intelligence (AI) can support diagnostic workflows in oncology by aiding diagnosis and providing biomarkers. AI applications are therefore expected to evolve from academic prototypes to commercial products in the coming years. However, AI applications are vulnerable to adversarial attacks, such as malicious interference with test data aiming to cause misclassifications. Therefore, it is essential for the use of AI-based diagnostic devices to secure them against such attacks before widespread use. Unfortunately, no resistant systems exist in computational pathology so far. To address this problem, we investigate the susceptibility of convolutional neural networks (CNNs) to multiple types of white- and black-box attacks. We demonstrate that both attacks can easily confuse CNNs in clinically relevant pathology tasks and impair classification performance. Classical adversarially robust training and dual batch normalization (DBN) are possible mitigation strategies but require precise knowledge of the type of attack used in the inference. We demonstrate that vision transformers (ViTs) perform equally well compared to CNNs at baseline and are orders of magnitude more robust to different types of white-box and black-box attacks. At a mechanistic level, we show that this is associated with a more robust latent representation of clinically relevant categories in ViTs compared to CNNs. Our results are in line with previous theoretical studies. We show that ViTs are robust learners in computational pathology. This implies that large-scale rollout of AI models in computational pathology should rely on ViTs rather than CNN-based classifiers to provide inherent protection against adversaries.","",""
2,"Nistha Tandiya, E. Colbert, V. Marojevic, Jeffrey H. Reed","Biologically Inspired Artificial Intelligence Techniques",2018,"","","","",26,"2022-07-13 09:19:22","","10.1007/978-3-319-77492-3_13","","",,,,,2,0.50,1,4,4,"","",""
2,"L. Bortolussi, G. Sanguinetti","Intrinsic Geometric Vulnerability of High-Dimensional Artificial Intelligence",2018,"","","","",27,"2022-07-13 09:19:22","","","","",,,,,2,0.50,1,2,4,"The success of modern Artificial Intelligence (AI) technologies depends critically on the ability to learn non-linear functional dependencies from large, high dimensional data sets. Despite recent high-profile successes, empirical evidence indicates that the high predictive performance is often paired with low robustness, making AI systems potentially vulnerable to adversarial attacks. In this report, we provide a simple intuitive argument suggesting that high performance and vulnerability are intrinsically coupled, and largely dependent on the geometry of typical, high-dimensional data sets. Our work highlights a major potential pitfall of modern AI systems, and suggests practical research directions to ameliorate the problem.","",""
20,"Sheeba Lal, S. Rehman, J. H. Shah, Talha Meraj, Hafiz Tayyab Rauf, Robertas Damaševičius, M. Mohammed, Karrar Hameed Abdulkareem","Adversarial Attack and Defence through Adversarial Training and Feature Fusion for Diabetic Retinopathy Recognition",2021,"","","","",28,"2022-07-13 09:19:22","","10.3390/s21113922","","",,,,,20,20.00,3,8,1,"Due to the rapid growth in artificial intelligence (AI) and deep learning (DL) approaches, the security and robustness of the deployed algorithms need to be guaranteed. The security susceptibility of the DL algorithms to adversarial examples has been widely acknowledged. The artificially created examples will lead to different instances negatively identified by the DL models that are humanly considered benign. Practical application in actual physical scenarios with adversarial threats shows their features. Thus, adversarial attacks and defense, including machine learning and its reliability, have drawn growing interest and, in recent years, has been a hot topic of research. We introduce a framework that provides a defensive model against the adversarial speckle-noise attack, the adversarial training, and a feature fusion strategy, which preserves the classification with correct labelling. We evaluate and analyze the adversarial attacks and defenses on the retinal fundus images for the Diabetic Retinopathy recognition problem, which is considered a state-of-the-art endeavor. Results obtained on the retinal fundus images, which are prone to adversarial attacks, are 99% accurate and prove that the proposed defensive model is robust.","",""
16,"Deqiang Li, Qianmu Li, Yanfang Ye, Shouhuai Xu","A Framework for Enhancing Deep Neural Networks Against Adversarial Malware",2021,"","","","",29,"2022-07-13 09:19:22","","10.1109/TNSE.2021.3051354","","",,,,,16,16.00,4,4,1,"Machine learning-based malware detection is known to be vulnerable to adversarial evasion attacks. The state-of-the-art is that there are no effective defenses against these attacks. As a response to the adversarial malware classification challenge organized by the MIT Lincoln Lab and associated with the AAAI-19 Workshop on Artificial Intelligence for Cyber Security (AICS'2019), we propose six guiding principles to enhance the robustness of deep neural networks. Some of these principles have been scattered in the literature, but the others are introduced in this paper for the first time. Under the guidance of these six principles, we propose a defense framework to enhance the robustness of deep neural networks against adversarial malware evasion attacks. By conducting experiments with the Drebin Android malware dataset, we show that the framework can achieve a 98.49% accuracy (on average) against grey-box attacks, where the attacker knows some information about the defense and the defender knows some information about the attack, and an 89.14% accuracy (on average) against the more capable white-box attacks, where the attacker knows everything about the defense and the defender knows some information about the attack. The framework wins the AICS'2019 challenge by achieving a 76.02% accuracy, where neither the attacker (i.e., the challenge organizer) knows the framework or defense nor we (the defender) know the attacks. This gap highlights the importance of knowing about the attack.","",""
0,"Da Teng, Xiao Song, Guanghong Gong, Liang Han","ROBUSTNESS OF DEEP NEURAL NETWORKS TO ADVERSARIAL EXAMPLES",2017,"","","","",30,"2022-07-13 09:19:22","","","","",,,,,0,0.00,0,4,5,"Deep neural networks have achieved state-of-the-art performance in many artificial intelligence areas, such as object recognition, speech recognition and machine translation. While the deep neural networks have high expression capabilities, they are prone to over fitting due to the high dimensionalities of the networks. Recently, deep neural networks are found to be unstable to adversarial perturbations, which are small but can maximize the network’s prediction error. This paper proposes a novel training algorithm to improve the robustness of the neural networks to adversarial examples.","",""
5,"M. Usama, R. N. Mitra, Inaam Ilahi, Junaid Qadir, M. Marina","Examining Machine Learning for 5G and Beyond Through an Adversarial Lens",2020,"","","","",31,"2022-07-13 09:19:22","","10.1109/MIC.2021.3049190","","",,,,,5,2.50,1,5,2,"Spurred by the recent advances in deep learning to harness rich information hidden in large volumes of data and to tackle problems that are hard to model/solve (e.g., resource allocation problems), there is currently tremendous excitement in the mobile networks domain around the transformative potential of data-driven artificial intelligence/machine learning (AI/ML) based network automation, control and analytics for 5G and beyond. In this article, we present a cautionary perspective on the use of AI/ML in the 5G context by highlighting the adversarial dimension spanning multiple types of ML (supervised/unsupervised/reinforcement learning) and support this through three case studies. We also discuss approaches to mitigate this adversarial ML risk, offer guidelines for evaluating the robustness of ML models, and call attention to issues surrounding ML oriented research in 5G more generally.","",""
0,"Wei He, Bingbing Song, Ruxin Wang, Wenyu Peng, Shenghong He, Wei Zhou","TDNN:A Tensor Decomposition Adversarial Defense Method Based on Neural Network",2021,"","","","",32,"2022-07-13 09:19:22","","10.1109/acait53529.2021.9731274","","",,,,,0,0.00,0,6,1,"In recent years, neural networks have shown strong performance on various tasks. However, neural networks show the vulnerability to carefully designed noise of adversarial examples. Through research, it is found that the neural networks usually have good robustness to common noise, but almost no resistance to carefully designed imperceptible perturbations noise of adversarial examples. To solve this problem, related works have proposed to transform the noise of the adversarial sample into random ordinary noise, which greatly protects the model from adversarial attack. To solve this problem, we propose an adversarial defense method based on tensor decomposition, which use tensor decomposition technology to decompose and reconstruct the image, and retain the main features of the image and remove the perturbation of adversarial examples. Based on traditional tensor decomposition method, we further propose the tensor decomposition of neural networks method (TDNN). Compared with traditional tensor decomposition, TDNN has better defense effect and lower running time. Beside TDNN can be combined with existing defense methods and does not require extra changes for model. Through Rigorous experiments show that TDNN can remove carefully added perturbation and greatly improve the robustness of the model.","",""
0,"Huajian Ding, Shumeng He, Yanwen Wu, Yongli Jin, Lin Gan, Gaodi Xu, Houqun Yang","An efficient face recognition attack method based on generative adversarial networks and cosine metrics",2021,"","","","",33,"2022-07-13 09:19:22","","10.1109/acait53529.2021.9731246","","",,,,,0,0.00,0,7,1,"Deep neural networks are vulnerable to attacks on adversarial samples. These attacks are caused by adding small magnitude perturbations to the input samples, which may lead to misclassification of the deep neural network. Based on the study of the adversarial sample attack network model, we propose an attack sample based on the generative adversarial network HNUGAN, incorporating the cosine metric of disparity recognition, for the features of the face dataset, to construct an attack sample to attack the face recognition system. Using these adversarial samples can reduce the recognition accuracy of models such as GoogleNet and ResNet to a very low level, and thus complete the attack on the target model. Interestingly, we can obtain a batch of adversarial samples through adversarial training to expand the dataset and retrain the target model to improve its robustness and resistance to attacks.","",""
0,"Jinye Cai, Zhao Qiu, Xiaoran Yang, Chao Li, Yu Jin, Liwen Shen","Chinese Text Classification Based on Adversarial Training",2021,"","","","",34,"2022-07-13 09:19:22","","10.1007/978-3-030-78615-1_28","","",,,,,0,0.00,0,6,1,"","",""
0,"Chuan Zhou, Duohe Ma, Tianwei Zhang, Liming Wang","Generating Adversarial Examples for Robust Deception against Image Transfer and Reloading",2021,"","","","",35,"2022-07-13 09:19:22","","10.1109/icaice54393.2021.00159","","",,,,,0,0.00,0,4,1,"Adversarial examples play an irreplaceable role in evaluating DNNs' security and robustness. It's essential to understanding the adversarial examples' effectiveness to utilize them for model improvement. In this paper, we explore the impact of input transformation on adversarial examples. First, we discover a new phenomenon. The process of RELOAD or TRANSFER may deactivate adversarial examples' malicious functionality. The reason is that these processes would reduce pixel precision, which counters the perturbation added by the adversary. We validate this finding on different mainstream adversarial algorithms. Second, we propose a novel Confidence Iteration method, which can generate more robust adversarial examples. The key idea is to set the confidence threshold and add the pixel loss caused by image reloading or transferring into the calculation. We integrate our solution with existing adversarial algorithms. Experiments indicate that such integration can significantly increase the adversarial attacks' success rate.","",""
0,"Xueyang Wang, Xiaozhang Liu","Enhancing Robustness of Classifiers Based on PCA",2021,"","","","",36,"2022-07-13 09:19:22","","10.1109/PRAI53619.2021.9550807","","",,,,,0,0.00,0,2,1,"To date, deep learning techniques have been widely used. However, deep neural networks (DNNs) are vulnerable to adversarial attacks, which has become one of the hidden risks issues affecting system security. The adversarial sample is a perturbation input to fool the deep learning model. The inherent weakness of DNNs that lacks robustness to adversarial samples brings security problems, especially for tasks that require high reliability. This paper proposed a robustness enhancing method based on principal component analysis (PCA) and applied it to deep networks, which enhanced the ability of DNNs to resist adversarial attacks. Specifically, the proposed method firstly used PCA to downscale the clean samples, and then, chose two non-target attacks, DeepFool and FGSM, to craft adversarial samples pre-and-post downscale. Finally, by evaluating the changes in the robustness of the classifier, we draw the corresponding analytical conclusions. Experimental results on MNIST show that the proposed method makes deep networks more robust against white-box attacks.","",""
0,"A. Ramanathan, Sumit Kumar Jha","Adversarial Attacks against AI-driven Experimental Peptide Design Workflows",2021,"","","","",37,"2022-07-13 09:19:22","","10.1109/xloop54565.2021.00010","","",,,,,0,0.00,0,2,1,"Artificial intelligence and machine learning (AI/ML) techniques are fueling a revolution in how scientific experiments are designed, implemented and automated. Specifically, increasing high-bandwidth instruments coupled to new hardware and software systems can significantly improve the throughput of experimental results, while AI/ML techniques can provide insights into novel science and theories that were hitherto inaccessible. Despite recent progress in such “self-driving labs”, these automated platforms are susceptible to adversarial attacks as well as more traditional cybersecurity attacks. Using a motivating example of an automated approach to design anti-microbial peptides (AMP), our position paper seeks to demonstrate how a lack of adversarial robustness of AI systems such as protein folding networks may affect the execution of such experimental workflows. We highlight important problems in adversarial robustness that may need to be resolved in order to establish a trustworthy and safe AI -driven AMP synthesis system.","",""
0,"Jiakai Wang","Adversarial Examples in Physical World",2021,"","","","",38,"2022-07-13 09:19:22","","10.24963/ijcai.2021/694","","",,,,,0,0.00,0,1,1,"Although deep neural networks (DNNs) have already made fairly high achievements and a very wide range of impact, their vulnerability attracts lots of interest of researchers towards related studies about artificial intelligence (AI) safety and robustness this year. A series of works reveals that the current DNNs are always misled by elaborately designed adversarial examples. And unfortunately, this peculiarity also affects real-world AI applications and places them at potential risk. we are more interested in physical attacks due to their implementability in the real world. The study of physical attacks can effectively promote the application of AI techniques, which is of great significance to the security development of AI.","",""
0,"Mingu Kang, HyeungKyeom Kim, Suchul Lee, Seokmin Han","Resilience against Adversarial Examples: Data-Augmentation Exploiting Generative Adversarial Networks",2021,"","","","",39,"2022-07-13 09:19:22","","10.3837/tiis.2021.11.013","","",,,,,0,0.00,0,4,1,"Recently, malware classification based on Deep Neural Networks (DNN) has gained significant attention due to the rise in popularity of artificial intelligence (AI). DNN-based malware classifiers are a novel solution to combat never-before-seen malware families because this approach is able to classify malwares based on structural characteristics rather than requiring particular signatures like traditional malware classifiers. However, these DNNbased classifiers have been found to lack robustness against malwares that are carefully crafted to evade detection. These specially crafted pieces of malware are referred to as adversarial examples. We consider a clever adversary who has a thorough knowledge of DNN-based malware classifiers and will exploit it to generate a crafty malware to fool DNN-based classifiers. In this paper, we propose a DNN-based malware classifier that becomes resilient to these kinds of attacks by exploiting Generative Adversarial Network (GAN) based data augmentation. The experimental results show that the proposed scheme classifies malware, including AEs, with a false positive rate (FPR) of 3.0% and a balanced accuracy of 70.16%. These are respective 26.1% and 18.5% enhancements when compared to a traditional DNNbased classifier that does not exploit GAN.","",""
0,"Shuo Liu, Liwen Xu","Anomaly Detection with Dual Adversarial Training",2021,"","","","",40,"2022-07-13 09:19:22","","10.1109/ICDMW53433.2021.00063","","",,,,,0,0.00,0,2,1,"Anomaly detection is of paramount importance in data mining and artificial intelligence. Deep generative models have been widely used in anomaly detection as a dominant paradigm to model complex and high-dimensional data distribution. However, developing effective and robust anomaly detection systems for complex and high-dimensional data using generative models remains a challenge. In this paper, we propose a novel Dual Adversarial Training method for Anomaly Detection (DAT-AD), which uses the adversarial training idea in Generative Adversarial Network (GAN) and the Virtual Adversarial Training (VAT) idea to improve the effectiveness and robustness of anomaly detector, respectively. In addition, we have also carefully designed the network architecture and the loss function of our method to ensure that the trained network can be utilized to the greatest extent. We demonstrate the superiority of our method by conducting various experiments on tabular and image data.","",""
0,"Liangyu Ji, Tian Yao, Ge Wu, Liquan Chen, Zhongyuan Qin","ERGA: An Effective Region Gradient Algorithm for Adversarial Example Generation",2021,"","","","",41,"2022-07-13 09:19:22","","10.1145/3501409.3501611","","",,,,,0,0.00,0,5,1,"In recent years, with the rapid development of deep neural networks in the field of artificial intelligence, some of the security issues involved have gradually attracted attention in the industry, one of which is adversarial sample attacks. The attacker inputs carefully designed adversarial samples to the deep learning model, causing the attacked model to output misclassification results with high confidence, which seriously threatens the robustness of the deep learning model. Based on the commonly used deep learning network model, combined with the interpretability research of neural networks, we propose an effective region generation algorithm (ERGA) for adversarial sample generation, which can overcome the defects of the current commonly used algorithms. In our approach, the effective region selection step is added in the adversarial sample generation process, which overcomes the limitation of common adversarial sample generation algorithms that are limited to global pixel perturbation. We try to limit the number of pixels to be changed while maintaining a higher attack success. The algorithm also optimizes the process of counter disturbance generation, solves the uncertainty of the gradient update direction and amplitude in the iterative process. In addition, it also introduces the interpretability research of counter samples, which can be used to a certain extent in the deep learning network. At the same time, ERGA can improve the ability of supervision and self-examination of the classification results.","",""
14,"Weiren Kong, Deyun Zhou, Zhen Yang, Yiyang Zhao, Kai Zhang","UAV Autonomous Aerial Combat Maneuver Strategy Generation with Observation Error Based on State-Adversarial Deep Deterministic Policy Gradient and Inverse Reinforcement Learning",2020,"","","","",42,"2022-07-13 09:19:22","","10.3390/electronics9071121","","",,,,,14,7.00,3,5,2,"With the development of unmanned aerial vehicle (UAV) and artificial intelligence (AI) technology, Intelligent UAV will be widely used in future autonomous aerial combat. Previous researches on autonomous aerial combat within visual range (WVR) have limitations due to simplifying assumptions, limited robustness, and ignoring sensor errors. In this paper, in order to consider the error of the aircraft sensors, we model the aerial combat WVR as a state-adversarial Markov decision process (SA-MDP), which introduce the small adversarial perturbations on state observations and these perturbations do not alter the environment directly, but can mislead the agent into making suboptimal decisions. Meanwhile, we propose a novel autonomous aerial combat maneuver strategy generation algorithm with high-performance and high-robustness based on state-adversarial deep deterministic policy gradient algorithm (SA-DDPG), which add a robustness regularizers related to an upper bound on performance loss at the actor-network. At the same time, a reward shaping method based on maximum entropy (MaxEnt) inverse reinforcement learning algorithm (IRL) is proposed to improve the aerial combat strategy generation algorithm’s efficiency. Finally, the efficiency of the aerial combat strategy generation algorithm and the performance and robustness of the resulting aerial combat strategy is verified by simulation experiments. Our main contributions are three-fold. First, to introduce the observation errors of UAV, we are modeling air combat as SA-MDP. Second, to make the strategy network of air combat maneuver more robust in the presence of observation errors, we introduce regularizers into the policy gradient. Third, to solve the problem that air combat’s reward function is too sparse, we use MaxEnt IRL to design a shaping reward to accelerate the convergence of SA-DDPG.","",""
6,"Kartik Gupta, Thalaiyasingam Ajanthan","Improved Gradient based Adversarial Attacks for Quantized Networks",2020,"","","","",43,"2022-07-13 09:19:22","","10.1609/aaai.v36i6.20637","","",,,,,6,3.00,3,2,2,"Neural network quantization has become increasingly popular due to efficient memory consumption and faster computation resulting from bitwise operations on the quantized networks. Even though they exhibit excellent generalization capabilities, their robustness properties are not well-understood. In this work, we systematically study the robustness of quantized networks against gradient based adversarial attacks and demonstrate that these quantized models suffer from gradient vanishing issues and show a fake sense of robustness. By attributing gradient vanishing to poor forward-backward signal propagation in the trained network, we introduce a simple temperature scaling approach to mitigate this issue while preserving the decision boundary. Despite being a simple modification to existing gradient based adversarial attacks, experiments on multiple image classification datasets with multiple network architectures demonstrate that our temperature scaled attacks obtain near-perfect success rate on quantized networks while outperforming original attacks on adversarially trained models as well as floating-point networks.","",""
10,"Ke Yan, Jian-nan Su, Jing Huang, Yu-chang Mo","Chiller Fault Diagnosis Based on VAE-Enabled Generative Adversarial Networks",2022,"","","","",44,"2022-07-13 09:19:22","","10.1109/tase.2020.3035620","","",,,,,10,10.00,3,4,1,"Artificial intelligence (AI)-enhanced automated fault diagnosis (AFD) has become increasingly popular for chiller fault diagnosis with promising classification performance. In practice, a sufficient number of fault samples are required by the AI methods in the training phase. However, faulty training samples are generally much more difficult to be collected than normal training samples. Data augmentation is introduced in these scenarios to enhance the training data set with synthetic data. In this study, a variational autoencoder-based conditional Wasserstein GAN with gradient penalty (CWGAN-GP-VAE) is proposed to diagnose various faults for chillers. A detailed comparative study has been conducted with real-world fault data samples to verify the effectiveness and robustness of the proposed methodology. Note to Practitioners—This work attacks the fact that faulty training samples are usually much harder to be collected than the normal training samples in the practice of chiller automated fault diagnosis (AFD). Modern supervised learning chiller AFD relies on a sufficient number of faulty training samples to train the classifier. When the number of faulty training samples is insufficient, the conventional AFD methods fail to work. This study proposed a variational autoencoder-based conditional Wasserstein GAN with gradient penalty (CWGAN-GP-VAE) framework for generating synthetic faulty training samples to enrich the training data set for machine learning-based AFD methods. The proposed algorithm has been carefully designed, implemented, and practically proved to be more effective than the existing methods in the literature.","",""
3,"Duo Liu, Yang Sun, Xiaoyan Zhao, Gengxiang Zhang, R. Liu","Adversarial Training for Session-based Item Recommendations",2020,"","","","",45,"2022-07-13 09:19:22","","10.1109/ITAIC49862.2020.9338819","","",,,,,3,1.50,1,5,2,"Deep neural networks (DNN) have become the mainstream in session-based recommendations (SR) due to their capability of learning feature representations from scratch and capturing the dynamic non-linear user-item relationships. Despite their popularity, these DNN-based SR models tend to be unstable, and small scale perturbations to the inputs may lead to incorrect predictions. In this paper, we propose two adversarial training (AT) methods to construct adversarial perturbations and add them to the DNN-based SR models so as to improve their robustness against adversarial perturbations and overfitting. The generated adversarial perturbations are applicable to any DNN-based SR models, which can be treated as common techniques for further improving the performance of DNN-based SR models. We investigate the effectiveness of the constructed adversarial perturbations by applying them to the two most important DNN architectures (recurrent neural network and convolutional neural network) on two benchmark datasets. Experimental results demonstrate that the AT can significantly improve the performance of the baseline methods.","",""
1,"Chia-Yi Hsu, Pin-Yu Chen, Songtao Lu, Sijia Liu, Chia-Mu Yu","Adversarial Examples Can Be Effective Data Augmentation for Unsupervised Machine Learning",2021,"","","","",46,"2022-07-13 09:19:22","","10.1609/aaai.v36i6.20650","","",,,,,1,1.00,0,5,1,"Adversarial examples causing evasive predictions are widely used to evaluate and improve the robustness of machine learning models. However, current studies focus on supervised learning tasks, relying on the ground truth data label, a targeted objective, or supervision from a trained classifier. In this paper, we propose a framework of generating adversarial examples for unsupervised models and demonstrate novel applications to data augmentation. Our framework exploits a mutual information neural estimator as an information theoretic similarity measure to generate adversarial examples without supervision. We propose a new MinMax algorithm with provable convergence guarantees for the efficient generation of unsupervised adversarial examples. Our framework can also be extended to supervised adversarial examples. When using unsupervised adversarial examples as a simple plugin data augmentation tool for model retraining, significant improvements are consistently observed across different unsupervised tasks and datasets, including data reconstruction, representation learning, and contrastive learning. Our results show novel methods and considerable advantages in studying and improving unsupervised machine learning via adversarial examples.","",""
6,"Paolo Arcaini, A. Bombarda, S. Bonfanti, A. Gargantini","Dealing with Robustness of Convolutional Neural Networks for Image Classification",2020,"","","","",47,"2022-07-13 09:19:22","","10.1109/AITEST49225.2020.00009","","",,,,,6,3.00,2,4,2,"SW-based systems depend more and more on AI also for critical tasks. For instance, the use of machine learning, especially for image recognition, is increasing ever more. As state-of-the-art, Convolutional Neural Networks (CNNs) are the most adopted techniques for image classification. Although they are proved to have optimal results, it is not clear what happens when unforeseen modifications during the image acquisition and elaboration occur. Thus, it is very important to assess the robustness of a CNN, especially when it is used in a safety critical system, as, e.g., in the medical domain or in automated driving systems. Most of the analyses made about the robustness of CNNs are focused on adversarial examples which are created by exploiting the CNN internal structure; however, these are not the only problems we can encounter with CNNs and, moreover, they may be unlikely in some fields. This is why, in this paper, we focus on the robustness analysis when plausible alterations caused by an error during the acquisition of the input images occur. We give a novel definition of robustness w.r.t. possible input alterations for a CNN and we propose a framework to compute it. Moreover, we analyse four methods (data augmentation, limited data augmentation, network parallelization, and limited network parallelization) which can be used to improve the robustness of a CNN for image classification. Analyses are conducted over a dataset of histologic images.","",""
3,"M. Pautov, Nurislam Tursynbek, Marina Munkhoeva, Nikita Muravev, Aleksandr Petiushko, I. Oseledets","CC-Cert: A Probabilistic Approach to Certify General Robustness of Neural Networks",2021,"","","","",48,"2022-07-13 09:19:22","","10.1609/aaai.v36i7.20768","","",,,,,3,3.00,1,6,1,"In safety-critical machine learning applications, it is crucial to defend models against adversarial attacks --- small modifications of the input that change the predictions. Besides rigorously studied $\ell_p$-bounded additive perturbations, semantic perturbations (e.g. rotation, translation) raise a serious concern on deploying ML systems in real-world. Therefore, it is important to provide provable guarantees for deep learning models against semantically meaningful input transformations. In this paper, we propose a new universal probabilistic certification approach based on Chernoff-Cramer bounds that can be used in general attack settings. We estimate the probability of a model to fail if the attack is sampled from a certain distribution. Our theoretical findings are supported by experimental results on different datasets.","",""
7,"Flávio Luis de Mello","A Survey on Machine Learning Adversarial Attacks",2020,"","","","",49,"2022-07-13 09:19:22","","10.17648/jisc.v7i1.76","","",,,,,7,3.50,7,1,2,"It is becoming notorious several types of adversaries based on their threat model leverage vulnerabilities to compromise a machine learning system. Therefore, it is important to provide robustness to machine learning algorithms and systems against these adversaries. However, there are only a few strong countermeasures, which can be used in all types of attack scenarios to design a robust artificial intelligence system. This paper is structured and comprehensive overview of the research on attacks to machine learning systems and it tries to call the attention from developers and software houses to the security issues concerning machine learning.","",""
7,"Vasisht Duddu, N. Pillai, D. V. Rao, V. Balas","Fault Tolerance of Neural Networks in Adversarial Settings",2019,"","","","",50,"2022-07-13 09:19:22","","10.3233/JIFS-179677","","",,,,,7,2.33,2,4,3,"Artificial Intelligence systems require a through assessment of different pillars of trust, namely, fairness, interpretability, data and model privacy, reliability (safety) and robustness against against adversarial attacks. While these research problems have been extensively studied in isolation, an understanding of the trade-off between different pillars of trust is lacking. To this extent, the trade-off between fault tolerance, privacy and adversarial robustness is evaluated for the specific case of Deep Neural Networks, by considering two adversarial settings under a security and a privacy threat model. Specifically, this work studies the impact of the fault tolerance of the Neural Network on training the model by adding noise to the input (Adversarial Robustness) and noise to the gradients (Differential Privacy). While training models with noise to inputs, gradients or weights enhances fault tolerance, it is observed that adversarial robustness and fault tolerance are at odds with each other. On the other hand, ($\epsilon,\delta$)-Differentially Private models enhance the fault tolerance, measured using generalisation error, theoretically has an upper bound of $e^{\epsilon} - 1 + \delta$. This novel study of the trade-off between different elements of trust is pivotal for training a model which satisfies the requirements for different pillars of trust simultaneously.","",""
0,"Sandhya Aneja, Nagender Aneja, Pg Emeroylariffion Abas, A. G. Naim","Defense against adversarial attacks on deep convolutional neural networks through nonlocal denoising",2022,"","","","",51,"2022-07-13 09:19:22","","10.11591/ijai.v11.i3.pp961-968","","",,,,,0,0.00,0,4,1,"Despite substantial advances in network architecture performance, the susceptibility of adversarial attacks makes deep learning challenging to implement in safety-critical applications. This paper proposes a data-centric approach to addressing this problem. A nonlocal denoising method with different luminance values has been used to generate adversarial examples from the Modified National Institute of Standards and Technology database (MNIST) and Canadian Institute for Advanced Research (CIFAR-10) data sets. Under perturbation, the method provided absolute accuracy improvements of up to 9.3% in the MNIST data set and 13% in the CIFAR-10 data set. Training using transformed images with higher luminance values increases the robustness of the classifier. We have shown that transfer learning is disadvantageous for adversarial machine learning. The results indicate that simple adversarial examples can improve resilience and make deep learning easier to apply in various applications.","",""
0,"Seungyong Moon, Gaon An, Hyun Oh Song","Preemptive Image Robustification for Protecting Users against Man-in-the-Middle Adversarial Attacks",2021,"","","","",52,"2022-07-13 09:19:22","","10.1609/aaai.v36i7.20751","","",,,,,0,0.00,0,3,1,"Deep neural networks have become the driving force of modern image recognition systems. However, the vulnerability of neural networks against adversarial attacks poses a serious threat to the people affected by these systems. In this paper, we focus on a real-world threat model where a Man-in-the-Middle adversary maliciously intercepts and perturbs images web users upload online. This type of attack can raise severe ethical concerns on top of simple performance degradation. To prevent this attack, we devise a novel bi-level optimization algorithm that finds points in the vicinity of natural images that are robust to adversarial perturbations. Experiments on CIFAR-10 and ImageNet show our method can effectively robustify natural images within the given modification budget. We also show the proposed method can improve robustness when jointly used with randomized smoothing.","",""
0,"Garrett Hall, Arun Das, J. Quarles, P. Rad","Studying Adversarial Attacks on Behavioral Cloning Dynamics",2020,"","","","",53,"2022-07-13 09:19:22","","10.1109/ICTAI50040.2020.00077","","",,,,,0,0.00,0,4,2,"High-fidelity visual simulation-based environments and advanced learning algorithms can be used to train robots to carry out specific tasks. Behavior cloning is a fast and easy way to train robots to learn from experience by modeling their actions according to human actions. As we make use of these agents in our day-to-day life, the robustness of such system-of-systems trained on simulation environments are of great concern. In this paper, we explore adversarial attacks in simulation environments, specifically for behavioral cloning models that cause the adversary to be able to take control of the steering mechanism of an autonomous agent. We focus our attention on improving latency and noticeability, two fundamental issues with adversarial attacks, by reducing the number of iterations to a single step during a white-box adversarial attack within a noticeability threshold. More specifically, the gradients at the image input layer and the output layer of the neural network are utilized in the adversarial attack. We implement a hybridized version of the fast gradient sign and basic iterative methods to attack the input image and fool the agent. We've shown that our method reduces the attack time per frame to within 3 milliseconds.","",""
0,"Jing Li, Aidong Deng, Yong Yang, Jing Zhu, Minqiang Deng","Research on the Method of Rub-Impact Fault Recognition Based on the Conditional Generative Adversarial Nets and Acoustic Emission",2020,"","","","",54,"2022-07-13 09:19:22","","10.1109/ICSMD50554.2020.9261705","","",,,,,0,0.00,0,5,2,"A large number of effective annotated data is the key support for learning a fault diagnosis model of mechanical equipment. However, the existing practical samples used for training fault classifiers are usually small and interfered by noise .According to this problem the paper presents a rub impact fault recognition method based on the Conditional Generative Adversarial Nets (CGAN) and Acoustic Emission (AE) technology. The data are from the Wind Turbine Train test bed. AE features are extracted from various views such as time, frequency and energy intensity under different operation state. The proposed CGAN model adds useful auxiliary information into each layer of GAN generation model to improve the quality of generated pseudo-samples. It is further to evaluate the probability of samples from the training set or real set. The experimental results show that this model can effectively identify the rub impact fault and have strong robustness. It is an effective way to solve the problem of inadequate sample and improve the recognition performance of rub impact fault.","",""
0,"Dongfang Li, Dongfang Li, Baotian Hu, Qingcai Chen, Tujie Xu, Jingcong Tao, Yunan Zhang","Unifying Model Explainability and Robustness for Joint Text Classification and Rationale Extraction",2021,"","","","",55,"2022-07-13 09:19:22","","10.1609/aaai.v36i10.21342","","",,,,,0,0.00,0,7,1,"Recent works have shown explainability and robustness are two crucial ingredients of trustworthy and reliable text classification. However, previous works usually address one of two aspects: i) how to extract accurate rationales for explainability while being beneficial to prediction; ii) how to make the predictive model robust to different types of adversarial attacks. Intuitively, a model that produces helpful explanations should be more robust against adversarial attacks, because we cannot trust the model that outputs explanations but changes its prediction under small perturbations. To this end, we propose a joint classification and rationale extraction model named AT-BMC. It includes two key mechanisms: mixed Adversarial Training (AT) is designed to use various perturbations in discrete and embedding space to improve the model’s robustness, and Boundary Match Constraint (BMC) helps to locate rationales more precisely with the guidance of boundary information. Performances on benchmark datasets demonstrate that the proposed AT-BMC outperforms baselines on both classification and rationale extraction by a large margin. Robustness analysis shows that the proposed AT-BMC decreases the attack success rate effectively by up to 69%. The results indicate that there are connections between robust models and better explanations.","",""
0,"Pei Huang, Yuting Yang, Fuqi Jia, Minghao Liu, Feifei Ma, Jian Zhang","Word Level Robustness Enhancement: Fight Perturbation with Perturbation",2022,"","","","",56,"2022-07-13 09:19:22","","10.1609/aaai.v36i10.21324","","",,,,,0,0.00,0,6,1,"State-of-the-art deep NLP models have achieved impressive improvements on many tasks. However, they are found to be vulnerable to some perturbations. Before they are widely adopted, the fundamental issues of robustness need to be addressed. In this paper, we design a robustness enhancement method to defend against word substitution perturbation, whose basic idea is to fight perturbation with perturbation. We find that: although many well-trained deep models are not robust in the setting of the presence of adversarial samples, they satisfy weak robustness. That means they can handle most non-crafted perturbations well. Taking advantage of the weak robustness property of deep models, we utilize non-crafted perturbations to resist the adversarial perturbations crafted by attackers. Our method contains two main stages. The first stage is using randomized perturbation to conform the input to the data distribution. The second stage is using randomized perturbation to eliminate the instability of prediction results and enhance the robustness guarantee. Experimental results show that our method can significantly improve the ability of deep models to resist the state-of-the-art adversarial attacks while maintaining the prediction performance on the original clean data.","",""
0,"Emanuele La Malfa, M. Kwiatkowska","The King is Naked: on the Notion of Robustness for Natural Language Processing",2021,"","","","",57,"2022-07-13 09:19:22","","10.1609/aaai.v36i10.21353","","",,,,,0,0.00,0,2,1,"There is growing evidence that the classical notion of adversarial robustness originally introduced for images has been adopted as a de facto standard by a large part of the NLP research community.  We show that this notion is problematic in the context of NLP as it considers a narrow spectrum of linguistic phenomena. In this paper, we argue for semantic robustness, which is better aligned with the human concept of linguistic fidelity. We characterize semantic robustness in terms of biases that it is expected to induce in a model. We study semantic robustness of a range of vanilla and robustly trained architectures using a template-based generative test bed. We complement the analysis with empirical evidence that, despite being harder to implement, semantic robustness can improve performance %gives guarantees for on complex linguistic phenomena where models robust in the classical sense fail.","",""
1,"Zuzanna Klawikowska, Agnieszka Mikołajczyk, M. Grochowski","Explainable AI for Inspecting Adversarial Attacks on Deep Neural Networks",2020,"","","","",58,"2022-07-13 09:19:22","","10.1007/978-3-030-61401-0_14","","",,,,,1,0.50,0,3,2,"","",""
1,"Qi Xuan, Yalu Shan, Jinhuan Wang, Zhongyuan Ruan, Guanrong Chen","Adversarial attack on BC classification for scale-free networks.",2020,"","","","",59,"2022-07-13 09:19:22","","10.1063/5.0003707","","",,,,,1,0.50,0,5,2,"Adversarial attacks have been alerting the artificial intelligence community recently since many machine learning algorithms were found vulnerable to malicious attacks. This paper studies adversarial attacks on Broido and Clauset classification for scale-free networks to test its robustness in terms of statistical measures. In addition to the well-known random link rewiring (RLR) attack, two heuristic attacks are formulated and simulated: degree-addition-based link rewiring (DALR) and degree-interval-based link rewiring (DILR). These three strategies are applied to attack a number of strong scale-free networks of various sizes generated from the Barabási-Albert model and the uncorrelated configuration model. It is found that both DALR and DILR are more effective than RLR in the sense that rewiring a smaller number of links can succeed in the same attack. However, DILR is as concealed as RLR in the sense that they both are introducing a relatively small change on several typical structural properties, such as the average shortest path-length, the average clustering coefficient, the average diagonal distance, and the Kolmogorov-Smirnov test of the degree distribution. The results of this paper suggest that to classify a network to be scale-free, one has to be very careful from the viewpoint of adversarial attack effects.","",""
0,"Nyee Thoang Lim, Meng Yi Kuan, Muxin Pu, Mei Kuan Lim, Chun Yong Chong","Metamorphic Testing-based Adversarial Attack to Fool Deepfake Detectors",2022,"","","","",60,"2022-07-13 09:19:22","","10.48550/arXiv.2204.08612","","",,,,,0,0.00,0,5,1,"—Deepfakes utilise Artiﬁcial Intelligence (AI) tech- niques to create synthetic media where the likeness of one person is replaced with another. There are growing concerns that deepfakes can be maliciously used to create misleading and harmful digital contents. As deepfakes become more common, there is a dire need for deepfake detection technology to help spot deepfake media. Present deepfake detection models are able to achieve outstanding accuracy ( > 90%). However, most of them are limited to within-dataset scenario. Most models do not gener- alise well enough in cross-dataset scenario. Furthermore, state-of-the-art deepfake detection models rely on neural network- based classiﬁcation models that are known to be vulnerable to adversarial attacks. Motivated by the need for a robust deepfake detection model, this study adapts metamorphic testing (MT) principles to help identify potential factors that could inﬂuence the robustness of the examined model, while overcoming the test oracle problem in this domain. Metamorphic testing is speciﬁcally chosen as the testing technique as it ﬁts our demand to address learning-based system testing with probabilistic outcomes from largely black-box components, based on potentially large input domains. We performed our evaluations on MesoInception-4 and TwoStreamNet models, which are the state-of-the-art deepfake detection models. This study identiﬁed makeup application as an adversarial attack that could fool deepfake detectors. Our experimental results demonstrate that both the MesoInception-4 and TwoStreamNet models degrade in their performance by up to 30% when the input data is perturbed with makeup.","",""
0,"S. Asha, P. Vinod","Evaluation of adversarial machine learning tools for securing AI systems",2021,"","","","",61,"2022-07-13 09:19:22","","10.1007/s10586-021-03421-1","","",,,,,0,0.00,0,2,1,"","",""
0,"Mohammad Khalooei, M. Homayounpour, M. Amirmazlaghani","Layer-wise Regularized Adversarial Training using Layers Sustainability Analysis (LSA) framework",2022,"","","","",62,"2022-07-13 09:19:22","","","","",,,,,0,0.00,0,3,1,"Deep neural network models are used today in various applications of artificial intelligence, the strengthening of which, in the face of adversarial attacks is of particular importance. An appropriate solution to adversarial attacks is adversarial training, which reaches a trade-off between robustness and generalization. This paper introduces a novel framework (Layer Sustainability Analysis (LSA)) for the analysis of layer vulnerability in an arbitrary neural network in the scenario of adversarial attacks. LSA can be a helpful toolkit to assess deep neural networks and to extend the adversarial training approaches towards improving the sustainability of model layers via layer monitoring and analysis. The LSA framework identifies a list of Most Vulnerable Layers (MVL list) of the given network. The relative error, as a comparison measure, is used to evaluate representation sustainability of each layer against adversarial inputs. The proposed approach for obtaining robust neural networks to fend off adversarial attacks is based on a layer-wise regularization (LR) over LSA proposal(s) for adversarial training (AT); i.e. the AT-LR procedure. AT-LR could be used with any benchmark adversarial attack to reduce the vulnerability of network layers and to improve conventional adversarial training approaches. The proposed idea performs well theoretically and experimentally for state-of-the-art multilayer perceptron and convolutional neural network architectures. Compared with the AT-LR and its corresponding base adversarial training, the classification accuracy of more significant perturbations increased by 16.35%, 21.79%, and 10.730% on Moon, MNIST, and CIFAR-10 benchmark datasets, respectively. The LSA framework is available and published at https://github.com/khalooei/LSA.","",""
0,"Ngoc Dung Huynh, Mohamed Reda Bouadjenek, I. Razzak, Kevin Lee, C. Arora, Ali Hassani, A. Zaslavsky","Adversarial Attacks on Speech Recognition Systems for Mission-Critical Applications: A Survey",2022,"","","","",63,"2022-07-13 09:19:22","","","","",,,,,0,0.00,0,7,1,"A Machine-Critical Application is a system that is fundamentally necessary to the success of specific and sensitive operations such as search and recovery, rescue, military, and emergency management actions. Recent advances in Machine Learning, Natural Language Processing, voice recognition, and speech processing technologies have naturally allowed the development and deployment of speech-based conversational interfaces to interact with various machine-critical applications. While these conversational interfaces have allowed users to give voice commands to carry out strategic and critical activities, their robustness to adversarial attacks remains uncertain and unclear. Indeed, Adversarial Artificial Intelligence (AI) which refers to a set of techniques that attempt to fool machine learning models with deceptive data, is a growing threat in the AI and machine learning research community, in particular for machine-critical applications. The most common reason of adversarial attacks is to cause a malfunction in a machine learning model. An adversarial attack might entail presenting a model with inaccurate or fabricated samples as it’s training data, or introducing maliciously designed data to deceive an already trained model. While focusing on speech recognition for machine-critical applications, in this paper, we first review existing speech recognition techniques, then, we investigate the effectiveness of adversarial attacks and defenses against these systems, before outlining research challenges, defense recommendations, and future work. This paper is expected to serve researchers and practitioners as a reference to help them in understanding the challenges, position themselves and, ultimately, help them to improve existing models of speech recognition for mission-critical applications.","",""
0,"M. Dedicatoria, S. Klaus, R. Case, S. Na, E. Ludwick, D. Wu, L. Quattrochi","AI detection of M. Tuberculosis pathogens using Generative Adversarial Network (GAN) analyses",2020,"","","","",64,"2022-07-13 09:19:22","","10.1093/eurpub/ckaa165.320","","",,,,,0,0.00,0,7,2,"      Rapid identification of pathogens is critical to outbreak detection and sentinel surveillance; however most diagnoses are made in laboratory settings. Advancements in artificial intelligence (AI) and computer vision offer unprecedented opportunities to facilitate detection and reduce response time in field settings. An initial step is the creation of analysis algorithms for offline mobile computing applications.        AI models to identify objects using computer vision are typically “trained” on previously labeled images. The scarcity of labeled image-libraries creates a bottleneck, requiring thousands of labor hours to annotate images by hand to create “training data.” We describe the applicability of Generative Adversarial Network (GAN) methods to amass sufficient training data with minimal manual input.        Our AI models are built with a performance score of 0.84-0.93 for M. Tuberculosis, a measure of the AI model's accuracy using precision and recall. Our results demonstrate that our GAN pipeline boosts model robustness and learnability of sparse open source data.        The use of labeled training data to identify M. Tuberculosis developed using our GAN pipeline techniques demonstrates the potential for rapid identification of known pathogens in field settings. Our work paves the way for the development of offline mobile computing applications to identify pathogens outside of a laboratory setting. Advancements in artificial intelligence (AI) and computer vision offer unprecedented opportunities to decrease detection time in field settings by combining these technologies. Further development of these capabilities can improve time-to-detection and outbreak response significantly.        Rapidly deploy AI detectors to aid in disease outbreak and surveillance. Our concept aligns with deploying responsive alerting capabilities to address dynamic threats in low resource, offline computing environs. ","",""
0,"Hatma Suryotrisongko, Y. Musashi, A. Tsuneda, K. Sugitani","Robust Botnet DGA Detection: Blending XAI and OSINT for Cyber Threat Intelligence Sharing",2022,"","","","",65,"2022-07-13 09:19:22","","10.1109/ACCESS.2022.3162588","","",,,,,0,0.00,0,4,1,"We investigated 12 years DNS query logs of our campus network and identified phenomena of malicious botnet domain generation algorithm (DGA) traffic. DGA-based botnets are difficult to detect using cyber threat intelligence (CTI) systems based on blocklists. Artificial intelligence (AI)/machine learning (ML)-based CTI systems are required. This study (1) proposed a model to detect DGA-based traffic based on statistical features with datasets comprising 55 DGA families, (2) discussed how CTI can be expanded with computable CTI paradigm, and (3) described how to improve the explainability of the model outputs by blending explainable AI (XAI) and open-source intelligence (OSINT) for trust problems, an antidote for skepticism to the shared models and preventing automation bias. We define the XAI-OSINT blending as aggregations of OSINT for AI/ML model outcome validation. Experimental results show the effectiveness of our models (96.3% accuracy). Our random forest model provides better robustness against three state-of-the-art DGA adversarial attacks (CharBot, DeepDGA, MaskDGA) compared with character-based deep learning models (Endgame, CMU, NYU, MIT). We demonstrate the sharing mechanism and confirm that the XAI-OSINT blending improves trust for CTI sharing as evidence to validate our proposed computable CTI paradigm to assist security analysts in security operations centers using an automated, explainable OSINT approach (for second opinion). Therefore, the computable CTI reduces manual intervention in critical cybersecurity decision-making.","",""
4,"Pavel Rytír, L. Chrpa, B. Bosanský","Using Classical Planning in Adversarial Problems",2019,"","","","",66,"2022-07-13 09:19:22","","10.1109/ICTAI.2019.00185","","",,,,,4,1.33,1,3,3,"Many problems from classical planning are applied in the environment with other, possibly adversarial agents. However, plans found by classical planning algorithms lack the robustness against the actions of other agents - the quality of computed plans can be significantly worse compared to the model. To explicitly reason about other (adversarial) agents, the game-theoretic framework can be used. The scalability of game-theoretic algorithms, however, is limited and often insufficient for real-world problems. In this paper, we combine classical domain-independent planning algorithms and game-theoretic strategy-generation algorithm where plans form strategies in the game. Our contribution is threefold. First, we provide the methodology for using classical planning in this game-theoretic framework. Second, we analyze the trade-off between the quality of the planning algorithm and the robustness of final randomized plans and the computation time. Finally, we analyze different variants of integration of classical planning algorithms into the game-theoretic framework and show that at the cost a minor loss in the robustness of final plans, we can significantly reduce the computation time.","",""
10,"Yi Huang, A. Kong, Kwok-Yan Lam","Adversarial Signboard against Object Detector",2019,"","","","",67,"2022-07-13 09:19:22","","","","",,,,,10,3.33,3,3,3,"Object detector is an indispensable component in many computer vision and artificial intelligence systems, such as autonomous robot and image analyzer for profiling social media users. Analyzing its vulnerabilities is essential for detecting and preventing attacks and minimizing potential loss. Researchers have proposed a number of adversarial examples to evaluate the robustness of object detectors. All these adversarial examples change pixels inside target objects to carry out attacks but only some of them are suitable for physical attacks. According to the best knowledge of the authors, no published work successfully attacks object detector without changing pixels inside the target object. In an unpublished work, the authors designed an adversarial border which tightly surrounds target object and successfully misleads Faster R-CNN and YOLOv3 digitally and physically. Adversarial border does not change pixels inside target object but makes it look weird. In this paper, a new adversarial example named adversarial signboard, which looks like a signboard, is proposed. By putting it below a target object, it can mislead the state-of-the-art object detectors. Using stop sign as a target object, adversarial signboard is evaluated on 48 videos with totally 5416 frames. The experimental results show that adversarial signboard derived from Faster R-CNN with ResNet-101 as a backbone network can mislead Faster R-CNN with a different backbone network, Mask R-CNN, YOLOv3 and R-FCN digitally and physically.","",""
2,"Salah-ud-din Farooq, M. Usama, Junaid Qadir, M. Imran","Adversarial ML Attack on Self Organizing Cellular Networks",2019,"","","","",68,"2022-07-13 09:19:22","","10.1109/UCET.2019.8881842","","",,,,,2,0.67,1,4,3,"Deep Neural Networks (DNN) have been widely adopted in self-organizing networks (SON) for automating different networking tasks. Recently, it has been shown that DNN lack robustness against adversarial examples where an adversary can fool the DNN model into incorrect classification by introducing a small imperceptible perturbation to the original example. SON is expected to use DNN for multiple fundamental cellular tasks and many DNN-based solutions for performing SON tasks have been proposed in the literature have not been tested against adversarial examples. In this paper, we have tested and explained the robustness of SON against adversarial example and investigated the performance of an important SON use case in the face of adversarial attacks. We have also generated explanations of incorrect classifications by utilizing an explainable artificial intelligence (AI) technique.","",""
0,"Di Jin, Zhigang Li, Liang Yang, Dongxiao He, Pengfei Jiao, Lu Zhai","Adversarial Capsule Learning for Network Embedding",2019,"","","","",69,"2022-07-13 09:19:22","","10.1109/ICTAI.2019.00038","","",,,,,0,0.00,0,6,3,"The purpose of network embedding is to learn a low-dimensional representation for each node in the network. One can then use this low-dimensional representation to solve some network analysis tasks, such as node classification and node clustering. At present, there are several network embedding learning methods based on GAN (Generative Adversarial Networks) to enhance the robustness of representations. However, these methods have two drawbacks. First, they are often too difficult to be trained stably. Second, they only learn the robust representations by matching the posterior distribution of the latent representations to the given priors. On the contrary, Capsule Networks can learn a more equivariant representation of images that is more robust to the changes in pose and spatial relationships of parts of objects in images. However, there is still no research using Capsule Network for network embedding since the social network is essentially different from images. For this problem, we propose a new approach of adversarial capsule learning (ACL) for network embedding, which is the first time to use Capsule Network in the network analysis tasks. To be specific, the new model consists of two parts, a generator and discriminator. We use Graph Convolutional Networks (GCN) as the generator to learn the embedding of nodes, and use Capsule Network as the discriminator to distinguish between the real and fake samples as accurately as possible. The experimental results demonstrate the effectiveness of the proposed new method.","",""
1,"Ling Liu","Deception, Robustness and Trust in Big Data Fueled Deep Learning Systems",2019,"","","","",70,"2022-07-13 09:19:22","","10.1109/BigData47090.2019.9005597","","",,,,,1,0.33,1,1,3,"We are entering an exciting era where human intelligence is being enhanced by machine intelligence through big data fueled artificial intelligence (AI) and machine learning (ML). However, recent work shows that DNN models trained privately are vulnerable to adversarial inputs. Such adversarial inputs inject small amount of perturbations to the input data to fool machine learning models to misbehave, turning a deep neural network against itself. As new defense methods are proposed, more sophisticated attack algorithms are surfaced. This arms race has been ongoing since the rise of adversarial machine learning. This keynote provides a comprehensive analysis and characterization of the most representative attacks and their defenses. As more and more mission critical systems are incorporating machine learning and AI as an essential component in their real-world big data applications and their big data service provisioning platforms or products, understanding and ensuring the verifiable robustness of deep learning becomes a pressing challenge in the presence of adversarial attacks. This includes (1) the development of formal metrics to quantitatively evaluate and measure the robustness of a DNN prediction with respect of intentional and unintentional artifacts and deceptions, (2) the comprehensive understanding of the blind spots and the invariants in the DNN trained models and the DNN training process, and (3) the statistical measurement of trust and distrust that we can place on a deep learning algorithm to perform reliably and truthfully. In this keynote talk, I will use empirical analysis and evaluation of our cross-layer strategic teaming defense framework and techniques to illustrate the feasibility of ensuring robust deep learning.","",""
55,"Sayak Paul, Pin-Yu Chen","Vision Transformers are Robust Learners",2021,"","","","",71,"2022-07-13 09:19:22","","10.1609/aaai.v36i2.20103","","",,,,,55,55.00,28,2,1,"Transformers, composed of multiple self-attention layers, hold strong promises toward a generic learning primitive applicable to different data modalities, including the recent breakthroughs in computer vision achieving state-of-the-art (SOTA) standard accuracy. What remains largely unexplored is their robustness evaluation and attribution. In this work, we study the robustness of the Vision Transformer (ViT) (Dosovitskiy et al. 2021) against common corruptions and perturbations, distribution shifts, and natural adversarial examples. We use six different diverse ImageNet datasets concerning robust classification to conduct a comprehensive performance comparison of ViT(Dosovitskiy et al. 2021) models and SOTA convolutional neural networks (CNNs), Big-Transfer (Kolesnikov et al. 2020). Through a series of six systematically designed experiments, we then present analyses that provide both quantitative andqualitative indications to explain why ViTs are indeed more robust learners. For example, with fewer parameters and similar dataset and pre-training combinations, ViT gives a top-1accuracy of 28.10% on ImageNet-A which is 4.3x higher than a comparable variant of BiT. Our analyses on image masking, Fourier spectrum sensitivity, and spread on discrete cosine energy spectrum reveal intriguing properties of ViT attributing to improved robustness. Code for reproducing our experiments is available at https://git.io/J3VO0.","",""
4,"Jiarong Xu, Junru Chen, Yang Yang, Yizhou Sun, Chunping Wang, Jiangang Lu","Unsupervised Adversarially-Robust Representation Learning on Graphs",2020,"","","","",72,"2022-07-13 09:19:22","","10.1609/aaai.v36i4.20349","","",,,,,4,2.00,1,6,2,"Unsupervised/self-supervised pre-training methods for graph representation learning have recently attracted increasing research interests, and they are shown to be able to generalize to various downstream applications. Yet, the adversarial robustness of such pre-trained graph learning models remains largely unexplored. More importantly, most existing defense techniques designed for end-to-end graph representation learning methods require pre-specified label definitions, and thus cannot be directly applied to the pre-training methods. In this paper, we propose an unsupervised defense technique to robustify pre-trained deep graph models, so that the perturbations on the input graph can be successfully identified and blocked before the model is applied to different downstream tasks. Specifically, we introduce a mutual information-based measure, graph representation vulnerability (GRV), to quantify the robustness of graph encoders on the representation space. We then formulate an optimization problem to learn the graph representation by carefully balancing the trade-off between the expressive power and the robustness (i.e., GRV) of the graph encoder. The discrete nature of graph topology and the joint space of graph data make the optimization problem intractable to solve. To handle the above difficulty and to reduce computational expense, we further relax the problem and thus provide an approximate solution. Additionally, we explore a provable connection between the robustness of the unsupervised graph encoder and that of models on downstream tasks. Extensive experiments demonstrate that even without access to labels and tasks, our model is still able to enhance robustness against adversarial attacks on three downstream tasks (node classification, link prediction, and community detection) by an average of +16.5% compared with existing methods.","",""
3,"Da Xu, Yuting Ye, Chuanwei Ruan, Bo Yang","Towards Robust Off-policy Learning for Runtime Uncertainty",2022,"","","","",73,"2022-07-13 09:19:22","","10.1609/aaai.v36i9.21249","","",,,,,3,3.00,1,4,1,"Off-policy learning plays a pivotal role in optimizing and evaluating policies prior to the online deployment. However, during the real-time serving, we observe varieties of interventions and constraints that cause inconsistency between the online and offline setting, which we summarize and term as runtime uncertainty. Such uncertainty cannot be learned from the logged data due to its abnormality and rareness nature. To assert a certain level of robustness, we perturb the off-policy estimators along an adversarial direction in view of the runtime uncertainty. It allows the resulting estimators to be robust not only to observed but also unexpected runtime uncertainties. Leveraging this idea, we bring runtime-uncertainty robustness to three major off-policy learning methods: the inverse propensity score method, reward-model method, and doubly robust method. We theoretically justify the robustness of our methods to runtime uncertainty, and demonstrate their effectiveness using both the simulation and the real-world online experiments.","",""
3,"Satya M. Muddamsetty, M. N. Jahromi, Andreea-Emilia Ciontos, Laura M. Fenoy, T. Moeslund","Introducing and assessing the explainable AI (XAI)method: SIDU",2021,"","","","",74,"2022-07-13 09:19:22","","","","",,,,,3,3.00,1,5,1,"Explainable Artificial Intelligence (XAI) has in recent years become a well-suited framework to generate human understandable explanations of black box models. In this paper, we present a novel XAI visual explanation algorithm denoted SIDU that can effectively localize entire object regions responsible for prediction in a full extend. We analyze its robustness and effectiveness through various computational and human subject experiments. In particular, we assess the SIDU algorithm using three different types of evaluations (Application, Human and Functionally-Grounded) to demonstrate its superior performance. The robustness of SIDU is further studied in presence of adversarial attack on black box models to better understand its performance.","",""
2,"Szu-Hao Huang, Yu-Hsiang Miao, Yi-Ting Hsiao","Novel Deep Reinforcement Algorithm With Adaptive Sampling Strategy for Continuous Portfolio Optimization",2021,"","","","",75,"2022-07-13 09:19:22","","10.1109/ACCESS.2021.3082186","","",,,,,2,2.00,1,3,1,"Quantitative trading targets favorable returns by determining patterns in historical data through statistical or mathematical approaches. With advances in artificial intelligence, many studies have indicated that deep reinforcement learning (RL) can perform well in quantitative trading by predicting price change trends in the financial market. However, most of the related frameworks display poor generalizability in the testing stage. Thus, we incorporated adversarial learning and a novel sampling strategy for RL portfolio management. The goal was to construct a portfolio comprising five assets from the constituents of the Dow Jones Industrial Average and to achieve excellent performance through our trading strategy. We used adversarial learning during the RL process to enhance the model’s robustness. Moreover, to improve the model’s computational efficiency, we introduced a novel sampling strategy to determine which data are worth learning by observing the learning condition. The experimental results revealed that the model with our sampling strategy had more favorable performance than the random learning strategy. The Sharpe ratio increased by 6 %–7 %, and profit increased by nearly 45 %. Thus, our proposed learning framework and the sampling strategy we employed are conducive to obtaining reliable trading rules.","",""
2,"Shubham Sharma, A. Gee, D. Paydarfar, J. Ghosh","FaiR-N: Fair and Robust Neural Networks for Structured Data",2020,"","","","",76,"2022-07-13 09:19:22","","10.1145/3461702.3462559","","",,,,,2,1.00,1,4,2,"Fairness and robustness in machine learning are crucial when individuals are subject to automated decisions made by models in high-stake domains. To promote ethical artificial intelligence, fairness metrics that rely on comparing model error rates across subpopulations have been widely investigated for the detection and mitigation of bias. However, fairness measures that rely on comparing the ability to achieve recourse have been relatively unexplored. In this paper, we present a novel formulation for training neural networks that considers the distance of data observations to the decision boundary such that the new objective: (1) reduces the disparity in the average ability of recourse between individuals in each protected group, and (2) increases the average distance of data points to the boundary to promote adversarial robustness. We demonstrate that models trained with this new objective are more fair and adversarially robust neural networks, with similar accuracies, when compared to models without it. We also investigate a trade-off between the recourse-based fairness and robustness objectives. Moreover, we qualitatively motivate and empirically show that reducing recourse disparity across protected groups also improves fairness measures that rely on error rates. To the best of our knowledge, this is the first time that recourse disparity across groups are considered to train fairer neural networks.","",""
1,"Fernando Martínez-Plumed, David Castellano Falcón, Carlos Monserrat Aranda, J. Hernández-Orallo","When AI Difficulty Is Easy: The Explanatory Power of Predicting IRT Difficulty",2022,"","","","",77,"2022-07-13 09:19:22","","10.1609/aaai.v36i7.20739","","",,,,,1,1.00,0,4,1,"One of challenges of artificial intelligence as a whole is robustness. Many issues such as adversarial examples, out of distribution performance, Clever Hans phenomena, and the wider areas of AI evaluation and explainable AI, have to do with the following question: Did the system fail because it is a hard instance or because something else? In this paper we address this question with a generic method for estimating IRT-based instance difficulty for a wide range of AI domains covering several areas, from supervised feature-based classification to automated reasoning. We show how to estimate difficulty systematically using off-the-shelf machine learning regression models. We illustrate the usefulness of this estimation for a range of applications.","",""
1,"Javier Maroto, Gérôme Bovet, P. Frossard","On the benefits of robust models in modulation recognition",2021,"","","","",78,"2022-07-13 09:19:22","","10.1117/12.2587156","","",,,,,1,1.00,0,3,1,"Given the rapid changes in telecommunication systems and their higher dependence on artificial intelligence, it is increasingly important to have models that can perform well under different, possibly adverse, conditions. Deep Neural Networks (DNNs) using convolutional layers are state-of-the-art in many tasks in communications. However, in other domains, like image classification, DNNs have been shown to be vulnerable to adversarial perturbations, which consist of imperceptible crafted noise that when added to the data fools the model into misclassification. This puts into question the security of DNNs in communication tasks, and in particular in modulation recognition. We propose a novel framework to test the robustness of current state-of-the-art models where the adversarial perturbation strength is dependent on the signal strength and measured with the “signal to perturbation ratio” (SPR). We show that current state-of-the-art models are susceptible to these perturbations. In contrast to current research on the topic of image classification, modulation recognition allows us to have easily accessible insights on the usefulness of the features learned by DNNs by looking at the constellation space. When analyzing these vulnerable models we found that adversarial perturbations do not shift the symbols towards the nearest classes in constellation space. This shows that DNNs do not base their decisions on signal statistics that are important for the Bayes-optimal modulation recognition model, but spurious correlations in the training data. Our feature analysis and proposed framework can help in the task of finding better models for communication systems.","",""
1,"Wenqian Shang, Sunyu Zhu, Dong Xiao","Research On Human-computer Dialogue Based On Improved Seq2seq Model",2021,"","","","",79,"2022-07-13 09:19:22","","10.1109/icisfall51598.2021.9627419","","",,,,,1,1.00,0,3,1,"With the constant maturity of deep learning technology, human-computer dialogue has become a research hotspot in natural language processing. People in academia and industry are very concerned about it. The extensive use of artificial intelligence and deep learning technology in the human-machine dialogue system and the deep neural network modeling for text semantics are of great significance in promoting human-computer dialogue technologies and the application of human-computer dialogue to serve humanity better. Based on the above background, this paper focuses on the research of the human-computer dialogue system based on the improved seq2seq model, using the pre-trained Bert improved model as the codec modeling, and addressing the lack of Q&A data sets, the imbalance of category distribution, and the robustness of the model. These problems can be solved by adding disturbance structure adversarial sample training.","",""
0,"Lijie Wang, Hao Liu, Shu-ping Peng, Hongxuan Tang, Xinyan Xiao, Ying Chen, Hua Wu, Haifeng Wang","DuTrust: A Sentiment Analysis Dataset for Trustworthiness Evaluation",2021,"","","","",80,"2022-07-13 09:19:22","","","","",,,,,0,0.00,0,8,1,"While deep learning models have greatly improved the performance of most artificial intelligence tasks, they are often criticized to be untrustworthy due to the black-box problem. Consequently, many works have been proposed to study the trustworthiness of deep learning. However, as most open datasets are designed for evaluating the accuracy of model outputs, there is still a lack of appropriate datasets for evaluating the inner workings of neural networks. The lack of datasets obviously hinders the development of trustworthiness research. Therefore, in order to systematically evaluate the factors for building trustworthy systems, we propose a novel and well-annotated sentiment analysis dataset to evaluate robustness and interpretability. To evaluate these factors, our dataset contains diverse annotations about the challenging distribution of instances, manual adversarial instances and sentiment explanations. Several evaluation metrics are further proposed for interpretability and robustness. Based on the dataset and metrics, we conduct comprehensive comparisons for the trustworthiness of three typical models, and also study the relations between accuracy, robustness and interpretability. We release this trustworthiness evaluation dataset at https://github/xyz and hope our work can facilitate the progress on building more trustworthy systems for real-world applications.","",""
0,"A. Papandreou, A. Kloukiniotis, A. Lalos, K. Moustakas","Deep multi-modal data analysis and fusion for robust scene understanding in CAVs",2021,"","","","",81,"2022-07-13 09:19:22","","10.1109/MMSP53017.2021.9733604","","",,,,,0,0.00,0,4,1,"Deep learning (DL) tends to be the integral part of Autonomous Vehicles (AVs). Therefore the development of scene analysis modules that are robust to various vulnerabilities such as adversarial inputs or cyber-attacks is becoming an imperative need for the future AV perception systems. In this paper, we deal with this issue by exploring the recent progress in Artificial Intelligence (AI) and Machine Learning (ML) to provide holistic situational awareness and eliminate the effect of the previous attacks on the scene analysis modules. We propose novel multi-modal approaches against which achieve robustness to adversarial attacks, by appropriately modifying the analysis Neural networks and by utilizing late fusion methods. More specifically, we propose a holistic approach by adding new layers to a 2D segmentation DL model enhancing its robustness to adversarial noise. Then, a novel late fusion technique has been applied, by extracting direct features from the 3D space and project them into the 2D segmented space for identifying inconsistencies. Extensive evaluation studies using the KITTI odometry dataset provide promising performance results under various types of noise.","",""
0,"Lujun Li, Ludwig Kurzinger, Tobias Watzel, G. Rigoll","A Global Discriminant Joint Training Framework for Robust Speech Recognition",2021,"","","","",82,"2022-07-13 09:19:22","","10.1109/ICTAI52525.2021.00088","","",,,,,0,0.00,0,4,1,"Robustness in adverse acoustic conditions is critical for practical human-machine interaction. A common solution for this problem is adding an independent speech enhancement front-end. Nonetheless, due to being trained separately from the automatic speech recognition (ASR) module, the independent enhancement front-end falls into the sub-optimum easily. Besides, the handcrafted loss function of the enhancement module tends to introduce unseen distortions, which even degrade the ASR performance. To address this concern, a promising idea of the joint training is progressively drawing more interests. Nevertheless, none of the previously proposed joint-training frameworks is built on the increasingly popular self-attention mechanism or generative adversarial architecture. This paper proposes a novel joint-training framework, concatenating a speech enhancement generative adversarial network as the front-end and a self-attention based ASR module as the back-end to be jointly trained as an extensive network, to boost the noise robustness of the end-to-end ASR system. A Sinc convolution layer is usefully merged into the speech enhancement front-end for more representative features extraction. Moreover, a discriminant component plays the role of the local guide of the enhancement module and the global guide in the joint training simultaneously, which guides the enhancement front-end to output more desirable features for the subsequent ASR module and thereby offsets the limitation of the separate training and handcrafted loss functions.Systematic experiments reveal that the proposed framework significantly overtakes other competitive solutions, especially in challenging environments.","",""
0,"Dejiang Xu, M. Lee, W. Hsu","Classification with Dynamic Data Augmentation",2021,"","","","",83,"2022-07-13 09:19:22","","10.1109/ICTAI52525.2021.00228","","",,,,,0,0.00,0,3,1,"Data augmentation has improved the accuracy and robustness of deep neural networks. Research has focused on finding an optimal augmentation policy that generates good quality training images to improve classification accuracy. However, searching for this optimal augmentation policy is computationally expensive and is dependent on the neural architecture. In this work, we design a dynamic augmentation approach that automatically adjusts the number of transformation operations and their magnitudes during the training of deep neural networks. We also address the shift in the test data distribution by proposing to perform augmentation on the test data. We validate the effectiveness of our solution on CIFAR-10, CIFAR-100, ImageNet, and the perturbed datasets including CIFAR-10-C, CIFAR-100-C, ImageNet-A, ImageNet-C and ImageNet-P. Experiment results show that our proposed dynamic augmentation approach is scalable and gives good performances on clean, adversarial and corrupt datasets, reducing the best published results by a significant margin.","",""
0,"Timothy Zee, Alexander Ororbia, A. Mali, Ifeoma Nwogu","A Robust Backpropagation-Free Framework for Images",2022,"","","","",84,"2022-07-13 09:19:22","","10.48550/arXiv.2206.01820","","",,,,,0,0.00,0,4,1,"While current deep learning algorithms have been successful for a wide variety of artiﬁcial intelligence (AI) tasks, including those involving structured image data, they present deep neurophysiological conceptual issues due to their reliance on the gradients computed by backpropagation of errors (backprop) to obtain synaptic weight adjustments; hence are biologically implausible. We present a more biologically plausible approach, the error-kernel driven activation alignment (EKDAA) algorithm, to train convolution neural networks (CNNs) using locally derived error transmission kernels and error maps. We demonstrate the efﬁcacy of EKDAA by performing the task of visual-recognition on the Fashion MNIST, CIFAR-10 and SVHN benchmarks as well as conducting blackbox robustness tests on adversarial examples derived from these datasets. Furthermore, we also present results for a CNN trained using a non-differentiable activation function. All recognition results nearly matches that of backprop and exhibit greater adversarial robustness compared to backprop.","",""
0,"Carl Remlinger, Joseph Mikael, R. Elie","Conditional Loss and Deep Euler Scheme for Time Series Generation",2021,"","","","",85,"2022-07-13 09:19:22","","10.1609/aaai.v36i7.20782","","",,,,,0,0.00,0,3,1,"We introduce three new generative models for time series that are based on Euler discretization of Stochastic Differential Equations (SDEs) and Wasserstein metrics. Two of these methods rely on the adaptation of generative adversarial networks (GANs) to time series. The third algorithm, called Conditional Euler Generator (CEGEN), minimizes a dedicated distance between the transition probability distributions over all time steps. In the context of Itô processes, we provide theoretical guarantees that minimizing this criterion implies accurate estimations of the drift and volatility parameters. Empirically, CEGEN outperforms state-of-the-art and GANs on both marginal and temporal dynamic metrics. Besides, correlation structures are accurately identified in high dimension. When few real data points are available, we verify the effectiveness of CEGEN when combined with transfer learning methods on model-based simulations. Finally, we illustrate the robustness of our methods on various real-world data sets.","",""
0,"Lujia Bao, K. Zheng","RCC: A Paradigm for Training a Robust Chinese Text Classification Model",2022,"","","","",86,"2022-07-13 09:19:22","","10.1109/iwecai55315.2022.00090","","",,,,,0,0.00,0,2,1,"Adversarial attacks against language models have gained more and more attention in recent years, and various adversarial text generation models have been proposed. Chinese language models are more vulnerable to character-level tampering attacks due to the language nature. In this paper, we implement a set of white-box attack algorithms against Chinese text classification models, which significantly reduce the accuracy of multiple baseline models on multiple classification tasks while ensuring that one can recover the original utterance. We utilize follow strategies, and propose a paradigm to enhance the robustness of Chinese classification models: 1) generating adversarial text during training as a dynamic data augmentation, 2) introducing extra glyph and phonology information into Chinses language models.","",""
0,"Jun Zhuang, M. Hasan","Defending Graph Convolutional Networks against Dynamic Graph Perturbations via Bayesian Self-supervision",2022,"","","","",87,"2022-07-13 09:19:22","","10.48550/arXiv.2203.03762","","",,,,,0,0.00,0,2,1,"In recent years, plentiful evidence illustrates that Graph Convolutional Networks (GCNs) achieve extraordinary accomplishments on the node classification task. However, GCNs may be vulnerable to adversarial attacks on label-scarce dynamic graphs. Many existing works aim to strengthen the robustness of GCNs; for instance, adversarial training is used to shield GCNs against malicious perturbations. However, these works fail on dynamic graphs for which label scarcity is a pressing issue. To overcome label scarcity, self-training attempts to iteratively assign pseudo-labels to highly confident unlabeled nodes but such attempts may suffer serious degradation under dynamic graph perturbations. In this paper, we generalize noisy supervision as a kind of self-supervised learning method and then propose a novel Bayesian self-supervision model, namely GraphSS, to address the issue. Extensive experiments demonstrate that GraphSS can not only affirmatively alert the perturbations on dynamic graphs but also effectively recover the prediction of a node classifier when the graph is under such perturbations. These two advantages prove to be generalized over three classic GCNs across five public graph datasets.","",""
0,"Motasem Alfarra, Juan C. P'erez, Ali K. Thabet, Adel Bibi, Philip H. S. Torr, Bernard Ghanem","Combating Adversaries with Anti-Adversaries",2021,"","","","",88,"2022-07-13 09:19:22","","10.1609/aaai.v36i6.20545","","",,,,,0,0.00,0,6,1,"Deep neural networks are vulnerable to small input perturbations known as adversarial attacks. Inspired by the fact that these adversaries are constructed by iteratively minimizing the confidence of a network for the true class label, we propose the anti-adversary layer, aimed at countering this effect. In particular, our layer generates an input perturbation in the opposite direction of the adversarial one and feeds the classifier a perturbed version of the input. Our approach is training-free and theoretically supported. We verify the effectiveness of our approach by combining our layer with both nominally and robustly trained models and conduct large-scale experiments from black-box to adaptive attacks on CIFAR10, CIFAR100, and ImageNet. Our layer significantly enhances model robustness while coming at no cost on clean accuracy.","",""
0,"Deepak Ravikumar, Sangamesh Kodge, Isha Garg, K. Roy","TREND: Transferability based Robust ENsemble Design",2020,"","","","",89,"2022-07-13 09:19:22","","10.1109/tai.2022.3175172","","",,,,,0,0.00,0,4,2,"Deep Learning models hold state-of-the-art performance in many fields, but their vulnerability to adversarial examples poses a threat to their ubiquitous deployment in practical settings. Additionally, adversarial inputs generated on one classifier have been shown to transfer to other classifiers trained on similar data, which makes the attacks possible even if the model parameters are not revealed to the adversary. This property of transferability has not yet been systematically studied, leading to a gap in our understanding of robustness of neural networks to adversarial inputs. In this work, we study the effect of network architecture, initialization, input, weight and activation quantization on transferability. Our experiments reveal that transferability is significantly hampered by input quantization and architectural mismatch between source and target, is unaffected by initialization and is architecture-dependent for both weight and activation quantization. To quantify transferability, we propose a simple metric, which is a function of the attack strength. We demonstrate the utility of the proposed metric in designing a methodology to build ensembles with improved adversarial robustness. Finally, we show that an ensemble consisting of carefully chosen input quantized networks achieves better adversarial robustness than would otherwise be possible with a single network.","",""
7,"Christian Berghoff, Matthias Neu, Arndt von Twickel","Vulnerabilities of Connectionist AI Applications: Evaluation and Defense",2020,"","","","",90,"2022-07-13 09:19:22","","10.3389/fdata.2020.00023","","",,,,,7,3.50,2,3,2,"This article deals with the IT security of connectionist artificial intelligence (AI) applications, focusing on threats to integrity, one of the three IT security goals. Such threats are for instance most relevant in prominent AI computer vision applications. In order to present a holistic view on the IT security goal integrity, many additional aspects, such as interpretability, robustness and documentation are taken into account. A comprehensive list of threats and possible mitigations is presented by reviewing the state-of-the-art literature. AI-specific vulnerabilities, such as adversarial attacks and poisoning attacks are discussed in detail, together with key factors underlying them. Additionally and in contrast to former reviews, the whole AI life cycle is analyzed with respect to vulnerabilities, including the planning, data acquisition, training, evaluation and operation phases. The discussion of mitigations is likewise not restricted to the level of the AI system itself but rather advocates viewing AI systems in the context of their life cycles and their embeddings in larger IT infrastructures and hardware devices. Based on this and the observation that adaptive attackers may circumvent any single published AI-specific defense to date, the article concludes that single protective measures are not sufficient but rather multiple measures on different levels have to be combined to achieve a minimum level of IT security for AI applications.","",""
3,"Yulong Wang, Hang Su, Bo Zhang, Xiaolin Hu","Interpret Neural Networks by Extracting Critical Subnetworks",2020,"","","","",91,"2022-07-13 09:19:22","","10.1109/TIP.2020.2993098","","",,,,,3,1.50,1,4,2,"In recent years, deep neural networks have achieved excellent performance in many fields of artificial intelligence. The requirements for the interpretability and robustness of neural networks are also increasing. In this paper, we propose to understand the functional mechanism of neural networks by extracting critical subnetworks. Specifically, we denote the critical subnetworks as a group of important channels across layers such that if they were suppressed to zeros, the final test performance would deteriorate severely. This novel perspective can not only reveal the layerwise semantic behavior within the model but also present more accurate visual explanations appearing in the data through attribution methods. Moreover, we propose two adversarial example detection methods based on the properties of sample-specific and class-specific subnetworks, which provides the possibility for increasing the model robustness.","",""
1,"Mingxing Wang","Video Description with GAN",2020,"","","","",92,"2022-07-13 09:19:22","","10.1109/CCET50901.2020.9213129","","",,,,,1,0.50,1,1,2,"Video description is to convert rich information of video data into text information, Which has been attracting broad research attention in the Artificial Intelligence Community. Deep learning has given computers a strong understanding of one-dimensional picture data and two-dimensional video data. However, In real application scenarios, it still faces the problem of insufficient robustness. For example, the generated text information is unreasonable, the scene information and semantic information rich in video data cannot be extracted effectively. GAN (Generative Adversarial Nets) is a model that generates data using countermeasures, in recent years, which is widely used in text generation, dialogue system, image synthesis, etc. However, there has not been much effort on exploring GAN for Video description. In this paper, we design a new discriminant network on the basis of the traditional text description. In addition, we add the long-short Time memory networks into the model to minimize the loss of information in the encoding or decoding process, so as to generate more reasonable sentences. Experimental results demonstrate that our proposed model exceeds most video description methods in public datasets.","",""
1,"Yiwei Li, Guoliang Xu, Wanlin Li","FA: A Fast Method to Attack Real-time Object Detection Systems",2020,"","","","",93,"2022-07-13 09:19:22","","10.1109/ICCC49849.2020.9238807","","",,,,,1,0.50,0,3,2,"With the development of deep learning, image and video processing plays an important role in the age of 5G communication. However, deep neural networks are vulnerable: subtle perturbations can lead to incorrect classification results. Nowadays, adversarial attacks on artificial intelligence models have seen increasing interest. In this study, we propose a new method named FA to generate adversarial examples of object detection models. Based on the generative adversarial network (GAN), we combine the classification and location information to make the generated image look as real as possible. Experimental results on the PASCAL VOC dataset show that our method efficiently and quickly generates the image. Then, we test the transferability of adversarial samples on different datasets and object detection models such as YOLOv4, which also achieve certain transfer performance. Our work provides a basis for further exploring the defects of deep learning and improving the robustness of the systems.","",""
0,"Jiefei Wei, Qinggang Meng","AdversarialStyle: GAN Based Style Guided Verification Framework for Deep Learning Systems",2020,"","","","",94,"2022-07-13 09:19:22","","10.1109/INDIN45582.2020.9442144","","",,,,,0,0.00,0,2,2,"Verification and validation of deep learning algorithms is an important and challenging topic of artificial intelligence. Without approving by reliable and rigorous verification methods, deep learning algorithms, for instance, the convolutional neural networks, are not qualified to be used in real-world applications, especially in safety-critical areas. The gap between deep learning systems and the requirements in safety-critical application areas, such as autonomous robotics and self-driving vehicles, is the lack of Black-box V&V techniques that can test and evaluate the performance and the robustness of deep learning systems. To bridge this gap, we proposed a GAN based Black-box verification framework called AdversarialStyle for generating and searching adversarial examples in both targeted and non-targeted way from different styles or domains of interest. The AdversarialStyle can not only evaluate deep learning models but also can discover the robustness level of every instance in the test set. Therefore, this framework can support deep learning model designers to understand and to explore their algorithms and improve the trustworthiness of AI techniques.","",""
0,"Shizhe Zhou, Zeyu Liu","Sketch2Relief: Generating Bas-relief from Sketches with Deep Generative Networks",2020,"","","","",95,"2022-07-13 09:19:22","","10.1109/ICTAI50040.2020.00085","","",,,,,0,0.00,0,2,2,"We present a novel sketch-based system for generating digital bas-relief sculptures. All existing computational methods for generating digital bas-reliefs first require the input of a three-dimensional (3D) scene, thus preventing artists from freely creating or exploring designs when 3D data are not available. Motivated by this limitation, we propose a generative adversarial network (GAN)-based sketch modeling system for generating digital bas-reliefs from freehand user sketches (see Figure 1, 5). The basic tool underpinning the interface is a conditional GAN (cGAN) that digitally learns a functional map from a contour image to a 3D model for any given viewpoint of the corresponding bas-relief model. When using our system for designing bas-reliefs, the user only needs to draw 2D sketch lines without having to designate any additional hints on the lines. The interface returns bas-relief results in interactive time (500 ms per bas-relief on average). We tested the quality and robustness of our approach with extensive and comprehensive experiments. By carefully analyzing the results, we verified that our system can faithfully reconstruct bas-reliefs from a test dataset and can generate completely new reliefs from raw amateur sketches.","",""
8,"C. Lebiere, B. Best","From Microcognition to Macrocognition: Architectural Support for Adversarial Behavior",2009,"","","","",96,"2022-07-13 09:19:22","","10.1518/155534309X441844","","",,,,,8,0.62,4,2,13,"Asymmetric adversarial behavior is a complex naturalistic domain that involves multiple macrocognitive processes. Traditional techniques that have been applied to that domain from game theory and artificial intelligence have generalized poorly from simplified paradigms to real-world conditions. We describe a competing approach rooted in cognitive architectures and distinguished by multiple levels of processes that involve complex interactions of microcognitive constructs. The naturalistic requirements of the task impose upon the cognitive architecture additional constraints beyond those involved in modeling laboratory experiments. We describe a number of improvements to the cognitive architecture designed to boost its robustness in uncertain, unpredictable, and adaptive environments characteristic of adversarial behavior. What emerges is a symbiotic relation between macrocognitive processes that drive improvements in microcognitive constructs, which in turn provide a computational account of the realization of those processes in human cognition.","",""
3,"Yong Wang, Xiuqian Jia, Mu Zhou, L. Xie, Z. Tian","A novel F-RCNN based hand gesture detection approach for FMCW systems",2019,"","","","",97,"2022-07-13 09:19:22","","10.1007/S11276-019-02096-2","","",,,,,3,1.00,1,5,3,"","",""
3,"Sicheng Jiang, Sirui Lu, D. Deng","Vulnerability of Machine Learning Phases of Matter",2019,"","","","",98,"2022-07-13 09:19:22","","","","",,,,,3,1.00,1,3,3,"Classifying different phases and the transitions between them is a major task in condensed matter physics. Machine learning, which has achieved dramatic success recently in a broad range of artificial intelligence applications, may bring an unprecedented perspective for this challenging task. In this paper, we study the robustness of this intriguing machine-learning approach to adversarial perturbations, with a focus on supervised learning scenarios. We find that typical phase classifiers based on deep neural networks are extremely vulnerable to adversarial examples: adding a tiny amount of carefully-crafted noises, which are imperceptible to human eyes and ineffective to traditional methods, into the original legitimate data obtained from numerical simulations or real experiments will cause the classifiers to make incorrect predictions at a notably high confidence level. Our results reveal a novel vulnerability aspect in applying machine learning techniques to condensed matter physics, which provides a valuable guidance for both theoretical and experimental future studies along this direction.","",""
13,"Erik Blasch, Shuo Liu, Zheng Liu, Yufeng Zheng","Deep Learning Measures of Effectiveness",2018,"","","","",99,"2022-07-13 09:19:22","","10.1109/NAECON.2018.8556808","","",,,,,13,3.25,3,4,4,"The resurgence of interest in artificial intelligence (AI) stem from impressive deep learning (DL) performance such as hierarchical supervised training using a Convolutional Neural Network (CNN). Current DL needs to focus on contextual reasoning, explainable results, and repeatable understanding that require evaluation methods. This paper presents measures of effectiveness (MOE) for DL techniques that extend measures of performance (MOP). MOPs include: Timeliness: computational efficiency, Accuracy: operational robustness, and Confidence: semi-supervised representation. MOE concerns include Throughput: data efficiency, Security: adversarial robustness, and Completeness: problem representation. DL evaluation requires verification and validation testing in realistic environments. An example is shown for Deep Multimodal Image Fusion (DMIF) that evaluates MOEs of information gain, robustness, and quality.","",""
0,"Yanshan Chen, Ziyuan Wang, Dong Wang, Yongming Yao, Zhenyu Chen","Behavior Pattern-Driven Test Case Selection for Deep Neural Networks",2019,"","","","",100,"2022-07-13 09:19:22","","10.1109/AITest.2019.000-2","","",,,,,0,0.00,0,5,3,"With the widespread application of deep learning systems, the robustness of deep neural networks (DNNs) is received increasing attentions recently. By studying the distribution of neurons outputs in DNN models, we found that the behavior patterns of neurons are different for different kinds of DNNs' inputs, e.g. test cases generated by different adversarial attack techniques. In this paper, we extract the neuron behavior patterns of DNNs under different adversarial attack techniques, use them as the guidance for test case selection. Experimental results show that this method is more efficient than random technology.","",""
0,"Yuxing Peng, J. Hsu","Named Entity Filters for Robust Machine Reading Comprehension",2018,"","","","",101,"2022-07-13 09:19:22","","10.1109/TAAI.2018.00048","","",,,,,0,0.00,0,2,4,"The machine reading comprehension problem aims to extract crucial information from the given document to answer the relevant questions. Although many methods regarding the problem have been proposed, the similarity distraction problem inside remains unsolved. The similarity distraction problem addresses the error caused by some sentences being very similar to the question but not containing the answer. Named entities have the uniqueness which can be utilized to distinguish similar sentences to prevent models from being distracted. In this paper, named entity filters (NE filters) are proposed. NE filters can utilize the information of named entities to alleviate the similarity distraction problem. Experiment results in this paper show that the NE filter can enhance the robustness of the used model. The baseline model increases 5% to 10% F1 score on two adversarial SQuAD datasets without decreasing the F1 score on the original SQuAD dataset. Besides, by adding the NE filter, other existing models increases 5% F1 score on the adversarial datasets with less than 1% loss on the original one.","",""
8,"Xinyi Li, C. Wang, Y. Sheng, Jiahan Zhang, Wentao Wang, F. Yin, Qiuwen Wu, Q. Wu, Y. Ge","An Artificial Intelligence-Driven Agent for Real-Time Head-and-Neck IMRT Plan Generation using Conditional Generative Adversarial Network (cGAN).",2020,"","","","",102,"2022-07-13 09:19:22","","10.1002/mp.14770","","",,,,,8,4.00,1,9,2,"PURPOSE To develop an Artificial-Intelligence (AI) agent for fully automated rapid head-and-neck IMRT plan generation without time-consuming dose-volume-based inverse planning.   METHODS This AI agent was trained via implementing a conditional Generative Adversarial Network (cGAN) architecture. The generator, PyraNet, is a novel Deep Learning network that implements 28 classic ResNet blocks in pyramid-like concatenations. The discriminator is a customized 4-layer DenseNet. The AI agent first generates multiple customized 2D projections at 9 template beam angles from a patient's 3D CT volume and structures. These projections are then stacked as 4D inputs of PyraNet, from which 9 radiation fluence maps of the corresponding template beam angles are generated simultaneously. Finally, the predicted fluence maps are automatically post-processed by Gaussian deconvolution operations and imported into a commercial treatment planning system (TPS) for plan integrity check and visualization. The AI agent was built and tested upon 231 oropharyngeal IMRT plans from a TPS plan library. 200/16/15 plans were assigned for training/validation/testing, respectively. Only the primary plans in the sequential boost regime were studied. All plans were normalized to 44Gy prescription (2Gy/fx). A customized Harr wavelet loss was adopted for fluence map comparison during the training of the PyraNet. For test cases, isodose distributions in AI plans and TPS plans were qualitatively evaluated for overall dose distributions. Key dosimetric metrics were compared by Wilcoxson Signed Rank tests with a significance level of 0.05.   RESULTS All 15 AI plans were successfully generated. Isodose gradients outside of PTV in AI plans were comparable to those of the TPS plans. After PTV coverage normalization, Dmean of left parotid (DAI =23.1±2.4Gy; DTPS =23.1±2.0Gy), right parotid (DAI =23.8±3.0Gy; DTPS =23.9±2.3Gy), and oral cavity (DAI =24.7±6.0Gy; DTPS =23.9±4.3Gy) in the AI plans and the TPS plans were comparable without statistical significance. AI plans achieved comparable results for maximum dose at 0.01cc of brainstem (DAI =15.0±2.1Gy; DTPS =15.5±2.7Gy) and cord+5mm (DAI =27.5±2.3Gy; DTPS =25.8±1.9Gy) without clinically-relevant differences, but body Dmax results (DAI =121.1±3.9Gy; DTPS =109.0±0.9Gy) were higher than the TPS plan results. The AI agent needed ~3s for predicting fluence maps of an IMRT plan.   CONCLUSIONS With rapid and fully automated execution, the developed AI agent can generate complex head-and-neck IMRT plans with acceptable dosimetry quality. This approach holds great potential for clinical applications in pre-planning decision-making and real-time planning.","",""
6,"Zixiao Kong, Jingfeng Xue, Yong Wang, Lu Huang, Zequn Niu, Feng Li","A Survey on Adversarial Attack in the Age of Artificial Intelligence",2021,"","","","",103,"2022-07-13 09:19:22","","10.1155/2021/4907754","","",,,,,6,6.00,1,6,1,"With the rapid evolution of the Internet, the application of artificial intelligence fields is more and more extensive, and the era of AI has come. At the same time, adversarial attacks in the AI field are also frequent. Therefore, the research into adversarial attack security is extremely urgent. An increasing number of researchers are working in this field. We provide a comprehensive review of the theories and methods that enable researchers to enter the field of adversarial attack. This article is according to the “Why? →What? → How?” research line for elaboration. Firstly, we explain the significance of adversarial attack. Then, we introduce the concepts, types, and hazards of adversarial attack. Finally, we review the typical attack algorithms and defense techniques in each application area. Facing the increasingly complex neural network model, this paper focuses on the fields of image, text, and malicious code and focuses on the adversarial attack classifications and methods of these three data types, so that researchers can quickly find their own type of study. At the end of this review, we also raised some discussions and open issues and compared them with other similar reviews.","",""
6,"Rowan T. Hughes, Liming Zhu, Tomasz Bednarz","Generative Adversarial Networks–Enabled Human–Artificial Intelligence Collaborative Applications for Creative and Design Industries: A Systematic Review of Current Approaches and Trends",2021,"","","","",104,"2022-07-13 09:19:22","","10.3389/frai.2021.604234","","",,,,,6,6.00,2,3,1,"The future of work and workplace is very much in flux. A vast amount has been written about artificial intelligence (AI) and its impact on work, with much of it focused on automation and its impact in terms of potential job losses. This review will address one area where AI is being added to creative and design practitioners’ toolbox to enhance their creativity, productivity, and design horizons. A designer’s primary purpose is to create, or generate, the most optimal artifact or prototype, given a set of constraints. We have seen AI encroaching into this space with the advent of generative networks and generative adversarial networks (GANs) in particular. This area has become one of the most active research fields in machine learning over the past number of years, and a number of these techniques, particularly those around plausible image generation, have garnered considerable media attention. We will look beyond automatic techniques and solutions and see how GANs are being incorporated into user pipelines for design practitioners. A systematic review of publications indexed on ScienceDirect, SpringerLink, Web of Science, Scopus, IEEExplore, and ACM DigitalLibrary was conducted from 2015 to 2020. Results are reported according to PRISMA statement. From 317 search results, 34 studies (including two snowball sampled) are reviewed, highlighting key trends in this area. The studies’ limitations are presented, particularly a lack of user studies and the prevalence of toy-examples or implementations that are unlikely to scale. Areas for future study are also identified.","",""
3,"Vibekananda Dutta, T. Zielińska","An Adversarial Explainable Artificial Intelligence (XAI) Based Approach for Action Forecasting",2021,"","","","",105,"2022-07-13 09:19:22","","10.14313/JAMRIS/4-2020/38","","",,,,,3,3.00,2,2,1,"Abstract: Despite the growing popularity of machine learning technology, vision‐based action recognition/forecasting systems are seen as black‐boxes by the user. The effecti‐ veness of such systems depends on the machine learning algorithms, it is difficult (or impossible) to explain the de‐ cisions making processes to the users. In this context, an approach that offers the user understanding of these re‐ asoning models is significant. To do this, we present an Explainable Artificial Intelligence (XAI) based approach to action forecasting using structured database and object affordances definition. The structured database is sup‐ porting the prediction process. The method allows to vi‐ sualize the components of the structured database. Later, the components of the base are used for forecasting the nominally possible motion goals. The object affordance explicated by the probability functions supports the se‐ lection of possiblemotion goals. The presentedmethodo‐ logy allows satisfactory explanations of the reasoning be‐ hind the inference mechanism. Experimental evaluation was conducted using the WUT‐18 dataset, the efficiency of the presented solution was compared to the other ba‐ seline algorithms.","",""
52,"Hamon Ronan, Junklewitz Henrik, S. Ignacio","Robustness and Explainability of Artificial Intelligence",2020,"","","","",106,"2022-07-13 09:19:22","","10.2760/57493","","",,,,,52,26.00,17,3,2,"","",""
0,"A. S. Rakin, Ye Wang, S. Aeron, T. Koike-Akino, P. Moulin, K. Parsons","Towards Universal Adversarial Examples and Defenses /Author=Rakin, Adnan S; Wang, Ye; Aeron, Shuchin; Koike-Akino, Toshiaki; Moulin, Pierre; Parsons, Kieran /CreationDate=October 21, 2021 /Subject=Artificial Intelligence, Machine Learning, Signal Processing",2021,"","","","",107,"2022-07-13 09:19:22","","","","",,,,,0,0.00,0,6,1,"Adversarial example attacks have recently exposed the severe vulnerability of neural network models. However, most of the existing attacks require some form of target model information (i.e., weights/model inquiry/architecture) to improve the efficacy of the attack. We leverage the information-theoretic connections between robust learning and generalized rate-distortion theory to formulate a universal adversarial example (UAE) generation algorithm. Our algorithm trains an offline adversarial generator to minimize the mutual information of a given data distribution. At the inference phase, our UAE can efficiently generate effective adversary examples without high computation cost.These adversarial examples in turn allow for developing universal defense responses through adversarial training. Our experiments demonstrate promising gains in improving the training efficiency of conventional adversarial training IEEE Information Theory Workshop 2021 c © 2021 IEEE. Personal use of this material is permitted. Permission from IEEE must be obtained for all other uses, in any current or future media, including reprinting/republishing this material for advertising or promotional purposes, creating new collective works, for resale or redistribution to servers or lists, or reuse of any copyrighted component of this work in other works. Mitsubishi Electric Research Laboratories, Inc. 201 Broadway, Cambridge, Massachusetts 02139 Towards Universal Adversarial Examples and Defenses Adnan Siraj Rakin, Ye Wang, Shuchin Aeron, Toshiaki Koike-Akino, Pierre Moulin, Kieran Parsons ∗Arizona State University, †Mitsubishi Electric Research Laboratories (MERL), ‡Tufts University, §University of Illinois at Urbana-Champaign ∗asrakin@asu.edu, †{yewang, koike, parsons}@merl.com, ‡shuchin@ece.tufts.edu, §pmoulin@illinois.edu Abstract—Adversarial examples have recently exposed the severe vulnerability of neural network models. However, most of the existing attacks require some form of target model information (i.e., weights/model inquiry/architecture) to improve the efficacy of the attack. We leverage the information-theoretic connections between robust learning and generalized rate-distortion theory to formulate a universal adversarial example (UAE) generation algorithm. Our algorithm trains an offline adversarial generator to minimize the mutual information between the label and perturbed data. At the inference phase, our UAE method can efficiently generate effective adversarial examples without high computation cost. These adversarial examples in turn allow for developing universal defenses through adversarial training. Our experiments demonstrate promising gains in improving the training efficiency of conventional adversarial training.Adversarial examples have recently exposed the severe vulnerability of neural network models. However, most of the existing attacks require some form of target model information (i.e., weights/model inquiry/architecture) to improve the efficacy of the attack. We leverage the information-theoretic connections between robust learning and generalized rate-distortion theory to formulate a universal adversarial example (UAE) generation algorithm. Our algorithm trains an offline adversarial generator to minimize the mutual information between the label and perturbed data. At the inference phase, our UAE method can efficiently generate effective adversarial examples without high computation cost. These adversarial examples in turn allow for developing universal defenses through adversarial training. Our experiments demonstrate promising gains in improving the training efficiency of conventional adversarial training.","",""
0,"Massimiliano Lupo Pasini, Junqi Yin","Stable Parallel Training of Wasserstein Conditional Generative Adversarial Neural Networks : *Full/Regular Research Paper submission for the symposium CSCI-ISAI: Artificial Intelligence",2021,"","","","",108,"2022-07-13 09:19:22","","10.1109/CSCI54926.2021.00026","","",,,,,0,0.00,0,2,1,"We use a stable parallel approach to train Wasserstein Conditional Generative Adversarial Neural Networks (W-CGANs). The parallel training reduces the risk of mode collapse and enhances scalability by using multiple generators that are concurrently trained, each one of them focusing on a single data label. The use of the Wasserstein metric reduces the risk of cycling by stabilizing the training of each generator. We apply the approach on the CIFAR10 and the CIFAR100 datasets, two standard benchmark datasets with images of the same resolution, but different number of classes. Performance is assessed using the inception score, the Fréchet inception distance, and image quality. An improvement in inception score and Fréchet inception distance is shown in comparison to previous results obtained by performing the parallel approach on deep convolutional conditional generative adversarial neural networks (DC-CGANs). Weak scaling is attained on both datasets using up to 100 NVIDIA V100 GPUs on the OLCF supercomputer Summit.","",""
14,"I. Tyukin, D. Higham, A. Gorban","On Adversarial Examples and Stealth Attacks in Artificial Intelligence Systems",2020,"","","","",109,"2022-07-13 09:19:22","","10.1109/IJCNN48605.2020.9207472","","",,,,,14,7.00,5,3,2,"In this work we present a formal theoretical framework for assessing and analyzing two classes of malevolent action towards generic Artificial Intelligence (AI) systems. Our results apply to general multi-class classifiers that map from an input space into a decision space, including artificial neural networks used in deep learning applications. Two classes of attacks are considered. The first class involves adversarial examples and concerns the introduction of small perturbations of the input data that cause misclassification. The second class, introduced here for the first time and named stealth attacks, involves small perturbations to the AI system itself. Here the perturbed system produces whatever output is desired by the attacker on a specific small data set, perhaps even a single input, but performs as normal on a validation set (which is unknown to the attacker).We show that in both cases, i.e., in the case of an attack based on adversarial examples and in the case of a stealth attack, the dimensionality of the AI’s decision-making space is a major contributor to the AI’s susceptibility. For attacks based on adversarial examples, a second crucial parameter is the absence of local concentrations in the data probability distribution, a property known as Smeared Absolute Continuity. According to our findings, robustness to adversarial examples requires either (a) the data distributions in the AI’s feature space to have concentrated probability density functions or (b) the dimensionality of the AI’s decision variables to be sufficiently small. We also show how to construct stealth attacks on high-dimensional AI systems that are hard to spot unless the validation set is made exponentially large.","",""
7,"Kwonsang Sohn, Christine Sung, Gukwon Koo, O. Kwon","Artificial intelligence in the fashion industry: consumer responses to generative adversarial network (GAN) technology",2020,"","","","",110,"2022-07-13 09:19:22","","10.1108/IJRDM-03-2020-0091","","",,,,,7,3.50,2,4,2,"This study examines consumers' evaluations of product consumption values, purchase intentions and willingness to pay for fashion products designed using generative adversarial network (GAN), an artificial intelligence technology. This research investigates differences between consumers' evaluations of a GAN-generated product and a non-GAN-generated product and tests whether disclosing the use of GAN technology affects consumers' evaluations.,Sample products were developed as experimental stimuli using cycleGAN. Data were collected from 163 members of Generation Y. Participants were assigned to one of the three experimental conditions (i.e. non-GAN-generated images, GAN-generated images with disclosure and GAN-generated images without disclosure). Regression analysis and ANOVA were used to test the hypotheses.,Functional, social and epistemic consumption values positively affect willingness to pay in the GAN-generated products. Relative to non-GAN-generated products, willingness to pay is significantly higher for GAN-generated products. Moreover, evaluations of functional value, emotional value and willingness to pay are highest when GAN technology is used, but not disclosed.,This study evaluates the utility of GANs from consumers' perspective based on the perceived value of GAN-generated product designs. Findings have practical implications for firms that are considering using GANs to develop products for the retail fashion market.","",""
5,"Jing Qiu, Lei Du, Yuanyuan Chen, Zhihong Tian, Xiaojiang Du, M. Guizani","Artificial Intelligence Security in 5G Networks: Adversarial Examples for Estimating a Travel Time Task",2020,"","","","",111,"2022-07-13 09:19:22","","10.1109/mvt.2020.3002487","","",,,,,5,2.50,1,6,2,"With the rapid development of the Internet, the nextgeneration network (5G) has emerged. 5G can support a variety of new applications, such as the Internet of Things (IoT), virtual reality (VR), and the Internet of Vehicles. Most of these new applications depend on deep learning algorithms, which have made great advances in many areas of artificial intelligence (AI). However, researchers have found that AI algorithms based on deep learning pose numerous security problems. For example, deep learning is susceptible to a well-designed input sample formed by adding small perturbations to the original sample. This well-designed input with small perturbations, which are imperceptible to humans, is called an adversarial example. An adversarial example is similar to a truth example, but it can render the deep learning model invalid. In this article, we generate adversarial examples for spatiotemporal data. Based on the travel time estimation (TTE) task, we use two methods-white-box and blackbox attacks-to invalidate deep learning models. Experiment results show that the adversarial examples successfully attack the deep learning model and thus that AI security is a big challenge of 5G.","",""
0,"Charlie T. Veal, Marshall Lindsay, S. Kovaleski, Derek T. Anderson, Stanton R. Price","Evolutionary Algorithm Driven Explainable Adversarial Artificial Intelligence",2020,"","","","",112,"2022-07-13 09:19:22","","10.1109/SSCI47803.2020.9308361","","",,,,,0,0.00,0,5,2,"It is well-known that machine learning algorithms can be susceptible to undesirable effects when exposed to conditions that are not expressed adequately in the training dataset. This leads to a growing interest throughout many communities; where do algorithms and trained models break? Recently, methods such as generative adversarial neural networks and variational autoencoders were proposed to create adversarial examples that challenge algorithms. This results in artificial intelligence having higher false detections or completely losing recognition. The problem is that existing solutions, are for the most part, black boxes. Current gaps include how do we better control and understand adversarial algorithms. Herein, we propose the concept of an adversarial modifier set as an understandable and controlled way to generate adversarial examples. This is achieved by exploiting the improved evolution-constructed algorithm to identify ideal features that a victim algorithm values in imagery. These features are combined to realize a tuple library that preserves spatial relations. Last, a set of algorithmically controlled modifiers that generate the imagery are found by examining the content of the false imagery. Preliminary results are encouraging and demonstrate that this approach has benefits in both generating explainable adversarial examples, as well as shedding some insight into victim algorithm decision making.","",""
2,"Erik Hemberg, Linda Zhang, Una-May O’Reilly","Exploring Adversarial Artificial Intelligence for Autonomous Adaptive Cyber Defense",2020,"","","","",113,"2022-07-13 09:19:22","","10.1007/978-3-030-33432-1_3","","",,,,,2,1.00,1,3,2,"","",""
51,"Shubham Sharma, Jette Henderson, Joydeep Ghosh","CERTIFAI: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models",2019,"","","","",114,"2022-07-13 09:19:22","","10.1145/3375627.3375812","","",,,,,51,17.00,17,3,3,"As artificial intelligence plays an increasingly important role in our society, there are ethical and moral obligations for both businesses and researchers to ensure that their machine learning models are designed, deployed, and maintained responsibly. These models need to be rigorously audited for fairness, robustness, transparency, and interpretability. A variety of methods have been developed that focus on these issues in isolation, however, managing these methods in conjunction with model development can be cumbersome and timeconsuming. In this paper, we introduce a unified and model-agnostic approach to address these issues: Counterfactual Explanations for Robustness, Transparency, Interpretability, and Fairness of Artificial Intelligence models (CERTIFAI). Unlike previous methods in this domain, CERTIFAI is a general tool that can be applied to any black-box model and any type of input data. Given a model and an input instance, CERTIFAI uses a custom genetic algorithm to generate counterfactuals: instances close to the input that change the prediction of the model. We demonstrate how these counterfactuals can be used to examine issues of robustness, interpretability, transparency, and fairness. Additionally, we introduce CERScore, the first black-box model robustness score that performs comparably to methods that have access to model internals.","",""
18,"Hwiyoung Kim, D. Jung, B. Choi","Exploiting the Vulnerability of Deep Learning-Based Artificial Intelligence Models in Medical Imaging: Adversarial Attacks",2019,"","","","",115,"2022-07-13 09:19:22","","10.3348/JKSR.2019.80.2.259","","",,,,,18,6.00,6,3,3,"Due to rapid developments in the deep learning model, artificial intelligence (AI) models are expected to enhance clinical diagnostic ability and work efficiency by assisting physicians. Therefore, many hospitals and private companies are competing to develop AI-based automatic diagnostic systems using medical images. In the near future, many deep learning-based automatic diagnostic systems would be used clinically. However, the possibility of adversarial attacks exploiting certain vulnerabilities of the deep learning algorithm is a major obstacle to deploying deep learning-based systems in clinical practice. In this paper, we will examine in detail the kinds of principles and methods of adversarial attacks that can be made to deep learning models dealing with medical images, the problems that can arise, and the preventive measures that can be taken against them.","",""
67,"Zhiwen Deng, Chuan He, Yingzheng Liu, Kyung Chun Kim","Super-resolution reconstruction of turbulent velocity fields using a generative adversarial network-based artificial intelligence framework",2019,"","","","",116,"2022-07-13 09:19:22","","10.1063/1.5127031","","",,,,,67,22.33,17,4,3,"","",""
14,"A. Zaji, H. Bonakdari","Robustness lake water level prediction using the search heuristic-based artificial intelligence methods",2019,"","","","",117,"2022-07-13 09:19:22","","10.1080/09715010.2018.1424568","","",,,,,14,4.67,7,2,3,"Abstract Lakes have a crucial role in the industrial, agricultural, environment, and drinking water fields. Accurate prediction of lake levels is one of the most important parameters in the reservoir management and lakeshore structure designing. The goal of the present study is to examine the robustness of two different Genetic Algorithm-based regression methods namely the Genetic Algorithm Artificial neural network (GAA) and the Genetic Programming (GP) by considering their performance in predicting the non-observed lakes. To do that, data collected from the four-year daily measurements of the Chahnimeh#1 lake in Eastern Iran were used for developing the GAA and GP models and after that, the performance of the considered models are examined to predict the lake water levels of an adjacent lake namely Chahnimeh#4 as the non-observed information. The results showed that both model has the ability to simulate adjacent lakes using the considered lake water levels for the training procedure. In addition, another goal is to develop simple, practical formulation for predicting the lake water level, So that, using the GP method, as the superior model, three different formulations are proposed in order to predict the one, three, and five days ahead lake water level, respectively.","",""
2,"Utku Kose","Techniques for Adversarial Examples Threatening the Safety of Artificial Intelligence Based Systems",2019,"","","","",118,"2022-07-13 09:19:22","","","","",,,,,2,0.67,2,1,3,"Artificial intelligence is known as the most effective technological field for rapid developments shaping the future of the world. Even today, it is possible to see intense use of intelligence systems in all fields of the life. Although advantages of the Artificial Intelligence are widely observed, there is also a dark side employing efforts to design hacking oriented techniques against Artificial Intelligence. Thanks to such techniques, it is possible to trick intelligent systems causing directed results for unsuccessful outputs. That is critical for also cyber wars of the future as it is predicted that the wars will be done unmanned, autonomous intelligent systems. Moving from the explanations, objective of this study is to provide information regarding adversarial examples threatening the Artificial Intelligence and focus on details of some techniques, which are used for creating adversarial examples. Adversarial examples are known as training data, which can trick a Machine Learning technique to learn incorrectly about the target problem and cause an unsuccessful or maliciously directed intelligent system at the end. The study enables the readers to learn enough about details of recent techniques for creating adversarial examples.","",""
755,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xisheng Fang, Shiqin Zhang, J. Xia, Jun Xia","Artificial Intelligence Distinguishes COVID-19 from Community Acquired Pneumonia on Chest CT",2020,"","","","",119,"2022-07-13 09:19:22","","10.1148/radiol.2020200905","","",,,,,755,377.50,76,18,2,"Background Coronavirus disease has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performances. Materials and Methods In this retrospective and multi-center study, a deep learning model, COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT exams for the detection of COVID-19. Community acquired pneumonia (CAP) and other non-pneumonia CT exams were included to test the robustness of the model. The datasets were collected from 6 hospitals between August 2016 and February 2020. Diagnostic performance was assessed by the area under the receiver operating characteristic curve (AUC), sensitivity and specificity. Results The collected dataset consisted of 4356 chest CT exams from 3,322 patients. The average age is 49±15 years and there were slightly more male patients than female (1838 vs 1484; p-value=0.29). The per-exam sensitivity and specificity for detecting COVID-19 in the independent test set was 114 of 127 (90% [95% CI: 83%, 94%]) and 294 of 307 (96% [95% CI: 93%, 98%]), respectively, with an AUC of 0.96 (p-value<0.001). The per-exam sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175) and 92% (239 of 259), respectively, with an AUC of 0.95 (95% CI: 0.93, 0.97). Conclusions A deep learning model can accurately detect COVID-19 and differentiate it from community acquired pneumonia and other lung diseases.","",""
26,"M. Kuzlu, Corinne Fair, Ozgur Guler","Role of Artificial Intelligence in the Internet of Things (IoT) cybersecurity",2021,"","","","",120,"2022-07-13 09:19:22","","10.1007/S43926-020-00001-4","","",,,,,26,26.00,9,3,1,"","",""
19,"Ruhhee Tabbussum, A. Q. Dar","Performance evaluation of artificial intelligence paradigms—artificial neural networks, fuzzy logic, and adaptive neuro-fuzzy inference system for flood prediction",2021,"","","","",121,"2022-07-13 09:19:22","","10.1007/s11356-021-12410-1","","",,,,,19,19.00,10,2,1,"","",""
822,"Lin Li, Lixin Qin, Zeguo Xu, Youbing Yin, Xin Wang, Bin Kong, Junjie Bai, Yi Lu, Zhenghan Fang, Q. Song, K. Cao, Daliang Liu, Guisheng Wang, Qizhong Xu, Xi Fang, Shiqin Zhang, J. Xia, Jun Xia","Using Artificial Intelligence to Detect COVID-19 and Community-acquired Pneumonia Based on Pulmonary CT: Evaluation of the Diagnostic Accuracy",2020,"","","","",122,"2022-07-13 09:19:22","","10.1148/RADIOL.2020200905","","",,,,,822,411.00,82,18,2,"Background Coronavirus disease 2019 (COVID-19) has widely spread all over the world since the beginning of 2020. It is desirable to develop automatic and accurate detection of COVID-19 using chest CT. Purpose To develop a fully automatic framework to detect COVID-19 using chest CT and evaluate its performance. Materials and Methods In this retrospective and multicenter study, a deep learning model, the COVID-19 detection neural network (COVNet), was developed to extract visual features from volumetric chest CT scans for the detection of COVID-19. CT scans of community-acquired pneumonia (CAP) and other non-pneumonia abnormalities were included to test the robustness of the model. The datasets were collected from six hospitals between August 2016 and February 2020. Diagnostic performance was assessed with the area under the receiver operating characteristic curve, sensitivity, and specificity. Results The collected dataset consisted of 4352 chest CT scans from 3322 patients. The average patient age (±standard deviation) was 49 years ± 15, and there were slightly more men than women (1838 vs 1484, respectively; P = .29). The per-scan sensitivity and specificity for detecting COVID-19 in the independent test set was 90% (95% confidence interval [CI]: 83%, 94%; 114 of 127 scans) and 96% (95% CI: 93%, 98%; 294 of 307 scans), respectively, with an area under the receiver operating characteristic curve of 0.96 (P < .001). The per-scan sensitivity and specificity for detecting CAP in the independent test set was 87% (152 of 175 scans) and 92% (239 of 259 scans), respectively, with an area under the receiver operating characteristic curve of 0.95 (95% CI: 0.93, 0.97). Conclusion A deep learning model can accurately detect coronavirus 2019 and differentiate it from community-acquired pneumonia and other lung conditions. © RSNA, 2020 Online supplemental material is available for this article.","",""
1,"Karunanithi, O. AlHeyasat, D. Thomas, G. Kavitha","Attacks on Artificial Intelligence Applications through Adversarial Image",2018,"","","","",123,"2022-07-13 09:19:22","","","","",,,,,1,0.25,0,4,4,"Artificial Intelligence (AI) is a subset of Machine Learning. Cyber Security is always a game between defender and attacker. AI in cyber security plays a vital role in automating the security process which reduces human monitoring round the clock.Cyber attacks on AI applications are discussed in this paper. Adversarial image attack is a way to fool AI and humans in cyber world. Adversarial Image is altering the original image with algorithm which makes difficult to identify or classify the image by AI through Convolutional Neural Network. Even if both image looks similar to human eye, internal bits are altered to misclassify the image. Several methodologies are adopted to create these adversarial images attack by the attackers. Impacts of these Adversarial image attacks cause severe damages in various domains like social networking, Medical Image Diagnosis, Text Extraction from images, Security Cameras image analysis etc., This Adversarial attacks is future challenging aspect of the AI Cyber Security mechanism.","",""
164,"M. Jamshidi, A. Lalbakhsh, J. Talla, Z. Peroutka, F. Hadjilooei, Pedram Lalbakhsh, M. Jamshidi, L. Spada, M. Mirmozafari, Mojgan Dehghani, Asal Sabet, Saeed Roshani, S. Roshani, Nima Bayat-Makou, B. Mohamadzade, Zahra Malek, A. Jamshidi, S. Kiani, H. Hashemi‐Dezaki, Wahab Mohyuddin","Artificial Intelligence and COVID-19: Deep Learning Approaches for Diagnosis and Treatment",2020,"","","","",124,"2022-07-13 09:19:22","","10.1109/ACCESS.2020.3001973","","",,,,,164,82.00,16,20,2,"COVID-19 outbreak has put the whole world in an unprecedented difficult situation bringing life around the world to a frightening halt and claiming thousands of lives. Due to COVID-19’s spread in 212 countries and territories and increasing numbers of infected cases and death tolls mounting to 5,212,172 and 334,915 (as of May 22 2020), it remains a real threat to the public health system. This paper renders a response to combat the virus through Artificial Intelligence (AI). Some Deep Learning (DL) methods have been illustrated to reach this goal, including Generative Adversarial Networks (GANs), Extreme Learning Machine (ELM), and Long/Short Term Memory (LSTM). It delineates an integrated bioinformatics approach in which different aspects of information from a continuum of structured and unstructured data sources are put together to form the user-friendly platforms for physicians and researchers. The main advantage of these AI-based platforms is to accelerate the process of diagnosis and treatment of the COVID-19 disease. The most recent related publications and medical reports were investigated with the purpose of choosing inputs and targets of the network that could facilitate reaching a reliable Artificial Neural Network-based tool for challenges associated with COVID-19. Furthermore, there are some specific inputs for each platform, including various forms of the data, such as clinical data and medical imaging which can improve the performance of the introduced approaches toward the best responses in practical applications.","",""
10,"Qiang Zhang, M. Burrage, E. Lukaschuk, M. Shanmuganathan, Iulia A. Popescu, C. Nikolaidou, Rebecca Mills, K. Werys, Evan Hann, A. Barutcu, Suleyman D Polat, M. Salerno, M. Jerosch-Herold, R. Kwong, H. Watkins, C. Kramer, S. Neubauer, V. Ferreira, S. Piechnik","Toward Replacing Late Gadolinium Enhancement With Artificial Intelligence Virtual Native Enhancement for Gadolinium-Free Cardiovascular Magnetic Resonance Tissue Characterization in Hypertrophic Cardiomyopathy",2021,"","","","",125,"2022-07-13 09:19:22","","10.1161/CIRCULATIONAHA.121.054432","","",,,,,10,10.00,1,19,1,"Supplemental Digital Content is available in the text. Background: Late gadolinium enhancement (LGE) cardiovascular magnetic resonance (CMR) imaging is the gold standard for noninvasive myocardial tissue characterization but requires intravenous contrast agent administration. It is highly desired to develop a contrast agent–free technology to replace LGE for faster and cheaper CMR scans. Methods: A CMR virtual native enhancement (VNE) imaging technology was developed using artificial intelligence. The deep learning model for generating VNE uses multiple streams of convolutional neural networks to exploit and enhance the existing signals in native T1 maps (pixel-wise maps of tissue T1 relaxation times) and cine imaging of cardiac structure and function, presenting them as LGE-equivalent images. The VNE generator was trained using generative adversarial networks. This technology was first developed on CMR datasets from the multicenter Hypertrophic Cardiomyopathy Registry, using hypertrophic cardiomyopathy as an exemplar. The datasets were randomized into 2 independent groups for deep learning training and testing. The test data of VNE and LGE were scored and contoured by experienced human operators to assess image quality, visuospatial agreement, and myocardial lesion burden quantification. Image quality was compared using a nonparametric Wilcoxon test. Intra- and interobserver agreement was analyzed using intraclass correlation coefficients (ICC). Lesion quantification by VNE and LGE were compared using linear regression and ICC. Results: A total of 1348 hypertrophic cardiomyopathy patients provided 4093 triplets of matched T1 maps, cines, and LGE datasets. After randomization and data quality control, 2695 datasets were used for VNE method development and 345 were used for independent testing. VNE had significantly better image quality than LGE, as assessed by 4 operators (n=345 datasets; P<0.001 [Wilcoxon test]). VNE revealed lesions characteristic of hypertrophic cardiomyopathy in high visuospatial agreement with LGE. In 121 patients (n=326 datasets), VNE correlated with LGE in detecting and quantifying both hyperintensity myocardial lesions (r=0.77–0.79; ICC=0.77–0.87; P<0.001) and intermediate-intensity lesions (r=0.70–0.76; ICC=0.82–0.85; P<0.001). The native CMR images (cine plus T1 map) required for VNE can be acquired within 15 minutes and producing a VNE image takes less than 1 second. Conclusions: VNE is a new CMR technology that resembles conventional LGE but without the need for contrast administration. VNE achieved high agreement with LGE in the distribution and quantification of lesions, with significantly better image quality.","",""
139,"O. Méndez-Lucio, B. Baillif, Djork-Arné Clevert, D. Rouquié, J. Wichard","De novo generation of hit-like molecules from gene expression signatures using artificial intelligence",2018,"","","","",126,"2022-07-13 09:19:22","","10.1038/s41467-019-13807-w","","",,,,,139,34.75,28,5,4,"","",""
7,"Yulei Wu","Robust Learning-Enabled Intelligence for the Internet of Things: A Survey From the Perspectives of Noisy Data and Adversarial Examples",2021,"","","","",127,"2022-07-13 09:19:22","","10.1109/JIOT.2020.3018691","","",,,,,7,7.00,7,1,1,"The Internet of Things (IoT) has been widely adopted in a range of verticals, e.g., automation, health, energy, and manufacturing. Many of the applications in these sectors, such as self-driving cars and remote surgery, are critical and high stakes applications, calling for advanced machine learning (ML) models for data analytics. Essentially, the training and testing data that are collected by massive IoT devices may contain noise (e.g., abnormal data, incorrect labels, and incomplete information) and adversarial examples. This requires high robustness of ML models to make reliable decisions for IoT applications. The research of robust ML has received tremendous attention from both academia and industry in recent years. This article will investigate the state of the art and representative works of robust ML models that can enable high resilience and reliability of IoT intelligence. Two aspects of robustness will be focused on, i.e., when the training data of ML models contain noises and adversarial examples, which may typically happen in many real-world IoT scenarios. In addition, the reliability of both neural networks and reinforcement learning framework will be investigated. Both of these two ML paradigms have been widely used in handling data in IoT scenarios. The potential research challenges and open issues will be discussed to provide future research directions.","",""
90,"R. Shafin, Lingjia Liu, V. Chandrasekhar, Hao Chen, J. Reed, Jianzhong Zhang","Artificial Intelligence-Enabled Cellular Networks: A Critical Path to Beyond-5G and 6G",2019,"","","","",128,"2022-07-13 09:19:22","","10.1109/MWC.001.1900323","","",,,,,90,30.00,15,6,3,"Mobile network operators (MNOs) are in the process of overlaying their conventional macro cellular networks with shorter range cells such as outdoor pico cells. The resultant increase in network complexity creates substantial overhead in terms of operating expenses, time, and labor for their planning and management. Artificial intelligence (AI) offers the potential for MNOs to operate their networks in a more organic and cost-efficient manner. We argue that deploying AI in fifth generation (5G) and beyond will require surmounting significant technical barriers in terms of robustness, performance, and complexity. We outline future research directions, identify top five challenges, and present a possible roadmap to realize the vision of AI-enabled cellular networks for Beyond- 5G and sixth generation (6G) networks.","",""
34,"T. H. Aldhyani, M. Al-Yaari, Hasan Alkahtani, Mashael S. Maashi","Water Quality Prediction Using Artificial Intelligence Algorithms",2020,"","","","",129,"2022-07-13 09:19:22","","10.1155/2020/6659314","","",,,,,34,17.00,9,4,2,"During the last years, water quality has been threatened by various pollutants. Therefore, modeling and predicting water quality have become very important in controlling water pollution. In this work, advanced artificial intelligence (AI) algorithms are developed to predict water quality index (WQI) and water quality classification (WQC). For the WQI prediction, artificial neural network models, namely nonlinear autoregressive neural network (NARNET) and long short-term memory (LSTM) deep learning algorithm, have been developed. In addition, three machine learning algorithms, namely, support vector machine (SVM), K-nearest neighbor (K-NN), and Naive Bayes, have been used for the WQC forecasting. The used dataset has 7 significant parameters, and the developed models were evaluated based on some statistical parameters. The results revealed that the proposed models can accurately predict WQI and classify the water quality according to superior robustness. Prediction results demonstrated that the NARNET model performed slightly better than the LSTM for the prediction of the WQI values and the SVM algorithm has achieved the highest accuracy (97.01%) for the WQC prediction. Furthermore, the NARNET and LSTM models have achieved similar accuracy for the testing phase with a slight difference in the regression coefficient (RNARNET = 96.17% and RLSTM = 94.21%). This kind of promising research can contribute significantly to water management.","",""
5,"Ayodeji Oseni, Nour Moustafa, H. Janicke, Peng Liu, Z. Tari, A. Vasilakos","Security and Privacy for Artificial Intelligence: Opportunities and Challenges",2021,"","","","",130,"2022-07-13 09:19:22","","","","",,,,,5,5.00,1,6,1,"The increased adoption of Artificial Intelligence (AI) presents an opportunity to solve many socio-economic and environmental challenges; however, this cannot happen without securing AI-enabled technologies. In recent years, most AI models are vulnerable to advanced and sophisticated hacking techniques. This challenge has motivated concerted research efforts into adversarial AI, with the aim of developing robust machine and deep learning models that are resilient to different types of adversarial scenarios. In this paper, we present a holistic cyber security review that demonstrates adversarial attacks against AI applications, including aspects such as adversarial knowledge and capabilities, as well as existing methods for generating adversarial examples and existing cyber defence models. We explain mathematical AI models, especially new variants of reinforcement and federated learning, to demonstrate how attack vectors would exploit vulnerabilities of AI models. We also propose a systematic framework for demonstrating attack techniques against AI applications, and reviewed several cyber defences that would protect the AI applications against those attacks. We also highlight the importance of understanding the adversarial goals and their capabilities, especially the recent attacks against industry applications, to develop adaptive defences that assess to secure AI applications. Finally, we describe the main challenges and future research directions in the domain of security and privacy of AI technologies.","",""
5,"Thulsiram Gantala, K. Balasubramaniam","Automated Defect Recognition for Welds Using Simulation Assisted TFM Imaging with Artificial Intelligence",2021,"","","","",131,"2022-07-13 09:19:22","","10.1007/s10921-021-00761-1","","",,,,,5,5.00,3,2,1,"","",""
51,"Mohammad Behdad Mohammad Behdad Jamshidi Jamshidi, Ali Ali Lalbakhsh Lalbakhsh, Jakub Jakub Talla Talla, Zdeněk Zdenek Peroutka Peroutka, Farimah Farimah Hadjilooei Hadjilooei, Pedram Pedram Lalbakhsh Lalbakhsh, Morteza Morteza Jamshidi Jamshidi, Luigi La Luigi La Spada Spada, Mirhamed Mirhamed Mirmozafari Mirmozafari, Mojgan Mojgan Dehghani Dehghani, Asal Asal Sabet Sabet, Saeed Saeed Roshani Roshani, Sobhan Sobhan Roshani Roshani, Nima Nima Bayat-Makou Bayat-Makou, Bahare Bahare Mohamadzade Mohamadzade, Zahra Zahra Malek Malek, Alireza Alireza Jamshidi Jamshidi, Sarah Sarah Kiani Kiani, Hamed Hamed Hashemi-Dezaki Hashemi-Dezaki, Wahab Wahab Mohyuddin Mohyuddin","Artificial Intelligence and COVID-19: Deep Learning Approaches for Diagnosis and Treatment",2020,"","","","",132,"2022-07-13 09:19:22","","10.1109/ACCESS.2020.3001973","","",,,,,51,25.50,5,20,2,"COVID-19 outbreak has put the whole world in an unprecedented difficult situation bringing life around the world to a frightening halt and claiming thousands of lives. Due to COVID-19’s spread in 212 countries and territories and increasing numbers of infected cases and death tolls mounting to 5,212,172 and 334,915 (as of May 22 2020), it remains a real threat to the public health system. This paper renders a response to combat the virus through Artificial Intelligence (AI). Some Deep Learning (DL) methods have been illustrated to reach this goal, including Generative Adversarial Networks (GANs), Extreme Learning Machine (ELM), and Long/Short Term Memory (LSTM). It delineates an integrated bioinformatics approach in which different aspects of information from a continuum of structured and unstructured data sources are put together to form the user-friendly platforms for physicians and researchers. The main advantage of these AI-based platforms is to accelerate the process of diagnosis and treatment of the COVID-19 disease. The most recent related publications and medical reports were investigated with the purpose of choosing inputs and targets of the network that could facilitate reaching a reliable Artificial Neural Network-based tool for challenges associated with COVID-19. Furthermore, there are some specific inputs for each platform, including various forms of the data, such as clinical data and medical imaging which can improve the performance of the introduced approaches toward the best responses in practical applications.","",""
42,"M. Nassar, K. Salah, M. H. Rehman, D. Svetinovic","Blockchain for explainable and trustworthy artificial intelligence",2019,"","","","",133,"2022-07-13 09:19:22","","10.1002/widm.1340","","",,,,,42,14.00,11,4,3,"The increasing computational power and proliferation of big data are now empowering Artificial Intelligence (AI) to achieve massive adoption and applicability in many fields. The lack of explanation when it comes to the decisions made by today's AI algorithms is a major drawback in critical decision‐making systems. For example, deep learning does not offer control or reasoning over its internal processes or outputs. More importantly, current black‐box AI implementations are subject to bias and adversarial attacks that may poison the learning or the inference processes. Explainable AI (XAI) is a new trend of AI algorithms that provide explanations of their AI decisions. In this paper, we propose a framework for achieving a more trustworthy and XAI by leveraging features of blockchain, smart contracts, trusted oracles, and decentralized storage. We specify a framework for complex AI systems in which the decision outcomes are reached based on decentralized consensuses of multiple AI and XAI predictors. The paper discusses how our proposed framework can be utilized in key application areas with practical use cases.","",""
5,"Xiaochen Zhang, Dayu Yang","Research on Music Assisted Teaching System Based on Artificial Intelligence Technology",2021,"","","","",134,"2022-07-13 09:19:22","","10.1088/1742-6596/1852/2/022032","","",,,,,5,5.00,3,2,1,"With the advent of the information age, computer technology has been greatly developed, especially the development of Artificial Intelligence(AI). And with the passage of time, AI began to involve various fields, music education is no exception. In this paper, after a detailed understanding of some research results of AI on music assisted instruction system, we mainly analyze the students’ video, audio and other related information, and save it in the database. This paper first introduces the evaluation process by using AI technology. In fact, it is necessary to find out the relationship between the influencing factors and evaluation of music assisted teaching system. Neural network(NN) is actually a model proposed by simulating the way people think in the brain. It has no strict requirements for data distribution. In terms of nonlinear data processing method, robustness and dynamics, it is very suitable to be used as a model for evaluating music assisted instruction system. Then each factor is taken as the input parameter of the NN. According to the evaluation index of music teaching, a special modeling system is designed. With the help of technical personnel, we obtained the sample data of music performance and completed the neural training. The experimental results show that the development of AI technology has broken the original situation of traditional teaching, especially the application of music system and intelligent music software based on AI in music teaching.","",""
3,"Yupeng Hu, Wenxin Kuang, Zheng Qin, Kenli Li, Jiliang Zhang, Yansong Gao, Wenjia Li, Keqin Li","Artificial Intelligence Security: Threats and Countermeasures",2021,"","","","",135,"2022-07-13 09:19:22","","10.1145/3487890","","",,,,,3,3.00,0,8,1,"In recent years, with rapid technological advancement in both computing hardware and algorithm, Artificial Intelligence (AI) has demonstrated significant advantage over human being in a wide range of fields, such as image recognition, education, autonomous vehicles, finance, and medical diagnosis. However, AI-based systems are generally vulnerable to various security threats throughout the whole process, ranging from the initial data collection and preparation to the training, inference, and final deployment. In an AI-based system, the data collection and pre-processing phase are vulnerable to sensor spoofing attacks and scaling attacks, respectively, while the training and inference phases of the model are subject to poisoning attacks and adversarial attacks, respectively. To address these severe security threats against the AI-based systems, in this article, we review the challenges and recent research advances for security issues in AI, so as to depict an overall blueprint for AI security. More specifically, we first take the lifecycle of an AI-based system as a guide to introduce the security threats that emerge at each stage, which is followed by a detailed summary for corresponding countermeasures. Finally, some of the future challenges and opportunities for the security issues in AI will also be discussed.","",""
2,"Jeong Yeop Ryu, H. Chung, K. Choi","Potential role of artificial intelligence in craniofacial surgery",2021,"","","","",136,"2022-07-13 09:19:22","","10.7181/acfs.2021.00507","","",,,,,2,2.00,1,3,1,"The field of artificial intelligence (AI) is rapidly advancing, and AI models are increasingly applied in the medical field, especially in medical imaging, pathology, natural language processing, and biosignal analysis. On the basis of these advances, telemedicine, which allows people to receive medical services outside of hospitals or clinics, is also developing in many countries. The mechanisms of deep learning used in medical AI include convolutional neural networks, residual neural networks, and generative adversarial networks. Herein, we investigate the possibility of using these AI methods in the field of craniofacial surgery, with potential applications including craniofacial trauma, congenital anomalies, and cosmetic surgery.","",""
32,"D. Bates, A. Auerbach, Peter F. Schulam, A. Wright, S. Saria","Reporting and Implementing Interventions Involving Machine Learning and Artificial Intelligence",2020,"","","","",137,"2022-07-13 09:19:22","","10.7326/M19-0872","","",,,,,32,16.00,6,5,2,"Increasingly, interventions aimed at improving care are likely to use such technologies as machine learning and artificial intelligence. However, health care has been relatively late to adopt them. This article provides clinical examples in which machine learning and artificial intelligence are already in use in health care and appear to deliver benefit. Three key bottlenecks toward increasing the pace of diffusion and adoption are methodological issues in evaluation of artificial intelligence-based interventions, reporting standards to enable assessment of model performance, and issues that need to be addressed for an institution to adopt these interventions. Methodological best practices will include external validation, ideally at a different site; use of proactive learning algorithms to correct for site-specific biases and increase robustness as algorithms are deployed across multiple sites; addressing subgroup performance; and communicating to providers the uncertainty of predictions. Regarding reporting, especially important issues are the extent to which implementing standardized approaches for introducing clinical decision support has been followed, describing the data sources, reporting on data assumptions, and addressing biases. Although most health care organizations in the United States have adopted electronic health records, they may be ill prepared to adopt machine learning and artificial intelligence. Several steps can enable this: preparing data, developing tools to get suggestions to clinicians in useful ways, and getting clinicians engaged in the process. Open challenges and the role of regulation in this area are briefly discussed. Although these techniques have enormous potential to improve care and personalize recommendations for individuals, the hype regarding them is tremendous. Organizations will need to approach this domain carefully with knowledgeable partners to obtain the hoped-for benefits and avoid failures.","",""
0,"S. Bhasin, S. Garg, F. Regazzoni","Special Section on Attacking and Protecting Artificial Intelligence",2021,"","","","",138,"2022-07-13 09:19:22","","10.1049/CIT2.12023","","",,,,,0,0.00,0,3,1,"Modern Artificial Intelligence (AI) systems largely rely on advanced algorithms, including machine learning techniques such as deep learning. The research community has invested significant efforts in understanding these algorithms, optimally tuning them, and improving their performance, but it has mostly neglected the security facet of the problem. Recent attacks and exploits demonstrated that machine learning‐based algorithms are susceptible to attacks targeting computer systems, including backdoors, hardware Trojans and fault attacks, but are also susceptible to a range of attacks specifically targeting them, such as adversarial input perturbations. Implementations of machine learning algorithms are often crucial proprietary assets for companies and thus are required to be protected. It follows that implementations of AI‐based algorithms are an attractive target for piracy and illegitimate use and, as such, they need to be protected as all other IPs. This is equally important for machine learning algorithms running on remote servers vulnerable to micro‐architectural exploits. Protecting AI algorithms from all these attacks is not a trivial task. While vast research in hardware and software security have established several sound countermeasures, the specificity of the algorithms used in AI could make such countermeasures ineffective (or simply inapplicable), given the complex and resource intensive nature of the algorithms. The task of protection will become even more difficult in the near future, given the trend where part of the intelligence will be deployed directly into resource constrained cyber‐physical systems and IoT devices. AI models themselves should be protected against illegitimate and unauthorized use and distribution. Because of this, IP protection techniques such as watermarking, fingerprinting and attestation have been proposed, but, especially the last two, should be studied more in depth. To address all these security challenges, two actions are needed. First, we need a complete understanding of the attackers' capabilities. Second, novel and lightweight approaches for protecting AI algorithms, given the distributed level of intelligence, should be conceived and developed, including (but not limited to) obfuscation, finger‐printing, homomorphic encryption, and a new set of countermeasures to protect AI algorithms from adversarial input, backdooring and physical attacks. This Special Section covers problems related to attacking and protecting implementations of AI algorithms, and the use of AI to improve state‐of‐the‐art attacks such as physical attacks. It consists of three articles which are selected for publication after multiple rounds of peer review and scrutiny. An overview of these articles is discussed in the following. The first article reports different types of adversarial attacks, considering various threat models, followed by a discussion on the efficiency and challenges of state‐of‐the‐art countermeasures against them. It also provides a taxonomy for adversarial learning which can help future research to correctly categorize discovered vulnerabilities and plan protection mechanisms accordingly. The article concludes discussing open problems that can trigger further research on the topic. The second article takes a step towards disseminating knowledge about the widely popular and critical threat of side‐ channel attacks on neural networks. This survey considers and categorizes the most relevant threat models and corresponding attacks with different objectives including recovery of hyper‐ parameters, secret weights and inputs. The article differentiates between types of side‐channel attacks like physical, local or remote to highlight the applicability of various attacks and concludes with a discussion of countermeasures. The third article surveys AI model ownership protection techniques, the majority of them being based on watermarking, reporting advantages and disadvantage of them and highlighting possible research directions. The authors identified that, to date, the most studied technique is watermarking, that has been proposed in white box and black box settings. The articles also survey existing attacks aiming at removing or making ineffective IP protection techniques, and identify fingerprinting and attestation as two approaches are not yet studied in depth. Overall, the articles accepted cover a wide spectrum of problem providing readers with a perspective on the underlying","",""
0,"J. Curzon, T. A. Kosa, R. Akalu, K. El-Khatib","Privacy and Artificial Intelligence",2021,"","","","",139,"2022-07-13 09:19:22","","10.1109/TAI.2021.3088084","","",,,,,0,0.00,0,4,1,"Artificial intelligence is a rapidly developing field of research with many practical applications. Congruent to advances in technologies that enable big data, deep learning, and neural networks to train, learn, and predict, artificial intelligence creates new risks that are difficult to predict and manage. Such risks include economic turmoil, existential crises, and the dissolution of individual privacy. If unchecked, the capabilities of artificially intelligent systems could pose a fundamental threat to privacy in their operation or these systems may leak information under adversarial conditions. In this article, we survey the literature and provide various scenarios for the use of artificial intelligence, highlighting potential risks to privacy and offering various mitigating strategies. For the purpose of this research, a North American perspective of privacy is adopted. Impact statement—While an appreciation of the privacy risks associated with artificial intelligence is important, a thorough understanding of the assortment of different technologies that comprise artificial intelligence better prepares those implementing such systems in assessing privacy impacts. This can be achieved through the independent consideration of each constituent of an artificially intelligent system and its interactions. Under individual consideration, privacy-enhancing tools can be applied in a targeted manner to reduce the risk associated with specific components of an artificially intelligent system. A generalized North American approach to assess privacy risks in such systems is proposed that will retain applicability as the field of research evolves and can be adapted to account for various sociopolitical influences. With such an approach, privacy risks in artificial intelligent systems can be well understood, measured, and reduced.","",""
0,"A. Di Ieva, C. Russo, Abdulla Al Suman, Sidong Liu","IOTG-01. Computational Neurosurgery in Brain Tumors: A paradigm shift on the use of Artificial Intelligence and Connectomics in pre- and intra-operative imaging",2021,"","","","",140,"2022-07-13 09:19:22","","10.1093/neuonc/noab196.910","","",,,,,0,0.00,0,4,1,"  Computational Neurosurgery is a novel field where computational modeling and artificial intelligence (AI) are used to analyze diseases of neurosurgical interest. Our aim is to apply AI models to brain tumor (BT) images to a) automatically segment BTs on pre-operative MRI, b) predict the genetic subtype of glioma on intra- and post-operative histological specimens; and c) predict the extent of resection according to connectomics data. For the segmentation task, we used 510 BT images to train a deep learning (DL) model for automatic segmentation of the tumors’ edges and comparison of the AI-generated masks versus experts’ consensus (quantified by means of the dice score). For the histopathology task, we digitalized 266 hematoxylin/eosin slides of gliomas (including 130 IDH-wildtype and 136 IDH-mutant) and applied a DL architecture to predict the IDH genetic status, then validated by immunohistochemistry and genetic sequencing. The datasets were also augmented by generating synthetic glioma images by means of a Generative Adversarial Network methodology. The resection of 10 BTs was also customized according to connectomics data. In the segmentation experiment, we reached a dice score of ~0.9 (out of 1.0), while further demonstrating that only the T1, T1 after gadolinium, and FLAIR sequences are necessary for accurate automatic segmentation. In the histopathology task, we were able to predict the genetic status with accuracy between 76% and 95% using the DL model. The machine learning-based connectome analysis allowed us to perform safe supramaximal resection. We have shown the robustness of applying AI methodology or the automatic segmentation of BTs in MR imaging. Moreover, we have also shown that AI can be used to predict the genetic status, specifically, IDH, in histopathology images of gliomas. Our results support the use of AI in the clinical scenario for a fast and objective computerized characterization of patients affected by BTs.","",""
0,"Jie Wang, Xiangyuan Zheng, Qingdong He","Artificial Intelligence Applied to Extreme Value Prediction of Non-Gaussian Processes with Bandwidth Effect and Non-monotonicity",2021,"","","","",141,"2022-07-13 09:19:22","","10.1109/ICAICA52286.2021.9498204","","",,,,,0,0.00,0,3,1,"Extreme value prediction of a short-term non-Gaussian random process like ocean waves has been a tough issue for decades. In the 1990’s Winterstein proposed a cubic Hermite transformation using skewness and kurtosis, which has been widely applied in many areas for its accuracy and robustness. However, this approach is valid for monotonic transformation and narrow-banded processes. When the bandwidth of a random process is wide, no reasonable methods are available for acquiring the extreme value. This paper therefore applies the artificial neural network and genetic algorithm to do the extreme value prediction, without seeking rigorous mathematical derivations. Not only skewness and kurtosis are used, the spectral moments up to 4th-order reflecting bandwidth effects are also adopted. The results of many random case studies show that the artificial intelligence method is more accurate than the Hermite method in most of situations, especially for non-monotonic transformations. Besides, the artificial intelligence method has a wider application range.","",""
30,"Xinqin Liao, Wei‐ming Song, X. Zhang, Chaoqun Yan, Tianliang Li, Hongliang Ren, Cunzhi Liu, Yongtian Wang, Yuanjin Zheng","A bioinspired analogous nerve towards artificial intelligence",2020,"","","","",142,"2022-07-13 09:19:22","","10.1038/s41467-019-14214-x","","",,,,,30,15.00,3,9,2,"","",""
2,"Marie Franziska Thomas, F. Kofler, Lioba Grundl, T. Finck, Hongwei Li, C. Zimmer, Bjorn H. Menze, B. Wiestler","Improving Automated Glioma Segmentation in Routine Clinical Use Through Artificial Intelligence-Based Replacement of Missing Sequences With Synthetic Magnetic Resonance Imaging Scans.",2021,"","","","",143,"2022-07-13 09:19:22","","10.1097/RLI.0000000000000828","","",,,,,2,2.00,0,8,1,"OBJECTIVES Although automated glioma segmentation holds promise for objective assessment of tumor biology and response, its routine clinical use is impaired by missing sequences, for example, due to motion artifacts. The aim of our study was to develop and validate a generative adversarial network for synthesizing missing sequences to allow for a robust automated segmentation.   MATERIALS AND METHODS Our model was trained on data from The Cancer Imaging Archive (n = 238 WHO II-IV gliomas) to synthesize either missing FLAIR, T2-weighted, T1-weighted (T1w), or contrast-enhanced T1w images from available sequences, using a novel tumor-targeting loss to improve synthesis of tumor areas. We validated performance in a test set from both the REMBRANDT repository and our local institution (n = 68 WHO II-IV gliomas), using qualitative image appearance metrics, but also segmentation performance with state-of-the-art segmentation models. Segmentation of synthetic images was compared with 2 commonly used strategies for handling missing input data, entering a blank mask or copying an existing sequence.   RESULTS Across tumor areas and missing sequences, synthetic images generally outperformed both conventional approaches, in particular when FLAIR was missing. Here, for edema and whole tumor segmentation, we improved the Dice score, a common metric for evaluation of segmentation performance, by 12% and 11%, respectively, over the best conventional method. No method was able to reliably replace missing contrast-enhanced T1w images.   DISCUSSION Replacing missing nonenhanced magnetic resonance sequences via synthetic images significantly improves segmentation quality over most conventional approaches. This model is freely available and facilitates more widespread use of automated segmentation in routine clinical use, where missing sequences are common.","",""
56,"Sylvestre-Alvise Rebuffi, Sven Gowal, D. A. Calian, Florian Stimberg, Olivia Wiles, Timothy A. Mann","Fixing Data Augmentation to Improve Adversarial Robustness",2021,"","","","",144,"2022-07-13 09:19:22","","","","",,,,,56,56.00,9,6,1,"Adversarial training suffers from robust overfitting, a phenomenon where the robust test accuracy starts to decrease during training. In this paper, we focus on both heuristics-driven and data-driven augmentations as a means to reduce robust overfitting. First, we demonstrate that, contrary to previous findings, when combined with model weight averaging, data augmentation can significantly boost robust accuracy. Second, we explore how state-of-the-art generative models can be leveraged to artificially increase the size of the training set and further improve adversarial robustness. Finally, we evaluate our approach on CIFAR-10 against `∞ and `2 norm-bounded perturbations of size = 8/255 and = 128/255, respectively. We show large absolute improvements of +7.06% and +5.88% in robust accuracy compared to previous state-of-the-art methods. In particular, against `∞ norm-bounded perturbations of size = 8/255, our model reaches 64.20% robust accuracy without using any external data, beating most prior works that use external data.","",""
0,"Fadheela Hussain, Riadh Ksantini, M. Hammad","A Review of Malicious Altering Healthcare Imagery using Artificial Intelligence",2021,"","","","",145,"2022-07-13 09:19:22","","10.1109/3ICT53449.2021.9582068","","",,,,,0,0.00,0,3,1,"During the second half of 2020, healthcare is and has been the number one target for cybercrime, enormous amount of cyberattacks on hospitals and health systems increased, and specialists trust there are more to come. Attackers who can get the way to reach the electronic health record would exploit it and will use it for their own interest like deal or vend it on the underground economy, hostage the systems and the sensitive data, that has a significant impact on operations. This review tried to analyze how cyber attacker employ Generative Adversarial Networks (GANs) to alter the evidences of patient's medical conditions from image scans and reports. Cyber attacker has different purposes in order to obstruct a political applicant, lockup investigations, obligate insurance scam, execute an act of violence, or even commit homicide. Numerous correlated works constructed on gan in medical images practices had been reviews in the period between 2000 to 2021. Many papers showed how hospital system, physicians and radiology's specialists and the most recent researches showed an extremely exposed to different types of intrusion gan attacks.","",""
21,"Chuan Zhang, Yeong-Luh Ueng, Christoph Studer, A. Burg","Artificial Intelligence for 5G and Beyond 5G: Implementations, Algorithms, and Optimizations",2020,"","","","",146,"2022-07-13 09:19:22","","10.1109/JETCAS.2020.3000103","","",,,,,21,10.50,5,4,2,"The communication industry is rapidly advancing towards 5G and beyond 5G (B5G) wireless technologies in order to fulfill the ever-growing needs for higher data rates and improved quality-of-service (QoS). Emerging applications require wireless connectivity with tremendously increased data rates, substantially reduced latency, and growing support for a large number of devices. These requirements pose new challenges that can no longer be efficiently addressed by conventional approaches. Artificial intelligence (AI) is considered as one of the most promising solutions to improve the performance and robustness of 5G and B5G systems, fueled by the massive amount of data generated in 5G and B5G networks and the availability of powerful data processing fabrics. As a consequence, a plethora of research on AI-based communication technologies has emerged recently, promising higher data rates and improved QoS with affordable implementation overhead. In this overview paper, we summarize the state-of-the-art of AI-based 5G and B5G techniques on the algorithm, implementation, and optimization levels. We shed light on the advantages and limitations of AI-based solutions, and we provide a summary of emerging techniques and open research problems.","",""
18,"Martin Ragot, Nicolas Martin, Salomé Cojean","AI-generated vs. Human Artworks. A Perception Bias Towards Artificial Intelligence?",2020,"","","","",147,"2022-07-13 09:19:22","","10.1145/3334480.3382892","","",,,,,18,9.00,6,3,2,"Via generative adversarial networks (GANs), artificial intelligence (AI) has influenced many areas, especially the artistic field, as symbol of a human task. In human-computer interaction (HCI) studies, perception biases against AI, machines, or computers are generally cited. However, experimental evidence is still lacking. This paper presents a wide-scale experiment in which 565 participants are asked to evaluate paintings (which were created by humans or AI) on four dimensions: liking, perceived beauty, novelty, and meaning. A priming effect is evaluated using two between-subject conditions: Artworks presented as created by an AI, and artworks presented as created by a human artist. Finally, the paintings perceived as being drawn by human are evaluated significantly more highly than those perceived as being made by AI. Thus, using such a methodology and sample in an unprecedented way, the results show a negative bias of perception towards AI and a preference bias towards human systems.","",""
10,"Timothy Tadros, G. Krishnan, Ramyaa, M. Bazhenov","Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks",2020,"","","","",148,"2022-07-13 09:19:22","","","","",,,,,10,5.00,3,4,2,"Current artificial neural networks (ANNs) can perform and excel at a variety of tasks ranging from image classification to spam detection through training on large datasets of labeled data. While the trained network usually performs well on similar testing data, certain inputs that differ even slightly from the training data may trigger unpredictable behavior. Due to this limitation, it is possible to generate inputs with very small designed perturbations that can result in misclassification. These adversarial attacks present a security risk to deployed ANNs and indicate a divergence between how ANNs and humans perform classification. Humans are robust at behaving in the presence of noise and are capable of correctly classifying objects that are occluded, blurred, or otherwise distorted. It has been hypothesized that sleep promotes generalization and improves robustness against noise in animals and humans. In this work, we utilize a biologically inspired sleep phase in ANNs and demonstrate the benefit of sleep on defending against adversarial attacks as well as increasing ANN classification robustness. We compare the sleep algorithm's performance on various robustness tasks with two previously proposed adversarial defenses, defensive distillation and fine-tuning. We report an increase in robustness after sleep to adversarial attacks as well as to general image distortions for three datasets: MNIST, CUB200, and a toy dataset. Overall, these results demonstrate the potential for biologically inspired solutions to solve existing problems in ANNs and guide the development of more robust, human-like ANNs.","",""
11,"Abhiroop Bhattacharjee, P. Panda","Rethinking Non-idealities in Memristive Crossbars for Adversarial Robustness in Neural Networks",2020,"","","","",149,"2022-07-13 09:19:22","","","","",,,,,11,5.50,6,2,2,"\textit{Deep Neural Networks} (DNNs) have been shown to be prone to adversarial attacks. With a growing need to enable intelligence in embedded devices in this \textit{Internet of Things} (IoT) era, secure hardware implementation of DNNs has become imperative. Memristive crossbars, being able to perform \textit{Matrix-Vector-Multiplications} (MVMs) efficiently, are used to realize DNNs on hardware. However, crossbar non-idealities have always been devalued since they cause errors in performing MVMs, leading to degradation in the accuracy of the DNNs. Several software-based adversarial defenses have been proposed in the past to make DNNs adversarially robust. However, no previous work has demonstrated the advantage conferred by the non-idealities present in analog crossbars in terms of adversarial robustness. In this work, we show that the intrinsic hardware variations manifested through crossbar non-idealities yield adversarial robustness to the mapped DNNs without any additional optimization. We evaluate resilience of state-of-the-art DNNs (VGG8 \& VGG16 networks) using benchmark datasets (CIFAR-10 \& CIFAR-100) across various crossbar sizes towards both hardware and software adversarial attacks. We find that crossbar non-idealities unleash greater adversarial robustness ($>10-20\%$) in DNNs than baseline software DNNs. We further assess the performance of our approach with other state-of-the-art efficiency-driven adversarial defenses and find that our approach performs significantly well in terms of reducing adversarial losses.","",""
6,"Alfred Laugros, A. Caplier, Matthieu Ospici","Addressing Neural Network Robustness with Mixup and Targeted Labeling Adversarial Training",2020,"","","","",150,"2022-07-13 09:19:22","","10.1007/978-3-030-68238-5_14","","",,,,,6,3.00,2,3,2,"","",""
4,"Sajjad Amini, S. Ghaemmaghami","Towards Improving Robustness of Deep Neural Networks to Adversarial Perturbations",2020,"","","","",151,"2022-07-13 09:19:22","","10.1109/TMM.2020.2969784","","",,,,,4,2.00,2,2,2,"Deep neural networks have presented superlative performance in many machine learning based perception and recognition tasks, where they have even outperformed human precision in some applications. However, it has been found that human perception system is much more robust to adversarial perturbation, as compared to these artificial networks. It has been shown that a deep architecture with a lower Lipschitz constant can generalize better and tolerate higher level of adversarial perturbation. Smooth regularization has been proposed to control the Lipschitz constant of a deep architecture and in this work, we show how a deep convolutional neural network (CNN), based on non-smooth regularization of convolution and fully connected layers, can present enhanced generalization and robustness to adversarial perturbation, simultaneously. We propose two non-smooth regularizers that present specific features for adversarial samples with different levels of signal-to-noise ratios. The regularizers build direct interconnections for the weight matrices in each layer, through which they control the Lipschitz constant of architecture and improve the consistency of input-output mapping of the network. This leads to more reliable and interpretable network mapping and reduces abrupt changes in the networks output. We develop an efficient algorithm to solve the non-smooth learning problems, which presents a gradual complexity addition property. Our simulation results over three benchmark datasets signify the superiority of the proposed formulations over previously reported methods for improving the robustness of deep architecture, towards human robustness to adversarial samples.","",""
3,"Zhuorong Li, Chao Feng, Jianwei Zheng, Ming-hui Wu, Hongchuan Yu","Towards Adversarial Robustness via Feature Matching",2020,"","","","",152,"2022-07-13 09:19:22","","10.1109/ACCESS.2020.2993304","","",,,,,3,1.50,1,5,2,"Image classification systems are known to be vulnerable to adversarial attacks, which are imperceptibly perturbed but lead to spectacularly disgraceful classification. Adversarial training is one of the most effective defenses for improving the robustness of classifiers. We introduce an enhanced adversarial training approach in this work. Motivated by human’s consistently accurate perception of surroundings, we explore the artificial attention of deep neural networks in the context of adversarial classification. We begin with an empirical analysis of how the attention of artificial systems will change as the model undergoes adversarial attacks. Observation is that the class-specific attention gets diverted and subsequently induces wrong prediction. To that end, we propose a regularizer encouraging the consistency in the artificial attention on the clean image and its adversarial counterpart. Our method shows improved empirical robustness over the state-of-the-art, secures 55.74% adversarial accuracy on CIFAR-10 with perturbation budget of 8/255 under the challenging untargeted attack in white-box settings. Further evaluations on CIFAR-100 also show our potential for a desirable boost in adversarial robustness for deep neural networks. Code and trained models of our work are available at: https://github.com/lizhuorong/Towards-Adversarial-Robustness-via-Feature-matching","",""
13,"Yuanbin Wang, P. Zheng, Tao Peng, Huayong Yang, J. Zou","Smart additive manufacturing: Current artificial intelligence-enabled methods and future perspectives",2020,"","","","",153,"2022-07-13 09:19:22","","10.1007/s11431-020-1581-2","","",,,,,13,6.50,3,5,2,"","",""
23,"M. Mitchell","Abstraction and analogy‐making in artificial intelligence",2021,"","","","",154,"2022-07-13 09:19:22","","10.1111/nyas.14619","","",,,,,23,23.00,23,1,1,"Conceptual abstraction and analogy‐making are key abilities underlying humans' abilities to learn, reason, and robustly adapt their knowledge to new domains. Despite a long history of research on constructing artificial intelligence (AI) systems with these abilities, no current AI system is anywhere close to a capability of forming humanlike abstractions or analogies. This paper reviews the advantages and limitations of several approaches toward this goal, including symbolic methods, deep learning, and probabilistic program induction. The paper concludes with several proposals for designing challenge tasks and evaluation measures in order to make quantifiable and generalizable progress in this area.","",""
9,"M. Gorris, S. Hoogenboom, M. Wallace, J. V. van Hooft","Artificial intelligence for the management of pancreatic diseases",2020,"","","","",155,"2022-07-13 09:19:22","","10.1111/den.13875","","",,,,,9,4.50,2,4,2,"Novel artificial intelligence techniques are emerging in all fields of healthcare, including gastroenterology. The aim of this review is to give an overview of artificial intelligence applications in the management of pancreatic diseases. We performed a systematic literature search in PubMed and Medline up to May 2020 to identify relevant articles. Our results showed that the development of machine‐learning based applications is rapidly evolving in the management of pancreatic diseases, guiding precision medicine in clinical, endoscopic and radiologic settings. Before implementation into clinical practice, further research should focus on the external validation of novel techniques, clarifying the accuracy and robustness of these models.","",""
6,"N. Elkin-Koren","Contesting algorithms: Restoring the public interest in content filtering by artificial intelligence",2020,"","","","",156,"2022-07-13 09:19:22","","10.1177/2053951720932296","","",,,,,6,3.00,6,1,2,"In recent years, artificial intelligence has been deployed by online platforms to prevent the upload of allegedly illegal content or to remove unwarranted expressions. These systems are trained to spot objectionable content and to remove it, block it, or filter it out before it is even uploaded. Artificial intelligence filters offer a robust approach to content moderation which is shaping the public sphere. This dramatic shift in norm setting and law enforcement is potentially game-changing for democracy. Artificial intelligence filters carry censorial power, which could bypass traditional checks and balances secured by law. Their opaque and dynamic nature creates barriers to oversight, and conceals critical value choices and tradeoffs. Currently, we lack adequate tools to hold them accountable. This paper seeks to address this gap by introducing an adversarial procedure— – Contesting Algorithms. It proposes to deliberately introduce friction into the dominant removal systems governed by artificial intelligence. Algorithmic content moderation often seeks to optimize a single goal, such as removing copyright-infringing materials or blocking hate speech, while other values in the public interest, such as fair use or free speech, are often neglected. Contesting algorithms introduce an adversarial design which reflects conflicting values, and thereby may offer a check on dominant removal systems. Facilitating an adversarial intervention may promote democratic principles by keeping society in the loop. An adversarial public artificial intelligence system could enhance dynamic transparency, facilitate an alternative public articulation of social values using machine learning systems, and restore societal power to deliberate and determine social tradeoffs.","",""
2,"Mikel Arbiza Goenaga","A critique of contemporary artificial intelligence art: Who is Edmond de Belamy?",2020,"","","","",157,"2022-07-13 09:19:22","","10.1387/ausart.21490","","",,,,,2,1.00,2,1,2,"Edmond de Belamy is a 2018 painting made by french collective Obvious, created using a type of Artificial Intelligence algorithms called Generative Adversarial Networks, which was sold at Christie's auction house in New York for $432,500. This historic event the so-called auction of the ""first artwork made by an AI"" raises 3 interesting questions about authorship, originality, and the arts as a space for scientific inquiry. While some think that the current deployment of Machine Learning algorithms and Artificial Intelligence techniques that we are seeing in the art world today may be seen as the ultimate ""Gesamtkunstwerk"" or total artwork, other points of view express that not only we need this type of cultural artifacts as a critique of industrialized use of Artificial Intelligence, but also a strict criteria has to be delimited in order to review contemporary art made with Machine Learning techniques.","",""
5,"C. Kyrkou, A. Papachristodoulou, A. Kloukiniotis, A. Papandreou, A. Lalos, K. Moustakas, T. Theocharides","Towards Artificial-Intelligence-Based Cybersecurity for Robustifying Automated Driving Systems Against Camera Sensor Attacks",2020,"","","","",158,"2022-07-13 09:19:22","","10.1109/isvlsi49217.2020.00-11","","",,,,,5,2.50,1,7,2,"CARAMEL is a European project that aims amongst others to improve and extend cyberthreat detection and mitigation techniques for automotive driving systems. This paper highlights the important role that advanced artificial intelligence and machine learning techniques can have in proactively addressing modern autonomous vehicle cybersecurity challenges and on mitigating associated safety risks when dealing with targetted attacks on a vehicle's camera sensors. The cybersecurity solutions developed by CARAMEL are based on powerful AI tools and algorithms to combat security risks in automated driving systems and will be hosted on embedded processors and platforms. As such, it will be possible to have a specialized anti-hacking device that addresses newly introduced technological dimensions for increased robustness and cybersecurity in addition to industry needs for high speed, low latency, functional safety, light weight, low power consumption.","",""
111,"Zhihan Lv, Yang Han, A. Singh, Gunasekaran Manogaran, Haibin Lv","Trustworthiness in Industrial IoT Systems Based on Artificial Intelligence",2021,"","","","",159,"2022-07-13 09:19:22","","10.1109/TII.2020.2994747","","",,,,,111,111.00,22,5,1,"The intelligent industrial environment developed with the support of the new generation network cyber-physical system (CPS) can realize the high concentration of information resources. In order to carry out the analysis and quantification for the reliability of CPS, an automatic online assessment method for the reliability of CPS is proposed in this article. It builds an evaluation framework based on the knowledge of machine learning, designs an online rank algorithm, and realizes the online analysis and assessment in real time. The preventive measures can be taken timely, and the system can operate normally and continuously. Its reliability has been greatly improved. Based on the credibility of the Internet and the Internet of Things, a typical CPS control model based on the spatiotemporal correlation detection model is analyzed to determine the comprehensive reliability model analysis strategy. Based on this, in this article, we propose a CPS trusted robust intelligent control strategy and a trusted intelligent prediction model. Through the simulation analysis, the influential factors of attack defense resources and the dynamic process of distributed cooperative control are obtained. CPS defenders in the distributed cooperative control mode can be guided and select the appropriate defense resource input according to the CPS attack and defense environment.","",""
63,"Rohan Gupta, Devesh Srivastava, Mehar Sahu, Swati Tiwari, R. K. Ambasta, Pravir Kumar","Artificial intelligence to deep learning: machine intelligence approach for drug discovery",2021,"","","","",160,"2022-07-13 09:19:22","","10.1007/s11030-021-10217-3","","",,,,,63,63.00,11,6,1,"","",""
54,"G. Collins, P. Dhiman, Constanza L. Andaur Navarro, Jie Ma, L. Hooft, J. Reitsma, P. Logullo, Andrew Beam, Lily Peng, B. van Calster, M. van Smeden, R. Riley, K. Moons","Protocol for development of a reporting guideline (TRIPOD-AI) and risk of bias tool (PROBAST-AI) for diagnostic and prognostic prediction model studies based on artificial intelligence",2021,"","","","",161,"2022-07-13 09:19:22","","10.1136/bmjopen-2020-048008","","",,,,,54,54.00,5,13,1,"Introduction The Transparent Reporting of a multivariable prediction model of Individual Prognosis Or Diagnosis (TRIPOD) statement and the Prediction model Risk Of Bias ASsessment Tool (PROBAST) were both published to improve the reporting and critical appraisal of prediction model studies for diagnosis and prognosis. This paper describes the processes and methods that will be used to develop an extension to the TRIPOD statement (TRIPOD-artificial intelligence, AI) and the PROBAST (PROBAST-AI) tool for prediction model studies that applied machine learning techniques. Methods and analysis TRIPOD-AI and PROBAST-AI will be developed following published guidance from the EQUATOR Network, and will comprise five stages. Stage 1 will comprise two systematic reviews (across all medical fields and specifically in oncology) to examine the quality of reporting in published machine-learning-based prediction model studies. In stage 2, we will consult a diverse group of key stakeholders using a Delphi process to identify items to be considered for inclusion in TRIPOD-AI and PROBAST-AI. Stage 3 will be virtual consensus meetings to consolidate and prioritise key items to be included in TRIPOD-AI and PROBAST-AI. Stage 4 will involve developing the TRIPOD-AI checklist and the PROBAST-AI tool, and writing the accompanying explanation and elaboration papers. In the final stage, stage 5, we will disseminate TRIPOD-AI and PROBAST-AI via journals, conferences, blogs, websites (including TRIPOD, PROBAST and EQUATOR Network) and social media. TRIPOD-AI will provide researchers working on prediction model studies based on machine learning with a reporting guideline that can help them report key details that readers need to evaluate the study quality and interpret its findings, potentially reducing research waste. We anticipate PROBAST-AI will help researchers, clinicians, systematic reviewers and policymakers critically appraise the design, conduct and analysis of machine learning based prediction model studies, with a robust standardised tool for bias evaluation. Ethics and dissemination Ethical approval has been granted by the Central University Research Ethics Committee, University of Oxford on 10-December-2020 (R73034/RE001). Findings from this study will be disseminated through peer-review publications. PROSPERO registration number CRD42019140361 and CRD42019161764.","",""
2,"Sunny Raj","Towards Robust Artificial Intelligence Systems",2020,"","","","",162,"2022-07-13 09:19:22","","","","",,,,,2,1.00,2,1,2,"Adoption of deep neural networks (DNNs) into safety-critical and high-assurance systems has been hindered by the inability of DNNs to handle adversarial and out-of-distribution input. State-ofthe-art DNNs misclassify adversarial input and give high confidence output for out-of-distribution input. We attempt to solve this problem by employing two approaches, first, by detecting adversarial input and, second, by developing a confidence metric that can indicate when a DNN system has reached its limits and is not performing to the desired specifications. The effectiveness of our method at detecting adversarial input is demonstrated against the popular DeepFool adversarial image generation method. On a benchmark of 50,000 randomly chosen ImageNet adversarial images generated for CaffeNet and GoogLeNet DNNs, our method can recover the correct label with 95.76% and 97.43% accuracy, respectively. The proposed attribution-based confidence (ABC) metric utilizes attributions used to explain DNN output to characterize whether an output corresponding to an input to the DNN can be trusted. The attribution based approach removes the need to store training or test data or to train an ensemble of models to obtain confidence scores. Hence, the ABC metric can be used when only the trained DNN is available during inference. We test the effectiveness of the ABC metric against both adversarial and out-of-distribution input. We experimental demonstrate that the ABC metric is high for ImageNet input and low for adversarial input generated by FGSM, PGD, DeepFool, CW, and adversarial patch methods. For a DNN trained on MNIST images, ABC metric is high for in-distribution MNIST input and low for out-of-distribution Fashion-MNIST and notMNIST input.","",""
21,"Adrien Bécue, Isabel Praça, J. Gama","Artificial intelligence, cyber-threats and Industry 4.0: challenges and opportunities",2021,"","","","",163,"2022-07-13 09:19:22","","10.1007/S10462-020-09942-2","","",,,,,21,21.00,7,3,1,"","",""
15,"S. Ebrahimian, Fatemeh Homayounieh, M. Rockenbach, Preetham Putha, T. Raj, I. Dayan, B. Bizzo, Varun Buch, Dufan Wu, Kyungsang Kim, Quanzheng Li, S. Digumarthy, M. Kalra","Artificial intelligence matches subjective severity assessment of pneumonia for prediction of patient outcome and need for mechanical ventilation: a cohort study",2021,"","","","",164,"2022-07-13 09:19:22","","10.1038/s41598-020-79470-0","","",,,,,15,15.00,2,13,1,"","",""
16,"A. Amritphale, Ranojoy Chatterjee, Suvo Chatterjee, N. Amritphale, Ali Rahnavard, G. Awan, B. Omar, G. Fonarow","Predictors of 30-Day Unplanned Readmission After Carotid Artery Stenting Using Artificial Intelligence",2021,"","","","",165,"2022-07-13 09:19:22","","10.1007/s12325-021-01709-7","","",,,,,16,16.00,2,8,1,"","",""
0,"Cyril Goutte, Xiao-Dan Zhu, R. Goebel, Yuzuru Tanaka","Advances in Artificial Intelligence: 33rd Canadian Conference on Artificial Intelligence, Canadian AI 2020, Ottawa, ON, Canada, May 13–15, 2020, Proceedings",2020,"","","","",166,"2022-07-13 09:19:22","","10.1007/978-3-030-47358-7","","",,,,,0,0.00,0,4,2,"","",""
52,"Jian-hua Li","Cyber security meets artificial intelligence: a survey",2018,"","","","",167,"2022-07-13 09:19:22","","10.1631/FITEE.1800573","","",,,,,52,13.00,52,1,4,"There is a wide range of interdisciplinary intersections between cyber security and artificial intelligence (AI). On one hand, AI technologies, such as deep learning, can be introduced into cyber security to construct smart models for implementing malware classification and intrusion detection and threating intelligence sensing. On the other hand, AI models will face various cyber threats, which will disturb their sample, learning, and decisions. Thus, AI models need specific cyber security defense and protection technologies to combat adversarial machine learning, preserve privacy in machine learning, secure federated learning, etc. Based on the above two aspects, we review the intersection of AI and cyber security. First, we summarize existing research efforts in terms of combating cyber attacks using AI, including adopting traditional machine learning methods and existing deep learning solutions. Then, we analyze the counterattacks from which AI itself may suffer, dissect their characteristics, and classify the corresponding defense methods. Finally, from the aspects of constructing encrypted neural network and realizing a secure federated deep learning, we expatiate the existing research on how to build a secure AI system.","",""
17,"Alfred Laugros, A. Caplier, Matthieu Ospici","Are Adversarial Robustness and Common Perturbation Robustness Independent Attributes ?",2019,"","","","",168,"2022-07-13 09:19:22","","10.1109/ICCVW.2019.00134","","",,,,,17,5.67,6,3,3,"Neural Networks have been shown to be sensitive to common perturbations such as blur, Gaussian noise, rotations, etc. They are also vulnerable to some artificial malicious corruptions called adversarial examples. The adversarial examples study has recently become very popular and it sometimes even reduces the term ""adversarial robustness"" to the term ""robustness"". Yet, we do not know to what extent the adversarial robustness is related to the global robustness. Similarly, we do not know if a robustness to various common perturbations such as translations or contrast losses for instance, could help with adversarial corruptions. We intend to study the links between the robustnesses of neural networks to both perturbations. With our experiments, we provide one of the first benchmark designed to estimate the robustness of neural networks to common perturbations. We show that increasing the robustness to carefully selected common perturbations, can make neural networks more robust to unseen common perturbations. We also prove that adversarial robustness and robustness to common perturbations are independent. Our results make us believe that neural network robustness should be addressed in a broader sense.","",""
17,"Saima Sharmin, P. Panda, Syed Shakib Sarwar, Chankyu Lee, Wachirawit Ponghiran, K. Roy","A Comprehensive Analysis on Adversarial Robustness of Spiking Neural Networks",2019,"","","","",169,"2022-07-13 09:19:22","","10.1109/IJCNN.2019.8851732","","",,,,,17,5.67,3,6,3,"In this era of machine learning models, their functionality is being threatened by adversarial attacks. In the face of this struggle for making artificial neural networks robust, finding a model, resilient to these attacks, is very important. In this work, we present, for the first time, a comprehensive analysis of the behavior of more bio-plausible networks, namely Spiking Neural Network (SNN) under state-of-the-art adversarial tests. We perform a comparative study of the accuracy degradation between conventional VGG-9 Artificial Neural Network (ANN) and equivalent spiking network with CIFAR-10 dataset in both whitebox and blackbox setting for different types of single-step and multi-step FGSM (Fast Gradient Sign Method) attacks. We demonstrate that SNNs tend to show more resiliency compared to ANN under blackbox attack scenario. Additionally, we find that SNN robustness is largely dependent on the corresponding training mechanism. We observe that SNNs trained by spike-based backpropagation are more adversarially robust than the ones obtained by ANN-to-SNN conversion rules in several whitebox and blackbox scenarios. Finally, we also propose a simple, yet, effective framework for crafting adversarial attacks from SNNs. Our results suggest that attacks crafted from SNNs following our proposed method are much stronger than those crafted from ANNs.","",""
15,"J. Janet, Chenru Duan, A. Nandy, Fang Liu, H. Kulik","Navigating Transition-Metal Chemical Space: Artificial Intelligence for First-Principles Design.",2021,"","","","",170,"2022-07-13 09:19:22","","10.1021/acs.accounts.0c00686","","",,,,,15,15.00,3,5,1,"ConspectusThe variability of chemical bonding in open-shell transition-metal complexes not only motivates their study as functional materials and catalysts but also challenges conventional computational modeling tools. Here, tailoring ligand chemistry can alter preferred spin or oxidation states as well as electronic structure properties and reactivity, creating vast regions of chemical space to explore when designing new materials atom by atom. Although first-principles density functional theory (DFT) remains the workhorse of computational chemistry in mechanism deduction and property prediction, it is of limited use here. DFT is both far too computationally costly for widespread exploration of transition-metal chemical space and also prone to inaccuracies that limit its predictive performance for localized d electrons in transition-metal complexes. These challenges starkly contrast with the well-trodden regions of small-organic-molecule chemical space, where the analytical forms of molecular mechanics force fields and semiempirical theories have for decades accelerated the discovery of new molecules, accurate DFT functional performance has been demonstrated, and gold-standard methods from correlated wavefunction theory can predict experimental results to chemical accuracy.The combined promise of transition-metal chemical space exploration and lack of established tools has mandated a distinct approach. In this Account, we outline the path we charted in exploration of transition-metal chemical space starting from the first machine learning (ML) models (i.e., artificial neural network and kernel ridge regression) and representations for the prediction of open-shell transition-metal complex properties. The distinct importance of the immediate coordination environment of the metal center as well as the lack of low-level methods to accurately predict structural properties in this coordination environment first motivated and then benefited from these ML models and representations. Once developed, the recipe for prediction of geometric, spin state, and redox potential properties was straightforwardly extended to a diverse range of other properties, including in catalysis, computational ""feasibility"", and the gas separation properties of periodic metal-organic frameworks. Interpretation of selected features most important for model prediction revealed new ways to encapsulate design rules and confirmed that models were robustly mapping essential structure-property relationships. Encountering the special challenge of ensuring that good model performance could generalize to new discovery targets motivated investigation of how to best carry out model uncertainty quantification. Distance-based approaches, whether in model latent space or in carefully engineered feature space, provided intuitive measures of the domain of applicability. With all of these pieces together, ML can be harnessed as an engine to tackle the large-scale exploration of transition-metal chemical space needed to satisfy multiple objectives using efficient global optimization methods. In practical terms, bringing these artificial intelligence tools to bear on the problems of transition-metal chemical space exploration has resulted in ML-model assessments of large, multimillion compound spaces in minutes and validated new design leads in weeks instead of decades.","",""
31,"T. Ertekin, Qian Sun","Artificial Intelligence Applications in Reservoir Engineering: A Status Check",2019,"","","","",171,"2022-07-13 09:19:22","","10.3390/EN12152897","","",,,,,31,10.33,16,2,3,"This article provides a comprehensive review of the state-of-art in the area of artificial intelligence applications to solve reservoir engineering problems. Research works including proxy model development, artificial-intelligence-assisted history-matching, project design, and optimization, etc. are presented to demonstrate the robustness of the intelligence systems. The successes of the developments prove the advantages of the AI approaches in terms of high computational efficacy and strong learning capabilities. Thus, the implementation of intelligence models enables reservoir engineers to accomplish many challenging and time-intensive works more effectively. However, it is not yet astute to completely replace the conventional reservoir engineering models with intelligent systems, since the defects of the technology cannot be ignored. The trend of research and industrial practices of reservoir engineering area would be establishing a hand-shaking protocol between the conventional modeling and the intelligent systems. Taking advantages of both methods, more robust solutions could be obtained with significantly less computational overheads.","",""
21,"N. Anantrasirichai, D. Bull","Artificial intelligence in the creative industries: a review",2020,"","","","",172,"2022-07-13 09:19:22","","10.1007/s10462-021-10039-7","","",,,,,21,10.50,11,2,2,"","",""
29,"Melanie Mitchell","Artificial Intelligence Hits the Barrier of Meaning",2019,"","","","",173,"2022-07-13 09:19:22","","10.3390/info10020051","","",,,,,29,9.67,29,1,3,"Today’s AI systems sorely lack the essence of human intelligence: Understanding the situations we experience, being able to grasp their meaning. The lack of humanlike understanding in machines is underscored by recent studies demonstrating lack of robustness of state-of-the-art deep-learning systems. Deeper networks and larger datasets alone are not likely to unlock AI’s “barrier of meaning”; instead the field will need to embrace its original roots as an interdisciplinary science of intelligence.","",""
0,"K. Hemachandran, Priti Verma, Purvi Pareek, Nidhi Arora, Korupalli V. Rajesh Kumar, T. Ahanger, Anil Audumbar Pise, R. Ratna","Artificial Intelligence: A Universal Virtual Tool to Augment Tutoring in Higher Education",2022,"","","","",174,"2022-07-13 09:19:22","","10.1155/2022/1410448","","",,,,,0,0.00,0,8,1,"Artificial intelligence is an emerging technology that revolutionizes human lives. Despite the fact that this technology is used in higher education, many professors are unaware of it. In this current scenario, there is a huge need to arise, implement information bridge technology, and enhance communication in the classroom. Through this paper, the authors try to predict the future of higher education with the help of artificial intelligence. This research article throws light on the current education system the problems faced by the subject faculties, students, changing government rules, and regulations in the educational sector. Various arguments and challenges on the implementation of artificial intelligence are prevailing in the educational sector. In this concern, we have built a use case model by using a student assessment data of our students and then built a synthesized using generative adversarial network (GAN). The dataset analyzed, visualized, and fed to different machine learning algorithms such as logistic Regression (LR), linear discriminant analysis (LDA), K-nearest neighbors (KNN), classification and regression trees (CART), naive Bayes (NB), support vector machines (SVM), and finally random forest (RF) algorithm and achieved a maximum accuracy of 58%. This article aims to bridge the gap between human lecturers and the machine. We are also concerned about the psychological emotions of the faculty and the students when artificial intelligence takes control.","",""
0,"Konstantinos G. Liakos, G. Georgakilas, F. Plessas, P. Kitsos","GAINESIS: Generative Artificial Intelligence NEtlists SynthesIS",2022,"","","","",175,"2022-07-13 09:19:22","","10.3390/electronics11020245","","",,,,,0,0.00,0,4,1,"A significant problem in the field of hardware security consists of hardware trojan (HT) viruses. The insertion of HTs into a circuit can be applied for each phase of the circuit chain of production. HTs degrade the infected circuit, destroy it or leak encrypted data. Nowadays, efforts are being made to address HTs through machine learning (ML) techniques, mainly for the gate-level netlist (GLN) phase, but there are some restrictions. Specifically, the number and variety of normal and infected circuits that exist through the free public libraries, such as Trust-HUB, are based on the few samples of benchmarks that have been created from circuits large in size. Thus, it is difficult, based on these data, to develop robust ML-based models against HTs. In this paper, we propose a new deep learning (DL) tool named Generative Artificial Intelligence Netlists SynthesIS (GAINESIS). GAINESIS is based on the Wasserstein Conditional Generative Adversarial Network (WCGAN) algorithm and area–power analysis features from the GLN phase and synthesizes new normal and infected circuit samples for this phase. Based on our GAINESIS tool, we synthesized new data sets, different in size, and developed and compared seven ML classifiers. The results demonstrate that our new generated data sets significantly enhance the performance of ML classifiers compared with the initial data set of Trust-HUB.","",""
0,"M. Florez, M. Caporale, Pakpoom Buabthong, Z. Ross, D. Asimaki, Men‐Andrin Meier","Data-Driven Synthesis of Broadband Earthquake Ground Motions Using Artificial Intelligence",2022,"","","","",176,"2022-07-13 09:19:22","","10.1785/0120210264","","",,,,,0,0.00,0,6,1,"  Robust estimation of ground motions generated by scenario earthquakes is critical for many engineering applications. We leverage recent advances in generative adversarial networks (GANs) to develop a new framework for synthesizing earthquake acceleration time histories. Our approach extends the Wasserstein GAN formulation to allow for the generation of ground motions conditioned on a set of continuous physical variables. Our model is trained to approximate the intrinsic probability distribution of a massive set of strong-motion recordings from Japan. We show that the trained generator model can synthesize realistic three-component accelerograms conditioned on magnitude, distance, and VS30. Our model captures most of the relevant statistical features of the acceleration spectra and waveform envelopes. The output seismograms display clear P- and S-wave arrivals with the appropriate energy content and relative onset timing. The synthesized peak ground acceleration estimates are also consistent with observations. We develop a set of metrics that allow us to assess the training process’s stability and to tune model hyperparameters. We further show that the trained generator network can interpolate to conditions in which no earthquake ground-motion recordings exist. Our approach allows for the on-demand synthesis of accelerograms for engineering purposes.","",""
0,"Pan Wang, Yangyang Zhong, Zhenan Yao","Modeling and Estimation of CO2 Emissions in China Based on Artificial Intelligence",2022,"","","","",177,"2022-07-13 09:19:22","","10.1155/2022/6822467","","",,,,,0,0.00,0,3,1,"Since China’s reform and opening up, the social economy has achieved rapid development, followed by a sharp increase in carbon dioxide (CO2) emissions. Therefore, at the 75th United Nations General Assembly, China proposed to achieve carbon peaking by 2030 and carbon neutrality by 2060. The research work on advance forecasting of CO2 emissions is essential to achieve the above-mentioned carbon peaking and carbon neutrality goals in China. In order to achieve accurate prediction of CO2 emissions, this study establishes a hybrid intelligent algorithm model suitable for CO2 emissions prediction based on China’s CO2 emissions and related socioeconomic indicator data from 1971 to 2017. The hyperparameters of Least Squares Support Vector Regression (LSSVR) are optimized by the Adaptive Artificial Bee Colony (AABC) algorithm to build a high-performance hybrid intelligence model. The research results show that the hybrid intelligent algorithm model designed in this paper has stronger robustness and accuracy with relative error almost within ±5% in the advance prediction of CO2 emissions. The modeling scheme proposed in this study can not only provide strong support for the Chinese government and industry departments to formulate policies related to the carbon peaking and carbon neutrality goals, but also can be extended to the research of other socioeconomic-related issues.","",""
10,"T. Penzkofer, A. Padhani, B. Turkbey, M. Haider, H. Huisman, J. Walz, G. Salomon, I. Schoots, J. Richenberg, G. Villeirs, V. Panebianco, O. Rouvière, V. Løgager, J. Barentsz","ESUR/ESUI position paper: developing artificial intelligence for precision diagnosis of prostate cancer using magnetic resonance imaging",2021,"","","","",178,"2022-07-13 09:19:22","","10.1007/s00330-021-08021-6","","",,,,,10,10.00,1,14,1,"","",""
10,"Zihao Chen, Long Hu, Baoting Zhang, Aiping Lu, Yaofeng Wang, Yuanyuan Yu, Ge Zhang","Artificial Intelligence in Aptamer–Target Binding Prediction",2021,"","","","",179,"2022-07-13 09:19:22","","10.3390/ijms22073605","","",,,,,10,10.00,1,7,1,"Aptamers are short single-stranded DNA, RNA, or synthetic Xeno nucleic acids (XNA) molecules that can interact with corresponding targets with high affinity. Owing to their unique features, including low cost of production, easy chemical modification, high thermal stability, reproducibility, as well as low levels of immunogenicity and toxicity, aptamers can be used as an alternative to antibodies in diagnostics and therapeutics. Systematic evolution of ligands by exponential enrichment (SELEX), an experimental approach for aptamer screening, allows the selection and identification of in vitro aptamers with high affinity and specificity. However, the SELEX process is time consuming and characterization of the representative aptamer candidates from SELEX is rather laborious. Artificial intelligence (AI) could help to rapidly identify the potential aptamer candidates from a vast number of sequences. This review discusses the advancements of AI pipelines/methods, including structure-based and machine/deep learning-based methods, for predicting the binding ability of aptamers to targets. Structure-based methods are the most used in computer-aided drug design. For this part, we review the secondary and tertiary structure prediction methods for aptamers, molecular docking, as well as molecular dynamic simulation methods for aptamer–target binding. We also performed analysis to compare the accuracy of different secondary and tertiary structure prediction methods for aptamers. On the other hand, advanced machine-/deep-learning models have witnessed successes in predicting the binding abilities between targets and ligands in drug discovery and thus potentially offer a robust and accurate approach to predict the binding between aptamers and targets. The research utilizing machine-/deep-learning techniques for prediction of aptamer–target binding is limited currently. Therefore, perspectives for models, algorithms, and implementation strategies of machine/deep learning-based methods are discussed. This review could facilitate the development and application of high-throughput and less laborious in silico methods in aptamer selection and characterization.","",""
4,"Dulanga Weerakoon, Kasthuri Jayarajah, Randy Tandriansyah, Archan Misra","Resilient Collaborative Intelligence for Adversarial IoT Environments",2019,"","","","",180,"2022-07-13 09:19:22","","","","",,,,,4,1.33,1,4,3,"Many IoT networks, including for battlefield deployments, involve the deployment of resource-constrained sensors with varying degrees of redundancy/overlap (i.e., their data streams possess significant spatiotemporal correlation). Collaborative intelligence, whereby individual nodes adjust their inferencing pipelines to incorporate such correlated observations from other nodes, can improve both inferencing accuracy and performance metrics (such as latency and energy overheads). Using realworld data from a multicamera deployment, we first demonstrate the significant performance gains (up to 14% increase in accuracy) from such collaborative intelligence, achieved through two different approaches: (a) one involving statistical fusion of outputs from different nodes, and (b) another involving the development of new collaborative deep neural networks (DNNs). We then show that these collaboration-driven performance gains are susceptible to adversarial behaviour by one or more nodes, and thus need resilient mechanisms to provide robustness against such malicious behaviour. We also introduce an under-development testbed at Singapore Management University (SMU), specifically designed to enable real-world experimentation with such collaborative IoT intelligence techniques.","",""
11,"Qiuju Yang, Qiuju Yang, D. Tao, Desheng Han, Jimin Liang","Extracting Auroral Key Local Structures From All‐Sky Auroral Images by Artificial Intelligence Technique",2019,"","","","",181,"2022-07-13 09:19:22","","10.1029/2018JA026119","","",,,,,11,3.67,2,5,3,"Extracting auroral key local structures (KLS) containing both morphological information and spatial location from large amount of auroral images is the key for automatic auroral classification and event recognition and thus is very important for improving the efficiency of aurora study. However, it is labor‐intensive for human experts and challenging for computer execution because of the blurry boundary and inhomogeneous luminosity of aurora. In this work, we formulate the automatic KLS extraction to learn a mapping from an all‐sky auroral image to a KLS mask style with artificial intelligence technique. We explore the Cycle‐consistent Generative Adversarial Network to generate the KLS masks based on the unpaired training data set consisting of 2508 all‐sky auroral images observed between years 2003–2009 at the Arctic Yellow River Station, of which only 200 images were manually annotated as the ground truth of KLS masks. The extracted KLS are consistent with human visual perception, and the intersection‐over‐union metric is significantly improved by at least 17% than previous method by Niu et al. (2018) (https://doi.org/10.1109/TGRS.2018.2848725). Classification accuracy has been improved when combining the KLS masks further indicates the effectiveness and value of this work.","",""
9,"David Manheim","Multiparty Dynamics and Failure Modes for Machine Learning and Artificial Intelligence",2018,"","","","",182,"2022-07-13 09:19:22","","10.3390/BDCC3020021","","",,,,,9,2.25,9,1,4,"An important challenge for safety in machine learning and artificial intelligence systems is a set of related failures involving specification gaming, reward hacking, fragility to distributional shifts, and Goodhart’s or Campbell’s law. This paper presents additional failure modes for interactions within multi-agent systems that are closely related. These multi-agent failure modes are more complex, more problematic, and less well understood than the single-agent case, and are also already occurring, largely unnoticed. After motivating the discussion with examples from poker-playing artificial intelligence (AI), the paper explains why these failure modes are in some senses unavoidable. Following this, the paper categorizes failure modes, provides definitions, and cites examples for each of the modes: accidental steering, coordination failures, adversarial misalignment, input spoofing and filtering, and goal co-option or direct hacking. The paper then discusses how extant literature on multi-agent AI fails to address these failure modes, and identifies work which may be useful for the mitigation of these failure modes.","",""
8,"Philip G. Feldman, Aaron Dant, Aaron K. Massey","Integrating Artificial Intelligence into Weapon Systems",2019,"","","","",183,"2022-07-13 09:19:22","","","","",,,,,8,2.67,3,3,3,"The integration of Artificial Intelligence (AI) into weapon systems is one of the most consequential tactical and strategic decisions in the history of warfare. Current AI development is a remarkable combination of accelerating capability, hidden decision mechanisms, and decreasing costs. Implementation of these systems is in its infancy and exists on a spectrum from resilient and flexible to simplistic and brittle. Resilient systems should be able to effectively handle the complexities of a high-dimensional battlespace. Simplistic AI implementations could be manipulated by an adversarial AI that identifies and exploits their weaknesses.  In this paper, we present a framework for understanding the development of dynamic AI/ML systems that interactively and continuously adapt to their user's needs. We explore the implications of increasingly capable AI in the kill chain and how this will lead inevitably to a fully automated, always on system, barring regulation by treaty. We examine the potential of total integration of cyber and physical security and how this likelihood must inform the development of AI-enabled systems with respect to the ""fog of war"", human morals, and ethics.","",""
143,"G. Marcus","The Next Decade in AI: Four Steps Towards Robust Artificial Intelligence",2020,"","","","",184,"2022-07-13 09:19:22","","","","",,,,,143,71.50,143,1,2,"Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.","",""
7,"J. Haubold","[Artificial Intelligence in radiology : What can be expected in the next few years?]",2019,"","","","",185,"2022-07-13 09:19:22","","10.1007/s00117-019-00621-0","","",,,,,7,2.33,7,1,3,"","",""
86,"Helin Yang, A. Alphones, Zehui Xiong, D. Niyato, Jun Zhao, Kaishun Wu","Artificial-Intelligence-Enabled Intelligent 6G Networks",2019,"","","","",186,"2022-07-13 09:19:22","","10.1109/MNET.011.2000195","","",,,,,86,28.67,14,6,3,"With the rapid development of smart terminals and infrastructures, as well as diversified applications (e.g., virtual and augmented reality, remote surgery and holographic projection) with colorful requirements, current networks (e.g., 4G and upcoming 5G networks) may not be able to completely meet quickly rising traffic demands. Accordingly, efforts from both industry and academia have already been put to the research on 6G networks. Recently, artificial intelligence (Ai) has been utilized as a new paradigm for the design and optimization of 6G networks with a high level of intelligence. Therefore, this article proposes an Ai-enabled intelligent architecture for 6G networks to realize knowledge discovery, smart resource management, automatic network adjustment and intelligent service provisioning, where the architecture is divided into four layers: intelligent sensing layer, data mining and analytics layer, intelligent control layer and smart application layer. We then review and discuss the applications of Ai techniques for 6G networks and elaborate how to employ the Ai techniques to efficiently and effectively optimize the network performance, including Ai-empowered mobile edge computing, intelligent mobility and handover management, and smart spectrum management. We highlight important future research directions and potential solutions for Ai-enabled intelligent 6G networks, including computation efficiency, algorithms robustness, hardware development and energy management.","",""
4,"Osonde A. Osoba, P. Davis","An Artificial Intelligence/Machine Learning Perspective on Social Simulation: New Data and New Challenges",2018,"","","","",187,"2022-07-13 09:19:22","","10.1002/9781119485001.CH19","","",,,,,4,1.00,2,2,4,"We review issues of data infrastructure and artificial intelligence for social and behavioral modeling. Among the newer machine learning methods,adversarial training and fuzzy cognitive maps have particular unrealized potential.","",""
63,"M. VerMilyea, J. Hall, S. Diakiw, A. Johnston, T. Nguyen, D. Perugini, A. Miller, A. Picou, A. P. Murphy, M. Perugini","Development of an artificial intelligence-based assessment model for prediction of embryo viability using static images captured by optical light microscopy during IVF",2020,"","","","",188,"2022-07-13 09:19:22","","10.1093/humrep/deaa013","","",,,,,63,31.50,6,10,2,"Abstract STUDY QUESTION Can an artificial intelligence (AI)-based model predict human embryo viability using images captured by optical light microscopy? SUMMARY ANSWER We have combined computer vision image processing methods and deep learning techniques to create the non-invasive Life Whisperer AI model for robust prediction of embryo viability, as measured by clinical pregnancy outcome, using single static images of Day 5 blastocysts obtained from standard optical light microscope systems. WHAT IS KNOWN ALREADY Embryo selection following IVF is a critical factor in determining the success of ensuing pregnancy. Traditional morphokinetic grading by trained embryologists can be subjective and variable, and other complementary techniques, such as time-lapse imaging, require costly equipment and have not reliably demonstrated predictive ability for the endpoint of clinical pregnancy. AI methods are being investigated as a promising means for improving embryo selection and predicting implantation and pregnancy outcomes. STUDY DESIGN, SIZE, DURATION These studies involved analysis of retrospectively collected data including standard optical light microscope images and clinical outcomes of 8886 embryos from 11 different IVF clinics, across three different countries, between 2011 and 2018. PARTICIPANTS/MATERIALS, SETTING, METHODS The AI-based model was trained using static two-dimensional optical light microscope images with known clinical pregnancy outcome as measured by fetal heartbeat to provide a confidence score for prediction of pregnancy. Predictive accuracy was determined by evaluating sensitivity, specificity and overall weighted accuracy, and was visualized using histograms of the distributions of predictions. Comparison to embryologists’ predictive accuracy was performed using a binary classification approach and a 5-band ranking comparison. MAIN RESULTS AND THE ROLE OF CHANCE The Life Whisperer AI model showed a sensitivity of 70.1% for viable embryos while maintaining a specificity of 60.5% for non-viable embryos across three independent blind test sets from different clinics. The weighted overall accuracy in each blind test set was >63%, with a combined accuracy of 64.3% across both viable and non-viable embryos, demonstrating model robustness and generalizability beyond the result expected from chance. Distributions of predictions showed clear separation of correctly and incorrectly classified embryos. Binary comparison of viable/non-viable embryo classification demonstrated an improvement of 24.7% over embryologists’ accuracy (P = 0.047, n = 2, Student’s t test), and 5-band ranking comparison demonstrated an improvement of 42.0% over embryologists (P = 0.028, n = 2, Student’s t test). LIMITATIONS, REASONS FOR CAUTION The AI model developed here is limited to analysis of Day 5 embryos; therefore, further evaluation or modification of the model is needed to incorporate information from different time points. The endpoint described is clinical pregnancy as measured by fetal heartbeat, and this does not indicate the probability of live birth. The current investigation was performed with retrospectively collected data, and hence it will be of importance to collect data prospectively to assess real-world use of the AI model. WIDER IMPLICATIONS OF THE FINDINGS These studies demonstrated an improved predictive ability for evaluation of embryo viability when compared with embryologists’ traditional morphokinetic grading methods. The superior accuracy of the Life Whisperer AI model could lead to improved pregnancy success rates in IVF when used in a clinical setting. It could also potentially assist in standardization of embryo selection methods across multiple clinical environments, while eliminating the need for complex time-lapse imaging equipment. Finally, the cloud-based software application used to apply the Life Whisperer AI model in clinical practice makes it broadly applicable and globally scalable to IVF clinics worldwide. STUDY FUNDING/COMPETING INTEREST(S) Life Whisperer Diagnostics, Pty Ltd is a wholly owned subsidiary of the parent company, Presagen Pty Ltd. Funding for the study was provided by Presagen with grant funding received from the South Australian Government: Research, Commercialisation and Startup Fund (RCSF). ‘In kind’ support and embryology expertise to guide algorithm development were provided by Ovation Fertility. J.M.M.H., D.P. and M.P. are co-owners of Life Whisperer and Presagen. Presagen has filed a provisional patent for the technology described in this manuscript (52985P pending). A.P.M. owns stock in Life Whisperer, and S.M.D., A.J., T.N. and A.P.M. are employees of Life Whisperer.","",""
9,"B. N. Manjunatha Reddy, S. K. Pramada, T. Roshni","Monthly surface runoff prediction using artificial intelligence: A study from a tropical climate river basin",2021,"","","","",189,"2022-07-13 09:19:22","","10.1007/s12040-020-01508-8","","",,,,,9,9.00,3,3,1,"","",""
8,"Linbo Liu, Mingcheng Bi, Yunhua Wang, Junfeng Liu, Xiwen Jiang, Zhongbin Xu, Xingcai Zhang","Artificial intelligence-powered microfluidics for nanomedicine and materials synthesis.",2021,"","","","",190,"2022-07-13 09:19:22","","10.1039/d1nr06195j","","",,,,,8,8.00,1,7,1,"Artificial intelligence (AI) is an emerging technology with great potential, and its robust calculation and analysis capabilities are unmatched by traditional calculation tools. With the promotion of deep learning and open-source platforms, the threshold of AI has also become lower. Combining artificial intelligence with traditional fields to create new fields of high research and application value has become a trend. AI has been involved in many disciplines, such as medicine, materials, energy, and economics. The development of AI requires the support of many kinds of data, and microfluidic systems can often mine object data on a large scale to support AI. Due to the excellent synergy between the two technologies, excellent research results have emerged in many fields. In this review, we briefly review AI and microfluidics and introduce some applications of their combination, mainly in nanomedicine and material synthesis. Finally, we discuss the development trend of the combination of the two technologies.","",""
4,"M. Balachander, Shibashis Guha, J. Raskin","Fragility and Robustness in Mean-Payoff Adversarial Stackelberg Games",2020,"","","","",191,"2022-07-13 09:19:22","","10.4230/LIPIcs.CONCUR.2021.9","","",,,,,4,2.00,1,3,2,"Two-player mean-payoff Stackelberg games are nonzero-sum infinite duration games played on a bi-weighted graph by Leader (Player 0) and Follower (Player 1). Such games are played sequentially: first, Leader announces her strategy, second, Follower chooses his best-response. If we cannot impose which best-response is chosen by Follower, we say that Follower, though strategic, is adversarial towards Leader. The maximal value that Leader can get in this nonzero-sum game is called the adversarial Stackelberg value (ASV) of the game. We study the robustness of strategies for Leader in these games against two types of deviations: (i) Modeling imprecision - the weights on the edges of the game arena may not be exactly correct, they may be delta-away from the right one. (ii) Sub-optimal response - Follower may play epsilon-optimal best-responses instead of perfect best-responses. First, we show that if the game is zero-sum then robustness is guaranteed while in the nonzero-sum case, optimal strategies for ASV are fragile. Second, we provide a solution concept to obtain strategies for Leader that are robust to both modeling imprecision, and as well as to the epsilon-optimal responses of Follower, and study several properties and algorithmic problems related to this solution concept. Graphs: Applications to Reactive Synthesis and Beyond (Fédération Wallonie-Bruxelles), the EOS project Verifying Learning Artificial Intel-ligence Systems (F.R.S.-FNRS and FWO), and the COST Action 16228 GAMENET (European Cooperation in Science and Technology).","",""
0,"S. Sadeghi, M. Amiri, Farzaneh Mansoori Mooseloo","Artificial Intelligence and Its Application in Optimization under Uncertainty",2021,"","","","",192,"2022-07-13 09:19:22","","10.5772/intechopen.98628","","",,,,,0,0.00,0,3,1,"Nowadays, the increase in data acquisition and availability and complexity around optimization make it imperative to jointly use artificial intelligence (AI) and optimization for devising data-driven and intelligent decision support systems (DSS). A DSS can be successful if large amounts of interactive data proceed fast and robustly and extract useful information and knowledge to help decision-making. In this context, the data-driven approach has gained prominence due to its provision of insights for decision-making and easy implementation. The data-driven approach can discover various database patterns without relying on prior knowledge while also handling flexible objectives and multiple scenarios. This chapter reviews recent advances in data-driven optimization, highlighting the promise of data-driven optimization that integrates mathematical programming and machine learning (ML) for decision-making under uncertainty and identifies potential research opportunities. This chapter provides guidelines and implications for researchers, managers, and practitioners in operations research who want to advance their decision-making capabilities under uncertainty concerning data-driven optimization. Then, a comprehensive review and classification of the relevant publications on the data-driven stochastic program, data-driven robust optimization, and data-driven chance-constrained are presented. This chapter also identifies fertile avenues for future research that focus on deep-data-driven optimization, deep data-driven models, as well as online learning-based data-driven optimization. Perspectives on reinforcement learning (RL)-based data-driven optimization and deep RL for solving NP-hard problems are discussed. We investigate the application of data-driven optimization in different case studies to demonstrate improvements in operational performance over conventional optimization methodology. Finally, some managerial implications and some future directions are provided.","",""
103,"F. Schwendicke, W. Samek, J. Krois","Artificial Intelligence in Dentistry: Chances and Challenges",2020,"","","","",193,"2022-07-13 09:19:22","","10.1177/0022034520915714","","",,,,,103,51.50,34,3,2,"The term “artificial intelligence” (AI) refers to the idea of machines being capable of performing human tasks. A subdomain of AI is machine learning (ML), which “learns” intrinsic statistical patterns in data to eventually cast predictions on unseen data. Deep learning is a ML technique using multi-layer mathematical operations for learning and inferring on complex data like imagery. This succinct narrative review describes the application, limitations and possible future of AI-based dental diagnostics, treatment planning, and conduct, for example, image analysis, prediction making, record keeping, as well as dental research and discovery. AI-based applications will streamline care, relieving the dental workforce from laborious routine tasks, increasing health at lower costs for a broader population, and eventually facilitate personalized, predictive, preventive, and participatory dentistry. However, AI solutions have not by large entered routine dental practice, mainly due to 1) limited data availability, accessibility, structure, and comprehensiveness, 2) lacking methodological rigor and standards in their development, 3) and practical questions around the value and usefulness of these solutions, but also ethics and responsibility. Any AI application in dentistry should demonstrate tangible value by, for example, improving access to and quality of care, increasing efficiency and safety of services, empowering and enabling patients, supporting medical research, or increasing sustainability. Individual privacy, rights, and autonomy need to be put front and center; a shift from centralized to distributed/federated learning may address this while improving scalability and robustness. Lastly, trustworthiness into, and generalizability of, dental AI solutions need to be guaranteed; the implementation of continuous human oversight and standards grounded in evidence-based dentistry should be expected. Methods to visualize, interpret, and explain the logic behind AI solutions will contribute (“explainable AI”). Dental education will need to accompany the introduction of clinical AI solutions by fostering digital literacy in the future dental workforce.","",""
7,"I. Arel","The Threat of a Reward-Driven Adversarial Artificial General Intelligence",2012,"","","","",194,"2022-07-13 09:19:22","","10.1007/978-3-642-32560-1_3","","",,,,,7,0.70,7,1,10,"","",""
50,"Emilio Calvano, G. Calzolari, V. Denicoló, S. Pastorello","Artificial Intelligence, Algorithmic Pricing, and Collusion",2020,"","","","",195,"2022-07-13 09:19:22","","10.1257/AER.20190623","","",,,,,50,25.00,13,4,2,"Increasingly, pricing algorithms are supplanting human decision making in real marketplaces. To inform the competition policy debate on the possible consequences of this development, we experiment with pricing algorithms powered by Artificial Intelligence (AI) in controlled environments (computer simulations), studying the interaction among a number of Q-learning algorithms in a workhorse oligopoly model of price competition with Logit demand and constant marginal costs. In this setting the algorithms consistently learn to charge supra-competitive prices, without communicating with one another. The high prices are sustained by classical collusive strategies with a finite phase of punishment followed by a gradual return to cooperation. This finding is robust to asymmetries in cost or demand and to changes in the number of players.","",""
45,"Avishek Choudhury, Onur Asan","Role of Artificial Intelligence in Patient Safety Outcomes: Systematic Literature Review",2020,"","","","",196,"2022-07-13 09:19:22","","10.2196/18599","","",,,,,45,22.50,23,2,2,"Background Artificial intelligence (AI) provides opportunities to identify the health risks of patients and thus influence patient safety outcomes. Objective The purpose of this systematic literature review was to identify and analyze quantitative studies utilizing or integrating AI to address and report clinical-level patient safety outcomes. Methods We restricted our search to the PubMed, PubMed Central, and Web of Science databases to retrieve research articles published in English between January 2009 and August 2019. We focused on quantitative studies that reported positive, negative, or intermediate changes in patient safety outcomes using AI apps, specifically those based on machine-learning algorithms and natural language processing. Quantitative studies reporting only AI performance but not its influence on patient safety outcomes were excluded from further review. Results We identified 53 eligible studies, which were summarized concerning their patient safety subcategories, the most frequently used AI, and reported performance metrics. Recognized safety subcategories were clinical alarms (n=9; mainly based on decision tree models), clinical reports (n=21; based on support vector machine models), and drug safety (n=23; mainly based on decision tree models). Analysis of these 53 studies also identified two essential findings: (1) the lack of a standardized benchmark and (2) heterogeneity in AI reporting. Conclusions This systematic review indicates that AI-enabled decision support systems, when implemented correctly, can aid in enhancing patient safety by improving error detection, patient stratification, and drug management. Future work is still needed for robust validation of these systems in prospective and real-world clinical environments to understand how well AI can predict safety outcomes in health care settings.","",""
43,"M. González-Rivero, Oscar Beijbom, A. Rodriguez-Ramirez, D. Bryant, A. Ganase, Y. González-Marrero, A. Herrera-Reveles, E. Kennedy, Catherine J. S. Kim, S. Lopez-Marcano, Kathryn Markey, B. Neal, K. Osborne, C. Reyes-Nivia, E. Sampayo, Kristin Stolberg, Abbie Taylor, J. Vercelloni, Mathew Wyatt, O. Hoegh‐Guldberg","Monitoring of Coral Reefs Using Artificial Intelligence: A Feasible and Cost-Effective Approach",2020,"","","","",197,"2022-07-13 09:19:22","","10.3390/rs12030489","","",,,,,43,21.50,4,20,2,"Ecosystem monitoring is central to effective management, where rapid reporting is essential to provide timely advice. While digital imagery has greatly improved the speed of underwater data collection for monitoring benthic communities, image analysis remains a bottleneck in reporting observations. In recent years, a rapid evolution of artificial intelligence in image recognition has been evident in its broad applications in modern society, offering new opportunities for increasing the capabilities of coral reef monitoring. Here, we evaluated the performance of Deep Learning Convolutional Neural Networks for automated image analysis, using a global coral reef monitoring dataset. The study demonstrates the advantages of automated image analysis for coral reef monitoring in terms of error and repeatability of benthic abundance estimations, as well as cost and benefit. We found unbiased and high agreement between expert and automated observations (97%). Repeated surveys and comparisons against existing monitoring programs also show that automated estimation of benthic composition is equally robust in detecting change and ensuring the continuity of existing monitoring data. Using this automated approach, data analysis and reporting can be accelerated by at least 200x and at a fraction of the cost (1%). Combining commonly used underwater imagery in monitoring with automated image annotation can dramatically improve how we measure and monitor coral reefs worldwide, particularly in terms of allocating limited resources, rapid reporting and data integration within and across management areas.","",""
38,"I. Stafford, M. Kellermann, E. Mossotto, R. M. Beattie, B. MacArthur, S. Ennis","A systematic review of the applications of artificial intelligence and machine learning in autoimmune diseases.",2020,"","","","",198,"2022-07-13 09:19:22","","10.1038/s41746-020-0229-3","","",,,,,38,19.00,6,6,2,"","",""
41,"Alexander Campolo, K. Crawford","Enchanted Determinism: Power without Responsibility in Artificial Intelligence",2020,"","","","",199,"2022-07-13 09:19:22","","10.17351/ests2020.277","","",,,,,41,20.50,21,2,2,"Deep learning techniques are growing in popularity within the field of artificial intelligence (AI). These approaches identify patterns in large scale datasets, and make classifications and predictions, which have been celebrated as more accurate than those of humans. But for a number of reasons, including nonlinear path from inputs to outputs, there is a dearth of theory that can explain why deep learning techniques work so well at pattern detection and prediction. Claims about “superhuman” accuracy and insight, paired with the inability to fully explain how these results are produced, form a discourse about AI that we call enchanted determinism . To analyze enchanted determinism, we situate it within a broader epistemological diagnosis of modernity: Max Weber’s theory of disenchantment. Deep learning occupies an ambiguous position in this framework. On one hand, it represents a complex form of technological calculation and prediction, phenomena Weber associated with disenchantment. On the other hand, both deep learning experts and observers deploy enchanted, magical discourses to describe these systems’ uninterpretable mechanisms and counter-intuitive behavior. The combination of predictive accuracy and mysterious or unexplainable properties results in myth-making about deep learning’s transcendent, superhuman capacities, especially when it is applied in social settings. We analyze how discourses of magical deep learning produce techno-optimism, drawing on case studies from game-playing, adversarial examples, and attempts to infer sexual orientation from facial images. Enchantment shields the creators of these systems from accountability while its deterministic, calculative power intensifies social processes of classification and control.","",""
37,"Z. Yaseen, Z. H. Ali, Sinan Q. Salih, N. Al‐Ansari","Prediction of Risk Delay in Construction Projects Using a Hybrid Artificial Intelligence Model",2020,"","","","",200,"2022-07-13 09:19:22","","10.3390/su12041514","","",,,,,37,18.50,9,4,2,"Project delays are the major problems tackled by the construction sector owing to the associated complexity and uncertainty in the construction activities. Artificial Intelligence (AI) models have evidenced their capacity to solve dynamic, uncertain and complex tasks. The aim of this current study is to develop a hybrid artificial intelligence model called integrative Random Forest classifier with Genetic Algorithm optimization (RF-GA) for delay problem prediction. At first, related sources and factors of delay problems are identified. A questionnaire is adopted to quantify the impact of delay sources on project performance. The developed hybrid model is trained using the collected data of the previous construction projects. The proposed RF-GA is validated against the classical version of an RF model using statistical performance measure indices. The achieved results of the developed hybrid RF-GA model revealed a good resultant performance in terms of accuracy, kappa and classification error. Based on the measured accuracy, kappa and classification error, RF-GA attained 91.67%, 87% and 8.33%, respectively. Overall, the proposed methodology indicated a robust and reliable technique for project delay prediction that is contributing to the construction project management monitoring and sustainability.","",""
