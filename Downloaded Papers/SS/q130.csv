Cites,Authors,Title,Year,Source,Publisher,ArticleURL,CitesURL,GSRank,QueryDate,Type,DOI,ISSN,CitationURL,Volume,Issue,StartPage,EndPage,ECC,CitesPerYear,CitesPerAuthor,AuthorCount,Age,Abstract,FullTextURL,RelatedURL
77,"A. Qayyum, Junaid Qadir, M. Bilal, A. Al-Fuqaha","Secure and Robust Machine Learning for Healthcare: A Survey",2020,"","","","",1,"2022-07-13 10:07:22","","10.1109/RBME.2020.3013489","","",,,,,77,38.50,19,4,2,"Recent years have witnessed widespread adoption of machine learning (ML)/deep learning (DL) techniques due to their superior performance for a variety of healthcare applications ranging from the prediction of cardiac arrest from one-dimensional heart signals to computer-aided diagnosis (CADx) using multi-dimensional medical images. Notwithstanding the impressive performance of ML/DL, there are still lingering doubts regarding the robustness of ML/DL in healthcare settings (which is traditionally considered quite challenging due to the myriad security and privacy issues involved), especially in light of recent results that have shown that ML/DL are vulnerable to adversarial attacks. In this paper, we present an overview of various application areas in healthcare that leverage such techniques from security and privacy point of view and present associated challenges. In addition, we present potential methods to ensure secure and privacy-preserving ML for healthcare applications. Finally, we provide insight into the current research challenges and promising directions for future research.","",""
49,"Eric Wong, J. Z. Kolter","Learning perturbation sets for robust machine learning",2020,"","","","",2,"2022-07-13 10:07:22","","","","",,,,,49,24.50,25,2,2,"Although much progress has been made towards robust deep learning, a significant gap in robustness remains between real-world perturbations and more narrowly defined sets typically studied in adversarial defenses. In this paper, we aim to bridge this gap by learning perturbation sets from data, in order to characterize real-world effects for robust training and evaluation. Specifically, we use a conditional generator that defines the perturbation set over a constrained region of the latent space. We formulate desirable properties that measure the quality of a learned perturbation set, and theoretically prove that a conditional variational autoencoder naturally satisfies these criteria. Using this framework, our approach can generate a variety of perturbations at different complexities and scales, ranging from baseline spatial transformations, through common image corruptions, to lighting variations. We measure the quality of our learned perturbation sets both quantitatively and qualitatively, finding that our models are capable of producing a diverse set of meaningful perturbations beyond the limited data seen during training. Finally, we leverage our learned perturbation sets to train models which are empirically and certifiably robust to adversarial image corruptions and adversarial lighting variations, while improving generalization on non-adversarial data. All code and configuration files for reproducing the experiments as well as pretrained model weights can be found at this https URL.","",""
103,"Yunchao Liu, Srinivasan Arunachalam, K. Temme","A rigorous and robust quantum speed-up in supervised machine learning",2020,"","","","",3,"2022-07-13 10:07:22","","10.1038/s41567-021-01287-z","","",,,,,103,51.50,34,3,2,"","",""
2,"Ye Wang, S. Aeron, A. S. Rakin, T. Koike-Akino, P. Moulin","Robust Machine Learning via Privacy/ Rate-Distortion Theory",2020,"","","","",4,"2022-07-13 10:07:22","","10.1109/ISIT45174.2021.9517751","","",,,,,2,1.00,0,5,2,"Robust machine learning formulations have emerged to address the prevalent vulnerability of deep neural networks to adversarial examples. Our work draws the connection between optimal robust learning and the privacy-utility tradeoff problem, which is a generalization of the rate-distortion problem. The saddle point of the game between a robust classifier and an adversarial perturbation can be found via the solution of a maximum conditional entropy problem. This information-theoretic perspective sheds light on the fundamental tradeoff between robustness and clean data performance, which ultimately arises from the geometric structure of the underlying data distribution and perturbation constraints.","",""
53,"M. Shafique, Mahum Naseer, T. Theocharides, C. Kyrkou, O. Mutlu, Lois Orosa, Jungwook Choi","Robust Machine Learning Systems: Challenges,Current Trends, Perspectives, and the Road Ahead",2020,"","","","",5,"2022-07-13 10:07:22","","10.1109/MDAT.2020.2971217","","",,,,,53,26.50,8,7,2,"Currently, machine learning (ML) techniques are at the heart of smart cyber-physical systems (CPS) and Internet-of-Things (IoT). This article discusses various challenges and probable solutions for security attacks on these ML-inspired hardware and software techniques. —Partha Pratim Pande, Washington State University","",""
0,"Ransalu Senanayake, Daniel J. Fremont, Mykel J. Kochenderfer, A. Lomuscio, D. Margineantu, Cheng Soon Ong","Guest Editorial: Special issue on robust machine learning",2021,"","","","",6,"2022-07-13 10:07:22","","10.1007/s10994-021-06113-4","","",,,,,0,0.00,0,6,1,"","",""
21,"Lie He, Sai Praneeth Karimireddy, Martin Jaggi","Secure Byzantine-Robust Machine Learning",2020,"","","","",7,"2022-07-13 10:07:22","","","","",,,,,21,10.50,7,3,2,"Increasingly machine learning systems are being deployed to edge servers and devices (e.g. mobile phones) and trained in a collaborative manner. Such distributed/federated/decentralized training raises a number of concerns about the robustness, privacy, and security of the procedure. While extensive work has been done in tackling with robustness, privacy, or security individually, their combination has rarely been studied. In this paper, we propose a secure two-server protocol that offers both input privacy and Byzantine-robustness. In addition, this protocol is communication-efficient, fault-tolerant and enjoys local differential privacy.","",""
31,"Chongchong Qi, Qiu-song Chen, S. Sonny Kim","Integrated and intelligent design framework for cemented paste backfill: A combination of robust machine learning modelling and multi-objective optimization",2020,"","","","",8,"2022-07-13 10:07:22","","10.1016/j.mineng.2020.106422","","",,,,,31,15.50,10,3,2,"","",""
7,"Md. Kowsher, A. Tahabilder, S. Murad","Impact-Learning: A Robust Machine Learning Algorithm",2020,"","","","",9,"2022-07-13 10:07:22","","10.1145/3411174.3411185","","",,,,,7,3.50,2,3,2,"The ultimate goal of this research paper is to introduce a robust machine learning algorithm called Impact-Learning, which is being used widely to achieve more advanced results on many machine-learning related challenges. Impact learning is a supervised machine learning algorithm for resolving classification and linear or polynomial regression knowledge from examples. It also contributes to analyzing systems for competitive data. This algorithm is unique for being capable of learning from a competition, which is the impact of independent features. In other words, it is trained by the impacts of the features from the intrinsic rate of natural increase (RNI). The input to the Impact Learning is a training set of numerical data. In this work, we used six datasets related to regressions and classifications as the experiment of the Impact Learning, and the comparison indicates that at outperforms other standard machine learning regressions and classifications algorithms such as Random forest tree, SVM, Naive Bayes, Logistic regression and so forth.","",""
6,"F. Farokhi","Regularization Helps with Mitigating Poisoning Attacks: Distributionally-Robust Machine Learning Using the Wasserstein Distance",2020,"","","","",10,"2022-07-13 10:07:22","","","","",,,,,6,3.00,6,1,2,"We use distributionally-robust optimization for machine learning to mitigate the effect of data poisoning attacks. We provide performance guarantees for the trained model on the original data (not including the poison records) by training the model for the worst-case distribution on a neighbourhood around the empirical distribution (extracted from the training dataset corrupted by a poisoning attack) defined using the Wasserstein distance. We relax the distributionally-robust machine learning problem by finding an upper bound for the worst-case fitness based on the empirical sampled-averaged fitness and the Lipschitz-constant of the fitness function (on the data for given model parameters) as regularizer. For regression models, we prove that this regularizer is equal to the dual norm of the model parameters. We use the Wine Quality dataset, the Boston Housing Market dataset, and the Adult dataset for demonstrating the results of this paper.","",""
6,"N. Simos, S. Dimitriadis, E. Kavroulakis, G. Manikis, G. Bertsias, P. Simos, T. Maris, E. Papadaki","Quantitative Identification of Functional Connectivity Disturbances in Neuropsychiatric Lupus Based on Resting-State fMRI: A Robust Machine Learning Approach",2020,"","","","",11,"2022-07-13 10:07:22","","10.3390/brainsci10110777","","",,,,,6,3.00,1,8,2,"Neuropsychiatric systemic lupus erythematosus (NPSLE) is an autoimmune entity comprised of heterogenous syndromes affecting both the peripheral and central nervous system. Research on the pathophysiological substrate of NPSLE manifestations, including functional neuroimaging studies, is extremely limited. The present study examined person-specific patterns of whole-brain functional connectivity in NPSLE patients (n = 44) and age-matched healthy control participants (n = 39). Static functional connectivity graphs were calculated comprised of connection strengths between 90 brain regions. These connections were subsequently filtered through rigorous surrogate analysis, a technique borrowed from physics, novel to neuroimaging. Next, global as well as nodal network metrics were estimated for each individual functional brain network and were input to a robust machine learning algorithm consisting of a random forest feature selection and nested cross-validation strategy. The proposed pipeline is data-driven in its entirety, and several tests were performed in order to ensure model robustness. The best-fitting model utilizing nodal graph metrics for 11 brain regions was associated with 73.5% accuracy (74.5% sensitivity and 73% specificity) in discriminating NPSLE from healthy individuals with adequate statistical power. Closer inspection of graph metric values suggested an increased role within the functional brain network in NSPLE (indicated by higher nodal degree, local efficiency, betweenness centrality, or eigenvalue efficiency) as compared to healthy controls for seven brain regions and a reduced role for four areas. These findings corroborate earlier work regarding hemodynamic disturbances in these brain regions in NPSLE. The validity of the results is further supported by significant associations of certain selected graph metrics with accumulated organ damage incurred by lupus, with visuomotor performance and mental flexibility scores obtained independently from NPSLE patients.","",""
43,"A. Abrol, Z. Fu, Mustafa S. Salman, Rogers F. Silva, Y. Du, S. Plis, V. Calhoun","Deep learning encodes robust discriminative neuroimaging representations to outperform standard machine learning",2021,"","","","",12,"2022-07-13 10:07:22","","10.1038/s41467-020-20655-6","","",,,,,43,43.00,6,7,1,"","",""
15,"Ezekiel O. Soremekun, Sakshi Udeshi, Sudipta Chattopadhyay, A. Zeller","Exposing Backdoors in Robust Machine Learning Models",2020,"","","","",13,"2022-07-13 10:07:22","","","","",,,,,15,7.50,4,4,2,"The introduction of robust optimisation has pushed the state-of-the-art in defending against adversarial attacks. However, the behaviour of such optimisation has not been studied in the light of a fundamentally different class of attacks called backdoors. In this paper, we demonstrate that adversarially robust models are susceptible to backdoor attacks. Subsequently, we observe that backdoors are reflected in the feature representation of such models. Then, this observation is leveraged to detect backdoor-infected models via a detection technique called AEGIS. Specifically, AEGIS uses feature clustering to effectively detect backdoor-infected robust Deep Neural Networks (DNNs).  In our evaluation of major classification tasks using CIFAR-10, MNIST and FMNIST datasets, AEGIS effectively detects robust DNNs infected with backdoors. Overall, AEGIS has 97% (70/72) detection accuracy and 0.3% (2/648) false positive rate, for all configurations. Our investigation reveals that salient features of adversarially robust DNNs break the stealthy nature of backdoor attacks.","",""
11,"B. Nartowt, Gregory R Hart, W. Muhammad, Ying Liang, Gigi F Stark, Jun Deng","Robust Machine Learning for Colorectal Cancer Risk Prediction and Stratification",2020,"","","","",14,"2022-07-13 10:07:22","","10.3389/fdata.2020.00006","","",,,,,11,5.50,2,6,2,"While colorectal cancer (CRC) is third in prevalence and mortality among cancers in the United States, there is no effective method to screen the general public for CRC risk. In this study, to identify an effective mass screening method for CRC risk, we evaluated seven supervised machine learning algorithms: linear discriminant analysis, support vector machine, naive Bayes, decision tree, random forest, logistic regression, and artificial neural network. Models were trained and cross-tested with the National Health Interview Survey (NHIS) and the Prostate, Lung, Colorectal, Ovarian Cancer Screening (PLCO) datasets. Six imputation methods were used to handle missing data: mean, Gaussian, Lorentzian, one-hot encoding, Gaussian expectation-maximization, and listwise deletion. Among all of the model configurations and imputation method combinations, the artificial neural network with expectation-maximization imputation emerged as the best, having a concordance of 0.70 ± 0.02, sensitivity of 0.63 ± 0.06, and specificity of 0.82 ± 0.04. In stratifying CRC risk in the NHIS and PLCO datasets, only 2% of negative cases were misclassified as high risk and 6% of positive cases were misclassified as low risk. In modeling the CRC-free probability with Kaplan-Meier estimators, low-, medium-, and high CRC-risk groups have statistically-significant separation. Our results indicated that the trained artificial neural network can be used as an effective screening tool for early intervention and prevention of CRC in large populations.","",""
23,"Jianqing Fan, Kaizheng Wang, Yiqiao Zhong, Ziwei Zhu","Robust high dimensional factor models with applications to statistical machine learning.",2018,"","","","",15,"2022-07-13 10:07:22","","10.1214/20-STS785","","",,,,,23,5.75,6,4,4,"Factor models are a class of powerful statistical models that have been widely used to deal with dependent measurements that arise frequently from various applications from genomics and neuroscience to economics and finance. As data are collected at an ever-growing scale, statistical machine learning faces some new challenges: high dimensionality, strong dependence among observed variables, heavy-tailed variables and heterogeneity. High-dimensional robust factor analysis serves as a powerful toolkit to conquer these challenges. This paper gives a selective overview on recent advance on high-dimensional factor models and their applications to statistics including Factor-Adjusted Robust Model selection (FarmSelect) and Factor-Adjusted Robust Multiple testing (FarmTest). We show that classical methods, especially principal component analysis (PCA), can be tailored to many new problems and provide powerful tools for statistical estimation and inference. We highlight PCA and its connections to matrix perturbation theory, robust statistics, random projection, false discovery rate, etc., and illustrate through several applications how insights from these fields yield solutions to modern challenges. We also present far-reaching connections between factor models and popular statistical learning problems, including network analysis and low-rank matrix recovery.","",""
52,"Hana Dureckova, M. Krykunov, M. Z. Aghaji, T. Woo","Robust Machine Learning Models for Predicting High CO2 Working Capacity and CO2/H2 Selectivity of Gas Adsorption in Metal Organic Frameworks for Precombustion Carbon Capture",2019,"","","","",16,"2022-07-13 10:07:22","","10.1021/ACS.JPCC.8B10644","","",,,,,52,17.33,13,4,3,"This work is devoted to the development of quantitative structure–property relationship (QSPR) models using machine learning to predict CO2 working capacity and CO2/H2 selectivity for precombustion carbon capture using a topologically diverse database of hypothetical metal–organic framework (MOF) structures (358 400 MOFs, 1166 network topologies). Such a diversity of the networks topology is much higher than previously used (<20 network topologies) for rapid and accurate recognition of high-performing MOFs for other gas-separation applications. The gradient boosted trees regression method allowed us to use 80% of the database as a training set, while the rest was used for the validation and test set. The QSPR models are first built using purely geometric descriptors of MOFs such as gravimetric surface area and void fraction. Additional models which account for chemical features of MOFs are constructed using atomic property weighted radial distribution functions (AP-RDFs) with a novel normalization to acco...","",""
27,"Anastasios Petropoulos, Vasilis Siakoulis, Evaggelos Stavroulakis, A. Klamargias","A robust machine learning approach for credit risk analysis of large loan level datasets using deep learning and extreme gradient boosting",2019,"","","","",17,"2022-07-13 10:07:22","","","","",,,,,27,9.00,7,4,3,"In the aftermath of global financial crisis of 2007–2008, central banks have put forward data statistics initiatives in order to boost their supervisory and monetary policy functions which will lead to central banks possessing big databases increasing the need for robust data mining processes and financial statistical modelling to support more informed decision making. Conventional econometric methods fail to capture efficiently the information contained in the full spectrum of the datasets. To address these challenges, in this work we investigate the analysis of a corporate credit loans big dataset using cutting edge machine learning techniques and deep learning neural networks. The novelty of our approach lies in the combination of a data mining algorithms that aim to reduce dimensionality in the data and increase accuracy in predicting the future behaviour of corporate loans, to facilitate a more effective micro and macro supervision of credit risk in the Greek banking system. Our analysis is based on a large dataset of loan level data, spanning a 10 year period of the Greek economy with the purpose of performing obligor credit quality classification and quantification of Probability of Default under a through the cycle setup. We perform extensive comparisons of the classification and forecasting accuracy of the proposed methods, using a 3-years’ period out-of-time sample. Our experimental results are benchmarked against other traditional methods, like logistic regression and discriminant analysis methods, yielding significantly superior performance. In the final stage of our analysis, a robust through the cycle financial credit rating is developed which can offer a proactive monitoring mechanism of the credit risk dynamics in a financial system. Finally the methodological framework introduced can support a more in depth analysis of database initiatives like ECB AnaCredit.","",""
50,"Soohyun Nam Liao, Daniel Zingaro, Kevin Thai, Christine Alvarado, W. Griswold, Leo Porter","A Robust Machine Learning Technique to Predict Low-performing Students",2019,"","","","",18,"2022-07-13 10:07:22","","10.1145/3277569","","",,,,,50,16.67,8,6,3,"As enrollments and class sizes in postsecondary institutions have increased, instructors have sought automated and lightweight means to identify students who are at risk of performing poorly in a course. This identification must be performed early enough in the term to allow instructors to assist those students before they fall irreparably behind. This study describes a modeling methodology that predicts student final exam scores in the third week of the term by using the clicker data that is automatically collected for instructors when they employ the Peer Instruction pedagogy. The modeling technique uses a support vector machine binary classifier, trained on one term of a course, to predict outcomes in the subsequent term. We applied this modeling technique to five different courses across the computer science curriculum, taught by three different instructors at two different institutions. Our modeling approach includes a set of strengths not seen wholesale in prior work, while maintaining competitive levels of accuracy with that work. These strengths include using a lightweight source of student data, affording early detection of struggling students, and predicting outcomes across terms in a natural setting (different final exams, minor changes to course content), across multiple courses in a curriculum, and across multiple institutions.","",""
23,"J. Zhang, Kang Liu, Faiq Khalid, Muhammad Abdullah Hanif, Semeen Rehman, T. Theocharides, Alessandro Artussi, M. Shafique, S. Garg","Building Robust Machine Learning Systems: Current Progress, Research Challenges, and Opportunities",2019,"","","","",19,"2022-07-13 10:07:22","","10.1145/3316781.3323472","","",,,,,23,7.67,3,9,3,"Machine learning, in particular deep learning, is being used in almost all the aspects of life to facilitate humans, specifically in mobile and Internet of Things (IoT)-based applications. Due to its state-of-the-art performance, deep learning is also being employed in safety-critical applications, for instance, autonomous vehicles. Reliability and security are two of the key required characteristics for these applications because of the impact they can have on human's life. Towards this, in this paper, we highlight the current progress, challenges and research opportunities in the domain of robust systems for machine learning-based applications.","",""
49,"Ran Xin, S. Kar, U. Khan","Decentralized Stochastic Optimization and Machine Learning: A Unified Variance-Reduction Framework for Robust Performance and Fast Convergence",2020,"","","","",20,"2022-07-13 10:07:22","","10.1109/MSP.2020.2974267","","",,,,,49,24.50,16,3,2,"Decentralized methods to solve finite-sum minimization problems are important in many signal processing and machine learning tasks where the data samples are distributed across a network of nodes, and raw data sharing is not permitted due to privacy and/or resource constraints. In this article, we review decentralized stochastic first-order methods and provide a unified algorithmic framework that combines variance reduction with gradient tracking to achieve robust performance and fast convergence. We provide explicit theoretical guarantees of the corresponding methods when the objective functions are smooth and strongly convex and show their applicability to nonconvex problems via numerical experiments. Throughout the article, we provide intuitive illustrations of the main technical ideas by casting appropriate tradeoffs and comparisons among the methods of interest and by highlighting applications to decentralized training of machine learning models.","",""
103,"Muhammad Attique Khan, I. Ashraf, M. Alhaisoni, Robertas Damaševičius, R. Scherer, A. Rehman, S. Bukhari","Multimodal Brain Tumor Classification Using Deep Learning and Robust Feature Selection: A Machine Learning Application for Radiologists",2020,"","","","",21,"2022-07-13 10:07:22","","10.3390/diagnostics10080565","","",,,,,103,51.50,15,7,2,"Manual identification of brain tumors is an error-prone and tedious process for radiologists; therefore, it is crucial to adopt an automated system. The binary classification process, such as malignant or benign is relatively trivial; whereas, the multimodal brain tumors classification (T1, T2, T1CE, and Flair) is a challenging task for radiologists. Here, we present an automated multimodal classification method using deep learning for brain tumor type classification. The proposed method consists of five core steps. In the first step, the linear contrast stretching is employed using edge-based histogram equalization and discrete cosine transform (DCT). In the second step, deep learning feature extraction is performed. By utilizing transfer learning, two pre-trained convolutional neural network (CNN) models, namely VGG16 and VGG19, were used for feature extraction. In the third step, a correntropy-based joint learning approach was implemented along with the extreme learning machine (ELM) for the selection of best features. In the fourth step, the partial least square (PLS)-based robust covariant features were fused in one matrix. The combined matrix was fed to ELM for final classification. The proposed method was validated on the BraTS datasets and an accuracy of 97.8%, 96.9%, 92.5% for BraTs2015, BraTs2017, and BraTs2018, respectively, was achieved.","",""
29,"Nishat Koti, Mahak Pancholi, A. Patra, A. Suresh","SWIFT: Super-fast and Robust Privacy-Preserving Machine Learning",2020,"","","","",22,"2022-07-13 10:07:22","","","","",,,,,29,14.50,7,4,2,"Performing ML computation on private data while maintaining data privacy aka Privacy-preserving Machine Learning (PPML) is an emergent field of research. Recently, PPML has seen a visible shift towards the adoption of Secure Outsourced Computation (SOC) paradigm, due to the heavy computation that it entails. In the SOC paradigm, computation is outsourced to a set of powerful and specially equipped servers that provide service on a pay-per-use basis. In this work, we propose SWIFT, a robust PPML framework for a range of ML algorithms in SOC setting, that guarantees output delivery to the users irrespective of any adversarial behaviour. Robustness, a highly desirable feature, evokes user participation without the fear of denial of service.  At the heart of our framework lies a highly-efficient, maliciously-secure, three-party computation (3PC) over rings that provides guaranteed output delivery (GOD) in the honest-majority setting. To the best of our knowledge, SWIFT is the first robust and efficient PPML framework in the 3PC setting. SWIFT is as fast as the best-known 3PC framework BLAZE (Patra et al. NDSS'20) which only achieves fairness. Fairness ensures either all or none receive the output, whereas GOD ensures guaranteed output delivery no matter what. We extend our 3PC framework for four parties (4PC). In this regime, SWIFT is as fast as the best known fair 4PC framework Trident (Chaudhari et al. NDSS'20) and twice faster than the best-known robust 4PC framework FLASH (Byali et al. PETS'20).  We demonstrate the practical relevance of our framework by benchmarking two important applications-- i) ML algorithms: Logistic Regression and Neural Network, and ii) Biometric matching, both over a 64-bit ring in WAN setting. Our readings reflect our claims as above.","",""
57,"Lal Hussain","Detecting epileptic seizure with different feature extracting strategies using robust machine learning classification techniques by applying advance parameter optimization approach",2018,"","","","",23,"2022-07-13 10:07:22","","10.1007/s11571-018-9477-1","","",,,,,57,14.25,57,1,4,"","",""
36,"J. Li","Principled approaches to robust machine learning and beyond",2018,"","","","",24,"2022-07-13 10:07:22","","","","",,,,,36,9.00,36,1,4,"As we apply machine learning to more and more important tasks, it becomes increasingly important that these algorithms are robust to systematic, or worse, malicious, noise. Despite considerable interest, no efficient algorithms were known to be robust to such noise in high dimensional settings for some of the most fundamental statistical tasks for over sixty years of research. In this thesis we devise two novel, but similarly inspired, algorithmic paradigms for estimation in high dimensions in the presence of a small number of adversarially added data points. Both algorithms are the first efficient algorithms which achieve (nearly) optimal error bounds for a number fundamental statistical tasks such as mean estimation and covariance estimation. The goal of this thesis is to present these two frameworks in a clean and unified manner. We show that these insights also have applications for other problems in learning theory. Specifically, we show that these algorithms can be combined with the powerful Sum-of-Squares hierarchy to yield improvements for clustering high dimensional Gaussian mixture models, the first such improvement in over fifteen years of research. Going full circle, we show that Sum-of-Squares also can be used to improve error rates for robust mean estimation. Not only are these algorithms of interest theoretically, but we demonstrate empirically that we can use these insights in practice to uncover patterns in high dimensional data that were previously masked by noise. Based on our algorithms, we give new implementations for robust PCA, new defenses for data poisoning attacks for stochastic optimization, and new defenses for watermarking attacks on deep nets. In all of these tasks, we demonstrate on both synthetic and real data sets that our performance is substantially better than the state-of-the-art, often able to detect most to all corruptions when previous methods could not reliably detect any. Thesis Supervisor: Ankur Moitra Title: Rockwell International CD Associate Professor of Mathematics","",""
55,"Samantha Joel, Paul W. Eastwick, C. J. Allison, X. Arriaga, Zachary G. Baker, E. Bar-Kalifa, S. Bergeron, G. Birnbaum, R. Brock, C. Brumbaugh, Cheryl L. Carmichael, Serena Chen, Jennifer A. Clarke, Rebecca J. Cobb, Michael K. Coolsen, Jody L. Davis, David C de Jong, Anik Debrot, E. DeHaas, Jaye L. Derrick, Jami Eller, Marie-Joelle Estrada, Ruddy Faure, E. Finkel, R. C. Fraley, S. Gable, Reuma Gadassi-Polack, Yuthika U. Girme, Amie M. Gordon, Courtney Gosnell, Matthew D. Hammond, P. Hannon, Cheryl Harasymchuk, W. Hofmann, A. Horn, E. Impett, Jeremy P Jamieson, D. Keltner, James J Kim, Jeffrey L. Kirchner, E. Kluwer, M. Kumashiro, Grace M Larson, Gal Lazarus, Jill M. Logan, Laura B. Luchies, G. Macdonald, Laura V. Machia, Michael R Maniaci, J. Maxwell","Machine learning uncovers the most robust self-report predictors of relationship quality across 43 longitudinal couples studies",2020,"","","","",25,"2022-07-13 10:07:22","","10.1073/pnas.1917036117","","",,,,,55,27.50,6,50,2,"Significance What predicts how happy people are with their romantic relationships? Relationship science—an interdisciplinary field spanning psychology, sociology, economics, family studies, and communication—has identified hundreds of variables that purportedly shape romantic relationship quality. The current project used machine learning to directly quantify and compare the predictive power of many such variables among 11,196 romantic couples. People’s own judgments about the relationship itself—such as how satisfied and committed they perceived their partners to be, and how appreciative they felt toward their partners—explained approximately 45% of their current satisfaction. The partner’s judgments did not add information, nor did either person’s personalities or traits. Furthermore, none of these variables could predict whose relationship quality would increase versus decrease over time. Given the powerful implications of relationship quality for health and well-being, a central mission of relationship science is explaining why some romantic relationships thrive more than others. This large-scale project used machine learning (i.e., Random Forests) to 1) quantify the extent to which relationship quality is predictable and 2) identify which constructs reliably predict relationship quality. Across 43 dyadic longitudinal datasets from 29 laboratories, the top relationship-specific predictors of relationship quality were perceived-partner commitment, appreciation, sexual satisfaction, perceived-partner satisfaction, and conflict. The top individual-difference predictors were life satisfaction, negative affect, depression, attachment avoidance, and attachment anxiety. Overall, relationship-specific variables predicted up to 45% of variance at baseline, and up to 18% of variance at the end of each study. Individual differences also performed well (21% and 12%, respectively). Actor-reported variables (i.e., own relationship-specific and individual-difference variables) predicted two to four times more variance than partner-reported variables (i.e., the partner’s ratings on those variables). Importantly, individual differences and partner reports had no predictive effects beyond actor-reported relationship-specific variables alone. These findings imply that the sum of all individual differences and partner experiences exert their influence on relationship quality via a person’s own relationship-specific experiences, and effects due to moderation by individual differences and moderation by partner-reports may be quite small. Finally, relationship-quality change (i.e., increases or decreases in relationship quality over the course of a study) was largely unpredictable from any combination of self-report variables. This collective effort should guide future models of relationships.","",""
101,"G. Lecu'e, M. Lerasle","Robust machine learning by median-of-means: Theory and practice",2017,"","","","",26,"2022-07-13 10:07:22","","10.1214/19-AOS1828","","",,,,,101,20.20,51,2,5,"We introduce new estimators for robust machine learning based on median-of-means (MOM) estimators of the mean of real valued random variables. These estimators achieve optimal rates of convergence under minimal assumptions on the dataset. The dataset may also have been corrupted by outliers on which no assumption is granted. We also analyze these new estimators with standard tools from robust statistics. In particular, we revisit the concept of breakdown point. We modify the original definition by studying the number of outliers that a dataset can contain without deteriorating the estimation properties of a given estimator. This new notion of breakdown number, that takes into account the statistical performances of the estimators, is non-asymptotic in nature and adapted for machine learning purposes. We proved that the breakdown number of our estimator is of the order of (number of observations)*(rate of convergence). For instance, the breakdown number of our estimators for the problem of estimation of a d-dimensional vector with a noise variance sigma^2 is sigma^2d and it becomes sigma^2 s log(d/s) when this vector has only s non-zero component. Beyond this breakdown point, we proved that the rate of convergence achieved by our estimator is (number of outliers) divided by (number of observation).  Besides these theoretical guarantees, the major improvement brought by these new estimators is that they are easily computable in practice. In fact, basically any algorithm used to approximate the standard Empirical Risk Minimizer (or its regularized versions) has a robust version approximating our estimators. As a proof of concept, we study many algorithms for the classical LASSO estimator. A byproduct of the MOM algorithms is a measure of depth of data that can be used to detect outliers.","",""
34,"Muhammad Abdullah Hanif, Faiq Khalid, Rachmad Vidya Wicaksana Putra, Semeen Rehman, M. Shafique","Robust Machine Learning Systems: Reliability and Security for Deep Neural Networks",2018,"","","","",27,"2022-07-13 10:07:22","","10.1109/IOLTS.2018.8474192","","",,,,,34,8.50,7,5,4,"Machine learning is commonly being used in almost all the areas that involve advanced data analytics and intelligent control. From applications like Natural Language Processing (NLP) to autonomous driving are based upon machine learning algorithms. An increasing trend is observed in the use of Deep Neural Networks (DNNs) for such applications. While the slight inaccuracy in applications like NLP does not have any severe consequences, it is not the same for other safety-critical applications, like autonomous driving and smart healthcare, where a small error can lead to catastrophic effects. Apart from high-accuracy DNN algorithms, there is a significant need for robust machine learning systems and hardware architectures that can generate reliable and trustworthy results in the presence of hardware-level faults while also preserving security and privacy. This paper provides an overview of the challenges being faced in ensuring reliable and secure execution of DNNs. To address the challenges, we present several techniques for analyzing and mitigating the reliability and security threats in machine learning systems.","",""
1,"T. Schmid","Batch-like Online Learning for More Robust Hybrid Artificial Intelligence: Deconstruction as a Machine Learning Process",2021,"","","","",28,"2022-07-13 10:07:22","","","","",,,,,1,1.00,1,1,1,"Continuous streams of data are a common, yet challenging phenomenon of modern information processing. Traditional approaches to adopt machine learning techniques to this setting, like offline and online learning, have demonstrated several critical drawbacks. In order to avoid known disadvantages of both approaches, we propose to combine their complementary advantages in a novel machine learning process called deconstruction. Similar to supervised and unsupervised learning, this novel process provides a fundamental learning functionality modeled after human learning. This functionality integrates mechanisms for partitioning training data, managing learned knowledge representations and integrating newly acquired knowledge with previously learned knowledge representations. A prerequisite for this concept is that learning data can be partitioned and that resulting knowledge partitions may be accessed by formal means. In the proposed approach, this is achieved by the recently introduced Constructivist Machine Learning framework, which allows to create, exploit and maintain a knowledge base. In this work, we highlight the design concepts for the implementation of such a deconstruction process. In particular, we describe required subprocesses and how they can be combined.","",""
36,"Daniel S. Berger","Towards Lightweight and Robust Machine Learning for CDN Caching",2018,"","","","",29,"2022-07-13 10:07:22","","10.1145/3286062.3286082","","",,,,,36,9.00,36,1,4,"Recent advances in the field of reinforcement learning promise a general approach to optimize networking systems. This paper argues against the recent trend for generalization by introducing a case study where domain-specific modeling enables the application of lightweight and robust learning techniques. We study CDN caching systems, which make a good case for optimization as their performance directly affects operational costs, while currently relying on many hand-tuned parameters. In caching, reinforcement learning has been shown to perform suboptimally when compared to simple heuristics. A key challenge is that rewards (cache hits) manifest with large delays, which prevents timely feedback to the learning algorithm and introduces significant complexity. This paper shows how to significantly simplify this problem by explicitly modeling optimal caching decisions (OPT). While prior work considered deriving OPT impractical, recent theoretical modeling advances change this assumption. Modeling OPT enables even lightweight decision trees to outperform state-of-the-art CDN caching heuristics.","",""
11,"Sherri Rose","Robust Machine Learning Variable Importance Analyses of Medical Conditions for Health Care Spending",2018,"","","","",30,"2022-07-13 10:07:22","","10.1111/1475-6773.12848","","",,,,,11,2.75,11,1,4,"OBJECTIVE To propose nonparametric double robust machine learning in variable importance analyses of medical conditions for health spending.   DATA SOURCES 2011-2012 Truven MarketScan database.   STUDY DESIGN I evaluate how much more, on average, commercially insured enrollees with each of 26 of the most prevalent medical conditions cost per year after controlling for demographics and other medical conditions. This is accomplished within the nonparametric targeted learning framework, which incorporates ensemble machine learning. Previous literature studying the impact of medical conditions on health care spending has almost exclusively focused on parametric risk adjustment; thus, I compare my approach to parametric regression.   PRINCIPAL FINDINGS My results demonstrate that multiple sclerosis, congestive heart failure, severe cancers, major depression and bipolar disorders, and chronic hepatitis are the most costly medical conditions on average per individual. These findings differed from those obtained using parametric regression.   CONCLUSIONS The literature may be underestimating the spending contributions of several medical conditions, which is a potentially critical oversight. If current methods are not capturing the true incremental effect of medical conditions, undesirable incentives related to care may remain. Further work is needed to directly study these issues in the context of federal formulas.","",""
189,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Auto-sklearn: Efficient and Robust Automated Machine Learning",2019,"","","","",31,"2022-07-13 10:07:22","","10.1007/978-3-030-05318-5_6","","",,,,,189,63.00,32,6,3,"","",""
19,"Mahbub E. Khoda, Tasadduq Imam, J. Kamruzzaman, I. Gondal, Ashfaqur Rahman","Robust Malware Defense in Industrial IoT Applications Using Machine Learning With Selective Adversarial Samples",2020,"","","","",32,"2022-07-13 10:07:22","","10.1109/TIA.2019.2958530","","",,,,,19,9.50,4,5,2,"Industrial Internet of Things (IIoT) deploys edge devices to act as intermediaries between sensors and actuators and application servers or cloud services. Machine learning models have been widely used to thwart malware attacks in such edge devices. However, these models are vulnerable to adversarial attacks where attackers craft adversarial samples by introducing small perturbations to malware samples to fool a classifier to misclassify them as benign applications. Literature on deep learning networks proposes adversarial retraining as a defense mechanism where adversarial samples are combined with legitimate samples to retrain the classifier. However, existing works select such adversarial samples in a random fashion which degrades the classifier's performance. This work proposes two novel approaches for selecting adversarial samples to retrain a classifier. One, based on the distance from malware cluster center, and the other, based on a probability measure derived from a kernel-based learning (KBL). Our experiments show that both of our sample selection methods outperform the random selection method and the KBL selection method improves detection accuracy by 6%. Also, while existing works focus on deep neural networks with respect to adversarial retraining, we additionally assess the impact of such adversarial samples on other classifiers and our proposed selective adversarial retraining approaches show similar performance improvement for these classifiers as well. The outcomes from the study can assist in designing robust security systems for IIoT applications.","",""
11,"Daniyal Amir Awan, R. L. Cavalcante, S. Stańczak","A Robust Machine Learning Method for Cell-Load Approximation in Wireless Networks",2017,"","","","",33,"2022-07-13 10:07:22","","10.1109/ICASSP.2018.8462320","","",,,,,11,2.20,4,3,5,"We propose a learning algorithm for cell-load approximation in wireless networks. The proposed algorithm is robust in the sense that it is designed to cope with the uncertainty arising from a small number of training samples. This scenario is highly relevant in wireless networks where training has to be performed on short time scales because of a fast time-varying communication environment. The first part of this work studies the set of feasible rates and shows that this set is compact. We then prove that the mapping relating a feasible rate vector to the unique fixed point of the non-linear cell-load mapping is monotone and uniformly continuous. Utilizing these properties, we apply an approximation framework that achieves the best worst-case performance. Furthermore, the approximation preserves the monotonicity and continuity properties. Simulations show that the proposed method exhibits better robustness and accuracy for small training sets in comparison with standard approximation techniques for multivariate data.","",""
148,"Amedeo Sapio, M. Canini, Chen-Yu Ho, J. Nelson, Panos Kalnis, Changhoon Kim, A. Krishnamurthy, M. Moshref, Dan R. K. Ports, Peter Richtárik","Scaling Distributed Machine Learning with In-Network Aggregation",2019,"","","","",34,"2022-07-13 10:07:22","","","","",,,,,148,49.33,15,10,3,"Training complex machine learning models in parallel is an increasingly important workload. We accelerate distributed parallel training by designing a communication primitive that uses a programmable switch dataplane to execute a key step of the training process. Our approach, SwitchML, reduces the volume of exchanged data by aggregating the model updates from multiple workers in the network. We co-design the switch processing with the end-host protocols and ML frameworks to provide a robust, efficient solution that speeds up training by up to 300%, and at least by 20% for a number of real-world benchmark models.","",""
209,"J. Blanchet, Yang Kang, M. KarthyekRajhaaA.","Robust Wasserstein profile inference and applications to machine learning",2016,"","","","",35,"2022-07-13 10:07:22","","10.1017/jpr.2019.49","","",,,,,209,34.83,70,3,6,"We show that several machine learning estimators, including square-root least absolute shrinkage and selection and regularized logistic regression, can be represented as solutions to distributionally robust optimization problems. The associated uncertainty regions are based on suitably defined Wasserstein distances. Hence, our representations allow us to view regularization as a result of introducing an artificial adversary that perturbs the empirical distribution to account for out-of-sample effects in loss estimation. In addition, we introduce RWPI (robust Wasserstein profile inference), a novel inference methodology which extends the use of methods inspired by empirical likelihood to the setting of optimal transport costs (of which Wasserstein distances are a particular case). We use RWPI to show how to optimally select the size of uncertainty regions, and as a consequence we are able to choose regularization parameters for these machine learning estimators without the use of cross validation. Numerical experiments are also given to validate our theoretical findings.","",""
106,"Joelle Pineau, Philippe Vincent-Lamarre, Koustuv Sinha, V. Larivière, A. Beygelzimer, Florence d'Alché-Buc, E. Fox, H. Larochelle","Improving Reproducibility in Machine Learning Research (A Report from the NeurIPS 2019 Reproducibility Program)",2020,"","","","",36,"2022-07-13 10:07:22","","","","",,,,,106,53.00,13,8,2,"One of the challenges in machine learning research is to ensure that presented and published results are sound and reliable. Reproducibility, that is obtaining similar results as presented in a paper or talk, using the same code and data (when available), is a necessary step to verify the reliability of research findings. Reproducibility is also an important step to promote open and accessible research, thereby allowing the scientific community to quickly integrate new findings and convert ideas to practice. Reproducibility also promotes the use of robust experimental workflows, which potentially reduce unintentional errors. In 2019, the Neural Information Processing Systems (NeurIPS) conference, the premier international conference for research in machine learning, introduced a reproducibility program, designed to improve the standards across the community for how we conduct, communicate, and evaluate machine learning research. The program contained three components: a code submission policy, a community-wide reproducibility challenge, and the inclusion of the Machine Learning Reproducibility checklist as part of the paper submission process. In this paper, we describe each of these components, how it was deployed, as well as what we were able to learn from this initiative.","",""
665,"Weihua Hu, Matthias Fey, M. Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, J. Leskovec","Open Graph Benchmark: Datasets for Machine Learning on Graphs",2020,"","","","",37,"2022-07-13 10:07:22","","","","",,,,,665,332.50,83,8,2,"We present the Open Graph Benchmark (OGB), a diverse set of challenging and realistic benchmark datasets to facilitate scalable, robust, and reproducible graph machine learning (ML) research. OGB datasets are large-scale (up to 100+ million nodes and 1+ billion edges), encompass multiple important graph ML tasks, and cover a diverse range of domains, ranging from social and information networks to biological networks, molecular graphs, source code ASTs, and knowledge graphs. For each dataset, we provide a unified evaluation protocol using meaningful application-specific data splits and evaluation metrics. In addition to building the datasets, we also perform extensive benchmark experiments for each dataset. Our experiments suggest that OGB datasets present significant challenges of scalability to large-scale graphs and out-of-distribution generalization under realistic data splits, indicating fruitful opportunities for future research. Finally, OGB provides an automated end-to-end graph ML pipeline that simplifies and standardizes the process of graph data loading, experimental setup, and model evaluation. OGB will be regularly updated and welcomes inputs from the community. OGB datasets as well as data loaders, evaluation scripts, baseline code, and leaderboards are publicly available at this https URL .","",""
58,"Luis Muñoz-González, Kenneth T. Co, Emil C. Lupu","Byzantine-Robust Federated Machine Learning through Adaptive Model Averaging",2019,"","","","",38,"2022-07-13 10:07:22","","","","",,,,,58,19.33,19,3,3,"Federated learning enables training collaborative machine learning models at scale with many participants whilst preserving the privacy of their datasets. Standard federated learning techniques are vulnerable to Byzantine failures, biased local datasets, and poisoning attacks. In this paper we introduce Adaptive Federated Averaging, a novel algorithm for robust federated learning that is designed to detect failures, attacks, and bad updates provided by participants in a collaborative model. We propose a Hidden Markov Model to model and learn the quality of model updates provided by each participant during training. In contrast to existing robust federated learning schemes, we propose a robust aggregation rule that detects and discards bad or malicious local model updates at each training iteration. This includes a mechanism that blocks unwanted participants, which also increases the computational and communication efficiency. Our experimental evaluation on 4 real datasets show that our algorithm is significantly more robust to faulty, noisy and malicious participants, whilst being computationally more efficient than other state-of-the-art robust federated learning methods such as Multi-KRUM and coordinate-wise median.","",""
169,"D. Kuhn, Peyman Mohajerin Esfahani, Viet Anh Nguyen, Soroosh Shafieezadeh-Abadeh","Wasserstein Distributionally Robust Optimization: Theory and Applications in Machine Learning",2019,"","","","",39,"2022-07-13 10:07:22","","10.1287/EDUC.2019.0198","","",,,,,169,56.33,42,4,3,"Many decision problems in science, engineering and economics are affected by uncertain parameters whose distribution is only indirectly observable through samples. The goal of data-driven decision-making is to learn a decision from finitely many training samples that will perform well on unseen test samples. This learning task is difficult even if all training and test samples are drawn from the same distribution---especially if the dimension of the uncertainty is large relative to the training sample size. Wasserstein distributionally robust optimization seeks data-driven decisions that perform well under the most adverse distribution within a certain Wasserstein distance from a nominal distribution constructed from the training samples. In this tutorial we will argue that this approach has many conceptual and computational benefits. Most prominently, the optimal decisions can often be computed by solving tractable convex optimization problems, and they enjoy rigorous out-of-sample and asymptotic consistency guarantees. We will also show that Wasserstein distributionally robust optimization has interesting ramifications for statistical learning and motivates new approaches for fundamental learning tasks such as classification, regression, maximum likelihood estimation or minimum mean square error estimation, among others.","",""
50,"Megha Byali, Harsh Chaudhari, A. Patra, A. Suresh","FLASH: Fast and Robust Framework for Privacy-preserving Machine Learning",2020,"","","","",40,"2022-07-13 10:07:22","","10.2478/popets-2020-0036","","",,,,,50,25.00,13,4,2,"Abstract Privacy-preserving machine learning (PPML) via Secure Multi-party Computation (MPC) has gained momentum in the recent past. Assuming a minimal network of pair-wise private channels, we propose an efficient four-party PPML framework over rings ℤ2ℓ, FLASH, the first of its kind in the regime of PPML framework, that achieves the strongest security notion of Guaranteed Output Delivery (all parties obtain the output irrespective of adversary’s behaviour). The state of the art ML frameworks such as ABY3 by Mohassel et.al (ACM CCS’18) and SecureNN by Wagh et.al (PETS’19) operate in the setting of 3 parties with one malicious corruption but achieve the weaker security guarantee of abort. We demonstrate PPML with real-time efficiency, using the following custom-made tools that overcome the limitations of the aforementioned state-of-the-art– (a) dot product, which is independent of the vector size unlike the state-of-the-art ABY3, SecureNN and ASTRA by Chaudhari et.al (ACM CCSW’19), all of which have linear dependence on the vector size. (b) Truncation and MSB Extraction, which are constant round and free of circuits like Parallel Prefix Adder (PPA) and Ripple Carry Adder (RCA), unlike ABY3 which uses these circuits and has round complexity of the order of depth of these circuits. We then exhibit the application of our FLASH framework in the secure server-aided prediction of vital algorithms– Linear Regression, Logistic Regression, Deep Neural Networks, and Binarized Neural Networks. We substantiate our theoretical claims through improvement in benchmarks of the aforementioned algorithms when compared with the current best framework ABY3. All the protocols are implemented over a 64-bit ring in LAN and WAN. Our experiments demonstrate that, for MNIST dataset, the improvement (in terms of throughput) ranges from 24 × to 1390 × over LAN and WAN together.","",""
55,"Georgios Damaskinos, El-Mahdi El-Mhamdi, R. Guerraoui, Arsany Guirguis, Sébastien Rouault","AGGREGATHOR: Byzantine Machine Learning via Robust Gradient Aggregation",2019,"","","","",41,"2022-07-13 10:07:22","","","","",,,,,55,18.33,11,5,3,"We present AGGREGATHOR, a framework that implements state-of-the-art robust (Byzantine-resilient) distributed stochastic gradient descent. Following the standard parameter server model, we assume that a minority of worker machines can be controlled by an adversary and behave arbitrarily. Such a setting has been theoretically studied with several of the existing approaches using a robust aggregation of the workers’ gradient estimations. Yet, the question is whether a Byzantine-resilient aggregation can leverage more workers to speedup learning. We answer this theoretical question, and implement these state-of-the-art theoretical approaches on AGGREGATHOR, to assess their practical costs. We built AGGREGATHOR around TensorFlow and introduce modifications for vanilla TensorFlow towards making it usable in an actual Byzantine setting. AGGREGATHOR also permits the use of unreliable gradient transfer over UDP to provide further speed-up (without losing the accuracy) over the native communication protocols (TCP-based) of TensorFlow in saturated networks. We quantify the overhead of Byzantine resilience of AGGREGATHOR to 19% and 43% (to ensure weak and strong Byzantine resilience respectively) compared to vanilla TensorFlow.","",""
29,"Fahad Shabbir Ahmad, Liaqat Ali, Liaqat Ali, Raza-Ul-Mustafa, Hasan Ali Khattak, Tahir Hameed, Iram Wajahat, Seifedine Kadry, S. Bukhari","A hybrid machine learning framework to predict mortality in paralytic ileus patients using electronic health records (EHRs)",2020,"","","","",42,"2022-07-13 10:07:22","","10.1007/s12652-020-02456-3","","",,,,,29,14.50,3,9,2,"","",""
36,"Galal Omer, O. Mutanga, E. Abdel-Rahman, E. Adam","Empirical Prediction of Leaf Area Index (LAI) of Endangered Tree Species in Intact and Fragmented Indigenous Forests Ecosystems Using WorldView-2 Data and Two Robust Machine Learning Algorithms",2016,"","","","",43,"2022-07-13 10:07:22","","10.3390/rs8040324","","",,,,,36,6.00,9,4,6,"Leaf area index (LAI) is an important biophysical trait for forest ecosystem and ecological modeling, as it plays a key role for the forest productivity and structural characteristics. The ground-based methods like the handheld optical instruments for predicting LAI are subjective, pricy and time-consuming. The advent of very high spatial resolutions multispectral data and robust machine learning regression algorithms like support vector machines (SVM) and artificial neural networks (ANN) has provided an opportunity to estimate LAI at tree species level. The objective of the this study was therefore to test the utility of spectral vegetation indices (SVI) calculated from the multispectral WorldView-2 (WV-2) data in predicting LAI at tree species level using the SVM and ANN machine learning regression algorithms. We further tested whether there are significant differences between LAI of intact and fragmented (open) indigenous forest ecosystems at tree species level. The study shows that LAI at tree species level could accurately be estimated using the fragmented stratum data compared with the intact stratum data. Specifically, our study shows that the accurate LAI predictions were achieved for Hymenocardia ulmoides using the fragmented stratum data and SVM regression model based on a validation dataset (R2Val = 0.75, RMSEVal = 0.05 (1.37% of the mean)). Our study further showed that SVM regression approach achieved more accurate models for predicting the LAI of the six endangered tree species compared with ANN regression method. It is concluded that the successful application of the WV-2 data, SVM and ANN methods in predicting LAI of six endangered tree species in the Dukuduku indigenous forest could help in making informed decisions and policies regarding management, protection and conservation of these endangered tree species.","",""
24,"P. Zivich, A. Breskin","Machine Learning for Causal Inference: On the Use of Cross-fit Estimators",2020,"","","","",44,"2022-07-13 10:07:22","","10.1097/EDE.0000000000001332","","",,,,,24,12.00,12,2,2,"Supplemental Digital Content is available in the text. Background: Modern causal inference methods allow machine learning to be used to weaken parametric modeling assumptions. However, the use of machine learning may result in complications for inference. Doubly robust cross-fit estimators have been proposed to yield better statistical properties. Methods: We conducted a simulation study to assess the performance of several different estimators for the average causal effect. The data generating mechanisms for the simulated treatment and outcome included log-transforms, polynomial terms, and discontinuities. We compared singly robust estimators (g-computation, inverse probability weighting) and doubly robust estimators (augmented inverse probability weighting, targeted maximum likelihood estimation). We estimated nuisance functions with parametric models and ensemble machine learning separately. We further assessed doubly robust cross-fit estimators. Results: With correctly specified parametric models, all of the estimators were unbiased and confidence intervals achieved nominal coverage. When used with machine learning, the doubly robust cross-fit estimators substantially outperformed all of the other estimators in terms of bias, variance, and confidence interval coverage. Conclusions: Due to the difficulty of properly specifying parametric models in high-dimensional data, doubly robust estimators with ensemble learning and cross-fitting may be the preferred approach for estimation of the average causal effect in most epidemiologic studies. However, these approaches may require larger sample sizes to avoid finite-sample issues.","",""
75,"K. Kashinath, M. Mustafa, A. Albert, J.-L. Wu, C. Jiang, S. Esmaeilzadeh, K. Azizzadenesheli, R. Wang, A. Chattopadhyay, A. Singh, A. Manepalli, D. Chirila, R. Yu, R. Walters, B. White, H. Xiao, H. Tchelepi, P. Marcus, A. Anandkumar, P. Hassanzadeh, Prabhat","Physics-informed machine learning: case studies for weather and climate modelling",2021,"","","","",45,"2022-07-13 10:07:22","","10.1098/rsta.2020.0093","","",,,,,75,75.00,8,21,1,"Machine learning (ML) provides novel and powerful ways of accurately and efficiently recognizing complex patterns, emulating nonlinear dynamics, and predicting the spatio-temporal evolution of weather and climate processes. Off-the-shelf ML models, however, do not necessarily obey the fundamental governing laws of physical systems, nor do they generalize well to scenarios on which they have not been trained. We survey systematic approaches to incorporating physics and domain knowledge into ML models and distill these approaches into broad categories. Through 10 case studies, we show how these approaches have been used successfully for emulating, downscaling, and forecasting weather and climate processes. The accomplishments of these studies include greater physical consistency, reduced training time, improved data efficiency, and better generalization. Finally, we synthesize the lessons learned and identify scientific, diagnostic, computational, and resource challenges for developing truly robust and reliable physics-informed ML models for weather and climate processes. This article is part of the theme issue ‘Machine learning for weather and climate modelling’.","",""
19,"A. Aravkin, Damek Davis","A SMART Stochastic Algorithm for Nonconvex Optimization with Applications to Robust Machine Learning",2016,"","","","",46,"2022-07-13 10:07:22","","","","",,,,,19,3.17,10,2,6,"In this paper, we show how to transform any optimization problem that arises from fitting a machine learning model into one that (1) detects and removes contaminated data from the training set while (2) simultaneously fitting the trimmed model on the uncontaminated data that remains. To solve the resulting nonconvex optimization problem, we introduce a fast stochastic proximal-gradient algorithm that incorporates prior knowledge through nonsmooth regularization. For datasets of size $n$, our approach requires $O(n^{2/3}/\varepsilon)$ gradient evaluations to reach $\varepsilon$-accuracy and, when a certain error bound holds, the complexity improves to $O(\kappa n^{2/3}\log(1/\varepsilon))$. These rates are $n^{1/3}$ times better than those achieved by typical, full gradient methods.","",""
14,"J. Y. Wang, Michael M. Ngo, D. Hessl, R. Hagerman, S. Rivera","Robust Machine Learning-Based Correction on Automatic Segmentation of the Cerebellum and Brainstem",2016,"","","","",47,"2022-07-13 10:07:22","","10.1371/journal.pone.0156123","","",,,,,14,2.33,3,5,6,"Automated segmentation is a useful method for studying large brain structures such as the cerebellum and brainstem. However, automated segmentation may lead to inaccuracy and/or undesirable boundary. The goal of the present study was to investigate whether SegAdapter, a machine learning-based method, is useful for automatically correcting large segmentation errors and disagreement in anatomical definition. We further assessed the robustness of the method in handling size of training set, differences in head coil usage, and amount of brain atrophy. High resolution T1-weighted images were acquired from 30 healthy controls scanned with either an 8-channel or 32-channel head coil. Ten patients, who suffered from brain atrophy because of fragile X-associated tremor/ataxia syndrome, were scanned using the 32-channel head coil. The initial segmentations of the cerebellum and brainstem were generated automatically using Freesurfer. Subsequently, Freesurfer’s segmentations were both manually corrected to serve as the gold standard and automatically corrected by SegAdapter. Using only 5 scans in the training set, spatial overlap with manual segmentation in Dice coefficient improved significantly from 0.956 (for Freesurfer segmentation) to 0.978 (for SegAdapter-corrected segmentation) for the cerebellum and from 0.821 to 0.954 for the brainstem. Reducing the training set size to 2 scans only decreased the Dice coefficient ≤0.002 for the cerebellum and ≤ 0.005 for the brainstem compared to the use of training set size of 5 scans in corrective learning. The method was also robust in handling differences between the training set and the test set in head coil usage and the amount of brain atrophy, which reduced spatial overlap only by <0.01. These results suggest that the combination of automated segmentation and corrective learning provides a valuable method for accurate and efficient segmentation of the cerebellum and brainstem, particularly in large-scale neuroimaging studies, and potentially for segmenting other neural regions as well.","",""
220,"Ian J. Goodfellow, P. Mcdaniel, Nicolas Papernot","Making machine learning robust against adversarial inputs",2018,"","","","",48,"2022-07-13 10:07:22","","10.1145/3134599","","",,,,,220,55.00,73,3,4,"Such inputs distort how machine-learning-based systems are able to function in the world as it is.","",""
16,"F. Calabrese, F. Pezzuto, F. Fortarezza, A. Boscolo, F. Lunardi, C. Giraudo, A. Cattelan, C. Del Vecchio, G. Lorenzoni, L. Vedovelli, N. Sella, M. Rossato, F. Rea, R. Vettor, M. Plebani, E. Cozzi, A. Crisanti, P. Navalesi, D. Gregori","Machine learning‐based analysis of alveolar and vascular injury in SARS‐CoV‐2 acute respiratory failure",2021,"","","","",49,"2022-07-13 10:07:22","","10.1002/path.5653","","",,,,,16,16.00,2,19,1,"Severe acute respiratory syndrome‐coronavirus‐2 (SARS‐CoV‐2) pneumopathy is characterized by a complex clinical picture and heterogeneous pathological lesions, both involving alveolar and vascular components. The severity and distribution of morphological lesions associated with SARS‐CoV‐2 and how they relate to clinical, laboratory, and radiological data have not yet been studied systematically. The main goals of the present study were to objectively identify pathological phenotypes and factors that, in addition to SARS‐CoV‐2, may influence their occurrence. Lungs from 26 patients who died from SARS‐CoV‐2 acute respiratory failure were comprehensively analysed. Robust machine learning techniques were implemented to obtain a global pathological score to distinguish phenotypes with prevalent vascular or alveolar injury. The score was then analysed to assess its possible correlation with clinical, laboratory, radiological, and tissue viral data. Furthermore, an exploratory random forest algorithm was developed to identify the most discriminative clinical characteristics at hospital admission that might predict pathological phenotypes of SARS‐CoV‐2. Vascular injury phenotype was observed in most cases being consistently present as pure form or in combination with alveolar injury. Phenotypes with more severe alveolar injury showed significantly more frequent tracheal intubation; longer invasive mechanical ventilation, illness duration, intensive care unit or hospital ward stay; and lower tissue viral quantity (p < 0.001). Furthermore, in this phenotype, superimposed infections, tumours, and aspiration pneumonia were also more frequent (p < 0.001). Random forest algorithm identified some clinical features at admission (body mass index, white blood cells, D‐dimer, lymphocyte and platelet counts, fever, respiratory rate, and PaCO2) to stratify patients into different clinical clusters and potential pathological phenotypes (a web‐app for score assessment has also been developed; https://r-ubesp.dctv.unipd.it/shiny/AVI-Score/). In SARS‐CoV‐2 positive patients, alveolar injury is often associated with other factors in addition to viral infection. Identifying phenotypical patterns at admission may enable a better stratification of patients, ultimately favouring the most appropriate management. © 2021 The Pathological Society of Great Britain and Ireland. Published by John Wiley & Sons, Ltd.","",""
21,"A. Naimi, Alan Mishler, Edward H. Kennedy","Challenges in Obtaining Valid Causal Effect Estimates with Machine Learning Algorithms.",2017,"","","","",50,"2022-07-13 10:07:22","","10.1093/aje/kwab201","","",,,,,21,4.20,7,3,5,"Unlike parametric regression, machine learning (ML) methods do not generally require precise knowledge of the true data generating mechanisms. As such, numerous authors have advocated for ML methods to estimate causal effects. Unfortunately, ML algorithmscan perform worse than parametric regression. We demonstrate the performance of ML-based single- and double-robust estimators. We use 100 Monte Carlo samples with sample sizes of 200, 1200, and 5000 to investigate bias and confidence interval coverage under several scenarios. In a simple confounding scenario, confounders were related to the treatment and the outcome via parametric models. In a complex confounding scenario, the simple confounders were transformed to induce complicated nonlinear relationships. In the simple scenario, when ML algorithms were used, double-robust estimators were superior to single-robust estimators. In the complex scenario, single-robust estimators with ML algorithms were at least as biased as estimators using misspecified parametric models. Double-robust estimators were less biased, but coverage was well below nominal. The use of sample splitting, inclusion of confounder interactions, reliance on a richly specified ML algorithm, and use of doubly robust estimators was the only explored approach that yielded negligible bias and nominal coverage. Our results suggest that ML based singly robust methods should be avoided.","",""
1146,"Matthias Feurer, Aaron Klein, Katharina Eggensperger, Jost Tobias Springenberg, Manuel Blum, F. Hutter","Efficient and Robust Automated Machine Learning",2015,"","","","",51,"2022-07-13 10:07:22","","","","",,,,,1146,163.71,191,6,7,"The success of machine learning in a broad range of applications has led to an ever-growing demand for machine learning systems that can be used off the shelf by non-experts. To be effective in practice, such systems need to automatically choose a good algorithm and feature preprocessing steps for a new dataset at hand, and also set their respective hyperparameters. Recent work has started to tackle this automated machine learning (AutoML) problem with the help of efficient Bayesian optimization methods. Building on this, we introduce a robust new AutoML system based on scikit-learn (using 15 classifiers, 14 feature preprocessing methods, and 4 data preprocessing methods, giving rise to a structured hypothesis space with 110 hyperparameters). This system, which we dub AUTO-SKLEARN, improves on existing AutoML methods by automatically taking into account past performance on similar datasets, and by constructing ensembles from the models evaluated during the optimization. Our system won the first phase of the ongoing ChaLearn AutoML challenge, and our comprehensive analysis on over 100 diverse datasets shows that it substantially outperforms the previous state of the art in AutoML. We also demonstrate the performance gains due to each of our contributions and derive insights into the effectiveness of the individual components of AUTO-SKLEARN.","",""
92,"Martin Rozycki, T. Satterthwaite, N. Koutsouleris, G. Erus, J. Doshi, D. Wolf, Yong Fan, R. Gur, R. Gur, E. Meisenzahl, C. Zhuo, Hong Yin, Hao Yan, W. Yue, Dai Zhang, C. Davatzikos","Multisite Machine Learning Analysis Provides a Robust Structural Imaging Signature of Schizophrenia Detectable Across Diverse Patient Populations and Within Individuals",2018,"","","","",52,"2022-07-13 10:07:22","","10.1093/schbul/sbx137","","",,,,,92,23.00,9,16,4,"Past work on relatively small, single-site studies using regional volumetry, and more recently machine learning methods, has shown that widespread structural brain abnormalities are prominent in schizophrenia. However, to be clinically useful, structural imaging biomarkers must integrate high-dimensional data and provide reproducible results across clinical populations and on an individual person basis. Using advanced multi-variate analysis tools and pooled data from case-control imaging studies conducted at 5 sites (941 adult participants, including 440 patients with schizophrenia), a neuroanatomical signature of patients with schizophrenia was found, and its robustness and reproducibility across sites, populations, and scanners, was established for single-patient classification. Analyses were conducted at multiple scales, including regional volumes, voxelwise measures, and complex distributed patterns. Single-subject classification was tested for single-site, pooled-site, and leave-site-out generalizability. Regional and voxelwise analyses revealed a pattern of widespread reduced regional gray matter volume, particularly in the medial prefrontal, temporolimbic and peri-Sylvian cortex, along with ventricular and pallidum enlargement. Multivariate classification using pooled data achieved a cross-validated prediction accuracy of 76% (AUC = 0.84). Critically, the leave-site-out validation of the detected schizophrenia signature showed accuracy/AUC range of 72-77%/0.73-0.91, suggesting a robust generalizability across sites and patient cohorts. Finally, individualized patient classifications displayed significant correlations with clinical measures of negative, but not positive, symptoms. Taken together, these results emphasize the potential for structural neuroimaging data to provide a robust and reproducible imaging signature of schizophrenia. A web-accessible portal is offered to allow the community to obtain individualized classifications of magnetic resonance imaging scans using the methods described herein.","",""
0,"H. Anh, Cao Van Kien","Robust extreme learning machine neural approach for uncertain nonlinear hyper‐chaotic system identification",2021,"","","","",53,"2022-07-13 10:07:22","","10.1002/rnc.5756","","",,,,,0,0.00,0,2,1,"This paper proposes a novel nonlinearly parameterized advanced single‐hidden layer neural extreme learning machine (ASHLN‐ELM) model in which the hidden and output weighting values are simultaneously updated using adaptively robust rules that are implemented based on Lyapunov stability principle. The proposed scheme guarantees the fast convergence speed of the state‐estimation residual errors bounded to null regarding to the influence of time‐varied disturbances. Additionally, proposed method needs no any knowledge related to desired weighting values or required approximating error. Typical uncertain hyper‐chaotic benchmark systems are used as to verify the new ASHLN‐ELM approach and to demonstrate the efficiency and the robustness of proposed method.","",""
125,"Ya Zhuo, Aria Mansouri Tehrani, A. Oliynyk, Anna C. Duke, Jakoah Brgoch","Identifying an efficient, thermally robust inorganic phosphor host via machine learning",2018,"","","","",54,"2022-07-13 10:07:22","","10.1038/s41467-018-06625-z","","",,,,,125,31.25,25,5,4,"","",""
104,"Sujan Kumar Gonugondla, Mingu Kang, Naresh R Shanbhag","A 42pJ/decision 3.12TOPS/W robust in-memory machine learning classifier with on-chip training",2018,"","","","",55,"2022-07-13 10:07:22","","10.1109/ISSCC.2018.8310398","","",,,,,104,26.00,35,3,4,"Embedded sensory systems (Fig. 31.2.1) continuously acquire and process data for inference and decision-making purposes under stringent energy constraints. These always-ON systems need to track changing data statistics and environmental conditions, such as temperature, with minimal energy consumption. Digital inference architectures [1,2] are not well-suited for such energy-constrained sensory systems due to their high energy consumption, which is dominated (>75%) by the energy cost of memory read accesses and digital computations. In-memory architectures [3,4] significantly reduce the energy cost by embedding pitch-matched analog computations in the periphery of the SRAM bitcell array (BCA). However, their analog nature combined with stringent area constraints makes these architectures susceptible to process, voltage, and temperature (PVT) variation. Previously, off-chip training [4] has been shown to be effective in compensating for PVT variations of in-memory architectures. However, PVT variations are die-specific and data statistics in always-ON sensory systems can change over time. Thus, on-chip training is critical to address both sources of variation and to enable the design of energy efficient always-ON sensory systems based on in-memory architectures. The stochastic gradient descent (SGD) algorithm is widely used to train machine learning algorithms such as support vector machines (SVMs), deep neural networks (DNNs) and others. This paper demonstrates the use of on-chip SGD-based training to compensate for PVT and data statistics variation to design a robust in-memory SVM classifier.","",""
242,"I. Evtimov, Kevin Eykholt, Earlence Fernandes, T. Kohno, Bo Li, Atul Prakash, Amir Rahmati, D. Song","Robust Physical-World Attacks on Machine Learning Models",2017,"","","","",56,"2022-07-13 10:07:22","","","","",,,,,242,48.40,30,8,5,"Deep neural network-based classifiers are known to be vulnerable to adversarial examples that can fool them into misclassifying their input through the addition of small-magnitude perturbations. However, recent studies have demonstrated that such adversarial examples are not very effective in the physical world--they either completely fail to cause misclassification or only work in restricted cases where a relatively complex image is perturbed and printed on paper. In this paper we propose a new attack algorithm--Robust Physical Perturbations (RP2)-- that generates perturbations by taking images under different conditions into account. Our algorithm can create spatially-constrained perturbations that mimic vandalism or art to reduce the likelihood of detection by a casual observer. We show that adversarial examples generated by RP2 achieve high success rates under various conditions for real road sign recognition by using an evaluation methodology that captures physical world conditions. We physically realized and evaluated two attacks, one that causes a Stop sign to be misclassified as a Speed Limit sign in 100% of the testing conditions, and one that causes a Right Turn sign to be misclassified as either a Stop or Added Lane sign in 100% of the testing conditions.","",""
67,"Jiaoping Zhang, H. S. Naik, Teshale Assefa, S. Sarkar, R. V. C. Reddy, Arti Singh, B. Ganapathysubramanian, Ashutosh Kumar Singh","Computer vision and machine learning for robust phenotyping in genome-wide studies",2017,"","","","",57,"2022-07-13 10:07:22","","10.1038/srep44048","","",,,,,67,13.40,8,8,5,"","",""
879,"Tyler Martin","Interpretable Machine Learning",2019,"","","","",58,"2022-07-13 10:07:22","","","","",,,,,879,293.00,879,1,3,"Interpretable machine learning has become a popular research direction as deep neural networks (DNNs) have become more powerful and their applications more mainstream, yet DNNs remain difficult to understand. Testing with Concept Activation Vectors, TCAV, (Kim et al. 2017) is an approach to interpreting DNNs in a human-friendly way and has recently received significant attention in the machine learning community. The TCAV algorithm achieves a degree of global interpretability for DNNs through human-defined concepts as explanations. This project introduces Robust TCAV, which builds on TCAV and experimentally determines best practices for this method. The objectives for Robust TCAV are 1) Making TCAV more consistent by reducing variance in the TCAV score distribution and 2) Increasing CAV and TCAV score resistance to perturbations. A difference of means method for CAV generation was determined to be the best practice to achieve both objectives. Many areas of the TCAV process are explored including CAV visualization in low dimensions, negative class selection, and activation perturbation in the direction of a CAV. Finally, a thresholding technique is considered to remove noise in TCAV scores. This project is a step in the direction of making TCAV, an already impactful algorithm in interpretability, more reliable and useful for practitioners.","",""
11,"B. Celik, J. Vanschoren","Adaptation Strategies for Automated Machine Learning on Evolving Data",2020,"","","","",59,"2022-07-13 10:07:22","","10.1109/TPAMI.2021.3062900","","",,,,,11,5.50,6,2,2,"Automated Machine Learning (AutoML) systems have been shown to efficiently build good models for new datasets. However, it is often not clear how well they can adapt when the data evolves over time. The main goal of this study is to understand the effect of concept drift on the performance of AutoML methods, and which adaptation strategies can be employed to make them more robust to changes in the underlying data. To that end, we propose 6 concept drift adaptation strategies and evaluate their effectiveness on a variety of AutoML approaches for building machine learning pipelines, including Bayesian optimization, genetic programming, and random search with automated stacking. These are evaluated empirically on real-world and synthetic data streams with different types of concept drift. Based on this analysis, we propose ways to develop more sophisticated and robust AutoML techniques.","",""
112,"Heinrich Jiang, Ofir Nachum","Identifying and Correcting Label Bias in Machine Learning",2019,"","","","",60,"2022-07-13 10:07:22","","","","",,,,,112,37.33,56,2,3,"Datasets often contain biases which unfairly disadvantage certain groups, and classifiers trained on such datasets can inherit these biases. In this paper, we provide a mathematical formulation of how this bias can arise. We do so by assuming the existence of underlying, unknown, and unbiased labels which are overwritten by an agent who intends to provide accurate labels but may have biases against certain groups. Despite the fact that we only observe the biased labels, we are able to show that the bias may nevertheless be corrected by re-weighting the data points without changing the labels. We show, with theoretical guarantees, that training on the re-weighted dataset corresponds to training on the unobserved but unbiased labels, thus leading to an unbiased machine learning classifier. Our procedure is fast and robust and can be used with virtually any learning algorithm. We evaluate on a number of standard machine learning fairness datasets and a variety of fairness notions, finding that our method outperforms standard approaches in achieving fair classification.","",""
70,"Md. Kamrul Hasan, Md. Ashraful Alam, Dola Das, Eklas Hossain, Mahmudul Hasan","Diabetes Prediction Using Ensembling of Different Machine Learning Classifiers",2020,"","","","",61,"2022-07-13 10:07:22","","10.1109/ACCESS.2020.2989857","","",,,,,70,35.00,14,5,2,"Diabetes, also known as chronic illness, is a group of metabolic diseases due to a high level of sugar in the blood over a long period. The risk factor and severity of diabetes can be reduced significantly if the precise early prediction is possible. The robust and accurate prediction of diabetes is highly challenging due to the limited number of labeled data and also the presence of outliers (or missing values) in the diabetes datasets. In this literature, we are proposing a robust framework for diabetes prediction where the outlier rejection, filling the missing values, data standardization, feature selection, K-fold cross-validation, and different Machine Learning (ML) classifiers (k-nearest Neighbour, Decision Trees, Random Forest, AdaBoost, Naive Bayes, and XGBoost) and Multilayer Perceptron (MLP) were employed. The weighted ensembling of different ML models is also proposed, in this literature, to improve the prediction of diabetes where the weights are estimated from the corresponding Area Under ROC Curve (AUC) of the ML model. AUC is chosen as the performance metric, which is then maximized during hyperparameter tuning using the grid search technique. All the experiments, in this literature, were conducted under the same experimental conditions using the Pima Indian Diabetes Dataset. From all the extensive experiments, our proposed ensembling classifier is the best performing classifier with the sensitivity, specificity, false omission rate, diagnostic odds ratio, and AUC as 0.789, 0.934, 0.092, 66.234, and 0.950 respectively which outperforms the state-of-the-art results by 2.00 % in AUC. Our proposed framework for the diabetes prediction outperforms the other methods discussed in the article. It can also provide better results on the same dataset which can lead to better performance in diabetes prediction. Our source code for diabetes prediction is made publicly available.","",""
20,"L. Pan, Caixia Cheng, U. Haberkorn, A. Dimitrakopoulou-Strauss","Machine learning-based kinetic modeling: a robust and reproducible solution for quantitative analysis of dynamic PET data.",2017,"","","","",62,"2022-07-13 10:07:22","","10.1088/1361-6560/aa6244","","",,,,,20,4.00,5,4,5,"A variety of compartment models are used for the quantitative analysis of dynamic positron emission tomography (PET) data. Traditionally, these models use an iterative fitting (IF) method to find the least squares between the measured and calculated values over time, which may encounter some problems such as the overfitting of model parameters and a lack of reproducibility, especially when handling noisy data or error data. In this paper, a machine learning (ML) based kinetic modeling method is introduced, which can fully utilize a historical reference database to build a moderate kinetic model directly dealing with noisy data but not trying to smooth the noise in the image. Also, due to the database, the presented method is capable of automatically adjusting the models using a multi-thread grid parameter searching technique. Furthermore, a candidate competition concept is proposed to combine the advantages of the ML and IF modeling methods, which could find a balance between fitting to historical data and to the unseen target curve. The machine learning based method provides a robust and reproducible solution that is user-independent for VOI-based and pixel-wise quantitative analysis of dynamic PET data.","",""
94,"Jiangpeng Wu, Pengyi Zhang, Liting Zhang, W. Meng, Junfeng Li, C. Tong, Yonghong Li, Jing Cai, Zengwei Yang, Jinhong Zhu, M. Zhao, Huirong Huang, Xiaodong Xie, Shuyan Li","Rapid and accurate identification of COVID-19 infection through machine learning based on clinical available blood test results",2020,"","","","",63,"2022-07-13 10:07:22","","10.1101/2020.04.02.20051136","","",,,,,94,47.00,9,14,2,"Since the sudden outbreak of coronavirus disease 2019 (COVID-19), it has rapidly evolved into a momentous global health concern. Due to the lack of constructive information on the pathogenesis of COVID-19 and specific treatment, it highlights the importance of early diagnosis and timely treatment. In this study, 11 key blood indices were extracted through random forest algorithm to build the final assistant discrimination tool from 49 clinical available blood test data which were derived by commercial blood test equipments. The method presented robust outcome to accurately identify COVID-19 from a variety of suspected patients with similar CT information or similar symptoms, with accuracy of 0.9795 and 0.9697 for the cross-validation set and test set, respectively. The tool also demonstrated its outstanding performance on an external validation set that was completely independent of the modeling process, with sensitivity, specificity, and overall accuracy of 0.9512, 0.9697, and 0.9595, respectively. Besides, 24 samples from overseas infected patients with COVID-19 were used to make an in-depth clinical assessment with accuracy of 0.9167. After multiple verification, the reliability and repeatability of the tool has been fully evaluated, and it has the potential to develop into an emerging technology to identify COVID-19 and lower the burden of global public health. The proposed tool is well-suited to carry out preliminary assessment of suspected patients and help them to get timely treatment and quarantine suggestion. The assistant tool is now available online at http://lishuyan.lzu.edu.cn/COVID2019_2/.","",""
8,"M. Heidari, S. Lakshmivarahan, Seyedehnafiseh Mirniaharikandehei, Gopichandh Danala, S. K. Maryada, Hong Liu, B. Zheng, School of Electrical, Computer Engineering, U. Oklahoma, Norman, Ok, Usa, S. O. Sciences","Applying a Random Projection Algorithm to Optimize Machine Learning Model for Breast Lesion Classification",2020,"","","","",64,"2022-07-13 10:07:22","","10.1109/TBME.2021.3054248","","",,,,,8,4.00,1,14,2,"Objective: Since computer-aided diagnosis (CAD) schemes of medical images usually computes large number of image features, which creates a challenge of how to identify a small and optimal feature vector to build robust machine learning models, the objective of this study is to investigate feasibility of applying a random projection algorithm (RPA) to build an optimal feature vector from the initially CAD-generated large feature pool and improve performance of machine learning model. Methods: We assemble a retrospective dataset involving 1,487 cases of mammograms in which 644 cases have confirmed malignant mass lesions and 843 have benign lesions. A CAD scheme is first applied to segment mass regions and initially compute 181 features. Then, support vector machine (SVM) models embedded with several feature dimensionality reduction methods are built to predict likelihood of lesions being malignant. All SVM models are trained and tested using a leave-one-case-out cross-validation method. SVM generates a likelihood score of each segmented mass region depicting on one-view mammogram. By fusion of two scores of the same mass depicting on two-view mammograms, a case-based likelihood score is also evaluated. Results: Comparing with the principle component analyses, nonnegative matrix factorization, and Chi-squared methods, SVM embedded with RPA yielded a significantly higher case-based lesion classification performance with the area under ROC curve of 0.84 ± 0.01 (p<0.02). Conclusion: The study demonstrates that RPA is a promising method to generate optimal feature vectors and improve SVM performance. Significance: This study presents a new method to develop CAD schemes with significantly higher and robust performance.","",""
122,"Daniel R. Schrider, A. Kern","S/HIC: Robust Identification of Soft and Hard Sweeps Using Machine Learning",2015,"","","","",65,"2022-07-13 10:07:22","","10.1371/journal.pgen.1005928","","",,,,,122,17.43,61,2,7,"Detecting the targets of adaptive natural selection from whole genome sequencing data is a central problem for population genetics. However, to date most methods have shown sub-optimal performance under realistic demographic scenarios. Moreover, over the past decade there has been a renewed interest in determining the importance of selection from standing variation in adaptation of natural populations, yet very few methods for inferring this model of adaptation at the genome scale have been introduced. Here we introduce a new method, S/HIC, which uses supervised machine learning to precisely infer the location of both hard and soft selective sweeps. We show that S/HIC has unrivaled accuracy for detecting sweeps under demographic histories that are relevant to human populations, and distinguishing sweeps from linked as well as neutrally evolving regions. Moreover we show that S/HIC is uniquely robust among its competitors to model misspecification. Thus even if the true demographic model of a population differs catastrophically from that specified by the user, S/HIC still retains impressive discriminatory power. Finally we apply S/HIC to the case of resequencing data from human chromosome 18 in a European population sample and demonstrate that we can reliably recover selective sweeps that have been identified earlier using less specific and sensitive methods.","",""
6,"A. Soni, Dharamvir Dharmacharya, A. Pal, V. Srivastava, R. Shaw, Ankush Ghosh","Design of a Machine Learning-Based Self-driving Car",2021,"","","","",66,"2022-07-13 10:07:22","","10.1007/978-981-16-0598-7_11","","",,,,,6,6.00,1,6,1,"","",""
75,"M. Prosperi, Yi Guo, M. Sperrin, J. Koopman, Jae Min, Xing He, S. Rich, Mo Wang, I. Buchan, J. Bian","Causal inference and counterfactual prediction in machine learning for actionable healthcare",2020,"","","","",67,"2022-07-13 10:07:22","","10.1038/s42256-020-0197-y","","",,,,,75,37.50,8,10,2,"","",""
32,"A. Serban, K. V. D. Blom, H. Hoos, Joost Visser","Adoption and Effects of Software Engineering Best Practices in Machine Learning",2020,"","","","",68,"2022-07-13 10:07:22","","10.1145/3382494.3410681","","",,,,,32,16.00,8,4,2,"Background. The increasing reliance on applications with machine learning (ML) components calls for mature engineering techniques that ensure these are built in a robust and future-proof manner. Aim. We aim to empirically determine the state of the art in how teams develop, deploy and maintain software with ML components. Method. We mined both academic and grey literature and identified 29 engineering best practices for ML applications. We conducted a survey among 313 practitioners to determine the degree of adoption for these practices and to validate their perceived effects. Using the survey responses, we quantified practice adoption, differentiated along demographic characteristics, such as geography or team size. We also tested correlations and investigated linear and non-linear relationships between practices and their perceived effect using various statistical models. Results. Our findings indicate, for example, that larger teams tend to adopt more practices, and that traditional software engineering practices tend to have lower adoption than ML specific practices. Also, the statistical models can accurately predict perceived effects such as agility, software quality and traceability, from the degree of adoption for specific sets of practices. Combining practice adoption rates with practice importance, as revealed by statistical models, we identify practices that are important but have low adoption, as well as practices that are widely adopted but are less important for the effects we studied. Conclusion. Overall, our survey and the analysis of responses received provide a quantitative basis for assessment and step-wise improvement of practice adoption by ML teams.","",""
33,"JungHo Kong, Heetak Lee, Donghyo Kim, S. Han, Doyeon Ha, Kunyoo Shin, Sanguk Kim","Network-based machine learning in colorectal and bladder organoid models predicts anti-cancer drug efficacy in patients",2020,"","","","",69,"2022-07-13 10:07:22","","10.1038/s41467-020-19313-8","","",,,,,33,16.50,5,7,2,"","",""
32,"M. L. Prado","Machine Learning for Asset Managers",2020,"","","","",70,"2022-07-13 10:07:22","","10.1017/9781108883658","","",,,,,32,16.00,32,1,2,"Successful investment strategies are specific implementations of general theories. An investment strategy that lacks a theoretical justification is likely to be false. Hence, an asset manager should concentrate her efforts on developing a theory rather than on backtesting potential trading rules. The purpose of this Element is to introduce machine learning (ML) tools that can help asset managers discover economic and financial theories. ML is not a black box, and it does not necessarily overfit. ML tools complement rather than replace the classical statistical methods. Some of ML's strengths include (1) a focus on out-of-sample predictability over variance adjudication; (2) the use of computational methods to avoid relying on (potentially unrealistic) assumptions; (3) the ability to â€œlearnâ€ complex specifications, including nonlinear, hierarchical, and noncontinuous interaction effects in a high-dimensional space; and (4) the ability to disentangle the variable search from the specification search, robust to multicollinearity and other substitution effects.","",""
2,"Yifeng Gao, Hosein Mohammadi Makrani, Mehrdad Aliasgari, Amin Rezaei, Jessica Lin, H. Homayoun, H. Sayadi","Adaptive-HMD: Accurate and Cost-Efficient Machine Learning-Driven Malware Detection using Microarchitectural Events",2021,"","","","",71,"2022-07-13 10:07:22","","10.1109/IOLTS52814.2021.9486701","","",,,,,2,2.00,0,7,1,"To address the high complexity and computational overheads of conventional software-based detection techniques, Hardware Malware Detection (HMD) has shown promising results as an alternative anomaly detection solution. HMD methods apply Machine Learning (ML) classifiers on microarchitectural events monitored by built-in Hardware Performance Counter (HPC) registers available in modern microprocessors to recognize the patterns of anomalies (e.g., signatures of malicious applications). Existing hardware malware detection solutions have mainly focused on utilizing standard ML algorithms to detect the existence of malware without considering an adaptive and cost-efficient approach for online malware detection. Our comprehensive analysis across a wide range of malicious software applications and different branches of machine learning algorithms indicates that the type of adopted ML algorithm to detect malicious applications at the hardware level highly correlates with the type of the examined malware, and the ultimate performance evaluation metric (F-measure, robustness, latency, detection rate/cost, etc.) to select the most efficient ML model for distinguishing the target malware from benign program. Therefore, in this work we propose Adaptive-HMD, an accurate and cost-efficient machine learning-driven framework for online malware detection using low-level microarchitectural events collected from HPC registers. Adaptive-HMD is equipped with a lightweight tree-based decision-making algorithm that accurately selects the most efficient ML model to be used for the inference in online malware detection according to the users' preference and optimal performance vs. cost (hardware overhead and latency) criteria. The experimental results demonstrate that Adaptive-HMD achieves up to 94% detection rate (F-measure) while improving the cost-efficiency of ML-based malware detection by more than 5X as compared to existing ensemble-based malware detection methods.","",""
242,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",72,"2022-07-13 10:07:22","","10.1145/3308809.3308857","","",,,,,242,48.40,81,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server andm working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of them working machines suffer Byzantine faults - a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks.","",""
44,"Min Zhang, Haoxuan You, Pranav Kadam, Shan Liu, C.-C. Jay Kuo","PointHop: An Explainable Machine Learning Method for Point Cloud Classification",2019,"","","","",73,"2022-07-13 10:07:22","","10.1109/TMM.2019.2963592","","",,,,,44,14.67,9,5,3,"An explainable machine learning method for point cloud classification, called the PointHop method, is proposed in this work. The PointHop method consists of two stages: 1) local-to-global attribute building through iterative one-hop information exchange and 2) classification and ensembles. In the attribute building stage, we address the problem of unordered point cloud data using a space partitioning procedure and developing a robust descriptor that characterizes the relationship between a point and its one-hop neighbor in a PointHop unit. When we put multiple PointHop units in cascade, the attributes of a point will grow by taking its relationship with one-hop neighbor points into account iteratively. Furthermore, to control the rapid dimension growth of the attribute vector associated with a point, we use the Saab transform to reduce the attribute dimension in each PointHop unit. In the classification and ensemble stage, we feed the feature vector obtained from multiple PointHop units to a classifier. We explore ensemble methods to improve the classification performance furthermore. It is shown by experimental results that the PointHop method offers classification performance that is comparable with state-of-the-art methods while demanding much lower training complexity.","",""
22,"Lal Hussain, I. Awan, W. Aziz, Sharjil Saeed, Amjad Ali, Farukh Zeeshan, K. Kwak","Detecting Congestive Heart Failure by Extracting Multimodal Features and Employing Machine Learning Techniques",2020,"","","","",74,"2022-07-13 10:07:22","","10.1155/2020/4281243","","",,,,,22,11.00,3,7,2,"The adaptability of heart to external and internal stimuli is reflected by the heart rate variability (HRV). Reduced HRV can be a predictor of negative cardiovascular outcomes. Based on the nonlinear, nonstationary, and highly complex dynamics of the controlling mechanism of the cardiovascular system, linear HRV measures have limited capability to accurately analyze the underlying dynamics. In this study, we propose an automated system to analyze HRV signals by extracting multimodal features to capture temporal, spectral, and complex dynamics. Robust machine learning techniques, such as support vector machine (SVM) with its kernel (linear, Gaussian, radial base function, and polynomial), decision tree (DT), k-nearest neighbor (KNN), and ensemble classifiers, were employed to evaluate the detection performance. Performance was evaluated in terms of specificity, sensitivity, positive predictive value (PPV), negative predictive value (NPV), and area under the receiver operating characteristic curve (AUC). The highest performance was obtained using SVM linear kernel (TA = 93.1%, AUC = 0.97, 95% CI [lower bound = 0.04, upper bound = 0.89]), followed by ensemble subspace discriminant (TA = 91.4%, AUC = 0.96, 95% CI [lower bound 0.07, upper bound = 0.81]) and SVM medium Gaussian kernel (TA = 90.5%, AUC = 0.95, 95% CI [lower bound = 0.07, upper bound = 0.86]). The results reveal that the proposed approach can provide an effective and computationally efficient tool for automatic detection of congestive heart failure patients.","",""
35,"Yan Naung Soe, Yaokai Feng, P. Santosa, Rudy Hartanto, K. Sakurai","Machine Learning-Based IoT-Botnet Attack Detection with Sequential Architecture †",2020,"","","","",75,"2022-07-13 10:07:22","","10.3390/s20164372","","",,,,,35,17.50,7,5,2,"With the rapid development and popularization of Internet of Things (IoT) devices, an increasing number of cyber-attacks are targeting such devices. It was said that most of the attacks in IoT environments are botnet-based attacks. Many security weaknesses still exist on the IoT devices because most of them have not enough memory and computational resource for robust security mechanisms. Moreover, many existing rule-based detection systems can be circumvented by attackers. In this study, we proposed a machine learning (ML)-based botnet attack detection framework with sequential detection architecture. An efficient feature selection approach is adopted to implement a lightweight detection system with a high performance. The overall detection performance achieves around 99% for the botnet attack detection using three different ML algorithms, including artificial neural network (ANN), J48 decision tree, and Naïve Bayes. The experiment result indicates that the proposed architecture can effectively detect botnet-based attacks, and also can be extended with corresponding sub-engines for new kinds of attacks.","",""
41,"Sirui Lu, L. Duan, D. Deng","Quantum Adversarial Machine Learning",2019,"","","","",76,"2022-07-13 10:07:22","","10.1103/PHYSREVRESEARCH.2.033212","","",,,,,41,13.67,14,3,3,"Adversarial machine learning is an emerging field that focuses on studying vulnerabilities of machine learning approaches in adversarial settings and developing techniques accordingly to make learning robust to adversarial manipulations. It plays a vital role in various machine learning applications and has attracted tremendous attention across different communities recently. In this paper, we explore different adversarial scenarios in the context of quantum machine learning. We find that, similar to traditional classifiers based on classical neural networks, quantum learning systems are likewise vulnerable to crafted adversarial examples, independent of whether the input data is classical or quantum. In particular, we find that a quantum classifier that achieves nearly the state-of-the-art accuracy can be conclusively deceived by adversarial examples obtained via adding imperceptible perturbations to the original legitimate samples. This is explicitly demonstrated with quantum adversarial learning in different scenarios, including classifying real-life images (e.g., handwritten digit images in the dataset MNIST), learning phases of matter (such as, ferromagnetic/paramagnetic orders and symmetry protected topological phases), and classifying quantum data. Furthermore, we show that based on the information of the adversarial examples at hand, practical defense strategies can be designed to fight against a number of different attacks. Our results uncover the notable vulnerability of quantum machine learning systems to adversarial perturbations, which not only reveals a novel perspective in bridging machine learning and quantum physics in theory but also provides valuable guidance for practical applications of quantum classifiers based on both near-term and future quantum technologies.","",""
37,"Efstathios D. Gennatas, J. Friedman, L. Ungar, R. Pirracchio, Eric Eaton, L. Reichman, Y. Interian, C. Simone, A. Auerbach, E. Delgado, M. J. Laan, T. Solberg, G. Valdes","Expert-augmented machine learning",2019,"","","","",77,"2022-07-13 10:07:22","","10.1073/pnas.1906831117","","",,,,,37,12.33,4,13,3,"Significance Machine learning is increasingly used across fields to derive insights from data, which further our understanding of the world and help us anticipate the future. The performance of predictive modeling is dependent on the amount and quality of available data. In practice, we rely on human experts to perform certain tasks and on machine learning for others. However, the optimal learning strategy may involve combining the complementary strengths of humans and machines. We present expert-augmented machine learning, an automated way to automatically extract problem-specific human expert knowledge and integrate it with machine learning to build robust, dependable, and data-efficient predictive models. Machine learning is proving invaluable across disciplines. However, its success is often limited by the quality and quantity of available data, while its adoption is limited by the level of trust afforded by given models. Human vs. machine performance is commonly compared empirically to decide whether a certain task should be performed by a computer or an expert. In reality, the optimal learning strategy may involve combining the complementary strengths of humans and machines. Here, we present expert-augmented machine learning (EAML), an automated method that guides the extraction of expert knowledge and its integration into machine-learned models. We used a large dataset of intensive-care patient data to derive 126 decision rules that predict hospital mortality. Using an online platform, we asked 15 clinicians to assess the relative risk of the subpopulation defined by each rule compared to the total sample. We compared the clinician-assessed risk to the empirical risk and found that, while clinicians agreed with the data in most cases, there were notable exceptions where they overestimated or underestimated the true risk. Studying the rules with greatest disagreement, we identified problems with the training data, including one miscoded variable and one hidden confounder. Filtering the rules based on the extent of disagreement between clinician-assessed risk and empirical risk, we improved performance on out-of-sample data and were able to train with less data. EAML provides a platform for automated creation of problem-specific priors, which help build robust and dependable machine-learning models in critical applications.","",""
2,"Sarath Shekkizhar, Antonio Ortega","Revisiting Local Neighborhood Methods in Machine Learning",2021,"","","","",78,"2022-07-13 10:07:22","","10.1109/DSLW51110.2021.9523409","","",,,,,2,2.00,1,2,1,"Several machine learning methods leverage the idea of locality by using k-nearest neighbor (KNN) techniques to design better pattern recognition models. However, the choice of KNN parameters such as k is often made experimentally, e.g., via cross-validation, leading to local neighborhoods without a clear geometric interpretation. In this paper, we replace KNN with our recently introduced polytope neighborhood scheme - Non Negative Kernel regression (NNK). NNK formulates neighborhood selection as a sparse signal approximation problem and is adaptive to the local distribution of samples in the neighborhood of the data point of interest. We analyze the benefits of local neighborhood construction based on NNK. In particular, we study the generalization properties of local interpolation using NNK and present data dependent bounds in the non asymptotic setting. The applicability of NNK in transductive few shot learning setting and for measuring distance between two datasets is demonstrated. NNK exhibits robust, superior performance in comparison to standard locally weighted neighborhood methods.","",""
28,"Katie Aafjes-van Doorn, Céline Kamsteeg, Jordan Bate, Marc Aafjes","A scoping review of machine learning in psychotherapy research",2020,"","","","",79,"2022-07-13 10:07:22","","10.1080/10503307.2020.1808729","","",,,,,28,14.00,7,4,2,"Abstract Machine learning (ML) offers robust statistical and probabilistic techniques that can help to make sense of large amounts of data. This scoping review paper aims to broadly explore the nature of research activity using ML in the context of psychological talk therapies, highlighting the scope of current methods and considerations for clinical practice and directions for future research. Using a systematic search methodology, fifty-one studies were identified. A narrative synthesis indicates two types of studies, those who developed and tested an ML model (k=44), and those who reported on the feasibility of a particular treatment tool that uses an ML algorithm (k=7). Most model development studies used supervised learning techniques to classify or predict labeled treatment process or outcome data, whereas others used unsupervised techniques to identify clusters in the unlabeled patient or treatment data. Overall, the current applications of ML in psychotherapy research demonstrated a range of possible benefits for indications of treatment process, adherence, therapist skills and treatment response prediction, as well as ways to accelerate research through automated behavioral or linguistic process coding. Given the novelty and potential of this research field, these proof-of-concept studies are encouraging, however, do not necessarily translate to improved clinical practice (yet).","",""
0,"Yinyihong Liu","Airbnb Pricing Based on Statistical Machine Learning Models",2021,"","","","",80,"2022-07-13 10:07:22","","10.1109/CONF-SPML54095.2021.00042","","",,,,,0,0.00,0,1,1,"Being one of the largest online accommodation booking platforms, Airbnb has many hosts who are seeking for more proper prices to increase their booking rate. To develop a good pricing prediction model, this paper has employed machine learning models including KNN, MLR, LASSO regression, Ridge regression, Random Forest, Gradient Boosting and XGBoost etc. While past studies on Airbnb pricing have applied quantitative pricing, some face the problems that the models are not robust enough and some face the problem of not training the model plentily. To fill this gap, we give careful consideration in exploratory data analysis to make the dataset more reasonable, apply many robust models ranging from regularized regression to ensemble models and use cross validation and random search to tune each parameter in each model. In this way, we not only select XGBoost as the best model for price prediction with R2 score 0.6321, but also uncover the features which have statistical significance with the target price.","",""
0,"Uma Gunasilan","Debate as a learning activity for teaching programming: a case in the subject of machine learning",2021,"","","","",81,"2022-07-13 10:07:22","","10.1108/heswbl-01-2021-0006","","",,,,,0,0.00,0,1,1,"PurposeDebates are well known to encompass a variety of skills we would like higher education candidates to embody when they graduate.Design/methodology/approachDebates in a classroom with computer science as the main subject has been popular in high schools particularly with emerging issues around the area, however it does not have as an extensive similar documented outreach in tertiary education, particularly in the area of hard computer sciences and more recent concentrations of computer science, such as machine learning, artificial intelligence and cloud computing.FindingsTo explore further, the debate dataset had more methodologies applied and was split into training and testing sets, whose results were then compared by a standardized measure: Root Mean Square Error (RMSE) which is currently standard in the industry. The rationale of the approach is to quantify that debate activities have an immensely positive impact towards both the teaching and learning in technical subjects and needs to be more often and robustly used within higher education.Originality/valueThe rationale of the approach is that classroom debate activities equip students with verbal and social learning styles and an opportunity to engage with content in a way that is more comfortable than working with traditional lecture-and-laboratory style learning.","",""
0,"Yan Zhou, Murat Kantarcioglu, B. Xi","A Game Theoretic Perspective on Adversarial Machine Learning and Related Cybersecurity Applications",2021,"","","","",82,"2022-07-13 10:07:22","","10.1002/9781119723950.ch13","","",,,,,0,0.00,0,3,1,"In cybersecurity applications where machine learning algorithms are increasingly used to detect vulnerabilities, a somewhat unique challenge arises as exploits targeting machine learning models are constantly devised by the attackers. Traditional machine learning models are no longer robust and reliable when they are under attack. The action and reaction between machine learning systems and the adversary can be modeled as a game between two or more players. Under well‐defined attack models, game theory can provide robustness guarantee for machine learning models that are otherwise vulnerable to application‐time data corruption. We review two cases of game theory‐based machine learning techniques: in one case, players play a zero sum game by following a minimax strategy, while in the other case, players play a sequential game with one player as the leader and the rest as the followers. Experimental results on e‐mail spam and web spam datasets are presented. In the zero sum game, we demonstrate that an adversarial SVM model built upon the minimax strategy is much more resilient to adversarial attacks than standard SVM and one‐class SVM models. We also show that optimal learning strategies derived to counter overly pessimistic attack models can produce unsatisfactory results when the real attacks are much weaker. In the sequential game, we demonstrate that the mixed strategy, allowing a player to randomize over available strategies, is the best solution in general without knowing what types of adversaries machine learning applications are facing in the wild. We also discuss scenarios where players' behavior may derail rational decision making and models that consider such decision risks.","",""
34,"Abhay Lokesh Kashyap, Lushan Han, Roberto Yus, Jennifer Sleeman, Taneeya Satyapanich, S. Gandhi, Timothy W. Finin","Robust semantic text similarity using LSA, machine learning, and linguistic resources",2016,"","","","",83,"2022-07-13 10:07:22","","10.1007/s10579-015-9319-2","","",,,,,34,5.67,5,7,6,"","",""
37,"Leif Hancox-Li","Robustness in machine learning explanations: does it matter?",2020,"","","","",84,"2022-07-13 10:07:22","","10.1145/3351095.3372836","","",,,,,37,18.50,37,1,2,"The explainable AI literature contains multiple notions of what an explanation is and what desiderata explanations should satisfy. One implicit source of disagreement is how far the explanations should reflect real patterns in the data or the world. This disagreement underlies debates about other desiderata, such as how robust explanations are to slight perturbations in the input data. I argue that robustness is desirable to the extent that we're concerned about finding real patterns in the world. The import of real patterns differs according to the problem context. In some contexts, non-robust explanations can constitute a moral hazard. By being clear about the extent to which we care about capturing real patterns, we can also determine whether the Rashomon Effect is a boon or a bane.","",""
41,"F. B. Banadkooki, M. Ehteram, A. Ahmed, F. Teo, C. Fai, H. Afan, Michelle Sapitang, A. El-Shafie","Enhancement of Groundwater-Level Prediction Using an Integrated Machine Learning Model Optimized by Whale Algorithm",2020,"","","","",85,"2022-07-13 10:07:22","","10.1007/s11053-020-09634-2","","",,,,,41,20.50,5,8,2,"","",""
87,"G. Peng, M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, Savador Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Multiscale modeling meets machine learning: What can we learn?",2019,"","","","",86,"2022-07-13 10:07:22","","10.1007/s11831-020-09405-5","","",,,,,87,29.00,9,12,3,"","",""
63,"Z. Yao, A. Gholami, Sheng Shen, K. Keutzer, M. Mahoney","ADAHESSIAN: An Adaptive Second Order Optimizer for Machine Learning",2020,"","","","",87,"2022-07-13 10:07:22","","","","",,,,,63,31.50,13,5,2,"We introduce AdaHessian, a second order stochastic optimization algorithm which dynamically incorporates the curvature of the loss function via ADAptive estimates of the Hessian. Second order algorithms are among the most powerful optimization algorithms with superior convergence properties as compared to first order methods such as SGD and ADAM. The main disadvantage of traditional second order methods is their heavier per-iteration computation and poor accuracy as compared to first order methods. To address these, we incorporate several novel approaches in AdaHessian, including: (i) a new variance reduction estimate of the Hessian diagonal with low computational overhead; (ii) a root-mean-square exponential moving average to smooth out variations of the Hessian diagonal across different iterations; and (iii) a block diagonal averaging to reduce the variance of Hessian diagonal elements. We show that AdaHessian achieves new state-of-the-art results by a large margin as compared to other adaptive optimization methods, including variants of ADAM. In particular, we perform extensive tests on CV, NLP, and recommendation system tasks and find that AdaHessian: (i) achieves 1.80\%/1.45\% higher accuracy on ResNets20/32 on Cifar10, and 5.55\% higher accuracy on ImageNet as compared to ADAM; (ii) outperforms ADAMW for transformers by 0.27/0.33 BLEU score on IWSLT14/WMT14 and 1.8/1.0 PPL on PTB/Wikitext-103; and (iii) achieves 0.032\% better score than AdaGrad for DLRM on the Criteo Ad Kaggle dataset. Importantly, we show that the cost per iteration of AdaHessian is comparable to first-order methods, and that it exhibits robustness towards its hyperparameters. The code for AdaHessian is open-sourced and publicly available.","",""
22,"M. Magdon-Ismail","Machine Learning the Phenomenology of COVID-19 From Early Infection Dynamics",2020,"","","","",88,"2022-07-13 10:07:22","","10.1101/2020.03.17.20037309","","",,,,,22,11.00,22,1,2,"We present a robust data-driven machine learning analysis of the COVID-19 pandemic from its early infection dynamics, specifically infection counts over time. The goal is to extract actionable public health insights. These insights include the infectious force, the rate of a mild infection becoming serious, estimates for asymtomatic infections and predictions of new infections over time. We focus on USA data starting from the first confirmed infection on January 20 2020. Our methods reveal significant asymptomatic (hidden) infection, a lag of about 10 days, and we quantitatively confirm that the infectious force is strong with about a 0.14% transition from mild to serious infection. Our methods are efficient, robust and general, being agnostic to the specific virus and applicable to different populations or cohorts.","",""
29,"Chenglong You, M. A. Quiroz-Juárez, A. Lambert, N. Bhusal, Chao Dong, A. Pérez-Leija, A. Javaid, R. León-Montiel, O. Magaña-Loaiza","Identification of light sources using machine learning",2019,"","","","",89,"2022-07-13 10:07:22","","10.1063/1.5133846","","",,,,,29,9.67,3,9,3,"The identification of light sources represents a task of utmost importance for the development of multiple photonic technologies. Over the last decades, the identification of light sources as diverse as sunlight, laser radiation and molecule fluorescence has relied on the collection of photon statistics or the implementation of quantum state tomography. In general, this task requires an extensive number of measurements to unveil the characteristic statistical fluctuations and correlation properties of light, particularly in the low-photon flux regime. In this article, we exploit the self-learning features of artificial neural networks and naive Bayes classifier to dramatically reduce the number of measurements required to discriminate thermal light from coherent light at the single-photon level. We demonstrate robust light identification with tens of measurements at mean photon numbers below one. Our work demonstrates an improvement in terms of the number of measurements of several orders of magnitude with respect to conventional schemes for characterization of light sources. Our work has important implications for multiple photonic technologies such as LIDAR and microscopy.","",""
0,"T. Martin, S. Areibi, G. Grewal","Effective Machine-Learning Models for Predicting Routability During FPGA Placement",2021,"","","","",90,"2022-07-13 10:07:22","","10.1109/MLCAD52597.2021.9531243","","",,,,,0,0.00,0,3,1,"The ability to efficiently and accurately predict placement routability, while avoiding the large computational cost of performing routing, is an asset when seeking to reduce total placement and routing runtime. In this paper, we present a series of simple ML models and ensembles to predict the routability of a placement solution. Ensembles based on Bagging, Boosting and Stack of classifiers are introduced to produce more accurate and robust solutions than single/simple models. Our results show an improvement in prediction accuracy and runtime compared to the best published results in the literature.","",""
30,"J. Bullock, A. Momeni","Ml.lib: robust, cross-platform, open-source machine learning for max and pure data",2015,"","","","",91,"2022-07-13 10:07:22","","","","",,,,,30,4.29,15,2,7,"This paper documents the development of ml.lib: a set of open-source tools designed for employing a wide range of machine learning techniques within two popular real-time programming environments, namely Max and Pure Data. ml.lib is a cross-platform, lightweight wrapper around Nick Gillian's Gesture Recognition Toolkit, a C++ library that includes a wide range of data processing and machine learning techniques. ml.lib adapts these techniques for real-time use within popular data-flow IDEs, allowing instrument designers and performers to integrate robust learning, classification and mapping approaches within their existing workflows. ml.lib has been carefully de-signed to allow users to experiment with and incorporate ma-chine learning techniques within an interactive arts context with minimal prior knowledge. A simple, logical and consistent, scalable interface has been provided across over sixteen exter-nals in order to maximize learnability and discoverability. A focus on portability and maintainability has enabled ml.lib to support a range of computing architectures - including ARM - and operating systems such as Mac OS, GNU/Linux and Win-dows, making it the most comprehensive machine learning implementation available for Max and Pure Data.","",""
1920,"A. Kurakin, Ian J. Goodfellow, Samy Bengio","Adversarial Machine Learning at Scale",2016,"","","","",92,"2022-07-13 10:07:22","","","","",,,,,1920,320.00,640,3,6,"Adversarial examples are malicious inputs designed to fool machine learning models. They often transfer from one model to another, allowing attackers to mount black box attacks without knowledge of the target model's parameters. Adversarial training is the process of explicitly training a model on adversarial examples, in order to make it more robust to attack or to reduce its test error on clean inputs. So far, adversarial training has primarily been applied to small problems. In this research, we apply adversarial training to ImageNet. Our contributions include: (1) recommendations for how to succesfully scale adversarial training to large models and datasets, (2) the observation that adversarial training confers robustness to single-step attack methods, (3) the finding that multi-step attack methods are somewhat less transferable than single-step attack methods, so single-step attacks are the best for mounting black-box attacks, and (4) resolution of a ""label leaking"" effect that causes adversarially trained models to perform better on adversarial examples than on clean examples, because the adversarial example construction process uses the true label and the model can learn to exploit regularities in the construction process.","",""
7,"Rahul, Priyansh Kedia, Subrat Sarangi, Monika","Analysis of machine learning models for malware detection",2020,"","","","",93,"2022-07-13 10:07:22","","10.1080/09720529.2020.1721870","","",,,,,7,3.50,2,4,2,"Abstract With the increasing importance of the internet and computers in the modern world, the task of its maintenance and protection from the threats posed by malicious software has become incredibly important. Malwares interfere with the regular working operations of other files and are responsible for corruption and leakage of private data to the outside world. The overall purpose of this research was to handle this exponentially growing threat to information technology and find a robust machine learning model required for the correct detection of malware. A more efficient and real-time working model is required for detection. The aim of this paper is to provide a concise analysis of malware detection methods using machine learning (ML) models having high detection rates, that have been proposed for the past few years. As the result of our analysis we found that the detection techniques can be divided into three sections on the basis of analysis carried out for feature study viz. Static, Dynamic and Hybrid. Classification algorithms such as Support Vector Machine (SVM), K-Nearest Neighbor (KNN), Random Forest (RF), Decision Trees (DT), Naïve Bayes (NB), Neural Networks (NN), and many others were used for classification purpose. Some of the ML models have also combined big data, data mining and deep learning algorithms to achieve high accuracy rates. We concluded that ML methods have proven to give better results than the old signature-based or heuristics-based detection techniques and have high possibilities of further development.","",""
85,"P. Graff, F. Feroz, M. Hobson, A. Lasenby","SKYNET: an efficient and robust neural network training tool for machine learning in astronomy",2013,"","","","",94,"2022-07-13 10:07:22","","10.1093/mnras/stu642","","",,,,,85,9.44,21,4,9,"We present the first public release of our generic neural network training algorithm, called SKYNET. This efficient and robust machine-learning tool is able to train large and deep feedforward neural networks, including autoencoders, for use in a wide range of supervised and unsupervised learning applications, such as regression, classification, density estimation, clustering and dimensionality reduction. SKYNET uses a powerful ‘pre-training’ method, to obtain a set of network parameters close to the true global maximum of the training objective function, followed by further optimisation using an automatically-regularised variant of Newton’s method; the latter uses second-order derivative information to improve convergence, but without the need to evaluate or store the full Hessian matrix, by using a fast approximate method to calculate Hessian-vector products. This combination of methods allows for the training of complicated networks that are difficult to optimise using standard backpropagation techniques. SKYNET employs convergence criteria that naturally prevent overfitting, and also includes a fast algorithm for estimating the accuracy of network outputs. The utility and flexibility of SKYNET are demonstrated by application to a number of toy problems, and to astronomical problems focusing on the recovery of structure from blurred and noisy images, the identification of gamma-ray bursters, and the compression and denoising of galaxy images. The SKYNET software, which is implemented in standard ANSI C and fully parallelised using MPI, is available at http://www.mrao.cam.ac.uk/software/skynet/.","",""
25,"M. Fornasier, Hui Huang, L. Pareschi, Philippe Sünnen","Consensus-based Optimization on the Sphere II: Convergence to Global Minimizers and Machine Learning",2020,"","","","",95,"2022-07-13 10:07:22","","","","",,,,,25,12.50,6,4,2,"We present the implementation of a new stochastic Kuramoto-Vicsek-type model for global optimization of nonconvex functions on the sphere. This model belongs to the class of Consensus-Based Optimization. In fact, particles move on the sphere driven by a drift towards an instantaneous consensus point, which is computed as a convex combination of particle locations, weighted by the cost function according to Laplace's principle, and it represents an approximation to a global minimizer. The dynamics is further perturbed by a random vector field to favor exploration, whose variance is a function of the distance of the particles to the consensus point. In particular, as soon as the consensus is reached the stochastic component vanishes. The main results of this paper are about the proof of convergence of the numerical scheme to global minimizers provided conditions of well-preparation of the initial datum. The proof combines previous results of mean-field limit with a novel asymptotic analysis, and classical convergence results of numerical methods for SDE. We present several numerical experiments, which show that the algorithm proposed in the present paper scales well with the dimension and is extremely versatile. To quantify the performances of the new approach, we show that the algorithm is able to perform essentially as good as ad hoc state of the art methods in challenging problems in signal processing and machine learning, namely the phase retrieval problem and the robust subspace detection.","",""
658,"Wieland Brendel, Jonas Rauber, M. Bethge","Decision-Based Adversarial Attacks: Reliable Attacks Against Black-Box Machine Learning Models",2017,"","","","",96,"2022-07-13 10:07:22","","","","",,,,,658,131.60,219,3,5,"Many machine learning algorithms are vulnerable to almost imperceptible perturbations of their inputs. So far it was unclear how much risk adversarial perturbations carry for the safety of real-world machine learning applications because most methods used to generate such perturbations rely either on detailed model information (gradient-based attacks) or on confidence scores such as class probabilities (score-based attacks), neither of which are available in most real-world scenarios. In many such cases one currently needs to retreat to transfer-based attacks which rely on cumbersome substitute models, need access to the training data and can be defended against. Here we emphasise the importance of attacks which solely rely on the final model decision. Such decision-based attacks are (1) applicable to real-world black-box models such as autonomous cars, (2) need less knowledge and are easier to apply than transfer-based attacks and (3) are more robust to simple defences than gradient- or score-based attacks. Previous attacks in this category were limited to simple models or simple datasets. Here we introduce the Boundary Attack, a decision-based attack that starts from a large adversarial perturbation and then seeks to reduce the perturbation while staying adversarial. The attack is conceptually simple, requires close to no hyperparameter tuning, does not rely on substitute models and is competitive with the best gradient-based attacks in standard computer vision tasks like ImageNet. We apply the attack on two black-box algorithms from Clarifai.com. The Boundary Attack in particular and the class of decision-based attacks in general open new avenues to study the robustness of machine learning models and raise new questions regarding the safety of deployed machine learning systems. An implementation of the attack is available as part of Foolbox at this https URL .","",""
91,"M. Dyrba, M. Ewers, Martin Wegrzyn, I. Kilimann, C. Plant, Annahita Oswald, T. Meindl, M. Pievani, A. Bokde, A. Fellgiebel, M. Filippi, H. Hampel, S. Klöppel, Karlheinz Hauenstein, T. Kirste, S. Teipel","Robust Automated Detection of Microstructural White Matter Degeneration in Alzheimer’s Disease Using Machine Learning Classification of Multicenter DTI Data",2013,"","","","",97,"2022-07-13 10:07:22","","10.1371/journal.pone.0064925","","",,,,,91,10.11,9,16,9,"Diffusion tensor imaging (DTI) based assessment of white matter fiber tract integrity can support the diagnosis of Alzheimer’s disease (AD). The use of DTI as a biomarker, however, depends on its applicability in a multicenter setting accounting for effects of different MRI scanners. We applied multivariate machine learning (ML) to a large multicenter sample from the recently created framework of the European DTI study on Dementia (EDSD). We hypothesized that ML approaches may amend effects of multicenter acquisition. We included a sample of 137 patients with clinically probable AD (MMSE 20.6±5.3) and 143 healthy elderly controls, scanned in nine different scanners. For diagnostic classification we used the DTI indices fractional anisotropy (FA) and mean diffusivity (MD) and, for comparison, gray matter and white matter density maps from anatomical MRI. Data were classified using a Support Vector Machine (SVM) and a Naïve Bayes (NB) classifier. We used two cross-validation approaches, (i) test and training samples randomly drawn from the entire data set (pooled cross-validation) and (ii) data from each scanner as test set, and the data from the remaining scanners as training set (scanner-specific cross-validation). In the pooled cross-validation, SVM achieved an accuracy of 80% for FA and 83% for MD. Accuracies for NB were significantly lower, ranging between 68% and 75%. Removing variance components arising from scanners using principal component analysis did not significantly change the classification results for both classifiers. For the scanner-specific cross-validation, the classification accuracy was reduced for both SVM and NB. After mean correction, classification accuracy reached a level comparable to the results obtained from the pooled cross-validation. Our findings support the notion that machine learning classification allows robust classification of DTI data sets arising from multiple scanners, even if a new data set comes from a scanner that was not part of the training sample.","",""
165,"S. Ardabili, A. Mosavi, Pedram Ghamisi, F. Ferdinand, A. Várkonyi-Kóczy, U. Reuter, T. Rabczuk, P. Atkinson","COVID-19 Outbreak Prediction with Machine Learning",2020,"","","","",98,"2022-07-13 10:07:22","","10.1101/2020.04.17.20070094","","",,,,,165,82.50,21,8,2,"Several outbreak prediction models for COVID-19 are being used by officials around the world to make informed-decisions and enforce relevant control measures. Among the standard models for COVID-19 global pandemic prediction, simple epidemiological and statistical models have received more attention by authorities, and they are popular in the media. Due to a high level of uncertainty and lack of essential data, standard models have shown low accuracy for long-term prediction. Although the literature includes several attempts to address this issue, the essential generalization and robustness abilities of existing models needs to be improved. This paper presents a comparative analysis of machine learning and soft computing models to predict the COVID-19 outbreak. Among a wide range of machine learning models investigated, two models showed promising results (i.e., multi-layered perceptron, MLP, and adaptive network-based fuzzy inference system, ANFIS). Based on the results reported here, and due to the highly complex nature of the COVID-19 outbreak and variation in its behavior from nation-to-nation, this study suggests machine learning as an effective tool to model the outbreak.","",""
29,"Violeta Mirchevska, M. Luštrek, M. Gams","Combining domain knowledge and machine learning for robust fall detection",2014,"","","","",99,"2022-07-13 10:07:22","","10.1111/exsy.12019","","",,,,,29,3.63,10,3,8,"This paper presents a method for combining domain knowledge and machine learning (CDKML) for classifier generation and online adaptation. The method exploits advantages in domain knowledge and machine learning as complementary information sources. Whereas machine learning may discover patterns in interest domains that are too subtle for humans to detect, domain knowledge may contain information on a domain not present in the available domain dataset. CDKML has three steps. First, prior domain knowledge is enriched with relevant patterns obtained by machine learning to create an initial classifier. Second, genetic algorithms refine the classifier. Third, the classifier is adapted online on the basis of user feedback using the Markov decision process. CDKML was applied in fall detection. Tests showed that the classifiers developed by CDKML have better performance than machine‐learning classifiers generated on a training dataset that does not adequately represent all real‐life cases of the learned concept. The accuracy of the initial classifier was 10 percentage points higher than the best machine‐learning classifier and the refinement added 3 percentage points. The online adaptation improved the accuracy of the refined classifier by an additional 15 percentage points.","",""
2,"S. Siltanen, Takanori Ide","Electrical Impedance Tomography, Enclosure Method and Machine Learning",2020,"","","","",100,"2022-07-13 10:07:22","","10.1109/MLSP49062.2020.9231717","","",,,,,2,1.00,1,2,2,"Electrical impedance tomography (EIT) is a non-destructive imaging method, where a physical body is probed with electric measurements at the boundary, and information about the internal conductivity is extracted from the data. The enclosure method of Ikehata [J. Inv. III-Posed Prob. 8(2000)] recovers the convex hull of an inclusion of unknown conductivity embedded in known background conductivity. Practical implementations of the enclosure method are based on least-squares (LS) fitting of lines to noise-robust values of the so-called indicator function. It is shown how a convolutional neural network instead of LS fitting improves the accuracy of the enclosure method significantly while retaining interpretability.","",""
166,"M. Alber, A. Buganza Tepole, W. R. Cannon, S. De, S. Dura-Bernal, K. Garikipati, G. Karniadakis, W. Lytton, P. Perdikaris, L. Petzold, E. Kuhl","Integrating machine learning and multiscale modeling—perspectives, challenges, and opportunities in the biological, biomedical, and behavioral sciences",2019,"","","","",101,"2022-07-13 10:07:22","","10.1038/s41746-019-0193-y","","",,,,,166,55.33,17,11,3,"","",""
138,"A. Garcez, M. Gori, L. Lamb, L. Serafini, Michael Spranger, S. Tran","Neural-Symbolic Computing: An Effective Methodology for Principled Integration of Machine Learning and Reasoning",2019,"","","","",102,"2022-07-13 10:07:22","","","","",,,,,138,46.00,23,6,3,"Current advances in Artificial Intelligence and machine learning in general, and deep learning in particular have reached unprecedented impact not only across research communities, but also over popular media channels. However, concerns about interpretability and accountability of AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have identified the need for principled knowledge representation and reasoning mechanisms integrated with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities: the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic computing has been an active topic of research for many years, reconciling the advantages of robust learning in neural networks and reasoning and interpretability of symbolic representation. In this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology for integrated machine learning and reasoning. We illustrate the effectiveness of the approach by outlining the main characteristics of the methodology: principled integration of neural learning with symbolic knowledge representation and reasoning allowing for the construction of explainable AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly prominent need for interpretable and accountable AI systems.","",""
1203,"Keith Bonawitz, Vladimir Ivanov, Ben Kreuter, Antonio Marcedone, H. B. McMahan, Sarvar Patel, D. Ramage, Aaron Segal, Karn Seth","Practical Secure Aggregation for Privacy-Preserving Machine Learning",2017,"","","","",103,"2022-07-13 10:07:22","","10.1145/3133956.3133982","","",,,,,1203,240.60,134,9,5,"We design a novel, communication-efficient, failure-robust protocol for secure aggregation of high-dimensional data. Our protocol allows a server to compute the sum of large, user-held data vectors from mobile devices in a secure manner (i.e. without learning each user's individual contribution), and can be used, for example, in a federated learning setting, to aggregate user-provided model updates for a deep neural network. We prove the security of our protocol in the honest-but-curious and active adversary settings, and show that security is maintained even if an arbitrarily chosen subset of users drop out at any time. We evaluate the efficiency of our protocol and show, by complexity analysis and a concrete implementation, that its runtime and communication overhead remain low even on large data sets and client pools. For 16-bit input values, our protocol offers $1.73 x communication expansion for 210 users and 220-dimensional vectors, and 1.98 x expansion for 214 users and 224-dimensional vectors over sending data in the clear.","",""
0,"D. Efremenko, Himani Jain, Jian Xu","Two Machine Learning Based Schemes for Solving Direct and Inverse Problems of Radiative Transfer Theory",2020,"","","","",104,"2022-07-13 10:07:22","","10.51130/graphicon-2020-2-3-45","","",,,,,0,0.00,0,3,2,"Artificial neural networks (ANNs) are used to substitute computationally expensive radiative transfer models (RTMs) and inverse operators (IO) for retrieving optical parameters of the medium. However, the direct parametrization of RTMs and IOs by means of ANNs has certain drawbacks, such as loss of generality, computations of huge training datasets, robustness issues etc. This paper provides an analysis of different ANN-related methods, based on our results and those published by other authors. In particular, two techniques are proposed. In the first method, the ANN substitutes the eigenvalue solver in the discrete ordinate RTM, thereby reducing the computational time. Unlike classical RTM parametrization schemes based on ANN, in this method the resulting ANN can be used for arbitrary geometry and layer optical thicknesses. In the second method, the IO is trained by using the real measurements (preprocessed Level-2 TROPOMI data) to improve the stability of the inverse operator. This method provides robust results even without applying the Tikhonov regularization method.","",""
26,"Theja Tulabandhula, C. Rudin","Robust Optimization using Machine Learning for Uncertainty Sets",2014,"","","","",105,"2022-07-13 10:07:22","","","","",,,,,26,3.25,13,2,8,"Our goal is to build robust optimization problems for making decisions based on complex data from the past. In robust optimization (RO) generally, the goal is to create a policy for decision-making that is robust to our uncertainty about the future. In particular, we want our policy to best handle the the worst possible situation that could arise, out of an uncertainty set of possible situations. Classically, the uncertainty set is simply chosen by the user, or it might be estimated in overly simplistic ways with strong assumptions; whereas in this work, we learn the uncertainty set from data collected in the past. The past data are drawn randomly from an (unknown) possibly complicated high-dimensional distribution. We propose a new uncertainty set design and show how tools from statistical learning theory can be employed to provide probabilistic guarantees on the robustness of the policy.","",""
101,"Andreas K Triantafyllidis, A. Tsanas","Applications of Machine Learning in Real-Life Digital Health Interventions: Review of the Literature",2019,"","","","",106,"2022-07-13 10:07:22","","10.2196/12286","","",,,,,101,33.67,51,2,3,"Background Machine learning has attracted considerable research interest toward developing smart digital health interventions. These interventions have the potential to revolutionize health care and lead to substantial outcomes for patients and medical professionals. Objective Our objective was to review the literature on applications of machine learning in real-life digital health interventions, aiming to improve the understanding of researchers, clinicians, engineers, and policy makers in developing robust and impactful data-driven interventions in the health care domain. Methods We searched the PubMed and Scopus bibliographic databases with terms related to machine learning, to identify real-life studies of digital health interventions incorporating machine learning algorithms. We grouped those interventions according to their target (ie, target condition), study design, number of enrolled participants, follow-up duration, primary outcome and whether this had been statistically significant, machine learning algorithms used in the intervention, and outcome of the algorithms (eg, prediction). Results Our literature search identified 8 interventions incorporating machine learning in a real-life research setting, of which 3 (37%) were evaluated in a randomized controlled trial and 5 (63%) in a pilot or experimental single-group study. The interventions targeted depression prediction and management, speech recognition for people with speech disabilities, self-efficacy for weight loss, detection of changes in biopsychosocial condition of patients with multiple morbidity, stress management, treatment of phantom limb pain, smoking cessation, and personalized nutrition based on glycemic response. The average number of enrolled participants in the studies was 71 (range 8-214), and the average follow-up study duration was 69 days (range 3-180). Of the 8 interventions, 6 (75%) showed statistical significance (at the P=.05 level) in health outcomes. Conclusions This review found that digital health interventions incorporating machine learning algorithms in real-life studies can be useful and effective. Given the low number of studies identified in this review and that they did not follow a rigorous machine learning evaluation methodology, we urge the research community to conduct further studies in intervention settings following evaluation principles and demonstrating the potential of machine learning in clinical practice.","",""
10,"Saeed Mian Qaisar","Signal-piloted processing and machine learning based efficient power quality disturbances recognition",2021,"","","","",107,"2022-07-13 10:07:22","","10.1371/journal.pone.0252104","","",,,,,10,10.00,10,1,1,"Significant losses can occur for various smart grid stake holders due to the Power Quality Disturbances (PQDs). Therefore, it is necessary to correctly recognize and timely mitigate the PQDs. In this context, an emerging trend is the development of machine learning assisted PQDs management. Based on the conventional processing theory, the existing PQDs identification is time-invariant. It can result in a huge amount of unnecessary information being collected, processed, and transmitted. Consequently, needless processing activities, power consumption and latency can occur. In this paper, a novel combination of signal-piloted acquisition, adaptive-rate segmentation and time-domain features extraction with machine learning tools is suggested. The signal-piloted acquisition and processing brings real-time compression. Therefore, a remarkable reduction can be secured in the data storage, processing and transmission requirement towards the post classifier. Additionally, a reduced computational cost and latency of classifier is promised. The classification is accomplished by using robust machine learning algorithms. A comparison is made among the k-Nearest Neighbor, Naïve Bayes, Artificial Neural Network and Support Vector Machine. Multiple metrics are used to test the success of classification. It permits to avoid any biasness of findings. The applicability of the suggested approach is studied for automated recognition of the power signal’s major voltage and transient disturbances. Results show that the system attains a 6.75-fold reduction in the collected information and the processing load and secures the 98.05% accuracy of classification.","",""
87,"J. Collins, K. Howe, B. Nachman","Extending the search for new resonances with machine learning",2019,"","","","",108,"2022-07-13 10:07:22","","10.1103/physrevd.99.014038","","",,,,,87,29.00,29,3,3,"The oldest and most robust technique to search for new particles is to look for ``bumps'' in invariant mass spectra over smoothly falling backgrounds. We present a new extension of the bump hunt that naturally benefits from modern machine learning algorithms while remaining model agnostic. This approach is based on the classification without labels (CWoLa) method where the invariant mass is used to create two potentially mixed samples, one with little or no signal and one with a potential resonance. Additional features that are uncorrelated with the invariant mass can be used for training the classifier. Given the lack of new physics signals at the Large Hadron Collider (LHC), such model-agnostic approaches are critical for ensuring full coverage to fully exploit the rich datasets from the LHC experiments. In addition to illustrating how the new method works in simple test cases, we demonstrate the power of the extended bump hunt on a realistic all-hadronic resonance search in a channel that would not be covered with existing techniques.","",""
87,"P. C. Jennings, S. Lysgaard, J. Hummelshøj, T. Vegge, T. Bligaard","Genetic algorithms for computational materials discovery accelerated by machine learning",2019,"","","","",109,"2022-07-13 10:07:22","","10.1038/s41524-019-0181-4","","",,,,,87,29.00,17,5,3,"","",""
57,"A. Chelli, M. Pätzold","A Machine Learning Approach for Fall Detection and Daily Living Activity Recognition",2019,"","","","",110,"2022-07-13 10:07:22","","10.1109/ACCESS.2019.2906693","","",,,,,57,19.00,29,2,3,"The number of older people in western countries is constantly increasing. Most of them prefer to live independently and are susceptible to fall incidents. Falls often lead to serious or even fatal injuries which are the leading cause of death for elderlies. To address this problem, it is essential to develop robust fall detection systems. In this context, we develop a machine learning framework for fall detection and daily living activity recognition. We use acceleration and angular velocity data from two public databases to recognize seven different activities, including falls and activities of daily living. From the acceleration and angular velocity data, we extract time- and frequency-domain features and provide them to a classification algorithm. In this paper, we test the performance of four algorithms for classifying human activities. These algorithms are the artificial neural network (ANN),  $K$ -nearest neighbors (KNN), quadratic support vector machine (QSVM), and ensemble bagged tree (EBT). New features that improve the performance of the classifier are extracted from the power spectral density of the acceleration. In the first step, only the acceleration data are used for activity recognition. Our results reveal that the KNN, ANN, QSVM, and EBT algorithms could achieve overall accuracy of 81.2%, 87.8%, 93.2%, and 94.1%, respectively. The accuracy of fall detection reaches 97.2% and 99.1% without any false alarms for the QSVM and EBT algorithms, respectively. In a second step, we extract features from the autocorrelation function and the power spectral density of both the acceleration and the angular velocity data, which improves the classification accuracy. By using the proposed features, we could achieve overall accuracy of 85.8%, 91.8%, 96.1%, and 97.7% for the KNN, ANN, QSVM, and EBT algorithms, respectively. The accuracy of fall detection reaches 100% for both the QSVM and EBT algorithms without any false alarm, which is the best achievable performance.","",""
59,"Yang Long, Jie Ren, Yunhui Li, Hong Chen","Inverse design of photonic topological state via machine learning",2019,"","","","",111,"2022-07-13 10:07:22","","10.1063/1.5094838","","",,,,,59,19.67,15,4,3,"The photonics topological state plays an important role in recent optical physics and has led to devices with robust properties. However, the design of optical structures with the target topological states is a challenge for current research. Here, we propose an approach to achieve this goal by exploiting machine learning technologies. In our work, we focus on Zak phases, which are the topological properties of one-dimensional photonics crystals. After learning the principle between the geometrical parameters and the Zak phases, the neural network can obtain the appropriate structures of photonics crystals by applying the objective Zak phase properties. Our work would give more insights into the application of machine learning on the inverse design of the complex material properties and could be extended to other fields, i.e., advanced phononics devices.","",""
76,"Yicheng Wang, Mohit Bansal","Robust Machine Comprehension Models via Adversarial Training",2018,"","","","",112,"2022-07-13 10:07:22","","10.18653/v1/N18-2091","","",,,,,76,19.00,38,2,4,"It is shown that many published models for the Stanford Question Answering Dataset (Rajpurkar et al., 2016) lack robustness, suffering an over 50% decrease in F1 score during adversarial evaluation based on the AddSent (Jia and Liang, 2017) algorithm. It has also been shown that retraining models on data generated by AddSent has limited effect on their robustness. We propose a novel alternative adversary-generation algorithm, AddSentDiverse, that significantly increases the variance within the adversarial training data by providing effective examples that punish the model for making certain superficial assumptions. Further, in order to improve robustness to AddSent’s semantic perturbations (e.g., antonyms), we jointly improve the model’s semantic-relationship learning capabilities in addition to our AddSentDiverse-based adversarial training data augmentation. With these additions, we show that we can make a state-of-the-art model significantly more robust, achieving a 36.5% increase in F1 score under many different types of adversarial evaluation while maintaining performance on the regular SQuAD task.","",""
40,"K. Colangelo, Ying-Ying Lee","Double debiased machine learning nonparametric inference with continuous treatments",2019,"","","","",113,"2022-07-13 10:07:22","","10.1920/wp.cem.2019.5419","","",,,,,40,13.33,20,2,3,"We propose a nonparametric inference method for causal effects of continuous treatment variables, under unconfoundedness and in the presence of high-dimensional or nonparametric nuisance parameters. Our double debiased machine learning (DML) estimators for the average dose-response function (or the average structural function) and the partial effects are asymptotically normal with nonparametric convergence rates. The nuisance estimators for the conditional expectation function and the conditional density can be nonparametric kernel or series estimators or ML methods. Using a kernel-based doubly robust influence function and cross-fitting, we give tractable primitive conditions under which the nuisance estimators do not affect the first-order large sample distribution of the DML estimators. We justify the use of kernel to localize the continuous treatment at a given value by the Gateaux derivative. We implement various ML methods in Monte Carlo simulations and an empirical application on a job training program evaluation.","",""
41,"Xiang Lu, M. Hasanipanah, Kathirvel Brindhadevi, H. Bakhshandeh Amnieh, Seyedamirhesam Khalafi","ORELM: A Novel Machine Learning Approach for Prediction of Flyrock in Mine Blasting",2019,"","","","",114,"2022-07-13 10:07:22","","10.1007/s11053-019-09532-2","","",,,,,41,13.67,8,5,3,"","",""
103,"Baibhab Chatterjee, D. Das, Shovan Maity, Shreyas Sen","RF-PUF: Enhancing IoT Security Through Authentication of Wireless Nodes Using In-Situ Machine Learning",2018,"","","","",115,"2022-07-13 10:07:22","","10.1109/JIOT.2018.2849324","","",,,,,103,25.75,26,4,4,"Traditional authentication in radio-frequency (RF) systems enable secure data communication within a network through techniques such as digital signatures and hash-based message authentication codes (HMAC), which suffer from key-recovery attacks. State-of-the-art Internet of Things networks such as Nest also use open authentication (OAuth 2.0) protocols that are vulnerable to cross-site-recovery forgery (CSRF), which shows that these techniques may not prevent an adversary from copying or modeling the secret IDs or encryption keys using invasive, side channel, learning or software attacks. Physical unclonable functions (PUFs), on the other hand, can exploit manufacturing process variations to uniquely identify silicon chips which makes a PUF-based system extremely robust and secure at low cost, as it is practically impossible to replicate the same silicon characteristics across dies. Taking inspiration from human communication, which utilizes inherent variations in the voice signatures to identify a certain speaker, we present RF-PUF: a deep neural network-based framework that allows real-time authentication of wireless nodes, using the effects of inherent process variation on RF properties of the wireless transmitters (Tx), detected through in-situ machine learning at the receiver (Rx) end. The proposed method utilizes the already-existing asymmetric RF communication framework and does not require any additional circuitry for PUF generation or feature extraction. The burden of device identification is completely shifted to the gateway Rx, similar to the operation of a human listener’s brain. Simulation results involving the process variations in a standard 65-nm technology node, and features such as local oscillator offset and  ${I}$ – ${Q}$  imbalance detected with a neural network having 50 neurons in the hidden layer indicate that the framework can distinguish up to 4800 Tx(s) with an accuracy of 99.9% [≈99% for 10000 Tx(s)] under varying channel conditions, and without the need for traditional preambles. The proposed scheme can be used as a stand-alone security feature, or as a part of traditional multifactor authentication.","",""
37,"Mariam Nassar, M. Doan, A. Filby, O. Wolkenhauer, D. Fogg, J. Piasecka, C. Thornton, Anne E Carpenter, H. Summers, P. Rees, H. Hennig","Label‐Free Identification of White Blood Cells Using Machine Learning",2019,"","","","",116,"2022-07-13 10:07:22","","10.1002/cyto.a.23794","","",,,,,37,12.33,4,11,3,"White blood cell (WBC) differential counting is an established clinical routine to assess patient immune system status. Fluorescent markers and a flow cytometer are required for the current state‐of‐the‐art method for determining WBC differential counts. However, this process requires several sample preparation steps and may adversely disturb the cells. We present a novel label‐free approach using an imaging flow cytometer and machine learning algorithms, where live, unstained WBCs were classified. It achieved an average F1‐score of 97% and two subtypes of WBCs, B and T lymphocytes, were distinguished from each other with an average F1‐score of 78%, a task previously considered impossible for unlabeled samples. We provide an open‐source workflow to carry out the procedure. We validated the WBC analysis with unstained samples from 85 donors. The presented method enables robust and highly accurate identification of WBCs, minimizing the disturbance to the cells and leaving marker channels free to answer other biological questions. It also opens the door to employing machine learning for liquid biopsy, here, using the rich information in cell morphology for a wide range of diagnostics of primary blood. © 2019 The Authors. Cytometry Part A published by Wiley Periodicals, Inc. on behalf of International Society for Advancement of Cytometry.","",""
117,"Xiao Chen, Chaoran Li, Derui Wang, S. Wen, Jun Zhang, S. Nepal, Yang Xiang, K. Ren","Android HIV: A Study of Repackaging Malware for Evading Machine-Learning Detection",2018,"","","","",117,"2022-07-13 10:07:22","","10.1109/TIFS.2019.2932228","","",,,,,117,29.25,15,8,4,"Machine learning-based solutions have been successfully employed for the automatic detection of malware on Android. However, machine learning models lack robustness to adversarial examples, which are crafted by adding carefully chosen perturbations to the normal inputs. So far, the adversarial examples can only deceive detectors that rely on syntactic features (e.g., requested permissions, API calls, etc.), and the perturbations can only be implemented by simply modifying application’s manifest. While recent Android malware detectors rely more on semantic features from Dalvik bytecode rather than manifest, existing attacking/defending methods are no longer effective. In this paper, we introduce a new attacking method that generates adversarial examples of Android malware and evades being detected by the current models. To this end, we propose a method of applying optimal perturbations onto Android APK that can successfully deceive the machine learning detectors. We develop an automated tool to generate the adversarial examples without human intervention. In contrast to existing works, the adversarial examples crafted by our method can also deceive recent machine learning-based detectors that rely on semantic features such as control-flow-graph. The perturbations can also be implemented directly onto APK’s Dalvik bytecode rather than Android manifest to evade from recent detectors. We demonstrate our attack on two state-of-the-art Android malware detection schemes, MaMaDroid and Drebin. Our results show that the malware detection rates decreased from 96% to 0% in MaMaDroid, and from 97% to 0% in Drebin, with just a small number of codes to be inserted into the APK.","",""
52,"Yudong Chen, Lili Su, Jiaming Xu","Distributed Statistical Machine Learning in Adversarial Settings: Byzantine Gradient Descent",2017,"","","","",118,"2022-07-13 10:07:22","","10.1145/3219617.3219655","","",,,,,52,10.40,17,3,5,"We consider the distributed statistical learning problem over decentralized systems that are prone to adversarial attacks. This setup arises in many practical applications, including Google's Federated Learning. Formally, we focus on a decentralized system that consists of a parameter server and m working machines; each working machine keeps N/m data samples, where N is the total number of samples. In each iteration, up to q of the m working machines suffer Byzantine faults -- a faulty machine in the given iteration behaves arbitrarily badly against the system and has complete knowledge of the system. Additionally, the sets of faulty machines may be different across iterations. Our goal is to design robust algorithms such that the system can learn the underlying true parameter, which is of dimension d, despite the interruption of the Byzantine attacks. In this paper, based on the geometric median of means of the gradients, we propose a simple variant of the classical gradient descent method. We show that our method can tolerate q Byzantine failures up to 2(1+ε)q łe m for an arbitrarily small but fixed constant ε>0. The parameter estimate converges in O(łog N) rounds with an estimation error on the order of max √dq/N, ~√d/N , which is larger than the minimax-optimal error rate √d/N in the centralized and failure-free setting by at most a factor of √q . The total computational complexity of our algorithm is of O((Nd/m) log N) at each working machine and O(md + kd log 3 N) at the central server, and the total communication cost is of O(m d log N). We further provide an application of our general results to the linear regression problem. A key challenge arises in the above problem is that Byzantine failures create arbitrary and unspecified dependency among the iterations and the aggregated gradients. To handle this issue in the analysis, we prove that the aggregated gradient, as a function of model parameter, converges uniformly to the true gradient function.","",""
102,"Peng Xu, Farbod Roosta-Khorasani, Michael W. Mahoney","Second-Order Optimization for Non-Convex Machine Learning: An Empirical Study",2017,"","","","",119,"2022-07-13 10:07:22","","10.1137/1.9781611976236.23","","",,,,,102,20.40,34,3,5,"While first-order optimization methods such as stochastic gradient descent (SGD) are popular in machine learning (ML), they come with well-known deficiencies, including relatively-slow convergence, sensitivity to the settings of hyper-parameters such as learning rate, stagnation at high training errors, and difficulty in escaping flat regions and saddle points. These issues are particularly acute in highly non-convex settings such as those arising in neural networks. Motivated by this, there has been recent interest in second-order methods that aim to alleviate these shortcomings by capturing curvature information. In this paper, we report detailed empirical evaluations of a class of Newton-type methods, namely sub-sampled variants of trust region (TR) and adaptive regularization with cubics (ARC) algorithms, for non-convex ML problems. In doing so, we demonstrate that these methods not only can be computationally competitive with hand-tuned SGD with momentum, obtaining comparable or better generalization performance, but also they are highly robust to hyper-parameter settings. Further, in contrast to SGD with momentum, we show that the manner in which these Newton-type methods employ curvature information allows them to seamlessly escape flat regions and saddle points.","",""
69,"Martins Ezuma, F. Erden, C. K. Anjinappa, O. Ozdemir, I. Guvenc","Micro-UAV Detection and Classification from RF Fingerprints Using Machine Learning Techniques",2019,"","","","",120,"2022-07-13 10:07:22","","10.1109/AERO.2019.8741970","","",,,,,69,23.00,14,5,3,"This paper focuses on the detection and classification of micro-unmanned aerial vehicles (UAVs)using radio frequency (RF)fingerprints of the signals transmitted from the controller to the micro-UAV. In the detection phase, raw signals are split into frames and transformed into the wavelet domain to remove the bias in the signals and reduce the size of data to be processed. A naive Bayes approach, which is based on Markov models generated separately for UAV and non-UAV classes, is used to check for the presence of a UAV in each frame. In the classification phase, unlike the traditional approaches that rely solely on time-domain signals and corresponding features, the proposed technique uses the energy transient signal. This approach is more robust to noise and can cope with different modulation techniques. First, the normalized energy trajectory is generated from the energy-time-frequency distribution of the raw control signal. Next, the start and end points of the energy transient are detected by searching for the most abrupt changes in the mean of the energy trajectory. Then, a set of statistical features is extracted from the energy transient. Significant features are selected by performing neighborhood component analysis (NCA)to keep the computational cost of the algorithm low. Finally, selected features are fed to several machine learning algorithms for classification. The algorithms are evaluated experimentally using a database containing 100 RF signals from each of 14 different UAV controllers. The signals are recorded wirelessly using a high-frequency oscilloscope. The data set is randomly partitioned into training and test sets for validation with the ratio 4:1. Ten Monte Carlo simulations are run and results are averaged to assess the performance of the methods. All the micro-UAVs are detected correctly and an average accuracy of 96.3% is achieved using the k-nearest neighbor (kNN)classification. Proposed methods are also tested for different signal-to-noise ratio (SNR)levels and results are reported.","",""
27,"Vikash Sehwag, A. Bhagoji, Liwei Song, Chawin Sitawarin, Daniel Cullina, M. Chiang, Prateek Mittal","Analyzing the Robustness of Open-World Machine Learning",2019,"","","","",121,"2022-07-13 10:07:22","","10.1145/3338501.3357372","","",,,,,27,9.00,4,7,3,"When deploying machine learning models in real-world applications, an open-world learning framework is needed to deal with both normal in-distribution inputs and undesired out-of-distribution (OOD) inputs. Open-world learning frameworks include OOD detectors that aim to discard input examples which are not from the same distribution as the training data of machine learning classifiers. However, our understanding of current OOD detectors is limited to the setting of benign OOD data, and an open question is whether they are robust in the presence of adversaries. In this paper, we present the first analysis of the robustness of open-world learning frameworks in the presence of adversaries by introducing and designing øodAdvExamples. Our experimental results show that current OOD detectors can be easily evaded by slightly perturbing benign OOD inputs, revealing a severe limitation of current open-world learning frameworks. Furthermore, we find that øodAdvExamples also pose a strong threat to adversarial training based defense methods in spite of their effectiveness against in-distribution adversarial attacks. To counteract these threats and ensure the trustworthy detection of OOD inputs, we outline a preliminary design for a robust open-world machine learning framework.","",""
65,"Jonas Rauber, Roland S. Zimmermann, M. Bethge, Wieland Brendel","Foolbox Native: Fast adversarial attacks to benchmark the robustness of machine learning models in PyTorch, TensorFlow, and JAX",2020,"","","","",122,"2022-07-13 10:07:22","","10.21105/JOSS.02607","","",,,,,65,32.50,16,4,2,"Machine learning has made enormous progress in recent years and is now being used in many real-world applications. Nevertheless, even state-of-the-art machine learning models can be fooled by small, maliciously crafted perturbations of their input data. Foolbox is a popular Python library to benchmark the robustness of machine learning models against these adversarial perturbations. It comes with a huge collection of state-of-the-art adversarial attacks to find adversarial perturbations and thanks to its framework-agnostic design it is ideally suited for comparing the robustness of many different models implemented in different frameworks. Foolbox 3 aka Foolbox Native has been rewritten from scratch to achieve native performance on models developed in PyTorch (Paszke et al., 2019), TensorFlow (Abadi et al., 2016), and JAX (Bradbury et al., 2018), all with one codebase without code duplication.","",""
21,"Mohammadreza Mirzahosseini, Pengcheng Jiao, Kaveh Barri, K. Riding, A. Alavi","New machine learning prediction models for compressive strength of concrete modified with glass cullet",2019,"","","","",123,"2022-07-13 10:07:22","","10.1108/EC-08-2018-0348","","",,,,,21,7.00,4,5,3,"PurposeRecycled waste glasses have been widely used in Portland cement and concrete as aggregate or supplementary cementitious material. Compressive strength is one of the most important properties of concrete containing waste glasses, providing information about the loading capacity, pozzolanic reaction and porosity of the mixture. This study aims to propose highly nonlinear models to predict the compressive strength of concrete containing finely ground glass particles.Design/methodology/approachA robust machine leaning method called genetic programming is used the build the compressive strength prediction models. The models are developed using a number of test results on 50-mm mortar cubes containing glass powder according to ASTM C109. Parametric and sensitivity analyses are conducted to evaluate the effect of the predictor variables on the compressive strength. Furthermore, a comparative study is performed to benchmark the proposed models against classical regression models.FindingsThe derived design equations accurately characterize the compressive strength of concrete with ground glass fillers and remarkably outperform the regression models. A key feature of the proposed models as compared to the previous studies is that they include the simultaneous effect of various parameters such as glass compositions, size distributions, curing age and isothermal temperatures. Parametric and sensitivity analyses indicate that compressive strength is very sensitive to the curing age, curing temperature and particle surface area.Originality/valueThis study presents accurate machine learning models for the prediction of one of the most important mechanical properties of cementitious mixtures modified by waste glass, i.e. compressive strength. In addition, it provides an insight into the effect of several parameters influencing the compressive strength. From a computing perspective, a robust machine learning technique that overcomes the shortcomings of existing soft computing methods is introduced.","",""
28,"A. Rácz, D. Bajusz, K. Héberger","Multi-Level Comparison of Machine Learning Classifiers and Their Performance Metrics",2019,"","","","",124,"2022-07-13 10:07:22","","10.3390/molecules24152811","","",,,,,28,9.33,9,3,3,"Machine learning classification algorithms are widely used for the prediction and classification of the different properties of molecules such as toxicity or biological activity. The prediction of toxic vs. non-toxic molecules is important due to testing on living animals, which has ethical and cost drawbacks as well. The quality of classification models can be determined with several performance parameters. which often give conflicting results. In this study, we performed a multi-level comparison with the use of different performance metrics and machine learning classification methods. Well-established and standardized protocols for the machine learning tasks were used in each case. The comparison was applied to three datasets (acute and aquatic toxicities) and the robust, yet sensitive, sum of ranking differences (SRD) and analysis of variance (ANOVA) were applied for evaluation. The effect of dataset composition (balanced vs. imbalanced) and 2-class vs. multiclass classification scenarios was also studied. Most of the performance metrics are sensitive to dataset composition, especially in 2-class classification problems. The optimal machine learning algorithm also depends significantly on the composition of the dataset.","",""
21,"P. Dasgupta, J. B. Collins","A Survey of Game Theoretic Approaches for Adversarial Machine Learning in Cybersecurity Tasks",2019,"","","","",125,"2022-07-13 10:07:22","","10.1609/aimag.v40i2.2847","","",,,,,21,7.00,11,2,3,"Machine learning techniques are used extensively for automating various cybersecurity tasks. Most of these techniques use supervised learning algorithms that rely on training the algorithm to classify incoming data into categories, using data encountered in the relevant domain. A critical vulnerability of these algorithms is that they are susceptible to adversarial attacks by which a malicious entity called an adversary deliberately alters the training data to misguide the learning algorithm into making classification errors. Adversarial attacks could render the learning algorithm unsuitable for use and leave critical systems vulnerable to cybersecurity attacks. This article provides a detailed survey of the stateof-the-art techniques that are used to make a machine learning algorithm robust against adversarial attacks by using the computational framework of game theory. We also discuss open problems and challenges and possible directions for further research that would make deep machine learning–based systems more robust and reliable for cybersecurity tasks.","",""
23,"P. Mondal, Xue Liu, T. Fatoyinbo, D. Lagomasino","Evaluating Combinations of Sentinel-2 Data and Machine-Learning Algorithms for Mangrove Mapping in West Africa",2019,"","","","",126,"2022-07-13 10:07:22","","10.3390/rs11242928","","",,,,,23,7.67,6,4,3,"Creating a national baseline for natural resources, such as mangrove forests, and monitoring them regularly often requires a consistent and robust methodology. With freely available satellite data archives and cloud computing resources, it is now more accessible to conduct such large-scale monitoring and assessment. Yet, few studies examine the reproducibility of such mangrove monitoring frameworks, especially in terms of generating consistent spatial extent. Our objective was to evaluate a combination of image processing approaches to classify mangrove forests along the coast of Senegal and The Gambia. We used freely available global satellite data (Sentinel-2), and cloud computing platform (Google Earth Engine) to run two machine learning algorithms, random forest (RF), and classification and regression trees (CART). We calibrated and validated the algorithms using 800 reference points collected using high-resolution images. We further re-ran 10 iterations for each algorithm, utilizing unique subsets of the initial training data. While all iterations resulted in thematic mangrove maps with over 90% accuracy, the mangrove extent ranges between 827–2807 km2 for Senegal and 245–1271 km2 for The Gambia with one outlier for each country. We further report “Places of Agreement” (PoA) to identify areas where all iterations for both methods agree (506.6 km2 and 129.6 km2 for Senegal and The Gambia, respectively), thus have a high confidence in predicting mangrove extent. While we acknowledge the timeand cost-effectiveness of such methods for the landscape managers, we recommend utilizing them with utmost caution, as well as post-classification on-the-ground checks, especially for decision making.","",""
33,"V. Chernozhukov, W. Newey, Rahul Singh","Automatic Debiased Machine Learning of Causal and Structural Effects",2018,"","","","",127,"2022-07-13 10:07:22","","10.3982/ecta18515","","",,,,,33,8.25,11,3,4,"Many causal and structural effects depend on regressions. Examples include policy effects, average derivatives, regression decompositions, average treatment effects, causal mediation, and parameters of economic structural models. The regressions may be high‐dimensional, making machine learning useful. Plugging machine learners into identifying equations can lead to poor inference due to bias from regularization and/or model selection. This paper gives automatic debiasing for linear and nonlinear functions of regressions. The debiasing is automatic in using Lasso and the function of interest without the full form of the bias correction. The debiasing can be applied to any regression learner, including neural nets, random forests, Lasso, boosting, and other high‐dimensional methods. In addition to providing the bias correction, we give standard errors that are robust to misspecification, convergence rates for the bias correction, and primitive conditions for asymptotic inference for estimators of a variety of estimators of structural and causal effects. The automatic debiased machine learning is used to estimate the average treatment effect on the treated for the NSW job training data and to estimate demand elasticities from Nielsen scanner data while allowing preferences to be correlated with prices and income.","",""
48,"Clemens Stachl, F. Pargent, S. Hilbert, Gabriella M. Harari, Ramona Schoedel, Sumer S. Vaid, S. Gosling, M. Bühner","Personality Research and Assessment in the Era of Machine Learning",2019,"","","","",128,"2022-07-13 10:07:22","","10.1002/per.2257","","",,,,,48,16.00,6,8,3,"The increasing availability of high–dimensional, fine–grained data about human behaviour, gathered from mobile sensing studies and in the form of digital footprints, is poised to drastically alter the way personality psychologists perform research and undertake personality assessment. These new kinds and quantities of data raise important questions about how to analyse the data and interpret the results appropriately. Machine learning models are well suited to these kinds of data, allowing researchers to model highly complex relationships and to evaluate the generalizability and robustness of their results using resampling methods. The correct usage of machine learning models requires specialized methodological training that considers issues specific to this type of modelling. Here, we first provide a brief overview of past studies using machine learning in personality psychology. Second, we illustrate the main challenges that researchers face when building, interpreting, and validating machine learning models. Third, we discuss the evaluation of personality scales, derived using machine learning methods. Fourth, we highlight some key issues that arise from the use of latent variables in the modelling process. We conclude with an outlook on the future role of machine learning models in personality research and assessment.","",""
39,"Yinjun Wu, E. Dobriban, S. Davidson","DeltaGrad: Rapid retraining of machine learning models",2020,"","","","",129,"2022-07-13 10:07:22","","","","",,,,,39,19.50,13,3,2,"Machine learning models are not static and may need to be retrained on slightly changed datasets, for instance, with the addition or deletion of a set of data points. This has many applications, including privacy, robustness, bias reduction, and uncertainty quantifcation. However, it is expensive to retrain models from scratch. To address this problem, we propose the DeltaGrad algorithm for rapid retraining machine learning models based on information cached during the training phase. We provide both theoretical and empirical support for the effectiveness of DeltaGrad, and show that it compares favorably to the state of the art.","",""
44,"Sijia Liu, Pin-Yu Chen, B. Kailkhura, Gaoyuan Zhang, A. Hero III, P. Varshney","A Primer on Zeroth-Order Optimization in Signal Processing and Machine Learning: Principals, Recent Advances, and Applications",2020,"","","","",130,"2022-07-13 10:07:22","","10.1109/MSP.2020.3003837","","",,,,,44,22.00,7,6,2,"Zeroth-order (ZO) optimization is a subset of gradient-free optimization that emerges in many signal processing and machine learning (ML) applications. It is used for solving optimization problems similarly to gradient-based methods. However, it does not require the gradient, using only function evaluations. Specifically, ZO optimization iteratively performs three major steps: gradient estimation, descent direction computation, and the solution update. In this article, we provide a comprehensive review of ZO optimization, with an emphasis on showing the underlying intuition, optimization principles, and recent advances in convergence analysis. Moreover, we demonstrate promising applications of ZO optimization, such as evaluating robustness and generating explanations from black-box deep learning (DL) models and efficient online sensor management.","",""
34,"D. Simester, Artem Timoshenko, S. Zoumpoulis","Targeting Prospective Customers: Robustness of Machine-Learning Methods to Typical Data Challenges",2020,"","","","",131,"2022-07-13 10:07:22","","10.1287/mnsc.2019.3308","","",,,,,34,17.00,11,3,2,"We investigate how firms can use the results of field experiments to optimize the targeting of promotions when prospecting for new customers. We evaluate seven widely used machine-learning methods using a series of two large-scale field experiments. The first field experiment generates a common pool of training data for each of the seven methods. We then validate the seven optimized policies provided by each method together with uniform benchmark policies in a second field experiment. The findings not only compare the performance of the targeting methods, but also demonstrate how well the methods address common data challenges. Our results reveal that when the training data are ideal, model-driven methods perform better than distance-driven methods and classification methods. However, the performance advantage vanishes in the presence of challenges that affect the quality of the training data, including the extent to which the training data captures details of the implementation setting. The challenges we study are covariate shift, concept shift, information loss through aggregation, and imbalanced data. Intuitively, the model-driven methods make better use of the information available in the training data, but the performance of these methods is more sensitive to deterioration in the quality of this information. The classification methods we tested performed relatively poorly. We explain the poor performance of the classification methods in our setting and describe how the performance of these methods could be improved. This paper was accepted by Matthew Shum, marketing.","",""
0,"Zhang Jing, Ren Yong-gong","Robust Multi-feature Extreme Learning Machine",2017,"","","","",132,"2022-07-13 10:07:22","","10.1007/978-3-030-01520-6_13","","",,,,,0,0.00,0,2,5,"","",""
27,"Cong Xie, O. Koyejo, Indranil Gupta","SLSGD: Secure and Efficient Distributed On-device Machine Learning",2019,"","","","",133,"2022-07-13 10:07:22","","10.1007/978-3-030-46147-8_13","","",,,,,27,9.00,9,3,3,"","",""
104,"Yangkang Zhang","Automatic microseismic event picking via unsupervised machine learning",2020,"","","","",134,"2022-07-13 10:07:22","","10.1093/GJI/GGX420","","",,,,,104,52.00,104,1,2,"  Effective and efficient arrival picking plays an important role in microseismic and earthquake data processing and imaging. Widely used short-term-average long-term-average ratio (STA/LTA) based arrival picking algorithms suffer from the sensitivity to moderate-to-strong random ambient noise. To make the state-of-the-art arrival picking approaches effective, microseismic data need to be first pre-processed, for example, removing sufficient amount of noise, and second analysed by arrival pickers. To conquer the noise issue in arrival picking for weak microseismic or earthquake event, I leverage the machine learning techniques to help recognizing seismic waveforms in microseismic or earthquake data. Because of the dependency of supervised machine learning algorithm on large volume of well-designed training data, I utilize an unsupervised machine learning algorithm to help cluster the time samples into two groups, that is, waveform points and non-waveform points. The fuzzy clustering algorithm has been demonstrated to be effective for such purpose. A group of synthetic, real microseismic and earthquake data sets with different levels of complexity show that the proposed method is much more robust than the state-of-the-art STA/LTA method in picking microseismic events, even in the case of moderately strong background noise.","",""
74,"Kai Fukami, K. Fukagata, K. Taira","Machine-learning-based spatio-temporal super resolution reconstruction of turbulent flows",2020,"","","","",135,"2022-07-13 10:07:22","","10.1017/jfm.2020.948","","",,,,,74,37.00,25,3,2,"Abstract We present a new data reconstruction method with supervised machine learning techniques inspired by super resolution and inbetweening to recover high-resolution turbulent flows from grossly coarse flow data in space and time. For the present machine-learning-based data reconstruction, we use the downsampled skip-connection/multiscale model based on a convolutional neural network, incorporating the multiscale nature of fluid flows into its network structure. As an initial example, the model is applied to the two-dimensional cylinder wake at $Re_D = 100$. The reconstructed flow fields by the present method show great agreement with the reference data obtained by direct numerical simulation. Next, we apply the current model to a two-dimensional decaying homogeneous isotropic turbulence. The machine-learned model is able to track the decaying evolution from spatial and temporal coarse input data. The proposed concept is further applied to a complex turbulent channel flow over a three-dimensional domain at $Re_{\tau }=180$. The present model reconstructs high-resolved turbulent flows from very coarse input data in space, and also reproduces the temporal evolution for appropriately chosen time interval. The dependence on the number of training snapshots and duration between the first and last frames based on a temporal two-point correlation coefficient are also assessed to reveal the capability and robustness of spatio-temporal super resolution reconstruction. These results suggest that the present method can perform a range of flow reconstructions in support of computational and experimental efforts.","",""
74,"M. Bogojeski, Leslie Vogt-Maranto, M. Tuckerman, K. Müller, K. Burke","Quantum chemical accuracy from density functional approximations via machine learning",2019,"","","","",136,"2022-07-13 10:07:22","","10.1038/s41467-020-19093-1","","",,,,,74,24.67,15,5,3,"","",""
78,"Xianfang Wang, Peng Gao, Yifeng Liu, Hongfei Li, Fan Lu","Predicting Thermophilic Proteins by Machine Learning",2020,"","","","",137,"2022-07-13 10:07:22","","10.2174/1574893615666200207094357","","",,,,,78,39.00,16,5,2,"  Thermophilic proteins can maintain good activity under high temperature, therefore, it is important to study thermophilic proteins for the thermal stability of proteins.    In order to solve the problem of low precision and low efficiency in predicting thermophilic proteins, a prediction method based on feature fusion and machine learning was proposed in this paper.    For the selected thermophilic data sets, firstly, the thermophilic protein sequence was characterized based on feature fusion by the combination of g-gap dipeptide, entropy density and autocorrelation coefficient. Then, Kernel Principal Component Analysis (KPCA) was used to reduce the dimension of the expressed protein sequence features in order to reduce the training time and improve efficiency. Finally, the classification model was designed by using the classification algorithm.    A variety of classification algorithms was used to train and test on the selected thermophilic dataset. By comparison, the accuracy of the Support Vector Machine (SVM) under the jackknife method was over 92%. The combination of other evaluation indicators also proved that the SVM performance was the best.     Because of choosing an effectively feature representation method and a robust classifier, the proposed method is suitable for predicting thermophilic proteins and is superior to most reported methods. ","",""
74,"Monika A. Myszczynska, P. Ojamies, Alix M. B. Lacoste, Daniel Neil, Amir Saffari, R. Mead, G. Hautbergue, J. Holbrook, L. Ferraiuolo","Applications of machine learning to diagnosis and treatment of neurodegenerative diseases",2020,"","","","",138,"2022-07-13 10:07:22","","10.1038/s41582-020-0377-8","","",,,,,74,37.00,8,9,2,"","",""
65,"Kai Fukami, K. Fukagata, K. Taira","Assessment of supervised machine learning methods for fluid flows",2020,"","","","",139,"2022-07-13 10:07:22","","10.1007/s00162-020-00518-y","","",,,,,65,32.50,22,3,2,"","",""
15,"Helmut Farbmacher, M. Huber, Henrika Langen, M. Spindler","Causal mediation analysis with double machine learning",2020,"","","","",140,"2022-07-13 10:07:22","","10.1093/ectj/utac003","","",,,,,15,7.50,4,4,2,"  This paper combines causal mediation analysis with double machine learning for a data-driven control of observed confounders in a high-dimensional setting. The average indirect effect of a binary treatment and the unmediated direct effect are estimated based on efficient score functions, which are robust w.r.t. misspecifications of the outcome, mediator, and treatment models. This property is key for selecting these models by double machine learning, which is combined with data splitting to prevent overfitting. We demonstrate that the effect estimators are asymptotically normal and n−1/2-consistent under specific regularity conditions and investigate the finite sample properties of the suggested methods in a simulation study when considering lasso as machine learner. We also provide an empirical application to the U.S. National Longitudinal Survey of Youth, assessing the indirect effect of health insurance coverage on general health operating via routine checkups as mediator, as well as the direct effect.","",""
60,"Isaac Shiri, H. Maleki, G. Hajianfar, H. Abdollahi, S. Ashrafinia, M. Hatt, M. Oveisi, A. Rahmim","Next-Generation Radiogenomics Sequencing for Prediction of EGFR and KRAS Mutation Status in NSCLC Patients Using Multimodal Imaging and Machine Learning Algorithms",2019,"","","","",141,"2022-07-13 10:07:22","","10.1007/s11307-020-01487-8","","",,,,,60,20.00,8,8,3,"","",""
58,"S. Ardabili, A. Mosavi, A. Várkonyi-Kóczy","Advances in Machine Learning Modeling Reviewing Hybrid and Ensemble Methods",2019,"","","","",142,"2022-07-13 10:07:22","","10.1007/978-3-030-36841-8_21","","",,,,,58,19.33,19,3,3,"","",""
60,"M. Hannan, M. Lipu, A. Hussain, P. Ker, T. Mahlia, M. Mansor, A. Ayob, M. Saad, Z. Dong","Toward Enhanced State of Charge Estimation of Lithium-ion Batteries Using Optimized Machine Learning Techniques",2020,"","","","",143,"2022-07-13 10:07:22","","10.1038/s41598-020-61464-7","","",,,,,60,30.00,7,9,2,"","",""
38,"M. Yoosefzadeh-Najafabadi, H. Earl, D. Tulpan, J. Sulik, M. Eskandari","Application of Machine Learning Algorithms in Plant Breeding: Predicting Yield From Hyperspectral Reflectance in Soybean",2021,"","","","",144,"2022-07-13 10:07:22","","10.3389/fpls.2020.624273","","",,,,,38,38.00,8,5,1,"Recent substantial advances in high-throughput field phenotyping have provided plant breeders with affordable and efficient tools for evaluating a large number of genotypes for important agronomic traits at early growth stages. Nevertheless, the implementation of large datasets generated by high-throughput phenotyping tools such as hyperspectral reflectance in cultivar development programs is still challenging due to the essential need for intensive knowledge in computational and statistical analyses. In this study, the robustness of three common machine learning (ML) algorithms, multilayer perceptron (MLP), support vector machine (SVM), and random forest (RF), were evaluated for predicting soybean (Glycine max) seed yield using hyperspectral reflectance. For this aim, the hyperspectral reflectance data for the whole spectra ranged from 395 to 1005 nm, which were collected at the R4 and R5 growth stages on 250 soybean genotypes grown in four environments. The recursive feature elimination (RFE) approach was performed to reduce the dimensionality of the hyperspectral reflectance data and select variables with the largest importance values. The results indicated that R5 is more informative stage for measuring hyperspectral reflectance to predict seed yields. The 395 nm reflectance band was also identified as the high ranked band in predicting the soybean seed yield. By considering either full or selected variables as the input variables, the ML algorithms were evaluated individually and combined-version using the ensemble–stacking (E–S) method to predict the soybean yield. The RF algorithm had the highest performance with a value of 84% yield classification accuracy among all the individual tested algorithms. Therefore, by selecting RF as the metaClassifier for E–S method, the prediction accuracy increased to 0.93, using all variables, and 0.87, using selected variables showing the success of using E–S as one of the ensemble techniques. This study demonstrated that soybean breeders could implement E–S algorithm using either the full or selected spectra reflectance to select the high-yielding soybean genotypes, among a large number of genotypes, at early growth stages.","",""
42,"Durga Prasad Penumuru, Sreekumar Muthuswamy, Premkumar Karumbu","Identification and classification of materials using machine vision and machine learning in the context of industry 4.0",2020,"","","","",145,"2022-07-13 10:07:22","","10.1007/s10845-019-01508-6","","",,,,,42,21.00,14,3,2,"","",""
46,"F. Yuan, S. A. Zargar, Qiuyi Chen, Shaohan Wang","Machine learning for structural health monitoring: challenges and opportunities",2020,"","","","",146,"2022-07-13 10:07:22","","10.1117/12.2561610","","",,,,,46,23.00,12,4,2,"A physics-based approach to structural health monitoring (SHM) has practical shortcomings which restrict its suitability to simple structures under well controlled environments. With the advances in information and sensing technology (sensors and sensor networks), it has become feasible to monitor large/diverse number of parameters in complex real-world structures either continuously or intermittently by employing large in-situ (wireless) sensor networks. The availability of this historical data has engendered a lot of interest in a data-driven approach as a natural and more viable option for realizing the goal of SHM in such structures. However, the lack of sensor data corresponding to different damage scenarios continues to remain a challenge. Most of the supervised machine-learning/deep-learning techniques, when trained using this inherently limited data, lack robustness and generalizability. Physics-informed learning, which involves the integration of domain knowledge into the learning process, is presented here as a potential remedy to this challenge. As a step towards the goal of automated damage detection (mathematically an inverse problem), preliminary results are presented from dynamic modelling of beam structures using physics-informed artificial neural networks. Forward and inverse problems involving partial differential equations are solved and comparisons reveal a clear superiority of physics-informed approach over one that is purely datadriven vis-à-vis overfitting/generalization. Other ways of incorporating domain knowledge into the machine learning pipeline are then presented through case-studies on various aspects of NDI/SHM (visual inspection, impact diagnosis). Lastly, as the final attribute of an optimal SHM approach, a sensing paradigm for non-contact full-field measurements for damage diagnosis is presented.","",""
83,"Xiaoxuan Lu, Han Zou, Hongming Zhou, Lihua Xie, G. Huang","Robust Extreme Learning Machine With its Application to Indoor Positioning",2016,"","","","",147,"2022-07-13 10:07:22","","10.1109/TCYB.2015.2399420","","",,,,,83,13.83,17,5,6,"The increasing demands of location-based services have spurred the rapid development of indoor positioning system and indoor localization system interchangeably (IPSs). However, the performance of IPSs suffers from noisy measurements. In this paper, two kinds of robust extreme learning machines (RELMs), corresponding to the close-to-mean constraint, and the small-residual constraint, have been proposed to address the issue of noisy measurements in IPSs. Based on whether the feature mapping in extreme learning machine is explicit, we respectively provide random-hidden-nodes and kernelized formulations of RELMs by second order cone programming. Furthermore, the computation of the covariance in feature space is discussed. Simulations and real-world indoor localization experiments are extensively carried out and the results demonstrate that the proposed algorithms can not only improve the accuracy and repeatability, but also reduce the deviation and worst case error of IPSs compared with other baseline algorithms.","",""
127,"Lei Zhang, D. Zhang","Robust Visual Knowledge Transfer via Extreme Learning Machine-Based Domain Adaptation",2016,"","","","",148,"2022-07-13 10:07:22","","10.1109/TIP.2016.2598679","","",,,,,127,21.17,64,2,6,"We address the problem of visual knowledge adaptation by leveraging labeled patterns from source domain and a very limited number of labeled instances in target domain to learn a robust classifier for visual categorization. This paper proposes a new extreme learning machine (ELM)-based cross-domain network learning framework, that is called ELM-based Domain Adaptation (EDA). It allows us to learn a category transformation and an ELM classifier with random projection by minimizing the ℓ2,1-norm of the network output weights and the learning error simultaneously. The unlabeled target data, as useful knowledge, is also integrated as a fidelity term to guarantee the stability during cross-domain learning. It minimizes the matching error between the learned classifier and a base classifier, such that many existing classifiers can be readily incorporated as the base classifiers. The network output weights cannot only be analytically determined, but also transferrable. In addition, a manifold regularization with Laplacian graph is incorporated, such that it is beneficial to semisupervised learning. Extensively, we also propose a model of multiple views, referred as MvEDA. Experiments on benchmark visual datasets for video event recognition and object recognition demonstrate that our EDA methods outperform the existing cross-domain learning methods.","",""
37,"B. Pham, A. Jaafari, Mohammadtaghi Avand, N. Al‐Ansari, Tran Dinh Du, H. Yen, T. Phong, D. H. Nguyen, H. V. Le, D. Gholami, Indra Prakash, Hoang Thi Thuy, T. T. Tuyen","Performance Evaluation of Machine Learning Methods for Forest Fire Modeling and Prediction",2020,"","","","",149,"2022-07-13 10:07:22","","10.3390/sym12061022","","",,,,,37,18.50,4,13,2,"Predicting and mapping fire susceptibility is a top research priority in fire-prone forests worldwide. This study evaluates the abilities of the Bayes Network (BN), Naive Bayes (NB), Decision Tree (DT), and Multivariate Logistic Regression (MLP) machine learning methods for the prediction and mapping fire susceptibility across the Pu Mat National Park, Nghe An Province, Vietnam. The modeling methodology was formulated based on processing the information from the 57 historical fires and a set of nine spatially explicit explanatory variables, namely elevation, slope degree, aspect, average annual temperate, drought index, river density, land cover, and distance from roads and residential areas. Using the area under the receiver operating characteristic curve (AUC) and seven other performance metrics, the models were validated in terms of their abilities to elucidate the general fire behaviors in the Pu Mat National Park and to predict future fires. Despite a few differences between the AUC values, the BN model with an AUC value of 0.96 was dominant over the other models in predicting future fires. The second best was the DT model (AUC = 0.94), followed by the NB (AUC = 0.939), and MLR (AUC = 0.937) models. Our robust analysis demonstrated that these models are sufficiently robust in response to the training and validation datasets change. Further, the results revealed that moderate to high levels of fire susceptibilities are associated with ~19% of the Pu Mat National Park where human activities are numerous. This study and the resultant susceptibility maps provide a basis for developing more efficient fire-fighting strategies and reorganizing policies in favor of sustainable management of forest resources.","",""
6,"Rudrasis Chakraborty, Liu Yang, Søren Hauberg, B. Vemuri","Intrinsic Grassmann Averages for Online Linear, Robust and Nonlinear Subspace Learning",2017,"","","","",150,"2022-07-13 10:07:22","","10.1109/tpami.2020.2992392","","",,,,,6,1.20,2,4,5,"Principal component analysis (PCA) and Kernel principal component analysis (KPCA) are fundamental methods in machine learning for dimensionality reduction. The former is a technique for finding this approximation in finite dimensions and the latter is often in an infinite dimensional reproducing Kernel Hilbert-space (RKHS). In this paper, we present a geometric framework for computing the principal linear subspaces in both (finite and infinite) situations as well as for the robust PCA case, that amounts to computing the intrinsic average on the space of all subspaces: the Grassmann manifold. Points on this manifold are defined as the subspaces spanned by <inline-formula><tex-math notation=""LaTeX"">$K$</tex-math><alternatives><mml:math><mml:mi>K</mml:mi></mml:math><inline-graphic xlink:href=""chakraborty-ieq1-2992392.gif""/></alternatives></inline-formula>-tuples of observations. The intrinsic Grassmann average of these subspaces are shown to coincide with the principal components of the observations when they are drawn from a Gaussian distribution. We show similar results in the RKHS case and provide an efficient algorithm for computing the projection onto the this average subspace. The result is a method akin to KPCA which is substantially faster. Further, we present a novel online version of the KPCA using our geometric framework. Competitive performance of all our algorithms are demonstrated on a variety of real and synthetic data sets.","",""
0,"Yiyang Chen, Wei Jiang, Themistoklis Charalambous","Machine learning based iterative learning control for non-repetitive time-varying systems",2021,"","","","",151,"2022-07-13 10:07:22","","10.1002/rnc.6272","","",,,,,0,0.00,0,3,1,"The repetitive tracking task for time-varying systems (TVSs) with non-repetitive time-varying parameters, which is also called non-repetitive TVSs, is realized in this paper using iterative learning control (ILC). A machine learning (ML) based nominal model update mechanism, which utilizes the linear regression technique to update the nominal model at each ILC trial only using the current trial information, is proposed for non-repetitive TVSs in order to enhance the ILC performance. Given that the ML mechanism forces the model uncertainties to remain within the ILC robust tolerance, an ILC update law is proposed to deal with non-repetitive TVSs. How to tune parameters inside ML and ILC algorithms to achieve the desired aggregate performance is also provided. The robustness and reliability of the proposed method are verified by simulations. Comparison with current state-of-the-art demonstrates its superior control performance in terms of controlling precision. This paper broadens ILC applications from time-invariant systems to non-repetitive TVSs, adopts ML regression technique to estimate non-repetitive time-varying parameters between two ILC trials and proposes a detailed parameter tuning mechanism to achieve desired performance, which are the main contributions.","",""
20,"Hiroshi Kuwajima, Hirotoshi Yasuoka, Toshihiro Nakae","Engineering problems in machine learning systems",2019,"","","","",152,"2022-07-13 10:07:22","","10.1007/s10994-020-05872-w","","",,,,,20,6.67,7,3,3,"","",""
2,"Jiyuan Tu, Weidong Liu, Xiaojun Mao","Byzantine-robust distributed sparse learning for M-estimation",2021,"","","","",153,"2022-07-13 10:07:22","","10.1007/S10994-021-06001-X","","",,,,,2,2.00,1,3,1,"","",""
29,"Debbie Rankin, Michaela M. Black, R. Bond, J. Wallace, M. Mulvenna, G. Epelde","Reliability of Supervised Machine Learning Using Synthetic Data in Health Care: Model to Preserve Privacy for Data Sharing",2020,"","","","",154,"2022-07-13 10:07:22","","10.2196/18910","","",,,,,29,14.50,5,6,2,"Background The exploitation of synthetic data in health care is at an early stage. Synthetic data could unlock the potential within health care datasets that are too sensitive for release. Several synthetic data generators have been developed to date; however, studies evaluating their efficacy and generalizability are scarce. Objective This work sets out to understand the difference in performance of supervised machine learning models trained on synthetic data compared with those trained on real data. Methods A total of 19 open health datasets were selected for experimental work. Synthetic data were generated using three synthetic data generators that apply classification and regression trees, parametric, and Bayesian network approaches. Real and synthetic data were used (separately) to train five supervised machine learning models: stochastic gradient descent, decision tree, k-nearest neighbors, random forest, and support vector machine. Models were tested only on real data to determine whether a model developed by training on synthetic data can used to accurately classify new, real examples. The impact of statistical disclosure control on model performance was also assessed. Results A total of 92% of models trained on synthetic data have lower accuracy than those trained on real data. Tree-based models trained on synthetic data have deviations in accuracy from models trained on real data of 0.177 (18%) to 0.193 (19%), while other models have lower deviations of 0.058 (6%) to 0.072 (7%). The winning classifier when trained and tested on real data versus models trained on synthetic data and tested on real data is the same in 26% (5/19) of cases for classification and regression tree and parametric synthetic data and in 21% (4/19) of cases for Bayesian network-generated synthetic data. Tree-based models perform best with real data and are the winning classifier in 95% (18/19) of cases. This is not the case for models trained on synthetic data. When tree-based models are not considered, the winning classifier for real and synthetic data is matched in 74% (14/19), 53% (10/19), and 68% (13/19) of cases for classification and regression tree, parametric, and Bayesian network synthetic data, respectively. Statistical disclosure control methods did not have a notable impact on data utility. Conclusions The results of this study are promising with small decreases in accuracy observed in models trained with synthetic data compared with models trained with real data, where both are tested on real data. Such deviations are expected and manageable. Tree-based classifiers have some sensitivity to synthetic data, and the underlying cause requires further investigation. This study highlights the potential of synthetic data and the need for further evaluation of their robustness. Synthetic data must ensure individual privacy and data utility are preserved in order to instill confidence in health care departments when using such data to inform policy decision-making.","",""
0,"J. Figuerêdo, V. T. Sarinho, R. Calumby","Low-Cost Machine Learning for Effective and Efficient Bad Smells Detection",2021,"","","","",155,"2022-07-13 10:07:22","","10.5753/kdmile.2021.17468","","",,,,,0,0.00,0,3,1,"Bad smells are characteristics of software that indicate a code or design problem which can make information system hard to understand, evolve, and maintain. To address this problem, different approaches, manual and automated, have been proposed over the years, including more recently machine learning alternatives. However, despite the advances achieved, some machine learning techniques have not yet been effectively explored, such as the use of feature selection techniques. Moreover, it is not clear to what extent the use of numerous source-code features are necessary for reasonable bad smell detection success. Therefore, in this work we propose an approach using low-cost machine learning for effective and efficient detection of bad smells, through explicit feature selection. Our results showed that the selection allowed to statistically improve the effectiveness of the models. For some cases, the models achieved statistical equivalence, but relying on a highly reduced set of features. Indeed, by using explicit feature selection, simpler models such as Naive Bayes became statistically equivalent to robust models such as Random Forest. Therefore, the selection of features allowed keeping competitive or even superior effectiveness while also improving the efficiency of the models, demanding less computational resources for source-code preprocessing, model training and bad smell detection.","",""
0,"W. Zong, Yang-Wai Chow, W. Susilo","Visual Analysis of Adversarial Examples in Machine Learning",2021,"","","","",156,"2022-07-13 10:07:22","","10.1007/978-981-33-6726-5_4","","",,,,,0,0.00,0,3,1,"","",""
138,"Han Zou, Baoqi Huang, Xiaoxuan Lu, Hao Jiang, Lihua Xie","A Robust Indoor Positioning System Based on the Procrustes Analysis and Weighted Extreme Learning Machine",2016,"","","","",157,"2022-07-13 10:07:22","","10.1109/TWC.2015.2487963","","",,,,,138,23.00,28,5,6,"Indoor positioning system (IPS) has become one of the most attractive research fields due to the increasing demands on location-based services (LBSs) in indoor environments. Various IPSs have been developed under different circumstances, and most of them adopt the fingerprinting technique to mitigate pervasive indoor multipath effects. However, the performance of the fingerprinting technique severely suffers from device heterogeneity existing across commercial off-the-shelf mobile devices (e.g., smart phones, tablet computers, etc.) and indoor environmental changes (e.g., the number, distribution and activities of people, the placement of furniture, etc.). In this paper, we transform the received signal strength (RSS) to a standardized location fingerprint based on the Procrustes analysis, and introduce a similarity metric, termed signal tendency index (STI), for matching standardized fingerprints. An analysis of the capability of the proposed STI to handle device heterogeneity and environmental changes is presented. We further develop a robust and precise IPS by integrating the merits of both the STI and weighted extreme learning machine (WELM). Finally, extensive experiments are carried out and a performance comparison with existing solutions verifies the superiority of the proposed IPS in terms of robustness to device heterogeneity.","",""
8,"Mustafa Anil Koçak, David Ramirez, E. Erkip, D. Shasha","SafePredict: A Meta-Algorithm for Machine Learning That Uses Refusals to Guarantee Correctness",2017,"","","","",158,"2022-07-13 10:07:22","","10.1109/TPAMI.2019.2932415","","",,,,,8,1.60,2,4,5,"<italic>SafePredict</italic> is a novel meta-algorithm that works with any base prediction algorithm for online data to guarantee an arbitrarily chosen correctness rate, <inline-formula><tex-math notation=""LaTeX"">$1-\epsilon$</tex-math><alternatives><mml:math><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mi>ε</mml:mi></mml:mrow></mml:math><inline-graphic xlink:href=""kocak-ieq1-2932415.gif""/></alternatives></inline-formula>, by allowing refusals. Allowing refusals means that the meta-algorithm may refuse to emit a prediction produced by the base algorithm so that the error rate on non-refused predictions does not exceed <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq2-2932415.gif""/></alternatives></inline-formula>. The SafePredict error bound does not rely on any assumptions on the data distribution or the base predictor. When the base predictor happens not to exceed the target error rate <inline-formula><tex-math notation=""LaTeX"">$\epsilon$</tex-math><alternatives><mml:math><mml:mi>ε</mml:mi></mml:math><inline-graphic xlink:href=""kocak-ieq3-2932415.gif""/></alternatives></inline-formula>, SafePredict refuses only a finite number of times. When the error rate of the base predictor changes through time SafePredict makes use of a weight-shifting heuristic that adapts to these changes without knowing when the changes occur yet still maintains the correctness guarantee. Empirical results show that (i) SafePredict compares favorably with state-of-the-art confidence-based refusal mechanisms which fail to offer robust error guarantees; and (ii) combining SafePredict with such refusal mechanisms can in many cases further reduce the number of refusals. Our software is included in the supplementary material, which can be found on the Computer Society Digital Library at <uri>http://doi.ieeecomputersociety.org/10.1109/TPAMI.2019.2932415</uri>.","",""
80,"N. Ball, R. Brunner, A. Myers, N. E. Strand, S. Alberts, D. T. D. O. Astronomy, U. I. Urbana-Champaign, National Center for Supercomputing Applications, D. Physics","Robust Machine Learning Applied to Astronomical Data Sets. III. Probabilistic Photometric Redshifts for Galaxies and Quasars in the SDSS and GALEX",2008,"","","","",159,"2022-07-13 10:07:22","","10.1086/589646","","",,,,,80,5.71,9,9,14,"We apply machine learning in the form of a nearest neighbor instance-based algorithm (NN) to generate full photometric redshift probability density functions (PDFs) for objects in the Fifth Data Release of the Sloan Digital Sky Survey (SDSS DR5). We use a conceptually simple but novel application of NN to generate the PDFs, perturbing the object colors by their measurement error and using the resulting instances of nearest neighbor distributions to generate numerous individual redshifts. When the redshifts are compared to existing SDSS spectroscopic data, we find that the mean value of each PDF has a dispersion between the photometric and spectroscopic redshift consistent with other machine learning techniques, being σ = 0.0207 ± 0.0001 for main sample galaxies to r < 17.77 mag, σ = 0.0243 ± 0.0002 for luminous red galaxies to r 19.2 mag, and σ = 0.343 ± 0.005 for quasars to i < 20.3 mag. The PDFs allow the selection of subsets with improved statistics. For quasars, the improvement is dramatic: for those with a single peak in their probability distribution, the dispersion is reduced from 0.343 to σ = 0.117 ± 0.010, and the photometric redshift is within 0.3 of the spectroscopic redshift for 99.3% ± 0.1% of the objects. Thus, for this optical quasar sample, we can virtually eliminate ""catastrophic"" photometric redshift estimates. In addition to the SDSS sample, we incorporate ultraviolet photometry from the Third Data Release of the Galaxy Evolution Explorer All-Sky Imaging Survey (GALEX AIS GR3) to create PDFs for objects seen in both surveys. For quasars, the increased coverage of the observed-frame UV of the SED results in significant improvement over the full SDSS sample, with σ = 0.234 ± 0.010. We demonstrate that this improvement is genuine and not an artifact of the SDSS-GALEX matching process.","",""
1,"Ramkumar Harikrishnakumar, A. Dand, S. Nannapaneni, K. Krishnan","Supervised Machine Learning Approach for Effective Supplier Classification",2019,"","","","",160,"2022-07-13 10:07:22","","10.1109/ICMLA.2019.00045","","",,,,,1,0.33,0,4,3,"Supplier assessment plays a critical role in the supply chain management, which involves the flow of goods and services from the initial stage (raw material procurement) to the final stage (delivery). Supplier assessment is a multi-criteria decision-making (MCDM) approach that requires several criteria for the proper assessment of the suppliers. When there are several criteria involved, it makes the supplier assessment process more complicated. For a comprehensive and robust assessment process, we propose the use of supervised machine learning algorithms to classify various suppliers into four categories: excellent, good, satisfactory, and unsatisfactory. In this paper, supervised learning (classification) algorithms are applied for a supplier assessment problem where a model is trained based on the previous historical data and then tested on the new unseen data set. This method will provide an efficient way for supplier assessment that is more effective in terms of accuracy and time when compared to MCDM approach. Classification algorithms such as support vector machines (with linear, polynomial and radial basis kernels), logistic regression, k-nearest neighbors, and naïve Bayes methods are used to train the model and their performance is assessed against a test data. Finally, the performance measures from all the classification methods are used to assess the best supplier.","",""
21,"El-Mahdi El-Mhamdi, R. Guerraoui, Arsany Guirguis, L. Hoang, Sébastien Rouault","Genuinely Distributed Byzantine Machine Learning",2019,"","","","",161,"2022-07-13 10:07:22","","10.1145/3382734.3405695","","",,,,,21,7.00,4,5,3,"Machine Learning (ML) solutions are nowadays distributed, according to the so-called server/worker architecture. One server holds the model parameters while several workers train the model. Clearly, such architecture is prone to various types of component failures, which can be all encompassed within the spectrum of a Byzantine behavior. Several approaches have been proposed recently to tolerate Byzantine workers. Yet all require trusting a central parameter server. We initiate in this paper the study of the ""general"" Byzantine-resilient distributed machine learning problem where no individual component is trusted. In particular, we distribute the parameter server computation on several nodes. We show that this problem can be solved in an asynchronous system, despite the presence of ⅓ Byzantine parameter servers and ⅓ Byzantine workers (which is optimal). We present a new algorithm, ByzSGD, which solves the general Byzantine-resilient distributed machine learning problem by relying on three major schemes. The first, Scatter/Gather, is a communication scheme whose goal is to bound the maximum drift among models on correct servers. The second, Distributed Median Contraction (DMC), leverages the geometric properties of the median in high dimensional spaces to bring parameters within the correct servers back close to each other, ensuring learning convergence. The third, Minimum-Diameter Averaging (MDA), is a statistically-robust gradient aggregation rule whose goal is to tolerate Byzantine workers. MDA requires loose bound on the variance of non-Byzantine gradient estimates, compared to existing alternatives (e.g., Krum [12]). Interestingly, ByzSGD ensures Byzantine resilience without adding communication rounds (on a normal path), compared to vanilla non-Byzantine alternatives. ByzSGD requires, however, a larger number of messages which, we show, can be reduced if we assume synchrony. We implemented ByzSGD on top of TensorFlow, and we report on our evaluation results. In particular, we show that ByzSGD achieves convergence in Byzantine settings with around 32% overhead compared to vanilla TensorFlow. Furthermore, we show that ByzSGD's throughput overhead is 24--176% in the synchronous case and 28--220% in the asynchronous case.","",""
0,"Muhammad Abdullah Hanif, R. Hafiz, M. Javed, Semeen Rehman, M. Shafique","Energy-Efficient Design of Advanced Machine Learning Hardware",2019,"","","","",162,"2022-07-13 10:07:22","","10.1007/978-3-030-04666-8_21","","",,,,,0,0.00,0,5,3,"","",""
52,"Sahrish Khan Tayyaba, Hasan Ali Khattak, Ahmad S. Almogren, M. A. Shah, Ikram Ud Din, Ibrahim Alkhalifa, M. Guizani","5G Vehicular Network Resource Management for Improving Radio Access Through Machine Learning",2020,"","","","",163,"2022-07-13 10:07:22","","10.1109/ACCESS.2020.2964697","","",,,,,52,26.00,7,7,2,"The current cellular technology and vehicular networks cannot satisfy the mighty strides of vehicular network demands. Resource management has become a complex and challenging objective to gain expected outcomes in a vehicular environment. The 5G cellular network promises to provide ultra-high-speed, reduced delay, and reliable communications. The development of new technologies such as the network function virtualization (NFV) and software defined networking (SDN) are critical enabling technologies leveraging 5G. The SDN-based 5G network can provide an excellent platform for autonomous vehicles because SDN offers open programmability and flexibility for new services incorporation. This separation of control and data planes enables centralized and efficient management of resources in a very optimized and secure manner by having a global overview of the whole network. The SDN also provides flexibility in communication administration and resource management, which are of critical importance when considering the ad-hoc nature of vehicular network infrastructures, in terms of safety, privacy, and security, in vehicular network environments. In addition, it promises the overall improved performance. In this paper, we propose a flow-based policy framework on the basis of two tiers virtualization for vehicular networks using SDNs. The vehicle to vehicle (V2V) communication is quite possible with wireless virtualization where different radio resources are allocated to V2V communications based on the flow classification, i.e., safety-related flow or non-safety flows, and the controller is responsible for managing the overall vehicular environment and V2X communications. The motivation behind this study is to implement a machine learning-enabled architecture to cater the sophisticated demands of modern vehicular Internet infrastructures. The inclination towards robust communications in 5G-enabled networks has made it somewhat tricky to manage network slicing efficiently. This paper also presents a proof of concept for leveraging machine learning-enabled resource classification and management through experimental evaluation of special-purpose testbed established in custom mininet setup. Furthermore, the results have been evaluated using Long Short-Term Memory (LSTM), Convolutional Neural Network (CNN), and Deep Neural Network (DNN). While concluding the paper, it is shown that the LSTM has outperformed the rest of classification techniques with promising results.","",""
33,"A. Samarakoon, K. Barros, Y. Li, M. Eisenbach, Qiang Zhang, F. Ye, V. Sharma, Z. Dun, Haidong Zhou, S. A. Grigera, C. Batista, D. Tennant","Machine-learning-assisted insight into spin ice Dy2Ti2O7",2019,"","","","",164,"2022-07-13 10:07:22","","10.1038/s41467-020-14660-y","","",,,,,33,11.00,3,12,3,"","",""
142,"Hassan Rafique, Mingrui Liu, Qihang Lin, Tianbao Yang","Non-Convex Min-Max Optimization: Provable Algorithms and Applications in Machine Learning",2018,"","","","",165,"2022-07-13 10:07:22","","","","",,,,,142,35.50,36,4,4,"Min-max saddle-point problems have broad applications in many tasks in machine learning, e.g., distributionally robust learning, learning with non-decomposable loss, or learning with uncertain data. Although convex-concave saddle-point problems have been broadly studied with efficient algorithms and solid theories available, it remains a challenge to design provably efficient algorithms for non-convex saddle-point problems, especially when the objective function involves an expectation or a large-scale finite sum. Motivated by recent literature on non-convex non-smooth minimization, this paper studies a family of non-convex min-max problems where the minimization component is non-convex (weakly convex) and the maximization component is concave. We propose a proximally guided stochastic subgradient method and a proximally guided stochastic variance-reduced method for expected and finite-sum saddle-point problems, respectively. We establish the computation complexities of both methods for finding a nearly stationary point of the corresponding minimization problem.","",""
0,"Murilo Cruz Lopes, Marília de Matos Amorim, V. S. Freitas, R. Calumby","Survival Prediction for Oral Cancer Patients: A Machine Learning Approach",2021,"","","","",166,"2022-07-13 10:07:22","","10.5753/kdmile.2021.17466","","",,,,,0,0.00,0,4,1,"There is a high incidence of oral cancer in Brazil, with 150,000 new cases estimated for 2020-2022. In most cases, it is diagnosed at an advanced stage and are related to many risk factors. The Registro Hospitalar de Câncer (RHC), managed by Instituto Nacional de Câncer (INCA), is a nation-wide database that integrates cancer registers from several hospitals in Brazil. RHC is mostly an administrative database but also include clinical, socioeconomic and hospitalization data for each patient with a cancer diagnostic in the country. For these patients, prognostication is always a difficult task a demand multi-dimensional analysis. Therefore, exploiting large-scale data and machine intelligence approaches emerge as promising tool for computer-aided decision support on death risk estimation. Given the importance of this context, some works have reported high prognostication effectiveness, however with extremely limited data collections, relying on weak validation protocols or simple robustness analysis. Hence, this work describes a detailed workflow and experimental analysis for oral cancer patient survival prediction considering careful data curation and strict validation procedures. By exploiting multiple machine learning algorithms and optimization techniques the proposed approach allowed promising survival prediction effectiveness with F1 and AuC-ROC over 0.78 and 0.80, respectively. Moreover, a detailed analysis have shown that the minimization of different types of prediction errors were achieved by different models, which highlights the importance of the rigour in this kind of validation.","",""
132,"Kai Zhang, M. Luo","Outlier-robust extreme learning machine for regression problems",2015,"","","","",167,"2022-07-13 10:07:22","","10.1016/j.neucom.2014.09.022","","",,,,,132,18.86,66,2,7,"","",""
299,"J Zhang, M. Harman, Lei Ma, Yang Liu","Machine Learning Testing: Survey, Landscapes and Horizons",2019,"","","","",168,"2022-07-13 10:07:22","","10.1109/tse.2019.2962027","","",,,,,299,99.67,75,4,3,"This paper provides a comprehensive survey of techniques for testing machine learning systems; Machine Learning Testing (ML testing) research. It covers 144 papers on testing properties (e.g., correctness, robustness, and fairness), testing components (e.g., the data, learning program, and framework), testing workflow (e.g., test generation and test evaluation), and application scenarios (e.g., autonomous driving, machine translation). The paper also analyses trends concerning datasets, research trends, and research focus, concluding with research challenges and promising research directions in ML testing.","",""
141,"F. Thabtah","Machine learning in autistic spectrum disorder behavioral research: A review and ways forward",2019,"","","","",169,"2022-07-13 10:07:22","","10.1080/17538157.2017.1399132","","",,,,,141,47.00,141,1,3,"ABSTRACT Autistic Spectrum Disorder (ASD) is a mental disorder that retards acquisition of linguistic, communication, cognitive, and social skills and abilities. Despite being diagnosed with ASD, some individuals exhibit outstanding scholastic, non-academic, and artistic capabilities, in such cases posing a challenging task for scientists to provide answers. In the last few years, ASD has been investigated by social and computational intelligence scientists utilizing advanced technologies such as machine learning to improve diagnostic timing, precision, and quality. Machine learning is a multidisciplinary research topic that employs intelligent techniques to discover useful concealed patterns, which are utilized in prediction to improve decision making. Machine learning techniques such as support vector machines, decision trees, logistic regressions, and others, have been applied to datasets related to autism in order to construct predictive models. These models claim to enhance the ability of clinicians to provide robust diagnoses and prognoses of ASD. However, studies concerning the use of machine learning in ASD diagnosis and treatment suffer from conceptual, implementation, and data issues such as the way diagnostic codes are used, the type of feature selection employed, the evaluation measures chosen, and class imbalances in data among others. A more serious claim in recent studies is the development of a new method for ASD diagnoses based on machine learning. This article critically analyses these recent investigative studies on autism, not only articulating the aforementioned issues in these studies but also recommending paths forward that enhance machine learning use in ASD with respect to conceptualization, implementation, and data. Future studies concerning machine learning in autism research are greatly benefitted by such proposals.","",""
27,"S. Qu, Zhe Guan, E. Verschuur, Yangkang Zhang","Automatic high-resolution microseismic event detection via supervised machine learning",2019,"","","","",170,"2022-07-13 10:07:22","","10.1093/GJI/GGAA193","","",,,,,27,9.00,7,4,3,"Microseismic methods are crucial for real-time monitoring of the hydraulic fracturing dynamic status during the development of unconventional reservoirs. However, unlike the active-source seismic events, the microseismic events usually have low signal-to-noise ratio (SNR), which makes its data processing challenging. To overcome the noise issue of the weak microseismic events, we propose a new workflow for high-resolution microseismic event detection. For the preprocessing, fix-sized segmentation with a length of 2*wavelength is used to divide the data into segments. Later on, 191 features have been extracted and used as the input data to train the support vector machine (SVM) model. These features include 63 1-D time/spectral-domain features, and 128 2-D texture features, which indicate the continuity, smoothness, and irregularity of the events/noise. The proposed feature extraction maximally exploits the limited information of each segment. Afterward, we use a combination of univariate feature selection and random-forest-based recursive feature elimination for feature selection to avoid overfitting. This feature selection strategy not only finds the best features, but also decides the optimal number of features that are needed for the best accuracy. Regarding the training process, SVM with a Gaussian kernel is used. In addition, a cross-validation (CV) process is implemented for automatic parameter setting. In the end, a group of synthetic and field microseismic data with different levels of complexity show that the proposed workflow is much more robust than the state-of-the-art short-term-average over long-term-average ratio (STA/LTA) method and also performs better than the convolutional-neural-networks (CNN), for this case where the amount of training data sets is limited. A demo for the synthetic example is available: https://github.com/shanqu91/ML_event_detection_microseismic.","",""
97,"M. Maniruzzaman, M. Rahman, Md. Al-MehediHasan, Harman S. Suri, M. Abedin, A. El-Baz, J. Suri","Accurate Diabetes Risk Stratification Using Machine Learning: Role of Missing Value and Outliers",2018,"","","","",171,"2022-07-13 10:07:22","","10.1007/s10916-018-0940-7","","",,,,,97,24.25,14,7,4,"","",""
17,"Hong Chang, T. Nguyen, Sasi Kumar Murakonda, Ehsan Kazemi, R. Shokri","On Adversarial Bias and the Robustness of Fair Machine Learning",2020,"","","","",172,"2022-07-13 10:07:22","","","","",,,,,17,8.50,3,5,2,"Optimizing prediction accuracy can come at the expense of fairness. Towards minimizing discrimination against a group, fair machine learning algorithms strive to equalize the behavior of a model across different groups, by imposing a fairness constraint on models. However, we show that giving the same importance to groups of different sizes and distributions, to counteract the effect of bias in training data, can be in conflict with robustness. We analyze data poisoning attacks against group-based fair machine learning, with the focus on equalized odds. An adversary who can control sampling or labeling for a fraction of training data, can reduce the test accuracy significantly beyond what he can achieve on unconstrained models. Adversarial sampling and adversarial labeling attacks can also worsen the model's fairness gap on test data, even though the model satisfies the fairness constraint on training data. We analyze the robustness of fair machine learning through an empirical evaluation of attacks on multiple algorithms and benchmark datasets.","",""
29,"Yuhui Zheng, Le Sun, Shunfeng Wang, Jianwei Zhang, J. Ning","Spatially Regularized Structural Support Vector Machine for Robust Visual Tracking",2019,"","","","",173,"2022-07-13 10:07:22","","10.1109/TNNLS.2018.2855686","","",,,,,29,9.67,6,5,3,"Structural support vector machine (SSVM) is popular in the visual tracking field as it provides a consistent target representation for both learning and detection. However, the spatial distribution of feature is not considered in standard SSVM-based trackers, therefore leading to limited performance. To obtain a robust discriminative classifier, this paper proposes a novel tracking framework that spatially regularizes SSVM, which yields a new spatially regularized SSVM (SRSSVM). We utilize the spatial regularization prior to penalize the learning classifier with the same size as the target region. The location of classifier spatially located far from the center of region is assigned large weight and vice versa. Then, it is introduced into the SSVM model as a regularization factor to learn the robust discriminative model. Furthermore, an optimizing algorithm with dual coordination descent is presented to efficiently solve the SRSSVM tracking model. Our proposed SRSSVM tracking method has low computational cost like the traditional linear SSVM tracker while can significantly improve the robustness of the discriminative classifier. The experimental results on three popular tracking benchmark data sets show that the proposed SRSSVM tracking method performs favorably against the state-of-the-art trackers.","",""
82,"Qian Yang, Jina Suh, N. Chen, Gonzalo A. Ramos","Grounding Interactive Machine Learning Tool Design in How Non-Experts Actually Build Models",2018,"","","","",174,"2022-07-13 10:07:22","","10.1145/3196709.3196729","","",,,,,82,20.50,21,4,4,"Machine learning (ML) promises data-driven insights and solutions for people from all walks of life, but the skill of crafting these solutions is possessed by only a few. Emerging research addresses this issue by creating ML tools that are easy and accessible to people who are not formally trained in ML (non-experts). This work investigated how non-experts build ML solutions for themselves in real life. Our interviews and surveys revealed unique potentials of non-expert ML, as well several pitfalls that non-experts are susceptible to. For example, many perceived percentage accuracy as a sole measure of performance, thus problematic models proceeded to deployment. These observations suggested that, while challenging, making ML easy and robust should both be important goals of designing novice-facing ML tools. To advance on this insight, we discuss design implications and created a sensitizing concept to demonstrate how designers might guide non-experts to easily build robust solutions.","",""
85,"Neoklis Polyzotis, Sudip Roy, S. E. Whang, Martin A. Zinkevich","Data Lifecycle Challenges in Production Machine Learning",2018,"","","","",175,"2022-07-13 10:07:22","","10.1145/3299887.3299891","","",,,,,85,21.25,21,4,4,"Machine learning has become an essential tool for gleaning knowledge from data and tackling a diverse set of computationally hard tasks. However, the accuracy of a machine learned model is deeply tied to the data that it is trained on. Designing and building robust processes and tools that make it easier to analyze, validate, and transform data that is fed into large-scale machine learning systems poses data management challenges. Drawn from our experience in developing data-centric infrastructure for a production machine learning platform at Google, we summarize some of the interesting research challenges that we encountered, and survey some of the relevant literature from the data management and machine learning communities. Specifically, we explore challenges in three main areas of focus - data understanding, data validation and cleaning, and data preparation. In each of these areas, we try to explore how different constraints are imposed on the solutions depending on where in the lifecycle of a model the problems are encountered and who encounters them.","",""
0,"Hsiao-Chi Li, Chang-Yu Cheng, Chia Chou, Chien-Chang Hsu, Meng-Lin Chang, Y. Chiu, J. Chai","Multi-Class Brain Age Discrimination Using Machine Learning Algorithm",2019,"","","","",176,"2022-07-13 10:07:22","","10.1109/ICMLC48188.2019.8949317","","",,,,,0,0.00,0,7,3,"Resting-state functional connectivity analyses have revealed a significant effect on the inter-regional interactions in brain. The brain age prediction based on resting-state functional magnetic resonance imaging has been proved as biomarkers to characterize the typical brain development and neuropsychiatric disorders. The brain age prediction model based on functional connectivity measurements derived from resting-state functional magnetic resonance imaging has received a lots of interest in recent years due to its great success in age prediction. However, some of the recent studies rely on experienced neuroscientist experts to select appropriate connectivity features in order to build a robust model for prediction while the others just selected the features based on trial-and-error test. Besides, the subjects used in this studies omitted some subjects that can be divided into two groups with less similarity which may confused the prediction model. In this study, we proposed a multi-class age categories discrimination method with the connectivity features selected via K-means clustering with no prior knowledge provided. The experimental results show that with K-means selected features the proposed model better discriminate multi-class age categories.","",""
83,"S. Kendale, Prathamesh Kulkarni, A. Rosenberg, Jing Wang","Supervised Machine-learning Predictive Analytics for Prediction of Postinduction Hypotension",2018,"","","","",177,"2022-07-13 10:07:22","","10.1097/ALN.0000000000002374","","",,,,,83,20.75,21,4,4,"What We Already Know about This Topic The ability to predict postinduction hypotension remains limited and challenging due to the multitude of data elements that may be considered Novel machine-learning algorithms may offer a systematic approach to predict postinduction hypotension, but are understudied What This Article Tells Us That Is New Among 13,323 patients undergoing a variety of surgical procedures, 8.9% experienced a mean arterial pressure less than 55 mmHg within 10 min of induction start While some machine-learning algorithms perform worse than logistic regression, several techniques may be superior Gradient boosting machine, with tuning, demonstrates a receiver operating characteristic area under the curve of 0.76, a negative predictive value of 19%, and positive predictive value of 96% Background: Hypotension is a risk factor for adverse perioperative outcomes. Machine-learning methods allow large amounts of data for development of robust predictive analytics. The authors hypothesized that machine-learning methods can provide prediction for the risk of postinduction hypotension. Methods: Data was extracted from the electronic health record of a single quaternary care center from November 2015 to May 2016 for patients over age 12 that underwent general anesthesia, without procedure exclusions. Multiple supervised machine-learning classification techniques were attempted, with postinduction hypotension (mean arterial pressure less than 55 mmHg within 10 min of induction by any measurement) as primary outcome, and preoperative medications, medical comorbidities, induction medications, and intraoperative vital signs as features. Discrimination was assessed using cross-validated area under the receiver operating characteristic curve. The best performing model was tuned and final performance assessed using split-set validation. Results: Out of 13,323 cases, 1,185 (8.9%) experienced postinduction hypotension. Area under the receiver operating characteristic curve using logistic regression was 0.71 (95% CI, 0.70 to 0.72), support vector machines was 0.63 (95% CI, 0.58 to 0.60), naive Bayes was 0.69 (95% CI, 0.67 to 0.69), k-nearest neighbor was 0.64 (95% CI, 0.63 to 0.65), linear discriminant analysis was 0.72 (95% CI, 0.71 to 0.73), random forest was 0.74 (95% CI, 0.73 to 0.75), neural nets 0.71 (95% CI, 0.69 to 0.71), and gradient boosting machine 0.76 (95% CI, 0.75 to 0.77). Test set area for the gradient boosting machine was 0.74 (95% CI, 0.72 to 0.77). Conclusions: The success of this technique in predicting postinduction hypotension demonstrates feasibility of machine-learning models for predictive analytics in the field of anesthesiology, with performance dependent on model selection and appropriate tuning.","",""
60,"P. Hawryło, B. Bednarz, P. Wężyk, Marta Szostak","Estimating defoliation of Scots pine stands using machine learning methods and vegetation indices of Sentinel-2",2018,"","","","",178,"2022-07-13 10:07:22","","10.1080/22797254.2017.1417745","","",,,,,60,15.00,15,4,4,"ABSTRACT In the presented study, the Sentinel-2 vegetation indices (VIs) were evaluated in context of estimating defoliation of Scots pine stands in western Poland. Regression and classification models were built based on reference data from 50 field plots and Sentinel-2 satellite images from three acquisition dates. Three machine-learning (ML) methods were tested: k-nearest neighbors (kNN), random forest (RF), and support vector machines (SVM). Regression models predicted stands defoliation with moderate accuracy. R2 values for regression models amounted to 0.53, 0.57, 0.57 for kNN, RF and SVM, accordingly. Analogically, the following values of normalized root mean squared error were obtained: 12.2%, 11.9% and 11.6%. Overall accuracies for two-class classification models were 78%, 75%, 78% for kNN, RF and SVM methods. The Green Normalized Difference Vegetation Index and MERIS Terrestrial Chlorophyll Index VIs were found to be most robust defoliation predictors regardless of the ML method. We conclude that Sentinel-2 satellite images provide useful information about forest defoliation and may contribute to forest monitoring systems.","",""
79,"A. Nandy, Chenru Duan, J. Janet, Stefan Gugler, H. Kulik","Strategies and Software for Machine Learning Accelerated Discovery in Transition Metal Chemistry",2018,"","","","",179,"2022-07-13 10:07:22","","10.1021/ACS.IECR.8B04015","","",,,,,79,19.75,16,5,4,"Machine learning the electronic structure of open shell transition metal complexes presents unique challenges, including robust and automated data set generation. Here, we introduce tools that simplify data acquisition from density functional theory (DFT) and validation of trained machine learning models using the molSimplify automatic design (mAD) workflow. We demonstrate this workflow by training and comparing the performance of LASSO, kernel ridge regression (KRR), and artificial neural network (ANN) models using heuristic, topological revised autocorrelation (RAC) descriptors we have recently introduced for machine learning inorganic chemistry. On a series of open shell transition metal complexes, we evaluate set aside test errors of these models for predicting the HOMO level and HOMO-LUMO gap. The best performing models are ANNs, which show 0.15 and 0.25 eV test set mean absolute errors on the HOMO level and HOMO-LUMO gap, respectively. Poor performing KRR models using the full 153-feature RAC set are improved to nearly the same performance as the ANNs when trained on down-selected subsets of 20-30 features. Analysis of the essential descriptors for HOMO and HOMO-LUMO gap prediction as well as comparison to subsets previously obtained for other properties reveals the paramount importance of non-local, steric properties in determining frontier molecular orbital energetics. We demonstrate our model performance on diverse complexes and in the discovery of molecules with target HOMO-LUMO gaps from a large 15,000 molecule design space in minutes rather than days that full DFT evaluation would require.","",""
75,"J. Ko, S. Baldassano, Po-Ling Loh, Konrad Paul Kording, B. Litt, D. Issadore","Machine learning to detect signatures of disease in liquid biopsies - a user's guide.",2018,"","","","",180,"2022-07-13 10:07:22","","10.1039/c7lc00955k","","",,,,,75,18.75,13,6,4,"New technologies that measure sparse molecular biomarkers from easily accessible bodily fluids (e.g. blood, urine, and saliva) are revolutionizing disease diagnostics and precision medicine. Microchip devices can measure more disease biomarkers with better sensitivity and specificity each year, but clinical interpretation of these biomarkers remains a challenge. Single biomarkers in 'liquid biopsy' often cannot accurately predict the state of a disease due to heterogeneity in phenotype and disease expression across individuals. To address this challenge, investigators are combining multiplexed measurements of different biomarkers that together define robust signatures for specific disease states. Machine learning is a useful tool to automatically discover and detect these signatures, especially as new technologies output increasing quantities of molecular data. In this paper, we review the state of the field of machine learning applied to molecular diagnostics and provide practical guidance to use this tool effectively and to avoid common pitfalls.","",""
62,"M. Shafiq, Xiangzhan Yu, A. Bashir, Hassan Nazeer Chaudhry, Dawei Wang","A machine learning approach for feature selection traffic classification using security analysis",2018,"","","","",181,"2022-07-13 10:07:22","","10.1007/s11227-018-2263-3","","",,,,,62,15.50,12,5,4,"","",""
61,"Unseok Lee, Sungyul Chang, G. A. Putra, Hyoungseok Kim, Dong Hwan Kim","An automated, high-throughput plant phenotyping system using machine learning-based plant segmentation and image analysis",2018,"","","","",182,"2022-07-13 10:07:22","","10.1371/journal.pone.0196615","","",,,,,61,15.25,12,5,4,"A high-throughput plant phenotyping system automatically observes and grows many plant samples. Many plant sample images are acquired by the system to determine the characteristics of the plants (populations). Stable image acquisition and processing is very important to accurately determine the characteristics. However, hardware for acquiring plant images rapidly and stably, while minimizing plant stress, is lacking. Moreover, most software cannot adequately handle large-scale plant imaging. To address these problems, we developed a new, automated, high-throughput plant phenotyping system using simple and robust hardware, and an automated plant-imaging-analysis pipeline consisting of machine-learning-based plant segmentation. Our hardware acquires images reliably and quickly and minimizes plant stress. Furthermore, the images are processed automatically. In particular, large-scale plant-image datasets can be segmented precisely using a classifier developed using a superpixel-based machine-learning algorithm (Random Forest), and variations in plant parameters (such as area) over time can be assessed using the segmented images. We performed comparative evaluations to identify an appropriate learning algorithm for our proposed system, and tested three robust learning algorithms. We developed not only an automatic analysis pipeline but also a convenient means of plant-growth analysis that provides a learning data interface and visualization of plant growth trends. Thus, our system allows end-users such as plant biologists to analyze plant growth via large-scale plant image data easily.","",""
47,"N. Ball, R. Brunner, A. Myers, N. E. Strand, S. Alberts, D. Tcheng, X. L. D. O. Astronomy, U. I. Urbana-Champaign, National Center for Supercomputing Applications, D. Physics","Robust Machine Learning Applied to Astronomical Data Sets. II. Quantifying Photometric Redshifts for Quasars Using Instance-based Learning",2006,"","","","",183,"2022-07-13 10:07:22","","10.1086/518362","","",,,,,47,2.94,5,10,16,"We apply instance-based machine learning in the form of a k-nearest neighbor algorithm to the task of estimating photometric redshifts for 55,746 objects spectroscopically classified as quasars in the Fifth Data Release of the Sloan Digital Sky Survey. We compare the results obtained to those from an empirical color-redshift relation (CZR). In contrast to previously published results using CZRs, we find that the instance-based photometric redshifts are assigned with no regions of catastrophic failure. Remaining outliers are simply scattered about the ideal relation, in a manner similar to the pattern seen in the optical for normal galaxies at redshifts z 1. The instance-based algorithm is trained on a representative sample of the data and pseudo-blind-tested on the remaining unseen data. The variance between the photometric and spectroscopic redshifts is σ2 = 0.123 ± 0.002 (compared to σ2 = 0.265 ± 0.006 for the CZR), and 54.9% ± 0.7%, 73.3% ± 0.6%, and 80.7% ± 0.3% of the objects are within Δz < 0.1, 0.2, and 0.3, respectively. We also match our sample to the Second Data Release of the Galaxy Evolution Explorer legacy data, and the resulting 7642 objects show a further improvement, giving a variance of σ2 = 0.054 ± 0.005, with 70.8% ± 1.2%, 85.8% ± 1.0%, and 90.8% ± 0.7% of objects within Δz < 0.1, 0.2, and 0.3. We show that the improvement is indeed due to the extra information provided by GALEX, by training on the same data set using purely SDSS photometry, which has a variance of σ2 = 0.090 ± 0.007. Each set of results represents a realistic standard for application to further data sets for which the spectra are representative.","",""
47,"Sujan Kumar Gonugondla, Mingu Kang, Naresh R Shanbhag","A Variation-Tolerant In-Memory Machine Learning Classifier via On-Chip Training",2018,"","","","",184,"2022-07-13 10:07:22","","10.1109/JSSC.2018.2867275","","",,,,,47,11.75,16,3,4,"This paper presents a robust deep in-memory machine learning classifier with a stochastic gradient descent (SGD)-based on-chip trainer using a standard 16-kB 6T SRAM array. The deep in-memory architecture (DIMA) enhances both energy efficiency and throughput over conventional digital architectures by reading multiple bits per bit line (BL) per read cycle and by employing mixed-signal processing in the periphery of the bit-cell array. Though these techniques improve the energy efficiency and latency, DIMA’s analog nature makes it sensitive to process, voltage, and temperature (PVT) variations, especially under reduced BL swings. On-chip training enables DIMA to adapt to chip-specific variations in PVT as well as data statistics, thereby further enhancing its energy efficiency. The 65-nm CMOS prototype IC demonstrates this improvement by realizing an on-chip trainable support vector machine. By learning chip-specific weights, on-chip training enables robust operation under reduced BL swing leading to a 2.4 times reduction in energy over an off-chip trained DIMA. The prototype IC in 65-nm CMOS consumes 42 pJ/decision at 32 M decisions/s, corresponding to 3.12 TOPS/W (1 OP = one  $8{-}b\times 8{-}b$  MAC) during inference, thereby achieving a reduction of 21 times in energy and 100 times in energy-delay product as compared with a conventional digital architecture. The energy overhead of training is <26% per decision for SGD batch sizes of 128 and higher.","",""
297,"Andrius Vabalas, E. Gowen, E. Poliakoff, A. Casson","Machine learning algorithm validation with a limited sample size",2019,"","","","",185,"2022-07-13 10:07:22","","10.1371/journal.pone.0224365","","",,,,,297,99.00,74,4,3,"Advances in neuroimaging, genomic, motion tracking, eye-tracking and many other technology-based data collection methods have led to a torrent of high dimensional datasets, which commonly have a small number of samples because of the intrinsic high cost of data collection involving human participants. High dimensional data with a small number of samples is of critical importance for identifying biomarkers and conducting feasibility and pilot work, however it can lead to biased machine learning (ML) performance estimates. Our review of studies which have applied ML to predict autistic from non-autistic individuals showed that small sample size is associated with higher reported classification accuracy. Thus, we have investigated whether this bias could be caused by the use of validation methods which do not sufficiently control overfitting. Our simulations show that K-fold Cross-Validation (CV) produces strongly biased performance estimates with small sample sizes, and the bias is still evident with sample size of 1000. Nested CV and train/test split approaches produce robust and unbiased performance estimates regardless of sample size. We also show that feature selection if performed on pooled training and testing data is contributing to bias considerably more than parameter tuning. In addition, the contribution to bias by data dimensionality, hyper-parameter space and number of CV folds was explored, and validation methods were compared with discriminable data. The results suggest how to design robust testing methodologies when working with small datasets and how to interpret the results of other studies based on what validation method was used.","",""
46,"A. Hammouri, M. Hammad, Mohammad M. Alnabhan, Fatima Alsarayrah","Software Bug Prediction using Machine Learning Approach",2018,"","","","",186,"2022-07-13 10:07:22","","10.14569/IJACSA.2018.090212","","",,,,,46,11.50,12,4,4,"Software Bug Prediction (SBP) is an important issue in software development and maintenance processes, which concerns with the overall of software successes. This is because predicting the software faults in earlier phase improves the software quality, reliability, efficiency and reduces the software cost. However, developing robust bug prediction model is a challenging task and many techniques have been proposed in the literature. This paper presents a software bug prediction model based on machine learning (ML) algorithms. Three supervised ML algorithms have been used to predict future software faults based on historical data. These classifiers are Naive Bayes (NB), Decision Tree (DT) and Artificial Neural Networks (ANNs). The evaluation process showed that ML algorithms can be used effectively with high accuracy rate. Furthermore, a comparison measure is applied to compare the proposed prediction model with other approaches. The collected results showed that the ML approach has a better performance.","",""
46,"T. Kirchner, J. Gröhl, L. Maier-Hein","Context encoding enables machine learning-based quantitative photoacoustics",2017,"","","","",187,"2022-07-13 10:07:22","","10.1117/1.JBO.23.5.056008","","",,,,,46,9.20,15,3,5,"Abstract. Real-time monitoring of functional tissue parameters, such as local blood oxygenation, based on optical imaging could provide groundbreaking advances in the diagnosis and interventional therapy of various diseases. Although photoacoustic (PA) imaging is a modality with great potential to measure optical absorption deep inside tissue, quantification of the measurements remains a major challenge. We introduce the first machine learning-based approach to quantitative PA imaging (qPAI), which relies on learning the fluence in a voxel to deduce the corresponding optical absorption. The method encodes relevant information of the measured signal and the characteristics of the imaging system in voxel-based feature vectors, which allow the generation of thousands of training samples from a single simulated PA image. Comprehensive in silico experiments suggest that context encoding-qPAI enables highly accurate and robust quantification of the local fluence and thereby the optical absorption from PA images.","",""
40,"Yue Zhao, M. K. Hryniewicki, Francesca Cheng, Boyang Fu, Xiaoyu Zhu","Employee Turnover Prediction with Machine Learning: A Reliable Approach",2018,"","","","",188,"2022-07-13 10:07:22","","10.1007/978-3-030-01057-7_56","","",,,,,40,10.00,8,5,4,"","",""
37,"I. Pashchenko, K. Sokolovsky, P. Gavras","Machine learning search for variable stars",2017,"","","","",189,"2022-07-13 10:07:22","","10.1093/mnras/stx3222","","",,,,,37,7.40,12,3,5,"Photometric variability detection is often considered as a hypothesis testing problem: an object is variable if the null-hypothesis that its brightness is constant can be ruled out given the measurements and their uncertainties. Uncorrected systematic errors limit the practical applicability of this approach to high-amplitude variability and well-behaving data sets. Searching for a new variability detection technique that would be applicable to a wide range of variability types while being robust to outliers and underestimated measurement uncertainties, we propose to consider variability detection as a classification problem that can be approached with machine learning. We compare several classification algorithms: Logistic Regression (LR), Support Vector Machines (SVM), k-Nearest Neighbors (kNN) Neural Nets (NN), Random Forests (RF) and Stochastic Gradient Boosting classifier (SGB) applied to 18 features (variability indices) quantifying scatter and/or correlation between points in a light curve. We use a subset of OGLE-II Large Magellanic Cloud (LMC) photometry (30265 light curves) that was searched for variability using traditional methods (168 known variable objects identified) as the training set and then apply the NN to a new test set of 31798 OGLE-II LMC light curves. Among 205 candidates selected in the test set, 178 are real variables, 13 low-amplitude variables are new discoveries. We find that the considered machine learning classifiers are more efficient (they find more variables and less false candidates) compared to traditional techniques that consider individual variability indices or their linear combination. The NN, SGB, SVM and RF show a higher efficiency compared to LR and kNN.","",""
90,"Nagdev Amruthnath, Tarun Gupta","A research study on unsupervised machine learning algorithms for early fault detection in predictive maintenance",2018,"","","","",190,"2022-07-13 10:07:22","","10.1109/IEA.2018.8387124","","",,,,,90,22.50,45,2,4,"The area of predictive maintenance has taken a lot of prominence in the last couple of years due to various reasons. With new algorithms and methodologies growing across different learning methods, it has remained a challenge for industries to adopt which method is fit, robust and provide most accurate detection. Fault detection is one of the critical components of predictive maintenance; it is very much needed for industries to detect faults early and accurately. In a production environment, to minimize the cost of maintenance, sometimes it is required to build a model with minimal or no historical data. In such cases, unsupervised learning would be a better option model building. In this paper, we have chosen a simple vibration data collected from an exhaust fan, and have fit different unsupervised learning algorithms such as PCA T2 statistic, Hierarchical clustering, K-Means, Fuzzy C-Means clustering and model-based clustering to test its accuracy, performance, and robustness. In the end, we have proposed a methodology to benchmark different algorithms and choosing the final model.","",""
94,"Qiang Zhu, A. Samanta, Bingxi Li, R. Rudd, T. Frolov","Predicting phase behavior of grain boundaries with evolutionary search and machine learning",2017,"","","","",191,"2022-07-13 10:07:22","","10.1038/s41467-018-02937-2","","",,,,,94,18.80,19,5,5,"","",""
0,"E. Kondrateva, Polina Belozerova, M. Sharaev, Evgeny Burnaev, A. Bernstein, I. Samotaeva","Machine learning models reproducibility and validation for MR images recognition",2020,"","","","",192,"2022-07-13 10:07:22","","10.1117/12.2559525","","",,,,,0,0.00,0,6,2,"In the present work, we introduce a data processing and analysis pipeline, which ensures the reproducibility of machine learning models chosen for MR image recognition. The proposed pipeline is applied to solve the binary classification problems: epilepsy and depression diagnostics based on vectorized features from MR images. This model is then assessed in terms of classification performance, robustness and reliability of the results, including predictive accuracy on unseen data. The classification performance achieved with our approach compares favorably to ones reported in the literature, where usually no thorough model evaluation is performed.","",""
65,"N. Ball, R. Brunner, A. Myers, D. E. U. O. I. A. Urbana-Champaign, National Center for Supercomputing Applications","Robust machine learning applied to astronomical data sets. I. Star-galaxy classification of the sloan digital sky survey DR3 using decision trees",2006,"","","","",193,"2022-07-13 10:07:22","","10.1086/507440","","",,,,,65,4.06,13,5,16,"We provide classifications for all 143 million nonrepeat photometric objects in the Third Data Release of the SDSS using decision trees trained on 477,068 objects with SDSS spectroscopic data. We demonstrate that these star/galaxy classifications are expected to be reliable for approximately 22 million objects with r 20. The general machine learning environment Data-to-Knowledge and supercomputing resources enabled extensive investigation of the decision tree parameter space. This work presents the first public release of objects classified in this way for an entire SDSS data release. The objects are classified as either galaxy, star, or nsng (neither star nor galaxy), with an associated probability for each class. To demonstrate how to effectively make use of these classifications, we perform several important tests. First, we detail selection criteria within the probability space defined by the three classes to extract samples of stars and galaxies to a given completeness and efficiency. Second, we investigate the efficacy of the classifications and the effect of extrapolating from the spectroscopic regime by performing blind tests on objects in the SDSS, 2dFGRS, and 2QZ surveys. Given the photometric limits of our spectroscopic training data, we effectively begin to extrapolate past our star-galaxy training set at r ~ 18. By comparing the number counts of our training sample with the classified sources, however, we find that our efficiencies appear to remain robust to r ~ 20. As a result, we expect our classifications to be accurate for 900,000 galaxies and 6.7 million stars and remain robust via extrapolation for a total of 8.0 million galaxies and 13.9 million stars.","",""
43,"Saiyed Kashif Shaukat, V. Ribeiro","RansomWall: A layered defense system against cryptographic ransomware attacks using machine learning",2018,"","","","",194,"2022-07-13 10:07:22","","10.1109/COMSNETS.2018.8328219","","",,,,,43,10.75,22,2,4,"Recent worldwide cybersecurity attacks caused by Cryptographic Ransomware infected systems across countries and organizations with millions of dollars lost in paying extortion amounts. This form of malicious software takes user files hostage by encrypting them and demands a large ransom payment for providing the decryption key. Signature-based methods employed by Antivirus Software are insufficient to evade Ransomware attacks due to code obfuscation techniques and creation of new polymorphic variants everyday. Generic Malware Attack vectors are also not robust enough for detection as they do not completely track the specific behavioral patterns shown by Cryptographic Ransomware families. This work based on analysis of an extensive dataset of Ran-somware families presents RansomWall, a layered defense system for protection against Cryptographic Ransomware. It follows a Hybrid approach of combined Static and Dynamic analysis to generate a novel compact set of features that characterizes the Ransomware behavior. Presence of a Strong Trap Layer helps in early detection. It uses Machine Learning for unearthing zero-day intrusions. When initial layers of RansomWall tag a process for suspicious Ransomware behavior, files modified by the process are backed up for preserving user data until it is classified as Ransomware or Benign. We implemented RansomWall for Microsoft Windows operating system (the most attacked OS by Cryptographic Ransomware) and evaluated it against 574 samples from 12 Cryptographic Ransomware families in real-world user environments. The testing of RansomWall with various Machine Learning algorithms evaluated to 98.25% detection rate and near-zero false positives with Gradient Tree Boosting Algorithm. It also successfully detected 30 zero-day intrusion samples (having less than 10% detection rate with 60 Security Engines linked to VirusTotal).","",""
35,"Vasisht Duddu","A Survey of Adversarial Machine Learning in Cyber Warfare",2018,"","","","",195,"2022-07-13 10:07:22","","10.14429/DSJ.68.12371","","",,,,,35,8.75,35,1,4,"The changing nature of warfare has seen a paradigm shift from the conventional to asymmetric, contactless warfare such as information and cyber warfare. Excessive dependence on information and communication technologies, cloud infrastructures, big data analytics, data-mining and automation in decision making poses grave threats to business and economy in adversarial environments. Adversarial machine learning is a fast growing area of research which studies the design of Machine Learning algorithms that are robust in adversarial environments. This paper presents a comprehensive survey of this emerging area and the various techniques of adversary modelling. We explore the threat models for Machine Learning systems and describe the various techniques to attack and defend them. We present privacy issues in these models and describe a cyber-warfare test-bed to test the effectiveness of the various attack-defence strategies and conclude with some open problems in this area of research. ","",""
241,"B. Goldstein, A. Navar, R. Carter","Moving beyond regression techniques in cardiovascular risk prediction: applying machine learning to address analytic challenges",2016,"","","","",196,"2022-07-13 10:07:22","","10.1093/eurheartj/ehw302","","",,,,,241,40.17,80,3,6,"Abstract Risk prediction plays an important role in clinical cardiology research. Traditionally, most risk models have been based on regression models. While useful and robust, these statistical methods are limited to using a small number of predictors which operate in the same way on everyone, and uniformly throughout their range. The purpose of this review is to illustrate the use of machine-learning methods for development of risk prediction models. Typically presented as black box approaches, most machine-learning methods are aimed at solving particular challenges that arise in data analysis that are not well addressed by typical regression approaches. To illustrate these challenges, as well as how different methods can address them, we consider trying to predicting mortality after diagnosis of acute myocardial infarction. We use data derived from our institution's electronic health record and abstract data on 13 regularly measured laboratory markers. We walk through different challenges that arise in modelling these data and then introduce different machine-learning approaches. Finally, we discuss general issues in the application of machine-learning methods including tuning parameters, loss functions, variable importance, and missing data. Overall, this review serves as an introduction for those working on risk modelling to approach the diffuse field of machine learning.","",""
33,"Ved P. Kafle, Y. Fukushima, P. Martinez-Julia, T. Miyazawa","Consideration On Automation of 5G Network Slicing with Machine Learning",2018,"","","","",197,"2022-07-13 10:07:22","","10.23919/ITU-WT.2018.8597639","","",,,,,33,8.25,8,4,4,"Machine learning has the capability to provide simpler solutions to complex problems by analyzing a huge volume of data in a short time, learning for adapting its functionality to dynamically changing environments, and predicting near future events with reasonably good accuracy. The 5G communication networks are getting complex due to emergence of unprecedentedly huge number of new connected devices and new types of services. Moreover, the requirements of creating virtual network slices suitable to provide optimal services for diverse users and applications are posing challenges to the efficient management of network resources, processing information about a huge volume of traffic, staying robust against all potential security threats, and adaptively adjustment of network functionality for time-varying workload. In this paper, we introduce about the envisioned 5G network slicing and elaborate the necessity of automation of network functions for the design, construction, deployment, operation, control and management of network slices. We then revisit the machine learning techniques that can be applied for the automation of network functions. We also discuss the status of artificial intelligence and machine learning related activities being progressed in standards development organizations and industrial forums.","",""
34,"V. Saccá, A. Sarica, F. Novellino, S. Barone, T. Tallarico, E. Filippelli, A. Granata, C. Chiriaco, R. Bruno Bossio, P. Valentino, A. Quattrone","Evaluation of machine learning algorithms performance for the prediction of early multiple sclerosis from resting-state FMRI connectivity data",2018,"","","","",198,"2022-07-13 10:07:22","","10.1007/s11682-018-9926-9","","",,,,,34,8.50,3,11,4,"","",""
24,"Lal Hussain, W. Aziz, Sharjil Saeed, Saima Rathore, M. Rafique","Automated Breast Cancer Detection Using Machine Learning Techniques by Extracting Different Feature Extracting Strategies",2018,"","","","",199,"2022-07-13 10:07:22","","10.1109/TrustCom/BigDataSE.2018.00057","","",,,,,24,6.00,5,5,4,"This Breast Cancer in women is the most frequency diagnosed and second leading cause of cancer deaths. Due to complex nature of microcalcification and masses, radiologist fail to properly diagnose breast cancer. In past researchers developed Computer aided diagnosis (CAD) systems that help the radiologist to detect abnormalities in an efficient manner. In this research, we have employed robust Machine learning classification techniques such as Support vector machine (SVM) kernels and Decision Tree to distinguish cancer mammograms from normal subjects. Different features are proposed such as texture, morphological entropy based, scale invariant feature transform (SIFT), and elliptic Fourier descriptors (EFDs). These features are passed as input to ML classifiers. Jack-knife 10-fold cross validation was used and performance evaluated in term of specificity, sensitivity, Positive predive value (PPV), negative predictive value (NPV), false positive rate (FPR) and receive operating curve (ROC). The highest performance based on single feature extracting strategy was obtained using Bayesian approach with texture and EFDs features, and SVM RBF and Gaussian kernels with EFDs features whereas highest AUC with single feature was obtained using Bayesian approach by extracting texture, morphological, EFDs and entropy features and SVM RBF and Gaussian kernels with EFDs features.","",""
29,"M. Bray, Anne E Carpenter","Quality Control for High-Throughput Imaging Experiments Using Machine Learning in Cellprofiler.",2018,"","","","",200,"2022-07-13 10:07:22","","10.1007/978-1-4939-7357-6_7","","",,,,,29,7.25,15,2,4,"","",""
